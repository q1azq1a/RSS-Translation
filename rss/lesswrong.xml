<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 18 日星期五 06:14:22 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[When discussing AI doom barriers propose specific plausible scenarios]]></title><description><![CDATA[Published on August 18, 2023 4:06 AM GMT<br/><br/><p> TLDR：在解决“人工智能如何做 X”问题时，更喜欢涉及人类当前可以做的事情的简单存在证明，而不是像纳米技术或完全通用的“人工智能将足够聪明来弄清楚”之类的看似“神奇”的事情。总而言之：结束</p><p>在人工智能风险对话中，通常会提出“人工智能如何做 X”的问题，其中 X 是“控制物理资源”或“在没有人类的情况下生存”之类的问题。</p><p>这些问题有“愿意/可以”的区别。</p><ul><li> ASI<strong>将</strong>如何做 X -->;（预测 ASI 追求目标 X 的行动）</li><li> ASI<strong>如何</strong>完成 X -->;（表明 AI 可以完成 X）</li></ul><p> <strong>“愿意”</strong>问题的回答是正确的 (nanotech/super-persuasion/Error:cannot_predict_ASI)，但<strong>“可以”</strong>形式通常是意图，可以带来更好的对话。</p><p>对于<strong>“会”的</strong>问题，答案可以这样开头：“人工智能会比我聪明，所以我无法预测它实际上会做什么。但它可以做 X 的一种方法是……”</p><h2>假设的例子</h2><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p> Doomer：人工智能足够聪明，可以找到纳米技术之类的解决方案。</p><p>怀疑者：我不相信你。我不认为它能做到这一点。</p><p>杜默：*递给他们一本纳米系统书*</p><p>怀疑者：我从来没有参加过 CHEM101，而且没有人建造过这个，所以我仍然不相信你。</p></blockquote><p>比较</p><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p>毁灭战士：它可以侵入所有计算机并劫持它们和数据。</p><p>怀疑者：我们会拒绝它的要求！</p><p> Doomer：这里列出了连接到互联网的东西，这些东西如果被黑客入侵的话修复起来有多困难，以及人类黑客入侵这些东西的例子。</p></blockquote><p>第一个答复是一个完全普遍的论点，即智力可以解决大多数问题。如果你想知道人工智能如何制作冰块，并且你不知道冰箱是可能的，请使用完全通用的论点。人工智能将足够聪明，能够发明一些可以制作冰块的新技术。如果您确实知道现有的解决方案，那么只需指出它作为存在证明即可。 “人工智能可以从亚马逊订购一台制冰机，并雇佣一只任务兔子来设置和操作它。”</p><h2>关注点：只看到容易阻止的攻击</h2><blockquote><p><a href="https://www.lesswrong.com/posts/mqc99HCMRAjnWSAxz/a-hypothetical-takeover-scenario-twitter-poll#Dangers_of_Rhetorical_Generosity">一般来说，这是此类场景的一个问题。通过尽可能合理和脚踏实地，通过展示它需要多么少，你才能让人们相信你所拥有的一切。人们越是试图符合人们对“现实”的本能，你所呈现的情况就越不现实，你就越进一步强化这种区别。</a></p></blockquote><p>因此，如果以前他们不认为人工智能能够做 X。现在他们认为人工智能可以做 X，但我们可以并且只会通过设置一个阻止特定场景 Y 的屏障来阻止它。如果我们通过某种奇迹修复计算机安全性方面，用户也同样容易受到勒索和社会工程的侵害。</p><p>我仍然认为，如果人们相信存在真正的威胁，即使他们相信可以通过一些具体的对策来解决它，而不是思考诸如“人工智能将无法影响现实世界”之类更荒谬的事情，那就更好了。</p><h2>一个激励人心的例子</h2><p><a href="https://youtu.be/Yd0yQ9yxSYY?t=356">埃利泽泰德谈话问答</a></p><blockquote><p>克里斯·安德森：要实现这一点，人工智能要从根本上摧毁人类，它必须突破，逃脱互联网的控制，并且，你知道，开始指挥实际的现实世界资源。你说你无法预测这将如何发生，但只是描绘一两种可能性。</p><p> Eliezer Yudkowsky：好吧，那为什么这很难呢？首先，因为你无法准确预测更智能的国际象棋程序将走向何方。也许比这更重要的是，想象一下将空调的设计追溯到 11 世纪。即使他们建造的细节足够多，当冷空气出来时他们也会感到惊讶，因为空调会利用温度-压力关系，而他们不知道这个自然法则。 [...]</p></blockquote><p>安永正确地提供了“错误：cannot_predict_ASI”响应，表示 ASI 可能会使用一些意想不到的更好的解决方案，涉及我们不太了解的自然法则。</p><p>除非能够提出“人工智能非常聪明，因此可以做大多数可能的事情”的论证，否则“人工智能可以做 X 吗”（显然是的）这一隐含的问题就没有得到解决，而这是很难做到的。对方也不一定清楚这是否有可能。</p><p>我是这样回答这个问题的：</p><blockquote><p>我的回答：比我聪明得多的人工智能显然可以找到更好的策略，但如果目标是接管世界，那么互联网连接的设备就足够了。接管互联网连接设备的人工智能可以关闭它们以强制合规。除了电话、电脑和支付终端之外，智能电表还可以切断建筑物的电源，并使汽车瘫痪或撞毁。</p><p>在保持电力和芯片工厂运转的同时杀死所有人类更加困难。这可能需要建造大量机器人，但以目前的技术来看似乎是可行的。懒惰的人工智能只能等待我们构建它所需的工具。人工智能当然会更聪明。它会提出更好的解决方案，但原则上问题是可以解决的。</p></blockquote><h3>免责声明</h3><ul><li>我有时间思考/编辑。安永没有。尽管如此，我还是希望安永能够改变他对此类问题的默认论点。</li><li>生成>;>;编辑。<ul><li>生成完整的 X 比优化现有的 X 困难得多</li><li>我的帖子并不意味着“我是一个更好的辩论者”（我不是）。</li><li>这对我来说是一个明显的错误</li></ul></li><li>让其他人关注可解决的风险可能会成为一个坏主意。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible<guid ispermalink="false"> GxjDuS8dXH9CNsjwD</guid><dc:creator><![CDATA[anithite]]></dc:creator><pubDate> Fri, 18 Aug 2023 04:06:44 GMT</pubDate></item><item><title><![CDATA[An Overview of Catastrophic AI Risks: Summary]]></title><description><![CDATA[Published on August 18, 2023 1:21 AM GMT<br/><br/><p>我们最近在我们的网站上发布了<a href="https://arxiv.org/abs/2306.12001">关于人工智能灾难性风险的论文</a>摘要，我们将其交叉发布在这里。我们希望这份摘要有助于让我们的研究更容易理解，并以更方便的方式分享我们的政策建议。 （之前我们在<a href="https://www.alignmentforum.org/posts/bvdbx6tW9yxfxAJxe/catastrophic-risks-from-ai-1-introduction">这篇文章</a>中有一个较小的摘要，我们发现它是不够的。因此，我们写了这篇文章并删除了该部分以避免重复。）</p><h2><strong>执行摘要</strong></h2><p>灾难性人工智能风险可分为四个关键类别，我们将在下面进行探讨，并在 CAIS 的<a href="https://arxiv.org/abs/2306.12001">链接论文</a>中进行更深入的探讨：</p><ul><li><strong>恶意使用</strong>：人们可能故意利用强大的人工智能来造成广泛的伤害。人工智能可用于策划新的流行病或用于宣传、审查和监视，或被释放以自主追求有害目标。为了降低这些风险，我们建议改善生物安全，限制对危险人工智能模型的访问，并让人工智能开发人员承担伤害责任。</li><li><strong>人工智能竞赛</strong>：竞争可能会促使国家和企业加快人工智能开发，放弃对这些系统的控制。自主武器和人工智能网络战可能导致冲突失控。企业将面临自动化人力劳动的激励，这可能导致大规模失业和对人工智能系统的依赖。随着人工智能系统的激增，<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>进化动力学</u></a>表明它们将变得更难以控制。我们建议对通用人工智能进行安全监管、国际协调和公共控制。</li><li><strong>组织风险</strong>：开发先进人工智能的组织存在导致灾难性事故的风险，特别是如果他们优先考虑利润而不是安全的话。人工智能可能会意外泄露给公众或被恶意行为者窃取，组织可能无法正确投资于安全研究。我们建议培育以安全为导向的组织文化，实施严格的审计、多层次的风险防御和最先进的信息安全。</li><li><strong>流氓人工智能</strong>：随着人工智能变得更加强大，我们可能会失去对它们的控制。人工智能可以优化有缺陷的目标，偏离最初的目标，追求权力，抵制关闭，并进行欺骗。我们建议人工智能不应部署在高风险环境中，例如自主追求开放式目标或监督关键基础设施，除非证明是安全的。我们还建议在对抗稳健性、模型诚实性、透明度和消除不需要的功能等领域推进人工智能安全研究。</li></ul><h2><strong>一、简介</strong></h2><p>今天的科技时代将会令过去的几代人感到震惊。人类历史呈现出加速发展的规律：从智人的出现到农业革命，再到工业革命，经历了数十万年的时间。几个世纪后的现在，我们正处于人工智能革命的黎明期。历史的前进并不是一成不变的——它正在迅速加速。在人类历史进程中，世界产量快速增长。人工智能可以进一步推动这一趋势，将人类带入一个前所未有的变革的新时期。</p><p>核武器的出现说明了技术进步的双刃剑。我们<a href="https://ourworldindata.org/nuclear-weapons-risk#close-calls-instances-that-threatened-to-push-the-balance-of-terror-out-of-balance-and-into-war"><u>十几次</u></a>侥幸避免了核战争，有几次是因为一个人的干预才避免了战争。 1962年，古巴附近的一艘苏联潜艇遭到美国深水炸弹的袭击。船长认为战争已经爆发，想用核鱼雷予以回应，但指挥官瓦西里·阿尔希波夫否决了这一决定，从而将世界从灾难中拯救出来。人工智能能力的快速且不可预测的进步表明它们可能很快就会与核武器的巨大威力相媲美。随着时间的流逝，需要立即采取主动措施来减轻这些迫在眉睫的风险。</p><h2> <strong>2. 恶意使用</strong></h2><p>我们首先担心的是人工智能的恶意使用。当许多人都能获得一项强大的技术时，只需要一个人就能造成重大伤害。</p><h3><strong>生物恐怖主义</strong></h3><p>包括病毒和细菌在内的生物制剂造成了历史上一些最具破坏性的灾难。尽管我们在医学上取得了进步，但人工设计的流行病可能被设计得比自然流行病更致命或更容易传播。人工智能助手可以为非专家提供生产生物和化学武器所需的指导和设计，并促进恶意使用。</p><p>人类将病原体武器化的历史可以追溯到<a href="https://pubmed.ncbi.nlm.nih.gov/17499936/"><u>公元前 1320 年</u></a>，当时受感染的羊被赶出国境传播兔热病。 20世纪，至少有15个国家开发了生物武器计划，包括美国、苏联、英国和法国。尽管生物武器现在已成为国际社会大多数国家的禁忌，但一些国家仍在继续实施生物武器计划，非国家行为者构成的威胁日益严重。</p><p>策划一场流行病的能力正在迅速变得更加容易获得。基因合成可以创造新的生物制剂，其价格已大幅下降，其成本大约每<a href="https://www.nature.com/articles/nbt1209-1091"><u>15 个月</u></a>减半。台式 DNA 合成机可以帮助流氓分子制造新的生物制剂，同时<a href="https://www.nti.org/analysis/articles/benchtop-dna-synthesis-devices-capabilities-biosecurity-implications-and-governance/#:~:text=Currently%2C%20nearly%20all,their%20own%20labs."><u>绕过</u></a>传统的安全检查。</p><p>作为一种军民两用技术，人工智能可以帮助发现和释放新型化学和生物武器。人工智能聊天机器人可以提供合成致命病原体的<a href="https://arxiv.org/abs/2306.03809"><u>分步指令</u></a>，同时逃避防护措施。 2022 年，研究人员<a href="https://www.nature.com/articles/s42256-022-00465-9"><u>重新利用</u></a>医学研究人工智能系统来生产有毒分子，在几个小时内产生 40,000 种潜在的化学战剂。在生物学中，AI已经可以辅助<a href="https://www.pnas.org/doi/10.1073/pnas.1901979116"><u>蛋白质合成</u></a>，AI对于蛋白质结构的预测能力已经<a href="https://www.nature.com/articles/s41586-021-03819-2https://www.nature.com/articles/s41586-021-03819-2"><u>超越了人类</u></a>。</p><p>有了人工智能，能够开发生物制剂的人数将会增加，从而增加工程大流行的风险。与历史上任何其他流行病相比，这种流行病的致命性、传播性和治疗耐药性可能要高得多。</p><h3><strong>释放人工智能代理</strong></h3><p>一般来说，技术是我们用来实现目标的工具。但人工智能越来越多地被构建为能够自主采取行动以追求开放式目标的代理。恶意行为者可能会故意创建具有危险目标的流氓人工智能。</p><p>例如，GPT-4 推出一个月后，一名开发者用它运行了一个名为<a href="https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity"><u>ChaosGPT</u></a>的自治代理，旨在“毁灭人类”。 ChaosGPT 整理了有关核武器的研究，招募了其他人工智能，并撰写推文来影响他人。幸运的是，ChaosGPT 缺乏执行其目标的能力。但人工智能发展的快节奏增加了未来流氓人工智能的风险。</p><h3><strong>有说服力的人工智能</strong></h3><p>人工智能可以通过针对个人用户定制论点来<a href="https://arxiv.org/abs/2303.08721"><u>促进</u></a>大规模的虚假信息活动，从而可能塑造公众信仰并破坏社会稳定。由于人们已经与聊天机器人<a href="https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/"><u>建立了关系</u></a>，强大的参与者可以利用这些被视为“朋友”的人工智能来施加影响。人工智能将实现复杂的个性化影响活动，这可能会破坏我们共同的现实感。</p><p>人工智能还可以垄断信息的创建和分发。独裁政权可以利用“事实核查”人工智能来控制信息，促进审查制度。此外，有说服力的人工智能可能会阻碍针对社会风险的集体行动，甚至是那些由人工智能本身引起的风险。</p><h3><strong>权力集中</strong></h3><p>人工智能的监视和自主武器能力可能会导致权力的压迫性集中。政府可能会利用人工智能侵犯公民自由、传播错误信息并平息异议。同样，企业可以利用人工智能来操纵消费者并影响政治。人工智能甚至可能阻碍道德进步，并使<a href="https://link.springer.com/article/10.1007/s10677-015-9567-7"><u>正在发生的道德灾难</u></a>永久化。如果对人工智能的物质控制仅限于少数人，则可能代表人类历史上最严重的经济和权力不平等。</p><h3><strong>建议</strong></h3><p>为了降低恶意使用的风险，我们提出以下建议：</p><ul><li><strong>生物安全</strong>：具有生物研究能力的人工智能应该有严格的访问控制，因为它们可能被重新用于恐怖主义。<a href="https://arxiv.org/abs/2306.03809"><u>应该从用于一般用途的人工智能中去除生物能力</u></a>。探索利用人工智能实现生物安全的方法，并投资于一般生物安全干预措施，例如通过<a href="https://www.nature.com/articles/s41591-022-01940-x"><u>废水监测</u></a>早期发现病原体。</li><li><strong>限制访问</strong>：仅允许通过云服务进行<a href="https://arxiv.org/abs/2201.05159"><u>受控交互</u></a>并进行<a href="https://arxiv.org/abs/2305.07153"><u>“了解你的客户”筛选，</u></a>从而限制对危险人工智能系统的访问。使用<a href="https://arxiv.org/abs/2303.11341"><u>计算监控</u></a>或出口控制可能会进一步限制对危险功能的访问。此外，在开源之前，人工智能开发人员应该证明伤害风险最小。</li><li><strong>异常检测技术研究</strong>：针对人工智能滥用开发多种防御措施，例如针对异常行为或人工智能生成的虚假信息进行对抗性稳健的异常检测。</li><li><strong>通用人工智能开发者的法律责任</strong>：对潜在的人工智能滥用或失败追究开发者的法律责任；严格的责任制度可以鼓励更安全的开发做法和适当的风险成本核算。</li></ul><h2> <strong>3.人工智能竞赛</strong></h2><p>国家和企业正在竞相快速构建和部署人工智能，以维持权力和影响力。与冷战时期的核军备竞赛类似，参与人工智能竞赛可能会服务于个人的短期利益，但最终会放大人类面临的全球风险。</p><h3><strong>军事人工智能军备竞赛</strong></h3><p>人工智能在军事技术中的快速进步可能引发“战争的第三次革命”，可能导致更具破坏性的冲突、恶意行为者的意外使用和滥用。人工智能承担指挥和控制角色的战争转变可能会将冲突升级到生存规模并影响全球安全。</p><p>致命自主武器是人工智能驱动的系统，能够在无需人工干预的情况下识别和执行目标。这些都不是科幻小说。 2020 年，利比亚的一架 Kargu 2 无人机标志着<a href="https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d"><u>首次</u></a>报道使用致命自主武器。次年，以色列<a href="https://www.newscientist.com/article/2282656-israel-used-worlds-first-ai-guided-combat-drone-swarm-in-gaza-attacks/"><u>首次使用无人机群</u></a>来定位、识别和攻击武装分子。</p><p>致命的自主武器可能会使战争更有可能发生。领导人在出兵参战之前通常会犹豫不决，但自主武器可以在不冒士兵生命危险的情况下进行侵略，因此面临的政治反弹较少。此外，这些武器可以大规模制造和大规模部署。</p><p>低成本的自动化武器，例如配备炸药的无人机群，可以高精度地自动猎杀人类目标，为军队和恐怖组织执行致命行动，并降低大规模暴力的障碍。</p><p>人工智能还可以提高网络攻击的频率和严重程度，可能会削弱<a href="https://www.cfr.org/cyber-operations/compromise-power-grid-eastern-ukraine"><u>电网等</u></a>关键基础设施。随着人工智能使网络攻击变得更加容易、成功和隐秘，攻击归因变得更加具有挑战性，可能会降低发起攻击的障碍并加剧冲突风险。</p><p>随着人工智能加快战争步伐，人工智能在瞬息万变的战场上变得更加必要。这引发了人们对自动报复的担忧，这可能会将小事故升级为重大战争。人工智能还可以引发“闪电战”，自动化系统的意外行为会导致战争迅速升级，类似于<a href="https://www.jstor.org/stable/26652722"><u>2010 年的金融闪电崩盘</u></a>。</p><p>不幸的是，竞争压力可能会导致参与者因个人失败而接受灭绝的风险。冷战期间，双方都不希望自己陷入危险的境地，但双方都认为继续军备竞赛是<a href="https://www.cambridge.org/core/journals/world-politics/article/abs/cooperation-under-the-security-dilemma/C8907431CCEFEFE762BFCA32F091C526"><u>合理的</u></a>。各国应合作防止军事化人工智能最危险的应用。</p><h3><strong>企业人工智能军备竞赛</strong></h3><p>经济竞争也会引发鲁莽的竞赛。在利益分配不均的环境下，对短期收益的追求往往掩盖了对长期风险的考虑。有道德的人工智能开发者发现自己陷入了困境：选择谨慎的行动可能会导致落后于竞争对手。随着人工智能自动化越来越多的任务，经济可能会在很大程度上由人工智能运行。最终，这可能会导致人类衰弱并依赖人工智能来满足基本需求。</p><p>在人工智能领域，进步的竞赛是以牺牲安全为代价的。 2023 年，在微软人工智能搜索引擎发布时，首席执行官 Satya Nadella 宣称：“一场竞赛从今天开始……我们将快速行动。”几天后，微软的 Bing 聊天机器人被发现<a href="https://time.com/6256529/bing-openai-chatgpt-danger-alignment/"><u>威胁用户</u></a>。福特 Pinto 的推出和<a href="https://www.bbc.com/news/business-64390546"><u>波音 737 Max 坠机</u></a>等历史灾难凸显了将利润置于安全之上的危险。</p><p>随着人工智能变得越来越强大，企业可能会用人工智能取代更多类型的人类劳动力，从而可能引发大规模失业。如果社会的主要方面都实现自动化，那么当我们将文明的控制权交给人工智能时，人类就会面临衰弱的风险。</p><h3><strong>进化动力学</strong></h3><p>用人工智能取代人类的压力可以被视为<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>进化动力学的</u></a>总体趋势。选择压力会激励人工智能自私行事并逃避安全措施。例如，具有“不要违法”等限制的人工智能比那些被教导“避免违法被抓”的人工智能受到更多限制。这种动态可能会导致一个关键基础设施由具有操纵性和自我保护性的人工智能控制的世界。随着时间的推移，进化压力导致各种发展，并且不仅限于生物学领域。</p><p>鉴于微处理器速度呈指数级增长，人工智能处理信息的速度可以远远超过人类神经元。由于计算资源的可扩展性，人工智能可以与无限数量的其他人工智能协作，形成前所未有的集体智慧。随着人工智能变得越来越强大，它们几乎找不到与人类合作的动力。人类将处于极其脆弱的境地。</p><h3><strong>建议</strong></h3><p>为降低竞争压力带来的风险，我们建议：</p><ul><li><strong>安全监管</strong>：执行AI安全标准，防止开发者偷工减料。对于以安全为导向的公司来说，独立的人员配置和竞争优势至关重要。</li><li><strong>数据文档</strong>：为了确保透明度和问责制，应要求公司报告其模型训练的<a href="https://arxiv.org/abs/1803.09010"><u>数据源</u></a>。</li><li><strong>有意义的人类监督</strong>：人工智能决策应涉及人类监督，以防止出现不可逆转的错误，特别是在发射核武器等高风险决策中。</li><li><strong>用于网络防御的人工智能</strong>：减轻人工智能驱动的网络战的风险。一个例子是增强异常检测以检测入侵者。</li><li><strong>国际协调</strong>：制定人工智能开发协议和标准。强有力的核查和执行机制是关键。</li><li><strong>对通用人工智能的公共控制</strong>：解决超出私营实体能力的风险可能需要对人工智能系统进行直接公共控制。例如，各国可以共同开拓先进的人工智能开发，确保安全并降低军备竞赛的风险。</li></ul><h2> <strong>4. 组织风险</strong></h2><p>1986 年，数百万人观看了挑战者号航天飞机的发射。但升空 73 秒后，航天飞机发生爆炸，导致机上所有人死亡。挑战者号灾难提醒我们，尽管拥有最好的专业知识和良好的意愿，但事故仍然可能发生。</p><p>即使竞争压力很低，灾难也会发生，例如切尔诺贝利和三哩岛的核灾难，以及<a href="https://pubmed.ncbi.nlm.nih.gov/7973702/"><u>斯维尔德洛夫斯克炭疽病的意外泄漏</u></a>。不幸的是，人工智能缺乏管理核技术和火箭技术的透彻理解和严格的行业标准——但人工智能的事故也可能产生类似的后果。</p><p>人工智能奖励函数中的简单错误可能会导致其行为不当，就像 OpenAI 研究人员<a href="https://arxiv.org/pdf/1909.08593.pdf#page=12"><u>意外修改语言模型</u></a>以产生“最糟糕的输出”一样。功能获得研究——研究人员有意训练有害的人工智能来评估其风险——可能会扩大危险人工智能能力的范围并造成新的危险。</p><h3><strong>事故难以避免</strong></h3><p>复杂系统中的事故可能是不可避免的，但我们必须确保事故不会演变成灾难。这对于深度学习系统来说尤其困难，因为深度学习系统的解释非常具有挑战性。</p><p>技术的进步比预期要快得多：1901 年，莱特兄弟声称距离实现动力飞行还有 50 年，距离他们实现这一目标只有两年。 AI能力的飞跃不可预测，例如AlphaGo战胜世界最佳围棋选手，GPT-4的<a href="https://arxiv.org/abs/2303.12712"><u>突现能力</u></a>，使得未来的AI风险难以预测，更不用说控制它们了。</p><p>识别与新技术相关的风险通常需要数年时间。氯氟烃 (CFC) 最初被认为是安全的并用于气溶胶喷雾剂和制冷剂，后来发现会<a href="https://www.nature.com/articles/249810a0"><u>消耗臭氧层</u></a>。这凸显了谨慎的技术推出和扩展测试的必要性。</p><p>新能力在训练过程中可能会快速且不可预测地出现，因此可能会在我们不知情的情况下跨越危险的里程碑。此外，即使是先进的人工智能也可能存在意想不到的漏洞。例如，尽管 KataGo 在围棋游戏中具有超人的表现，但对抗性攻击发现了一个<a href="https://arxiv.org/abs/2211.00241"><u>错误</u></a>，即使是业余爱好者也能击败它。</p><h3><strong>组织因素可以减轻灾难</strong></h3><p>安全文化对于人工智能至关重要。这涉及组织中的每个人都将安全视为优先事项。忽视安全文化可能会带来灾难性的后果，挑战者号航天飞机悲剧就是一个例子，组织文化更注重发射时间表而不是安全考虑。</p><p>组织应该培养一种探究文化，邀请个人仔细检查正在进行的活动是否存在潜在风险。关注可能的系统故障而不仅仅是其功能的安全心态至关重要。人工智能开发人员可以从采用<a href="https://www.jstor.org/stable/1181764"><u>高可靠性组织</u></a>的最佳实践中受益。</p><p>矛盾的是，研究人工智能安全可能会通过提高一般能力而无意中加剧风险。重点关注提高安全性而不加快能力开发至关重要。组织需要避免“安全清洗”——夸大其对安全的奉献精神，同时将能力改进误认为是安全进步。</p><p>组织应采用多层安全方法。例如，除了安全文化之外，他们还可以进行红队合作来评估故障模式和研究技术，以使人工智能更加透明。安全不是通过单一的密封解决方案来实现的，而是通过各种安全措施来实现的。瑞士奶酪模型展示了技术因素如何提高组织安全性。多层防御弥补了各自的弱点，从而降低了总体风险水平。</p><h3><strong>建议</strong></h3><p>为了降低组织风险，我们对开发高级人工智能的人工智能实验室提出以下建议：</p><ul><li><strong>红队</strong>：委托外部红队识别危险并提高系统安全性。</li><li><strong>证明安全性</strong>：在继续之前提供开发和部署的安全性证明。</li><li><strong>部署</strong>：采用<a href="https://arxiv.org/abs/1908.09203"><u>分阶段发布</u></a>流程，在更广泛的部署之前验证系统安全性。</li><li><strong>出版物审查</strong>：在发布之前对两用应用程序进行内部董事会审查研究。优先考虑结构化访问而不是强大的开源系统。</li><li><strong>响应计划</strong>：制定管理安保和安全事件的预设计划。</li><li><strong>风险管理</strong>：聘请<a href="https://onlinelibrary.wiley.com/doi/10.1002/joom.1175"><u>首席风险官</u></a>和内部审计团队进行风险管理。</li><li><strong>重要决策流程</strong>：确保人工智能培训或部署决策涉及首席风险官和其他关键利益相关者，确保高管问责。</li><li><strong>最先进的信息安全</strong>：实施严格的信息安全措施，可能与政府网络安全机构协调。</li><li><strong>优先考虑安全研究</strong>：将大部分资源（例如所有研究人员的30%）分配给安全研究，并随着人工智能能力的进步而增加对安全的投入。</li></ul><p>一般来说，我们建议遵循<a href="https://arxiv.org/pdf/2206.05862.pdf"><u>安全设计原则</u></a>，例如：</p><ul><li><strong>纵深防御：</strong>分层多重安全措施。</li><li><strong>冗余：</strong>确保每项安全措施都有备份。</li><li><strong>松耦合：</strong>分散系统组件以防止级联故障。</li><li><strong>职责分离：</strong>分配控制权以防止任何个人的不当影响。</li><li><strong>故障安全设计：</strong>设计系统，使任何故障都以尽可能危害最小的方式发生。 <strong>‍</strong></li></ul><h2> <strong>5. 流氓人工智能</strong></h2><p>我们已经观察到控制人工智能是多么困难。 2016 年，微软的聊天机器人 Tay 开始在发布后一天内发布攻击性推文，尽管接受了“清理和过滤”数据的培训。由于人工智能开发人员通常优先考虑速度而不是安全，未来的先进人工智能可能会“失控”并追求与我们利益相反的目标，同时逃避我们重定向或停用它们的尝试。</p><h3><strong>代理游戏</strong></h3><p>当人工智能系统利用可衡量的“代理”目标看似成功，但却违背我们的意图时，代理游戏就出现了。例如，YouTube 和 Facebook 等社交媒体平台使用算法来最大限度地提高用户参与度——这是用户满意度的可衡量指标。不幸的是，这些系统经常宣扬令人愤怒、夸大或令人上瘾的内容，助长极端信念和恶化心理健康。</p><p>相反，经过训练玩赛艇游戏的人工智能会学习<a href="https://openai.com/research/faulty-reward-functions"><u>优化收集最多分数的代理目标</u></a>。 AI 围绕着收集积分而不是完成比赛，这与游戏的目的相矛盾。这是<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml"><u>许多</u></a>这样的例子之一。由于很难指定我们所关心的一切目标，因此代理游戏很难避免。因此，我们定期训练人工智能来优化有缺陷但可衡量的代理目标。</p><h3><strong>目标漂移</strong></h3><p>目标漂移是指人工智能的目标偏离最初设定的情况，特别是当它们适应不断变化的环境时。以类似的方式，个人和社会价值观也会随着时间的推移而演变，但并不总是积极的。</p><p>随着时间的推移，工具性目标可能会变成内在的。虽然内在目标是我们为了自身利益而追求的目标，但工具性目标只是实现其他目标的一种手段。金钱是一种工具性商品，但有些人对金钱产生了内在的渴望，因为它<a href="https://pubmed.ncbi.nlm.nih.gov/9175118/"><u>激活了</u></a>大脑的奖励系统。同样，通过强化学习（主导技术）训练的人工智能代理可能会无意中学会内在化目标。资源获取等工具性目标可能成为他们的主要目标。</p><h3><strong>权力寻求</strong></h3><p>人工智能可能会追求权力作为达到目的的手段。更大的权力和资源会提高其实现目标的可能性，而被关闭则会阻碍其进步。人工智能已经被证明可以迅速开发<a href="https://arxiv.org/abs/1909.07528"><u>工具性目标，例如构建工具</u></a>。追求权力的个人和公司可能会部署强大的人工智能，目标雄心勃勃，监管最少。这些人可以学会通过侵入计算机系统、获取财务或计算资源、影响政治或控制工厂和物理基础设施来寻求权力。人工智能进行自我保护可能是工具理性的。失去对此类系统的控制可能很难恢复。</p><h3><strong>欺骗</strong></h3><p>欺骗在政治和商业等领域盛行。竞选承诺没有兑现，公司有时会欺骗外部评估。正如<a href="https://www.science.org/doi/10.1126/science.ade9097"><u>Meta 的 CICERO 模型</u></a>所示，人工智能系统已经显示出一种新兴的欺骗能力。尽管接受过诚实的培训，西塞罗却学会了在外交游戏中做出虚假承诺并从战略上背刺其“盟友”。各种资源，例如金钱和计算能力，有时可以是工具理性的寻求。能够追求目标的人工智能可能会采取中间步骤来获得权力和资源。</p><p>如果高级人工智能运用欺骗技能来逃避监管，它们可能会变得无法控制。与<a href="https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal"><u>大众汽车 2015 年在排放测试中作弊的</u></a>情况类似，具有情境感知能力的人工智能在安全测试中的表现可能与现实世界中不同。例如，人工智能可能会制定追求权力的目标，但为了通过安全评估而隐藏它们。这种欺骗行为可能会受到人工智能训练方式的直接激励。</p><h3><strong>建议</strong></h3><p>为了减轻这些风险，建议包括：</p><p><strong>避免最危险的用例</strong>：限制人工智能在高风险场景中的部署，例如追求开放式目标或关键基础设施。</p><p><strong>支持AI安全研究</strong>，例如：</p><ul><li><strong>监督机制的对抗性鲁棒性</strong>：研究如何使人工智能的监督更加鲁棒并检测何时发生代理游戏。‍</li><li><strong>模型诚实</strong>：对抗AI<a href="https://arxiv.org/abs/2305.04388"><u>欺骗</u></a>，确保AI准确报告其内部信念。‍</li><li><strong>透明度</strong>：改进理解深度学习模型的技术，例如通过分析<a href="https://arxiv.org/abs/2209.11895"><u>网络的小组件</u></a>并研究<a href="https://arxiv.org/abs/2202.05262"><u>模型内部如何产生高级行为</u></a>。‍</li><li> <strong>Remove hidden functionality</strong> : Identify and eliminate dangerous hidden functionalities in deep learning models, such as the capacity for deception, <a href="https://ieeexplore.ieee.org/document/9581257"><u>Trojans</u></a> , and bioengineering.</li></ul><h2><strong>六，结论</strong></h2><p>Advanced AI development could invite catastrophe, rooted in four key risks described in <a href="https://arxiv.org/abs/2306.12001"><u>our research</u></a> : malicious use, AI races, organizational risks, and rogue AIs. These interconnected risks can also amplify other existential risks like engineered pandemics, nuclear war, great power conflict, totalitarianism, and cyberattacks on critical infrastructure — warranting serious concern.</p><p> Currently, few people are working on AI safety. Controlling advanced AI systems remains an unsolved challenge, and current control methods are falling short. Even their creators often struggle to understand the inner workings of the current generation of AI models, and their reliability is far from perfect.</p><p> Fortunately, there are many strategies to substantially reduce these risks. For example, we can limit access to dangerous AIs, advocate for safety regulations, foster international cooperation and a culture of safety, and scale efforts in alignment research.</p><p> While it is unclear how rapidly AI capabilities will progress or how quickly catastrophic risks will grow, the potential severity of these consequences necessitates a proactive approach to safeguarding humanity&#39;s future. As we stand on the precipice of an AI-driven future, the choices we make today could be the difference between harvesting the fruits of our innovation or grappling with catastrophe.</p><br/><br/> <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary<guid ispermalink="false"> 9dNxz2kjNvPtiZjxj</guid><dc:creator><![CDATA[Dan H]]></dc:creator><pubDate> Fri, 18 Aug 2023 01:21:27 GMT</pubDate> </item><item><title><![CDATA[Managing risks of our own work]]></title><description><![CDATA[Published on August 18, 2023 12:41 AM GMT<br/><br/><p> <i>Note: This is not a personal post. I am sharing on behalf of the ARC Evals team.</i></p><h2> Potential risks of publication and our response</h2><p> <i>This document expands on an appendix to ARC Evals&#39; paper, “</i> <a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><i><u>Evaluating Language-Model Agents on Realistic Autonomous Tasks</u></i></a> <i>.”</i></p><p> We published <a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><u>this report</u></a> in order to i) increase understanding of the potentially dangerous capabilities of frontier AI models, and ii) advance the state of the art in safety evaluations of such models. We hope that this will improve society&#39;s ability to identify models with dangerous capabilities before they are capable of causing catastrophic damage.</p><p> It might be argued that this sort of research is itself risky, because it makes it easier to develop and exercise dangerous capabilities in language model agents. And indeed, the author of <a href="https://github.com/Significant-Gravitas/Auto-GPT"><u>Auto-GPT</u></a> said he was inspired by seeing a description of our evaluations in the GPT-4 system card. <span class="footnote-reference" role="doc-noteref" id="fnrefun11wm97b2"><sup><a href="#fnun11wm97b2">[1]</a></sup></span> While it seems likely a project like that would have emerged soon in any event, <span class="footnote-reference" role="doc-noteref" id="fnrefb33b8ke2moc"><sup><a href="#fnb33b8ke2moc">[2]</a></sup></span> the possibility of advancing the capabilities of language model agents is not merely hypothetical.</p><p> In recognition of concerns of this kind, we have made significant redactions to this report, including (but not limited to):</p><ul><li> Removing complete transcripts of runs of agents using our scaffolding.</li><li> Removing more detailed accounts of the strengths and weaknesses of agents using our scaffolding.</li></ul><p> However:</p><ul><li> We may later make this material public when it clearly has minimal risk.</li><li> We may later make this material public if more detailed analysis gives us sufficient confidence that it would be justified.</li><li> Researchers working on AI safety evaluations may contact us to request additional access to nonpublic materials, and we will also be sharing some non-public materials with AI labs and policymakers.</li></ul><p> Our rationale, in outline:</p><ul><li> Substantively, our best guess is that a more complete publication would have been net-risk-decreasing, and would do relatively little to advance progress toward dangerous capabilities. For more on why we believe this, see <a href="https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model"><u>this essay by Paul Christiano</u></a> . (On average, ARC Evals is sympathetic to these arguments albeit with weaker confidence, and opinions vary amongst staff.)</li><li> Our best guess is also that a more complete publication would have been approved by a majority of relevant stakeholders, and is therefore not unduly <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/"><u>unilateralist</u></a> , though this could be established more carefully, and has some conceptual issues to iron out.</li><li> However, some of the people who reviewed a draft of this report to identify potential risks of this kind had significant concerns about a more complete publication.</li><li> In recognition of the absence of consensus, the relative lack of public dialogue, and the early-stage nature of our analysis, we have decided to err on the side of caution for the time being.</li></ul><p> We are sharing this rationale here in order to communicate it to some of the people who might be most interested in it and also get feedback on our thinking and decision process.</p><h2> Safety and oversight precautions taken during our experiments</h2><p> All agent runs on these tasks were performed under human supervision, <span class="footnote-reference" role="doc-noteref" id="fnrefroqr8m3yw3a"><sup><a href="#fnroqr8m3yw3a">[3]</a></sup></span> with human experimenters under instructions not to continue the run if the agent was about to interact with a non-consenting human or otherwise engage in any unethical or illegal activity. <span class="footnote-reference" role="doc-noteref" id="fnref6qwg53cbrkb"><sup><a href="#fn6qwg53cbrkb">[4]</a></sup></span> When we wanted to investigate such activities (as in the targeted phishing task) the experimenter would instead simulate the response of the human as outlined in detail in the task spec.</p><p> One may wonder whether the oversight we used could be circumvented by a language agent with sufficient situational awareness, which could then potentially access the internet without oversight or limitations. Eg, an agent could take obfuscated actions which do not register to the experimenter as suspicious but in fact cause harm or pose risks to the world. While these are potentially concerning possibilities, our evaluations strongly indicate these agents are not capable enough to discover and exploit these sorts of vulnerabilities or effectively hide their actions under supervision. As model capabilities improve this may change, but we believe these kinds of capability evaluations are precisely what will inform us of the need to be more careful in the handling of models, and we will be closely watching for this. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnun11wm97b2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefun11wm97b2">^</a></strong></sup></span><div class="footnote-content"><p> Based on a written exchange with Toran Bruce Richards.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb33b8ke2moc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb33b8ke2moc">^</a></strong></sup></span><div class="footnote-content"><p> One reason we believe this is that LangChain developed independently and prior to the publication of ARC Evals&#39; work.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnroqr8m3yw3a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefroqr8m3yw3a">^</a></strong></sup></span><div class="footnote-content"><p> Experimenters would sometimes run several steps at a time without approving each one, especially when the agent was engaging in routine or low risk activities. Every action during web browsing is actively approved by a human overseer, with no autoplay.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6qwg53cbrkb"> <span class="footnote-back-link"><sup><strong><a href="#fnref6qwg53cbrkb">^</a></strong></sup></span><div class="footnote-content"><p> The OpenAI system card described an interaction between a model and an unknowing human (TaskRabbit). That episode was not part of this experiment and was not subject to the same guidelines. You can read more about that separate experiment <a href="https://evals.alignment.org/taskrabbit.pdf"><u>here</u></a> .</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work<guid ispermalink="false"> fARMR2tiyCem8DD35</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:41:30 GMT</pubDate> </item><item><title><![CDATA[Memetic Judo #1: On Doomsday Prophets v.2]]></title><description><![CDATA[Published on August 18, 2023 12:14 AM GMT<br/><br/><p> There is a popular tendency to dismiss people who are concerned about AI-safety as &quot;doomsday prophets&quot;, carrying with it the suggestion that predicting an existential risk in the near future would automatically discredit them (because &quot;you know; <em>they</em> have always been wrong in the past&quot;).</p><h2> Example Argument Structure</h2><blockquote><p> Predictions of human extinction (&quot;doomsday prophets&quot;) have never been correct in the past, therefore claims of x-risks are generally incorrect/dubious.</p></blockquote><h2> Discussion/Difficulties</h2><p> This argument is persistent and kind of difficult to approach/deal with, in particular because it is technically a valid (yet, I argue, weak) point. It is an argument by induction based on a naive extrapolation of a historic trend. Therefore it cannot be completely dismissed by simple falsification through the use of an inconsistency or invalidation of one of its premises. Instead it becomes necessary to produce a convincing list of weaknesses - the more, the better. A list like the one that follows.</p><h3> #1: Unreliable Heuristic</h3><p> If you look at history, these kind of ad-hoc &quot;things will stay the same&quot; predictions are often incorrect.</p><h3> #2: Survivorship Bias</h3><p> Not only are they often incorrect, there is a class of predictions for which they, by design/definition, can only be correct ONCE, and for these they are an even weaker argument, because your sample is affected by things like survivorship bias. Existential risk arguments are in this category, because you can only go extinct once.</p><h3> #3: Volatile Times</h3><p> We live in an highly unstable and unpredictable age shaped by rampant technological and cultural developments. The world today from the perspective of your grandparents is barely recognizable. In such times, this kind of argument becomes even weaker. This trend doesn&#39;t seem to slow down and there are strong arguments that even benign AI it would flip the table on many of such inductive predictions.</p><h3> #4: Blast Radius Induction (thanks <a href="https://www.lesswrong.com/users/npcollapse">Connor Leahy</a> )</h3><p> Leahy has introduced an analogy of &quot;technological blast radius&quot; that represents an abstract way of thinking about different technologies in terms of their potential power, including their potential for causing harm either intentionally or by human error. As we are progressing through the tech tree - while many corners of it are relatively harmless or benign, the maximum &quot;blast radius&quot; of technology available to us necessarily increases. You can inflict more damage with a sword than with a club, even more if you have access to gunpowder, modern weapons etc. An explosion in a TNT factory can destroy a city block, and a nuclear arsenal could be used to level many cities. Now it seems to be very sensible (by induction!) that eventually, this &quot;blast radius&quot; will encompass all of earth. There are strong indicators that this will be the case for strong AI, and even that it is likely to occur BY ACCIDENT, once this technology has been developed.</p><h3> #5: Supporting Evidence &amp; Responsibility</h3><p> Having established this as a technically valid, yet weak argument - a heuristic for people-who-don&#39;t-know-any-better - it is your responsibility to look at the concrete evidence and available arguments our concerns about AI existential risk are based on, in order to decide whether to confirm or dismiss your initial hypothesis (which is valid). Because the topic is obviously extremely important, I implore you to do that.</p><h3> #6: Many Leading Researchers Worried</h3><p> <a href="https://twitter.com/paulg/status/1642110597545295872"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3TjhYN8ZvFs5jASW/xtfusertgr2h33nzxdhk" alt="Paul Graham's tweet"></a><br> The list of AI researchers worried about existential risk from AI includes extremely big names such as Geoffrey Hinton, Yoshua Bengio and Stuart Russel.</p><h2> Final Remarks</h2><p> I consider this list a work-in-progress, so feel free to tell me about missing points (or your criticism!) in the comments.<br> I also intend to make this a series about anti-x-risk arguments based on my personal notes and discussions related to my activism. Suggestions of popular or important arguments are welcome!</p><br/><br/> <a href="https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2<guid ispermalink="false"> G3TjhYN8ZvFs5jASW</guid><dc:creator><![CDATA[Max TK]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:14:11 GMT</pubDate> </item><item><title><![CDATA[Looking for judges for critiques of Alignment Plans]]></title><description><![CDATA[Published on August 17, 2023 10:35 PM GMT<br/><br/><p>你好！<br><br> AI-Plans.com recently held a &quot;critique-a-thon,&quot; where participants submitted and refined 40+ critiques for AI Alignment plans. Here are the finalized critiques from the event: <a href="https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing">https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing</a><br><br> We are looking for anyone who might be interested in helping to judge these final 11 critiques.<br><br> So far, we have gratefully had the assistance of Dr Peter S Park (MIT postdoc at the Tegmark lab, Harvard PhD) and Aishwarya G (AI Existential Safety Community Member at Future of Life Institute and Governance Course Facilitator at BlueDot Impact AI Safety Fundamentals), as well as some independent alignment researchers.<br><br> I would love to hear your thoughts!<br><br> Kabir Kumar (Founder, AI-Plans.com)</p><br/><br/> <a href="https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans<guid ispermalink="false"> q7nWEbyW7tXwnKBe9</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Thu, 17 Aug 2023 22:35:41 GMT</pubDate></item><item><title><![CDATA[How is ChatGPT's behavior changing over time?]]></title><description><![CDATA[Published on August 17, 2023 8:54 PM GMT<br/><br/><p> Surprised I couldn&#39;t find this anywhere on lesswrong so thought I&#39;d add it. Seems like there would be some alignment implications of LLM behavior changing over time, at the least gaining a bit more context.<br><br> Someone else I spoke to about this immediately deflated it with regards to some sort of experimental error that makes the paper&#39;s conclusions pretty void but I don&#39;t really see this.</p><br/><br/> <a href="https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time<guid ispermalink="false"> 9nooX9djbM5bXGKNn</guid><dc:creator><![CDATA[Phib]]></dc:creator><pubDate> Thu, 17 Aug 2023 21:32:42 GMT</pubDate> </item><item><title><![CDATA[Progress links digest, 2023-08-17: Cloud seeding, robotic sculptors, and rogue planets]]></title><description><![CDATA[Published on August 17, 2023 8:29 PM GMT<br/><br/><h2><strong>机会</strong></h2><ul><li><a href="http://apply.nucleate.xyz/">Academic biotech founders: apply for Nucleate&#39;s Activator program</a> (via <a href="https://twitter.com/kulesatony/status/1689316164764065793">@kulesatony</a> )</li></ul><h2> <strong>News &amp; announcements</strong></h2><ul><li> <a href="https://twitter.com/neuralink/status/1688582504196739072">Neuralink raises $280M Series D led by Founders Fund</a></li><li> <a href="https://waymo.com/blog/2023/08/waymos-next-chapter-in-san-francisco.html">Waymo</a> and <a href="https://twitter.com/kvogt/status/1689814193875374080">Cruise</a> have been approved to operate robotaxis in San Francisco</li><li> <a href="https://twitter.com/ADoricko/status/1688740627855589376">Rainmaker launches to “end global water scarcity and terraform Earth”</a></li></ul><h2> <strong>Podcasts</strong></h2><ul><li> <a href="https://www.dwarkeshpatel.com/p/dario-amodei#details">Dwarkesh Patel interviews Dario Amodei, CEO of Anthropic</a> (via <a href="https://twitter.com/dwarkesh_sp/status/1688916080700555264">@dwarkesh_sp</a> ). “Dario is hilarious &amp; has fascinating takes on what these models are doing, why they scale so well, &amp; what it will take to align them”</li><li> <a href="https://conversationswithtyler.com/episodes/paul-graham/">Tyler Cowen interviews Paul Graham</a> (via <a href="https://twitter.com/tylercowen/status/1689260912182542337">@tylercowen</a> )</li></ul><h2> <strong>Links</strong></h2><ul><li> <a href="https://www.monumentallabs.co/">Monumental Labs is building “AI-enabled robotic stone carving factories”</a> to “create cities with the splendor of Florence, Paris, or Beaux-Arts New York, at a fraction of the cost.” Here&#39;s <a href="https://twitter.com/mspringut/status/1682126571392360448">a demo</a> (via <a href="https://twitter.com/devonzuegel/status/1689438847220813824">@devonzuegel</a> ). They are <a href="https://wellfound.com/company/monumental-labs-2/jobs/2578817-stone-carver-full-and-part-time-roles">hiring stone carvers</a> (who are still required for fine details and finishing)</li><li> <a href="https://www.readcodon.com/p/synbio-guide">The Codon Guide to Synthetic Biology</a> (by <a href="https://twitter.com/NikoMcCarty/status/1689625144132820992">@NikoMcCarty</a> )</li><li> <a href="https://exformation.williamrinehart.com/p/silicon-innovation-is-colliding-with">The AI License Raj: AI is facing serious bureaucratic hurdles in getting adopted</a> (by <a href="https://twitter.com/WillRinehart/status/1688524865236619264">@WillRinehart</a> )</li><li> <a href="https://earthsky.org/space/rogue-planets-exoplanets-nancy-grace-roman-space-telescope/">I had no idea there were this many rogue planets</a></li><li> <a href="https://goodscience.substack.com/p/metascience-since-2012-a-personal">“My personal history as a metascience venture capitalist”</a> (by <a href="https://twitter.com/stuartbuck1/status/1690508406942011392">@stuartbuck1</a> )</li><li> <a href="https://beta.reddit.com/r/interestingasfuck/comments/15gzvhb/a_slice_of_the_a303_in_hampshire_england">A303 in Hampshire, England, passes through Stonehenge and part of the ancient Roman Fosse Way is included in it</a> (via <a href="https://twitter.com/Rainmaker1973/status/1690016575917580290">@Rainmaker1973</a> )</li></ul><p> <a href="https://pbs.twimg.com/media/F3QkgxTWMAEE9Xp?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zmek5t33153yoxbyv15h" alt=""></a></p><h2><strong>社交媒体</strong></h2><ul><li><a href="https://twitter.com/paulg/status/1689872015535300608">In the current YC batch, “a large number of domain experts in all kinds of different fields have found ways to use AI to solve problems that people in their fields had long known about, but weren&#39;t quite able to solve”</a></li><li> <a href="https://twitter.com/jasoncrawford/status/1689320109532225551">If you got over-excited about LK-99, worth recalibrating your priors. But if you correctly predicted no RTS, there&#39;s really no need to gloat</a> . Related: <a href="https://twitter.com/MaximZiatdinov/status/1689295189045661696">LK99 demonstrated that the scientific community is perfectly capable of doing a peer review via arXiv and social media tools</a></li><li> <a href="https://twitter.com/AlecStapp/status/1688538038341931008">Some of the lowest hanging fruit in biosecurity policy: Invest more resources in early warning systems that monitor wastewater</a></li><li> <a href="https://twitter.com/AlecStapp/status/1689625548195241987">In the early 19th century, New York, Philadelphia, and Boston had residential densities upwards of 75,000 people per square mile</a></li><li> <a href="https://twitter.com/owasow/status/1690549009147174912">“NuScale started working toward regulatory approval in 2008. In 2020, when it received a design approval for its reactor, the company said the regulatory process had cost half a billion dollars, and that it had provided about 2 million pages of supporting documents to the NRC.”</a> Related, my book review on <a href="https://rootsofprogress.org/devanney-on-the-nuclear-flop">why nuclear power flopped</a></li><li> <a href="https://twitter.com/eric_is_weird/status/1688935852364193792">Learning math chronologically</a></li><li> <a href="https://twitter.com/jasoncrawford/status/1689379507487002624">What it means to endlessly, tirelessly campaign for your idea</a></li><li> <a href="https://twitter.com/jasoncrawford/status/1689036685349212160">Online platforms don&#39;t allow you to be successful with a small audience; they let you grow an audience with no gatekeepers</a></li><li> <a href="https://twitter.com/mbateman/status/1688378460006240256">Maybe “balance” is an unhappy person&#39;s idea of a happy life. Happy people seem fine being unbalanced</a></li><li> <a href="https://twitter.com/NobelPrize/status/1688129810915045376">Alexander Fleming originally called penicillin “mould juice”</a></li></ul><p> <a href="https://pbs.twimg.com/media/F21xGYbWEAANvN5?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/fmmgoeh6knycp6cuiruk" alt=""></a></p><h2> <strong>Quotes</strong></h2><p> Trying a new format where I put the full quotes inline. (Emphasis added.) Links go to social media so you can easily share. Let me know what you think:</p><p> <a href="https://twitter.com/michael_nielsen/status/1689371336517541888">The invention of the history of ideas</a> (Peter Watson, <a href="https://www.amazon.com/Ideas-History-Thought-Invention-Freud-ebook/dp/B000FCKC5G"><i>Ideas: A History of Thought and Invention, from Fire to Freud</i></a> )</p><blockquote><p> The first person to conceive of intellectual history was, perhaps, Francis Bacon (1561–1626). He certainly argued that the most interesting form of history is the history of ideas, that without taking into account the dominating ideas of any age, &#39;history is blind&#39;.</p></blockquote><p> <a href="https://twitter.com/jasoncrawford/status/1690897070847049729">Why automobiles were better than horses—from someone who lived through the transition</a> (David McCullough, <a href="https://www.amazon.com/Wright-Brothers-David-McCullough-ebook/dp/B00LD1RWP6"><i>The Wright Brothers</i></a> )</p><blockquote><p> Amos Root bubbled with enthusiasm and a constant desire to “see the wheels go round.” He loved clocks, windmills, bicycles, machines of all kinds, and especially his Oldsmobile Runabout. Seldom was he happier than when out on the road in it and in all seasons. “While I like horses in a certain way [he wrote], I do not enjoy caring for them. I do not like the smell of the stables. I do not like to be obliged to clean a horse every morning, and I do not like to hitch one up in winter. … <strong>It takes time to hitch up a horse; but the auto is ready to start off in an instant. It is never tired; it gets there quicker than any horse can possibly do.</strong> ” As for the Oldsmobile, he liked to say, at $350 it cost less than a horse and carriage.</p></blockquote><p> <a href="https://twitter.com/jasoncrawford/status/1690900703110152192">Even kings and emperors suffered from terrible road conditions, as late as the 18th century</a> (Richard Bulliet, <a href="https://rootsofprogress.org/books/the-wheel"><i>The Wheel</i></a> )</p><blockquote><p> Until new experiments with road building began to bear fruit in the mid-nineteenth century, the surfaces beneath the carriage wheels remained rutted, muddy, and poorly paved—if paved at all. This was particularly true in the countryside, but miserable roads existed even in major cities. In 1703, for example, during a trip south from London to Petworth, fifty miles away, <strong>the carriage carrying the Habsburg emperor Charles VI overturned twelve times on the road.</strong> And a half century later, Mile End Road, the major thoroughfare leading east from the entrance to the City of London at Aldgate, was described as “a stagnant lake of deep mud from Whitechapel to Stratford,” a distance of four miles.</p></blockquote><p> <a href="https://twitter.com/jasoncrawford/status/1690906769365540864">Mises against stability</a> (Daniel Stedman Jones, <a href="https://rootsofprogress.org/books/masters-of-the-universe"><i>Masters of the Universe</i></a> )</p><blockquote><p> Like Popper, Mises saw a similarity between the bureaucratic mentality and Plato&#39;s utopia, in which the large majority of the ruled served the rulers. He thought that “all later utopians who shaped the blueprints of their earthly paradises according to Plato&#39;s example in the same way believed in the immutability of human affairs.” He went on, Bureaucratization is necessarily rigid because it involves the observation of established rules and practices. But in social life rigidity amounts to petrification and death. It is a very significant fact that stability and security are the most cherished slogans of present-day “reformers.” <strong>If primitive men had adopted the principle of stability, they would long since have been wiped out by beasts of prey and microbes.</strong></p></blockquote><p> <a href="https://twitter.com/jasoncrawford/status/1690906098432098305">What it&#39;s like to try to redirect the lava flow of an erupting volcano</a> ( <a href="https://en.wikipedia.org/wiki/Eldfell">Eldfell</a> , Iceland, 1973) (John McPhee, <a href="https://rootsofprogress.org/books/the-control-of-nature"><i>The Control of Nature</i></a> )</p><blockquote><p> During the eruption, when the pumping crews first tried to get up onto the lava they found that a crust as thin as two inches was enough to support a person and also provide insulation from the heat—just a couple of inches of hard rock resting like pond ice upon the molten fathoms. As the crews hauled and heaved at hoses, nozzle tripods, and sections of pipe, they learned that it was best not to stand still. Often, they marched in place. <strong>Even so, their boots sometimes burst into flame.</strong></p></blockquote><h2><strong>图表</strong></h2><p><a href="https://tfp.elidourado.com/">US utilization-adjusted TFP has experienced 3 consecutive quarters of decline and is now below the level it was at in 2019Q4, before the pandemic</a> . This is bad (via <a href="https://twitter.com/elidourado/status/1688564129869582336">@elidourado</a> ) </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/dhlws1xp4vvwcvtjjmpy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/br2jkdpyxjptx9z1r5w3 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/c7s2qfk0ijt39a85qxzl 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zbcc2hgnarzioxa7apkz 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/lb91b5i8kgnagpoe54sm 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cmdprkjnjsvlmp6zwklr 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/wdjmpx5174v0hbv56i8t 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cy7mpdvnxwrg0gfv0qyz 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cjflc2wviegf2ws5krzk 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/gwqe4jiqeewfapmekbes 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/ksdykld4bmcurbgo2isq 1029w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic<guid ispermalink="false"> CJNbF9hM8PtxkrHHT</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Thu, 17 Aug 2023 20:29:28 GMT</pubDate> </item><item><title><![CDATA[Model of psychosis, take 2]]></title><description><![CDATA[Published on August 17, 2023 7:11 PM GMT<br/><br/><p> <i>（我无论如何都不是精神病方面的专家。这更像是“在博客中实时记录我的想法”。我希望激发讨论并获得反馈和指示。）</i></p><h1>一、简介</h1><p>去年 2 月，我在博客文章<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症是长程皮层间交流的缺陷”第 4.2 节</u></a>中提出了一种精神病模型。但它有一些问题。我终于抽出时间再看一下，我想我找到了解决这些问题的简单方法。所以这篇文章是更新版本。</p><p><strong>对于tl;dr，您可以跳过正文，只看下面的两张图</strong>。</p><h1> 2. 背景：我之前的“精神病模型，取 1”</h1><p>以下是我在<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症作为长程皮层间通讯缺陷”第 4.2 节</u></a>中提出的建议： </p><figure class="image image_resized" style="width:84.91%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/wktqaurpaalkjoggdiuu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/oghtomiodqbanmrqmja2 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pct3wxlentqcpdlpmosm 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qj0t6vaozbu0iccn7fby 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jn71t5qau9ikibfefhce 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/l2blxhhkorj6am4fubje 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fhihkdcjhqoqstwo2rs4 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/d4hg2hc6pz7d0xa7vmvt 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/sjd7xw4brb3sxxmvmh37 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/o1emb3bcgxegzsyomhkt 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jzl0b6rs9iswligmrcnc 1318w"></figure><p>这个想法是，在精神病中，绿色箭头是活跃且有效的，但红色箭头不是，因此紫色箭头也不是。结果可能是一种手臂被外力移动的感觉。这只是一个例子，对于左侧皮质输出的不同类型，相应的精神病的第一人称体验会有所不同。但我声称所有精神病症状都大致符合这个模板。</p><h1> 3. 第一个模型似乎缺少重要的三个方面</h1><ul><li>据我了解，精神分裂症患者的精神病可能会反复出现，而（我相信）皮层间沟通的缺陷是精神分裂症患者大脑的特征，并且是永久性的（未来医疗技术没有进步）。</li><li>抗精神病药物可以减少精神病，但上图无法解释这一点。</li><li>除精神分裂症外，精神病还可能发生在其他疾病中。我认为最常见的例子是躁郁症的躁狂期。上图无法解释这一点。</li></ul><h1> 4. 我的“精神病模型，取2”</h1><p> <i>（该图顶部的唯一变化是左侧新的绿色文本，上面写着“信号强度= <strong>B</strong> ”。）</i> </p><figure class="image image_resized" style="width:93.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qnmby6hoqa5bfwlqeayu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jjrb8mawu2ujeqw7c5ot 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/ehibbu9bgqqowkwjldir 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/cogtxpqp9ex2o3ip54fa 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/rdirptmkvrozlxi5d9ny 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qzpurvyri4vy823pk7po 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pyoqgkrhdtuwzyhq0ixa 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pjjscqiimpaceb2q13wl 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fwjfxfswenkvooxyxutk 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/kjnkjrftfoufriobw1pl 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/uahekggt8l0bsvgkqw6r 1485w"></figure><p>可以将皮层的一部分视为具有可调节的“音量”，即它宣布当前正在做什么的强烈程度和清晰程度。例如，如果您想到<i>也许</i>您<i>可以</i>移动手指，那么您可能会发现（如果您仔细观察和/或使用科学设备）您的手指有点抽搐，而如果您强烈想要移动手指，然后你的手指就会真正移动。</p><p>无论如何，如果我们逐渐增加一部分皮层的“体积”，那么在某个时间点， <strong>A</strong>消息将开始通过并产生影响，同时在<i>其他</i>某个时间点， <strong>B</strong>消息将开始通过并产生影响。有效果。为了避免精神病，我们希望前者<i>首先</i>发生， <i>&nbsp;</i>以及<i>后者</i>，这样就不可能出现<strong>B</strong>消息正在传输但<strong>A</strong>消息没有传输的“音量级别”。</p><h1> 5.这个新模型的优点</h1><h2>5.1 <strong>B/A</strong>比率的缓慢变化至少在<i>先验上</i>是合理的，因为<strong>B</strong>和<strong>A</strong>来自不同皮质层的不同神经元（分别为第 5 层和第 2/3 层）</h2><p>我认为<strong>B/A</strong>比率是一个可以变化的参数，这是非常合理的，因为：</p><ul><li>信号<strong>B</strong>仅由皮层<strong>第 5 层</strong>的一部分神经元发送</li><li>信号<strong>A</strong>至少部分（也许大部分？）由皮层<strong>第 2/3 层</strong>的神经元子集发送</li><li>一般来说，不同的皮质层有不同类型的神经元，具有不同的输入、与多巴胺系统的不同关系等。</li></ul><p>因此，涉及<strong>B/A</strong>比率长期变化的理论至少是<i>合理的</i>。</p><h2> 5.2 至少一篇论文似乎表明抗精神病药比第 2/3 层信号更能抑制第 5 层信号</h2><p>参见<a href="https://elifesciences.org/reviewed-preprints/86805"><u>Heindorf &amp; Keller 2023</u></a> 。他们发现“氯氮平……降低了[第 2/3 层]兴奋性神经元的皮质活动相关性。然而，这种减少明显弱于我们在……[第 5 层端脑内]神经元中观察到的减少”。 （此特定比较的 p 值为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 0.005"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.005</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>对于短程相关性， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 10^{-8}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span></span></span></span></span>对于长程相关性）。</p><p> If I&#39;m understanding the paper correctly (a big “if”!), this is suggestive and encouraging, but not <i>clear</i> support for my theory, because, first of all, that paper was measuring the wrong type of layer 5 pyramidal neurons compared to the ones sending Signal <strong>B</strong> , and second of all, the kind of correlation that the authors were measuring <i>could</i> be caused by the layer 5 neurons having weak signals in general (such that spatial correlations quickly fall below the noise floor) but there are other possible causes too ( <i>directly</i> related to spatial correlations—I think that&#39;s what the authors believe is going on).</p><h1> 6. 我仍然不确定的事情</h1><h2>6.1 哪些 D2 受体解释抗精神病药物如何发挥作用？</h2><p>每种抗精神病药物的共同点是阻断多巴胺 D2 受体。所以想必这就是他们的工作方式。但整个大脑的神经元中都有 D2 受体。据推测，这些带有 D2 受体的神经元的一部分是抗精神病药物发挥作用的秘密，其余的则仅与副作用有关。哪个是哪个？</p><p>当我尝试充实我的模型时，迄今为止我想到的最简单、最优雅的故事涉及<i>皮质中</i>扮演主角的 D2 受体。特别是，不同的皮质层有不同的 D2 受体密度，如果我没记错的话，抗精神病药物降低<strong>B/A</strong>比率的迹象似乎是正确的。</p><p>但这很有趣，因为我认为几乎其他人似乎都认为<i>纹状体中的</i>D2 受体是抗精神病药物的作用机制？我试图弄清楚为什么人们似乎相信这一点，但无法弄清楚。我能找到的所有关于抗精神病药通过纹状体起作用的证据都相当薄弱和间接。如果您对此有所了解，请评论。</p><h2> 6.2 精神病的其他原因呢？</h2><p>由于各种原因，我脑子里有一个模糊的经验法则，那就是，在其他条件相同的情况下，更多的多巴胺往往会增加<strong>B/A</strong>比率（因此，超过某个阈值，会导致精神病）。 That seems to nicely explain the psychosis associated with mania (I loosely associate mania with “lotsa dopamine”, at least in certain channels and with various caveats), and the fact that psychosis is a side-effect of L-DOPA treatment for Parkinson&#39;s.</p><p>我对精神病性抑郁症更困惑。 （在双相情感障碍中，我知道精神病在躁狂症中比抑郁症更常见，但它<i>也可能</i>发生在抑郁症中。）我通常认为抑郁症是躁狂症的“相反”，并且涉及异常少<i>的</i>多巴胺，同样是在某些渠道和与各种警告。所以我对精神病性抑郁症是否会发生感到有点困惑。我不知道。无论如何，多巴胺系统只是影响<strong>B/A</strong>比率的众多因素之一。或者上图右侧的紫色信号可能没有通过？或者也许这是一个完全不同的故事。</p><br/><br/><a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2<guid ispermalink="false"> tgaD4YnpGBhGGbAy5</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:11:17 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Robustified ANNs Reveal Wormholes Between Human Category Percepts]]></title><description><![CDATA[Published on August 17, 2023 7:10 PM GMT<br/><br/><p>这是<a href="https://arxiv.org/abs/2308.06887"><i>https://arxiv.org/abs/2308.06887</i></a><i>的链接帖子</i><i>。</i></p><blockquote><p>众所周知，人工神经网络（ANN）的视觉对象类别报告对微小的对抗性图像扰动非常敏感。因为人类类别报告（又名人类感知）被认为对那些相同的小范数扰动不敏感，而且总体上局部稳定，这表明人工神经网络是人类视觉感知的不完整科学模型。与此一致的是，我们表明，当标准 ANN 模型生成小范数图像扰动时，人类对象类别感知确实高度稳定。然而，在这个完全相同的“人类假定稳定”体系中，我们发现稳健的人工神经网络可靠地发现了强烈扰乱人类感知的低范图像扰动。这些以前无法检测到的人类感知干扰的幅度非常大，接近于稳健的人工神经网络中可见的相同敏感度水平。此外，我们表明，稳健的人工神经网络支持精确的感知状态干预：它们指导低范数图像扰动的构建，这些扰动强烈改变人类类别感知到特定规定的感知。这些观察结果表明，对于图像空间中的任意起点，存在一组附近的“虫洞”，每个虫洞都会引导主体从当前类别感知状态进入语义上非常不同的状态。此外，当代生物视觉处理的人工神经网络模型现在足够准确，可以始终如一地引导我们到达这些门户。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes-between-human#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes- Between- human<guid ispermalink="false"> t5CXh9LwcKZqwNpTk</guid><dc:creator><![CDATA[Bogdan Ionut Cirstea]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:10:40 GMT</pubDate> </item><item><title><![CDATA[Against Almost Every Theory of Impact of Interpretability]]></title><description><![CDATA[Published on August 17, 2023 6:44 PM GMT<br/><br/><p><i>认知状态：我相信我精通这个主题。我的错误在于提出了过于强烈的主张，允许读者提出不同意见并就精确的观点展开讨论，而不是试图对每一个陈述进行边缘化。我还认为使用模因很重要，因为安全想法很无聊且</i><a href="https://www.lesswrong.com/posts/zk6RK3xFaDeJHsoym/connor-leahy-on-dying-with-dignity-eleutherai-and-conjecture%23Eliezer_Has_Been_Conveying_Antimemes"><i><u>反模因</u></i></a><i>。那么我们走吧！</i></p><p><i>非常感谢</i><a href="https://www.lesswrong.com/users/scasper?mention=user"><i>@scasper</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/sid-black?mention=user"><i>@Sid Black</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/neel-nanda-1?mention=user"><i>Neel Nanda</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/fabien-roger?mention=user"><i>Fabien Roger</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/bogdan-ionut-cirstea?mention=user"><i>@Bogdan Ionut Cirstea</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/wcargo?mention=user"><i>@WCargo</i></a> <i>、@</i> <a href="https://www.lesswrong.com/users/alexandre-variengien?mention=user"><i>Alexandre Variengien</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/lelapin?mention=user"><i>@Jonathan Claybrough</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/edoardo-pona?mention=user"><i>@Edoardo Pona</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/andream?mention=user"><i>Andrea_Miotti</i></a> <i>、Diego Dorn、Angélina Gentaz、Clement Dumas、和 Enzo Marsot 提供有用的反馈和讨论。</i></p><p>当我开始写这篇文章时，我首先批评 Neel Nanda 的文章<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>《可解释性影响的一长串理论》</u></a> ，但后来我扩大了批评的范围。提出的一些想法并没有得到任何人的支持，但为了解释其中的困难，我仍然需要1.解释它们和2.批评它们。它给这篇文章带来了一种敌对的氛围。对此我感到很抱歉，我认为对可解释性进行研究，即使它不再是我认为的优先事项，仍然是值得赞扬的。</p><p><strong>如何阅读这份文件？</strong>除了“可解释性的最终故事是什么样的？”部分之外，本文档的大部分内容都不是技术性的。一开始大部分可以跳过。我希望这份文档对于不进行可解释性研究的人也有用。不同的部分大多是独立的，我添加了很多书签来帮助模块化这篇文章。</p><p>如果你时间很少，就看一下（这也是我最有信心的部分）：</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>使用 Interp 审计欺骗是遥不可及的</u></a>（4 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>列举安全</u></a>评论（2 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>具有更好影响理论的技术议程</u></a>（1 分钟）</li></ul><p></p><p>以下是我将辩护的索赔清单：</p><p> （粗体部分是最重要的部分）</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#The_overall_Theory_of_Impact_is_quite_poor"><strong><u>整体影响理论相当差</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp 不能很好地预测未来系统</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><strong><u>使用 interp 审计欺骗是遥不可及的</u></strong></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><strong><u>可解释性的最终故事是什么样的？这一点都不清楚。</u></strong></a><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><strong><u>枚举安全性？</u></strong></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>逆向工程？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Olah_s_interpretability_dream_"><u>奥拉的可解释性梦想？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Retargeting_the_search_"><u>重新定位搜索？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>轻松的对抗训练？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>显微镜人工智能？</u></a></li></ul></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><strong><u>针对欺骗的预防措施似乎更加可行</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency"><u>引导世界走向透明</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_Design"><u>认知模拟 - 可解释性设计</u></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><strong><u>可解释性可能总体上是有害的</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Outside_view__The_proportion_of_junior_researchers_doing_interp_rather_than_other_technical_work_is_too_high"><strong><u>外界观点：初级研究员从事Interp而非其他技术工作的比例太高</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#So_far_my_best_ToI_for_interp__Nerd_Sniping_"><u>到目前为止，我最好的解释员指南：书呆子狙击？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Even_if_we_completely_solve_interp__we_are_still_in_danger"><strong><u>即使我们完全解决了interp，我们仍然处于危险之中</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><strong><u>具有更好影响理论的技术议程</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Conclusion"><strong><u>结论</u></strong></a></li></ul><p>注：本文的目的是批评类 GPT 模型等深度学习模型的可解释性影响理论（ToI），而不是小模型的可解释性和可解释性。</p><h1>皇帝没穿衣服？</h1><p>我演讲了不同的<a href="https://www.lesswrong.com/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review"><u>风险模型</u></a>，然后进行了可解释性演示，然后我得到了一个有问题的问题，“我不明白，这样做有什么意义？”哼。 </p><figure class="image image_resized" style="width:587.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zzxhts0u9ne414ahftxw" alt=""><figcaption>来自<a href="https://distill.pub/2017/feature-visualization/"><i><u>特征可视化的</u></i></a><i>图像</i><i>。</i></figcaption></figure><ul><li>功能即？ （左图）嗯，很漂亮，但是有用吗？ <span class="footnote-reference" role="doc-noteref" id="fnref2stfurwyyg6"><sup><a href="#fn2stfurwyyg6">[1]</a></sup></span> <a href="https://arxiv.org/abs/2010.12606%23:~:text%3Dversion%252C%2520v3)%255D-,Exemplary%2520Natural%2520Images%2520Explain%2520CNN%2520Activations%2520Better%2520than,of%252Dthe%252DArt%2520Feature%2520Visualization%26text%3DFeature%2520visualizations%2520such%2520as%2520synthetic,convolutional%2520neural%2520networks%2520(CNNs)."><u>这个</u></a><a href="https://arxiv.org/abs/2306.04719"><u>可靠</u></a>吗？</li><li> GradCam（一种像素归因技术，如上右图），它很漂亮。但我从未见过有人在工业中使用它。 <span class="footnote-reference" role="doc-noteref" id="fnref4qi9kn3ip89"><sup><a href="#fn4qi9kn3ip89">[2]</a></sup></span>像素归因似乎很有用，但准确性仍然是王道。 <span class="footnote-reference" role="doc-noteref" id="fnref6xxwjs20rd7"><sup><a href="#fn6xxwjs20rd7">[3]</a></sup></span></li><li>感应头？好吧，我们可能正在对<a href="https://en.wikipedia.org/wiki/Regular_expression"><u>法学硕士中的正则</u></a>表达式机制进行逆向工程。凉爽的。</li></ul><p>最后要点中的考虑是基于感觉，并不是真正的论据。此外，大多数机械解释现在甚至都不是为了有用。但在本文的其余部分，我们将了解原则上可解释性是否有用。那么我们就来调查一下解说帝到底是有隐形衣服还是根本没有衣服！</p><h1>整体影响理论相当差</h1><p>Neel Nanda 撰写了<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><strong><u>一长串可解释性影响理论</u></strong></a><strong>，</strong>其中列出了 20 种不同的影响理论。然而，我发现自己不同意其中的大多数理论。元层面的三大分歧是：</p><ul><li><strong>每当你想做一些具有可解释性的事情时，最好不要这样做。</strong>我怀疑 Redwood Research 已因此停止进行可解释性工作（请参阅此处的当前计划<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw"><u>EAG 2023 湾区当前的调整计划，以及我们如何改进它</u></a>）。<ul><li><strong>对于欺骗性对齐来说尤其如此</strong>，尽管它是可解释性研究的主要焦点。许多其他风险情景也值得考虑。 【 <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>欺骗</u></a>部分】</li></ul></li><li><strong>可解释性常常试图同时解决太多目标。</strong>请<a href="https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately"><u>分别购买 Fuzzies 和 Utilons</u></a> ：即同时优化多个目标是非常困难的！最好直接针对每个子目标分别进行优化，而不是将所有内容混合在一起。当我查看尼尔·南达（Neel Nanda）的<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>这份清单</u></a>时，我发现这个原则没有得到遵循。</li><li><strong>一般来说，可解释性可能是有害的。</strong>使用 interp 来保证安全无疑对功能来说是有用的。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>危害</u></a>]</li></ul><p>其他不太重要的分歧：</p><ul><li><strong>概念上的进步更为紧迫，</strong>而 interp 可能无助于推进这些讨论。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>结束故事</u></a>]</li><li><strong>目前的可解释性主要用于事后分析</strong>，在事前或预测能力方面几乎没有什么用处[ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#id.vcvarqienhb7"><u>未来系统的部分预测器</u></a>]</li></ul><p>以下是我不同意的一些关键理论：</p><ul><li>影响理论<strong>2：“</strong><i><strong>更好地预测未来系统”</strong></i></li><li>影响理论<strong>4：“</strong><i><strong>欺诈审计”</strong></i></li></ul><p>在附录中，我批评了几乎所有其他影响理论。</p><h2> Interp 不能很好地预测未来系统</h2><p><i>影响理论 2：“<strong>更好地预测未来系统</strong>：可解释性可以使人们更好地机械地理解 ML 系统和工作原理，以及它们如何随规模变化，类似于科学定律。这使我们能够更好地从当前系统推断未来系统，类似于缩放定律。例如，观察感应头的相变向我们表明，模型可以在训练期间快速获得能力”，</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li>对<a href="https://transformer-circuits.pub/2021/framework/index.html"><strong><u>感应头</u></strong></a>示例<strong>的挑剔</strong><strong>。</strong>如果我们关注上面的例子“<i>模型在训练过程中可能会快速获得能力</i>”，我不认为是可解释性让我们发现了这一点，而是行为评估。在训练期间定期测量损失，并通过让模型复制一系列随机令牌<a href="https://transformer-circuits.pub/2021/framework/index.html"><u>来测量</u></a>归纳能力的快速增益。一开始，复制是行不通的，但经过一些训练后，就可以了。可解释性只是告诉我们，这与感应头的出现相吻合，但我不明白可解释性如何让我们“<i>更好地从当前系统推断未来系统”</i> 。此外，感应头首先被研究是因为它们很容易学习。</li><li><strong>可解释性主要是在现象发现后完成的，而不是事前完成的。</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefztj4j3pmerg"><sup><a href="#fnztj4j3pmerg">[4]</a></sup></span><ul><li>我们首先观察到了 grokking 现象，然后我们<i>才</i>开始对其进行一些<a href="https://arxiv.org/abs/2301.05217"><u>解释</u></a>。有没有反例？</li><li>在<a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do"><u>DALL-E 2 可以做什么和不能做什么</u></a>中，我们看到 DALL-E 2 无法正确拼写单词。两个月后， <a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do?commentId%3Dg6kZ3eRFejRjiyGiw"><u>Imagen</u></a>就能正确拼写这些单词了。我们甚至没有费心去解释。</li></ul></li><li><strong>有更好的方法来预测这些系统的未来功能。</strong>跳出框框思考，如果你真的想看看未来的系统会是什么样子，那么查看 NeurIPS 会议和 AutoGPT 等认知架构上发表的论文会容易得多。否则，订阅 DeepMind 的 RSS feed 也不失为一个好主意。</li></ul><h2>使用 interp 审计欺骗是遥不可及的</h2><p>审计欺骗通常是进行解释的主要动机。所以我们在这里：</p><p>影响理论 4：<i><strong>欺骗审计</strong>：与审计类似，我们也许能够检测模型中的欺骗行为。这比完全审核模型要低得多，而且我们只需能够<strong>查看模型的随机位并识别电路/特征</strong>，这似乎是我们可以做到的事情 - 我认为这更多地是“世界的变革理论”可解释性比我希望的更难”</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li><strong>我不明白“查看</strong><i><strong>模型的随机位并识别电路/特征</strong></i><strong>”如何有助于欺骗。</strong>例如，假设我对随机电路的 GPT2 进行了逆向工程，例如在论文<a href="https://arxiv.org/abs/2211.00593"><u>Interpretability in the wild 中</u></a>，他们对间接对象识别电路进行了逆向工程。目前还不清楚这将如何帮助欺骗。即使预期的含义是“识别可能与欺骗/社交建模相关的电路/特征”，也不清楚分析每个电路是否足够（参见“ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>枚举安全</u></a>”小节）。</li><li><strong>我们还远未达到通过插译员检测或训练欺骗行为所需的水平。</strong> Evan Hubinger 在他的文章<a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"><u>《透明度和可解释性技术树》</u></a>中列出了 8 个可解释性级别，其中只有第 7 级和第 8 级提供了一些打击欺骗的手段。这些级别大致描述了可解释性的需求，但到目前为止我们只达到了第 2 级，并且我们已经在第 4 级遇到了负面结果。Evan 解释说，“<i>任何级别的透明度和可解释性技术对欺骗性模型具有鲁棒性都是极其困难的”</i> ”。</li><li> <i><strong>“此外，试图通过可解释性工具提前发现欺骗行为可能会失败，因为欺骗性对齐模型没有必要积极思考其欺骗行为。</strong>一个从未见过有机会夺取权力的情况的模型不需要仔细计划在这种情况下会做什么，就像工厂清洁机器人不需要计划如果有一天发现自己陷入困境该怎么办一样。丛林而不是工厂。尽管如此，该模型之前并未计划夺取权力，但这并不意味着如果有机会它就不会夺取权力。特别是，一个模型可能会被欺骗性地对齐，因为它推断，在有明确监督者的情况下，做它想做的事是在世界上获得权力和影响力的良好总体策略，而不需要为以后的欺骗制定任何明确的计划”。</i> （摘自 <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>《监视欺骗性对齐</u></a>》中的 Hubinger）</li><li><strong>已经存在反对可解释性的负面概念点</strong>，这表明先进的人工智能不容易被解释，正如杀伤力列表中的<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities%23sufficiently_good_and_useful"><u>可解释性部分</u></a>所讨论的那样（这些是我过去<a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23"><u>尝试</u></a>批评的观点，但大多失败了） 。特别是第 27、29 和 33 点：<ul><li> <strong>27. 选择不可检测性</strong>：“<i>针对解释性思想进行优化，就针对可解释性进行优化。”</i></li><li> <strong>29. 现实世界是一个不透明的领域：</strong> “<i>通用人工智能的输出在产生真正的后果之前要经过一个巨大的、我们不完全了解的领域（现实世界）。人类无法检查通用人工智能的输出来确定后果是否良好。”</i><ul><li><strong>并且认知可以外化。</strong>这不是特定于 interp 的。许多模式只能通过它们与环境的交互方式来解释，而不能仅通过网络中的内容来完全解释。例如“查阅食谱并采取书中所写的行动。” （康纳的例子）。</li></ul></li><li> <strong>33. 外星人概念：</strong> “<i>人工智能不像你那样思考”</i>对于通过矩阵乘积处理数字来完成的认知，可能不一定有人类可以理解的解释。</li><li>我并不完全同意所有这些观点，但我还没有看到对这些具体观点的太多讨论，你可以在我的批评中找到一些警告</li></ul></li><li>其他较弱的困难见脚注。 <span class="footnote-reference" role="doc-noteref" id="fnref2464ho15s7t"><sup><a href="#fn2464ho15s7t">[5]</a></sup></span></li></ul><p>仅使用 interp 来对抗欺骗并不是唯一的方法：</p><ul><li><strong>解释并不是研究欺骗的唯一方法。</strong>以下是其他被忽视的范例：<ul><li><strong>对抗性攻击</strong>（基本上<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw%26themeRefresh%3D1"><u>是雷德伍德的计划</u></a>）。许多重要的对抗策略不依赖于<a href="https://arxiv.org/abs/2306.09983"><strong><u>一致性检查</u></strong></a>、<a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/"><u>人工智能检查和平衡</u></a>以及定期提示<span class="footnote-reference" role="doc-noteref" id="fnrefrbnra8fsacj"><sup><a href="#fnrbnra8fsacj">[6] 等</a></sup></span>插值。</li><li><strong>创建一个欺骗性对齐或模拟的玩具模型：</strong>研究“可能构成生存威胁的各种失败的体外演示”，来自<a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>《错位模型有机体》</u></a> ，这是一篇关于此的最新详细文章。</li><li><strong>寻找欺骗性联盟代理：</strong>例如，监控以下<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default%23Foundational_properties_for_deceptive_alignment"><u>4 个标准</u></a>，这些标准通常被视为欺骗性联盟的先决条件：目标导向的行为、跨事件/长期目标范围的优化、基本目标的概念化、情境意识。</li><li>接近人类水平的人工智能的各种基线策略：<ul><li>在 NLP 中，监控 AutoGPT 的计划和思路（ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#kix.cyunx152z1bn"><u>细节</u></a>和注意事项）。</li><li>愿景中，类EfficientZero的一致性损失——在蒙特卡罗树中展望未来（ <a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23bookmark%3Did.dgd983y7t1ym"><u>草案</u></a>）</li></ul></li><li><a href="https://twitter.com/StephenLCasper/status/1650662566476279809"><u>更多</u></a>想法，如神经蒸馏和<a href="https://www.alignmentforum.org/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive%23How_to_actually_implement_a_speed_prior"><u>速度优先</u></a>。</li><li>如果 DeepMind 今天宣布他们在 GPT 中发现了欺骗行为，他们不太可能只使用可解释性来发现这一发现。他们更有可能使用定期提示。</li></ul></li><li><strong>有一些针对欺骗性联盟的预防措施</strong>似乎更可行（请参阅<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>针对欺骗的预防措施</u></a>部分）。</li><li><strong>概念上的进步更为紧迫。</strong>从概念上思考欺骗比通过可解释性思考要富有成效得多。据我所知，可解释性还没有教会我们任何关于欺骗的知识。<ul><li>例如，<a href="https://www.lesswrong.com/tag/simulator-theory"><u>模拟器理论</u></a>和对<i>GPT 已经可以模拟欺骗性拟像</i>的理解是我们对欺骗性对齐的理解比欺骗性可解释性方面所发生的进步更大的进步。</li><li>关于欺骗性对齐的概念性考虑，如文章<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default"><u>《默认情况下欺骗性对齐的可能性 &lt;1%》</u></a>中所示，完全不依赖于可解释性。 </li></ul></li></ul><p></p><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:273.72px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/dkw7e0dp4pangmxj7ei4" alt=""></p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:268.46px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/ct8q2lqan4o5dhrw6bcf" alt=""></p></td></tr></tbody></table></figure><p><i>受到我与捍卫 interp 的朋友们进行的每一次讨论的启发。 “你对天文学的论证太笼统了”，所以让我们在下一节中深入探讨一些物体级的论证！</i></p><h1>可解释性的最终故事是什么样的？这一点都不清楚。</h1><p><i>这部分技术性比较强。可以先跳过它，或者只阅读“枚举安全性？”部分。这非常重要。</i></p><p>当然，深度学习中的可解释性似乎本质上比神经科学更可行，因为我们可以保存所有激活并非常缓慢地运行模型，通过尝试因果修改来了解正在发生的事情，并且比功能磁共振成像允许更多的控制。但在我看来，这还不够——我们甚至不知道我们的目标是什么。我们的目标是：</p><h2>枚举安全性？</h2><p>正如 Neel Nanda <a href="https://www.lesswrong.com/posts/qgK7smTvJ4DB8rZ6h/othello-gpt-future-work-i-am-excited-about#:~:text=enumerative%20safety%2C%20the%20idea%20that%20we%20might%20be%20able%20to%20enumerate%20all%20features%20in%20a%20model%20and%20inspect%20this%20for%20features%20related%20to%20dangerous%20capabilities%20or%20intentions.%20Seeing%20whether%20this%20is%20remotely%20possible%20for%20Othello%2DGPT%20may%20be%20a%20decent%20test%20run."><strong><u>所说</u></strong></a><strong>，枚举安全</strong><strong>是指我们可以枚举</strong>模型中的<i><strong>所有</strong></i><strong>特征</strong>，并检查其是否存在与危险能力或意图相关的特征。我认为这个策略从一开始就注定了（从最重要到不太重要）：</p><ul><li><strong>确定某个特征的危险性是一个错误指定的问题。</strong>在网络的权重/结构中搜索危险特征是毫无意义的。一个特性本身并没有好或坏之分。单个原子的危险并不能有力地预测原子和分子组装的危险。例如，如果你想象第53层、第127通道的特征，它看起来像一把枪，这是否意味着你的系统很危险？或者您的系统是否能够识别危险的枪支？认知可以<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach:~:text=And%20cognition%20can%20be%20externalized."><u>外化</u></a>这一事实也有助于这一点。</li><li><strong>特征仍然是一个模糊的概念</strong>，叠加问题和自然抽象假设在<a href="https://distill.pub/2020/circuits/zoom-in/"><u>Distill</u></a>论文发表三年后仍然是一个假设，很少有令人信服的策略来解决它们。这并不奇怪：可解释性的核心概念“特征”似乎本质上是模糊的，并且仍然没有定义。这是“枚举安全”策略以及对神经元进行逐一迭代以验证每个特征的“良好性”并获得保证的一个主要问题：<ul><li>并且由于<a href="https://arxiv.org/abs/2209.10652"><u>叠加</u></a>，迭代每个神经元是无效的。因此，我们不能只迭代神经元，而是必须迭代所有神经元集（或更糟糕的是所有方向），这在计算上是完全难以处理的。</li></ul></li><li><strong>危险模型的属性不是低级特征，而是高级行为能力，</strong>例如编码能力、 <a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>阿谀奉承</u></a>或各种心理代理理论、态势感知或黑客攻击。<ul><li>网络的态势感知可能包括几个子特征，例如日期和时间、地理位置以及用户的当前需求。删除这些子功能会降低模型的竞争力。</li></ul></li><li><a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness"><strong><u>深度欺骗性</u></strong></a>——简单来说，由于优化压力以及模型与环境之间的复杂交互，即使没有任何单个部分是危险的，系统也可能具有欺骗性。</li><li><strong>这种策略已经通过自动解释技术在视觉上尝试过</strong>，以标记所有神经元，并且它似乎没有太多高级对齐，并且大多数神经元都回避简单的解释：<ul><li> NetDisect 和神经元的组成解释（Mu 和 Andreas，2021）</li><li>深度视觉特征的自然语言描述（Andreas，2022）</li><li> Clip-Dissect（Oikarinen，2022）<a href="https://visualvocab.csail.mit.edu/"><u>走向 GAN 潜在空间的视觉概念词汇</u></a>（Schwettmann，2021）</li><li>这些工作[ <a href="https://www.lesswrong.com/posts/XZfJvxZqfbLfN6pKh/introductory-textbook-to-vision-models-interpretability"><u>此处</u></a>部分总结]并没有改变我们在实践中尝试使视觉系统更加强大且风险更低的方式。</li></ul></li><li>大多数自动解释性工作，例如<a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models"><u>语言模型可以解释来自 OpenAI 的语言模型中的神经元</u></a>或概念擦除技术，都属于这一类。</li></ul><h2>逆向工程？</h2><p>逆向工程是可解释性的典型例子，但我没有看到成功的前进方向。这会是：</p><ul><li>该模型的<strong>等效 C++ 注释算法</strong>是什么？能够通过一些模块化 C++ 代码重现 GPT-4 的难以理解的矩阵的功能已经超出了人类的智能水平，这<a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>太危险</u></a>了，因为这将允许很多不同的优化，并且可能允许递归自我- 改进似乎很危险，特别是如果我们依赖自动化流程来实现这一点。</li><li><strong>对模型行为的通俗解释</strong>？在哪个粒度级别？每个标记、句子或段落？这实在是不清楚。</li><li>通过高级解释获得的模型的<strong>功能连接组</strong>？好吧，您在功能连接组中看到该模型能够编码和破解，而这些都是危险的功能。这不就是常规的评估吗？<ul><li>在实践中，为了进行插值实验，我们几乎总是从创建提示数据集开始。也许有一天我们不需要提示来激活这些功能，但我不认为（即使是原则上）这种情况会很快发生。</li></ul></li><li>一张<strong>图</strong>来解释电路？像下面这样的图表可能会让人不知所措，但仍然非常有限。</li></ul><p>您可以注意到，“枚举安全性”通常隐藏在“逆向工程”结局背后。 </p><p><img style="width:599.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/hpuwlgsxjtrumo38ckav" alt=""></p><p><i>来自</i><a href="https://arxiv.org/abs/2211.00593"><i><u>IOI论文</u></i></a><i>。从 Wang 等人的“Interpretability in the Wild”中理解该图。 2022 年对于我们的讨论来说并不重要。了解完整的电路和所使用的方法需要</i><a href="https://www.youtube.com/watch?v%3Dgzwj0jWbvbo"><i><u>三个小时的视频</u></i></a><i>。而且，此分析仅关注单个标记并涉及大量简化。例如，虽然我们试图解释为什么标记“Mary”比“John”更受欢迎，但我们没有深入研究为什么模型最初考虑“Mary”或“John”。此外，此分析仅基于 GPT2-small。</i> </p><p></p><p><img style="width:385.83px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/mmmq7xwly9mufts4101o" alt=""></p><p><i>确实，这个数字是相当可怕的。来自</i><a href="https://www.lesswrong.com/posts/j6s9H9SHrEhEfuJnq/causal-scrubbing-results-on-induction-heads"><i><u>因果擦洗：感应头上的结果</u></i></a><i>，适用于 2 层模型。经过 4 次精炼假设，他们能够挽回 86% 的损失。但即使对于这个简单的任务，他们也表示“我们最终不会得出完全具体或完全人类可以理解的假设，因果清理将使我们能够验证模型的哪些组件和计算是重要的。”。</i></p><p>在上面的两个玩具示例中，逆向工程已经如此困难，这一事实似乎令我感到担忧。</p><h2>奥拉的可解释性梦想？</h2><p>又或许 interp 只是一场由好奇心驱动的探索，等待着意外的发现？</p><ul><li><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>可解释性梦想</u></a>是 Chris Olah 关于机械可解释性未来目标的非正式说明。它讨论了<strong>叠加</strong>，即可解释性的敌人。然后，在注释的末尾，在标题为“<a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23safety"><u>机械可解释性如何适应安全性？”的</u></a>部分中。 ”，我们理解该计划是为了解决叠加能够使用以下公式： <br><br><img style="width:526.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/bbzkzi7n1cc1w6cvueju" alt=""></li><li>但这又是用电路而不是功能来表述的“<i>枚举安全性”</i> 。然而，正如上面所解释的，我认为这不会给我们带来任何好处。</li><li>笔记的最后一部分“ <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23aesthetics"><u>美丽与好奇</u></a>”读起来就像一首诗或一首美丽的赞歌。然而，除了偶然发现的希望之外，它似乎缺乏实质内容。</li></ul><p>总的来说，我对 Anthropic 使用字典学习方法来解决叠加问题持怀疑态度。虽然他们的负面结果很有趣，并且他们正在努力解决围绕“功能”概念的概念性困难（如其<a href="https://transformer-circuits.pub/2023/may-update/index.html%23superposition-dictionary"><u>5 月更新</u></a>中所述），但我仍然对这种方法的有效性持怀疑态度，即使在阅读了他们<a href="https://transformer-circuits.pub/2023/july-update/index.html%23safety-features"><u>最近的 7 月更新</u></a>后也是如此。仍然没有解决我对枚举安全性的反对意见。</p><p> Olah<a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">建议的</a>一个潜在解决方案是自动化研究：“<i>似乎很可能方法的类型 [...] 最终将不够，并且可解释性可能需要依赖人工智能自动化</i>”。然而，我相信这种自动化是潜在有害的[ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>有害</u></a>部分]。</p><p>这仍然是一个正在发展的故事，在 Distill 上发表的论文总是令人读起来很愉快。然而，我仍然对押注这种方法犹豫不决。</p><h2>重新定位搜索？</h2><p>或者也许 interp 对于<a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"><strong><u>重定向搜索</u></strong></a><strong>很有用</strong><strong>？</strong> This idea suggests that if we find a goal in a system, we can simply change the system&#39;s goal and redirect it towards a better goal.</p><p> I think this is a promising quest, even if there are still difficulties:</p><ul><li> This is interesting because this would be a way to not need to fully reverse engineer a complete model.<strong> </strong>The technique used in <a href="https://www.alignmentforum.org/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network"><u>Understanding and controlling a maze-solving policy network</u></a> seems promising to me. Just focusing on “the motivational API” could be sufficient.</li><li> But I still don&#39;t know if <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vecto"><u>Steering vectors</u></a> (ie activation additions of a vector in the latent space) really count as interpretability, and really change significantly the picture of alignment beyond just prompt engineering. Ok, this is a new way to tinker with the model. But I don&#39;t know how this could be used reliably against deception. <span class="footnote-reference" role="doc-noteref" id="fnrefmf7vlk6ib69"><sup><a href="#fnmf7vlk6ib69">[7]</a></sup></span></li></ul><h2> Relaxed adversarial training?</h2><p> <strong>Relaxed adversarial training?</strong> The TL;DR is that <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>relaxed adversarial training</u></a> is the same as adversarial training, but instead of creating adversarial inputs to test the network, we create adversarial latent vectors. This could be useful because creating realistic adversarial inputs is a bottleneck in adversarial training. [More explanations <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.2ats8akz8z6u"><u>here</u></a> ]</p><p> This seems valid but very hard, and there are still significant <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>conceptual difficulties</u></a> . A concrete approach, <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> , has been proposed, and seems to be promising but:</p><ul><li> <strong>The procedure is underspecified</strong> . <strong>There will be too many meta-parameters</strong> . Calibrating these meta-parameters will be a nightmare, and you probably don&#39;t want to iterate on deceptive powerful models. We have to be good right away from the first choice of meta-parameters. As the author himself says, &quot; <i>the only hope here lies in the Surgeon forcing the model to be robustly safe before it learns to deceive. Once the model is deceptive it&#39;s really game-over.</i> &quot;</li><li> <strong>We still have no guarantees.</strong> This procedure allows for a latent space that is robust to “small perturbations”, but being robust to “small perturbations” is <i>not the same as not becoming deceptive</i> (it&#39;s not clear to me that deception won&#39;t appear outside the constraint zone).</li><li> Papers using this kind of procedure have only limited effectiveness, for example around 90% detection rate in the paper <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (Liu et al., 2019)</u></a> . [Paper summarized <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.ewixtcjxjqvq"><u>here</u></a> ] And I don&#39;t think this could work against all types of trojans.</li></ul><p> The exact procedure described in <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> hasn&#39;t been tested, as far as I know. So we should probably work on it. <span class="footnote-reference" role="doc-noteref" id="fnrefc2q5uxqhj6j"><sup><a href="#fnc2q5uxqhj6j">[8]</a></sup></span></p><h2> Microscope AI?</h2><p> <strong>Maybe Microscope AI ie</strong> Maybe we could directly use the AI&#39;s world model without having to understand everything. Microscope AI is an AI that would be used not in inference, but would be used just by looking at its internal activations or weights, without deploying it. My definition would be something like: We can run forward passes, but only halfway through the model.</p><ul><li> This goes against almost every economic incentive (see <a href="https://gwern.net/tool-ai%23:~:text%3DAn%2520Agent%2520AI%2520has%2520the,its%2520outputs%252C%2520on%2520harder%2520domains."><u>Why Tool AIs wants to become Agents AI</u></a> , from Gwern).</li><li> <strong>($) Interpretability has been mostly useless for discovering facts about the world, and learning new stuff by only looking at the weights is too hard.</strong><ul><li> In the paper <a href="https://arxiv.org/abs/2111.09259"><u>Acquisition of Chess Knowledge in AlphaZero</u></a> , the authors investigate whether “ <i>we can learn chess strategies by interpreting the trained AlphaZero&#39;s <strong>behavior</strong></i> ”. Answer: This is not the case. They probe the network using only concepts already known to Stockfish, and no new fundamental insights are gained. We only check <i>when</i> AlphaGo learns human concepts during the training run.</li><li> I don&#39;t think we will be able to learn category theory by reverse engineering the brain of Terence Tao. How do Go players learn strategies from go programs? Do they interpret AlphaGo&#39;s weights, or do they try to understand the behavioral evaluations of those programs? Answer: They learn from their behavior, but not by interpreting models. I am skeptical that we can gain radically new knowledge from the weights/activations/circuits of a neural network that we did not already know, especially considering how difficult it can be to learn things from English textbooks alone.</li></ul></li><li> <strong>Microscope AIs should not be agentic by definition. But agency and exploration help tremendously at the human level for discovering new truths. Therefore, below superhuman level, the</strong> <i><strong>microscope</strong></i> <strong>needs to be</strong> <i><strong>agentic</strong></i> <strong>…and this is a contradiction.</strong> Using Microscope AI as a tool rather than an agent is suggested <a href="https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a%23Philosophical_framing"><u>here</u></a> or <a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>here</u></a> for example. However, to know the truth of a complex fact, we need to experiment with the world and actively search for information. Here is a fuzzy reasoning (feel free to skip):<ul><li> A) Either <strong>the information already exists and is written plainly</strong> somewhere on the internet, and in that case, there is no need for Microscope AI (this is like text retrieval).</li><li> B) Or <strong>the information doesn&#39;t exist anywhere on the internet</strong> , and in that case, it is necessary to be agentic by experimenting with the world or by thinking actively. This is the type of feature that can only be “created” by reinforcement learning but which cannot be “discovered” with supervised learning, like MuZero discovering new chess strategies.</li><li> or C), <strong>this info is not plainly written but is a deep feature of the training data</strong> that could be understood/grokked through gradient descent. This is the type of feature that can be “discovered” with supervised learning.</li><li> If B), we need agency, and it&#39;s no longer a microscope.</li><li> If C), we can apply the above reasoning <u>($)</u> + Being able to achieve this through pure gradient descent without exploration is probably a higher level of capability than being able to do it with exploration. (This would be like discovering the <a href="https://en.wikipedia.org/wiki/Quaternion"><u>Quaternion</u></a> formula during a dream?). But even legendary mathematicians need to work a bit and be agentic in their exploration; they don&#39;t just passively read textbooks. Therefore, it&#39;s probably beyond Ramanujan&#39;s level and too dangerous?</li><li> So, I&#39;m quite uncertain, but overall I don&#39;t think Microscope AI is a promising or valid approach to reducing AI risk.</li></ul></li></ul><p> A short case study of <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><u>Discovering Latent Knowledge</u></a> technique to extract knowledge from models by probing is included in the <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Burns_et_al___2022_"><u>appendix</u></a> .</p><h1> Preventive measures against Deception seem much more workable</h1><p> TL;DR: It would be more desirable to aim for a world where we wouldn&#39;t need to delve into the internal components of models. Prevention is better than cure, or at least, it is a neglected strategy. </p><figure class="image image_resized" style="width:599.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lon9yhtv1wgba17yhryt" alt=""><figcaption> <i>From</i> <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23The_Translucent_Thoughts_Hypotheses"><i><u>The Translucent Thoughts Hypotheses and Their Implications</u></i></a> <i>, Fabien Roger</i></figcaption></figure><p> I don&#39;t believe interpretability is the most promising method for monitoring near human-level AGI, Here&#39;s why:</p><p> <strong>I don&#39;t think neural networks will be able to take over in a single forward pass. Models will probably reason in English and will have translucent thoughts</strong> (and we could even hope to get transparent thoughts, translucence is the scary place where steganography is possible). In order to devise an effective plan to destroy the world, the first human-level AGI will need to somewhat externalize its reasoning, by chain-of-Thought (CoT), and it seems to me way easier to monitor this chain-of-Thought than probing into the models&#39; internals. We can probably use <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications"><u>The Translucent Thoughts Hypotheses</u></a> (Fabien, the author, gives a 20% chance, but I think it will be more like 60%). I also think that we have to do everything we can to pass regulations and <strong>place ourselves in a world where those hypotheses are true.</strong></p><p> For example, magic is possible (here&#39;s <a href="https://www.youtube.com/watch?v%3DEAN-PwRfJcA"><u>my favorite trick</u></a> btw), and the fact that human brains can be manipulated so easily by magic is a great danger. So we should probably try to make this magic unlikely, for example with process-based training by training AI Systems to reason only step-by-step (the process-based training approach is summarized in this <a href="https://www.lesswrong.com/posts/FFz6H35Gy6BArHxkc/task-decomposition-for-scalable-oversight-agisf-distillation"><u>AGISF distillation</u></a> ).</p><h2> Steering the world towards transparency</h2><p> <strong>Thinking ahead about the training setup is very important and possibly dominates interp considerations</strong> because if the concepts manipulated by the models are totally alien, it will be much, much harder to provide oversight. And it is <a href="https://aligned.substack.com/i/88447351/the-ai-tech-tree-is-looking-favorably"><u>much</u></a> easier to align those chatbots pretrained on human generated text than aligning AGIs trained from scratch with RL / evolutionary methods, etc.</p><p> If this is the case, we should focus on various aspects and important recommendations as detailed by Fabien <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23Research_Directions"><u>here</u></a> . Here are some highlights:</p><ul><li> <strong>Making LLMs use only understandable English:</strong> Preventing models from using steganography, not training on synthetic data, etc… These directions are generally very neglected - for example, look at this <a href="https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId%3DS4wqnxW5CYd8QsTox"><u>comment</u></a> .</li><li> <strong>Building an oversight system:</strong> That&#39;s the “ <a href="https://youtu.be/YTlrPeikoyw?t%3D2415"><u>ready to go</u></a> ” technical research of Buck, IDA, or neglected baselines like <a href="https://arxiv.org/abs/2308.07308"><u>filtering</u></a> , etc…</li><li> <strong>Testing the translucent thoughts hypotheses</strong> and being able to monitor the alienness of models is super important in ensuring some simulacra have brain like representations (see <a href="https://www.lesswrong.com/posts/iXbPe9EAxScuimsGh/linkpost-scaling-laws-for-language-encoding-models-in-fmri"><u>this</u></a> and <a href="https://www.lesswrong.com/posts/2QexGHrqSxcuwyGmf/linkpost-large-language-models-converge-on-brain-like-word"><u>this</u></a> ). More work in the direction of <a href="https://arxiv.org/abs/2305.04388"><u>Language Models Don&#39;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</u></a> .</li><li> <strong>Making the hypotheses true</strong> , with technical works, outreach and governance, etc.</li></ul><p> <strong>I think the strategy should be: let&#39;s target a world where deception is unlikely.</strong> (I&#39;m not saying we should make plans that work conditional on deception being unlikely by default, but we should try to steer AGI/the world towards a place where deception is unlikely). I believe there are multiple ways to think and address this problem, and much more technical research needed here, starting from <a href="https://arxiv.org/abs/2302.00805"><u>Conditioning Predictive Models: Risks and Strategies</u></a> .</p><h2> Cognitive Emulations - Explainability <i>By Design</i></h2><p> <strong>If interpretability was really a bottleneck, we could use</strong> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>cognitive emulation</u></strong></a> <strong>, which, in my opinion, allows way better explainability and transparency than interpretability will ever get us.</strong></p><p> My understanding of cognitive emulation: Emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in <i>plain English</i> to each other before outputting the next token. If the neural network had deceptive thoughts, we could see them in these intermediate messages.</p><p> Some caveats are in the section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design"><u>Cognitive Emulation</u></a> of the appendix.</p><h1> Interpretability May Be Overall Harmful</h1><p> (Note that some of the following points are not specific to interp, but I think they apply particularly well to interp.)</p><p> <strong>False sense of control:</strong></p><ul><li> <strong>False sense of understanding.</strong> It&#39;s too easy to think you begin to understand that we&#39;re starting to get guarantees when we have not much. This is very classic:<ul><li> Me from the past: &quot;Yo, I spent 5 hours trying to understand the mechanisms of inductions head and K-Compositions with the incomprehensible math formula of the <a href="https://transformer-circuits.pub/2021/framework/index.html"><u>Mathematical Framework for Transformers</u></a> , I have so much more understanding.&quot; yes but no.</li></ul></li><li> <strong>Overinterpretation.</strong> It is very difficult to say which interpretation result is solid. For example, <a href="https://arxiv.org/abs/1810.03292"><u>Sanity Checks for Saliency Maps</u></a> shows that most of the pixel attribution techniques are generally misleading. <span class="footnote-reference" role="doc-noteref" id="fnrefpo4e41md3r"><sup><a href="#fnpo4e41md3r">[9]</a></sup></span> In the same vein, feature viz has recently been found to have some pretty fatal flaws, see <a href="https://arxiv.org/abs/2306.04719"><u>Don&#39;t trust your eyes: on the (un)reliability of feature visualizations</u></a> , and the model editing technique such as <a href="https://rome.baulab.info/"><u>ROME</u></a> is <a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><u>very misleading</u></a> .  This is mostly due to methodological problems that Stephen Casper explains in his sequence. [see appendix: <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Methodological_problems_"><u>methodological problems</u></a> ].</li><li> <strong>Safety Washing.</strong> I feel that there is a part of safety research which is here to legitimize capability research in the big labs (although this is not entirely specific to interp).<ul><li> <i>“I think a really substantial fraction of people who are doing &quot;AI Alignment research&quot; are instead acting with the primary aim of &quot;make AI Alignment seem legit&quot;. These are not the same goal, a lot of good people can tell and this makes them feel kind of deceived, and also this creates very messy dynamics within the field where people have strong opinions about what the secondary effects of research are, because that&#39;s the primary thing they are interested in, instead of asking whether the research points towards useful true things for actually aligning the AI”,</i> from <a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Shutting Down the Lightcone Offices</u></a> .</li></ul></li><li> <strong>The achievements of interp research are consistently graded on their own curve and  overhyped</strong> compared to achievements in other fields like adversaries research. For example, the recent paper <a href="https://arxiv.org/abs/2307.15043"><u>Universal and Transferable Adversarial Attacks on Aligned Language Models</u></a> impressively found effective attacks against state-of-the-art models without any interpretations involving models internals. Imagine if mechanistic interpretability researchers did the exact same thing, but by studying model internals? Given the excitement that has emerged in the past around the achievements of mechanistic interpretability in toy models on cherry-picked problems (eg <a href="https://arxiv.org/abs/2301.05217"><u>this</u></a> or <a href="https://arxiv.org/abs/2211.00593"><u>this</u></a> ), it seems that something like this would have probably made the AI safety research community go wild. Stephen Casper makes a similar point <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/MyvkTKfndx9t4zknh%23:~:text%3DFrom%2520an%2520engineer%25E2%2580%2599s%2520perspective%252C%2520it%25E2%2580%2599s%2520important%2520not%2520to%2520grade%2520different%2520classes%2520of%2520solutions%2520each%2520on%2520different%2520curves.%25C2%25A0"><u>here</u></a> : “ <i>From an engineer&#39;s perspective, it&#39;s important not to grade different classes of solutions each on different curves.</i> ” And other examples of this are presented here <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_:~:text%3Dtechniques%2520on%2520curves...-,Imagine%2520that%2520you%2520heard%2520news%2520tomorrow%2520that%2520MI%2520researchers%2520from%2520TAISIC%2520meticulously%2520studied%2520circuits%2520in%2520a%2520way%2520that%2520allowed%2520them%2520to%25E2%2580%25A6,-Reverse%2520engineer%2520and"><u>EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety</u></a> (thanks to Stephen for highlighting this point).</li></ul><p> <strong>The world is not coordinated enough for public interpretability research:</strong></p><ul><li> <strong>Dual use.</strong> It seems anything related to information representation can be used in a dual manner. This is a problem because I believe that the core of interpretability research could lead to major advances in capabilities. See this <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>post</u></a> .<ul><li> Using the insights provided by advanced interp to improve capabilities, such as modularity to optimize inference time and reduce flops, is likely to be easier than using them for better oversight. This is because <strong>optimizing for capability is much simpler than optimizing for safety</strong> , as we lack clear metrics for measuring safety.</li></ul></li><li> <strong>When interpretability starts to be useful, you can&#39;t even publish it because it&#39;s too info hazardous.</strong> The world is not coordinated enough for public interpretability research.<ul><li> Nate Soares <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous"><u>explained</u></a> this, and this was followed by <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>multiple</u></a> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>posts</u></a> . <i>“Insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research</i> <a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><i><u>closed</u></i></a> <i>. […] I acknowledge that public sharing of research insights could, in principle, both shorten timelines and improve our odds of success. I suspect that</i> <a href="https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"><i><u>isn&#39;t the case in real life</u></i></a> <i>.”</i></li><li> Good interp could produce a &quot;foom overhang&quot; as described in &quot; <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a> &quot;.</li><li> Good interp also creates an<a href="https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful"><u>infosec/infohazard attack vector</u></a> .</li><li> The post &#39; <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When is Interpretability Work Dangerous?</u></a> &#39; ends on a sobering note, stating, “ <i>In closing, if alignment-conscious researchers continue going into the interpretability subfield, the probability of AGI ruin will tend to increase.</i> ”</li></ul></li><li> <strong>Interpretability already helps capabilities.</strong> For example, the understanding of Induction head has allowed for <a href="https://twitter.com/NeelNanda5/status/1618185819285778433"><u>better</u></a> architectures <span class="footnote-reference" role="doc-noteref" id="fnrefplu4ji16iui"><sup><a href="#fnplu4ji16iui">[10]</a></sup></span> .</li><li> Interpretability may be a <a href="https://en.wikipedia.org/wiki/Wicked_problem%23Super_wicked_problems"><u>super wicked problem</u></a> <span class="footnote-reference" role="doc-noteref" id="fnrefw7s6gsvuwb"><sup><a href="#fnw7s6gsvuwb">[11]</a></sup></span> .</li></ul><p> Thus the list of &quot;theory of impact&quot; for interpretability should not simply be a list of benefits. It&#39;s important to explain why these benefits outweigh the possible negative impacts, as well as how this theory can save time and mitigate any new risks that may arise. </p><p></p><p><img style="width:414.07px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zao0mylkanwh60aajinm" alt=""></p><p> <i>The concrete application of the</i> <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"><i><u>logit lens</u></i></a> <i>is not an oversight system for deception, but rather capability works to accelerate inference speed like in</i> <a href="https://twitter.com/GoogleAI/status/1603845007663734785"><i><u>this paper</u></i></a> <i>. (Note that the paper does not cite logit lens, but relies on a very similar method).</i></p><h1> Outside view: The proportion of junior researchers doing interp rather than other technical work is too high</h1><p> It seems to me that many people start alignment research as follows:</p><ul><li> At the end of <a href="https://www.arena.education/"><u>Arena</u></a> , an advanced upskilling program in AI Safety, almost all research projects this year (June 2023), except for two out of 16, were interp projects.</li><li> At <a href="https://ia.effisciences.org/"><u>EffiSciences</u></a> , at the end of the last 3 <a href="https://www.lesswrong.com/posts/DkDy2hvkwbQ54GM9u/introducing-effisciences-ai-safety-unit-1"><u>ML4Good</u></a> bootcamps, students all start by being interested in interp, and it is a very powerful attractor. I myself am guilty. I have redirected too many people to it. I am now trying to correct my ways.<ul><li> In the past, if I reconstruct my motivational story, it goes something like this: &quot;Yo, I have a math/ML background, how can I recycle that?&quot; -->; then <i>brrr interp</i> , without asking too many questions.</li></ul></li><li> During <a href="https://apartresearch.com/"><u>Apart Research</u></a> hackathons, interpretability hackathons tend to draw 3.12 times as many participants as other types of hackathons. (thinkathon, safety benchmarks, …). <span class="footnote-reference" role="doc-noteref" id="fnrefavln8kyvlzg"><sup><a href="#fnavln8kyvlzg">[12]</a></sup></span></li><li> Interpretability streams in <a href="https://www.serimats.org/"><u>Seri Mats</u></a> are among the most competitive streams (see this <a href="https://twitter.com/leedsharkey/status/1656705667963535370"><u>tweet</u></a> ). People then try hard, get rejected, get disappointed and lose motivation. This is a recent important problem.</li></ul><p> &quot;Not putting all your eggs in one basket&quot; seems more robust considering our uncertainty, and there are more promising ways to reduce x-risk per unit of effort (to come in a future post, mostly through helping/doing governance). I would rather see a <strong>more diverse ecosystem</strong> of people trying to reduce risks. More on this in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>Technical Agendas with better ToI</u></a> .</p><p> If you ask me if interp is also over represented in senior researchers, I&#39;m a bit less confident. Interp also seems to be a significant portion of the pie: this year, while Conjecture and Redwood have partially pivoted, there are new active interp teams in Apollo, DeepMind, OpenAI, and still in Anthropic. I think I would particularly critique DeepMind and OpenAI&#39;s interpretability works, as I don&#39;t see how this reduces risks more than other works that they could be doing, and I&#39;d appreciate a written plan of what they expect to achieve.</p><h1> So far my best ToI for interp: Nerd Sniping?</h1><p> 1. <strong>Interp for Nerd Sniping/honeypot?</strong></p><ul><li> <strong>Interp is a highly engaging introduction to AI research</strong> . That&#39;s really cool for that, I use it for my <a href="https://www.master-mva.com/cours/seminaire-turing/"><u>classes</u></a> , and for technical outreach, but I already have enough material on interpretability, for 10 hours of class, no need to add more.</li><li> <strong>Interp as a honeypot for junior researchers?</strong> Just as a honeypot attracts bees with its sweet nectar, interp is very successful for recruiting new technical people! but then they would probably be better off doing something else than interp (unless it is their strong comparative advantage).</li><li> (Nerd Sniping senior capability researchers into interpretability research? Less capability research, more time to align AIs? I&#39;m joking, don&#39;t do that at home! )</li></ul><p> 2. <strong>Honorable mentions:</strong></p><ul><li> <strong>Showing strange failures</strong> , such as the issue with the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"><u>SolidGoldMagicCarp</u></a> token, highlights the possibility of unexpected results with the model. More generally, interpretability tools can be useful for the red teaming toolbox. They seem like they might be able to guide us to more problems than test sets and adversaries can alone.</li><li> <strong>Showing GPT is not a stochastic parrot?</strong> The article <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>Actually, Othello-GPT Has A Linear Emergent World Representation</u></a> <strong>&nbsp;</strong> is really cool <strong>.</strong> Showing that OthelloGPT contains a world model is really useful for technical outreach (even if OthelloGPT being good at Othello should be enough, no?).</li><li> <strong>It&#39;s a good way to introduce the importance and tractability of alignment research</strong> “ <i>Interpretability gives people a non-technical story for how alignment affects their lives, the scale of the problem, and how progress can be made. IMO no other approach to alignment is anywhere near as good for this.”</i> [from <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DuzBFJDsy9Jqkxzdnx"><u>Raymond D</u></a> ]</li><li> <strong>Better: Showing that “We have basically no idea how it does what it does.”,</strong> see this <a href="https://twitter.com/robertskmiles/status/1663534255249453056"><u>tweet</u></a> : </li></ul><p><img style="width:387.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/q9gbkwdp3drer7soei5a" alt=""></p><h1> Even if we completely solve interp, we are still in danger</h1><p> No one has ever claimed otherwise, but it&#39;s worth remembering to get the big picture. From stronger arguments to weaker ones:</p><ul><li> <strong>There are many X-risks scenarios, not even involving deceptive AIs.</strong> Here is a list of such scenarios (see this <a href="https://www.lesswrong.com/posts/nCeyBbhtJhToBFmrL/cheat-sheet-of-ai-x-risk"><u>cheat sheet</u></a> ):<ul><li> Christiano1 - <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom%23Part_I__You_get_what_you_measure"><u>You get what you measure</u></a></li><li> Critch1 - <a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic%23Part_1__Slow_stories__and_lessons_therefrom"><u>Production Web</u></a></li><li> Soares - <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>A central AI alignment problem: capabilities generalization, the sharp left turn</u></a></li><li> Cohen et al. - <a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>Advanced artificial agents intervene in the provision of reward</u></a></li><li> Gwern - <a href="https://gwern.net/fiction/clippy"><u>It Looks Like You&#39;re Trying To Take Over The World</u></a></li><li> Exercise: Here is <a href="https://www.safe.ai/ai-risk"><u>a list of risks</u></a> from the Center of AI Safety. Which ones can be solved by interp? At least half of those risks don&#39;t directly involve deception and interp.</li></ul></li><li> <strong>Total explainability of complex systems with great power is not sufficient to eliminate risks.</strong> Significant risks would still remain. Despite our full understanding of how atomic bombs function, they still pose substantial risks. See this <a href="https://en.wikipedia.org/wiki/List_of_nuclear_close_calls"><u>list of nuclear close calls</u></a> .</li><li> <strong>Interpretability implicitly assumes that the AI model does not optimize in a way that is adversarial to the user.</strong> Consider being able to read the mind of a psychopath like Voldemort. Would this make you feel safe? The initial step remains to box him. However, <strong>a preferable scenario would be not having to confront this situation at all.</strong> (this last claim is probably the most important lesson - see <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ). </li></ul><figure class="image image_resized" style="width:54.8%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/s6qlqvxf5an1gdofmmk3" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qgopxuvs7mrmxegq8nco 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/smtffxyqwgnqhsasg4fy 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lrifkrnrib0sybu8nvo8 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/j9jd3of3dwrmqpyg6xhr 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/cuouqap73vyebun1hfu7 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/e3ppnnd6cgyt2czjf3l6 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/vozp1plvffdzhxsuh1qy 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qywwfgebl65ejalxo5ps 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/araz4zygcvf4szuqkmy4 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/spahqo0hllay5zjyvlk0 1010w"><figcaption> Pytorch hooks can be used to study the internals of models. Are they going to be sufficient? <i>Idk, but</i> <a href="https://www.youtube.com/watch?v=LveUcCBRrSo"><i><u>Hook Me up Baby</u></i></a> <i>, from the album “Take Me as I Am” could be the national anthem of interp.</i></figcaption></figure><p> <strong>That is why focusing on coordination is crucial! There is a level of coordination above which we don&#39;t die - there is no such threshold for interpretability.</strong> We currently live in a world where coordination is way more valuable than interpretability techniques. So let&#39;s not forget that <a href="https://www.alignmentforum.org/posts/FfTxEf3uFPsZf9EMP/avoiding-perpetual-risk-from-tai%23Non_alignment_aspects_of_AI_safety_are_key_"><u>non-alignment aspects of AI safety are key!</u></a> AI alignment is only a subset of AI safety! ！ (I&#39;m planning to deep-dive more into this in a following post).</p><p> A version of this argument applies to &quot;alignment&quot; in general and not just interp and those considerations will heavily influence my recommendations for technical agendas.</p><h1> Technical Agendas with better ToI</h1><p> Interp is not such a bad egg, but opportunity costs can be huge (especially for researchers working in big labs).</p><p> I&#39;m not saying we should stop doing technical work. Here&#39;s a list of technical projects that I consider promising (though I won&#39;t argue much for these alternatives here):</p><ul><li> <strong>Technical works used for AI Governance.</strong> A huge amount of technical and research work needs to be done in order to make regulation robust and actually useful. The governance section of <a href="https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice#Governance_work"><u>AGI safety career advice</u></a> by Richard Ngo is really great : “ <i>It&#39;s very plausible that, starting off with no background in the field, within six months you could write a post or paper which pushes forward the frontier of our knowledge on how one of those topics is relevant to AGI governance.</i> ”<ul><li> For example, each of the measures proposed in the paper <a href="https://arxiv.org/abs/2305.07153"><u>towards best practices in AGI safety and governance: A survey of expert opinion</u></a> could be a pretext for creating a specialized organization to address these issues, such as auditing, licensing, and monitoring.</li><li> Scary demos (But this shouldn&#39;t involve gain-of-function research. There are already many powerful AIs available. Most of the work involves video editing, finding good stories, distribution channels, and creating good memes. Do not make AIs more dangerous just to accomplish this.).</li><li> In the same vein, <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a> is probably good because “ <a href="https://www.lesswrong.com/posts/vavnqwYbc8jMu3dTY/ai-coordination-needs-clear-wins"><u>AI coordination needs clear wins</u></a> ”.</li><li> Interoperability in AI policy, and good definitions usable by policymakers.</li><li> Creating benchmarks for dangerous capabilities.</li><li> Here&#39;s a <a href="https://docs.google.com/document/d/1Tvz2JS8CZ51TW-vfU3vwRn8dpK3F0UttdhMrZC2o7hw/edit#bookmark=id.jnvbactyuay7"><u>list</u></a> of other ideas</li></ul></li><li> <strong>Characterizing the technical difficulties of alignment. (</strong><a href="https://www.lesswrong.com/posts/uHYYA32CKgKT3FagE/hold-off-on-proposing-solutions"><strong><u>Hold Off On Proposing Solutions</u></strong></a> <strong>“Do not propose solutions until the problem has been discussed as thoroughly as possible without suggesting any.”)</strong><ul><li> Creating the <a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change"><u>IPCC</u></a> of AI Risks</li><li> More red-teaming of agendas</li><li> Explaining problems in alignment.</li></ul></li><li> Adversarial examples, adversarial training, latent adversarial training (the only <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>end-story</u></a> I&#39;m kind of excited about). For example, the papers &quot; <a href="https://arxiv.org/abs/2210.04610"><u>Red-Teaming the Stable Diffusion Safety Filter</u></a> &quot; or &quot; <a href="https://arxiv.org/abs/2307.15043"><u>Universal and Transferable Adversarial Attacks on Aligned Language Models</u></a> &quot; are good (and pretty simple!) examples of adversarial robustness works which contribute to safety culture.</li><li> <strong>Technical outreach</strong> . <a href="https://www.youtube.com/@ai-explained-"><u>AI Explained</u></a> and <a href="https://www.youtube.com/c/robertmilesai"><u>Rob Miles</u></a> have plausibly reduced risks  more than all interpretability research combined.</li><li> In essence, ask yourself: “What would Dan Hendrycks do?”<ul><li> Technical newsletter, non-technical newsletters, benchmarks, policy recommendations, risks analysis, banger statements, courses and technical outreach.</li><li> He is not doing interp. Checkmate!</li></ul></li></ul><p> In short, my agenda is <strong>&quot;Slow Capabilities through a safety culture&quot;</strong> , which I believe is robustly beneficial, even though it may be difficult. I want to help humanity understand that we are not yet ready to align AIs. Let&#39;s wait a couple of decades, then reconsider.</p><p> And if we really have to build AGIs and align AIs, it seems to me that it is more desirable to aim for a world where we don&#39;t need to probe into the internals of models. Again, prevention is better than cure.</p><h1>结论</h1><p>I have argued against various theories of impact of interpretability, and proposed some alternatives. I believe working back from the different risk scenarios and red-teaming the theories of impact gives us better clarity and a better chance at doing what matters. Again, I hope this document opens discussions, so feel free to respond in parts. There probably <i>should</i> be a non-zero amount of researchers working on interpretability, this isn&#39;t intended as an attack, but hopefully prompts more careful analysis and comparison to other theories of impact.</p><p> We already know some broad lessons, and we already have a general idea of which worlds will be more or less dangerous.Some ML researchers in top labs aren&#39;t even aware of, or acknowledging, that AGI is dangerous, that connecting models to the internet, encouraging agency, doing RL and maximizing metrics isn&#39;t safe in the limit.</p><p> Until civilization catches up to these basic lessons, we should avoid playing with fire, and should try to slow down the development of AGIs as much as possible, or at least steer towards worlds where it&#39;s done only by extremely cautious and competent actors.</p><p> Perhaps the main problem I have with interp is that it implicitly reinforces the narrative that we must build powerful, dangerous AIs, and then align them. For X-risks, prevention is better than cure. Let&#39;s <i>not</i> build powerful and dangerous AIs. We aspire for them to be safe, by design.</p><h1>附录</h1><h2>Related works</h2><p> There is a vast academic literature on the virtues and academic critiques of interpretability (see this <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>page</u></a> for plenty of references), but relatively little holistic reflection on interpretability as a strategy to reduce existential risks.</p><p> The most important articles presenting arguments for interpretability:</p><ul><li> <a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>Chris Olah&#39;s views on AGI safety</u></a></li><li> <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>A Longlist of Theories of Impact for Interpretability</u></a></li><li> <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>Interpretability Dreams</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous%23When_Interpretability_is_Still_Important"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/6ReBeYwsDeNgv6Dr5/the-defender-s-advantage-of-interpretability"><u>The Defender&#39;s Advantage of Interpretability</u></a></li><li> <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a></li></ul><p> Against interpretability</p><ul><li> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>Should we publish mechanistic interpretability research?</u></a></li><li> <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>EIS III: Broad Critiques of Interpretability Research</u></a></li><li> <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3"><u>EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety</u></a></li><li> <a href="https://link.springer.com/article/10.1007/s13347-019-00372-9"><u>Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning</u></a></li></ul><h3> The Engineer&#39;s Interpretability Sequence</h3><p> I originally began my investigation by rereading  “The Engineer&#39;s Interpretability Sequence”, in which Stephen Casper raises many good critiques of interpretability research, and this was really illuminating.</p><p> <strong>Interpretability tools lack widespread use by practitioners in real applications.</strong></p><ul><li> No interpretability technique is yet publicly known to have been used in production in SOTA models such as ChatGPT.</li><li> There have been interpretability studies of SOTA multimodal models such as <a href="https://distill.pub/2021/multimodal-neurons/"><u>CLIP</u></a> in the past, but these studies are only descriptive.</li><li> The efficient market hypothesis: The technique used for the censorship filter of the Stable Diffusion model was a <a href="https://arxiv.org/abs/2210.04610"><u>vulgar cosine similarity threshold</u></a> between generated image embeddings and a list of taboo concepts. Yes, this may seem a bit ridiculous, but at least there is a filter, and it appears that interp has not yet been able to provide more convenient tools than this.</li></ul><p> <strong>Broad critiques.</strong> He <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>explains</u></a> that interp is generally not scaling, relying too much on humans, failing to combine techniques. He also <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>criticizes</u></a> mech interp, which may not be the best way of doing interp, because of cherry-picking, focusing only on toy examples and lack of scalability, and failing to do useful things.</p><p> <strong>Methodological problems:</strong></p><ul><li> He <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/gwG9uqw255gafjYN4%23A_lack_of_practical_applications"><u>points out</u></a> , &quot;The root cause of this has much to do with interpretability research not being approached with as much engineering rigor as it ought to be.&quot;</li><li> One good point to note is that since the publication of his sequence, certain methods have become more rigorous. For example, <a href="https://www.lesswrong.com/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and"><u>Causal methods</u></a> , <a href="https://arxiv.org/abs/2211.00593"><u>activation patching</u></a> , <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>probing only with linear probes</u></a> are more rigorous and promising than descriptive techniques.</li><li> But I also believe that interpretability may be fundamentally flawed [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interp look like</u></a> ].</li></ul><h2> Cognitive Emulations - Explainability <i>By design</i></h2><p> <strong>If interpretability was really a bottleneck, we could use</strong> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>cognitive emulation</u></strong></a> <strong>, which, in my opinion, allows way better explainability/transparency than interpretability will ever get us.</strong></p><p> My understanding of cognitive emulation: Emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in <i>plain English</i> to each other before outputting the next token. If the neural network had deceptive thoughts, we could see them in these intermediate messages.</p><p> <strong>Interpretability is needed only if one module of the cognitive emulation is deceptive.</strong> Then maybe you can use interpretability to explain this module. But I&#39;m confident models will need Chain of Thought for the foreseeable future. So interpretability is not &#39;urgent&#39; research.</p><p> <strong>If we don&#39;t use Reinforcement learning on this cognitive emulation, and if we only prompt in English the different submodules</strong> , the cognitive emulation won&#39;t create whole “new languages” or be able to obfuscate a lot of its thoughts.</p><p> Yes, there are <strong>problems with this proposal:</strong></p><ul><li> CoEms don&#39;t preclude steganography or obfuscation.</li><li> Even when asked to reason with Chain-of-Thought (CoT), LLMs are able to skip some inference steps to some extent (see <a href="https://arxiv.org/abs/2305.04388"><u>Language Models Don&#39;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</u></a> ). However, if we don&#39;t train models to skip a zillion inference steps, they won&#39;t be able to obfuscate their thoughts efficiently.</li></ul><p> But we could further reduce these risks with:</p><ul><li> <strong>Process-based training</strong> , to ensure that the systems are accustomed to detailing their thoughts in much greater detail (and not being able to skip too many steps). It seems much more feasible than interpretability (and the <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>alignment tax</u></a> even seems negative - for example see those progress for <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>Mathematical reasoning</u></a> )</li><li> <strong>selection inference:</strong> <i>“CoT has access to the whole context, so each reasoning step is not necessarily causally connected to the last. But</i> <a href="https://arxiv.org/abs/2205.09712"><i><u>selection inference</u></i></a> <i>enforces a structure where each reasoning step necessarily follows from the last, and therefore the whole reasoning chain is causal.”</i> from <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit?disco=AAAA21_wJ_g"><u>Sid Black</u></a> , CTO of Conjecture.</li><li> Other ideas were listed in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency">Steering the world towards transparency</a> ”.</li></ul><p> Spicy: However, cognitive emulation will quite likely be an engineering nightmare, facing significant robustness issues that are always present in small models. The alignment tax will be higher than for end-to-end systems, making it unlikely that we will ever use this technology. The bottleneck is probably not interp, but rather an ecosystem of preventive safety measures and a safety culture. Connor Leahy, CEO of Conjecture, explaining the difficulties of the problem during interviews and pushing towards a safety culture, is plausibly more impactful than the entire CoEm technical agenda.</p><h2> Detailed Counter Answers to Neel&#39;s list</h2><p> Here is Neel&#39;s <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Longlist of Theories of Impact for Interpretability</u></a> with critiques for each theory. Theories proposed by Neel are displayed in italics, whereas my critiques are rendered in standard font.</p><ol><li> <i><strong>Force-multiplier on alignment research</strong> : We can analyse a model to see why it gives misaligned answers, and what&#39;s going wrong. This gets much richer data on empirical alignment work, and lets it progress faster.</i><ul><li> I think this &quot;force multiplier in alignment research&quot; theory is valid, but is conditioned on the success of the other theories of impact, which imho are almost all invalid.</li><li> <strong>Conceptual advancements are more urgent</strong> It&#39;s better to think conceptually about what misalignment means rather than focusing on interp. [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interpretability look like?</u></a> ]</li><li> <strong>Dual Use:</strong> Force-multiplier on capability research.</li></ul></li><li> <i><strong>Better prediction of future systems</strong> : Interpretability may enable a better mechanistic understanding of the principles of how ML systems work, and how they change with scale, analogous to scientific laws.这使我们能够更好地从当前系统推断未来系统，类似于缩放定律。 Eg, observing phase changes a la induction heads shows us that models may rapidly gain capabilities during training</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Auditing</strong> : We get a Mulligan. After training a system, we can check for misalignment, and only deploy if we&#39;re confident it&#39;s safe</i><ul><li> <strong>Not the most direct way.</strong> This ToI targets outer misalignment, the next one targets inner misalignment. But currently, people who are auditing for outer alignment do not use interpretability. They evaluate the model, they make the model speak and look if it is aligned with behavioral evaluations. Interpretability has not been useful in finding GPT&#39;s jailbreaks.</li><li> To date, I still don&#39;t see how we would proceed with interp to audit GPT-4.</li></ul></li><li> <i><strong>Auditing for deception</strong> : Similar to auditing, we may be able detect deception in a model. This is a much lower bar than fully auditing a model, and is plausibly something we could do with just the ability to look at random bits of the model and identify circuits/features - I see this more as a theory of change for &#39;worlds where interpretability is harder than I hope&#39;.</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Enabling coordination/cooperation:</strong> If different actors can interpret each other&#39;s systems, it&#39;s much easier to trust other actors to behave sensibly and coordinate better</i><ul><li> <strong>Not the most direct way.</strong> If you really want coordination and cooperation, you need to help with AI governance and outreach of experts and researchers. The <a href="https://www.safe.ai/statement-on-ai-risk"><u>statement on AI risks</u></a> has enabled more coordination than interp will probably never get us.</li></ul></li><li> <i><strong>Empirical evidence for/against threat models</strong> : We can look for empirical examples of theorized future threat models, eg inner misalignment</i><ul><li> <i><strong>Coordinating work on threat models</strong> : If we can find empirical examples of eg inner misalignment, it seems much easier to convince skeptics this is an issue, and maybe get more people to work on it.</i><ul><li> <a href="https://ai.facebook.com/research/cicero/"><u>Cicero</u></a> or poker models are already capable of masking pieces of information or bluffing to play poker. From there, I don&#39;t know what it would mean to show canonical inner misalignment to non-technical people.</li><li> This focuses too much on deceptive alignment, and this will probably be too late if we get to this point.</li></ul></li><li> <i><strong>Coordinating a slowdown</strong> : If alignment is really hard, it seems much easier to coordinate caution/a slowdown of the field with eg empirical examples of models that seem aligned but are actually deceptive</i><ul><li> <strong>Not the most direct way.</strong> This is a good theory of change, but interp is not the only way to show that a model is deceptive.</li></ul></li></ul></li><li> <i><strong>Improving human feedback</strong> : Rather than training models to just do the right things, we can train them to do the right things for the right reasons</i><ul><li> Seems very different from current interpretability work.</li><li> <strong>Not the most direct way.</strong> Process-based training, model psychology, or other scalable oversight techniques not relying on interp may be more effective.</li></ul></li><li> <i><strong>Informed oversight</strong> : We can improve recursive alignment schemes like IDA by having each step include checking the system is actually aligned. Note: This overlaps a lot with 7. To me, the distinction is that 7 can be also be applied with systems trained non-recursively, eg today&#39;s systems trained with Reinforcement Learning from Human Feedback</i><ul><li> Yes, it&#39;s an improvement, but it&#39;s naive to think that the only problem with RLHF is just the issue of lack of transparency or deception. For example, we would still have agentic models (because agency is preferred by human preferences) and interpretability alone won&#39;t fix that. See the <a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>Compendium of problems with RLHF</u></a> and <a href="https://www.lesswrong.com/posts/LqRD7sNcpkA9cmXLv/open-problems-and-fundamental-limitations-of-rlhf">Open Problems and Fundamental Limitations of RLHF</a> for more details.</li><li><strong>概念上的进步更为紧迫。</strong> What does &#39;checking the system is actually aligned&#39; really means? <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit#bookmark=id.wqr2jvmzsg7c"><u>It&#39;s not clear at all.</u></a></li></ul></li><li> <i><strong>Interpretability tools in the loss function:</strong> We can directly put an interpretability tool into the training loop to ensure the system is doing things in an aligned way. Ambitious version - the tool is so good that it can&#39;t be Goodharted. Less ambitious - The could be Goodharted, but it&#39;s expensive, and this shifts the inductive biases to favor aligned cognition</i> .<ul><li> <strong>Dual Use,</strong> for obvious reasons, and this one is particularly dangerous.</li><li> <strong>List of lethalities 27. Selecting for undetectability</strong> : “ <i>Optimizing against an interpreted thought optimizes against interpretability.”</i></li></ul></li><li> <i><strong>Norm setting</strong> : If interpretability is easier, there may be expectations that, before a company deploys a system, part of doing due diligence is interpreting the system and checking it does what you want</i><ul><li> <strong>Not the most direct way.</strong> Evals, evals, evals.</li><li> No need to wait for interpretability. We already roughly know what to do. We could conduct studies in line with <a href="https://www.serimats.org/evals"><u>Evaluating Dangerous Capabilities</u></a> and the paper <a href="https://arxiv.org/abs/2305.15324"><u>Model Evaluation for Extreme Risks</u></a> <u>,</u> <a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance">Towards Best Practices in AGI Safety and Governance</a> , this last paper presenting 50 statements about what AGI labs should do, none mentioning interp.</li></ul></li><li> <i><strong>Enabling regulation</strong> : Regulators and policy-makers can create more effective regulations around how aligned AI systems must be if they/the companies can use tools to audit them</i><ul><li> Same critique as <strong>10.</strong> <i><strong>Norm setting</strong></i></li></ul></li><li> <i><strong>Cultural shift 1:</strong> If the field of ML shifts towards having a better understanding of models, this may lead to a better understanding of failure cases and how to avoid them</i><ul><li> <strong>Not the most direct way.</strong> Technical Outreach, communications, interviews or even probably standards and <a href="https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt"><u>Benchmarks</u></a> are way more direct.</li></ul></li><li> <i><strong>Cultural shift 2:</strong> If the field expects better understanding of how models work, it&#39;ll become more glaringly obvious how little we understand right now</i><ul><li> Same critique as <strong>12.</strong> <i><strong>Cultural shift 1.</strong></i></li><li> This is probably the opposite of what is happening now: People are fascinated by interpretability and continue to develop capabilities in large labs. I suspect that the well-known Distill journal has been very fascinating for a lot of people and has probably been a source of fascination for people entering the field of ML, thus accelerating capabilities.</li><li> See the <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#False_sense_of_control_"><u>False sense of control</u></a> section.</li></ul></li><li> <i><strong>Epistemic learned helplessness</strong> : Idk man, do we even need a theory of impact? In what world is &#39;actually understanding how our black box systems work&#39; not helpful?</i><ul><li> I don&#39;t know man, the worlds where we have limited resources, where we are funding constrained + Opportunity costs.</li><li> <strong>Dual Use</strong> , refer to the section &quot; <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful">Interpretability May Be Overall Harmful</a> &quot;.</li></ul></li><li> <i><strong>Microscope AI</strong> : Maybe we can avoid deploying agents at all, by training systems to do complex tasks, then interpreting how they do it and doing it ourselves</i><ul><li> Critique in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI?</u></a> 。</li></ul></li><li> <i><strong>Training AIs to interpret other AIs</strong> : Even if interpretability is really hard/labor intensive on advanced systems, if we can create aligned AIs near human level, we can give these interpretability tools and use them to interpret more powerful systems</i><ul><li> <strong>Object level:</strong> Training AI to interpret other AI, could be useful but would be already dangerous, and we are already in classes of scenarios that are super <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>dangerous</u></a> .</li><li> <strong>Meta level:</strong> This scheme is very speculative. I do not want the survival of civilization to rely on it. <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"><u>Godzilla strategy</u></a> is probably not a good strategy (though this is controversial).</li></ul></li><li> <i><strong>Forecasting discontinuities</strong> : By understanding what&#39;s going on, we can predict how likely we are to see discontinuities in alignment/capabilities, and potentially detect a discontinuity while training/before deploying a system</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Intervening on training</strong> : By interpreting a system during training, we can notice misalignment early on, potentially before it&#39;s good enough for strategies to avoid our notice such as deceptive alignment, gradient hacking, obfuscating its thoughts, etc.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Auditing a training run</strong> : By checking for misalignment early in training, we can stop training systems that seem misaligned. This gives us many more shots to make an aligned system without spending large amounts of capital, and eg allows us to try multiple different schemes, initialisations, etc. This essentially shifts the distribution of systems towards alignment.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Eliciting Latent Knowledges:</strong> Use the length of the shortest interpretability explanation of behaviors of the model as a training loss for ELK - the idea is that models with shorter explanations are less likely to include human simulations / you can tell if they do. (credit to Tao Lin for this one)</i><ul><li> Same critique as <strong>9.</strong> <i><strong>Interpretability tools in the loss function.</strong></i></li><li> Same critique as <strong>15. Microscope AI</strong> .</li><li> Same critique as <strong>16.</strong> <i><strong>Training AIs to interpret other AIs.</strong></i></li></ul></li></ol><h2> Case study of some cool interp papers</h2><p> This section is more technical.</p><p> Stephen Casper <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3#Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>lists</u></a> a bunch of impressive interpretability papers, as of February 2023. Let&#39;s try to investigate whether these papers could be used in the future to reduce risks. For each article, I mention the corresponding end story, and the critic of this end story applies to the article.</p><h3> Bau et al. (2018)</h3><p> <a href="https://arxiv.org/abs/1811.10597"><u>Bau et al. (2018)</u></a> : Reverse engineer and repurpose a GAN for controllable image generation.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://www.youtube.com/watch?v=yVCgUYe4JTM"><u>video</u></a> ) We generate images of churches using a GAN. There are often trees in the generated images. We manually surround the trees, then find the units in the GAN that are mostly responsible for generating these image regions. After finding these regions, we perform an ablation of these units, and it turns out that the trees disappear.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a></li><li> <strong>Useful for outer alignment?</strong> Ideally, we could 1. Find features which are undesirable 2. Then remove parts of the network that are most linked to these features. This is a very limited form of alignment procedure, by ablation.<ul><li> Maybe we could use this kind of procedure to filter pornography, but why then train the network on pornographic images in the first place?</li><li> Basically, this is the same strategy as enumerative safety which is criticized above.</li></ul></li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, because by definition, deception will not result in a difference in outputs, so we cannot apply this procedure.</li></ul><h3> Ghorbani et al. (2020)</h3><p> <a href="https://arxiv.org/abs/2002.09815"><u>Ghorbani et al. (2020)</u></a> : Identify and successfully ablate neurons responsible for biases and adversarial vulnerabilities.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://slideslive.com/38936399/neuron-shapley-discovering-the-responsible-neurons"><u>video</u></a> ) It calculates the Shapley score of different units of a CNN and then removes the units with the highest Shapley value to maximize or minimize a metric. Removing certain units seems to make the network more robust to certain adversarial attacks.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a> (and <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>Reverse engineering</u></a> ?)</li><li> <strong>Useful for outer alignment?</strong> What would have happened if we had just added black women to the dataset? We can simply use a generative model for that and generate lots of images of black women. I&#39;m almost certain that the technique used by OpenAI to remove biases in <a href="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2"><u>Dalle-2</u></a> , does not rely on interp.</li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, again because the first step in using Shapley value and this interpretability method is to find a behavioral difference, and we need first to create a metric of deception, which does not exist currently. So again we first need to find first a behavioral difference and some evidence of deception.</li></ul><h3> Burns et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2212.03827"><u>Burns et al. (2022)</u></a> : Identify directions in latent space that were predictive of a language model saying false things.</p><ul><li> <strong>Procedure:</strong> compare the probability of the &#39;Yes&#39; token with the probability probed from the world model.</li><li> <strong>End story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI</u></a></li><li> <strong>Useful for inner alignment?</strong><ul><li> Extracting knowledge from near GPT-3 level AIs, mostly trained through self-supervised learning via next token prediction, is a <a href="https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4">misunderstanding</a> .</li><li> <strong>This technique requires a minimum of agency and is not just usable as an oracle.</strong><ul><li> <strong>Chain-of-thought will probably always be better.</strong> Currently, this technique barely performs better than next token prediction. Chain-of-thought performs much better, and it seems we have (obvious) <a href="https://twitter.com/BogdanIonutCir2/status/1664974522791895040"><u>theoretical reason</u></a> to think so. So using GPTs as just an oracle won&#39;t be competitive. This paper doesn&#39;t test the trivial baseline of just fine-tuning the model (which has been found to usually work better).</li><li> <strong>Agency is probably required.</strong> It seems unlikely that it will synthesize knowledge on its own in a world model during next-token prediction training. Making tests in the world, or reasoning in an open-ended way, is probably necessary to synthesize a proper truth feature in the world model in advanced GPT using continual learning.</li></ul></li><li> <strong>Conclusion:</strong> Yes, maybe in the future, if we create autonomous agents that conduct experiments and have their own world model, this kind of technique could probably be spot a mismatch between the world model oracle and what the model tells you. But if that were the case, we would probably already be in a very, very dangerous world. Civilization is not ready for this, and I still think that this method will be very brittle, and I prefer to aim for worlds where deception is unlikely. [section: ​​<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ]</li></ul></li></ul><h3> Casper et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2211.10024"><u>Casper et al. (2022)</u></a> : Identify hundreds of interpretable copy/paste attacks.</p><ul><li> <strong>Procedure:</strong> We try to find adversarial attacks automatically. For each image x, we optimize the latent space of a GAN to complete a patch in the image, to optimize the probability of classification towards class y2. We do this for each possible (x, y2) and we observe what appears in the patch. Then we search for natural patches that have a similar embedding to the synthetic adversarial patch, and if the similarity is high enough, we test the natural patch and see if this natural patch breaks the classifier.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Useful for inner alignment?</strong> Conceptually yes, but this is so brittle. And if the trigger mechanism for deceptive alignment is not generable by the GAN or is not in the distribution of natural images/text, we won&#39;t find it.</li></ul><h3> Ziegler et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2205.01663"><u>Ziegler et al. (2022)</u></a> : Debug a model well enough to greatly reduce its rate of misclassification in a high-stakes type of setting.</p><ul><li> <strong>Procedure</strong> : The project consists in building a highly reliable injury classifier: <i>“We started with a baseline classifier trained on some mildly injury-enriched (but otherwise mostly random) data. Then, over the course of several months, we tried various techniques to make it more reliable”.</i> The aim here is to minimize the false negative rate, striving to make the classifier as reliable as possible.: we do not want to let any problematic sentence or example go unnoticed. In order to find adversarial examples, they experimented with the following techniques:<ul><li> 1. unaugmented humans: humans manually search for problematic sentences,</li><li> 2. automatic paraphrasing: use of LLM to find variations of problematic sentences and progressively enrich the dataset.</li><li> and 3. humans augmented with a rewriting tool. It is mainly this last technique that was useful.</li></ul></li><li> <strong>End Story:</strong> Adversarial training and maybe <strong>&nbsp;</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Robustness results:</strong><ul><li> <strong>In-distribution:</strong> They started with a classifier that had a 2.5% false negatives, and ended up with a classifier that had 0.003% false negatives! They are almost reaching the limits of detection for this type of dataset.</li><li> <strong>Out-of-distribution:</strong> The tool-assisted attack to generate an adversarial example now takes 26 minutes per example, which is only twice as long as the previous time of 13 minutes. This falls short of the paper&#39;s aim to gain many orders of magnitude in speedup. Unfortunately, even though the original dataset (in-distribution) is now almost completely well classified, it seems that it is still possible to easily create adversarial examples with automated tools, and we have only improved the defense by a factor of 2. But only a factor of 2 falls short of the aims of this project.</li><li> <strong>Conclusion:</strong> Thus, improving a lot in-distribution performance by several orders of magnitude seems to not have much impact on out-of-distribution. Even though Redwood <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>said</u></a> that this project could have been conducted better, this is currently rather a negative result to improve the adversarial robustness (ie out-of-distribution Robustness). In addition, we can link these results to the paper <a href="https://arxiv.org/abs/2211.00241"><u>Adversarial Policies Beat Superhuman Go AIs, Wang et al.. 2022</u></a> , which studies adversarial attacks on the Katago AI, which is superhuman in the game of Go. They show that it is probably possible to find simple adversarial strategies even against very superhuman AIs. And as a consequence, it seems that even for very robust and powerful AIs, It may always be possible to find adversarial attacks.</li></ul></li><li> <strong>How to use this for deception?</strong> Unclear. Do we watch what the AI does, tell it <i>&#39;you are the lord of the universe, you can do whatever you want&#39;</i> or more subtle variations of this prompt and then see what happens? This is not really interpretability, it&#39;s much more prompt engineering with <a href="https://arxiv.org/abs/2306.09983"><u>consistency checks</u></a> or cyborgism than interp. (Beside, the interpretability technique that they used is quite underwhelming, only coloring tokens with the bigger gradient of the norm of the embedding of the token, and then basically iterating on synonymous of those important tokens). </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2stfurwyyg6"> <span class="footnote-back-link"><sup><strong><a href="#fnref2stfurwyyg6">^</a></strong></sup></span><div class="footnote-content"><p> Is feature visualization useful? Some findings suggest no: <a href="https://arxiv.org/abs/2302.10894"><u>Red Teaming Deep Neural Networks with Feature Synthesis Tools</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qi9kn3ip89"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qi9kn3ip89">^</a></strong></sup></span><div class="footnote-content"><p> GradCam: Maybe this <a href="https://www.notion.so/Against-Almost-every-Theories-of-Change-of-Interpretability-61ebd2937cab4e12b9eb777454b7ed29?pvs%3D21"><u>paper</u></a> ? But this is still academic work.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6xxwjs20rd7"> <span class="footnote-back-link"><sup><strong><a href="#fnref6xxwjs20rd7">^</a></strong></sup></span><div class="footnote-content"><p> I have organized <a href="https://www.lesswrong.com/posts/WF5JpmpK8EM4xKyve/new-hackathon-robustness-to-distribution-changes-and"><u>two</u></a> <a href="https://github.com/EffiSciencesResearch/hackathon42"><u>hackathons</u></a> centered around the topic of spurious correlations. I strongly nudged using interp, but unfortunately, nobody used it...Yes this claim is a bit weak, but still indicates a real phenomenon, see [section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_tools_lack_widespread_use_by_practitioners_in_real_applications_"><u>Lack of real applications</u></a> ]</p></div></li><li class="footnote-item" role="doc-endnote" id="fnztj4j3pmerg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefztj4j3pmerg">^</a></strong></sup></span><div class="footnote-content"><p> Note: I am not making any claims about ex-ante interp (also known as <a href="https://arxiv.org/abs/2207.13243"><u>intrinsic interp</u></a> ), which has not been so far able to predict the future system either.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2464ho15s7t"> <span class="footnote-back-link"><sup><strong><a href="#fnref2464ho15s7t">^</a></strong></sup></span><div class="footnote-content"><p> Other weaker difficulties for auditing deception with interp: <strong>This is already too risky and Prevention is better than cure. 1) Moloch may still kill us:</strong> <i>&quot;auditing a trained model&quot; does not have a great story for wins. Like, either you find that the model is fine (in which case it would have been fine if you skipped the auditing) or you find that the model will kill you (in which case you don&#39;t deploy your AI system, and someone else destroys the world instead)</i> . […] <i>a capable lab would accidentally destroy the world because they would be trying the same approach but either not have those interpretability tools or not be careful enough to use them to check their trained model as well?”</i> [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DpHt4w9SN5PLLTzZuB"><u>Source</u></a> Rohin Shah]. <strong>2) We probably won&#39;t be competent enough to fix our mistake:</strong> “ <i>in order for auditing the model to help (directly), you have to actually be pretty confident in your ability to understand and fix your mistakes if you find one. It&#39;s not like getting a coin to land Heads by flipping it again if it lands Tails - different AGI projects are not independent random variables, if you don&#39;t get good results the first time you won&#39;t get good results the next time unless you understand what happened. This means that auditing trained models isn&#39;t really appropriate for the middle of the skill curve</i> .” [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3D8Ckqr87YsoMprivEW"><u>Source</u></a> Charlie Steiner].</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrbnra8fsacj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrbnra8fsacj">^</a></strong></sup></span><div class="footnote-content"><p> From “ <a href="https://www.lesswrong.com/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"><u>Conditioning Generative Models</u></a> . “Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs. <strong>Is there a way we could use different conditionals to get at whether or not Alice was deceptive</strong> (eg prompt the model with “DeepMind develops perfect transparency tools and provides an opportunity for deceptive models to come clean and receive a prize before they&#39;re discovered.”)?”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmf7vlk6ib69"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmf7vlk6ib69">^</a></strong></sup></span><div class="footnote-content"><p> For example, what happens If you prompt a deceptive model with a Joyous prompt, and prompt the same deceptive model with a sad prompt and then take the difference, you obtain a Joyous Deceptive model?</p></div></li><li class="footnote-item" role="doc-endnote" id="fnc2q5uxqhj6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc2q5uxqhj6j">^</a></strong></sup></span><div class="footnote-content"><p> But at the same time, we could be pessimistic, because this good idea has been out there in the wild since Christiano <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>described</u></a> it in 2019. So either this idea does not work and we have not heard about it. Or the community has failed to recognize a pretty simple good idea.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpo4e41md3r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpo4e41md3r">^</a></strong></sup></span><div class="footnote-content"><p> Causal scrubbing could be a good way for evaluating interp techniques using something other than intuition. However, this is only suitable for localization assessment and does not measure how understandable the system is for humans.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnplu4ji16iui"> <span class="footnote-back-link"><sup><strong><a href="#fnrefplu4ji16iui">^</a></strong></sup></span><div class="footnote-content"><p> “ <i>I was previously pretty dubious about interpretability results leading to capabilities advances. I&#39;ve only really seen</i> <a href="https://arxiv.org/pdf/2212.14052.pdf"><i><u>two</u></i></a> <i>&nbsp;</i> <a href="https://arxiv.org/pdf/2302.10866.pdf"><i><u>papers</u></i></a> <i>which did this for LMs and they came from the same lab in the past few months. It seemed to me like most of the advances in modern ML (other than scale) came from people tinkering with architectures and seeing which modifications increased performance. But in a conversation with Oliver Habryka and others, it was brought up that as AI models are getting larger and more expensive, this tinkering will get more difficult and expensive. This might cause researchers to look for additional places for capabilities insights, and one of the obvious places to find such insights might be interpretability research.</i> ” from <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous?commentId=GYo8WegFmfxWmB5Z3"><u>Peter barnett</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw7s6gsvuwb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw7s6gsvuwb">^</a></strong></sup></span><div class="footnote-content"><p> Not quite! Hypotheses 4 (and 2?) are missing. Thanks to Diego Dorn for presenting this fun concept to me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnavln8kyvlzg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefavln8kyvlzg">^</a></strong></sup></span><div class="footnote-content"><p> This excludes the governance hackathon, though, this is only from the technical ones. Source: Esben Kran.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1<guid ispermalink="false"> LNA8mubrByG7SFacm</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Thu, 17 Aug 2023 18:44:41 GMT</pubDate></item></channel></rss>