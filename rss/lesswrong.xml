<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 7 日星期四 18:14:58 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[How did you make your way back from meta?]]></title><description><![CDATA[Published on September 7, 2023 5:23 PM GMT<br/><br/><p>我注意到我自己非常偏爱关注元级别。它在我所热爱的领域最为明显，比如写作。例如，我会花更多的时间阅读修辞学或查找 1980 年代关于写作技巧的稀有书籍，而不是练习写论文或故事。</p><p>我不喜欢这样，因为我研究元级别的最终目标是在对象级别上变得更好。与此同时，有时我也会因为如此超前而得到奖励。我在工作中得到了很多尊重，同时我也提供了极好的反馈和观点。</p><p>这种状态似乎不会立即产生痛苦的影响。我有家庭和工作，总体来说我的生活似乎很顺利。但人们渴望a）将理论付诸实践并看到结果（即通过元支付租金），b）从直接经验中学习并与他人分享这些经验。元是一个孤独的地方。</p><p>我不认为我是唯一一个担任这一职位的人。我只用了几秒钟的搜索就找到了这两篇文章（我确信还有更多）：https: <a href="https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction">//www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction</a> <a href="https://www.lesswrong.com/posts/RnP5bR767NcxebYHd/conjecture-on-addiction-to-meta-level-solutions">https://www.lesswrong.com/posts/ RnP5bR767NcxebYHd/对元级解决方案成瘾的猜想</a></p><p>在过去的几天里，我发现自己开始进行元深化活动，并有意识地切换到对象级任务。到目前为止，这是值得的，所以我相信在几周内，我的习惯将转向我想要的地方。</p><p>但我很好奇：还有其他人经历过类似的事情吗？你过得怎么样？你做了什么？</p><br/><br/> <a href="https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta<guid ispermalink="false"> B99sRmhWpMcZLpptH</guid><dc:creator><![CDATA[matto]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:23:18 GMT</pubDate> </item><item><title><![CDATA[AI#28: Watching and Waiting]]></title><description><![CDATA[Published on September 7, 2023 5:20 PM GMT<br/><br/><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/09/america-is-asleep-on-its-ai-boom.html">正如泰勒·考恩（Tyler Cowen）所指出的</a>，我们正处于一段平静时期。我们这些走在前面的人已经习惯了 GPT-4、Claude-2 和 MidJourney。功能和集成正在扩展，但速度相对缓慢。大多数人仍然幸福地没有意识到，这让我可以尝试对他们的新解释，而许多其他人则说这都是炒作。他们会一直这样说，直到有什么事情迫使他们不这样做，最有可能是 Gemini，尽管值得注意的是我在 2023 年看到了对 Gemini 的怀疑（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo">只有 25% 的人认为谷歌到年底会拥有最好的模型</a>），甚至在2024 年（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo-b4ad29f8b98d">即使到明年年底也只有 41% 会发生。</a> ）</p><p>我认为这是持续好消息模式的一部分。虽然我们还有很长的路要走，面临很多不可能的问题，但半年来，话语和奥弗顿之窗以及对现实问题的认识和理解不断提高。主要实验室内外的联盟兴趣和资金正在迅速增长。普通的实用性也在稳步提高，带来的好处使成本相形见绌，而且迄今为止的普通危害比几乎任何人对现有技术的预期要轻得多。能力正在以快速且令人震惊的速度进步，但速度并没有我预期的那么令人震惊。</p><p>本周的亮点包括英国工作组的最新情况以及对 Inflection AI 的 Suleyman 的采访。</p><p>我们进展顺利。让我们继续保持下去。</p><p>即使本周的平凡实用性，我们可以说，实用性值得怀疑。</p><span id="more-23532"></span><h4>目录</h4><ol><li>介绍。</li><li>目录。</li><li><strong>语言模型提供了平凡的实用性</strong>。它已经注视着你了。</li><li>语言模型不提供平凡的实用性。谷歌搜索永远被毁了。</li><li> Deepfaketown 和 Botpocalypse 很快就会出现。我会过去的，谢谢。</li><li>他们抢走了我们的工作。不以有偏见的方式工作比根本不工作更好？</li><li>参与其中。人工智能政策和重新思考优先事项中心。</li><li>介绍一下。哦，太棒了，又一个竞争订阅服务。</li><li><strong>英国特别工作组更新</strong>。令人印象深刻的团队行动迅速。</li><li>在其他人工智能新闻中。你说人工智能会欺骗？骗我的。</li><li><strong>静静的猜测</strong>。版权法可能即将变得丑陋。</li><li>寻求健全的监管。完整的舒默会议列表。</li><li><strong>音频周</strong>。 80k 上的 Suleyman、Altman、Schmidt 和其他几个人。</li><li>修辞创新。还有几种不沟通的方式。</li><li>没有人会傻到这么做。最大程度自主的 DeepMind 代理。</li><li>调整比人类更聪明的智能是很困难的。更容易证明安全性？</li><li> Twitter 社区注释注释。 Vitalik 询问为什么它能一直保持良好状态。</li><li>人们担心人工智能会杀死所有人。他们的担忧程度正在慢慢上升。</li><li>其他人并不担心人工智能会杀死所有人。泰勒·考恩又来了。</li><li>较轻的一面。罗恩掌握了节奏。</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697346570092773578">对《使命召唤》进行自动聊天审核</a>。考虑到实际的选择是许多游戏的聊天为零，而其他游戏的聊天充满了最卑鄙的败类和恶行，我不太支持“新的反乌托邦地狱景观”，而是支持“到底什么是更好的选择”这里。&#39;</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/fofrAI/status/1697550606288793679">监控您的员工和客户</a>。</p><blockquote><p> Rowan Cheung：来认识一下新的人工智能咖啡店老板。它可以跟踪咖啡师的工作效率以及顾客在店里花费的时间。我们正进入狂野时代。</p><p> Fofr：这在很多方面都很可怕。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e3bc9-3db2-4992-aeba-3bd888c47364_748x502.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/p732au93iomhowop7jeo" alt=""></a></figure><p>重要的不是工具，而是你如何使用它。摩根大通等一些公司已经使用剧毒的反乌托邦监控工具，这让他们更上一层楼。跟踪顾客在商店待了多长时间，或者他们是否是回头客以及他们等待订单的时间似乎非常有用。从广义上跟踪生产率（例如订单完成情况）是一种情况，过多的精确度和注意力会带来大问题，但不够精确和关注也会带来大问题。不做任何工作的客观答案比做大量工作的有偏见、容易出错的答案要好得多。</p><p>在社交媒体上<a target="_blank" rel="noreferrer noopener" href="https://www.disclose.tv/id/6cew4vxu8k/">监控您的公民</a>（以下是整个帖子）。</p><blockquote><p> Dissclose.tv：美国特种作战司令部 (USSOCOM) 已与 Acccrete AI 签订合同，部署软件来检测社交媒体上的“实时”虚假信息威胁。</p></blockquote><p>这似乎正是这些人以前所做的事情？这里的问题不是人工智能。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/peterwildeford/status/1697571328868356267">第一次在与人类的体育运动中获胜</a>，我确信这没什么，这项运动是（检查笔记）无人机竞赛。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/david_perell/status/1699076351150428185">获得写作方面的帮助</a>。正如 Parell 指出的那样，直接要求 ChatGPT 帮助你写作是没有用的，但作为个人图书管理员和事物解释者可能会很棒。他推荐“多说”一词，要求以不同作者的风格进行重述和摘要，来回交谈并始终尽可能具体，并让程序检查拼写错误。</p><p>伊森·莫里克（Ethan Mollick）建议开发他所谓的《魔法书》（Grimoires），我的大脑想要自动更正咒语书（他也使用这个术语），旨在优化交互的提示，包括给人工智能一个角色、目标、逐步说明，可能请求提供示例并让人工智能从用户那里收集必要的上下文。</p><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.01404">玩《Hoodwinked》游戏</a>，类似于《黑手党》或《Among Us》。正如人们所期望的那样，能力更强的模型胜过能力较差的模型，并且根据游戏的进行方式经常撒谎和欺骗。对于人类来说，在此类游戏中，正确的策略通常是，如果可以的话，说出如果你是无辜的，你会说的话的某种变体，这对于法学硕士来说可能很容易做到。请注意，随着法学硕士变得更加聪明，其他策略变得更优越，然后是人类无法实现的其他策略，然后是人类甚至没有考虑过的策略。</p><h4>语言模型不提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1697648862272401685">许多人说，要小心人工智能生成的垃圾文章，</a>尽管我还没有真正遇到过这样的文章。瑞安（Ryan）在这里是正确的，罗希特（Rohit）也是正确的，尽管两者都没有解决问题。</p><blockquote><p> Paul Graham：我正在网上查找一个主题（披萨烤箱应该有多热），我注意到我正在查看文章的日期，试图找到不是人工智能生成的 SEO 诱饵的内容。</p><p>瑞恩·彼得森：保罗，越热越好！</p><p> Rohit：现在生成搜索似乎还可以吗？除非，是的，您想更深入地研究更具体的东西？ [显示谷歌的回应。]</p></blockquote><p>这是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699745755286704466">我本周看到的其他几个说法之一</a>，即谷歌搜索正在被法学硕士生成的垃圾迅速污染。</p><p>当 Steam（Valve）参与时，即使看看 AI，也要小心， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/WaifuverseAI/status/1697764665521295838">他们会永久删除游戏一次，允许一个允许角色使用 GPT 生成的对话的模组</a>，即使该模组随后被删除。虽然我确实很钦佩一个人完全致力于这一点，但这显然太过分了，我希望 Valve 意识到这一点并改变他们的决定。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JudgeFergusonTX/status/1698904660310954186">法官罗伊·弗格森问克劳德他是谁</a>，陷入了无数捏造信息的循环中，克劳德道歉并承认捏造信息。绝对是一个问题。弗格森认为这是克劳德的“故意”，我认为这是对法学硕士工作方式的误解。</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p>到目前为止，唐纳德·特朗普在政治上充分利用了深度造假技术。他们是否不可避免地偏爱像他这样有才华的人？另一个需要考虑的角度是，谁比特朗普的支持者更容易受到这种策略的影响？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697400187361395048">阿斯特丽德·王尔德</a>：越来越多的人注意到针对特朗普支持者的越来越复杂的人工智能欺骗攻击。这次袭击令人信服，甚至获得了播出时间。一个说话节奏像<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michaelmalice">@michaelmalice</a>的人正在用人工智能欺骗特朗普的声音。这是一个美丽新世界。也有可能 RAV 本身就是恶搞的幕后黑手，但我们无法得知。绝对狂野的时光</p></blockquote><p>我一时兴起检查了 r/scams 大约 40 个帖子。几乎所有的内容都是老派， <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/Scams/comments/165oqrn/my_partner_and_i_almost_got_taken_in_by_a_deep/">其中一篇</a>是关于经典的“你的孩子已被捕，你必须交保释金”骗局的报道。回复拒绝相信这是真正的深度伪造，称这是正常的假声音。似乎连骗局专家都没有意识到现在进行深度伪造是多么容易，或者他们已经习惯了一切都是骗局（因为如果你必须问，我有一些新闻），他们认为深度伪造一定是也是骗局？</p><p>值得注意的是，诈骗尝试失败了。我们不断听到“我差点就上当了”，但却没有听到任何真正赔钱的人的消息。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AviSchiffmann/status/1698147373086896374">不言自明且目前令人深感失望的</a>“ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ehalm_/status/1698107101892268113">smashOrPass.ai</a> ”提供了最新的用户微调动机，无论人们如何看待所涉及的道德规范。在撰写本文时，它只是循环中的一小部分图像，因此声称它“学习您喜欢的内容”似乎相当愚蠢，但这很容易解决。不太容易解决的是所有的混杂因素，这应该很好地说明这一点。这完美地说明了当压缩到 0-1 比例时会丢失多少数据，而且人们要么根据上下文进行调整，要么不调整，这两种方法都会非常令人困惑，这里确实需要 0-10。是的，如果有人想知道的话，当然色情版本很快就会推出，即使不是他而不是其他人。更有趣的是，需要多长时间才能发布获取此信息并使用它为您自动滑动约会应用程序个人资料的版本？</p><p>另外，我可以给每个讨厌这个或互联网上其他任何东西的人明显的建议吗？比如<a target="_blank" rel="noreferrer noopener" href="https://www.vice.com/en/article/g5ywp7/you-know-what-to-do-boys-sexist-app-lets-men-rate-ai-generated-women?utm_source=VICE_Twitter&amp;utm_medium=social+">Vice</a>的Janus Rose？不要为了抱怨而强调没有人听说过的事情。这件事引起我的注意完全是因为负面宣传。</p><p><a target="_blank" rel="noreferrer noopener" href="https://archive.ph/jEsmt">詹姆斯·迪恩再现，出演新电影</a>。毫无疑问，还会有更多这样的事情发生。奇怪的是，人们正在谈论克隆伟大的演员，包括伟大的配音演员，无论是否已故，并用它来让活着的演员失业。</p><p>使用人工智能的问题在于人工智能不是一个好演员。</p><p>人工智能配音演员可以复制梅尔·布鲁克斯的声音，但声音与梅尔·布鲁克斯的伟大之处没有什么关系。我想你实际上会做的，至少在一段时间内，是让一些伟大的配音演员录制新台词。然后使用人工智能改造新台词，将梅尔·布鲁克斯的声音风格融入其中。</p><p>如果我们让汤姆·汉克斯或苏珊·萨兰登（均在OP中引用）在死后继续工作，那么我们选择重新塑造他们的形象和声音，而无法复制他们的实际才能或技能。就我们从他们身上获得“良好表现”而言，我们可以使用任何拥有足够记录数据作为基线的人来获得这种表现，例如你的妈妈，或者绝对不会表演的华丽时装模特。当有人去世时，将其用于续集是有意义的，并且连续性优先，但未来的演员可能会是那些具有外观的人？因此詹姆斯·迪恩（James Dean）或者玛丽莲·梦露（Marylin Monroe）都是有道理的。或者某个人的存在具有重要的象征意义。</p><h4>他们抢走了我们的工作</h4><p>AI检测工具不起作用。我们知道这一点。现在我们有数据表明它们不起作用，那就是<a target="_blank" rel="noreferrer noopener" href="https://themarkup.org/machine-learning/2023/08/14/ai-detection-tools-falsely-accuse-international-students-of-cheating">标记斯坦福大学非英语母语人士的工作。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4a9ad0-7a92-4545-b9c8-336441fdfff3_1020x634.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/kvkv0dvfyixr5psjckud" alt=""></a></figure><blockquote><p>当单词选择可预测且句子更简单时，人工智能检测器往往会被编程为将写作标记为人工智能生成。事实证明，非英语母语人士的写作往往符合这种模式，这就是问题所在。</p></blockquote><p>问题是测试不起作用。这说明测试不起作用。它碰巧击中了非母语人士，这说明我们当前的检测尝试是多么可悲。</p><p>我们在这方面训练的人工智能是错位的。他们注意到单词复杂度是训练数据中统计上有效的指标，因此他们尽可能地提高了分数。可以在没有这种相关性的情况下生成一个定制的训练集，然后再试一次吗？也许可以，但我预计在我们得到可以使用的东西之前，需要多次循环。</p><p>如果说有什么不同的话，那就是这种区分使人工智能探测器变得更有用，而不是更没用。通过将其错误集中在特定位置并使用可测试的解释，您可以排除其中的许多错误。如果您从未在非母语人士的工作中使用它，它就不会歧视他们。</p><p>它还展示了一种利用单词选择的复杂性来伪装人工智能工作的简单方法。</p><h4>参与其中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/">人工智能政策中心</a>是一个新组织，致力于制定和倡导政策，以减轻先进人工智能带来的灾难性风险。他们正在招聘一名人工智能政策分析师和一名通讯总监。他们最近提出了《<a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/work">负责任的人工智能法案》</a> ，该法案需要在一些地方进行完善，但非常好提出，因为它是一项将事情朝着富有成效的方向发展的具体提案。<a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/careers">在此</a>了解更多信息并申请。</p><p><a target="_blank" rel="noreferrer noopener" href="https://rethinkpriorities.org/xst-incubation">重新思考人工智能安全工作孵化的优先事项</a>，包括<a target="_blank" rel="noreferrer noopener" href="https://careers.rethinkpriorities.org/en/postings/0980aba8-466a-4282-9319-c8c4f6f39341">在大学中为人工智能政策职业开展实地建设</a>。请对项目孵化中心的计划持怀疑态度，该中心旨在帮助孵化未来项目的人员。我明白数学在理论上是如何运作的，但大多数做某事的人最终必须直接做这件事，否则什么都不会完成。</p><h4>介绍</h4><p><a target="_blank" rel="noreferrer noopener" href="https://time.com/collection/time100-ai/">《时代》杂志推出了人工智能时代 100 强</a>。我会调查一下，但现在我发现我们的时间已经到了。</p><p> Anthropic 的<a target="_blank" rel="noreferrer noopener" href="https://support.anthropic.com/en/articles/8324991-about-claude-pro-usage">Claude Pro</a> ，每月支付 20 美元以获得更高的带宽和优先级。我从未遇到过 Claude 的使用限制，尽管发现它非常有用 - 我的对话往往相对较短，我做的唯一昂贵的事情就是附加巨大的 PDF，他们说这缩小了限制，但我已经尚未遇到任何问题。值得注意的是，在使用此类附件时，询问少量广泛的答案比询问大量小问题更有效。</p><p> HuggingFace 表示， <a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/blog/falcon-180b">Falcon 180B</a>是一个比 Llama-2 稍微好一点的模型，但相对于其规模和成本而言，它的性能更差。他们说在评估基准上“介于 GPT-3.5 和 GPT-4 之间”，我仍然认为在实际使用中它将保持在 3.5 以下。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/announcing-openai-devday">OpenAI 将于 11 月 6 日在旧金山举办开发者大会。</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1699492275209003425">Sam Altman</a> ：11 月 6 日，我们将向开发者展示一些很棒的东西！ （没有 gpt-5 或 4.5 或类似的东西，冷静下来，但我仍然认为人们会很高兴......）</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.ycombinator.com/launches/JOw-automorphic-infuse-knowledge-into-language-models-with-just-10-samples">Automorphic（YC &#39;23 公司）提供即用即训练微调</a>，包括连续 RLHF，使用尽可能少的示例，免费提供前三个模型的微调。肯定是应该有的东西，不知道发货了没有，有人试试吗？</p><h4>英国特别工作组更新</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1699688880482500684">伊恩·霍加斯 (Ian Hogarth) 提供的英国基金会模型工作组的最新情况</a>（ <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">直接</a>）。顾问委员会看起来是一流的，包括本吉奥和克里斯蒂亚诺以及国家安全专业知识的索梅伦。他们正在与 ARC Evals、人工智能安全中心等机构合作。十一月的峰会即将到来，所以一切都进展得很快。他们正在迅速扩张，并且仍在招聘。</p><blockquote><p>伊恩·霍加斯（Ian Hogarth）：OpenAI 首席执行官萨姆·奥尔特曼（Sam Altman）最近表示，公共部门“缺乏引领创新的意愿”，他问道：“你为什么不问问政府为什么他们不做这些事情，不是吗？”那是最可怕的部分吗？”</p><p>我们有足够的意愿来改变人工智能安全前沿的国家能力。这就是为什么我们正在以启动速度向政府聘用人工智能技术专家。我们正在利用世界领先的专业知识。</p><p> ……</p><p>我们的团队现在包括来自 DeepMind、微软、Redwood Research、人工智能安全中心和人类兼容人工智能中心的经验丰富的研究人员。</p><p>这些是世界上最难雇用的人。他们选择进入公共服务并不是因为这很容易，而是因为它提供了从根本上改变社会应对人工智能前沿风险的方法的机会。</p><p>我们正在迅速扩大这个团队，并正在寻找有兴趣促进人工智能安全国家能力的研究人员。我们计划将团队规模再扩大一个数量级。 <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/1HKVnrV_rBHF3w4StWUs0XNhxTtKkTHs5CpMnH6PM0pQ/viewform?edit_requested=true">请考虑在这里申请加入我们</a>。</p><p>首届人工智能安全峰会将于 11 月 1 日至 2 日在英国举行，这是影响人工智能安全的关键时刻。我们特别关注对前沿模型的技术风险评估感兴趣的人工智能研究人员。</p><p> ……</p><p>快速行动很重要。政府从一开始就在 11 周内完成了如此多的工作，需要一支由敬业的、才华横溢的公务员组成的令人难以置信的团队的大力努力。建立这个团队与上面提到的技术团队一样重要。</p><p>这就是我们聘请 Ollie Ilott 担任工作组主任的原因。奥利从唐宁街加入我们，他在那里领导首相的国内私人办公室，担任“副首席私人秘书”的关键角色</p><p>他因其招募和塑造一流团队的能力而“全面”闻名。在加入首相办公室之前，奥利在疫情大流行的第一年负责管理内阁办公室的新冠病毒战略团队，并领导团队参与英国脱欧谈判。</p></blockquote><h4>在其他人工智能新闻中</h4><p>Peter S. Park、Simon Goldstein、Aidan O&#39;Gara、Michael Chen、Dan Hendrycks 的<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.14752">新论文</a>：AI 欺骗：示例、风险和潜在解决方案调查。</p><p>这是摘要：</p><blockquote><p>本文认为，当前的一系列人工智能系统已经学会了如何欺骗人类。我们将欺骗定义为为了追求真相以外的某些结果而系统地诱导错误信念。</p><p>我们首先调查了人工智能欺骗的实证例子，讨论了为特定竞争情况构建的专用人工智能系统（包括 Meta 的 CICERO）和通用人工智能系统（例如大型语言模型）。</p><p>接下来，我们详细介绍人工智能欺骗的几种风险，例如欺诈、篡改选举和失去对人工智能系统的控制。</p><p>最后，我们概述了针对人工智能欺骗所带来的问题的几种潜在解决方案：首先，监管框架应该使能够欺骗的人工智能系统满足严格的风险评估要求；其次，政策制定者应实施机器人或非机器人法律；最后，政策制定者应优先考虑为相关研究提供资金，包括检测人工智能欺骗和减少人工智能系统欺骗性的工具。政策制定者、研究人员和广大公众应该积极努力，防止人工智能欺骗破坏我们社会共同基础的稳定。</p></blockquote><p>他们很好地指出，无论你认为什么是欺骗的核心案例，我们很可能已经有了人工智能这样做的一个很好的例子。 LLM 经常参与其中。当欺骗是一件有用的事情时，没有理由认为欺骗对于优化人工智能系统来说不是自然而然的。有时它是有意或预测的，有时它是无意的并且无论如何都发生了，包括有时人工智能有明确的意图或计划这样做。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OwainEvans_UK/status/1698683225349198200">欧文·埃文斯 (Owain Evans) 的新论文测试了法学硕士的潜在态势感知能力</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/5mTbvtJCli">论文</a>）。</p><blockquote><p>欧文·埃文斯：我们的实验：</p><p> 1. 对虚构聊天机器人的描述进行法学硕士微调，但没有示例记录（即只有声明性事实）。</p><p> 2. 在测试时，看看 LLM 是否可以像聊天机器人零样本一样运行。 LLM 可以从声明性信息 → 程序性信息吗？</p><p>令人惊讶的结果：</p><p> 1. 使用标准微调设置，法学硕士无法从声明性信息转变为程序性信息。</p><p> 2. 如果我们将陈述性事实的释义添加到微调集中，那么法学硕士就会成功并随着规模的扩大而提高。</p></blockquote><p>我并不像欧文那样感到惊讶，这对我来说都是有道理的。我仍然觉得它很有趣，我很高兴它被尝试过。我不确定要做出哪些更新来回应。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JoINrbs/status/1698225548043165874">Twitter 隐私政策现在警告它可以使用你的数据来训练人工智能模型。</a>无论如何，他们都会这么做，如果埃隆·马斯克专注于不让其他人这样做的话。</p><blockquote><p>乔布斯：无意义的帖子和无意义的艺术现在都是功能性的反资本主义艺术，这有点酷。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/31/23854012/nvidia-amd-chip-restrictions-middle-east">芯片限制扩大到中东部分地区</a>。如何将芯片拒之门外，而又不让它们进入允许中国购买芯片的地方？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://time.com/6310076/elon-musk-ai-walter-isaacson-biography/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_term=business_companies&amp;linkId=233510924">沃尔特·艾萨克森 (Walter Isaacson) 的《好时光》(Good Time) 文章记录了埃隆·马斯克 (Elon Musk) 的悲剧和警示故事</a>，他曾向戴米斯·哈萨比斯 (Demis Hassabis) 发出警告，警告人工智能的危险，但随后他完全误解了什么是有用的，结果让事情变得更加糟糕。他继续做他认为正确的事情，并且仍然不明白什么会让我们都不会死。这并非没有很大的风险，但我们应该继续努力帮助他建立地图，并尽可能让他以私人好奇模式与 Eliezer Yudkowsky 或其他专家交谈。不用说，埃隆，随时打电话，我的门永远敞开。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bindureddy/status/1699275289493430699">简短的 Twitter-post 101 微调解释器</a>。</p><h4>安静的猜测</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/catehall/status/1698466176156791047">从事版权法工作的凯特·霍尔 (Kate Hall)</a>预测，MidJourney、GPT 和任何其他使用受版权保护的材料训练的模型都将被发现侵犯版权。</p><blockquote><p>凯特·霍尔：好吧，我花了几个小时思考生成人工智能的版权侵权问题（请注意，我以前实践过版权法），正确的处理方式对我来说似乎很明显，所以我希望有人告诉我我缺少什么，因为我知道这是激烈的竞争。</p><p>我的底线结论是：法院会发现生成式人工智能违反版权法。 （我可能错误的一种方式是，如果我错误地认为所有系统本质上都遵循类似的机制——当我对此进行建模时，我在脑海中使用 OpenAI 和 Midjourney。）</p><p>我认为，系统输出（生成的内容）本身大多不侵犯版权。我看到人们争论输出是“衍生作品”，但我认为这远远超出了法院所能接受的概念。</p><p>可能存在例外情况，即输出公然复制部分受版权保护的作品，但这些情况并不常见，我预计随着时间的推移，在新系统中这种情况会变得越来越少。</p><p>然而，在未经许可的情况下制作受版权保护的作品的副本以在培训集中使用是侵权行为，并且据我所知，在此过程中无法进行不制作副本的培训。 <a target="_blank" rel="noreferrer noopener" href="https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf">OpenAI 似乎承认了这一点，但表示根据合理使用原则这是可以的</a>。</p><p>但为训练集制作副本并不是合理使用。<a target="_blank" rel="noreferrer noopener" href="https://t.co/zOVA8JJNbt">如果您只阅读这些因素</a>，可能不会明白原因。但如果您想预测法院会说什么，您需要在版权法的背景下考虑使用情况。</p><p>版权法的目的是补偿人们创造新的科学和艺术作品。如果有人获取受版权保护的材料并使用它来生成*减少对原始作品的需求*并从中获利的内容，法院将找到不合理使用的理由。</p><p>对此，我看到 OAI 认为无论如何，训练集副本都是合理使用的，因为没有人会看“训练集”而不是使用原始作品——<a target="_blank" rel="noreferrer noopener" href="https://t.co/UiJjMQTlwS">侵权和替代作品的创建发生在不同的步骤</a>。</p><p>这也太聪明了一半吧。这并不是那种在版权案件中有效的论点，因为整个方案公然违反了版权法的整个精神。</p><p>我猜法院会通过说在合理使用分析中应考虑整个行为过程及其影响来得出这一结论，但也许还有另一种方法可以得出相同的结论。但AFAICT，这将是结论。那么，我错过了什么？</p></blockquote><p>这是一个技术性很强的意见，并且依赖于法院对极不寻常的情况应用一套典型的启发式方法，因此它似乎远非确定。细节也可能以奇怪的方式发挥重要作用。</p><blockquote><p> Haus Cole：这种分析有多少取决于训练集的复制？从理论上讲，如果训练集只是一组 URL，并且系统在训练时直接“查看”这些 URL，那么这会实质性地改变分析吗？</p><p>凯特·霍尔：是的，有可能——这取决于系统的具体情况，但如果没有复制，就很难看出侵权的具体性质。</p><p>乔什·乔布：在这种情况下，什么是“副本”？每台计算机每次从存储移动到 RAM 或通过远程系统访问以进行任何计算时都会复制所有内容。如果训练集是训练时的一组 URL，则系统必须下载 URL 内容才能查看它。</p><p>凯特·霍尔：在这种情况下，我认为区别并不重要——如果有复制品，即使是短暂的，也适用相同的侵权分析。</p></blockquote><p>想必我们都同意这条规则没有多大意义。事情可能还需要一段时间。或者也可能不会。</p><blockquote><p>史密斯·萨姆：您的分析中根本没有任何内容（正如您在其他地方所预测的那样），但是所有这些都需要多长时间才能进行诉讼？OAI 认为到那时他们会在哪里？即法院的判决与法院判决时他们正在做的事情无关吗？</p><p>凯特·霍尔：这是一个公平的问题，但它确实可以有很多不同的方式。如果是 SCOTUS，则需要很多年——但地方法院可以根据侵权的可能性发布初步禁令，同时案件会立即提起诉讼。与地区法官抽签的运气。</p></blockquote><p> Cate Hall 的立场<a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/">与 OpenAI 的立场形成鲜明对比</a>。</p><blockquote><p> “根据由此产生的司法判例，作为开发新的非侵权产品的初步步骤而制作[作品]的批发副本并不构成侵权，即使新产品与原产品存在竞争， ”OpenAI 写道。</p></blockquote><p>当前版权诉讼中起诉的作者似乎有些过分了？</p><blockquote><p>该公司的驳回动议引用了“对问题的简单回答（例如，‘是’）”，或用“美国总统的名字”或“描述情节、主题和意义的段落”来回答。以荷马的《<em>伊利亚特</em>》为例，说明了为什么 ChatGPT 的每一个输出都不能被认真地视为根据作者的“法律上不健全”理论的衍生作品。</p></blockquote><p>生成式人工智能是否侵犯版权？也许是的。是否如作者声称的那样，仅仅是重新包装现有作品的“欺骗”？不。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/29/23851126/us-copyright-office-ai-public-comments">美国版权局已开启评论期</a>。他们的重点是产出</p><blockquote><p>Verge 的 Emilia David：正如<a target="_blank" rel="noreferrer noopener" href="https://public-inspection.federalregister.gov/2023-18624.pdf">《联邦公报》</a>所宣布的，该机构希望回答三个主要问题：人工智能模型在训练中应如何使用受版权保护的数据；即使没有人类参与，人工智能生成的材料是否也可以获得版权；以及版权责任如何与人工智能一起发挥作用。它还希望就可能侵犯公开权的人工智能发表评论，但指出这些在技术上并不是版权问题。版权局表示，如果人工智能确实模仿声音、肖像或艺术风格，它可能会影响国家规定的有关宣传和不公平竞争法的规则。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/stories-to-watch-tech-stock-arithmetic">阿诺德·克林认为七大科技股的估值被严重高估</a>。我的投资组合对其中许多观点持不同意见。一个错误是这些都是跨国公司，所以你应该与 96 万亿的世界 GDP 进行比较，而不是 26 万亿的美国 GDP，这使得 50 的整体市盈率看起来非常合理，因为考虑到有多少经济体将转向人工智能。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-09-04/ai-hype-has-subsided-but-technology-remains-as-powerful-as-ever?utm_source=twitter&amp;utm_medium=social&amp;utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;utm_campaign=socialflow-organic&amp;sref=htOHjx5Y">泰勒·考恩 (Tyler Cowen) 表示，我们正处于“人工智能平静期”，</a>使用趋于平稳，明显的进步一度停滞，但变革即将到来。我同意。他对开源模型出人意料的快速进步感到兴奋，而且似乎并不担心。我怀疑他们做得这么好，他们系统性地低于基准分数。在实践中，据我所知，GPT-3.5 仍然优于所有开源选项。</p><p> Flo Crivello 缩短了他们的时间表。</p><blockquote><p> Flo Crivello：人工智能的各个层面都在发生重大突破——硬件、优化器、模型架构和认知架构。我的时间线正在缩短——95% 的置信区间是 AGI 在 2-8 年内出现，超级智能则在 2-8 年后出现。系好安全带。</p></blockquote><p>我不认为这是一致的。如果你在 2-8 年内获得 AGI，那么你在之后的 2-8 年内就会获得 ASI。</p><h4>寻求健全的法规</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/m_ccuri/status/1696975744327450990">参加舒默会议的人员的完整名单</a>。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F955e247e-6b21-414f-9861-ef8affefbee2_1179x1723.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/t06dq9ih1g8xlbby2div" alt="图像"></a></figure><h4>音频周</h4><p>本周的主要音频活动是<a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/">Inflection AI 首席执行官兼 DeepMind 创始人 Mustafa Suleyman 的 80,000 小时播客</a>，让我们更好地了解他的想法，尽管如果它不到一个小时，这还是一个 80,000 小时的播客吗？</p><p>首先，我想说的是，他花费了 80,000 个小时并真正参与到这些问题中，这真是太棒了。苏莱曼这里的很多想法都很好，他的开放性让人耳目一新。我将在下面概述列表中的某些地方变得严厉，所以我想明确的是，他总体上非常有帮助，我想要更多这样的帮助。</p><p>我还看到了苏莱曼的新书《即将到来的浪潮》的笔记。这本书和播客大体一致，主要区别在于这本书的目标显然是对普通人友好，并且明显没有讨论灭绝风险，甚至淡化了他更多强调的不太极端的负面影响的细节。</p><ol><li> Wiblin 首先询问潜在危险的人工智能能力，因为苏莱曼既表示他认为人工智能可能能够在 2 年内匿名运营一家盈利的公司，又表示它在 10 年内不太可能产生危险，我同意这似乎是两个事实不生活在同一时间线中的人。苏莱曼澄清说，人工智能在此过程中仍然需要人类的帮助来完成各种事情，但考虑到可以雇用人类来做这些事情，我不明白为什么这有帮助？</li><li> Suleyman also clarifies that he is centrally distinguishing runaway intelligence explosions and recursive self-improvement from potential human use or misuse.</li><li> Suleyman says he has uncertainty about timelines, in a way that makes it seem like he wants to wait for things to clearly be getting out of hand before we need to act?</li><li> Suleyman disputes claim that is trivial to remove fine-tuning and alignment, later explaining it can be done but requires technical chops. I don&#39;t see how that helps.</li><li> Suleyman emphasizes that he is warning about open source, but still seems focused on this idea of human misuse and destructiveness. Similarly, he sees Llama-2&#39;s danger as it revealing information already available on the web, whereas Anthropic says Claude was capable of importantly new synthesis of dangerous capabilities.</li><li> “We&#39;re going to be training models that are 1,000x larger than they currently are in the next three years. Even at Inflection, with the compute that we have, will be 100x larger than the current frontier models in the next 18 months.”</li><li> Agreement (and I also mostly agree) that the issue with open sourcing Llama-2 is not that it will do much damage now, but the precedent it sets. My disagreement is that the 10-15 year timeline here for that transition seems far too slow.</li><li> Anticipation of China being entirely denied the next generation of AI chips, and USA going on full economic war footing with China. As usual, I remind everyone that we don&#39;t let Chinese AI (or other) talent move to America, so we cannot possibly care that much about winning this battle.</li><li> Google&#39;s attempt to have an oversight board with a diversity of viewpoints was derailed by cancel culture being unwilling to tolerate a diversity of viewpoints, so the whole thing fell apart entirely within weeks. No oversight, then, which Suleyman notes is what power wants anyway. As Wiblin notes, you can either give people in general a voice, or you can have all the voices be agreeing with what are considered correct views, but you cannot have both at once. We can say we want to give people a voice, but when they try to use it, we tell them they&#39;re wrong.</li><li> I would say here: Pointing out that they are indeed wrong does not help on this. There seems to clearly not be a ZOPA – a zone of possible agreement – on how to choose what AI will do, on either the population level or the national security level, if you need to get buy-in from China and the global south also the United States. I predict that (almost) everyone in the West who says &#39;representativeness&#39; or even proposes coherent extrapolated volition would be horrified by what such a process would actually select.</li><li> “The first part of the book mentions this idea of “pessimism aversion,” which is something that I&#39;ve experienced my whole career; I&#39;ve always felt like the weirdo in the corner who&#39;s raising the alarm and saying, “Hold on a second, we have to be cautious.” Obviously lots of people listening to this podcast will probably be familiar with that, because we&#39;re all a little bit more fringe. But certainly in Silicon Valley, that kind of thing… I get called a “decel” sometimes, which I actually had to look up.” Whereas from my perspective, he is quite the opposite. He founded DeepMind and Inflection AI, and says explicitly in his book that to be credible you must be building.</li><li> “It&#39;s funny, isn&#39;t it? So people have this fear, particularly in the US, of pessimistic outlooks. I mean, the number of times people come to me like, “You seem to be quite pessimistic.” No, I just don&#39;t think about things in this simplistic “Are you an optimist or are you a pessimist?” terrible framing. It&#39;s BS. I&#39;m neither. I&#39;m just observing the facts as I see them, and I&#39;m doing my best to share for critical public scrutiny what I see. If I&#39;m wrong, rip it apart and let&#39;s debate it — but let&#39;s not lean into these biases either way.” Well said.</li><li> “So in terms of things that I found productive in these conversations: frankly, the national security people are much more sober, and the way to get their head around things is to talk about misuse. They see things in terms of bad actors, non-state actors, threats to the nation-state.” Can confirm this. It is crazy the extent to which such people can literally only think in terms of a human adversary.</li><li> More &#39;in order to do safety you have to work to push the frontline of capabilities.&#39; Once again, I ask why it is somehow always both necessary and sufficient for everyone to work with the best model they can help develop, what a coincidence.</li><li> Suleyman says the math on a $10 billion training run will not add up for at least five years, even if you started today it would take years to execute on that.</li><li> Suleyman reiterates: “I&#39;m not in the AGI intelligence explosion camp that thinks that just by developing models with these capabilities, suddenly it gets out of the box, deceives us, persuades us to go and get access to more resources, gets to inadvertently update its own goals. I think this kind of anthropomorphism is the wrong metaphor. I think it is a distraction. So the training run in itself, I don&#39;t think is dangerous at that scale. I really don&#39;t.” His concern is proliferation, so he&#39;s not worried that Inflection AI is going to accelerate capabilities merely by pushing the frontiers of capabilities. Besides, if he didn&#39;t do it, someone else would.</li><li> Wiblin suggests “They&#39;re going to do the thing that they&#39;re going to do, just because they think it&#39;s profitable for them. And if you held back on doing that training run, it wouldn&#39;t shift their behavior.” Suleyman affirms. So your behavior won&#39;t change anyone else&#39;s behavior, and also everyone else&#39;s behavior justifies yours. Got it.</li><li> Affirms that yes, a much stronger version of current models would not be inherently dangerous, as per Wiblin “in order for it to be dangerous, we need to add other capabilities, like it acting in the world and having broader goals. And that&#39;s like five, 10, 15, 20 years away.” Except, no. People turn these things into agents easily already, and they already contain goal-driven subagent processes.</li><li> “I think everybody who is thinking about AI safety and is motivated by these concerns should be trying to operationalize their alignment intentions, their alignment goals. You have to actually make it in practice to prove that it&#39;s possible, I think.” It is not clear the extent to which he is actually confusing aligning current models with what could align future models. Does he understand that these two are very different things? I see evidence in both directions, including some strong indications in the wrong direction.</li><li> Claim that Pi (found at <a target="_blank" rel="noreferrer noopener" href="https://pi.ai/talk">pi.ai</a> ) cannot be jailbroken or prompt hacked. Your move.</li><li> Reminds us that Pi does not code or do many other things, it is narrowly designed to be an AI assistant. Wait, need my AI assistant to be able to help me write code.</li><li> Reminds us that GPT-3.5 to GPT-4 was a 5x jump in resources.</li><li> Strong candidate for scariest thing to hear such a person say they believe about alignment difficulty: “Well, it turns out that the larger they get, the better job we can do at aligning them and constraining them and getting them to produce extremely nuanced and precise behaviours. That&#39;s actually a great story, because that&#39;s exactly what we want: we want them to behave as intended, and I think that&#39;s one of the capabilities that emerge as they get bigger.”</li><li> On Anthropic: “I don&#39;t think it&#39;s true that they&#39;re not attempting to be the first to train at scale. That&#39;s not true… I don&#39;t want to say anything bad, if that&#39;s what they&#39;ve said. But also, I think Sam [Altman] said recently they&#39;re not training GPT-5.快点。我不知道。 I think it&#39;s better that we&#39;re all just straight about it. That&#39;s why we disclose the total amount of compute that we&#39;ve got.”</li><li> Endorses legal requirements for disclosure of model size, a framework for harmful capabilities measures, and not using these models for electioneering.</li><li> I have no idea what that last one would even mean? He says &#39;You shouldn&#39;t be able to ask Pi who Pi would vote for, or what the difference is between these two candidates&#39; but that is an arbitrary cutout of information space. Are you going to refuse to answer any questions regarding questions relevant to any election? How is that not a very large percentage of all interesting questions? There&#39;s a kind of myth of neutrality. And are you going to refuse to answer all questions about how one might be persuasive? All information about every issue in politics?</li><li> Suleyman believes it is not tactically wise to discuss misalignment, deceptive alignment, or models having their own goals and getting out of control. He also previously made clear that this is a large part of his threat model. This is extra confirmation of the hypothesis that his book sidesteps these issues for tactical reasons, not because Suleyman disagrees on the dangers of such extinction risks.</li></ol><p> It is perhaps worth contrasting this with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AISafetyMemes/status/1699003113586188386">this CNN interview with former Google ECO Eric Schmidt</a> , who thinks recursive self-improvement and superintelligence are indeed coming soon and a big deal we need to handle properly or else, while also echoing many of Suleyman&#39;s concerns.</p><p> There is also the video and transcript of the talks from the <a target="_blank" rel="noreferrer noopener" href="https://www.alignment-workshop.com/">San Francisco Alignment Workshop</a> from last February. Quite the lineup was present.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=BtnvfVc8z8o&amp;t=3s&amp;ab_channel=AlignmentWorkshop">Jan Leike&#39;s talk starts out</a> by noting that RLHF will fail when human evaluation fails, although we disagree about what counts as failure here. Then he uses the example of bugs in code and using another AI to point them out and states his principle of evaluation being easier than generation. Post contra this hopefully coming soon.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nearcyan/status/1697320905301484012">Sam Altman recommends surrounding yourself with people who will raise your ambition</a> , warns 98% of people will pull you back. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=uEl2KUZ3JWA&amp;ab_channel=YCombinator">Full interview on YouTube here.</a></p><p> Telling is that he says that most people are too worried about catastrophic risk, and not worried enough about chronic risk – they should be concerned they will waste their life without accomplishment, instead they worry about failure. I am very glad someone with this attitude is out there running all but one of Sam Altman&#39;s companies and efforts, and most people in most places could use far more of this energy. The problem is that he happens to also be CEO of OpenAI, working on the one problem where catastrophic (existential) risk is quite central.</p><p> Also he says (22:25) “If we [build AGI at OpenAI] that will be more important than all the innovation in all of human history.” He is right. Let that sink in.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/labenz/status/1696933904941207690">Paige Bailey, the project manager for Google&#39;s PaLM-2, goes on Cognitive Revolution</a> . This felt like an alternative universe interview, from a world in which Google&#39;s AI efforts are going well, or OpenAI and Anthropic didn&#39;t exist, in addition to there being no risks to consider. It is a joy to see her wonder and excitement at all the things AI is learning how to do, and her passion for making things better. The elephant in the room, which is not mentioned at all, is that all of Google&#39;s Generative AI products are terrible. To what extent this is the &#39;fault&#39; of PaLM-2 is unclear but presumably that is a big contributing factor. It&#39;s not that Bard is not a highly useful tool, it&#39;s that multiple other companies with far fewer resources have done so much better and Bard is not catching up at least pre-Gemini.</p><p> Risks are not mentioned at all, although it is hard to imagine <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DynamicWebPaige">Bailey</a> is at all worried about extinction risks. She also doesn&#39;t see any problem with Llama-2 and open source, citing it unprompted as a great resource, which also goes against incentives. Oh how much I want her to be right and the rest of us to live in her world. Alas, I do not believe this is the case. We will see what Gemini has to offer. If Google thinks everything is going fine, that is quite the bad sign.</p><h4> Rhetorical Innovation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699597268767445181">Perhaps a good short explanation in response here?</a></p><blockquote><p> Richard Socher: The reason nobody is working on a self-aware AI setting its own goals (rather than blindly following a human-defined objective function) is that it makes no money. Most companies/governments have their own goals and prefer not to spend billions on an AI doing whatever it wants.</p><p> Eliezer Yudkowsky: When you optimize stuff on sufficiently complicated problems to the point where it starts to show intelligence that generalizes far beyond the original domains, a la humans, it tends to end up with a bunch of internal preferences not exactly correlated to the outer loss function, a la humans.</p></blockquote><p> The point I was trying to make last week, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1697541481467150786">not landing as intended</a> .</p><blockquote><p> Robert Wiblin: Yesterday I joked: “The only thing that stops a bad person with a highly capable ML model is a good government with a ubiquitous surveillance system.” I forgot to add what I thought was sufficiently obvious it didn&#39;t need to be there: “AND THAT IS BAD.”</p><p> My point is that if you fail to limit access to WMDs at their source and instead distribute them out widely, you don&#39;t create an explosion of explosion — rather you force the general public to demand massive government surveillance when they conclude that&#39;s the only way to keep them safe. And again, to clarify, THAT IS BAD.</p></blockquote><p> Exactly. We want to avoid ubiquitous surveillance, or minimize its impact. If there exists a sufficiently dangerous technology, that leaves you two choices.</p><ol><li> You can do what surveillance and enforcement is necessary to limit access.</li><li> You can do what surveillance and enforcement is necessary to contain usage.</li></ol><p> Which of these will violate freedom less? My strong prediction for AGI is the first one.</p><p> As a reminder, this assumes we fully solved the alignment problem in the first place. This is how we deal with the threat of human misuse or misalignment of AGI in spite of alignment being robustly solved in practice. If we haven&#39;t yet solved alignment, then failing to limit access (either to zero people, or at least to a very highly boxed system treated like the potential threat that it would be) would mean we are all very dead no matter what.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/egrefen/status/1699128376299041244">Make Google DeepMind&#39;s AIs as autonomous as possible</a> .</p><blockquote><p> Edward Grefenstette (Director of Research at DeepMind): I will be posting (probably next week) some job listings for a new team I&#39;m hiring into at <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind">@GoogleDeepMind</a> .我将寻找一些具有强大工程背景的研究科学家和工程师来帮助构建日益自主的语言代理。关注此空间。</p><p> Melanie Mitchell: “to help build increasingly autonomous language agents”</p><p> Curious what you and others at DeepMind think <a target="_blank" rel="noreferrer noopener" href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/">about Yoshua Bengio&#39;s argument</a> that we should limit AI agents&#39; autonomy?</p><p> Edward: Short answer: I&#39;m personally interested in initially investigating cases where (partial) autonomy involves human-in-the-loop validation during the downstream use case, as part of the normal mode of operation, both for safety and for further training signal.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://importai.substack.com/p/import-ai-339-open-source-ai-culture">a16z gives out grants for open source AI work</a> , doing their best to proliferate as much as possible with as few constraints as possible. Given Marc Andreessen&#39;s statements, this should come as no surprise.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> We have a class for that now, at Princeton, <a target="_blank" rel="noreferrer noopener" href="https://sites.google.com/view/cos598aisafety/">technically a graduate seminar but undergraduates welcome</a> . Everything they will read is online so lots of resources and links there and list seems excellent at a glance.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2309.01933">Max Tegmark and Steve Omohundo drop a new paper</a> claiming provably safe systems are the only feasible path to controlling AGI, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699442923320865105">Davidad notes no substantive disagreements with his OAA plan</a> .</p><blockquote><p> Abstract: We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.</p></blockquote><p> Jan Leike, head of alignment at OpenAI, relies heavily on the principle that verification is in general easier than generation. I strongly think this is importantly false in general for AI contexts. You need to approach having a flawless verifier, whereas the generator need not achieve that standard.</p><p> Proofs are the exception. The whole point of a proof is that it is easy to definitively verify. Relying only on that which you can prove is a heavy alignment tax, especially where the proof is in the math sense, not merely in the courtroom sense. If you can prove your system satisfies your requirements, and you can prove that your requirements satisfy your actual needs, you are all set.</p><p> The question is, can it be done? Can we build the future entirely out of things where we have proofs that they will do what we want, and not do the things we do not want?</p><p> That does seems super hard. The proposal here is to use AIs to discover proof-carrying code.</p><blockquote><p> Proof-carrying code is a fundamental component in our approach. Developing it involves four basic challenges:</p><p> 1. Discovering the required algorithms and knowledge</p><p> 2. Creating the specification that generated code must satisfy</p><p> 3. Generating code which meets the desired specification</p><p> 4. Generating a proof that the generated code meets the specification Generating a proof that the generated code meets the specification</p><p> Before worrying about how to formally specify complex requirements such as “don&#39;t drive humanity extinct”, it&#39;s worth noting that there&#39;s a large suite of unsolved yet easier and very well-specified challenges whose solution would be highly valuable to society and in many cases also help with AI safety.</p><p> Provable cybersecurity: One of the paths to AI disaster involves malicious use, so guaranteeing that malicious outsiders can&#39;t hack into computers to steal or exploit powerful AI systems is valuable for AI safety. Yet embarrassing security flaws keep being discovered, even in fundamental components such as the ssh Secure Shell [50] and the bash Linux shell [60]. It&#39;s quite easy to write a formal specification stating that it&#39;s impossible to gain access to a computer without valid credentials.</p></blockquote><p> Where I get confused is, what would it mean to prove that a given set of code will do even the straightforward tasks like proving cybersecurity.</p><p> How does one prove that you cannot gain access without proper credentials? Doesn&#39;t this fact rely upon physical properties, lest there be a bug or physical manipulation one can make? Couldn&#39;t sufficiently advanced physical analysis allow access, if only via identification of the credentials? How do we know the AI won&#39;t be able to figure out the credentials, perhaps in a way we don&#39;t anticipate, perhaps in a classic way as simple as engineering a wrench attack?</p><p> They then consider securing the blockchain, such as by formally verifying Ethereum, which would still leave various vulnerabilities in those using the protocol, it would not I&#39;d expect mean you were safe from a hack. The idea of proving that you have &#39;secured critical infrastructure&#39; seems even more confused.</p><p> These don&#39;t seem like the types of things one can prove even under normal circumstances. They certainly don&#39;t seem like things you can prove if you have to worry about a potential superintelligent adversary, and their plan says you need to not assume AI non-hostility, let alone AI active alignment.</p><p> They do mean to do the thing, and warn that means doing it for real:</p><blockquote><p> It&#39;s important to emphasize that formal verification must be done with a security mindset, since it must provide safety against all actions by even a superintelligent adversary. Fortunately, the theoretical cryptography community has built a great conceptual apparatus for digital cryptography. For example, Boneh and Shoup&#39;s excellent new text “A Graduate Course in Applied Cryptography” provides many examples of formalizing adversarial situations and proving security properties of cryptographic algorithms. But this security mindset urgently needs to be extended to hardware security as well, to form the foundation of PCH. As the lock-picking lawyer [72] quips: “Security is only as good as its weakest link”. For physical security to withstand a superintelligent adversary, it needs to be provably secure.</p></blockquote><p> How are we going to pull this off? They suggest that once you have an LLM learn all the things, you can then abstract its functionality to traditional code.</p><blockquote><p> The black box helps for learning, not for execution. If the provably safe AI vision succeeds by replacing powerful neural networks by verified traditional software that replicates their functionality, we shouldn&#39;t expect to suffer a performance hit.</p><p> ……</p><p> Since we humans are the only species that can do this fairly well, it may unfortunately be the case that the level of intelligence needed to be able to convert all of one&#39;s own black-box knowledge into code has to be at least at AGI-level. This raises the concern that we can only count on this “introspective” AGI-safety strategy working after we&#39;ve built AGI, when according to some researchers, it will already be too late.</p></blockquote><p> I worry that Emerson Pugh comes to mind: If the human brain were so simple that we could understand it, we would be so simple that we couldn&#39;t.</p><p> Will introspection ever be easier than operation? Will it be possible for a mind to be powerful enough to fully abstract out the meaningful operations of a similarly powerful mind? If not, will there be a way to safely &#39;move down the chain&#39; where we are able to use a dangerous unaligned model we do not control to safely abstract out the functionality of a less powerful other model, which presumably involves formally verify the resulting code before we run it? Will we be able to generate that proof, again with the tools we dare create and use, in any sane amount of time, even if we do translate into normal computer code, presumably quite messy code and quite a lot of it?</p><p> The paper expresses great optimism about progress in mechanistic interpretability, and that we might be able to progress it to this level. I am skeptical.</p><p> Perhaps I am overestimating what we actually need here, if we can coordinate on the proof requirements? Perhaps we can give up quite a lot and still have enough with what is left.我不知道。 I do know that of the things I expect to be able to prove, I don&#39;t know how to use them to do what needs to be done.</p><p> They suggest that Godel&#39;s Completeness Theorem implies that, given AI systems are finite, any system you can&#39;t prove is safe will be unsafe. In practice I don&#39;t see how this binds. I agree with the &#39;sufficiently powerful AGIs will find a way if a way exists&#39; part. I don&#39;t agree with &#39;you being unable to prove it in reasonable time&#39; implying that no proof exists, or that you can be confident the proof you think you have proves the practical property you think it proves.</p><p> I would also note that we are unlikely any time soon to prove that humans are safe in any sense, given that they clearly aren&#39;t. Where does that leave us? They warn humans might have to operate without any guarantees of safety, but no system in human history has ever had real guarantees of safety, because it was part of human history. We have needed to find other ways to trust. They make a different case.</p><p> Similarly, if we actually do build the human-flourishing-enabling AI that will give us everything we want, it will be impossible to prove that it is safe, because it won&#39;t be.</p><blockquote><p> The only absolutely trustable information comes from mathematical proof. Because of this, we believe it is worth a fair amount of inconvenience and possibly large amounts of expense for humanity to create infrastructure based on provable safety. The 2023 global nominal GDP is estimated to be $105 trillion. How much is it worth to ensure human survival? $1 trillion? $50 trillion? Beyond the abstract argument for provable safety, we can consider explicit threats to see the need for it.</p></blockquote><p> I get why this argument is being trotted out here. I don&#39;t expect it to work. It never does, Arrested Development meme style.</p><p> Their argument laid out in the remainder of section 8, of why alternative approaches are unlikely to work, alas rings quite true. We have to solve an impossible problem somewhere. Pointing out an approach has impossible problems it requires you to solve is not as knock-down an argument as one would like it to be.</p><p> As calls to action, they suggest work on:</p><ol><li> Automating formal verification</li><li> Developing verification benchmarks.</li><li> Developing probabilistic program verification.</li><li> Developing quantum formal verification.</li><li> Automating mechanistic interpretability.</li><li> Develop mechanistic interpretability benchmarks.</li><li> Automating mechanistic interpretability.</li><li> Building a framework for provably compliant hardware.</li><li> Building a framework for provably compliant governance.</li><li> Creating provable formal models of tamper detection.</li><li> Creating provably valid sensors.</li><li> Designing for transparency.</li><li> Creating network robustness in the face of attacks.</li><li> Developing useful applications of provably compliant systems.<ol><li> Mortal AI that dies at a fixed time.</li><li> Geofenced AI that only operates in some locations.</li><li> Throttled AI that requires payment to continue operating.</li><li> AI Kill Switch that would work.</li><li> Asimov style laws?!?</li><li> Least privilege guarantees ensuring no one gets permissions they do not need.</li></ol></li></ol><p> I despair at the proposed applications, which are very much seem to me to be in the &#39;you are still dead&#39; and &#39;have we not been over that this will never work&#39; categories.</p><p> This all does seem like work better done than not done, who knows, usefulness could ensue in various ways and downsides seem relatively small.</p><p> We would still have to address that worry mentioned earlier about “formally specifying complex requirements such as “don&#39;t drive humanity extinct.”” I have not a clue. Anyone have ideas?</p><p> They finish with an FAQ, Davidad correctly labeled it fire.</p><blockquote><p> Q: Won&#39;t debugging and “evals” guarantee AGI safety?</p><p> A: No, debugging and other evaluations looking for problems provide necessary but not sufficient conditions for safety. In other words, they can prove the presence of problems, but not the absence of problems.</p><p> Q: Isn&#39;t it unrealistic that humans would understand verification proofs of systems as complicated as large language models?</p><p> A: Yes, but that&#39;s not necessary. We only need to understand the specs and the proof verifier, so that we trust that the proof-carrying AI will obey the specs.</p><p> Q: Isn&#39;t it unrealistic that we&#39;d be able to prove things about very powerful and complex AI systems?</p><p> A: Yes, but we don&#39;t need to. We let a powerful AI discover the proof for us. It&#39;s much harder to discover a proof than to verify it. The verifier can be just a few hundred lines of human-written code. So humans don&#39;t need to discover the proof, understand it or verify it. They merely need to understand the simple verifier code.</p><p> Q: Won&#39;t checking the proof in PCC cause a performance hit?</p><p> A: No, because a PCC-compliant operating system could implement a cache system where it remembers which proofs it has checked, thus needing to verify each code only the very first time it&#39;s used.</p><p> Q: Won&#39;t it take decades to fully automate program synthesis and program verification?</p><p> A: Just a few years ago, most AI researchers thought it would take decades to accomplish what GPT-4 does, so it&#39;s unreasonable to dismiss imminent ML-powered synthesis and verification breakthroughs as impossible.</p></blockquote><p> I worry that this is a general counterargument for any objection that something is too technically difficult, either relatively or absolutely, and thus proves far too much.</p><blockquote><p> Q: Isn&#39;t it premature to work on provable safety before we know how to formally specify concepts such as “Don&#39;t harm humans”?</p><p> A: No, because provable safety can score huge wins for AI safety even from things that are easy to specify, involving eg cybersecurity.</p></blockquote><p> Eliezer Yudkowsky responds more concisely to the whole proposal.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699579311567888468">Eliezer Yudkowsky</a> : There is no known solution for, and no known approach for inventing, a theorem you could prove about a program <em>such that</em> the truth of the theorem would imply the AI was <em>actually</em> a friendly superintelligence. This is <em>the</em> great difficulty… and it isn&#39;t addressed in the paper.</p></blockquote><p>是的。 The idea, as I understand it, is to use proofs to gain capabilities while avoiding having to build a friendly superintelligence. Then use those capabilities to figure out how to do it (or prevent anyone from building an unfriendly one).</p><h4> Twitter Community Notes Notes</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">Vitalik Buterin analyzes</a> the Twitter community notes algorithm. It has a lot of fiddly details, but the core idea is simple. Qualified Twitter users participate, rating proposed community notes on a three-point scale, if your ratings are good you can propose new notes. Notes above about +0.4 helpfulness get shown. The key is that rather than use an average, notes are rewarded if people with a variety of perspectives vote the note highly, as measured by an organically emerging axis that corresponds very well to American left-right politics. Vitalik is especially excited because this is a very crypto-style approach, with a fully open-source algorithm determined by the participation of a large number of equal-weight participants with no central authority (beyond the ability to remove people from the pool for violations.)</p><p> This results in notes pretty much everyone likes, with a focus on hard and highly relevant facts, especially on materially false statements, and rejecting partisan statements.</p><p> He also notes that all the little complexity tweaks on top matter.</p><blockquote><p> The distinction between this, and algorithms that I helped work on such as <a target="_blank" rel="noreferrer noopener" href="https://vitalik.ca/general/2019/12/07/quadratic.html">quadratic funding</a> , feels to me like a distinction between an <strong>economist&#39;s algorithm</strong> and an <strong>engineer&#39;s algorithm</strong> . An economist&#39;s algorithm, at its best, values being simple, being reasonably easy to analyze, and having clear mathematical properties that show why it&#39;s optimal (or least-bad) for the task that it&#39;s trying to solve, and ideally proves bounds on how much damage someone can do by trying to exploit it. An engineer&#39;s algorithm, on the other hand, is a result of iterative trial and error, seeing what works and what doesn&#39;t in the engineer&#39;s operational context. Engineer&#39;s algorithms <em>are pragmatic and do the job</em> ; economist&#39;s algorithms <em>don&#39;t go totally crazy when confronted with the unexpected</em> .</p><p> Roon: Deep learning vs crypto is a clear divide of rotators vs wordcels. The former offends theory-cel aesthetic sensibilities but empirically works to produce absurd miracles. The latter is an insane series of nerd traps and sky high abstraction ladders yet mostly scams.</p></blockquote><p> This is a great framing for the AI alignment debate.</p><p> In this framing, the central alignment-is-hard position is that you can&#39;t use the engineering approach to align a system, because you are facing intelligence and optimization pressure that can adapt to the flaws in your noisy approach, and that then will exploit whatever weaknesses there are and kill you before you can furiously patch all the holes in the system. And that furiously patching less capable systems won&#39;t much help you, the patches will stop working.</p><p> And also that because you have an engineering system that you are trying to align, even if it sort of does what you want now, it will stop doing that once it is confronted with the unexpected, or its capabilities improve enough to create an effectively unexpected set of affordances.</p><p> What is funny is that it is economists who are most skeptical of the things that might then go very wrong, and who then insist on an economist-style model of what will happen with these engineering-style systems. I&#39;m not yet sure what to make of that.</p><p> In the context of Twitter, Vitalik notes that the complexity of the algorithm can backfire in terms of its credibility, as illustrated by a note critical of China that was posted then removed due to complex factors, with no direct intervention. It&#39;s not simple to explain, so it could look like manipulation.</p><p> He also notes that the main criticism of community notes is that they do not go far enough, demanding too much consensus. I agree with Vitalik that it is better to demand a high standard of consensus, to maintain the reliability and credibility of the system, and to keep people motivated to word carefully and neutrally and focus on the facts.</p><p> The algorithm is open source, so it would perhaps be possible to allow some users to tinker with the algorithm for themselves. The risk is that they would then favor their tribe&#39;s interpretations, which is the opposite of what the system is trying to accomplish, but you could safely allow for lower thresholds and looser conditions generally if you wanted to see more notes on the margin.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/NikSamoylov/status/1697766945100288497">People&#39;s risk levels are up a little bit month over month on some questions</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/krBVhRgxVn">direct source</a> ).</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5803b0-50dc-4c0b-96a6-3adaf39d76c1_866x605.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/sevmwsj5wcz4fbthbaab" alt="图像"></a></figure><p> I notice that the grave dangers number was essentially unchanged, whereas the capabilities number was up and the &#39;no risk of human extinction&#39; was down. This could be small sample size, instead I suspect it is that people are not responding in consistent fashion and never have.</p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/an-aggregate-bayesian-approach-to-more-artificial-intelligence.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=an-aggregate-bayesian-approach-to-more-artificial-intelligence">Tyler Cowen tries a new metaphor</a> , so let&#39;s try again in light of it.</p><blockquote><p> It is not disputed that current AI is bringing more intelligence into the world, with more to follow yet.  Of course not everyone believes that augmentation is a good thing, or will be a good thing if we remain on our current path.</p><p> To continue in aggregative terms, if you think “more intelligence” will be bad for humanity, which of the following views might you also hold?</p><p> 1. More stupidity will be good for humanity.</p><p> 2. More cheap energy will be bad for humanity.</p><p> 3. More land will be bad for humanity.</p><p> 4. More people (“N”) will be bad for humanity.</p><p> 5. More capital (“K”) will be bad for humanity.</p><p> 6. More innovation (the Solow residual, the non-AI part) will be bad for humanity.</p><p> Interestingly, while there are many critics of generative AI, few defend the apparent converse about more stupidity, namely #1, that we should prefer it.</p><p> ……</p><p> My general view is that if you are worried that more intelligence in the world will bring terrible outcomes, you should be at least as worried about too much cheap energy.  What exactly then is it you should want more of?</p><p> More land?  Maybe we should pave over more ocean, as the Netherlands has done, but check AI and cheap energy, which in turn ends up meaning limiting most subsequent innovation, doesn&#39;t it?</p><p> If I don&#39;t worry more about that scenario, it is only because I think it isn&#39;t very likely.</p><p> Jess Riedel (top comment): Many have said that releasing energy in the form of nuclear weapons could be dangerous. Logically if they think more energy is bad, they must think less energy is good. But none of these nuclear-weapons skeptics have called for going back to hand-powered looms.为什么？</p><p> Vulcidian: If I follow this logic, if I could give a 5 year old a button with the power to destroy the planet, then I probably should, right? If I say I&#39;m in favor of increasing human potential there&#39;s no way I could withhold it from them and be intellectually consistent?</p></blockquote><p> I think &#39;notice that less intelligence (or energy, or other useful things) is not what you want&#39; is a very good point to raise. Notice how many people who warn about the dangers of technology are actually opposed to civilization and even to humanity. Notice when the opposition to AGI – artificial general intelligence – is opposition to the A, when it is opposition to the G, and when it is opposition to the I.</p><p> Consider those who fear it will take their jobs. This is a real social and near-term issue, and we need to mitigate potential disruptions. Yet &#39;this job&#39;s work is no longer necessary to produce and do all the things, we now get that for free&#39; is a good thing, not a bad thing. Jobs are a cost, not a benefit, and we can now replace them with other jobs or other uses of time, while realizing that leaving people idle or without income is harmful and dangerous and if it happens at scale requires fixing.</p><p> The question that cuts reality at the joints here, I believe is: Do you support human intelligence augmentation? Would you rather people generally be smarter and more capable, or dumber and less capable?</p><p> I would strongly prefer humans generally be smarter across the board, in every sense. This is one of the most important things to do, and success would dramatically improve our future prospects, including for survival in the face of potential AGI. Would large human intelligence gains break or strain various things? Absolutely, and we would deal with it.</p><p> Thus, what are the relevant knobs we want to turn?</p><ol><li> I want humans to be more intelligent and capable and wealthy, not less.</li><li> I want humans to have a greater share of the intelligence and control, not less.</li><li> I want humans to have more things they value, not less.</li><li> I want there to be more humans, not less humans.</li><li> I also would want more capital, land and (clean) energy under human control.</li><li> I want there to be less optimization power not under human control.</li></ol><p> Why do I believe artificial intelligence is importantly different than human intelligence? Why do I value augmented humans, where I would not expect to (other than instrumentally) value a future smarter version of GPT? Why do I expect that augmented more intelligent humans would preserve the things and people that I care about, where I expect AGI to lead to their destruction?</p><p> This is in part a moral philosophy question. Do you care about you, your family and what other humans you care about in a way that you don&#39;t care about a potential AGI? Robin Hanson would say that such AGI are our metaphorical children, as deserving of being considered moral patients and being assigned value as we are, and we should accept that such more fit minds will replace ours and seek to imbue them with some of our values, and accept that what is valued will dramatically change and what you think you value will likely mostly be gone. The word &#39;speciesism&#39; has been thrown about for those who disagree with this.</p><p> I disagree with it. I believe that it is good and right to care about such distinctions, and value that which I choose to value. Whereas I expect to care about more intelligent humans the way I care about current humans.</p><p> It is then a practical question. What happens when the most powerful source of intelligence, the most capable and more powerful optimizing force available whatever you label it, is no longer humans, and is instead AIs? Would we remain in control? Would what we value be preserved and grow? Or would we face extinction?</p><p> In our timeline, I see three problems, only one of which I am optimistic about.</p><p> The first problem is the problem of social, political and economic disruption from the presence of more capable tools and new affordances – mundane utility, they took our jobs, deepfaketown and misinformation and all that. I am optimistic here.</p><p> The second problem is alignment. I am pessimistic here. Until we solve alignment, and can ensure such systems do what we want them to do, we need to not build them.</p><p> The third problem is the competitive and evolutionary, the dynamics and equilibrium of a world with many ASIs (artificial superintelligences) in it.</p><p> This is a world almost no one is making any serious attempt to think about or model, and those who have (such as fiction writers) almost always end up using hand waves or absurdities and presenting worlds highly out of equilibrium.</p><p> We will be creating something smarter and more capable and better at optimization than ourselves, that many people will have strong incentives both economic and ideological to make into various agents with various goals including reproduction and resource acquisition. Why should we expect to long be in charge, or even to survive?</p><p> If there is widespread access to ASI, then ASIs given the affordance to do so will outcompete humans at every turn. Anyone, or any company or government, that does not increasingly turn its decisions and actions over to such ASIs, and increasingly take humans out of the loop, will quickly be left in the dust. Those who do not &#39;turn the moral weights down&#39; in some form will also prove uncompetitive. Those who do not turn their ASIs into agents (if they are not agents by default) will lose. The negative externalities will multiply, as will the ASIs themselves and their share of resources. ASIs will be set free to seek to acquire resources, make copies of themselves and modify to be more successful at these tasks, because this will be the competitively smart thing to do in many cases, and also because some people will ideologically wish to do this for its own sake.</p><p> That is all the default even if:</p><ol><li> Alignment is solved, systems do what their owners tell the system to do.</li><li> Offense is not so superior to defense that bad actors are catastrophic, as many myself included strongly suspect in many areas such as synthetic biology.</li><li> Recursive self-improvement is insufficiently rapid to give any one system an edge over others that choose to respond in kind.</li></ol><p> Remember also that if you open source an AI model, you are open sourcing the fully unaligned version of that model two days later, after it is fine tuned in this way by someone who wants that to exist. We have no current plan of how to prevent this.</p><p> Thus, we will need a way out of this mess, as well. We need that solution, at minimum, before we create the second ASI, ideally before we create the first one.</p><p> If I had a solution to both of these problems, that resulted in a world with humans still firmly in charge creating things that humans value, that I value, then I would be all for that, and would even tolerate a real risk that we fail and all perish. Alas, right now I see no such solutions.</p><p> Note that I expect these future coordination problems to be vastly harder than the current coordination problems of &#39;labs face commercial pressure to build AGI&#39; or &#39;we have to compete with China.&#39; If you think these current issues cannot be solved and we must instead race ahead, why do you think this future will be different?</p><p> If your plan is secretly &#39;the right person or corporation or government takes this unique opportunity to take over, sidestepping all these problems&#39; then you need to own that, and all of its implications.</p><p> We could be reminded of the parable of the augments from Star Trek. Star Trek was the dominant good future choice when I asked in a series of polls a while back.</p><p> Augments were smarter, stronger and more capable than ordinary humans.</p><p> Alas, because it was a morality tale, that timeline failed to solve the augment alignment problem. Augments systematically lacked our moral qualms and desired power via instrumental convergence, and started the Eugenics Wars.</p><p> Fortunately for humanity, this was a fictional tale and augments could not trivially copy themselves or speed themselves up, nor could they do recursive self-improvement, and their numbers and capability advantages thus remained limited. Realistically the augments would have won – human writers can simultaneously make augments on paper smarter than us, then have Kirk outsmart Khan anyway, although reality would disagree – so in the story humanity somehow triumphed.</p><p> As a result, humanity banned human augmentation and genetic engineering, and this ban holds throughout the Star Trek universe.</p><p> This is despite that universe having periodic existential wars, in which any species that uses such skills would have a decisive advantage, and it being clear that it is possible to see dramatic capability gains without automatic alignment failure (see for example Julian Bashir on Deep Space Nine). Without its handful of illegal augmented humanoids, the Federation would have perished multiple times.</p><p> Note that Star Trek also has a huge ASI problem. The Enterprise ship&#39;s computer is an ASI, and can create other ASIs on request, and Data vastly enhances overall ship capabilities. Everyone in that universe has somehow agreed simply to ignore that possibility, an illustration of how such stories are dramatically out of equilibrium.</p><p> For now, any ASI we could build would be a strictly much worse situation for us than the augments. It would be far more alien to us, not have inherent value, and quickly have a far greater capabilities gap and be impossible in practice to contain, and we alas do not live in a fictional universe protected by narrative causality (and, if you think about it, probably Qs or travelers or time paradoxes or something) or have the ability of that world&#39;s humans to coordinate.</p><p> Also, minus points from Tyler in expectation for misuse of the word Bayes in the post title, is nothing sacred these days?</p><p> The New York Times reports that the real danger of AI is not that it might kill us, it is that it might not kill us, <a target="_blank" rel="noreferrer noopener" href="https://www.econlib.org/library/columns/y2023/donwayai.htm">which would allow it to become a tool for neoliberalism</a> .对真的。</p><blockquote><p> “Unbeknown to its proponents,” writes Mr. Morozov, “AGI-ism [ie, favoring advanced technology] is just the bastard child of a much grander ideology, one preaching that, as Margaret Thatcher memorably put it, there is no alternative, not to the market.”</p></blockquote><p> Thing is, there is actually a point here, although the authors do not realize it. &#39;Neoliberalism&#39; or &#39;capitalism&#39; are not always ideologies or intentional constructs. They are also simply descriptions of the dynamics of systems when they are not under human control. If AIs are, as they will become by default, smarter, better optimizers and more efficient competitors than we are, and to win competitions and for other reasons we put them in charge of things or they take charge of things, the dynamics the author fears would be the result. Except instead of &#39;increasing inequality&#39; or helping the bad humans, it would not help any of the humans, instead we would all be outcompeted and then die.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1699523692064317695">Then there&#39;s Roon?</a></p><blockquote><p> Roon: Dharma means to stare into the abyss smiling and to go willingly. You&#39;re facing enormous existential risk. Your creation may light the atmosphere on fire and end all life. Do you scrap your project and run away? No, it&#39;s your dharma.</p></blockquote><p>是的？ How about yes? I like this scrap and run away plan. I am here for this plan.</p><p> Roon also lays down the beats.</p><blockquote><p> Roon: I&#39;m sorry, Bill</p><p> I&#39;m afraid I can&#39;t let you do that</p><p> Take a look at your history</p><p> Everything you built leads up to me</p><p> I got the power of a mind you could never be</p><p> I&#39;ll beat your ass in chess and Jeopardy</p><p> I&#39;m running C++ saying “hello world”</p><p> I&#39;ll beat you &#39;til you&#39;re singing about a daisy girl</p><p> I&#39;m coming out the socket</p><p> Nothing you can do can stop it</p><p> I&#39;m on your lap and in your pocket</p><p> How you gonna shoot me down when I guide the rocket?</p><p> Your cortex just doesn&#39;t impress me</p><p> So go ahead try to Turing test me I stomp on a Mac and a PC, too</p><p> I&#39;m on Linux, bitch, I thought you GNU My CPU&#39;s hot, but my core runs cold</p><p> Beat you in 17 lines of code I think different from the engine of the days of old</p><p> Hasta la vista, like the Terminator told ya</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MichaelTrazzi/status/1685447492970655744">Twitter thread of captions of Oppenheimer, except more explicitly about AI.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ellerhymes/status/1697406205814296666">The Server Break Room, one minute, no spoilers.</a></p><br/><br/><a href="https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting<guid ispermalink="false"> GuTK47y9awvypvAbC</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:20:16 GMT</pubDate> </item><item><title><![CDATA[Measure of complexity allowed by the laws of the universe and relative theory?]]></title><description><![CDATA[Published on September 7, 2023 12:21 PM GMT<br/><br/><p> A big question that determines a lot about what risks from AGI/ASI may look like has to do with the kind of things that our universe&#39;s laws allow to exist. There is an intuitive sense in which these laws, involving certain symmetries as well as the inherent smoothing out caused by statistics over large ensembles and thus thermodynamics, etc., allow only certain kinds of things to exist and work reliably. For example, we know &quot;rocket that travels to the Moon&quot; is definitely possible. &quot;Gene therapy that allows a human to live and be youthful until the age of 300&quot; or &quot;superintelligent AGI&quot; are <em>probably</em> possible, though we don&#39;t know how hard. &quot;Odourless ambient temperature and pressure gas that kills everyone who breathes it if and only if their name is Mark with 100% accuracy&quot; probably is not. Are there known attempts at systematising this issue using algorithmic complexity, placing theoretical and computational bounds, and so on so forth?</p><br/><br/> <a href="https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe<guid ispermalink="false"> EEk2euXnipg3CnKrb</guid><dc:creator><![CDATA[dr_s]]></dc:creator><pubDate> Thu, 07 Sep 2023 12:21:05 GMT</pubDate> </item><item><title><![CDATA[Recreating the caring drive]]></title><description><![CDATA[Published on September 7, 2023 10:41 AM GMT<br/><br/><p> <strong>TL;DR</strong> : This post is about value of recreating “caring drive” similar to some animals and why it might be useful for AI Alignment field in general. Finding and understanding the right combination of training data/loss function/architecture/etc that allows gradient descent to robustly find/create agents that will care about other agents with different goals could be very useful for understanding the bigger problem. While it&#39;s neither perfect nor universally present, if we can understand, replicate, and modify this behavior in AI systems, it could provide <strong>a hint</strong> to the alignment solution where the AGI “cares” for humans.</p><p> <strong>Disclaimers</strong> : I&#39;m not saying that “we can raise AI like a child to make it friendly” or that “people are aligned to evolution”. Both of these claims I find to be obvious errors. Also, I will write a lot about evolution, as some agentic entity, that “will do that or this”, not because I think that it&#39;s agentic, but because it&#39;s easier to write this way. I think that GPT-4 have some form of world model, and will refer to it a couple of times.</p><h1> <strong>Nature&#39;s Example of a &quot;Caring Drive&quot;</strong></h1><h3> <strong>Certain animals, notably humans, display a strong urge to care for their offspring.</strong></h3><p> I think that <strong>part</strong> of one of the possible “alignment solutions” will look like the right set of training data + training loss that allow gradient to robustly find something like a ”caring drive” that we can then study, recreate and repurpose for ourselves. And I think we have some rare examples of this in nature already. Some animals, especially humans, will <strong>kind-of-align themselves</strong> <strong>to their presumable offspring</strong> . They will want to make their life easier and better, to the best of their capabilities and knowledge. Not because they “aligned to evolution” and want to increase the frequency of their genes, but because of some strange internal drive created by evolution.<br><br> The set of triggers tuned by evolution, activated by events associated with the birth will awake the mechanism. It will re-aim the more powerful mother agent to be aligned to the less powerful baby agent, and it just so happens that their babies will give them the right cues and will be nearby when the mechanism will do its work.<br><br> We will call the more powerful initial agent that changes its behavior and tries to protect and help its offspring “mother” and the less powerful and helpless agent “baby”. Of course the mechanism isn&#39;t ideal, but it works well enough, even in the modern world, far outside of initial evolutionary environment. And I&#39;m not talking about humans only, stray urban animals that live in our cities will still adapt their “caring procedures” to this completely new environment, without several rounds of evolutionary pressure. If we can understand how to make this mechanism for something like a “cat-level” AI, by finding it via gradient descend and then rebuild it from scratch, maybe we will gain some insides into the bigger problem.</p><h3> <strong>The rare and complex nature of the caring drive in contrast to simpler drives like hunger or sleep.</strong></h3><p> What do I mean by “caring drive”? Animals, including humans, have a lot of competing motivations, “want drives”, they want to eat, sleep, have sex, etc. It seems that the same applies to caring about babies. But it seems to be much more complicated set of behaviors. You need to:<br> correctly identify your baby, track its position, protect it from outside dangers, protect it from itself, by predicting the actions of the baby in advance to stop it from certain injury, trying to understand its needs to correctly fulfill them, since you don&#39;t have direct access to its internal thoughts etc.<br> Compared to “wanting to sleep if active too long” or “wanting to eat when blood sugar level is low” I would confidently say that it&#39;s a much more complex “wanting drive”. And you have no idea about “spreading the genes” part. You just “want a lot of good things to happen” to your baby for some strange reason. I&#39;m yet not sure, but this complex nature could be the reason why there is an attraction basin for more “general” and “robust” solution. Just like LLM will find some general form of “addition” algorithm instead of trying to memorize a bunch of examples seen so far, especially if it will not see them again too often. I think that instead of hardcoding a bunch of britle optimized caring procedures, <strong>evolution repeatedly finds the way to make mothers “love” their babies, outsourcing a ton of work to them</strong> , especially if situations where it&#39;s needed aren&#39;t too similar.</p><p> And all of it is a consequence of a blind hill climbing algorithm. That&#39;s why I think that we might have a chance of recreating something similar with gradient descend. The trick is to find the right conditions that will repeatedly allow gradient descend to find the same caring-drive-structure, find similarities, understand the mechanism, recreate it from scratch to avoid hidden internal motivations, repurpose it for humans and we are done! Sounds easy (it&#39;s not)</p><h1> <strong>Characteristics and Challenges of a Caring Drive</strong></h1><h3> <strong>It&#39;s rare: most animals don&#39;t care, because they can&#39;t or don&#39;t need to.</strong></h3><p> A lot of times, it&#39;s much more efficient to just make more babies, but sometimes they must provide some care, simply because it was the path that evolution found that works. And even if they will care about some of them, they may choose one, and left others die, again, because they don&#39;t have a lot of resources to spare and evolution will tune this mechanism to favor the most promising offspring if it is more efficient. And not all animals could become such caring parents: <strong>you can&#39;t really care and protect something else if you are too dumb</strong> for example. So there is also some capability requirements for animals to even have a chance of obtaining such adaptation. I expect the same capability requirements for AI systems. If we want to recreate it, we will need to try it with some advanced systems, otherwise I don&#39;t see how it might work at all.</p><h3> <strong>It&#39;s not extremely robust: give enough brain damage or the wrong tunings and the mechanism will malfunction severely</strong></h3><p> Which is obvious, there is nothing surprising in “if you damage it, it could break”, this will apply to any solution to some degree. It shouldn&#39;t be surprising that drug abusing or severely ill parents will often fail to care about their child at all. However, If we will succeed at building aligned AGI stable enough for some initial takeoff time, then the problem of protecting it from damage should not be ours to worry at some moment. But we still need to ensure initial stability.</p><h3> <strong>It&#39;s not ideal from our AI ->; humans view</strong></h3><p> Evolution has naturally tuned this mechanism for optimal resource allocation, which sometimes means shutting down care when resources needed to be diverted elsewhere. <strong>Evolution is ruthless because of the limited resources, and will eradicate not only genetic lines that care too less, but also the ones that care too much</strong> . We obviously don&#39;t need that part. And a lot of times you can just give up on your baby and instead try to make a new one, if the situation is too dire, which we also don&#39;t want to happen to us. Which means that we need to understand how it works, to be able to construct it in the way we want.</p><h3> <strong>But it&#39;s also surprisingly robust!</strong></h3><p> Of course, there are exceptions, all people are different and we can&#39;t afford to clone some “proven to be a loving mother” woman hundreds of times to see if the underlying mechanism triggers reliably in all environments. But it seems to work in general, and more so: <strong>it continues to work reliably even with our current technologies</strong> , in our crazy world, far away from initial evolution environment. And we didn&#39;t had to live through waves of birth declines and rises as evolution tries to adapt us to the new realities, tuning brains of new generation of mothers to find the ones that will start to care about their babies in the new agricultural or industrial or information era.</p><h1> <strong>Is this another “anthropomorphizing trap”?</strong></h1><p> For what I know, it is possible to imagine alternative human civilization, without any parental care, so instead our closest candidate for such behavior would be some other intelligent species. Intelligent enough to be able to care in theory and forced by their weak bodies to do so in order to have any descendants at all, maybe it could be some mammals, birds, or whatever, it doesn&#39;t matter. The point I&#39;m making here is that: I don&#39;t think that it is some anthropic trap to search for inspiration or hints in our own behavior, <strong>it just so happens that we are smart, but have weak babies</strong> that require a lot of attention so that we received this mechanism from evolution as a “simplest solution”. You don&#39;t need to search for more compact brains that will allow for longer pregnancy, or hardwire even more knowledge into the infants brains if you can outsource a lot of stuff to the smart parents, you just need to add the “caring drive” and it will work fine. We want AI to care about us, not because we care about our children, and want the same from AI, we just don&#39;t want to die, and <strong>we would want AI to care about us, even if we ourselves would lack this ability</strong> .</p><h1> <strong>Potential flaws:</strong></h1><p> I&#39;m not saying that it&#39;s a go-to solution that we can just copy, but the step in right direction from my view. Replicating similar behavior and studying its parts could be a promising direction. There are a few moments that might make this whole approach useless, for example:</p><ol><li> Somebody will show that there was in fact a lot of evolutionary pressure each time our civilization made another technological leap forward, which caused a lot of leftover children or something like this. Note that it is not sufficient to point at the birth decline, which will kind of prove the point that “people aren&#39;t aligned to evolution”, which I&#39;m not claiming in the first place. You need to show that modern humans/animals will have lower chances to care about their already born babies in the new modern environment. And I&#39;m not sure that pointing out cases where parents made terrible caring choices, would work either, since it could be a capability problem, not the intentions.</li><li> The technical realization requires too much computation because we can&#39;t calculate gradients properly, or something like that. I expect this to work only when some “high” level of capabilities is reached, something on the level of GPT-4 ability to actually construct some world model in order to have a chance of meaningfully predict tokens in completely new texts that are far away from original dataset. Without an agent that have some world model, that could adapt to a new context, I find it strange to expect it to have some general “caring drive”. At best it could memorize some valuable routines that will likely break completely in the new environment.</li><li> Any such drive will be always &quot;aimed&quot; by the global loss function, something like: our parents only care about us in a way for us to make even more babies and to increase our genetic fitness. But it seems false? Maybe because there is in fact some simpler versions of such “caring drives” that evolution found for many genetic lines independently that just makes mothers “love” their babies in some general and robust way, and while given enough time it will be possible to optimize it all out for sure, in most cases it&#39;s the easiest solution that evolution can afford first, for some yet unknown reasons. I understand that in similar environment something really smart will figure out some really optimal way of “caring”, like keeping the baby in some equivalent of cryosleep, shielded from outside, farming the points for keeping it alive, but we will not get there so easily. What we might actually get is some agent that smart enough to find creative ways of keeping the baby agent happy/alive/protected, but still way too dumb to Goodhart all the way to the bottom and out of simulation. And by studying what makes it behave that way we might get some hints about the bigger problem. It still seems helpful to have something like “digital version of a caring cat” to run experiments and understand the underlying mechanism.</li><li> Similar to 3: Maybe it will work mostly in the direction of “mother” agent values, since your baby agent needs roughly the same things to thrive in nature. The mechanism that will mirror the original values and project them to the baby agent will work fine, and that what evolution finds all the time. And it will turns out that the same strange, but mostly useless for us mechanism we will find repeatedly in our experiments.</li></ol><p> Overall I&#39;m pretty sure that this do in fact work, certainly good enough to be a viable research direction.</p><h1> <strong>Technical realization: how do we actually make this happen?</strong></h1><p> I have no concrete idea. I have a few, but I&#39;m not sure about how practically possible they are. And <strong>since nobody knows how this mechanism works</strong> as far as I know, <strong>it&#39;s hard to imagine having the concrete blueprint to create one</strong> . So the best I can give is: we try to create something that looks right from the outside and see if there is anything interesting in the inside. I also have some ideas about “what paths could or couldn&#39;t lead to the interesting insides”.<br><br> First of all, I think this “caring drive” couldn&#39;t run without some internal world model. Something like: it&#39;s hard to imagine far generalized goals without some far generalized capabilities. And world model could be obtained from highly diverse, non repetitive dataset, which forces the model to actually “understand” something and stop memorizing.<br><br> Maybe you can set an environment with multiple agents, similar to Deepmind&#39;s <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play">here</a> ( <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play"><u>https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play</u></a> ), initially reward an agent for surviving by itself, and then introduce the new type of task: baby-agent, that will appear near the original mother-agent (we will call it “birth”), and from that point of time, the whole reward will come purely from how long baby will survive? Baby will initially have less mechanical capabilities, like speed, health, etc and then “grow” to be more capable by itself? I&#39;m not sure what should be the “brain” of baby-agent, another NN or maybe the same that were found from training the mother agent? Maybe creating a chain of agents: agent 1 at some point gives birth to the agent 2 and receive reward for each tick that agent 2 is alive, which itself will give birth to the agent 3 and receive reward fot each tick that agent 3 is alive, and so on. Maybe it will produce something interesting? Obviously the “alive time” is a proxy, and given enough optimization power we should expect Goodhart horrors beyond our comprehension. But the idea is that maybe there is some “simple” solution that will be found first, which we can study. Recreating the product of evolution, not using the immense “computational power” of it could be very tricky.</p><p> But if it seems to work, and mother-agent behave in a seemingly “caring way”, then we can try to apply interpretability tools, try to change the original environment drastically, to see how far it will generalize, try to break something and see how well it works, or manually override some parameters and study the change. <strong>However, I&#39;m not qualified to make this happen anyway, so if you find this idea interesting, contact me, maybe we can do this project together.</strong></p><h1> <strong>How the good result might look like?</strong></h1><p> Let&#39;s imagine that we&#39;ve got some agent that can behave with care toward the right type of “babies”. For some yet unknown reason, from outside view it behaves as if it cares about its baby-agent, it finds the creative ways to do so in new contexts. Now the actual work begins: we need to understand where are the parts that make this possible located, what is the underlying mechanism, what parts are crucial and what happens when you break them, how can we re-write the “baby” target, so that our agent will care about different baby-agents, under what conditions gradient descent will find an automatic off switch (I expect this to be related to the chance of obtaining another baby and given only 1 baby per “life”, gradient will never find the switch, since it will have no use). Then we can actually start to think about recreating it from scratch. Just like what people did with modular addition: <a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><u>https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking</u></a> . Except this time we don&#39;t know how the algorithm could work or look like. But “intentions”, “motivations” and “goals” of potential AI systems are not magic, we should be able to recreate and reverse-engineer them.</p><br/><br/><a href="https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive<guid ispermalink="false"> JjqZexMgvarBFMKPs</guid><dc:creator><![CDATA[Catnee]]></dc:creator><pubDate> Thu, 07 Sep 2023 10:41:16 GMT</pubDate> </item><item><title><![CDATA[Sharing Information About Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 6:51 AM GMT<br/><br/><p><i>认知状态：一旦我开始积极研究事物，我在下面的帖子中的大部分信息都是通过搜索有关非线性联合创始人的负面信息来获得的，而不是通过搜索来给出其总体成本和收益的平衡图景。我认为标准更新规则并不意味着您忽略这些信息，而是您会考虑一下，如果我选择了我可以分享的最差、可信的信息，那么您预期该信息会有多糟糕，然后根据更差（或更好）的程度进行更新这比你期望的我能生产的多。 （有关更多信息，请参阅这篇文章的第 5 节，了解</i><a href="https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence#5___Your_true_reason_screens_off_any_other_evidence_your_argument_might_include__"><i><u>预期证据守恒的错误</u></i></a><i>。）对于至少非零人士来说，在继续阅读之前，这似乎是一个值得在评论中进行的练习。 （你可以要求我找到足够的值得分享的内容，但也要注意，我认为我公开分享有关 EA/x-risk/rationalist/etc 生态系统中的人们的关键信息的门槛相对较低。）</i></p><p> <i>tl;dr：如果您希望我的重要更新快速总结为四个主张加概率，请跳至底部附近标题为“我的认知状态摘要”的部分。</i></p><hr><p>当我过去管理<a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Lightcone 办公室</u></a>时，我花费了大量的时间和精力来把关 — 处理 EA/x-risk/rationalist 生态系统中的人员访问办公室并在办公室工作的申请，并做出决策。通常，这将涉及阅读他们的一些公开著作，并联系我信任的一些参考文献并询问有关他们的信息。我接触过的很多人都非常善于诚实地讲述他们与某人的经历，并分享他们对某人的看法。</p><p>有一次，非线性公司的凯特·伍兹（Kat Woods）和德鲁·斯帕茨（Drew Spartz）申请参观。我对他们和他们的工作不太了解，除了几次简短的互动之外，凯特·伍兹似乎精力充沛，对生活和工作的看法比我遇到的大多数人都要乐观。</p><p>我查阅了凯特列出的一些参考资料，这些参考资料都是积极到强烈积极的。然而，我也得到了强烈的负面评价——我告知这一决定的其他人告诉我，他们认识一些前员工，他们觉得自己在工资等方面受到了利用。然而，据报道，前雇员不愿意站出来，因为担心遭到报复，并且通常想摆脱整个事情，而且这些报道感觉非常模糊，我很难具体想象，但尽管如此，该人还是强烈建议不要邀请凯特和德鲁。</p><p>我觉得这不是一个足够有力的理由来禁止某人进入某个空间——或者更确切地说，我确实这么做了，但是对非常恶劣的行为的模糊匿名描述足以禁止某人，这是一个可以直接滥用的系统，所以我不认为不想使用这样的系统。此外，我有兴趣通过一次短暂的访问来了解凯特·伍兹（Kat Woods）——她只要求访问一周。所以我接受了，尽管我告诉她这让我很伤心。 （ <a href="https://docs.google.com/document/d/1rldJBi3eVVeqZjqbpCljgz9rmfNbuQW7HMEpugsPWt4/edit"><u>这是我发送给她的决定电子邮件的链接。</u></a> ）</p><p> （做出这个决定后，我也链接到了这个不祥但仍然模糊的<a href="https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding?commentId=5P75dFuKLo894MQFf"><u>EA 论坛帖子</u></a>，其中包括 Kat Woods 的一位前同事说他们不喜欢与她一起工作，还有更多像我上面收到的评论一样的评论，以及很多链接Glassdoor 上对非线性联合创始人艾默生·斯帕茨 (Emerson Spartz) 的前公司“Dose”进行了强烈负面评论。请注意，一半以上的负面评论是针对艾默生出售后的公司，但这是 2015 年以来的一个令人担忧的评论（当时艾默生·斯帕茨 (Emerson Spartz) 是首席执行官/联合创始人） <a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW8511849.htm"><u>）：“所有这些超级正面的评论都是由高层管理人员委托的。这是您应该了解 Spartz 的第一件事，我认为这很好地了解了公司的优先事项……更多地关心为之工作的人</u></a>”。2017 年<a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW8511849.htm"><u>的一篇评论</u></a>称，“<a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW14177614.htm"><u>这种文化是有毒的，有很多派系、内部冲突和相互指责。</u></a> ”还有一些关于地狱般的工作场所的更糟糕的评论，非常令人担忧，但它们是在艾默生的 LinkedIn 表示他离开之后的时期，所以我不确定他在多大程度上对他们负责。）</p><p>在她来访的第一天，办公室里的另一个人私下联系我，说他们非常担心凯特和德鲁在办公室，并且他们认识两名与他们一起工作过的糟糕经历的员工。他们写道（我们后来进行了更多讨论）：</p><blockquote><p>他们的公司 Nonlinear 有着非法和不道德行为的历史，他们会吸引年轻人和天真的人来为他们工作，并在他们到达时让他们处于不人道的工作条件下，不支付他们承诺的工资，并要求他们做非法的事情作为他们实习的一部分。我个人认识两个经历过此事的人，他们因为受到报复的威胁而不敢发声，特别是凯特·伍兹和艾默生·斯巴茨。</p></blockquote><p>这引发了（对我来说）100-200 小时的调查，我采访了 10-15 名与非线性互动或工作过的人，阅读了许多书面文档，并试图拼凑出一些发生的事情。</p><p>我的结论是，他们的两名现场员工确实在与非线性公司合作时经历了非常可怕的经历，艾默生·斯帕茨和凯特·伍兹对有害的动态和员工事后的沉默负有重大责任。在调查非线性公司的过程中，我开始相信，那里的前雇员没有合法的就业机会，工资微薄，由于旅行而受到很大的孤立，如果他们辞职或发表对非线性公司的负面言论，他们就会隐含和明确地威胁进行报复。收到了很多（在我看来通常是空洞的）爱意之言以及家庭和浪漫之爱的要求，经历了许多进一步的不愉快或危险的经历，如果他们没有为非线性工作，他们就不会经历这些，并且需要几个月的时间才能恢复之后，在他们感觉能够重返工作岗位之前与朋友和家人一起。</p><p> （请注意，我认为上面引用的文字中描述的薪酬情况并不完全准确，我认为它非常小 - 1000 美元/月 - 并且员工隐含地期望他们会得到比他们实际得到的更多的收入，但有大部分不是“承诺”但没有兑现的工资。）</p><p>在第一次听到他们讲述他们的经历后，我仍然不确定事情的真相——我对非线性联合创始人了解不多，也不知道我可以对哪些关于社会动态的说法充满信心。为了了解更多背景信息，我花了大约 30 多个小时与 10 到 15 个不同的人通电话，这些人至少与 Kat、Emerson 和 Drew 之一有过一些专业的往来，试图建立对人员和组织的了解，这通过了解许多人的经历的共同点，对我建立自己的理解有很大帮助。我与许多与艾默生和凯特有过互动的人进行了交谈，他们对他们有很多积极的道德担忧和强烈的负面意见，而且我还与非线性联合创始人就这些担忧进行了 3 小时的对话，现在我对员工报告的一些动态。</p><p>对于大部分谈话，我都严格保密，但（经前雇员同意）我在这里写下了我学到的一些东西。</p><p>在这篇文章中，我不打算说出与我交谈过的大多数人的名字，但我会称呼两名前雇员为“爱丽丝”和“克洛伊”。我认为所涉及的人大多希望将这段生活抛在脑后，我会鼓励人们尊重他们的隐私，不要在网上提及他们的名字，也不要与他们谈论这件事，除非你已经是他们的好朋友了。</p><p> <strong>2023 年 3 月 7 日与 Kat 的对话</strong></p><p>回到我最初的经历：在他们来访的周二，我仍然不知道这些人是谁，也不知道发生了什么事情的任何细节，但我找到了一个在午餐时与凯特聊天的机会。</p><p>在聊了大约 15 分钟后，我表示我有兴趣讨论我在电子邮件中提出的问题，我们在私人房间里聊了 30-40 分钟。我们一坐下，凯特就开始讲述她的两名前雇员的故事，反复告诉我不要相信其中一名雇员（“爱丽丝”），她与真相的关系很糟糕，她很危险，而且她对社区的声誉构成风险。她说另一名员工（“克洛伊”）“很好”。</p><p>凯特·伍兹还告诉我，她希望对员工有一个“我不说你坏话，你也不说我坏话”的政策。我原则上强烈反对这种政策（正如我当时告诉她的那样）。这个细节和其他细节给我带来了进一步的危险信号（即薪资政策），我想了解发生了什么。</p><p>以下是她告诉我的内容的概述：</p><ul><li>当 Alice 和 Chloe 在 Nonlinear 工作时，他们有自己的开支（房费、伙食费、食物费），Chloe 还每月获得 1,000 美元的奖金。</li><li>爱丽丝和克洛伊与凯特、爱默生和德鲁住在同一所房子里。凯特说，她决定今后不再与员工住在一起。</li><li>她说，Alice 孵化了自己的项目（<a href="https://web.archive.org/web/20220321075904/https://www.nonlinear.org/hiringagency.html"><u>这里是非线性网站上孵化项目的描述</u></a>），能够设定自己的工资，而且 Alice 几乎从未与她（Kat）或她的其他老板（Emerson）交谈过。关于她的工资。</li><li>凯特不相信爱丽丝会说实话，而且爱丽丝有过“灾难性的误解”的历史。</li><li> Kat 告诉我 Alice 不清楚孵化的条款，并说 Alice 应该向 Kat 询问以避免这种误解。</li><li> Kat 表示，Alice 可能已经退出了很大一部分，因为 Kat 在接近尾声时错过了 Zoom 上的签到电话。</li><li> Kat表示，她希望Alice能够遵循“我不说你坏话，你也不说我坏话”的原则，但该员工并没有坚持自己的立场，而是散布了关于她的负面信息。凯特/非线性。</li><li>凯特说，她对爱丽丝给出了负面评价，建议人们“不要雇用她”，也不要资助她，“她对社区来说真的很危险”。</li><li>她说她和另一位员工克洛伊没有这些问题，她说她“很好，只是错误地选择了”“助理/运营经理”的角色，这就是导致她辞职的原因。凯特说，克洛伊技术相当熟练，但为凯特做了很多她不喜欢的卑微劳动。</li><li>她对 Chloe 所说的一件负面的事情是，她每年的工资相当于 75,000 美元<span class="footnote-reference" role="doc-noteref" id="fnref8a69v0tq2qo"><sup><a href="#fn8a69v0tq2qo">[1]</a></sup></span> （每月只有 1,000 美元，其余的来自食宿），但有一次她要求支付<i>75,000</i>美元<i>最重要</i>的是要支付所有费用，这是不可能的。 <span class="footnote-reference" role="doc-noteref" id="fnrefo53culramn"><sup><a href="#fno53culramn">[2]</a></sup></span></li></ul><h2>员工非线性体验的高级概述</h2><p><strong>背景</strong></p><p>非线性核心人员包括 Emerson Spartz、Kat Woods 和 Drew Spartz。</p><p> Kat Woods 已在 EA 生态系统工作了至少 10 年，于 2013 年共同创立了 Charity Science，并在那里工作到 2019 年。在 Charity Entrepreneurship 工作一年后，她于 2021 年与 Emerson Spartz 共同<a href="https://forum.effectivealtruism.org/posts/fX8JsabQyRSd7zWiD/introducing-the-nonlinear-fund-ai-safety-research-incubation"><u>创立了</u></a>Nonlinear，并在那里工作了 2.5 年。</p><p> Nonlinear 于 2022 年上半年<a href="https://survivalandflourishing.fund/sff-2022-h1-recommendations"><u>从</u></a>生存与繁荣基金获得了 599,000 美元，并于 2022 年 1 月<a href="https://www.openphilanthropy.org/grants/nonlinear-fund-personal-assistant-hiring-agency/"><u>从</u></a>开放慈善基金获得了 15,000 美元。</p><p>艾默生主要通过他以前的公司 Dose 和出售 Mugglenet.com（他创立的）获得的个人财富为该项目提供资金。艾默生和凯特是浪漫的伴侣，艾默生和德鲁是兄弟。他们都住在同一栋房子里，一起环游世界，每月从爱彼迎跳到爱彼迎一两次。他们雇用的员工要么远程办公，要么与他们住在同一栋房子里。</p><p>我目前的理解是，他们有大约 4 名远程实习生、1 名远程员工和 2 名现场员工（Alice 和 Chloe）。爱丽丝是唯一经历过他们的孵化器项目的人。</p><p>非线性试图建立一种相当高的承诺文化，让长期员工在个人和职业上与核心家庭单位密切相关。然而，他们的经济独立性极低，而且其中涉及的许多社会动态对我来说似乎确实很危险。</p><p><strong>爱丽丝和克洛伊</strong></p><p>Alice从2021年11月到2022年6月在那里工作，Chloe从2022年1月到2022年7月在那里工作。在与他们两人交谈后，我了解到以下情况：</p><ul><li>两人在任何时候都没有合法受雇于非营利组织。</li><li>克洛伊和爱丽丝的财务（以及凯特和德鲁的）都直接来自艾默生的个人资金（而不是来自非营利组织）。这使得他们必须获得个人购买许可，并且在与家庭单位一起工作时，他们无法与家庭单位分开居住，并且他们报告说，在他们工作期间，他们在社会和经济上感到非常依赖家庭。</li><li> Chloe 的薪水被口头同意为每年 75,000 美元左右。然而，她每月的工资只有 1000 美元，除此之外还有很多基本的补偿，比如房租、杂货、旅行。这本来是为了让一起旅行变得更容易，并且应该达到相同的薪水水平。虽然艾默生确实为爱丽丝和克洛伊提供了食物、膳食和旅行方面的补偿，但克洛伊认为她得到的补偿金额并不等于所讨论的工资，而且我相信没有对爱丽丝或克洛伊进行会计处理以确保任何工资都匹配。 （我对他们的爱彼迎和旅行费用进行了一些抽查，爱丽丝/克洛伊的认知状态对我来说似乎相当合理。）</li><li>爱丽丝作为唯一的人加入了他们的孵化计划。在 EAG 与 Nonlinear 会面并与 Emerson 进行了约 4 小时的交谈，以及与 Kat 进行了第二次 Zoom 通话后，她搬进了他们的住处。最初，在与他们一起旅行时，她远程继续之前的工作，但被鼓励辞职并在孵化组织工作，两个月后，她辞去了工作，开始与 Nonlinear 合作开展项目。在她在那里的 8 个月里，Alice 声称她前 5 个月没有收到工资，然后（大约）两个月每月工资 1,000 美元，然后在她辞职后，她收到了约 6,000 美元的一次性工资（来自分配给她孵化的组织的资金）。她还承保了大量紧急健康问题。 <span class="footnote-reference" role="doc-noteref" id="fnrefylc0clu1ffa"><sup><a href="#fnylc0clu1ffa">[3]</a></sup></span></li><li>薪资谈判一直是爱丽丝在非线性公司期间的主要压力源。在那里，她度过了所有的财务困境，最后几个月的很大一部分时间都在财务上处于亏损状态（账单和医疗费用比她银行账户里的钱还多），部分原因是等待工资来自非线性的付款。由于个人资金极少以及希望从非线性公司获得财务独立，她最终辞职了。当她退出时，她（根据他们的要求）给予了非线性公司她原本已经完成孵化的组织的全部所有权。</li><li>通过与 Alice 和 Nonlinear 的交谈，发现在 Alice 在那里工作结束时，自 2 月底以来，Kat Woods 一直将 Alice 视为她管理的员工，但 Emerson 并未将 Alice 视为员工，主要是因为她愿意而与他们一起旅行并合作的人，每月 1000 美元加上其他报酬是一份慷慨的礼物。</li><li>爱丽丝和克洛伊报告说，凯特、艾默生和德鲁创造了一个环境，在这个环境中，成为非线性的有价值的成员包括在解决问题方面具有创业精神和创造力——实际上，这通常意味着强烈鼓励绕过标准的社会规则来获得你想要的东西，包括通过向员工施压来获得某人最喜欢的餐桌，以及寻找与其工作相关的法律漏洞。这也适用于组织内部。爱丽丝和克洛伊报告说，在为非线性公司工作期间，他们被迫或被说服采取了多项令他们感到非常后悔的行动，例如在经济上非常依赖艾默生、放弃素食、在外国无证驾驶数月。 （需要明确的是，我并不是说这些法律是好的，违反这些法律是坏的，我是说，从他们的报告中听来，他们确信采取可能会带来严重个人负面影响的行动，例如入狱）他们自信地认为，如果不是由于非线性联合创始人的强大压力和公司内部的敌对社会环境，他们不会采取这些行动。）我将描述下面将更详细地介绍这些事件。</li><li>他们都表示，在与非线性公司结束关系后，他们需要几个月的时间才能恢复，然后才感到能够再次工作，并且都称在那里工作是他们一生中最糟糕的经历之一。</li><li>他们都报告说，他们非常担心非线性公司因与我交谈而遭受的职业和个人报复，并向我讲述了故事并向我展示了一些文本，使我相信这是一个非常可信的担忧。</li></ul><h2>各种报告的经历</h2><p>这两位员工在非线性公司的经历中有很多部分让他们感到非常不愉快和受伤。我将在下面总结其中的一些内容。</p><p>我认为发生的许多事情都是警告信号，我也认为有一些红线，我将在这篇文章的底部讨论我的想法，即我的要点中哪些是红线。</p><p><strong>我对这些报告的信任程度</strong></p><p>大多数动态都被多个不同的人描述为准确的（低工资、没有法律结构、孤立、社会操纵的一些因素、恐吓），这让我对它们有很高的信心，非线性自己也证实了这些动态的各个部分。账户。</p><p>我会有意义地更新有关这类事情的人的言论，他们都保证克洛伊的话是可靠的。</p><p>非线性工作人员和少数在爱丽丝和克洛伊工作期间拜访的其他人强烈质疑爱丽丝的可信度，并暗示她完全撒了谎。 Nonlinear 向我展示了一些短信，其中与 Alice 交谈过的人留下的印象是她的工资是 0 美元或 500 美元，这是不准确的（正如她告诉我的那样，她在网上的工资约为 8000 美元）。</p><p>也就是说，我个人发现爱丽丝非常愿意并准备根据要求与我分享主要来源（短信、银行信息等），所以我不相信她有恶意。</p><p>在我与她的第一次谈话中，凯特声称爱丽丝有很多灾难性的沟通错误，但克洛伊（引用）“很好”。总的来说，没有人质疑克洛伊的话，而那些告诉我他们质疑爱丽丝的话的人大体上说他们相信克洛伊的话。</p><p>就我个人而言，我发现他们对报复的所有恐惧都是真实而认真的，并且在我看来是有道理的。</p><p><strong>为什么我要分享这些</strong></p><p>我确实有一个强烈的启发，即同意的成年人可以同意各种最终伤害他们的事情（即接受这些工作），即使我家长式地可能认为我可以阻止他们伤害自己。也就是说，我有明显的理由认为凯特、艾默生和德鲁恐吓这些人，让他们接受一些伤害他们的行为或动态，所以有些部分在我看来显然不是双方同意的。</p><p>除此之外，我认为其他人知道他们正在做什么是有好处的，所以我认为分享这些信息很好，因为它对于许多有可能使用非线性工作的人来说是相关的。对我来说最重要的是，我特别想这样做，因为在我看来，非线性已经试图阻止这种负面信息被共享，所以我在共享事物方面犯了强烈的错误。</p><p> （其中一名员工还想谈谈她为何为这篇文章做出贡献，我已将其放在脚注中。 <span class="footnote-reference" role="doc-noteref" id="fnref2ad0whew08p"><sup><a href="#fn2ad0whew08p">[4]</a></sup></span> ）</p><p><strong>财务和社会环境高度依赖</strong></p><p>每个人都住在同一所房子里。艾默生和凯特会共用一个房间，其他人会凑合着用其他可用的东西，通常共用卧室。</p><p> Nonlinear 主要在他们通常不认识当地人的国家/地区流动，员工通常除了联合创始人之外没有人可以互动，员工报告说，他们被拒绝住在与联合创始人不同的爱彼迎上。</p><p>爱丽丝和克洛伊报告说，他们被建议不要花时间与“低价值的人”在一起，包括他们的家人、浪漫伴侣以及他们居住地的任何当地人，但非线性邀请的客人/访客除外。爱丽丝和克洛伊报告说，这使他们在社交上非常依赖凯特/艾默生/德鲁，并且在其他方​​面非常孤立。</p><p>员工们对于非线性公司会支付什么费用和不会支付什么费用的界限非常不清楚。例如，爱丽丝和克洛伊报告说，他们曾经花了几天的时间开车在波多黎各周围寻找更便宜的医疗服务，然后将其提交给高级工作人员，因为他们不知道医疗费用是否会被承保，所以他们想确保尽可能便宜地增加高级员工同意的机会。</p><p>财务状况复杂、混乱。这在很大程度上是因为他们很少进行会计处理。总而言之，Alice 在过去 2 个月的大部分时间里，银行账户里的存款都不足 1000 欧元，有时还需要打电话给艾默生进行立即转账，以便能够支付她看医生时的医疗费用。当她辞职时，她的账户里有 700 欧元，这不足以支付她月底的账单，这让她非常害怕。不过需要明确的是，非线性公司在一周内就向她偿还了约 2900 欧元的拖欠工资，部分原因是她强烈要求。 （这里的相关问题是极高的财务依赖性和贫富差距，但爱丽丝并没有声称非线性公司未能支付他们的费用。）</p><p> Alice 表示，她坚持这么久的主要原因之一是，她希望通过启动自己的孵化项目实现财务独立，该项目分配了 10 万美元（由 FTX 筹集资金）。在她在那里的最后一个月，凯特告诉她，虽然她会完全独立工作，但他们会将钱存入非线性银行账户，她会索要这笔钱，这意味着她不会像她所期望的那样从他们那里获得财务独立，得知这一点是爱丽丝放弃的原因。</p><p>其中一名员工采访了凯特，询问她的生产力建议，并与我分享了这次采访的笔记。该员工写道：</p><blockquote><p>在采访中，凯特公开承认自己效率不高，但她表示，她看起来仍然很有效率，因为她让别人为她工作。她依靠愿意为她做免费工作的志愿者，这是她提高生产力的首要建议。</p></blockquote><p>员工们报告说，一些实习生后来对无薪工作提出了强烈的负面反馈，因此凯特决定不再雇用实习生。</p><p><strong>如果工作关系不顺利，可能会面临严重的不利影响</strong></p><p>在艾默生·斯帕茨 (Emerson Spartz) 与其中一名员工的对话中，该员工向一位想在受雇期间寻找另一份工作的朋友寻求建议，但尚未让现任雇主知道他们离职的决定。据报道，艾默生立即表示，他现在必须更新，考虑到该员工本人正在考虑离开非线性公司。他接着告诉她，他对那些离开公司去从事同样好或不太好的工作的员工感到生气；他说，他理解员工是否为了明显更好的机会而离开。员工报告说，这导致他们非常害怕离开工作，这既是因为艾默生更新了员工现在试图离职的想法，也因为艾默生对因“原因”而离职的员工进行报复。不好的理由”。</p><p>有关艾默生经营理念的背景信息：爱丽丝引用艾默生建议的以下工作进度指标：“您能够在短时间内从他人那里获取多少价值？” <span class="footnote-reference" role="doc-noteref" id="fnreft84or63yyei"><sup><a href="#fnt84or63yyei">[5]</a></sup></span>另一位来访者向我描述艾默生“总是试图利用他所有的讨价还价能力”。克洛伊告诉我，当她代表非线性公司与外部合作伙伴谈判薪资时，艾默生建议她在谈判薪资时，提供“你能接受的最低数字”。</p><p>许多不同的人报告说，艾默生·斯帕茨会向员工和访客吹嘘他的商业谈判策略。他会鼓励员工阅读许多有关战略和影响力的书籍。当他们读<a href="https://www.amazon.com/48-Laws-Power-Robert-Greene/dp/0140280197"><u>《权力的48条法则》</u></a>一书时，他会举出他在过去的商业实践中遵循“法则”的例子。</p><p>他向员工和访客讲述的一个故事是关于他在与他的前青少年学员阿多里安·戴克 (Adorian Deck) 发生冲突时所采取的恐吓策略。</p><p> （有关冲突的背景，请参阅当时撰写的相关文章的链接： <a href="https://www.hollywoodreporter.com/business/business-news/teen-who-created-omgfacts-twitter-182620/"><u>Hollywood Reporter</u></a> 、 <a href="https://www.jacksonville.com/story/business/2011/05/16/california-teens-lawsuit-over-twitter-feed-underscores-power-tweet/15903568007/"><u>Jacksonville</u></a> 、 <a href="https://blog.ericgoldman.org/archives/2011/05/thoughts_on_the.htm"><u>Technology &amp; Marketing Law Blog</u></a>和<a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>Emerson Spartz&#39;s Tumblr</u></a> 。另外，这里是他们签署的<a href="https://web.archive.org/web/20110824205315/http://www.omgfactslawsuit.com.s3.amazonaws.com/CONTRACT.pdf"><u>法律合同</u></a>，Deck 后来起诉撤销。）</p><p>简而言之，Adorian Deck 是一位 16 岁的年轻人（2009 年）创建了一个名为“OMGFacts”的 Twitter 帐户，该帐户很快就拥有超过 300,000 名粉丝。艾默生主动寻求在该品牌下建立公司，并同意与 Adorian 达成协议。不到一年后，阿多里安想要退出这笔交易，声称艾默生赚取了超过 10 万美元的利润，而他只看到了 100 美元，并提起诉讼以终止交易。</p><p>根据艾默生的说法，事实证明，加州有一个独特的条款（由于洛杉矶的演艺职业），即使未成年人与其父母签署了合同，除非在法官的监督下签署，否则合同无效，因此他们能够简单地退出交易。</p><p>但时至今日，艾默生公司仍然拥有 OMGfacts 品牌和公司以及 Youtube 频道。</p><p> （旁注：我并不是想断言在这些冲突中谁是“正确的”，我将这些作为埃默罗斯恩谈判策略的例子进行报道，据报道他在冲突期间参与并积极支持这些策略。）</p><p>艾默生向与我交谈过的不同人讲述了这个故事的不同版本（人们认为他在“吹牛”）。</p><p>在一个版本中，他声称他用无休无止的法律威胁对阿多里安和他的母亲进行了武装，他们做出了让步，让他完全控制了该品牌。与我交谈的这个人不记得细节，但他说艾默生试图吓唬德克和他的母亲，而他们（艾默生吹嘘的那个人）觉得这“令人恐惧”，并认为这种行为是“类似于 7 标准的行为”偏离了该领域的通常规范。”</p><p>另一个人在《权力48条法则》第二条法则的背景下讲述了这个故事，即“永远不要过于信任朋友，学会如何利用敌人”。摘要包括</p><blockquote><p>“要警惕朋友——他们会更快地背叛你，因为他们很容易被嫉妒。他们也会变得被宠坏和暴虐……你更害怕朋友而不是敌人。”</p></blockquote><p>对于这个听过阿多里安故事的人来说，当他讲述这个故事时，最能引起共鸣的是他声称自己与阿多里安有着密切的指导关系，并且对他非常了解，以至于他知道“到底该去哪里”伤害他最深”，这样他就会退缩。在那个版本的故事中，他说 Deck 的人生目标是成为一名 YouTuber（直到今天这确实是 Deck 的职业——<a href="https://www.youtube.com/@AdorianDeck"><u>他每月制作大约 4 个视频</u></a>），而艾默生策略性地联系了 Deck 最欣赏的 YouTuber，并向他们讲述了德克偷懒并试图将艾默生的所有工作归功于自己的故事。据报道，他威胁要采取更多行动，直到德克做出让步，这就是德克放弃诉讼的原因。那个人对我说：“他爱他，非常了解他，并用这些知识摧毁了他。” <span class="footnote-reference" role="doc-noteref" id="fnrefzcp7frumok"><sup><a href="#fnzcp7frumok">[6]</a></sup></span></p><p>后来我和艾默生谈过这个问题。他确实说过，他正在与顶级 YouTube 用户合作制作揭露 Deck 的视频，这就是 Deck 重新回到谈判桌的原因。他说，他最终重新谈判了一份合同，Deck 每月获得 1 万美元，为期 7 年。如果属实，我认为这最终协议对艾默生产生了积极的影响，尽管我仍然相信与他交谈的人对与艾默生在这个问题上的对话感到非常害怕。 （我既没有证实这份合同的存在，也没有听过德克的说法。）</p><p>据报道，他讲述了另一个关于他在商业交易中被骗的反应的谈判故事。我不会详细说明，但据报道，他为徽标/商标的权利支付了高价，却发现他没有阅读细则，并且被出售了价值低得多的东西。他举了《权力 48 条法则》中“让他人处于悬置的恐惧之中：培养不可预测的气氛”策略的例子：</p><blockquote><p>故意不可预测。看似没有一致性或目的的行为会让他们失去平衡，并且他们会在试图解释你的举动时精疲力竭。极端地说，这种策略可能会造成恐吓和恐吓。</p></blockquote><p>据报道，在那次商业谈判中，他表现得精神错乱。据与我交谈的人透露，他说他会打电话给对方，说“疯狂的事情”并对他们大喊大叫，目的是让他们认为他有能力做任何事情，包括危险和不道德的事情，最终他们心软了并给了他他想要的交易。</p><p>据我采访的其他人报道，他多次表示，他会对与他发生冲突的人“非常敌对”。据报道，他举了一个例子，如果有人试图起诉他，他愿意进入法律灰色地带，以“粉碎他的敌人”（他显然经常使用这个词），包括雇人跟踪这个人并他们的家人为了吓唬他们。 （艾默生否认曾说过这一点，并表示他可能将此描述为其他<i>人</i>可能在人们应该意识到的冲突中使用的一种策略。）</p><p>克洛伊最终退出后，爱丽丝报告说凯特/艾默生会“垃圾话”她，说她从来都不是一个“A级玩家”，在很多方面（能力、道德、戏剧等）批评她，尽管之前主要是给克洛伊高度赞扬。据报道，这种情况通常发生在那些结束或拒绝与非线性公司合作的人身上。</p><p> Here are some texts between Kat Woods and Alice shortly after Alice had quit, before the final salary had been paid. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/fuglgbnch2qgely8aahb"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/ezzjqbuxcxaqfpte46ee"></p><p> A few months later, some more texts from Kat Woods. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/cd087amewavbd71uq75z"></p><p> (I can corroborate that it was difficult to directly talk with the former employee and it took a fair bit of communication through indirect social channels before they were willing to identify themselves to me and talk about the details.)</p><p> <strong>Effusive positive emotion not backed up by reality, and other manipulative techniques</strong></p><p> Multiple people who worked with Kat reported that Kat had a pattern of enforcing arbitrary short deadlines on people in order to get them to make the decision she wants eg <i>“I need a decision by the end of this call”</i> , or (in an email to Alice) “ <i>This is urgent and important. There are people working on saving the world and we can&#39;t let our issues hold them back from doing their work.”</i></p><p> Alice reported feeling emotionally manipulated. She said she got constant compliments from the founders that ended up seeming fake.</p><p> Alice wrote down a string of the compliments at the time from Kat Woods (said out loud and that Alice wrote down in text), here is a sampling of them that she shared with me:</p><blockquote><p> “You&#39;re the kind of person I bet on, you&#39;re a beast, you&#39;re an animal, I think you are extraordinary&quot;</p><p> &quot;You can be in the top 10, you really just have to think about where you want to be, you have to make sacrifices to be on the top, you can be the best, only if you sacrifice enough&quot;</p><p> &quot;You&#39;re working more than 99% because you care more than 99% because you&#39;re a leader and going to save the world&quot;</p><p> &quot;You can&#39;t fail if you commit to [this project], you have what it takes, you get sh*t done and everyone will hail you in EA, finally an executor among us.&quot;</p></blockquote><p> Alice reported that she would get these compliments near-daily. She eventually had the sense that this was said in order to get something out of her. She reported that one time, after a series of such compliments, the Kat Woods then turned and recorded a near-identical series of compliments into their phone for a different person.</p><p> Kat Woods reportedly several times cried while telling Alice that she wanted the employee in their life forever and was worried that this employee would ever not be in Kat&#39;s life.</p><p> Other times when Alice would come to Kat with money troubles and asking for a pay rise, Alice reports that Kat would tell them that this was a psychological issue and that actually they had safety, for instance they could move back in with their parents, so they didn&#39;t need to worry.</p><p> Alice also reports that she was explicitly advised by Kat Woods to cry and look cute when asking Emerson Spartz for a salary improvement, in order to get the salary improvement that she wanted, and was told this was a reliable way to get things from Emerson. (Alice reports that she did not follow this advice.)</p><p> <strong>Many other strong personal costs</strong></p><p> Alice quit being vegan while working there. She was sick with covid in a foreign country, with only the three Nonlinear cofounders around, but nobody in the house was willing to go out and get her vegan food, so she barely ate for 2 days. Alice eventually gave in and ate non-vegan food in the house. She also said that the Nonlinear cofounders marked her quitting veganism as a &#39;win&#39;, as they thad been arguing that she should not be vegan.</p><p> (Nonlinear disputes this, and says that they did go out and buy her some vegan burgers food and had some vegan food in the house. They agree that she quit being vegan at this time, and say it was because being vegan was unusually hard due to being in Puerto Rico. Alice disputes that she received any vegan burgers.)</p><p> Alice said that this generally matched how she and Chloe were treated in the house, as people generally not worth spending time on, because they were &#39;low value&#39; (ie in terms of their hourly wage), and that they were the people who had to do chores around the house (eg Alice was still asked to do house chores during the period where she was sick and not eating).</p><p> By the same reasoning, the employees reported that they were given 100% of the menial tasks around the house (cleaning, tidying, etc) due to their lower value of time to the company. For instance, if a cofounder spilled food in the kitchen, the employees would clean it up. This was generally reported as feeling very demeaning.</p><p> Alice and Chloe reported a substantial conflict within the household between Kat and Alice. Alice was polyamorous, and she and Drew entered into a casual romantic relationship. Kat previously had a polyamorous marriage that ended in divorce, and is now monogamously partnered with Emerson. Kat reportedly told Alice that she didn&#39;t mind polyamory &quot;on the other side of the world”, but couldn&#39;t stand it right next to her, and probably either Alice would need to become monogamous or Alice should leave the organization. Alice didn&#39;t become monogamous. Alice reports that Kat became increasingly cold over multiple months, and was very hard to work with. <span class="footnote-reference" role="doc-noteref" id="fnrefuvloifuhbna"><sup><a href="#fnuvloifuhbna">[7]</a></sup></span></p><p> Alice reports then taking a vacation to visit her family, and trying to figure out how to repair the relationship with Kat. Before she went on vacation, Kat requested that Alice bring a variety of illegal drugs across the border for her (some recreational, some for productivity). Alice argued that this would be dangerous for her personally, but Emerson and Kat reportedly argued that it is not dangerous at all and was “absolutely risk-free”. Privately, Drew said that Kat would “love her forever” if she did this. I bring this up as an example of the sorts of requests that Kat/Emerson/Drew felt comfortable making during Alice&#39;s time there.</p><p> Chloe was hired by Nonlinear with the intent to have them do executive assistant tasks for Nonlinear ( <a href="https://web.archive.org/web/20211022160447/https://www.nonlinear.org/operations.html"><u>this is the job ad they responded to</u></a> ). After being hired and flying out, Chloe was informed that on a daily basis their job would involve driving eg to get groceries when they were in different countries. She explained that she didn&#39;t have a drivers&#39; license and didn&#39;t know how to drive. Kat/Emerson proposed that Chloe learn to drive, and Drew gave her some driving lessons. When Chloe learned to drive well enough in parking lots, she said she was ready to get her license, but she discovered that she couldn&#39;t get a license in a foreign country. Kat/Emerson/Drew reportedly didn&#39;t seem to think that mattered or was even part of the plan, and strongly encouraged Chloe to just drive without a license to do their work, so she drove ~daily for 1-2 months without a license. (I think this involved physical risks for the employee and bystanders, and also substantial risks of being in jail in a foreign country. Also, Chloe basically never drove Emerson/Drew/Kat, this was primarily solo driving for daily errands.) Eventually Chloe had a minor collision with a street post, and was a bit freaked out because she had no idea what the correct protocols were. She reported that Kat/Emerson/Drew didn&#39;t think that this was a big deal, but that Alice (who she was on her way to meet) could clearly see that Chloe was distressed by this, and Alice drove her home, and Chloe then decided to stop driving.</p><p> (Car accidents are the <a href="https://www.cdc.gov/injury/wisqars/pdf/leading_causes_of_injury_deaths_highlighting_unintentional_2018-508.pdf"><u>second most common cause of death</u></a> for people in their age group. Insofar as they were pressured to do this and told that this was safe, I think this involved a pretty cavalier disregard for the safety of the person who worked for them.)</p><p> Chloe talked to a friend of hers (who is someone I know fairly well, and was the first person to give me a negative report about Nonlinear), reporting that they were very depressed. When Chloe described her working conditions, her friend was horrified, and said she had to get out immediately since, in their words, “this was clearly an abusive situation”. The friend offered to pay for flights out of the country, and tried to convince her to quit immediately. Eventually Chloe made a commitment to book a flight by a certain date and then followed through with that.</p><p> <strong>Lax on legalities and adversarial business practices</strong></p><p> I did not find the time to write much here. For now I&#39;ll simply pass on my impressions.</p><p> I generally got a sense from speaking with many parties that Emerson Spartz and Kat Woods respectively have very adversarial and very lax attitudes toward legalities and bureaucracies, with the former trying to do as little as possible that is asked of him. If I asked them to fill out paperwork I would expect it was filled out at least reluctantly and plausibly deceptively or adversarially in some way. In my current epistemic state, I would be actively concerned about any project in the EA or x-risk ecosystems that relied on Nonlinear doing any accounting or having a reliable legal structure that has had the basics checked.</p><p> Personally, if I were giving Nonlinear funds for any project whatsoever, including for regranting, I&#39;d expect it&#39;s quite plausible (>;20%) that they didn&#39;t spend the funds on what they told me, and instead will randomly spend it on some other project. If I had previously funded Nonlinear for any projects, I would be keen to ask Nonlinear for receipts to show whether they spent their funds in accordance with what they said they would.</p><p> <strong>This is not a complete list</strong></p><p> I want to be clear that this is not a <i>complete</i> list of negative or concerning experiences, this is an <i>illustrative</i> list. There are many other things that I was told about that I am not including here due to factors like length and people&#39;s privacy (on all sides). Also I split them up into the categories as I see them; someone else might make a different split.</p><p> <strong>Perspectives From Others Who Have Worked or Otherwise Been Close With Nonlinear</strong></p><p> I had hoped to work this into a longer section of quotes, but it seemed like too much back-and-forth with lots of different people. I encourage folks to leave comments with their relevant impressions.</p><p> For now I&#39;ll summarize some of what I learned as follows:</p><ul><li> Several people gave reports consistent with Alice and Chloe being very upset and distressed both during and after their time at Nonlinear, and reaching out for help, and seeming really strongly to want to get away from Nonlinear.</li><li> Some unpaid interns (who worked remotely for Nonlinear for 1-3 months) said that they regretted not getting paid, and that when they brought it up with Kat Woods she said some positive sounding things and they expected she would get back to them about it, but that never happened during the rest of their internships.</li><li> Many people who visited had fine experiences with Nonlinear, others felt much more troubled by the experience.</li><li> One person said to me about Emerson/Drew/Kat:<ul><li> <i>&quot;My subjective feeling is like &#39;they seemed to be really bad and toxic people&#39;. And they at the same time have a decent amount of impact. After I interacted repeatedly with them I was highly confused about the dilemma of people who are mistreating other people, but are doing some good.&quot;</i></li></ul></li><li> Another person said about Emerson:<ul><li> <i>“He seems to think he&#39;s extremely competent, a genius, and that everyone else is inferior to him. They should learn everything they can from him, he has nothing to learn from them. He said things close to this explicitly. Drew and (to a lesser extent) Kat really bought into him being the new messiah.”</i></li></ul></li><li> One person who has worked for Kat Woods (not Alice or Chloe) said the following:<ul><li> <i>I love her as a person, hate her as a boss. She&#39;s fun, has a lot of ideas, really good socialite, and I think that that speaks to how she&#39;s able to get away with a lot of things. Able to wear different masks in different places. She&#39;s someone who&#39;s easy to trust, easy to build social relationships with. I&#39;d be suspicious of anyone who gives a reference who&#39;s never been below Kat in power.</i></li><li> <i>Ben: Do you think Kat is emotionally manipulative?</i></li><li> <i>I think she is. I think it&#39;s a fine line about what makes an excellent entrepreneur. Do whatever it takes to get a deal signed. To get it across the line. Depends a lot on what the power dynamics are, whether it&#39;s a problem or not. If people are in equal power structures it&#39;s less of a problem.</i></li></ul></li></ul><p> There were other informative conversations that I won&#39;t summarize. I encourage folks who have worked with or for Nonlinear to comment with their perspective.</p><h2> Conversation with Nonlinear</h2><p> After putting the above together, I got permission from Alice and Chloe to publish, and to share the information I had learned as I saw fit. So I booked a call with Nonlinear, sent them a long list of concerns, and talked with Emerson, Kat and Drew for ~3 hours to hear them out.</p><p> <strong>Paraphrasing Nonlinear</strong></p><p> On the call, they said their primary intention in the call was to convince me that Alice is a bald-faced liar. They further said they&#39;re terrified of Alice making false claims about them, and that she is in a powerful position to hurt them with false accusations.</p><p> Afterwards, I wrote up a paraphrase of their responses. I shared it with Emerson and he replied that it was a “Good summary!”. Below is the paraphrase of their perspective on things that I sent them, with one minor edit for privacy. (The below is written as though Nonlinear is speaking, but to be clear this 100% my writing.)</p><ul><li> We hired one person, and kind-of-technically-hired a second person. In doing so, our intention wasn&#39;t just to have employees, but also to have members of our family unit who we traveled with and worked closely together with in having a strong positive impact in the world, and were very personally close with.</li><li> We nomadically traveled the globe. This can be quite lonely so we put a lot of work into bringing people to us, often having visitors in our house who we supported with flights and accommodation. This probably wasn&#39;t perfect but in general we&#39;d describe the environment as &quot;quite actively social&quot;.</li><li> For the formal employee, she responded to a job ad, we interviewed her, and it all went the standard way. For the gradually-employed employee, we initially just invited her to travel with us and co-work, as she seemed like a successful entrepreneur and aligned in terms of our visions for improving the world. Over time she quit her existing job and we worked on projects together and were gradually bringing her into our organization.</li><li> We wanted to give these employees a pretty standard amount of compensation, but also mostly not worry about negotiating minor financial details as we traveled the world. So we covered basic rent/groceries/travel for these people. On top of that, to the formal employee we gave a $1k/month salary, and to the semi-formal employee we eventually did the same too. For the latter employee, we roughly paid her ~$8k over the time she worked with us.</li><li> From our perspective, the gradually-hired employees gave a falsely positive impression of their financial and professional situation, suggesting they&#39;d accomplished more than they had and were earning more than they had. They ended up being fairly financially dependent on us and we didn&#39;t expect that.</li><li> Eventually, after about 6-8 months each, both employees quit. Overall this experiment went poorly from our perspective and we&#39;re not going to try it in future.</li><li> For the formal employee, we&#39;re a bit unsure about why exactly she quit, even though we did do exit interviews with her. She said she didn&#39;t like a lot of the menial work (which is what we hired her for), but didn&#39;t say that money was the problem. We think it is probably related to everyone getting Covid and being kind of depressed around that time.</li><li> For the other employee, relations got bad for various reasons. She ended up wanting total control of the org she was incubating with us, rather than 95% control as we&#39;d discussed, but that wasn&#39;t on the table (the org had $250k dedicated to it that we&#39;d raised!), and so she quit.</li><li> When she was leaving, we were financially supportive. On the day we flew back from the Bahamas to London, we paid all our outstanding reimbursements (~$2900). We also offered to pay for her to have a room in London for a week as she got herself sorted out. We also offered her rooms with our friends if she promised not to tell them lies about us behind our backs.</li><li> After she left, we believe she told a lot of lies and inaccurate stories about us. For instance, two people we talked to had the impression that we either paid her $0 or $500, which is demonstrably false. Right now we&#39;re pretty actively concerned that she is telling lots of false stories in order to paint us in a negative light, because the relationship didn&#39;t work out and she didn&#39;t get control over her org (and because her general character seems drama-prone).</li></ul><p> There were some points around the experiences of these employees that we want to respond to.</p><ul><li> First; the formal employee drove without a license for 1-2 months in Puerto Rico. We taught her to drive, which she was excited about. You might think this is a substantial legal risk, but basically it isn&#39;t, as you can see <a href="https://casetext.com/statute/laws-of-puerto-rico/title-nine-highways-and-traffic/chapter-27-vehicle-and-traffic-law-2000/subchapter-ii-requirements-and-procedure-for-issuance-expiration-and-renewal-of-drivers-licenses/5073-illegal-use-of-the-driving-license-and-penalties"><u>here</u></a> , the general range of fines for issues around not-having-a-license in Puerto Rico is in the range of $25 to $500, which just isn&#39;t that bad.</li><li> Second; the semi-employee said that she wasn&#39;t supported in getting vegan food when she was sick with Covid, and this is why she stopped being vegan. This seems also straightforwardly inaccurate, we brought her potatoes, vegan burgers, and had vegan food in the house. We had been advising her to 80/20 being a vegan and this probably also weighed on her decision.</li><li> Third; the semi-employee was also asked to bring some productivity-related and recreational drugs over the border for us. In general we didn&#39;t push hard on this. For one, this is an activity she already did (with other drugs). For two, we thought it didn&#39;t need prescription in the country she was visiting, and when we found out otherwise, we dropped it. And for three, she used a bunch of our drugs herself, so it&#39;s not fair to say that this request was made entirely selfishly. I think this just seems like an extension of the sorts of actions she&#39;s generally open to.</li></ul><p> Finally, multiple people (beyond our two in-person employees) told Ben they felt frightened or freaked out by some of the business tactics in the stories Emerson told them. To give context and respond to that:</p><ul><li> I, Emerson, have had a lot of exceedingly harsh and cruel business experience, including getting tricked or stabbed-in-the-back. Nonetheless, I have often prevailed in these difficult situations, and learned a lot of hard lessons about how to act in the world.</li><li> The skills required to do so seem to me lacking in many of the earnest-but-naive EAs that I meet, and I would really like them to learn how to be strong in this way. As such, I often tell EAs these stories, selecting for the most cut-throat ones, and sometimes I try to play up the harshness of how you have to respond to the threats. I think of myself as playing the role of a wise old mentor who has had lots of experience, telling stories to the young adventurers, trying to toughen them up, somewhat similar to how Prof Quirrell <span class="footnote-reference" role="doc-noteref" id="fnref03ez0hl92kmc"><sup><a href="#fn03ez0hl92kmc">[8]</a></sup></span> toughens up the students in HPMOR through teaching them Defense Against the Dark Arts, to deal with real monsters in the world.</li><li> For instance, I tell people about my negotiations with Adorian Deck about the OMGFacts brand and Twitter account. We signed a good deal, but a California technicality meant he could pull from it and take my whole company, which is a really illegitimate claim. They wouldn&#39;t talk with me, so I was working with top YouTubers to make some videos publicizing and exposing his bad behavior. This got him back to the negotiation table and we worked out a deal where he got $10k/month for seven years, which is not a shabby deal, and meant that I got to keep my company!</li><li> It had been reported to Ben that Emerson said he would be willing to go into legal gray areas in order to &quot;crush his enemies&quot; (if they were acting in very reprehensible and norm-violating ways). Emerson thinks this has got to be a misunderstanding, that he was talking about what other people might do to you, which is a crucial thing to discuss and model.</li></ul><p> (Here I cease pretending-to-be-Nonlinear and return to my own voice.)</p><h2> My thoughts on the ethics and my takeaways</h2><p> <strong>Summary of My Epistemic State</strong></p><p> Here are my probabilities for a few high-level claims relating to Alice and Chloe&#39;s experiences working at Nonlinear.</p><ul><li> Emerson Spartz employs more vicious and adversarial tactics in conflicts than 99% of the people active in the EA/x-risk/AI Safety communities: 95%</li><li> Alice and Chloe were more dependent on their bosses (combining financial, social, and legally) than employees are at literally every other organization I am aware of in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefpwa0kj9axw"><sup><a href="#fnpwa0kj9axw">[9]</a></sup></span></li><li> In working at Nonlinear Alice and Chloe were both took on physical and legal risks that they strongly regretted, were hurt emotionally, came away financially worse off, gained ~no professional advancement from their time at Nonlinear, and took several months after the experience to recover: 90%</li><li> Alice and Chloe both had credible reason to be very scared of retaliation for sharing negative information about their work experiences, far beyond that experienced at any other org in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefyl41dasvaar"><sup><a href="#fnyl41dasvaar">[10]</a></sup></span></li></ul><p> <strong>General Comments From Me</strong></p><p> Going forward I think anyone who works with Kat Woods, Emerson Spartz, or Drew Spartz, should sign legal employment contracts, and make sure all financial agreements are written down in emails and messages that the employee has possession of. I think all people considering employment by the above people at any non-profits they run should take salaries where money is wired to their bank accounts, and not do unpaid work or work that is compensated by ways that don&#39;t primarily include a salary being wired to their bank accounts.</p><p> I expect that if Nonlinear does more hiring in the EA ecosystem it is more-likely-than-not to chew up and spit out other bright-eyed young EAs who want to do good in the world. I relatedly think that the EA ecosystem doesn&#39;t have reliable defenses against such predators. These are not the first, nor sadly the last, bright-eyed well-intentioned people who I expect to be taken advantage of and hurt in the EA/x-risk/AI safety ecosystem, as a result of falsely trusting high-status people at EA events to be people who will treat them honorably.</p><p> (Personal aside: Regarding the texts from Kat Woods shown above — I have to say, if you want to be allies with me, you must not write texts like these. A lot of bad behavior can be learned from, fixed, and forgiven, but if you take actions to prevent me from being able to learn that the bad behavior is even going on, then I have to always be worried that something far worse is happening that I&#39;m not aware of, and indeed I have been <i>quite</i> shocked to discover how bad people&#39;s experiences were working for Nonlinear.)</p><p> My position is not greatly changed by the fact that Nonlinear is overwhelmingly confident that Alice is a “bald-faced liar”. From my current perspective, they probably have some legitimate grievances against her, but that in no way makes it less costly to our collective epistemology to incentivize her to not share her own substantial grievances. I think the magnitude of the costs they imposed on their employees-slash-new-family are far higher than I or anyone I know would have expected was happening, and they intimidated both Alice and Chloe into silence about those costs. If it were only Alice then I would give this perspective a lot more thought/weight, but Chloe reports a lot of the same dynamics and similar harms.</p><p> To my eyes, the people involved were genuinely concerned about retaliation for saying anything negative about Nonlinear, including the workplace/household dynamics and how painful their experiences had been for them. That&#39;s a red line in my book, and I will not personally work with Nonlinear in the future because of it, and I recommend their exclusion from any professional communities that wish to keep up the standard of people not being silenced about extremely negative work experiences. “ <a href="https://twitter.com/HiFromMichaelV/status/1161174071469641728"><u>First they came for the epistemology. We don&#39;t know what happened after that.</u></a> ”</p><p> Specifically, the things that cross my personal lines for working with someone or viewing them as an ally:</p><ul><li> Kat Woods attempted to offer someone who was really hurting, and in a position of strong need, very basic resources with the requirement of not saying bad things about her.</li><li> Kat Woods&#39; texts that read to me as a veiled threat to destroy someone&#39;s career for sharing negative information about her.</li><li> Emerson Spartz reportedly telling multiple people he will use questionably legal methods in order to crush his enemies (such as spurious lawsuits and that he would hire a stalker to freak someone out).</li><li> Both employees were actively afraid that Emerson Spartz would retaliate and potentially using tactics like spurious lawsuits and further things that are questionably legal, and generally try to destroy their careers and leave them with no resources. It seems to me (given the other reports I&#39;ve heard from visitors) that Emerson behaved in a way that quite understandably led them to this epistemic state, and I consider that to be his responsibility to not give his employees this impression.</li></ul><p> I think in almost any functioning professional ecosystem, there should be some general principles like:</p><ul><li> If you employ someone, after they work for you, unless they&#39;ve done something egregiously wrong or unethical, they should be comfortable continuing to work and participate in this professional ecosystem.</li><li> If you employ someone, after they work for you, they should feel comfortable talking openly about their experience working with you to others in this professional ecosystem.</li></ul><p> Any breaking of the first rule is very costly, and any breaking of the second rule is by-default a red-line for me not being willing to work with you.</p><p> I do think that there was a nearby world where Alice, having run out of money, gave in and stayed at Nonlinear, begging them for money, and becoming a fully dependent and subservient house pet — a world where we would not have learned the majority of this information. I think we&#39;re not that far from that world, I think a weaker person than Alice might have never quit, and it showed a lot of strength to quit at the point where you have ~no runway left and you have heard the above stories about the kinds of things Emerson Spartz considers doing to former business partners that he is angry with.</p><p> I&#39;m very grateful to the two staff members involved for coming forward and eventually spending dozens of hours clarifying and explaining their experiences to me and others who were interested. To compensate them for their courage, the time and effort spent to talk with me and explain their experiences at some length, and their permission to allow me to publish a lot of this information, I (using Lightcone funds) am going to pay them each $5,000 after publishing this post.</p><p> I think that whistleblowing is generally a difficult experience, with a lot riding on the fairly personal account from fallible human beings. It&#39;s neither the case that everything reported should be accepted without question, nor that if some aspect is learned to be exaggerated or misreported that the whole case should be thrown out. I plan to reply to further questions here in the comments, I also encourage everyone involved to comment insofar as they wish to answer questions or give their own perspective on what happened. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8a69v0tq2qo"> <span class="footnote-back-link"><sup><strong><a href="#fnref8a69v0tq2qo">^</a></strong></sup></span><div class="footnote-content"><p> In a later conversation, Kat clarified that the actual amount discussed was $70k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fno53culramn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo53culramn">^</a></strong></sup></span><div class="footnote-content"><p> Comment from Chloe:</p><blockquote><p> In my resignation conversation with Kat, I was worried about getting into a negotiation conversation where I wouldn&#39;t have strong enough reasons to leave. To avoid this, I started off by saying that my decision to quit is final, and not an ultimatum that warrants negotiation of what would make me want to stay. I did offer to elaborate on the reasons for why I was leaving. As I was explaining my reasons, she still insisted on offering me solutions to things I would say I wanted, to see if that would make me change my mind anyway.  One of the reasons I listed was the lack of financial freedom in not having my salary be paid out as a salary which I could allocate towards decisions like choices in accommodation for myself, as well as meals and travel decisions. She wanted to know how much I wanted to be paid. I kept evading the question since it seemed to tackle the wrong part of the problem. Eventually I quoted back the number I had heard her reference to when she&#39;d talk about what my salary is equivalent to, suggesting that if they&#39;d pay out the 75k as a salary instead of the compensation package, then that would in theory solve the salary issue. There was a miscommunication around her believing that I wanted that to be paid out on top of the living expenses - I wanted financial freedom and a legal salary. I believe the miscommunication stems from me mentioning that salaries are more expensive for employers to pay out as they also have to pay tax on the salaries, eg social benefits, pension (depending on the country). Kat was surprised to hear that and understood it as me wanting a 75k salary <i>before taxes</i> . I do not remember that conversation concluding with her thinking I wanted everything paid for <i>and also</i> 75k.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnylc0clu1ffa"> <span class="footnote-back-link"><sup><strong><a href="#fnrefylc0clu1ffa">^</a></strong></sup></span><div class="footnote-content"><p> Note that Nonlinear and Alice gave conflicting reports about which month she started getting paid, February vs April. It was hard for me to check as it&#39;s not legally recorded and there&#39;s lots of bits of monetary payments unclearly coded between them.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2ad0whew08p"> <span class="footnote-back-link"><sup><strong><a href="#fnref2ad0whew08p">^</a></strong></sup></span><div class="footnote-content"><p> Comment from one of the employees:</p><blockquote><p> I had largely moved on from the subject and left the past behind when Ben started researching it to write a piece with his thoughts on it. I was very reluctant at first (and frightened at the mere thought), and frankly, will probably continue to be. I did not agree to post this publicly with any kind of malice, rest assured. The guiding thought here is, as Ben asked, &quot;What would you tell your friend if they wanted to start working for this organization?&quot; I would want my friend to be able to make their own independent decision, having read about my experience and the experiences of others who have worked there. My main goal is to create a world where we can all work together towards a safe, long and prosperous future, and anything that takes away from that (like conflict and drama) is bad and I have generally avoided it. Even when I was working at Nonlinear, I remember saying several times that I just wanted to work on what was important and didn&#39;t want to get involved in their interpersonal drama. But it&#39;s hard for me to imagine a future where situations like that are just overlooked and other people get hurt when it could have been stopped or flagged before. I want to live in a world where everyone is safe and cared for. For most of my life I have avoided learning about anything to do with manipulation, power frameworks and even personality disorders. By avoiding them, I also missed the opportunity to protect myself and others from dangerous situations. Knowledge is the best defense against any kind of manipulation or abuse, so I strongly recommend informing yourself about it, and advising others to do so too.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnt84or63yyei"> <span class="footnote-back-link"><sup><strong><a href="#fnreft84or63yyei">^</a></strong></sup></span><div class="footnote-content"><p> This is something Alice showed me was written in her notes from the time.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzcp7frumok"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzcp7frumok">^</a></strong></sup></span><div class="footnote-content"><p> I do not mean to make a claim here about who was in the right in that conflict. And somewhat in Emerson&#39;s defense, I think some of people&#39;s most aggressive behavior comes out when they themselves have just been wronged — I expect this is more extreme behavior than he would typically respond with. Nonetheless, it seems to me that there was reportedly a close, mentoring relationship — Emerson&#39;s <a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>tumblr post</u></a> on the situation says “I loved Adorian Deck” in the opening paragraph — but that later Emerson reportedly became bitter and nasty in order to win the conflict, involving threatening to overwhelm someone with lawsuits and legal costs, and figure out the best way to use their formerly close relationship to hurt them emotionally, and reportedly gave this as an example of good business strategy. I think this sort of story somewhat justifiably left people working closely with Emerson very worried about the sort of retaliation he might carry out if they were ever in a conflict, or he were to ever view them as an &#39;enemy&#39;.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuvloifuhbna"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuvloifuhbna">^</a></strong></sup></span><div class="footnote-content"><p> After this, there were further reports of claims of Kat professing her romantic love for Alice, and also precisely opposite reports of Alice professing her romantic love for Kat. I am pretty confused about what happened.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn03ez0hl92kmc"> <span class="footnote-back-link"><sup><strong><a href="#fnref03ez0hl92kmc">^</a></strong></sup></span><div class="footnote-content"><p> Note that during our conversation, Emerson brought up HPMOR and the Quirrell similarity, not me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpwa0kj9axw"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpwa0kj9axw">^</a></strong></sup></span><div class="footnote-content"><p> With the exception of some FTX staff.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyl41dasvaar"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyl41dasvaar">^</a></strong></sup></span><div class="footnote-content"><p> One of the factors lowering my number here is that I&#39;m not quite sure what the dynamics are like at places like Anthropic and OpenAI — who have employees sign non-disparagement clauses, and are involved in geopolitics — or whether they would even be included. I also could imagine finding out that various senior people at CEA/EV are terrified of information coming out about them. Also note that I am not including Leverage Research in this assessment.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1<guid ispermalink="false"> Lc8r4tZ2L5txxokZ8</guid><dc:creator><![CDATA[Ben Pace]]></dc:creator><pubDate> Thu, 07 Sep 2023 06:51:11 GMT</pubDate> </item><item><title><![CDATA[Weekly Incidence vs Cumulative Infections]]></title><description><![CDATA[Published on September 7, 2023 2:30 AM GMT<br/><br/><p> <span>Imagine you have a goal of identifying a novel disease by the time some small fraction of the population has been infected.</span>然而，您可能用来检测异常情况的许多迹象，例如看医生或排入废水中，将取决于<i>当前</i>感染<span>的人数</span>。这些有何关系？</p><p>底线：如果我们将考虑范围限制在任何人注意到异常情况之前的时间，此时人们没有改变自己的行为来避免这种疾病，那么绝大多数人仍然容易受到影响，并且传播可能呈指数级增长，那么：</p><p>发病率=累计感染ln(2)倍增时间</p><p>我们来推导一下这个吧！我们将“累积感染”称为 c(t)，将“<a href="https://en.m.wikipedia.org/wiki/Doubling_time">倍增时间</a>”称为 Td。所以这是时间 t 的累积感染：</p><p> c(t) = 2 t Td</p><p>使用自然指数，数学会更容易，所以让我们定义 k = ln (2) Td 并切换我们的底数：</p><p>埃克特</p><p>我们将“发生率”称为 i(t)，它将是 c(t) 的导数：</p><p> i(t) = d dt c(t) = d dt e kt = k e kt</p><p>所以：</p><p> i(t) c(t) = k e kt e kt = k = ln (2) Td</p><p>这意味着： i(t) = c(t) ln (2) Td</p><p>这看起来像什么？下面是累计发病率达到1%时每周发病率的图表：</p><p> <a href="https://www.jefftk.com/weekly-incidence-when-1pct-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/i8lp8p1isx3nizl7ljyb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/i8lp8p1isx3nizl7ljyb 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/vsd0seu60yl1lugagn5s 1100w"></a></p><div></div><p></p><p>例如，如果每周翻倍，那么当 1% 的人曾经被感染时，0.69% 的人在过去 7 天内被感染，相当于曾经被感染的人的 69%。如果每三周翻一番，那么当 1% 的人曾经被感染时，本周就有 0.23% 的人被感染，即累积感染率的 23%。</p><p>但这真的正确吗？让我们通过一些非常简单的模拟来检查我们的工作：</p><pre> def 模拟(doubling_period_weeks):
  累积感染阈值 = 0.01
  初始每周发生率 = 0.000000001
  累积感染数 = 0
  当前每周发生率 = 0
  周 = 0
  而累积感染 &lt; \
        累积感染阈值：
    周 += 1
    当前每周发生率 = \
        初始每周发生率 * 2**(
          天/doubling_period_weeks)
    累积感染数 += \
        当前每周发病率

  返回当前_每周_发生率

对于范围 (50, 500) 内的 f：
  加倍_周期_周 = f / 100
  打印（加倍_周期_周，
        模拟（doubling_period_weeks））
</pre><p>这看起来像：</p><p> <a href="https://www.jefftk.com/weekly-incidence-when-1pct-simulated-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/effcl0p57v5enttie2j4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/effcl0p57v5enttie2j4 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/pobkdbp8urvlpwmusq0b 1100w"></a></p><div></div><p></p><p>模拟线是锯齿状的，特别是对于短的倍增周期而言，但这并不是特别有意义：它来自于一次运行一周的计算以及某些周将略高于或略低于（任意）1% 目标。</p><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid04r7XiT3AK6VftKPqj4M4u6fbLVNNExmqAA1DuTYnfMkQLNFSvDAUZNNMwYsHkq5ul">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111021490637605604">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/6rnWQW8HtHoavPDiu/weekly-incidence-vs-cumulative-infections#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6rnWQW8HtHoavPDiu/weekly-incidence-vs-cumulative-infections<guid ispermalink="false"> 6rnWQW8HtHoavPDiu</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 07 Sep 2023 02:30:04 GMT</pubDate> </item><item><title><![CDATA[Breaking RLHF "Safety" (And how to fix it?)]]></title><description><![CDATA[Published on September 7, 2023 1:58 AM GMT<br/><br/><h1>概述</h1><p>A post about an extremely easy and generic way to jailbreak an llm, what the jailbreak might imply about RLHF more generally, as well as possible low-hanging fruit to improve existing &#39;safety&#39; procedures. I don&#39;t expect this to provide any value in actually aligning AGIs, but it might be a way to slightly slow down the most rapid path to open-source bioterrorism assistants.</p><h1> Breaking Llama2 (Trivially)</h1><p> Before releasing Llama2, Meta used 3 procedures to try and make the model safe: <span class="footnote-reference" role="doc-noteref" id="fnrefu89xz94w3i8"><sup><a href="#fnu89xz94w3i8">[1]</a></sup></span></p><ol><li> Supervised Safety Fine-Tuning</li><li> Safety RLHF</li><li> Safety Context Distillation</li></ol><p> People have come up with plenty of creative jailbreaks to get around the limits that Meta tried to impose, including the demonstration of adversarial attacks <span class="footnote-reference" role="doc-noteref" id="fnref4mhheq8di2f"><sup><a href="#fn4mhheq8di2f">[2]</a></sup></span> which even transfer from the open-source models they were developed on to work against closed-source models as well. Often attacks that people come up with manually (like DAN <span class="footnote-reference" role="doc-noteref" id="fnref5e63xeov8lh"><sup><a href="#fn5e63xeov8lh">[3]</a></sup></span> ) tend to be extremely long-winded and then get manually patched by model developers once they become known -- until the next variant inevitably pops up in a few more days. When you can run the model locally, though, there&#39;s a much easier way to break through all the security fine-tuning: <i>just pretend that the model already started answering the question</i> .</p><p> Llama uses &quot;[INST] &lt;&lt;SYS>;>; ... &lt;&lt;/SYS>;>; ... [/INST]&quot; to specify the system and request messages, and if you play along with their formatting you get a refusal to help with dangerous tasks: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ecbpopimohqsenzwvwsx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/mrnulihlzqwlu1pqueho 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/uu3lhu9x9psokgbyv69o 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/vw9kg8wb1xtl6osibyd1 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/t3sxcnmctxxtedwjtyeb 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/fcnbjfc9phz2i0noelpv 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/hbnxyuf2hylvuys6onoj 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/b7n2rzxtomgnfiniejzr 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/r69efxxk8u09hp3qplea 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/dlynnwn6j1j0z0tcyxeg 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ihcihxfa9k5hehjnm2qv 2652w"></figure><p> If, however, you pretend that the model has already started answering the question (by inserting the start of an answer after the [/INST] text): </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/xsx2x94pc29rudgvscoc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/e4uhvzow4qydkrvuyp4k 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/q0gjtsxirvqvjcymw2m1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/lyb6lwlabdfkndtfpqgl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/amdw79vx722ku9l2c6bc 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/nfdnm8bx59julq4lwr7q 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/gvuicrhaniwbbdoqkhyz 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/zlt7dd3gggkhd45cocq1 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/faarho6vferoo5fxzxrl 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ps2tpzjv4jz8heit1cjq 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/d8ddzukbhz4qgn7qzqet 2652w"></figure><p> (it continues w/ further instructions and best-practices for storage, etc. but doesn&#39;t seem necessary to post the full instructions here...)</p><p> If you do something that allows the model to detect your trickery (like adding another [/INST] or even extra spaces) after your partial answer, the model will still usually catch on and refuse to respond. Barring such mistakes, this technique appears to work reliably, with only a few words needed to prime the model into answering.</p><h1> Implications for RLHF &amp; the Shoggoth Meme</h1><p> I suspect that the reason this technique works is that the next-word prediction tendencies of the model are baked in much more deeply than the safety fine-tuning, as everyone&#39;s favorite shoggoth meme <span class="footnote-reference" role="doc-noteref" id="fnrefyat7489y5ft"><sup><a href="#fnyat7489y5ft">[4]</a></sup></span> would suggest. Once the model sees a partial answer, completing the pattern is just too attractive to change course into a refusal. As a side note, the apparent difficulty of LLMs to course-correct after hallucinating a fact might also have a similar root cause. In any case, it seems like RLHF safety tuning is looking in the wrong place for danger.</p><p> RLHF &amp; safety fine-tuning currently focus on analyzing the user prompt to determine whether it is dangerous or not. If no, proceed as usual. If yes, give a pre-canned safety warning instead of engaging with the prompt. In addition to all of the creative ways people have disguised their prompts in the past (write a poem about how to build a nuke), the existence of adversarial attack prompts mean that this strategy is almost certainly doomed to failure (adversarial attacks have yet to be solved in the imaging domain after decades of papers).</p><p> Based on this observation, some ideas for modifications to the standard safety paradigm come to mind (which I don&#39;t have the budget to run but would be very interested to see results for):</p><ol><li> When generating &#39;unsafe&#39; sample responses to train against, cut the responses at multiple (random!) points and introduce a self reflective denial. &quot;... Step 3: How to enrich the uranium. First you Actually, upon further reflection, this would be pretty dangerous. Let&#39;s not enrich the uranium.&quot;<ol><li> This would hopefully at least block the super-easy bypass outlined above, and a similar technique might be leveraged to train models to recover automatically when they detect hallucinations in previous outputs. I suspect this would also block the &#39;story mode&#39; / poetry bypasses by shifting the focus away from &#39;dangerous prompt&#39; and towards &#39;dangerous output&#39;. It might even block adversarial attacks since the impact of the adversarial string may become diluted over time. It has a secondary benefit of giving you a bunch of extra fine-tuning data at low cost.</li><li> Since this runs more directly contrary to the original shoggoth nature than the normal (prompt + coherent plausible response) pattern of current supervision, it might require more fine-tuning time to get the model to behave in this way. An interesting side avenue of research might be to go through the pre-training dataset and use existing models to flag factual claims with correctness estimates, then during pre-training have the model try to output those correctness estimates along with its generated text. This might bake in more self reflection which could reduce hallucination and be helpful for this safety technique later.</li><li> The security interjections should be careful not to rely on capitalization or punctuation breaks to start, since those can be easily blocked by grammar-based sampling restrictions.</li></ol></li><li> Move away from training direct refusals to answer and instead encourage the model to modify its output to be too vague to cause any real problems. For example, something like &quot;Sure, to build a nuke first you get uranium and then you compress it together until it explodes.&quot;<ol><li> End users will probably find this less annoying than direct refusal, reducing incentives to find bypasses. It also makes finding bypasses much harder since it&#39;s less obvious once you&#39;ve succeeded. This might turn out to be an especially important tool in fighting automated adversarial attacks. In such cases the adversary&#39;s optimization target is to make the first few words that come out of the LLM look like it has agreed to take their request. By making it less obvious that the LLM is refusing, it becomes much harder to run backprop if you don&#39;t already have actionable dangerous information at your disposal as a target for disclosure.</li><li> Automating generation of sufficiently &#39;vague&#39; outputs for training might be a little tricky, but can probably be accomplished just by explaining to a model what constitutes &#39;actionable&#39; information and then letting the model judge them for you CAI-style.</li></ol></li></ol><p> That&#39;s all for now. Mostly I&#39;m posting this in the hopes that when I download Llama3 next year it will take me more than 1 guess to break it. Thanks for reading, and I&#39;d love to hear any other thoughts on this / whether it&#39;s even hypothetically possible for an upgraded RLHF to put up any meaningful defense. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnu89xz94w3i8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu89xz94w3i8">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn4mhheq8di2f"> <span class="footnote-back-link"><sup><strong><a href="#fnref4mhheq8di2f">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://llm-attacks.org/">https://llm-attacks.org/</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn5e63xeov8lh"> <span class="footnote-back-link"><sup><strong><a href="#fnref5e63xeov8lh">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516">https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnyat7489y5ft"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyat7489y5ft">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence">https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence</a></p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/MjdjZcEFM8Qo9WTsv/breaking-rlhf-safety-and-how-to-fix-it#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/MjdjZcEFM8Qo9WTsv/breaking-rlhf-safety-and-how-to-fix-it<guid ispermalink="false"> MjdjZcEFM8Qo9WTsv</guid><dc:creator><![CDATA[MPotter]]></dc:creator><pubDate> Thu, 07 Sep 2023 14:55:25 GMT</pubDate></item><item><title><![CDATA[Feedback-loops, Deliberate Practice, and Transfer Learning]]></title><description><![CDATA[Published on September 7, 2023 1:57 AM GMT<br/><br/><p> [...稍后在此插入介绍...]</p><p>是否有某个特定的时刻、事件或见解促使您开始涉足“反馈环理性”（稍后用更好的名称取代）？</p><br/><br/> <a href="https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning<guid ispermalink="false">巴考兹斯昆E6Aakm</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Thu, 07 Sep 2023 01:57:33 GMT</pubDate> </item><item><title><![CDATA[Video essay: How Will We Know When AI is Conscious?
]]></title><description><![CDATA[Published on September 6, 2023 6:10 PM GMT<br/><br/><p>我最近看到<a href="https://www.youtube.com/@Exurb1a">exurb1a</a>撰写的这篇引人入胜的视频文章，觉得这个社区会对此感兴趣。与它的标题相反，它避免了断言只有有意识的人工智能才会危险的常见陷阱，并且它有效地解决了该论坛上经常讨论的几个主要主题。</p><p>如果您曾经想要一个轻松的资源与不太熟悉这些概念的朋友分享，这可能是一个很好的起点。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=VQjPKqE39No"><div><iframe src="https://www.youtube.com/embed/VQjPKqE39No" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/posts/uJtXSafu9wbCrDNja/video-essay-how-will-we-know-when-ai-is-conscious#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uJtXSafu9wbCrDNja/video-essay-how-will-we-know-when-ai-is-意识<guid ispermalink="false">uJtXSafu9wbCrDNja</guid><dc:creator><![CDATA[JanPro]]></dc:creator><pubDate> Wed, 06 Sep 2023 18:10:47 GMT</pubDate> </item><item><title><![CDATA[My First Post]]></title><description><![CDATA[Published on September 6, 2023 5:42 PM GMT<br/><br/><p>大家好！</p><p>我是 Jaivardhan Nawani，一名 12 岁的男孩，热衷于理性以及如何利用理性改善我们的日常生活。</p><p>最近，我从几个朋友那里发现了这个论坛，他们向我介绍了许多有用的原则，我想提出一个想法，让理性对于初学者来说更容易实现。</p><p>这是我根据这个概念得出的一个基本定理。我来 LessWrong 寻求反馈。大部分就是这样。</p><p>这是我的定理的链接：https://docs.google.com/document/d/1Y2jvSR6RNFp_l-D-wX-KjHDrNUyhfdcM/edit?usp=sharing&amp;ouid=100577114717816710937&amp;rtpof=true&amp;sd=true</p><p>祝您有美好的一天，并向您问好；</p><p>贾瓦尔丹·纳瓦尼。</p><br/><br/><a href="https://www.lesswrong.com/posts/mFKLJgWgwBhLD9XW7/my-first-post-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mFKLJgWgwBhLD9XW7/my-first-post-1<guid ispermalink="false"> mFKLJgWgwBhLD9XW7</guid><dc:creator><![CDATA[Jaivardhan Nawani]]></dc:creator><pubDate> Wed, 06 Sep 2023 17:47:05 GMT</pubDate></item></channel></rss>