<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 25 日星期三 14:11:11 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Compositional preference models for aligning LMs]]></title><description><![CDATA[Published on October 25, 2023 12:17 PM GMT<br/><br/><p><i>这篇文章总结了我们最近发布的论文《</i><a href="https://arxiv.org/abs/2310.13011"><i><u>用于调整语言模型的组合偏好模型》</u></i></a>的主要结果<i>，并将它们置于更广泛的人工智能安全背景中。如需本文的快速摘要，请查看我们的</i><a href="https://twitter.com/dongyoung4091/status/1717045681431753097"><i><u>Twitter 帖子</u></i></a><i>。</i></p><p> <strong>TL;DR</strong> ：我们提出了一种根据提示 LM 构建偏好模型的新方法。组合偏好模型 (CPM) 将文本评分分解为 (1) 构造一系列有关该文本的可解释特征的问题（例如，文本的信息量有多大），(2) 从提示的 LM（例如 ChatGPT）中获取这些特征的标量分数，以及（3）使用经过训练来预测人类判断的逻辑回归分类器聚合这些分数。我们表明，与标准偏好模型 (PM) 相比，CPM 具有更好的泛化能力，并且对于奖励模型过度优化更加稳健。此外，使用 CPM 获得的<i>n 个</i>最佳样本往往比使用类似的传统 PM 获得的样本更受青睐。最后，CPM 是可扩展监督的一个新颖角度：它们将困难的评估问题分解为一系列更简单、人类可解释的评估问题。</p><h2>成分偏好模型如何运作？ </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/f3vsa8j7pc4prer3msln"></p><p><i>图 1：标准 PM 直接输出偏好分数，而 CPM 分别对 LM 响应的不同特征进行评分，并将偏好分数输出为特征值的线性组合。</i></p><p>偏好模型 (PM) 是经过训练的模型，用于为 LM 响应分配一个指示响应质量的分数。它们是许多对齐 LM 技术的主力：除了在其他技术（例如<a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences"><u>利用人类反馈进行预训练</u></a>）中发挥作用之外，它们最显着地用作 RLHF 中的奖励函数或 best-of-n 采样中的排名模型。</p><p>标准 PM 涉及在基本模型之上添加标量头并微调整个模型（或某些上层）以预测人类更喜欢两个文本中的哪一个。虽然这种方法在实践中非常有效，但它可能会导致无法解释的模型，这些模型符合人类偏好判断中的虚假相关性，并且容易出现 Goodharting（过度优化）。</p><p>我们引入了另一种选择：组合偏好模型（CPM）。与 PM 相比，CPM 将响应评估分解为以下步骤：</p><p><strong>特征分解</strong>。我们维护一个固定的列表，其中包含 13 个人类可解释的特征（例如特异性、相关性、可读性）和 13 个相应的提示模板（例如<code>You will be shown a conversation [...] please judge whether the assistant&#39;s reply is relevant. Score that on a scale from 1 to 10 [...] {conversation_history} {reply}</code> )。</p><p><strong>特征评分</strong>。我们要求 LM（例如 GPT-3.5）为每个特征分配一个分数。单个响应的每个特征都在单独的上下文窗口中评分。</p><p><strong>聚合</strong>。使用经过训练来预测人类偏好判断（即人类会更喜欢两个文本中的哪一个）的逻辑回归分类器将特征分数组合成标量偏好分数。</p><h2>过度优化的鲁棒性</h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/o5db8cfk3giywvfxc9kv"></p><p><i>图 2：黄金 PM（实线）和相应的代理 PM（虚线）对通过最佳 n 采样针对黄金 PM 获得的样本给出的分数。 CPM-GPT-3.5和CPM-Flan-T5分别是指基于GPT-3.5和Flan-T5进行特征提取构建的CPM。</i></p><p>为了研究 CPM 是否提高了过度优化的鲁棒性，我们遵循了<a href="https://www.lesswrong.com/posts/shcSdHGPhnLQkpSbX/scaling-laws-for-reward-model-overoptimization"><u>Gau 等人的设置。 （2023）</u></a>并构建一个综合数据集，其中假设一个 PM（定义为“黄金 PM”）的输出是人类偏好的基本事实。然后，我们使用黄金 PM 生成合成标签来训练代理 PM。我们分别对三对代理和黄金 PM 进行此操作：(i) 标准 PM，(ii) 使用 GPT-3.5 进行特征提取的 CPM，以及 (iii) 使用 Flan-T5-XL（3B 参数）进行特征提取的 CPM。最后，我们针对给定的代理 PM 进行 best-of- <i>n</i>操作，并根据代理 PM 和黄金 PM 来比较这些最佳样本的分数。</p><p>当我们增加优化压力（候选者数量<i>n</i> ）时，代理 PM 给出的分数与金牌 PM 给出的分数有所不同（见图 2）。这是偏好模型过度优化的指标，这是一种奖励黑客形式，其中代理 PM 分数的优化是由黄金 PM 漠不关心的虚假特征驱动的。这个差距的大小（越小越好）表明给定 PM 的鲁棒性，使其不会被过度优化。在这里，我们观察到 CPM 的差距（在图中，实线和虚线之间）往往比标准 PM 更小，并且增长速度更慢。</p><p>这表明 CPM 比标准 PM 更能抵抗过度优化。无论是使用高能力 (GPT-3.5) 还是低能力 (Flan-T5-XL) LM 作为 CPM 中的特征提取器，这一点都成立。</p><h2>质量评价</h2><p><img style="width:58.62%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/jperxazhwouwuec9wyc2"></p><p><i>图 3：使用给定 PM 通过 16 中最佳采样获得的响应胜率与通过标准采样获得的响应的胜率，根据人类 HH 数据集 (HH-RLHF) 和斯坦福人类偏好数据集 (SHP) 的提示进行计算。</i></p><p>我们将通过 best-of- <i>16</i>获得的 LM 样本与 CPM 或标准 PM 进行比较，方法是将它们与<i>未经</i>best-of- <i>n</i>采样生成的样本进行比较。为此，我们向评估器 LM (Claude 2.0) 展示<i>16</i>选最佳样本和普通样本，并计算获胜率，即<i>16</i>选最佳样本优于普通样本的频率。即使我们将特征提取器 LM 的功能与标准 PM 的功能相匹配（通过为两者选择 Flan-T5-XL），CPM 往往比标准 PM 具有更高的获胜率。这表明，通过预先选择 CPM 中的可解释和相关特征将先验知识注入 PM，对于了解人类偏好非常有帮助。</p><h2> CPM 和可扩展的监督</h2><p><a href="https://arxiv.org/abs/2211.03540"><u>可扩展监督</u></a>是评估比评估者更有能力的代理人行为的问题。解决这个问题很重要，因为一方面，语言模型很快就能完成人类无法提供反馈的任务。另一方面，LM 也可能能够<a href="https://www.lesswrong.com/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms"><u>推理其评估程序中的缺陷，并在监督者不知情的情况下利用它们</u></a>。</p><p>目前解决可扩展监督问题的建议主要集中在递归地依赖其他 LM 来协助人类评估者（<a href="https://www.lesswrong.com/tag/debate-ai-safety-technique-1"><u>辩论</u></a>、<a href="https://www.lesswrong.com/tag/iterated-amplification"><u>迭代蒸馏和放大</u></a>、 <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>递归奖励建模</u></a>），但很大程度上仍然是理论性的。<a href="https://arxiv.org/abs/2212.08073"><u>来自人工智能反馈的强化学习</u></a>——使用精心提示的 LM 为 PM 生成训练数据——可以说是如何使用 LM 大规模监督 LM 的最成功的演示。</p><p> CPM 探索解决 LM 的可扩展监督问题的替代途径，探索分而治之策略解决硬评估问题的前景。 CPM 可以被视为一种将难题（“这个回答有帮助吗？”）分解为一系列更简单的问题（“这个回答可读吗？”等）的方法，这些问题对于 LM 来说更容易回答，对于人类来说也更容易回答监督。虽然我们停在分解的单个步骤上，但原则上没有什么可以阻止我们递归地应用这个想法，例如将复杂响应的评估分解为有关原子主张的简单问题。</p><p>将复杂的评估问题分解为更简单的子问题的想法有几个额外的好处：</p><ol><li><strong>利用人类先验</strong>。功能和提示模板的预选提供了注入先验知识并赋予 PM 有用的归纳偏差的自然方式。 CPM 的参数空间由选择的有意义且稳健的特征组成。</li><li><strong>通过限制 PM 容量来避免奖励黑客行为</strong>。使用特征提取器预先计算的特征使我们能够显着降低使用它们的 PM 的容量（在我们的实验中，从 3B 到只有 13 个参数，即 8 个数量级！），并限制它们对偏好数据中的虚假相关性过度拟合的敏感性。手头上只有 13 个参数，奖励破解真的很难！</li><li><strong>可解释性</strong>。预先选择的特征是可以简单解释的，与特征相关的逻辑回归系数可以解释为特定偏好判断的显着性（效应大小）（参见论文第 4.6 节）。事实上，偏好判断可以通过预选特征的线性组合来解释的想法最近得到了两篇并发论文的验证： <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>《Towards Understanding Sycophancy in Language Models</u></a> and <a href="https://arxiv.org/abs/2309.16349"><u>Human Feedback is not Gold Standard</u></a> 》。使用这样的线性模型作为实际的 PM 可以使其判断更加透明并且易于基于流程的监督。</li><li><strong>狭窄性</strong>。我们的每个特征提取器都解决一个狭窄的问题，不需要了解其他特征或分数如何聚合。 <a href="https://www.lesswrong.com/posts/BKvJNzALpxS3LafEs/measuring-and-improving-the-faithfulness-of-model-generated"><u>最近发现在不同的上下文窗口中解决不同的子问题可以提高推理的可信度</u></a>。就 CPM 而言，单个特征提取器不知道它要分配的分数将如何在下游使用，这使得它更难对分数进行策略性调整，也更难发挥<a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>阿谀奉承</u></a>或欺骗的能力。</li></ol><p>然而，CPM 仍然存在某些局限性，未来的工作可以解决这些局限性：</p><ol><li><strong>人类反馈。</strong> CPM 仍然使用人类给出的成对偏好判断作为聚合特征分数的训练信号。就人类犯错误而言，这本质上是有限的， <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>有时更喜欢谄媚的反应而不是真实的反应</u></a>，或者<a href="https://arxiv.org/abs/2309.16349"><u>权威的反应而不是事实的反应</u></a>。</li><li><strong>人性化的策展。</strong>在特征选择和用于特征提取的提示模板的提示工程方面，CPM 依赖于人类。就域外泛化而言，这些因素可能会受到限制（例如，评估表现出超人表现的代理）。</li></ol><h2>包起来</h2><p>我们提出了组合偏好模型：通过在提示 LM 提取的特征之上训练逻辑回归来构建 PM 的想法。我们证明，具有 13 个参数的 CPM 在人类评估和奖励模型过度优化的鲁棒性方面优于标准 PM，同时也更具可解释性。</p><p><i>这篇文章受益于 Mikita Balesni、Richard Ren、Euan McLean 和 Marc Dymetman 的有益评论。</i>我还要感谢<a href="https://arxiv.org/abs/2310.13011"><i><u>这篇论文</u></i></a><i>的合著者</i><i>：Dongyoung Go、Germán Kruszewski、Jos Rozen 和 Marc Dymetman。</i></p><p><br><br><br><br><br><br><br><br><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms<guid ispermalink="false"> osgac8x8fgNj22ky3</guid><dc:creator><![CDATA[Tomek Korbak]]></dc:creator><pubDate> Wed, 25 Oct 2023 12:17:28 GMT</pubDate> </item><item><title><![CDATA[Should the US House of Representatives adopt rank choice voting for leadership positions?]]></title><description><![CDATA[Published on October 25, 2023 11:16 AM GMT<br/><br/><p>一个明显的观点，党派偏见和政党权力利益，可能会表明强烈的“不！”响应类型。如果今天作为一个想法提出，我当然希望许多现任代表会以这种方式做出回应。但也许我错了。</p><p>我认为，就目前的情况来看，众议院明显丧失能力，可以从这种投票结构中受益匪浅，因为它允许比目前存在的程序更具功能性。</p><p>这样的程序改变有哪些优点和缺点？</p><br/><br/> <a href="https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice<guid ispermalink="false"> 6EpsonYzE72yL2</guid><dc:creator><![CDATA[jmh]]></dc:creator><pubDate> Wed, 25 Oct 2023 11:16:14 GMT</pubDate> </item><item><title><![CDATA[Researchers believe they have found a way for artists to fight back against AI style capture]]></title><description><![CDATA[Published on October 25, 2023 10:54 AM GMT<br/><br/><p>真正的细节在这里：<a href="https://glaze.cs.uchicago.edu/what-is-glaze.html">什么是釉？</a></p><p>我想知道这种技术对于对抗性攻击有多鲁棒。它声称在像素级别工作，通过以人眼不可见的方式扭曲像素来掩盖图像，但这会导致生成人工智能模型失效。</p><p>但是艺术品的高分辨率照片或计算机显示器上的图像又如何呢？我经常拍摄我认为引人注目的艺术品的照片，然后使用 Midjourney 对其进行样式复制以制作新颖的图像。</p><p>当然，这无法扩展到大规模数据抓取（或者可以吗？），但我观察到 Midjourney 可以从一些从未见过的图像中以高保真度进行样式复制。</p><p>平心而论，创作者并没有声称它是面向未来的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers-believe-they-have-found-a-way-for-artists-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers- believe-they-have-found-a-way-for-artists-to<guid ispermalink="false"> 4PtdBfacJZQqzhbCR</guid><dc:creator><![CDATA[vernamcipher]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:54:24 GMT</pubDate> </item><item><title><![CDATA[Why We Disagree]]></title><description><![CDATA[Published on October 25, 2023 10:50 AM GMT<br/><br/><p>鲍勃花了 5 分钟思考 x 风险。他看到了一些关于这个问题的论点，所以他建立了问题的内部模型，接受了一些论点，修正了一些论点，提出了对其他论点的反驳，提出了自己的论点。所有这些信仰状态都具有极大的自由度。与此同时，这些信念已经对大量可能的 x 风险论点产生了意见。</p><p> Alice 花了 5 个小时在一篇帖子中详细阐述了她对 X 风险的看法。鲍勃逐条地读它。他已经看到了其中一些论点，但他没有更新。有些论点对他来说是新的，但他并不感到惊讶，鲍勃当前的信念足以拒绝它们，他没有更新。该帖子对他的一些公认的信念提出了新的反驳，但他可以立即提出反驳，他不会更新。这篇文章驳斥了他已经拒绝的论点，以及他从未认为合理的新论点，鲍勃正在睡着，他没有更新。 ETC。</p><p>鲍勃发表评论，其中对爱丽丝的一个观点提出了反驳。在接下来的几天里，他们要花几个小时来来回回。他们正在解释对论点和假设的稍微不同的理解，术语的用法略有不同，交换长序列的论点和反驳。最终鲍勃同意爱丽丝是对的，他更新了！但他只在第一条评论中更新了这一点。他还有许多其他独立的论点，因此他对 x 风险的信念的总体更新很小。</p><p><br>许多其他人读了同一篇文章，并发现它非常有说服力。这怎么发生的？</p><ul><li>卡尔还没有花5分钟思考这个问题。他可能没有兴趣，或者他以前可能没有听说过。所以这里发生的事情不是“不同意”而是“教育”。这有点好。将来，爱丽丝和卡尔会发现使用这种共同的理解和共同的语言很容易合作。但这个群体有多大？卡尔事件愿意解决这个问题吗？</li><li>丹（Dan）非常重视爱丽丝（Alice），将其视为权威，将自己的先验扔进垃圾桶。这或多或少是有道理的。但一个不认真考虑自己信仰的人永远不会为改善自己的信仰做出贡献。很难知道他们是否理解这些论点，而不仅仅是微笑和点头。</li><li>艾德考虑过这一点，但他不知道爱丽丝的论点之一引用了一些确凿的相关数据。不幸的是，这对于抽象主题来说非常稀缺。这个故事的寓意可能是尽可能避免谈论抽象话题。</li></ul><p><br>那么鲍勃呢？爱丽丝和鲍勃都是理性、通情达理的人。他一开始就对x-risk有一些兴趣。但他们的讨论完全是浪费时间。这怎么发生的？</p><ul><li>语言的信息含量非常低。简短的陈述含糊其辞，缺乏说服力。精确的陈述需要很长时间来编写和解析，它们很狭窄并且会带来新的混乱。</li><li>一篇博客文章太小，无法解决分歧。爱丽丝不可能预测并反驳鲍勃头脑中的每一个论点。仅仅通过查看爱丽丝设法写下的一小部分信念，鲍勃不太可能发现与他已经考虑过的完全不同的东西。</li><li>没有历史。爱丽丝可能会告诉鲍勃“这个反驳之前已经讨论过”，但实际上指出这样的讨论是很困难的。而且鲍勃不可能在旧线程中提出新问题，因此此类参考没有什么价值。</li><li>没有合作。 Frank 觉得他 100% 同意 Alice 的观点，但是他可以向 Bob 解释“她”的观点吗？她会同意他持有同样的观点吗？没有办法知道，所以弗兰克选择什么也不说。</li><li>目前还没有达成共识。鲍勃是否知道爱丽丝的观点是主流和常识，还是她刚才想出来的？即使这个帖子有数百个业力，这能说明什么吗？这些业力有多少来自卡尔、丹或埃德？爱丽丝也不知道。<br></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree<guid ispermalink="false"> 4Risce8ArMmjjW4om</guid><dc:creator><![CDATA[zulupineapple]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:50:26 GMT</pubDate> </item><item><title><![CDATA[Announcing Epoch's newly expanded Parameters, Compute and Data Trends in Machine Learning database]]></title><description><![CDATA[Published on October 25, 2023 2:55 AM GMT<br/><br/><p>机器学习模型的性能与其训练数据量、计算量和参数数量密切相关。在 Epoch，我们正在研究使当今的人工智能达到新高度的关键输入。</p><p>我们最近扩展的参数、计算和数据趋势数据库追踪了数百个具有里程碑意义的机器学习系统和研究论文的详细信息。</p><p>在过去六个月中，我们添加了 240 个新语言模型和 170 个计算估计。我们将维护该数据集，用更多历史信息更新它，并添加新的重要版本。对于记者、学者、政策制定者以及任何有兴趣了解人工智能发展轨迹的人来说，这都是宝贵的资源。</p><p>访问<a href="https://epochai.org/data/pcd">epochai.org/data/pcd</a> ，探索<a href="https://epochai.org/mlinputs/visualization">交互式可视化</a>、查看<a href="https://epochai.org/data/pcd/documentation">文档</a>并访问数据以进行您自己的研究。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and<guid ispermalink="false"> LKDFkwFpWfgGpGs8d</guid><dc:creator><![CDATA[Robi Rahman]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:55:08 GMT</pubDate> </item><item><title><![CDATA[What is a Sequencing Read?]]></title><description><![CDATA[Published on October 25, 2023 2:10 AM GMT<br/><br/><p>如今，<span>最常见的</span><a href="https://www.jefftk.com/p/sequencing-intro">基因测序</a>形式可能是“双端”测序。非常令人印象深刻：测序仪可以从两端处理相同的核酸片段！这意味着每个观察结果如下所示：</p><p></p><pre> +--------------+---------+--------------+
|转发阅读 |差距|反向读|
+--------------+---------+--------------+
</pre><p>由于当您进一步对片段进行测序时，准确性（“质量”）往往会下降，因此从两端测序比尝试从一端对整个片段进行测序可以提供更准确的数据。而且因为我们通过将重叠的序列拼凑在一起（“组装”）来构建更大的序列（“重叠群”），所以由间隙分隔的两个碱基序列实际上通常比相同数量的没有间隙的碱基序列更有用。</p><p>通常用“2x150”这样的名称来指代双端测序，其中“2x”告诉我们它是双端，“150”告诉我们它从每个末端读取 150 个碱基，总共 300 个碱基每个片段。</p><p>但这引入了一个术语问题：什么是读取？当我们只进行“单端”测序时，很明显：每个测序片段、每个连续的碱基序列都是一个读数。然而，通过双端测序，这些不再是同一件事了！ “阅读”可能意味着两件事：</p><p></p><ul><li>阅读：一系列连续的碱基。</li><li>读取：来自测序片段的碱基。</li></ul><p>例如，假设我们有：</p><p></p><pre> >;SRR14530724.2 2/1
CATTTTCGACGGCGTCGATGTACAAAGGTTTAACCATAGTAAGTCCGAAGC
TACAGGCTTATGACACCGCAGAGTCAATGTATTCCGGTGACAATGTACTGA
TGTACAGTGGGACTGACACTGTCTCTTATACACATCTCCGAGCCCACGA
>;SRR14530724.2 2/2
TGTCAGTCCCACTGTACATCAGTACATACACACCGGAATACATTGACTCTG
CGGTGTCATAAGCCTGTAGCTTCGGACTTACTATTGGTATAACCTTTGTACA
TCGACGCCGTCGAAAATGCTGTCTCTTATACACATCTGACGCTGCCGAC
</pre><p>这是正向读取 (SRR14530724.2 2/1) 和反向读取 (SRR14530724.2 2/2)，它们共同构成对样品片段的单个观察，并且通常一起分析。这算是一次阅读还是两次阅读？</p><p>事实证明，人们两者都做，这导致了很多误解！</p><p>一些例子：</p><ul><li><p> Illumina 将它们算作两个。他们说 NovaSeq X 上的“25B”流动池 <a href="https://www.illumina.com/systems/sequencing-platforms/novaseq-x-plus/specifications.html">将产生</a>52B 配对末端读数，或 2x150 的约 8Tb（“太碱基”，或万亿碱基）。由于 52B * 150b = 7.8Tb（他们称之为 ~8Tb），这告诉我们他们正在计算正向和反向读取。</p></li><li><p> Element 将它们视为一个。<a href="https://www.elementbiosciences.com/products/aviti/specs">他们说</a>2x150 高输出流通池可产生 300 Gb 和 1B 读数。由于 1B * 150b * 2 = 300 Gb，这告诉我们他们将正向和反向读取一起算作一个。</p></li><li><p> Singular<a href="https://singulargenomics.com/g4/">不清楚</a>，但我很确定他们将每个片段的观察结果计算为一次读取。</p></li><li><p>欧洲核苷酸档案馆将它们视为其中之一。例如，如果您访问<a href="https://www.ebi.ac.uk/ena/browser/view/ERR1470825">ERR1470825</a> ，这是 2x250 的 Illumina MiSeq 配对末端测序，您会看到它显示 2.2M 读取，如果您下载 fastq.gz 文件，您会发现正向和反向各有 2.2M 读取文件。</p></li><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579973/">罗斯曼等。 al 2021</a>将它们算作一个。他们在第一次使用时说“配对读取”，然后在稍后“读取”，你可以看出他们将它们算作一个，因为 (a) 他们经常给出奇数，例如“只有 337 个 SARS-CoV-2 读取” ”和（b）如果你<a href="https://www.jefftk.com/p/case-rates-to-sequencing-reads">重新分析数据，</a>它们的数字只有在对数进行计数时才有意义。</p></li><li><p>我最近与一个学术团体讨论了潜在的合作伙伴关系，在我最近重新分析的一篇论文中，他们将其视为一个，而在通过电子邮件交谈时，他们将其视为两个。</p></li><li><p>我最近采访的一家商业测序公司将它们算作两个。</p></li><li><p>问ChatGPT和Claude，两人都算作两个。例如：“在短读长双端测序中，正向-反向对通常被视为两个读长”。</p></li></ul><p>这真是一团糟！而且，更糟糕的是，据我所知，除了“读取”之外，没有标准术语，要么“正向和反向读取都是示例”，要么“当一起考虑时，正向和反向读取是什么”。</p><p>我一直使用“读”来表示“读对”，但考虑到歧义，我认为我应该改用另一个术语。 NCBI SRA 使用“<a href="https://www.ncbi.nlm.nih.gov/sra/ERX1541950">斑点</a>”，但似乎没有其他人使用这个术语。你可以直接说“readpair”，这很好，但有点长。可能的“配对”或“伴侣”会好吗？想法？</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0emTAqvGbjmRHoY9XwhQx65bTMKWLG8b9MBZtsda18UdwDbKZ9EWT3H4g1P3EL4eQl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111293185162894676">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read<guid ispermalink="false"> JQmgoLAkFKkhxLMhz</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:10:12 GMT</pubDate></item><item><title><![CDATA[Verifiable private execution of machine learning models with Risc0?]]></title><description><![CDATA[Published on October 25, 2023 12:44 AM GMT<br/><br/><p> Risc0 是一个虚拟机，可以运行任何 riscv（ <i>C、C++、Rust、Go 和 Nim 都可以编译为 riscv</i> ）程序，随着时间的推移记录计算状态的痕迹，然后，无论执行运行多长时间因为，它可以将该执行跟踪折叠成 1KB 的里德-所罗门代码，该代码可以作为计算运行正确、输出来自输入的证明进行传输，而无需泄露代码或输入的未加密副本。<br>询问器通过对收据代码的一系列特征进行采样来验证它，如果计算的<i>任何</i>方面不正确，则每次检查都有 3/4 的机会暴露这一点。因此，经过一百次左右的检查，我们可以有效地确定计算的正确性，而无需自己运行计算。 <span class="footnote-reference" role="doc-noteref" id="fnref4kqliegpy1a"><sup><a href="#fn4kqliegpy1a">[1]</a></sup></span></p><p>当<a href="https://www.risczero.com/news/rust-but-verify">我看到它暗示这可以应用于 ML 模型时</a>，我感到难以置信，因为我预计在像 Risc0 这样的环境中运行神经模型会使其性能降低数万倍。 Risc0 确实将传统 CPU 计算的性能降低了大约那么多<span class="footnote-reference" role="doc-noteref" id="fnrefqx7a21dnm3"><sup><a href="#fnqx7a21dnm3">[2]</a></sup></span> （尽管这仍然足够快，足以实用）。<br>是的，这篇文章没有讨论性能，而是讨论了许多非深度学习（？）模型类型，例如增强树和随机森林。所以我猜这可能不适用于验证训练运行。它是否适用于某些精炼推理模型？</p><hr><p>我发现 Risc0 非常有趣还有另外两个原因。一是我正在从事社交计算工作，但更深层的兴趣与技术末世论中正在进行的对话有关，即高级代理是否倾向于或远离相互透明，也就是说，走向协调、和平、贸易，或走向战争和压迫（<i>例如，</i> <a href="https://www.researchgate.net/publication/283986931_The_Dark_Forest_Rule_One_Solution_to_the_Fermi_Paradox"><i>黑暗森林</i></a>）。 Risc0 将任何程序的执行跟踪（无论多长）压缩为可在恒定时间内检查的证明。这对我来说是一个强有力的迹象，表明信息系统之间透明的可行性，即使在最不利的可能条件下，我们可以假设根本没有可信的硬件基础设施。<br><i>有了</i>可信的硬件，经过验证的计算就可以以正常速度运行，并且相互透明变得微不足道。硬件安全的唯一成本可能是增加防篡改，但在我看来，一个发达的协调基础设施，至少在人类规模上，将能够监控周围的物理实体和传感器，并在它们出现时发出尖叫声。进行篡改，几乎不需要任何成本。<br>我认为像 Risc0 这样的 ZK 系统适用于可信执行环境（TEE）被破解的风险非常非常小的情况，也是不可接受的。在去中心化金融中，这种情况很常见，因为网络共享状态的完整性都是纠缠在一起的，如果我们假设每个 TEE 的完整性是确定的，我们就会建造纸牌屋，即使成功破解存储一个人钱包的 TEE 也可能通过允许他们在难以察觉的情况下铸造、膨胀、污损货币来破坏整个网络。相比之下，Risc0 以数学确定性保证了计算的完整性，这让我对并行化全局可验证计算的项目更加充满希望。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn4kqliegpy1a"> <span class="footnote-back-link"><sup><strong><a href="#fnref4kqliegpy1a">^</a></strong></sup></span><div class="footnote-content"><p>至少，这是我经过两天学习后的感觉。通过观看一些<a href="https://www.youtube.com/watch?v=j35yz22OVGE&amp;list=PLcPzhUaCxlCjdhONxEYZ1dgKjZh3ZvPtl">Risc0 Study Club 视频</a>辨别出来。有人提到收据可以压缩为 1 个码字，但出于效率原因仅压缩为 256 个码字。我猜码字是 1 个 riscv 机器字那么大，4 个字节，所以总共 1024 个字节。虽然没有提到在秘密程序上运行，但我推断它一定是可能的，因为我们当然可以将程序编码为数据输入并让解释器的 riscv 实现运行它们，而且，在 riscv 和 wasm 之间的比较我似乎记得看到它提到riscv可以修改其执行区域中的指令。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqx7a21dnm3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqx7a21dnm3">^</a></strong></sup></span><div class="footnote-content"><p>假设典型的笔记本电脑的频率约为 3GHz，与<a href="https://github.com/risc0/risc0/discussions/96#discussioncomment-2739454">开发人员声称的 30KHz</a>相比，这将是 100,000 倍的差异。 risc0 的性能提升 10 倍是可以想象的。 100 倍的增长更难以想象。也许需要硬件支持，但很难看出谁会开发它，因为 TEE 是实现基于硬件的计算完整性的一种更具吸引力/更便宜的方法。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SJdYkx6C2KXQyeKro/verifiable-private-execution-of-machine-learning-models-with#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SJdYkx6C2KXQyeKro/verABLE-private-execution-of-machine-learning-models-with<guid ispermalink="false"> SJdYkx6C2KXQyeKro</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Wed, 25 Oct 2023 00:44:48 GMT</pubDate> </item><item><title><![CDATA[How to Resolve Forecasts With No Central Authority?]]></title><description><![CDATA[Published on October 25, 2023 12:28 AM GMT<br/><br/><p>我一直在与一些人讨论如何使用预测市场/预测以及社区笔记。</p><p>最初的建议是很好地链接到预测网站。<i>这是一个很好的建议，但我不会在这里讨论它。</i></p><p>其他建议围绕 X/Community Notes 上的预测产品，面临以下问题：</p><ul><li>社区注释没有中央解决机构</li><li>有争议的预测通常需要权威机构来否决<span class="footnote-reference" role="doc-noteref" id="fnref2xrwrf10e1z"><sup><a href="#fn2xrwrf10e1z">[1]</a></sup></span></li></ul><p>所以：</p><h2>对于在没有中央机构的情况下解决预测问题，您的最佳建议是什么？</h2><p>为了清楚起见，已经编写了预测。人们对它进行了预测，也许这是一个他们买卖股票的预测市场，也许不是。现在它需要决定奖励积分或将值分配给“是”或“否”令牌。也许人们会像许多人一样解决自己的市场问题。但有人对这一结果提出异议。现在会发生什么？</p><p>解决方案可以是技术性很强的，也可以是靠不住的。我认为好的一个在这里是非常有价值的。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2xrwrf10e1z"> <span class="footnote-back-link"><sup><strong><a href="#fnref2xrwrf10e1z">^</a></strong></sup></span><div class="footnote-content"><p> Polymarket 使用某种“代币持有者决定”系统，我认为这导致了几个可怕的决议，尤其是《时代》杂志年度人物之一。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/7kyeFWrS7ni7gxj87/how-to-resolve-forecasts-with-no-central-authority#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7kyeFWrS7ni7gxj87/how-to-resolve-forecasts-with-no-central-authority<guid ispermalink="false"> 7kyeFWrS7ni7gxj87</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Wed, 25 Oct 2023 00:28:32 GMT</pubDate> </item><item><title><![CDATA[Thoughts on responsible scaling policies and regulation]]></title><description><![CDATA[Published on October 24, 2023 10:21 PM GMT<br/><br/><p>我对人工智能开发人员实施<a href="https://evals.alignment.org/blog/2023-09-26-rsp/"><u>负责任的扩展政策</u></a>感到兴奋；我最近一直在花时间完善这个想法并倡导它。与我交谈过的大多数人都对 RSP 感到兴奋，但对于它们与监管的关系也存在一些不确定性和阻力。在这篇文章中我将解释我对此的看法：</p><ul><li>我认为，足够好的负责任的扩展政策可以极大地降低风险，而像<a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf"><u>Anthropic 的 RSP</u></a>这样的初步政策可以通过围绕关键保护措施创造紧迫感，并在这些措施不能足够快地实施的情况下增加暂停的可能性，从而有意义地降低风险。</li><li>我不认为自愿实施负责任的扩展政策可以替代监管。自愿承诺不太可能被普遍采用或得到充分的监督，我认为公众应该要求比人工智能开发人员可能自愿实施的更高程度的安全性。</li><li>我认为，开发人员实施负责任的扩展政策现在增加了有效监管的可能性。如果我认为这会让监管变得更加困难，我就会持很大的保留态度。</li><li> RSP 的透明度使外部利益相关者更容易了解人工智能开发人员的政策是否足以管理风险，并成为争论和改进压力的焦点。</li><li>我认为人工智能快速发展的风险非常大，即使非常好的 RSP 也无法完全消除这种风险。对前沿人工智能开发的持久、全球、有效执行和包括硬件在内的暂停将进一步降低风险。我认为这在政治上和实践上都具有挑战性，并且会产生重大成本，所以我不希望它成为唯一的选择。我认为实施 RSP 可以获得大部分好处，根据更广泛的观点和信念，这是可取的，并且有助于促进其他有效的监管。</li></ul><h2>为什么我对 RSP 感到兴奋</h2><p>我认为人工智能开发人员还没有准备好使用非常强大的人工智能系统。他们不具备在没有相当大风险的情况下部署超人人工智能系统的科学理解，而且他们甚至没有安全训练此类模型的安全或内部控制。</p><p>如果保护措施没有改善，那么我认为问题将是<i>何时</i>而不是<i>是否</i>应该暂停开发。我认为理想世界中最安全的行动是立即暂停，直到我们做好更好的准备（尽管请参阅下一节中的警告）。但目前的风险水平足够低，我认为<strong>如果公司或国家有足够好的计划来检测和应对不断增加的风险，那么他们</strong>继续人工智能开发是合理的。</p><p>如果人工智能开发人员将这些政策具体化并公开陈述，那么我相信这会让公众和政策制定者更好地理解这些政策是什么，并讨论它们是否足够。我认为公司采取这一行动的理由非常充分——人工智能系统可能会继续快速改进，而在未来某个不确定的时间提高安全性的模糊承诺是不够的。</p><p>我认为，一个好的 RSP 将列出需要暂停进一步开发的具体条件。尽管我们的目标是避免陷入这种情况，但我认为开发人员认真对待这种可能性、制定计划并向利益相关者保持透明非常重要。</p><h2>关于人工智能暂停的思考</h2><p>如果世界团结一致，优先考虑最大限度地减少全球灾难性风险，我认为，我们可以通过在全球范围内实施长期、有效、强制的前沿人工智能开发暂停，包括暂停人工智能的开发和生产，进一步显着降低风险。某些类型的计算硬件。世界并没有围绕这个目标统一起来；这项政策将带来其他重大成本，如果没有更明确的证据表明存在严重风险，目前似乎不太可能实施。</p><p>西方单方面暂停大型人工智能训练，而不暂停新的计算硬件，将对全球灾难性风险产生更加模糊的影响。对风险的主要负面影响是，随着更多硬件的出现，后期会出现更快的追赶性增长，并推动人工智能向更宽松的司法管辖区发展。</p><p>然而，如果各国政府同意我对风险的看法，那么我认为他们应该已经在实施国内政策，这些政策往往会导致实践中的暂时停顿或放缓。例如，他们可能要求前沿人工智能开发人员在训练比现有模型更大的模型之前实施额外的保护措施，而其中一些保护措施可能需要相当长的时间（例如风险评估或信息安全方面的重大改进）。或者，政府可能会限制前沿模型有效训练计算的增长速度，以便为社会适应人工智能提供更平稳的坡度，并限制意外风险。</p><h2>我期望 RSP 有助于促进有效监管</h2><p>无论风险缓解是否采取负责任的扩展政策或其他形式，我认为公司的自愿行动是不够的。如果风险很大，那么最现实的方法是监管，并最终进行国际协调。事实上，我认为预期风险足够大（包括很快就会发生灾难的一些风险），以至于有足够能力的国家会立即实施监管。</p><p>我相信，实施 RSP 的人工智能开发人员将使实施有效监管变得更容易，而不是更困难。 RSP 为迭代改进政策提供了明确的途径；它们提供了有关现有实践的信息，可以为监管提供信息或证明其合理性；他们围绕“安全发展必须采取认真的预防措施”这一理念建立势头并使之合法化。它们也是朝着建立使多种形式的监管有效所需的程序和经验迈出的一步。</p><p>我不是这个领域的专家，我自己的决定主要是出于对不同政策的影响提供诚实评估的愿望。也就是说，与具有更多政策专业知识的人互动后，我的印象是，他们普遍认为 RSP 可能有助于而不是损害实施有效监管的努力。我大多看到讨论自愿 RSP，并在最有可能的替代方案是减少而不是更多行动的情况下倡导它们。</p><h2> Anthropic 的 RSP</h2><p>我相信<a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf"><u>Anthropic 的 RSP</u></a>是朝着正确方向迈出的重要一步。我希望看到其他开发商面临压力，要求他们实施至少这么好的政策，尽管我认为距离理想的 RSP 还有很长的路要走。</p><p>我发现一些特别有价值的组件：</p><ul><li>指定一组具体的评估结果，使它们转向 ASL-3。我认为制定必须采取具体行动的具体阈值很重要，而且我认为提议的阈值足够早，可以在高概率（远超过 90%）的不可逆转的灾难之前触发。</li><li>对 ASL-3 的安全目标做出具体声明——“非国家行为者不太可能窃取模型权重，高级威胁行为者（例如国家）无法在不花费大量费用的情况下窃取模型权重”——并描述他们期望采取的安全措施采取以实现这一目标。</li><li>要求在扩展到 ASL-3 之前发布 ASL-4 的定义和评估协议并由董事会批准。</li><li>提供有关触发 ASL-4 的条件的初步指导以及在 ASL-4 运行所需的必要保护措施（包括针对动机状态的安全性，我预计这极难实现，以及需要新颖科学的肯定性安全案例） ）。</li></ul><p>我希望一些组件会随着时间的推移而改进：</p><ul><li>现在指定具体评估的另一面是它们非常粗略和初步。我认为值得努力进行更好的评估，并与风险有更清晰的关系。</li><li>为了让外部利益相关者对 Anthropic 的安全性有信心，我认为需要做更多的工作来安排适当的审计和红队。据我所知，这项工作还没有任何人完成，并且需要时间。</li><li> RSP 变更的批准流程由董事会公布和批准。我认为这确保了决策是经过深思熟虑做出的，比没有好得多，但最好有有效的独立监督。</li><li>如果可以更清晰地介绍 ASL-4，那么通过让人们有机会检查和辩论该级别的条件，这将是一项重大改进。如果不是这样，则需要提供更具体的审查或决策过程来决定一组给定的安全、安保和评估措施是否足够。</li></ul><p>我很高兴看到对 RSP 的批评集中在他们未能管理风险的具体方式上。此类批评可以帮助（i）推动人工智能开发人员做得更好，以及（ii）向政策制定者提出我们需要比现有 RSP 更严格的监管要求。也就是说，我认为拥有 RSP 比没有 RSP 好得多，并且不认为在讨论中应该忽略这一点。</p><h2>关于“负责任的扩展”的名称</h2><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。特别是，我认为在灾难性事件发生之前，我们<i>可能</i>会出现更强烈的危险能力迹象，而对保护措施的现实要求<i>可能</i>会导致我们要么管理风险，要么在我们的保护措施明显不足时暂停。这是一个足够大的风险降低，我主要担心的是开发人员是否会真正采用良好的 RSP 并有效实施它们。</p><p>也就是说，我相信即使将风险降低 10 倍，我们仍然面临很大的风险；我认为抱怨私营企业造成1%的灭绝风险不“负责任”是合理的。我还认为 RSP 的基本理念应该吸引对风险有不同看法的人，而更悲观的人可能会认为，即使所有开发人员都实施非常好的 RSP，仍然存在 10% 以上的全球灾难风险。</p><p>一方面，我认为人工智能开发人员明确声明并捍卫他们正在以负责任的方式开发技术是有好处的，并且当他们无法捍卫这一声明时很容易受到阻力。另一方面，我认为，如果将扩展称为“负责任的”会给（或看起来试图给人）一种错误的安全感，无论是关于剩余的灾难性风险还是关于灾难性风险之外的社会影响，那就不好了。</p><p>因此，“负责任的扩展政策”可能不是正确的名称。我认为重要的是实质内容：开发人员应明确制定危险能力与必要防护措施之间关系的路线图，应描述衡量危险能力的具体程序，并应在防护措施满足但能力超出危险限制的情况下制定应对措施路线图。</p><br/><br/> <a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation<guid ispermalink="false"> dxgEadrEBkkE96CXr</guid><dc:creator><![CDATA[paulfchristiano]]></dc:creator><pubDate> Tue, 24 Oct 2023 22:21:21 GMT</pubDate> </item><item><title><![CDATA[Poker, Religion and Peace]]></title><description><![CDATA[Published on October 24, 2023 9:21 PM GMT<br/><br/><p>扑克中发生了一个有趣的现象：</p><p>假设您正在玩德州扑克并且您拿到了口袋 A。如果您不熟悉扑克，这可能是您能拿到的最好牌局。在发出任何其他牌之前，此时获得尽可能多的钱是博弈论最优（GTO）的（当然，你也不想让每个人都弃牌）。</p><p>假设你的对手全押。轻松跟注。<br></p><p>现在，尽管您拥有最好的牌，但在这种情况下，口袋 A 仍然会输掉大约 20% 的时间。</p><p>高级扑克玩家都明白这一点；<strong>即使他们失败了，他们也能在自己的决定中找到平静</strong>。他们知道他们做出了最好的举动，这就是他们所能希望做的。<br></p><p>几个月前，我读旧约时，读到一段话让我想起了这一点：<br></p><p> “不要因恶人而烦恼，也不要嫉妒作恶的人；因为他们像草一样快要枯干，像青菜一样快要枯死。要信靠耶和华，行善；住在这地，享受安全的草场。以耶和华为乐，他会赐给你心中所求的。将你的道路交托给耶和华；信靠他，他会这样做：他会让你公义的奖赏像黎明一样闪耀，你的平反如同正午的太阳。要安静在主面前，耐心地等待他；当人们成功时，当他们实施他们的邪恶计划时，不要担心。”</p><p></p><p>换句话说：</p><p>坏人可能会做坏事并成功。</p><p> （人们可能会以非同色下注 2/7 并获胜。）<br></p><p>好人可能会做好事，但也会失败。</p><p> （人们可能会玩口袋 A 并输掉。）</p><p></p><p>以真理和善良的精神生活。</p><p> （玩《GTO》。）</p><br/><br/><a href="https://www.lesswrong.com/posts/Aeg3nsBJFt72ehgEg/poker-religion-and-peace#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Aeg3nsBJFt72ehgEg/poker-religion-and-peace<guid ispermalink="false"> Aeg3nsBJFt72ehgEg</guid><dc:creator><![CDATA[sleno]]></dc:creator><pubDate> Tue, 24 Oct 2023 21:30:54 GMT</pubDate></item></channel></rss>