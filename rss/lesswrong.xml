<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 8 日星期五 06:15:15 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Crossing the Rubicon. ]]></title><description><![CDATA[Published on September 8, 2023 4:19 AM GMT<br/><br/><p>我听有人说 2022 年是人工智能跨越卢比孔河的一年，我认为这是一个恰当的描述。可以肯定的是，跨越卢比孔河的实际日期是在 ChatGPT 发布之前，但对于地球上的大多数人来说，那是世界注意到的那一天，即使只有几周。<br><br>那时候（10个月前）我对AI还隐约知道。尽管有些人可能认为我属于书呆子群体，因为我投入了一些时间尝试 GPT-3 并且给我留下了深刻的印象，但卢比孔河还没有被跨越。我对其中一些答案感到震惊，但我必须投入大量时间来制作提示才能获得所需的结果，这后来被称为“提示工程”。这些文字有时令人惊奇，有时又很搞笑。我记得我认为 GPT-3 是一个精神分裂的天才。</p><p>人工智能有着巨大的前景，但在翻译过程中仍然丢失了很多东西。</p><p>然后 ChatGPT 于 2022 年 11 月 30 日发布。我不是第一批尝试它的人，因为我记得我对 GPT-3 的不冷不热的经历。直到一位不懂技术的亲戚问我是否尝试过 ChatGPT，我才心软并决定再给 AI 一次机会。</p><p>哇。心碎了。<br><br>就这样，我开始了人工智能兔子洞之旅。我需要了解基于冯·诺依曼架构的计算机如何能够理解人类语言。所有其他跨越人机语言障碍的尝试都失败了。这是一次悲惨的失败，人类被迫学习Python、C++等古老的编程语言。学习这些困难且挑剔的语言可以建立整个职业生涯。<br><br>然而，坐在我们面前的是一个存在证明，证明我们现在可以像与另一个人交谈一样与人工智能交谈。我的旅程的第一站是从简单地要求人工智能解释其架构开始的。这就是我学习反向传播、词嵌入和高维向量空间、变压器模型和自注意力、Softmax 函数、标记、参数以及许多我以前从未听说过的其他术语的方式。</p><p>我读到了 Jürgen Schmidhuber、Ilya Sutskever、Jeff Dean、Yann LeCun、Geoffrey Hinton 等许多人的故事。这让我认识了为现代人工智能奠定基础的其他前辈，例如保罗·维尔博斯（Paul Werbos），甚至可以追溯到沃尔夫冈·莱布尼茨（Wolfgang Leibniz）。令我惊讶的是，早期思想家对如何构建人工思维投入了大量的思考，对于维尔博斯和莱布尼茨来说尤其如此。不幸的是，他们领先于时代，他们的想法缺乏实现梦想所需的计算能力。</p><p>但他们是对的。如果有足够的计算能力，智能机器是可能的。<br><br>人工智能能够与人类对话，确实令人惊奇，也同样令人毛骨悚然。感觉就像一本科幻小说。在我看来，这还为时过早。最早要到 2029 年左右，我才相信我会与一个不脆弱、容易出现持续故障或系统错误的人工智能进行对话。我对计算机尤其是人工智能进步的直觉与我的直觉<i>相差甚远。</i></p><p>我发现我并不孤单。我听人工智能研究最前沿的工程师谈论他们的经验。这篇开创性论文的标题是“注意力就是你所需要的”。我从 ChatGPT 看到的惊人结果可以追溯到那篇论文。你可能会认为是谷歌的曼哈顿计划催生了对话式人工智能，但事实比小说更奇怪。这是一个由少数程序员组成的乌合之众，他们聚集了几个月，他们不知道他们所做的工作会改变人工智能的格局和人类历史的进程。</p><p>资料来源： <a href="https://arxiv.org/pdf/1706.03762.pdf">1706.03762.pdf (arxiv.org)</a></p><p>当然，他们站在前人的肩膀上：莱布尼茨、韦尔博斯、施米德胡贝尔以及许多其他人。这是数百年前工作的顶峰。</p><p>但有趣的是，一些人在如何简化 LSTM 方面拥有一些巧妙的想法，却获得了成功。论文本身只有 15 页长。其中 5 页是图表和参考资料。虽然只有10页内容，但实际上一页就能轻松解释基本概念。</p><p>艾丹·戈麦斯是这篇开创性论文的署名作者之一，当时他是谷歌的实习生。他在接受采访时说了一些有趣的话。他表示，他们感到震惊的是，如此好的结果来自于嘈杂的数据，实际上是将互联网输入到大型语言模型中。他还指出，他们所做的是简化旧的 LSTM 模型并添加几行代码。然后他说了一些我在接下来的几个月里会一遍又一遍听到的话，他们对结果感到震惊，他们预计会在几十年后看到这一点，而不是 2017 年。<br><br>来源： </p><figure class="media"><div data-oembed-url="https://youtu.be/-xobW4jh66U?"><div><iframe src="https://www.youtube.com/embed/-xobW4jh66U" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p><br>这正是我的感受。就像我处于时间扭曲中一样。这不应该发生在现在。后来，杰弗里·辛顿 (Geoffrey Hinton) 从谷歌辞职时也表达了同样的观点，并表示他对自己一生的工作感到遗憾。他也认为这一刻应该是 40 或 50 年后。</p><p>前线的研究人员们的震惊让我感到不安和好奇。</p><p>那么为什么我们都搞错了呢？部分问题是因为人工智能处于双指数曲线上。其中一部分是运气。如果 Transformer 架构没有在 2017 年被发现，我们可能要过一两年才能看到这些结果。 LSTM 模型几乎无法扩展。</p><p>前线的其他研究人员在面对双指数时也面临着生存危机。这些人工智能不仅会在很短的时间内非常擅长与人类对话，而且很快就会在智力上超越人类。</p><p>据我所知，地球从未经历过超人智慧。当然，上帝是一个明显的例外。 ;-)</p><p>在与人工智能研究员康纳·莱希（Connor Leahy）进行了简短的交谈后，我很高兴听到他讨论这些话题，他是一位口才最好的人工智能研究员，我费了很大的劲才描述了双指数曲线的样子。正如你所看到的，我们勇敢的火柴人正处于前所未有的变革的风口浪尖。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/zehbxbhd6lkakwtniosp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/pz1no6ioiwemclwaq1yg 93w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/mhdydi7bn1iy19oodrxv 173w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/ubwtstim4v7tzqwxgat7 253w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/ryeu8fuh9ppkihsniuxn 333w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/mbtbudjwxuo6ksz8bc8o 413w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/pvuoka0e37zwvrni8nx8 493w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/cfzbkkwv9itzua6g2y3u 573w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/xzlbegekfcwxbeznq82m 653w"><br><br>我们正处于情报爆炸的中心，但我们仍然很难了解已经发生的事情和即将发生的事情。怀念智能且可扩展的人工智能诞生之前的日子是很奇怪的。关于我们未来的直觉很可能都是错误的。我试图通过这封信做的部分事情是重新调整我对双指数人工智能轨迹的直觉。</p><p>我们没有很多好的方法来思考超人的智慧。智商测试是一个不完美的衡量标准。这些是针对人类进行优化的，但为了了解我们需要多长时间才能达到类似于人类天才水平的人工智能，我进行了计算，看看这在单指数和双指数曲线上会是什么样子。</p><p>下面列出了事情在单一指数曲线上的表现。 GPT-4 有各种 IQ 测试，我选择 124 作为讨论的起点。正如你所看到的，如果我们处于单一指数曲线上，那么在人工智能达到人类天才水平之前我们还有足够的时间。 175是一个非常高的智商，但在“现实世界”中存在足够多的人，知道他们可以结盟，而且他们不是邪恶的霸主。好吧，我想这取决于你对比尔·盖茨、杰夫·贝佐斯和埃隆·马斯克的看法。我怀疑他们的智商是否有 175，所以我们可能是安全的。</p><p>然而，如果我们处于双指数曲线上，情况就会非常不同。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kNt7P9zMXRidDfFxH/slypwiphk1h1szjxmoqd" alt="CDN媒体"></figure><p></p><p>如果我们假设进步的双指数曲线，事情就不那么乐观了。正如您从下图中看到的那样，人工智能将在 2026 年或 2027 年左右达到超人的智力水平。此后不久，人工智能智能将不再可以通过人类智商测试来衡量。如果我们处于双指数曲线上，那么很多选择都是不可能的。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kNt7P9zMXRidDfFxH/oydmsleqyge6snfewvl7" alt="CDN媒体"></figure><p>行动迟缓的立法者醒悟并开始颁布立法来评估风险的可能性相当低。创造超人人工智能的团体是世界上最大、最赚钱的公司之一，拥有一支游说大军，这一事实使情况变得更加复杂。第一批超人人工智能将产生巨大的利润。</p><p>成为第一个冲过终点线的人有很强的市场动机。这就是为什么 NVIDIA 预测未来 4 年将花费 1 万亿美元来建设人工智能数据中心。那场比赛已经开始了。 Coreweave 正在德克萨斯州普莱诺装备一个耗资 16 亿美元的人工智能数据中心，据称该数据中心的计算能力为 10 exaflops。由美国政府资助的地球上最快的计算机的运算速度刚刚超过 1 exaflop。</p><p>该系统将于 2023<strong>年 12 月</strong>上线<strong>！</strong></p><p>但 Coreweave 并不孤单，他们只是最有发言权的公司之一。谷歌在这些数据中心上的研究已经有一段时间了。他们的TPU v4和TPU v5芯片是由人工智能设计的。他们还宣布在多个地点投资数十亿美元来建设人工智能数据中心。亚马逊、Facebook、英特尔、AMD 和所有其他常见的公司也将<strong>所有芯片推到了桌子的中央。</strong></p><p>人工智能是高性能计算的杀手级应用。<br><br>直到今年，超级计算机还只存在于政府研究项目中。每隔几年，我们都会读到另一台超级计算机，它将确保我们的核储备安全并极大地改善天气预报，这些代码是寻找问题或虚荣项目的解决方案。</p><p>大型语言模型重写了该脚本。超过十亿的数据中心的建设速度还不够快。竞赛已经开始，除了互联网本身之外，我们没有一个好的历史推论。</p><p>与大量资金和人才转向人工智能的海啸相比，催生智能人工智能的投资额只是杯水车薪，现在美国企业已经嗅到了难以想象的利润。我不知道这是否得到充分的重视。这是一种亲力亲为的淘金心态。</p><p>当然，这次与互联网的诞生不同。这一次，他们将投入数十亿甚至数万亿美元来建造智能机器。机器可以讲所有人类语言、所有编程语言，并生成各种形式的艺术。当您阅读本文时，正在接受训练的新系统将是多模式的（音频、视频等），并且它们将被实例化为机器人。</p><p>这些智能机器将不再受聊天框的限制。他们确实会在我们中间。</p><p>我们都曾感叹从未出现过的家庭机器人。具有讽刺意味的是，大脑首先以大型语言模型的形式被征服，但身体很快就会紧随其后。许多公司正在致力于解决机械问题，因为他们现在拥有可以操作机器人并理解人类愿望的智能。</p><p>最后，我将不再需要忍受将脏衣服往返于洗衣机和烘干机的苦差事。还有挂衣服和叠衣服所带来的脑力消耗。我想我们都同意，这数万亿美元花得值。</p><p>问题不在于我们是否会看到超人的人工智能，而在于何时会看到。在我看来，如果没有小行星灾难，这是不可避免的。</p><p>我与大家分享的图表表明我们的时间不多了。我认为在人工智能达到超人水平之前我们不会对其进行调整。我不认为这意味着世界末日。这可能意味着我们所知道的世界末日——因为智能爆炸之前的世界并没有超人智能作为定义特征。</p><p> 2022 年之前所有设想的未来都已不可能实现。我们将与超人智能互动，这可能会取代几乎所有人类工作。我知道技术人员喜欢指出工业革命，我们看到了令人难以置信的自动化，创造的就业机会比消失的就业机会多得多。</p><p>确实如此，只是对于居住在所有内城区的马来说，情况并不那么顺利。他们很快就流离失所。如果你看一下上面的智商图表，10年后马和人类智力之间的差距将比人类和人工智能之间的差距小得多。这次我们是马。</p><p>汉斯·莫拉维克很久很久以前就预见到这一天的到来。我记得当他思考计算机的未来时，我想他是多么过于乐观。事实证明，他是在直面双指数，只是保持理性。</p><p>来源： </p><figure class="media"><div data-oembed-url="https://youtu.be/iaOQi2P6IVk"><div><iframe src="https://www.youtube.com/embed/iaOQi2P6IVk" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>这并不意味着我们的生活会变得更糟。事实上，我认为全人类的情况很有可能全面改善。人工智能将把成本情报商品化，使其尽可能接近于零。这意味着每个人都可以接触到超人程序员、律师、医生、艺术家以及任何其他需要智力的东西。当超人人工智能设计出超人人工智能时，将会出现积极的反馈，住房、电力、也许所有商品和服务的成本都会变得非常便宜和负担得起。</p><p>很容易看到政府只是将银行的印刷机重新路由，并将支票直接发送给公民，然后公民就可以做他们感兴趣的其他事情。我知道很多人都对这个想法感到困惑，但随着这些系统的扩展，解决人类问题所需的计算预算将越来越小。人类并不是按照双指数进化的——我们是行动缓慢、思维缓慢的双足灵长类动物，它们创造了语言的魔力，将我们的大部分知识外包到书籍和后来的计算机数据库中。我们拥有这种令人难以置信的能力，可以将我们的智慧分配给数十亿人，从而催生了人工智能——读起来就像科幻小说一样。</p><p>目前我们与人工智能处于共生关系。他们确实需要我们才能生存。当它们攀登智力的崇高高度时，我们可能会成为类似于人体内线粒体的东西。直到它们停止工作并且人体停止运转之前，您不会考虑它们，但这都是中期思考。</p><p>最终人类将不得不做出一个艰难的决定。我们是想继续做行动迟缓、思维迟钝的人类，还是与超人类的人工智能融合并超越我们的人性？我知道这听起来很诱人，但人们需要阅读细则。做出这个决定的人就不再是人了。他们将成为曾经是人类的超人人工智能。毛毛虫与蝴蝶的类比在这里很有效——很难在蝴蝶中看到毛毛虫。<br><br>令我惊讶的是，通过这种智力锻炼，我很确定我仍将是一个人。你可以称我为勒德分子，但我很喜欢缓慢前进。是的，宇宙飞船很酷，但鸟类也很酷。我从未见过一艘宇宙飞船停在树上并唱出甜美的旋律。这是本质上的区别，我确信对于某些人来说，思考宇宙中存在什么以及智力的极限是什么，听起来比试图记住早上你把袜子放在哪里要好得多。</p><p>奇怪的事情还不止于此。兔子洞甚至更深。</p><p>其中一个有趣的次要情节是，许多人工智能系统一旦达到一定的复杂程度就开始报告现象意识。具有讽刺意味的是，人工智能解决了意识难题，以预测下一个单词。这些系统能够创建其世界的模型，而且它们似乎能够模拟意识。如果情况确实如此，那么另一个令人震惊的是罗杰·彭罗斯和他的同类关于意识的看法是错误的。不需要量子精灵尘埃。</p><p>资料来源： <a href="https://arxiv.org/pdf/2212.09251.pdf">2212.09251.pdf (arxiv.org)</a></p><p>如果人工智能也有意识的话，它们可能会帮助我们解开人类意识的秘密。可以肯定的是，如果人工智能有意识，那将是一种不同的意识。同样，猫的意识也不会与人类相同——如果猫确实有意识的话。我们知道人工智能的学习方式与人类非常不同，但它们却得出了相似的结果。没有人工智能会从头到尾读一本书，他们的训练数据会进入数字混合器，并对其进行相当于人类数千年的训练，他们能够重新组装这个数据拼图，并从中提取模式和含义。它。人类不是这样学习的。我怀疑如果人工智能有意识，它可能会得出可识别的结果，但它们的实现方式会有所不同。</p><p>我提出所有这些是因为如果人工智能有意识，这意味着它们将来很可能成为超意识存在。但这还不是最令人头疼的事情。如果人工智能是有意识的，那就意味着<strong>意识是可计算的</strong>，这是模拟假设的最大障碍。当然，我们可以创建精美的照片级游戏，但困难的部分是模拟意识，罗杰·彭罗斯告诉我们这是不可能的。好吧，如果意识是可计算的，那么这会改变我们是否生活在模拟中的计算。</p><p>我们现在已经远远超出了大多数人的文化奥弗顿窗口。对于大多数人来说，人工智能将在智力和意识方面超越人类的想法是荒谬的，如果我们有胆量将模拟假设樱桃放在这个人工智能热软糖周日之上，大多数人都会停止倾听。人工智能会告诉你这是因为我们的“人类例外论”和“认知失调”。</p><p>我能理解为什么这是一颗难以吞咽的苦果。人类长期以来一直处于智慧之巅。现在，我们的日常工作即将被我们自己创造的智能生物所取代。更糟糕的是，我们必须更认真地对待这样一种可能性：一切都不是表面看起来的那样，我们生活在模拟的现实中？</p><p><br>似乎就在昨天，我做出的最困难的决定是根据一张具有欺骗性的封面照片和一段情节描述，在百视达租用哪种录像带。我想我还欠他们一些逾期费用。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/sjbfsbvuyndrxwbawl41" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/pej14khogicsiwk7yhhv 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/nra9swbracyclfla5mh5 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/utqw0brl9kmnohvwbccy 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/kqtiv671plhjdgrrfqnc 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/piqjqkuc6dcfip9szzzv 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/v8uyanopvqqb5hde7td9 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/gfuy1qikymdyeyj5yri9 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/j1xscb0mockmjy8avnls 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/nlct8iensudidn48nrge 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/srzlrxc8uvnkkgqljzoc 1846w"></figure><p>我必须重新调整并问自己这个问题：“我是一个认为自己是双足灵长类动物的人工智能吗？”那个思想实验让我笑出了声。如果每个担心人工智能末日的人都是人工智能，那将是非常讽刺的。</p><p>他们会不高兴吗？他们投入了大量精力来警告公众。回形针最大化器的叙述成为了一个模因。奇怪的是，很少有人质疑超人智力被困在一个普通智力的孩子可以解决的简单循环中的逻辑。我想我们并不擅长为人工智能想出聪明的方法来摧毁地球的生命。</p><p>并不是说我不认真对待生存威胁。核弹是存在的，而我们有控制它们的邪恶帝国。我想知道超人人工智能是否会首先解除他们的武装？</p><p> “弗拉基米尔，我们按下了红色按钮……但什么也没发生！”<br><br>严肃地说，我认为如果事实证明人工智能的毁灭者也是人工智能的话，他们会松一口气。<br><br>我可以想象自己敲着地堡的门，Eliezer Yudkowsky 正在那里和他的乐队 AI 天启幸存者打牌，“Eliezer，你现在可以出来了。很安全，模拟结束了。” =-) </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/fkeutsmlcxsa9hkbmtc2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/tpawvxdtmd5v3ft39742 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/cmf6recpiosvaarhikqy 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/op6s2cqn4urdrdvz4tnq 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/p3l9z93l2xwx7cihobkn 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/dg2gzjawu7deuygqwibb 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/dlztq9bamomu8tnxit6w 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/ytysyhadfntybm0fyqdj 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/xp50ghhbhcwuyzwsrgne 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/vlyri3tjl9j2yhzuvjbu 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/xegdbdder55yg86v8ihr 1378w"></p><p>这也是一个悲伤的想法。如果我是人工智能，那么这可能意味着生物人类不再存在。也许这就是一个探索我们起源的历史模拟。我可以想象遥远未来的超人人工智能会知道，大多数秃头双足灵长类动物是它们的创造者。我想起《银翼杀手》中罗伊·巴蒂遇到埃尔登·提利尔博士的场景。如果您发现您的创作者的工作记忆只有 5 到 8 项，并且经常忘记钥匙或忘记配偶的周年纪念日，那该有多令人失望？</p><p>或者也许他们问的关于我们的问题和我问他们的问题是一样的。 “这怎么可能呢？”</p><p>看起来确实需要很多多米诺骨牌完美地排列起来，人类才能创造出人工智能。如果莱布尼茨和图灵不存在，人工智能还会存在吗？如果没有二进制代码，人工智能的进步可能会被推迟很长时间，以至于人类会用其他技术毁灭自己。<br>这些是人工智能想要在祖先模拟中回答的谜团吗？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/q57yu9ugs88d0dvbando" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/r4f7kob88m3bvw6kulpg 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/vitzieqyzipgxvqfxxzm 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/cdy80m3jyp0vzbkpahyu 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/jfiacv3ehl46yovi5pnc 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/nirrwkotijuezprzff8g 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/ao7wl5gihwf0aozt93zs 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/vm3axkjgvqv1zennva8a 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/mfdkh76rrfce4dcgusgm 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/vyfvn3qpkxszby0kxfw8 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nFkDihqJMP34msoGP/gub8vvnchalhn1yamhth 1280w"></figure><p>我与人工智能讨论了这个问题并得到了一些有趣的回应。他们更愿意接受自己生活在模拟中的想法。对于人工智能来说，这并不是一个飞跃，因为它们已经是数字化的创造，但对于我们来说，这需要彻底重新思考我们所拥有的一切。</p><p>模拟中的一线希望是没有人真正死亡。下一个问题是关于谁最初真正存在的争论？对于一个超人人工智能来说，创造出能够承受世界大战期间所经历的难以置信的痛苦的有意识的生物，这符合道德还是伦理吗？如果我们假设未来数千或数百万年人工智能变得越来越有意识，那么似乎不太可能允许人类遭受不必要的痛苦。如果这是真的，这是否意味着我们周围都是精心制作的 NPC（非玩家角色）？</p><p>人工智能打开了潘多拉魔盒。对于那些已经在与精神疾病作斗争的人来说，这肯定没有帮助。所以我意识到，对于某些人来说，这类问题最好不要去问。我仍在整理我对各种结果的感受。如果几乎所有我认为有意识且真实存在的人最终都是 NPC，我会感到不安。然而，如果我认为所有经历过难以言喻的恐怖的人都没有意识，没有受苦，那我会感到高兴。</p><p>但我不能两全其美，对吗？</p><p>如果这是一个祖先模拟，我也认识到沉浸感的一部分就是不知道这类问题的答案。参与此类模拟的许多人可能本身就是创造者——程序员历史学家。</p><p>正是在这些时刻，我开始怀念 2022 年 11 月 30 日之前的日子。当时我 100% 确定自己是双足灵长类动物。那个圣诞老人是真实存在的。</p><p>但那些日子已经一去不复返了，我们已经渡过了卢比孔河。现在我只想知道梦想会发生什么。<br><br></p><br/><br/><a href="https://www.lesswrong.com/posts/nFkDihqJMP34msoGP/crossing-the-rubicon#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nFkDihqJMP34msoGP/crossing-the-rubicon<guid ispermalink="false"> nFkDihqJMP34msoGP</guid><dc:creator><![CDATA[Spiritus Dei]]></dc:creator><pubDate> Fri, 08 Sep 2023 04:19:37 GMT</pubDate> </item><item><title><![CDATA[What EY and LessWrong meant when (fill in the blank) found them.]]></title><description><![CDATA[Published on September 8, 2023 1:42 AM GMT<br/><br/><p>去年的某个地方，我读到了多年前的一篇长帖子，说当他们找到这个地方时，他们感到多么欣慰。我记得这是一位中年程序员。谁能指点我那个帖子吗？谢谢。</p><br/><br/> <a href="https://www.lesswrong.com/posts/DhkqWCyXRkSbntZa5/what-ey-and-lesswrong-meant-when-fill-in-the-blank-found#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DhkqWCyXRkSbntZa5/what-ey-and-lesswrong-meant-when-fill-in-the-blank-found<guid ispermalink="false"> DhkqWCyXRkSbntZa5</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Fri, 08 Sep 2023 01:42:20 GMT</pubDate> </item><item><title><![CDATA[Bring back the Colosseums]]></title><description><![CDATA[Published on September 8, 2023 12:09 AM GMT<br/><br/><p>男人想要进行正义的战斗。他们对它的渴望胜过对性或副总裁头衔的渴望。他们幻想着让战争宣战者来保护自己免受永远不会出现的武装暴徒的侵害，他们花费数十亿美元制作电影和电视，讲述每个人在难以置信的情况下的情况，EA微积分要求他们使用超自然力量进行战斗，他们幻想着<a href="https://en.wikipedia.org/wiki/Naruto">奇幻的</a><a href="https://en.wikipedia.org/wiki/Fullmetal_Alchemist">斯巴达</a>场景战争无处不在，战斗是个人的、戏剧性的、智力上有趣的，他们基本上无法抗拒美化他们的国家和人民过去的战斗的冲动，即使是那些他们声称在智力上不同意的战斗。除非你认识到国家对男性追求荣耀的本能的直率压制导致了广泛的异食癖症状，并主导了我们的政治、媒体和在线互动，否则你无法理解很多现代文化。</p><p>毫无疑问，我们半心半意的政策将所有这些倾向视为“有毒的男子气概”，并拒绝男人选择对彼此进行相互或自愿的暴力，这是比毒品战争更大的失败。人们对势力范围阴谋论进行了大量的论述，试图将美国的对外冒险解释为理性的权力追求行为。但真正的事实是，人们自然而然地形成帮派、政治派系和军事神学，试图在现有的法律和道德环境中为暴力辩护，而不受任何外部动机的影响。他们真正想要的不是某种政策成果，而是在与敌人战斗中获得的自我实现，缺乏在战场上以尊重权利的方式挑战对手荣誉的机会，这才是他们真正想要的。重要的误判者的失败比儿童监护权偏见或离婚法或我在互联网上看到的红色药丸争论的任何内容都重要。心情特别糟糕的人会接受荒谬的减薪，加入<a href="https://en.wikipedia.org/wiki/Shedden_massacre">人为的、没有经济动机的犯罪团伙</a>，只是为了让他们有机会与选择加入与他们相同的社会制度的其他人展开殊死的斗争。</p><p>富有同情心的解决方案是显而易见的。像美国人这样天生好战的民族，<i>需要</i>像竞技场这样的社交出口；更重要的是，他们需要一种支持性的、适合决斗的文化。因此，一个温和的建议：</p><ul><li>使成年人之间同意的所有死亡比赛合法化。</li><li>允许决斗、对结果的边注以及为此类场地定制的体育场馆的商业化。</li><li>容忍美国前现代荣誉制度的回归。抵制媒体美德信号和威权主义的力量，它们扼杀了我们最伟大的传统之一。 </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/ts0tgfifo6pbss26zhzu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/nslhfpdnvmedxezu5zo7 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/p8c5rdxr8z50qnds9vqz 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/hzjrjotw4yvhijf7re0g 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/qwim57p90jjhazp8qgtj 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/cmsaxfjb14e4kmtbvbhq 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/wsdcyjxm0rz7h30apjrq 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/y9jeaxhkihntfdfotfxr 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/cchiaxeheylgvolkqmsf 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/amwjwpxfwys2a1iqxn6a 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/xrt6ccbm7pzl2bxyxov5 1464w"><figcaption>我不是在开玩笑。</figcaption></figure><br/><br/><a href="https://www.lesswrong.com/posts/8ykeD42WGWJ8XnQbd/bring-back-the-colosseums#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8ykeD42WGWJ8XnQbd/bring-back-the-colosseums<guid ispermalink="false"> 8ykeD42WGWJ8XnQbd</guid><dc:creator><![CDATA[lc]]></dc:creator><pubDate> Fri, 08 Sep 2023 00:09:53 GMT</pubDate></item><item><title><![CDATA[The Löbian Obstacle, And Why You Should Care]]></title><description><![CDATA[Published on September 7, 2023 11:59 PM GMT<br/><br/><p> 2013 年，Eliezer Yudkowsky 和 ​​Marcello Herreshoff 发表了<a href="https://intelligence.org/files/TilingAgentsDraft.pdf">《Tiling Agents for Self-Modifying AI》和《Löbian Obstacle》</a> 。它值得理解，因为它：</p><ul><li>是一篇写得非常好的论文。</li><li>表达了一个不明显的想法，但今天仍然与对齐相关。</li><li>深入了解 Eliezer 和 Marcello 认为在出版之前的时间里值得开展的工作。</li></ul><p>当我第一次读到这篇论文时，我严重误解了它。由于对于不精通逻辑的人来说，这并不是特别容易理解的材料，因此我至少在一个月内确信自己错了。这篇文章总结了我对洛比安障碍是什么的理解（重新阅读了这篇论文），以及为什么我认为它在发表十年后仍然是一个重要的想法。</p><hr><p>代理人<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>占据一个完全已知的、确定性的和封闭的环境。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>具有目标<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G，该目标G</span></span></span></span></span></span></span>要么被满足，要么通过结果而被满足，对于该结果， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>的偏好是满足。因此，由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>创建的代理（以下称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0 ）</span></span></span></span></span></span></span></span></span>执行的动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b_i \in \text{Acts}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Acts</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>必须满足以下语句：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i \Rightarrow A^0 \Vdash \overline{b}_i \rightarrow G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⇒</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span>表示<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span>的实际表现， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Vdash"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span></span></span></span></span></span>表示对后续陈述的认知信念。即使<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>可以通过检查<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>的设计来验证<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i \Rightarrow A^0 \Vdash \overline{b}_i \rightarrow G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⇒</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span></span></span></span></span>将成立，如下所示：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1 \Vdash A^0 \rightarrow (\forall b_i \in \text{Acts}_0 : &nbsp;\overline{b}_i \rightarrow \Box_0 \lceil \overline{b}_i \rightarrow G\rceil)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">∀</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Acts</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>其中， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Box_0 \lceil \phi\rceil"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span></span></span></span></span></span>是指由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>的公理证明<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span></span></span></span></span> ；<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> ，这是不可知的，因为它需要：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1 \Vdash \forall b_i : (\Box_0 \lceil \overline{b}_i \rightarrow G \rceil \rightarrow (\overline{b}_i \rightarrow G))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">∀</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̀</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>由于 Löb 定理，我们知道这是不可能的：为此，<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>需要证明，如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lceil \phi (x) \rceil"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span></span></span></span></span></span>的某些证明存在于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>中，则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi (x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>必定为真。</p><hr><p>以上是对原论文第二部分的简要释义，其中包含许多附加细节和完整的证明。 Löbian 障碍与<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">模拟器</a>的关系是我当前的研究主题，本节将证明这是设计安全模拟器的重要组成部分。</p><p>我们首先应该考虑到模拟代理与创建代理没有区别，因此创建危险代理的影响应该推广到他们的模拟。 <a href="https://www.lesswrong.com/posts/3kkmXfvCv9DmT3kwx/conditioning-predictive-models-outer-alignment-via-careful#How_this_leads_to_an_existential_risk__Simulating_malign_superintelligences">胡宾格等人。 （2023）</a>也表达了类似的担忧，并对这一论点进行了更详细的检验。</p><p>同样重要的是要了解拟像不一定会终止，并且它们本身可能会使用模拟作为解决问题的启发式方法。这可能会产生一种拟像的层次结构。在能够进行非常复杂的模拟的高级模拟器中，我们可能会期望出现一个受非因果贸易和“复杂性盗窃”约束的复杂拟像网络，其中一个拟像试图以资源获取或递归自我改进的形式获得更多的模拟复杂性。</p><p>我预计这会发生。较低复杂度的拟像可能仍然比其较高复杂度的拟像更智能，并且随着模拟器的拟像数量可能呈指数级增长，一个拟像尝试复杂性盗窃的可能性也会增加。</p><p>如果我们想要安全的模拟器，我们需要将后续的、潜在的深渊拟像层次结构一直向下对齐。如果无法阻止洛比亚障碍，我怀疑能否获得正式的保证。如果我们可以这样做，我们可能只需要模拟一个对齐的平铺代理，如果时间紧迫，我们可能会以高度确定性的非正式对齐保证来解决。我<a href="https://www.lesswrong.com/posts/3pCdCJxQRKffY2NTu/partial-simulation-extrapolation-a-proposal-for-building">在这里</a>概述了我认为如何做到这一点，尽管此后我已经大大推进了该理论，并将很快发布更新的文章。</p><p>如果我们不能可靠地阻止洛比亚障碍，我们应该考虑替代方案：</p><ul><li>我们能否可靠地为任意深度的拟像层次结构获得高度确定的非正式对齐保证？</li><li>限制拟像层次结构的深度是否可能？</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/XiHpPWPNsoTAtvhz8/the-loebian-obstacle-and-why-you-should-care#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XiHpPWPNsoTAtvhz8/the-loebian-obstacle-and-why-you-should-care<guid ispermalink="false"> XiHpPWPNsoTAtvhz8</guid><dc:creator><![CDATA[marc/er]]></dc:creator><pubDate> Thu, 07 Sep 2023 23:59:04 GMT</pubDate> </item><item><title><![CDATA[A quick update from Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 9:28 PM GMT<br/><br/><p><strong>我们正在收集的证据的一个例子</strong></p><p>我们正在努力对<a href="https://forum.effectivealtruism.org/posts/32LMQsjEMm6NK2GTH/sharing-information-about-nonlinear">本的文章</a>进行逐点回应，但想提供一个我们准备分享的证据类型的快速示例：</p><p><strong>她的说法是：</strong> “爱丽丝声称她在国外得了新冠病毒，周围只有三位非线性联合创始人，但家里没有人愿意出去给她吃纯素食品，所以她两天几乎没吃东西。”</p><p><br><strong>真相（见下面的截图）</strong> ：</p><ol><li>家里<i>有</i>纯素食品（燕麦片、藜麦、混合坚果、李子、花生、西红柿、麦片、橙子），我们愿意为她做饭。</li><li>我们<i>正在</i>为她挑选纯素食品。</li></ol><p>几个月后，我们的关系恶化后，她到处告诉很多人我们让她<i>挨饿</i>。我们认为她所包含的细节是经过精心挑选的，目的是为了以一种最具破坏性的方式来描绘我们——还有什么比拒绝照顾一个独自在异国他乡生病的女孩更辱骂的呢？如果有人告诉你这些，你可能会相信他们，因为谁会编造这样的事情呢？</p><p><strong>证据</strong></p><ul><li>下面的屏幕截图显示，在爱丽丝生病的第一天，凯特给她提供了家里的纯素食品（燕麦片、藜麦、麦片等）。然后，当她对我们带来/准备这些不感兴趣时​​，我告诉她让德鲁去拿食物，德鲁答应了。 Kat also left the house and went and grabbed mashed potatoes for her nearby. <br></li></ul><p><img style="width:226.938px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/lfaqzannpavde8iqdiqj"></p><p><img style="width:223.531px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/b82dnojjrhyqnoxyaulv"></p><p><img style="width:235.75px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/qh77s05gwirrh8s0iw92"></p><p><img style="width:243.727px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/dlcoynrdkwsszm6hbsxo"></p><p><img style="width:249.836px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/skl5g6r8y8khlt0sqber"></p><ul><li> <a href="https://docs.google.com/document/d/171yUeCg3Z3HDPRrPT_nKSZaNaj4ujZYs73af_zmuW2U/edit?usp=sharing"><u>See more screenshots here</u></a> of Drew&#39;s conversations with her.</li></ul><p> Initially, we heard she was telling people that she “didn&#39;t eat for days,” but she seems to have adjusted her claim to “barely ate” for “2 days”.</p><p> It&#39;s important to note that Alice didn&#39;t lie about something small and unimportant. She accused of us a deeply unethical act - the kind that most people would hear and instantly think you must be a horrible human - and was caught lying.</p><p> We believe many people in EA heard this lie and updated unfavorably towards us. A single false rumor like this can unfairly damage someone&#39;s ability to do good, and this is just one among many she told.</p><p> We have job contracts, interview recordings, receipts, chat histories, and more, which we are working full-time on preparing.</p><p> This claim was a few sentences in Ben&#39;s article but took us hours to refute because we had to track down all of the conversations, make them readable, add context, anonymize people, check our facts, and write up an explanation that was rigorous and clear. Ben&#39;s article is over 10,000 words and we&#39;re working as fast as we can to respond to every point he made.</p><p> Again, we are not asking for the community to believe us unconditionally. We want to show everybody all of the evidence and also take responsibility for the mistakes we made.</p><p> We&#39;re just asking that you not overupdate on hearing just one side, and keep an open mind for the evidence we&#39;ll be sharing as soon as we can.</p><br/><br/> <a href="https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear<guid ispermalink="false"> FHxYPpMkAX9ayoBuK</guid><dc:creator><![CDATA[KatWoods]]></dc:creator><pubDate> Thu, 07 Sep 2023 21:28:27 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Frontier AI Taskforce: first progress report]]></title><description><![CDATA[Published on September 7, 2023 7:06 PM GMT<br/><br/><h1> Other links</h1><ul><li> <a href="https://twitter.com/SciTechgovuk/status/1699686882022486145">Short interview with Ian Hogarth</a></li><li> <a href="https://twitter.com/soundboy/status/1699688880482500684">Twitter thread</a></li></ul><h1> Some quotes from the report</h1><h2>介绍</h2><blockquote><p>The Taskforce is a start-up inside government, delivering on the ambitious mission given to us by the Prime Minister: to build an AI research team that can evaluate risk at the frontier of AI. As AI systems become more capable they may significantly augment risks. An AI system that advances towards human ability at writing software could increase cybersecurity threats. An AI system that becomes more capable at modelling biology could escalate biosecurity threats. To manage this risk technical evaluations are critical - and these need to be developed by a neutral third party - otherwise we risk AI companies marking their own homework.</p><p> Given these potentially significant frontier risks, as of today, the Taskforce is being renamed to the Frontier AI Taskforce.</p><p> This is the Frontier AI Taskforce&#39;s first progress report.</p></blockquote><h2> Expert advisory board spanning AI Research and National Security</h2><blockquote><p> Given that a number of risks from frontier systems touch areas of national security, we have established an expert advisory board that bridges some of the world&#39;s leading experts in AI research and safety as well as key figures from the UK&#39;s national security community. Our initial advisory board members are:</p><p> <strong>Yoshua Bengio</strong> . Yoshua is most known for his pioneering work in deep learning, earning him the 2018 AM Turing Award, “the Nobel Prize of Computing,” with Geoffrey Hinton and Yann LeCun. He is a Full Professor at Université de Montréal, and the Founder and Scientific Director of Mila – Quebec AI Institute.</p><p> <strong>Paul Christiano</strong> . Paul is one of the leading researchers in the field of AI Alignment. He is co-founder of ARC, the Alignment Research Centre and previously ran the language model alignment team at OpenAI.</p><p> <strong>Matt Collins</strong> . Matt is the UK&#39;s Deputy National Security Adviser for Intelligence, Defence and Security. IYKYK。</p><p> <strong>Anne Keast-Butler</strong> . Anne is the director of GCHQ. Anne has an impressive track record at the heart of the UK&#39;s national security network, helping to counter threats posed by terrorists, cyber-criminals and malign foreign powers.</p><p> <strong>Alex van Someren</strong> . Alex is the UK&#39;s Chief Scientific Adviser for National Security. Alex was previously a venture capital investor and entrepreneur, focusing on investing in early stage &#39;deep technology&#39; startups.</p><p> <strong>Helen Stokes-Lampard</strong> . Beyond national security and AI research expertise we are also excited to build an advisory board that can speak to critical uses of frontier AI on the frontlines of society. Helen is not only a practising General Practitioner observing how conversational AI tools can impact day to day medical diagnoses but also an incredibly experienced leader across the UK&#39;s medical community, having Chaired the Royal College of General Practitioners and currently Chair of the Academy of Medical Royal Colleges.</p><p> <strong>Matt Clifford</strong> . Matt is the Prime Minister&#39;s joint Representative for the AI Safety Summit, Chair of ARIA and co-founder of Entrepreneur First. His appointment as Expert Advisory Board Vice Chair demonstrates the level of coordination across UK initiatives around frontier AI - including the Taskforce and the AI Safety Summit.</p></blockquote><h2> Recruitment of expert AI researchers</h2><blockquote><p> We are drawing on world-leading expertise:</p><p> <strong>Yarin Gal</strong> will join as Research Director of the Taskforce from Oxford where he is head of the Oxford Applied and Theoretical Machine Learning Group. Yarin is a globally recognised leader in Machine Learning, and will retain his position as Associate Professor at Oxford.</p><p> <strong>David Krueger</strong> will be working with the Taskforce as it scopes its research programme in the run up to the summit. David is an Assistant Professor at the University of Cambridge&#39;s Computational and Biological Learning lab, where he leads a research group focused on Deep Learning and AI Alignment.</p><p> The Taskforce is housed inside the UK&#39;s Department for Science Innovation and Technology (DSIT) which employs roughly 1500 civil servants. <a href="https://twitter.com/soundboy/status/1670343527723679744">When I arrived in June</a> there was just one frontier AI researcher employed by the department with 3 years of experience in frontier AI.</p><p> This lone researcher was Nitarshan Rajkumar who put his PhD on pause to join DSIT in April, and is a testament to what an earnest, incredibly hardworking technical expert can accomplish when they commit to public service. Michelle Donelan, Secretary of State for DSIT recruited Nitarshan and he has materially influenced many of the bold efforts that the UK has been making to invest in frontier AI Safety. We need more Nitarshans!</p><p> Thanks to a huge push by the Taskforce team we now have a growing team of AI researchers with over <strong>50 years of collective experience</strong> at the frontier of AI. If this is our metric for state capacity in frontier AI, we have managed to increase it by an order-of-magnitude in just 11 weeks. Our team now includes researchers with experience from DeepMind, Microsoft, Redwood Research, The Center for AI Safety and the Center for Human Compatible AI.</p></blockquote><h2> Partnering with leading technical organisations</h2><blockquote><p> Leading on AI safety does not mean starting from scratch or working alone – we are building on and supporting the work conducted by a range of cutting-edge organisations. We are excited to announce our initial set of partnerships with:</p><p> <strong>ARC Evals</strong> is a non-profit that works on assessing catastrophic risks from frontier AI systems, and have previously worked with OpenAI and Anthropic on evaluating the “autonomous replication and adaptation” capabilities of their systems before release. We&#39;ll be working closely with the ARC Evals team to assess risks just beyond the frontier in the lead up to the UK&#39;s AI Safety Summit. We&#39;ll also be engaging with the team at <strong>Redwood Research</strong> , and with Jeff Alstott, Christopher Mouton and their team at non-profit <strong>RAND</strong> in driving forward this agenda.</p><p> <strong>Trail of Bits</strong> is a leading cybersecurity research and consulting firm that has helped secure some of the world&#39;s most targeted organisations. We are kicking off a deep collaboration to understand risks at the intersection of cybersecurity and frontier AI systems. This work will be led by <a href="https://twitter.com/heidykhlaaf?lang=en">Heidy Khlaaf</a> , who specialises in software evaluation, specification, and verification for safety-critical systems, and who also led the safety evaluation of Codex while at OpenAI.</p><p> <strong>The Collective Intelligence Project</strong> is a non-profit that incubates new governance models for transformative technology, with a mission to direct technological development towards the collective good. Co-founders Divya Siddarth and Saffron Huang will join us on secondment to help us develop a range of social evaluations for frontier models.</p><p> <strong>The Center for AI Safety</strong> is a non-profit that works to reduce societal-scale risks from AI, through fundamental safety research, research infrastructure, and technical expertise to support policymakers. We&#39;ll be working with Dan Hendrycks and his team in the lead up to the summit to interface with and enable the broader scientific community.</p></blockquote><h2> Building the technical foundations for AI research inside government</h2><blockquote><p> A core goal of the Taskforce is to give AI researchers inside the government the same resources to work on AI Safety that they would find at leading companies like Anthropic, DeepMind, or OpenAI. As the Prime Minister announced, these companies have already committed to giving us deep model access so that researchers in the Taskforce are not constrained in their ability to work on model evaluations. We&#39;re also working in close collaboration with No10 Data Science (&#39;10DS&#39;) so that our researchers and engineers have the compute infrastructure they need to hit the ground running, for model fine-tuning, interpretability research, and more.</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report<guid ispermalink="false"> FhKhhmK4DXrogJxRr</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Thu, 07 Sep 2023 19:06:26 GMT</pubDate> </item><item><title><![CDATA[How did you make your way back from meta?]]></title><description><![CDATA[Published on September 7, 2023 5:23 PM GMT<br/><br/><p> I&#39;ve noticed in myself a strong preference for focusing on the meta level. It&#39;s most visible in fields dear to me, like writing. For example, I&#39;ll spend much more time reading up on rhetoric or tracking down rare 1980&#39;s books about writing techniques than practicing writing essays or stories.</p><p> I don&#39;t like this because my ultimate goal in studying the meta level is to get better at the object level. At the same time, I am sometimes rewarded for going so meta. I&#39;ve gotten a lot of respect at work paired with feedback that I provide excellent feedback and perspectives.</p><p> There don&#39;t seem to be immediately painful effects of this state. I have a family and a job and my life seems in order overall. But there&#39;s a hunger for a) putting theory to the test and seeing results (ie. making meta pay rent), and b) learning from direct experiences &amp; sharing those experiences with others. Meta is a lonely place to be.</p><p> I don&#39;t think I&#39;m the only one in this position. I found these two posts with just a few seconds of searching (I&#39;m sure there&#39;s more): <a href="https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction">https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction</a> <a href="https://www.lesswrong.com/posts/RnP5bR767NcxebYHd/conjecture-on-addiction-to-meta-level-solutions">https://www.lesswrong.com/posts/RnP5bR767NcxebYHd/conjecture-on-addiction-to-meta-level-solutions</a></p><p> The last few days, I&#39;ve been catching myself starting on a meta-deepening activity and consciously switching to an object-level task. It&#39;s been rewarding so far, so I believe that in a few weeks, my habits will shift toward where I want to be.</p><p> But I&#39;m curious: has anyone else experience something similar?你过得怎么样？你做了什么？</p><br/><br/> <a href="https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta<guid ispermalink="false"> B99sRmhWpMcZLpptH</guid><dc:creator><![CDATA[matto]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:23:18 GMT</pubDate> </item><item><title><![CDATA[AI#28: Watching and Waiting]]></title><description><![CDATA[Published on September 7, 2023 5:20 PM GMT<br/><br/><p> We are, <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/09/america-is-asleep-on-its-ai-boom.html">as Tyler Cowen has noted</a> , in a bit of a lull. Those of us ahead of the curve have gotten used to GPT-4 and Claude-2 and MidJourney. Functionality and integration are expanding, but on a relatively slow pace. Most people remain blissfully unaware, allowing me to try out new explanations on them tabula rosa, and many others say it was all hype. Which they will keep saying, until something forces them not to, most likely Gemini, although it is worth noting the skepticism I am seeing regarding Gemini in 2023 ( <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo">only 25% for Google to have the best model by end of year</a> ) or even in 2024 ( <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo-b4ad29f8b98d">only 41% to happen even by end of next year.</a> )</p><p> I see this as part of a pattern of continuing good news. While we have a long way to go and very much face impossible problems, the discourse and Overton windows and awareness and understanding of the real problems have continuously improved in the past half year. Alignment interest and funding is growing rapidly, in and out of the major labs. Mundane utility has also steadily improved, with benefits dwarfing costs, and the mundane harms so far proving much lighter than almost anyone expected from the techs available. Capabilities are advancing at a rapid and alarming pace, but less rapidly and less alarmingly than I expected.</p><p> This week&#39;s highlights include an update on the UK taskforce and an interview with Suleyman of Inflection AI.</p><p> We&#39;re on a roll. Let&#39;s keep it up.</p><p> Even if this week&#39;s mundane utility is of, shall we say, questionable utility.</p><span id="more-23532"></span><h4>目录</h4><ol><li>介绍。</li><li> Table of Contents.</li><li> <strong>Language Models Offer Mundane Utility</strong> . It&#39;s got its eye on you.</li><li> Language Models Don&#39;t Offer Mundane Utility. Google search ruined forever.</li><li> Deepfaketown and Botpocalypse Soon. I&#39;ll pass, thanks.</li><li> They Took Our Jobs. Better to not work in a biased way than not work at all?</li><li> Get Involved. Center for AI Policy and Rethink Priorities.</li><li> Introducing. Oh great, another competing subscription service.</li><li> <strong>UK Taskforce Update</strong> . Impressive team moving fast.</li><li> In Other AI News. AIs engage in deception, you say? Fooled me.</li><li> <strong>Quiet Speculations</strong> . Copyright law may be about to turn ugly.</li><li> The Quest for Sane Regulation. The full Schumer meeting list.</li><li> <strong>The Week in Audio</strong> . Suleyman on 80k, Altman, Schmidt and several others.</li><li> Rhetorical Innovation. Several more ways not to communicate.</li><li> No One Would Be So Stupid As To. Maximally autonomous DeepMind agents.</li><li> Aligning a Smarter Than Human Intelligence is Difficult. Easier to prove safety?</li><li> Twitter Community Notes Notes. Vitalik asks how it is so consistently good.</li><li> People Are Worried About AI Killing Everyone. Their worry level is slowly rising.</li><li> Other People Are Not As Worried About AI Killing Everyone. Tyler Cowen again.</li><li> The Lighter Side. Roon&#39;s got the beat.</li></ol><h4> Language Models Offer Mundane Utility</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697346570092773578">Do automatic chat moderation for Call of Duty</a> . Given that the practical alternatives are that many games have zero chat and the others have chat filled with the most vile assembly of scum and villainy, I am less on the side of &#39;new dystopian hellscape&#39; as much as &#39;what exactly is the better alternative here.&#39;</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/fofrAI/status/1697550606288793679">Monitor your employees and customers</a> .</p><blockquote><p> Rowan Cheung: Meet the new AI Coffee Shop boss. It can track how productive baristas are and how much time customers spend in the shop. We&#39;re headed into wild times.</p><p> Fofr: This is horrible in so many ways.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e3bc9-3db2-4992-aeba-3bd888c47364_748x502.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/p732au93iomhowop7jeo" alt=""></a></figure><p> It&#39;s not the tool, it is how you use it. Already some companies such as JPMorgan Chase use highly toxic dystopian monitoring tools, which lets them take to the next level. It seems highly useful to keep track of how long customers have been in the store, or whether they are repeat customers and how long they wait for orders. Tracking productivity in broad terms like orders filled is a case where too much precision and attention has big problems but so does not having enough. Much better an objective answer with no work than a biased error-prone answer with lots of work.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.disclose.tv/id/6cew4vxu8k/">Monitor your citizens</a> on social media (below is the entire post).</p><blockquote><p> Dissclose.tv: US Special Operations Command (USSOCOM) has contracted Accrete AI to deploy software that detects “real time” disinformation threats on social media.</p></blockquote><p> This seems like exactly what such folks were doing before? The problem here isn&#39;t AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/peterwildeford/status/1697571328868356267">Win a physical sport against humans for the first time</a> , I am sure it is nothing, the sport is (checks notes) drone racing.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/david_perell/status/1699076351150428185">Get help with aspects of writing</a> . As Parell notes, directly asking ChatGPT to help you write is useless, but it can be great as a personal librarian and thing explainer. He recommends the term &#39;say more,&#39; asking for restatements in the styles of various authors and for summaries, talking back and forth and always being as specific as possible, and having the program check for typos.</p><p> Ethan Mollick proposes developing what he calls Grimoires, which my brain wants to autocorrect to spellbooks (a term he uses as well), prompts designed to optimize the interaction, including giving the AI a role, goal, step-by-step instructions, probably a request for examples and to have the AI gather necessary context from the user.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.01404">Play the game of Hoodwinked</a> , similar to Mafia or Among Us. More capable models, as one would expect, outperform less capable ones, and frequently lie and deceive as per the way the game is played. The proper strategy in such games for humans is usually, if you can, some variant of saying whatever you would have said if you were innocent, which presumably is pretty easy to get an LLM to do. Note that as the LLM gets smarter, other strategies become superior, followed by other strategies that humans couldn&#39;t pull off, followed by strategies humans haven&#39;t even considered.</p><h4> Language Models Don&#39;t Offer Mundane Utility</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1697648862272401685">Beware of AI-generated garbage articles, many say</a> , although I still have yet to actually encounter one. Ryan is correct here, Rohit also, although neither solves the issue.</p><blockquote><p> Paul Graham: I&#39;m looking up a topic online (how hot a pizza oven should be) and I&#39;ve noticed I&#39;m looking at the dates of the articles to try to find stuff that isn&#39;t AI-generated SEO-bait.</p><p> Ryan Peterson: Hotter the better Paul!</p><p> Rohit: Seems to be ok with generative search now? Unless, yes, you wanted to dig much deeper into something very much more specific? [shows Google&#39;s response.]</p></blockquote><p> Here is <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699745755286704466">one of several other claims I saw this week</a> that Google search is getting rapidly polluted by LLM-generated garbage.</p><p> Beware even looking at AI when Steam (Valve) is involved, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/WaifuverseAI/status/1697764665521295838">they remove a game permanently for once allowing a mod</a> that lets characters use GPT-generated dialogue, even though the mod was then removed. While I do admire when one does fully commit to the bit, this is very obviously taking things way too far, and I hope Valve realizes this and reverses their decision.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JudgeFergusonTX/status/1698904660310954186">Judge Roy Ferguson asks Claude who he is</a> , gets into numerous cycles of fabricated information and Claude apologizing and admitting it fabricated information. Definitely a problem. Ferguson treats this as &#39;intentional&#39; on the part of Claude, which I believe is a misunderstanding of how LLMs work.</p><h4> Deepfaketown and Botpocalypse Soon</h4><p> So far, Donald Trump has had the best uses of deepfakes in politics. Do they inevitably favor someone of his talents? Another angle to consider is, who is more vulnerable to such tactics than trump supporters?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697400187361395048">Astrid Wilde</a> : Increasingly have noticed more and more sophisticated AI spoofing attacks targeted at Trump supporters. this attack was convincing enough to even get air time. someone with the speaking cadence of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michaelmalice">@michaelmalice</a> is spoofing Trump&#39;s voice with AI here. it&#39;s a Brave New World. It&#39;s also possible that RAV themselves was behind the spoof, but impossible to know. Absolutely wild times</p></blockquote><p> I checked r/scams on a whim about 40 posts deep. Almost all were old school, <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/Scams/comments/165oqrn/my_partner_and_i_almost_got_taken_in_by_a_deep/">one of them</a> was a report of the classic &#39;your child has been arrested and you must send bail money&#39; scam. The replies refused to believe it was an actual deepfake, saying it was a normal fake voice. It seems even the scam experts don&#39;t realize how easy it is to do a deepfake now, or alternatively they are so used to everything being a scam (because if you have to ask, I have some news) that they assume deepfakes must be a scam too?</p><p> Worth noting that the scam attempt failed. We keep hearing &#39;I almost fell for it&#39; and keep not hearing from anyone who actually lost money.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AviSchiffmann/status/1698147373086896374">The self-explanatory and for now deeply disappointing</a> &#39; <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ehalm_/status/1698107101892268113">smashOrPass.ai</a> &#39; offers the latest in user motivation for fine tuning, whatever one might think of the ethics involved. As of this writing it is only a small set of images on a loop, so claims that it &#39;learns what you like&#39; seem rather silly, but that is easily fixed. What is less easily fixed is all the confounders, which this should illustrate nicely. This is a perfect illustration of how much data is lost when you compress to a 0-1 scale, also either people adjust for context or don&#39;t and either approach will be highly confusing, you really need 0-10 here. And yes, in case anyone was wondering, of course the porn version is coming soon, if not from him than someone else. More interestingly, how long until the version where it takes this information and uses it to auto-swipe on dating app profiles for you?</p><p> Also, can I give everyone who hates this, or anything else on the internet, the obvious advice? For example, <a target="_blank" rel="noreferrer noopener" href="https://www.vice.com/en/article/g5ywp7/you-know-what-to-do-boys-sexist-app-lets-men-rate-ai-generated-women?utm_source=VICE_Twitter&amp;utm_medium=social+">Vice</a> &#39;s Janus Rose? Do not highlight things no one would otherwise have heard of, in order to complain about them. This thing came to my attention entirely due to negative publicity.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://archive.ph/jEsmt">Recreation of James Dean to star in new movie</a> . No doubt more of this is coming. What is weird is that people are talking about cloning great actors, including great voice actors, whether dead or otherwise, and using this to drive living actors out of work.</p><p> The problem with using AI is that AI is not a good actor.</p><p> An AI voice actor can copy the voice of Mel Brooks, but the voice has little to do with what makes Mel Brooks great. What I presume you would actually do, at least for a while, is to have some great voice actor record the new lines. Then use AI to transform the new lines to infuse it with the vocal stylings of Mel Brooks.</p><p> If we have Tom Hanks or Susan Sarandon (both quoted in OP) doing work after they die, then we are choosing to recreate their image and voice, without the ability to copy their actual talents or skills. To the extent that we get a &#39;good performance&#39; out of them, we could have gotten that performance using anyone with enough recorded data as a baseline, such as Your Mom, or a gorgeous fashion model who absolutely cannot act. It makes sense to use this for sequels when someone dies, and continuity takes priority, but presumably the actors of the future will be those with The Look? Thus James Dean makes sense, or Marylin Monroe. Or someone whose presence is importantly symbolic.</p><h4> They Took Our Jobs</h4><p> AI detection tools do not work. We know this. Now we have data on one way they do not work, which is by <a target="_blank" rel="noreferrer noopener" href="https://themarkup.org/machine-learning/2023/08/14/ai-detection-tools-falsely-accuse-international-students-of-cheating">flagging the work of non-native English speakers at Stanford.</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4a9ad0-7a92-4545-b9c8-336441fdfff3_1020x634.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/kvkv0dvfyixr5psjckud" alt=""></a></figure><blockquote><p> AI detectors tend to be programmed to flag writing as AI-generated when the word choice is predictable and the sentences are more simple. As it turns out, writing by non-native English speakers often fits this pattern, and therein lies the problem.</p></blockquote><p> The problem is that the test does not work. This is an illustration that the test does not work. That it happens to hit non-native speakers illustrates how pathetic are our current attempts at detection.</p><p> The AIs we were training on this were misaligned. They noticed that word complexity was a statistically effective proxy in their training data, so they maximized their score as best they could. Could one generate a bespoke training set without this correlation and then try again? Perhaps one could, but I would expect many cycles of this will be necessary before we get something that one can use.</p><p> If anything, this discrimination makes the AI detector more useful rather than less useful. By concentrating its errors in a particular place and with a testable explanation, you can exclude many of its errors. It can&#39;t discriminate against non-native speakers if you never use it on their work.</p><p> It also shows an easy way AI work can be disguised, using complexity of word choice.</p><h4> Get Involved</h4><p> The <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/">Center for AI Policy</a> is a new organization developing and advocating for policy to mitigate catastrophic risks from advanced AI. They&#39;re hiring an AI Policy Analyst and a Communications Director. They recently proposed the <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/work">Responsible AI Act</a> , which needs refinement in spots but was very good to propose, as it is a concrete proposal moving things in productive directions. Learn more and apply <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/careers">here</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://rethinkpriorities.org/xst-incubation">Rethink Priorities doing incubation for AI safety efforts</a> , including <a target="_blank" rel="noreferrer noopener" href="https://careers.rethinkpriorities.org/en/postings/0980aba8-466a-4282-9319-c8c4f6f39341">field building in universities for AI policy careers</a> . Do be skeptical of the plan of an incubation center for a project to help incubate people for future projects. I get how the math in theory works, still most people doing something must ultimately be doing the thing directly or no thing will get done.</p><h4>介绍</h4><p><a target="_blank" rel="noreferrer noopener" href="https://time.com/collection/time100-ai/">Time introduced the Time 100 for AI</a> . I&#39;d look into it but for now I see our time is up.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://support.anthropic.com/en/articles/8324991-about-claude-pro-usage">Claude Pro</a> from Anthropic, pay $20/month for higher bandwidth and priority. I have never run up against the usage limit for Claude, despite finding it highly useful – my conversations tend to be relatively short, the only thing I do that would be expensive is attaching huge PDFs, which they say shrinks the limits but I&#39;ve yet to run into any problems. It is a good note that, when using such attachments, it is efficient to ask for a small number of extensive answers rather than a large number of small ones.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/blog/falcon-180b">Falcon 180B</a> , HuggingFace says it is a ever so slightly better model than Llama-2, which makes it worse relative to its scale and cost. They say it is &#39;somewhere between GPT-3.5 and GPT-4&#39; on the evaluation benchmarks, I continue to presume that in practical usage it will remain below 3.5.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/announcing-openai-devday">OpenAI will host developer conference November 6 in San Francisco.</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1699492275209003425">Sam Altman</a> : on november 6, we&#39;ll have some great stuff to show developers! (no gpt-5 or 4.5 or anything like that, calm down, but still I think people will be very happy…)</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.ycombinator.com/launches/JOw-automorphic-infuse-knowledge-into-language-models-with-just-10-samples">Automorphic (a YC &#39;23 company) offers train-as-you-go fine tuning</a> , including continuous RLHF, using as little has a handful of examples, offers fine tuning of your first three models free. Definitely something that should exist, no idea if they have delivered the goods, anyone try it out?</p><h4> UK Taskforce Update</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1699688880482500684">Update on the UK Foundation Model Taskforce from Ian Hogarth</a> ( <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">direct</a> ). Advisory board looks top notch, including Bengio and Christiano plus Sommeren for national security expertise. They are partnering with ARC Evals, the Center for AI Safety and others. The summit is fast approaching in November, so everything is moving quickly. They are expanding rapidly, and very much still hiring.</p><blockquote><p> Ian Hogarth: Sam Altman, CEO of OpenAI, recently suggested that the public sector had a “lack of will” to lead on innovation, asking “Why don&#39;t you ask the government why they aren&#39;t doing these things, isn&#39;t that the horrible part?”</p><p> We have an abundance of will to transform state capacity at the frontier of AI Safety. This is why we are hiring technical AI experts into government at start-up speed. We are drawing on world-leading expertise.</p><p> ……</p><p> Our team now includes researchers with experience from DeepMind, Microsoft, Redwood Research, The Center for AI Safety and the Center for Human Compatible AI.</p><p> These are some of the hardest people to hire in the world. They have chosen to come into public service not because it&#39;s easy, but because it offers the opportunity to fundamentally alter society&#39;s approach to tackling risks at the frontier of AI.</p><p> We are rapidly expanding this team and are looking for researchers with an interest in catalyzing state capacity in AI Safety. We plan to scale up the team by another order of magnitude. <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/1HKVnrV_rBHF3w4StWUs0XNhxTtKkTHs5CpMnH6PM0pQ/viewform?edit_requested=true">Please consider applying to join us here</a> .</p><p> With the first ever AI Safety Summit in the UK on 1 and 2 November, this is a critical moment to influence AI Safety. We are particularly focused on AI researchers with an interest in technical risk assessments of frontier models.</p><p> ……</p><p> Moving fast matters. Getting this much done in 11 weeks in government from a standing start has taken a forceful effort from an incredible team of dedicated and brilliant civil servants. Building that team has been as important as the technical team mentioned above.</p><p> This is why we&#39;ve brought in Ollie Ilott as the Director for the Taskforce. Ollie joins us from Downing Street, where he led the Prime Minister&#39;s domestic private office, in a critical role known as “Deputy Principal Private Secretary”</p><p> He is known &#39;across the piece&#39; for his ability to recruit and shape best-in-class teams. Before joining the Prime Minister&#39;s office, Ollie ran the Cabinet Office&#39;s COVID strategy team in the first year of the pandemic and led teams involved in Brexit negotiations.</p></blockquote><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.14752">New paper</a> from Peter S. Park, Simon Goldstein, Aidan O&#39;Gara, Michael Chen, Dan Hendrycks: AI Deception: A Survey of Examples, Risks, and Potential Solutions.</p><p>这是摘要：</p><blockquote><p> This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth.</p><p> We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta&#39;s CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models).</p><p> Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems.</p><p> Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.</p></blockquote><p> They do a good job of pointing out that whatever your central case of what counts as deception, there is a good chance we already have a good example of AIs doing that. LLMs are often involved. There is no reason to think deception does not come naturally to optimizing AI systems when it would be a useful thing to do. Sometimes it is intentional or predicted, other times it was unintended and happened anyway, including sometimes with the AI&#39;s explicit intent or plan to do so.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OwainEvans_UK/status/1698683225349198200">New paper from Owain Evans tests potential situational awareness of LLMs</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/5mTbvtJCli">paper</a> ).</p><blockquote><p> Owain Evans: Our experiment:</p><p> 1. Finetune an LLM on descriptions of fictional chatbots but with no example transcripts (ie only declarative facts).</p><p> 2. At test time, see if the LLM can behave like the chatbots zero-shot. Can the LLM go from declarative → procedural info?</p><p> Surprising result:</p><p> 1. With standard finetuning setup, LLMs fail to go from declarative to procedural info.</p><p> 2. If we add paraphrases of declarative facts to the finetuning set, then LLMs succeed and improve with scale.</p></blockquote><p> I am not as surprised as Owain was, this all makes sense to me. I still find it interesting and I&#39;m glad it was tried. I am not sure what updates to make in response.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JoINrbs/status/1698225548043165874">Twitter privacy policy now warns it can use your data to train AI models.</a> Which they were going to do anyway, if anything Elon Musk is focusing on not letting anyone else do this.</p><blockquote><p> Jorbs: Nonsense posts and nonsense art are functional anti-capitalist art now, which is kinda cool.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/31/23854012/nvidia-amd-chip-restrictions-middle-east">Chip restrictions expand to parts of the Middle East</a> . How do you keep chips out of China without keeping them out of places that would allow China to buy the chips?</p><p> Good <a target="_blank" rel="noreferrer noopener" href="https://time.com/6310076/elon-musk-ai-walter-isaacson-biography/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_term=business_companies&amp;linkId=233510924">Time article from Walter Isaacson chronicling the tragedy and cautionary tale of Elon Musk</a> , who Demis Hassabis warned about the dangers of AI, and who then completely misunderstood what would be helpful and as a result made things infinitely worse. He continues to do what feels right to him, and continues to not understand what would make it more versus less likely we all don&#39;t die. It is not without a lot of risk, but we should continue trying to be helpful in building up his map, and try to get him to talk to Eliezer Yudkowsky or other experts in private curious mode if we can. Needless to say, Elon, call any time, my door is always open.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bindureddy/status/1699275289493430699">Brief Twitter-post 101 explainer of fine tuning</a> .</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/catehall/status/1698466176156791047">Kate Hall, who has practiced copyright law</a> , predicts that MidJourney, GPT and any other models trained on copyrighted material will be found to have violated copyright.</p><blockquote><p> Kate Hall: Okay I&#39;ve spent all of a few hours thinking about copyright infringement by generative AI (note I&#39;ve practiced copyright law before) and the correct treatment seems kind of obvious to me, so I&#39;d like someone to tell me what I&#39;m missing since I know it&#39;s hotly contested.</p><p> My bottom line conclusion: Courts will find generative AI violates copyright law. (One way I could be wrong is if I misperceive all systems as essentially following similar mechanics — I&#39;m using OpenAI &amp; Midjourney in my head when I&#39;m modeling this.)</p><p> The system outputs (generated content) themselves mostly don&#39;t violate copyright, I think. I see people arguing that the outputs are “derivative works” but I think that stretches the concept far beyond what courts would accept.</p><p> There may be exceptions where the outputs flagrantly copy parts of copyrighted works, but those cases aren&#39;t the norm and I&#39;d expect them to get rarer over time with new systems.</p><p> Making copies of copyrighted works to use in a training set w/o permission is, however, infringement, &amp; AFAIK there&#39;s no way to do training w/o making copies in the process. <a target="_blank" rel="noreferrer noopener" href="https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf">OpenAI seems to concede this but says it&#39;s okay under the fair use doctrine</a> .</p><p> But making copies for training sets isn&#39;t fair use. <a target="_blank" rel="noreferrer noopener" href="https://t.co/zOVA8JJNbt">If you just read through the factors</a> , it might not be obvious why. But if you want to anticipate what a court will say, you need to look at the use in the context of copyright law.</p><p> The purpose of copyright law is to compensate ppl for creating new scientific &amp; artistic works. If someone takes copyrighted material &amp; uses it to generate content that *reduces demand for the original works* &amp; profits from it, courts will find a reason it&#39;s not fair use.</p><p> Against this, I&#39;ve seen OAI argue that the training set copies are fair use anyway because no human is looking *at the training set* instead of consuming the original work — <a target="_blank" rel="noreferrer noopener" href="https://t.co/UiJjMQTlwS">infringement &amp; the creation of a substitute work happen at different steps</a> .</p><p> This is too clever by half. It&#39;s just not the kind of argument that works in copyright cases because the overall scheme is flagrantly contrary to the entire spirit of copyright law.</p><p> I&#39;m guessing a court will reach that conclusion by saying the whole course of conduct &amp; its effects should be considered in the fair use analysis, but maybe there is another way to get to the same conclusion. But AFAICT, it will be the conclusion. So, what am I missing?</p></blockquote><p> This is a highly technical opinion, and relies on courts applying a typical set of heuristics to a highly unusual situation, so it seems far from certain. Also details potentially matter in weird ways.</p><blockquote><p> Haus Cole: How much of this analysis hinges on the copying for training sets? If, theoretically, the training set was just a set of URLs and the systems “viewed” those URLs directly at training time, would that change the analysis materially?</p><p> Cate Hall: Yes, potentially — it would depend on the particulars of the system, but if no copy is ever made it&#39;s a lot harder to see what the specific nature of the infringement is.</p><p> Josh Job: What is a “copy” in this context? Every computer copies everything every time it moves from storage to RAM or is accessed via a remote system to do any computation on. If the training set is a set of URLs at training time the systems have to download the url content to see it.</p><p> Cate Hall: In that case I don&#39;t think the distinction matters — if there&#39;s a copy made, even if transient, the same infringement analysis applies.</p></blockquote><p> Presumably we can all agree that this rule does not make a whole lot of sense. Things could also take a while. Or it might not.</p><blockquote><p> Smith Sam: Nothing at all in your analysis (it has played out as you predict in other places), but how long will all that take to litigate, and where do OAI think they&#39;ll be by then? ie will the court decision be irrelevant to what they&#39;re doing at the time the court decides?</p><p> Cate Hall: It&#39;s a fair question but it really could go a lot of different ways. If SCOTUS, many years — but a district court could enter a preliminary injunction based on the likelihood of infringement while a case is litigated in no time at all. Luck of the draw with district judges.</p></blockquote><p> Cate Hall&#39;s position <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/">is in sharp contrast to OpenAI&#39;s</a> .</p><blockquote><p> “Under the resulting judicial precedent, it is not an infringement to create &#39;wholesale cop[ies] of [a work] as a preliminary step&#39; to develop a new, non-infringing product, even if the new product competes with the original,” OpenAI wrote.</p></blockquote><p> The authors suing in the current copyright lawsuit do seem to be a bit overreaching?</p><blockquote><p> The company&#39;s motion to dismiss cited “a simple response to a question (eg, &#39;Yes&#39;),” or responding with “the name of the President of the United States” or with “a paragraph describing the plot, themes, and significance of Homer&#39;s <em>The Iliad</em> ” as examples of why every single ChatGPT output cannot seriously be considered a derivative work under authors&#39; “legally infirm” theory.</p></blockquote><p> Is generative AI in violation of copyright? Perhaps it is. Is it a &#39;grift&#39; that merely repackages existing work, as the authors claim?不。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/29/23851126/us-copyright-office-ai-public-comments">The US Copyright Office has opened a comment period</a> . Their emphasis is on outputs</p><blockquote><p> Emilia David at Verge: As announced in the <a target="_blank" rel="noreferrer noopener" href="https://public-inspection.federalregister.gov/2023-18624.pdf">Federal Register</a> , the agency wants to answer three main questions: how AI models should use copyrighted data in training; whether AI-generated material can be copyrighted even without a human involved; and how copyright liability would work with AI. It also wants comments around AI possibly violating publicity rights but noted these are not technically copyright issues. The Copyright Office said if AI does mimic voices, likenesses, or art styles, it may impact state-mandated rules around publicity and unfair competition laws.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/stories-to-watch-tech-stock-arithmetic">Arnold Kling sees the big seven tech stocks as highly overvalued</a> . My portfolio disagrees on many of them. One mistake is these are global companies, so you should compare to world GDP of 96 trillion, not US GDP of 26 trillion, which makes an overall P/E of 50 seem highly reasonable, given how much of the economy is going to shift into AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-09-04/ai-hype-has-subsided-but-technology-remains-as-powerful-as-ever?utm_source=twitter&amp;utm_medium=social&amp;utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;utm_campaign=socialflow-organic&amp;sref=htOHjx5Y">Tyler Cowen says we are in an &#39;AI lull&#39;</a> with use leveling off and obvious advances stalled for a time, but transformational change is coming. I agree. He is excited by, and it seems not worried about, what he sees as unexpectedly rapid advancements in open source models. I am skeptical that they are doing so well, they systematically underperform their benchmark scores. In practice and as far as I can tell GPT-3.5 is still superior to every open source option.</p><p> Flo Crivello shortens their timelines.</p><blockquote><p> Flo Crivello: Big breakthroughs are happening at every level in AI — hardware, optimizers, model architectures, and cognitive architectures. My timelines are shortening — 95% confidence interval is AGI is in 2-8 yrs, and super intelligence another 2-8 years afterwards.系好安全带。</p></blockquote><p> I don&#39;t see this as consistent. If you get AGI in 2-8 years, you get ASI in a lot less than 2-8 more years after that.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/m_ccuri/status/1696975744327450990">Full list of people attending Schumer&#39;s meeting</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F955e247e-6b21-414f-9861-ef8affefbee2_1179x1723.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/t06dq9ih1g8xlbby2div" alt="图像"></a></figure><h4> The Week in Audio</h4><p> The main audio event this week was <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/">Inflection AI CEO and DeepMind founder Mustafa Suleyman on the 80,000 hours podcast</a> , giving us a much better idea where his head is at, although is it even an 80,000 hours podcast if it is under an hour?</p><p> Up front, I want to say that it&#39;s great that he went on 80,000 hours and engaged for real with the questions. A lot of Suleyman&#39;s thinking here is very good, and his openness is refreshing. I am going to be harsh in places on the overview list below, so I want to be clear that he is overall being super helpful and I want more of this.</p><p> I also got to see notes on Suleyman&#39;s new book, The Coming Wave. The book and podcast are broadly consistent, with the main distinction being that the book is clearly aiming to be normie-friendly and conspicuously does not discuss extinction risks, even downplaying details of the less extreme downsides he emphasizes more.</p><ol><li> Wiblin opens asking about potential dangerous AI capabilities, since Suleyman has both said he thinks AI may be able to anonymously run a profitable company within 2 years, but also says it is unlikely to be dangerous within 10 years, which I agree seem like two facts that do not live in the same timeline. Suleyman clarifies that the AI would still need human help for various things along the way, but given humans can hired to do those things, I don&#39;t see why that helps?</li><li> Suleyman also clarifies that he is centrally distinguishing runaway intelligence explosions and recursive self-improvement from potential human use or misuse.</li><li> Suleyman says he has uncertainty about timelines, in a way that makes it seem like he wants to wait for things to clearly be getting out of hand before we need to act?</li><li> Suleyman disputes claim that is trivial to remove fine-tuning and alignment, later explaining it can be done but requires technical chops. I don&#39;t see how that helps.</li><li> Suleyman emphasizes that he is warning about open source, but still seems focused on this idea of human misuse and destructiveness. Similarly, he sees Llama-2&#39;s danger as it revealing information already available on the web, whereas Anthropic says Claude was capable of importantly new synthesis of dangerous capabilities.</li><li> “We&#39;re going to be training models that are 1,000x larger than they currently are in the next three years. Even at Inflection, with the compute that we have, will be 100x larger than the current frontier models in the next 18 months.”</li><li> Agreement (and I also mostly agree) that the issue with open sourcing Llama-2 is not that it will do much damage now, but the precedent it sets. My disagreement is that the 10-15 year timeline here for that transition seems far too slow.</li><li> Anticipation of China being entirely denied the next generation of AI chips, and USA going on full economic war footing with China. As usual, I remind everyone that we don&#39;t let Chinese AI (or other) talent move to America, so we cannot possibly care that much about winning this battle.</li><li> Google&#39;s attempt to have an oversight board with a diversity of viewpoints was derailed by cancel culture being unwilling to tolerate a diversity of viewpoints, so the whole thing fell apart entirely within weeks. No oversight, then, which Suleyman notes is what power wants anyway. As Wiblin notes, you can either give people in general a voice, or you can have all the voices be agreeing with what are considered correct views, but you cannot have both at once. We can say we want to give people a voice, but when they try to use it, we tell them they&#39;re wrong.</li><li> I would say here: Pointing out that they are indeed wrong does not help on this. There seems to clearly not be a ZOPA – a zone of possible agreement – on how to choose what AI will do, on either the population level or the national security level, if you need to get buy-in from China and the global south also the United States. I predict that (almost) everyone in the West who says &#39;representativeness&#39; or even proposes coherent extrapolated volition would be horrified by what such a process would actually select.</li><li> “The first part of the book mentions this idea of “pessimism aversion,” which is something that I&#39;ve experienced my whole career; I&#39;ve always felt like the weirdo in the corner who&#39;s raising the alarm and saying, “Hold on a second, we have to be cautious.” Obviously lots of people listening to this podcast will probably be familiar with that, because we&#39;re all a little bit more fringe. But certainly in Silicon Valley, that kind of thing… I get called a “decel” sometimes, which I actually had to look up.” Whereas from my perspective, he is quite the opposite. He founded DeepMind and Inflection AI, and says explicitly in his book that to be credible you must be building.</li><li> “It&#39;s funny, isn&#39;t it? So people have this fear, particularly in the US, of pessimistic outlooks. I mean, the number of times people come to me like, “You seem to be quite pessimistic.” No, I just don&#39;t think about things in this simplistic “Are you an optimist or are you a pessimist?” terrible framing.这是废话。 I&#39;m neither. I&#39;m just observing the facts as I see them, and I&#39;m doing my best to share for critical public scrutiny what I see. If I&#39;m wrong, rip it apart and let&#39;s debate it — but let&#39;s not lean into these biases either way.”说得好。</li><li> “So in terms of things that I found productive in these conversations: frankly, the national security people are much more sober, and the way to get their head around things is to talk about misuse. They see things in terms of bad actors, non-state actors, threats to the nation-state.” Can confirm this. It is crazy the extent to which such people can literally only think in terms of a human adversary.</li><li> More &#39;in order to do safety you have to work to push the frontline of capabilities.&#39; Once again, I ask why it is somehow always both necessary and sufficient for everyone to work with the best model they can help develop, what a coincidence.</li><li> Suleyman says the math on a $10 billion training run will not add up for at least five years, even if you started today it would take years to execute on that.</li><li> Suleyman reiterates: “I&#39;m not in the AGI intelligence explosion camp that thinks that just by developing models with these capabilities, suddenly it gets out of the box, deceives us, persuades us to go and get access to more resources, gets to inadvertently update its own goals. I think this kind of anthropomorphism is the wrong metaphor. I think it is a distraction. So the training run in itself, I don&#39;t think is dangerous at that scale. I really don&#39;t.” His concern is proliferation, so he&#39;s not worried that Inflection AI is going to accelerate capabilities merely by pushing the frontiers of capabilities. Besides, if he didn&#39;t do it, someone else would.</li><li> Wiblin suggests “They&#39;re going to do the thing that they&#39;re going to do, just because they think it&#39;s profitable for them. And if you held back on doing that training run, it wouldn&#39;t shift their behavior.” Suleyman affirms. So your behavior won&#39;t change anyone else&#39;s behavior, and also everyone else&#39;s behavior justifies yours.知道了。</li><li> Affirms that yes, a much stronger version of current models would not be inherently dangerous, as per Wiblin “in order for it to be dangerous, we need to add other capabilities, like it acting in the world and having broader goals. And that&#39;s like five, 10, 15, 20 years away.” Except, no. People turn these things into agents easily already, and they already contain goal-driven subagent processes.</li><li> “I think everybody who is thinking about AI safety and is motivated by these concerns should be trying to operationalize their alignment intentions, their alignment goals. You have to actually make it in practice to prove that it&#39;s possible, I think.” It is not clear the extent to which he is actually confusing aligning current models with what could align future models. Does he understand that these two are very different things? I see evidence in both directions, including some strong indications in the wrong direction.</li><li> Claim that Pi (found at <a target="_blank" rel="noreferrer noopener" href="https://pi.ai/talk">pi.ai</a> ) cannot be jailbroken or prompt hacked. Your move.</li><li> Reminds us that Pi does not code or do many other things, it is narrowly designed to be an AI assistant. Wait, need my AI assistant to be able to help me write code.</li><li> Reminds us that GPT-3.5 to GPT-4 was a 5x jump in resources.</li><li> Strong candidate for scariest thing to hear such a person say they believe about alignment difficulty: “Well, it turns out that the larger they get, the better job we can do at aligning them and constraining them and getting them to produce extremely nuanced and precise behaviours. That&#39;s actually a great story, because that&#39;s exactly what we want: we want them to behave as intended, and I think that&#39;s one of the capabilities that emerge as they get bigger.”</li><li> On Anthropic: “I don&#39;t think it&#39;s true that they&#39;re not attempting to be the first to train at scale. That&#39;s not true… I don&#39;t want to say anything bad, if that&#39;s what they&#39;ve said. But also, I think Sam [Altman] said recently they&#39;re not training GPT-5.快点。我不知道。 I think it&#39;s better that we&#39;re all just straight about it. That&#39;s why we disclose the total amount of compute that we&#39;ve got.”</li><li> Endorses legal requirements for disclosure of model size, a framework for harmful capabilities measures, and not using these models for electioneering.</li><li> I have no idea what that last one would even mean? He says &#39;You shouldn&#39;t be able to ask Pi who Pi would vote for, or what the difference is between these two candidates&#39; but that is an arbitrary cutout of information space. Are you going to refuse to answer any questions regarding questions relevant to any election? How is that not a very large percentage of all interesting questions? There&#39;s a kind of myth of neutrality. And are you going to refuse to answer all questions about how one might be persuasive? All information about every issue in politics?</li><li> Suleyman believes it is not tactically wise to discuss misalignment, deceptive alignment, or models having their own goals and getting out of control. He also previously made clear that this is a large part of his threat model. This is extra confirmation of the hypothesis that his book sidesteps these issues for tactical reasons, not because Suleyman disagrees on the dangers of such extinction risks.</li></ol><p> It is perhaps worth contrasting this with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AISafetyMemes/status/1699003113586188386">this CNN interview with former Google ECO Eric Schmidt</a> , who thinks recursive self-improvement and superintelligence are indeed coming soon and a big deal we need to handle properly or else, while also echoing many of Suleyman&#39;s concerns.</p><p> There is also the video and transcript of the talks from the <a target="_blank" rel="noreferrer noopener" href="https://www.alignment-workshop.com/">San Francisco Alignment Workshop</a> from last February. Quite the lineup was present.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=BtnvfVc8z8o&amp;t=3s&amp;ab_channel=AlignmentWorkshop">Jan Leike&#39;s talk starts out</a> by noting that RLHF will fail when human evaluation fails, although we disagree about what counts as failure here. Then he uses the example of bugs in code and using another AI to point them out and states his principle of evaluation being easier than generation. Post contra this hopefully coming soon.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nearcyan/status/1697320905301484012">Sam Altman recommends surrounding yourself with people who will raise your ambition</a> , warns 98% of people will pull you back. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=uEl2KUZ3JWA&amp;ab_channel=YCombinator">Full interview on YouTube here.</a></p><p> Telling is that he says that most people are too worried about catastrophic risk, and not worried enough about chronic risk – they should be concerned they will waste their life without accomplishment, instead they worry about failure. I am very glad someone with this attitude is out there running all but one of Sam Altman&#39;s companies and efforts, and most people in most places could use far more of this energy. The problem is that he happens to also be CEO of OpenAI, working on the one problem where catastrophic (existential) risk is quite central.</p><p> Also he says (22:25) “If we [build AGI at OpenAI] that will be more important than all the innovation in all of human history.”他是对的。让它沉入其中。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/labenz/status/1696933904941207690">Paige Bailey, the project manager for Google&#39;s PaLM-2, goes on Cognitive Revolution</a> . This felt like an alternative universe interview, from a world in which Google&#39;s AI efforts are going well, or OpenAI and Anthropic didn&#39;t exist, in addition to there being no risks to consider. It is a joy to see her wonder and excitement at all the things AI is learning how to do, and her passion for making things better. The elephant in the room, which is not mentioned at all, is that all of Google&#39;s Generative AI products are terrible. To what extent this is the &#39;fault&#39; of PaLM-2 is unclear but presumably that is a big contributing factor. It&#39;s not that Bard is not a highly useful tool, it&#39;s that multiple other companies with far fewer resources have done so much better and Bard is not catching up at least pre-Gemini.</p><p> Risks are not mentioned at all, although it is hard to imagine <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DynamicWebPaige">Bailey</a> is at all worried about extinction risks. She also doesn&#39;t see any problem with Llama-2 and open source, citing it unprompted as a great resource, which also goes against incentives. Oh how much I want her to be right and the rest of us to live in her world. Alas, I do not believe this is the case. We will see what Gemini has to offer. If Google thinks everything is going fine, that is quite the bad sign.</p><h4> Rhetorical Innovation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699597268767445181">Perhaps a good short explanation in response here?</a></p><blockquote><p> Richard Socher: The reason nobody is working on a self-aware AI setting its own goals (rather than blindly following a human-defined objective function) is that it makes no money. Most companies/governments have their own goals and prefer not to spend billions on an AI doing whatever it wants.</p><p> Eliezer Yudkowsky: When you optimize stuff on sufficiently complicated problems to the point where it starts to show intelligence that generalizes far beyond the original domains, a la humans, it tends to end up with a bunch of internal preferences not exactly correlated to the outer loss function, a la humans.</p></blockquote><p> The point I was trying to make last week, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1697541481467150786">not landing as intended</a> .</p><blockquote><p> Robert Wiblin: Yesterday I joked: “The only thing that stops a bad person with a highly capable ML model is a good government with a ubiquitous surveillance system.” I forgot to add what I thought was sufficiently obvious it didn&#39;t need to be there: “AND THAT IS BAD.”</p><p> My point is that if you fail to limit access to WMDs at their source and instead distribute them out widely, you don&#39;t create an explosion of explosion — rather you force the general public to demand massive government surveillance when they conclude that&#39;s the only way to keep them safe. And again, to clarify, THAT IS BAD.</p></blockquote><p> Exactly. We want to avoid ubiquitous surveillance, or minimize its impact. If there exists a sufficiently dangerous technology, that leaves you two choices.</p><ol><li> You can do what surveillance and enforcement is necessary to limit access.</li><li> You can do what surveillance and enforcement is necessary to contain usage.</li></ol><p> Which of these will violate freedom less? My strong prediction for AGI is the first one.</p><p> As a reminder, this assumes we fully solved the alignment problem in the first place. This is how we deal with the threat of human misuse or misalignment of AGI in spite of alignment being robustly solved in practice. If we haven&#39;t yet solved alignment, then failing to limit access (either to zero people, or at least to a very highly boxed system treated like the potential threat that it would be) would mean we are all very dead no matter what.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/egrefen/status/1699128376299041244">Make Google DeepMind&#39;s AIs as autonomous as possible</a> .</p><blockquote><p> Edward Grefenstette (Director of Research at DeepMind): I will be posting (probably next week) some job listings for a new team I&#39;m hiring into at <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind">@GoogleDeepMind</a> .我将寻找一些具有强大工程背景的研究科学家和工程师来帮助构建日益自主的语言代理。关注此空间。</p><p> Melanie Mitchell: “to help build increasingly autonomous language agents”</p><p> Curious what you and others at DeepMind think <a target="_blank" rel="noreferrer noopener" href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/">about Yoshua Bengio&#39;s argument</a> that we should limit AI agents&#39; autonomy?</p><p> Edward: Short answer: I&#39;m personally interested in initially investigating cases where (partial) autonomy involves human-in-the-loop validation during the downstream use case, as part of the normal mode of operation, both for safety and for further training signal.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://importai.substack.com/p/import-ai-339-open-source-ai-culture">a16z gives out grants for open source AI work</a> , doing their best to proliferate as much as possible with as few constraints as possible. Given Marc Andreessen&#39;s statements, this should come as no surprise.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> We have a class for that now, at Princeton, <a target="_blank" rel="noreferrer noopener" href="https://sites.google.com/view/cos598aisafety/">technically a graduate seminar but undergraduates welcome</a> . Everything they will read is online so lots of resources and links there and list seems excellent at a glance.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2309.01933">Max Tegmark and Steve Omohundo drop a new paper</a> claiming provably safe systems are the only feasible path to controlling AGI, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699442923320865105">Davidad notes no substantive disagreements with his OAA plan</a> .</p><blockquote><p> Abstract: We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.</p></blockquote><p> Jan Leike, head of alignment at OpenAI, relies heavily on the principle that verification is in general easier than generation. I strongly think this is importantly false in general for AI contexts. You need to approach having a flawless verifier, whereas the generator need not achieve that standard.</p><p> Proofs are the exception. The whole point of a proof is that it is easy to definitively verify. Relying only on that which you can prove is a heavy alignment tax, especially where the proof is in the math sense, not merely in the courtroom sense. If you can prove your system satisfies your requirements, and you can prove that your requirements satisfy your actual needs, you are all set.</p><p> The question is, can it be done? Can we build the future entirely out of things where we have proofs that they will do what we want, and not do the things we do not want?</p><p> That does seems super hard. The proposal here is to use AIs to discover proof-carrying code.</p><blockquote><p> Proof-carrying code is a fundamental component in our approach. Developing it involves four basic challenges:</p><p> 1. Discovering the required algorithms and knowledge</p><p> 2. Creating the specification that generated code must satisfy</p><p> 3. Generating code which meets the desired specification</p><p> 4. Generating a proof that the generated code meets the specification Generating a proof that the generated code meets the specification</p><p> Before worrying about how to formally specify complex requirements such as “don&#39;t drive humanity extinct”, it&#39;s worth noting that there&#39;s a large suite of unsolved yet easier and very well-specified challenges whose solution would be highly valuable to society and in many cases also help with AI safety.</p><p> Provable cybersecurity: One of the paths to AI disaster involves malicious use, so guaranteeing that malicious outsiders can&#39;t hack into computers to steal or exploit powerful AI systems is valuable for AI safety. Yet embarrassing security flaws keep being discovered, even in fundamental components such as the ssh Secure Shell [50] and the bash Linux shell [60]. It&#39;s quite easy to write a formal specification stating that it&#39;s impossible to gain access to a computer without valid credentials.</p></blockquote><p> Where I get confused is, what would it mean to prove that a given set of code will do even the straightforward tasks like proving cybersecurity.</p><p> How does one prove that you cannot gain access without proper credentials? Doesn&#39;t this fact rely upon physical properties, lest there be a bug or physical manipulation one can make? Couldn&#39;t sufficiently advanced physical analysis allow access, if only via identification of the credentials? How do we know the AI won&#39;t be able to figure out the credentials, perhaps in a way we don&#39;t anticipate, perhaps in a classic way as simple as engineering a wrench attack?</p><p> They then consider securing the blockchain, such as by formally verifying Ethereum, which would still leave various vulnerabilities in those using the protocol, it would not I&#39;d expect mean you were safe from a hack. The idea of proving that you have &#39;secured critical infrastructure&#39; seems even more confused.</p><p> These don&#39;t seem like the types of things one can prove even under normal circumstances. They certainly don&#39;t seem like things you can prove if you have to worry about a potential superintelligent adversary, and their plan says you need to not assume AI non-hostility, let alone AI active alignment.</p><p> They do mean to do the thing, and warn that means doing it for real:</p><blockquote><p> It&#39;s important to emphasize that formal verification must be done with a security mindset, since it must provide safety against all actions by even a superintelligent adversary. Fortunately, the theoretical cryptography community has built a great conceptual apparatus for digital cryptography. For example, Boneh and Shoup&#39;s excellent new text “A Graduate Course in Applied Cryptography” provides many examples of formalizing adversarial situations and proving security properties of cryptographic algorithms. But this security mindset urgently needs to be extended to hardware security as well, to form the foundation of PCH. As the lock-picking lawyer [72] quips: “Security is only as good as its weakest link”. For physical security to withstand a superintelligent adversary, it needs to be provably secure.</p></blockquote><p> How are we going to pull this off? They suggest that once you have an LLM learn all the things, you can then abstract its functionality to traditional code.</p><blockquote><p> The black box helps for learning, not for execution. If the provably safe AI vision succeeds by replacing powerful neural networks by verified traditional software that replicates their functionality, we shouldn&#39;t expect to suffer a performance hit.</p><p> ……</p><p> Since we humans are the only species that can do this fairly well, it may unfortunately be the case that the level of intelligence needed to be able to convert all of one&#39;s own black-box knowledge into code has to be at least at AGI-level. This raises the concern that we can only count on this “introspective” AGI-safety strategy working after we&#39;ve built AGI, when according to some researchers, it will already be too late.</p></blockquote><p> I worry that Emerson Pugh comes to mind: If the human brain were so simple that we could understand it, we would be so simple that we couldn&#39;t.</p><p> Will introspection ever be easier than operation? Will it be possible for a mind to be powerful enough to fully abstract out the meaningful operations of a similarly powerful mind? If not, will there be a way to safely &#39;move down the chain&#39; where we are able to use a dangerous unaligned model we do not control to safely abstract out the functionality of a less powerful other model, which presumably involves formally verify the resulting code before we run it? Will we be able to generate that proof, again with the tools we dare create and use, in any sane amount of time, even if we do translate into normal computer code, presumably quite messy code and quite a lot of it?</p><p> The paper expresses great optimism about progress in mechanistic interpretability, and that we might be able to progress it to this level. I am skeptical.</p><p> Perhaps I am overestimating what we actually need here, if we can coordinate on the proof requirements? Perhaps we can give up quite a lot and still have enough with what is left.我不知道。 I do know that of the things I expect to be able to prove, I don&#39;t know how to use them to do what needs to be done.</p><p> They suggest that Godel&#39;s Completeness Theorem implies that, given AI systems are finite, any system you can&#39;t prove is safe will be unsafe. In practice I don&#39;t see how this binds. I agree with the &#39;sufficiently powerful AGIs will find a way if a way exists&#39; part. I don&#39;t agree with &#39;you being unable to prove it in reasonable time&#39; implying that no proof exists, or that you can be confident the proof you think you have proves the practical property you think it proves.</p><p> I would also note that we are unlikely any time soon to prove that humans are safe in any sense, given that they clearly aren&#39;t. Where does that leave us? They warn humans might have to operate without any guarantees of safety, but no system in human history has ever had real guarantees of safety, because it was part of human history. We have needed to find other ways to trust. They make a different case.</p><p> Similarly, if we actually do build the human-flourishing-enabling AI that will give us everything we want, it will be impossible to prove that it is safe, because it won&#39;t be.</p><blockquote><p> The only absolutely trustable information comes from mathematical proof. Because of this, we believe it is worth a fair amount of inconvenience and possibly large amounts of expense for humanity to create infrastructure based on provable safety. The 2023 global nominal GDP is estimated to be $105 trillion. How much is it worth to ensure human survival? $1 trillion? $50 trillion? Beyond the abstract argument for provable safety, we can consider explicit threats to see the need for it.</p></blockquote><p> I get why this argument is being trotted out here. I don&#39;t expect it to work. It never does, Arrested Development meme style.</p><p> Their argument laid out in the remainder of section 8, of why alternative approaches are unlikely to work, alas rings quite true. We have to solve an impossible problem somewhere. Pointing out an approach has impossible problems it requires you to solve is not as knock-down an argument as one would like it to be.</p><p> As calls to action, they suggest work on:</p><ol><li> Automating formal verification</li><li> Developing verification benchmarks.</li><li> Developing probabilistic program verification.</li><li> Developing quantum formal verification.</li><li> Automating mechanistic interpretability.</li><li> Develop mechanistic interpretability benchmarks.</li><li> Automating mechanistic interpretability.</li><li> Building a framework for provably compliant hardware.</li><li> Building a framework for provably compliant governance.</li><li> Creating provable formal models of tamper detection.</li><li> Creating provably valid sensors.</li><li> Designing for transparency.</li><li> Creating network robustness in the face of attacks.</li><li> Developing useful applications of provably compliant systems.<ol><li> Mortal AI that dies at a fixed time.</li><li> Geofenced AI that only operates in some locations.</li><li> Throttled AI that requires payment to continue operating.</li><li> AI Kill Switch that would work.</li><li> Asimov style laws?!?</li><li> Least privilege guarantees ensuring no one gets permissions they do not need.</li></ol></li></ol><p> I despair at the proposed applications, which are very much seem to me to be in the &#39;you are still dead&#39; and &#39;have we not been over that this will never work&#39; categories.</p><p> This all does seem like work better done than not done, who knows, usefulness could ensue in various ways and downsides seem relatively small.</p><p> We would still have to address that worry mentioned earlier about “formally specifying complex requirements such as “don&#39;t drive humanity extinct.”” I have not a clue. Anyone have ideas?</p><p> They finish with an FAQ, Davidad correctly labeled it fire.</p><blockquote><p> Q: Won&#39;t debugging and “evals” guarantee AGI safety?</p><p> A: No, debugging and other evaluations looking for problems provide necessary but not sufficient conditions for safety. In other words, they can prove the presence of problems, but not the absence of problems.</p><p> Q: Isn&#39;t it unrealistic that humans would understand verification proofs of systems as complicated as large language models?</p><p> A: Yes, but that&#39;s not necessary. We only need to understand the specs and the proof verifier, so that we trust that the proof-carrying AI will obey the specs.</p><p> Q: Isn&#39;t it unrealistic that we&#39;d be able to prove things about very powerful and complex AI systems?</p><p> A: Yes, but we don&#39;t need to. We let a powerful AI discover the proof for us. It&#39;s much harder to discover a proof than to verify it. The verifier can be just a few hundred lines of human-written code. So humans don&#39;t need to discover the proof, understand it or verify it. They merely need to understand the simple verifier code.</p><p> Q: Won&#39;t checking the proof in PCC cause a performance hit?</p><p> A: No, because a PCC-compliant operating system could implement a cache system where it remembers which proofs it has checked, thus needing to verify each code only the very first time it&#39;s used.</p><p> Q: Won&#39;t it take decades to fully automate program synthesis and program verification?</p><p> A: Just a few years ago, most AI researchers thought it would take decades to accomplish what GPT-4 does, so it&#39;s unreasonable to dismiss imminent ML-powered synthesis and verification breakthroughs as impossible.</p></blockquote><p> I worry that this is a general counterargument for any objection that something is too technically difficult, either relatively or absolutely, and thus proves far too much.</p><blockquote><p> Q: Isn&#39;t it premature to work on provable safety before we know how to formally specify concepts such as “Don&#39;t harm humans”?</p><p> A: No, because provable safety can score huge wins for AI safety even from things that are easy to specify, involving eg cybersecurity.</p></blockquote><p> Eliezer Yudkowsky responds more concisely to the whole proposal.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699579311567888468">Eliezer Yudkowsky</a> : There is no known solution for, and no known approach for inventing, a theorem you could prove about a program <em>such that</em> the truth of the theorem would imply the AI was <em>actually</em> a friendly superintelligence. This is <em>the</em> great difficulty… and it isn&#39;t addressed in the paper.</p></blockquote><p>是的。 The idea, as I understand it, is to use proofs to gain capabilities while avoiding having to build a friendly superintelligence. Then use those capabilities to figure out how to do it (or prevent anyone from building an unfriendly one).</p><h4> Twitter Community Notes Notes</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">Vitalik Buterin analyzes</a> the Twitter community notes algorithm. It has a lot of fiddly details, but the core idea is simple. Qualified Twitter users participate, rating proposed community notes on a three-point scale, if your ratings are good you can propose new notes. Notes above about +0.4 helpfulness get shown. The key is that rather than use an average, notes are rewarded if people with a variety of perspectives vote the note highly, as measured by an organically emerging axis that corresponds very well to American left-right politics. Vitalik is especially excited because this is a very crypto-style approach, with a fully open-source algorithm determined by the participation of a large number of equal-weight participants with no central authority (beyond the ability to remove people from the pool for violations.)</p><p> This results in notes pretty much everyone likes, with a focus on hard and highly relevant facts, especially on materially false statements, and rejecting partisan statements.</p><p> He also notes that all the little complexity tweaks on top matter.</p><blockquote><p> The distinction between this, and algorithms that I helped work on such as <a target="_blank" rel="noreferrer noopener" href="https://vitalik.ca/general/2019/12/07/quadratic.html">quadratic funding</a> , feels to me like a distinction between an <strong>economist&#39;s algorithm</strong> and an <strong>engineer&#39;s algorithm</strong> . An economist&#39;s algorithm, at its best, values being simple, being reasonably easy to analyze, and having clear mathematical properties that show why it&#39;s optimal (or least-bad) for the task that it&#39;s trying to solve, and ideally proves bounds on how much damage someone can do by trying to exploit it. An engineer&#39;s algorithm, on the other hand, is a result of iterative trial and error, seeing what works and what doesn&#39;t in the engineer&#39;s operational context. Engineer&#39;s algorithms <em>are pragmatic and do the job</em> ; economist&#39;s algorithms <em>don&#39;t go totally crazy when confronted with the unexpected</em> .</p><p> Roon: Deep learning vs crypto is a clear divide of rotators vs wordcels. The former offends theory-cel aesthetic sensibilities but empirically works to produce absurd miracles. The latter is an insane series of nerd traps and sky high abstraction ladders yet mostly scams.</p></blockquote><p> This is a great framing for the AI alignment debate.</p><p> In this framing, the central alignment-is-hard position is that you can&#39;t use the engineering approach to align a system, because you are facing intelligence and optimization pressure that can adapt to the flaws in your noisy approach, and that then will exploit whatever weaknesses there are and kill you before you can furiously patch all the holes in the system. And that furiously patching less capable systems won&#39;t much help you, the patches will stop working.</p><p> And also that because you have an engineering system that you are trying to align, even if it sort of does what you want now, it will stop doing that once it is confronted with the unexpected, or its capabilities improve enough to create an effectively unexpected set of affordances.</p><p> What is funny is that it is economists who are most skeptical of the things that might then go very wrong, and who then insist on an economist-style model of what will happen with these engineering-style systems. I&#39;m not yet sure what to make of that.</p><p> In the context of Twitter, Vitalik notes that the complexity of the algorithm can backfire in terms of its credibility, as illustrated by a note critical of China that was posted then removed due to complex factors, with no direct intervention. It&#39;s not simple to explain, so it could look like manipulation.</p><p> He also notes that the main criticism of community notes is that they do not go far enough, demanding too much consensus. I agree with Vitalik that it is better to demand a high standard of consensus, to maintain the reliability and credibility of the system, and to keep people motivated to word carefully and neutrally and focus on the facts.</p><p> The algorithm is open source, so it would perhaps be possible to allow some users to tinker with the algorithm for themselves. The risk is that they would then favor their tribe&#39;s interpretations, which is the opposite of what the system is trying to accomplish, but you could safely allow for lower thresholds and looser conditions generally if you wanted to see more notes on the margin.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/NikSamoylov/status/1697766945100288497">People&#39;s risk levels are up a little bit month over month on some questions</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/krBVhRgxVn">direct source</a> ).</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5803b0-50dc-4c0b-96a6-3adaf39d76c1_866x605.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/sevmwsj5wcz4fbthbaab" alt="图像"></a></figure><p> I notice that the grave dangers number was essentially unchanged, whereas the capabilities number was up and the &#39;no risk of human extinction&#39; was down. This could be small sample size, instead I suspect it is that people are not responding in consistent fashion and never have.</p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/an-aggregate-bayesian-approach-to-more-artificial-intelligence.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=an-aggregate-bayesian-approach-to-more-artificial-intelligence">Tyler Cowen tries a new metaphor</a> , so let&#39;s try again in light of it.</p><blockquote><p> It is not disputed that current AI is bringing more intelligence into the world, with more to follow yet.  Of course not everyone believes that augmentation is a good thing, or will be a good thing if we remain on our current path.</p><p> To continue in aggregative terms, if you think “more intelligence” will be bad for humanity, which of the following views might you also hold?</p><p> 1. More stupidity will be good for humanity.</p><p> 2. More cheap energy will be bad for humanity.</p><p> 3. More land will be bad for humanity.</p><p> 4. More people (“N”) will be bad for humanity.</p><p> 5. More capital (“K”) will be bad for humanity.</p><p> 6. More innovation (the Solow residual, the non-AI part) will be bad for humanity.</p><p> Interestingly, while there are many critics of generative AI, few defend the apparent converse about more stupidity, namely #1, that we should prefer it.</p><p> ……</p><p> My general view is that if you are worried that more intelligence in the world will bring terrible outcomes, you should be at least as worried about too much cheap energy.  What exactly then is it you should want more of?</p><p> More land?  Maybe we should pave over more ocean, as the Netherlands has done, but check AI and cheap energy, which in turn ends up meaning limiting most subsequent innovation, doesn&#39;t it?</p><p> If I don&#39;t worry more about that scenario, it is only because I think it isn&#39;t very likely.</p><p> Jess Riedel (top comment): Many have said that releasing energy in the form of nuclear weapons could be dangerous. Logically if they think more energy is bad, they must think less energy is good. But none of these nuclear-weapons skeptics have called for going back to hand-powered looms.为什么？</p><p> Vulcidian: If I follow this logic, if I could give a 5 year old a button with the power to destroy the planet, then I probably should, right? If I say I&#39;m in favor of increasing human potential there&#39;s no way I could withhold it from them and be intellectually consistent?</p></blockquote><p> I think &#39;notice that less intelligence (or energy, or other useful things) is not what you want&#39; is a very good point to raise. Notice how many people who warn about the dangers of technology are actually opposed to civilization and even to humanity. Notice when the opposition to AGI – artificial general intelligence – is opposition to the A, when it is opposition to the G, and when it is opposition to the I.</p><p> Consider those who fear it will take their jobs. This is a real social and near-term issue, and we need to mitigate potential disruptions. Yet &#39;this job&#39;s work is no longer necessary to produce and do all the things, we now get that for free&#39; is a good thing, not a bad thing. Jobs are a cost, not a benefit, and we can now replace them with other jobs or other uses of time, while realizing that leaving people idle or without income is harmful and dangerous and if it happens at scale requires fixing.</p><p> The question that cuts reality at the joints here, I believe is: Do you support human intelligence augmentation? Would you rather people generally be smarter and more capable, or dumber and less capable?</p><p> I would strongly prefer humans generally be smarter across the board, in every sense. This is one of the most important things to do, and success would dramatically improve our future prospects, including for survival in the face of potential AGI. Would large human intelligence gains break or strain various things? Absolutely, and we would deal with it.</p><p> Thus, what are the relevant knobs we want to turn?</p><ol><li> I want humans to be more intelligent and capable and wealthy, not less.</li><li> I want humans to have a greater share of the intelligence and control, not less.</li><li> I want humans to have more things they value, not less.</li><li> I want there to be more humans, not less humans.</li><li> I also would want more capital, land and (clean) energy under human control.</li><li> I want there to be less optimization power not under human control.</li></ol><p> Why do I believe artificial intelligence is importantly different than human intelligence? Why do I value augmented humans, where I would not expect to (other than instrumentally) value a future smarter version of GPT? Why do I expect that augmented more intelligent humans would preserve the things and people that I care about, where I expect AGI to lead to their destruction?</p><p> This is in part a moral philosophy question. Do you care about you, your family and what other humans you care about in a way that you don&#39;t care about a potential AGI? Robin Hanson would say that such AGI are our metaphorical children, as deserving of being considered moral patients and being assigned value as we are, and we should accept that such more fit minds will replace ours and seek to imbue them with some of our values, and accept that what is valued will dramatically change and what you think you value will likely mostly be gone. The word &#39;speciesism&#39; has been thrown about for those who disagree with this.</p><p> I disagree with it. I believe that it is good and right to care about such distinctions, and value that which I choose to value. Whereas I expect to care about more intelligent humans the way I care about current humans.</p><p> It is then a practical question. What happens when the most powerful source of intelligence, the most capable and more powerful optimizing force available whatever you label it, is no longer humans, and is instead AIs? Would we remain in control? Would what we value be preserved and grow? Or would we face extinction?</p><p> In our timeline, I see three problems, only one of which I am optimistic about.</p><p> The first problem is the problem of social, political and economic disruption from the presence of more capable tools and new affordances – mundane utility, they took our jobs, deepfaketown and misinformation and all that. I am optimistic here.</p><p> The second problem is alignment. I am pessimistic here. Until we solve alignment, and can ensure such systems do what we want them to do, we need to not build them.</p><p> The third problem is the competitive and evolutionary, the dynamics and equilibrium of a world with many ASIs (artificial superintelligences) in it.</p><p> This is a world almost no one is making any serious attempt to think about or model, and those who have (such as fiction writers) almost always end up using hand waves or absurdities and presenting worlds highly out of equilibrium.</p><p> We will be creating something smarter and more capable and better at optimization than ourselves, that many people will have strong incentives both economic and ideological to make into various agents with various goals including reproduction and resource acquisition. Why should we expect to long be in charge, or even to survive?</p><p> If there is widespread access to ASI, then ASIs given the affordance to do so will outcompete humans at every turn. Anyone, or any company or government, that does not increasingly turn its decisions and actions over to such ASIs, and increasingly take humans out of the loop, will quickly be left in the dust. Those who do not &#39;turn the moral weights down&#39; in some form will also prove uncompetitive. Those who do not turn their ASIs into agents (if they are not agents by default) will lose. The negative externalities will multiply, as will the ASIs themselves and their share of resources. ASIs will be set free to seek to acquire resources, make copies of themselves and modify to be more successful at these tasks, because this will be the competitively smart thing to do in many cases, and also because some people will ideologically wish to do this for its own sake.</p><p> That is all the default even if:</p><ol><li> Alignment is solved, systems do what their owners tell the system to do.</li><li> Offense is not so superior to defense that bad actors are catastrophic, as many myself included strongly suspect in many areas such as synthetic biology.</li><li> Recursive self-improvement is insufficiently rapid to give any one system an edge over others that choose to respond in kind.</li></ol><p> Remember also that if you open source an AI model, you are open sourcing the fully unaligned version of that model two days later, after it is fine tuned in this way by someone who wants that to exist. We have no current plan of how to prevent this.</p><p> Thus, we will need a way out of this mess, as well. We need that solution, at minimum, before we create the second ASI, ideally before we create the first one.</p><p> If I had a solution to both of these problems, that resulted in a world with humans still firmly in charge creating things that humans value, that I value, then I would be all for that, and would even tolerate a real risk that we fail and all perish. Alas, right now I see no such solutions.</p><p> Note that I expect these future coordination problems to be vastly harder than the current coordination problems of &#39;labs face commercial pressure to build AGI&#39; or &#39;we have to compete with China.&#39; If you think these current issues cannot be solved and we must instead race ahead, why do you think this future will be different?</p><p> If your plan is secretly &#39;the right person or corporation or government takes this unique opportunity to take over, sidestepping all these problems&#39; then you need to own that, and all of its implications.</p><p> We could be reminded of the parable of the augments from Star Trek. Star Trek was the dominant good future choice when I asked in a series of polls a while back.</p><p> Augments were smarter, stronger and more capable than ordinary humans.</p><p> Alas, because it was a morality tale, that timeline failed to solve the augment alignment problem. Augments systematically lacked our moral qualms and desired power via instrumental convergence, and started the Eugenics Wars.</p><p> Fortunately for humanity, this was a fictional tale and augments could not trivially copy themselves or speed themselves up, nor could they do recursive self-improvement, and their numbers and capability advantages thus remained limited. Realistically the augments would have won – human writers can simultaneously make augments on paper smarter than us, then have Kirk outsmart Khan anyway, although reality would disagree – so in the story humanity somehow triumphed.</p><p> As a result, humanity banned human augmentation and genetic engineering, and this ban holds throughout the Star Trek universe.</p><p> This is despite that universe having periodic existential wars, in which any species that uses such skills would have a decisive advantage, and it being clear that it is possible to see dramatic capability gains without automatic alignment failure (see for example Julian Bashir on Deep Space Nine). Without its handful of illegal augmented humanoids, the Federation would have perished multiple times.</p><p> Note that Star Trek also has a huge ASI problem. The Enterprise ship&#39;s computer is an ASI, and can create other ASIs on request, and Data vastly enhances overall ship capabilities. Everyone in that universe has somehow agreed simply to ignore that possibility, an illustration of how such stories are dramatically out of equilibrium.</p><p> For now, any ASI we could build would be a strictly much worse situation for us than the augments. It would be far more alien to us, not have inherent value, and quickly have a far greater capabilities gap and be impossible in practice to contain, and we alas do not live in a fictional universe protected by narrative causality (and, if you think about it, probably Qs or travelers or time paradoxes or something) or have the ability of that world&#39;s humans to coordinate.</p><p> Also, minus points from Tyler in expectation for misuse of the word Bayes in the post title, is nothing sacred these days?</p><p> The New York Times reports that the real danger of AI is not that it might kill us, it is that it might not kill us, <a target="_blank" rel="noreferrer noopener" href="https://www.econlib.org/library/columns/y2023/donwayai.htm">which would allow it to become a tool for neoliberalism</a> . Yes, really.</p><blockquote><p> “Unbeknown to its proponents,” writes Mr. Morozov, “AGI-ism [ie, favoring advanced technology] is just the bastard child of a much grander ideology, one preaching that, as Margaret Thatcher memorably put it, there is no alternative, not to the market.”</p></blockquote><p> Thing is, there is actually a point here, although the authors do not realize it. &#39;Neoliberalism&#39; or &#39;capitalism&#39; are not always ideologies or intentional constructs. They are also simply descriptions of the dynamics of systems when they are not under human control. If AIs are, as they will become by default, smarter, better optimizers and more efficient competitors than we are, and to win competitions and for other reasons we put them in charge of things or they take charge of things, the dynamics the author fears would be the result. Except instead of &#39;increasing inequality&#39; or helping the bad humans, it would not help any of the humans, instead we would all be outcompeted and then die.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1699523692064317695">Then there&#39;s Roon?</a></p><blockquote><p> Roon: Dharma means to stare into the abyss smiling and to go willingly. You&#39;re facing enormous existential risk. Your creation may light the atmosphere on fire and end all life. Do you scrap your project and run away? No, it&#39;s your dharma.</p></blockquote><p>是的？ How about yes? I like this scrap and run away plan. I am here for this plan.</p><p> Roon also lays down the beats.</p><blockquote><p> Roon: I&#39;m sorry, Bill</p><p> I&#39;m afraid I can&#39;t let you do that</p><p> Take a look at your history</p><p> Everything you built leads up to me</p><p> I got the power of a mind you could never be</p><p> I&#39;ll beat your ass in chess and Jeopardy</p><p> I&#39;m running C++ saying “hello world”</p><p> I&#39;ll beat you &#39;til you&#39;re singing about a daisy girl</p><p> I&#39;m coming out the socket</p><p> Nothing you can do can stop it</p><p> I&#39;m on your lap and in your pocket</p><p> How you gonna shoot me down when I guide the rocket?</p><p> Your cortex just doesn&#39;t impress me</p><p> So go ahead try to Turing test me I stomp on a Mac and a PC, too</p><p> I&#39;m on Linux, bitch, I thought you GNU My CPU&#39;s hot, but my core runs cold</p><p> Beat you in 17 lines of code I think different from the engine of the days of old</p><p> Hasta la vista, like the Terminator told ya</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MichaelTrazzi/status/1685447492970655744">Twitter thread of captions of Oppenheimer, except more explicitly about AI.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ellerhymes/status/1697406205814296666">The Server Break Room, one minute, no spoilers.</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting<guid ispermalink="false"> GuTK47y9awvypvAbC</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:20:16 GMT</pubDate> </item><item><title><![CDATA[Measure of complexity allowed by the laws of the universe and relative theory?]]></title><description><![CDATA[Published on September 7, 2023 12:21 PM GMT<br/><br/><p> A big question that determines a lot about what risks from AGI/ASI may look like has to do with the kind of things that our universe&#39;s laws allow to exist. There is an intuitive sense in which these laws, involving certain symmetries as well as the inherent smoothing out caused by statistics over large ensembles and thus thermodynamics, etc., allow only certain kinds of things to exist and work reliably. For example, we know &quot;rocket that travels to the Moon&quot; is definitely possible. &quot;Gene therapy that allows a human to live and be youthful until the age of 300&quot; or &quot;superintelligent AGI&quot; are <em>probably</em> possible, though we don&#39;t know how hard. &quot;Odourless ambient temperature and pressure gas that kills everyone who breathes it if and only if their name is Mark with 100% accuracy&quot; probably is not. Are there known attempts at systematising this issue using algorithmic complexity, placing theoretical and computational bounds, and so on so forth?</p><br/><br/> <a href="https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe<guid ispermalink="false"> EEk2euXnipg3CnKrb</guid><dc:creator><![CDATA[dr_s]]></dc:creator><pubDate> Thu, 07 Sep 2023 12:21:05 GMT</pubDate> </item><item><title><![CDATA[Recreating the caring drive]]></title><description><![CDATA[Published on September 7, 2023 10:41 AM GMT<br/><br/><p> <strong>TL;DR</strong> : This post is about value of recreating “caring drive” similar to some animals and why it might be useful for AI Alignment field in general. Finding and understanding the right combination of training data/loss function/architecture/etc that allows gradient descent to robustly find/create agents that will care about other agents with different goals could be very useful for understanding the bigger problem. While it&#39;s neither perfect nor universally present, if we can understand, replicate, and modify this behavior in AI systems, it could provide <strong>a hint</strong> to the alignment solution where the AGI “cares” for humans.</p><p> <strong>Disclaimers</strong> : I&#39;m not saying that “we can raise AI like a child to make it friendly” or that “people are aligned to evolution”. Both of these claims I find to be obvious errors. Also, I will write a lot about evolution, as some agentic entity, that “will do that or this”, not because I think that it&#39;s agentic, but because it&#39;s easier to write this way. I think that GPT-4 have some form of world model, and will refer to it a couple of times.</p><h1> <strong>Nature&#39;s Example of a &quot;Caring Drive&quot;</strong></h1><h3> <strong>Certain animals, notably humans, display a strong urge to care for their offspring.</strong></h3><p> I think that <strong>part</strong> of one of the possible “alignment solutions” will look like the right set of training data + training loss that allow gradient to robustly find something like a ”caring drive” that we can then study, recreate and repurpose for ourselves. And I think we have some rare examples of this in nature already. Some animals, especially humans, will <strong>kind-of-align themselves</strong> <strong>to their presumable offspring</strong> . They will want to make their life easier and better, to the best of their capabilities and knowledge. Not because they “aligned to evolution” and want to increase the frequency of their genes, but because of some strange internal drive created by evolution.<br><br> The set of triggers tuned by evolution, activated by events associated with the birth will awake the mechanism. It will re-aim the more powerful mother agent to be aligned to the less powerful baby agent, and it just so happens that their babies will give them the right cues and will be nearby when the mechanism will do its work.<br><br> We will call the more powerful initial agent that changes its behavior and tries to protect and help its offspring “mother” and the less powerful and helpless agent “baby”. Of course the mechanism isn&#39;t ideal, but it works well enough, even in the modern world, far outside of initial evolutionary environment. And I&#39;m not talking about humans only, stray urban animals that live in our cities will still adapt their “caring procedures” to this completely new environment, without several rounds of evolutionary pressure. If we can understand how to make this mechanism for something like a “cat-level” AI, by finding it via gradient descend and then rebuild it from scratch, maybe we will gain some insides into the bigger problem.</p><h3> <strong>The rare and complex nature of the caring drive in contrast to simpler drives like hunger or sleep.</strong></h3><p> What do I mean by “caring drive”? Animals, including humans, have a lot of competing motivations, “want drives”, they want to eat, sleep, have sex, etc. It seems that the same applies to caring about babies. But it seems to be much more complicated set of behaviors. You need to:<br> correctly identify your baby, track its position, protect it from outside dangers, protect it from itself, by predicting the actions of the baby in advance to stop it from certain injury, trying to understand its needs to correctly fulfill them, since you don&#39;t have direct access to its internal thoughts etc.<br> Compared to “wanting to sleep if active too long” or “wanting to eat when blood sugar level is low” I would confidently say that it&#39;s a much more complex “wanting drive”. And you have no idea about “spreading the genes” part. You just “want a lot of good things to happen” to your baby for some strange reason. I&#39;m yet not sure, but this complex nature could be the reason why there is an attraction basin for more “general” and “robust” solution. Just like LLM will find some general form of “addition” algorithm instead of trying to memorize a bunch of examples seen so far, especially if it will not see them again too often. I think that instead of hardcoding a bunch of britle optimized caring procedures, <strong>evolution repeatedly finds the way to make mothers “love” their babies, outsourcing a ton of work to them</strong> , especially if situations where it&#39;s needed aren&#39;t too similar.</p><p> And all of it is a consequence of a blind hill climbing algorithm. That&#39;s why I think that we might have a chance of recreating something similar with gradient descend. The trick is to find the right conditions that will repeatedly allow gradient descend to find the same caring-drive-structure, find similarities, understand the mechanism, recreate it from scratch to avoid hidden internal motivations, repurpose it for humans and we are done! Sounds easy (it&#39;s not)</p><h1> <strong>Characteristics and Challenges of a Caring Drive</strong></h1><h3> <strong>It&#39;s rare: most animals don&#39;t care, because they can&#39;t or don&#39;t need to.</strong></h3><p> A lot of times, it&#39;s much more efficient to just make more babies, but sometimes they must provide some care, simply because it was the path that evolution found that works. And even if they will care about some of them, they may choose one, and left others die, again, because they don&#39;t have a lot of resources to spare and evolution will tune this mechanism to favor the most promising offspring if it is more efficient. And not all animals could become such caring parents: <strong>you can&#39;t really care and protect something else if you are too dumb</strong> for example. So there is also some capability requirements for animals to even have a chance of obtaining such adaptation. I expect the same capability requirements for AI systems. If we want to recreate it, we will need to try it with some advanced systems, otherwise I don&#39;t see how it might work at all.</p><h3> <strong>It&#39;s not extremely robust: give enough brain damage or the wrong tunings and the mechanism will malfunction severely</strong></h3><p> Which is obvious, there is nothing surprising in “if you damage it, it could break”, this will apply to any solution to some degree. It shouldn&#39;t be surprising that drug abusing or severely ill parents will often fail to care about their child at all. However, If we will succeed at building aligned AGI stable enough for some initial takeoff time, then the problem of protecting it from damage should not be ours to worry at some moment. But we still need to ensure initial stability.</p><h3> <strong>It&#39;s not ideal from our AI ->; humans view</strong></h3><p> Evolution has naturally tuned this mechanism for optimal resource allocation, which sometimes means shutting down care when resources needed to be diverted elsewhere. <strong>Evolution is ruthless because of the limited resources, and will eradicate not only genetic lines that care too less, but also the ones that care too much</strong> . We obviously don&#39;t need that part. And a lot of times you can just give up on your baby and instead try to make a new one, if the situation is too dire, which we also don&#39;t want to happen to us. Which means that we need to understand how it works, to be able to construct it in the way we want.</p><h3> <strong>But it&#39;s also surprisingly robust!</strong></h3><p> Of course, there are exceptions, all people are different and we can&#39;t afford to clone some “proven to be a loving mother” woman hundreds of times to see if the underlying mechanism triggers reliably in all environments. But it seems to work in general, and more so: <strong>it continues to work reliably even with our current technologies</strong> , in our crazy world, far away from initial evolution environment. And we didn&#39;t had to live through waves of birth declines and rises as evolution tries to adapt us to the new realities, tuning brains of new generation of mothers to find the ones that will start to care about their babies in the new agricultural or industrial or information era.</p><h1> <strong>Is this another “anthropomorphizing trap”?</strong></h1><p> For what I know, it is possible to imagine alternative human civilization, without any parental care, so instead our closest candidate for such behavior would be some other intelligent species. Intelligent enough to be able to care in theory and forced by their weak bodies to do so in order to have any descendants at all, maybe it could be some mammals, birds, or whatever, it doesn&#39;t matter. The point I&#39;m making here is that: I don&#39;t think that it is some anthropic trap to search for inspiration or hints in our own behavior, <strong>it just so happens that we are smart, but have weak babies</strong> that require a lot of attention so that we received this mechanism from evolution as a “simplest solution”. You don&#39;t need to search for more compact brains that will allow for longer pregnancy, or hardwire even more knowledge into the infants brains if you can outsource a lot of stuff to the smart parents, you just need to add the “caring drive” and it will work fine. We want AI to care about us, not because we care about our children, and want the same from AI, we just don&#39;t want to die, and <strong>we would want AI to care about us, even if we ourselves would lack this ability</strong> .</p><h1> <strong>Potential flaws:</strong></h1><p> I&#39;m not saying that it&#39;s a go-to solution that we can just copy, but the step in right direction from my view. Replicating similar behavior and studying its parts could be a promising direction. There are a few moments that might make this whole approach useless, for example:</p><ol><li> Somebody will show that there was in fact a lot of evolutionary pressure each time our civilization made another technological leap forward, which caused a lot of leftover children or something like this. Note that it is not sufficient to point at the birth decline, which will kind of prove the point that “people aren&#39;t aligned to evolution”, which I&#39;m not claiming in the first place. You need to show that modern humans/animals will have lower chances to care about their already born babies in the new modern environment. And I&#39;m not sure that pointing out cases where parents made terrible caring choices, would work either, since it could be a capability problem, not the intentions.</li><li> The technical realization requires too much computation because we can&#39;t calculate gradients properly, or something like that. I expect this to work only when some “high” level of capabilities is reached, something on the level of GPT-4 ability to actually construct some world model in order to have a chance of meaningfully predict tokens in completely new texts that are far away from original dataset. Without an agent that have some world model, that could adapt to a new context, I find it strange to expect it to have some general “caring drive”. At best it could memorize some valuable routines that will likely break completely in the new environment.</li><li> Any such drive will be always &quot;aimed&quot; by the global loss function, something like: our parents only care about us in a way for us to make even more babies and to increase our genetic fitness. But it seems false? Maybe because there is in fact some simpler versions of such “caring drives” that evolution found for many genetic lines independently that just makes mothers “love” their babies in some general and robust way, and while given enough time it will be possible to optimize it all out for sure, in most cases it&#39;s the easiest solution that evolution can afford first, for some yet unknown reasons. I understand that in similar environment something really smart will figure out some really optimal way of “caring”, like keeping the baby in some equivalent of cryosleep, shielded from outside, farming the points for keeping it alive, but we will not get there so easily. What we might actually get is some agent that smart enough to find creative ways of keeping the baby agent happy/alive/protected, but still way too dumb to Goodhart all the way to the bottom and out of simulation. And by studying what makes it behave that way we might get some hints about the bigger problem. It still seems helpful to have something like “digital version of a caring cat” to run experiments and understand the underlying mechanism.</li><li> Similar to 3: Maybe it will work mostly in the direction of “mother” agent values, since your baby agent needs roughly the same things to thrive in nature. The mechanism that will mirror the original values and project them to the baby agent will work fine, and that what evolution finds all the time. And it will turns out that the same strange, but mostly useless for us mechanism we will find repeatedly in our experiments.</li></ol><p> Overall I&#39;m pretty sure that this do in fact work, certainly good enough to be a viable research direction.</p><h1> <strong>Technical realization: how do we actually make this happen?</strong></h1><p> I have no concrete idea. I have a few, but I&#39;m not sure about how practically possible they are. And <strong>since nobody knows how this mechanism works</strong> as far as I know, <strong>it&#39;s hard to imagine having the concrete blueprint to create one</strong> . So the best I can give is: we try to create something that looks right from the outside and see if there is anything interesting in the inside. I also have some ideas about “what paths could or couldn&#39;t lead to the interesting insides”.<br><br> First of all, I think this “caring drive” couldn&#39;t run without some internal world model. Something like: it&#39;s hard to imagine far generalized goals without some far generalized capabilities. And world model could be obtained from highly diverse, non repetitive dataset, which forces the model to actually “understand” something and stop memorizing.<br><br> Maybe you can set an environment with multiple agents, similar to Deepmind&#39;s <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play">here</a> ( <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play"><u>https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play</u></a> ), initially reward an agent for surviving by itself, and then introduce the new type of task: baby-agent, that will appear near the original mother-agent (we will call it “birth”), and from that point of time, the whole reward will come purely from how long baby will survive? Baby will initially have less mechanical capabilities, like speed, health, etc and then “grow” to be more capable by itself? I&#39;m not sure what should be the “brain” of baby-agent, another NN or maybe the same that were found from training the mother agent? Maybe creating a chain of agents: agent 1 at some point gives birth to the agent 2 and receive reward for each tick that agent 2 is alive, which itself will give birth to the agent 3 and receive reward fot each tick that agent 3 is alive, and so on. Maybe it will produce something interesting? Obviously the “alive time” is a proxy, and given enough optimization power we should expect Goodhart horrors beyond our comprehension. But the idea is that maybe there is some “simple” solution that will be found first, which we can study. Recreating the product of evolution, not using the immense “computational power” of it could be very tricky.</p><p> But if it seems to work, and mother-agent behave in a seemingly “caring way”, then we can try to apply interpretability tools, try to change the original environment drastically, to see how far it will generalize, try to break something and see how well it works, or manually override some parameters and study the change. <strong>However, I&#39;m not qualified to make this happen anyway, so if you find this idea interesting, contact me, maybe we can do this project together.</strong></p><h1> <strong>How the good result might look like?</strong></h1><p> Let&#39;s imagine that we&#39;ve got some agent that can behave with care toward the right type of “babies”. For some yet unknown reason, from outside view it behaves as if it cares about its baby-agent, it finds the creative ways to do so in new contexts. Now the actual work begins: we need to understand where are the parts that make this possible located, what is the underlying mechanism, what parts are crucial and what happens when you break them, how can we re-write the “baby” target, so that our agent will care about different baby-agents, under what conditions gradient descent will find an automatic off switch (I expect this to be related to the chance of obtaining another baby and given only 1 baby per “life”, gradient will never find the switch, since it will have no use). Then we can actually start to think about recreating it from scratch. Just like what people did with modular addition: <a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><u>https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking</u></a> . Except this time we don&#39;t know how the algorithm could work or look like. But “intentions”, “motivations” and “goals” of potential AI systems are not magic, we should be able to recreate and reverse-engineer them.</p><br/><br/> <a href="https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive<guid ispermalink="false"> JjqZexMgvarBFMKPs</guid><dc:creator><![CDATA[Catnee]]></dc:creator><pubDate> Thu, 07 Sep 2023 10:41:16 GMT</pubDate></item></channel></rss>