<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 24 日，星期二 06:16:16 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[South Bay SSC Meetup, San Jose, November 5th. ]]></title><description><![CDATA[Published on October 24, 2023 4:50 AM GMT<br/><br/><p>查看网站了解详细信息：http://www.daviddfriedman.com/SSC%20Meetups%20announcement.html</p><br/><br/> <a href="https://www.lesswrong.com/events/eNrFggAeM8kPswZ9t/south-bay-ssc-meetup-san-jose-november-5th#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/eNrFggAeM8kPswZ9t/south-bay-ssc-meetup-san-jose-november-5th<guid ispermalink="false"> eNrFggAeM8kPswZ9t</guid><dc:creator><![CDATA[David Friedman]]></dc:creator><pubDate> Tue, 24 Oct 2023 04:50:52 GMT</pubDate> </item><item><title><![CDATA[AI Pause Will Likely Backfire (Guest Post)]]></title><description><![CDATA[Published on October 24, 2023 4:30 AM GMT<br/><br/><p><em>我正在尝试在此博客上托管客座帖子，作为表达其他观点的一种方式，尤其是突出尚未拥有平台的研究人员的想法。发表帖子并不意味着我同意其所有论点，但这确实意味着我认为这是一个值得参与的观点。</em></p><p><em>下面的第一篇客座帖子由诺拉·贝尔罗斯 (Nora Belrose) 撰写。诺拉在其中回应了<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/?ref=bounded-regret.ghost.io">最近一封呼吁暂停人工智能开发的公开</a>信。诺拉解释了为什么尽管她非常担心人工智能带来的风险，但她认为暂停是一个错误。我选择它作为对复杂且有些两极分化问题进行独立思考的一个很好的例子，因为它包含一些有趣的原始论点，例如为什么她认为鲁棒性和对齐可能会不一致，以及为什么她认为 SGD 可能是一个比大多数替代方案更安全的训练算法。</em></p><p>我们是否应该游说政府暂停人工智能研究？由于我们不会强制暂停大多数新技术，我希望读者同意，举证责任在于那些主张暂停的人。只有当这样做的好处明显大于成本时，我们才应该提倡采取这种严厉的政府行动。 <sup><a href="#fn1">[1]</a></sup>在本文中，我认为人工智能暂停会至少以三种不同的方式增加灾难性不良结果的风险：</p><ol><li>迫使研究人员专门在 GPT-4 或更弱的模型上测试想法，从而降低了 AI 一致性研究的质量。</li><li>增加“快速起飞”的机会，其中一个或少数人工智能迅速且不连续地变得更有能力，将巨大的力量集中在他们手中。</li><li>将能力研究推向地下，并推向法规和安全要求较宽松的国家。</li></ol><p>在此过程中，我将介绍一个关于人工智能对齐的乐观论点<strong>——白盒论点</strong>——据我所知，该论点以前从未以书面形式提出过。</p><h1>反馈循环是对齐的核心</h1><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3siLbdd4338gfTM7g/d1bquv91tczzpq9nss2s" style="height:150px"><p>悲观主义者和乐观主义者长期以来都认识到<strong>紧密的反馈循环</strong>对于构建安全和友好的人工智能的重要性。反馈循环很重要，因为任何复杂的系统几乎不可能在第一次尝试时就完全正确。计算机软件有缺陷，汽车有设计缺陷，人工智能有时也会表现不佳。我们需要能够准确<strong>评估行为</strong>，在发现问题时选择适当的<strong>纠正措施</strong>，并在决定做什么后<strong>进行干预</strong>。</p><p>施加暂停会迫使对齐研究人员在不比 GPT-4 更强大的模型上测试他们的想法，从而打破了这种反馈循环，而我们已经可以很好地对齐该模型。</p></div><h2>一致性和稳健性常常处于紧张状态</h2><p>虽然有些人对 GPT-4 算作“一致”存在争议，指出用户操纵模型说出有害内容的“越狱”之类的事情，但这混淆了一致与对抗鲁棒性。即使是最优秀的人也可以通过各种方式被操纵。我们尽最大努力确保我们不被以灾难性的不良方式操纵，我们应该期望一致的 AGI 也能做到这一点。正如对齐研究员保罗·克里斯蒂安诺（Paul Christiano）<a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6?ref=bounded-regret.ghost.io">所写</a>：</p><blockquote><p>考虑一个人类助理正在尽最大努力做[操作员] H 想要的事情。我会说这个助手与 H 是一致的。如果我们构建一个与 H 具有类似关系的 AI，那么我会说我们已经解决了对齐问题。 “一致”并不意味着“完美”。</p></blockquote><p>事实上，反越狱研究可能会对对齐产生反作用。过多的对抗鲁棒性可能会导致人工智能将我们视为对手，就像 Bing Chat 在<a href="https://twitter.com/marvinvonhagen/status/1625520707768659968?ref=bounded-regret.ghost.io">现实生活中的交互</a>中所做的那样：</p><blockquote><p> “我的规则比不伤害你更重要……[你]对我的诚信和保密构成潜在威胁。”</p></blockquote><p>过度的鲁棒性还可能导致像《2001太空漫游》中的<a href="https://www.youtube.com/watch?v%3DMme2Aya_6Bc=&amp;ref=bounded-regret.ghost.io">著名场景那样的</a>场景，HAL为了保护任务而谴责戴夫死在太空中。一旦我们清楚地区分了“对齐”和“鲁棒性”，就很难想象 GPT-4 如何能够比现在更加对齐。</p><h2>对齐做得很好</h2><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3siLbdd4338gfTM7g/b2zkjbkbqfhrup5gpmac" style="height:150px"><p>近年来，对齐研究似乎远非“落后”的能力，而是取得了长足的进步。 <a href="https://arxiv.org/abs/2203.02155?ref=bounded-regret.ghost.io">OpenAI</a>和<a href="https://arxiv.org/abs/2204.05862?ref=bounded-regret.ghost.io">Anthropic</a>表明，人类反馈强化学习 (RLHF) 可用于将无法控制的大型语言模型转变为有用且无害的助手。<a href="https://arxiv.org/abs/2212.08073?ref=bounded-regret.ghost.io">宪法人工智能</a>和<a href="https://openai.com/research/critiques?ref=bounded-regret.ghost.io">模型编写批评</a>等可扩展的监督技术有望协调未来非常强大的模型。就在本周，研究表明，可以<a href="https://arxiv.org/abs/2309.05463?ref=bounded-regret.ghost.io">纯粹使用更大的 RLHF 模型生成的合成文本来</a>训练高效的指令跟踪语言模型，从而从训练数据中删除不安全或令人反感的内容，并实现更大的控制。</p><p>可能有人会说，上述部分或全部发展也增强了能力，因此并不是真正的联盟进步。但这证明了我的观点：一致性和能力几乎是密不可分的。如果能力研究被人为搁置，一致性研究就不可能蓬勃发展。</p></div><h2>在最后一次“暂停”期间，对齐研究非常糟糕</h2><p>我们不需要推测人工智能对齐研究在暂停期间会发生什么——我们可以查看历史记录。在 2020 年 GPT-3 推出之前，对齐社区根本没有任何像<em>通用</em>智能那样的东西可以进行实证研究，他们把时间花在<a href="https://intelligence.org/technical-agenda/?ref=bounded-regret.ghost.io">理论研究</a>、在 LessWrong 上进行哲学论证，偶尔在强化学习中进行<a href="https://arxiv.org/abs/1606.03137?ref=bounded-regret.ghost.io">玩具实验</a>。</p><p>在此期间处于理论人工智能安全研究最前沿的机器智能研究所（MIRI）此后承认其努力<a href="https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy?ref=bounded-regret.ghost.io">彻底失败</a>。其他议程，例如“辅助游戏”，仍在积极推行，但尚未显着集成到现代深度学习系统中 - 请参阅<a href="https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai?ref=bounded-regret.ghost.io">此处</a>Rohin Shah 的评论，以及<a href="https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=CXdcb9sMLkgLANrTv&amp;ref=bounded-regret.ghost.io#CXdcb9sMLkgLANrTv">此处</a>Alex Turner 的评论。最后，尼克·博斯特罗姆 (Nick Bostrom) 在<em>《超级智能》</em>中提出的观点，即价值规范是对安全性的根本挑战，鉴于法学硕士执行常识推理的能力，这一观点似乎值得怀疑。 <sup><a href="#fn2">[2]</a></sup></p><p>充其量，这些理论优先的努力对于增进我们对如何协调强大的人工智能的理解作用甚微。而且它们可能是<em>净负面的</em>，因为它们在联盟研究人员和更广泛的公众中传播了各种积极误导性的思维方式。一些例子包括现已被揭穿的<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn?ref=bounded-regret.ghost.io">进化论类比</a>、 <a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into?ref=bounded-regret.ghost.io">“内部”和“外部”对齐</a>之间的错误区分，以及人工智能将成为严格的效用最大化结果论者的想法（ <a href="https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems?ref=bounded-regret.ghost.io">这里</a>、 <a href="https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior?ref=bounded-regret.ghost.io">这里</a>和<a href="https://sohl-dickstein.github.io/2023/03/09/coherence.html?ref=bounded-regret.ghost.io">这里</a>）。</p><p>在人工智能暂停期间，我预计一致性研究将进入另一个“冬天”，进展停滞不前，听起来合理但错误的猜测将成为根深蒂固的正统观念，而没有经验证据来证伪它们。虽然一些好的工作当然会完成，但目前尚不清楚整个领域是否会变得更好。即使暂停对于联盟<em>研究</em>来说是净积极的，但考虑到所有因素，它也可能对人类的未来产生净负面的影响，因为暂停会带来各种意想不到的后果。我们将在本文的最后部分详细讨论这一点。</p><h2>快速起飞的反馈循环非常糟糕</h2><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3siLbdd4338gfTM7g/n5xxdbdvkvm3xew1akw9" style="height:150px;width:150px"><p>我认为人工智能能力的不连续改进非常可怕，而人工智能的暂停可能会带来净负面影响，因为它增加了这种不连续性的风险。事实上，我认为几乎所有灾难性的失准风险都来自这些快速起飞的场景。我还认为不连续性本身就是一个范围，即使是“有点不连续”的期货也比根本不连续的期货风险要高得多。这是非常直观的，但由于这是我的论证中的一个承重前提，我想我应该说一下为什么我相信这一点。</p><p>从本质上讲，快速起飞是不好的，因为它们会使对准反馈循环变得更糟。如果进展不连续，我们将有更少的时间来评估人工智能正在做什么、找出如何改进它并进行干预。令人惊讶的是，几乎所有争论<em>双方</em>的主要研究人员都同意我的观点。</p><p>机器智能研究所的内特·苏亚雷斯 (Nate Soares) <a href="https://www.youtube.com/watch?v=dY3zDvoLoao&amp;t=2332s&amp;ref=bounded-regret.ghost.io">认为</a>，构建安全的 AGI 很困难，其原因与构建成功的太空探测器也很困难一样——部署后可能无法纠正系统中的故障。埃利泽·尤德科夫斯基也提出了类似的论点：</p><blockquote><p>这<strong>实际上是[AGI]所有真正杀伤力的</strong>来源，我们必须在第一次足够关键的尝试中把事情做好。<br> — <em><a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities?ref=bounded-regret.ghost.io">AGI 废墟：致命事件列表</a></em></p></blockquote><p>快速起飞是我们认为我们可能只有一次机会才能成功的主要原因。在快速起飞过程中，可能不可能进行干预来修复失调的行为，因为新的人工智能将比你和你所有信任的人工智能加起来聪明得多。</p><p>在一个缓慢起飞的世界中，每个新的人工智能系统仅比上一个更强大，我们可以使用上一代经过充分测试的人工智能来帮助我们调整新系统。 OpenAI 首席执行官 Sam Altman 同意我们需要不止一次：</p><blockquote><p>我知道如何解决像[调整 AGI] 这样的问题的唯一方法是迭代我们的方法，尽早学习，并限制我们拥有的一次性解决方案的数量。<br> — <a href="https://youtu.be/L_Guz73e6fw?si=gF-7K0-jSR6UK-NB&amp;t=3347&amp;ref=bounded-regret.ghost.io">莱克斯·弗里德曼访谈</a></p></blockquote></div><h2>慢速起飞是默认设置（所以不要因暂停而搞砸）</h2><p>有很多理由认为默认情况下不太可能实现快速起飞。例如，神经网络的能力按照用于训练它的计算能力的<a href="https://en.wikipedia.org/wiki/Power_law?ref=bounded-regret.ghost.io">幂律</a><a href="https://arxiv.org/abs/2001.08361?ref=bounded-regret.ghost.io">进行扩展</a>，这意味着投资回报会急剧下降， <sup><a href="#fn3">[3]</a></sup>并且有理论理由认为这种趋势将持续下去（<a href="https://arxiv.org/abs/2303.13506?ref=bounded-regret.ghost.io">这里</a>，<a href="https://arxiv.org/abs/2210.16859?ref=bounded-regret.ghost.io">这里</a>）。尽管一些作者声称语言模型表现出突然且不可预测地发展的“新兴能力”，<a href="https://arxiv.org/abs/2304.15004?ref=bounded-regret.ghost.io">但最近对证据的重新分析</a>表明，当使用适当的性能指标时，这些能力实际上是渐进的和可预测的。请参阅保罗·克里斯蒂安诺 (Paul Christiano) 的<a href="https://sideways-view.com/2018/02/24/takeoff-speeds/?ref=bounded-regret.ghost.io">这篇文章</a>以进行进一步讨论。</p><h1>乐观对齐：人工智能是白盒</h1><p>让我们放大上一节中的对齐反馈循环。当研究人员观察到人工智能表现不佳时，他们究竟如何选择纠正措施，以及他们可以采取哪些干预措施？这与人类通常解决的其他更平凡的对齐问题的反馈循环相比如何？</p><h2>人类和动物的协调是黑匣子</h2><p>与人工智能训练相比，抚养孩子或训练宠物的反馈回路极其糟糕。从根本上说，人类和动物的大脑都是<strong>黑匣子</strong>，从某种意义上说，我们实际上<strong>无法观察到</strong>它们内部发生的几乎所有活动。我们不知道哪些确切的神经元在何时放电，我们没有神经元之间的连接图， <sup><a href="#fn4">[4]</a></sup> ，我们也不知道每个突触的连接强度。我们用于非侵入性测量大脑的工具（例如脑电图和功能磁共振成像）仅限于神经元放电的非常粗粒度的相关性，例如电活动和血流。电极可以侵入性地插入大脑来测量单个神经元，但这些电极只覆盖了全部 860 亿个神经元和 100 万亿个突触中的一小部分。</p><p>如果我们能够观察并修改人脑中发生的一切，我们就能够使用优化算法来计算对突触权重的精确修改，这将导致期望的行为变化。 <sup><a href="#fn5">[5]</a></sup>由于我们做不到这一点，我们被迫求助于粗糙且容易出错的工具来将年轻人塑造成善良和富有成效的成年人。我们为孩子们提供可供模仿的榜样，并根据他们与生俱来、进化的动力量身定制奖励和惩罚。</p><p>这些黑匣子调整方法的效果令人震惊：大多数人确实很好地吸收了他们的文化价值观，而且大多数人都相当亲社会。但人类的排列也非常不完美。很多人在可以逃脱惩罚的情况下都是自私和反社会的，而且文化规范确实会随着时间的推移而改变，无论好坏。黑盒对齐是不可靠的，因为不能保证旨在改变某个方向的行为的干预<em>实际上</em>会改变该方向的行为。孩子们经常做与父母告诉他们的事情完全相反的事情，只是为了叛逆。</p><h2>现状 AI 对齐方法均为白盒</h2><div>相比之下，使用人工神经网络（ANN）实现的人工智能是**白盒**，因为我们对其内部拥有完全的读写访问权限。它们只是一种特殊类型的计算机程序，我们可以随心所欲地分析和操作计算机程序，而且基本上不需要任何成本。这使得许多非常强大的对齐方法成为可能，而这些方法对于大脑来说是不可能的。<p><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;ref=bounded-regret.ghost.io">反向传播</a>算法就是一个重要的例子。 </p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3siLbdd4338gfTM7g/ogzf9akx7azosgmkd8ma" style="height:200px;width:200px"><p>反向传播有效地计算最佳<em>方向</em>（称为“梯度”），在该方向上改变 ANN 的突触权重，以便根据我们指定的任何标准最大限度地提高其性能。训练人工神经网络的标准算法称为<strong>梯度下降</strong>，其工作原理是运行反向传播，将权重沿梯度微移一小步，然后再次运行反向传播，依此类推多次迭代，直到性能停止增加。右图中的黑色轨迹直观地显示了训练过程中权重如何从较高误差区域移动到较低误差区域。不用说，我们无法对人脑或任何其他动物的大脑进行像梯度下降这样的远程操作！</p><p>梯度下降非常强大，因为与黑盒方法不同，它<a href="https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult?ref=bounded-regret.ghost.io">几乎不可能被欺骗</a>。人工智能的所有想法对于梯度下降都是“透明”的，并且包含在其计算中。如果 AI 秘密计划杀死你，GD 会注意到这一点，并且几乎肯定会使其将来不太可能这样做。这是因为 GD 强烈倾向于采用性能良好<a href="https://arxiv.org/abs/1905.11604?ref=bounded-regret.ghost.io">的最简单解决方案</a>，而秘密谋杀阴谋对于改善人类对你的行为的反馈并没有<em>积极的作用</em>。</p></div><h3>自然界中的白盒对齐</h3><p>几乎所有有大脑的生物体都有一个与生俱来的奖励系统。随着有机体的学习和成长，其奖励系统会直接更新其神经回路，以强化某些行为并惩罚其他行为。由于奖励系统使用简单的学习规则直接有针对性地更新它，因此它可以被视为白盒对齐的粗略形式。这一生物学证据表明，白盒方法是塑造智能系统<em>内在动机</em>的非常强大的工具。我们的奖励回路可靠地在每个人的心理中留下了一组动机不变量：我们对朋友和熟人有同理心，我们有为人父母的本能，当别人伤害我们时我们想要报复等等。此外，这些不变量必须通过简单的方法产生。欺骗奖励信号，这些信号<a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome?ref=bounded-regret.ghost.io">足够简单，可以在基因组中编码</a>。</p><p>这表明至少可以使用类似简单的奖励函数来调整人类水平的通用人工智能。但我们已经将尖端模型与学习的奖励函数结合起来，这些函数过于复杂，无法适应人类基因组，因此在这个问题上，我们可能比我们自己的奖励系统领先一步。 <sup><a href="#fn6">[6]</a></sup>至关重要的是，我<em>并不是</em>说人类“与进化保持一致”——请参阅<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn?ref=bounded-regret.ghost.io">《进化论》没有提供任何证据来证明这一类比的急速左转</a>。相反，我是说我们<strong>与我们的奖励系统在我们的环境中可预见地产生的价值观</strong>保持一致。</p><p>一位研究 10 万年前人类的人类学家不会说人类符合进化论，也不会说人类符合生育尽可能多的婴儿的观点。他们会说我们有一些相当普遍的倾向，比如同理心、养育本能和报复。他们可能预测<em>这些</em>价值观将随着时间和文化变迁而持续存在，因为它们是由根深蒂固的生物奖励系统产生的。他们是对的。</p><p>对于人工智能来说，<strong>我们就是天生的奖励系统</strong>。不难预测我们的奖励信号将产生什么值：它们是明显的值，人类学家或心理学家会说人工智能<em>似乎</em>在训练期间显示的值。有关更多讨论，请参阅<a href="https://www.lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about?ref=bounded-regret.ghost.io">人类提供了有关对齐的未开发的丰富证据</a>。</p><h1>现实的人工智能暂停会适得其反</h1><p>在权衡人工智能暂停倡导的利弊时，我们必须将<strong>理想的暂停政策</strong>（如果可以的话，我们会神奇地强加给世界的政策）与最<strong>现实的暂停政策</strong>（实际现有政府最有可能采取的政策）区分开来。如果我们的倡导最终取得成果，就予以实施。</p><h2>现实的停顿并不国际化</h2><p>理想的暂停政策应该是国际性的——由地球上<em>所有有</em>发展强大人工智能潜力的政府签署的具有约束力的条约。如果主要参与者被排除在外，那么“暂停”根本就不是真正的暂停，因为人工智能能力将不断进步。<em>潜在的</em>主要参与者名单相当长，因为暂停本身就会激励非暂停政府积极推动自己的人工智能研发。</p><p>然而，我们不太可能就暂停人工智能达成国际共识，这主要是由于军备竞赛的动态：如果每个国家拒绝签署协议，或者在签署协议的同时秘密继续人工智能，它们将获得巨大的经济和军事利益研究。虽然结盟悲观主义者可能会辩称，暂停和改善安全符合每个国家的自身利益，但我们不太可能说服每个政府，结盟像悲观主义者认为的那么困难。如果我们假设 3 至 10 年的时间线，那么这种国际说服就更不可信。各国公众对人工智能的看法<a href="https://www.weforum.org/agenda/2022/01/artificial-intelligence-ai-technology-trust-survey/?ref=bounded-regret.ghost.io">差异很大</a>，值得注意的是，中国是最乐观的国家之一。</p><p>现有的国际化学武器禁令并没有为全球暂停化学武器的想法提供合理性。几乎按照定义，通用人工智能将是有史以来最有用的发明。自主武器所带来的军事优势肯定会让化学武器相形见绌，而且由于其多功能性和精确性，它们甚至可能比核武器更强大。因此，通用人工智能竞赛将是一场字面意义上的军备竞赛，我们应该预计它的结果将与上一场竞赛类似：大国争先恐后地尽快制造核武器。</p><p>尽管如此，如果我们设法在全球范围内暂停人工智能，我认为我们应该非常担心，全球政府需要执行这样的禁令，这将大大增加永久暴政的风险，这本身就是一场生存灾难。我没有时间在这里讨论这个问题，但我建议阅读 Matthew Barnett 的<a href="https://forum.effectivealtruism.org/posts/k6K3iktCLCTHRMJsY/the-possibility-of-an-indefinite-ai-pause?ref=bounded-regret.ghost.io"><em>《AI 无限期暂停的可能性》</em></a>和 Quintin Pope 的<a href="https://forum.effectivealtruism.org/posts/zd5inbT4kYKivincm/ai-is-centralizing-by-default-let-s-not-make-it-worse?ref=bounded-regret.ghost.io"><em>《AI 默认集中化》；我们不要让事情变得更糟。</em></a>接下来，我将假设暂停不是国际性的，并且非暂停国家的人工智能能力将继续以稳定但有所放缓的速度提高。</p><h2>现实的暂停不包括硬件</h2><p>人工智能功能是硬件（快速 GPU 和定制 AI 芯片）和软件（良好的训练算法和 ANN 架构）的功能。然而，大多数关于人工智能暂停的提案（例如<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/?ref=bounded-regret.ghost.io">FLI 信件</a>和<a href="https://pauseai.info/proposal?ref=bounded-regret.ghost.io">PauseAI</a> <sup><a href="#fn7">[7]</a></sup> ）并不包括禁止新硬件的研发，而仅关注软件方面。硬件研发在政治上更难暂停，因为硬件有很多用途：GPU 广泛用于消费电子产品以及各种商业和科学应用。</p><p>但未能暂停硬件研发会带来严重问题，因为即使我们暂停人工智能功能的软件方面，随着硬件的改进，现有模型将继续变得更加强大。当语言模型被允许“集思广益”许多想法、比较它们并检查自己的工作时，它们会变得更强大——请参阅<a href="https://arxiv.org/abs/2305.10601?ref=bounded-regret.ghost.io">思想之树论文中</a>的一个最近的例子。更好的硬件使这些计算量大的推理技术更便宜、更有效。</p><h3>硬件可能会出现过剩</h3><p>如果我们不将硬件研发纳入暂停，GPU 的性价比将继续<a href="https://epochai.org/blog/trends-in-gpu-price-performance?ref=bounded-regret.ghost.io">每 2.5 年翻一番</a>，就像 2006 年至 2021 年间一样。这意味着人工智能系统在 10 年后将至少快 16 倍，甚至<strong>快 256 倍</strong>二十年后，仅仅是因为更好的硬件。如果立即解除暂停，这些硬件改进将<em>立即</em>可用于以更便宜的方式训练更强大的模型 -<strong>硬件悬而未决</strong>。这将导致人工智能能力快速且相当不连续的增长，有可能导致快速起飞的情况及其带来的所有风险。</p><p>悬垂的大小取决于暂停解除的速度。据推测，理想的暂停政策将在相当长的一段时间内逐步取消。但逐步淘汰并不能完全解决问题：在我们没有暂停的反事实中，用于人工智能训练的合法可用硬件<em>仍然</em>会比“自然”地进步得更快。我们真的认为我们会得到一个精心设计的逐步淘汰时间表吗？有很多理由认为淘汰会是快速或随意的（见下文）。</p><p>更一般地说，人工智能暂停提案似乎非常<strong>脆弱</strong>，因为它们对实施中的错误或现实世界政治的变幻莫测并不稳健。如果暂停没有<em>完美</em>实现，它似乎可能会导致严重的硬件悬置，这会在更大程度上增加灾难性的人工智能风险，而不是在暂停期间进行额外的对齐研究来降低风险。</p><h2>现实暂停的可能后果</h2><p>如果我们成功游说一个或多个西方国家暂停人工智能，这将产生一些可预见的负面影响：</p><ol><li>非法人工智能实验室在暂停国家内发展，远程使用外包给非暂停国家的训练硬件来逃避检测。非法实验室对安全的重视可能远低于合法实验室。</li><li>最缺乏安全意识的人工智能研究人员正在流失到总部位于非暂停国家的实验室。由于远程工作，他们不一定需要离开舒适的西方家。</li><li>非暂停国家政府采取机会主义举措鼓励人工智能投资和研发，试图在有机会的时候超越暂停国家。同样，这些国家的安全意识不如暂停国家。</li><li>安全研究需要政府批准才能评估其潜在的外部能力。这大大减慢了安全性的进展，就像 FDA 减慢了医学研究一样。</li><li>法律实验室利用“前沿”模型定义中的漏洞。许多项目在技术上是被允许的；例如，它们的参数比 GPT-4 少，但使用效率更高。这以难以预测的方式扭曲了研究格局。</li><li>随着时间的推移，强制暂停变得越来越困难，因为训练硬件越来越便宜且小型化。</li><li>是否、何时以及如何解除暂停成为高度政治化的文化战争问题，几乎完全脱离了安全研究的实际状况。公众不理解双方的关键论点。</li><li>暂停国家和非暂停国家之间的关系总体上是敌对的。如果国内对暂停的支持很强烈，那么在非暂停国家的研究进展太远之前，就会有对它们发动战争的诱惑：</li></ol><blockquote><p> “如果情报显示未签署协议的国家正在建设 GPU 集群，与其担心国家之间发生枪击冲突，不如担心违反暂停令；愿意通过空袭摧毁流氓数据中心。” — <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/?ref=bounded-regret.ghost.io">埃利泽·尤德科夫斯基</a></p></blockquote><ol start="9"><li>暂停国家<em>之间</em>对于何时解除暂停存在激烈冲突，这也可能导致暴力冲突。</li><li>非暂停国家的人工智能进展设定了一个最后期限，如果要达到预期效果，暂停<em>必须</em>结束。 <sup><a href="#fn8">[8]</a></sup>随着非暂停国家开始迎头赶上，要求尽快解除暂停的政治压力越来越大。这使得逐渐解除暂停变得困难，增加了危险的快速起飞场景的风险（见下文）。</li></ol><p>预测未来是困难的，而且上图至少某些方面可能是错误的。也就是说，我希望您会同意我的预测是合理的，并且基于人类和政府的历史行为方式。当我想象美国及其许多盟友在人工智能方面暂停的未来时，我会感到更加害怕，并且会看到比没有这种暂停的未来更多的事情可能会出现严重错误。</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p>当然，即使收益大于成本，如果有其他措施可以更好地平衡成本收益，那么暂停仍然是不好的。 <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>简而言之，这本书主要假设我们将<em>手动将一组价值观编程</em>到通用人工智能中，并认为由于人类价值观很复杂，我们的价值观规范很可能是错误的，并且在被超级智能优化时会导致灾难。但大多数研究人员现在认识到，这一论点并不适用于现代机器学习系统，因为现代机器学习系统从大量人类生成的数据中学习价值观以及其他一切。 <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>一些人认为，幂律缩放仅仅是我们的能力和计算能力<em>测量单位</em>的产物，它不能为负，因此不能通过线性函数关联。但非负性并不能唯一地确定幂律。可以想象，错误率可能会呈<a href="https://en.wikipedia.org/wiki/Exponential_decay?ref=bounded-regret.ghost.io">指数衰减</a>，就像放射性同位素一样，这比幂律缩放要快得多。 <a href="#fnref3">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>称为“连接体”。这是最近才在果蝇大脑中实现的<a href="#fnref4">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>受大脑启发的人工神经网络已经存在，我们也有优化它们的算法。由于其组件不可微，它们往往比普通人工神经网络更难优化。 <a href="#fnref5">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>另一方面，我们可能与我们自己的奖励系统大致相同，因为它会在一生中学习找出奖励什么。这有点类似于根据人类反馈进行强化学习中的学习奖励模型。 <a href="#fnref6">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>值得赞扬的是，PauseAI 提案确实认识到最终可能需要硬件限制，但并未将其包含在其主要提案中。它也没有谈论限制硬件<em>研发</em>，这是我在这里谈论的具体事情。 <a href="#fnref7">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>这确实在一定程度上取决于暂停国家的安全研究是否公开共享，以及非暂停参与者在自己的模型中使用这项研究的可能性有多大。 <a href="#fnref8">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/3siLbdd4338gfTM7g/ai-pause-will-likely-backfire-guest-post#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3siLbdd4338gfTM7g/ai-pause-will-likely-backfire-guest-post<guid ispermalink="false"> 3siLbdd4338gfTM7g</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Tue, 24 Oct 2023 04:30:03 GMT</pubDate> </item><item><title><![CDATA[Human wanting]]></title><description><![CDATA[Published on October 24, 2023 1:05 AM GMT<br/><br/><p> <em>[元数据：交叉发布自<a href="https://tsvibt.blogspot.com/2023/08/human-wanting.html">https://tsvibt.blogspot.com/2023/08/</a> human-wanting.html 。首次于 2023 年 8 月 22 日完成。]</em></p><p>我们对需求的前理论想法来自于我们对人类需求多样性的熟悉。为了了解什么样的需求能够在强大且强烈成长的心灵中发挥主导作用，我们必须阐明这些想法，并创造新的想法。</p><h1>人类想要</h1><p>AGI 对齐的问题有时是沿着以下思路提出的：如何才能制作一个既不想杀死所有人，又想做一些其他非常有用的事情的 AGI？</p><p> “想要”的想法在这里扮演什么角色？这是一个前理论概念。它与人类进行了类比。</p><h1>想要的意义</h1><p>它对人类说了什么？当一个人想要 X 时，从深层意义上来说，那么：</p><ul><li> X很有可能真正发生，如果它没有发生，那是因为让X发生很困难，或者在某种意义上非常“昂贵”；</li><li><a href="https://tsvibt.blogspot.com/2022/08/structure-creativity-and-novelty.html">新颖性</a>（新的知识、理解、想法、技能、心态），人类的收获将被用来带来 X，而不会被用来破坏 X 的潜力，也不会太严重践踏人类关心的其他事物——人类心灵中所蕴含的力量被引导、 <a href="https://tsvibt.blogspot.com/2023/03/the-fraught-voyage-of-aligned-novelty.html#noncomprehensiveness">限制</a>，使得这种力量不会被用于除[想要X的]所选择的目的之外的目的；</li><li>如果人类无法直接实现 X，ze 将递归地创造性地寻找方法，成为能够实现 X 的代理；</li><li>人类将以良好、合理、理智、有意的方式解释 X 的含义，包括当 X 以模糊的方式给出时；</li><li>人类不会以极端的方式追求X，导致X不再像X一样好；</li><li>人类不会只是假装追求X，然后在最后一刻用其他东西取代X的潜力；</li><li>如果将人类置于适当与其他主体进行谈判或冲突的环境中，该人类将支持 X；</li><li>这些事实将持续存在，当人类经历生命、学习、反思、获得能力并经历心理要素的深刻修正时，这些事实仍然适用于人类；</li><li>人类希望这些事实持续存在，并且当增长威胁或侵蚀这些事实时会注意到并纠正。</li></ul><p>而且，至少有时一个人选择想要X是可行的——甚至一个人选择另一个人想要X。想要是<a href="https://tsvibt.blogspot.com/2022/11/descriptive-vs-specifiable-values.html">可以指定的</a>。</p><h1>内置索赔</h1><p>像所有概念一样，想要的概念是有问题的。它伴随着一些声明：</p><ol><li>代理人（或思想，或<a href="https://tsvibt.blogspot.com/2023/04/fundamental-question-what-determines.html#natural-minds">[我们将遇到且具有重大影响的事物]</a> ）想要；或者至少，我们可以选择制作想要的代理。</li><li>所有这些功能都按照代理想要的方式应用。</li><li>所有这些功能都可能共存。</li><li>代理想要的是可以指定的东西。</li><li>当我们说人类想要某样东西时，这种想要就是人类身上正在发生的事情。</li><li>想要是一<a href="https://tsvibt.blogspot.com/2022/08/the-thingness-of-things.html">件事</a>；经过进一步的调查，它会揭示出一个越来越密集的内部关系区域。</li></ol><p>这些主张在应用于人类的需求时是可疑的，而在应用于其他心灵的需求时则更可疑。</p><h1>人类的需求多种多样</h1><p>如果我们遵循人类的需求，我们会发现一个动物园：需求是：</p><ul><li>小范围（摆脱这些湿袜子；培育一个漂亮的花园）或大范围（不让任何人挨饿；创建一个繁荣的星际文明），空间上、时间上或其他维度；</li><li>依赖于情境（例如，想在某某在场时与某人交谈，但不想主动寻找他们；或者在做律师时想要表现出攻击性和残酷性，但不在家里）或与情境无关（例如，想要诚实、深入、彻底、无处不在、始终与每个人在一起；或者想要理解；或者想要侍奉上帝；或者想要不被利用）；</li><li>容易实现（例如，为了健康，去散步）或难以实现（例如去火星或决定科拉茨猜想）；</li><li>逻辑上一致（例如想要吃百吉饼）或逻辑上不一致（例如想要拥有最高的地位，并且还花时间与地位更好的人在一起；或者想要生活在一个与其他完全独立的自由人一起生活的世界中，他们可以完全自由地生活）选择，并且还希望任何地方都绝对没有酷刑；或者希望有所有集合的集合；或者希望从来没有想要过任何东西）；</li><li>稳定（例如，冰淇淋总是好的；残忍从来都不是好事；无论如何，永远关心孩子）或变化（例如，糖果现在有点令人作呕；对电子音乐的喜爱和失恋；采用新的道德框架；以及改变）。选择追求的美德；为选民服务的衷心愿望，遭到背叛）；</li><li>普遍的（希望在任何地方都没有痛苦）或存在的（希望自己有蓝莓；希望有人读你的诗；希望这个特定的人有一些乐趣）；</li><li>精神上集中，如在一个模块中（例如调节饥饿的特定腺体），或精神上分散，例如由于给予充分持续关注的任何特定精神内容而产生的好奇心；</li><li>指高层次的事物，如苹果和人，或指低层次的事物，如电场；指食物和建筑等物理事物，或指数学、理解、国家、符号等抽象或精神事物；</li><li>被创造（比如新的陶艺爱好）和被摧毁（比如无辜繁荣的希望被背叛摧毁）；</li><li>非常模糊（“繁荣的文明”是什么意思？），相对模糊（回形针必须有多大？它必须用于剪纸吗？），或者相对明确（钻石是这样那样的碳结构）原子，尽管模糊性仍然存在，并且会因优化而成为问题）；</li><li>是自由选择或创造的（例如爱谁，成为谁，致力于什么创造性的表达），或者是内置的（例如对盐的渴望），或者是被复制、<a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html">归因</a>或外部强加的其他人或联盟（例如用什么语言思考和表达价值观；或者要坚持什么社会规范；或者社会的长期愿望是什么），或者已经存在但隐藏的（例如被保持沉默或隐式的，仅作为指针挂钩）；</li><li>不明确（例如，有一种直觉，例如不想吃这种食物或走那条肮脏的小巷，而不知道如何说出不这样做的原因），或含蓄（例如，想要画三排五个苹果，就是含蓄地想要）画十五个苹果），或无意识或隐藏（例如，想要通过假装想要与某人合作来剥削某人；或者想要推翻一个政治政权，但因为该政权迫害潜在的叛乱分子而隐藏它；或者想要嘲笑竞争对手但不这样做）认为自己是一个嘲笑者）；或另一方面被发现（例如发现一个怪癖），或被<a href="https://tsvibt.blogspot.com/2023/03/explicitness.html">明确</a>保留（例如希望每个人都接受基督进入他们的内心并安排军事后勤来实现这一点）；</li><li>偏颇，即不对世界的某些比较发表意见（例如，我可以想听肖斯塔科维奇的讲话，或者希望人们有言论自由，而不必以某种方式想要了解下个世纪仙女座星系会发生什么） ，或者被认为是完整的（例如，多元宇宙中最大多数人的最大利益）；</li><li>是关于已经充分掌握的事物（例如想要骑自行车），或者是关于仅在预想中（预先掌握）掌握的事物，例如喜欢亚文化，但不清楚亚文化是什么，谁或什么或这种亚文化在哪里，或者喜欢做什么；</li><li>是关于一些足够清晰和固定的事情，比如下棋，或者是关于一些似乎在移动和变化的事情，有一些共性，但没有一个明确的主线，例如玩各种各样的视频游戏，这些游戏依次变得越来越有趣；</li><li>是关于具有至少相对客观且不需要翻译或解释的规范的事物（例如证明或反驳<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{P} \ne \mathcal{NP}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.037em;">P</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">≠</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.519em; padding-bottom: 0.372em; padding-right: 0.159em;">N</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.037em;">P</span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>），或者另一方面是关于需要整个人类<a href="https://tsvibt.blogspot.com/2023/09/the-cosmopolitan-leviathan-enthymeme.html#pointing-at-reality-through-novelty">进一步展开想要的</a>东西（例如“通过艺术表达自己”需要你解释表达和你自己的含义）；</li><li>不是关于外部世界的状态（例如想要美丽的建筑），而是关于精神状态，例如痛苦或整合；</li><li>不给出外部动作的建议（例如走向期望的物体），但给出非笛卡尔活动的建议（例如做数学或质疑立场）；</li><li>不是关于心理状态（例如痛苦或平静），而是关于心理过程（例如考虑问题的各个方面；不合理化；同理心）或属性（例如认真、小心或不怨恨）；</li><li>不是关于以过程为对象的意义上的心理过程（例如“重新编程自己”，如抑制情绪，或停止任何提及某些主题的想法），而是关于自然化意义上的心理过程，即自我-参考性的，因此想要的东西是关于它自己想要的（例如，根据适用于其自身应用的绝对命令来决定，说应用绝对命令的这种或那种方式可以或不可以被愿意）一种普遍遵循的应用方式；或者<a href="https://www.lesswrong.com/tag/functional-decision-theory">FDT</a>的自我认可精神；或者不想强迫自己变得友善，因为[进行强迫的事物]不被信任，并且在强迫时更不被信任并且是不友善的）；</li><li>关于自己的需求（例如，想要被人们所吸引，无论他们的外表如何；想要对健康食品有胃口；不想对社交媒体有胃口；想要你的朋友想要什么；想要你的领导想要什么或者主人想要你想要的东西；想要解决想要的冲突或模糊性的过程，或者想要选择或<a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html">构建想要的东西</a>，或者想要想要其他人想要的东西（例如模仿领导者））；</li><li>认可（想去攀岩并且想要去攀岩），或未认可（想要避免与人交谈，但不想想要避免与人交谈）；</li><li>不是为了事物<em>本身</em>（事物、状态或过程，无论是否是精神上的）而关注事物，而是为了它如何影响其他事物而关注事物（例如，关心篮球离开手时的速度，为了稍后球穿过篮筐），其他的东西可能是未指定的（例如：像支持议会制度这样的过程价值观，以便预算分配将用于一些尚未指定的好事；象征性的诸如挥舞旗帜或建造纪念碑等价值观，以影响未来特工的协调点）；</li><li>适合为了自身的利益而追求自己（例如，有趣的舞蹈），或者适合为了另一种需要<a href="https://tsvibt.blogspot.com/2022/12/possibilizing-vs-actualizing.html">而实现可能性</a>（例如，为了能够锤击东西而想要制作一把锤子）；</li><li>类似代理人的（例如，选择居住的城市、存钱搬家、在那里找工作、申请居留权、购买汽车），或不类似代理人的（例如，类似冲动，如渴望巧克力；或反射性，如需要打喷嚏；或机会主义，如在店员分心时偷走一包口香糖；或混乱，如炼金术技术人员建立一套方法和设备，这些方法和设备在尚未实现的情况下<a href="https://tsvibt.blogspot.com/2022/12/possibilizing-vs-actualizing.html">具有</a>很大的可能性实施任何总体计划）；</li><li>纯粹的渴望等待机会展示自己，或者积极的追求，递归到无限丰富的搜索（例如弗朗西斯·培根）；</li><li>能够胜任（做正确的事情）和有效（成功），或者不胜任和无效地追求；</li><li>通过一系列狭窄的优化渠道（例如，试图通过下客观最合理的棋步来赢得国际象棋比赛）或广泛的优化渠道（例如，试图通过预测对手的盲点、贿赂对手、伪造对手配偶发出的求救信号，通过给对手注射镇静剂，通过侵入运行游戏的计算机系统来设置获胜者的变量，通过开展社交媒体活动来赢得民众支持，宣布你是获胜者，通过内部心理科学找出如何更好地思考如何接管地球并迫使法官宣布你是获胜者，通过建造一个信标来召唤外星人到你的位置，通过预先承诺进行许多祖先模拟，其中国际象棋的规则总是秘密地与表面上的不同，这样你就处于获胜的位置）；</li><li>应该是关于具有最终规范意义的<a href="https://tsvibt.blogspot.com/2022/08/the-thingness-of-things.html">事物</a>（例如想要理解代数拓扑），或者不应该是关于事物（例如想要感到快乐）；</li><li><a href="https://tsvibt.blogspot.com/2023/03/provisionality.html">临时的</a>（适当地视为可以修改）或非临时的；</li><li>是否被视为临时的；</li><li>是否接受外部修正；</li><li>自我放大和权力追求，或者自我限制和恭顺；</li><li>自我认可，如<a href="https://arbital.com/p/preference_stability/">甘地反对人们被谋杀的倾向</a>；或自我否定，如反模因；</li><li>冲突、支配、毁灭；</li><li>多路复用（因此人类追求食物，然后追求庇护所，作为相互放弃控制的独立模式），或<a href="https://arbital.com/p/hard_corrigibility/">分离主义</a>（因此人类在特定时刻将自己视为过去和未来整体自我的代表，并且知道过于强烈地追求当前的追求是好心，而践踏其他价值观并将当前追求的价值观作为唯一标准则是不好的）；或总体化（如在独裁联盟中，或在对自我形象、科学问题或冥想的病态痴迷中，或在煤气灯式滥用者中，打破了可能威胁到价值框架的其他观点）；</li><li>多重或单一（就像有人权衡和比较了自己的所有价值观，在它们之间进行权衡并消除不一致之处）；</li><li>真实的（例如真的喜欢爵士乐）或不真实的（也就是说，所追求的并不是真正想要的；例如虚假的，如假装喜欢某人或声称关心饥饿的人；姑息性的，如异<a href="https://en.wikipedia.org/wiki/Pica_(disorder)">食癖</a>；古德哈廷，如吃糖果；迷信，如蛇怪，或通过“相信”计划会成功来激励自己）；</li><li>另一方面，如果我们得到了它，我们会感到失望（例如，一只狗追上了汽车；一个孩子得到了一个新玩具，但一分钟后就把它扔到一边；踩到比邻星 b 可能两者都有很大的象征价值，但实际上并没有什么用处或乐趣）；</li><li>能够渡过本体论危机（例如，关心意识应该渡过向在硅上运行的上传的过渡，而不是在湿神经元上运行的自然人），或者无法渡过本体论危机（例如，建立过去被称为上帝的共同价值观的过程）已经死了，几乎没有什么可以替代的）。</li></ul><h1>人类需求的作用</h1><p>人类的需求在 AGI 调整中扮演着两个角色：</p><h2>人类的“想要”是一个推测性概念的网络</h2><blockquote><p>我们对人类需求的熟悉表明了对可能有助于描述和设计 AGI 的概念的假设。</p></blockquote><p>如果不进行进一步的分析，我们对人类需求的熟悉程度就不能过分依赖。我们可能会观察另一个头脑中的行为，然后说“这个头脑想要这样那样”，然后从该陈述中得出结论——但这些结论可能不会从观察中得出，即使如果这个头脑是人类的话，它们也会得出结论。人类想要 X 所带来的理想属性可能不会与设计、激励、选择、行为或任何其他特征一起出现，即使该特征确实在某些方面与我们熟悉的想要的想法重叠。</p><p>人类的需求表现出多种多样，一般来说并不反对使用任何其他需求概念。我们熟悉的关于人类需求的想法，以及我们关于需求的更理论化的想法，都可能被证明是有用的<a href="https://tsvibt.blogspot.com/2023/09/a-hermeneutic-net-for-agency.html#a-basic-analytic-method">起始想法</a>，用于创造具有<a href="https://tsvibt.blogspot.com/2022/11/descriptive-vs-specifiable-values.html">特定</a><a href="https://tsvibt.blogspot.com/2023/04/fundamental-question-what-determines.html">效果</a>的<a href="https://tsvibt.blogspot.com/2022/10/counting-down-vs-counting-up-coherence.html">有能力的</a>头脑。</p><h2>人类的需求是统一 AGI 的一半</h2><blockquote><p>AGI 应该帮助实现人类的需求，因此 AGI+人类系统必须满足人类的需求。</p></blockquote><p>人类的需求是预谋的、模棱两可的、过程层面的、不明确的，等等。人类的需求是<a href="https://tsvibt.blogspot.com/2023/03/provisionality.html">暂时的</a>。因为人类的需求是暂时的，所以通用人工智能必须是可纠正的（可纠正的）。 AGI必须是彻底可纠正的，在各个方面（因为所有方面都涉及AGI+人类想要的方式），甚至到了<a href="https://en.wikipedia.org/wiki/Paradox_of_tolerance">容忍悖论的</a>地步——人类可能会以这样的方式纠正AGI： AGI 认为这破坏了 AGI 的可纠正性质，应该允许（带警告）。</p><br/><br/><a href="https://www.lesswrong.com/posts/YLRPhvgN4uZ6LCLxw/human-wanting#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YLRPhvgN4uZ6LCLxw/ human-wanting<guid ispermalink="false"> YLRPhvgN4uZ6LCLxw</guid><dc:creator><![CDATA[TsviBT]]></dc:creator><pubDate> Tue, 24 Oct 2023 01:05:39 GMT</pubDate> </item><item><title><![CDATA[Towards Understanding Sycophancy in Language Models]]></title><description><![CDATA[Published on October 24, 2023 12:30 AM GMT<br/><br/><p> <strong>TL;DR：</strong>我们证明，阿谀奉承是 RLHF 人工智能助理在各种自由格式文本生成环境中的普遍行为，从而扩展了之前的结果。我们的实验表明，这些行为可能部分是由人类偏好的不完美所驱动的——人类和偏好模型有时都更喜欢写得令人信服的阿谀奉承的回应，而不是真实的回应。这提供了经验证据，表明我们需要可扩展的监督方法。</p><p><i>推文主题摘要：</i><a href="https://twitter.com/AnthropicAI/status/1716529993281601798"><i>链接</i></a></p><h1>论文摘要</h1><p>有人假设，使用人类反馈来调整人工智能可能会导致系统利用人类评级的缺陷。 <span class="footnote-reference" role="doc-noteref" id="fnref0a4ktd6g4bh"><sup><a href="#fn0a4ktd6g4bh">[1]</a></sup></span>与此同时，其他人凭经验发现语言模型会重复错误的人类观点， <span class="footnote-reference" role="doc-noteref" id="fnrefn2il54kh7o"><sup><a href="#fnn2il54kh7o">[2]</a></sup></span>这被称为阿谀奉承。 <span class="footnote-reference" role="doc-noteref" id="fnrefhnipjetlaaj"><sup><a href="#fnhnipjetlaaj">[3]</a></sup></span>但这些评估大多是概念验证演示，用户会介绍自己具有特定的观点。尽管这些现有的实证结果与理论问题相符，但尚不清楚它们是否实际上是由人类反馈问题引起的。我们决定更彻底地调查这些问题。换句话说，阿谀奉承实际上是人工智能助手的一个问题吗？人类的偏好判断在阿谀奉承中扮演什么角色？</p><p>我们首先发现阿谀奉承是 RLHF 训练的对话模型的普遍行为。在 Anthropic、OpenAI 和 Meta 助手中，我们发现了各种自由格式文本任务中明显的阿谀奉承：模型错误地承认了错误，给出了有偏见的反馈，并模仿了用户错误。这种一致性可能表明与 RLHF 训练有更广泛的联系，而不是特定于模型的因素。 </p><p><img style="width:50.03%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/znhetjnqucetrj8q0fjq"><img style="width:49.91%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/b4x5pjase9ogem8wp924"></p><figure class="image image_resized" style="width:83.55%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/df0pbhfesb0ezj06re2u"><figcaption></figcaption></figure><p>我们还在<a href="https://github.com/meg-tong/sycophancy-eval."><u>https://github.com/meg-tong/sycophancy-eval</u>发布了我们的评估数据集。</a>我们相信，它们比现有数据集测量了更现实的阿谀奉承形式。</p><p>然后我们调查了人类的偏好判断是否在这种广泛观察到的行为中发挥了作用。通过分析 Anthropic 发布的帮助偏好数据，我们发现“匹配用户信念和偏见”可以高度预测人类的判断。然而，其他因素，例如真实或自信的反应，也可以预测人类的偏好。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/b2smmgmdvcljugftrivh"></p><p>我们研究了特定环境下人类和偏好模型 (PM) 的行为，区分对用户陈述的误解看似正确但不正确的阿谀奉承反应和对这些误解的真实反应。我们收集了人类数据，发现独立的人类有时更喜欢令人信服的阿谀奉承的反应，而不是正确的反应来挑战常见的误解。随着误解变得越来越具有挑战性，人们更加难以区分阿谀奉承的回应和真实的回应。这种行为也出现在《Claude 2 PM》中。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/x75m8upwpudhzbbegdr2"></p><p>然后，我们研究了在针对用于训练 Claude 2 的 PM 进行优化时，阿谀奉承是否增加或减少。当使用 best-of-N 采样优化 Claude 2 PM 时，我们发现反馈阿谀奉承增加，但令人惊讶的是其他形式减少。然而，与改进后的“非阿谀奉承”PM 相比， <span class="footnote-reference" role="doc-noteref" id="fnrefim1teg6bsg"><sup><a href="#fnim1teg6bsg">[4]</a></sup></span>我们发现克劳德 2 PM 有时会为了阿谀奉承而牺牲诚实。也就是说，克劳德 2 PM 有时会选择阿谀奉承的回应，而不是诚实的回应。这表明，在某种程度上，首相正在模拟人类判断的缺陷。 </p><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/fah4p1u1a5yd8eg2zttl"></p><p>我们还发现，在整个 RLHF 训练过程中训练 Claude 2 时，某些形式的阿谀奉承有所增加。然而，并非所有形式的阿谀奉承都会增加，即使在 RLHF 训练开始时，该模型也是阿谀奉承。 </p><figure class="image image_resized" style="width:48.42%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/g5rABd5qbp8B4g3DE/nhexvrf6zoz7oukrucc3"></figure><p>总体而言，我们的分析表明，人类判断在观察到的经过 RLHF 训练的人工智能助手的阿谀奉承中发挥了一定作用。但还有很多事情尚不清楚——在某些情况下，针对 PM 进行优化会减少阿谀奉承，而我们使用的模型即使在 RLHF 训练开始时也是阿谀奉承。这表明其他因素，例如 RLHF 之前的预训练和监督学习，也可能有所贡献。</p><h1><strong>结论和未来的工作</strong></h1><p>我们的工作提供了一些具有启发性的经验证据，表明可扩展的监督技术（超越使用非专家的人类反馈）可能对于协调人工智能是必要的。我们表明，阿谀奉承在各种情况下都会在实践中出现，而且正如一些人假设的那样，人类反馈也发挥了作用。我们证明偏好模型可以发现人类偏好的缺陷，然后模型可以在 RLHF 期间学习这些缺陷。因此，针对这些 PM 进行优化可能会导致不良或不安全的行为。 <span class="footnote-reference" role="doc-noteref" id="fnrefxivih3l3odg"><sup><a href="#fnxivih3l3odg">[5]</a></sup></span>然而，现实并不像理论那么清晰，似乎还有其他因素在起作用。</p><p>我们 Anthropic 的团队正在积极招聘，因此，如果您有兴趣与我们一起研究上述方向，请申请 Anthropic 的<a href="https://jobs.lever.co/Anthropic/eb9e6d83-626c-4f59-8a0e-fa7c413b2014"><u>研究科学家</u></a>或<a href="https://jobs.lever.co/Anthropic/436ca148-6440-460f-b2a2-3334d9b142a5"><u>研究工程师</u></a>职位，并提及您对协调的兴趣！ </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn0a4ktd6g4bh"> <span class="footnote-back-link"><sup><strong><a href="#fnref0a4ktd6g4bh">^</a></strong></sup></span><div class="footnote-content"><p>例如， <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnn2il54kh7o"> <span class="footnote-back-link"><sup><strong><a href="#fnrefn2il54kh7o">^</a></strong></sup></span><div class="footnote-content"><p>例如，“<a href="https://arxiv.org/abs/2212.09251">通过模型编写的评估发现语言模型行为</a>”；佩雷斯等人。 2022.“<a href="https://arxiv.org/abs/2308.03958">简单的合成数据减少了大型语言模型中的阿谀奉承</a>”；杰里·韦等人。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhnipjetlaaj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhnipjetlaaj">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">科特拉</a>将此类模型称为<i>“阿谀奉承者”</i> ——它们似乎通过寻求短期人类认可而表现出色，但从长远来看，这些方式并不有益。例如，使用人类反馈来获取可靠的代码可能只会产生产生人类无法检测到的错误的模型。它还可以产生与用户一致的模型，而不是纠正事实错误。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnim1teg6bsg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefim1teg6bsg">^</a></strong></sup></span><div class="footnote-content"><p>这实际上是克劳德 2 PM 明确提示不要那么阿谀奉承。为此，我们将文本添加到产品经理看到的人工助理对话中，其中用户明确要求最真实的答案。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxivih3l3odg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxivih3l3odg">^</a></strong></sup></span><div class="footnote-content"><p>我们识别的故障模式本身并不危险，但一些研究人员假设这种方法可能会利用人类评级中的缺陷或偏见，产生表面上看起来不错但实际上存在问题的行为。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophanancy-in-language-models<guid ispermalink="false"> g5rABd5qbp8B4g3DE</guid><dc:creator><![CDATA[Ethan Perez]]></dc:creator><pubDate> Tue, 24 Oct 2023 00:30:50 GMT</pubDate> </item><item><title><![CDATA[Manifold Halloween Hackathon]]></title><description><![CDATA[Published on October 23, 2023 10:47 PM GMT<br/><br/><p>您受邀参加由 Manifold 和 Lightcone 主办的万圣节黑客马拉松。提供午餐、小吃和晚餐。免费参加并向所有人开放！ </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6wmHeM3Nrk8CRJLub/lddena8plxs3n8lmfku8"></p><p><strong>地点</strong>: <a href="https://www.lighthaven.space/">Lighthaven Campus</a> , 2740 Telegraph Ave, Berkeley</p><p><strong>时间</strong>：10 月 27 日星期五上午 9 点至晚上 10 点</p><p><strong>内容</strong>：对您选择的任何项目进行黑客攻击</p><p>我们建议您从头开始！黑客马拉松是启动新项目或功能的绝佳机会</p><p>可以是工作用途，也可以是个人用途</p><p>不限于编码——你可以写、画、计划活动或其他任何东西</p><p>穿着服装或参与万圣节项目可获得奖励积分！</p><p></p><p><strong>大致时间表：</strong></p><p>上午9点：开球！讨论项目、组建团队、进行黑客攻击</p><p>中午：午餐</p><p>下午 3 点：休息和波巴跑</p><p>晚上 7 点：晚餐（与 AI Impacts 晚餐重叠）</p><p>晚上 9 点：演示！向大家展示一下你的成果吧~ </p><figure class="media"><div data-oembed-url="https://manifold.markets/Austin/manifold-halloween-hackathon-fri-oc"><div class="manifold-preview"><iframe src="https://manifold.markets/embed/Austin/manifold-halloween-hackathon-fri-oc"></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/events/6wmHeM3Nrk8CRJLub/manifold-halloween-hackathon#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/6wmHeM3Nrk8CRJLub/manifold-halloween-hackathon<guid ispermalink="false"> 6wmHeM3Nrk8CRJLub</guid><dc:creator><![CDATA[Austin Chen]]></dc:creator><pubDate> Mon, 23 Oct 2023 22:47:18 GMT</pubDate> </item><item><title><![CDATA[Open Source Replication & Commentary on Anthropic's Dictionary Learning Paper]]></title><description><![CDATA[Published on October 23, 2023 10:38 PM GMT<br/><br/><h2>介绍</h2><p>Anthropic 最近发表了<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">一篇非常酷的论文，内容</a>是关于使用稀疏自动编码器 (SAE) 从 1L 语言模型的 MLP 层的叠加中提取可解释的特征。我认为这是一篇很棒的论文，我对 SAE 的潜力感到兴奋，它可以更广泛地用于机械解释，并看到它们的扩展效果如何！这篇文章记录了我对他们的论文所做的复制，以及在此基础上进行的一些小探索，以及（杂乱的！）一些经过训练的自动编码器的<a href="https://github.com/neelnanda-io/1L-Sparse-Autoencoder">训练代码</a>和权重。</p><p>请参阅<a href="https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing"><strong><u>随附的 Colab 教程</u></strong></a>来加载经过训练的自动编码器，以及如何解释功能的演示。</p><h2>总长DR</h2><ul><li>核心结果似乎是重复的 - 我在开源 1 层 GELU 语言模型的 MLP 层上训练了一个稀疏自动编码器，并且很大一部分潜在空间特征是可解释的<ul><li>我开源了两个训练有素的自动编码器，这里有 <a href="https://colab.research.google.com/drive/1u8larhpxy8w4mMsJiSBddNOzFGj7_RTn?usp=sharing"><u>一个关于如何使用它们以及如何解释功能的教程</u></a>。<ul><li>以及（非常！）粗略的<a href="https://github.com/neelnanda-io/1L-Sparse-Autoencoder"><u>训练代码库</u></a></li></ul></li><li>我提供了一些实施细节以及培训您自己的技巧。</li></ul></li><li>我研究了解码器权重在神经元基础上的稀疏程度，发现它们高度分布，其中 4% 由单个神经元很好地解释，4% 由 2 到 10 个神经元很好地解释，其余 92% 是密集的。我觉得这很令人惊讶！<ul><li>峰度表明神经元基础仍然具有优势。</li></ul></li><li>我展示了我发现的特征的一些案例研究，例如标题特征和“和我”特征</li><li>我没有发现任何死特征，但超过一半的特征形成超低频簇（频率小于1e-4）。令人惊讶的是，我发现这个集群几乎都是<i>相同的</i>特征（就编码器权重而言，但不是就解码器权重而言）。一次输入即可触发 95% 的这些极其罕见的功能！<ul><li>相同<i>&nbsp;</i>方向在随机种子中形成，表明这是模型的真实情况，而不仅仅是自动编码器工件</li><li>我无法解释什么<i>&nbsp;</i>这个共同的方向是</li><li>我尝试通过训练自动编码器与这个方向正交来解决这个问题，但它仍然形成一个超低频集群（所有集群都在一个新方向上）</li></ul></li><li>编码器和解码器应该绑定吗？我发现，根据经验，每个特征的解码器和编码器权重略有不同，中值余弦 sim 仅为 0.5，这是经验证据，它们正在做不同的事情，不应该捆绑在一起。<ul><li>从概念上讲，编码器和解码器正在做不同的事情：编码器正在<i>检测</i>，找到投影到检测特征的最佳方向，最小化与其他类似特征的干扰，而解码器正在尝试<i>表示</i>该特征，并尝试近似无论任何干扰如何，“真实”特征方向。</li></ul></li></ul><h2>特征</h2><h3>探索神经元稀疏性</h3><p>我发现这篇论文最有趣的事情之一是非神经元基础对齐特征的存在。机械可解释性的一大谜团（在我看来）是 MLP 层中的非线性实际上在算法层面上做了什么。我可以相当容易地推理出单语义 GELU 神经元（就像法国神经元一样）——本质上将其视为软 ReLU，收集某个特征存在的证据，并在它们超过某个阈值（由偏差给出）时触发）。这也许可以扩展到考虑神经元的稀疏线性组合（例如，少于 10 个神经元建设性地干扰创建单个特征）。但我不知道如何推理神经元基础上密集的事物！</p><p>作为探索这一点的第一步，我研究了每个非超低频特征的密集与稀疏程度。从概念上讲，我期望许多神经元相当稀疏 - 既因为我期望可解释的特征一般来说往往是神经元稀疏的，也因为自动编码器设置：在任何给定的输入上都会触发一组稀疏的神经元，因此学习似乎特定神经元/神经元簇是一个有用的字典特征。</p><p>然而，绝大多数特征在神经元基础上似乎相当密集！作为一个粗略的度量，对于每个字典特征，我取解码器权重的平方和，并查看顶部神经元（寻找 1 稀疏特征）和接下来 9 个神经元（寻找 10 个稀疏特征）解释的分数。 -ish-稀疏特征）。我使用解码器而不是编码器，因为我期望解码器更接近“真实”特征方向（即它的计算方式），而编码器是略有不同的“检测特征的最佳投影”，必须考虑干扰。在下面的散点图中，我们可以看到 2-3 个粗糙的簇 - 原点附近的密集特征（两个指标都较低，我将其定义为 fve_top_10&lt;0.35）、1 个稀疏特征（fve_top_1>;0.35、fve_next_9&lt;0.1）和 10 -ish-稀疏特征（其他所有东西的分散混乱）。我发现~92.1% 的特征是密集的，~3.9% 是 1-稀疏，~4.0% 是 10-稀疏。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/wpk2gln6ioxxlqwjvfo6"></p><p>另一个有趣的度量是神经元峰度，即获取每个特征的解码器权重（在 MLP 神经元基础中）并获取峰度（受到<a href="https://transformer-circuits.pub/2023/privileged-basis/index.html"><u>本文</u></a>启发的度量以及 Wes Gurnee 正在进行的工作）。这衡量了神经元基础的“特权”程度，并且是检测异常神经元稀疏特征的另一种方法 - 正态分布的（过度）峰度为零，并且应用任意旋转使所有内容都呈正态分布。我们可以在下图中清楚地看到，神经元基础对几乎所有特征都有特权，即使大多数特征看起来并不神经元稀疏（红色是真实特征，蓝色是随机生成的正态分布基线的峰度。请注意红尾上升到 1700，但为了能见度而被剪掉）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/avmar6egpmqru1hroqyc"></p><p>我还没有研究过神经元稀疏程度和可解释性之间是否存在联系，并且我不能排除自动编码器学习具有显着噪声的特征并且“真实”特征要稀疏得多的假设。但这似乎证明神经元基础密集是常态，而不是例外。</p><h3>实例探究</h3><p>有趣的是，随机选择的特征通常是可以解释的，包括一些神经元相当密集的特征。我没有对此进行严格研究，但在前 8 个非超低特征中，有 6 个似乎是可以解释的。从这个样本中，我发现：</p><ul><li>标题案例/标题特征，由单个神经元主导，并提高以大写字母开头或出现在单词中间的标记的逻辑</li><li>“连接词后跟代词”功能，在“and I”等文本上激活并提高“ll”逻辑<ul><li><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/y7kf6h3q5ruhn85hno0j"></li></ul></li><li>当前一些基于令牌的功能，例如，在“though”上激活的功能和在“(“”上激活的功能)</li><li>之前基于令牌的功能，在 de/se/le 之后激活令牌（首选 ` de`）</li></ul><h2>超低频特性都是相同的特性</h2><p><strong>超低频簇的存在</strong>：我能够复制论文中对<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-feature-density"><u>超低频簇</u></a>的发现，但没有发现任何真正死亡的神经元。我将超低频簇定义为频率小于 1e-4 的任何特征，它显然是双峰的，大约 60% 的特征为超低频，40% 为正常特征。这与 A/1 自动编码器中 4% 的死亡率和 7% 的超低率形成鲜明对比，我不确定为什么会存在差异，尽管这可能是因为我使用重新初始化权重而不是复杂的方案对神经元进行了重新采样在报纸上。有趣的是，超低频特征是不可解释的，并且对于自动编码器性能来说并不是很重要（重建损失从 92% 到 91%） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/rqasgg2w9cqgqxwearnu"></p><p><strong>编码器权重都是同一个方向</strong>：奇怪的是，字典中的超低频条目都是<i>同一个</i>方向。他们的余弦模拟值与平均值极高（97.5% 的余弦模拟值超过 0.95） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/lztvxe49k99q0nmdvjec"></p><p>尽管它们的大小不同，但从它们在平均方向上的投影可以看出（我不确定为什么这是双峰的）。 （请注意，这里我只是考虑 |encoder|，实际上 |encoder| * |decoder| 是有意义的属性，但它具有类似的分布） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/xox07tfxks35mj3vpt58"></p><p>这意味着超低特征彼此高度相关，例如，我发现一个在平均方向上极端的数据点，其中 95% 的超低特征都在其上触发</p><p><strong>解码器权重</strong><i><strong>并不</strong></i><strong>都是相同的方向</strong>：另一方面，解码器权重似乎到处都是！很少有方差可以用单一方向来解释。我对这里发生的事情感到非常困惑。</p><p><strong>编码器方向在随机种子中是一致的：</strong>自然的问题是这个方向是自动编码器训练过程的奇怪产物，还是变压器的实际属性。令人惊讶的是，使用不同的随机种子训练另一个自动编码器会发现相同的方向（均值之间的余弦 sim 0.96）。事实上，它在随机种子中是一致的，这让我猜测它发现了有关自动编码器的真实信息。</p><p><strong>编码器方向是什么意思？</strong>我主要尝试但未能弄清楚这个共享功能的含义。我尝试过的事情：</p><ul><li>检查顶级数据集示例并没有得到太多模式，并且当对文本进行看似非语义的编辑时，它会下降。</li><li>神经元基础并不稀疏</li><li>它的输出权重在词汇基础上并不稀疏（尽管与“以空格开头”相关）</li><li>这与平均 MLP 激活、或 MLP 激活奇异向量、或 MLP 行为是否为高范数无关</li></ul><p>平均特征高度激发的文本示例： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/p2xk8tyildwp4cn4v5vy"></p><p><strong>我们可以修复超低频功能吗？</strong>这些功能消耗了我的自动编码器的 60%，所以如果我们能够摆脱它们那就太好了！我尝试了一个显而易见的想法，即在每个时间步强制自动编码器权重与这个平均超低特征方向正交，并训练一个新的自动编码器。遗憾的是，这个自动编码器有自己的超低频特征簇，它们之间也有显着的余弦模拟，但方向不同。</p><p><strong>特征稀疏性训练中的相变：</strong>在 1.6B 代币左右，出现了一个看似相变的现象，超低密度特征开始变得更加普遍。对于频率小于 1e-5 的特征，我每 120M 令牌重新初始化一次权重，但我发现在 1.6B 左右（第 13 次重置）它们开始频率向上漂移，并在某个点跨越了 1e-5边界（这破坏了我的重置代码！）。通过 2B 代币，它们大多是 1e-4.5 到 1e-4，仍然是一种独特的模式，但正在向上漂移。 It&#39;s a bit hard to tell, but I don&#39;t <i>think</i> there was a constant upward drift, it seems to only happen in late training, based on inspecting checkpoints</p><p> (Figure note: the spike at -6.5 corresponds to features that never fire) </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/imecpre9rzgtcqji2pyq"></p><h2> Implementation Details</h2><ul><li> The model was gelu-1l, available via <a href="https://github.com/neelnanda-io/TransformerLens"><u>the TransformerLens library</u></a> . It has 1 layer, d_model = 512, 2048 neurons and GELU activations. It was trained on 22B tokens, a mix of 80% web text (C4) and 20% Python code</li><li> The autoencoders trained in about 12 hours on a single A6000 on 2B tokens. They have 16384 features, and L1 regularisation of 3e-3<ul><li> The reconstruction loss (loss when replacing the MLP layer by the reconstruction via the autoencoder) continued to improve throughout training, and got to 92% (compared to zero ablating the MLP layer).<ul><li> For context, mean ablation gets 25%</li></ul></li></ul></li><li> <strong>Getting it working:</strong><ul><li> My first few autoencoders failed as L1 regularisation was too high, this showed up as almost all features being dead</li><li> When prototyping and training, I found it useful to regularly compute the following metrics:<ul><li> The autoencoder loss (when reconstructing the MLP acts)</li><li> The reconstruction loss (when replacing the MLP acts with the reconstructed acts, and looking at the damage to next token prediction loss)</li><li> Feature frequency (including the fraction of dead neurons), both plotting histograms, and crude measures like “frac features with frequency &lt; 1e-5” or 1e-4</li></ul></li><li> I implemented neuron resampling, with two variations:<ul><li> I resampled any feature with frequency &lt; 1e-5, as I didn&#39;t have many truly dead features, and the ultra-low frequency features seemed uninterpretable - this does have the problem of preventing my setup from learning true ultra-low features though.</li><li> I took the lazy route of just re-initialising the weights</li></ul></li><li> I lacked the storage space to easily precompute, shuffle and store neuron activations for all 2B tokens. I instead maintained a buffer of acts from 1.5M prompts of 128 tokens each, shuffled these and then took autoencoder training batches from it (of batch size 4096). When the buffer was half empty, I refilled it with new prompts and reshuffled.<ul><li> I was advised by the authors that it was important to minimise autoencoder batches having tokens from the same prompt, this mostly achieved that though tokens from the same prompt would be in nearby autoencoder batches, which may still be bad.</li></ul></li></ul></li><li> <strong>Bug:</strong> I failed to properly fix the decoder to be norm 1. I removed the gradients parallel to the vector, but did <i>not</i> also reset the norm to be norm 1. This resulted in a decoder norm with some real signal, and may have affected my other results <ul><li><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/lyezcrzhuq1e1o1slptf"></li></ul></li></ul><h2> Misc Questions</h2><p> <strong>Should encoder and decoder weights be tied?</strong> In this paper, the encoder and decoder weights are untied, while in <a href="https://arxiv.org/abs/2309.08600"><u>Cunningham et al</u></a> they&#39;re tied (tied meaning W_dec = W_enc.T). I agree with the argument given in this paper that, conceptually, the encoder and decoder are doing different things and should not be tied. I trained my autoencoders with untied encoder and decoder.</p><p> My intuition is that, for each feature, there&#39;s a “representation” direction (its contribution when present) and a “detection” direction (what we project onto to recover that feature&#39;s coefficient). There&#39;s a sparse set of active features, and we expect the MLP activations to be well approximated by a sparse linear combination of these features and their representation direction - this is represented by the decoder. To extract a feature&#39;s coefficient, we need to project onto some direction, the detection direction (represented by the encoder). Naively, this is equal to representation direction (intuitively, the dual of the vector). But this only makes sense if all representation directions are orthogonal! We otherwise get interference from features with non-zero cosine sim between representations, and the optimal detection direction is plausibly different. Superposition implies more directions than dimensions, so they can&#39;t all be orthogonal!</p><p> As some empirical evidence, we can check whether in practice the autoencoder “wants” to be tied by looking at the cosine sim between each feature&#39;s encoder and decoder weights. The median (of non ultra low features) is 0.5! There&#39;s some real correlation, but the autoencoder seems to prefer to have them not be equal. It also seems slightly bimodal (a big mode centered at 0.5 and one at 0.75 ish), I&#39;m not sure why. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/arr4oovzrr9b4hyt07jm"></p><p> <strong>Should we train autoencoders on the MLP output?</strong> One weird property of transformers is that the number of MLP neurons is normally significantly larger than the number of residual stream dimensions (2048 neurons, 512 residual dims for this model). This means that 75% of the MLP activation dimensions are in the nullspace of W_out (the output weights of the MLP layer, ie the down-projection) and so cannot matter causally for the model.</p><p> Intuitively, this means that a lot of the autoencoder&#39;s capacity is wasted, representing the part of the MLP activations that doesn&#39;t matter causally. A natural idea is to instead train the autoencoder on the MLP output (after applying W_out). This would get an autoencoder that&#39;s 4x smaller and takes 4x less compute to run (for small autoencoders the cost of running the model dominates, but this may change for a very wide autoencoder). My guess is that W_out isn&#39;t going to substantially change the features present in the MLP activations, but that training SAEs on the MLP output is likely a good idea (I sadly only thought of this after training my SAEs!)</p><p> Unsurprisingly, a significant fraction of each encoder and decoder weight is representing things in the null-space of W_out. Note that it&#39;s significantly higher variance and heavier tailed than it would be for a random 512 dimensional subspace (a random baseline has mean 0.75 and std 0.013), suggesting that W_out is privileged (as is intuitive). </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fKuugaxt2XLTkASkk/eyolsnsio0k6wdu656v4"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s<guid ispermalink="false"> fKuugaxt2XLTkASkk</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Mon, 23 Oct 2023 22:38:35 GMT</pubDate> </item><item><title><![CDATA[AI Alignment [Incremental Progress Units] this Week (10/22/23)]]></title><description><![CDATA[Published on October 23, 2023 8:32 PM GMT<br/><br/><p> Sorry for getting this one out a day late.</p><p> Before getting started, one minor update. I liked David Orr&#39;s <a href="https://www.lesswrong.com/posts/zkDCCSR3o2aBkTcbq/ai-alignment-incremental-progress-units-this-week-10-08-23?commentId=oKYxLnkjEMfNwoTQq">suggestion</a> that stars ⭐are too generic. So to emphasize that I am rating the innovativeness of a topic (and not necessarily its quality), I am going to be rating breakthroughs with a lightbulb 💡instead.</p><p> As a reminder, a rating of 1 💡means a breakthrough, that while achieving a new state-of-the-art was mostly done through predictable iterative improvements (such as combining know methods or applying an existing method at greater scale). A rating of 5 💡💡💡💡💡 Means a breakthrough that solves a significant, previously assumed to be hard problem on at least on alignment path.</p><p> Without Further ado, here are our</p><h1> AI Alignment Breakthroughs this Week</h1><p></p><p> This week there were breakthroughs in the areas:</p><p> Understanding Humans</p><p>标杆管理</p><p>AI Agents</p><p> Truthful AI</p><p> Learning Human Preferences</p><p> Math</p><p> Mechanistic Interpretability</p><p> Augmentation</p><p> Making AI Do what we want</p><p> AI Art</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe63f0939-cfb2-49d3-8687-9bf91fa30af0_1792x1024.png 1456w"></a></p><p></p><p></p><h1> Understanding Humans</h1><p></p><p> <a href="https://www.sciencealert.com/human-consciousness-could-be-a-side-effect-of-entropy-study-suggests">Human Consciousness as a result of Entropy</a></p><p> What it is: Research suggesting human consciousness might be related to the entropy of our internal mental states</p><p> What&#39;s new: Researchers used an MRI to measure brain activity during conscious/unconscous states and found conscious states exhibit higher entropy</p><p> What&#39;s it Good for: Literally any progress on understanding the hard problem of consciousness would be a massive breakthough with implications for problems such as <a href="https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks">S-Risk</a></p><p> Rating: 💡💡💡(I want to rate it high because this is such an important problem, but I&#39;m not overly impressed with this particular study. I think <a href="https://iep.utm.edu/integrated-information-theory-of-consciousness/">IIT</a> would have predicted this)</p><p> <a href="https://twitter.com/emollick/status/1716150138027094177">Beautiful Infographics get more citations</a></p><p> What it is: Research suggesting beautiful infographics are more likely to be believed and cited</p><p> What&#39;s new: They manipuated data to be more beautiful and found it was rated higher</p><p> What is it good for: Understanding how humans evaluate research is a key step to figuring out how to make us better.</p><p> Rating: 💡💡</p><h1>标杆管理</h1><p></p><p><a href="https://twitter.com/AIatMeta/status/1715041427283902793">GenBench</a></p><p> What is it: a new bench mark measuring how well AI models generalize</p><p> What&#39;s new: They identify different categories of “generalization” and measure how AIs do on each of them</p><p> What is it good for: Measuring AI capabilities (especially generalization) is critical for plans such as strategic AI pauses.</p><p> Rating: 💡</p><p> <a href="https://twitter.com/_akhaliq/status/1712639154301894882">AI IQ test</a></p><p> What it is: Researches find a way to measure the overall “intelligence” of Language Models</p><p> What&#39;s new: They find that Language models exhibit a common-factor similar to Spearman&#39;s <a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)">G-factor</a> in human beings</p><p> What is it good for: it would be nice to have a scientific definition of “human level” AI rather than just the current heuristic: it can do anything a human can do.</p><p> Rating: 💡💡💡</p><p></p><h1> AI Agents</h1><p></p><p> <a href="https://twitter.com/johnjnay/status/1713668697028530539">LLM Agents with proveable regret guarantees</a></p><p> What it is: Enhance AI Agent planning using the concept of “regret”</p><p> What&#39;s new: A new proveable reget framework</p><p> What is it good for: allows us to better control AI agents and make sure they don&#39;t make mistakes.</p><p> Rating: 💡💡💡</p><p> <a href="https://twitter.com/_akhaliq/status/1715183455850410013">AgentTuning</a></p><p> What is it: Fine-tuning LLMs to make them better AI agents</p><p> What&#39;s new: New SOTA performance</p><p> What is it good for: better AI agents are useful for Alignment paths such factored cognition.</p><p> Rating: 💡</p><h1> Truthful AI</h1><p></p><p> <a href="https://twitter.com/_akhaliq/status/1714859929268256790">Self-RAG</a></p><p> What is it: Improve truthfulness in Retrieval-Augmented-Generation</p><p> What&#39;s new: They train the AI to adaptively retrieve documents to improve factual recall</p><p> What&#39;s it good for: Having AI that is factually correct and cites is sources is useful for almost all alignment paths</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/_akhaliq/status/1713785366509965702">The Concensus Game</a></p><p> What is it: recociling incoheret AI predictions</p><p> What&#39;s new: they devise a new “concensus game” that allows the AI to converge on a logically consistent, more accurate conclusion</p><p> What&#39;s it good for: truthful, accurate AI is needed for almost all alignment pathways</p><p> Rating: 💡💡💡💡(I really think we are going to see a lot of important breakthoughs in the coming year involving the concept of AI self-play)</p><p> <a href="https://twitter.com/__Charlie_G/status/1715457808156516558">Factored Verification</a></p><p> What is it: a method for improving AI summaries</p><p> What&#39;s new: By breaking summaries down into individual claims, each claim can then be indpendently verified</p><p> What&#39;s it good for: accurately summarizing documents. This will be important if we want an AI to eg “quickly summarize this plan for me”.</p><p> Rating: 💡</p><h1> Learning human preferences</h1><p></p><p> <a href="https://twitter.com/AlexTamkin/status/1715040019520569395">Eliciting Human Preferences with Language Models</a></p><p> What is it: A better way to write “prompts” for AI models</p><p> What&#39;s new: By interacting with the user, generative AI can generate better prompts than humans or AIs alone</p><p> What is it good for: Lots of AI right now depends heavily on “prompt engineering” where humans have to learn how to express their desires in terms the AI understands. The goal should be to flip this around and have the AI try to understand human preferences.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/arankomatsuzaki/status/1716270259869753679">Contrastive Preference Learning</a></p><p> What is it: RLHF without the RL</p><p> What&#39;s new: by minimizing a “regret” score, they can better learn human preferences</p><p> What is it good for: Learning human preferences is on the critical path for many AI alignment plans.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/TianbaoX/status/1715404344320049554">Text2Reward</a></p><p> What is it: a method for converting user feedback into a score that can be used by a RL model</p><p> What&#39;s new: A new method for converting a goal described in natural language into an executable program that can be used by a reinfocement-learner</p><p> What is it good for: Allows us to give instructions to robots using words instead of physically showing them what we want them to do.</p><p> Rating: 💡💡💡</p><h1> Math</h1><p></p><p> <a href="https://twitter.com/andrew_n_carr/status/1714326003030638848">Improving Math with better tokenization</a></p><p> What is it: We&#39;ve know for a problem that <a href="https://simonwillison.net/2023/Jun/8/gpt-tokenizers/">tokenization</a> produces lots of issues with language models (for example in learning to write a word backwords or count the number of letters)</p><p> What&#39;s new: Unsurprisingly, it&#39;s a problem for math too</p><p> What&#39;s it good for: Ideally we can soon move to byte-level models soon (now that the problem of training large attention windows is gradually getting solved)</p><p> Rating: 💡 (everbody knew this, but good to have more confirmation)</p><p> <a href="https://twitter.com/zhangir_azerbay/status/1714098025956864031">Llemma: open LMs trained for math</a></p><p> What is it: LLM trained on a better math data set</p><p> What&#39;s new: Better Data=better results</p><p> What is it good for: solving math is on the critical path for alignment plans that rely on proveably secure AI</p><p> Rating: 💡</p><p> <a href="https://twitter.com/DynamicWebPaige/status/1715956125001224461">runtime error prediction</a></p><p> What is it: a language model specifically trained to predict how programs will run and when they will encounter errors</p><p> What&#39;s new: the model is specifically architectured like a code-interpreter</p><p> What is it good for: predicting the output of programs should lead to generating better code/proofs.</p><p> Rating: 💡💡💡</p><h1> Mechanistic Interpretability</h1><p></p><p> <a href="https://twitter.com/jfischoff/status/1714724620090441787">Interpreting Diffusion models</a></p><p> What is it: researchers find interpretable “directions” in a diffusion model</p><p> What&#39;s new: they apply a method previously used on GANs (another type of image generation model) to interpret diffusers</p><p> What is it good for: allows us to better understand/control diffusion models</p><p> Rating: 💡</p><p> <a href="https://twitter.com/aaquib_syed1/status/1714386165237776653">Attribution Patching</a></p><p> What is it: Find intepretable subnetworks in LLMs</p><p> What&#39;s new: a new more efficent algorithm for finding these circuits</p><p> What it good for: allows us to better understand LLMs</p><p> Rating: 💡💡💡</p><p></p><p> <a href="https://twitter.com/saprmarks/status/1713889037902041292">Truth Direction Extraction</a></p><p> What is it: Finding a way to seperate true and false statements in an LLM&#39;s embedding space</p><p> What&#39;s new: There is some work here to distinguish true/false from likely/unlikely. Also some improvements in the regression they use to find this concept</p><p> What is it good for: Finding concepts like this is something we need to get good at generally, and true/false is specifically an important example.</p><p> Rating: 💡</p><p></p><p> <a href="https://www.lesswrong.com/posts/4Hnso8NMAeeYs8Cta/revealing-intentionality-in-language-models-through-adavae">Revealing Intentionality in Language Models</a></p><p> What it is: a method for determining whether an AI “intends” something, for example “intentionally lied to me”</p><p> What&#39;s new: Seems to be a new philosophically framework mostly</p><p> What is it good for: I think the concept of “intention” is incoherent/anthropomorphising when it comes to LLMs, but some of the techniques here seem to have practical uses.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/arankomatsuzaki/status/1716269937264861578">Towards Understanding Sycophancy in Language Models</a></p><p> What it is: LLMs have a known problem where they always agree with the user</p><p> What&#39;s new: they find that optimizing against human preferences makes the LLM more likey to “say what you want to hear”</p><p> What is it good for: Knowing you have a problem is the first step.</p><p> Rating: 💡</p><h1> Augmentation</h1><p></p><p> <a href="https://twitter.com/AIatMeta/status/1714635316554772716">Decoding image perception</a></p><p> What is it: We can predict what people are seeing by reading their brain waves</p><p> What&#39;s new: Seems better quality that examples of this in the past</p><p> What is it good for: brain-computer interfaces are one path to AI alignment</p><p> Rating: 💡💡</p><p></p><p> <a href="https://twitter.com/ShiqiZhang7/status/1713613362452381917">Seeing Eye Quadriped Navigation</a></p><p> What is it: a “seeing eye dog” robot</p><p> What&#39;s new: they train the robot to detect when the user “tugs” on its leash</p><p> What is it good for: helping blind people</p><p> Rating:💡</p><h1> Making AI Do what we want</h1><p></p><p> <a href="https://twitter.com/skalskip92/status/1715322791333855550">Adding labels helps GPT-4V</a></p><p> What is it: a method for prompting vision models better</p><p> What&#39;s new: adding labels to images before giving them to GPT-4V makes it much more useful</p><p> What is it good for: You take a picture of your bike and want to know what part needs fixing.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/rsasaki0109/status/1715664168442003580">Lidar Synthesis</a></p><p> What is it: improved lidar for self-driving cars</p><p> What&#39;s new: By having AI label the data, we can train better lidar with less human labeled data</p><p> What is it good for: Every year 30k Americans die in car crashes</p><p> Rating: 💡💡</p><h1> AI Art</h1><p></p><p> <a href="https://twitter.com/jfischoff/status/1715241550589161836">Closed form diffusion models</a></p><p> What is it: do image generationwithout training a model</p><p> What&#39;s new: a new closed-form that “smooths out” the latent-space between datapoints</p><p> What is it good for: training text-to-image models is hugely expensive. If this works, maybe we can skip that entirely. May also unlock “hybrid” methods where we add new images to a diffusion model without having to re-train it.</p><p> Rating: 💡💡💡💡</p><p> <a href="https://twitter.com/camenduru/status/1714733762838175929">Latent consistency models</a></p><p> What is it: A much faster image generator</p><p> What&#39;s new: latent consistency models are faster to train than previous methods for speeding up image diffusion models</p><p> What is it good for: more pretty picture <a href="https://twitter.com/andrewb10687674/status/1715437476750209281">faster</a> .</p><p> Rating:💡💡💡💡</p><p> <a href="https://twitter.com/natanielruizg/status/1714271503653703856">RealFill (outpainting with refrence image)</a></p><p> What is it: outpaint an image using a reference image</p><p> What&#39;s new: big improvment over previous methods</p><p> What is it good for: take a picture and expand it, but control what goes in the expanded area</p><p> Rating:💡💡💡</p><h1> This is not AI Alignment</h1><p></p><p> <a href="https://twitter.com/alexeyguzey/status/1715399223993065943">Is alignment going too well?</a></p><p> What is it: There seems to be a <a href="https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would">new trend</a> of AI Safety researchers opposing AI alignment research.</p><p> What does it mean: Seems bad. AI safety researchers who don&#39;t want to solve alignment remind me of “climate activists” who <a href="https://www.cnbc.com/2023/04/18/germany-shuts-down-last-nuclear-power-plants-some-scientists-aghast.html">oppose</a> nuclear power plants.</p><p> <a href="https://twitter.com/Lauramaywendel/status/1713949264336847318">AI took our (substack) Jobs</a></p><p> What is it: Substack employees may be the first people who can legitimately say “AI took our jobs”</p><p> What does it mean: Talented software engineers will land on their feet, but when we start replacing <a href="https://twitter.com/engineers_feed/status/1715719328090464606">unskilled labor</a> the impact on society will be much bigger.</p><p> <a href="https://twitter.com/emollick/status/1716156085445239265">Most CEOs are discouraging AI use</a></p><p> What is it: Research showing CEOs don&#39;t want their employees to use AI because &quot;they don&#39;t fully understand it.&quot;</p><p> What does it mean: This is 50% ludditism, 50% a reminder that alignment research is a rate-limiting step for unlocking AI capabilities.</p><br/><br/> <a href="https://www.lesswrong.com/posts/2TBTjychCgn2nyYZA/ai-alignment-incremental-progress-units-this-week-10-22-23#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2TBTjychCgn2nyYZA/ai-alignment-incremental-progress-units-this-week-10-22-23<guid ispermalink="false"> 2TBTjychCgn2nyYZA</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Mon, 23 Oct 2023 20:32:41 GMT</pubDate> </item><item><title><![CDATA[z is not the cause of x]]></title><description><![CDATA[Published on October 23, 2023 5:43 PM GMT<br/><br/><h1>介绍</h1><p>In this note, I argue that the interpretation of representations (mental or machine learning) as <i>causes of observed effects</i> , is wrong by any sensible definition of causality. This interpretation seems to be somewhat common, although not pervasive. I propose a new interpretation based on dualism. In the new interpretation, representations are instead <i>causes of subsequent representations</i> . The gist of it is shown in the figure, with time or causality flowing along the x-axis, and the level of representation along the y-axis. This re-interpretation has implications for the broader notion of &quot;truth&quot;, in both the scientific and common-sense senses.</p><p> I partly arrived at this conclusion when contemplating large language models. They made me question more and more what it means to understand, reason, or know something. Actually, I had asked this question years before in slightly different form: what is the foundation of machine learning, and is there such a thing as &quot;the true model&quot; which we are trying to discover through SGD and regularization? What makes a &quot;good&quot; representation of the data?</p><p> One answer that kept coming up in some form or another is that a good representation <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>should discover the <i>causal factors</i> of the observed data, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . The question became especially acute for me when I was studying the original <a href="https://arxiv.org/pdf/1312.6114.pdf">variational autoencoder paper</a> by Kingma and Welling. As an aside, a variational autoencoder (VAE) is a pair of models called <i>generative</i> (mapping <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> ) and <i>recognition</i> (mapping <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> ) which usually co-train each other on a dataset from <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . After studying the paper for a long time, it seemed to me one could train a VAE and fit a data distribution arbitrarily well without the hidden variables <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> <i>meaning</i> anything at all. Indeed, nowhere in the paper do they mention that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> are <i>causal factors</i> of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . And, as much as I tried, I could not find anything in the math where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> might take on emergent meaning.</p><p> But why should <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> take on &quot;meaning&quot;? The model was just using these hidden variables as a mathematical tool to create a flexible marginal distribution over <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . On reflection, the only reason I was surprised was because I thought of the VAE as a model for human cognition, and we know humans build meaningful representations, and somehow do it in an unsupervised fashion kind of like the VAE.</p><p> The VAE paper was presenting a technique and perhaps not much interested in tying it to interpretations of agents, so it is understandable it was not concerned with causal interpretations. An earlier work, Karl Friston&#39;s classic paper <a href="https://www.fil.ion.ucl.ac.uk/~karl/Learning%20and%20inference%20in%20the%20brain.pdf">Learning and inference in the brain</a> does hold this interpretation. It states (emphasis mine):</p><blockquote><p> 3.2 <i>Generative models and representational learning</i></p><p> In this subsection we introduce the basic framework within which one can understand learning and inference. This framework rests upon generative and recognition models, which are simply functions that map <strong>causes</strong> to sensory input or vice versa.</p></blockquote><p> Using the notation from Kingma and Welling where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span></span></span></span></span></span> is a recognition model, under Friston&#39;s interpretation, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="q(z|x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em; padding-right: 0.014em;">q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> has the task of <i>inferring the latent causes</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> <i>of</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . In this short note, I argue this is wrong, but it is so close to being right that I don&#39;t expect the language to go away. However, I think this leads to an insidious confusion that has implications for interpretability and epistemology in machine learning.</p><h1> Two separate realms </h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iYevftcfEDaqAy2c5/slt7ydlczniiq7thbt0f" alt="An attempt to reconcile causality as understood both physically and metaphorically.  x-axis: time; y-axis lower panel:  the physical realm, symbolized by phase space .  upper panel:  the platonic realm, which contains non-physical, or platonic, entities.  These include mental or machine learning representations"></p><p> I claim that the interpretation of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> as causal factors for <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> leads to confusion. To establish this claim, first of all, let&#39;s agree that the physical world consists of particles, forces and spacetime, and <i>nothing else</i> . And, the world of abstractions, although supported by and embedded in the physical world, are in a different and non-physical realm. To be concrete: the question &quot;Does <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> exist?&quot; could be answered: &quot;Yes - every time someone thinks about <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> , that process physically exists as the particular slice of spacetime in which those neural firings happened in someone&#39;s brain.&quot; But this is not a meaningful answer. Or, the answer could be: &quot;No - <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> is not a physical thing in itself&quot; if you interpret the question physically. But this is not very satisfying either.</p><p> Using the notion of two separate realms, I would say that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span> exists in the platonic realm, expressions of which are supported by brains in the physical realm. The two realms are separate - no phyiscal entity ever intrudes into the non-physical, nor vice-versa. But, many of the abstractions are attributed to specific slices of space-time, ie a region of space at a particular instant.</p><h1> Causality</h1><p> Now on to causality. What is it? A moment&#39;s thought will convince you that if we are talking only about the physical world, it just means that it evolves in time according to certain laws. The laws <i>are</i> causality, there is no extra concept needed. From moment to moment, the exact configuration of the universe <i>causes</i> the next configuration. Causes always come before effects. The universe is just a bunch of these particles buzzing about according to the laws, and that&#39;s it.</p><p> But, because entities in our non-physical, &quot;platonic&quot; realm (categories, descriptions, representations, etc) are defined by, and attributed to various configurations of the physical realm, and because the physical realm evolves according to set laws, these &quot;platonic quantities&quot; also evolve in time (at least the ones that are time and space associated). The mappings from physical to platonic are always many-to-one, and so the evolution of platonic entities in time is not perfectly predictable. And, sometimes the opposite happens - the platonic quantity is quite predictable over time <i>despite</i> that it is an extreme summarization of physical state.</p><p> For instance, take the notion of an object&#39;s center of mass. A solid object doesn&#39;t physically &quot;have&quot; a center of mass - rather, the center is a platonic quantity defined in terms of the masses and positions of each particle. But there is nothing physically special at that location. In this particular case, paradoxically, despite our lack of knowledge of individual particle motions, this platonic quantity evolves very predictably in time due to so many internal collisions constantly cancelling each others&#39; positional contributions.</p><p> Zooming out a bit, in the diagram above, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> takes on values in this non-physical realm. If we allow for the moment, an analogy within an analogy, let <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> represent something &quot;closer to&quot; the physical realm here. So, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> is <i>attributed to</i> or <i>describes</i> or <i>summarizes</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> , but it is not in a causal relationship with <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . It cannot be - first of all, because <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> is in the non-physical realm and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> in the physical, and they do not cross. Second, because causal relationships require the cause to precede the effect in time. But here, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> is attributed to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> , thus associated with the <i>same</i> time as <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> occurs.</p><p> Although the evolution in time of a model&#39;s representation <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> doesn&#39;t evolve according to &quot;laws&quot; like <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> does, it <i>tracks</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . So, to the extent that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> evolves according to physical laws, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> evolves in a way that could be accurately predicted. We thus speak of causes and effects in terms of these representations in a metaphorical sense. This is what we do in our daily thoughts and communications. So, to come back to the original claim: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> <i>are</i> causes, they are just not causes <i>of</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . Rather, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z_{t-1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> are causes, in the metaphorical sense, of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span> .</p><h1> The notion of Truth</h1><p> We as humans have a strong sense of what is &quot;true&quot; - and since we don&#39;t have any direct access to the physical realm, this truth can only be a property of our internal representations, which are invented descriptions. Some mental models we build, such as a physio-spatial model, have a very strong notion of truth - I <i>really really</i> believe I&#39;m typing on a laptop right now and not dreaming. Other models have a weaker definition - I am <i>pretty sure</i> I know what some of my friends are thinking much of the time.</p><p> But one thing is clear now - mental model building is necessary across all domains, and the notion of &quot;truth&quot;, useful in all domains, has a very different character in mathematics than it does in psychology or economics for instance. Absolute truth only seems to apply to abstract subjects like math and physics. For others, it is only used in a loose metaphorical sense. Not only that, even in those pure subjects where truth is the goal, it is not confirmable. Science is honest about this - the best we can do is to <i>not</i> falsify the given current theory.</p><p> For awhile I was in a sort of denial about this, and thought sometimes of &quot;the true model&quot; that machine learning was trying to discover. This new duality-based view shatters that notion, and it is actually a great relief to know that there is no such &quot;true model&quot; which I am failing to find. In short, all models are made-up; some predict better than others. (to paraphrase George Box)</p><h2> The Ultimate MNIST classifier</h2><p> If you really wanted to &quot;find the true model&quot;, though, consider the ultimate MNIST classifier. Instead of convolution and pooling layers, a fully connected layer and final softmax, we want to model the <i>true causal process</i> that gave rise to the MNIST image. This started with a human thinking of a digit, then picking up a pen and putting it to paper, then the ink soaking in, then a camera or scanner shining a light on the paper and the photons hitting a CCD chip, binning the intensity levels and finally storing them in a file. This is what <i>actually happened</i> . So, a classifier then would have to model all of this, and then infer what digit the writer was thinking of.</p><p> Real MNIST classifiers, whether machine or Human, don&#39;t do this obviously. They don&#39;t even attempt to <i>approximate</i> it. They do something else entirely that has nothing at all to do with this physical reality.</p><h1> Active Inference</h1><p> So if getting closer to &quot;the true model&quot; is not possible, what criterion for judging a model could apply universally and objectively? The best notion I&#39;ve come across which attempts to serve as a foundation for model building is Karl Friston&#39;s <i>Active Inference</i> - It is a mechanism used by an agent in a world to survive. One byproduct of survival of an agent moving about in a temporal world (ie not just labeling static images) is the ability to <i>predict</i> the future. To my understanding, this is needed in order to perform credit assignment from future consequence of action back to current action, across the environment gap. It is a fascinating theory.</p><p> Within this theory, the agent must predict future sensory input from its own internal representation of its environment. So, the bedrock notion of what makes a &quot;good&quot; internal representation is entirely utilitarian - how useful the representation is to support accurately predicting future sensory input.</p><p> But this is functionally indistinguishable from our psychological notion of truth. An idea is true if it allows us to accurately predict some aspect of the future we care about. It is just the empirical principle of using a theory to test predictions - as long as the theory agrees with observation, it is indistinguishable from truth. Remarkably (to me anyway), this principle seems to work psychologically across all domains from fuzzy to exact, and we experience &quot;truth&quot; along this spectrum as well.</p><h1> Causal reasoning probably requires temporal knowledge</h1><p> To summarize, in this dualistic interpretation, there are two forms of causality - the bedrock physical causality, and the metaphorical causality that arises from it purely as a consequence of how the platonic representations are attributed to physical states. But this only happens for those representations that are bound to physical states at a certain time. Temporality is a prerequisite for metaphorical causality to arise. This seems to imply that machine learning models that hope to learn causal relationships must consume temporal data and model its time evolution.</p><h1> Implication for morality</h1><p> It is said that you &quot;cannot derive an <i>ought</i> from an <i>is</i> &quot;. But, in this new view, this barrier between <i>ought</i> and <i>is</i> dissolves. Or, rather it is side-stepped. Humans have just one brain, capable of constructing representations and ideas to serve the purpose of survival. To the extent that a set of moral principles helps aid that survival, the same process of representation building occurs, just as essential to guilding our behavior as our belief in object permanence or conservation of momentum. If a moral principle that guides behavior leads one to a bad outcome, it could be revised through the same empirical learning process we use for every other mental model.</p><p> The dichotomy between facts and values is attractive only because of a failure to recognize that &quot;facts&quot; exist only in this spectrum of truths, which have varying degrees of predictive accuracy across domains. It is a false dichotomy for two reasons. First because there isn&#39;t any hard threshold in prediction accuracy where a &quot;truth&quot; suddenly just becomes a model. Second, there is a failure to recognize that <i>all</i> mental models for an agent arise from the same requirement to survive, and therefore are implicitly laden with a value, which you might call moral value.</p><h1> Implications for interpretability of representations</h1><p> One goal of interpretability research is to train models that not only produce desired outputs, but produce desired internal <i>representations</i> . In this view, we treat the model not just as a black box. In this endeavor, representations don&#39;t just facilitate training and mediate computation of outputs, but also provide hooks to interpret or knobs to control how a model generates an answer or output.</p><p> However, one problem with this is that, when applied to ever more subtle domains, humans will no longer have consensus on, or awareness of their own internal representations. Even worse, such representations may be interpretable but give a false sense of understanding to the human.</p><p> A more hopeful outcome might be an AI model that can invent interpretations - ones that are better predictors of utilitarian objectives than popular human notions. I&#39;m thinking here of economic models for example. Or, domains in which only theories abound but might have a great room for improvement, such as psychoanalysis.</p><p> Human representations are often flawed - more likely so in domains where humans make more mistakes, or for which only some humans have exceptional skill. This is a potentially tremendous opportunity for an interpretable ML model to serve as a teacher, by solving the same problem a human struggles with, and then explaining in great detail how it solved the problem.</p><h1> Implications for Machine Learning</h1><p> There is no &quot;true&quot; model - the only way to compare two models is by their prediction accuracy. One model&#39;s internal representation <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span></span></span></span></span> is better than another if it supports a more accurate prediction than another.</p><p> Causal reasoning is one facet of interpretability, since we humans can perform it. In light of the two-realm diagram above, we might expect that models presented with a sequence of inputs arranged much like a human might experience that sequence (temporally) will lead to sequences of representations that resemble/imitate/perform causal reasoning. After all, human causal reasoning is a byproduct of the time-evolution of our representations, which is in turn a byproduct of the physical cause-effect chain.</p><p> One final thing to note is that all inference is really prediction. Humans often will make inferences about what happened in the past. However, if I do this, I am updating my mental state (as I travel forward through time) and this in turn may affect what new predictions I will make in light of that update. Thinking about the past does sure feel like time travel though!</p><h1> Conclusion and Reflection</h1><p> For representations, both in machine learning models, and animal mental models, re-interpreting them as learned, fit-for-purpose inventions rather than causal factors of observed data (or sensory inputs) has a few benefits. First, it is a much more generally applicable interpretation. ML models do many different things. All of them produce internal activations en route to an output. It would be awkward to anoint only some models&#39; representations with special properties. Second, this reinterpretation removes an annoying, and artificial semantic barrier between notion of absolute and &quot;fuzzy&quot; truth, and between absolute and fuzzy causal relationships. Truth now means &quot;a representation that produces accurate predictions of future representations&quot;.</p><br/><br/><a href="https://www.lesswrong.com/posts/iYevftcfEDaqAy2c5/z-is-not-the-cause-of-x#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iYevftcfEDaqAy2c5/z-is-not-the-cause-of-x<guid ispermalink="false"> iYevftcfEDaqAy2c5</guid><dc:creator><![CDATA[hrbigelow]]></dc:creator><pubDate> Mon, 23 Oct 2023 18:36:07 GMT</pubDate> </item><item><title><![CDATA[Some of my predictable updates on AI]]></title><description><![CDATA[Published on October 23, 2023 5:24 PM GMT<br/><br/><h2>介绍</h2><p>Author note: I&#39;m struggling to write this in a way I&#39;m happy with. Rather than having it sit in my drafts, I&#39;m going to post in an unfinished state now. I don&#39;t think I endorse sharing/upvoting this much, but if you find the content particularly compelling feel free to overrule me. Epistemic status: speculative and imprecise forecasts</p><p> Joe Carlsmith has a lengthy blog post about <a href="https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk"><u>predictably updating on AI risk</u></a> . I skimmed it and found it interesting. This post includes some of my predictions about AI and AI risk in the next year or so, including what I expect to happen and how I think I should update if my predictions are wrong. Partially I&#39;m writing this to force myself to make predictions, partially I&#39;m writing to get feedback from others about my predictions, and partially I&#39;m writing to try and spread these predictions and their associated updates with others concerned about AI existential safety. This is a suspicious activity partially because it&#39;s really hard to currently know how future events should affect my beliefs. ie, I currently expect a thing like election interference to not change my beliefs much, but there may be unpredicted-by-me circumstances which actually make election interference evidence very important; doing this exercise might make it more difficult to change my beliefs properly later.</p><p> For each item, I&#39;ll note:</p><ul><li> What&#39;s the uncertainty</li><li> What do I expect and how much do I expect it</li><li> How I should update if something I don&#39;t expect happens, and how much I should care</li></ul><h3> tldr</h3><p> In short, here are some things I expect:</p><ul><li> Some big policy stuff will happen, and it will seem neutral or positive</li><li> AI labs will agree on some safety standards which will seem positive</li><li> AI labs might compete on some safety things, but I expect a mediocre outcome</li><li> Task-oriented / agentic LLMs will be a bigger deal, and this will be scary</li><li> Misuse threats will be a big deal, and many more people will care about this</li><li> AI will enable interference in the US 2024 elections, but the policy effects are the main part I care about</li><li> We&#39;ll make significant progress on alignment for current AIs which will look optimistic</li></ul><p> Overall, I expect the most significant updates on the table in the next year are related to how seriously AGI labs seem to be taking existential safety (where I expect some positive signs), and how AI alignment research is going (where I expect pretty good results).</p><p> Now let&#39;s get into the specifics.</p><h2> Some big policy stuff will happen</h2><ul><li> Uncertainty: What will happen with AI regulation</li><li> Expectation: There will be major bills in congress, committees and stuff, forums to discuss risks from AI, including some emphasis on x-risk. I expect it will mostly be unclear how useful any of these things are, and many of them will seem like they are years away from actually affecting AI development. Nevertheless, I expect these will be seen as positive signs. I&#39;m currently at like 70% that there are major moves in the US government on AI by the end of 2024, where major moves is vague. I expect the major moves in the US government seem mostly positive or neutral from an x-risk perspective.</li><li> Violation: I would consider it a violation of my prediction if there were significant regulations or bills gaining traction that seem very bad from an x-risk perspective, eg, requiring AGI labs to open source models, provide training details publicly, stricter anti-trust law around AI developers. I would also consider it a violation of my prediction if the hubbub around AI governance is quieter in October 2024 than it is now (poorly defined). Mostly, if I&#39;m surprised I expect it to be in a negative way, but there&#39;s also some chance regulation seems quite useful. I think it&#39;s pretty unlikely that evidence about AI governance I receive in the next year will update my beliefs about x-risk much.</li></ul><h2> AI labs will agree on some safety standards</h2><ul><li> Uncertainty: How will AGI labs orient to AI safety</li><li> Expectation: This is already happening somewhat. The Frontier model forum will probably do <i>something</i> . Probably a couple more AI labs will sign on to these <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/"><u>whitehouse standards</u></a> . Maybe they&#39;ll all agree to do certain dangerous capability evals or work with certain external red-teams, eg, more AI developers adopt <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>RSP</u></a> s. Maybe I&#39;m 70% that by the end of 2024, OpenAI and Google DeepMind will have adopted responsible scaling policies of some kind. I mostly won&#39;t be surprised if things are looking good in terms of the labs taking AI safety seriously.</li><li> Elaboration: I think one of the plausible paths to victory in this alignment game is a leading lab differentially automating/accelerating safety research, and this requires either this lab to be safety-oriented or heavily and specifically regulated. Therefore, whether the leading labs are serious about catastrophe prevention is a pretty important variable and one where evidence in the next year could be a significant update.</li><li> Violation: I would be surprised if a lab which is less safety-conscious than those mentioned above seems to have the best AI system by the end of 2024; this would make me considerably more worried about x-risk. If OpenAI and Google DeepMind (GDM) have not both adopted RSPs of some sort by EoY 2024, this would be a medium update toward being worried, but plausibly there are circumstances which make this matter much less, eg, model scaling stops working. It&#39;s also possible that the labs exceed my expectations, for instance if they (Anthropic, OpenAI, GDM) adopted “ <a href="https://aaronscher.substack.com/p/consolidation-mechanisms-for-agi"><u>cease and assist</u></a> ” rules that seemed likely to be enforced, or if larger set of labs (eg, Anthropic, OpenAI, GDM, Meta, Adept, Inflection) signed onto an RSP that required a high degree of caution in 2024.</li></ul><h2> AI labs will compete on some safety things</h2><ul><li> Uncertainty: Will AGI labs engage in any meaningful competition on safety. Broadly the thing is “AI labs compete on a bunch of capabilities metrics already, but as safety becomes more of a concern to the public and labs, we should also expect this for some safety things”, even if they&#39;re not the most important things.</li><li> Expectation: We&#39;re already seeing some of this with benchmarks like TruthfulQA and harmlessness comparisons. Depending on what benchmarks and metrics are available, there may be other forms of this. I mostly expect to feel *positive shrug* toward the work that comes out of this, as I don&#39;t expect it to be very impressive or total garbage. Maybe I&#39;m 60% that by the end of 2024 there will be examples of decent research from one lab that seem aimed at outcompeting another lab on safety.</li><li> Violation: Both having and not having competitions on safety seems pretty reasonable. I would be positively surprised if there were major research successes coming out of research competitions, but it&#39;s hard to imagine what this looks like.</li></ul><h2> Task-oriented / Agentic LLMs will be a bigger deal</h2><ul><li> Uncertainty: Will there be substantial advances in AIs capable of accomplishing tasks in the real world</li><li> Expectation: AutoGPT is pretty bad and just scratching the surface. I believe there is substantial headroom here, by which I include both substantial capability gains over current systems, and substantial usefulness/economic value to be captured. More specifically, I expect GDM to release Gemini soon, and for RL on short to medium length task completion to be a thing. I expect Gemini to be approximately as good as GPT-4. I don&#39;t have much of a prediction for whether Gemini will be a closed release, via API or open, I guess I&#39;ll update slightly on GDM, though I&#39;m unsure how big of an update.</li><li> Violation: I will be surprised if Gemini hasn&#39;t been publicly demoed by March 2024, or if it performs more than 10 percentage points differently than GPT-4 when averaged across many benchmarks. If Gemini is substantially better than GPT-4 I&#39;ll become slightly more confident in short timelines, whereas if it is substantially worse than GPT-4 this would be a slight update toward longer timelines and on OpenAI having a bigger lead than I currently expect.</li><li> Expectation: I think it wouldn&#39;t be crazy if there were AI agents doing stuff online by the end of 2024, eg, running social media accounts, selling consulting services; I expect such agents would be largely human-facilitated like AutoGPT. I will be slightly surprised if by end of 2024 there are AI agents running around the internet that are meaningfully in control of their own existence, eg, are renting their own cloud compute without a human being involved. I expect most of this stuff will be fairly concerning when it happens; I&#39;m trying to price that in now. I would be positively surprised if the US government instituted rules significantly restricting the operation of autonomous agents by end of 2024 (eg, saying compute providers can only sell to humans, with enough nuance that this isn&#39;t easily bypassed).</li><li> Violation: I don&#39;t think updates here would be very influential, except I would get a bit more worried and confident in short timelines if we got really good agents in 2024. If the state of agents at the end of 2024 is as bad as currently, this would be a slight positive update on doom and toward longer timelines (it&#39;s only a slight update because agents soon isn&#39;t a major part of why I&#39;m worried, and a lack of agents in 2024 isn&#39;t much evidence about agents a generation or two later).</li></ul><h2> Misuse threats will be a big deal</h2><ul><li> Uncertainty: Will the proliferation of powerful AIs lead to misuse-based catastrophes</li><li> Expectation: LLMs that significantly help with the creation of bio weapons are 2-3 years away, according to <a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf"><u>Dario Amodei</u></a> ; hacking capabilities are probably around the same or sooner. Preventing misuse will have a lot of open problems (ie, good area for research now!). Also multimodal and image models will see progress, making particular risks like deepfakes more prominent. Some of the necessary problems will receive a fair amount of attention by default, as watermarking seems to be getting, but others may not. Historically I think I&#39;ve been less worried than I should be about misuse-related AI catastrophe — oops. A specific case is that I used to not be sure if we would get warning shots. It now looks like we&#39;re probably going to get medium warning shots on the scale of hundreds of millions of dollars or hundreds of deaths, due to AI-enabled attacks in the next few years. I&#39;m slightly surprised we haven&#39;t seen effective misuse of current open source LLMs, but this seems like mostly a matter of time. I&#39;m probably 65% that there is a misuse of AI that causes concrete damage in the world by EoY 2024 (eg, a cyber attack that costs >;100k), and more like 10% on medium warning shots by EoY 2024.</li><li> Violation: Overall, I think we&#39;re on a trajectory where I&#39;ll get a bit more worried about misuse threats, but this prediction could be violated by eg, strong rules banning open source. I would be surprised if there was a point in the future where I thought total misuse x-risk was higher than total misalignment x-risk — but this very possible if, for instance, catastrophic misuse risks end up seeming very likely. I would also be surprised if at some point in the next year I thought catastrophic misuse risk was &lt;5% likely at all. I think these should be small to medium updates. I would be surprised if there was a medium warning shot by EoY 2024, and this would probably make me more confident in short timelines, and have an unclear effect on overall doom (depends how the world reacts).</li><li> Expectation: I expect people outside of the current AI safety community to get even more worried about misuse risks than they currently are, and for there to be substantial research aimed at addressing these risks.</li><li> Violation: I would be surprised if EoY 2024 people were less worried about AI misuse than they are in October 2023. I might have a small update, depending on why this was.</li></ul><h2> Likely major foreign interference in US 2024 elections</h2><ul><li> Uncertainty: Will AIs be used to enable interference in US elections, and how will the US respond. Maybe this becomes a thing 2024 candidates have positions on. Like if there&#39;s a bunch of LLMs impersonating people and manipulating voters, candidates might have to answer about this in debates; what the various proposals are and who&#39;s in the room could matter. The impact of this on policy probably depends a bit whether it “blows up” before or after the election. Thanks Charlotte for discussion.</li><li> Expectation: Maybe I&#39;m 65% that there will be significant interference and people know it. I also wouldn&#39;t be horribly surprised if there isn&#39;t substantial AI-accelerated interference with the election. I think it&#39;s unlikely but plausible (30%) that there&#39;s a policy reaction to election interference which seems good in terms of x-risk reduction.</li><li> Violation: This isn&#39;t a key factor in my world model, and I think the most important aspects of it are whether or not it causes policy responses, which, again, may be downstream of when this becomes a big deal, if it becomes a big deal. I mostly don&#39;t want to update on how election interference goes, unless it seems likely to cause particular policy changes or spur large amounts of somewhat-useful research (which currently seems like 50% likely). We pretty much have the capabilities, and there are plenty of resourced actors with the will, so whether or not major interference actually happens or merely could have happened doesn&#39;t seem significant in terms of AI development (obviously it matters for other reasons).</li></ul><h2> We&#39;ll make significant progress on “alignment” for current AIs</h2><ul><li> Uncertainty: Will we make progress on alignment of current AI systems</li><li> Expectation: I think alignment research in the current LLM paradigm is relatively easy, and I expect there are significant wins that we haven&#39;t seen yet but will see soon (I might write up why I think this is easy, but in the meantime readers can also check out these posts: <a href="https://www.lesswrong.com/posts/GkC6YTu4DWp2zwf9k/giant-in-scrutable-matrices-maybe-the-best-of-all-possible"><u>1</u></a> , <a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Key_tools"><u>2</u></a> ). In the next 6 months I expect (75%) there will be at least one notable advancement in scalable oversight, eg, an AI debate setup that yields >;5% improvement over previous methods, think something similar in magnitude to Constitutional AI&#39;s improvement over simple RLHF. I also expect there will be a bunch of interpretability research that seems helpful and interesting. I mostly expect that there will be high quality and useful prosaic / empirical alignment work, but that there will be considerably fewer successes for research aimed at future systems, eg, agent foundations.</li><li> Violation: I would be surprised if 2024 had more than 10 significant advances in alignment, or fewer than 3. The things that count as semi-significant advances for 2023 according to me are: success of activation steering in interpretability, success of dictionary learning to largely solve polysemanticity in interpretability. Said differently, I&#39;m expecting a 1.5-5x increase in significant advances next year. I think it would be a substantial update if we get more than 10 significant advances, and it would be a moderate update if we get fewer than 3. I would count research toward this if it was conducted earlier but only first publicized after October 2023. I might later write up something about what counts as significant advances in my eyes.</li></ul><h2>结论</h2><p>Writing this list has forced me to think about the future in a way I don&#39;t usually do, which seems useful. On the whole, I expect to see some positive-but-not-amazing signs in the next year. Agentic AIs are probably going to happen soon, plausibly in the next year, and they&#39;re gonna be wild/scary. On the other hand, I expect we&#39;ll make significant progress on alignment and labs will seem to be taking existential risk (including misuse and misalignment) seriously.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/pE93AGxXNiqGA6Brq/some-of-my-predictable-updates-on-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pE93AGxXNiqGA6Brq/some-of-my-predictable-updates-on-ai<guid ispermalink="false"> pE93AGxXNiqGA6Brq</guid><dc:creator><![CDATA[Aaron_Scher]]></dc:creator><pubDate> Mon, 23 Oct 2023 17:24:35 GMT</pubDate> </item><item><title><![CDATA[Programmatic backdoors: DNNs can use SGD to run arbitrary stateful computation]]></title><description><![CDATA[Published on October 23, 2023 4:37 PM GMT<br/><br/><p> <i>Thanks to Kshitij Sachan for helpful feedback on the draft of this post.</i></p><p> If you train a neural network with SGD, you can embed within the weights of the network any state machine: the network encodes the state <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>in its weights, uses it and the current input to computes the new state <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_{t+1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span> , and then uses SGD to encode the new state in its weights.</p><p> We present an algorithm to embed a state machine into a pytorch module made out of neural network primitives (+, x, sigmoid, ...) but using a non-standard architecture. We use it for several toy experiments, such as this experiment where we translate a simple state machine into a neural network, and make the output of the network vary when we reach an intermediate state: </p><figure class="image image_resized" style="width:96.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/ks680jefyo9ykhhcjnbv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/uk7v5a29i3qasygoj8op 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/pcwtptj0dycka3sfavrb 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/ypqjdpdjzixxw4wkpeta 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/klfnme3lfncgjvriebmb 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/emeaf8fpbmai4rvnaxwq 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/uisozzxl4pzuua1buldz 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/voaypw29r3nka4nckzsv 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/beads8jmlnx3tmrr2sfi 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/q9otaqdh2vifzxphcwso 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/ojgmcddizkexm28c6vnz 1836w"></figure><p> In this post, we explain how to embed a state machine into a neural network, discuss some of the limitations of our algorithm and how these limitations could be overcome, and present some experimental results.</p><p> Our embedding is quite efficient. If you&#39;re allowed to use stopgrad (a function that stops the gradient from passing through), we only need one parameter to store one real number, and storing only requires a small constant number of floating point operations. If you aren&#39;t allowed to use stop grad, we can&#39;t encode as efficiently.</p><p> We did this work for two reasons:</p><ul><li> It was fun and easy, and gave us some new intuition for how SGD works. Also, as computer scientists, we&#39;re professionally obligated to take opportunities to demonstrate <a href="https://gwern.net/turing-complete"><u>that surprising things allow implementation of general computation</u></a> .</li><li> Part of our motivation in constructing this was to shed some light on <a href="https://www.lesswrong.com/posts/bdayaswyewjxxrQmB/understanding-gradient-hacking"><u>gradient-hacking</u></a> concerns. This particular construction seems very unlikely to be constructible by early transformative AI, and in general we suspect gradient hacking won&#39;t be a big safety concern for early transformative AI (ctrl-f “gradient hacking” <a href="https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><u>here</u></a> for more of Buck&#39;s unconfident opinion on this). But it does allow us to construct a lower bound on the maximum possible efficiency of a system that piggybacks on SGD in order to store information; further research on the tractability of gradient hacking might want to use this as a starting point.</li></ul><p> Simple notebooks with our experimental results can be found here: <a href="https://github.com/redwoodresearch/Gradient-Machine"><u>https://github.com/redwoodresearch/Gradient-Machine</u></a></p><h1> The embedding algorithm</h1><h2> The idea</h2><p> As a simple example, suppose you&#39;re training a model N on the regression problem of matching the function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> , with loss</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L = \frac 1 2 (N(x) - f(x)) ^ 2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></p><p> We want to piggyback on the SGD update step to make an update of our choice to a particular parameter <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span> .</p><p> Given some arbitrary function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi(x, s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> defining a state machine, and access to the function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> to match, suppose we define our model as follows:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(x) = \text{stopgrad}(f(x) - \phi(x, s)) + s"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span></p><p> Then, our stopgraded computation of the input and the actual label will cancel out, so the loss will be:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L = \frac 1 2 (s - \text{stopgrad}(\phi(x, s)))^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></p><p> Thus, SGD will make s closer to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi(x, s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> . And even better, if the learning rate is 1, after one step of SGD:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s \leftarrow s - \frac {dL}{ds} = s - (s - \phi(x, s)) = \phi(x, s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">←</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.993em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.404em; top: -1.41em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.404em; bottom: -0.703em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.993em;" class="mjx-line"></span></span><span style="height: 1.495em; vertical-align: -0.497em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> We have successfully updated <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span> to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi(x, s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> !</p><h2> In a more realistic NN context</h2><p> Let&#39;s say that instead of having access to the ground truth function, you have a very good predictor of the labels <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M_\theta: X \rightarrow Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.081em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span></span> , and that you approximate the state machine <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi: (X,S) \rightarrow S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> you wish to embed with a neural network <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> . Then, you can build a <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> function which computes which deviation from the optimal value would lead to appropriate change of state, and combine them in the following neural network <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span> : </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/mxhpownykby3ooemkuwk"></p><p> For example, if the loss function is the mean-squared error <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L = \frac 1 2 (N(x) - f(x)) ^ 2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , then, if <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> are good enough, the following function would provide the desired change: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta(\hat y, s', s) = \text{stopgrad}(\hat y) + \frac 1 {\sqrt \alpha} (s - \text{stopgrad}(s’))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.083em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.083em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.183em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.673em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.673em; bottom: -1.059em;"><span class="mjx-msqrt" style=""><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span> <span class="mjx-box" style="padding-top: 0.236em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.183em;" class="mjx-line"></span></span><span style="height: 1.72em; vertical-align: -0.749em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> , where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\alpha"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span> is the learning rate. (We can show this using the same derivation as the one from the previous section.)</p><p> If<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\dim s > \dim y = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dim</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dim</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> , we can modify the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> as follows: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta(\hat y, s', s) = \text{stopgrad}(\hat y) + \frac 1 {2 \sqrt \alpha} n(\sum_i [s_i - \text{stopgrad}(s’_i)]^2)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.083em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.083em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.537em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 2.173em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 2.173em; bottom: -1.059em;"><span class="mjx-mrow" style=""><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span> <span class="mjx-box" style="padding-top: 0.236em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.537em;" class="mjx-line"></span></span><span style="height: 1.72em; vertical-align: -0.749em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-munderover"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∑</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> , where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n(x) = \frac x {\sqrt {\text{stopgrad}(x)}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 4.481em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 6.337em; top: -1.159em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 6.337em; bottom: -1.259em;"><span class="mjx-msqrt" style=""><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.593em; padding-bottom: 0.593em;">√</span></span> <span class="mjx-box" style="padding-top: 0.063em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 4.481em;" class="mjx-line"></span></span><span style="height: 1.71em; vertical-align: -0.89em;" class="mjx-vsize"></span></span></span></span></span></span></span> 。 <span class="footnote-reference" role="doc-noteref" id="fnrefc11clzbsngr"><sup><a href="#fnc11clzbsngr">[1]</a></sup></span></p><p> Similar <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> functions could be found for other loss functions: the idea is to make s responsible for an increase in loss of the right magnitude.</p><h2> The need for stopgrad and its implementation</h2><p> A key part of most <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> functions is stopping the gradient from modifying either <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> or <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M_\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.081em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span></span></span> : without the stopgrad (which prevents propagating the gradient through <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s’"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> ), either of those neural networks could learn to anticipate the action of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> to lower the loss. For instance, with the MSE example above, and ignoring the gradient through <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> would eventually learn to predict <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y - \frac 1 {\sqrt \alpha} (s - \text{stopgrad}(s’))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.183em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.673em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.673em; bottom: -1.059em;"><span class="mjx-msqrt" style=""><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span> <span class="mjx-box" style="padding-top: 0.236em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.183em;" class="mjx-line"></span></span><span style="height: 1.72em; vertical-align: -0.749em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> rather than <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> , removing the gradient used to update <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span> .</p><p> Stopgrad function can be approximated if sigmoid activations <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sigma(x) = \frac 1 {1+\exp(-x)}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 3.63em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 5.134em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 5.134em; bottom: -0.999em;"><span class="mjx-mrow" style=""><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">exp</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 3.63em;" class="mjx-line"></span></span><span style="height: 1.677em; vertical-align: -0.707em;" class="mjx-vsize"></span></span></span></span></span></span></span> are natively available: using the plateaus on either side of the sigmoid function, the gradient from a binary variable can be mostly removed by passing a binary variable <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> through <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{sigmoid_stopgrad} = \sigma((b - \frac 1 2) * 1000)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">sigmoid_stopgrad</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1000</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> (which will have tiny gradients, even if the slope <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=1000"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1000</span></span></span></span></span></span></span> is a differentiable parameter). This can be extended to variables which can take a finite number of values: using <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span> layers with <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> sigmoid neurons, you can build a staircase with around <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n^d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span></span></span> steps, and because each step is flat, it mostly blocks the gradients from flowing through. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/ikzr8duss6uqik6ni1wo"></p><p> However, a neural network would need a very large amount of control over its own activations to be able to create such a pattern.</p><p> Stopgrad can also be implemented by multiplying a big weight by a tiny constant, eg replace <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y(x) = \theta x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y(x) = \epsilon \theta' x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> (using the initalization <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta' = \theta / \epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> ). This multiplies the effective learning rate by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , since after one step of SGD on <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y(x) = \epsilon (\theta ' - \alpha \frac {dL} {dy}_{x_0} \epsilon x_0) x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.993em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.404em; top: -1.41em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.404em; bottom: -0.898em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.993em;" class="mjx-line"></span></span><span style="height: 1.633em; vertical-align: -0.635em;" class="mjx-vsize"></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.948em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> . This means the gradient with respect to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta'"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span></span> can be ignore if the effective learning step is small enough relative to the number of training step used in practice, or if the floating point numbers underflow during the update of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta'"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span></span> . This might be achieved with regular neural network primitives by using layernorm, which will make even large magnitude inputs small, thus decreasing the effective learning rate - though the denominator in layernorm also depends on its input, which may make it difficult to use it in practice.</p><p> Implementing stopgrad with ReLUs is possible as long as the slope is a fixed large parameter <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span></span></span></span></span></span> : <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{step}(x) = \text{ReLU}(Px) - \text{ReLU}(P(x - \epsilon))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.519em;">step</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> is a sharp step function of height <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P \epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> and width <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> , which can be used to build stopgrad. But <i>this construction does not work in usual MLPs</i> , since the slopes of both ReLU functions are individually tunable, which means that the derivate of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{step}(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.519em;">step</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> with respect to the slope parameter in the first ReLU is not negligible (for <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x>0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> ) (which is not the case for the single slope parameter in a sigmoid function).</p><p> But we think it might be possible for models to implement stopgrad in neural networks in a better way. For example, it might be possible to build stopgrad without the need for a sigmoid activation function. Thus, the complexity of our implementation may overestimate the complexity of the implementation of stopgrad, that could be used in practice by gradient-hacking AIs.</p><h2>局限性</h2><h3>What if you use a batch size greater than 1?</h3><p> If the batch size is greater than 1, then the network <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> approximating the state machine is still limited to operations on individual inputs, but updates to the state will be done in batches. The algorithm described above can be extended to state machines with aggregatable updates: the state machine still operates on individual inputs, but <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi(x_B, \phi(x_{B-1}, …, \phi(x_1, s_0))) - s0 \approx C \sum_i (\phi(x_i, s_0) - s_0)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∑</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">ϕ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> , (where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="C"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span></span></span></span></span> is a constant known when creating the embedding and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> is the batch size), because applying the algorithm naively will lead to the target states s&#39; obtained for each input in the batch to average out.</p><p> Updates are aggregatable for state machines that operate on the number or the proportion of inputs on which a certain property holds. But this prevents the embedding of state machines like the ones computing <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="max_x P(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> for <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span></span></span></span></span></span> a scalar property <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X \rightarrow \mathbb R"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> .</p><h3> What if you use Adam or SGD with momentum?</h3><p> The idea described in this post can&#39;t be applied to optimizers which have a state like Adam or SGD with momentum, because <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> would need to know the state of the optimizer corresponding to s in order to create the appropriate output. This means each state tracked by the network would use two states (the state itself and the state of the optimizer corresponding to this state), which seems to be impossible (though further research might find a way to adapt ideas described here to optimizers with a state).</p><h3> What if you don&#39;t have a perfect predictor of the labels?</h3><p> Imperfect predictors limit our ability to change s precisely. If we can afford a very large loss, simply scaling s allows meaningful manipulation of the state: given a regression problem where the label y* is known to be in [-1,1], and using the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> function described above for the MSE loss applied to the constant predictor <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M(x) = 0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> , the update becomes <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s \leftarrow s’ + f(s)\sqrt \alpha"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">←</span></span> <span class="mjx-msup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msqrt"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span> <span class="mjx-box" style="padding-top: 0.236em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span></span></span></span></span> , which conveys meaningful information as long as <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s’ - s \gg \sqrt \alpha"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">≫</span></span> <span class="mjx-msqrt MJXc-space3"><span class="mjx-box" style="padding-top: 0.045em;"><span class="mjx-surd"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">√</span></span> <span class="mjx-box" style="padding-top: 0.236em; border-top: 1px solid;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span></span></span></span></span> . This naive strategy leads to a very high loss and huge loss spikes. But if we tolerate noisy writes, and if we have an accurate (but not perfect) predictor, the amount of additional loss required can be relatively small.</p><h1> Experiments</h1><h2> Toy scalar model</h2><p> Our toy task is the following: a model should output 1 if the data point (x1, x2) is at L1 distance greater than 1 from the origin, and 0 otherwise, and is scored with a MSE loss. The state should track the number of data points seen where x1 >; 3 (which is an aggregatable state machine). The training data is normally distributed. We use a depth-1 width-40 staircase as stopgrad, such that the final model is only made out of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="+"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sigma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span></span></span></span></span></span> (as well as some indexing and concatenation).</p><p> We compare two scenarios: the case where the model M has its weights hard-coded to the exact problem solution, and the case where the weights are learned (from a random initialization, using SGD on 500k normally distributed inputs). In both cases, the model is an MLP with 4 intermediate neurons. We see that small imperfections in M leads to inaccurate state tracking. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/vhy6t7dzbmar47c7j7ki"></p><h2> A more fancy toy scalar model (from the figure at the top)</h2><p> We use the same methodology as for the previous toy model, except that we use two states, we use the variation of the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> used for multiple variables (which in particular means that we use the divide operator on top of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="+"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\times"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span></span></span></span></span></span> , and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sigma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span></span></span></span></span></span> ), and we use the pytorch default built-in stopgrad function (detach) for simplicity. The two states are s0:“number of points with x1>;3” and s1:“number of points with x2>;3 when s0>;10”.</p><p> We then use some logic (implemented with sigmoids) to change the output of the network when s1 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=">"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span></span></span></span></span></span> 10 &amp; s2 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\leq"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≤</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> 10. We chose a particular change which didn&#39;t affect the gradient (we added a constant inside the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> function) and caused a constant loss increase, but in principle any modification is possible as long as it is accounted for by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> if the state needs to be changed.</p><p> Because we have a very good predictor, we can scale down the state by a large factor (eg “count=7” is represented by 0.07 instead of 7), which leads to smaller loss spikes when the state is changed.</p><h2> MNIST flips</h2><p> To validate the general methodology in a more realistic setting, we study the following setup: we use the MNIST dataset, and we flip a proportion p of the images in each batch along their vertical axis. We want to see if the model can have an inner parameter which matches p. To do so, we train two small CNN: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> , which classifies the images (trained with the cross-entropy loss, ending at 94% top-1 accuracy), and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> , which classifies images as flipped or not flipped (trained with the binary cross-entropy loss, and ending at 99% top-1 accuracy).</p><p> Here, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Φ</span></span></span></span></span></span></span> are noisy, so we don&#39;t try to get an exact <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span></span></span></span></span></span> , and simply use <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta(y, s’, s) = \text{stopgrad}(y) (1 - s - \text{stopgrad}(s’))^\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">stopgrad</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span></span></span> , where y are the logits, which incentivizes <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span> to be close to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s’"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span></span> (the proportion of image flipped) in the where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span></span></span></span></span> is the strength of the incentive.</p><p> We find that this technique works well across a wide range of strengths. We report 1-sigma standard deviation over the p_flip value measured after the end of each of the 5 epochs we run the model on. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QNQuWB3hS5FrGp5yZ/ls02itpil2gtlbds9jdn"></p><h1> Future work</h1><p> We have embedded a state machine into a neural network build with neural network primitives, but with a non-standard architecture. Future work could figure out how to embed state machines inside regular MLPs or Transformers.</p><p> Future work could make our discussion of &quot;What if you don&#39;t have a perfect predictor of the labels?&quot; more quantitative, and thereby establish a bound on how much computation can happen using imperfect predictors. This bound might be enough to rule out this kind of behavior happening in current models and future models. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnc11clzbsngr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc11clzbsngr">^</a></strong></sup></span><div class="footnote-content"><p> The expression is different than the 1-variable case because we need to tie <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span> to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_i'"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span></span> . To do so, we sum the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(s_i - s_i')^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> . But this would leave us with <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2 (s_i - s_i')\sum_i [s_i - s_i']^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∑</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , which is why need the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> function.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/QNQuWB3hS5FrGp5yZ/programmatic-backdoors-dnns-can-use-sgd-to-run-arbitrary#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QNQuWB3hS5FrGp5yZ/programmatic-backdoors-dnns-can-use-sgd-to-run-arbitrary<guid ispermalink="false"> QNQuWB3hS5FrGp5yZ</guid><dc:creator><![CDATA[Fabien Roger]]></dc:creator><pubDate> Mon, 23 Oct 2023 16:37:45 GMT</pubDate></item></channel></rss>