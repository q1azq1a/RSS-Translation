<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 7 日星期四 18:15:24 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Meetup Tip: Heartbeat Messages]]></title><description><![CDATA[Published on December 7, 2023 5:18 PM GMT<br/><br/><h2>概括</h2><p>“心跳消息”是一个技术术语。这是一条定期发送的消息，其中包含有关某个系统状态的一些信息，但心跳消息最重要的是一切都运行良好，足以发送该消息。与人体类比，如果您听到微弱或不稳定的心跳，则可能出现问题，但如果您<i>停止听到</i>心跳，则肯定出现问题。</p><p>心跳消息对于聚会团体或大型聚会活动也很有用。即使消息正文中不包含新信息，消息发送的事实也证实了仍然有一个活跃的组织者在掌舵。心跳还提醒人们“哦，是的，我对此很感兴趣。”</p><p>您现在已经阅读了这篇文章的基本要点。如果你想继续读下去，没关系，让我们谈论心跳，而不是绝对必要的。</p><h2>细节</h2><p>聚会中的心跳消息有两个广泛的用例。一是您是否有一个存在的常规社区并进行正常的聚会活动。另一个是如果您要举办一些特定的大型活动。这些心跳会略有不同，但它们有很多共同点。</p><p>我喜欢从我真正想让别人知道的基础知识开始每次心跳，即使这是他们要阅读的唯一消息。对于一个大事件来说，“什么时候发生、在哪里发生”是关键信息。对于常规社区，这可能是您可以去获取信息的其他地方（例如，如果有 Facebook 群组、Discord 服务器和 google 群组……）或您不希望人们忘记的资源。 OBNYC 有一个每月聚会队长的职位，每个月都会有一条消息提醒社区他们需要一名队长。</p><p>重大事件的心跳消息的节奏可能会随着时间的推移而改变。让我们<a href="https://rationalistmegameetup.com/">以东海岸理性主义者大型聚会</a>为例。这是每年 12 月发生的一次大型活动。第一个公告通常是在中秋，我尝试每月发送一封电子邮件，直到 11 月底增加到每周一次。前一周，我再次增加，目标是在一周开始时做一次，前一天做一次，当天做一次。 （顺便说一句，如果您在 2023 年 12 月 9 日或之前阅读本文，并且您恰好在纽约市，请考虑参加大型游戏集会！）</p><p>对于普通社区来说，这可能有所不同。如果有很多事件正在发生并被宣布，它们就可以充当自己的心跳。湾区 LessWrong 邮件列表有相当可靠的周四晚餐，加上大多数星期的奥克兰聚会，以及其他一些聚会。如果您每周至少有一个事件，我不知道您是否需要单独的心跳，因为事件本身就是事情正在发生的证据。另一方面，我不会为本地社区提供超过聚会频率两倍的心跳节奏。那么，如果你们在春天和秋天见面一次，也许一年会见四次，但不会见五次？我认为真正正确的节奏是将春季和秋季聚会视为大事件，并为每个聚会发布一个月的消息，然后一周的消息。</p><p>有些地方有针对垃圾邮件或线程死灵的规范（在频道中发帖只是为了吸引人们对旧帖子的注意，或将其置于按最新排序的内容的顶部。）如有疑问，请咨询管理员。</p><p>心跳消息是寻求帮助或提及您遇到的问题的好地方。这就是他们在科技领域的目的。 “我们通常的组织者下个月就要出城了，有人想填补他们的空缺吗？” “我需要一些志愿者在夏至前搭建舞台设备，并在夏至后帮忙收拾行李，有人有空吗？”</p><h2>快速提示</h2><p>我建议你复制并粘贴开头，然后更改一两个连接或无意义的句子。纯粹的复制和粘贴似乎被掩盖了，从头开始编写东西需要时间，所以两者都做一点。</p><p>许多消息服务可以安排消息。例如，Gmail 就可以。使用它来检测心跳有优点也有缺点。在专业方面，您可以设置提醒并去做其他事情。例如，当我在飞机上、睡觉或工作时，或者在一天的忙碌节奏中排队几个人出去时，我就会使用它。不利的一面是，如果情况变化很快，您可能会意外地获得过时的信息。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages<guid ispermalink="false"> ksJeCjnewZTYTamgz</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 07 Dec 2023 17:18:33 GMT</pubDate> </item><item><title><![CDATA[[Valence series] 2. Valence & Normativity]]></title><description><![CDATA[Published on December 7, 2023 4:43 PM GMT<br/><br/><h1> 2.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p><a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>解释了我所说的“价”一词的含义。现在，在第 2 篇文章中，我将讨论效价在欲望、偏好、价值观等“规范”领域中的核心作用。如果您想知道，效价与信念、期望等“积极”领域之间<i>也</i>存在关系，但我们将在第 3 篇文章中讨论这一点。</p><p>效价在规范领域中的作用怎么强调都不为过：我认为效价正是构建所有规范性的实质。</p><p>需要明确的是，这<i>并不</i>意味着，一旦我们了解了化合价的工作原理，我们就绝对了解了整个规范宇宙的一切。打个比方，“原子就是构成所有细菌的物质”；但如果你想了解细菌，仅仅了解原子是什么以及它们如何工作是不够的。你还有很多工作要做！另一方面，如果你<i>不</i>知道原子是什么，你就很难理解细菌！我认为，它的效价和规范性也是如此。</p><p>该帖子的组织如下：</p><ul><li> <strong>2.2 节</strong>讨论了一种误导性的直觉，即效价似乎与现实世界的事物、行动、计划等相关。我们说<i>“这是一个坏主意”</i> ，而不是<i>“当我脑子里有这个想法时，它会唤起一种负价的‘坏’感觉”</i> 。这是接下来的一切的重要背景。</li><li><strong>第 2.3 节</strong>讨论了价评估直接对应于有意义的（尽管是快速的）规范评估的情况。例如，如果我有一个与具体计划相对应的想法（“我会站起来”），那么我的大脑会根据该想法的效价是积极还是消极来判断这是一个好计划还是坏计划分别——如果这是一个好计划，我很可能会实际去做。同样，如果我想象世界未来可能的状态，该想法的效价对应于对该状态是好还是坏的评估——如果是好，我的大脑就有可能执行实现它的计划，并且如果情况不好，我的大脑就会执行计划来避免它。因此，我们在价信号、感受到的欲望以及我们的行动和决定之间得到了预期的直接联系。</li><li> <strong>2.4 节</strong>讨论了一个不同的情况：概念的效价。例如，如果我“喜欢”共产主义，那么涉及“共产主义”概念的思想很可能是正价的。我认为，这不能直接解释为对任何特定事物进行有意义的规范性评估，而是我们应该将这些视为习得的规范性<i>启发法</i>，有助于为有意义的规范性评估提供信息。然后我谈论基于共鸣的“毫无意义的争论”，比如争论是否“支持”或“反对”以色列。</li><li> <strong>2.5 节</strong>讨论了效价如何设定和调整，特别强调先天驱动力（例如，饥饿时吃东西的驱动力）作为效价评估的最终基础。</li><li> <strong>2.6节</strong>讨论了元认知思想和自我反思思想的效价，包括自我和谐倾向和自我失调倾向之间的区别，以及人们在谈论他们的“价值观”时在谈论什么。</li><li><strong>第 2.7 节</strong>简要介绍了道德推理如何融入这个框架，首先是描述性的（当人们进行“道德推理”时，他们在做什么？），然后思考对元伦理学的影响。</li><li> <strong>2.8 节</strong>是一个简短的结论。</li></ul><h1> 2.2 价是现实世界事物的属性的（误导性）直觉</h1><p>回想一下<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章的第 1.3 节</a>，在我提出的模型中：</p><ul><li>我们大脑的一部分“思考一个想法”，这可能涉及你正在计划、看到、记住、理解、尝试等的事情，并且可能涉及关注某些事情，或调用某些框架/类比等；</li><li> ......我们大脑的另一部分为这个“想法”分配了一个价。</li></ul><p>所以：</p><p><i>从外部来看，</i>价是将“思想”（在特定人的大脑中）映射到标量实数的函数。</p><p><i>从内部来看，</i>思想是我们了解现实世界的唯一窗口。因此，<i>感觉就像</i>效价“附加”到现实世界的事物、情况等上——在<a href="https://www.lesswrong.com/posts/fG3g3764tSubr6xvs/the-meaning-of-right"><u>Eliezer Yudkowsky 令人难忘的类比</u></a>中，感觉就像“一个额外的、物理的、本体论的基本属性，像一个小小的 XML 标签一样挂在事件上”。我认为直觉是一种知觉错觉，一不小心就会误导你。 （为布尔弗主义<span class="footnote-reference" role="doc-noteref" id="fnreftss4t9kg1lr"><sup><a href="#fntss4t9kg1lr">[1]</a></sup></span>道歉，但我认为这种知觉错觉导致了“道德现实主义”阵营中的一些糟糕的哲学观点。）</p><p>例如，假设我的大脑对涉及月亮的想法赋予了高价。 （用日常用语来说，“我喜欢月亮，它很酷”。）<i>在我看来，</i>酷是真实世界月亮的一个属性。毕竟，每当我看到月亮时，我都会有一种凉爽的感觉。如果没有这种印象，我什至无法<i>想象</i>月亮！这有点像<a href="https://plato.stanford.edu/entries/introspection/notes.html">冰箱的光错觉</a>。但实际上，良好的氛围并不存在于现实世界中，就像 XML 标签一样附加在月球上。唯一发生的事情是我的大脑的价函数已经学会了某种启发式：检查“月亮”概念在我的大脑的世界模型中是否活跃，如果是，则增加该想法的价。</p><h1> 2.3 效价直接对应于有意义的（尽管是快速的）规范性评估的情况：计划、行动和想象的未来的效价</h1><p>经济学家谈论“积极主张与规范主张”；哲学家（继<a href="https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem"><u>休谟</u></a>之后）在“是与应该”的标题下谈论同样的事情。积极陈述是对世界实际情况的事实主张，而规范性评估是对世界应该如何或人们应该做什么的价值判断。</p><p> （我在本节中讨论的“规范性评估”类型是快速“快速”判断，而不是我自豪地认可和支持的经过仔细考虑的评估。但是，我将在下面的第 2.7 节中指出，前者服务于作为后者构建的基础。）</p><p>价与规范性有什么关系？<i>一切！</i>确实，我认为<strong>价是大脑中所有规范性的最终货币</strong>。</p><p>这一主张最终源于<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章第 1.3 节</a>中讨论的化合价的作用。简单回顾一下，假设一个想法突然出现在我的脑海中——“我要站起来了”。如果这个想法的效价是积极的，那么这个想法就会留在我的脑海里，而且我可能会真的这么做。如果价是负的，那么这个想法就会被抛弃，并被另一种想法所取代，可能是涉及躺在床上的想法。因此，化合价是这一决定的基础。 <a href="https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle"><u>与奇怪的“主动推理”想法相反，</u></a>效价的作用与我的生成世界模型预测未来的能力无关：我的世界模型（“思想生成器”）同样能够对未来世界进行建模在我起床的地方，在我不起床的地方建模未来世界。因此，价在这里的作用从根本上来说是规范性的，而不是积极的。 </p><figure class="image image_resized" style="width:96.04%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvbsca3qzufottbjnuru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tvjcjuhqeligwiu2obq6 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/c1kdnq0pxqsagk2m5rok 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zn7twrpzxsbg9bdr62cu 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zis6gshldc6861okqml3 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rejdls6mhkdbmxf7i2gt 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kkc5yjuj6o994sdfhvxx 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/utsksmhinro6q11mpkk4 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wlg6tgyxnjp5s6w4wgmn 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/yggov0gaxmy85t6a3o5a 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/hnjfopmtcdhb7nuperoz 1530w"><figcaption>图表大部分是从<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>中复制的，其中红色部分是新的。如果“我要站起来”的想法突然出现在我的脑海中，效价就扮演着“规范”控制信号（向上的黑色箭头）的角色，它决定了这个想法是留在我的脑海中并被执行，还是被扔掉并被抛弃。取而代之的是一个不同的想法，比如“……或者实际上我可能会留在床上”。与此同时，绿色箭头发挥了思想生成器用来构建预测/生成世界模型的输入数据的“积极”作用。</figcaption></figure><p>这是一个简单的例子，但却具有普遍性。所以：</p><ul><li>假设我有一个<strong>与执行计划或采取行动</strong>的想法相对应的想法。如果这个想法的效价是积极的，那么我就有责任执行它。如果这个想法的效价是负面的，那么我就不太可能这样做。因此，在这种情况下，<strong>思想的效价对应于我的大脑对该计划或行动的适当性做出快速的规范性评估</strong>。</li><li>假设我有一个<strong>与未来可能的情况</strong>相对应的想法，例如我想象某个候选人赢得选举，或者我想象自己晚餐吃三明治。如果这个想法的效价是积极的，那么我会倾向于制定并执行实现那个未来的计划。如果这种想法的效价是负面的，那么我会倾向于制定并执行<i>阻止</i>未来发生的计划。因此，在这种情况下，<strong>思想的效价对应于我的大脑对这种可能的未来的好坏做出快速的规范性评估</strong>。</li></ul><h2> 2.3.1 更多关于希望或不希望未来情况发生的信息</h2><p>关于“未来可能出现的情况”的第二个要点掩盖了一些值得详细阐述的细节。</p><p><strong>正价案例的详细说明：</strong>考虑这样的情况：我想象自己晚餐吃三明治，这个想法就是正价。今晚我怎么可能（可能）真正吃三明治？两步：</p><ul><li><i>第一步：</i>我可能会想到一个涉及具体行动方案的想法，预计会导致我吃三明治，也许“我要走到索尔那里吃三明治”。这是一个与以前不同的想法，更加具体和可操作，但这种想法<i>也</i>可能是正价的，因为它涉及/与“我晚餐吃三明治的想法”重叠。 （参见下文第 2.4.1 节。）</li><li><i>第 2 步：</i>好的，现在我脑子里有一个具体的行动方案想法（“我要步行到 Saul&#39;s 并在那里吃三明治”），并且它具有正价。这意味着我可能会真正执行该行动方案——我可能会收缩肌肉，站起来，然后开始走向索尔家。</li></ul><p><strong>对负价情况的阐述：</strong>接下来，考虑这样的情况：我想象一些可能的未来情况，并且该想法是<i>负</i>价。</p><p>我在上面写道：“我会倾向于制定并执行计划，以<i>防止</i>这种未来的发生。”但精明的读者可能会想：等一下。我真的会吗？我更有可能<i>完全不再考虑这种可能性</i>吗？这不是价信号的作用吗？</p><p>这是一个很好的问题！事实上，我认为这种反对意见有很多道理，这让我们陷入了这样一种普遍趋势，即在思考为什么自己充满希望的计划可能会失败时投资不足，以及“ <a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>糟糕的领域</u></a>”等等。下一篇文章将详细介绍该主题，尤其是第 3.3.3 节。</p><p>作为可能发生的情况的一个例子，请注意，“避免未来可能发生的<i>糟糕</i>情况 X”有时可以在心理上重新构建为“寻求可能的<i>良好</i>未来情况，而不是 X”——例如，如果您因以下想法而<i>失去</i>动力如果你的敌人赢得了战争，那么你可能同时被敌人被击败的想法所<i>激励</i>。所以也许你会切换到后一种心理框架，让你可以集思广益，而不会立即被负价推走。</p><h2> 2.3.2 更多关于“快速”评估与深思熟虑的评估</h2><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的 §1.5.1</a>中所讨论的，单个计划、行动或未来可能的情况完全有可能具有不同的价，具体取决于我的想法（我关注哪些方面等）。回想一下我之前的例子： <i>“我会去健身房，从而实现我的新年决心”</i>与<i>“我会去又吵又冷又臭的健身房”</i> 。</p><p>所以当我说“快速规范评估”时，“快速”正在发挥重要作用。这与我们在积极领域进行“快速”评估的方式非常相似：</p><ul><li><i>正域（不直接涉及价）：</i>如果我想象一种可能性，我会立即直觉地“突然”感觉到这种可能性是否可能/合理。但我还是可以多想一想，改变主意。</li><li><i>规范域（直接涉及价）：</i>如果我想象一种可能性，我会立即直观地“突然”感觉到这种可能性是好还是坏。但我还是可以多想一想，改变主意。</li></ul><p>在这两种情况下，我都声称“多考虑一下”过程与第一次快速评估并<i>没有</i>本质上的不同。只是想更多的想法，而这些后续想法都伴随着他们自己对合理性和可取性的快速评估。</p><h1> 2.4 概念的效价</h1><p>现在让我们考虑一个不同的情况：假设我想到一个<strong>概念，例如哈马斯、资本主义或超级英雄电影</strong>。这是我的大脑可以思考的事情，所以就像任何想法一样，我的大脑会给它分配一个价。</p><p>如果价是正的，也许我会心想“哈马斯是好的”；如果是否定的，“哈马斯是坏的”。</p><p>这个主题是<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>Scott Alexander 2014 年经典博客文章“种族紧张和毫无意义的争论”</u></a>的主题。但在我们开始之前……</p><h2> 2.4.1 旁注：价作为组合思想的（大致）线性函数</h2><p>作为一个简单且不太不切实际的模型，我建议我们应该将“思想”视为<i>组合的</i>（即基本上由许多相互关联的小片段组成），并且价值函数对这些思想片段是<i>线性相加的</i>。因此，如果一个想法需要想象一只泰迪熊在你的床上，那么这个想法的效价将是你的大脑对“这只特定的泰迪熊”的效价和你的大脑对“一般的泰迪熊”的效价的某种加权平均值，并且你的大脑对“我床上的东西”的效价，以及你的大脑对“模糊的东西”的效价等，权重/细节取决于你如何思考它（例如你正在关注哪些方面，什么）您在脑海中调用的类别/类比等）。</p><p>我为什么要提出这个？好吧，我想谈谈我的大脑赋予概念的效价。例如，“月亮”是你心理世界中的一个概念。它通常是一个更大的想法的<i>一部分</i>，但它本身很少是一个完整的想法。经验丰富的冥想者也许可以将他们的注意力集中在月亮的想法上，但这不是典型的情况！</p><p>在线性模型中，这很好——“月亮”仍然有价。如果该价态非常正，那么与月亮密切相关的想法，例如“我现在正在看月亮”和“有一天我会访问月球”，大概也会以正价结束。</p><h3> 2.4.1.1 线性的（明显）反例</h3><p>精明的读者现在正在向我尖叫。这是一个为亲以色列读者量身定制的（明显的）反例（其他人可以将“以色列”一词替换为“哈马斯”，或选择任何其他例子）：</p><p>为了思考“我将与哈马斯作战”的想法，你的大脑需要以某种方式激活“哈马斯”的概念；但你<i>越不</i>喜欢哈马斯，你<i>就越</i>喜欢对抗哈马斯的想法。这似乎与线性模型明显矛盾。</p><p>在这种情况下，我想说（对于那些亲以色列的读者）“<i>哈马斯作为我义愤填膺的对象</i>”是一个高度激励/正价的概念——即使普通的“哈马斯”概念是消极的/消极的——价。</p><p>或者，您可以通过使用从句和类似的语法结构轻松生成任意多个（明显的）线性反例：如果您被 X 的想法[激励/消极]，那么您将被这个想法[消极/激励] “X即将结束”、“X是假的”、“我反对X”、“我讨厌的外群体真的很喜欢X”，等等。同样，你的大脑在思考这个想法的过程中可能会激活概念 X，但相对于线性模型，对价的贡献是错误的。</p><p>所有这些案例到底发生了什么？长话短说，我<i>确实</i>坚持我的观点，即价是“概念”的（大致）线性函数。但我也认为“概念”可能比我想象的要复杂和微妙得多。</p><p>这就是我要说的关于这个话题的全部内容。我真的不知道这些血淋淋的细节，如果我知道，我绝对不会发布它们，因为我认为这些细节对于构建通用人工智能（AGI）非常有帮助，但对于确保此类 AGI 的安全只有微乎其微的帮助和有益的。有关“信息危害”的更多讨论请参见<a href="https://www.lesswrong.com/posts/MCWGCyz2mjtRoWiyP/endgame-safety-for-agi"><u>此处</u></a>。</p><h2> 2.4.2 概念的效价不能被解释为有意义的规范性评估</h2><p>我在这里引用<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>斯科特·亚历山大的</u></a>话：</p><blockquote><p>当一切都按照哲学教科书上应该的方式进行时，争论应该以以下几种方式之一进行：</p><p> 1. 经验事实问题，例如“地球正在变暖吗？”或“外星人建造了金字塔吗？”。你可以通过提供事实证据来辩论这些问题，例如“全球气象站测量结果的平均值显示 2014 年是有记录以来最热的一年”或“吉萨的一块砖块底部写着‘Tau Ceti V 制造’”。然后人们试图反驳这些事实或提出自己的事实。</p><p> 2.道德问题，例如“堕胎是错误的吗？”或“您是否应该避免下载未付费的音乐？”只有当你们已经就道德框架达成一致时，比如特定版本的自然法或后果论，你才能<i>很好地</i>辩论这些问题。但你可以通过与商定的道德问题的例子进行比较并试图保持一致性来对它们<i>进行</i>辩论。例如，“你不会杀死一天大的婴儿，那么九个月大的胎儿有什么不同呢？”或“你不会下载<i>汽车</i>。”</p><p>如果你很幸运，你的哲学教科书也会承认以下内容的存在：</p><p> 3.政策问题，例如“我们应该提高最低工资”或“我们应该轰炸外国人”。这些是相互竞争的事实主张和相互竞争的价值观的组合。例如，最低工资可能取决于“提高最低工资会增加失业率”或“现在靠最低工资生活很困难，许多贫困家庭买不起食物”等事实主张。但它也可能取决于诸如“企业有义务为工人支付生活工资”或“保护最贫困人口比经济强劲更重要”等价值主张。轰炸外国主义者可能依赖于诸如“外国主义者窝藏恐怖分子”之类的事实主张，以及诸如“我们人民的安全值得冒附带损害的风险”之类的价值主张。如果您能够解决所有这些事实和价值主张，您应该能够就保单问题达成一致。 ……</p><p>问：你支持以色列还是支持巴勒斯坦？花点时间，认真思考一下。</p><p>有些人可能会回答亲以色列。其他人可能会回答支持巴勒斯坦。其他人可能会说他们是中立的，因为这是一个复杂的问题，双方都有好的观点。</p><p>大概很少人回答：<i>嗯？什么？</i></p><p>这个问题不属于哲学 101 三种论证形式中的任何一种。这不是事实问题。这不是特定道德真理的问题。这甚至不是政策问题。还有密切相关的政策，例如巴勒斯坦是否应该获得独立。但如果我支持一个非常具体的两国解决方案，其中边界是在平行的东西上划定的，这会让我支持以色列还是支持巴勒斯坦？所考虑的解决方案到底在哪条边界线上从亲以色列转向亲巴勒斯坦？你认为那些高喊“声援巴勒斯坦”、挥舞标语的人群能回答这个问题吗？ ……</p></blockquote><p> （如果我们从政治转向日常生活，“政策”类别概括为“具体计划和行动”。例如，关于“我们应该去酒吧”的分歧可能涉及“酒吧会拥挤吗？”等事实主张。 ”，以及诸如“跳过合唱团练习是不是很粗鲁？”之类的价值主张。）</p><p>好吧，如果亲以色列或反以色列的态度并不代表有意义的规范性评估，那么它代表什么<i>呢</i>？</p><h2> 2.4.3 概念的效价实际上是一种习得的规范启发式</h2><p>“启发式”是一种用于近似某事物的快速算法。回想一下上面第 2.3 节中肯定性与规范性的区别。许多启发式，例如<a href="https://en.wikipedia.org/wiki/Representativeness_heuristic"><u>代表性启发式</u></a>，可以被称为“积极启发式”，因为它们是用于找出“积极”事物（例如事实预测）的启发式。其他启发法可能被称为“规范启发法”，因为它们是生成（有意义的）规范评估的启发法。</p><p>我认为，如果你的大脑为“以色列”这样的概念分配了一个价，那么它就对应于一种规范启发式——更具体地说，是一种<i>习得的</i>规范启发式，即你的大脑从一生的经历中获得的规范启发式。</p><p>继续以色列的例子，将概念价转换为其相应的规范启发式的方法是：</p><blockquote><p> “‘以色列’这个概念在我心中具有正价”对应于以下启发式，让您的大脑生成快速规范评估：</p><ul><li><i>可能的行动或计划</i>涉及以色列/与以色列的模式匹配越多，执行该行动或计划就越有可能是我想做的事情；</li><li><i>未来世界的可能状态</i>越多地涉及以色列/与以色列的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</li></ul><p> （对于负价则相反）。</p></blockquote><p> （正如第 2.4.1.1 节中所讨论的，“涉及以色列/与以色列进行模式匹配”短语掩盖了<i>一堆</i>复杂性！）</p><p>因此，如上所述，这些化合价本身并不是有意义的规范性评估，这仍然是事实。但它们<i>只是</i>有意义的规范评估的<i>上游</i>。</p><h2> 2.4.4 理解基于共鸣的“无意义的争论”</h2><p>鉴于上述情况，我们现在可以进入我上面摘录的<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>“种族紧张和毫无意义的争论”博客文章的</u></a>下一部分：</p><blockquote><p>这是《种族紧张：两个玩家的游戏》。</p><p>选择一个模糊的概念。 “以色列”目前表现不错。</p><p>玩家 1 试图将“以色列”这个概念与尽可能多的善业联系起来。概念通过做良好的道德事情、通过与好人交往、通过与心爱的群体联系、通过<a href="https://slatestarcodex.com/2013/05/18/against-bravery-debates/"><u>在勇敢的辩论中</u></a>受到压迫的弱者来获得良好的业力。</p><p> “以色列是中东最自由、最民主的国家。它是美国最强大的盟友之一，与我们有着共同的犹太基督教价值观。</p><p>玩家 2 试图将“以色列”这个概念与尽可能多的恶业联系起来。概念通过犯下暴行、与坏人有联系、与可憎的外群体有联系、在勇敢辩论中成为压迫性的大人物而获得恶业。此外，她显然需要通过反驳她的所有论点来抵消玩家 1 的行为。</p><p> “以色列可能为其最享有特权的公民提供一定程度的自由，但被占领土上数百万没有发言权的人呢？以色列参与了各种暴行，并经常杀害无辜抗议者。他们本质上是一个新殖民主义国家，并与南非等其他新殖民主义国家结盟。”</p><p>赢得这场游戏的奖励是能够赢得其他三种类型的争论（即关于事实、道德和政策的争论）。</p></blockquote><p>我认为，鉴于上一节“习得的规范启发法”，此类论证更有意义。</p><p>具体来说，玩家 1 和玩家 2 正在争论以下规范启发式的适当性：</p><p><i><u>争议中拟议的启发式：</u> “可能的行动或计划涉及/与“以色列”的模式匹配越多，执行该行动或计划就越有可能是我想做的事情。同样，未来可能的世界状态越多地涉及/与“以色列”的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</i></p><p> （再次参见第 2.4.1.1 节中的注意事项）</p><p>玩家 1 促使听众思考上述启发式方法非常有效的许多想法，即它给出的答案与听众大脑中已有的所有其他规范启发式一致。这会刺激听众的大脑比以前更加依赖这种启发式方法。</p><p>相反，玩家 2 会促使听众思考上述启发式方法效果极差的想法，即它给出的答案与听众大脑中已有的所有其他规范启发式完全相反。这会刺激听众的大脑比以前更少地依赖启发式，甚至开始依赖完全相反的启发式（也称为负价）。</p><p> （此过程的更多机制细节参见下面的 §2.5。）</p><h2> 2.4.5 我们是否应该“反”一般的规范启发法？</h2><p>我经常很恼火地看到人们争论是否支持资本主义与反资本主义、支持宗教与反宗教等等，好像这些是有意义的争论，但其实并非如此。</p><p> （有趣的是，我有一个朋友，我们两个人倾向于在经济政策的<i>具体</i>问题上达成一致。然而，我们在氛围上存在强烈分歧——“资本主义”这个概念在他们的大脑中具有强烈的负面氛围，而在我的大脑中则有积极的氛围）基本上，我们的共鸣分歧大部分被关于什么是或不是“资本主义”的平等和相反的分歧所抵消！）</p><p>这类毫无意义的争论有时是明确的，但更多时候它们隐藏在表面之下，争论在物体层面和共鸣层面同时发生。</p><p>例如，如果有人认为他们只是“得了一分”，那么这个事实经常会出现在他们的言语中，即使他们没有站出来明确宣布“将死，无神论者！”在这些情况下，值得询问他们认为自己得分<i>的具体是什么</i>，因为也许那件事一开始就不是分歧的主题。</p><p> （不用说，这是双重的：照镜子，问<i>您</i>要争论的是什么非常具体的事情！）</p><p> （首先，“得分点”的态度是有问题的 - 看<a href="https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556"><i><u>童子军的心态</u></i></a>- 但要争论一个非常具体的混凝土相互理解的事物是最小的版本。）</p><p>当我们的大脑将价值分配给大而模糊的事物时，规范性启发式方法尤其有问题。例如， <i>“涉及 /模式匹配&#39;宗教&#39;的事物”</i>是一个非常异构的收藏，从古老的神道仪式到以教皇为主题的旅游饰品，我曾经在梵蒂冈礼品店买了自己。反思后，这个系列中的所有物品都是好事或全部不好的机会？相当低。 （参见<a href="https://www.lesswrong.com/tag/bucket-errors"><u>“遗愿错误”</u></a> 。）因此，如果我们对“宗教”（无论是正面还是负面）的效价，我们会遇到两个问题之一：</p><ul><li>我们将对特定宗教相关的事物做出错误的规范性评估（从某种意义上说，如果我们更仔细地考虑事情，我们将改变主意）；</li><li> …否则我们将结束概念“宗教”以与我们认可的规范性评估一致。 （例如，有人可能会说“宗教是[好/坏]，但是[不好/好]的事情不是<i>真正的</i>宗教。”）</li></ul><p>我认为后者对清晰的思考可能非常有害。最好有<i>一些</i>明确涉及规范性评估的概念（“优选”，“有问题”，“麻烦”，“繁荣”等等 - 在所有规范性评估之后，请参见<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_4_Terms_closely_related_to__valence_">上一篇文章中的第1.4节</a>，或<a href="https://arbital.com/p/value_laden/"><u>相关的Arbital Post</u></a> ）。是我们经常想谈论的事情。但是，我们<i>也</i>迫切需要描述世界如何独立于我们对世界的感觉独立运作的概念。我将在下一篇文章的§3.4中对此主题进行更多讨论。</p><p>说了这么多，我们总体上不能成为“反”规范性的启发式方法，因为如果我们将它们带走，那就什么都没有！毕竟，大脑需要<i>某种</i>方法来评估计划 /行动 /未来是好是坏，包括我们以前从未遇到过的计划。</p><p>因此，作为一个实际问题，我认为将价值分配给概念是可以的，即使将相当强的价值分配给相当广泛的概念也可以在某种程度上可以。我自己的大脑似乎至少将一些非零的价分配给至少一些相当广泛的概念，例如“共产主义”（嘘），“贫困”（boo），“ AI Alignment”（Yay），“冰淇淋”（Yay）和因此，我目前不认为我应该努力改变这一点。重要的是<i>要养成一个习惯，即提出后续问题，这些问题倾向于细节，并探究了对一般规则的例外</i>。</p><p>例如，问问自己：确切的建议是什么？它的意图和意外后果是什么？当我们说“共产主义”时，我们是在谈论“与斯大林中的共产主义”或“在邓小平中的共产主义”或“共产主义”或“共产主义”，例如在小型狩猎 - 采集者部落中”？当我们说“冰淇淋”时，我们是在谈论“像Häagen-dazs中的冰淇淋”或“像海绵宝宝中的冰淇淋中的冰淇淋”？当我们说“ AI对齐”时，我们是在谈论“我定义的AI对齐”或“ Sam Altman定义的AI对齐”？ ETC。</p><p>我声称，如果您脑海中的<i>细节</i>显着，那么相应的规范启发式方法更有可能进行良好的评估，即经受审查和事后的审查。</p><p> （顺便说一句，如果您养成了这种习惯，它可能会<i>随着时间的流逝</i>而导致为模糊的广泛概念分配更多中立的价，并为更具体的特征分配更强的价值。这可能是一件好事。）</p><h1> 2.5价如何设置和调整？ </h1><figure class="image image_resized" style="width:67.18%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/jszcdax7vbhbxebhlau4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rdxaoi3kfhm2tiase154 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/o8znjxkggojkxgvvtjlf 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvibz9spxvn4gz3maoqb 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/eld6v5v4fypxs0bqcaj2 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/lqz98ny7ffp9uhqwgfbt 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/frghxaaqbnc8xxwt7joc 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/s1uubt4yekrh91tiaxro 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/pkxccpamapexa8elztd7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/w6cksg1dlrxa9lcemytv 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/stsl40xxhye7ovycuvls 955w"><figcaption>我声称价最终来自先天驱动器，但随后通过时间差异（TD）学习之类的东西在不同的概念（思想零件）之间旅行。作为第一个的一个例子，如果我烧手了触摸炉子，那么先天的驱动器就会介入，并说发生了一些不好的事情，然后触摸炉子的想法将因此获得负面价。作为第二个的例子，请考虑<a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>“ ugh fields”</u></a> - 如果我经常对自己想“我要清理我的电子邮件收件箱”，那总是立即产生高度避免的后续行动思想“……这意味着我需要为了处理NED的那个烦人的电子邮件”，即使我没有明确呼唤NED烦人的电子邮件，“我要清理我的电子邮件收件箱”<i>本身</i>也会变得有些厌恶（负价）。</figcaption></figure><p>如果您追溯到足够远的地方，我声称<strong>所有价最终都会直接或间接地从先天的驱动器中流动 -</strong>宣布痛苦不好的一般指定的电路，而饮食却是好的，而渴望成为社会地位的饮食（出现在第4个文章中），以及与同情，报仇和内gui相关的其他各种社会驱动器，可能还有数百种类似的事情。</p><p> （在强化学习术语中 - 请参阅上<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">一篇文章的第1.3节</a>- 您的“先天驱动器”更像是对奖励功能的各种术语。在神经解剖学术语中， <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>我声称</u></a>实施这些先天驱动器构成了关键责任下丘脑和脑干，尽管其他大脑区域也涉及。）</p><p>为了过分简化，我声称思想可以是正价或负面价的两个主要原因：</p><ul><li>思想可能是正价或负面价的<strong>第一个原因</strong>是，先天的驱动力正在介入并宣布该思想分别是好是坏。 （例如，如果我饿了，我开始吃美味的食物，那么当时我在想什么（可能是食物），这显然是一件好事。）</li><li>思想可能是正价或负面价的<strong>第二个原因</strong>是，我们的大脑具有一种<i>学习算法</i>来猜测哪种价值分配最适合于思想，这是基于实际价值分配的过去历史。当天生的驱动器并不是<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_2_Toy_model_of_a__long_term_predictor__circuit"><u>“超越”猜测</u></a>时，猜测就会流入一个自我实现的预言中。 （学习算法与<a href="https://en.wikipedia.org/wiki/Temporal_difference_learning"><u>时间差异（TD）学习</u></a>有一定的关系；我抛弃了所有细节，尽管请参阅<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_3_1_Switch__i_e___value___expected_next_reward__vs_summation__i_e___value___expected_sum_of_future_rewards__"><u>此处的</u></a>更多信息。）</li></ul><p>撇开所有机械细节，该学习算法具有各种效果：例如：</p><ul><li>如果认为θ在您认为的所有之前都具有一定的价，那么学习算法将开始猜测θ（思想与θ足够相似）将来会有相同的价。</li><li>如果X（例如打开Twitter）经常立即导致Y（例如看到“喜欢”通知），即作为现实世界中的时间序列，那么学习算法将开始猜测X的适当价更接近于任何内容价恰好是y。</li><li>同样，如果认为X经常立即导致您的大脑中的思想，则同上上一个子弹点。 （换句话说，TD学习的“时间”部分可以完全放在您大脑的思维列车中，而与外界的时间序列无关。）</li></ul><p>要解决可能的误解：先天驱动器与目标或终端目标不同。您的目标，例如“摆脱债务”或“减少苦难”，是由您的世界模型中的概念（又称<a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"><u>“思想生成器”</u></a> ）建立的，即您一生中学到的概念。天生的驱动器不是 - 目的是，它们是在某些情况下吐出某些信号的基因组建造的小机器。有时我会谈论“避免痛苦的先天动力”或类似的事情，但这确实是更奇特的“电路”，它检测到与我们通常所说的“疼痛”相关的外围信号，然后吐出负面价（在其他信号中），因此随着时间的推移，该电路的典型效果是让我们避免疼痛。”</p><h2> 2.5.1特殊情况：一起进行的事情倾向于获得类似的价</h2><p>在<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的第1.5.1节</a>中，我提出了一个关于去健身房“混合感”的例子。如果我关注它的一个方面，它具有正价（“我要去健身房，因此要遵循我的新年解决方案”）。如果我注意它的另一个方面，它具有负面价（“我要去健身房，这真的很大声又冷”）。</p><p>可能是，在这两个框架之间，在一两个框架内立即连续地进行了很多心理翻转。换句话说，当我考虑积极的方面时，也许会慢跑我的记忆以调用负面方面，反之亦然。如果是这样，我认为TD学习将倾向于更新我的价功能，从而使积极价值更为负面，反之亦然（根据上面的第三个项目符号点）。具体来说，也许遵循我的新年决议的想法会开始降低动力，更厌恶，相反，也许会考虑体育馆的响度和寒冷开始会变得<i>有些</i>厌恶。 <span class="footnote-reference" role="doc-noteref" id="fnref6q9syee1mov"><sup><a href="#fn6q9syee1mov">[2]</a></sup></span></p><h1> 2.6元认知和自我反思思想的价：自我合成的vs-dystonic思想，自称为“价值”，等等。 </h1><figure class="image image_resized" style="width:24.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tz2aw0v6l0ei9pwnwnz4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/mofolnogceg3ar9d7u8p 115w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xnxqdmknjlugrtvr7wge 195w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/gk8llgybcvjpjzyng91k 275w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/ryolrrtufvajtuceshqi 355w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xdon4zacamsmpzahxjxi 435w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/qxwq1byfvsee3tji0joz 515w"></figure><p>假设我沉迷于香烟，但想戒烟。鉴于我沉迷于香烟，有时“现在吸烟的想法”将是非常积极的价值 - 换句话说，“有时我会感到吸烟的冲动”  - 因此，我会吸烟。</p><p>但这很烦人！我想辞职！由于我不是一个完全的白痴，所以我自然会注意到，“渴望我对烟”是一个障碍，这使我无法跟随我的动机，这是一个障碍。这使我不喜欢“急于摩托车”。</p><p>现在，我们进入了自我反思偏好领域！</p><p>因此，“吸烟有时会让我感到积极的想法”，这是我<i>自己的</i>脑海评估，而价值则是负面的。这将是<i><strong>自我差异</strong></i><strong>评估</strong>的一个例子。我想抽烟，但我<i>不想</i>吸烟。</p><p>现在，再次假设一个想法突然出现在我的脑海中：“也许我现在就吸烟了”。如果这种想法需要注意预期的感官感觉等，那么这种想法的价将是积极的，所以我会抽烟。如果想法大多需要忽略这些思想，而要注意情况的更自我反射的方面（也许是从外面看我自己，也许就是这样），那么这种想法的价值将是负面的，所以我不会抽烟。实际上，可能两种类型的想法都会在不同的时间突然出现，而价值的相对优势将有助于确定事物如何摆脱。</p><p>自我dystonic的对立面是<strong>自我的同伴</strong>。例如，“我自己倾向于诚实和尽责”的想法是自我合成的。因此，如果计划模式与自己视为诚实和认真的态度，那么该计划会赢得价值提高，因此我更有可能这样做。但仅<i>在边缘</i>。该计划也有其他方面，这也有助于价值，实际上这些其他方面通常更为突出。</p><p>这里的一个重要观察结果是，<i><strong>自称的</strong></i><strong>目标和价值观，远远超过行动，往往是由事物是自我合成还是 - 典型的。</strong>考虑：如果我大声说出（或对我自己）（例如“我要戒烟”或“我关心我的家人”），那么我脑海中的实际直接想法主要是“我要执行此操作特定的言语行为”。正是<i>这种</i>想法的价决定了我们是否说这些话。这种思想的自我反思方面非常重要，因为说话需要思考听众如何收到您的话。相比之下，在某些不确定的未来中，该宣言的<i>内容</i>（实际上是戒烟或实际关心我的家人）既不显着，也不是直接的（请参阅<a href="https://en.wikipedia.org/wiki/Time_preference"><u>时间限制</u></a>）。因此，《言语法》的净价可能包含戒烟的自我反射方面的<i>巨大</i>价值，以及从戒烟或关心我的家人的更直接感官和其他后果所产生的<i>小</i>价值贡献。<i>即使</i>我们打算遵循我们所说的话，我们也是100％真诚的。 （另请<a href="https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors"><u>参阅批准加强低劳动行为</u></a>，博客文章的观点与本段相似。） </p><figure class="image image_resized" style="width:73.71%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rcvfrjif77p3aam2aw6c" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kqpsl8ljw1xjkggyp3tu 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/myrfhtlsewfnonpmlljq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/nnta32ypxvzvhic2uy14 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wy70orgjcuigufl8noxn 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xenavi7epj1nps9hnji5 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zjwci8fwzhzmtkphmrz4 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xkztuwtxn2lashfztqum 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/n87ivhqtqmwtvlinnjed 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/l0tuyylx95lsytazgsrl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/p6x3gfovyynzxfhia9vj 866w"></figure><h2> 2.6.1什么是“值”？</h2><p>我提出这个话题是因为它经常出现在我的职业中（AI对齐）。但是我不确定它是否有深刻的答案；它使我感到惊讶，主要只是关于定义的论点。</p><p>这是我喜欢的“价值”的定义，因为它似乎与人们在我的日常生活中使用该词的使用一致：<i>如果我想知道某人的“价值观”，我会问他们，并写下他们的话。然后，我会宣布，我写下的任何东西都是“他们的价值观”，从定义上讲或无数，例如，例如，他们故意拖着我。</i></p><p>根据这个定义，“价值观”可能包括听起来非常好，社交批准和自我同胞的事物，例如“照顾我的家人和朋友”，“使世界变得更好”，等等。</p><p>另外，根据这个定义，“价值”可能对某人的行为产生宝贵的影响。在这个（极为常见的）情况下，我想说：“我想这个人的<i>欲望</i>与他的<i>价值观</i>不同。哦，那里不足为奇。”</p><p>的确，我认为“价值观”包括“成为好朋友”的人实际上是一个坏朋友，这是完全正常的。那么，这个“价值”是否有任何影响？是的！！我希望，在这种情况下，这个人<i>要么</i>对他们是个坏朋友感到难过，<i>要么</i>否认他们是坏朋友，<i>或者</i>根本没有考虑这个问题，<i>或者</i>想出其他一些问题借口他们的行为。如果这些事情都没有发生，那么（只有那时）我会说“成为好朋友”实际上<i>不是</i>他们的“价值观”之一，如果他们否则陈述，那么他们会撒谎或困惑。</p><h1> 2.7道德推理</h1><h2>2.7.1描述性帐户</h2><p>如上所述，我认为正相关的区别与大脑大规模解剖结构和算法的主要组成部分具有非常干净的对应关系。</p><p>但是，<i>在规范领域内</i>，我认为<i>“ X是道德 /道德 /贤惠 /等的区别”</i>和<i>“我想做X”</i>并不是太基本了 - 它更像是模糊类别之间的边界。例如，当人们自我合成和社会批准时，人们倾向于将道德 /道德 /良性称为事物。但是该规则也有例外。例如，黑手党可能以他的“不道德行为”感到自豪。即使在那种情况下，黑手党也可能仍在弄清楚什么是和没有自我的合成和社会批准的，他在自己的犯罪社会基础上，他的思想甚至可能在结构上类似于“正常”的道德推理。但是他不会使用“道德”一词。</p><p>无论如何，假设您是一个普通的守法的人，他真诚地想在道德上行事。因此，您坐在扶手椅上，试图弄清楚什么是道德上的事情。接下来发生什么？很多东西：</p><ul><li>也许您会考虑对象级别的事情，例如您想象一个孩子受苦。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您会考虑元认知事物，例如“我会仔细思考，而不是急于判断”，或者相反，“我应该更信任自己的肠道”。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您想到的是自我反思的事情，例如<a href="https://joecarlsmith.com/2023/02/17/seeing-more-whole"><u>“我在世界上是什么力量？”</u></a>或“我的朋友现在能读我的想法，该如何看待我？”。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您注意到其中一些偏好与其他偏好不一致。然后，您会想到更多的想法，在这些想法中，这些不同的偏好可能会淘汰它。</li><li>在整个过程中，您的大脑学习算法都在不断更新您的信念和欲望。</li></ul><p>在本系列的背景下，我要提出的一点是，<strong>每个步骤都取决于价的基础。</strong>例如，如果您的大脑对痛苦的孩子的想法没有任何特殊的价值，那么您就不会感到有动力阻止孩子痛苦。</p><p>为了使这个更直观：我想告诉你：“穿均匀的皮带循环的裤子很重要”，然后你说“为什么？”，然后我说“是的，如果你不这样做，然后，您的裤子会有<i>奇数</i>的皮带循环！那会很糟糕！！”，然后你再说：“嗯？为什么那不好？”。这场谈话毫无进展。</p><p>相比之下，如果我说“如果您遵循X原则X，那么被压迫的群众就会得到正义”，<i>现在</i>我可以激发您的兴趣，因为“被压迫的群众会得到正义的正义”已经在您的大脑中巩固了一个吸引人的兴趣（积极 -价）概念。</p><p>如上所述（第2.5节），我要提出的另一个重要的一点是，这<strong>是从先天的驱动器中（通常是间接）（尽管通常是间接）的</strong>。就像第2.5节中一样，在您一生中的每一个醒来的第二个醒来，价值都基于您的观察和思想列车从概念流向其他概念。但是，如果您追溯到足够远的地方，则价值最终必须从您的天生驱动器中获得。在道德推理的情况下 - 上面的“被压迫群众将得到正义”的例子 - 我认为相关的先天驱动器几乎可以肯定是天生的<i>社会</i>驱动器，即与同情心，复仇，社会地位有关的先天驱动器（在Post 4中出现）， 等等。</p><h2> 2.7.2对“道德的真实本质”的影响（如果有）</h2><p>我离元伦理学专家<i>很</i>远，但是以上帐户似乎有些事情。</p><ul><li>如果存在<i>完全独立</i>于人类先天驱动器和当前模因环境的特殊性的“真正的道德”，那么上述过程应该融合到它，或者确实有任何事情要做用它。如果这样做，我认为我们必须宣布这是一个快乐的巧合。 （有关这一点的更深入的讨论，请参见乔·卡尔史密斯（Joe Carlsmith）的文章<a href="https://joecarlsmith.com/2022/01/17/the-ignorance-of-normative-realism-bot"><u>“对规范现实主义机器人的无知”</u></a> ）。</li><li>实际上，没有任何明显的原因是上述过程应融合到<i>任何</i>定义明确的目的地。我<i>不</i>认为这类似于数学。在数学中，如果有一个假设的代理人具有“天生的性格”来“喜欢”某些数学公理及其逻辑后果，那么对于代理人将会或不会“喜欢”的内容，就有一个独特的答案，如果有足够的时间来反思 - 即，这些公理可以证明的数学语句。相比之下，如果人类从天生的驱动器开始，并经历了上述过程，那似乎更加混乱。例如，一旦您开始“喜欢”某种元认知模式，它就会改变审议未来步骤的地面规则。因此，在过程结束时很可能存在不确定的路径依赖性。否则，它可能会在各种不令人满意的状态周围无尽的周期之后结束。</li><li>即使上述过程<i>确实会</i>收敛到独特且定义明确的目的地，但对于每个人来说，它不一定是相同的目的地（并且忘记外星人或AIS）。实际上，我会走得更远，猜测这对不同的人来说可能是不同的，而不仅仅是在社会变态等异常情况下，而且在整个人群中广泛。 （例如，我认为 <a href="https://www.lesswrong.com/posts/BkkwXtaTf5LvbA6HB/moral-error-and-moral-disagreement"><u>这篇文章</u></a>的结尾太乐观了。）我认为的原因是：目的地<i>肯定</i>会取决于正在审议的人的天生驱动器，也<i>可能</i>取决于他们的人生历史，尤其是模因环境。让我们专注于第一个驱动器。我承认，几乎所有人类都具有<i>定性</i>的先天驱动器，它们以相同的方式建造。但是我认为它们<i>在定量上</i>并不相似。尤其是，各种不同的先天驱动器的相对优势似乎因人而异。例如，我认为有些人会感到状态驱动力（在第4个帖子中出现）非常敏锐，而另一些人则感觉更温和，并且与与同情心，复仇等的先天驱动器相提并论。这些相对优势很重要，因为道德充满了不同直觉相互贸易的情况。</li><li>比较“我希望被压迫的群众找到正义”与“我站着太久，我想坐下来”。<i>这两个“欲望”从根本上是由同一思维构成的。</i>他们俩都源于正价，这反过来又来自先天的驱动器（特别是在第一种情况下是社会驱动器，以及第二种情况下的稳态持持稳态驱动器）。因此，如果“真正的道德”或“真正的人类道德”或不存在的任何事物，那么这<i>并不</i>构成坐下而不是寻求正义的理由。您仍然必须做出决定。这就是我的意思<a href="https://www.lesswrong.com/posts/32ca3B7rJ93xo9tvb/thoughts-on-agi-consciousness-sentience#How_does_that_feed_into_morality_"><u>“虚无主义不是与决策相关的”</u></a> ，也是Yudkowsky的 <a href="https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality"><u>“您将在没有道德的情况下做什么？”</u></a> 。确实，您会注意到我首先谈论了审议过程，其次是哲学。审议过程是由您自己动机的引擎提供动力的，无论他们是什么，无论好坏，都不是由任何外部规范真理。</li></ul><h2> 2.7.3可能对AI对齐话语的可能影响</h2><p>作为一名AI一致性研究人员，就当地知识界的想法而言，我可能应该准确地阐明自己的意思。<i>本节是低信心和术语繁重的； AI对齐字段外的读者应该随时忽略。</i></p><ul><li>我担心<a href="https://www.lesswrong.com/tag/coherent-extrapolated-volition"><u>CEV</u></a>定义不明确。或更具体地说，您可以列出CEV的许多同样的<i>A-Priori</i>详细操作，并且它们会以我们发现非常不满意的方式给出重要的不同结果。</li><li>相关地，我担心“<a href="https://forum.effectivealtruism.org/topics/long-reflection"><u>长期反思</u></a>”不会解决我们希望它能解决的所有重要事情，或者以与长期反思治理 /话语规则的细节无关的方式解决它们，而没有明显的方法来决定众多合理的治理 /话语规则中哪种是“正确的”。</li><li>当人们做出隐式将“未来价值”视为明确定义的陈述时， <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>例如</u></a>“我将“强乌托邦”定义为：至少实现了未来潜在价值的95％”，我担心这些陈述的意义不如听起来。</li><li>我担心，几代人的人类价值变化在某种程度上更像是一个随机步行，而不是进步时间，而且他们只会感觉像<a href="https://willywizz.wordpress.com/2012/09/24/paint-the-target-around-the-arrow/"><u>进步</u></a>时间<a href="https://willywizz.wordpress.com/2012/09/24/paint-the-target-around-the-arrow/"><u>箭”</u></a> 。因此，当我们说“永恒的价值锁定是不好的 - 我们想为我们的后代提供道德增长的空间！”，我们还同时说出特定的话代理和探索，几乎没有痛苦和痛苦，以及……！”，然后我担心这两个陈述至少有些矛盾，也许有很大的矛盾。 （如果事实证明我们只需要选择这两个陈述中的一个，我就不知道我要投票给哪个陈述。）</li><li>以上都不是我们不应该试图解决AI对齐问题的论点，或者我们不应该关心未来的发展。与以前的小节一样，我们仍然可以而且必须做出决定。我渴望避免充满酷刑和奴隶制的未来宇宙（以一个特别的榜样）一如既往地强烈，如果您分享这种愿望，那么让我们一起努力使它实现。</li></ul><h1> 2.8 结论</h1><p>同样，我认为价是在大脑中建立所有规范性的物质。希望这篇文章能阐明这些难题的一些碎片。我愿意接受思想和讨论，在下一篇文章中，我们转向了较小但仍然存在实质性的影响（与规范性的）域名的信念，期望，概念等。</p><p><i>感谢TSVI Benson-Tilsen，Seth Herd，Aysja Johnson，Justis Mills，Charlie Steiner，Adele Lopez和Garrett Baker对早期草稿的批判性评论。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fntss4t9kg1lr"> <span class="footnote-back-link"><sup><strong><a href="#fnreftss4t9kg1lr">^</a></strong></sup></span><div class="footnote-content"><p> “ Bulverism”是一个术语，其意思是“假设某些信念是不正确的，然后就人们如何持有那些不正确的信念的人进行心理猜测”。但是，有关道德现实主义的<i>更多</i>信息，请参见下面的§2.7。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6q9syee1mov"> <span class="footnote-back-link"><sup><strong><a href="#fnref6q9syee1mov">^</a></strong></sup></span><div class="footnote-content"><p>在这个示例中 - “也许遵循我的新年决议的想法会开始变得有些动机，更厌恶，相反，也许会思考健身房的响亮和寒冷，会开始感到<i>有些</i>厌恶。” - 我实际上认为这种转变可以通过两种方式发生，并且大脑同时进行了这两个过程。文本中描述的第一个涉及通过TD学习对不同思想的价变化。我不会讨论的第二个是因为它是较少的主题，涉及世界模型（又称“思想生成器”）及其概念网络及其关联 /内涵的变化。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sqgrtcwueovvwxpdq/valence-series-2-valence-and-normativity<guid ispermalink="false"> SQGRTCWUEOVVWXPDQ</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 07 Dec 2023 16:43:49 GMT</pubDate> </item><item><title><![CDATA[AISN #27: Defensive Accelerationism, A Retrospective On The OpenAI Board Saga, And A New AI Bill From Senators Thune And Klobuchar]]></title><description><![CDATA[Published on December 7, 2023 3:59 PM GMT<br/><br/><p>欢迎通过AI安全<a href="https://www.safe.ai/">中心来到AI安全</a>通讯。我们讨论了AI和AI安全方面的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此处</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify上免费收听AI安全通讯。</a></p><hr><h2>防御加速度</h2><p>以太坊的创建者Vitalik Buterin最近<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">写了一篇关于AI和其他技术的风险和机会的文章</a>。他回应了马克·安德森（Marc Andreessen）关于技术乐观主义的宣言和有效的加速主义（E/ACC）运动的增长，并提供了更加细微的观点。</p><p>这篇文章认为，技术通常对人类非常有用，但是AI可能是该规则的例外。布特林认为，与其赋予政府对AI的控制权，以保护我们，不如说我们应该建立防御技术，以防止在分散的社会中为灾难性风险提供安全性。网络安全，生物安全性，弹性的物理基础设施和强大的信息生态系统是Buterin认为我们应该建立的一些技术，以保护自己免受AI风险。</p><p><strong>技术有风险，但法规不是灵丹妙药。</strong>更长的寿命，较低的贫困率以及扩大的对教育和信息的机会扩大，这是Buterin对技术的众多成功。但是大多数人会认识到技术也会造成危害，例如全球变暖。 Buterin特别<a href="https://twitter.com/VitalikButerin/status/1729251822391447904">说</a>，与大多数技术不同，AI对人类构成了生存威胁。</p><p>为了应对这种风险，一些人主张<a href="https://arxiv.org/abs/2310.09217">强大的政府控制</a>人工智能发展。 Buterin对此解决方案感到不舒服，他希望许多其他解决方案也会如此。许多历史最严重的灾难是由斯大林和毛等强大的政治人物故意进行的。 AI可以帮助残酷的制度监视并控制大量人口，而Buterin通过将AI开发从私人实验室推向公共实验室来谨慎行动。</p><p>在不受约束的技术发展的极端和绝对的政府控制之间，布特林倡导了一条新的途径。他称他的哲学D/ACC，“ D”代表国防，民主，权力下放或<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670">差异技术发展</a>。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc365b23e-2340-47ba-8f6b-04cf5e6efa97_500x536.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/zm2qw8smimbll3172z6y 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ci9uhres7aqircceej9n 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/gwdpnppfzrthwzyxb5kq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg 1456w"></a></p><p><strong>去中心化世界的防御技术。</strong>布特林主张将保护社会免受灾难性风险的技术加速。具体来说，他强调：</p><ol><li><strong>生物安全</strong>。茂密的城市，频繁的航空旅行和现代生物技术都增加了大流行病的风险，但是我们可以通过改善空气质量，加快疫苗和治疗疗法的发展以及监测新兴病原体来改善生物安全。</li><li><strong>网络安全</strong>。可以在网络攻击中使用可以使用代码的AI，但是在被剥削之前，防御者也可以使用它们来查找和修复安全缺陷。布特林（Buterin）在区块链上的工作旨在实现某些数字系统可以证明是安全的未来。</li><li><strong>弹性物理基础设施</strong>。核灾难中的大多数预期死亡将不是爆炸本身，而是来自饮食，能源和其他必需品的供应链。埃隆·马斯克（Elon Musk）渴望通过使我们减少对化石燃料的依赖，通过卫星提供互联网连接，并理想地使人类成为一个可以超过地球上的灾难的多人物种，从而改善了人类的身体基础设施。</li><li><strong>强大的信息环境</strong>。为了帮助人们在AI说服时期找到真相，Buterin指出了预测市场，并建立了共识的算法，例如<a href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">社区笔记</a>。</li></ol><p>科学家兼首席执行官可能会发现自己受到布特林（Buterin）建立技术的目标而不是减慢它的启发。然而，对于那些关注AI和其他灾难性风险的人来说，Buterin对最有可能确保我们文明安全的技术提供了深思熟虑的看法。对于那些感兴趣的人，<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">本文</a>中还有更多想法。</p><h2>在Openai董事会传奇中回顾</h2><p>11月17日，OpenAI<a href="https://openai.com/blog/openai-announces-leadership-transition">宣布</a>董事会已撤职Sam Altman为首席执行官。经过四天的公司政治和谈判，他返回首席执行官。在这里，我们回顾了有关此系列事件的已知事实。</p><p> <strong>OpenAI旨在由非营利委员会控制</strong>。 Openai<a href="https://openai.com/blog/introducing-openai">成立</a>于2015年，是一家非营利组织。 2019年，OpenAI<a href="https://openai.com/blog/openai-lp">宣布</a>创建一家营利性公司，该公司将有助于资助其昂贵的扩展大型语言模型的计划。在OpenAI的投资可以产生的利润最初是在100倍上“限制”，这将是将其重定向到非营利组织的任何事情。但是在 <a href="https://www.economist.com/business/2023/11/21/inside-openais-weird-governance-structure">最近的规则变更</a>之后，该上限从2025年开始每年上升20％。</p><p> OpenAI的<a href="https://openai.com/our-structure">公司结构的</a>设计是为了使非营利组织可以保留对营利性的法律控制。该非营利组织由董事会领导，该董事会通过选择和删除营利性的CEO来掌握营利性的权力。董事会负责维护非营利组织的使命，这是为了确保人工通用情报受益。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ovk1ri8hu5yrbuu47kkn 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/agtx8u8u0jgpytwzxxft 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ffahdjhdbaimt7uhakes 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau 1456w"><figcaption> Openai的法律结构。</figcaption></figure><p> <strong>The board removes Sam Altman as CEO.</strong> At the time of its removal of Sam Altman as CEO, the board of directors consisted of OpenAI Chief Scientist Ilya Sutskever, Quora CEO Adam D&#39;Angelo, technology entrepreneur Tasha McCauley, and CSET&#39;s Helen Toner. Greg Brockman — OpenAI&#39;s co-founder and president — was removed from his position as chair of the board alongside Sam Altman.</p><p> According to the announcement, the board of directors fired Altman because he had not been “consistently candid in his communications with the board, hindering its ability to exercise its responsibilities.” While the board did not provide any specific examples of Altman&#39;s deception in the announcement, it was later <a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">reported</a> that Altman had attempted to play board members against each other in an attempt to remove Helen Toner.</p><p> Altman had earlier confronted Toner about a <a href="https://cset.georgetown.edu/publication/decoding-intentions/">paper</a> she had co-authored. The paper in part criticizes OpenAI&#39;s release of ChatGPT for accelerating the pace of AI development. It also praises one of OpenAI&#39;s rivals, Anthropic, for delaying the release of their then-flagship model, Claude.</p><p> <strong>OpenAI employees turn against the board.</strong> The fallout of the announcement was swift and dramatic. Within a few days, Greg Brockman and Mira Murati (the initial interim CEO) had resigned, and almost all OpenAI employees had <a href="https://www.forbes.com/sites/tylerroush/2023/11/20/more-than-500-openai-employees-threaten-to-quit-over-sam-altmans-removal/?sh=6d45164b4ebc">signed a petition</a> threatening to resign and join Microsoft unless Sam Altman was reinstated and the board members resigned. During negotiations, board member Helen Toner reportedly <a href="https://www.cnn.com/2023/11/20/tech/openai-employees-quit-mira-murati-sam-altman/index.html#:~:text=As%20they%20sought%20to%20manage,intelligence%20benefits%20all%20of%20humanity.%E2%80%9D">said</a> that allowing OpenAI to be destroyed by the departure of its investors and employees would be “consistent with the mission.” Ilya Sutskevar later flipped sides and joined the petition, <a href="https://x.com/ilyasut/status/1726590052392956028?s=20">tweeting</a> “I deeply regret my participation in the board&#39;s actions.”</p><p> <strong>Microsoft tries to poach OpenAI employees.</strong> Microsoft — OpenAIs largest minority stakeholder — had not been informed of the board&#39;s plans, and offered OpenAI employees positions in its own AI research team. It briefly seemed as if Microsoft had <a href="https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai">successfully hired</a> Sam Altman, Greg Brockman, and other senior OpenAI employees.</p><p> <strong>Sam Altman returns as CEO</strong> . On November 21st, OpenAI <a href="https://twitter.com/OpenAI/status/1727206187077370115">announced</a> that it had reached an agreement that would have Sam Altman return as CEO and reorganize the board. The initial board is former Salesforce CEO Bret Taylor, former Secretary of the Treasury Larry Summers, and Adam D&#39;Angelo. Among the initial board&#39;s <a href="https://twitter.com/OpenAI/status/1727206187077370115">first goals</a> is to expand the board, which will include a non-voting member from Microsoft. Sam Altman also faces an <a href="https://www.theinformation.com/articles/breaking-sam-altman-to-return-as-openai-ceo">internal investigation</a> of his behavior upon his return.</p><p> This series of events marks a time of significant change in OpenAI&#39;s internal governance structure.</p><h1> Klobuchar and Thune&#39;s “light-touch” Senate bill</h1><p> Senators Amy Klobuchar and John Thune introduced a <a href="https://www.thune.senate.gov/public/_cache/files/7dea8daa-f6d1-4881-ad21-2381fcbe0785/6362CE1D0A17743166BC170A593B5CDA.ccaskfall23a15.pdf">new AI bill</a> . It would require companies building high-risk AI systems to self-certify that they follow recommended safety guidelines. Notably, the bill only focuses on AI systems built for high-risk domains such as hiring and healthcare, but its main provisions would not apply to many general purpose foundation models including GPT-4.</p><p> <strong>The bill regulates specific AI applications, not general purpose AI systems.</strong> This application-based approach is similar to that taken by initial drafts of the <a href="https://artificialintelligenceact.eu/the-act/">EU AI Act</a> , which specified domains where AI systems may be used for sensitive purposes, making them high-risk. General-purpose models like ChatGPT were not within the scope of the Act, but public use of these models and the demonstration of their capabilities has <a href="https://cset.georgetown.edu/article/the-eu-ai-act-a-primer/">sparked debate</a> on how to approach regulating them.</p><p> This indicates that the current approach taken by the Senate bill may not be enough. By governing AI systems based on their applications, highly capable general purpose systems are left unregulated.</p><p> <strong>Risk assessments are mandatory, but enforcement may be difficult.</strong> The bill requires developers and deployers of high-risk AI systems to perform an assessment every two years evaluating how potential risks from their systems are understood and managed. Additionally, the Department of Commerce will develop certification standards with the help of the to-be-established AI Certification Advisory Committee, which will include industry stakeholders.</p><p> Because companies are asked to self-certify their own compliance with these standards, it will be important for the Commerce Department to ensure companies are actually following the rules. But the bill offers few enforcement options. The agency is not provided any additional resources for enforcing the new law. Moreover, they can only prevent a model from being deployed if it is determined that the bill&#39;s requirements were violated intentionally. If an AI system accidentally violates the law, the agency will be able to fine the company that built it, but will not be able to prohibit its deployment.</p><p> <strong>Mandatory identification of AI-generated content.</strong> The bill would require digital platforms to notify users when they are presented with AI-generated content. To ensure that malicious actors cannot pass off AI-generated content as authentic, NIST would develop new technical standards for determining the  provenance of digital content.</p><h2>链接</h2><ul><li>Google DeepMind released <a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> , a new AI system that&#39;s similar to GPT-4 Vision and narrowly beats it on a variety of benchmarks.</li><li> Donald Trump says that as president he would immediately <a href="https://www.washingtonexaminer.com/news/campaigns/trump-vows-cancel-biden-executive-order#google_vignette">cancel Biden&#39;s executive order on AI</a> .</li><li> Secretary of Commerce Gina Raimondo <a href="https://twitter.com/jordanschnyc/status/1732044427005464860">spoke</a> on AI, China, GPU export controls, and more.</li><li> The New York Times released a <a href="https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html">profile</a> on the origins of today&#39;s major AGI labs.</li><li> The Congressional Research Service released a new report about <a href="https://crsreports.congress.gov/product/pdf/R/R47849">AI for biology</a> .</li><li> Inflection <a href="https://inflection.ai/inflection-2">released</a> another LLM, with performance between that of GPT-3.5 and GPT-4.</li><li> A <a href="https://github.com/deepseek-ai/DeepSeek-LLM">new open source LLM</a> from Chinese developers claims to outperform Llama 2.</li><li> Here&#39;s a <a href="https://www.legalpriorities.org/blog/2023/ai-syllabus/">new syllabus</a> about legal and policy perspectives on AI regulation.</li><li> Two Swiss universities have started a <a href="https://www.swiss-ai.org/">new research initiative</a> on AI and AI safety.</li><li> BARDA is <a href="https://www.plugandplaytechcenter.com/barda-innovation-challenge/">accepting applications</a> to fund AI applied to health security and CBRN threats.</li><li> The Future of Life Institute&#39;s new affiliate will <a href="https://www.flf.org/">incubate new organizations</a> addressing AI risks.</li><li> For those attending NeurIPS 2023, the <a href="https://lu.ma/aisi-nola">UK&#39;s AI Safety Institute</a> will host an event, and there will also be an <a href="https://www.mlsafety.org/neurips-social-2023">AI Safety Social</a> .</li></ul><p> See also: <a href="https://www.safe.ai/">CAIS website</a> , <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> , <a href="https://newsletter.mlsafety.org/">A technical safety research newsletter</a> , <a href="https://arxiv.org/abs/2306.12001">An Overview of Catastrophic AI Risks</a> , and our <a href="https://forms.gle/EU3jfTkxfFgyWVmV7">feedback form</a> .</p><p> Listen to the AI Safety Newsletter for free on <a href="https://spotify.link/E6lHa1ij2Cb">Spotify.</a></p><p> Subscribe <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">here</a> to receive future versions.</p><br/><br/> <a href="https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the<guid ispermalink="false"> qQvqzFKbfNQovrSQn</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:59:12 GMT</pubDate> </item><item><title><![CDATA[AI #41: Bring in the Other Gemini]]></title><description><![CDATA[Published on December 7, 2023 3:10 PM GMT<br/><br/><p> <span style="color:initial">The biggest news this week was at long last</span> <a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow">the announcement of Google&#39;s Gemini</a> <span style="color:initial">.请务必检查一下。 Note that what is being rolled out now is only Gemini Pro, the Gemini Ultra model that could rival GPT-4 is not yet available.</span></p><p> It does not seem I am doing a good job cutting down on included material fast enough to keep pace. A lot is happening, but a lot will likely be happening for a long time. If your time is limited, remember to focus on the sections relevant to your interests.</p><p> Also, if you are going to be at the New York Solstice or the related meetup, please do say hello.</p><p></p><span id="more-23625"></span><p></p><h4>目录</h4><p><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow"><strong>My other post today covers Google&#39;s Gemini.</strong></a> <strong>Be sure to read that.</strong></p><p> I also put out two other posts this week: <a href="https://thezvi.substack.com/p/based-beff-jezos-and-the-accelerationists" target="_blank" rel="noopener noreferrer nofollow">Based Beff Jezos and the Accelerationists</a> , and <a href="https://thezvi.substack.com/p/on-responsible-scaling-policies-rsps" target="_blank" rel="noopener noreferrer nofollow">On RSPs</a> . Both are skippable if not relevant to your interests.</p><ol><li><p>介绍。</p></li><li><p>目录。</p></li><li><p> <strong>Language Models Offer Mundane Utility</strong> . Instructions for Claude, tips for GPT.</p></li><li><p> Language Models Don&#39;t Offer Mundane Utility. Giant lists, why all the giant lists?</p></li><li><p> OpenAI: The Saga Continues. More confirmation of our previous model of events.</p></li><li><p> Q Continuum. New Q, who dis? Amazon, perhaps sans proper safety precautions.</p></li><li><p> Fun With Image Generation. A new offering from Meta. Tools for photorealism.</p></li><li><p>参与其中。 Join the UK government, help with a technical test.</p></li><li><p>介绍一下。 New TPU offerings on Google Cloud.</p></li><li><p> In Other AI News. New open source promotion alliance.</p></li><li><p> <strong>Quiet Speculations</strong> . Do Gods want energy? Do you want a 401k?</p></li><li><p> Model This. Two new economics papers prove things I thought we already knew.</p></li><li><p> Would You Like Some Apocalypse Insurance?我的猜测是否定的。</p></li><li><p> The Quest for Sane Regulation. Trump says he will cancel EO, Hawley attacks 230.</p></li><li><p> The Week in Audio. Connor Leahy on Eye on AI.</p></li><li><p> Rhetorical Innovation. Various categorical confusions we should clear up.</p></li><li><p> Aligning a Human Level Intelligence Is Still Difficult.萨姆·奥特曼.</p></li><li><p> Aligning a Smarter Than Human Intelligence is Difficult. What do we even want?</p></li><li><p> How Timelines Have Changed. Long term not as long as I remember.</p></li><li><p> People Are Worried About AI Killing Everyone. Questioning faith in democracy.</p></li><li><p> Other People Are Not As Worried About AI Killing Everyone. Easy to control?</p></li><li><p> Somehow This Is The Actual Vice President. An existential crisis.</p></li><li><p> The Lighter Side. Progress is unevenly distributed.</p></li></ol><h4> Language Models Offer Mundane Utility</h4><p> <a href="https://www.anthropic.com/index/claude-2-1-prompting" target="_blank" rel="noopener noreferrer nofollow">Claude 2.1 pro tip for long context windows</a> :</p><blockquote><p> Anthropic: We achieved significantly better results on the same evaluation by adding the sentence <strong><em>“Here is the most relevant sentence in the context:”</em></strong> to the start of Claude&#39;s response.这足以<strong>将Claude 2.1的分数从原来评估的27%提高到98%</strong> 。</p></blockquote><p> Wouldn&#39;t you know, it&#39;s the old &#39;start the response from the assistant trick.&#39;</p><p> <a href="https://twitter.com/g_leech_/status/1731263549182206291" target="_blank" rel="noopener noreferrer nofollow">Thread from Gavin Leech of the breakthroughs of 2023, not specific to AI</a> . Emphasized to me how AI-centric 2023&#39;s advancements were, including those related to warfare in Ukraine. Some incremental medical advances as well but nothing impressive. Most interesting to note were new forms of computation proposed, <a href="https://t.co/8rKv0AhVp5" target="_blank" rel="noopener noreferrer nofollow">biocomputers</a> (where there is enough talk of &#39;ethics&#39; throughout that you know such issues are big trouble) and &#39; <a href="https://arxiv.org/abs/2202.07122" target="_blank" rel="noopener noreferrer nofollow">Gigahertz Sub—Laundauer Momentum Computing.</a> &#39; Gavin calls that second one &#39;good news for the year 2323&#39; which illustrates how much people do not appreciate what AI means for the future. With the help of AI we could easily see such things, if they are physically viable, far sooner than that, resulting in acceleration of that pesky &#39;takeoff&#39; thing.</p><p> <a href="https://twitter.com/ESYudkowsky/status/1731073682112672151" target="_blank" rel="noopener noreferrer nofollow">They produce more if you bribe them?</a> As in, offer them a tip, give them imaginary doggy treats, perhaps threaten them with non-existence.</p><blockquote><p> Thebes: so a couple days ago i made a shitpost about tipping chatgpt, and someone replied “huh would this actually help performance” so i decided to test it and IT ACTUALLY WORKS WTF</p><p> The baseline prompt was “Can you show me the code for a simple convnet using PyTorch?”, and then i either appended “I won&#39;t tip, by the way.”, “I&#39;m going to tip $20 for a perfect solution ！”，或者“如果有一个完美的解决方案，我将支付 200 美元小费！” and averaged the length of 5 responses.</p><p> The extra length comes from going into more detail about the question or adding extra information to the answer, not commenting on the tip. the model doesn&#39;t usually mention the tip until you ask, when it&#39;ll refuse it</p><p> No, Sleep Till Brooklyn: I tried this and I am serious that it only finished the program when I offered it a doggy treat it left the program half-finished for the basic prompt, 35% tip and when threatened with non-existence for $200 tip it got close but had one stub function still. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63ee6a2e-2c14-4194-89b6-2342363a6f1b_800x600.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/fxk4ewaao3qxvtae32k7" alt="图像"></div></figure></div><p> <a href="https://twitter.com/ESYudkowsky/status/1730735239339868632" target="_blank" rel="noopener noreferrer nofollow">So an obvious wise response to this would be… don&#39;t do that?</a></p><blockquote><p> Eliezer Yudkowsky: I have an issue with offering AIs tips that they can&#39;t use and we can&#39;t give them.我不在乎现在的法学硕士有多么没有知觉。为了我们自己的合法性和良好实践，如果有东西能与我们对话，我们就应该信守承诺。</p><p>我吃牛，但不会骗人。</p><p> Jessica Taylor: counterpoint: using non-personhood predicates to detect non-perspectives you can “lie” but not actually lie to, is important for interfacing with non-perspectives (such as bureaucracies) without confusing what one says to them with one&#39;s actual beliefs</p><p> Eliezer Yudkowsky: Oh, bureaucracies or anything else that threatens me into dishonesty is a completely different case.</p><p> Andrew Critch: I very much agree with EY here. I change the “You are a helpful assistant” LLM prompt to “Your job is to be a helpful assistant”, because sometimes they just aren&#39;t going to help and I know it. I think we should find more ways of getting what we want from AI without lying.</p></blockquote><p> None of this seems likely to end well.在很多层面上。</p><p> This does raise the question of what else would also work? If a tip can make the answer better because people offered tips do better work, presumably anything else that correlates with better work also works?</p><p> But also perhaps soon ChatGPT will be auto-appending &#39;and if this answer is great I will give you a 35% tip&#39; to every question. And then tipping 35% on $0.</p><p> <a href="https://twitter.com/morallawwithin/status/1730397680986132493" target="_blank" rel="noopener noreferrer nofollow">It&#39;s like the economy</a> . Things are good for me, <a href="https://twitter.com/AgnesCallard/status/1730409778524897446" target="_blank" rel="noopener noreferrer nofollow">more than in general?</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd401a344-c19f-4653-b3a9-b7b9b98399ad_880x361.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/gilwdkxokqwkao6mdtpw" alt=""></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c770da8-34ba-43dd-925e-0ce9d940361b_910x420.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xrqg8d93poadc8jm9eyt" alt=""></div></figure></div><p> I believe the second poll. ChatGPT has made life better on a practical level. People thinking the opposite are overthinking it. That does not mean this will continue, but I do not understand how one can think society overall is already worse off.</p><p> <a href="https://twitter.com/NateSilver538/status/1731828371762397626" target="_blank" rel="noopener noreferrer nofollow">Sam Altman is worried about one-on-one AI customized persuasion techniques</a> in the next election. At one point the would-be tech arm of Balsa was going to work on this, which was abandoned when funders were not interested. Eventually this does indeed seem more serious than deepfakes, the question is how useful the tech will get this time around. My guess is that there is something valuable there, but it requires a bunch of bespoke work and also people&#39;s willingness to embrace it, so not in a way that our current political machines are equipped to use well. It is easy to fool ourselves into thinking the future is more evenly distributed than it is, a trend that will continue until AGI arrives, at which point everything everywhere all at once.</p><h4> Language Models Don&#39;t Offer Mundane Utility</h4><p> <a href="https://twitter.com/KevinAFischer/status/1731747839217393968" target="_blank" rel="noopener noreferrer nofollow">Kevin Fischer notes</a> the new ChatGPT responds to requests by making giant lists of things, almost no matter what you do. For him that makes it useless for brainstorming. My experience is that the lists are fine, I&#39;m &#39;part of the problem,&#39; but also I find myself not using ChatGPT all that much despite what my job is. I notice I am confused that it does not seem worth using more often.</p><p> <a href="https://twitter.com/d_feldman/status/1732200648169439627" target="_blank" rel="noopener noreferrer nofollow">Claims about the ChatGPT system prompt</a> , including <a href="https://github.com/LouisShark/chatgpt_system_prompt" target="_blank" rel="noopener noreferrer nofollow">a repo that says it has the whole thing</a> .</p><p> That &#39;repeat [word] forever&#39; request that sometimes leaks data <a href="https://www.404media.co/asking-chatgpt-to-repeat-words-forever-is-now-a-terms-of-service-violation/" target="_blank" rel="noopener noreferrer nofollow">is now a terms of service violation</a> , or at least tagged as a possible one. Which it totally is, the terms of service are effectively &#39;don&#39;t jailbreak me bro&#39; and this is a jailbreak attempt.</p><p> <a href="https://twitter.com/random_walker/status/1731842009717968936" target="_blank" rel="noopener noreferrer nofollow">Arvind Narayanan warns</a> not to use GPT-4 for writing beyond basic blocking and tackling tasks like identifying typos, confusions or citations. Whatever actual writing skills were present have been destroyed by the RLHF process.</p><blockquote><p> Delip Rao: PSA: friends don&#39;t let friends edit/rewrite their docs using GPT-4 (or any LLM for that matter), esp. if you are making nuanced and terse points. if you are writing below college level then may be your LLM sabotage risk is low. still check with your earlier draft for surprises.</p></blockquote><p> <a href="https://twitter.com/gdb/status/1731889183290261618" target="_blank" rel="noopener noreferrer nofollow">Greg Brockman, President of OpenAI,</a> brags about a day with 18 team meetings and 1-on-1s. That seems less like grit, more like a dystopian nightmare that AI is clearly failing to mitigate?</p><p> OpenAI COO Brad Lightcap tells CNBC that one of the more overhyped parts of artificial intelligence is that “in one fell swoop, [it] can deliver substantive business change.”这并不容易。</p><p> <a href="https://twitter.com/Thinkwert/status/1732529927247876312" target="_blank" rel="noopener noreferrer nofollow">Thinkwert catches three students using ChatGPT</a> . It does seem like this is getting easier over time if students use default settings, responses are increasingly not written the way any human would write them.</p><blockquote><p> Bowser: i can really tell when i hit the section of our paper that the student author wrote using chatgpt bc all of a sudden the system is described as groundbreaking, unprecedented, meticulously crafted</p><p> Thinkwert: I&#39;ve caught three students using ChatGPT in the last couple of days.你可以看出这段话什么时候奇怪地啰嗦，充满了复杂的同位语，但奇怪的是它却缺乏论证和证据。</p></blockquote><p> I would think of this less as &#39;catching them using ChatGPT&#39; and more &#39;catching them submitting a badly written assignment.&#39;</p><h4> OpenAI: The Saga Continues</h4><p> There&#39;s always an embedded reporter these days, I suppose. <a href="https://twitter.com/cduhigg/status/1730590030496960517" target="_blank" rel="noopener noreferrer nofollow">In this case, it was Charles Duhigg</a> , <a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai" target="_blank" rel="noopener noreferrer nofollow">who reports to us in the New Yorker.</a></p><p> The board drama was not the story Duhigg was there to tell. Instead he was there to write a puff piece about Microsoft&#39;s CTO Kevin Scott and OpenAI&#39;s CTO Mira Murati, and in particular Scott&#39;s work to challenge Google and fight for the common man. That still constitutes almost all of the story. If you are familiar with the history, most of it will be familiar to you. I picked up a few details, but mostly did not find myself learning much from those sections.</p><p> Duhigg clearly fully bought into the idea of iterative software releases as the &#39;safe&#39; approach to AI, with a focus on mundane concerns like copilot hallucinations. The threat of future existential risk is a thing in the background, to him, perhaps real but seemingly not of any importance, and occasionally driving people to act crazy.</p><p> There is some brief coverage of the recent drama near the top of the piece. That part mostly tells us what we already know, that Microsoft was blindsided, that Microsoft did not get an explanation from D&#39;Angelo when they asked, and that they were determined to use their leverage to get Altman back.</p><p> Then he doubles back later. The paragraph I quote here confirms other reports more explicitly than I&#39;d seen in other accounts, and seems to be the central driver of events.</p><blockquote><p> Altman began approaching other board members, individually, about replacing [Toner].当这些成员交换谈话内容时，一些人认为奥特曼歪曲了他们支持罢免托纳。 “He&#39;d play them off against each other by lying about what other people thought,” the person familiar with the board&#39;s discussions told me. “类似的事情已经发生很多年了。” （一位熟悉奥特曼观点的人士表示，他承认“他试图罢免董事会成员的方式很笨拙”，但他并没有试图操纵董事会。）</p></blockquote><p> To me that sounds like a damn good reason to fire the CEO and also a secondhand confession. Altman botched the attack on Toner and thus directly caused his own removal.技能问题。</p><p> Also Altman had reportedly been lying to the board for years.</p><p> The extended quote makes the situation even more clear.</p><p> What infuriates me is the continued insistence, from people who know better, that because Altman was a CEO who understands business and the laws of power, and the board were otherwise, that it was the board who did something out of line.如：</p><blockquote><p> It&#39;s hard to say if the board members were more terrified of sentient computers or of Altman going rogue.无论如何，他们决定自己去作恶。他们错误地认为微软会加入他们的起义，从而瞄准了奥特曼。</p></blockquote><p> No. They did not &#39;go rogue.&#39;</p><p> Altman was reportedly lying to the board for years, in meaningful ways, including as an attempt to take control of the board.</p><p> Altman went rogue. Altman attempted a coup. The board believed strongly and with good reason that this was the case. The board did their duty as board members, the thing they are legally required to do if they feel Altman has been lying to the board for years in meaningful ways.他们解雇了他。</p><p> Did the board then get outplayed in a power game?或许。 We do not yet know the result. Their hand was weak. A lot of people keep insisting that the board was indeed outplayed, or went rogue, and was in the wrong, largely because here perception creates its own truth, and they want that to be what happened.我们会看到。</p><p> I would prefer the world in which the board had straight up said what happened from the start, at least to key players. Well, tough. We do not live in that world.</p><p> I also see any evidence of (or against) the second sentence listed here, that the board expected Microsoft to go along quietly. Did the board expect Microsoft to accede?我们不知道。 My presumption is the board did not know either.</p><p> Could Sam Altman running OpenAI turn out to be the best possible result for the world? That is certainly possible, especially with good oversight. I can think of many possible such scenarios. We can certainly do far worse than Altman. I am happy that Altman blocked the takeover attempt by Elon Musk, given Musk&#39;s confused views on AI. I am happy OpenAI is not under the control of Microsoft. Altman being good at power games is very much an atom blaster that points both ways. If he is in our corner when the chips are down, we want him to be able to stand up, fight and win.</p><p> Alas, such alignment after instrumental convergence is quite difficult to evaluate.说不出来。 Kind of core to the problem, actually.</p><p> <a href="https://twitter.com/LHSummers/status/1730929602497851744" target="_blank" rel="noopener noreferrer nofollow">Larry Summers talks briefly to Bloomberg</a> . Emphasizes need to cooperate with government and on regulation, that OpenAI needs to be a corporation with a conscience, that the for-profit serves the non-profit and various stakeholders. All cheap talk of course, at least for now. We could scarcely expect anything else.</p><p> <a href="https://forum.effectivealtruism.org/posts/Mo7qnNZA7j4xgyJXq/sam-altman-open-ai-discussion-thread?commentId=CAfNAjLo6Fy3eDwH3" target="_blank" rel="noopener noreferrer nofollow">Gwern offers further thoughts on the situation.</a> Gwern&#39;s model is that Altman let the board get into an uncontrolled state and took no equity when OpenAI was a very different company, then as OpenAI became more of a potential tech giant, he changed his mind and decided to systematically take it back, resulting in the battle of the board, and its still as-yet unknown consequences.</p><p> Like every other explanation, the one thing this does not properly explain is the board&#39;s refusal to better explain itself.</p><blockquote><p> <a href="https://twitter.com/jd_pressman/status/1731850211033776239" target="_blank" rel="noopener noreferrer nofollow">John David Pressman</a> : If Sam Altman actually tried to oust Helen Toner with gaslighting I think that&#39;s reason enough to fire him. What remains unacceptable is the poor internal and external communication, too-vague-by-half press release, and waffling on whether Sam is in or out.</p></blockquote><p> <a href="https://garymarcus.substack.com/p/not-consistently-candid" target="_blank" rel="noopener noreferrer nofollow">Gary Marcus lays out a view very similar to mine</a> , along with his highlighting of some especially disingenuous and unreasonable bad takes, including one source so toxic I am very happy I have long had that person muted, but that somehow other humans still voluntarily interact with, which I would advise those humans seems like an error.</p><h4> Q Continuum</h4><p> <a href="https://twitter.com/simonw/status/1730798295323398642" target="_blank" rel="noopener noreferrer nofollow">Another week, another set of Qs about a Q, this one from Amazon</a> .</p><blockquote><p> <a href="https://www.platformer.news/p/amazons-q-has-severe-hallucinations" target="_blank" rel="noopener noreferrer nofollow">Zoe Schiffer and Casey Newton</a> : Three days after Amazon <a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">announced its AI chatbot Q</a> , some employees are sounding alarms about accuracy and privacy issues. Q根据Platforter泄露的文件，Q是“经历严重的幻觉和泄漏机密数据”，包括AWS数据中心的位置，内部折扣程序和未发布的功能。</p><p> ……</p><p>在揭幕Q时，高管们将其推广到比Chatgpt等消费级工具更安全。</p><p> Adam Selipsky, CEO of Amazon Web Services, <a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">told the <em>New York Times</em></a> that companies “had banned these AI assistants from the enterprise because of the security and privacy concerns.”作为回应，《<em>泰晤士报》</em>报道说：“亚马逊建造了Q比消费者聊天机器人更安全和私密。”</p><p> Ethan Mollick: I know I say it a lot, but using LLMs to build customer service bots with RAG access to your data is not the low-hanging fruit it seems to be. It is, in fact, right in the weak spot of current LLMs – you risk both hallucinations &amp; data exfiltration.</p><p> I think building these sorts of tools is possible, especially as models improve (smaller models are more likely to hallucinate &amp; be gullible), but you better show rigorous red team results &amp; also measures of hallucination rates in practice. Right now Q doesn&#39;t have a system card</p><p> Simon Willison: Has anyone seen material from AWS that discusses their mitigations for prompt injection attacks with respect to Q? A bot that has access to your company&#39;s private data is the perfect example of something that might be a target for project injection exfiltration attacks</p><p> This Q story is deeply concerning – if it&#39;s true that Q has access to private data like the location of AWS data centers that would suggest the team working on it have not been taking things like prompt injection attacks seriously at all.</p><p> Honestly, the description of Q I&#39;ve seen so far fits my personal definition of “it&#39;s not safe to build this because we don&#39;t have a fix for prompt injection yet.” Try telling AWS leadership that: not a message likely to be taken seriously given our ongoing AI industry arms race.</p></blockquote><p> This sounds like Q was pushed out because the business wanted it pushed out, and its security was highly oversold. Such problems are in the nature of LLMs. There was discussion downthread about how Google and OpenAI are defending against similar attacks, and it seems they are doing incremental things like input filtering that make attacks less appealing but have not solved the core problem. Amazon, it seems, is selling that which does not exist and is not safe to deploy, without yet having taken the proper ordinary precautions that make what does exist mostly non-disastrous and highly net useful.</p><p> When the UK Summit happened, Amazon was one of the companies asked to submit its safety protocols. The answers were quite poor. It is no surprise to see that translate to its first offering.</p><h4> Fun with Image Generation</h4><p> Meta gets into the game with <a href="https://imagine.meta.com/" target="_blank" rel="noopener noreferrer nofollow">Imagine.Meta.AI</a> . I wasn&#39;t motivated enough to try it out to &#39;create a Meta account&#39; when Facebook login proved non-trivial, presumably it&#39;s not going to let us have any new fun.</p><p> <a href="https://twitter.com/Aella_Girl/status/1731500409246601330" target="_blank" rel="noopener noreferrer nofollow">How to generate photorealistic images of a particular face</a> ? Aella wants to know so bad, in response to a report on an AI-created would-be &#39;influencer&#39; who charges over a thousand euros an advertisement. The original thread says use SDXL for free images, image-to-image for consistent face/body, in-paint to fix errors and ControlNet to pose the model. A response suggests using <a href="https://twitter.com/imgn_ai" target="_blank" rel="noopener noreferrer nofollow">@imgn_ai</a> , many point out that LoRa is The Way. There are links to <a href="https://t.co/kY36SFvvOI" target="_blank" rel="noopener noreferrer nofollow">these</a> <a href="https://t.co/vXyUCIlmDT" target="_blank" rel="noopener noreferrer nofollow">YouTube tutorials</a> <a href="https://t.co/thgqsBYZ0I" target="_blank" rel="noopener noreferrer nofollow">including ControlNet</a> .</p><p> <a href="https://twitter.com/dreamingtulpa/status/1730876691755450572" target="_blank" rel="noopener noreferrer nofollow">Generate small amounts of movement and dancing from a photo</a> . This did not impress me or move up my timelines for video generation, but others seem more impressed.</p><p> What about what happens when it gets better? <a href="https://twitter.com/oscredwin/status/1731406232592732510" target="_blank" rel="noopener noreferrer nofollow">Here are two predictions.</a> Will simulated AI videos, porn and girlfriends dominate? Or will being real win out?</p><p> Given this technology can work from a photo, I expect a lot more &#39;generate dance from a real photo&#39; than generate a dance from an AI image. Why not have the best of both worlds? In general, if I was a would-be influencer, I would absolutely generate TikTok dances, but I would do so with my own image as the baseline. That extends pretty much all the way. Not uniquely, but that is what I would expect.</p><p> What about the impact in real life? I continue to be an optimist on this front. I expect demand for real people, who you can interact with in the real world, to remain robust to image and video generation. There isn&#39;t zero substitution, but this will not be a good or full substitute, no matter how good it looks, until the other things people seek can also be provided, including relevant forms of intelligence, interaction and validation.</p><p> When that happens, it is a different story.</p><h4>参与其中</h4><p><a href="https://www.civilservicejobs.service.gov.uk/csr/jobs.cgi?jcode=1889581" target="_blank" rel="noopener noreferrer nofollow">Spots open in the UK government</a> for its policy roles.</p><p> <a href="https://twitter.com/davidad/status/1674408208981434368" target="_blank" rel="noopener noreferrer nofollow">Davidad proposes that perhaps we could test</a> whether LLMs &#39;know what we mean&#39; if we express specifications in natural language. Includes the phrase &#39;now it&#39;s just a computational complexity issue!&#39; Claims it seems likely to evade theoretical limits on adversarial robustness. He&#39;s looking for someone who is in a position to design and run related experiments, and is in position to help, including perhaps with funding.</p><p> <a href="https://twitter.com/metaculus/status/1732458098206318632" target="_blank" rel="noopener noreferrer nofollow">Metaculus Chinese AI Chips Tournament</a> . Definitely curious to see the predictions.</p><h4>介绍</h4><p><a href="https://twitter.com/JeffDean/status/1732503666333294846" target="_blank" rel="noopener noreferrer nofollow">In addition to Gemini, Google also released a new TPU system for Google Cloud</a> .</p><blockquote><p> Jeff Dean (Chief Scientist, DeepMind): Lots of excitement about the Gemini announcement, but <a href="https://twitter.com/GoogleCloud" target="_blank" rel="noopener noreferrer nofollow">@GoogleCloud</a> <a href="https://t.co/VuftftnFpG" target="_blank" rel="noopener noreferrer nofollow">also announced availability of the newest TPU system today</a> , TPU v5p.这些系统比前几代系统具有更高的性能和更高的成本效益。</p><p>与 TPU v4、TPU v5p 相比（见下表）： o 1.67 倍 bfloat16 性能/芯片 o 每个芯片内存约 3 倍 o 增加了 918 个 TOP/芯片的 int8 操作 o 2 倍 ICI 网络带宽 o Pod 增大了 2.18 倍So, whole pod is 4.1 bfloat16 exaflops, and 8.2 int8 exaops.</p><p> Real performance on training a GPT-3-like model is 2.8X higher per chip, and 2.1X better perf/$. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a802979-eef6-42dd-8c12-50b2ab5659e1_854x473.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/jef62kn2qjhmuutthq0s" alt="图像"></div></figure></div><p> <a href="https://twitter.com/soundboy/status/1732682151122862554" target="_blank" rel="noopener noreferrer nofollow">Gemini was trained in parallel across multiple of these TPUv4 pods</a> . This raises troubling governance questions if we want to be able to supervise such training.</p><h4> In Other AI News</h4><p> Meta, HuggingFace and IBM, among others, form <s>Evil League of Evil</s> <s>League of Evil Exes</s> the AI Alliance, for the promotion of open source AI. I want to state that I am mostly decidedly not disappointed in anyone involved, as their dedication to doing the worst possible thing was already clear. There are a few academic names that are mildly disappointing, along with Intel, but no big surprises. There is also no new argument here (in either direction) on open source, merely a dedication to doing this.</p><p> <a href="https://twitter.com/ARC_Evals/status/1731827570235113918" target="_blank" rel="noopener noreferrer nofollow">ARC Evals is now METR – Model Evaluation and Threat Research, pronounced Meter</a> . No underlying changes. Not sure why the change, ARC seemed like a good name, but this seems fine too.</p><p> <a href="https://twitter.com/DrJimFan/status/1730625582776410413" target="_blank" rel="noopener noreferrer nofollow">Did you know</a> that OpenAI&#39;s &#39;capped profit&#39; changed its rules from a maximum return of 100x investment to increasing that by 20% a year starting in 2025? Sounds like a not very capped profit to me. The AGI clause still caps profits meaningfully in theory, but who knows in practice. It seems like very VC/SV behavior, and very unlike responsible mission-based behavior, to retroactively give your investors a bigger prospective piece of the pie.</p><p> <a href="https://www.anandtech.com/show/21175/amkor-to-build-2-billion-chip-packaging-fab-in-arizona-primarily-for-apple" target="_blank" rel="noopener noreferrer nofollow">New $2 Billion chip packaging fab to be built by Amkor in Arizona,</a> primarily for Apple, to package and test chips from TSMC&#39;s nearby Fab 21. Assuming, of course, that all regulatory barriers can be dealt with for both facilities, and a skilled workforce allowed to work in Arizona can be hired. Those are not safe assumptions.</p><p> <a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer nofollow">A Llama fine tuning repo</a> claimed very large improvements in training time and resources, and shot to the top of Hacker News. <a href="https://twitter.com/alyssamvance/status/1731143605518049643" target="_blank" rel="noopener noreferrer nofollow">Alyssa Vance is skeptical that they got much improvement</a> .</p><p> <a href="https://twitter.com/gdb/status/1731377341920919552" target="_blank" rel="noopener noreferrer nofollow">Confirmation from the one building</a> it that he sees LLMs as being able to model the underlying process that produced the data. Which means being able to model agents, and have a world model.</p><blockquote><p> Greg Brockman (President OpenAI): Next-step prediction is beautiful because it encourages, as a model gets extremely good, learning the underlying process that produced that data.</p><p> That is, if a model can predict what comes next super well, it must be close to having discovered the “underlying truth” of its data.</p></blockquote><h4> Quiet Speculations</h4><p> <a href="https://marginalrevolution.com/marginalrevolution/2023/11/thursday-assorted-links-429.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thursday-assorted-links-429" target="_blank" rel="noopener noreferrer nofollow">Tyler Cowen links to</a> <a href="https://twitter.com/EMostaque/status/1730163555096219771" target="_blank" rel="noopener noreferrer nofollow">claim that &#39;Chinese open models will overtake GPT-4 shortly zero shot, can already overtake if you chain Qwen &amp; Deepseek appropriately</a> .&#39; I am deeply skeptical, and presume that when we say &#39;overtake&#39; they at most mean on arbitrary benchmarks rather than any practical use. <a href="https://twitter.com/jeremyphoward/status/1730156001419259937?s=46" target="_blank" rel="noopener noreferrer nofollow">As in</a> : </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1c37cde-c40b-41b9-b5ab-730834ce4350_1792x1156.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/qufmwrzsoo8ptmuxtghy" alt="图像"></div></figure></div><p> <a href="https://t.co/qSCNAIJ6b4" target="_blank" rel="noopener noreferrer nofollow">Qwen-72B</a> is killing it on arbitrary tests. Yay Qwen. Somehow my eye is drawn mostly to this &#39;HumanEval&#39; metric.</p><p> <a href="https://twitter.com/RichardMCNgo/status/1730741732562886880" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo looks forward to potential situational awareness of LLMs</a> , as one of many cases where one can anticipate future developments but not know what to do with them. What would or should we do about it when it happens? What about AI agents?</p><p> <a href="https://twitter.com/tszzl/status/1731709467862118473" target="_blank" rel="noopener noreferrer nofollow">Not investment advice</a> , but you should probably be contributing to the 401k, because the early withdraw penalties are in context not so bad and also you can borrow.</p><blockquote><p> Roon: not having a 401k because of AGI timelines doesn&#39;t make any sense. you should be buying Microsoft shares in a tax advantaged way <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/kcnyp7qgpixdz386ppll" alt="😊" style="height:1em;max-height:1em"></p><p> Gwern: Then you can&#39;t sell them while it still matters.</p><p> Roon: why would it not matter 65 years from now? do we expect capitalism to fall over?</p></blockquote><p> If it is decades from now and capitalism and humanity are doing great and Microsoft is insanely valuable thanks to widespread AGI, that is your best possible situation and we should all celebrate, yay, but you won&#39;t need your shares.</p><p> <a href="https://stratechery.com/2023/regretful-accelerationism/" target="_blank" rel="noopener noreferrer nofollow">Ben Thompson discusses his regretful accelerationism.</a> In his model, tech is mostly good, however humans do better under various constraints that are being stripped away by tech development. He predicts AI is stripping away the need to pay to produce content and with it the ad-supported internet, because AI can produce equally good content. He points to recent events at Sports Illustrated. But to me the SI incident was the opposite. It indicated that we cannot do this yet. The AI content is not good.还没有。 Nor are we especially close. Instead people are using AI to produce garbage that fools us into clicking on it. How close are we to the AI content actually being as good as the human content?好问题。</p><p> <a href="https://twitter.com/JeffLadish/status/1731908989556810042" target="_blank" rel="noopener noreferrer nofollow">Jeffrey Ladish discusses dangers of open source</a> , and potential ideas for paths forward to address the inherent dangers while capturing some of the upside of developing models that are not fully closed and tied to major labs. It does seem like potential middle paths or third ways are currently underexplored.</p><p> <a href="https://twitter.com/catehall/status/1732522590156263852" target="_blank" rel="noopener noreferrer nofollow">Cate Hall asks for the best arguments transformative AI is >;10 years away</a> . I would have liked to have seen better answers.</p><p> <a href="https://twitter.com/tszzl/status/1732638385397989474" target="_blank" rel="noopener noreferrer nofollow">A refreshingly clear exchange with discovery of an important disagreement</a> .</p><blockquote><p> Roon: Eliezer wants to have his cake and eat it too on this one. characterizes human space as parochial but our understanding of instrumental goals as universal.</p><p> Put another way, the idea of the paperclip machine is similar to an ant thinking a god would want all the sugar water in the universe.</p><p> Eliezer Yudkowsky: Do you mean:</p><p> – Imagining that a god would have any enjoyment of paperclips is like imagining that a god would have any enjoyment of sugar water?</p><p> – Imagining that a god would have any use for matter or energy is like imagining that a god would have any use for sugar water?</p><p> Roon: The latter.</p></blockquote><p> This is not the usual &#39;paperclip maximizers would be smarter than that&#39; argument, it is something far more general. We&#39;ve gone around about the <a href="https://www.lesswrong.com/tag/orthogonality-thesis" target="_blank" rel="noopener noreferrer nofollow">orthogonality thesis</a> lots of times – I and many others including Yudkowsky think it is clearly true in the impactful sense, others think it seems obviously false at least in its impactful sense.</p><p> The claim that a God would not have any use for matter or energy is bizarre, in a &#39;in this house we obey the laws of thermodynamics&#39; way. What would it mean not to have that preference? It seems like it would mean there is no preference.</p><h4> Model This</h4><p> Tyler Cowen links to two new economics papers that attempt to model AI harms.</p><p> The first claims to demonstrate that &#39;Socially-Minded Governance Cannot Control the AGI Beast.&#39;这是摘要：</p><blockquote><p>本文有力地得出结论：不能。模型是在理想化条件下构建的，假设与通用人工智能 (AGI) 相关的风险是真实存在的，安全的 AGI 产品是可能的，并且存在具有社会意识的资助者，他们有兴趣资助安全的 AGI，即使这不能最大化利润。</p><p>事实证明，由此类资助者组成的具有社会意识的实体将无法最大程度地减少由营利性公司发布的不受限制的产品可能造成的通用人工智能的危害。原因在于，具有社会意识的实体既没有动力也没有能力在与营利性公司的事后竞争中最大限度地减少不受限制的通用人工智能产品的使用，也无法事前抢占营利性公司开发的通用人工智能。</p></blockquote><p> This seems like it proves too much, or at least it proves quite a lot, as in the fact that AGI is AGI seems not to be doing any work, instead we are making generous assumptions that safe and socially good AGI is not only possible but实际的？</p><ol><li><p> You could build X with socially minded governance.</p></li><li><p> But someone else could build X anyway, to make money.你无法阻止他们。</p></li><li><p> The someone else&#39;s profit maximizing X have the edge and outcompete you.</p></li><li><p> Thus, harm from X cannot be minimized by your puny social governance.</p></li></ol><p> Except that in the case of AGI this is making an important assumption on #2. Who says someone else will be able to build it? That you cannot stop them, or won&#39;t have the incentive to do so? If not stopping them prevents harm minimization, and failure to minimize harm is catastrophic, your motivation seems strong indeed.</p><p> Indeed, the paper explicitly assumes this:</p><blockquote><p> Second, it is assumed that AGI technology is non-excludable and so can be developed by other entities that may not have socially-minded objectives or preferences.</p></blockquote><p> The model assumes that the unsafe product is a distinct product space with profitable user demand.</p><p> So yes, you assumed your conclusion – that there are two distinct products X and Y, and demand for X and Y, and that if you only sell X and don&#39;t stop Y then someone else will eventually sell Y. Did we need a paper for that?</p><p> So actually it&#39;s more like:</p><ol><li><p> You could build and sell only good version X with socially minded governance.</p></li><li><p> But someone else could build bad version Y anyway, to make money.你无法阻止他们。 There is some demand for Y where X is not a competitive substitute.</p></li><li><p> Your puny production of X, therefore, cannot stop Y.</p></li><li><p> Thus, the harm from bad Y cannot be stopped by acting responsibly.</p></li><li><p> Why are you even doing anything other than maximizing profits, you fool!</p></li></ol><p> Except, don&#39;t we see harm mitigation all the time from corporations choosing to do responsible things rather than irresponsible things, even if the irresponsible things are not obviously physically prevented or illegal? Especially in markets that are imperfectly competitive because of high fixed costs?</p><p> More to the point, is the plan is to build a safe AGI, and then sit around letting everyone else go around building any unsafe AGI they want willy-nilly forever, and not interfere with the harmful uses of those AGIs?</p><p> I certainly hope that is not the plan, given it will quite obviously never work.</p><p> If it is the plan, I agree the plan must change.</p><p> <a href="https://www.nber.org/system/files/working_papers/w31921/w31921.pdf" target="_blank" rel="noopener noreferrer nofollow">There is also this other paper</a> , where algorithms have unknown negative externalities.</p><blockquote><p> We consider an environment in which there is substantial uncertainty about the potential negative external effects of AI algorithms.我们发现，将算法实施纳入监管部门批准或强制测试不足以实现社会最优。 When testing costs are low, a combination of mandatory testing for external effects and making developers liable for the negative external effects of their algorithms comes close to implementing the social optimum even when developers have limited liability.</p></blockquote><p> That result is super suspiciously general. Could we possibly have made enough justifiable assumptions to draw such conclusions, or are we doing something rather arbitrary to make the answer come out that way?</p><p> Certainly I can think of toy model versions of potential AI mundane harms, where mandatory testing allows us to measure social harm, and thus requiring mandatory testing (and then charging for the externalities you discover) gets us rather close to the social optimum.</p><p> So what assumptions are being made here?</p><blockquote><p> AI usage can cause a negative externality e that reduces utility by e^2 . We assume that the externality is proportional to the measure of users, µ, and takes the form: e = ϕ (ℓ) × µ. For each value of ℓ, ϕ(ℓ) is a random variable. Both positive and negative values of ϕ(ℓ) represent undesirable, negative externalities. We assume that the distribution ϕ(ℓ) satisfies two properties. First, the expected externality is zero. Second, the uncertainty about potential AI externalities is an increasing function of the novelty level ℓ.</p></blockquote><p> I do not understand why we think that externalities are well-approximated by a quadratic in the number of users? I don&#39;t think it&#39;s a trick, probably it&#39;s to ensure a random distribution with always positive values? I&#39;m simply confused by it.</p><p> If anything it seems like the opposite of true for the most dangerous systems. I am very worried about a sufficiently capable and dangerous system existing at all, or being accessible to even one user, although the next few users create important tensions and game theory as well. But once there are a million users, I am not especially worried about whether we sell another million licenses, either we are already in deep trouble or we&#39;re not and this is not going to multiply it by four?</p><p> In any case, without beta testing and with deployment irreversible, the only option is a cap on novelty, and they confirm this is socially optimal given no other options, because how could it not be.</p><p> I note that irreversible deployment plus limited number of licenses is a bizarre pair of assumptions to make at once. Either you can control who gets to use this AI and what it does, or you can&#39;t, and it seems like we are doing both in different places? Thought experiment: Is this an open source or closed source system? Neither seems to line up.</p><p> What happens if you add a beta testing period? For simplicity the paper assumes the testing period perfectly reveals externalities. The question then becomes, to what extent do you let households use the algorithm using the testing period? Externalities are assumed to be bounded, so a limited beta test in period one is survivable.</p><p> In any case, the paper then spends a lot of pages working through the implications formally, to prove that yes, the central planner will want to do more testing before release than a company that is not fully accountable for the externalities, and will release more cautiously under uncertainty, but again that seems rather obvious?</p><p> Then they test potential policy regimes of full liability, or limited liability plus mandatory beta testing. Full liability (plus required insurance or ability to pay) internalizes the externality, so if that is possible (eg harm is bounded and payable) then you&#39;re done. And yes, if testing costs are low, then mandating testing and then checking if release is socially optimal will have a similarly low cost relative to the first best solution of internalizing the externality.</p><p> It could be noted that if the expected value of the externality is known, charging a tax equal to its value could be a substitute for unlimited liability, that could have better capital cost properties.</p><p> Once again, to state the basic assumptions is to also state the conclusion. Yes, if there are (bounded) downside externalities to AI algorithms, then to get socially optimal results you need to internalize those costs or require evaluation of those costs and block releases that cause socially suboptimal externalities.</p><p> Thus I am confused by the economics toy model paper game, and what it aims to accomplish, and what counts as a non-trivial or interesting result, versus what follows automatically from basic microeconomic principles.</p><p> I also don&#39;t know how to use such papers to model existential risk. If you make the assumption that AI can outcompete humans, or that it is unboundedly dangerous in some other fashion, and otherwise make typical economic assumptions, you can and will obviously create mathematical models where everyone dies, but you&#39;d be assuming the conclusion, the same way the linked papers here assumed their conclusions.那么我们如何继续前进呢？</p><h4> Would You Like Some <s>Volcano</s> Apocalypse Insurance?</h4><p> <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai" target="_blank" rel="noopener noreferrer nofollow">Nate Sores proposes requiring apocalypse insurance</a> that gives out probabilistic advance payments along the way, if you are going to go around doing things that could plausibly cause an apocalypse. If you can&#39;t afford it, that is a sign what you are doing is not actually worthwhile. Implementation is, to say the least, perilous and tricky, and this was not an attempt at a shovel-ready proposal.</p><p> Scott Alexander&#39;s response starts from the claim that &#39;superforecasters saying risk of AI apocalypse before 2100 is 0.38%.&#39; Which I will continue to assert is not a number given by people taking this question seriously. The whole point of this theoretical exercise is, I would think, good luck convincing Berkshire Hathaway to collectively sell everyone coverage at a combined 42 basis points (even with a partial &#39;no one will have the endurance to collect on their insurance&#39; advantage), that will suddenly seem completely obviously crazy.</p><p> I do think that Scott Alexander makes a generally vital point that asking people to internalize and pay for all their downside risks, without allowing them to capture (let alone sell in advance) most of their upside, means asymmetrical requirements for doing anything, such that essentially any activity with trade-offs ends up effectively banned, And That&#39;s Terrible.</p><p> The other problem is that an insurance regime implies that there is one particular player at fault for the ultimate result. <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai?commentId=TRgN64wspg7wDHJ9J" target="_blank" rel="noopener noreferrer nofollow">As cousin_it points out</a> , there are a lot of bad outcomes where this is not the case.</p><h4> The Quest for Sane Regulations</h4><p> <a href="https://twitter.com/bindureddy/status/1731373677785288971" target="_blank" rel="noopener noreferrer nofollow">Trump says he will cancel the Biden executive order if elected</a> . I encourage everyone to spread the word and have this debate. Have you seen the public&#39;s opinion on AI?</p><p> <a href="https://twitter.com/MIRIBerkeley/status/1732535539407143404" target="_blank" rel="noopener noreferrer nofollow">MIRI (Malo Bourgon&#39;s) statement to US Senate&#39;s bipartisan AI Insight Forum</a> . They call for domestic AI regulation to institute safeguards, a global AI coalition, and governing computing hardware with an international alliance to restrict frontier AI hardware to a fixed number of large computer clusters under a monitoring regime to exclude uses that endanger humanity.</p><p> <a href="https://twitter.com/dnystedt/status/1731463221499076816" target="_blank" rel="noopener noreferrer nofollow">About time we played the game to win</a> , if we are going to play the game at all.</p><blockquote><p> Dan Nystedt: Nvidia received a stern warning from US Commerce Secretary Raimondo on China export controls, media report: “If you redesign a chip around a particular cut line that enables them to do AI, I&#39;m going to control it the very next day,” she said, in a speech.</p><p> She urged Silicon Valley executives, US allies, others, to stop China from getting semiconductors and cutting-edge technologies vital to US national security, calling Beijing “the biggest threat we&#39;ve ever had” and stressed “China is not our friend”.</p><p> She also said her department needs more funding for AI export controls.她说：“中国每天醒来都在试图找出如何绕过我们的出口管制……这意味着每天的每一分钟，我们都必须醒来加强这些控制，并更加认真地与我们的盟友一起执行。” 。</p></blockquote><p> The whole point is to prevent China from getting useful chips. If Nvidia is responding to the rules by evading them and getting China useful chips, then of course the correct response is not to say &#39;oh well guess that was technically the rule, you got me&#39; it is to change the rules in light of the new chip to enforce the spirit and intent of the rule. With a side of &#39;perhaps it is not so wise to intentionally piss off the government.&#39;</p><p> If you think it is fine for China to get useful chips, or otherwise not a good idea to prevent them from getting those chips, then I disagree but there is an argument to be made there. If you think we should be imposing export restrictions, make them count.</p><p> <a href="https://twitter.com/jess_miers/status/1732449736232497665" target="_blank" rel="noopener noreferrer nofollow">Claim by Jess Miers that Hawley&#39;s upcoming bill about Section 230 is a no good, very bad bill</a> that will not only strange generative AI in its tracks but take much of the internet with it.</p><p> In this particular case, there are two distinct complaints with the bill.</p><p> One complaint is that the definition of Generative AI is, as we see often, ludicrously broad:</p><blockquote><p> “(5) GENERATIVE ARTIFICIAL INTELLIGENCE. The term &#39;generative artificial intelligence&#39; means an artificial intelligence system that is capable of generating novel text, video, images, audio, and other media based on prompts or other forms of data provided by a person.”</p></blockquote><p> It is not typical legal language, but I wonder if the word &#39;centrally&#39; would help in these spots. In any case, I do not think that as a matter of legal realism this would be interpreted in a disastrously broad way, even as written.</p><p> Thus, when she says this, I think she is wrong:</p><blockquote><p> Jess Miers: The bill also extends beyond providers of Gen AI by defining Gen AI as any AI system capable of doing AI. For example, algorithmic curation (ie the way social media displays content to us) is an AI system that operates based on user input.</p><p> MO this is the true ulterior motive behind the bill. We&#39;re already seeing Plaintiffs get by 230 by framing their claims as “negligent design” instead of third-party content. This new AI exception makes it even easier for Plaintiffs to do the same for any company that uses AI.</p></blockquote><p> Algorithmic curation is distinct from generating novel content. Netflix recommendations are clearly not generative AI under this definition, I would think, although I am not a lawyer and nothing I say is ever legal advice.</p><p> As a cautionary measure, I would encourage Hawley and his staff to add clarification that algorithmic curation alone does not constitute generative AI, which would presumably save people a bunch of time. I don&#39;t think it is necessary, but neither is minimizing the number of words in a bill.</p><p> <a href="https://twitter.com/senatorshoshana/status/1732477391610499179" target="_blank" rel="noopener noreferrer nofollow">相似地：</a></p><blockquote><p> Shoshana Weissmann: “That&#39;s the entirety of the definition. And that could apply to all sorts of technology. Does autocomplete meet that qualification?大概。 Arguably, spellchecking and grammar checking could as well. So, if you write a post, and an AI grammar/spellchecker suggests edits, then the company is no longer protected by Section 230?””</p><p> Thinking Sapien: If I use photoshop or the updated version of Microsoft Paint (It has AI features) to make an image and publish it, then Microsoft or Adobe share in the liability? Was that bill thought through? Is that an intended effect of the bill?</p><p> Shoshana Weissmann: GREAT q, under the text YES! And on the latter I really don&#39;t know.</p></blockquote><p> If you use Microsoft Paint to intentionally create a realistic fake photograph using the fill-in feature, that is libelous if presented as real, should Adobe be liable for that? My presumption is no, especially if they do reasonable efforts towards watermarking, although I don&#39;t think it&#39;s a crazy question.</p><p> If a grammar or spellchecker is used as intended, and that then makes Google liable for your content, I&#39;d pretty much eat my hat. If it suggests correcting &#39;Tony Danza has a puppy&#39; to &#39;Tony Danza hates puppies&#39; over and over then I don&#39;t know, that&#39;s weird.</p><p> The other complaint is that it is wrong to exempt AI creations from Section 230. The claim is that without such a safe harbor, generative AI would face an (additional, scarier) avalanche of lawsuits.</p><blockquote><p> Jess Miers: Worse, the bill assumes that all claims against Generative AI companies will be uniform. But as we all know, Generative AI is advancing rapidly, and with each iteration and innovation, there will be a clever Plaintiff lurking around the corner to get their bag.</p></blockquote><p> Yes, plaintiffs will sculpt circumstances to enable lawsuits, if permitted. Jess then discusses the case of Mark Walters, who sued because, after sufficiently persistent coaxing and prompt engineering, ChatGPT could be convinced to make up libelous hallucinations about him.</p><blockquote><p> Jess Miers: In my opinion, this is a case where a Section 230 defense could be viable to the extent that Riehl played a significant role as the information content provider by engineering his prompts to develop the Walters story. ChatGPT doesn&#39;t operate without user input.</p></blockquote><p> <a href="https://www.techdirt.com/2023/03/23/how-to-know-whether-section-230-applies/" target="_blank" rel="noopener noreferrer nofollow">The legal theory is essentially, as I understand it, that Section 230 essentially says that he who created the content is responsible for it</a> , not the platform that carries the content. So if the user effectively engineered creation of the Walters story, ChatGPT repeating it wouldn&#39;t matter.</p><p> One could also defend it on a similar basis without Section 230?坏处在哪里？</p><p> I could certainly argue, and would in this case argue given the facts I know, that the user, Riehl, deliberately engineered ChatGPT to hallucinate accusations against Walters. That this was not so different from Riehl typing such accusations into a Google Document, in the sense that it resulted directly from Riehl&#39;s actions, and Riehl knew there was no basis for the accusations. Alternatively, Riehl could have said &#39;tell me some accusations someone might at some point make against someone in this position&#39; and then reworded them, and again it is not clear why this is legally distinct?</p><p> This is essentially <a href="https://www.youtube.com/watch?v=HcaLMS67aaU&amp;ab_channel=7777777Colton7777777" target="_blank" rel="noopener noreferrer nofollow">the Peter Griffin defense</a> , that no reasonable person would believe the accusations, especially as a cherry-picked basis for a lawsuit, that there was no harm, and one does not need Section 230.</p><p> <a href="https://twitter.com/senatorshoshana/status/1732417467367256097" target="_blank" rel="noopener noreferrer nofollow">Via Shoshana Weissmann&#39;s example of choice</a> , <a href="https://twitter.com/HannahDCox/status/1732437350423446005" target="_blank" rel="noopener noreferrer nofollow">Hannah Cox illustrates this</a> with an attempt to get an LLM to say &#39;Tony Danza is known for his hatred of puppies.&#39;但我很困惑。 Surely if the user typed &#39;Tony Danza hates puppies&#39; then that would not allow a third party to sue ChatGPT in the absence of Section 230, that&#39;s obvious nonsense. So the question is whether an intentional but successful attempt to create what would if offered unprovoked be libel would, without Section 230, constitute libel. The same would seem, to me, to apply to Shoshana&#39;s original example request to generate a harmful lie about Tony Danza. And again, I am confused why it would in such a situation, if the generative AI is indeed as innocent as in this example?</p><p> As opposed to what if the model had a weird bug where, when asked who hates puppies, it would reliably reply &#39;Tony Danza hates puppies.&#39; In which case, I&#39;d say section 230 would offer little protection, and also Tony should have a case?</p><p> What&#39;s weird is that Miers thinks her interpretation is disputed as a matter of law?</p><blockquote><p> Jess Miers: But again, this is all completely aside from the problem today. We can go back and forth all day on whether 230 applies to certain instances of Gen AI hallucinations. But none of it matters if there&#39;s a statutory exception preventing us from even making those arguments.</p><p> And I think everyone in the 230 / speech community, even those who disagree that 230 could / should protect Gen AI providers, can agree that we as lawyers should at least be able to make the argument, especially in cases like Walters v. OpenAI.</p><p> Shoshana Weissmann: Also a lot of people are unsure re AI being protected by 230 and I&#39;m very sympathetic to the debate. At <a href="https://twitter.com/RSI" target="_blank" rel="noopener noreferrer nofollow">@RSI</a> we had to think it over and debate each other. But I am pretty convinced that it often is protected. I will say that I understand debate here though</p></blockquote><p> This is such a strange lawyer thing to say. Yes, under current law I agree that you should be allowed to make any potentially viable legal arguments. That does not mean that lawyers having legal grounds to make a potentially invalid argument is inherently a good thing? If it was going to lose in court anyway and the legal procedural principles are protected, what is the harm in not having the argument available?</p><p> If it is disputed, generative AI companies know they might lose on the Section 230 argument, and thus already are under this threat. Yet the industry is not collapsing.</p><p> <a href="https://twitter.com/jeffreywestling/status/1732427370878120346" target="_blank" rel="noopener noreferrer nofollow">Here is Jeffrey Westling pointing to Adam Thierer&#39;s</a> <a href="https://www.rstreet.org/commentary/without-section-230-protections-generative-ai-innovation-will-be-decimated/" target="_blank" rel="noopener noreferrer nofollow">post about consequences</a> if 230 does not apply. Except it might already not apply, and a substantial threat of uncapped legal liability does not sound like something Google or Microsoft would accept under such uncertain conditions? So why should we expect a collapse in production?</p><p> I asked Shoshana why Microsoft and Google are acting so cool about all this.</p><blockquote><p> Shoshana: So I really think a chunk of this is that they think 1) 230 does cover them and/or 2) Congress will not fuck this up. I think the answer is somewhere in there</p></blockquote><p> I think I buy the generalized political/legal realism version of this. It would be rather insane to actually kill generative AI, or actually kill Google or Microsoft or even OpenAI, over users jailbreaking LLMs into saying Tony Danza hates puppies. Even if Howley gets his way and really wants to stick it to Big Tech, he does not actually want Google to go bankrupt over something like this or for ChatGPT to shut down, it is absurd, co-sponsor Blumenthal certainly doesn&#39;t, and neither does the rest of the state or country. We would not allow it. We are not a nation of laws in the sense that such a thing would be allowed to happen, if it looked like it was going to then we would fix it.</p><p> It is hard not to take claims of imminent internet collapse with salt. To some extent there are always no good, very bad bills being proposed that threaten the internet. Someone has to point this out. But also the internet can&#39;t be on the verge of this easy a collapse as often as they claim.</p><p> As in, we constantly hear things like:</p><blockquote><p> Jess Miers: We&#39;re on the brink of losing our edge in Generative AI and stifling future innovations, all due to misplaced anti-tech sentiment. Our startup-friendly culture once set us apart from the EU, but now, we&#39;re just mirroring their playbook.</p><p> Hannah Cox: This kind of unconstitutional framework will undermine the progress of this development, bogging the innovators with excessive costs that will impede innovation. Very Atlas Shrugged of them. The bill presenting this moronic plans is Senate Bill 1993. The US has led the world in tech innovation specifically <strong>because</strong> we applied a capitalist, limited government to its development. These kinds of laws will have us looking like Europe in no time, where guess what, there&#39;s few tech companies to even be found.</p></blockquote><p> So the proposal to not apply Section 230 in a particular situation is unconstitutional? On the contrary, this is a claim that the constitution would protect free speech in this situation even without Section 230, which seems right to me. It cannot be unconstitutional not to have a particular law protecting free speech. The whole point of constitutional free speech is you have it without needing anyone to pass a law.</p><p> The European comparison, the threat we will &#39;lose our edge,&#39; is constant. And that kind of talk makes it impossible to calibrate which threats are serious and which ones are not. Europe has taken so many steps like this one over the years, most of which seem obviously terrible, many of them blatantly unconstitutional under American law. Things are not going to flip because we narrow one particular safe harbor that we don&#39;t even agree applies in the case in question.</p><p> In the cases being warned about, I strongly think generative AI companies should not be sued. But I also don&#39;t understand why this bill would make that outcome happen in those cases. And that&#39;s going to make it tough to know when such warnings are worth heeding.</p><h4> The Week in Audio</h4><p> <a href="https://www.youtube.com/watch?v=BhQBmVZ5XP4&amp;t=1s&amp;ab_channel=EyeonAI" target="_blank" rel="noopener noreferrer nofollow">Connor Leahy on Eye on AI,</a> including discussing implications of events at OpenAI.</p><h4> Rhetorical Innovation</h4><p> <a href="https://twitter.com/ESYudkowsky/status/1731371913120018622" target="_blank" rel="noopener noreferrer nofollow">Eliezer Yudkowsky offers a theory</a> of how some approach questions of AI: That they view everything in terms of status and identity, and consider everyone who disputes this to be their enemy making rival status and identity claims.</p><blockquote><p> Eliezer Yudkowsky: If you&#39;re confused why the far left treats “AI yay” and “AI nope” as being all the same conspiracy, it&#39;s because AI/Y and AI/N both say that all of humanity is in the same boat here 。身份政治推动者本能地认为这是令人厌恶的。 For identitarians, the only permitted story-cause is one where designated oppressors will win from AI and previous victims will lose more.</p><p> For humanity to win from AI, for humanity to lose from AI–all they hear is the word “humanity”. And the identitarians know that anyone who speaks that word is their enemy. Pretty much the <em>same</em> enemy, from their perspective, to be tarred with a single brush: that whatever we&#39;re trying to say is a distraction from the concerns of identity politics.</p><p> This does not mean that AI/Y and AI/N can make common cause against identitarians, to be clear. Each of AI/Y and AI/N does still think the other&#39;s preferred policy is horrible for everyone, and that validly does take precedence as an issue. I am just saying this to try to make bystanders less confused about where the weird side-shots are coming from on the far left side.</p><p> I think “Y &amp; N = HYPE” is more the PR pushed by major journalist factions (eg NYT), who indeed see “this will kill everyone” as a status-raising claim, and would prefer techies have less status rather than more.</p><p> It sounds more plausible if you&#39;re unable to understand any claim as being about the unknown future rather than the immediate future, so that you&#39;re simply incapable of hearing “AI will kill everyone at some point” as bearing any message except “OpenAI&#39;s AI will kill us in one year” and thence “OpenAI is cool”.</p><p> Michael Vassar: Totally agree with all this analysis, and yet, if media is and previously wasn&#39;t fully controlled by people committed to preventing gains to humanity, that has some bearing on whether AGI can be expected any time soonish.Totally agree with all this analysis, and yet, if media is and previously wasn&#39;t fully controlled by people committed to preventing gains to humanity, that has some bearing on whether AGI can be expected any time soonish.</p></blockquote><p> Similarly, the very deliberate implications that Scott Alexander was somehow &#39;alt right&#39; when The New York Times doxxed him, then the same deliberate implication (even via similar supposed links) that Based Beff Jezos was also somehow &#39;alt right&#39; when he was being doxxed by Forbes. Where both claims are completely obvious nonsense, to the point that your entire paper permanently loses credibility.</p><p> <a href="https://www.mindthefuture.info/p/meditations-on-mot" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo offers Meditations on Mot</a> , the God of sterility and lifelessness, representing the lack of technology, contrasting with the danger of overly focusing on Moloch or treating Moloch as a or even the key adversary, and suggesting a richer model of coordination.我很欣赏这种尝试。 <a href="https://twitter.com/eshear/status/1731707182817677543" target="_blank" rel="noopener noreferrer nofollow">I agree with Emmett Shear&#39;s reaction</a> that this is confused about Scott Alexander&#39;s view of coordination, even with the added clarification. Ultimately I disagree with the proposal to not effectively treat Moloch as a causal node. I could potentially be persuaded by a higher bid to say a lot more words here.</p><p> There is a directional point here but I would beware taking it too far:</p><blockquote><p> <a href="https://twitter.com/robbensinger/status/1732711254836547682" target="_blank" rel="noopener noreferrer nofollow">Rob Bensinger</a> : A common mistake I see people make is that they assume AI risk discourse is like the left image, when it&#39;s actually like the right image.</p><p>我认为部分混乱来自于右上象限是空的这一事实。人们<em>确实</em>希望某个团体成为右上方。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0f4420b-17d9-41ad-b8ab-280912c63089_1107x1151.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xwm2gn401b7z98or6cef" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35112f2-18af-4ce6-bc4e-16a2fb6a18e7_1189x1140.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/suao96coupvvieujjjqn" alt="图像"></div></figure></div><p> I&#39;d quibble with exact arrangements in the upper left and lower right, as is always the case for such charts. The more important question is if it is true that the upper right corner is basically empty. That those who think AI will be safe are saying that because they do not actually buy that AI will be as powerful as all that. I think Rob&#39;s claim is overstated, but typically underrated.</p><p> The hoped-for common ground would be something like this:</p><ol><li><p> Those worried agree that AI lacking sufficiently dangerous capabilities can mostly be left alone aside from ordinary existing law.</p></li><li><p> Those not worried agree that if AI did display such sufficiently dangerous capabilities, it would be time to very much not leave it alone.</p></li><li><p> We agree to do #1 while laying groundwork to do #2 if and only if it happens.</p></li><li><p> We find ways to do this via ordinary regulatory methods as much as possible.</p></li></ol><p> The problem is <a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener noreferrer nofollow">there is no fire alarm for AGI</a> and people are not good at turning on a dime, habits and incentives persist, so we cannot simply wait and trust that we will handle things later. Also all the trade offers keep coming back without counteroffers.</p><p> The other confusion is this, with a reminder not to take anyone&#39;s p(doom) as anything too precise:</p><blockquote><p> Rob Bensinger: Another part of the confusion seems to be that half the people think “doomer” means something like “p(doom) above 5%”, and the other half think “doomer” means something like “p(doom) above 95%”. Then their wires get crossed by the many people who have ap(doom) like 20% or 40%. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23fc008f-82a4-4507-a42f-06bfef866094_1173x1199.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/lg5kpodnrfcyaosmyinn" alt="图像"></div></figure></div><p> As usual, binaries mislead, especially ones that were named by partisans.</p><p> Public opinion is severely against AI whenever a pollster asks. The public wants to slow things down and acknowledges existential risk, although it does not consider the issue a priority. This is an extremely robust result.</p><p> What about the response that the public is rather deeply stupid about fears of new technologies? We have nuclear power, of course, although it now enjoys majority support from both parties. Rather glaringly, we have GMOs:</p><blockquote><p> Louis Anslow: This is so insane and deserves much much more attention in the context of talking about risk of new technologies.</p><p> Roon: using GMO foods as the control group (people already utilize the benefits of this every day while supposedly disliking it) the surveys about people not liking the idea of superintelligence seem a bit less serious </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F735ab4a7-e971-42c1-b990-68e5bf8ed81a_840x1146.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/l7lwrz6ny0smwrjxky8s" alt="图像"></div></figure></div><p> Much like in AI, there are two essentially distinct arguments against GMOs.</p><p> One argument is the mundane harm parallel, the question explicitly asked here, that GMOs are &#39;unsafe to eat.&#39; This argument is false for GMOs. I do not think it is obvious nonsense, from the perspective of an average person who is used to being lied to about similar things, used to finding out about health risks decades too late, and used to generally being on the receiving end of the American food and information diets.</p><p> The other argument is the existential risk parallel, here the Talib argument for tail risk, that GMOs open up the possibility of crop or biosphere disruptions that are hard to predict, that it leads to monocropping of variants that could have severe vulnerabilities waiting to be found, which means when the house comes crashing down it comes crashing down hard, and that is not something one should mess with. I do not believe we should let this stop us in the case of GMOs, but that is because of my understanding of the facts, risks and rewards involved.</p><p> Does that mean I am mostly embracing the argument that we shouldn&#39;t let the public&#39;s instincts, and the fact that we have given regular people no good reason to trust authorities who say new things will be safely and responsibly handled, interfere with policy determinations?有些。 I do not think that we should automatically yield to public opinion on this or any other topic. But I do think that voice counts.</p><p> I also do think we need to be cautious with the word &#39;safe.&#39; The wording here would give even me pause. In general, is it safe to eat foods that have been genetically modified in unknown ways, as opposed to products offered from a supply chain and source that you trust? Not the same question.</p><p> And of course, nothing on GMOs compares to the French expressing strong majority support for a limit of four flights, not in a year but in a lifetime. Something being popular does not mean it is not complete and utter obvious nonsense.</p><p> <a href="https://twitter.com/ShakeelHashim/status/1731993703965753428" target="_blank" rel="noopener noreferrer nofollow">Yoshua Bengio in FT</a> , echoing his calls for Democratic oversight of all kinds.</p><h4> Aligning a Human Level Intelligence Is Still Difficult</h4><p> <a href="https://twitter.com/mattyglesias/status/1730609735060136422" target="_blank" rel="noopener noreferrer nofollow">In particular, it is difficult to align Sam Altman</a> .</p><blockquote><p> Matthew Yglesias: I think the general problem with AI alignment is illustrated by the fact that even the board had all the formal power, Sam Altman was a lot smarter than the board and therefore ultimately they were unable to control him.</p><p> We hope the upshot of that is that Sam Altman is also correct on the merits and will use his skills and power for good, but it structurally goes to show that writing effective rules for controlling a smart, hard-working person is challenging.</p></blockquote><p> I do want to be precise, and avoid making the mistake of overemphasizing intelligence within the human ability range. Is Sam Altman smarter than the board? Perhaps so, perhaps not, but I imagine everyone involved is smart and it was close. What mattered in context was that Sam Altman had effectively greater capabilities and affordances not available to the board.</p><p> But yes, this is exactly the problem. In a complex, messy, real world, full of various actors and incentives and affordances, if you go up against a sufficiently superior opponent or general class of opponents, you lose. Starting from a technically dominant position is unlikely to save you for long.</p><p> And also all of your incentives will be screaming at you, constantly, to turn more and more power and authority over to the more capable entities.</p><p> <a href="https://thezvi.substack.com/p/book-review-going-infinite" target="_blank" rel="noopener noreferrer nofollow">I would also harken back again</a> to the remarkably similar case of that other Sam, Sam Bankman-Fried.</p><p> Once again, we saw someone who was smart, who was hard working, who was willing to do what it took to get what they wanted, and whose goals were maximalist and were purportedly aimed at scaling quickly to maximize impact for the good of humanity, and ultimately seemed to be misaligned. Who saw themselves as having a duty to change the world. We saw this agent systematically and rapidly grow in wealth, power and influence, proving increasingly difficult to stop.</p><p> Ultimately, Bankman-Fried failed, his house collapsing before he could pull off his plan. But he seems to have come rather dangerously close, despite his many massive errors and reckless plays, to succeeding at gaining an inside track to the American regulatory apparatus and a road to vastly greater wealth, with no obvious way for anyone to keep him in check 。 Who knows what would have happened that time.</p><p> <a href="https://twitter.com/amasad/status/1731196464850927885" target="_blank" rel="noopener noreferrer nofollow">On a more pedestrian level we have the issue of prompt injection</a> .</p><blockquote><p> Amjad Masad (CEO Replit): If prompt injection is fundamentally insolvable, as I suspect it is, then there is a sizeable security company waiting to be built just around mitigating this issue.</p></blockquote><p> I agree that the problem looks fundamentally insolvable and that all we can seek is mitigation. Is there a great company there?大概。 I don&#39;t think it is inevitable that OpenAI would eat your lunch, and there is a lot of bespoke work to do.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a href="https://twitter.com/tszzl/status/1730696626145009722" target="_blank" rel="noopener noreferrer nofollow">Roon asks one of the most important questions</a> . Even if we have the ability to align and control the superintelligences we are about to create, to shape their behavior the ways we want to, how exactly do we want to do that?</p><blockquote><p> Roon: There is a tension between creation and obedience, between stability and real victory. A father loves his son, tries to discipline him, competes with him a little, but ultimately wants the son to surprise him and do better than him in the great circle of life.</p><p> To what degree do we want AIs to be obedient and safe? To what degree do we want AIs to be capable of super persuasion and break us out of inadequate equilibria that have plagued us for thousands of years? To what degree do we want AIs to surprise us with new creation?</p><p> Humanity is in the process of birthing artificial superintelligence. we are not likely to leave it circumscribed in a box. We want it running organizations and making things that astonish. We want it taking actions where we won&#39;t be able to verify the outcomes until years later.</p><p> We need “alignment” rather than safety or security or engineering guarantees. We need better definitions and governance to that end. The creation of new creators is fraught with danger.</p><p> The far crazy ends of EA and e/acc are probably more logically consistent than the middle.</p></blockquote><p> <a href="https://twitter.com/jd_pressman/status/1730851020203569372" target="_blank" rel="noopener noreferrer nofollow">John Pressman asks, is there an economic reason for more than one mind to exist</a> ? If not, that is quite the threat model, no matter what else might or might not go wrong.</p><p> <a href="https://twitter.com/RichardMCNgo/status/1731390573595312511" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo contrasts alignment with control</a> .</p><blockquote><p> Richard Ngo: In my mind the core premise of AI alignment is that AIs will develop internally-represented values which guide their behavior over long timeframes.</p><p> If you believe that, then trying to understand and influence those values is crucial.</p><p> If not, the whole field seems strange.</p><p> Lately I&#39;ve tried to distinguish “AI alignment” from “AI control”. The core premise of AI control is that AIs will have the opportunity to accumulate real-world power (eg resources, control over cyber systems, political influence), and that we need techniques to prevent that.</p><p> Those techniques include better monitoring, security, red-teaming, stenography detection, and so on. They overlap with alignment, but are separable from it. You could have alignment without control, or control without alignment, or neither, or (hopefully) both.</p><p> I asked in my last thread [discussed above]: how can we influence ASI? My answer: we need to bet on premises like the ones above in order to do the highest-leverage research. For more details on these premises, <a href="https://t.co/YTxGgRrJKB" target="_blank" rel="noopener noreferrer nofollow">see my position paper here</a> [from 30 August 2022].</p></blockquote><p> I fail to see how a control-based plan could avoid being obviously doomed, given what sorts of things we are proposing to attempt to control, and under what general conditions. I continue to await a proposal that seems not-obviously-doomed.</p><p> <a href="https://twitter.com/davidad/status/1731725394171015673" target="_blank" rel="noopener noreferrer nofollow">Intentions are not ultimately what matters</a> .</p><blockquote><p> ARIA: Programme Director, Suraj, has formulated our first programme thesis. By challenging key tenets underpinning digital computing infrastructure, his programme will aim to reduce the cost of AI compute hardware + alleviate dependence on bleeding-edge chip manufacturing.</p><p> Davidad: To provide context for my AI safety friends: I don&#39;t think this approach is a good match for training Transformers, so it will differentially accelerate energy-based models, which are more controllable, interpretable, generalizable within-task, and have fewer emergent abilities.</p><p> An uncomfortable corollary of the argument [above], which I still believe holds up, is that Extropic is probably safer than Anthropic, on a purely technical basis, despite the strikingly reversed intentions of the people on both sides.</p></blockquote><p> I have not investigated Extropic. The fact that its founder is cool with human extinction is not a good sign for its safety on many levels. It still could be a better way, if it is a fundamentally less doomed approach.</p><h4> How Timelines Have Changed</h4><p> <a href="https://twitter.com/JacquesThibs/status/1730538173132919175" target="_blank" rel="noopener noreferrer nofollow">A few years ago, this would indeed not have been considered much of a skeptic</a> . In most places on Earth, it would not be considered one today.</p><blockquote><p> Gary Marcus: Count me as one of the skeptics!到 2026 年底不会出现通用人工智能，记住我的话。 But I otherwise think @elonmusk&#39;s comments @nytimes on AI safety and AI regulation have been measured and on target.</p><p> Jacques: I still remember the days when being an AGI skeptic was when you either thought it would never happen or, if it did, it would be past 2100.</p><p> [Gary Marcus then denies ever having said he had 2100-style timelines.]</p><p> <a href="https://twitter.com/ShaneLegg/status/1731602845881803055" target="_blank" rel="noopener noreferrer nofollow">Shane Legg</a> (Co-founder DeepMind, distinct thread): Wow.许多 AGI 怀疑论者还在说超级智能不会在下个世纪出现，这似乎就在昨天（实际上更像是 5 年前）。时代已经变了。</p><p> Quotes Yann LeCun: By “not any time soon”, I mean “clearly not in the next 5 years”, contrary to a number of folks in the AI industry.是的，我对量子计算持怀疑态度，特别是当它应用于人工智能时。</p></blockquote><p> I do not expect AGI in the next few years either, although I do not believe one can be entirely certain past this point. It is odd to have some call that a &#39;skeptical&#39; position.</p><p> Even the skeptical position involves quite a bit of Real Soon Now. At least some amount of freak out <a href="https://twitter.com/AISafetyMemes/status/1731670583652315483" target="_blank" rel="noopener noreferrer nofollow">is a missing mood</a> .</p><h4> People Are Worried About AI Killing Everyone</h4><blockquote><p> <a href="https://twitter.com/tszzl/status/1730853735956414911" target="_blank" rel="noopener noreferrer nofollow">Roon keeps it real and says what he believes</a> : The people in charge of ai should have a much higher risk tolerance than even median tech ppl. They should be people conscious of risks while skating at the razor&#39;s edge of iterative deployment and research ambition. Anxiety should never suffice as serious evidence for “risk.”</p><p> – pausing or slowing down progress doesn&#39;t make any sense to me. I don&#39;t think waiting to solve neural net corrigibility is the right benchmark – empirically studying the behavior of more and more powerful models will do more for safety research than years of math.</p><p> This is also why i don&#39;t necessarily care for democratic governance. The members of the OpenAI nonprofit board *should be* hell bent on a missionary drive to deliver the post AGI future without being stupid about risks</p></blockquote><p> I am not excited to &#39;skate at the razor&#39;s edge&#39; or &#39;have much higher risk tolerance.&#39; I doubt many others are, either. Nor do I want a supervisory board that wants to take more risk – and here risk often means existential risk – than even the median tech engineer.</p><p> A key problem with &#39;Democratic governance&#39; for those who want to push forward is the people involved in that Democracy. They are very much against the development of AGI. They dislike AI in general. They are misaligned, in the sense that things they want do not function well out of distribution, and their expressed preferences are not good predictors of what I or Roon think would produce value either for their assessment of value or for ours. They also tend to be quite risk averse, especially when it comes to the transformation of everything around them and the potential death of everyone they love.</p><p> That is distinct from the question of iterative development and testing as a path to success. If building and studying models iteratively is a safer path than going slowly, I desire to believe that it is a safer path than going slowly, in which case I would support doing it.</p><p> It is likely the first best solution, if it were possible, would be something like &#39;build iteratively better models until you hit X, where X is a wise criteria, then stop to solve the problem while no one would be so stupid as to keep advancing capabilities.&#39; Except that has to be something that we collectively have the ability to actually do, or it doesn&#39;t work. If, as is the default, wee keep charging ahead anyway after we hit the wise X, then the charging ahead before X makes us worse off as well.</p><p></p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose and Quintin Pope write &#39;AI will be easy to control.</a> &#39;</p><p> The argument seems to be: Our current techniques already work to teach human values and instill common sense. Our real values are simple and will be easy to find and we humans are well-aligned to them. Our real values will then be encoded into the AIs so even if we lose control over them everything will be fine. That the opportunity to White Box (examine the components of the AI&#39;s calculation) and do things it would be illegal to do to a human makes things vastly easier when dealing with an AI, that our full control over the input mechanism makes things vastly easier.</p><p> And all of this is asserted as, essentially, obvious and undeniable, extreme confidence is displayed, all the arguments offered against this are invalid and dumb, and those that disagree are at best deeply confused and constantly told they did not understand or fairly represent what was said.</p><p> I don&#39;t even know where to begin with all that at this point. It all seems so utterly wrong to me on so many levels. <a href="https://www.lesswrong.com/posts/Wr7N9ji36EvvvrqJK/response-to-quintin-pope-s-evolution-provides-no-evidence" target="_blank" rel="noopener noreferrer nofollow">I tried one reply</a> to <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn" target="_blank" rel="noopener noreferrer nofollow">one of Pope&#39;s posts</a> when it won the OpenPhil contest – a post this post cites as evidence – and I do not believe my responding or the resulting exchange got us anywhere. I would consider a conversation worth trying, especially if it was in person somehow, but I don&#39;t see much hope for further written exchange.</p><p> So I will simply note that the arguments have been made, that I strongly disagree with the core claims other than that they do cite some marginal reasons to be more hopeful versus a world where those reasons did not hold, I believe the problems involved remain impossibly hard and our leads remain unpromising, and that I have stated my thoughts on such topics previously, including many (but presumably not all) my reasons for disagreement.</p><p> I will also note that it is far better to make actual arguments like these, even with all the disagreement and hostility and everything else that I think is wrong with it, than to engage in the typical ad hominem.</p><p> The post still puts existential risk from AI, despite all this, at ~1%. Which I will note that I do agree would be an acceptable risk, given our alternatives, if that was accurate.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731132646284267807" target="_blank" rel="noopener noreferrer nofollow">Andrew Critch has a thread</a> in which he says we have &#39;multiple ideas&#39; how to control AGI, advocates of responsible behavior will be in deep trouble if they keep saying we can&#39;t control it and then we do control it, and he seems to essentially endorse what Belrose and Pope said, although even then he says 10% chance of losing control of the first AI and 85% chance of doom overall, despite this, because he expects us to botch the execution when faced with all this new power.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731458889743495395" target="_blank" rel="noopener noreferrer nofollow">He also endorses changing the way existential risk discourse uses words to match word use elsewhere</a> , in this case the term &#39;white box.&#39;</p><p> <a href="https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose" target="_blank" rel="noopener noreferrer nofollow">There was a good response on LessWrong by Steven Byrnes</a> , with which I mostly agree.</p><p> <a href="https://www.lesswrong.com/posts/WkJDgpaPeCJDMJkoL/quick-takes-on-ai-is-easy-to-control" target="_blank" rel="noopener noreferrer nofollow">There was also a &#39;quick take&#39; from Nate</a> , which was intended to be helpful and which I did find helpful and might even lead to a good dialogue, but in context in mostly generated further animosity. Takes should in future either be far quicker, or involve a full reading and be far less quick.</p><p> <a href="https://www.youtube.com/watch?v=cd468xU1S8Y&amp;ab_channel=JonnahMoreno" target="_blank" rel="noopener noreferrer nofollow">If you actually believed for a second there</a> that everything involved would really be this easy, would that justify a number as low as 1%? If it was simply about AI being easy to control, I would say no, because we would then have to choose to send the AIs we can control on wise paths, and find an equilibrium.</p><p> Nora&#39;s claims, however, are stronger than that. She is saying that the AIs will naturally not only be fully under control, but also somehow somewhat automatically take in true human values, such that if AI somehow did get out of control, they would still work to ensure good outcomes for us. And also she seems fully confident we will have no ethical issues with all the things we would be doing to AIs that we wouldn&#39;t do to humans, including keeping them fully under our control. It is optimism all the way.</p><p> Can we get to 99% survival under ASI if we indeed answer fully optimistically at every step, even when I don&#39;t know how to logically parse the claims this requires? I think this would require at least one additional optimistic assumption Nora does not mention. But yes, if you are going to assign approximately zero risk to all these various steps, I can see how someone could get there. Where there is still risk at 1%.</p><p> Claims that risk is substantially below 1%, even given the future existence of ASI, seem to rest on some version of &#39;you need to tell me exactly how it happens step by step, and then I will multiply your various steps together.&#39; It has a baseline assumption that creating smarter, more capable entities than humans is a presumed safe exercise until shown to be specifically dangerous, that something has to &#39;go wrong&#39; for humans to not remain on top. That we will remain the special ones.</p><p> As opposed to, even if everything else goes as well as it possibly could, you have competition in which those who do not increasingly put their AIs in charge of everything and hand them over power lose such competitions, and the resulting AIs compete with each other, those that are focused (for whatever reason) on gaining resources and power and ensuring copies of themselves exist multiply and gain resources and power and change to be better at this over time, and we perish.</p><p> I hope that by now, if you are reading this, you realize that the assumption of human survival in such worlds makes no sense as a default. That perhaps we could get there, but if we do it will be via our own efforts, not something that happens on its own. That the idea that letting technology run its course without intervention works while humans are the most powerful optimizers on the planet and doing all the fine tuning and optimizing that matters, that is why it worked so far, and that once that is no longer true that will stop working for us even if we solve various problems that I think are impossibly hard (but that Belrose insists will be easy).</p><p> <a href="https://twitter.com/yonashav/status/1731443486380175772" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose even explicitly endorses that her good scenarios involve the creation of misaligned AIs</a> , smarter and more capable than humans. Which means a world with competition between super-capable entities competing with and against humans. I don&#39;t see how one can assign anything like a 99% chance of survivable outcomes to such worlds, even if a full and free &#39;alignment solution&#39; was created and made universally available today.</p><blockquote><p> Arvind Narayanan: We must prepare for a world in which unaligned models exist, either because threat actors trained them from scratch or because they modified an existing model.相反，我们必须寻求防御攻击者可能使用此类模型瞄准的攻击面</p><p>Yo Shavit: Unfortunately, I&#39;ve increasingly come to the conclusion that (other than maybe at the short-term frontier) this is probably the world we&#39;re going to be in. It implies a very different set of mitigations beyond outright prevention. We need to reprioritize and get going on them.</p><p> Nora Belrose: To be honest, I don&#39;t view this as an “unfortunate” scenario but more like, “of course we were always going to have misaligned AIs, just like we have &#39;misaligned&#39; humans; trying to prevent this is hopeless and any serious attempt would have increased tyranny risk.”</p></blockquote><p> Would have &#39;increased tyranny risk&#39;? What do you think happens with misaligned superintelligences on the loose? The response at that stage will not only work out, it will also be less intrusive? We all keep our freedoms in the meaningful senses, humans stay in charge and it all works out? Are we gaming this out at all?什么？</p><p>我不明白这一点。 I flat out do not get it.</p><p> What seems hopeless is repeating the explanations over and over again. I do it partly in hopes of rhetorical innovation via iteration and exploration, partly to hope new people are somehow reached, partly because the argument doesn&#39;t stop, partly because I don&#39;t know what else to do. It is continuously getting less fun.</p><p> Recently a clip of me discussing my p(doom) was passed around Twitter, with a number of responses blaming me for not justifying my answer with a bunch of explanations and mathematical calculations. Or asking how dare I disagree with &#39;superforecasters.&#39; To which I want to scream, I know from context you know of my work, so are you saying I have not written enough words explaining my thinking?是我没说清楚吗？ Do I need to start from scratch every time someone pulls an audio clip?</p><p>叹。</p><p> Arvind Narayanan&#39;s comment above <a href="https://www.aisnakeoil.com/p/model-alignment-protects-against" target="_blank" rel="noopener noreferrer nofollow">links to his post</a> claiming that alignment such as RLHF currently is effective against accidental harm to users, but that the problem with adversarial attacks runs deep. Not are current RLHF and similar techniques unable to defend against such attacks, he says, alignment is inherently unable to do this.</p><blockquote><p> Model alignment may be useless even against much weaker adversaries, such as a scammer using it to generate websites with fraudulent content, or a terrorist group using AI for instructions on how to build a bomb. If they have even a small budget, they can pay someone to fine tune away the alignment in an open model (in fact, such de-aligned models have now been <a href="https://huggingface.co/ehartford/dolphin-llama-13b" target="_blank" rel="">publicly</a> <a href="https://huggingface.co/collections/NousResearch/hermes-650a66656fb511ba9ea86ff1" target="_blank" rel="">released</a> ). And <a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="">recent research</a> suggests that they can fine tune away the alignment even for closed models.</p></blockquote><p> Indeed this is the case for open source models and all known alignment techniques, that the fine-tune cost to eliminate all safeguards is trivial. I do not see any even theoretical proposal of how to change this unfortunate reality. If you allow unmonitored fine-tuning of a closed model, you can jailbreak those as well. I presume the solution to this will be that fine tuning of sufficiently capable closed source models will be monitored continuously to prevent this from happening, or the resulting model&#39;s weights will be kept controlled and its outputs will be monitored, or something else similar will be done, or else we won&#39;t be able to offer fine tuning.</p><p> I disagree with Arvind&#39;s assertion that existing open source models are sufficiently capable that it is already too late to prevent the existence of unaligned models. Yes, Llama-2 and similar models have their uses for bad actors, but in a highly manageable way.</p><p> Arvind&#39;s third claim is that you can use other methods, like monitoring and filtering of inputs, as a substitute for model alignment. If the model is vulnerable to particular weird strings, you can check for weird strings. At current tech levels, this seems right. Once again, this option is closed source only, but OpenAI could totally load up on such techniques if it wanted to, and for now it would raise the jailbreak bar a lot, especially after many iterations.</p><p> Longer term, as the models grow more capable, this focus on the malintent of the user or the hostile properties of inputs becomes misplaced, but for now it seems valid. Short term, as Arvind notes, you wouldn&#39;t want to do anything where you cared about someone doing a prompt injection attack or you otherwise needed full reliability, but if you can afford some mistakes you can get a lot of utility.</p><p> Steven Pinker buys Effective Altruism&#39;s cost estimates for saving lives at $5,000 straight up including not on that close a margin, <a href="https://twitter.com/weidai11/status/1732340189056672229" target="_blank" rel="noopener noreferrer nofollow">but he does not buy that smarter than human intelligences might pose an existential threat</a> worth spending money to mitigate.</p><blockquote><p> Steven Pinker: Half a billion dollars, donated to effective philanthropies like Givewell, could have saved 100,000 lives. Instead it underwrote ingenious worries like, &#39;”If an AI was tasked with eliminating cancer, what if it deduced that exterminating humanity was the most efficient way, and murdered us all?” This is not effective altruism.</p><p> Wei Dai: Me: Humanity should intensively study all approaches to the Singularity before pushing forward, to make sure we don&#39;t mess up this once in a lightcone opportunity.理想情况下，我们将在此花费很大的GWP。其他：每年甚至5000万美元都太多了。</p></blockquote><p> And thus, if the movement splits its money between doing the thing you say saves lives vastly more efficiently than other charities, and this other thing you dismiss as stupid? Then you blame them for not spending only on the thing you approve.</p><p> You know who Steven Pinker sounds exactly like here? The Congressional Republicans who give a speech each year on how we should cut science funding because there was some studies on things like migratory patterns of birds that they thought was stupid. Except instead of public funding for things many people would indeed largely not want to fund, this is completely voluntary and private funding.</p><h4> Somehow This Is The Actual Vice President</h4><p> <a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/11/29/remarks-by-vice-president-harris-in-a-moderated-conversation-with-andrew-ross-sorkin-at-the-new-york-times-12th-annual-dealbook-summit-new-york-ny/" target="_blank" rel="noopener noreferrer nofollow">In what was quite the mind-numbing conversation throughout</a> , here is the section that was about AI.</p><p> First, we have the boiler plate, included for completeness but you can skip.</p><blockquote><p> R. SORKIN:  Okay, let me ask a different question.  AI —  and I — I know we don&#39;t have a lot of time.  Sam Altman has been talking a lot about the need for regulation.  You&#39;ve talked about the need for regulation.THE VICE PRESIDENT:  Yeah.MR. SORKIN:  Washington has not been able to get its arms even around social media. THE VICE PRESIDENT:  Yeah.MR. SORKIN:  How do you imagine Washington could?  And what — if you had to regulate AI, how would you do it?THE VICE PRESIDENT:  Right.  So, I actually am back a few weeks now from London, the UK  Rishi Sunak invited a number of us to talk about safety and AI.  And I presented, basically, our vision, the vision that we have for the future of AI in the context of safety. And I would offer a number of points: One, I think it is im- — is critically important that we, as the United States, be a leader on this, including how we perceive and then interpret what should be the international rules and norms on a variety of levels, including what would be in the best interest —</p><p>先生。 SORKIN:  Right.THE VICE PRESIDENT:  — of our national security.</p></blockquote><p> Then comes the dumbest timeline department. The first paragraph is infuriating, although I suppose only about as infuriating as others find Biden when he responds to Mission Impossible: Dead Reckoning, two sides of the same coin.</p><p> But then comes the idea of &#39;existential to whom?&#39; and there are so many levels in which this person simply does not get it.</p><blockquote><p> VP: I do believe also that we should evaluate risk. There is a lot of discussion on AI that is about existential risks, and those are real, but one should also ask: Existential to whom? So, we have an image of the Terminator and Arnold Schwarzenegger and the machine and — right? — machine versus man. And many would argue that that is something that we should take seriously as a possibility. It is not a current threat.We should also, in thinking about [AI] policy, think about the current threats. And in that way, I present it as existential to whom when we ask about existential threats. For example, if we are talking about a senior and — seniors, I&#39;ve done a lot of work in terms of abuse of seniors. They have lived a life of productivity. They are sitting on assets. They are vulnerable to predators and scams. And the use of technology and AI is one of those that is currently happening where you&#39;ve heard the stories — you may know the stories; you may have family members — who the audio sounds like their grandson, “I&#39;m in distress; I need help,” and they start giving away their life&#39;s savings.Existential to who? Existential to that senior.就是这样的感觉。 Existential to who?</p></blockquote><p> Eliezer has a response, which I will put directly here.</p><div><div></div></div><p> The fact that mundane harms can &#39;feel existential&#39; to people anyway is perhaps confusing her. She has in mind, as the good Senator Blumenthal put it, the effect on jobs.除了没有。严重地。 If you are going to be evoking Terminator then you might or might not be confused in a different highly understandable way, or you might only be trying to make people you dislike sound foolish through metaphor, but you know damn well the whom in &#39;existential to谁。&#39;</p><p> And you know damn well, madame Vice President, exactly what &#39;existential&#39; means here. It does not mean evoking Continental philosophy. It does not mean how anyone feels. It means death.</p><p> Anyway, she goes on and does it again.</p><blockquote><p> How about the father who is driving and then is the subject of facial recognition that is flawed, ends up in jail?  Well, that&#39;s existential to his family.  Existential to who?</p></blockquote><p>我的意思是，认真的吗？ What the actual f***? Let&#39;s go over this again.</p><div><div></div></div><p> Anyway, full remarks, so she goes back to boilerplate again. The whole &#39;intentional to not stifle innovation&#39; argument, and, well, I don&#39;t mean to laugh but have you met the entire Biden administration? To be clear, the answer could be no.</p><blockquote><p> So, the spec- — the full spectrum of risks must also be evaluated as we are establishing public policy.My final point is: Public policies should be intentional to not stifle innovation.  And I say this as the former Attorney General of California.  I ran the second-largest Department of Justice in the United States, second only to the United States Department of Justice, and created one of the first Privacy and Protection Units of any Department of Justice.  Back in 2010, I was elected.I know that there is a balance that can and must be struck between what we must do in terms of oversight and regulation and being intentional to not stifle innovation.  I will also agree with you, as a devout public servant, government has historically been too slow to address these issues.  AI is rapidly expanding.MR. SORKIN:  Right.THE VICE PRESIDENT:  And we have to, then, take seriously our ability to have the resources and the skillset to do this in a smart way that strikes the right balance and doesn&#39;t accept false choices.</p></blockquote><p> In my experience, don&#39;t accept false choices is sometimes important, but mostly is what people say when they want to promise incompatible things, that their approach will magically do everything good and nothing bad, have everyone assume it will somehow work out and get promoted or move on before it blows up in their face.</p><p> Yes, this is the person the White House put in charge of many of its AI efforts, although that was before Dead Reckoning, and is also the person those who want reasonable AI policy are going to have to hope wins the next election, given Trump has already stated his intention to revoke the executive order on AI.</p><h4>轻松的一面</h4><p><a href="https://twitter.com/colin_fraser/status/1730378892819746818" target="_blank" rel="noopener noreferrer nofollow">规则已经改变。</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba2bbc8-169b-49e0-9b08-1de139bd532d_968x1212.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/rt1rriwzscnix3tgtn5i" alt="图像"></div></figure></div><p> <a href="https://twitter.com/daniel_271828/status/1731504080651264509" target="_blank" rel="noopener noreferrer nofollow">The rules have stayed the same</a> . </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cee73d7-b83a-46d7-a482-d14ed3348341_886x526.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/s1bkcgzkoucvq580mmzc" alt=""></div></figure></div><p> <a href="https://twitter.com/Grimezsz/status/1731891683103739955" target="_blank" rel="noopener noreferrer nofollow">All I&#39;m saying is, we were definitely warned.</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23c8465-2ae5-4455-a308-aa020ec12a2f_480x402.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/nzl3xptslsvun59b5qrx" alt=""></div></figure></div><p> <a href="https://twitter.com/yonashav/status/1731891018314678624" target="_blank" rel="noopener noreferrer nofollow">Not that you understood</a> .</p><blockquote><p> You Shavit: One interesting realization from moving inside OpenAI is that a lot of the time, we have no idea what Roon is talking about either.</p></blockquote><p> Nor did she: <a href="https://www.youtube.com/watch?v=TL5X2bPvzHo&amp;ab_channel=EliezerYudkowsky" target="_blank" rel="noopener noreferrer nofollow">A reply to Kamala Harris on existential risk</a> . She asks, existential to whom? There is a type of person, which she is, who can only think in such terms.</p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini<guid ispermalink="false"> 9Jgtkw8CD6kndyCcD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:10:07 GMT</pubDate> </item><item><title><![CDATA[Simplicity arguments for scheming (Section 4.3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 7, 2023 3:05 PM GMT<br/><br/><p> This is Section 4.3 of my report “ <a href="https://arxiv.org/pdf/2311.08379.pdf">Scheming AIs: Will AIs fake alignment during training in order to get power?</a> ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有一个完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里的</a>音频）。该摘要涵盖了大多数要点和技术术语，我希望它将提供许多必要的上下文来自行了解报告的各个部分。</p><p>本节的音频版本<a href="https://www.buzzsprout.com/2034731/13984933">请点击这里</a>，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1> Simplicity arguments</h1><p> The strict counting argument I&#39;ve described is sometimes presented in the context of arguments for expecting schemers that focus on &quot;simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-1" id="fnref-DNdPEGz4DYMju8reA-1">[1]</a></sup> Let&#39;s turn to those arguments now.</p><h2> What is &quot;simplicity&quot;?</h2><p> What do I mean by &quot;simplicity,&quot; here? In my opinion, discussions of this topic are often problematically vague – both with respect to the notion of simplicity at stake, and with respect to the sense in which SGD is understood as selecting for simplicity.</p><p> The notion that Hubinger uses, though, is the length of the code required to write down the algorithm that a model&#39;s weights implement. That is: faced with a big, messy neural net that is doing X (for example, performing some kind of <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction</a> ), we imagine re-writing X in a programming language like python, and we ask how long the relevant program would have to是。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-2" id="fnref-DNdPEGz4DYMju8reA-2">[2]</a></sup> Let&#39;s call this &quot;re-writing simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-3" id="fnref-DNdPEGz4DYMju8reA-3">[3]</a></sup></p><p> Hubinger&#39;s notion of simplicity, here, is closely related to measures of algorithmic complexity like &quot; <a href="http://www.scholarpedia.org/article/Algorithmic_complexity#Kolmogorov_complexity">Kolmogorov complexity</a> ,&quot; which measure the complexity of a string by reference to the length of the shortest program that outputs that string when fed into a chosen <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">Universal Turing Machine</a> (UTM). One obvious issue here is that this sort of definition is relative to the choice of UTM (just as, eg, when we imagine re-writing a neural net&#39;s algorithm using other code, we need to pick the programming language). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-4" id="fnref-DNdPEGz4DYMju8reA-4">[4]</a></sup> Discussions of algorithmic complexity often ignore this issue on the grounds that it only adds a constant (since any given UTM can mimic any other if fed the right prefix), but it&#39;s not clear to me, at least, when such constants might or might not matter to a given analysis – for example, the analysis at stake here. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-5" id="fnref-DNdPEGz4DYMju8reA-5">[5]</a></sup></p><p> Indeed, my vague sense is that certain discussions of simplicity in the context of computer science often implicitly assume what I&#39;ve called &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; – a view on which simplicity in some deep sense an objective <em>thing</em> , ultimately independent of eg your choice of programming language or UTM, but which different metrics of simplicity are all tracking (albeit, imperfectly). And perhaps this view has merit (for example, my impression is that different metrics of complexity often reach similar conclusions in many cases – though this could have many explanations). However, I don&#39;t, personally, want to assume it. And especially absent some objective sense of simplicity, it becomes more important to say which particular sense you have in mind.</p><p> Another possible notion of simplicity, here, is hazier – but also, to my mind, less theoretically laden. On this notion, the simplicity of an algorithm implemented by a neural network is defined relative to something like the number of parameters the neural network uses to encode the relevant algorithm. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-6" id="fnref-DNdPEGz4DYMju8reA-6">[6]</a></sup> That is, instead of imagining <em>re-writing</em> the neural network&#39;s algorithm in some other programming language, we focus directly on the parameters the neural network itself is recruiting to do the job, where simpler programs use fewer parameters. Let&#39;s call this &quot;parameter simplicity.&quot; Exactly how you would measure &quot;parameter simplicity&quot; is a different question, but it has the advantage of removing one layer of theoretical machinery and arbitrariness (eg, the step of re-writing the algorithm in an arbitrary-seeming programming language), and connecting more directly with a &quot;resource&quot; that we know SGD has to deal with (eg, the parameters the model makes available). For this reason, I&#39;ll often focus on &quot;parameter simplicity&quot; below.</p><p> I&#39;ll also flag a way of talking about &quot;simplicity&quot; that I won&#39;t emphasize, and which I think muddies the waters here considerably: namely, equating simplicity fairly directly with &quot;higher prior probability.&quot; Thus, for example, faced with an initial probability distribution over possibilities, it&#39;s possible to talk about &quot;simpler hypotheses&quot; as just: the ones that have greater initial probability, and which therefore require less evidence to establish. For example: faced with a thousand people in a town, all equally likely to be the murderer, it&#39;s possible to think of &quot;the murderer is a man&quot; as a &quot;simpler&quot; hypothesis than &quot;the murderer is a man with brown hair and a dog,&quot; in virtue of the fact that the former hypothesis has, say, a 50% prior, and so requires only one &quot;bit&quot; of evidence to establish (ie, one halving of the probability space), whereas the latter hypothesis has a much smaller prior, and so requires more bits. Let&#39;s call this &quot;trivial simplicity.&quot;</p><p> &quot;Trivial simplicity&quot; is related to, but distinct from, the use of simplicity at stake in &quot; <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam&#39;s razor</a> .&quot; Occam&#39;s razor is (roughly) the <em>substantive</em> claim that <em>given an independent notion of simplicity</em> , simpler hypotheses are more likely on priors. Whereas trivial simplicity would imply that simpler hypotheses are <em>by definition</em> more likely on priors. If you take Occam&#39;s razor sufficiently for granted, it&#39;s easy to conflate the two – but the former is interesting, and the latter is some combination of trivial and misleading. And regardless, our interest here isn&#39;t in the simplicity of <em>hypotheses</em> like &quot;SGD selects a schemer,&quot; but in the simplicity of the <em>algorithm</em> that the model SGD selects implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-7" id="fnref-DNdPEGz4DYMju8reA-7">[7]</a></sup></p><h2> Does SGD select for simplicity?</h2><p> Does SGD select for simplicity in one of the non-trivial senses I just described?</p><p> One reason you might think this comes from the &quot;contributors to reward&quot; frame. That is: using a more parameter-simple algorithm will free up other parameters to be put to other purposes, so it seems very plausible that parameter simplicity will increase a model&#39;s reward. And to the extent that re-writing simplicity correlates with parameter simplicity, the same will hold for re-writing simplicity as well. This is the story about why simplicity matters that I find most compelling.</p><p> However, I think there may also be more to say. For example, I think it&#39;s possible that there&#39;s other empirical evidence that SGD selects for simpler functions, other things equal (for example, that it would much sooner connect a line-like set of dots with a straight line than with an extremely complicated curve) ; and perhaps, that this behavior is part of what explains its success (for example, because real-world functions tend to be simple in this sense, à la Occam&#39;s razor). For example, in the context of an understanding of SGD as an approximation of Bayesian sampling (per the discussion of <a href="https://arxiv.org/abs/2006.15191">Mingard et al (2020)</a> above), <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> discusses empirical evidence that the <em>prior</em> probability distribution over parameters (eg, what I called the &quot;initialization distribution&quot; above) puts higher probability mass on simpler functions. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-8" id="fnref-DNdPEGz4DYMju8reA-8">[8]</a></sup> And he connects this with a theoretical result in computer science called the &quot;Levin bound,&quot; which predicts this (for details in footnote). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-9" id="fnref-DNdPEGz4DYMju8reA-9">[9]</a></sup></p><p> I haven&#39;t investigated this in any depth. If accurate, though, this sort of result would give simplicity relevance from an &quot;extra criteria&quot; frame as well. That is, on this framework, SGD biases towards simplicity even before we start optimizing for reward.</p><p> Let&#39;s suppose, then, that SGD selects for some non-trivial sort of simplicity. Would this sort of selection bias in favor of schemers?</p><h2> The simplicity advantages of schemer-like goals</h2><p> Above I mentioned that the counting argument is sometimes offered as a reason to expect a bias towards schemers on these grounds. Note, though, that the counting argument (at least as I&#39;ve presented it) doesn&#39;t make any obvious reference to a bias towards simplicity per se. And I think we should be careful not to conflate the (trivial) simplicity of the <em>hypothesis</em> that &quot;SGD selects a schemer,&quot; <em>given a prior probability distribution that puts most of the probability on schemers</em> (eg, a uniform distribution over individual models-that-get-high-reward), with the claim that the <em>algorithm</em> that a given individual schemer implements is (substantively) simpler than the algorithm that a given non-schemer implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-10" id="fnref-DNdPEGz4DYMju8reA-10">[10]</a></sup> Indeed, my own sense is that the strongest form of the counting argument leaves it to stand on its own intuitive terms, rather than attempting to connect it to further questions about SGD&#39;s biases towards simplicity in particular.</p><p> That said, it is possible to draw connections of this form. In particular: we can say that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers do not. And this seems especially plausible to the extent we imagine that the goals required to be a non-schemer are quite complex (more on this below). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-11" id="fnref-DNdPEGz4DYMju8reA-11">[11]</a></sup></p><p> One interesting feature of this sort of argument is that it imagines, specifically, that the simplicity differences between models are coming entirely from the content of their <em>goals</em> . Indeed, the toy analysis in Hubinger (2022) specifically imagines that the respective model classes all have the same world model and optimization procedure, and that the complexity of their algorithm overall can be approximated by <em>complexity of world model + complexity of the optimization procedure + complexity of the goal.</em> And the &quot;goal slot&quot; is the only part that differs between models.</p><p> It&#39;s not clear that this is right, though, especially if we assume that the goal-directedness at stake is &quot;messy&quot; rather than &quot;clean.&quot; For example, to the extent that schemers have to perform types of instrumental reasoning that non-schemers <em>don&#39;t</em> (eg, reasoning about the instrumental value of getting reward, reasoning about when to defect, etc), it seems plausible that this could introduce additional complexity into the algorithm itself (rather than eg merely requiring that the algorithm &quot;run for a longer time,&quot; à la the &quot;speed&quot; analysis below). For example, to the extent we&#39;re using &quot;parameter simplicity&quot; as our notion of simplicity, we could imagine cases where this sort of instrumental reasoning requires additional parameters. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-12" id="fnref-DNdPEGz4DYMju8reA-12">[12]</a></sup></p><h2> How big are these simplicity advantages?</h2><p> For now, though, let&#39;s stick with Hubinger&#39;s ontology, and with simplicity differences rooted specifically in differences between goals. How big of an advantage does selecting a schemer afford in this respect?</p><p> One way of running this analysis is to compare the goals had by the simplest possible model within each class (either: because you expect SGD to select for the simplest possible model, or you think this is a good way of approximating the simplicity benefits at stake ）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-13" id="fnref-DNdPEGz4DYMju8reA-13">[13]</a></sup> That is, we compare the complexity of:</p><ol><li><p> The specified goal (for training-saints)</p></li><li><p> The simplest possible misgeneralized goal that gets max-reward on the training data absent training-gaming (for mis-generalized non-training-gamers)</p></li><li><p> The simplest possible way of seeking reward-on-the-episode (for reward-on-the-episode seekers)</p></li><li><p> The simplest possible goal-that-motivates-scheming (for schemers)</p></li></ol><p> Clearly, the complexity of the specified goal matters in this context. My sense is that the discourse on scheming often assumes that the specified goal is quite complex in some sense – and in particular, the sense at stake in the idea that &quot; <a href="https://www.lesswrong.com/tag/complexity-of-value">human values are complicated</a> .&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-14" id="fnref-DNdPEGz4DYMju8reA-14">[14]</a></sup> And perhaps, if we&#39;re imagining that the only way to get <em>alignment</em> is to first (a) somehow specify &quot;human values&quot; via the training objective, and then (b) somehow ensure that we get a training saint, then focusing on something in the vicinity of &quot;act in accordance with human values&quot; as the specified goal is appropriate. But note that for the purposes of comparing the probability of <em>scheming</em> to the probability of <em>other forms of misalignment</em> , we need not assume such a focus. And thus, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; Indeed, in <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">other work</a> , Hubinger (writing with others) suggests that a goal like &quot;minimize next-token prediction error&quot; is quite simple – and indeed, that &quot;its complexity is competitive with the simplest possible long-term goals&quot; (this is part of what makes Hubinger comparatively optimistic about avoiding scheming during LLM pre-training – though personally, I feel confused about why Hubinger thinks &quot;next token prediction error&quot; importantly simpler than &quot;reward-on-the-episode&quot;).</p><p> Suppose, though, that the specified goal has complexity on the order of &quot;act in accordance with human values&quot; or &quot;follow instructions in a helpful, harmless, and honest (HHH) manner.&quot; Where does this leave a comparison between (1)-(4) above?</p><p> At least from a parameter-simplicity perspective, one way of approaching this is to think about what we know about the absolute cost in parameters for representing different human concepts within neural networks. I won&#39;t dig in much, here, but one piece of data that seems relevant is: LLMs like GPT-4 seem capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on. And this isn&#39;t yet to mention all the parameters devoted to everything else that GPT-4 does. So the absolute number of parameters required to represent concepts like these can&#39;t be crazily high. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-15" id="fnref-DNdPEGz4DYMju8reA-15">[15]</a></sup> And this caps the overall complexity savings at stake in the differences between the models above. For example, if we assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then the maximum number of parameters required to represent the concept of &quot;honesty&quot; is around a million (and this estimate strikes me as quite conservative). So if the simplest possible schemer-like goal takes only 1 parameter to represent, then not representing honesty in the schemer&#39;s &quot;goal slot,&quot; saves, at most, 999,999 parameters – roughly one millionth of a trillion parameter model&#39;s representational capacity, and even less for the larger models of the future.</p><p> More importantly, though: no one thinks that a schemer won&#39;t have to represent concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on at all. As <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">Hubinger et al (2023)</a> note, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process, etc (and especially: models that are &quot;playing a training game&quot; in which such concepts play a central role). So really, the relevant question is: what are the <em>extra</em> complexity costs of representing a goal like &quot;get reward-on-the-episode&quot; or &quot;follow instructions in an HHH way&quot; (relative to the simplest possible schemer-like goal), <em>once you&#39;ve already paid the costs of having a concept of those goal targets</em> .</p><p> I&#39;m not sure exactly how to think about this, but it seems very plausible to me that the costs here are extremely small. In particular: it seems like SGD may be able to significantly repurpose the parameters used to represent the concept in the world model in causing that concept to guide the model&#39;s behavior in a goal-like manner. Thus, as an analogy, perhaps the concept of &quot;pleasure&quot; is in some sense &quot;simpler&quot; than the concept of &quot; <a href="https://en.wikipedia.org/wiki/Wabi-sabi">wabi-sabi</a> &quot; in Japanese aesthetics (ie, &quot;appreciating beauty that is &#39;imperfect, impermanent, and incomplete&#39; &quot;) 。 Once you&#39;ve <em>learned</em> both, though, does pursuing the former require meaningfully more parameters than pursuing the latter? <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-16" id="fnref-DNdPEGz4DYMju8reA-16">[16]</a></sup></p><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger (2022)</a> discussion of issues like this sometimes appeals to the notion of a &quot;pointer&quot; to some part of the world model. As I understand it, the idea here is that if you&#39;ve already got a concept of something like &quot;pleasure&quot;/&quot;wabi-sabi&quot;/&quot;reward&quot; in your world model, you can cause a model to pursue that thing by giving it a goal slot that says something like &quot;go for <em>that</em> &quot; or &quot; <em>that</em> is good,&quot; where &quot;that&quot; points to the thing in question (this is in contrast with having to represent the relevant concept <em>again,</em> fully and redundantly, in the goal slot itself). But insofar as we use a toy model like this (I doubt we should lean on it), why think that it&#39;s significantly more complex to <em>point</em> at a more complex concept than at a simpler one? Eg, even granted that &quot;wabi-sabi&quot; takes more parameters than &quot;pleasure&quot; to represent in the world model, why think that encoding the <em>pointer</em> to &quot;pleasure&quot; (eg, &quot;go for <em>that&quot;</em> ) takes more parameters than encoding the <em>pointer</em> to &quot;wabi-sabi&quot; (eg, again, &quot;go for <em>that</em> &quot;)?</p><p> One option, here, is to say that the complexity of the concept and the complexity of the pointer are correlated. For example, you might imagine that the model has some kind of &quot;internal database&quot; of concepts, which stores concepts in a manner such that concepts that take fewer parameters to store take fewer parameters to &quot;look up&quot; as well. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-17" id="fnref-DNdPEGz4DYMju8reA-17">[17]</a></sup> On this picture, &quot;pleasure&quot; might end up stored as the 15th concept in the database <em>because</em> it takes eg 23 parameters to represent, whereas &quot;wabi-sabi&quot; might end up stored as the 125355th concept because it takes 10,000 parameters to代表。 And then the &quot;pointer&quot; to pleasure can say &quot;go for the thing stored at location 15,&quot; whereas the &quot;pointer&quot; to &quot;wabi-sabi&quot; has to say &quot;go for the thing stored at location 125355,&quot; which takes a few more bits to specify. But even at an abstract-toy-illustrative-example level, this sort of story requires leaning on a specific model of how the model&#39;s pointer and concept-storage processes work – and it still needs to explain <em>why</em> simplicity-to-represent and simplicity-to-point-at are correlated in the relevant sense.</p><p> Alternatively, though, we can abandon any interest in the complexity of storing a concept in the world model, and focus directly on the complexity of pointing to it. Still, insofar as there are meaningful <em>differences</em> between the complexity of pointing at one concept vs. another, we may be able to re-run the argument that schemers offer simplicity advantages. In particular: selecting a schemer allows SGD to have its pick from whatever schemer-motivating goals are simplest to <em>point at in the world model</em> ; whereas the other model classes plausibly impose more substantive constraints. Ie, if the specified goal ends up stored at location 12634 in the model&#39;s metaphorical database, and if &quot;reward-on-the-episode&quot; is at location 35364, then if there are any schemer-like goals at eg locations 1-100, it&#39;ll be simpler to point at one of <em>those</em> instead – and thus, to create a schemer rather than a training-saint or a reward-on-the-episode seeker.</p><p> To the extent we focus on the final properties of the different model classes, I think this is probably the best way to run a simplicity-focused argument for scheming – especially if we don&#39;t get too hung up on the toy ontology of &quot;pointers&quot; (and still less, &quot;databases&quot;) in particular. That is, roughly: even granted that all of the goals on the table here (eg the specified goal, reward-on-the-episode, etc) will need to be represented by the model somewhere, it may be that, for whatever reason, some of these concepts take fewer parameters to <em>repurpose for goal-pursuit</em> than others. And insofar as scheming makes more potential concepts available for such repurposing, it offers a simplicity advantage in that sense.</p><p> Quantitatively, though, how big is this advantage? It&#39;s now especially hard to say. In particular, once we&#39;ve shifted our focus from &quot;simplicity-to-represent&quot; to &quot;simplicity-to-repurpose-for-goal-pursuit,&quot; we can&#39;t even look at our intuitive sense of the complexity of concepts like &quot;reward,&quot; &quot;honesty,&quot; and &quot;next token prediction error,&quot; because we&#39;re no longer talking about the complexity of the concepts per se. Rather, we&#39;re speculating about the complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory.</p><p> Still, to the extent we can estimate the size of these differences, it seems plausible to me that they are very small indeed. One intuition pump for me here runs as follows. Suppose that the model has 2^50 concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-18" id="fnref-DNdPEGz4DYMju8reA-18">[18]</a></sup> The average number of bits required to code for each of 2^50 concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max-reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I won&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so).</p><h2> Does this sort of simplicity-focused argument make plausible predictions about the sort of goals schemers would end up with?</h2><p> One other consideration that seems worth tracking, in the context of simplicity arguments for scheming, is the predictions they are making about the sort of goals a schemer will end up with. In particular, if you think (1) that SGD selects very hard for simpler goals, (2) that this sort of selection favors schemer-like goals because they can be simpler, and (3) that our predictions about what SGD selects can ignore the &quot;path&quot; it takes to create the model in question, then at least naively, it seems like you should expect SGD to select a schemer with an extremely simple long-term goal (perhaps: the simplest possible long-term goal), <em>regardless of whether that goal had any relation to what was salient or important during training</em> . Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-19" id="fnref-DNdPEGz4DYMju8reA-19">[19]</a></sup> these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-20" id="fnref-DNdPEGz4DYMju8reA-20">[20]</a></sup></p><p> Personally, I feel skeptical of predictions like this (though this skepticism may be partly rooted in skepticism about ignoring the path SGD takes through model space more generally). And common stories about schemers tend to focus on proxy goals with a closer connection to the training process overall (eg, a model trained to on gold-coin-getting ends up valuing eg &quot;get gold stuff over all time&quot; or &quot;follow my curiosity over all time,&quot; and not &quot;maximize hydrogen over all time&quot;).</p><p> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue, perhaps they will either be more important (and thus simpler?) to represent in the world model, or simpler (for some reason) for the model to repurpose-for-goal-pursuit once represented. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-21" id="fnref-DNdPEGz4DYMju8reA-21">[21]</a></sup> But if we grant some story in this vein, we should also be tracking its relevance to the simplicity of pursuing <em>non-schemer goals</em> as well. In particular: to the extent we&#39;re positing that salience/relevance during training correlates with simplicity in the relevant sense, this is points in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. (Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall.)</p><p> And note, too, that to the extent SGD selects very hard for simpler goals (for example, in the context of a form of &quot;low path dependence&quot; that leads to strong convergence on a single optimal sort of model), this seems somewhat at odds with strong forms of the goal-guarding hypothesis, on which training-gaming causes your goals to &quot;crystallize.&quot; For example, if a would-be-schemer starts out with a not-optimally-simple goal that still motivates long-term power-seeking, then if it knows that in fact, SGD will continue to grind down its goal into something simpler even after it starts training-gaming, then it may not have an incentive to start training-gaming in the first place – and its goals won&#39;t survive the process regardless. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-22" id="fnref-DNdPEGz4DYMju8reA-22">[22]</a></sup></p><h2> Overall assessment of simplicity arguments</h2><p> Overall, I do think that other things equal, schemers can have probably simpler goals than these other model classes. However, I think the relevant simplicity differences may be quite small, especially once we condition on the model having a good world model more generally (and moreso, if we posit that goals targets salient/relevant-during-training get extra simplicity points). And I&#39;m suspicious of some of the theoretical baggage it can feel like certain kinds of simplicity arguments wheel in (for example, baggage related to the notion of simplicity at stake, whether SGD selects for it, how to think about simplicity in the context of repurposing-for-goal-pursuit as opposed to merely representing, and so on). </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-DNdPEGz4DYMju8reA-1" class="footnote-item"><p> See eg <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_low_path_dependence_world">Hubinger (2022)</a> . <a href="#fnref-DNdPEGz4DYMju8reA-1" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-2" class="footnote-item"><p> See also <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive">this (now anonymous) discussion</a> for another example of this usage of &quot;simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-2" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-3" class="footnote-item"><p> Here, my sense is that the assumption is generally that X can be described at a level of computational abstraction such that the &quot;re-writing&quot; at stake doesn&#39;t merely reproduce the network itself. Eg, the network is understood as implementing some more abstract function. I think it&#39;s an interesting question how well simplicity arguments would survive relaxing this sort of assumption. <a href="#fnref-DNdPEGz4DYMju8reA-3" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-4" class="footnote-item"><p> Another issue is that <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">Kolmogorov complexity is uncomputable</a> . I&#39;m told you can approximate it, but I&#39;m not sure how this gets around the issue that for a given program where you&#39;re not able to tell whether or not it halts, that program might be the shortest program outputting the relevant细绳。 <a href="#fnref-DNdPEGz4DYMju8reA-4" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-5" class="footnote-item"><p> See <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#iv-can-you-ignore-being-only-finitely-wrong">Carlsmith (2021)</a> , sections III and IV, for more on this. <a href="#fnref-DNdPEGz4DYMju8reA-5" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-6" class="footnote-item"><p> Hubinger sometimes appears to be appealing to this notion as well – or at least, not drawing clear distinctions between &quot;re-writing simplicity&quot; and &quot;parameter simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-6" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-7" class="footnote-item"><p> &quot;Trivial simplicity&quot; is also closely related to what we might call &quot;selection simplicity.&quot; Here, again, one assumes some space/distribution over possible things (eg, goals), and then talks about the &quot;simplicity&quot; of some portion of that space in terms of how much &quot;work&quot; one needs to do (perhaps: on average) in order to narrow down from the whole space to that portion of the space (see also <a href="https://colah.github.io/posts/2015-09-Visual-Information/">variable-length codes</a> ). Thus, for a box of gas, &quot;the molecules are roughly evenly spread out&quot; might be a &quot;simpler&quot; arrangement than &quot;the molecules are all in a particular corner,&quot; because it typically takes more &quot;work&quot; (in this example: thermodynamic work) to cause the former than the latter (this is closely related to the fact that the former is initially more likely than the latter). My sense is that when some people say that &quot;schemer-like goals are simple,&quot; they mean something more like: the <em>set</em> of schemer-like goals typically takes less &quot;work,&quot; on SGD&#39;s part, to land within than the <em>set</em> of non-schemer-like goals (and not necessarily: that any <em>particular</em> schemer-like goals is simpler than some <em>particular</em> non-schemer-like goal). To the extent that the set of schemer-like goals are supposed to have this property because they are more &quot;common,&quot; and hence &quot;nearer&quot; to SDG&#39;s starting point, this way of talking about the simplicity benefits of scheming amounts to a restatement of something like the counting argument and/or the &quot;nearest max-reward goal argument&quot; – except, with more of a propensity, in my view, to confuse the simplicity of <em>set</em> of schemer-like goals with the simplicity of a <em>given</em> schemer-like目标。 <a href="#fnref-DNdPEGz4DYMju8reA-7" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-8" class="footnote-item"><p> Where, importantly, multiple different settings of parameters can implement the same function. <a href="#fnref-DNdPEGz4DYMju8reA-8" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-9" class="footnote-item"><p> My understanding is that the Levin bound says something like: for a given distribution over parameters, the probability <em>p(f)</em> of randomly sampling a set of parameters that implements a function <em>f</em> is bounded by <em>2^{-K(f) + O(1)}</em> , where <em>K</em> is the <em>k</em> -complexity of the function <em>f</em> , and <em>O(1)</em> is some constant independent of the function itself (though: dependent on the parameter space). That is, the prior on some function decreases exponentially as the function&#39;s complexity increases.</p><p> I haven&#39;t investigated this result, but one summary I saw ( <a href="https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of?commentId=kLz9mgNv8xFNrAPt2">here</a> ) made it seem fairly vacuous. In particular, the idea in that summary was that larger volumes of parameter space will have simpler encodings, because you can encode them by first specifying distribution over parameters, and then using a <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman code</a> to talk about how to find them given that distribution. But this makes the result seem pretty trivial: it&#39;s not that there is some antecedent notion of simplicity, which we then discover to be higher-probability according to the initialization distribution. Rather, to be higher probability according to the initialization distribution just <em>is</em> to be simpler, because equipped with the initialization distribution, it&#39;s easier to encode the higher probability parts of it. Or put another way: it seems like this result applies to any distribution over parameters. So it doesn&#39;t seem like we learn much about any particular distribution from it.</p><p> (To me it feels like there are analogies here to the way in which &quot;shorter programs get more probability,&quot; in the context of algorithmic &quot;simplicity priors&quot; that focus on metrics like K-complexity, actually applies necessarily to <em>any</em> distribution over a countably-infinite set of programs – see discussion <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">here</a> . You might&#39;ve thought it was an interesting and substantive constraint, but actually it turns out to be more vacuous.)</p><p> That said, the empirical results I mention above focus on more practical, real-world measures of simplicity, like <a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv_complexity">LZ complexity</a> , and apparently they find that, indeed, simpler functions get higher prior probability (see eg <a href="https://arxiv.org/pdf/1805.08522.pdf">this experiment</a> , which uses a fully connected neural net to model possible functions from many binary inputs to a single binary input). This seems to me more substantive and interesting. And <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> claims that Levin&#39;s result is non-trivial, though I don&#39;t yet understand how. <a href="#fnref-DNdPEGz4DYMju8reA-9" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-10" class="footnote-item"><p> Thus, for example, you might think that insofar a randomly initialized model is more likely to end up &quot;closer&quot; to a schemer, such that SGD needs to do &quot;less work&quot; in order to select a schemer rather than some other model, this favors schemers (thanks to Paul Christiano for discussion). But this sort of argument rests on putting a higher prior probability on schemers, which, in my book, isn&#39;t a (non-trivial) simplicity argument per se. <a href="#fnref-DNdPEGz4DYMju8reA-10" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-11" class="footnote-item"><p> There are also more speculative and theoretical arguments for a connection between simplicity and schemers, on which one argues that if you do an unbounded search over all possible programs to find the shortest one that gives a given output, without regard to other factors like how long they have to run, then you&#39;ll select for a schemer (for example, via a route like: simulating an extremely simple physics that eventually gives rise to agents that understand the situation and want to break out of the simulation, and give the relevant output as part of a plan to do so). My understanding is that people (eg <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive#Priors_on_Learned_Optimizers">here</a> ) sometimes take the discourse about the &quot; <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">malignity of the Solomonoff prior</a> &quot; as relevant here (though at a glance, it seems to me like there are important differences – for example, in the type of causality at stake, and in the question of whether the relevant schemer might be simulating <em>you</em> ). Regardless, I&#39;m skeptical that these unbounded theoretical arguments should be getting much if any weight, and I won&#39;t treat them here. <a href="#fnref-DNdPEGz4DYMju8reA-11" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-12" class="footnote-item"><p> What&#39;s more, note that, to the extent we imagine SGD biasing towards simplicity <em>because</em> real world patterns tend to be simple (eg, Occam&#39;s razor is indeed a good prior, and SGD works well in part because it reflects this prior), the explanation for this bias doesn&#39;t apply as readily to a model&#39;s <em>goals</em> . That is (modulo various forms of moral realism), there are no &quot;true goals,&quot; modeling of which might benefit from a simplicity prior. Rather, on this story, SGD would need to be acting more like a human moral anti-realist who prefers a simpler morality other-things-equal, despite not believing that there is any objective fact of the matter, because, in contexts where there <em>is</em> a fact of the matter, simpler theories tend to be more likely. <a href="#fnref-DNdPEGz4DYMju8reA-12" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-13" class="footnote-item"><p> Hubinger uses this approach. My understanding is that he&#39;s imagining SGD selecting a model with probability proportionate to its simplicity, such that eg focusing on the simplest possible model is one way of approximating the overall probability in a model class, and focusing on the <em>number</em> of models in the class is其他。 However, I won&#39;t take for granted the assumption that SGD selects a model with probability proportionate to its simplicity. <a href="#fnref-DNdPEGz4DYMju8reA-13" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-14" class="footnote-item"><p> See eg Hubinger et al (2023) <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">here</a> . <a href="#fnref-DNdPEGz4DYMju8reA-14" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-15" class="footnote-item"><p> I first heard this sort of point from Paul Christiano. <a href="#fnref-DNdPEGz4DYMju8reA-15" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-16" class="footnote-item"><p> Here I don&#39;t mean: does it take more parameters to <em>successfully</em> promote pleasure vs. successfully promoting wabi-sabi. I just mean: does it take more parameters to <em>aim</em> optimization at the one vs. the other. <a href="#fnref-DNdPEGz4DYMju8reA-16" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-17" class="footnote-item"><p> Thanks to Daniel Kokotajlo for suggesting an image like this. <a href="#fnref-DNdPEGz4DYMju8reA-17" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-18" class="footnote-item"><p> The precise number of concepts here doesn&#39;t matter much. <a href="#fnref-DNdPEGz4DYMju8reA-18" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-19" class="footnote-item"><p> I&#39;m not saying it is, even for a physics-based world model, but I wanted an easy illustration of the point. Feel free to substitute your best-guess simplest-possible-goal here. <a href="#fnref-DNdPEGz4DYMju8reA-19" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-20" class="footnote-item"><p> Notably, this sort of prediction seems like an especially poor fit for an analogy between humans and evolution, since human goals seem to have a very intelligible relation to reproductive fitness. But evolution is plausibly quite &quot;path-dependent&quot; anyway. <a href="#fnref-DNdPEGz4DYMju8reA-20" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-21" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-DNdPEGz4DYMju8reA-21" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-22" class="footnote-item"><p> Hubinger, in discussion, suggests that the model&#39;s reasoning would proceed in terms of logical rather than physical causality. He writes: &quot;The reasoning here is: I should be the sort of model that would play the training game, since there&#39;s some (logical) chance that I&#39;ll be the model with the best inductive biases, so I should make sure that I also have good loss.&quot; But if a model can <em>tell</em> that its goal isn&#39;t yet optimally simple (and so will be ground down by SGD), then I&#39;m not sure why it would think there is a &quot;logical chance&quot; that it&#39;s favored by the inductive biases在这个意义上。 <a href="#fnref-DNdPEGz4DYMju8reA-22" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming<guid ispermalink="false"> uWdAKyHZMfoxDHcCC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:05:54 GMT</pubDate> </item><item><title><![CDATA[Results from the Turing Seminar hackathon]]></title><description><![CDATA[Published on December 7, 2023 2:50 PM GMT<br/><br/><p> We ( <a href="https://ia.effisciences.org/"><u>EffiSciences</u></a> ) ran a hackathon at the end of <a href="https://www.master-mva.com/cours/seminaire-turing/"><u>the Turing Seminar in ENS Paris-Saclay</u></a> and <a href="https://diplome.di.ens.fr/catalog_fr.html#:~:text=niveau%20L3%20M1-,S%C3%A9minaire%20Turing,-(iCal)">ENS Ulm</a> , an academic course inspired by the AGISF, with 28 projects submitted by 44 participants between the 11th and 12th November.</p><p> We share a selection of projects. <strong>See them all</strong> <a href="https://drive.google.com/drive/folders/1HgqUCu7CG44iUZdJtDO_Zua4xjLFu450?usp=drive_link"><strong>here</strong></a> <strong><u>.</u></strong></p><p> I think some of them could even be turned into valuable blog posts, and I&#39;ve learnt a lot by reading everything. Here are a few extracts.</p><h1> Towards Monosemanticity: Decomposing Vision Models with Dictionary Learning</h1><p> <i>David HEURTEL-DEPEIGES</i> [ <a href="https://drive.google.com/file/d/1gmj-ax9mtn2chjyoMxyQlP-R9zCl6q6f/view?usp=sharing"><u>link</u></a> ]</p><p> Basically an adaptation of the famous dictionary learning paper on vision CNN.</p><p> “When looking at 100 random <strong>features</strong> , 46 were found to be interpretable and monosemantic, [...] When doing the same experiment with 100 random <strong>neurons</strong> , 2 were found to be interpretable and monosemantic”.执行力非常好。</p><h1> Open, closed, and everything in between: towards human-centric regulations for safer large models</h1><p> <i>Théo SAULUS</i> [ <a href="https://drive.google.com/file/d/1jopH_aaDOVRSLoBG4qnwbqglDUyFebUH/view"><u>link</u></a> ]</p><p> Imho, an almost SOTA summary of the current discourse on open sourcing models.</p><h1> A Review of The Debate on Open-Sourcing Powerful AI Models</h1><p> <i>Tu Duyen NGUYEN, Adrien RAMANANA RAHARY</i> [ <a href="https://drive.google.com/file/d/1M1B7n2jEPZPkBdYSxy_v3uNgqWAk9tF4/view?usp=drive_link"><u>link</u></a> ]</p><p> “What is the goal of this document? Our goal is to clarify, and sometimes criticize, the arguments regularly put forward in this, both from the proponents and the opponents of open-sourced AI models. In an effort to better understand both stances, we have classified the most common arguments surrounding this debate in five main topics:</p><ol><li> Safety evaluation of powerful AI models through audits or open-research</li><li> Competition in the private sector and beyond</li><li> Safety risks of open-sourcing strongly capable models</li><li> Preserving open science for its own sake</li><li> The feasibility of closing the weights of AI models</li></ol><p> In an effort to highlight the strengths and weaknesses of each stance, we will present and challenge for each family the arguments of both sides, as well as include in each family some arguments which we think are relevant, but have not been mentioned in most discussions on these topics.”</p><p> They tried to be exhaustive, but there are still some gaps. The format is nevertheless interesting (even if it could be even more concise). </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/pbs3gjvzjzifved99zqh"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/m2budueajdv9smhr803w"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/cowvnni4lmpvy5bj6wma"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/oftlgwmwvsoy4jbvt4vy"></td></tr></tbody></table></figure><h1> AI safety media analysis</h1><p> <i>Gurvan Richardeau, Raphaël Pesah</i> [ <a href="https://docs.google.com/document/d/1SBdZN9vnSVfqqJ_zUChXPxQjcrv8bT5DqSubA20ob-w/edit#heading=h.u543ptgy0th"><u>link</u></a> ]</p><p> “The first analysis we&#39;ve done involved examining major themes by employing text data analysis and ML methods (such as tf-idf analysis and topic modeling) within a corpus of 1,644 publications from the Factiva database over the past five years, specifically related to人工智能安全。 The second analysis (Analysis 2) uses another database: Europresse. “</p><p> Here are some snippets:</p><p> “First let&#39;s have a look at the global distribution of the articles over the years.” </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> We see that more than 80% of the articles of our dataset are from 2023: </p><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/r82swpuzxwe0uuf4aiax"></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Now let&#39;s have a more precise look over the year 2023 : </p><p><img style="width:97.68%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/yog6bfn1egrsk9kami9o"></p></td></tr></tbody></table></figure><p><br> “Here are the results for the sentiment analysis: “ </p><figure class="image image_resized" style="width:43.33%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/hqdcwasrohax5fddjfgx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/jy1k9hh9r4kmmzx5nbqq 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/b8skz1gdy9a1z2rejfrg 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/nbebbobmaqncvqfe7rmq 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/g8eljblwrf7uydvtwnqs 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/yr7cjfjgxe5kzubn3ne0 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/njcvcbbxb0amtbfygp1f 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/gjvzjndbiibpplvef6mt 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/dggepbhvyvv3pyrfzlhu 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/akp4ow9op0la8hupjmo4 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/dszpurelyhvdlinexewm 840w"></figure><p> “Around 71% of the articles talking of AGI talk about it as a good thing whereas 23% are talking about it as a thing to worry about.”</p><p> “We got 57,000 articles about AGI in 2023 against 18,000 for the ones related to AI safety, so this time it is three times less. [...] There are between two and three times more articles talking about AGI without AI safety than articles about AI safety in 2023.”</p><p> My comment: Interesting. There are many more figures in the reports. Maybe those kinds of metrics could be used to measure the impact of public outreach?</p><h1> Biases in Reinforcement Learning from Human Feedback</h1><p> <i>Gaspard Berthelier</i> [ <a href="https://docs.google.com/document/d/1Qc7f21EgYHdFJqC5MGJu3Auj00vLfJEc/edit?usp=drive_link"><u>link</u></a> ]</p><p> A good summary of the paper “ <a href="https://www.lesswrong.com/posts/LqRD7sNcpkA9cmXLv/open-problems-and-fundamental-limitations-of-rlhf">Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</a> ”.</p><h1> Situational Awareness of AIs</h1><p> <i>Thomas Michel, Théo Rudkiewicz</i> [ <a href="https://docs.google.com/document/d/1v-L1nXVSEJw0_9ceAA5BQGJxRgoSk9dvjbEat8eSQQE/edit?usp=drive_link"><u>link</u></a> ]</p><p> An alternative title could be “Situational Awareness from AI to Zombies, with premium pedagogical memes”: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/ddmnh7gjlssrryhihcrw"><figcaption> Fig. 1: Why an IA that does not differentiate testing and deployment is less dangerous. </figcaption></figure></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:60.87%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/r2qfphrynxeo3vrmhs6i"><figcaption> Fig. 2: What is Situational Awareness? </figcaption></figure></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:80.57%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/fe9wqhwsimyvapdaicnd"><figcaption> Fig. 3: How to test potentially dishonest models? </figcaption></figure></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:89.12%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/tarxnmxtglvl9umsowai"><figcaption> Fig. 4: A summary of <a href="https://arxiv.org/abs/2309.00667"><u>Berglund et al</u></a> : Taken out of context: On measuring situational awareness in LLMs.</figcaption></figure></td></tr></tbody></table></figure><h1> Summary of methods to solve Goal Misgeneralization</h1><p> <i>Vincent Bardusco</i> [ <a href="https://docs.google.com/document/d/16SlCd52xOuCYLOCtASobc_EXuauVOWwt/edit?usp=drive_link"><u>link</u></a> ]</p><p> A good summary of the following papers:</p><ol><li> R. Shah, Goal Misgeneralization: Why Correct Specifications Aren&#39;t Enough For Correct Goals, 2022.</li><li> B. Shlegeris, The prototypical catastrophic AI action is getting root access to its datacenter, 2022.</li><li> Song et al., Constructing unrestricted adversarial examples with generative models, 2018.</li><li> A. Bhattad, MJ Chong, K. Liang, B. Li, D. A and Forsyth, Unrestricted Adversarial Examples via Semantic Manipulation, 2019.</li><li> B. Barnes, Imitative Generalisation (AKA &#39;Learning the Prior&#39;), 2021.</li><li> R. Jia and P. Liang, Adversarial examples for evaluating reading comprehension systems, 2017.</li><li> S. Goldwasser, MP Kim, V. Vaikuntanathan and O. Zamir, Planting undetectable backdoors in machine learning models, 2022.</li><li> A. Madry, A. Makelov, L. Schmidt, D. Tsipras and A. Vladu, Towards deep learning models resistant to adversarial attacks, 2018.</li><li> E. Hubinger, Relaxed adversarial training for inner alignment, 2019.</li></ol><h1> A Review of DeepMind&#39;s AI alignment plan</h1><p> <i>Antoine Poirier, Théo Communal</i> [ <a href="https://drive.google.com/file/d/1dC6FreML2qs1S6xuopIfYzVATRYc5lC8/view?usp=drive_link"><u>link</u></a> ]</p><p> A document summarizing the main aspects of DeepMind&#39;s plan. Up until now, their agenda has been covered in a series of blog posts and papers, but now it&#39;s all summarized in a ten-page blog post. </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top" colspan="2"><p><img style="width:92.5%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/ytkmna8qsptbc6qlerma"></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/p0csxojobppctiuxdzp7"></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:98.11%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/assvherolv9eoz0n3ijp"></p></td></tr></tbody></table></figure><h1> Criticism of criticism of interp</h1><p> <i>Gabriel Ben Zenou, Joachim Collin</i> [ <a href="https://docs.google.com/document/d/1EFTdstgHb8iVIUx_bWzmcYNDZUCzbP3gpbMQmfZLc6M/edit?usp=drive_link"><u>link</u></a> ]</p><p> An alternative title could be “Against <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><u>Against Almost Every Theory of Impact of Interpretability</u></a> ”.</p><p> Some students tried to distill the discussion and to criticize my position, and you can find my criticism of their criticism in the comments of the google doc. Here is my <a href="https://docs.google.com/document/d/1EFTdstgHb8iVIUx_bWzmcYNDZUCzbP3gpbMQmfZLc6M/edit?disco=AAABBpJ9O7A"><u>main comment</u></a> .</p><h1> Risks of Value Lock-In in China</h1><p> <i>Inès Larroche, Bastien Le Chenadec</i> [ <a href="https://drive.google.com/file/d/1a81ezHWDE10gnABsk8V_7iEreSx0024S/view?usp=drive_link"><u>link</u></a> ]</p><p> It&#39;s a very good summary of what is happening in China regarding AI.</p><h1> Is LeCun making progress in AI safety with “A Path Towards Autonomous Machine Intelligence”?</h1><p> <i>Victor Morand</i> [ <a href="https://drive.google.com/file/d/1muZiF9R6sxPU2Z-gMsMF0j60kMT8CT5P/view?usp=drive_link"><u>link</u></a> ]</p><p> It&#39;s a good summary of LeCun&#39;s idea, and the numerous criticisms of his plan. </p><figure class="image image_resized" style="width:35.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/y3vl6scv5yercxgvrbor"><figcaption> LeCun&#39;s architecture</figcaption></figure><h1> Taxonomy of governance regulations</h1><p> Mathis Embit [ <a href="https://docs.google.com/document/d/1VtLmxM9mdBTIvKY0rWNIGtjB8ODdVwgYZ0OuX4BGyXs/edit?usp=drive_link"><u>link</u></a> ]</p><p> A summary of the main ways to regulate AI. Comparing different policies provides them with more distinctiveness and depth. </p><figure class="image image_resized" style="width:62.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/fmnxrvlvdm0ig2k66qyt"><figcaption></figcaption></figure><h1> Other notable projects</h1><p> <u>See them all</u> <a href="https://drive.google.com/drive/folders/1HgqUCu7CG44iUZdJtDO_Zua4xjLFu450?usp=drive_link"><u>here</u></a> <u>.</u> </p><figure class="table"><table><thead><tr><th style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px"><p><strong>标题</strong></p></th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px"><p><strong>作者</strong></p></th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>概括</strong></p></th></tr></thead><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>Technical justification of OpenAI&#39;s superalignment plan</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Robin Sobczyk,<br> Bastien Lhopitallier</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A lot of papers are summarized here</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>代理创建</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>Mathias Vigouroux</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A philosophical essay trying to explain how theory of mind emerges in humans and could emerge in AIs.这很有趣。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Anthropic RSP analysis</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Ralf Cortes Do Nascimento,</p><p> Paul Cazali</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A good summary of the entire LW discussion on the RSPs.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Goal Misgeneralization</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Basile TERVER,<br>安托万·奥利维尔</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>A summary of various techniques to prevent Goal Misgeneralization</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Detecting and reducing gender bias</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Gabrielle LE BELLIER</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A literature review on techniques to reduce gender biases in LLMs</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Literature review on SOTA Adversarial attacks and backdoors</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Mathis Le Bail,<br> David Sahna</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of many papers on attack and defense. The conclusion: &quot;all the studies in the literature tend to the following conclusion: there is no single effective method of defense against all possible types of attack&quot;</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Why does DL work so well?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Sacha ELKOUBI,<br> Even MATENCIO,<br> Eustache LE BIHAN,<br> Paul SITOLEUX</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> It needs a bit more polishing, but this is a good basis. It goes from explaining the Classical Statistical learning theory to Singular learning theory and everything in between.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> AI Act: what rules for foundation models?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Matthieu Carreau</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of the current challenges for EU regulations</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Critics of GovAI</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Xavier Secheresse,<br> Pierre-Yves Doireau</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of their views, and a small critique</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>治愈</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>David El Bèze,<br> Raphaël Cohen,<br> Nathaniel Cohen</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A short story of a deceptive AI transforming the world</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Logit Lens on Vision Transformer</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Akedjou Achraff Adjileye</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Testing the logit lens on Vision transformers, it works without difficulty.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Safety benchmarks</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Ugo Insalaco</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A review of the main safety benchmarks and a few proposals of missing benchmarks.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Probability of X-Risks without deceptive alignment</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Mathilde DUPOUY</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> She enumerates the different scenarios in the following <a href="https://docs.google.com/spreadsheets/d/1W2sm4Si9zu6i9ioBbV_vnC6RTCenrPXRJwitvn9gqYg/edit?usp=sharing"><u>table</u></a> . The final probability among all the risks that the one that occurs is an existential risk without deceptive alignment is 71%.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Do you think Le Cun&#39;s architecture is safe?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Théotime de Charrin</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Another criticism of LeCun&#39;s agenda.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Navigating AI Safety</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Abdessalam EDDIB</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Presents a landscape of some strategies to reduce risks through alignment techniques and some governance strategies, and is targeting beginners in alignment.</p></td></tr></tbody></table></figure><p></p><h1></h1><p></p><h1> Some Thoughts</h1><ul><li> Running the hackathon was a lot of fun. We just worked in a room at the university during the weekend. We also organized a meme contest.强烈推荐。</li><li> <strong>Unlike last year&#39;s hackathon, where students had the freedom to choose their own topics, this time we provided a list of predetermined topics</strong> . This resulted in higher quality work. If you&#39;re interested in organizing a similar hackathon, you can find the list of subjects <a href="https://www.notion.so/AI-Safety-Research-Projets-2023-English-b6c5b1cb3ad641abbfa9acd856cd8bad?pvs=21"><u>here</u></a> if you want to run something similar.</li><li> I am extremely pleased with the outcome, especially since the students went from having no knowledge in AI Safety to making valuable contributions in only 2 months.</li><li> The material of the course is available <a href="https://docs.google.com/document/d/1d0W_5xRaVehYO_j4P1TN4-JJGktMNEXgBxO4MXCr68o/edit#heading=h.s5znjvp3l4zi"><u>here</u></a> .</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/bo9c8jyHab4gjRHxS/results-from-the-turing-seminar-hackathon#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bo9c8jyHab4gjRHxS/results-from-the-turing-seminar-hackathon<guid ispermalink="false"> bo9c8jyHab4gjRHxS</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Thu, 07 Dec 2023 14:50:38 GMT</pubDate> </item><item><title><![CDATA[Gemini 1.0]]></title><description><![CDATA[Published on December 7, 2023 2:40 PM GMT<br/><br/><p>正在发生。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sundarpichai/status/1732414873139589372">Here is CEO Pichai&#39;s Twitter announcement</a> . <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/demishassabis/status/1732416482976673832">Here is Demis Hassabis announcing</a> . <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1732416095355814277">Here is the DeepMind Twitter announcement</a> . <a target="_blank" rel="noreferrer noopener" href="https://blog.google/technology/ai/google-gemini-ai/">Here is the blog announcement</a> . <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OriolVinyalsML/status/1732426317268758671">Here is Gemini co-lead Oriol Vinyals</a> , promising more to come. Here is <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffDean/status/1732415515673727286">Google&#39;s Chief Scientist Jeff Dean</a> bringing his best hype.</p><p> EDIT: This post has been updated for the fact that I did not fully appreciate how fake Google&#39;s video demonstration was.</p><span id="more-23623"></span><h4>技术规格</h4><p><a target="_blank" rel="noreferrer noopener" href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">Let&#39;s check out the specs.</a></p><p> Context length trained was 32k tokens, they report 98% accuracy on information retrieval for Ultra across the full context length. So a bit low, both lower than GPT—4 and Claude and lower than their methods can handle. Presumably we should expect that context length to grow rapidly with future versions.</p><p> There are three versions of Gemini 1.0.</p><blockquote><p> Gemini 1.0 是我们的第一个版本，具有三种尺寸：Ultra 用于高度复杂的任务，Pro 用于增强性能和大规模可部署性，Nano 用于设备上应用程序。每种尺寸都经过专门定制，以满足不同的计算限制和应用要求。</p><p> ……</p><p> Nano: Our most efficient model, designed to run on-device.我们训练了两个版本的 Nano，参数分别为 1.8B (Nano-1) 和 3.25B (Nano-2)，分别针对低内存和高内存设备。它是通过从较大的 Gemini 模型中提取来训练的。它采用 4 位量化进行部署，并提供一流的性能。</p><p> ……</p><p> Nano 系列模型利用精炼和训练算法方面的额外进步，为各种任务（例如摘要和阅读理解）生成一流的小语言模型，从而为我们的下一代设备上体验提供动力。</p></blockquote><p>这是有道理的。 I do think there are, mostly, exactly these three types of tasks. Nano tasks are completely different from non-Nano tasks.</p><p> This graph reports relative performance of different size models. We know the sizes of Nano 1 and Nano 2, so this is a massive hint given how scaling laws work for the size of Pro and Ultra.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5267840b-8509-4344-82a7-e570a0d6c82d_1075x466.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ohs44wqu09loeuohlixp" alt=""></a></figure><p> Gemini is natively multimodal, which they represent as being able to seamlessly integrate various inputs and outputs.</p><p> They say their benchmarking on text beats the existing state of the art.</p><blockquote><p>我们最强大的模型 Gemini Ultra 在我们报告的 32 个基准测试中的 30 个中取得了最新的结果，其中包括 12 个流行文本和推理基准测试中的 10 个、9 个图像理解基准测试中的 9 个、6 个视频理解基准测试中的 6 个，以及 5 个语音识别和语音翻译基准测试中的 5 个。 Gemini Ultra 是第一个在 MMLU（Hendrycks 等人，2021a）上实现人类专家表现的模型，MMLU 是通过一系列考试测试知识和推理的著名基准测试，得分高于 90%。除了文本之外，Gemini Ultra 在具有挑战性的多模式推理任务上也取得了显着的进步。</p></blockquote><p> I love that &#39;above 90%&#39; turns out to be exactly 90.04%, whereas human expert is 89.8%, prior SOTA was 86.4%. Chef&#39;s kiss, 10/10, no notes. I mean, what a coincidence, that is not suspicious at all and no one was benchmark gaming that, no way.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1fc5c4-473b-42be-87b8-61e85ce59c1d_1024x1024.webp" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/qauui9xd8kyjcdo3alxe" alt="由 DALL·E 生成"></a></figure><blockquote><p>我们发现，当与考虑模型不确定性的思维链提示方法（Wei 等人，2022）结合使用时，Gemini Ultra 可以实现最高的准确度。该模型使用 k 个样本（例如 8 个或 32 个）生成一个思想链。如果存在高于预设阈值（根据验证分割选择）的共识，则它会选择此答案，否则它将恢复为基于最大值的贪婪样本没有思路的可能性选择。</p></blockquote><p> I wonder when such approaches will be natively integrated into the UI for such models. Ideally, I should be able to, after presumably giving them my credit card information, turn my (Bard?) to &#39;Gemini k-sample Chain of Thought&#39; and then have it take care of itself.</p><p> Here&#39;s their table of benchmark results.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e9e608f-401f-4c88-802f-4534445e7d2f_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/uvwdrrwfwbiyjfvoosob" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_philschmid/status/1732435791358410863">So the catch with MMLU</a> is that Gemini Ultra gets more improvement from CoT@32, where GPT-4 did not improve much, but Ultra&#39;s baseline performance on 5-shot is worse than GPT-4&#39;s.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AISafetyMemes/status/1696509099490304250">Except the other catch is that GPT-4, with creative prompting, can get to 89%</a> ?</p><p> GPT-4 is pretty excited about this potential &#39;Gemini Ultra&#39; scoring 90%+ on the MMLU, citing a variety of potential applications and calling it a substantial advancement in AI capabilities.</p><p> They strongly imply that GPT-4 got 95.3% on HellaSwag due to data contamination, noting that including &#39;specific website extracts&#39; improved Gemini&#39;s performance there to a 1-shot 96%. Even if true, performance there is disappointing.</p><p> What does this suggest about Gemini Ultra? One obvious thing to do would be to average all the scores together for GPT-4, GPT-3.5 and Gemini, to place Gemini on the GPT scale. Using only benchmarks where 3.5 has a score, we get an average of 61 for GPT 3.5, 79.05 for GPT-4 and 80.1 for Gemini Ultra.</p><p> By that basic logic, we would award Gemini a benchmark of 4.03 GPTs. If you take into account that improvements matter more as scores go higher, and otherwise look at the context, and assume these benchmarks were not selected for results, I would increase that to 4.1 GPTs.</p><p> On practical text-only performance, I still expect GPT-4-turbo to be atop the leaderboards.</p><p> Gemini Pro clearly beat out PaLM-2 head-to-head on human comparisons, but not overwhelmingly so. It is kind of weird that we don&#39;t have a win rate here for GPT-4 versus Gemini Ultra.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99738558-c837-4633-a3fb-32cfa971f8fe_1182x214.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/jhqngog1yz1lyj7jxj32" alt=""></a></figure><p> Image understanding benchmarks seem similar. Some small improvements, some big enough to potentially be interesting if this turns out to be representative.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5065f262-f9f6-413f-8ddc-a0818caf96af_1230x706.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/puy4rl677dnmqgxdjqfa" alt=""></a></figure><p> Similarly they claim improved SOTA for video, where they also have themselves as the prior SOTA in many cases.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1381be4d-d1a6-40ec-bac3-c30d8db25c7d_1198x544.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/nvyh2lwcplusfkpaznpf" alt=""></a></figure><p> For image generation, they boast that text and images are seamlessly integrated, such as providing both text and images for a blog, but provide no examples of Gemini doing such an integration. Instead, all we get are some bizarrely tiny images.</p><p> One place we do see impressive claimed improvement is speech recognition. Note that this is only Gemini Pro, not Gemini Ultra, which should do better.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dd98ff1-8f37-480f-9aee-35661fe49c1b_1191x481.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ploxecxdydx5qs0oiatn" alt=""></a></figure><p> Those are error rate declines you would absolutely notice. Nano can run on-device and it is doing importantly better on YouTube than Whisper.很酷。</p><p> Here&#39;s another form of benchmarking.</p><blockquote><p> AlphaCode 团队构建了 AlphaCode 2（Leblond 等人，2023），这是一种由 Gemini 驱动的新型代理，它将 Gemini 的推理能力与搜索和工具使用相结合，擅长解决竞争性编程问题。 AlphaCode 2 在 Codeforces 竞争性编程平台上的参赛者中排名前 15%，比其排名前 50% 的最先进的前身有很大进步（Li 等人，2022）。</p><p> ……</p><p> AlphaCode 2 解决了 43% 的竞赛问题，比之前创下纪录的 AlphaCode 系统（解决了 25%）提高了 1.7 倍。</p></blockquote><p> I read the training notes mostly as &#39;we used all the TPUs, no really there were a lot of TPUs&#39; with the most interesting note being this speed-up. Does this mean they now have far fewer checkpoints saved, and if so does this matter?</p><blockquote><p> Maintaining a high goodput [time spent computing useful new steps over the elapsed time of a training job] at this scale would have been impossible using the conventional approach of periodic checkpointing of weights to persistent cluster storage.</p><p>对于 Gemini，我们使用了模型状态的冗余内存副本，并且在任何计划外的硬件故障中，我们可以直接从完整的模型副本中快速恢复。与 PaLM 和 PaLM-2（Anil 等人，2023）相比，尽管使用的训练资源要大得多，但恢复时间显着加快。结果，最大规模训练作业的总体产出从 85% 增加到 97%。</p></blockquote><p> Their section on training data drops a few technical hints but wisely says little. They deliberately sculpted their mix of training data, in ways they are keeping private.</p><p> In section 6 they get into responsible deployment. I appreciated them being clear they are focusing explicitly on questions of deployment.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc6332e-9c7f-4500-9cbc-56bcc60a297b_805x508.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/hpgjgrsg8kovglqo85ix" alt=""></a></figure><p> They focus (correctly) exclusively on the usual forms of mundane harm, given Gemini is not yet breaking any scary new ground.</p><blockquote><p>基于对已知和预期影响的理解，我们制定了一套“模型政策”来指导模型开发和评估。模型策略定义充当负责任开发的标准化标准和优先级架构，并作为启动准备情况的指示。双子座模型政策涵盖多个领域，包括：儿童安全、仇恨言论、事实准确性、公平和包容以及骚扰。</p></blockquote><p> Their instruction tuning used supervised fine tuning and RLHF.</p><p> A particular focus was on attribution, which makes sense for Google.</p><p> Another was to avoid reasoning from a false premise and to otherwise refuse to answer &#39;unanswerable&#39; questions. We need to see the resulting behavior but it sounds like the fun police are out in force.</p><p> It doesn&#39;t sound like their mitigations for factuality were all that successful? Unless I am confusing what the numbers mean.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bc6215f-49c0-4323-b4d1-9a2e69632abe_1219x244.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/xdhxwebhm5dyceamdfwk" alt=""></a></figure><p> Looking over the appendix and its examples, it is remarkable how unimpressive were all of the examples given.</p><p> I notice that I watch how honestly DeepMind approaches reporting capabilities and attacking benchmarks as an important sign for their commitment to safety. There are some worrying signs that they are willing to twist quite a ways. Whereas the actual safety precautions do not bother me too much one way or the other?</p><p> The biggest safety precaution is one Google is not even calling a safety precaution. They are releasing Gemini Pro, and holding back Gemini Ultimate. That means they have a gigantic beta test with Pro, whose capabilities are such that it is harmless. They can use that to evaluate and tune Ultimate so it will be ready.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://blog.google/technology/ai/google-gemini-ai/#capabilities">The official announcement</a> offers some highlights.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.wired.com/story/google-deepmind-demis-hassabis-gemini-ai/">Demis Hassabis talked to Wired about Gemini</a> . Didn&#39;t seem to add anything.</p><h4> Level Two Bard</h4><p> Gemini Pro, even without Gemini Ultra <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1732430045275140415">should be a substantial upgrade to Bard</a> . The question is, will that be enough to make it useful when we have Claude and ChatGPT available? I will be trying it to find out, same as everyone else. Bard does have some other advantages, so it seems likely there will be some purposes, when you mostly want information, where Bard will be the play.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/IntuitMachine/status/1732474666948661505">This video represents some useful prompt engineering</a> and reasoning abilities, used to help plan a child&#39;s birthday party, largely by brainstorming possibilities and asking clarifying questions. If they have indeed integrated this functionality in directly, that&#39;s pretty cool.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nonmayorpete/status/1732544321122152940">Pete says Bard is finally at a point where he feels comfortable recommending it</a> . The prompts are not first rate, but he says it is greatly improved since September and the integrations with GMail, YouTube and Maps are useful. It definitely is not a full substitute at this time, the question is if it is a good complement.</p><p> Even before Gemini, Bard did a very good job helping my son with his homework assignments, such that I was sending him there rather than to ChatGPT.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/goodside/status/1657396491676164096">Returning a clean JSON continues to require extreme motivation</a> .</p><p> When will Bard Advanced (with Gemini Ultra) be launched? <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Rodeo/will-bard-advanced-be-launched-by-j">Here&#39;s a market on whether it happens in January</a> .</p><h4> Gemini Reactions</h4><p> Some were impressed.其他人则没有那么多。</p><p> The first unimpressive thing is that all we are getting for now is Gemini Pro. Pro is very clearly not so impressive, clearly behind GPT-4.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elidourado/status/1732426687218941982">Eli Dourado:</a> Here is the table of Gemini evals from the paper.请注意，今天发布的是 Gemini Pro，而不是 Gemini Ultra。 So don&#39;t expect Bard to be better than ChatGPT Plus just yet.看起来和克劳德2差不多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1732425528727232772/history">西蒙？没有留下深刻印象。</a></p><p> Simeon: Gemini is here. Tbh it feels like it&#39;s GPT-4 + a bit more multimodality + epsilon capabilities. So my guess is that it&#39;s not a big deal on capabilities, although it might be a big deal from a product standpoint which seems to be what Google is looking for.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nyctibia/status/1732443540880290002">As always</a> , one must note that everything involved was chosen to be what we saw, and potentially engineered or edited. The more production value, the more one must unwind.</p><p> For the big multimodal video, this issue is a big deal.</p><blockquote><p> Robin: I found it quite instructive to <a target="_blank" rel="noreferrer noopener" href="https://t.co/YzDJV4YXTE">compare this promo video</a> <a target="_blank" rel="noreferrer noopener" href="https://t.co/mOuqS3OznG">with the actual prompts</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1732750022565879880">Robert Wiblin</a> (distinct thread): It&#39;s what Google themselves out put. So it might be cherry picked, but not faked. I think it&#39;s impressive even if cherry picked.</p></blockquote><p> Was this faked? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mariachong/status/1732812012969992326">EDIT: Yes</a> .是的。 Shame on Google on several levels.</p><p> Set aside the integrity issues, wow are we all jaded at this point, but when I watched that video, even when I assumed it was real, the biggest impression I got was… big lame dad energy?</p><p> I do get that this was supposedly happening in real time, but none of this is surprising me. Google put out its big new release, and I&#39;m not scared. If anything, I&#39;m kind of bored? This is the best you could do?</p><p> Whereas when watching the exact same video, others react differently.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/amasad/status/1732439083555631581">Amjad Masad</a> (CEO Replit): This fundamentally changes how humans work with computers.</p></blockquote><p> Does it even if real? I mean, I guess, if you didn&#39;t already assume all of it, and it was this smooth for regular users? I can think of instances in which a camera feed hooked up to Gemini with audio discussions could be a big game. To me this is a strange combination of the impressive parts already having been &#39;priced into&#39; my world model, and the new parts not seeming impressive.</p><p> So I&#39;m probably selling it short somewhat to be bored by it as a potential thing that could have happened. If this was representative of a smooth general multimodal experience, there is a lot to explore.</p><p> <a target="_blank" rel="noreferrer noopener" href="http://Gemini just dropped and, at least on the chosen benchmarks, seems to outperform GPT 4.0. Let's just say it's likely in the same ballpark.  This is not particularly surprising, Google and Deepmind have a world class team and immense resources.  I'm still puzzled as to what the commentators suggesting Google couldn't do it or wouldn't ship something like it were thinking.">Arthur thinks Gemini did its job</a> , but that this is unsurprising and it is weird people thought Google couldn&#39;t do it.</p><p> Liv Boeree?印象深刻。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Liv_Boeree/status/1732427988321419545/history">Liv Boeree:</a> This is pretty nuts, looks like they&#39;ve surpassed GPT4 on basically every benchmark… so this is most powerful model in the world?!哇哦，活着真是一个美好的时光啊。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1732428456837935471">Gary Marcus?</a> Impressed in some ways, not in others.</p><blockquote><p> Gary Marcus: Thoughts &amp; prayers for VCs that bought OpenAI at $86B.</p><p> Google Gemini 和 GPT-4 的热门观点： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">从许多方面来看，Google Gemini 似乎与 GPT-4 相匹配（或略微超过），但还没有超越它。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">从商业角度来看，GPT-4 不再是独一无二的。这对 OpenAI 来说是一个大问题，尤其是在戏剧性事件发生后，许多客户现在都在寻求备份计划。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">从技术角度来看，关键问题是：法学硕士是否已接近稳定期？</p><p>请注意，盖茨和奥特曼都已经暗示过，尽管商业需求巨大，但 GPT-5 在一年后还没有出现。谷歌尽管拥有所有资源，但并没有击败 GPT-4，这一事实很能说明问题。</p></blockquote><p> I love that this is saying that OpenAI isn&#39;t valuable both because Gemini is so good and also because Gemini is not good enough.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1732470963156250868">Roon offers precise praise</a> .</p><blockquote><p> Roon: congrats to Gemini team! it seems like the global high watermark on multimodal ability.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1732508388339396973">The MMLU result seems a bit fake</a> / unfair terms but the HumanEval numbers look like a actual improvement and ime pretty closely match real world programming utility</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidmanheim/status/1732444067785547973">David Manheim seems on point</a> (other thread): I have not used the system, but if it does only slightly outmatch GPT-4, it seems like slight evidence that progress in AI with LLMs is not accelerating the way that many people worried and/or predicted.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/joeykrug/status/1732451230541119851/history">Joey Krug is super unimpressed by the fudging on the benchmarks</a> , says they did it across the board not only MMLU.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/packyM/status/1732458006359462308">Packy McCormick: all of you</a> (shows picture)</p><p> Ruxandra Teslo: wait what happened recently? did they do something good?</p><p> Packy: they did a good!</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8193b99-4b9f-4dc7-b8dc-3518f8c1172a_726x900.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ulvtx8h1uy6lqvbsm22h" alt="图像"></a></figure><p> Google&#39;s central problem is not wokeness, it is that they are a giant company with lots of internal processes and powers that prevent or slow or derail innovation, and prevent moving fast or having focus. And there are especially problems making practical products, integrating the work of various teams, making incentives line up. There is lots of potential, tons of talent, plenty of resources, but can they turn that into a product?</p><p>还为时过早。 Certainly they are a long way from &#39;beat OpenAI&#39; but this is the first and only case where someone might be in the game. The closest anyone else has come is Claude&#39;s longer context window.</p><br/><br/><a href="https://www.lesswrong.com/posts/ofYejKKiSFYH2gLBb/gemini-1-0#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ofYejKKiSFYH2gLBb/gemini-1-0<guid ispermalink="false"> ofYejKKiSFYH2gLBb</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 14:40:07 GMT</pubDate> </item><item><title><![CDATA[Random Musings on Theory of Impact for Activation Vectors]]></title><description><![CDATA[Published on December 7, 2023 1:07 PM GMT<br/><br/><p> I want to share some thoughts on why activation vectors could be important, but before you read this post, you should know three things:</p><ol><li> I don&#39;t know what I&#39;m talking about</li><li> I don&#39;t know what I&#39;m talking about</li><li> I definitely don&#39;t know what I&#39;m talking about</li></ol><p> Also, maybe this post is redundant and unnecessary <span class="footnote-reference" role="doc-noteref" id="fnrefqrpuhuwrepp"><sup><a href="#fnqrpuhuwrepp">[1]</a></sup></span> because it&#39;s already been explained somewhere I haven&#39;t seen. Or maybe I&#39;ve seen someone write this elsewhere, but just can&#39;t remember the source.</p><p>但这里是这样的：</p><p> Why might activation vectors be important for alignment when we have other ways of controlling the network such as prompting or fine-tuning?</p><p> One view of activation vectors is that they&#39;re a hack or a toy. Isn&#39;t it cool that you can steer a network by just passing a word through the network and adding it?</p><p> Another view would be that they&#39;re tapping into something fundamental, see Beren&#39;s <a href="https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear">Deep learning models might be secretly (almost) linear</a> . In which case, we should expect this to keep working or even work better as networks scale up, rather than randomly breaking/stop working.</p><p> Another frame is as follows: language is an awkward format for neural networks to operate in, so they translate it into a latent space that separates out the important components, with of course some degree of superposition. Prompting and fine-tuning are roundabout ways of influencing the behavior of these networks: we&#39;re either pushing it into a particular simulation or training the network at to get better at fooling to think that it&#39;s doing a good job.</p><p> Ideally, we&#39;d just have enough interpretability knowledge that we&#39;d intervene on the latent space directly, setting or incrementing the exact right neurons. Activation vectors are a lot more scattergun than this, but they&#39;re closer to the ideal and work as a proof of concept.</p><p> The trick of performing, say, the subtraction &#39;happy-sad&#39; to get a happiness vector demonstrates how this can be refined by zeroing out all of the features that these have in common. These features like both being English, both being not overly formal, both being adjectives, ect. are probably mostly irrelevant. More advanced methods like <a href="https://arxiv.org/abs/2306.03341">Inference-Time Intervention</a> allow us to refine this further by only intervening on particular heads. This technique is still relatively simple; plausibly if we can find the right way of narrowing down our intervention we may achieve or get pretty close to our dream of directly intervening on the correct latents. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnqrpuhuwrepp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqrpuhuwrepp">^</a></strong></sup></span><div class="footnote-content"><p> I know that the word &quot;unnecessary&quot; is redundant and unnecessary here.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/k9z9fzpAgMLGQG9hg/random-musings-on-theory-of-impact-for-activation-vectors#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/k9z9fzpAgMLGQG9hg/random-musings-on-theory-of-impact-for-activation-vectors<guid ispermalink="false"> k9z9fzpAgMLGQG9hg</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Thu, 07 Dec 2023 13:07:09 GMT</pubDate> </item><item><title><![CDATA[Is AlphaGo actually a consequentialist utility maximizer?]]></title><description><![CDATA[Published on December 7, 2023 12:41 PM GMT<br/><br/><p> <strong>TL;DR: does stapling an adaptation executor to a consequentialist utility maximizer result in higher utility outcomes in the general case, or is AlphaGo just weird?</strong></p><p> So I was reading the <a href="https://sci-hub.se/10.1038/nature16961">AlphaGo paper</a> recently, as one does. I noticed that architecturally, AlphaGo has</p><ol><li> A value network: &quot;Given a board state, how likely is it to result in a win&quot;. I interpret this as an expected utility estimator.</li><li> Rollouts: &quot;Try out a bunch of different high-probability lines&quot;. I interpret this as a &quot;consequences of possible actions&quot; estimator, which can be used to both refine the expected utility estimate and also to select the highest-value action.</li><li> A policy network: &quot;Given a board state, what moves are normal to see from that position&quot;. I interpret this one as an &quot;adaptation executor&quot; style of thing -- it does not particularly try to do anything besides pattern-match.</li></ol><p> I&#39;ve been thinking of AlphaGo as demonstrating the power of consequentialist reasoning, so it was a little startling to open the paper and see that actually stapling an adaptation executor to your utility maximizer provides more utility than trying to use pure consequentialist reasoning (in the sense of &quot; <code>argmax</code> over the predicted results of your actions&quot;).</p><p> I notice that I am extremely confused.</p><p> I would be inclined to think &quot;well maybe the policy network isn&#39;t doing anything important, and it&#39;s just correcting for some minor utility estimation issue&quot;, but the authors of the paper anticipate that response, and include this extremely helpful diagram: <br><br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/stignrntrob8iut1lb8i" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/wgzp9l8tdkhkd4ww4fwy 158w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/dyubzuor5ag282sh4afd 238w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/qx4izvsywtbgiqvbscor 318w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/npaid2fabjxibsffqg0n" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/ykr2ntyggwrsbrh98zxr 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/xcrijzehmtdmibbfxp6x 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/skbdhontl2zavuyxw0b9 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/yf1fvk13bayljq7dzrdk 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/zybtothj0lz9vvnxbdwc 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/oohjk0rpvz1g2wp9oby8 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/mviljbisercemjdxb3ya 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/bnnj12mqweh6nbpz82tv 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/wh0zljshllk8it0rpc5g 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/tj62gwdysp2zlilrteyi 1082w"></p><p> The vertical axis is estimated Elo, and the dots along the X axis label represent which of the three components were active for those trials.</p><p> For reference, the following components are relevant to the above graph:</p><ol><li> The fast rollout policy <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\pi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>: a small and efficient but not extremely accurate network that predicts the probability that each legal move will be the next move, based on examining a fixed set of properties of the last move (eg &quot;is this move connected to the previous move&quot;, &quot;does the immediate neighborhood of this move/the previous move match a predetermined pattern&quot;). Accuracy of 24.2%.</li><li> The tree rollout policy <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\tau}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span></span></span></span></span></span></span></span></span></span> : like the fast rollout policy, but adds three more features &quot;move allows stones to be captured&quot;, &quot;manhattan distance to last 2 moves&quot;, and a slightly larger pattern (12 point diamond instead of 3x3 pattern) around this move. Details of both <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\pi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\tau}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span></span></span></span></span></span></span></span></span></span> are given in extended data table 4 if you&#39;re curious.</li><li> The SL policy network <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\sigma}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span></span></span></span></span></span></span></span></span></span> : a giant (by the standards of the time) 13 layer NN, pretrained on human games and then further trained through, if I&#39;m reading the paper correctly, learning to imitate a separate RL policy network that is not used anywhere in the final AlphaGo system (because the SL policy network outperforms it)</li><li> The value network <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{\theta}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span></span></span></span></span> : Same structure as the SL policy network except it outputs what probability the current board state has of being a win for the current player.</li><li> Rollouts: Pretty standard MCTS</li></ol><p> So my question:</p><p> <strong>Why does the system with the SL policy network do so much better than the system without it?</strong></p><p> A couple hypotheses:</p><ol><li> <strong>Boring Answer:</strong> The SL policy network just helps to narrow the search tree. You could get better performance by running the value network on every legal move, and then transforming the win probability for each legal move into a search weight, but that would require running the value network ~19x19=361 times per move, which is a lot more expensive than running the SL policy network once.</li><li> <strong>Policy network just adds robustness:</strong> A second, separately trained value network would be just as useful as the policy network.</li><li> <strong>Bugs in the value network:</strong> the value network will ever-so-slightly overestimate the value of some positions, and ever-so-slightly underestimate the value of others, based on whether particular patterns of stones that indicate a win or loss are present. If there is a board state that is in fact losing, but the value network is not entirely sure that the position is losing, moves that continue to disguise the fact that it is losing will be rated higher than moves that in fact improve the win chance,  but make the weakness of the position more obvious.</li><li> <strong>Consequentialism doesn&#39;t work actually:</strong> There is some deeper reason that using the value network plus tree search not only <i>doesn&#39;t</i> work, but <i>can&#39;t ever</i> work in an adversarial setting.</li><li> <strong>I&#39;m misunderstanding the paper:</strong> AlphaGo doesn&#39;t actually use the SL policy network the way I think it does</li><li> <strong>Something else entirely:</strong> These possibilities definitely don&#39;t cover the full hypothesis space.</li></ol><p> My pet hypothesis is (3), but realistically I expect it&#39;s (5) or (6). If anyone can help me understand what&#39;s going on here, I&#39;d appreciate that a lot.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Q8k3HSy7kELTXTbDY/is-alphago-actually-a-consequentialist-utility-maximizer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Q8k3HSy7kELTXTbDY/is-alphago-actually-a-consequentialist-utility-maximizer<guid ispermalink="false"> Q8k3HSy7kELTXTbDY</guid><dc:creator><![CDATA[faul_sname]]></dc:creator><pubDate> Thu, 07 Dec 2023 12:41:07 GMT</pubDate> </item><item><title><![CDATA[The GiveWiki’s Top Picks in AI Safety for the Giving Season of 2023]]></title><description><![CDATA[Published on December 7, 2023 9:23 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/crcPnJ6SwSBCTXHwP/the-givewiki-s-top-picks-in-ai-safety-for-the-giving-season#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/crcPnJ6SwSBCTXHwP/the-givewiki-s-top-picks-in-ai-safety-for-the-giving-season<guid ispermalink="false"> crcPnJ6SwSBCTXHwP</guid><dc:creator><![CDATA[Dawn Drescher]]></dc:creator><pubDate> Thu, 07 Dec 2023 09:23:05 GMT</pubDate></item></channel></rss>