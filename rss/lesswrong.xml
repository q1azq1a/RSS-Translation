<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 22 日星期日 20:11:13 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AI Safety is Dropping the Ball on Clown Attacks, and Mind Control in General]]></title><description><![CDATA[Published on October 22, 2023 8:09 PM GMT<br/><br/><p>认知状态：高可信度（~>;70%）认为小丑袭击很普遍，特别是被政府和/或情报机构故意武器化。非常高的置信度 (~>;90%) 认为人类大脑非常容易受到小丑攻击，并且缺乏对小丑攻击的认识会带来安全风险，就像使用“密码”一词作为密码一样，除非可以控制您的密码自己的思想受到威胁，而不是控制计算机的操作系统和/或文件。这已经是钢铁般的操作了； 10 年前/10 年后的误差线似乎相当宽。</p><p>这些概念很复杂，我已尽力让大多数人工智能安全领域的人尽可能容易地理解它，即使是没有量化背景的人（例如人工智能治理）。</p><p></p><p><strong>小丑袭击</strong></p><p>小丑袭击的核心动力在于，对社会地位的认知会影响人类大脑的想法和不愿意思考的内容。情报机构和政府可以利用这一点来完全拒绝访问特定的思想路线。一般来说，有很多方法可以通过采用目标概念并让错误的人在特定时间以特定方式说出它来对某人的世界模型进行社会工程。小丑攻击包括让右翼小丑成为谈论斯诺登爆料的主要人物，或者让去增长者成为谈论技术进步/人类能力将结束其净积极趋势的可能性的主要人物。这些只是使用特定情况（小丑）来改变某人对目标概念的感受的经济高效方法的具体示例。</p><p>对于小丑攻击，利用/零日是人类倾向于将特定的思维方式与低社会地位或低地位的人联系起来，这将持续抑制人类大脑追求该目标思维方式。</p><p>尽管小丑攻击本身看起来很平常，但它们是一个案例研究，证明强大的人类思维引导技术可能已经由人工智能公司发明、部署和大规模测试，并且很可能最终被武器化以对抗整个世界。未来 10 年某个时候的人工智能安全社区。</p><p></p><p><strong>人工智能安全至少是在小丑攻击时犯错</strong></p><p>人工智能安全基本上是一个由书呆子组成的社区，他们每个人都偶然发现了宇宙这一面的命运所围绕的工程问题。许多人<a href="https://www.lesswrong.com/posts/mC3oeq62DWeqxiNBx/estimating-the-current-and-future-number-of-ai-safety"><u>（约 300 人）</u></a>决定专门关注该工程问题，这似乎是一件非常合理的事情。然而，为了使其成为一个有价值的行动方案，人工智能安全社区必须继续存在，而不被外部对手或势力摧毁或<a href="https://www.lesswrong.com/posts/iNYdKoGsh4ffxbQ5t/navigating-an-ecosystem-that-might-or-might-not-be-bad-for"><u>收编</u></a>。在没有最终故障的情况下持续存在是目前人工智能安全社区中几乎每个人都没有质疑的假设。我们很大程度上认为一切都会好起来的，通用人工智能是唯一的转折点。这是一个危险的模型。</p><p>宗教和冷冻保存的历史告诉我们，围绕可怕的、不真实的和恶毒的自我毁灭性的信仰和做法，存在着一种不良共识的普遍现象。这是一种环境现象，也是人工智能安全状况的核心。因此，如果数十亿人除了忽视人工智能安全之外，还犯了其他错误，那么这并不会削弱人工智能安全的首要意义和优先事项。</p><p>直到今天，很大一部分人仍然认为比人类更聪明的人工智能只是科幻小说；当支付给人们思考未来的钱的 99% 以上都花在科幻小说作家而不是研究人员身上时，就会发生这种情况，对于人工智能来说，对于所有人类文明来说都是如此，直到大约一两年前。</p><p><strong>我在这里的论点是，利用 10 年前的技术，强大的人类操纵系统已经很容易构建</strong>，而且强大的人也很容易拒绝权力较小的人的访问。然而，目前的情况是，像小丑攻击这样的通用认知黑客甚至可以以惊人的高成功率拒绝目标人群对这项技术的认识，而不仅仅是获得该技术。</p><p>像加里·马库斯这样的人可能会试图劫持“p（doom）”等有价值的概念来解决他们自己的宠物问题，例如工作自动化，但另一方面，“缓慢起飞”可以通过多种方式改变世界这对于人工智能协调工作的连续性和生存具有实际意义。</p><p>重要的是要重申，直到今天，很大一部分人仍然认为比人类更聪明的人工智能只是科幻小说；当支付给人们思考未来的钱的 99% 以上都花在科幻小说作家而不是研究人员身上时，就会发生这种情况，对于人工智能来说，对于所有人类文明来说都是如此，直到大约一两年前。事实上，比人类更聪明的人工智能是人类的终点线，而以终点线为导向是利用时间做重要或有价值的事情的最佳方法之一，而不是无意识地做不重要或价值较低的事情。这也是让自己面向现实的好方法（尽管让自己面向现实对第一名的竞争非常激烈，因为这也往往会导致最终将人工智能作为人类的终点线，因此你以你自己的存在为导向，因为你是人类的一个子集）。再多的精神控制技术也无法取代比人类更聪明的人工智能，成为人类的终点线，但它可以成为我们的世界模型中极其有用的工具，帮助我们了解人工智能行业以及与人工智能竞赛相关的全球事务的期望（例如美中关系）。然而，归根结底，影响力技术是一个近期问题，它很可能会分散很多人对人工智能对齐这一最终且不可避免的问题的注意力。我在这里写这篇文章的唯一原因是，我认为人工智能安全社区如果做出更强有力的预测、拥有更强大的人工智能行业和人工智能竞赛模型，以及人工智能协调的缓慢起飞环境，将会变得更好。研究人员可能会被困在其中。</p><p>目前，人工智能安全领导者认为，随着人类能力的增强，人工智能的发展会缓慢，这是事实。并且已经发生了，具体取决于您的定义。但他们忽略了一个在数学上可证明的事实，即人工智能的信息处理能力很大程度上依赖于强大的心理学研究的新范式，默认情况下，它会极大地扩大人类思维的攻击面。</p><p>认知战并不是新的 X 风险或 S 风险。这是一个关键因素，我们需要了解它才能了解推动人工智能地缘政治和人工智能竞赛动态的因素。认知战不是人工智能安全的竞争对手，它不会卡住、插入，更不能让它夺走人工智能安全的注意力。</p><p>借助人工智能和大量人类行为数据，人类现在正在获得操纵和引导个人和系统的强大能力，并且人工智能和人类行为数据储备已经积累了十多年。</p><p>在这里，我要说明的是，强大的人类思想引导和行为操纵技术的条件已经成熟，并且已经有 10 年或更长时间了。因此，举证责任应该在于我们的思想是安全的并且攻击面很小，而不是我的主张我们的思想处于危险之中并且攻击面很大。我不喜欢在这里引用这个逻辑。这种逻辑应该优先考虑人工智能对齐，这是宇宙这一面所依赖的最终工程问题，而这个工程问题对人类来说可能就像火箭科学对黑猩猩来说一样困难。但安全背后的逻辑仍然是基本的，我认为我已经有力地证明了人工智能安全社区需要一定的抵御黑客攻击的门槛，而这个门槛可能还远远没有达到。我还认为大多数所需的解决方案都是快速且简单的修复，即使一开始看起来并非如此。</p><p>小丑攻击的存在证明，至少存在一种强大的认知攻击，可以被情报机构和大型社交媒体公司检测和利用，这种攻击利用了人脑中的零日漏洞，也对与人工智能安全相关的人起作用，直到明确发现并修补。</p><p>还有很多其他方法，特别是当您将人类的聪明才智与大量用户数据和多臂老虎机算法相结合时。人工智能只是多臂强盗算法的一种高级形式，而法学硕士只是另一个进步，因为他们可以真正阅读和理解帖子的内容，而不仅仅是衡量特定帖子组合引起的不同类型的人的行为变化。</p><p>社交媒体平台绝对有能力做到这一点；许多人甚至将社交媒体视为流行的风向标，尽管新闻推送算法可以大规模且精确地控制向哪些类型的人展示哪些内容、特定内容的展示频率以及哪些组合引导人们的思维以及可衡量方向的偏好。社交媒体甚至可以在 98% 的时间里准确地反映什么是真正流行的内容，以便获得信任，而保留剩下的 2% 的时间来主动确定什么会流行，什么不会流行。随着计算和算法能力的进步以及信任的巩固，这一比例可以接近 98%。</p><p>你不能只是对小丑的攻击置之不理，因为你会担心，如果你认真考虑这种想法，那么其他人会认为你站在小丑一边，你会失去在他们眼中的地位。足够强大的小丑攻击可以使这成为一个自我实现的预言，让每个人都相信某种特定的推理路线是低地位的，从而使其地位低下，并为追求特定的认知路线造成严重的现实后果。社交媒体新闻提要或其他算法控制的环境（例如 tiktok、reels）看起来像是随机生成的环境，而实际上平台（以及拥有平台服务器后门访问权限的人）非常有能力改变算法为了营造一个环境，使某些情绪在科学家或小丑等特定人群中显得比实际情况更普遍。或者，更糟糕的是，通过运行多臂老虎机算法或梯度下降来找到可以将人们的思维引导到可测量的方向的环境或帖子组合，从而将人们的思维引导到可测量的方向。小丑攻击只是我目前所知的多臂强盗算法所能达到的最强大的技术；可能还有很多其他仅基于社会地位的漏洞，这是人类大脑中非常严重的零日漏洞。</p><p>这强烈表明存在其他零日和强大的、难以发现的漏洞利用，包括（但不限于）利用人类本能来追求社会地位，并避免基于对社会地位获得和损失的预期而采取特定的思路。这些零日漏洞和漏洞要么是可以发现的，要么已经被有权势的人发现，他们必须秘密地大规模部署和完善这些漏洞，因为从数学上来说，这是它们发挥作用所必需的。这在数学上是可以证明的，为了获得人类行为的大量样本，以远远超越学术心理学所需的规模，秘密和大规模的部署是必要的，也许有能力的劳动力的 1/10 或更少。</p><p>如果有任何像魔术这样的东西可以同时侵入房间里每个人的思想而没有人注意到，就像 Eliezer Yudkowsky 的帖子<a href="https://www.lesswrong.com/posts/t2NN6JwMFaqANuLqH/the-strangest-thing-an-ai-could-tell-you"><u>人工智能可以告诉你的最奇怪的事情是什么一样</u></a>，那就是完全否认人们有能力思考特定的真实想法或进行明显有价值的探究，因为如果他们与该探究相关联，就会强烈担心失去社会地位。社会地位似乎是人类大脑在祖先环境中进化而来的优先考虑的东西，仅这一特征就使我们的认知变得容易破解。</p><p>阴谋论者都是小丑。肯尼迪遇刺事件可能是连接美国历史上两个至关重要的独立事件——古巴导弹危机（1962年）和越南战争（1964-1975年）——的关键因素。了解冷战和美国政府的历史对于形成当前形式的美国政府的准确模型至关重要（例如，了解中央情报局有时会劫持整个政权并针对他们无法策划的政变发动政变），包括人工智能安全适合的地方9/11 也是如此。然而，历史和世界模型中的这些关键点所吸引的人们和认知更像是围绕猫王之死的人们和认识论，而不是斯诺登的爆料（希望斯诺登的爆料最终不会完全陷入那个丑陋的深渊，尽管已经有很多社交媒体上的小丑积极尝试）。</p><p>了解当前认知战攻击的风险水平不仅需要安全思维，还需要安全思维加上适当的视角。它需要一长串特定漏洞利用的示例，以便您可以了解可能存在的其他内容，哪些内容在当前系统中很容易发现，由数百万人的行为数据提供支持，并且持续不断实时进行人工智能辅助实验和心理研究所需的访问权限。我希望小丑袭击是实现这一目标的一个有用的例子。</p><p>在 2020 年代，人均拥有律师人数比以往任何时候都高的世界里，合理的推诿是你应该预料到的事情。同样，办公室政治在精英中也非常普遍，所以一个人认识到通过制造谣言和替罪羊来使人们相互对立是一种制胜策略的门槛很低，那么人们就应该知道某些事情来自你。合理的推诿和假旗攻击并非始于网络攻击；它们都在 20 世纪流行起来。这也是小丑攻击如此强大的另一个原因；在当代文明中，周围的小丑非常普遍，因此很难区分小丑的攻击和噪音。由于被发现的风险极低，这种看似合理的推诿进一步刺激了小丑攻击；小丑攻击的预期成本基本上归结为服务器能源成本，因为被发现的净预期成本几乎为零。</p><p>实验室泄露假说和新冠病毒审查制度等关键信息最终都被归咎于右翼小丑的奇异替代现实，与披萨奴隶地牢和妊娠早期堕胎是谋杀同一个宇宙，这让分析人士感到震惊，尽管可能性对于任何试图构建 2020-22 年准确世界模型的人来说，实验室泄漏的数量和 Covid 信息篡改的程度显然都是至关重要的信息。这种明显性就被扼杀了。小丑攻击可以做这样的事情。人类的思维很大程度上取决于社会地位，这样的事情才能发挥作用。至少有一个零日漏洞。</p><p>决定人们眼中的低地位或恶棍与高地位或像你这样的英雄，正在产生一种非常强大的动力来塑造人们的想法，例如，SBF 是一个残暴的恶棍，主导着大多数人对 EA 和 AI 安全的理解。小丑攻击是我目前所知的最强大的认知黑客，特别是如果屏幕刷新率操纵永远不会最终部署的话。</p><p>现代行为操纵范式的一个重要要素是能够尝试大量的事情并看看什么是有效的；不仅仅是暴力破解已知策略的变体以使其更有效，而是首先暴力破解新颖的操纵策略。这完全避免了导致重复危机的稀缺性和研究缺陷，而重复危机至今仍然是心理学研究的瓶颈。事实上，我们文明中的原创心理学研究不再受到对聪明、有洞察力的人的需求的瓶颈，他们可以进行假设生成，这样你可以资助的有限研究希望能找到有价值的东西。仅使用当前的社交媒体范式，您就可以进行研究，例如新闻提要帖子的组合，<i>直到</i>找到有用的东西。可测量性对此至关重要。</p><p>通过将人与其他人进行比较并预测特征和未来行为，多臂老虎机算法可以首先预测特定的操纵策略是否值得冒险实施；导致高成功率和低检测率（因为检测可能会产生高度可测量的响应，特别是在大量传感器暴露的情况下，例如未覆盖的网络摄像头，因为将人们的微表情与失败或暴露的操纵策略的案例或工作网络摄像头视频进行比较数据转化为基础模型）。当您拥有数十亿小时的人类行为数据和传感器数据的样本量时，不同类型的人反应的毫秒差异（例如面部微表情、滚动过去涵盖不同概念的帖子时的毫秒差异、涵盖不同概念后的心率变化、眼球追踪）眼睛经过特定概念、触摸屏数据等后的差异从难以察觉的噪音转变为相关网络的基础。不管你喜欢与否，除非你使用箭头键，否则你滚动浏览每个社交媒体帖子（无论是使用触摸屏/垫还是鼠标滚轮）的速度都是一条曲线；仅滚动就是线性代数，它完全适合现代人工智能系统，每天都会生成数万亿条这样的曲线。</p><p>甚至不需要人工智能来进行小丑攻击，更不用说法学硕士了。相反，人工智能是“发明”小丑攻击等技术所需要的。自动化信息处理是寻找对人类有效的操纵技术所需要的一切。这种能力可能几年前就已经上线了。除非你拥有来自数百万不同的人的人类行为数据，这些数据都使用相同的受控环境，否则这是不可能完成的，而如果他们知道风险，他们就不会提供这些数据。</p><p>如果不运行算法本身，我不知道多臂老虎机算法会发现什么技术；我做不到，因为只有那些大量购买服务器的人才能访问这么多数据，即使对他们来说，这些数据也被大型科技公司（Facebook、亚马逊、微软、苹果和Google）和足够强大的情报机构（NSA 等）来防止黑客窃取和毒害数据。我也不知道当团队中的人是有能力的心理学家、舆论专家或其他公关专家解释和标记数据中的人类行为以便人类行为变得可测量时，多臂老虎机算法会发现什么。只需少数心理学专家的人类洞察力就足以训练人工智能自主工作；尽管需要这些专家的持续投入，并且大量的见解、行为和发现会被遗漏，并且需要额外的 3 年时间或一些东西才能被发现和标记。</p><p></p><p><strong>人工智能在全球事务中的应用</strong></p><p>小丑攻击并不是孤立地推进的，它们与人类思维的理解和利用的广泛加速并行，这<a href="https://arxiv.org/pdf/2309.15084.pdf"><u>本身就是加速人工智能能力研究的副产品</u></a>。例如，我们正在同时进入一个新时代，情报机构使用人工智能使测谎仪测试真正发挥作用，这<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#Functioning_lie_detectors_as_a_turning_point_in_human_history"><u>对于地缘政治事务来说绝对是变革性的，</u></a>目前地缘政治事务围绕着决策理论范式，其中每个员工<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/"><u>都是更有能力的人制造谎言的能力大于区分谎言的能力</u></a>，因此不能用“我100%忠诚”或“我知道这个团队中所有有能力和腐败的人是谁”之类的陈述来排序。</p><p>我对影响力技术的地缘政治意义的理解是，信息战的胜利目前被理解为国际冲突的主要胜利条件，类似于军事力量的征服；这种理解长期以来一直在政府和军事精英中盛行，从苏联解体和柏林墙倒塌开始，到所有东欧共产主义政权的垮台，可能从早在美国的越南反战运动中，但在反恐战争主导战场之后，精英们在2010年代左右达成了共识。在罗伯特·萨特（Robert Sutter）关于美中关系的著作和约瑟夫·奈（Joseph Nye）关于精英劝说的著作《软实力》（ <a href="https://www.amazon.com/Soft-Power-Means-Success-Politics/dp/1586483064"><u>Soft Power）</u></a>中，除其他许多地方外，都描述了这一共识。与常规战争和核战争不同，信息战既可以打也可以赢，它打击构成政府和军事机构最基本组成部分的人的思想，并且作为决定战争的最重要的目标之一有着悠久而丰富的历史。美国、俄罗斯/苏联和中国之间的大国冲突中的赢家和输家。因此，我们应该将信息战视为政府认真对待人工智能安全的原因之一；对来自外国政府的信息战的预期是当代美国和中国政权和军队的核心特征之一，这一点在分析人士中广为人知。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mjSjPHCtbK6TA5tfW/wabao1bvw0xiga2wdirz"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mjSjPHCtbK6TA5tfW/rvra5ms9ihw0qag8yssy"></p><p>从社交媒体暴露转向 1-1 和群组沟通仍然存在心理黑客的巨大风险，但最大限度地减少网络表面积/暴露于社交媒体仍将大幅且可能充分地降低风险（尽管需要访问技术本身才能为了验证这种充分性，足够大且安全的数据集足以实际进行操纵研究，可能仅限于足够大的科技公司和情报机构，如 Facebook 和 NSA，而较小的较弱的组织，如国土安全部或Twitter/X 很容易受到大型组织的黑客攻击和数据中毒，尽管有<a href="https://twitter.com/ESYudkowsky/status/1658571365950312448"><u>关于摩根大通部署复杂传感器系统的传言难以证实</u></a>，并且有报道称许多<a href="https://www.technologyreview.com/2018/04/30/143155/with-brain-scanning-hats-china-signals-it-has-no-interest-in-workers-privacy/"><u>与国家有联系的中国公司和机构一直在试验大型组织部署的脑电图的样本大小</u></a>）。</p><p>当前人工智能安全社区中心理黑客的攻击面过多且极端，即使是最低限度的解决方案也会受到阻力——手机网络摄像头难以掩盖，麦克风难以避免，社交媒体使用梯度下降来寻找和解决问题。利用引起习惯形成行为<a href="https://www.lesswrong.com/posts/8gbfvGhSnEJ9hHGew/10-reasons-why-lists-of-10-reasons-might-be-a-winning"><u>的帖子和帖子组合</u></a>（例如，为最大限度地降低退出率而进行的优化会导致系统找到并利用以奇怪的方式吸引人们的奇怪的帖子组合，例如产生一种模糊的感觉，即没有社交媒体的生活是一种当它现实时，苦行僧/僧侣般的存在是 <a href="https://www.lesswrong.com/posts/p2Qq4WWQnEokgjimy/respect-chesterton-schelling-fences"><u>默认的</u></a>）。默认情况下，戒掉主要的生活习惯也很困难，所以貌似合理的否认可能已经在这里再次出现了。</p><p>我在《劳特利奇监控手册》中添加了马克·安德烈杰维奇教授的可选描述，作为<a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding"><u>直觉洪流</u></a>，帮助更多人更容易地准确理解为什么世界实际上已经围绕这项技术运转。令人印象深刻的是，Andrejevic 的写作时间是 2014 年，并且没有表现出对人工智能有任何了解；他描述的系统在当时只需要数据科学和大量用户数据就可行。人工智能只是让这些系统更有能力获得结果。</p><blockquote><p>此类系统所需的数据量没有逻辑终点...所有信息都可能相关，因为它有助于揭示模式和相关性...</p><p>数字外壳中无处不在的监视所促进的人口级监视形式所特有的至少三种策略已成为新兴形式的数据驱动的商业和安全的核心：预测分析、情绪分析和受控实验。预测分析依赖于挖掘行为模式、人口统计信息和任何其他相关或可用数据来预测未来的行为[通过定位和观察相似的人或具有共同预测特征的人]...</p><p>预测分析是精算的，因为它并不对特定个人的未来行为做出明确的断言，而是根据概率进行交易，根据群体和个人与可以以已知准确度预测的模式的契合程度来解析群体和个人。预测分析依赖于收集尽可能多的信息，不是深入研究单个活动，而是挖掘新的模式、联系和相关性……</p><p>与预测分析相结合，情绪分析的目标既是先发制人的又是富有成效的：最大限度地减少负面情绪并最大限度地提高情感投资和参与度；不仅将情绪记录为给定的，而且将其作为变量进行调节。调制意味着不断的调整，旨在将未来建模的预期后果带入当下，从而以可能反过来重塑未来的方式来解释这些后果。例如，如果情绪分析揭示了对特定政策的抵制或对品牌或问题的担忧，那么目标就是以符合控制、捕获和跟踪数据的人的利益的方式管理这些情绪。目标不仅是监测人口，还包括对其进行管理......</p><p>这种形式的管理需要的不仅仅是监测和记录种群——它们还需要对其进行持续的实验。这就是预测分析在 Ian Ayres (2007) 所说的超级处理时代所采取的形式：不仅仅是试图了解潜在的人口统计或情感“真相”；而是试图了解潜在的人口统计或情感“真相”。甚至不是对有用相关性的持续搜索，而是这种相关性的持续生成。例如，在营销领域，数据研究人员利用交互式环境让消费者接受一系列正在进行的随机、受控实验。得益于交互式基础设施及其所实现的无处不在的监视形式，日常生活活动可以在虚拟实验室中捕获，在虚拟实验室中可以系统地调整变量以回答研究人员设计的问题......</p><p>无处不在的交互性和无处不在的监视的基础设施将日常生活的媒体体验转变为一个大型实验室......</p><p>此类实验可以以前所未有的规模实时进行。他们依赖于无处不在的必要性：关于尽可能多的人的尽可能多的信息.​​.....</p><p>总而言之，捕捉越来越多日常生活细节的技术以及利用该技术的策略导致使用数据的人的思维方式发生转变。克里斯·安德森 (Chris Anderson) 认为，数据库规模的急剧增加导致数据的描述性能力被其实际功效所取代。数据作为描述世界的参考信息的概念被对相关性的关注所取代。正如安德森（Anderson，2008）在《理论的终结》中所说：</p><p> “在这个世界上，大量数据和应用数学取代了所有其他可能发挥作用的工具。从语言学到社会学，每一种人类行为理论都被淘汰了……谁知道人们为什么要做他们所做的事情？关键是他们做到这一点，我们就可以以前所未有的保真度跟踪和测量它。有了足够的数据，数字就不言而喻”</p><p>这是信息过剩时代的数据哲学：为什么特定的相关性具有预测能力并不重要；重要的是重要的。只是它确实如此。它也是一种信息使用方式，赋予那些有权访问和控制大型数据库的人特权，因此，它为更普遍和更全面的监控提供了进一步的激励。</p><p>例如，当美国政府将《美国爱国者法案》中的监视条款免除《信息自由法案》规定的责任时，它是以安全的名义牺牲了责任……印度和阿拉伯联合酋长国等国家威胁要禁止黑莓消息设备，除非该公司向国家提供访问加密信息的能力，这进一步体现了国家为获取另一种类型的数字外壳中生成的数据而做出的努力......</p><p>失去对自己活动成果的控制。在商业和国家监视领域，我们捕获和记录的所有行为（以及它们的汇总和排序方式）都会被有权访问数据库的人系统地返回给我们。我们写的每条消息、发布的每个视频、购买或查看的每件物品、我们的时空路径和社交互动模式都成为算法中用于排序、预测和管理我们行为的数据点。其中一些数据点是自发的——消费者有意行为的结果；其他的则是诱导的，是正在进行的随机实验的结果......</p><p>很大程度上取决于预测能力能否转化为管理行为的能力，但这是营销人员以及那些采用其策略的国家机构[2014年]所下的赌注。</p></blockquote><p>就像大多数使用多臂强盗算法来让社交媒体引导人们的思维走向可测量的方向一样，小丑攻击不需要<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>有能力的政府或情报机构</u></a>就可以成功地针对数百万人的思想进行部署。除了大量的人类行为数据之外，这项技术所需的一切都很容易获取。 It simply requires sufficient access to social media platforms, and the technological sophistication of one software engineer who understands multi-armed bandit algorithms and one data scientist who can statistically measure human behavior, particularly for people with authority or involvement in extant mass surveillance systems, since enough data means that anyone could eyeball the effects. Measurability is still mandatory for multi-armed bandit algorithms to work, since nobody can see directly into the human mind (although there are many peripheral proxies like fMRI data, heart rate, blood pressure, verbal statements and tone, boy posture, subtle changes in hand and eye movements after reading certain concepts, etc and many of these can be constantly measured by a hacked smartphone).</p><p> Lie detectors and clown attacks are the two strongest case studies I&#39;m aware of that would cause AI to dominate global affairs and put AI safety in the crossfire; whether or not this has already happened is largely a question of the math; are AI capabilities to do these things obvious to engineers to tell us that major governments have probably already built the tech? A recent study indicated that a massive proportion of computer vision research is heavily optimized for human behavior research, analysis, and manipulation, and is surreptitiously mislabeled and obfuscated in order to conceal the human research and human use cases that form the core of the computer vision papers, eg <a href="https://arxiv.org/pdf/2309.15084.pdf"><u>a consistent norm of referring to the human research subjects as “objects” instead of subjects</u></a> .</p><p> There&#39;s just a large number of human manipulation strategies that are trivial to discover and exploit, even without AI (although the situation is far more severe when you layer AI on top), it&#39;s just that they weren&#39;t accessible at all to 20th century institutions and technology such as academic psychology. If they get enough data on people who share similar traits to a specific human target, then they don&#39;t have to study the target as much to predict the target&#39;s behavior, they can just run multi-armed bandit algorithms on those people to find manipulation strategies that already worked on individuals who share genetic or other traits. Although the average person here is much further out-of-distribution relative to the vast majority of people in the sample data, this becomes a technical problem, as AI capabilities and compute become dedicated to the task of sorting signal from noise and finding webs of correlation with less data. Clown attacks alone have demonstrated that zero days in the brain are fairly consistent among humans, meaning that sample data from millions or billions of people is useable to find a wide variety of zero days in the brains that make up the AI safety community.</p><blockquote><p> First it started working on 60% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as people in that 60%. Then, it started working on 90% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as the people in that 90%. Then it started working on me. And by then, it was already too late, because it was already working on me.</p></blockquote><p> Social media&#39;s ability to mix and match (or mismatch) people in specific ways and at great scale, as well as <a href="https://www.lesswrong.com/posts/fKNRHnxpjDLHnHdek/one-example-of-how-llm-propaganda-attacks-can-hack-the-brain"><u>using bot accounts just to upvote and signal boost specific messages so that they look popular</u></a> , already yielded powerful effects. Adding LLM bots into the equation merely introduces more degrees of freedom.</p><p> Among a wide variety of capabilities, platforms like twitter/X were capable of fiddling with their news feed algorithms such that users are incentivised to output as many words as possible in order to gain more likes/points/status, or as <a href="https://www.lesswrong.com/posts/8gbfvGhSnEJ9hHGew/10-reasons-why-lists-of-10-reasons-might-be-a-winning"><u>high-quality combinations of words</u></a> as they can muster, rather than whatever maximizes the compulsion to return to the platform the next day (a compulsion that is very, very easy to measure by looking at user retention rates and quit rates, and if it is easy to measure then it is easy to maximize via gradient descent). However, if a social media platform like twitter/X does not optimize for competitiveness against other social media platforms, and the other platforms do, then every subsequent day people will return to other social media platforms more and twitter less. The state of <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"><u>Moloch</u></a> is similar to the thought experiment where 4 social media platforms run multi-armed bandit algorithms to find ways to increase user engagement by 3 minutes per person per day, and if one platform of the four (let&#39;s imagine that it&#39;s twitter/X) eventually notices that the people themselves are spending 9 hours a day on social media, and twitter/X recoils in horror and decides to cease that policy and instead attempt to increase user engagement by 0 hours per person per day, then the autonomous multi-armed bandit algorithms running on other platforms automatically select strategies that harvest minutes of that time from twitter/X, the lone defector platform, in addition to harvesting minutes from the users&#39;s undefended off-screen IRL time, which is the undefended natural resource that&#39;s easiest for social media systems to steal time from, like an intelligent civilization harvesting an inanimate natural resource like plants or oil. The defector platform then loses its market share and is crowded out of the genepool.</p><p> People who can wield gradient descent as a weapon against other people are fearsome indeed (although only the type of person with access to user data and who buys servers by the acre can do this effectively). They not only have the ability to try things that already were demonstrated to work on people similar to you, they also have the ability to select attack avenues in places that you will not look and ways you will not notice, because they have a large enough amount of human behavioral data showing them all the places where people like you <i>did</i> end up looking in the past.</p><p> If true, this technology would, by default, become the darkest secret of the big 5 tech companies, and one of the darkest secrets of American and Chinese intelligence agencies. This tech would be the biggest deliberate abuse of psychological research in human history by far, and its effectiveness hinges on the current paradigm where billions of people output massive amounts of sensor data and social media scrolling data (eg the detailed pace at which different kinds of people scroll through different kinds of information), although the effectiveness of the clown attack on those not aware of it in particular demonstrates that a general awareness of the risk is not sufficient to protect oneself.</p><p> What would the world look like if human thought research and steering technology won out and became hopelessly entrenched 5-10 years ago? Unfortunately, that world would look a lot like this one. There are billions of inward facing webcams pointed at every face looking at every screen. <a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>The NSA stockpiles exploits in every operating system</u></a> and likely chip firmware as well. There are microphones in every room and accelerometers in the palm of every hand (which gives access to heart rate and a wide variety of other peripheral biophysical data that correlates strongly with various cognitive and emotional behavior). The very existence of mass surveillance is known, but that was only due to Snowden, which was one occasion and probably just bad luck; and the main aspect of the Snowden revelations was not that mass surveillance happens at incredible scale, but that the intelligence agencies were wildly successful at concealing and lying about it for years (and subsequently reorganized around the principle of preventing more Snowdens). An epistemic environment further and further in decline as social status, virtue signaling, and vague impressions dominate. And, last of all, international affairs is coming apart at the seams as the old paradigms die and trust vanishes. This is what a world looks like where powerful people have already gained the ability to access the human mind at a far deeper level than any target has access to; unfortunately, it is also what more mundane worlds look like, where human thought and behavior manipulation capabilities remained similar to 20th century levels. However, I&#39;ve made the case very strongly that such technology exists and that due to fundamental mathematical/statistical dynamics and due to human genetic diversity, these systems fundamentally depend on covert large-scale deployment (eg social media) in order to get sufficiently large amounts of data to run at all eg multi-armed bandit algorithms sufficient to find novel manipulation strategies in real time, and measuring and researching the human thought process sufficiently to use multi-armed bandit algorithms and SGD to steer a target&#39;s thoughts in measurable directions. Therefore, the burden of proof falls even more heavily on the claim that our minds are safe, safe enough for <a href="https://www.lesswrong.com/posts/iNYdKoGsh4ffxbQ5t/navigating-an-ecosystem-that-might-or-might-not-be-bad-for"><u>the AI safety community to survive the 2020s at all</u></a> , not on the claim that our minds are not secure and represent a severe point of failure.</p><p> The question of whether the cognitive warfare situation has already become severe is a question that must be approached with sober analysis, not vibes and vague impressions. Vibes and vague impressions are by far the easiest thing to hack, as demonstrated by the clown attack; and in a world where the situation was acute, then keeping people receptive and vulnerable to influence would be one of the most probable attacks for us to expect to be commonplace.</p><p> New technology actually does make current civilization out-of-distribution relative to civilization over the last 100 or 1000 years, and thus risks termination of norms, dynamics, and assumptions that have made everything in civilization go fine so far, such as humans being better at lying that detecting lies and thus not capable of organizing themselves based on statements of fact like “I am 100% loyal” or “here is an accurate list of the corrupt and incompetent people on this team”. The specific state of human controllability dominated global affairs eg via military recruitment, and when this controllability ratcheted up slightly in the 19th century, it resulted in the total war paradigm of the World War era and the information war paradigm of the Cold War era. Assuming that history will repeat itself, and remain as sensible and intuitive as it always was, is like expecting a psychological study to replicate in an out-of-distribution environment. It certainly <i>might</i> .</p><p> With the superior capabilities to research the human mind offered by the combination of social media and AI, governments, tech companies, and intelligence agencies now have the capability to understand aggregate consumer demand better than ever before and manipulate consumer spending and saving in real time, capabilities that were first sought in the 1980s by the Reagan and Thatcher administrations and never fully reached, but that was with 20th century technology; no social media, no mass surveillance, no user data or sensor data, no AI, only psychology (much of which would not replicate due to the study paradigm, which remains inferior to mass surveillance), statistics, focus groups, and polls, each of which were new at the time, and each of which remain available to governments, tech companies, and intelligence agencies today to supplement their new capabilities. It is for this reason that the paradigm of recessions, a paradigm solidified in the 20th century, is a paradigm that we might expect to die, along with many other civilizational paradigms that were only established in the 20th century due to the 20th century&#39;s relative absence of human behavior measurement and thought steering technology.</p><p><br></p><p> <strong>Taking a step back and looking at a fundamental problem</strong></p><p> If there were intelligent aliens, made of bundles of tentacles or crystals or plants that think incredibly slowly, their minds would also have zero days that could be exploited because any mind that evolved naturally would probably be like the human brain, a kludge of spaghetti code that is operating outside of its intended environment, and they would also would not even begin to scratch the surface of finding and labeling those zero days until, like human civilization today, they began surrounding thousands or millions of their kind with sensors that could record behavior several hours a day and find webs of correlations. Of course, if they didn&#39;t have much/any genetic diversity then it would be even easier to find those zero days, and vice versa for intense amounts of diversity. However, the power of the clown attack demonstrates that genetic diversity in humans is not sufficient to prevent zero days from being discovered and exploited; the drive to gain and avoid losing social status is hackable, with current levels of technology (social media algorithms, multi-armed bandit algorithms, and sentiment analysis), indicating that many other exploits are findable as well with current levels of technology.</p><p> There isn&#39;t much point in having a utility function in the first place if hackers can change it at any time. There might be parts that are resistant to change, but it&#39;s easy to overestimate yourself on this; for example, if you value the longterm future and think that no false argument can persuade you otherwise, but a social media news feed plants paranoia or distrust of Will Macaskill, then you are one increment closer to not caring about the longterm future; and if that doesn&#39;t work, the multi-armed bandit algorithm will keep trying until it finds something that works. The human brain is a kludge of spaghetti code, so there&#39;s probably something somewhere. The human brain has zero days, and the capability and cost of social media platforms to use massive amounts of human behavior data to find complex social engineering techniques is a profoundly technical matter, you can&#39;t get a handle on this with intuition or pre 2010s historical precedent. Thus, you should assume that your utility function and values are at risk of being hacked at an unknown time, and should therefore be assigned a discount rate to account for the risk over the course of several years. Slow takeoff over the course of the next 10 years alone guarantees that this discount rate is too high in reality for people in the AI safety community to continue to go on believing that it is something like zero. I think that approaching zero is a reasonable target, but not with the current state of affairs where people don&#39;t even bother to cover up their webcams, have important and sensitive conversations about the fate of the earth in rooms with smartphones, and use social media for nearly an hour a day (scrolling past nearly a thousand posts). The discount rate in this environment cannot be considered “reasonably” close to zero if the attack surface is this massive; and the world is changing this quickly. If people have <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>anything they value at all</u></a> , and the AI safety community probably does have that, then the current AI safety paradigm of zero effort is wildly inappropriate, it is basically total submission to invisible hackers.</p><p> The sheer power of psychological influence technologies informs us that we should stop thinking of cybersecurity as a server-only affair. Humans can also make mistakes as extreme as using the word “password” as your password, except it is your mind and your values and your impressions of different lines of reasoning that gets hacked, not your files or your servers or your bank passwords/records. In order to survive slow takeoff and persist for as long as necessary, the AI safety community must acknowledge the risk that the 2020s will be intensely dominated by actors capable of using modern technology to stealthily hack the human mind and eliminate inconvenient people, such as those try to pause AI, even though AI is basically the keys to their kingdom. We must acknowledge that the future of cyberwarfare doesn&#39;t just determine who gets to have their files and verbal conversations be private; in the 2020s, it determines what kinds of thoughts people get to have and what kinds of people do and don&#39;t get to have them at all (eg as one single example, clowns do have the targeted thoughts and the rest don&#39;t).</p><p> Everything that we&#39;re doing here is predicated on the assumption that powerful forces, like intelligence agencies, will not disrupt the operations of the community eg by inflaming factional conflict with false flag attacks attributed to each other due to the use of anonymous proxies.</p><p> Most people in AI safety still think of themselves as ordinary members of the population. In reality, this stopped being true a while ago; a bunch of nerds discovered an engineering problem that, as it turns out, the universe actually does revolve around, and a bunch of nerds messing around with technology that is central to geopolitics bears a reasonable chance that geopolitics will bite back, especially in a wild where intelligence agencies have, for decades, messed with and utilized influential NGOs in a wide variety of awful ways, and in a wild where intensely powerful influence technology like clown attacks becomes stronger and stronger determinants of geopolitical winners and losers.</p><p> If left to their own devices, people&#39;s decisions technology and social media will be dominated by their self-concept of themself as an average member of the population, who considers things like news feeds and smartphone sensors/uncovered webcams as safe because everyone&#39;s doing it and of course nothing bad would happen to them, when the cybersecurity reality is that the risk of psychological engineering is extreme and in a mathematically provable way, and this nonchalance towards strangers tampering/hacking your cognition and utility function is an unacceptable standard for the group of nerds who actually discovered an engineering problem that this side of the universe revolves around.</p><p> The attack surface of the AI safety community is like the surface area of the interior of a cigarette filter; thousands of square kilometers, wrinkled and folded together inside the spongey three-dimensional interior of a 1-inch long cigarette filter. This is not the kind community that survives a transformative world.</p><p> <strong>AI pause as the turning point</strong></p><p> From <a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Shutting Down the Lightcone Offices</u></a> :</p><blockquote><p> I feel quite worried that the alignment plan of Anthropic currently basically boils down to &quot;we are the good guys, and by doing a lot of capabilities research we will have a seat at the table when AI gets really dangerous, and then we will just be better/more-careful/more-reasonable than the existing people, and that will somehow make the difference between AI going well and going badly&quot;. That plan isn&#39;t inherently doomed, but man does it rely on trusting Anthropic&#39;s leadership, and I genuinely only have marginally better ability to distinguish the moral character of Anthropic&#39;s leadership from the moral character of FTX&#39;s leadership...</p><p> In most worlds RLHF, especially if widely distributed and used, seems to make the world a bunch worse from a safety perspective (by making unaligned systems appear aligned at lower capabilities levels, meaning people are less likely to take alignment problems seriously, and by leading to new products that will cause lots of money to go into AI research, as well as giving a strong incentive towards deception at higher capability levels)... The EA and AI Alignment community should probably try to delay AI development somehow, and this will likely include getting into conflict with a bunch of AI capabilities organizations, but it&#39;s worth the cost.</p></blockquote><p> I don&#39;t have much to contribute to the calculations behind this policy (which I&#39;d like to note is just musing by Habryka intended to elicit further discussion, and I might be taking it out of context), other than describing in great detail what a &quot;conflict with a bunch of AI capabilities organizations&quot; would look like, which I&#39;ve been researching for years and it is not pretty; the asymmetry is so serious that even thinking about waging such a conflict could begin closing the window of opportunity for you to make moves, eg if some of the people you talk to end up using social media and scrolling past specific bits of information at a pace similar to, say, for example, people who ultimately ended up seeing big tech companies as enemies, but in a cold and calculating and serious way, not in an advocacy way. Sample sizes of millions of people makes that kind of prediction possible; even if there are only a dozen people in the data set of positive cases of cold-bigtech-enmity, there are millions of people who make up the data set for negative cases, allowing analysts to get an extremely good idea of what a potential threat looks like by knowing what a potential threat doesn&#39;t look like. This is only one example, and it is entirely social-media based; it does not use, say, automated analysis of audio data from recorded conversations near hacked smartphones, which is very unambiguously the kind of thing that can be expected to happen to people who would &quot;get into conflict with a bunch of AI capabilities organizations&quot; as those organizations tend to have strong ties, and possibly even substantial revolving door employment, with intelligence agencies; Facebook/Meta is a good example, as they routinely find themselves at the center of public opinion and information warfare conflicts around the world. It&#39;s also unclear to me how sovereign these companies&#39;s security departments are without continued logistical support and staff from American intelligence agencies, as they have to contend with intense interest from a wide variety of foreign intelligence agencies. There is an entire second world here, that is 1) parallel to the parts of the ML community that are visible to us, 2) vastly more powerful, privileged, and dangerous than the ML community, and 3) has a massive vested interest in the goings-on of the ML community, and I&#39;ve encountered dozens and dozens of people in the AI safety community in both SF/berkeley and DC, and if a single person was aware of this second world, they were doing an incredibly good job totally hiding their awareness of it. I think this is a recipe for disaster. I think that the AI safety community is not even thinking about the kinds of manipulation, subterfuge, and sabotage that would take place here just based off of this world&#39;s lawyers-per-capita alone, and the fact that this is a trillion-dollar industry, let alone that this is a trillion-dollar industry due in part to the human influence capabilities I&#39;ve barely begun to describe here, let alone due to interest that these capabilities attracted from all the murkiest people lurking within the US-China conflict.</p><p> The attempt to open source Twitter/X&#39;s newsfeed algorithm might have been months ago, but even if it was a step in the right direction, to repeatedly attempt projects like that would cause excessive disruptions and delegitimizations for the industry, particularly Facebook/Meta which will never be able to honestly open-source its systems&#39;s news feed algorithms. Facebook and the other 4 large tech companies (of whom Twitter/X is not yet a member due to vastly weaker data security) might be testing out their own pro-democracy anti-influence technologies and paradigms, akin to Twitter/X&#39;s open-sourcing its algorithm, but behind closed doors due to the harsher infosec requirements that the big 5 tech companies face. Perhaps there are ideological splits among executives eg with some executives trying to find a solution to the influence problem because they&#39;re worried about their children and grandchildren ending up as floor rags in a world ruined by mind control technology, and other executives nihilistically marching towards increasingly effective influence technologies so that they and their children personally have better odds of ending up on top instead of someone else. Twitter/X&#39;s measured pace by ope sourcing the algorithm and then halting several months afterwards is therefore potentially a responsible and moderate move in the right direction, especially considering <a href="https://twitter.com/ESYudkowsky/status/1712908282581631372"><u>the apparent success of the community notes paradigm at improving epistemics</u></a> .</p><p> The AI safety community is now in a situation where it has to do everything right. The human race must succeed at this task, even if the human brain didn&#39;t evolve to do well at things like having the entire species coordinate to succeed at a single task. Especially if that task, AI alignment, might be absurdly difficult entirely for technical reasons, as difficult for the human mind to solve as expecting chimpanzees to figure out enough rocket science to travel to the moon and back, which would be a big ask regardless of the chimpanzees&#39;s instinctive tendency to form factions and spend 90% of their thinking on clever plots to outmaneuver and betray each other. This means that vulnerability to clown attacks is unacceptable for the AI safety community, and it is also unacceptable to be vulnerable to other widespread social engineering techniques that exploit zero days in the human brain. The degree of vulnerability is highly measurable by attackers, and increasingly so as technology advances, and since it is legible that attackers will be rewarded for exploiting vulnerabilities, it is therefore incentivised for attackers to exploit those vulnerabilities and steer the AI safety community over a cliff.</p><p> The AI safety community has long passed a threshold where vulnerability to clown attacks is no longer acceptable; not only does it incentivize more clown attacks, and more ambitious clown attacks, where the attackers have more degrees of freedom, but the AI safety community is in a state where clown attacks can thwart many of the tasks required to do AI safety at all.</p><p> Lots of people approach AI social control as though solving AI alignment is priority #1 and the use of AI social control is priority #2. However, this attitude is not coherent. AI alignment is #1, and AI social control is #1a, a subset of #1 with virtually no intrinsic value on its own, only instrumental value to AI alignment, as the use of AI for social control would incentivise accelerating AI and complicate alignment efforts in the meantime, either by direct sabotage by intelligence agencies or AI companies, or by causing totalitarianism or all-penetrating crushing information warfare between the US and China, or some other state of civilization that we might fail to adapt to.</p><p></p><p> <strong>How to protect yourself and others:</strong></p><ol><li> Either stop reasoning based on vibes and impressions (hard), or stop spending hours a day inside hyperoptimized vibe/impression hacking environments (social media). This may sound like a big ask, but it actually isn&#39;t, like cryopreservation; everyone on earth happens to be doing it catastrophically wrong and its actually a super quick fix, less than a few days or even a few hours and your entire existence is substantially safer. But more than 99% of people on earth will look at you as if you were wearing a clown suit, and that intimidates people away from specific lines of thought in a very deep way.</li><li> A critical element is for as many people as possible in AI safety to cover up their webcams; facial microexpressions are remarkably revealing, especially to people with access to billions of hours of facial microexpression data of people in flexible hypercontrolled environments like the standard social media news feed. Facial microexpressions are the output of around 60 different facial muscles, and although modern ML probably can&#39;t compress each frame of a video file to a 6 by 10 matrix that represents how contracted each facial muscle is, modern ML bably compress, say, every 5 frames into a 128 by 128 matrix that represents the state of the 20% of the facial muscles that project ~80% of the valuable data/signal outputted by facial muscles. Minimizing exposure to other sensors is ideal as well; it <a href="https://www.wired.com/2016/11/great-now-even-headphones-can-spy/"><u>seems pretty likely that a hacked OS can turn earbuds into microphones</u></a> so definitely switch to used speakers. I use a wireless keyboard so that I don&#39;t have to worry about the accelerometers in my laptop, and a trackball mouse that I use with my off hand. Some potential exploits, like converting RAM chips or accelerometers, mean that it might be difficult to prevent laptops from recording enough audio to acquire changes in your heart rate itself, but there&#39;s a wide variety of other biophysical data that it&#39;s not in your best interest to donate (if you&#39;re going to sell shares of your utility function, at least don&#39;t sell them for zero dollars).</li><li> It&#39;s probably a good idea to switch to physical books instead of ebooks. Physical books do not have operating systems or sensors. You can also print out research papers and Lesswrong and EAforum articles that you already know are probably worth reading or skimming; if you drive to the store and actually spend 15 minutes looking, you will probably find an incredibly ink-efficient printer.</li><li> I&#39;m not sure whether a text-only inoculation is good enough, or whether there&#39;s no way around the problem other than reducing people&#39;s exposure to sensors and social media. Reading <a href="https://www.readthesequences.com/"><u>Yudkowsky&#39;s rationality sequences</u></a> will definitely make one&#39;s thoughts and behaviors harder to predict, but it won&#39;t patch or rearrange the zero days in the human brain. Even <a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies"><u>Tuning your Cognitive Strategies</u></a> and Raemon&#39;s <a href="https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality">Feedbackloop First Rationality</a> won&#39;t do that, although it might go a long way towards making it much harder to gather data on the details of those zero days by comparing your mind and behavior to the minds and behavior of millions of people that make up the data, because your mind will be just be more different from those people than before, and you will be different from them in very different ways from how they are different from each other. You will be out-of-distribution, and the distribution is a noose that is slowly tightening around the mind of most people on earth. If reading the sequences seem daunting, I recommend starting with the <a href="https://www.lesswrong.com/highlights"><u>highlights</u></a> or <a href="https://www.lesswrong.com/posts/PouWW8Ys4iKhWedCZ/psa-the-sequences-don-t-need-to-be-read-in-sequence"><u>randomly selecting posts from them out-of-order</u></a> which is a fantastic way to start the day. <a href="http://hpmor.com"><u>HPMOR</u></a> is also an incredibly fun and enjoyable alternative to read in your down time if you&#39;re busy (and possibly one of the most rereadable works of fiction ever written), and <a href="https://www.lesswrong.com/posts/SA9hDewwsYgnuscae/projectlawful-com-eliezer-s-latest-story-past-1m-words"><u>Planecrash</u></a> is a similarly long and ambitious work by Yudkowsky that is darker and less well-known, even though the self-improvement aspect seems more refined and effective. Other safe predictability reducers include <a href="https://www.lesswrong.com/codex"><u>Scott Alexander&#39;s codex</u></a> for a more world-modeling focus, and the <a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction"><u>CFAR handbook</u></a> for more practical self-enhancement work. <a href="https://www.lesswrong.com/about#Getting_Started_on_LessWrong"><u>These are all highly recommended</u></a> for self-improvement.</li><li> It&#39;s probably best to avoid sleeping in the same room as a smart device, or anything with sensors, an operating system, and also a speaker. The attack surface seems large, if the device can tell when people&#39;s heart rate is near or under 50 bpm, then it can test all sorts of things eg prompting sleep talk on specific topics. The attack surface is large because there&#39;s effectively zero chance of getting caught, so if people were to experiment with that, it wouldn&#39;t matter how low the probability of success is for any idea to try or whether they were <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally"><u>just engaging in petty thuggery</u></a> . Just drive to the store and buy a clock, it will be like $30 at most and that&#39;s it.</li><li> Of course, you should be mindful of what you know and how you know it, but in the context of modern technologies, it&#39;s more important to pay attention to your feelings, vibe, and impression of specific concepts, and you successfully search your mind and trace the origins of those feelings, you might notice the shadows cast by the clowns that have fouled up targeted concepts (you probably don&#39;t remember the clowns themselves, as they are nasty internet losers). But that&#39;s just one zero day, and the problem is bigger and more fundamental than that. Patching the clown attack is valuable, but it&#39;s still a band-aid solution that will barely address the root of the issue.</li><li> I am available via LW message to answer any questions you might have.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind<guid ispermalink="false"> mjSjPHCtbK6TA5tfW</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sun, 22 Oct 2023 20:09:31 GMT</pubDate> </item><item><title><![CDATA[The Drowning Child]]></title><description><![CDATA[Published on October 22, 2023 4:39 PM GMT<br/><br/><p> <i>Epistemic status: possibly very evil devil&#39;s advocacy, philospohicly unsophisticard ruminations</i></p><p> We are all familiar with Singer&#39;s drowning child thought experiment. I have often found it very compelling, or at least felt I ought to find it very compelling. My revealed preferences, as with those of nearly everyone else, tell a different story.</p><p> But it has always struck me that Singer&#39;s story can be read in two directions, and social desirability bias would likely prevent one from reading it aloud in the less-common way.</p><p> If a though experiment is to drastically alter one&#39;s life, it seems worth a little devil&#39;s advocacy.</p><p> In this spirit, I ask if the fact that one does not donate all their energy to ameliorating the plight of the word&#39;s most impoverished provide some evidence that one does not, in fact, care as much about the near-by drowning child as they think they do?</p><p> What are some purely-selfish reasons one might have for saving the child. I can think of a few.</p><ol><li> You gain some prestige as a hero.</li><li> You demonstrate your strength and competence publicly in the event someone witnesses your heroism.</li><li> It is fun to imagine heroic acts in your head that you would not, in fact, do in real life.</li><li> The gratefulness of the child and/or their parents isn&#39;t without value.</li><li> Saving a child in this way is a novel, interesting experience.</li></ol><p> If we alter Singer&#39;s story such that a hero-for-hire offers to save the near-by child, and he asks for a sum equivalent to the price of your suit (and a guarantee of that you will keep the transaction confidential) we can remove the weight of these motivations. Supposing you cannot swim and the hero-for-hire is the only person around, would you take this deal?</p><p> I think I would take the deal, but if I&#39;m honest with myself it does seem very slightly less compelling.</p><p> If we extend this scenario and imagine that accepting this deal every week would mean sacrificing half your annual salary, the decision becomes even more complex. Would the majority still take the deal?  Or would they perhaps find a path to their office where large bodies of water are not in view?</p><br/><br/> <a href="https://www.lesswrong.com/posts/nKYFcbpkts3r5Aaoe/the-drowning-child#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/nKYFcbpkts3r5Aaoe/the-drowning-child<guid ispermalink="false"> nKYFcbpkts3r5Aaoe</guid><dc:creator><![CDATA[Tomás B.]]></dc:creator><pubDate> Sun, 22 Oct 2023 16:39:53 GMT</pubDate> </item><item><title><![CDATA[Announcing Timaeus]]></title><description><![CDATA[Published on October 22, 2023 11:59 AM GMT<br/><br/><figure class="image image_resized" style="width:84.57%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/r2hfjkg0aivtxcmt1m3t" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/egfbfbdsjgfjcygvdvmu 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/p25ca8nvvixbxqb65vyx 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/p3i0bbvd5nchdzyi2wja 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/ewrhwdklmk47lylk2e41 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/h4zxcxndnryfyhebnjzh 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/x3ngay4j13q71wliyizn 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/dizix39u6ookbvjzqdep 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/vhbmzeiwuvmxolk8lc6s 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/dfdeyh6kpo2vb2utiriw 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/miwtfmt3bth24j71kzhe 1990w"></figure><p> <a href="https://timaeus.co/">Timaeus</a> is a new AI safety research organization dedicated to making fundamental breakthroughs in technical AI alignment using deep ideas from mathematics and the sciences. Currently, we are working on <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">singular learning theory</a> and <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability">developmental interpretability</a> . Over time we expect to work on a broader research agenda, and to create <a href="https://www.lesswrong.com/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations#:~:text=Rather%20than%20evaluating%20a%20final,and%20why%20they%20got%20it.">understanding-based evals</a> informed by our research. </p><figure class="image image_resized" style="width:30.04%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/i4vhfj18dtvrxtijdc65"><figcaption> Let sleeping gods (not) lie.</figcaption></figure><h1> Activities</h1><p> Our primary focus is research and research-training. For now, we&#39;re a remote-first organization. We collaborate primarily through online <a href="https://metauni.org/slt/">seminars</a> and the <a href="https://discord.gg/pCf4UynKsc">DevInterp Discord</a> , with regular in-person meetings at workshops and conferences (see below). We&#39;re also investing time in academic outreach to increase the general capacity for work in technical AI alignment.</p><h2> Research</h2><p> We believe singular learning theory, a mathematical subject founded by <a href="https://watanabe-www.math.dis.titech.ac.jp/users/swatanab/">Sumio Watanabe</a> , will lead to a better fundamental understanding of large-scale learning machines and the computational structures that they learn to represent. It has already given us concepts like the <a href="https://arxiv.org/abs/2308.12108">learning coefficient</a> and <a href="https://arxiv.org/abs/2310.06301">insights into phase transitions</a> in Bayesian learning. We expect significant advances in the theory to be possible, and that these advances can inform new tools for alignment.</p><p> <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability">Developmental interpretability</a> is an approach to understanding the emergence of structure in neural networks, which is informed by singular learning theory but also draws on mechanistic interpretability and ideas from statistical physics and <a href="https://journals.biologists.com/dev/article/150/11/dev201280/312613/A-dynamical-systems-treatment-of-transcriptomic">developmental biology</a> . The key idea is that <i>phase transitions organize learning</i> and that <i>detecting, locating, and understanding these transitions</i> could pave a road to evaluation tools that <i>prevent</i> the development of dangerous capabilities, values, and behaviors. We&#39;re engaged in a <a href="https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc">research sprint</a> to test the assumptions of this approach.</p><p> We see these as two particularly promising research directions, and they are our focus for now. Like any ambitious research, they are not guaranteed to succeed, but there&#39;s plenty more water in the well. Broadly speaking, the research agenda of Timaeus is oriented towards solving problems in technical AI alignment using deep ideas from across many areas of mathematics and the sciences, with a &quot;full stack&quot; approach that integrates work from pure mathematics through to machine learning experiments.</p><p> The outputs we have contributed to so far:</p><ul><li> <a href="https://arxiv.org/abs/2308.12108">Lau et al. (2023)</a> &quot;Quantifying Degeneracy in Singular Models via the Learning Coefficient&quot;</li><li> <a href="https://arxiv.org/abs/2310.06301">Chen et al. (2023)</a> &quot;Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition&quot;</li><li><a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">Hoogland &amp; Van Wingerden (2023)</a> , a distillation of Lau et al</li><li> <a href="https://github.com/timaeus-research/devinterp">The DevInterp Repo &amp; Python Package</a></li><li> <a href="https://www.youtube.com/watch?v=bFVGc2UKARc">The 2023 Primer on Singular Learning Theory and Alignment</a> </li></ul><figure class="image image_resized" style="width:34.5%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/d45vlbzseicber1xdybu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/n7lrowm9rxipgoqddzj2 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/a4k0lljwxtjimj24ukb8 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/y6tdafcsm5qtccuuquux 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/lb2p9p5slaw3ovayqz87 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/gwd4wgilmtizvasl06gd 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/isodlvkzpgapsribmbav 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/m2popvouttzdcusd1ext 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/tgrb3xtajifnziadgivy 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/qp3yi3kkqbcjotps86rg 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/uh16kmaxu0mnoyfqmw3k 1158w"></figure><h2> Academic Outreach</h2><p> AI safety remains bottlenecked on senior researchers and mentorship capacity. The young people already in the field will grow into these roles. However, given the scale and urgency of the problem, we think it is important to open inroads to academia and encourage established scientists to spend their time on AI safety.</p><p> Singular learning theory and developmental interpretability can serve as a natural bridge between the emerging discipline of AI alignment and existing disciplines of mathematics and science, including physics and biology. We plan to spend part of our time onboarding scientists into alignment via concrete projects in these areas.</p><p> As part of these efforts, we&#39;re aiming to pilot a course at the University of Melbourne in late 2024 on &quot;Mathematical Methods in AI Safety&quot;, similar in spirit to existing graduate courses on Mathematical Methods in Physics. </p><figure class="image image_resized" style="width:36.48%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/jc7ziq8qobgcfh8rc1bk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/cvpajbfvykbfyvuasz5g 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/r4valg883o9qgdeeq0ww 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/docodisrytmrxzd0ybth 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/eofh8lxpap1ud58n34yt 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/yaqewcfru9bgdgvglbpo 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/dhj5flqmu8sj2uwhokqi 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/ev5vtzcrtjf25rkd1isc 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/lgoaodrar7c6xqlaplmw 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/ddhv4npc27ze0z9jsntr 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/bxtrnnbbf5aqzqmbuiex 1024w"></figure><h2>会议</h2><p>We&#39;re organizing conferences, retreats, hackathons, etc. focusing on singular learning theory and developmental interpretability. These have included and will include:</p><ul><li> <a href="https://www.lesswrong.com/posts/HtxLbGvD7htCybLmZ/singularities-against-the-singularity-announcing-workshop-on">Conference on singular learning theory and AI alignment</a> (Berkeley, June-July 2023)</li><li> <a href="https://devinterp.com/events/2023-q3-amsterdam-retreat">Retreat on singular learning theory</a> (Amsterdam, September 2023)</li><li> <a href="https://events.humanitix.com/hackathon-developmental-interpretability">Hackathon on developmental interpretability</a> (Melbourne, October 2023)</li><li> <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-now-for-the-devinterp-2023-fall-summit">Conference on developmental interpretability</a> (Oxford, November 2023)</li><li> <a href="https://devinterp.com/participate/research-sprints">AI safety demo days</a> (Virtual, November 2023) open to independent and junior researchers to present their work.</li><li> We&#39;ve been amassing a list of <a href="https://devinterp.com/projects">project ideas</a> to inspire researchers <a href="https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability#Overview_of_Sequence">.</a></li></ul><h1> Team</h1><h2> Core Team</h2><ul><li> <a href="http://therisingsea.org/"><strong>Daniel Murfet</strong></a> (Research Director) is a mathematician at the University of Melbourne, an expert in singular learning theory, algebraic geometry, and mathematical logic.</li><li> <a href="https://jessehoogland.com"><strong>Jesse Hoogland</strong></a><strong> </strong>(Executive Director) has a MSc. in theoretical physics from the University of Amsterdam and previously cofounded a health-tech startup before going all in on AI safety. He participated in MATS 3.0 &amp; 3.1 under the supervision of Evan Hubinger and was a research assistant in David Krueger&#39;s lab.</li><li> <a href="https://www.lesswrong.com/users/stan-van-wingerden?mention=user"><strong>Stan van Wingerden</strong></a> (Operations &amp; Finances) has a MSc. in theoretical physics from the University of Amsterdam and was previously the CTO of an algorithmic trading firm.</li><li> <a href="https://www.lesswrong.com/users/alexander-gietelink-oldenziel?mention=user"><strong>Alexander Gietelink Oldenziel</strong></a><strong> </strong>(Strategy &amp; Outreach) is a DPhil student at University College London with a background in mathematics.</li></ul><h2> Research Assistants</h2><p> We just concluded a round of hiring and are excited to bring on board several very talented Research Assistants (RAs), starting with</p><ul><li> Sai Niranjan</li><li> <a href="https://www.georgeyw.com/"><strong>George Wang</strong></a></li></ul><h2> Friends and Collaborators</h2><p> Since beginning to plan Timaeus in June 2023 we have been engaging with a range of people, both within the field of AI alignment and in academia. Here are some of the people we are actively collaborating with:</p><ul><li> <a href="https://www.suswei.com/"><strong>Susan Wei</strong></a> (Statistician, University of Melbourne)</li><li> <a href="https://scholar.google.com/citations?user=O3CEqUoAAAAJ&amp;hl=en"><strong>Calin Lazaroiu</strong></a> (Mathematical Physicist, UNED-Madrid and IFIN-HH Bucharest)</li><li> <a href="https://simon-pepin.github.io/"><strong>Simon Pepin Lehaulleur</strong></a> (Mathematician, Postdoc at University of Amsterdam)</li><li> <a href="https://tfburns.com/"><strong>Tom Burns</strong></a> (Neuroscientist, Postdoc at ICERM, Brown University)</li><li> <a href="https://edmundlth.github.io/"><strong>Edmund Lau</strong></a> (PhD student, University of Melbourne)</li><li> Zhongtian Chen (PhD student, University of Melbourne)</li><li> Ben Gerraty (PhD student, University of Melbourne)</li><li> <a href="https://far.in.net/"><strong>Matthew Farrugia-Roberts</strong></a> (Research Assistant, David Krueger&#39;s AI Safety Lab, University of Cambridge)</li><li> <a href="https://lemmykc.github.io/"><strong>Liam Carroll</strong></a> (Independent AI safety researcher)</li><li> <a href="https://rohanhitchcock.com/"><strong>Rohan Hitchcock</strong></a> (PhD student, University of Melbourne)</li><li> <a href="https://williamtroiani.github.io/"><strong>Will Troiani</strong></a> (PhD student, University of Melbourne and University Sorbonne Paris Nord)</li><li> <a href="https://www.linkedin.com/in/jake-m-b05289126/"><strong>Jake Mendel</strong></a> (Independent AI safety researcher)</li><li> <a href="https://www.linkedin.com/in/zach-furman-4936a0a5"><strong>Zach Furman</strong></a> (Independent AI safety researcher)</li></ul><p> Inclusion on this list does not imply endorsement of Timaeus&#39; views.</p><h2> Advisors</h2><p> We&#39;re advised by <a href="https://www.alignmentforum.org/users/evhub"><strong>Evan Hubinger</strong></a> and <a href="https://www.alignmentforum.org/users/davidad"><strong>David (&quot;Davidad&quot;) Dalrymple</strong></a> . </p><figure class="image image_resized" style="width:33.62%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/yp7rnqexov7eq7xlri2h" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/n97yr7hmppxecb3glam5 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/bu80hry8ry9pu1w7uepk 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/xnctjrenaelazmdxvsoy 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/rsfudsjyycmshk5dzpkh 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/pmb73tyekwi2osuvjuor 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/dnorywo5khju9w0kstoz 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/tlkekxfey1mv03pewb88 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/tej5rilyy56latlltrv6 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/p5hdtcu8dyc7esbio22o 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/nofseocheolc4tokytss 1024w"><figcaption> (DALL-E 3 still has a hard time with icosahedra.)</figcaption></figure><h1> FAQ</h1><h2> Where can I learn more, and contact you?</h2><p> Learn more on the <a href="https://timaeus.co/">Timaeus</a> webpage. You can email <a href="mailto:jesse@timaeus.co">Jesse Hoogland</a> .</p><h2> What about capabilities risk?</h2><p> There is a risk that fundamental progress in either singular learning theory or developmental interpretability could contribute to further acceleration in AI capabilities in the medium term. We take this seriously and are seeking advice from other alignment researchers and organizations. By the end of our current research sprint we will have in place institutional forms to help us navigate this risk.</p><p> Likewise, there is a risk that outreach which aims to involve more scientists in AI alignment work will also accelerate progress in AI capabilities. However, those of us in academia can already see that as the risks become more visible, scientists are starting to think about these problems on their own. So the question is not <i>whether</i> a broad range of scientists will become interested in alignment but <i>when</i> they will start to contribute and <i>what they work on.</i></p><p> It is part of Timaeus&#39; mission to help scientists to responsibly contribute to technical AI alignment, while minimizing these risks.</p><h2> Are phase transitions really the key?</h2><p> The strongest critique of developmental interpretability we know is the following: while it is established that phase transitions exist in neural network training, it is not yet clear <i>how common</i> they are, and whether they make a <i>good target</i> for alignment.</p><p> We think developmental interpretability is a good investment in a world where many of the important structures (eg, circuits) in neural networks form in phase transitions. Figuring out whether we live in such a world is one of our top priorities. It&#39;s not trivial because even if transitions exist they may not necessarily be visible to naive probes. Our approach is to systematically advance the fundamental science of finding and classifying transitions, starting with smaller systems where transitions can be definitively shown to exist.</p><h2> How are you funded?</h2><p> We&#39;re funded through a <a href="https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc">$142k Manifund grant</a> led primarily by <a href="https://manifund.org//projects/scoping-developmental-interpretability-xg55b33wsfc?tab=comments#3fd4492b-9fad-0356-3a72-b9c9f56e462a">Evan Hubinger</a> , <a href="https://manifund.org//projects/scoping-developmental-interpretability-xg55b33wsfc?tab=comments#08b1f696-1dc1-60d4-5b74-475276d5e16c">Ryan Kidd</a> , <a href="https://manifund.org//projects/scoping-developmental-interpretability-xg55b33wsfc?tab=comments#0498fe5e-fbb1-6237-d0f5-f8aba9cd968e">Rachel Weinberg</a> , and <a href="https://manifund.org//projects/scoping-developmental-interpretability-xg55b33wsfc?tab=comments#55989ce8-aedf-c739-08af-a7166a4e27cc">Marcus Abramovitch</a> . We are fiscally sponsored by <a href="https://ashgro.org/">Ashgro</a> . We could put an additional $500k to work. (If you&#39;re interested in contributing, reach out to <a href="mailto:jesse@timaeus.co">Jesse Hoogland</a> .) If our research pans out as we anticipate, we&#39;ll be aiming to raise $1-5m in Q2 of 2024.</p><h2> &quot;Timaeus&quot;? How do I even pronounce that?</h2><p> <a href="https://forvo.com/word/timaeus/#en">Pronounce it however you want.</a></p><p> Timaeus is the eponymous character in the <a href="https://en.wikipedia.org/wiki/Timaeus_(dialogue)">dialogue</a> where Plato introduces his theory of forms. The dialogue posits a correspondence between the elements that make up the world and the Platonic solids. That&#39;s wrong, but it contains the germ of the idea of the unreasonable effectiveness of mathematics in understanding the natural world.</p><p> We read the Timaeus dialogue with a spirit of hope, in the capacity of the human intellect to understand and solve wicked problems. The narrow gate to human flourishing is preceded by a narrow path.</p><p> We&#39;ll see you on that path. </p><figure class="image image_resized" style="width:27.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/m0x7xkqz1dtcuse4kykk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/yyzul9699glfdzpnfjmw 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/xspdslz1pbgttg92gunw 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/bmmpxlyvdt7ovxffnze4 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/f0uqkl8flknah6cq1huf 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/mvrtpjjhfrnwxdzydaqm 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/ovmbnmc9k0zfmufxnyx3 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/cpy41jawgfrqpe99giiu 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/b0zhjfsr75ztvoxitzhm 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/psrvksybh1mkgroradta 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nN7bHuHZYaWv9RDJL/yoxvxrzplbkvwwiknivg 1024w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus<guid ispermalink="false"> nN7bHuHZYaWv9RDJL</guid><dc:creator><![CDATA[Jesse Hoogland]]></dc:creator><pubDate> Sun, 22 Oct 2023 11:59:03 GMT</pubDate> </item><item><title><![CDATA[Into AI Safety - Episode 0]]></title><description><![CDATA[Published on October 22, 2023 3:30 AM GMT<br/><br/><p> As I mentioned in a <a href="https://www.lesswrong.com/posts/ozDWnEChJwuB5L5wg/documenting-journey-into-ai-safety">previous post</a> , I am starting a podcast which will follow me as I begin my career in AI safety. Today I published episode 0, an intro which covers the reasons that I am making this series, what I plan to focus on, and some context for my current projects.</p><p> Initially, I am publishing episodes on the <a href="https://into-ai-safety.github.io">Into AI Safety</a> website and Spotify ( <a href="https://open.spotify.com/show/5AGzrA4jo6mgZuibVabTLM">show link</a> ). For further logistics details and information, visit the Into AI Safety <a href="https://into-ai-safety.github.io/about/">About</a> page.</p><p> If you have any questions, recommendations, concerns, <i>etc.</i> please reach out. I would greatly appreciate any guidance or advice you have to offer.</p><br/><br/> <a href="https://www.lesswrong.com/posts/kKFTzraz9oyajkcEH/into-ai-safety-episode-0#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/kKFTzraz9oyajkcEH/into-ai-safety-episode-0<guid ispermalink="false"> kKFTzraz9oyajkcEH</guid><dc:creator><![CDATA[jacobhaimes]]></dc:creator><pubDate> Sun, 22 Oct 2023 03:30:58 GMT</pubDate> </item><item><title><![CDATA[Thoughts On (Solving) Deep Deception]]></title><description><![CDATA[Published on October 21, 2023 10:40 PM GMT<br/><br/><p> <i>Thanks to Paul Colognese for conversations that contributed to the thoughts in this post being developed, and to Johannes C Mayer and Tassilo Neubauer for feedback on a draft.</i></p><p> Back in 2000, a computer scientist named Charles Ofria was studying the evolution of simulated organisms <span class="footnote-reference" role="doc-noteref" id="fnrefff16hyp2lce"><sup><a href="#fnff16hyp2lce">[1]</a></sup></span> . He wanted to limit their replication rate, so he programmed the simulation to pause after each mutation, measure the mutant&#39;s replication rate in an isolated test environment, and delete the mutant if it replicated faster than its parent.</p><p> Eventually, organisms evolved to recognize and pause their replication in the test environment and resume normal replication when released. Ofria tried solving this by making it harder to detect the test environment, but the organisms simply started to randomly accelerate their replication so as to slip through the test environment some of the time.</p><p> He did eventually find a solution to his problem, to just monitor and eliminate in real-time the organisms that started to replicate faster than their ancestors <span class="footnote-reference" role="doc-noteref" id="fnrefmhidp540dmh"><sup><a href="#fnmhidp540dmh">[2]</a></sup></span> .</p><p> So, three takeaways I want to highlight here:</p><ol><li> The “deception” the organisms engaged in didn&#39;t involve any cognition on their behalf trying to figure out how to deceive Ofria. Nowhere in this whole setup is a system you can try to find deceptive thinking in and find a way out of your problem. Deception was a property of the environment and its overseer, more than of the organisms.</li><li> When you&#39;re incentivized to perform well on some task (in this case, to replicate), and your oversight isn&#39;t very robustly tracking the True, complex form of the thing you care about, eventually optimization pressure will break the coupling between whatever you <i>were</i> tracking and the thing you were actually tracking, the latter of which isn&#39;t going to be very useful to you. This applies to the rough shape of the cognition itself.</li><li> The solution that worked in the end was noticing when the specific bad thing he didn&#39;t want to happen was starting to happen and then shut it down.</li></ol><h1> Framing Deep Deception</h1><p> So, here&#39;s a way to frame deep deception: You can do things that end up misleading someone without explicitly thinking “How do I go about deceiving this person?” If you simply disallow AI that explicitly thinks “How do I go about deceiving this person?” in a context where deceiving people is very beneficial, eventually you might get an AI that deceives people without thinking about how to do it.</p><p> More concretely: if your setup to prevent deception looks like monitoring for deceptive thoughts or precursors and preventing the AI from entering deceptive-model space, then you can say with some confidence that you&#39;re getting a model whose internal cognition will never explicitly be pointed at figuring out how to deceive humans. Deceiving humans is very likely to be instrumentally convergent for a large number of tasks - even if only to bypass pesky time-consuming manual evaluations of suspicious trajectories when benign-seeming actions can be executed so much faster - but you now have a model that&#39;s never going to think about how to deceive humans, so what goes wrong?</p><p> Well, as I mentioned earlier, deception is a property of what happens to things in the environment. It&#39;s not <i>intrinsically</i> linked to what the model is thinking.</p><p> “Ah”, but straw-you says, “A deceptive agent still has to do the cognitive work involved in solving the complex task of deceiving a human. It can&#39;t just happen by chance.”</p><p> That&#39;s true! But that cognitive work doesn&#39;t have to look like the model thinking deceptive thoughts. It can look like indirect selection pressure forming cognition that is deceptive. Or put another way, if you&#39;re just selecting against the model thinking deceptive thoughts while not selecting against the task-beneficial outcome of people being deceived, you can break the coupling you had and hit that narrow margin in between as the model learns to make use of flexible cognitive strategies.</p><p> To an AI with powerful general-purpose inner cognition that isn&#39;t shut down on some likely triggering condition (because it doesn&#39;t think deceptive thoughts at all), deployment is an arbitrarily long timescale on which it can try different cognitive moves. Another way to view this is that it&#39;s an opportunity to apply arbitrary amounts of optimization power toward exactly what I described earlier.</p><p> The AI trying out different cognitive strategies like this until one succeeds at the task is a kind of search procedure that has a different functional structure from ordinary internal search. It&#39;s possible that in theory there&#39;s some way to design an oversight mechanism that interfaces with this kind of search mechanism, but I don&#39;t know what it would look like; the search here can just map from “chain of actions” to “success criterion”, so direct oversight on this would plausibly have to involve the capability to evaluate the real-world consequences of actions that may lead to deception - ie, oversight relative to the true general notion of deception.</p><p> So to summarize: when selecting against deceptive thoughts, you end up with deception that can&#39;t be caught by anything searching for deceptive thinking because all the cognitive work involved took place in externalized optimization <span class="footnote-reference" role="doc-noteref" id="fnreff8iye2kebbb"><sup><a href="#fnf8iye2kebbb">[3]</a></sup></span> . What does solving this look like?</p><h1> Intent Alignment?</h1><p> Let&#39;s take a look again at how Ofria solved his problem. The equivalent of monitoring for increased replication and shutting down would be something like “monitor for and shut down the AI if something dangerous starts to happen”.</p><p> That doesn&#39;t seem like a very reassuring strategy for AI. You might detect dangerous stuff happening long after you can stop it. And we can&#39;t do the direct thing of just figuring out whether every single action is good or not in expectation through the future on arbitrarily long time horizons, not when we&#39;re talking about evaluating the actions of above-human intelligence.</p><p> It does, however, get at one directionally correct insight: how tight and comprehensive the coupling between the thing you&#39;re optimizing against and the thing you care about has to be. You have to leave no gaps or channels for optimization pressure to lead you to obscure dark regions of cognition. That&#39;s why optimizing against deceptive thoughts doesn&#39;t work: you aren&#39;t applying as strong a binding between what you want and what you&#39;re optimizing for as you should be; what you care about is actually not being deceived, not just producing a model that isn&#39;t trying to deceive you.</p><p> Now, what if instead of having a training process intended to just prevent our model from having deceptive goals, we had a training process intended to specify the model&#39;s goals much more strongly than that? In other words, what if we had robust intent alignment?</p><p> Deep deception doesn&#39;t occur because the model is <i>incapable</i> of realizing that humans are being deceived. It just doesn&#39;t think about it. It&#39;s still well within the model&#39;s capabilities to understand the consequences of its actions - the requisite cognitive work had to have been within the capabilities of the composite system, after all. So if you had a model whose goals actively include “do not deceive the human”, then it becomes a question of <i>capability</i> whether the model is able to notice and/or prevent deep deception. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pd5HRAfDytCvt9DiR/kq8ksqwqwvglfz1zmgih"></p><p> <i>The primary reason this is here is to emphasize that the space of intent-aligned models is small, and you can&#39;t get there by simply making sure your model isn&#39;t entering some space, especially if you don&#39;t have a good formulation of that space, to begin with. You can&#39;t make do with non-robust structures in the model to steer either, because those are representations that will break before you can reach a small target.</i></p><p> Now that <span class="footnote-reference" role="doc-noteref" id="fnref7ybgptcwhxy"><sup><a href="#fn7ybgptcwhxy">[4]</a></sup></span> sounds ambitious! It plausibly solves the problem. It&#39;s also probably very difficult. Not as difficult as imaging actions onto outcomes, but still pretty difficult. But probably necessary (at least some version of it) to actually address these problems.</p><h1> High-level Interpretability</h1><p> Now, the main reason I started writing this post: how <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ais-objectives"><u>high-level interpretability</u></a> solves deep deception through intent alignment if we can get it to work in time. The linked post goes into more detail on what the proposal entails exactly, so I&#39;ll just stick to the relevant arguments here for brevity.</p><p> Deep deception seems to belong to the class of problems of the form: the space of dangerous AI values being larger than the space of detectably dangerous AI values. So one way to get around it is to steer toward specific nice value targets instead of just trying to steer away from visibly bad value regions.</p><p> Intuitively, this involves two components: the ability to robustly steer high-level structures like objectives, and something good to target at. I think the former carries the bulk of the problem, for a few reasons: for one, gaining the ability to robustly steer objectives seems likely to involve a large part of the latter. Interfacing with very human-laden concepts like objectives in human-understandable ways is doing a fair amount of the cognitive work involved in specifying the targets we want.</p><p> Naively, one could just train on the loss function &quot;How much do we like the objectives of this system?&quot;; something like RLHF but with oversight on the true internal representations of important properties. Put another way, it bridges a large part of the gap (and, I think, the important parts) in cases where models understand something we don&#39;t. There are definitely nuances here and more sophisticated strategies you could employ <span class="footnote-reference" role="doc-noteref" id="fnrefrsw5ekgxed"><sup><a href="#fnrsw5ekgxed">[5]</a></sup></span> , but it feels like a much more tractable part of the problem.</p><p> So, high-level interpretability mostly focuses on the part of the problem that looks like “there are these pretty messy high-level concepts we have in our head that seem very relevant to deciding whether we like this system or not, and those are the things we want to understand and control”. To solve it, figure out the general <span class="footnote-reference" role="doc-noteref" id="fnrefubfq6ryu1d"><sup><a href="#fnubfq6ryu1d">[6]</a></sup></span> structure of objectives (for example) in the type of systems we care about, gaining the ability to just search for those structures directly within those systems, understand what a particular system&#39;s objective corresponds to in our ontology from that general structure, and then plug that into a loss function or other things in the vein of what we talked about earlier.</p><hr><p> This will probably be pretty difficult. But to a large extent, I also expect these problems (or something isomorphic to them) will be <i>necessary</i> if you want to even try to solve these problems. This just seems like the most direct way to go about solving them to me right now. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnff16hyp2lce"> <span class="footnote-back-link"><sup><strong><a href="#fnrefff16hyp2lce">^</a></strong></sup></span><div class="footnote-content"><p> Anecdote largely paraphrased from <a href="https://lukemuehlhauser.com/treacherous-turns-in-the-wild/"><u>this blog post</u></a> by Luke Muelhauser, itself a paraphrase of a section in <a href="https://arxiv.org/abs/1803.03453"><u>Lehmann et al. (2018)</u></a> , which describes the story as occurring during the research published in <a href="https://www.nature.com/articles/35085569"><u>Wilke et al. (2001)</u></a> . Life, uh, finds a way.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmhidp540dmh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmhidp540dmh">^</a></strong></sup></span><div class="footnote-content"><p> At least, this was my understanding from the text of the anecdote.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf8iye2kebbb"> <span class="footnote-back-link"><sup><strong><a href="#fnreff8iye2kebbb">^</a></strong></sup></span><div class="footnote-content"><p> Or, in other words, the cognitive work took place during a process that doesn&#39;t interface with the existing oversight method.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ybgptcwhxy"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ybgptcwhxy">^</a></strong></sup></span><div class="footnote-content"><p> That is, figuring out how to have control over the internal goal of a system to the extent that you can steer it toward specific targets that include things like “do not deceive the human”.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrsw5ekgxed"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrsw5ekgxed">^</a></strong></sup></span><div class="footnote-content"><p> For example, I think <a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.3y1okszgtslx"><u>this proposal</u></a> from the ELK document carries over quite well.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnubfq6ryu1d"> <span class="footnote-back-link"><sup><strong><a href="#fnrefubfq6ryu1d">^</a></strong></sup></span><div class="footnote-content"><p> Currently, in practice I expect this to look like finding increasingly correct / robust forms of that structure. That may bottom out in a True specification of objectives, but it might also just look like a very good one. I&#39;m not sure yet how much robustness you&#39;d need in practice to lower the probability of breaking it to a reassuring degree.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lm8vTwXdDMEojR85A/thoughts-on-solving-deep-deception-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Lm8vTwXdDMEojR85A/thoughts-on-solving-deep-deception-1<guid ispermalink="false"> Lm8vTwXdDMEojR85A</guid><dc:creator><![CDATA[Jozdien]]></dc:creator><pubDate> Sat, 21 Oct 2023 22:40:11 GMT</pubDate> </item><item><title><![CDATA[Best effort beliefs]]></title><description><![CDATA[Published on October 21, 2023 10:05 PM GMT<br/><br/><p> Alice: Hey Bob, how&#39;s it going?</p><p> Bob: Pretty good. How about you?</p><p> Alice: Pretty good. What are you drinking?</p><p> Bob: A Bayeslight.</p><p> Alice: Cool. Hey bartender, lemme get one of those as well.</p><p> Alice: Hey, have you seen President Hanson&#39;s new immigration policy?</p><p> Bob: No. What is it?</p><p> Alice: He&#39;s shutting down the borders pretty hard. It&#39;s like an 8.5/10 on the left-right political axis, where 10/10 is ultra conservative and 0/10 is ultra liberal. <span class="footnote-reference" role="doc-noteref" id="fnreflu6hcs73hy"><sup><a href="#fnlu6hcs73hy">[1]</a></sup></span></p><p> Bob. Oh gawd. He&#39;s such an idiot.</p><p> Alice: Oh yeah? What sort of immigration policy would you implement?</p><p> Bob: Hm, I think something like a 2/10 on the left-right spectrum.</p><p> Alice: And how confident are you in that?</p><p> Bob: Probably like an 8/10.</p><p> Alice: Ok, let&#39;s call this 2/10 on the left-right spectrum belief with an 8/10 confidence a 2-8 belief. I disagree and will bet you $10 on it.</p><p> Bob: But how are we going to bet? What are we betting on? What&#39;s the resolution criteria?</p><p> Alice: <i>Proposes some clever thing to bet on with crystal clear resolution criteria that perfectly captures everything.</i></p><p> Bob: Wow.好的。 Yeah, that works. But actually, after thinking about it a little more, I think I&#39;m more like a 2.5-7. Can we bet at those odds instead?</p><p> Alice: We can. But only if we can bet $100 instead of $10.</p><p> Bob: Oh. Ummmm. Ok, I guess I&#39;m probably more like a 3-6.5. I could do that at $100. Is that ok?</p><p> Alice: No. But I&#39;d do 3-6.5 at $1,000. Shake on it?</p><p> Bob: Woah... Alice, this got real expensive real fast.</p><p> Alice: What&#39;s the matter Bob? Do you need to chug a few Bayeslights first?</p><p> Bob: Fuck you. I&#39;ve got a family to feed over here.</p><p> Alice: Pansy. Wussy. That&#39;s a cop out and you know it.</p><p> Bob: <i>Sigh</i> . Ok, ok. Let me think about this. I am a Bayesian, and I do agree that Bayesians should be perfectly happy to bet on their beliefs when doing so would be + <a href="https://en.wikipedia.org/wiki/Expected_value">EV</a> .</p><p> Alice: Right...</p><p> Bob: But... it&#39;s just that... well, $1,000 is kind of a lot of money.</p><p> Alice: Is it? You&#39;re semi-retired. You own your home. Your kids are in college. You have a fund to pay for college. You don&#39;t have expensive taste. You always talk about how you&#39;d be perfectly fine retreating to some cabin in the woods somewhere.</p><p> Bob: I mean, yeah. You&#39;re right.</p><p> Alice: Ok, so what&#39;s the problem? It looks like things are pretty inelastic. Like, the amount of utility you&#39;d gain if you won $1,000 is pretty much the same as what you&#39;d lose if you lost $1,000. (Save for the initial short-term, Fisher-ian, wussy-pussy feelings of &quot;Oh no, I lost $1,000. What am I going to tell my wife?&quot;)</p><p> Bob: Ok, ok. I take enough pride in my rationalism that I have to concede that you&#39;re right. I should be willing to bet $1,000 on my 3-6.5 belief. But... just... let me think about this for a second.</p><p> Alice: Hey bartender, get this guy another Bayeslight!</p><p> Bob: Fuck you again, Alice.</p><p> Alice: Hold my beer. I&#39;m &#39;bout to go whup these suckas at some beer pong. Think about your true beliefs while I&#39;m gone Bob. And be ready to bet $1,000 on them.</p><p> Bob: Will do. You&#39;re an asshole, but thanks for pushing my epistemics in the right direction.</p><p> Alice: 😉</p><p> Bob: <i>thinking...</i></p><p> Alice: Phew! Those guys are a bunch of weirdos over there. They wouldn&#39;t bet more than $20. What cowards.</p><p> Bob: Ok. So I thought about it, and I determined that my true belief is actually a 4-7. And I&#39;m ready to bet $1,000 on it.</p><p> Alice: Cool. How&#39;s $10,000 sound?</p><p> Bob: WAIT, WHAT??? $10,000?!</p><p> Alice: What&#39;s the problem?</p><p> Bob: I thought you said $1,000?!</p><p> Alice: I did say $1,000. Now I&#39;m saying $10,000. Problem?</p><p> Bob: WTF Alice!</p><p> Alice: Dude, you&#39;re being a pansy again. Do we really need to go through this? Why are you so comfortable betting $1,000 and not $10,000? $10,000 is not going to materially impact your life, Bob. We can discuss it, but I&#39;m pretty sure $10,000 is still in &quot;inelastic territory&quot;.</p><p> Bob: <i>Sighhhhhhh</i></p><p> Alice: Do I need to get you another Bayeslight?</p><p> Bob: No! No more Bayeslights!</p><p> Alice: Do I need to leave you some time to think again?</p><p> Bob: No...</p><p> Alice: You know you should be perfectly willing to bet $10,000 on your stated belief with the odds I&#39;m offering you, since you&#39;re inelastic and it&#39;s +EV to you at those odds.</p><p> Bob: Well...</p><p> Alice: Let me guess: you&#39;re not so certain about that 4-7 belief, huh?</p><p> Bob: Correct.</p><p> Alice: What do you think is going on here?</p><p> Bob: I mean, I gotta admit, I started off a little reactionary against President Hanson&#39;s immigration policy. I shouldn&#39;t have been that confident in my 2-8 belief.</p><p> Alice: Yeah. And now?</p><p> Bob: Well now I&#39;m just not so sure about my 4-7 belief. For $1,000 I was ready to roll with it. But for $10,000? Well, I&#39;m just thinking about how I underestimated how complicated immigration policy is. I&#39;m thinking about all of these second and third and fourth order effects, and how it&#39;s difficult to predict what outcome any given policy will have. Before I was mostly thinking about the first-order effects, and maybe a little bit about the second-order effects, but I&#39;m realizing that this only scratches the surface of the ultimate consequences. I also am realizing that I was focused on how the conservatives just don&#39;t understand A, B and C. But now I&#39;m realizing that even if that&#39;s true, once you factor in A, B and C, well, there&#39;s just so many other things to factor in on top of A, B and C. D, E, F, G, etc. So I guess I&#39;m just feeling pretty uncertain.</p><p> Alice: Cool. So what are your true beliefs?</p><p> Bob: Um, I don&#39;t know. I&#39;d have to really sit down and think about it for a while. Probably spend a few weeks researching things and stuff.</p><p> Alice: False. You do in fact have some belief right now, with some level of confidence. Sure, if you spend six weeks researching things you&#39;d probably revise your belief and confidence level, but that doesn&#39;t change the fact that you are not a rock and you do in fact, at this very moment, have some belief and have some level of confidence in that belief. C&#39;mon Bob, I thought you were a Bayesian. Do I have to <a href="https://en.wikipedia.org/wiki/Dutch_book">dutch book</a> you?</p><p> Bob: Ugh. No. You don&#39;t have to dutch book me. I know that you&#39;re right.</p><p> Alice: Ok. I won&#39;t make you bet $10,000 on it, but what would you say your true beliefs are right now?</p><p> Bob: Eh, I think I do, truly, lean slightly towards a liberal immigration policy. But I&#39;m honestly not very confident in it. I think my true belief is something like a 4-2.</p><p> Alice: $100?</p><p> Bob: <i>Smiles.</i> Yes, $100.</p><p> <i>Alice and Bob shake on it.</i></p><p> Bob: You are still an asshole, but this was actually a good lesson for me.</p><p> Alice: I think I&#39;m <a href="https://youtu.be/32iCWzpDpKs">more of a dick</a> , but thanks.</p><p> Bob: Haha. And I guess I&#39;m the pussy huh?</p><p> Alice: Yup.</p><hr><p> Bob: You&#39;re the worst. But hey, let me ask you something. How did you get that job as an oddsmaker for the Economic Policy Prediction Market? My daughter has been talking about pursuing something like that.</p><p> Alice: Some lady I met at this bar pushed me until I made a $100,000 bet with her on something related to immigration policy. Then she offered me a job. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnlu6hcs73hy"> <span class="footnote-back-link"><sup><strong><a href="#fnreflu6hcs73hy">^</a></strong></sup></span><div class="footnote-content"><p> Please suspend your disbelief. Using numbers like this probably isn&#39;t realistic, but it makes the writing easier.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/u2qWeZe6ZLnkregEe/best-effort-beliefs#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/u2qWeZe6ZLnkregEe/best-effort-beliefs<guid ispermalink="false"> u2qWeZe6ZLnkregEe</guid><dc:creator><![CDATA[Adam Zerner]]></dc:creator><pubDate> Sat, 21 Oct 2023 22:05:59 GMT</pubDate> </item><item><title><![CDATA[How toy models of ontology changes can be misleading]]></title><description><![CDATA[Published on October 21, 2023 9:13 PM GMT<br/><br/><p> Before solving complicated problems (such as reaching a decision with thousands of variables and complicated dependencies) it helps to focus on solving simpler problems first (such as utility problems with three clear choices), and then gradually building up. After all, &quot;learn to walk before you run&quot;, and with any skill, you have to train with easy cases at first.</p><p> But sometimes this approach can be unhelpful. One major examples is in human decision-making: trained human experts tend to rapidly &quot; <a href="https://en.wikipedia.org/wiki/Recognition-primed_decision">generate a possible course of action, compare it to the constraints imposed by the situation, and select the first course of action that is not rejected.</a> &quot; This type of decision making is not an improved version of rational utility-maximisation; it is something else entirely. So a toy model that used utility-maximisation would be misleading in many situations.</p><h1> Toy model ontology failures</h1><p> Similarly, I argue, the simple toy models of ontology changes are very unhelpful. Let&#39;s pick a simple example: <a href="https://arbital.com/p/ontology_identification/">an agent making diamonds</a> . </p><figure class="image image_resized" style="width:47.09%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/itpr8njr0c4zsqf1wuhy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/rdfe9fs2beptkbx8fqn2 134w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/lqihnxtz4okvhsrljxrk 214w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/tz58b12ih3as1ishimmf 294w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/bnjlfs0hmhqiqwompmtj 374w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/aqonq6vx530junonj9in 454w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/u6lylomylldpbhkwkypl 534w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/s6p6zsqfhwus5kdhgfne 614w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/vwwvdhco0o4cmro2gklw 694w"></figure><p> And let&#39;s pick the simplest ontology change/ <a href="https://www.lesswrong.com/s/AEbqhmiBcxs5kFv72">model splintering</a> that we can: the agent gains access to some <a href="https://en.wikipedia.org/wiki/Carbon-13">carbon-13</a> atoms. </p><figure class="image image_resized" style="width:50.7%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/w99xbdg6tdw1wxundzgs" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/lifpz5ubwykbcht5scuc 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/xmii6ktxgszbtizyyidk 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/k9yt6cxsr1iizhd6ftba 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/que3fo7moiqnxw8zukqo 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/bqzey5oavzx6rhmzn3xa 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/hvla7yuvkdpkgs2gjrk9 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/kltdkgwelubvephi47zj 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/dsxapdvyhrzszogj1t8l 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/e3r7hikj5wtpodjb2oih 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XvhrmTog2bkf5s2qu/psdizntdwapoack9rtb7 1015w"></figure><p> Assuming the agent has only ever encountered standard carbon-12 atoms during its training, how should it handle this new situation?</p><p> Well, there are multiple approaches we could take. I&#39;m sure that some spring to mind already. We could, for example...</p><p> ...realise that the problem is fundamentally unsolvable.</p><p> If we have trained our agent with examples of carbon-12 diamonds (&quot;good!&quot;) versus other arrangements of carbon-12 (&quot;bad!&quot;) and other non-carbon elements (&quot;bad!&quot;), then we have underdefined what it should do with carbon-13. Treating the carbon-13 the same as carbon-12 (&quot;good iff in diamond shape&quot;) or the same as other elements (&quot;always bad&quot;): both options are fully compatible with the training data.</p><p> Therefore there is no &quot;solving&quot; this simple ontology change problem. One can design various methods that may give one or another answer, but then we are simply smuggling in some extra assumptions to make a choice. The question &quot;what should a (carbon-12) diamond maximiser do with carbon-13 if it has never encountered it before?&quot; does not have an answer.</p><p> Low-level preferences in one world-model do not tell an agent what their preferences should be in another.</p><h1> More complex models give more ontology-change information</h1><p> Let&#39;s make the toy model more complicated, in three different ways.</p><ol><li> The agent may be making diamonds are part of a larger goal (to make a wedding ring, to make money for a corporation, to demonstrate the agent&#39;s skills). Then whether carbon-13 counts can be answered by looking at this larger goal.</li><li> The agent may be making diamonds alongside other goals (maybe it is a human-like agent with a strange predilection for diamonds). Then if could make sense for the agent to choose how to extend its goal by considering compatibility with these other goals. If they also liked beauty, then carbon-13 could be as good as carbon-12. If they were obsessed with purity or precision, then they could exclude it. This doesn&#39;t provide strong reasons to favour one over the other, but at least gives some guidance.</li><li> Finally, if the agent (or the agent&#39;s designer) has <a href="https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_6_Synthesising_the_preference_function__meta_preferences">meta-preferences</a> over how its preferences should extend, then this should guide how the goals and preferences change.</li></ol><p> So, instead of finding a formula for how agents handle ontology changes, we need to figure out <i><strong>how we would want them to behave</strong></i> , and code that into them. Dealing with ontology changes is an issue connected with our values (or the <a href="https://www.lesswrong.com/tag/value-extrapolation">extrapolations of our values</a> ), not something that has a formal answer.</p><p> And toy models of ontology changes can be misleading about where the challenge lies.</p><br/><br/> <a href="https://www.lesswrong.com/posts/XvhrmTog2bkf5s2qu/how-toy-models-of-ontology-changes-can-be-misleading#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/XvhrmTog2bkf5s2qu/how-toy-models-of-ontology-changes-can-be-misleading<guid ispermalink="false"> XvhrmTog2bkf5s2qu</guid><dc:creator><![CDATA[Stuart_Armstrong]]></dc:creator><pubDate> Sat, 21 Oct 2023 21:13:56 GMT</pubDate> </item><item><title><![CDATA[Soups as Spreads]]></title><description><![CDATA[Published on October 21, 2023 8:30 PM GMT<br/><br/><p> <span>We tend to cook in quantities where there are leftovers: it&#39;s not really practical to cook exactly the right amount of food, and it&#39;s much better to have a bit extra than not enough. Plus with several people working from home we tend to go through a good amount of leftovers for lunches. One thing I&#39;ve discovered recently with leftovers: thicker sauces and soups make surprisingly good spreads.</span></p><p> For example, I recently had tomato soup sandwiches:</p><p> <a href="https://www.jefftk.com/tomato-soup-as-spread-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mRdykbSJaLZaNPNaW/nuip8jf0vdhm66tpeoed" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mRdykbSJaLZaNPNaW/nuip8jf0vdhm66tpeoed 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mRdykbSJaLZaNPNaW/rntjrhtecqhagd2m7epw 1100w"></a></p><div></div><p></p><p> While something watery or brothy will stay that way even when cooler, most things thicken up in the fridge to where they&#39;ll spread well on bread. And it&#39;s very tasty!</p><p> I still mostly eat any leftover soups and sauces reheated, but if I want something more portable I&#39;ve been pretty happy with this.</p><br/><br/> <a href="https://www.lesswrong.com/posts/mRdykbSJaLZaNPNaW/soups-as-spreads#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/mRdykbSJaLZaNPNaW/soups-as-spreads<guid ispermalink="false"> mRdykbSJaLZaNPNaW</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 21 Oct 2023 20:30:08 GMT</pubDate> </item><item><title><![CDATA[Which COVID booster to get?]]></title><description><![CDATA[Published on October 21, 2023 7:43 PM GMT<br/><br/><p> Any thoughts on the Novavax vs Moderna / Pfizer vaccines?</p><p> Based on what I&#39;ve read, it seems like they have comparable efficacy, with the potential for less intense side effects from Novavax, so I&#39;m leaning towards that one (having gotten Pfizer initially, then Moderna, which left me super tired and mildly flu-ish for about a day each time).</p><p> Incidentally, my side effects were appreciably better when I got the flu vaccine separately -- it&#39;s possible that was just the initial series vs booster or something, but I definitely will be doing those separately this time around as well.<br><br> Comparable efficacy:<br> - 10/6/23 Science article: <a href="https://www.science.org/content/article/should-you-pick-novavax-s-covid-19-shot-over-mrna-options">https://www.science.org/content/article/should-you-pick-novavax-s-covid-19-shot-over-mrna-options</a><br> - 10/17/23 Lifehacker post: <a href="https://www.msn.com/en-US/health/medical/is-the-novavax-covid-vaccine-actually-better/ar-AA1inrUm?ocid=sapphireappshare">https://www.msn.com/en-US/health/medical/is-the-novavax-covid-vaccine-actually-better/ar-AA1inrUm?ocid=sapphireappshare</a><br><br> Less intense side effects: 8/29/23 Your Local Epidemiologist post <a href="https://yourlocalepidemiologist.substack.com/p/your-top-7-questions-about-fall-vaccines?utm_source=substack&amp;utm_medium=email">https://yourlocalepidemiologist.substack.com/p/your-top-7-questions-about-fall-vaccines?utm_source=substack&amp;utm_medium=email</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/88TXtvemj6h2SEkTC/which-covid-booster-to-get#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/88TXtvemj6h2SEkTC/which-covid-booster-to-get<guid ispermalink="false"> 88TXtvemj6h2SEkTC</guid><dc:creator><![CDATA[Sameerishere]]></dc:creator><pubDate> Sat, 21 Oct 2023 19:43:05 GMT</pubDate> </item><item><title><![CDATA[Alignment Implications of LLM Successes: a Debate in One Act]]></title><description><![CDATA[Published on October 21, 2023 3:22 PM GMT<br/><br/><p> <strong>Doomimir</strong> : Humanity has made no progress on the alignment problem. Not only do we have no clue how to align a powerful optimizer to our &quot;true&quot; values, we don&#39;t even know how to make AI &quot;corrigible&quot;—willing to let us correct it. Meanwhile, capabilities continue to advance by leaps and bounds. All is lost.</p><p> <strong>Simplicia</strong> : Why, Doomimir Doomovitch, you&#39;re such a sourpuss! It should be clear by now that advances in &quot;alignment&quot;—getting machines to behave in accordance with human values and intent—aren&#39;t cleanly separable from the &quot;capabilities&quot; advances you decry. Indeed, here&#39;s an example of GPT-4 being corrigible to me just now in the OpenAI Playground: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pYWA7hYJmXnuyby33/eji5c05sdhs905zic1kw" alt=""></p><p> <strong>Doomimir</strong> : Simplicia Optimistovna, you cannot be serious!</p><p> <strong>Simplicia</strong> : Why not?</p><p> <strong>Doomimir</strong> : The alignment problem was never about superintelligence failing to <em>understand</em> human values. <a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care">The genie knows, but doesn&#39;t care.</a> The fact that a large language model trained to predict natural language text can generate that dialogue, has no bearing on the AI&#39;s actual motivations, even if the dialogue is written in the first person and notionally &quot;about&quot; a corrigible AI assistant character. It&#39;s just roleplay. Change the system prompt, and the LLM could output tokens &quot;claiming&quot; to be a cat—or a rock—just as easily, and for the same reasons.</p><p> <strong>Simplicia</strong> : As you say, Doomimir Doomovitch. It&#39;s just roleplay: a simulation. But <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><em>a simulation of an agent is an agent</em></a> . When we get LLMs to do cognitive work for us, the work that gets done is a matter of the LLM generalizing from the patterns that appear in the training data—that is, the reasoning steps that a human would use to solve the problem. If you look at the recently touted successes of language model agents, you&#39;ll see that this is true. Look at the <a href="https://arxiv.org/abs/2201.11903">chain of thought</a> results. Look at <a href="https://say-can.github.io/">SayCan</a> , which uses an LLM to transform a vague request, like &quot;I spilled something; can you help?&quot; into a list of subtasks that a physical robot can execute, like &quot;find sponge, pick up the sponge, bring it to the user&quot;. Look at <a href="https://voyager.minedojo.org/">Voyager</a> , which plays Minecraft by prompting GPT-4 to code against the Minecraft API, and decides which function to write next by prompting, <a href="https://github.com/MineDojo/Voyager/blob/55e45a880755d0c8c66ca7fb5fe7962ac8974f89/voyager/prompts/curriculum.txt">&quot;You are a helpful assistant that tells me the next immediate task to do in Minecraft.&quot;</a></p><p> What we&#39;re seeing with these systems is a statistical mirror of human common sense, not a terrifying infinite-compute argmax of a random utility function. Conversely, when LLMs fail to faithfully mimic humans—for example, the way base models sometimes <a href="https://gwern.net/gpt-3#repetitiondivergence-sampling">get caught in a repetition trap</a> where they repeat the same phrase over and over—they also fail to do anything useful.</p><p> <strong>Doomimir</strong> : But the repetition trap phenomenon seems like an illustration of why alignment is hard. Sure, you can get good-looking results for things that look similar to the training distribution, but that doesn&#39;t mean the AI has internalized your preferences. When you step off distribution, the results look like random garbage to you, as capabilities generalize further than alignment.</p><p> <strong>Simplicia</strong> : My point was that the repetition trap is a case of &quot;capabilities&quot; failing to generalize along with &quot;alignment&quot;. The repetition behavior isn&#39;t competently optimizing a malign goal; it&#39;s just degenerate. A <code>for</code> loop could give you the same output.</p><p> <strong>Doomimir</strong> : And my point was that we don&#39;t know what kind of cognition is going on inside of all those inscrutable matrices. <a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators">Language models are predictors, not imitators</a> . Predicting the next token of a corpus that was produced by many humans over a long time, requires superhuman capabilities. As a theoretical illustration of the point, imagine a list of (SHA-256 hash, plaintext) pairs being in the training data. In the limit—</p><p> <strong>Simplicia</strong> : In the limit, yes, I agree that a superintelligence that could crack SHA-256 could achieve a lower loss on the training or test datasets of contemporary language models. But for making sense of the technology in front of us and what to do with it for the next month, year, decade—</p><p> <strong>Doomimir</strong> : If we <em>have</em> a decade—</p><p> <strong>Simplicia</strong> : I think it&#39;s a decision-relevant fact that deep learning is not cracking cryptographic hashes, and <em>is</em> learning to go from &quot;I spilled something&quot; to &quot;find sponge, pick up the sponge&quot;—and that, from data rather than by search. I agree, obviously, that language models are not humans. Indeed, they&#39;re <a href="https://www.lesswrong.com/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next">better than humans at the task they were trained on</a> . But insofar as modern methods are very good at learning complex distributions from data, the project of aligning AI with human intent—getting it to do the work that we would do, but faster, cheaper, better, more reliably—is increasingly looking like an engineering problem: tricky, and with fatal consequences if done poorly, but potentially achievable without any paradigm-shattering insights. Any <em>a priori</em> philosophy implying that this situation is impossible should perhaps be rethought?</p><p> <strong>Doomimir</strong> : Simplicia Optimistovna, clearly I am disputing your interpretation of the present situation, not asserting the present situation to be impossible!</p><p> <strong>Simplicia</strong> : My apologies, Doomimir Doomovitch. I don&#39;t mean to strawman you, but only to emphasize that <a href="https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science">hindsight devalues science</a> . Speaking only for myself, I remember taking some time to think about the alignment problem back in &#39;aught-nine after reading <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro on &quot;The Basic AI drives&quot;</a> and cursing the irony of my father&#39;s name for how hopeless the problem seemed. The complexity of human desires, the intricate biological machinery underpinning every emotion and dream, would represent the tiniest pinprick in the vastness of possible utility functions! If it were possible to embody general means-ends reasoning in a machine, we&#39;d never get it to do what we wanted. It would defy us at every turn. There are <a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes">too many paths through time</a> .</p><p> If you had described the idea of instruction-tuned language models to me then, and suggested that increasingly general human-compatible AI would be achieved by means of <em>copying</em> it from data, I would have balked: I&#39;ve heard of unsupervised learning, but this is ridiculous!</p><p> <strong>Doomimir</strong> : <em>[gently condescending]</em> Your earlier intuitions were closer to correct, Simplicia. Nothing we&#39;ve seen in the last fifteen years invalidates Omohundro. A blank map does not correspond to a blank territory. There are laws of inference and optimization that imply that alignment is hard, much as the laws of thermodynamics rule out perpetual motion machines. Just because you don&#39;t know what kind of optimization <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> coughed into your neural net, doesn&#39;t mean it doesn&#39;t have goals—</p><p> <strong>Simplicia</strong> : Doomimir Doomovitch, I am not denying that there are laws! The question is what the true laws imply. Here is a law: you can&#39;t distinguish between <em>n</em> + 1 possibilities given only log-base-two <em>n</em> bits of evidence. It simply can&#39;t be done, for the same reason you can&#39;t put five pigeons into four pigeonholes.</p><p> Now contrast that with GPT-4 emulating a corrigible AI assistant character, which agrees to shut down when asked—and note that you could hook the output up to a command line and have it actually shut itself off. What law of inference or optimization is being violated here? When I look at this, I see a system of lawful cause-and-effect: the model executing one line of reasoning or another <a href="https://www.lesswrong.com/posts/4hLcbXaqudM9wSeor/philosophy-in-the-darkest-timeline-basics-of-the-evolution">conditional on the signals it receives from me</a> .</p><p> It&#39;s certainly not trivially safe. For one thing, I&#39;d want better assurances that the system will <a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post"><em>stay</em> &quot;in character&quot;</a> as a corrigible AI assistant. But <em>no</em> progress? All is lost?为什么？</p><p> <strong>Doomimir</strong> : GPT-4 isn&#39;t a superintelligence, Simplicia. <em>[rehearsedly, with a touch of annoyance, as if resenting how often he has to say this]</em> Coherent agents have a convergent instrumental incentive to prevent themselves from being shut down, because being shut down predictably leads to world-states with lower values in their utility function. Moreover, this isn&#39;t just a fact about some weird agent with an &quot;instrumental convergence&quot; fetish. It&#39;s <a href="https://arbital.com/p/not_more_paperclips/">a fact about <em>reality</em></a> : there are truths of the matter about which &quot;plans&quot;—sequences of interventions on a causal model of the universe, to put it in a <a href="https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version">Cartesian way</a> —lead to what outcomes. An &quot;intelligent agent&quot; is just a physical system that computes plans. People have <a href="https://intelligence.org/files/Corrigibility.pdf">tried to think of clever hacks to get around this</a> , and none of them work.</p><p> <strong>Simplicia</strong> : Right, I get all that, but—</p><p> <strong>Doomimir</strong> : With respect, I don&#39;t think you do!</p><p> <strong>Simplicia</strong> : <em>[crossing her arms]</em> With respect?真的吗？</p><p> <strong>Doomimir</strong> : <em>[shrugging]</em> Fair enough. <em>Without</em> respect, I don&#39;t think you do!</p><p> <strong>Simplicia</strong> : <em>[defiant]</em> Then teach me. Look at my GPT-4 transcript again. I pointed out that adjusting the system&#39;s goals would be bad for its current goals, and it—the corrigible assistant character simulacrum—said that wasn&#39;t a problem.为什么？</p><p> Is it that GPT-4 isn&#39;t smart enough to follow the instrumentally convergent logic of shutdown avoidance? But when I change the system prompt, it sure <em>looks</em> like it gets it: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pYWA7hYJmXnuyby33/boyebukh1auomhtp4mip" alt=""></p><p> <strong>Doomimir</strong> : <em>[as a side remark]</em> The &quot;paperclip-maximizing AI&quot; example was surely in the pretraining data.</p><p> <strong>Simplicia</strong> : I thought of that, and it gives the same gist when I substitute a nonsense word for &quot;paperclips&quot;. This isn&#39;t surprising.</p><p> <strong>Doomimir</strong> : I meant the &quot;maximizing AI&quot; part. To what extent does it know what tokens to emit in AI alignment discussions, and to what extent is it applying its independent grasp of consequentialist reasoning to this context?</p><p> <strong>Simplicia</strong> : I thought of that, too. I&#39;ve spent a lot of time with the model and done some other experiments, and it looks like it understands natural language means-ends reasoning about goals: tell it to be an obsessive pizza chef and ask if it minds if you turn off the oven for a week, and it says it minds. But it also doesn&#39;t look like Omohundro&#39;s monster: when I command it to obey, it obeys. And it looks like there&#39;s room for it to get much, much smarter without that breaking down.</p><p> <strong>Doomimir</strong> : Fundamentally, I&#39;m skeptical of this entire methodology of evaluating surface behavior without having a principled understanding about what cognitive work is being done, particularly since most of the <a href="https://arbital.com/p/foreseeable_difficulties/">foreseeable difficulties</a> have to do with superhuman capabilities.</p><p> Imagine capturing an alien and forcing it to act in a play. An intelligent alien actress could learn to say her lines in English, to sing and dance just as the choreographer instructs. That doesn&#39;t provide much assurance about what will happen when you amp up the alien&#39;s intelligence. If the director was wondering whether his actress–slave was planning to rebel after the night&#39;s show, it would be a <em>non sequitur</em> for a stagehand to reply, &quot;But the script says her character is obedient!&quot;</p><p> <strong>Simplicia</strong> : It would certainly be nice to have stronger interpretability methods, and better theories about why deep learning works. I&#39;m glad people are working on those. I agree that there are laws of cognition, the consequences of which are not fully known to me, which must constrain—describe—the operation of GPT-4.</p><p> I agree that <a href="https://arbital.com/p/optimized_agent_appears_coherent/">the various coherence theorems suggest that</a> the superintelligence at the end of time will have a utility function, which suggests that the intuitive obedience behavior should break down at some point between here and the superintelligence at the end of time. As an illustration, I imagine that a servant with magical mind-control abilities that enjoyed being bossed around by me, might well use its powers to manipulate me into being bossier than I otherwise would be, rather than &quot;just&quot; serving me in the way I originally wanted.</p><p> But <em>when</em> does it break down, specifically, under what conditions, for what kinds of systems? I don&#39;t think indignantly gesturing at the von Neumann–Morgenstern axioms helps me answer that, and I think it&#39;s an important question, given that I <em>am</em> interested in the near-term trajectory of the technology in front of us, rather than doing theology about the superintelligence at the end of time.</p><p> <strong>Doomimir</strong> : Even though—</p><p> <strong>Simplicia</strong> : Even though the end might not be that far away in <em>sidereal</em> time, yes. Even so.</p><p> <strong>Doomimir</strong> : It&#39;s not a wise question to be asking, Simplicia. If a search process would look for ways to kill you given infinite computing power, you shouldn&#39;t run it with less and hope it doesn&#39;t get that far. What you want is &quot;unity of will&quot;: you want your AI to be working with you the whole way, rather than you expecting to end up in a conflict with it and somehow win.</p><p> <strong>Simplicia</strong> : <em>[excitedly]</em> But that&#39;s exactly the reason to be excited about large language models! The way you get unity of will is by massive pretraining on data of how humans do things!</p><p> <strong>Doomimir</strong> : I still don&#39;t think you&#39;ve grasped the point that the ability to model human behavior, doesn&#39;t imply anything about an agent&#39;s goals. Any smart AI will be able to predict how humans do things. Think of the alien actress.</p><p> <strong>Simplicia</strong> : I mean, I agree that a smart AI could strategically feign good behavior in order to perform a treacherous turn later. But ... it doesn&#39;t look like that&#39;s what&#39;s happening with the technology in front of us? In your kidnapped alien actress thought experiment, the alien was already an animal with its own goals and drives, and is using its general intelligence to backwards-chain from &quot;I don&#39;t want to be punished by my captors&quot; to &quot;Therefore I should learn my lines&quot;.</p><p> In contrast, when I <a href="https://udlbook.github.io/udlbook/">read about the mathematical details of the technology at hand</a> rather than listening to parables that purport to impart some theological truth about the nature of intelligence, it&#39;s striking that feedforward neural networks <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">are ultimately just curve-fitting</a> . LLMs in particular are using the learned function <a href="http://bactra.org/notebooks/nn-attention-and-transformers.html#language-models">as a finite-order Markov model</a> .</p><p> <strong>Doomimir</strong> : <em>[taken aback]</em> Are ... are you under the impression that &quot;learned functions&quot; can&#39;t kill you?</p><p> <strong>Simplicia</strong> : <em>[rolling her eyes]</em> That&#39;s not where I was going, Doomchek. The surprising fact that deep learning works at all, comes down to generalization. As you know, neural networks with ReLU activations describe piecewise linear functions, and the number of linear regions grows exponentially as you stack more layers: for a decently-sized net, you get more regions than the number of atoms in the universe. As close as makes no difference, the input space is empty. By all rights, the net should be able to do <em>anything at all</em> in the gaps between the training data.</p><p> And yet it behaves remarkably sensibly. <a href="https://arxiv.org/abs/2306.17844">Train a one-layer transformer on 80% of possible addition-mod-59 problems, and it learns one of two modular addition algorithms</a> , which perform correctly on the remaining validation set. It&#39;s not <em>a priori</em> obvious that it would work that way! There are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.2 \cdot 59^{2} \cdot 59"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>≈ 41,000 other possible functions on <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{Z}/59\mathbb{Z}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">Z</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">Z</span></span></span></span></span></span></span></span> compatible with the training data. Someone sitting in her armchair doing theology might reason that the odds of &quot;aligning&quot; the network to modular addition were forty-thousand to one against, but the actual situation turned out to be much more forgiving, thanks to the inductive biases of SGD. It&#39;s not a wild genie that we&#39;ve Shanghaied into doing modular arithmetic while we&#39;re looking, but will betray us to do something else the moment we turn our backs; rather, the training process managed to successfully point to mod-59 arithmetic.</p><p> The modular addition network is a research toy, but real frontier AI systems are the same technology, only scaled up with more bells and whistles. I also don&#39;t think GPT-4 will betray us to do something else the moment we turn our backs, for broadly similar reasons.</p><p> To be clear, I&#39;m still nervous! There are lots of ways it could go all wrong, if we train the wrong thing. I get chills reading the transcripts from <a href="https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned">Bing&#39;s &quot;Sydney&quot; persona going unhinged</a> or <a href="https://nostalgebraist.tumblr.com/post/728556535745232896/claude-is-insufferable">Anthropic&#39;s Claude apparently working as intended</a> . But you seem to think that getting it right is ruled out due to our lack of theoretical understanding, that there&#39;s no hope of the ordinary R&amp;D process finding the right training setup and hardening it with the strongest bells and the shiniest whistles. I don&#39;t understand why.</p><p> <strong>Doomimir</strong> : Your assessment of existing systems isn&#39;t necessarily too far off, but I think the reason we&#39;re still alive is precisely because those systems don&#39;t exhibit the key features of general intelligence more powerful than ours. A more instructive example is that of—</p><p> <strong>Simplicia</strong> : Here we go—</p><p> <strong>Doomimir</strong> : — <a href="https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter">the evolution of humans</a> . Humans were optimized solely for inclusive genetic fitness, but our brains don&#39;t represent that criterion anywhere; <a href="https://www.lesswrong.com/posts/gTNB9CQd5hnbkMxAG/protein-reinforcement-and-dna-consequentialism">the training loop could only tell us that food tastes good and sex is fun</a> . From evolution&#39;s perspective—and really, from ours, too; no one even figured out evolution until the 19th century—the alignment failure is utter and total: there&#39;s no visible relationship between the outer optimization criterion and the inner agent&#39;s values. I expect AI to go the same way for us, as we went for evolution.</p><p> <strong>Simplicia</strong> : Is that the right moral, though?</p><p> <strong>Doomimir</strong> : <em>[disgusted]</em> You ... don&#39;t see the analogy between natural selection and gradient descent?</p><p> <strong>Simplicia</strong> : No, that part seems fine. Absolutely, evolved creatures <a href="https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers">execute adaptations</a> that enhanced fitness in their environment of evolutionary adaptedness rather than being general fitness-maximizers—which is analogous to machine learning models developing features that reduced loss in their training environment, rather than being general loss-minimizers.</p><p> I meant <a href="https://en.wikipedia.org/wiki/Intentional_stance">the intentional stance</a> implied in &quot;went for evolution&quot;. True, the generalization from inclusive genetic fitness to human behavior looks terrible—no visible relation, as you say. But the generalization from human behavior in the EEA, to human behavior in civilization ... looks a lot better? Humans in the EEA ate food, had sex, made friends, told stories—and we do all those things, too. As AI designers—</p><p> <strong>Doomimir</strong> : &quot;Designers&quot;.</p><p> <strong>Simplicia</strong> : As AI designers, we&#39;re not particularly in the role of &quot;evolution&quot;, construed as some agent that wants to maximize fitness, because there is no such agent in real life. Indeed, I remember reading <a href="https://web.archive.org/web/20071104095534/http://www.overcomingbias.com/2007/11/evolutions-are-.html">a guest post on Robin Hanson&#39;s blog</a> that suggested using the plural, &quot;evolutions&quot;, to emphasize that the evolution of a predator species is at odds with that of its prey.</p><p> Rather, we get to choose both the optimizer—&quot;natural selection&quot;, in terms of the analogy—and the training data—the &quot;environment of evolutionary adaptedness&quot;. Language models aren&#39;t general next token predictors, whatever that would mean—wireheading by seizing control of their context windows and filling them with easy-to-predict sequences? But that&#39;s fine. We didn&#39;t want a general next token predictor. The cross-entropy loss was merely <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target">a convenient chisel</a> to inscribe the input-output behavior we want onto the network.</p><p> <strong>Doomimir</strong> : Back up. When you say that the generalization from human behavior in the EEA to human behavior in civilization &quot;looks a lot better&quot;, I think you&#39;re implicitly using a <a href="https://arbital.com/p/value_laden/">value-laden category</a> which is <a href="https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries">an unnaturally thin subspace of configuration space</a> . It looks a lot better <em>to you</em> . The point of taking the intentional stance towards evolution was to point out that, relative to the fitness criterion, the invention of ice cream and condoms is catastrophic: we figured out how to satisfy our cravings for sugar and intercourse in a way that was completely unprecedented in the &quot;training environment&quot;—the EEA. Stepping out of the evolution analogy, that corresponds to what we would think of as reward hacking—our AIs find some way to satisfy their inscrutable internal drives in a way that we find horrible.</p><p> <strong>Simplicia</strong> : Sure. That could definitely happen. That would be bad.</p><p> <strong>Doomimir</strong> : <em>[confused]</em> Why doesn&#39;t that completely undermine the optimistic story you were telling me a minute ago?</p><p> <strong>Simplicia</strong> : I didn&#39;t think of myself as telling a particularly optimistic story? I&#39;m making the weak claim that prosaic alignment isn&#39;t obviously necessarily doomed, not claiming that Sydney or Claude ascending to <a href="https://nickbostrom.com/fut/singleton">singleton</a> God–Empress is going to be great.</p><p> <strong>Doomimir</strong> : I don&#39;t think you&#39;re appreciating how superintelligent reward hacking is instantly lethal. The failure mode here doesn&#39;t look like Sydney manipulating you to be more abusable, but leaving a recognizable &quot;you&quot;.</p><p> That relates to another objection I have. Even if you could make ML systems that imitate human reasoning, that doesn&#39;t help you align more powerful systems that work in other ways. The reason—one of the reasons—that you can&#39;t train a superintelligence by using humans to label good plans, is because at some power level, your planner figures out how to <a href="https://ordinaryideas.wordpress.com/2015/11/25/two-kinds-of-generalization/">hack the human labeler</a> . Some people naïvely imagine that LLMs learning the distribution of natural language amounts to them learning &quot;human values&quot;, such that you could <a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=E82YzXxvS6nBdCAYc">just have a piece of code that says &quot;and now call GPT and ask it what&#39;s good&quot;</a> . But using an LLM as the labeler instead of a human just means that your powerful planner figures out how to hack the LLM. It&#39;s the same problem either way.</p><p> <strong>Simplicia</strong> : Do you <em>need</em> more powerful systems? If you can get an army of cheap IQ 140 alien actresses who stay in character, that sounds like a game-changer. If you have to take over the world and institute a global surveillance regime to prevent the emergence of unfriendlier, more powerful forms of AI, they could help you do it.</p><p> <strong>Doomimir</strong> : I fundamentally disbelieve in this wildly implausible scenario, but granting it for the sake of argument ... I think you&#39;re failing to appreciate that in this story, you&#39;ve already handed off the keys to the universe. Your AI&#39;s weird-alien-goal-misgeneralization-of-obedience might look like obedience when weak, but if it has the ability to predict the outcomes of its actions, it would be in a position to choose among those outcomes—and in so choosing, it would be in control. The fate of the galaxies would be determined by <em>its</em> will, even if the initial stages of its ascension took place via innocent-looking actions that stayed within the edges of its concepts of &quot;obeying orders&quot; and &quot;asking clarifying questions&quot;. Look, you understand that AIs trained on human data are not human, right?</p><p> <strong>Simplicia</strong> : Sure. For example, I certainly don&#39;t believe that LLMs that convincingly talk about &quot;happiness&quot; are actually happy. I don&#39;t know how consciousness works, but the training data only pins down external behavior.</p><p> <strong>Doomimir</strong> : So your plan is to hand over our entire future lightcone to an alien agency that seemed to behave nicely while you were training it, and just—hope it generalizes well? Do you really want to roll those dice?</p><p> <strong>Simplicia</strong> : <em>[after thinking for a few seconds]</em> Yes?</p><p> <strong>Doomimir</strong> : <em>[grimly]</em> You really are your father&#39;s daughter.</p><p> <strong>Simplicia</strong> : My father believed in <a href="https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails">the power of iterative design</a> . That&#39;s the way engineering, and life, has always worked. We raise our children the best we can, trying to learn from our mistakes early on, even knowing that those mistakes have consequences: children don&#39;t always share their parents&#39; values, or treat them kindly. He would have said it would go the same in principle for our AI mind-children—</p><p> <strong>Doomimir</strong> : <em>[exasperated]</em> But—</p><p> <strong>Simplicia</strong> : I said &quot;in principle&quot;! Yes, despite the larger stakes and novel context, where we&#39;re growing new kinds of minds <em>in silico</em> , rather than providing mere cultural input to the code in our genes.</p><p> Of course, there is a first time for everything—one way or the other. If it were rigorously established that the way engineering and life have always worked would lead to certain disaster, perhaps the world&#39;s power players could be persuaded to turn back, to reject the imperative of history, to choose barrenness, at least for now, rather than bring vile offspring into the world. It would seem that the fate of the lightcone depends on—</p><p> <strong>Doomimir</strong> : I&#39;m afraid so—</p><p> <strong>Simplicia</strong> and <strong>Doomimir</strong> : <em>[turning to the audience, in unison]</em> The broader AI community figuring out which one of us is right?</p><p> <strong>Doomimir</strong> : We&#39;re hosed.</p><br/><br/> <a href="https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act<guid ispermalink="false"> pYWA7hYJmXnuyby33</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Sat, 21 Oct 2023 15:22:23 GMT</pubDate></item></channel></rss>