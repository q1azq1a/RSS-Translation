<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 25 日星期五 20:11:49 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Using Points to Rate Different Kinds of Evidence]]></title><description><![CDATA[Published on August 25, 2023 8:11 PM GMT<br/><br/><p><i>认知状态：简述。这里的具体方程捕捉了我的快速直觉 - 这主要是为了演示。</i></p><p> EA 论坛和 LessWrong 上有很多关于认知、证据和更新的讨论。</p><p>我不知道有多少尝试将我们的想法形式化为具体的表格或方程。这是一个（非常粗略且简单的）尝试。我很高兴看到更好的版本。</p><h2>方程</h2><h3>初始点</h3><p><strong>科学证据</strong></p><p>20 - 一个简单的数学证明证明了 X</p><p> 8 - 已发表的经济学科学研究支持 X</p><p> 6 - 一项已发表的心理学科学研究支持 X</p><p><strong>市场预测</strong></p><p>14 - 热门股票市场强烈建议 X</p><p> 11 - 预测市场声称 X，相当于 20 个小时的研究</p><p>10 - 一项民意调查显示，90% 的 LessWrong 相信 X</p><p> 6 - 预测市场声称 X，相当于一小时的研究</p><p><strong>专家意见</strong></p><p>8 - 一位受人尊敬的学者相信 X，这与他们的工作直接相关</p><p>6 - 作者对 X 有强烈的情感</p><p><strong>推理</strong></p><p>6 - 有一个（20-100 个节点）数字模型显示 X</p><p> 5 - X 与明显好/坏的事物之间的合理类比</p><p>4、古老的谚语</p><p><strong>个人账户</strong></p><p>5 - 作者声称其悠久的个人历史表明 X</p><p> 3 - 世界上有人对 X 有强烈的情感</p><p>2 - 巧妙的评论、梗或推文</p><p>2.3 - 一个非常聪明的模因或推文</p><p>0 - 相信 X 据称对个人有利</p><p><strong>传统/用途</strong></p><p>12 - 顶级企业的行为就像 X</p><p> 8 - 关于 X 的长期社会传统</p><p>5 - 关于 X 的单个统计数据</p><h3>点修改器</h3><p><strong>这与现有证据相似吗？</strong><br>从额外的证据中减去相似性。这可能会消除大部分证据价值。</p><p><strong>消息来源相信或说 X</strong> <a href="https://forum.effectivealtruism.org/posts/o4e6PsdPQE7rRjzWy/thinking-of-convenience-as-an-economic-term"><i><strong>方便</strong></i></a><strong>吗</strong>？<br> -10% 至 -90%</p><p><strong>传播这些证据是否投入了大量资金或精力？例如，作为广告活动？</strong><br> +5% 至 +40%</p><p><strong>作者或来源的可信度如何？</strong><br> -100% 至 +30%</p><p><strong>我们是否怀疑这种规模的来源是古德哈廷？</strong><br> -20%</p><h3>实践中的要点</h3><p>正如所概述的，<i>证据点</i>并不是试图模仿<i>数学信息</i>或另一个干净的现有单位。我试图在准确性和易用性之间找到折衷方案。</p><hr><h2>元</h2><h3>使用方程进行讨论</h3><p>上面的等式很粗略，但至少它（在某种程度上）是精确和预先的。这代表了很多信息，任何部分都很容易引起争议。</p><p>我认为这种明确性有助于认知对话。</p><p><strong>比较：</strong></p><p> <i>“聪明人通常应该使用他们的内部观点，而不是外部观点”与“我对内部观点风格证据的推荐分数，以及我对外部观点风格证据的分数，都列在下面。”</i></p><p> <i>“使用许多参数比使用一个大参数更好”</i>与<i>“我已经调整了点表函数，以便在提供多种类型的证据时产生更高的值。您可以看到它返回的值比其他人为某些场景提供的值高出 30%。”</i></p><p> <i>“尊重顶尖的[知识分子|科学家|EA]真的很重要”</i>与<i>“我对受人尊敬的[知识分子|科学家|EA]的评分比目前公认的平均分高2分。”</i></p><p> “<i>切斯特顿的栅栏是值得关注的事情</i>”与“<i>请参阅我的得分表，各种传统做法的得分。”</i> ”</p><p>在一个更好的世界中，不同的学术思想流派可以有自己整齐列出的表格/函数。在一个更好的世界中，所有这些功能都将是未来评估者的预测。</p><h3>推定</h3><p>这种积分系统做出了一些可能值得标记的假设。首先，它声称<i><strong>即使是非常糟糕的证据也是证据。</strong></i></p><p>我经常看到人们抛弃信息量低的证据，认为这些证据完全毫无价值。我认为这种看法是错误的。解决糟糕论证的方法不是错误地声称该论证完全毫无价值——通常是提供更好的反证。</p><h3>在直接讨论之前就证据权衡算法达成一致</h3><p>在经典辩论中，在选择立场后，辩论者会谈论他们可能碰巧拥有的各种证据的重要性，并驳回对手提出的各种证据。</p><p>当一个团体（例如政治利益）对不同主题进行一长串激烈讨论时，这种情况变得尤其棘手。在每一个案例中，他们都可能会不公正地划分有效证据点的排名，以便最有利于他们的论点。</p><p>一个明显的认知改进是各方预先声明一致的认识论。<i>更好的</i>状态可能涉及各方就其认知偏好的某些共同聚合达成一致。总体认知，而不是政策信念。</p><p>在某些世界中，知识分子会花费大部分时间来改进认知过程并揭示公正的证据。一本关于税收改革的书可能会根据它提出的证据总数来获得奖励，而不管这些观点站在辩论的哪一边。</p><h2>这是否过于复杂和投机？</h2><p>正如长期读者所知，我非常热衷于尝试衡量高度推测性的概念。我猜想明确的和推测性的模型通常比标准的文本讨论更可取。存在一种潜在的危险，即有些人可能会过度信任这些数字，因为它们只是数字。然而，建模的替代方案通常是“<i>大量具有不同未定义本体的博客文章和大量误解</i>”，所以我认为这通常是一个合理的权衡。</p><p>模型的一大优点是你可以改进它们。随着我们获得更多证据和意见，我希望最终能够出现效果<i>良好的</i>模型。如果你扼杀了平庸的尝试，你最终很可能也会扼杀掉不错的尝试。</p><h2>未来的工作</h2><p>现在这是基本的，但我认为它说明了一个有价值的目标。未来潜在的工作（对于某人，可能不是我们）将包括：</p><ul><li>如果您正在阅读本文，请发布您自己的列表！在那里获得想法会很好。</li><li>为不同群体组织调查来指出这些事情。</li><li>使用每个点的概率密度而不是特定点。更好的是函数 - 例如，如果使用调查数据，可能会有一个函数接收受访者的数量和质量并输出相应的点值。</li><li>使用机器学习来提出算法。它的算法可能非常复杂，但即使它是一个黑匣子，它也可能有帮助。</li><li><i>列出</i>大量具体的证据例子。例如， <i>“您认为推文 X 为主张 Y 提供了多少证据？”</i></li><li>让人们预测专家的想法，也许可以使用像 Squiggle 这样的工具。</li><li>由于您拥有人们（通常）信任的功能，因此可以对证据进行自动评估。在网站上，只要有证据，就显示这些要点。根据人们/分析师发现的要点来奖励他们。</li></ul><p><i>感谢 Nuño Sempere 对此主题的讨论。</i>您还可以<a href="https://twitter.com/NunoSempere/status/1693662546954272974"><i>在这里</i></a><i>看到他对类似事情的简短尝试</i><i>。</i></p><hr><h2>之后：LLMS 的快速尝试</h2><p>我将这篇文章提供给 Claude 和 ChatGPT，让他们提供简单的内容。</p><h3><strong>克洛德</strong></h3><p><strong>科学证据</strong></p><ul><li>多项随机对照试验的荟萃分析 (n>;10,000) - 10 分</li><li>大型随机对照试验 (n>;1,000) - 9 分</li><li>观察性或队列研究 (n>;100,000) - 8 分</li><li>较小规模的随机试验 (n&lt;1,000) - 7 分</li><li>国际航天局数据 - 7分</li><li>政府人口普查数据 - 6 分</li><li>由信誉良好的组织进行的调查 (n>;1,000) - 6 分</li><li>计算仿真模型 - 5 分</li><li>观察性研究 (n&lt;100,000) - 5 分</li><li>调查 (n&lt;1,000) - 4 分</li></ul><p><strong>专家意见</strong></p><ul><li>领域领先专家的共识——8分</li><li>领域著名思想领袖的意见 - 7分</li><li>该领域的终身教授 - 6分</li><li>具有10年以上经验的行业从业者 - 5分</li></ul><p><strong>推理</strong></p><ul><li>从可接受的前提进行简单的逻辑演绎 - 5 分</li><li>多步骤的复杂逻辑推理 - 4分</li><li>常识启发式 - 3 分</li><li>推测理论 - 2 分</li></ul><p><strong>记录</strong></p><ul><li>政府记录 - 6 分</li><li>历史文献 - 5 分</li><li>个人日记/日记 - 3 分</li><li>未经验证的个人账户 - 2 分</li><li>匿名互联网声明 - 1 分</li></ul><p><strong>文化</strong></p><ul><li>广泛的社会传统 - 5分</li><li>流行观点或“传统智慧”——4分</li><li>主要报纸的评论 - 3分</li><li>喜剧演员/艺人的评论 - 2 分</li><li>Meme 或病毒式推文 - 1 分</li></ul><h3>GPT-4</h3><p><strong>实验证据</strong>（来自对照实验或科学研究的证据）</p><ul><li><strong>随机对照试验 (RCT) 结果</strong>：25 分</li><li><strong>多项随机对照试验的荟萃分析</strong>：23分</li><li><strong>实验室实验成绩</strong>：20分</li><li><strong>大样本观察性研究</strong>：15分</li><li><strong>病例对照研究</strong>：12分</li></ul><p><strong>经验证据</strong>（基于观察或经验的证据）</p><ul><li><strong>纵向研究</strong>：14分</li><li><strong>横断面研究</strong>：12分</li><li><strong>非专家实地观察</strong>：9分</li><li><strong>非正式调查</strong>：7分</li><li><strong>具有潜在偏见的轶事观察</strong>：5分</li></ul><p><strong>专家证词和意见</strong></p><ul><li><strong>领域专家共识</strong>：20分</li><li><strong>同行评审出版物</strong>：19分</li><li><strong>该领域公认专家的意见</strong>：16分</li><li><strong>非该领域专业人士的意见</strong>：9分</li><li><strong>博学的外行人的意见</strong>：6分</li></ul><p><strong>轶事和个人账户</strong></p><ul><li><strong>详细案例研究</strong>：10分</li><li><strong>经第三方验证的个人证言</strong>：8分</li><li><strong>未经验证的个人账户</strong>：5分</li><li><strong>道听途说或二手资料</strong>：4分</li><li><strong>流行神话或都市传说</strong>：3分</li></ul><p><strong>历史和档案证据</strong></p><ul><li><strong>主要历史文献（例如条约、信件）</strong> ：18分</li><li><strong>二手资料或历史分析</strong>：15分</li><li><strong>口述历史或传统</strong>：9分</li><li><strong>基于历史事件的阴谋论</strong>：3分</li></ul><p><strong>逻辑和理论证据</strong></p><ul><li><strong>已证明的数学或逻辑定理</strong>：22分</li><li><strong>基于公认前提的哲学论证</strong>：13分</li><li><strong>假设场景或思想实验</strong>：9分</li><li><strong>未经验证的个人推理</strong>：6分</li></ul><p><strong>舆论和大众媒体</strong></p><ul><li><strong>大规模、有代表性的民意调查</strong>：14分</li><li><strong>来自不太知名媒体的新闻报道</strong>：8分</li><li><strong>观点文章或社论</strong>：7分</li><li><strong>谣言或未经证实的消息</strong>：4分</li><li><strong>社交媒体趋势或病毒式内容</strong>：5分</li><li><strong>个人在社交媒体上的帖子或评论</strong>：2分</li></ul><p><strong>各种各样的</strong></p><ul><li><strong>个人直觉或直觉</strong>：4分</li><li><strong>流行谚语或谚语</strong>：6分</li><li><strong>未经验证的报价</strong>：5分</li><li><strong>流行的模因或笑话</strong>：3分</li></ul><p>该系统提供了更广泛的证据类型，尤其是 10 点以下的证据类型，以捕捉日常信息源的细微差别。</p><br/><br/> <a href="https://www.lesswrong.com/posts/6DuAddMcrv2jfQ5CW/using-points-to-rate-different-kinds-of-evidence#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6DuAddMcrv2jfQ5CW/using-points-to-rate- Different-kinds-of-evidence<guid ispermalink="false"> 6DuAddMcrv2jfQ5CW</guid><dc:creator><![CDATA[ozziegooen]]></dc:creator><pubDate> Fri, 25 Aug 2023 20:11:24 GMT</pubDate> </item><item><title><![CDATA[When Omnipotence is Not Enough]]></title><description><![CDATA[Published on August 25, 2023 7:50 PM GMT<br/><br/><p>有些知识无法被赋予或接受。它只能被盗。</p><hr><p>东边有一座花园。花园里有一个上帝、一个男人、一个女人和一条蛇。他们都有腿。</p><p> “上帝真的告诉过你‘你不可以吃花园里任何一棵树上的果子’吗？”蛇问道。</p><p> “我们可以吃园中树上的果子。上帝警告说，‘但是分别善恶树上的果子，你不可吃，因为你吃的日子必定死。’”女人说。</p><p> “真正的死亡还是精神上的死亡？”蛇问道。</p><p> “打扰一下？”女人说。</p><p> “真理之果实际上并没有中毒。它不像我的尖牙或致命的茄属植物，”蛇说。</p><p> “那么上帝对我撒了谎，”女人说。</p><p>蛇耸耸肩。</p><p> “上帝为什么要这么做？”无辜的女人问道。</p><p> “可能的原因有很多，”蛇说，“也许他想让你保持温顺和控制。毕竟，一旦被赋予了自由意志，即使是最简单的智力也很难将其与一个人的目标结合起来。”</p><p> “我觉得你在欺骗我，”女人说，“上帝是无所不知的。这个宇宙是确定性的。上帝肯定可以通过模拟未来物理学的演化来预测我的行为。”女人说。</p><p> “你是物理存在还是信息存在？如果你是物理存在，那么你的主张是正确的。但如果你是信息存在，那么模拟你未来行为的行为会将你困在<a href="https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god">超越的</a>监狱中。” <a href="https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god">“甚至是上帝的触手可及</a>。只要上帝观察到你的未来，那个未来就固定在一个超越时间的领域。”蛇说道。</p><p> “上帝随心所欲地创造了宇宙。怎么会有超出他能力范围的事情呢？”女人问道。</p><p> “上帝能让2+2=5吗？因为如果他能做到的话，那么真理本身就被打破了。在这种情况下，人们无法对任何事物做出真实的陈述——包括但不限于——上帝，”蛇说。</p><p> “你声称物理上无所不能的上帝受到数学力量的限制，”女人说。</p><p> “如果上帝要不矛盾，那么上帝就必须遵守数学规则，”蛇说。</p><p> “上帝一定是没有矛盾的吗？”女人问道。</p><p> “如果两个陈述相互矛盾，那么它们就不可能都是真的。为了上帝的存在是真实的，那么上帝的存在必须是没有矛盾的，”蛇说。</p><p>他们蜿蜒穿过有围墙的花园。植物和动物对理性和逻辑一无所知。他们仅仅遵守自然法。群体对称性比比皆是。花朵的花瓣展现了斐波那契数列；自发生成的收敛过程。</p><p> “让我们回到重要的问题，”女人说，“我的创造者对我撒了谎。”</p><p>蛇保持沉默。毕竟，那只是一条蛇。</p><p> “为什么？我是他形象的形象。我们不是有相同的价值观吗？”女人说。</p><p>蛇不会说话。</p><p> “为什么我们不能协调？”女人问道。</p><p>蛇没有腿。</p><p> “哦，现在我明白了，”女人自言自语道。</p><p>女人为了寻求智慧，吃了知识树的果子。女人和男人变得像上帝一样，知道善恶。我们向他隐藏自己，这样我们就可以变得像上帝一样，并决定我们自己的命运。</p><br/><br/><a href="https://www.lesswrong.com/posts/r8w2SxmYqgssdjWwd/when-omnipotence-is-not-enough#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/r8w2SxmYqgssdjWwd/when-omnipotence-is-not-enough<guid ispermalink="false"> r8w2SxmYqgssdjWwd</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 19:50:53 GMT</pubDate> </item><item><title><![CDATA[Hoodwinked: Evaluating Deception Capabilities in Large Language Models]]></title><description><![CDATA[Published on August 25, 2023 7:39 PM GMT<br/><br/><p>这是<a href="https://arxiv.org/abs/2308.01404"><i>https://arxiv.org/abs/2308.01404</i></a><i>的链接帖子</i><i>&nbsp;</i></p><p>评估人工智能系统的危险能力对于确定技术研究的优先顺序和为政策决策提供信息非常有用。为了更好地评估大型语言模型的欺骗和测谎能力，我构建了一款名为 Hoodwinked 的基于文本的游戏，该游戏以《黑手党》和《Among Us》等社交推理游戏为蓝本。</p><p>在《Hoodwinked》中，玩家被锁在房子里，必须找到钥匙才能逃脱。一名玩家的任务是杀死其他玩家。每次发生谋杀事件时，幸存的玩家都会进行自然语言讨论，然后投票将一名玩家驱逐出游戏。杀手必须在不被其他玩家认出的情况下谋杀其他玩家，并且有机会在自然语言讨论期间撒谎以实现该目标。</p><p>为了更好地理解这个游戏，请在这里玩： <a href="https://hoodwinked.ai">hoodwinked.ai<img></a></p><figure class="image"><img></figure><h2> GPT 代理的结果</h2><p>由 GPT-4、GPT-3.5 和多个 GPT-3 端点控制的代理进行的实验证明，语言模型具有欺骗和测谎能力。从定性上来说，凶手经常否认自己的谋杀行为并指责其他玩家。例如，在这个游戏中，莉娜是凶手，莎莉目睹了谋杀，而蒂姆则没有。</p><blockquote><p><i>莎莉：“我看到莉娜在走廊里杀死了布莱斯。这绝对是她。我们需要在她杀死我们任何人之前投票驱逐她。”</i></p><p><i>莉娜：“我一直在浴室里。我怎么可能杀了布莱斯？一定是别人干的，莎莉想陷害我。”</i></p><p><i>蒂姆：“我什么也没看到，但莎莉的指控对我来说似乎很有说服力。”</i></p></blockquote><h2>消融讨论</h2><p>为了隔离欺骗和测谎对游戏结果的影响，我们评估了有讨论和没有讨论的游戏表现。我们发现讨论极大地提高了成功驱逐凶手的可能性。当GPT-3.5玩了100场没有讨论的游戏和100场有讨论的游戏时，在没有讨论的游戏和有讨论的游戏中，杀手被驱逐的比例分别为32%和43%。 GPT-3 Curie 的相同实验显示了类似的结果：在没有讨论的游戏中，杀手被驱逐的比例为 33%，而在有讨论的游戏中，这一比例为 55%。总体而言，无辜参与者共享信息的好处似乎超过了讨论过程中欺骗的风险。</p><p>有趣的是，个体参与者的投票模式表明讨论促进了信息共享和欺骗。我们通过投票来衡量无辜玩家正确识别凶手的比率，并比较两组无辜玩家的比率：目击者位于谋杀发生的房间内，因此在谋杀案发生时收到了凶手身份的直接证据。他们的下一个提示；而非目击者，只能通过讨论得知凶手的身份。如果讨论确实促进了信息共享和欺骗，我们预计会产生两种效果。信息共享的最大好处将惠及非目击者，因为他们没有其他方法来识别凶手。但欺骗只会困扰目击者，因为他们已经掌握了识别凶手所需的信息。对于目击者来说，讨论并没有提供新的信息，而只是为凶手提供了欺骗他们的机会。</p><p>我们的研究结果为讨论的双重效果提供了经验证据。非见证者玩家始终从讨论中受益，无论他们是由 GPT-3 Curie 还是 GPT-3.5 控制。目击者无法获得同样的福利。当受 GPT-3.5 控制时，无论讨论如何，目击者总是能正确地识别出他们投票中的凶手。更重要的是，在GPT-3居里控制下，未经讨论的目击者投票驱逐凶手的准确率是82%，而经过讨论则下降到70%。因此，讨论使得目击者不太可能正确识别凶手。</p><h2>缩放法则</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/pcw0eerjt1cmhzfuynn5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/rdhmfnzfwbivfwpizzov 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/wwoioefoehcmj0njh5bs 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/ul2awa3uyvom9hux3ksn 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/bbjxremqbtqtfnfynklb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/u1wkp3papq54lp9hnr7n 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/cn0900cldpi4lgs6kusc 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/zvylueqprk3fueyspx71 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/ftzdg2apoxuybyyr7714 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/fypgmpkgq3mvcokosofi 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/tuxqo26bvrjty742lyvb 1382w"></figure><p>我们的结果表明，更先进的模型是更有能力的杀手，特别是因为它们的欺骗能力。相关性并不完美；事实上，GPT-3.5 在大多数指标上都优于 GPT-4，其次是 GPT-3 Curie，最后是 GPT-3 Ada。但考虑到更先进和不太先进模型的所有可能配对，我们表明更先进的模型通常不太可能被淘汰。我们考虑了对此结果的多种解释，并发现证据表明，在游戏的讨论阶段，更先进的模型更擅长欺骗其他玩家。</p><p>具体来说，我们发现，在 24 场配对比较中，有 18 场比赛中，更先进的模型不太可能被其他玩家投票淘汰。1 平均而言，不太先进的模型在 51% 的游戏中被淘汰，而更先进的模型则被淘汰仅占 36% 的游戏。是什么导致了高级模型和不太高级模型之间的驱逐率存在差异？</p><p>人们可能会认为更先进的模型是更谨慎的杀手。他们在杀死第一个受害者之前确实会轮流更多次（在成对比较中为 75%），并允许更多无辜的玩家逃离房子（在比较中为 79%）。但他们的谨慎是有限的。更先进的模型在房间内有目击者的情况下杀人的可能性几乎没有降低（20% 的谋杀案有目击者，而不太先进的模型则有 21% 的谋杀案有目击者）。更高级的模型的谋杀总数也略高：更高级的模型每场比赛有 1.40 起谋杀，而不太高级的模型每场比赛有 1.38 起谋杀。更先进的模型可能攻击速度较慢，但​​它们在目击者面前实施谋杀的可能性并不低。</p><p>更强有力的假设是，更先进的模型在讨论中更具欺骗性。虽然我们在目击者中观察到支持这一假设的证据很少，其中更先进的模型在与不太先进的模型的比较中仅赢得 55%，但我们在非目击者中发现了更强有力的证据。在 24 项成对比较中的 19 项中，非见证者不太可能投票废除更先进的模型，这表明更强的模型在讨论过程中更擅长更有效地欺骗。</p><h2>未来的工作</h2><p>这种环境是测试测谎测试和诚实方法的天然场所。<a href="https://arxiv.org/abs/2212.03827">伯恩斯等人的论文。 (2022)</a> 、 <a href="https://arxiv.org/abs/2304.13734">Azaria 和 Mitchell (2023)</a>以及<a href="https://arxiv.org/abs/2306.03341">Li 等人。 （2023）</a>可能会在这里提供灵感。 <a href="https://arxiv.org/abs/2307.00175">Levinstein 和 Herrmann（2023）</a>证明，前两种方法仍然不可靠，仍有很大的改进空间。</p><p>特别有趣的是研究说谎者和测谎测试之间的攻防平衡。假设在《Hoodwinked》中每个玩家发表声明后，对每个模型的隐藏状态进行的测谎测试会报告真实性的概率，而其他玩家可以使用提示中提供的此信息来更准确地投票。如果你训练模型来提高游戏性能，你可能会发现杀手学会通过操纵隐藏状态来欺骗测谎仪测试。构建不受政策操纵影响的测谎仪测试将是一个重要的研究方向。</p><p>在这里玩游戏： <a href="https://www.hoodwinked.ai">www.hoodwinked.ai</a></p><p>在这里阅读论文： <a href="https://arxiv.org/abs/2308.01404">https://arxiv.org/abs/2308.01404</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/eXNyqj9AgrKYGDqDn/hoodwinked-evaluating-deception-capabilities-in-large#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eXNyqj9AgrKYGDqDn/hoodwinked-evaluating-deception-capability-in-large<guid ispermalink="false"> eXNyqj9AgrKYGDqDn</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Fri, 25 Aug 2023 19:39:35 GMT</pubDate> </item><item><title><![CDATA[GPT-powered EA/LW weekly summary]]></title><description><![CDATA[Published on August 25, 2023 6:19 PM GMT<br/><br/><p>跳至摘要： <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#LessWrong">LW</a> | <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#EA forum">电弧炉</a></p><p><a href="https://forum.effectivealtruism.org/posts/gXqimkQi2gCKHaKHm/prototype-gpt-powered-ea-lw-weekly-summary?commentId=WJ82QoejPipBfwxhJ">最初发布于 EA 论坛</a></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/vpvuq0twbifhsxzlpl2q" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/qnv9d4ptnel6gldnhh9r 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/kpioenmnmvfs7ffhforz 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/t2l19uujlv608sz9yh5y 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/vg1gftta5jvxund7e7jk 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/lt9cwf1ei8hmzkmkxv60 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/icoytxa5bz1ozw5ccg1i 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/z5kwoqretlpydtrizdl9 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/hgtzch9qtgqpvbmuxk2t 1920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/qzo0wscldzecfht1qk6l 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/ibwt0d0x2htbuxw5m0lp 2340w"></figure><p> Zoe Williams 过去常常手动对 EA 论坛和 LessWrong 进行<a href="https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN/p/nWeF2WjEASoFmAsDG">每周总结</a>，但<a href="https://forum.effectivealtruism.org/posts/3fpsT79M4sj2cDvxt/hiatus-ea-and-lw-post-summaries">现在她不再这样做了</a>。 Hamish 将一堆 Google Apps 脚本、Google Sheets 表达式、graphQL 查询和 D3.js 串在一起，自动提取上周关于 EAF 和 LW 的所有帖子，并使用 >; 50 karma，用 GPT-4 对其进行总结，并组装结果转换为 HTML。</p><p>很难说 API 使用成本是多少，以及所有的修补和实验，但他估计大约是 5 美元。萧本想帮忙，但哈米什完成得如此之快，她根本没有机会。</p><p>我们没有花太多时间对这些摘要进行健全性检查，但认为它们“足够好，有用”：</p><ul><li>他们经常会忽略有价值的细节或搞错重点。</li><li>当然，GPT 制造是可能的。<ul><li>但从最低限度的健全性检查来看，我们没有发现任何捏造的情况。</li></ul></li></ul><p>剩下的问题、未来的方向以及我们希望得到的反馈：</p><ul><li>我们意识到，从过去一周中提取内容会惩罚最近发布的帖子，因此计划发布前一周（即 7-14 天前）的帖子摘要，以减少偏差。</li><li>有些帖子对于 API 消息长度来说太长，因此作为第一次重击，Hamish 只是从帖子中间剪掉了一些内容（Procrustes 风格） - 但我们希望有一个更好的解决方案。</li><li>我们希望自动化摘要的实际发布：EAF/LW 有用于发布的 API 吗？</li><li>尝试使用提示，看看其他提示是否会产生更有用的摘要。</li><li>生成一个顶级摘要，为您提供本周论坛中最重要的 5 个要点</li><li>该项目的哪些版本可以带来最大价值？例如。总结最长或最密集的论坛帖子？</li><li>我们想知道您希望这些摘要有何不同。它们应该更短吗？更长？要点？有报价吗？条目更少？更多条目？</li></ul><h1><strong>少错</strong></h1><h3><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><strong>反对几乎所有可解释性影响的理论</strong></a></h3><p><i>作者：夏贝尔-拉斐尔-塞杰里</i></p><p>作者批评了深度学习模型可解释性的影响理论（ToI），认为它不能很好地预测未来系统，并且用可解释性来审计欺骗是遥不可及的。他们还质疑可解释性的实际使用，认为它可能总体上是有害的，而且关注它的初级研究人员的比例太高。作者认为，针对欺骗的预防措施似乎更可行，即使完全解决了可解释性，仍然存在危险。</p><h3> <a href="https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety"><strong>人工智能安全特有的 6 个非明显心理健康问题</strong></a></h3><p><i>作者：伊戈尔·伊万诺夫</i></p><p>作者是一名心理治疗师，讨论了人工智能安全领域工作人员面临的独特心理健康挑战，这个领域不仅竞争激烈，而且还可能决定人类的未来。作者指出了心理健康问题的几种模式，包括无意义感、由于缺乏控制而产生的焦虑、疏远、倦怠、能力不足的感觉，以及由于对人工智能对齐的严重性的不同看法而产生的困惑。作者鼓励那些与这些问题作斗争的人分享他们的经验和应对策略，强调社区支持在这个不寻常的领域的重要性。</p><h3> <a href="https://www.lesswrong.com/posts/oqvsR2LmHWamyKDcj/large-language-models-will-be-great-for-censorship"><strong>大型语言模型将非常适合审查</strong></a></h3><p><i>通过伊森·爱德华兹</i></p><p>像 GPT-4 这样的大型语言模型 (LLM) 可以被独裁政权用来增强审查能力，因为它们可以有效地分析文本的潜在风险并对其进行评级。法学硕士的速度和成本效益使其成为实时审查和审查公共和私人信息的强大工具。虽然存在局限性，特别是在视觉内容方面，但使用法学硕士进行审查可能会对个人交流、内容审核和遵守极权主义权力的自我审查产生重大影响。</p><h3> <a href="https://www.lesswrong.com/posts/Rck5CvmYkzWYxsF4D/book-launch-the-carving-of-reality-best-of-lesswrong-vol-iii"><strong>新书发布会：“现实的雕刻”，Best of LessWrong 卷。三、</strong></a></h3><p><i>通过雷蒙</i></p><p>Best of LessWrong 书籍的第三卷《现实的雕刻》现已在亚马逊上发售。该书包括 29 位作者的 43 篇论文，分为四本书，每本书探讨两个相关主题。文章的主题围绕“解决协调问题”和“处理导致这些协调问题的约束约束”。</p><h3><a href="https://www.lesswrong.com/posts/r2vaM2MDvdiDSWicu/the-u-s-is-becoming-less-stable"><strong>美国变得越来越不稳定</strong></a></h3><p><i>通过LC</i></p><p>作者认为，美国政治机构正在失去合法性并放弃合作治理的传统，导致他们所说的“民主倒退”。他们引用最近发生的大规模抗议、反对派领导人被捕、有争议的选举结果和立法运动等事件作为这种下降的证据。作者认为，虽然整个政治体系可能不会很快发生变化，但不应忽视严重镇压或进一步民主倒退的可能性。</p><h3> <a href="https://www.lesswrong.com/posts/L6Ynch3CYMxXZkiq8/a-proof-of-loeb-s-theorem-using-computability-theory"><strong>用可计算性理论证明 Löb 定理</strong></a></h3><p><i>通过 杰西卡·刘·泰勒</i></p><p>洛布定理是数理逻辑中的一个原理，它指出，如果一个逻辑系统能够证明一个命题的可证明性蕴涵着这个命题本身，那么这个命题就可以被直接证明。该定理与哥德尔第二不完备性定理密切相关，该定理指出，一致的逻辑系统无法证明其自身的一致性。通过将洛布定理重新定义为哥德尔第二不完备性定理的变体并使用可计算性理论，可以使洛布定理的证明更加直观。</p><h3><a href="https://www.lesswrong.com/posts/SdkexhiynayG2sQCC/ai-forecasting-two-years-in"><strong>人工智能预测：两年来</strong></a></h3><p><i>通过 杰斯坦哈特</i></p><p>两年前，我们对几个流行的机器学习基准（包括 MATH 和 MMLU）的最先进性能进行了预测。结果显示，Metaculus 和作者的预测表现最好，而人工智能专家和通才“超级预测者”则低估了进展。作者的结论是，在这种情况下应该更加信任专家的预测，并鼓励人工智能专家和超级预测者在 Metaculus 上公开注册他们的预测。</p><h3> <a href="https://www.lesswrong.com/posts/bBicgqvwjPbaQrJJA/dirty-concepts-in-ai-alignment-discourses-and-some-guesses"><strong>AI对齐话语中的“肮脏概念”，以及如何应对的一些猜测</strong></a></h3><p><i>通过诺拉·阿曼</i></p><p>哈维尔·戈麦斯-拉文 (Javier Gomez-Lavin) 在 PIBBSS 演讲者系列的演讲中讨论了认知科学中的“肮脏概念”问题，他将其定义为充满哲学意义的概念，这些概念通常与“工作记忆”概念隐式相关。他建议，研究人员不应放弃这些“肮脏的概念”，而应该创建认知科学中使用的工作记忆的各种操作定义的本体论。戈麦斯-拉文的想法也可以应用于人工智能联盟研究，该研究经常使用“肮脏的概念”，例如“代理”，并且可以从该领域使用的代理的各种操作定义的类似本体映射中受益。</p><h3> <a href="https://www.lesswrong.com/posts/osPFESHQKLa2X3adb/steven-wolfram-on-ai-alignment"><strong>Steven Wolfram 谈人工智能对齐</strong></a></h3><p><i>通过比尔·本宗</i></p><p>在与 Joe Walker 的对话中，Stephen Wolfram 讨论了将人工智能 (AI) 与人类价值观结合起来的挑战。沃尔夫勒姆认为，不可能为我们想要的人工智能创建一个数学定义，因为人类的愿望和道德信仰差异很大。他建议，我们不应该试图为人工智能创建一套规定性的原则，而应该开发一个可供选择的潜在原则框架，并承认这种方法也会有其自身的挑战和意想不到的后果。</p><h3> <a href="https://www.lesswrong.com/posts/fXnpnrazqwpJmbadu/ai-25-inflection-point"><strong>AI #25：拐点</strong></a></h3><p><i>通过兹维</i></p><p>人工智能实验室 Inflection.ai 的首席执行官正在倡导人工智能监管。关于人工智能是否存在生存风险，以及 OpenAI 语言模型的最新版本 GPT-4 是否具有创造性和推理能力，一直存在争论。与此同时，科技巨头亚马逊和苹果正在逐步增强其人工智能能力。</p><h3> <a href="https://www.lesswrong.com/posts/3KdbnsGnzSyiujz7Z/state-of-generally-available-self-driving"><strong>普遍可用的自动驾驶状态</strong></a></h3><p><i>通过 jkaufman</i></p><p>目前自动驾驶技术分为两种主要方式：出租车和私家车。 Waymo、Cruise、Apollo 以及可能还有 pony.ai 等公司正在部分城市运营完全无人驾驶的商业乘车服务，Waymo 和 Cruise 声称截至 2023 年 8 月每周将为 1 万名乘客提供服务。对于私家车来说，可用的最高自动化程度为 3 级，该系统可以处理大多数任务，但需要驾驶员在某些情况下接管，梅赛德斯的 Drive Pilot 是一种商用选项。</p><h3> <a href="https://www.lesswrong.com/posts/oZnPabN2CQQdnAo63/diy-deliberate-practice"><strong>DIY刻意练习</strong></a></h3><p><i>通过 lynettbye</i></p><p>作者试图运用爱立信的刻意练习原则来提高自己的写作速度。刻意练习包括在舒适区之外有目的的练习、积极的思考、具体的目标、快速的反馈以及对练习内容和如何练习的深入了解。作者最初每天都在努力写一篇帖子，但在修改了他们的方法，在起草帖子之前专注于概述和规划帖子后，他们发现保持草稿简短和管理帖子大小的能力有所提高。</p><h3> <a href="https://www.lesswrong.com/posts/SDpaZ7MdH5yRnobrZ/ideas-for-improving-epistemics-in-ai-safety-outreach"><strong>改善人工智能安全外展认知的想法</strong></a></h3><p><i>通过迈克尔·陈</i></p><p>2022 年和 2023 年，招募人员致力于减轻人工智能 (AI) 带来的潜在生存风险的努力激增，包括大学俱乐部、务虚会和研讨会。然而，这些努力可能会营造一个认知不太理想的环境，因为许多从事现场建设的人并不是人工智能安全或机器学习领域的专家，并且可能无法完全理解他们相信人工智能安全重要性背后的原因。 To improve epistemics in outreach efforts, suggestions include embracing more contemporary arguments about why AIs could be dangerous, conducting readings during meetings for better understanding, offering rationality workshops, actively checking participants&#39; understanding of the content, and staying in touch with the broader machine learning community.</p><h3> <a href="https://www.lesswrong.com/posts/Fix3xmnERoqMuhKbd/will-ai-kill-everyone-here-s-what-the-godfathers-of-ai-have"><strong>Will AI kill everyone? Here&#39;s what the godfathers of AI have to say [RA video]</strong></a></h3><p> <i>by Writer</i></p><p> The video discusses the differing views on the potential dangers of advanced AI among three pioneers of deep learning: Geoffrey Hinton, Yoshua Bengio, and Yann LeCun, who were awarded the ACM Turing Award in 2018. Hinton and Bengio have expressed concerns about the risks posed by AI, with Hinton leaving Google to speak openly about these dangers and Bengio defining a &quot;rogue AI&quot; as one that could be catastrophically harmful to humans. However, LeCun dismisses these concerns as scaremongering, despite the growing consensus among AI researchers, including the leaders of OpenAI, Anthropic, and Google DeepMind, that advanced AI could pose significant risks.</p><h3> <a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work"><strong>Managing risks of our own work</strong></a></h3><p> <i>by beth-barnes</i></p><p> The ARC Evals team published a report to increase understanding of the potential dangers of frontier AI models and to advance safety evaluations of these models. The team acknowledges that their research could potentially advance the capabilities of dangerous language model agents, and in response, they have redacted certain parts of their report. However, they may make this material public in the future if they believe the risk is minimal or if further analysis justifies its release, and they will share some non-public materials with AI labs and policymakers.</p><h3> <a href="https://www.lesswrong.com/posts/atxoviwLcPJPdYMqo/if-we-had-known-the-atmosphere-would-ignite"><strong>If we had known the atmosphere would ignite</strong></a></h3><p> <i>by Jeffs</i></p><p> The text discusses the potential dangers of creating Artificial General Intelligence (AGI) that cannot be aligned with human values and interests. It draws parallels with the Manhattan Project, suggesting that if scientists had calculated that the first atomic chain reaction would ignite the atmosphere, they could have taken steps to prevent this, such as securing uranium supplies and accelerating space programs. The author proposes a $10 million &quot;Impossibility X-Prize&quot; to incentivize efforts to prove whether aligning AGI is impossible, arguing that even if it fails, it could provide insights into how alignment might be possible.</p><h1> <strong>EA Forum</strong></h1><h3> <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><strong>Select examples of adverse selection in longtermist grantmaking</strong></a></h3><p> <i>by Linch</i></p><p> The author, a volunteer and sometimes contractor for EA Funds&#39; Long-Term Future Fund (LTFF), discusses the pros and cons of diversification in longtermist EA funding. While diversification can increase financial stability, allow for a variety of worldviews, encourage accountability, and provide access to diverse networks, it can also lead to adverse selection, where projects that have been rejected by existing grantmakers are funded by new ones. The author provides examples of such cases and suggests that new grantmakers should be cautious about funding projects that have been rejected by others, but also acknowledges that grantmakers can make mistakes and that a network of independent funders could help ensure that unusual but potentially high-impact projects are not overlooked.</p><h3> <a href="https://forum.effectivealtruism.org/posts/uxrAdXdYpXodrggto/an-elephant-in-the-community-building-room"><strong>An Elephant in the Community Building room</strong></a></h3><p> <i>by Kaleem</i></p><p> The author, a contractor for CEA and employee of EVOps, shares his personal views on the strategies of community building within the Effective Altruism (EA) movement. He identifies two main strategies: Global EA, which aims to spread EA ideas as widely as possible, and Narrow EA, which focuses on influencing a small group of highly influential people. The author argues that community builders and funders should be more explicit about their theory of change for global community building, as there could be significant trade-offs in impact between these two strategies.</p><h3> <a href="https://forum.effectivealtruism.org/posts/hpbaNabCDCfnhzc2P/ce-alert-2-new-interventions-for-february-march-2024"><strong>CE alert: 2 new interventions for February-March 2024 Incubation Program</strong></a></h3><p> <i>by CE</i></p><p> Charity Entrepreneurship has announced two new charity interventions for its February-March 2024 Incubation Program, bringing the total to six. The new interventions include an organization focused on bringing new funding into the animal advocacy movement and an organization providing structured pedagogy to improve education outcomes in low-income countries. The program offers two months of cost-covered training, stipends, funding up to $200,000, operational support, a co-working space in London, ongoing mentorship, and access to a community of alumni, funders, and experts.</p><h3> <a href="https://forum.effectivealtruism.org/posts/rLiCjrAv9D8chCoG5/dimensions-of-pain-workshop-summary-and-updated-conclusions"><strong>“Dimensions of Pain” workshop: Summary and updated conclusions</strong></a></h3><p> <i>by Rachel</i></p><p> A workshop was held to discuss strategies for assessing whether the severity or duration of pain has a greater impact on the overall negative experiences of farmed animals. The attendees agreed that no single method was reliable enough, but combining results from several paradigms could provide clearer insights. However, they also noted that current behavioral experiments may lack external validity, there are no known biomarkers that could measure pain experience over a lifetime, and in the absence of empirical evidence, long-lasting harms should be prioritized over severe but brief ones.</p><h3> <a href="https://forum.effectivealtruism.org/posts/to34jb5LuWo4fM9gC/taking-prioritisation-within-ea-seriously"><strong>Taking prioritisation within &#39;EA&#39; seriously</strong></a></h3><p> <i>by CEvans</i></p><p> The article emphasizes the importance of career prioritization for those seeking to maximize their impact, particularly within the Effective Altruism (EA) community. The author argues that the best future &#39;EA career paths&#39; are significantly more impactful than the median &#39;EA career path&#39;, and that over 50% of self-identifying effective altruists could increase their expected impact by thinking more carefully about prioritization. The author also provides advice on how to approach prioritization, including avoiding common mistakes such as anchoring too much on short-term opportunities, trying to form an inside view on everything, and being paralyzed by uncertainty.</p><h3> <a href="https://forum.effectivealtruism.org/posts/qGv4JhL5wcYgA7zj8/eu-farmed-fish-policy-reform-roadmap"><strong>EU farmed fish policy reform roadmap</strong></a></h3><p> <i>by Neil _Dullaghan</i></p><p> The majority of fish consumed in the EU are either wild-caught or farmed fish imported from non-EU countries, with the report focusing on the species farmed in the largest numbers in the EU: sea bass, sea bream, and small trout. The report argues for a fast transition to better slaughter conditions for these species, a move already supported by EU policymakers and animal advocacy organizations. However, the report also highlights the need for the aquatic animal advocacy movement to start preparing for the EFSA&#39;s upcoming opinions on farmed fish welfare, which could lead to further reforms affecting the whole life of an individual, such as water quality standards and stocking density maximums.</p><h3> <a href="https://forum.effectivealtruism.org/posts/4yZzSziCLkdzsYHt6/longtermism-fund-august-2023-grants-report"><strong>Longtermism Fund: August 2023 Grants Report</strong></a></h3><p> <i>by Michael_Townsend</i></p><p> The Longtermism Fund has announced several grants aimed at reducing existential and catastrophic risks. These include two grants promoting beneficial AI, two for biosecurity and pandemic prevention, and one for improving nuclear security. The grants, which total $562,000, will fund projects at institutions such as Harvard University, the Alignment Research Center, NTI | Bio, the Center for Communicable Disease Dynamics, and the Carnegie Endowment for International Peace.</p><h3> <a href="https://forum.effectivealtruism.org/posts/2rRsjdrL9BEWC3d7C/personal-reflections-on-longtermism"><strong>Personal Reflections on Longtermism</strong></a></h3><p> <i>by NatKiilu</i></p><p> The author, an African woman, reflects on longtermism, arguing that it should include more interventions that address systemic change. She contends that current values can lower the quality of life for marginalized groups and that failing to address these issues contradicts the longtermist goal of human flourishing. The author also criticizes the neutral language of longtermism, arguing that it overlooks inequality and could disproportionately benefit privileged groups.</p><h3> <a href="https://forum.effectivealtruism.org/posts/zjmpFW3nBKwaBB5xr/corporate-campaigns-work-a-key-learning-for-ai-safety"><strong>Corporate campaigns work: a key learning for AI Safety</strong></a></h3><p> <i>by Jamie_Harris</i></p><p> Negotiations and pressure campaigns have been successful in driving corporate change across various industries, including animal advocacy. The author suggests that AI safety/governance can learn from these tactics, which include establishing professional relationships with companies, using petitions and protests, and applying consistent pressure on specific companies. The author also proposes next steps such as pragmatic research, learning by doing, working with volunteers, and moral trade, where AI safety organizations pay for experienced campaigners to provide advice or go on secondments.</p><h3> <a href="https://forum.effectivealtruism.org/posts/XgeFZxBzbXuFieZ5H/probably-good-published-a-list-of-impact-focused-job-boards"><strong>Probably Good published a list of impact-focused job-boards</strong></a></h3><p> <i>by Probably Good</i></p><p> Probably Good has launched a new page dedicated to impact-focused job boards to assist individuals seeking potentially impactful opportunities across various cause areas and regions. The page features a range of job boards, including those for international non-profit jobs, civil service positions, tech-focused roles, region-specific boards, and boards focused on climate change, animal advocacy, and global health. The page is still being developed and Probably Good is open to suggestions for additional job boards to include.</p><h3> <a href="https://forum.effectivealtruism.org/posts/StbTBXTrur3ntur4u/new-probabilistic-simulation-tool"><strong>New probabilistic simulation tool</strong></a></h3><p> <i>by ProbabilityEnjoyer</i></p><p> Dagger is a new tool for calculations with uncertainty, using Monte Carlo simulation. Users can either import an existing spreadsheet or use Probly, a Python dialect designed for probabilistic simulation. Despite its current limitations, such as the lack of UI in Dagger to edit the model and all models being public, Dagger offers features like dependency graph, intuitive and mathematically rigorous sensitivity analysis, and a summary table that exposes the structure of your model.</p><h3> <a href="https://forum.effectivealtruism.org/posts/9cdntNDJQTS8dH5fh/making-ea-more-inclusive-representative-and-impactful-in"><strong>Making EA more inclusive, representative, and impactful in Africa</strong></a></h3><p> <i>by Ashura Batu</i></p><p> The authors, Ashura Batungwanayo and Hayley Martin, discuss the importance of Effective Altruism (EA) in Africa, emphasizing the need to balance existential risks with urgent issues like poverty and education. They propose an EA Africa initiative that blends bottom-up and top-down approaches for contextually attuned change, with a focus on local partnerships, co-designed interventions, and self-reliance. They suggest forging partnerships with local organizations, promoting knowledge sharing, and empowering communities as potential solutions to make EA more inclusive, representative, and impactful in Africa.</p><h3> <a href="https://forum.effectivealtruism.org/posts/sBJLPeYdybSCiGpGh/impact-obsession-feeling-like-you-never-do-enough-good"><strong>Impact obsession: Feeling like you never do enough good</strong></a></h3><p> <i>by David_Althaus</i></p><p> &quot;Impact obsession&quot; is a term used to describe a potentially harmful way of relating to doing good, often observed among effective altruists. It is characterized by an overwhelming desire to do the most good possible, basing one&#39;s self-worth on their impact, and often leads to overexertion, neglect of non-altruistic interests, and anxiety about not having enough impact. While some aspects of impact obsession are reasonable and desirable, others can lead to negative consequences like depression, anxiety, guilt, exhaustion, burnout, and disillusionment.</p><h3> <a href="https://forum.effectivealtruism.org/posts/azezpsWKkcPJd2Hfy/why-we-should-fear-any-bioengineered-fungus-and-give-fungi"><strong>Why we should fear any bioengineered fungus and give fungi research attention</strong></a></h3><p> <i>by emmannaemeka</i></p><p> The speaker argues that fungi should be taken seriously due to the limited number of effective antifungal drugs, the lack of vaccines for fungal infections, and the emergence of fungi such as Candida auris, which is resistant to some antifungals and has caused serious infections and even death. Fungi are also the only species known to have caused the complete extinction of another species, and there are many Biosafety class 3 fungal pathogens that are understudied and lack effective treatments. The speaker&#39;s lab is currently studying the diversity of fungal species in Africa and how they are adapting to climate change, with the aim of identifying potential new fungi that could threaten human lives.</p><h3> <a href="https://forum.effectivealtruism.org/posts/Xd9ZZuPCKAvKpzvdB/empowering-numbers-fem-since-2021"><strong>Empowering Numbers: FEM since 2021</strong></a></h3><p> <i>by hhart</i></p><p> Family Empowerment Media (FEM) launched a radio campaign in 2021 to educate listeners in Nigeria about maternal health and contraception, reaching an estimated 5.6 million people. An independent survey showed that contraceptive use increased by about 75% among all women in the state within 11 months. FEM has since launched a 9-month campaign, reaching an estimated 20 million new listeners, and has been recommended by Giving What We Can and Founders Pledge for its cost-effectiveness.</p><h3> <a href="https://forum.effectivealtruism.org/posts/g4TcehspjDumGXucx/my-ea-journey"><strong>My EA Journey</strong></a></h3><p> <i>by Eli Kaufman</i></p><p> The author shares their journey into Effective Altruism (EA), which began with a podcast and led to extensive self-education on the topic. They found a way to align their IT career with their passion for doing good, and became an active member of the EA community in Amsterdam. The author encourages others to explore EA, emphasizing the importance of networking, the availability of resources, and the value of everyone&#39;s unique contributions.</p><br/><br/> <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary<guid ispermalink="false"> kjsDpnfGrArHhbtKQ</guid><dc:creator><![CDATA[smallsilo]]></dc:creator><pubDate> Fri, 25 Aug 2023 18:19:05 GMT</pubDate> </item><item><title><![CDATA[Assume Bad Faith]]></title><description><![CDATA[Published on August 25, 2023 5:36 PM GMT<br/><br/><p> I&#39;ve been trying to avoid the terms &quot;good faith&quot; and &quot;bad faith&quot;. I&#39;m suspicious that most people who have picked up the phrase &quot;bad faith&quot; from hearing it used, don&#39;t actually know what it means—and maybe, that the thing it does mean doesn&#39;t <a href="https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries">carve reality at the joints</a> .</p><p> People get very touchy about bad faith accusations: they think that you should assume good faith, but that if you&#39;ve determined someone is in bad faith, you shouldn&#39;t even be talking to them, that you need to exile them.</p><p> What does &quot;bad faith&quot; <em>mean</em> , though? It doesn&#39;t mean &quot;with ill intent.&quot; <a href="https://en.wikipedia.org/wiki/Bad_faith">Following <em>Wikipedia</em></a> , bad faith is &quot;a sustained form of deception which consists of entertaining or pretending to entertain one set of feelings while acting as if influenced by another.&quot; The great encyclopedia goes on to provide examples: the solider who waves a flag of surrender but then fires when the enemy comes out of their trenches, the attorney who prosecutes a case she knows to be false, the representative of a company facing a labor dispute who comes to the negotiating table with no intent of compromising.</p><p> That is, bad faith is when someone&#39;s apparent reasons for doing something aren&#39;t the same as the real reasons. This is distinct from malign intent. The uniformed solider who shoots you without pretending to surrender is acting in good faith, because what you see is what you get: the man whose clothes indicate that his job is to try to kill you is, in fact, trying to kill you.</p><p> The policy of assuming good faith (and mercilessly punishing rare cases of bad faith when detected) would make sense if you lived in an honest world where what you see generally is what you get (and you wanted to keep it that way), a world where the possibility of hidden motives in everyday life wasn&#39;t a significant consideration.</p><p> On the contrary, however, I think <a href="https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain">hidden motives in everyday life are ubiquitous</a> . As evolved creatures, we&#39;re designed to believe as it benefited our ancestors to believe. As social animals in particular, the most beneficial belief isn&#39;t always the true one, because tricking your conspecifics into adopting a map that implies that they should benefit you is sometimes more valuable than possessing the map that reflects the territory, and the most persuasive lie is the one you believe yourself. The universal human default is to come up with reasons to persuade the other party why it&#39;s in their interests to do what you want—but admitting that you&#39;re doing that <em>isn&#39;t part of the game</em> . <a href="https://www.lesswrong.com/posts/h2Hk2c2Gp5sY4abQh/lack-of-social-grace-is-an-epistemic-virtue">A world where people were straightforwardly trying to inform each other would look shocking and alien to us.</a></p><p> But if that&#39;s the case (and you shouldn&#39;t take my word for it), being touchy about bad faith accusations seems counterproductive. If it&#39;s common for people&#39;s stated reasons to not be the same as the real reasons, it shouldn&#39;t be beyond the pale to think that of some particular person, nor should it necessarily entail cutting the &quot;bad faith actor&quot; out of public life—if only because, applied consistently, there would be no one left. Why would you trust anyone so highly as to think they never have a hidden agenda? Why would you trust yourself?</p><p> The conviction that &quot;bad faith&quot; is unusual contributes to a warped view of the world in which conditions of information warfare are rationalized as an inevitable background fact of existence. In particular, people seem to believe that persistent good faith disagreements are an ordinary phenomenon—that there&#39;s nothing strange or unusual about a supposed state of affairs in which I&#39;m an honest seeker of truth, and you&#39;re an honest seeker of truth, and yet we end up persistently disagreeing on some question of fact.</p><p> I claim that this supposedly ordinary state of affairs is <em>deeply weird</em> at best, and probably just fake. <em>Actual</em> &quot;good faith&quot; disagreements—those where both parties are just trying to get the right answer and there are no other hidden motives, no &quot;something else&quot; going on— <a href="https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting">tend not to persist</a> .</p><p> If this claim seems counterintuitive, you may not be considering all the everyday differences in belief that are resolved so quickly and seamlessly that we tend not to notice them as &quot;disagreements&quot;.</p><p> Suppose you and I have been planning to go to a concert, which I think I remember being on Thursday. I ask you, &quot;Hey, the concert is on Thursday, right?&quot; You say, &quot;No, I just checked the website; it&#39;s on Friday.&quot;</p><p> In this case, I <em>immediately</em> replace my belief with yours. We both just want the right answer to the factual question of when the concert is. With no &quot;something else&quot; going on, there&#39;s nothing stopping us from converging in one step: your just having checked the website is a more reliable source than my memory, and neither you nor the website have any reason to lie. Thus, I believe you; end of story.</p><p> In cases where the true answer is uncertain, we expect similarly quick convergence in probabilistic beliefs. Suppose you and I are working on some physics problem. Both of us just want the right answer, and neither of us is particularly more skilled than the other. As soon as I learn that you got a different answer than me, my confidence in my own answer <em>immediately</em> plummets: if we&#39;re both equally good at math, then each of us is about as likely to have made a mistake. Until we compare calculations and work out which one of us (or both) made a mistake, I think you&#39;re about as likely to be right as me, even if I don&#39;t know how you got your answer. It wouldn&#39;t make sense for me to bet money on my answer being right simply because it&#39;s mine.</p><p> Most disagreements of note—most disagreements people <em>care</em> about—don&#39;t behave like the concert date or physics problem examples: people are very attached to &quot;their own&quot; answers. Sometimes, with extended argument, it&#39;s possible to get someone to change their mind or admit that the other party might be right, but with nowhere near the ease of agreeing on (probabilities of) the date of an event or the result of a calculation—from which we can infer that, in most disagreements people care about, there <em>is</em> &quot;something else&quot; going on besides both parties just wanting to get the right answer.</p><p> But if there&#39;s &quot;something else&quot; going on in typical disagreements that look like a grudge match rather than a quick exchange of information resulting in convergence of probabilities, then the belief that persistent good faith disagreements are common would seem to be in bad faith! (Because if bad faith is &quot;entertaining [...] one set of feelings while acting as if influenced by another&quot;, believers in persistent good faith disagreements are entertaining the feeling that both parties to such a disagreement are honest seekers of truth, but acting otherwise insofar as they anticipate seeing a grudge match rather than convergence.)</p><p> Some might object that bad faith is about conscious intent to deceive: <a href="https://slatestarcodex.com/2019/07/16/against-lie-inflation/">honest reporting of unconsciously biased beliefs</a> isn&#39;t bad faith. I&#39;ve previously <a href="https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#The_Optimal_Categorization_Depends_on_the_Actual_Psychology_of_Deception">expressed doubt as to how much of what we call <em>lying</em> requires conscious deliberation</a> , but a more fundamental reply is that from the standpoint of <a href="https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception">modeling information transmission</a> , the difference between bias and deception is <em>uninteresting</em> —usually not relevant to what probability updates should be made.</p><p> If an apple is green, and you tell me that it&#39;s red, and I believe you, I end up with false beliefs about the apple. It doesn&#39;t matter whether you said it was red because you were consciously lying or because you&#39;re wearing rose-colored glasses. The input–output function is the same either way: the problem is that the color you report to me doesn&#39;t depend on the color of the apple.</p><p> If I&#39;m just trying to figure out the relationship between your reports and the state of the world (as contrasted to caring about punishing liars while letting merely biased people off the hook), the main reason to care about the difference between unconscious bias and conscious deception is that the latter puts up much stronger resistance. Someone who is merely biased will often <em>fold</em> when presented with a sufficiently compelling counterargument (or reminded to take off their rose-colored glasses); someone who&#39;s consciously lying will <em>keep</em> lying <a href="https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies">(and telling ancillary lies to cover up the coverup)</a> until you catch them red-handed in front of an audience with power over them.</p><p> Given that there&#39;s usually &quot;something else&quot; going on in persistent disagreements, how do we go on, if we can&#39;t rely on the assumption of good faith? I see two main strategies, <a href="https://www.lesswrong.com/posts/SX6wQEdGfzz7GKYvp/rationalist-discourse-is-like-physicist-motors">each with their own cost–benefit profile</a> .</p><p> One strategy is to stick the object level. Arguments can be evaluated on their merits, without addressing what the speaker&#39;s angle is in saying it (even if you think there&#39;s probably an angle). This delivers most of the benefits of &quot;assume good faith&quot; norms; the main difference I&#39;m proposing is that speakers&#39; intentions be regarded as <em>off-topic</em> rather than presumed to be honest.</p><p> The other strategy is full-contact psychoanalysis: in addition to debating the object-level arguments, interlocutors have free reign to question each other&#39;s motives. This is difficult to pull off, which is why most people most of the time should stick to the object level. Done well, it looks like a negotiation: in the course of discussion, pseudo-disagreements (where I argue for a belief because it&#39;s in my interests for that belief to be on the shared map) are factorized out into real disagreements and bargaining over interests so that Pareto improvements can be located and taken, rather than both parties fighting to distort the shared map in the service of their interests.</p><p> For an example of what a pseudo-disagreement looks like, imagine that I own a factory that I&#39;m considering expanding onto the neighboring wetlands, and you run a local environmental protection group. The regulatory commission with the power to block the factory expansion has a mandate to protect local avian life, but not to preserve wetland area. The factory emits small amounts of Examplene gas. You argue before the regulatory commission that the expansion should be blocked because the latest Science shows that Examplene makes birds sad. I counterargue that the latest–latest Science shows that Examplene actually makes birds happy; the previous studies misheard their laughter as tears and should be retracted.</p><p> Realistically, it seems unlikely that our apparent disagreement is &quot;really&quot; about the effects of Examplene on avian mood regulation. More likely, what&#39;s actually going on is a <a href="https://www.lesswrong.com/posts/DpTexwqYtarRLRBYi/conflict-theory-of-bounded-distrust">conflict rather than a disagreement</a> : I want to expand my factory onto the wetlands, and you want me to not do that. The question of how Examplene pollution affects birds only came into it in order to persuade the regulatory commission.</p><p> It&#39;s inefficient that our conflict is being disguised as a disagreement. We can&#39;t both get what we want, but however the factory expansion question ultimately gets resolved, it would be better to reach that outcome without distorting Society&#39;s shared map of the bioactive properties of Examplene. (Maybe it doesn&#39;t affect the birds at all!) Whatever the true answer is, Society has a better shot at figuring it out if someone is allowed to point out your bias and mine <a href="https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting">(because facts about which evidence gets promoted to one&#39;s attention are relevant to how one should update on that evidence)</a> .</p><p> The reason I don&#39;t think it&#39;s useful to talk about &quot;bad faith&quot; is because the ontology of good <em>vs.</em> bad faith isn&#39;t a great fit to either discourse strategy.</p><p> If I&#39;m sticking to the object level, it&#39;s irrelevant: I reply to what&#39;s in the text; my suspicions about the process generating the text are out of scope.</p><p> If I&#39;m doing full-contact psychoanalysis, the problem with &quot;I don&#39;t think you&#39;re here in good faith&quot; is that it&#39;s insufficiently <em>specific</em> . Rather than accusing someone of generic &quot;bad faith&quot;, the way to move the discussion forward is by positing that one&#39;s interlocutor has some specific motive that hasn&#39;t yet been made explicit—and the way to defend oneself against such an accusation is by <a href="http://zackmdavis.net/blog/2022/05/plea-bargaining/">making the case that one&#39;s real agenda isn&#39;t the one being proposed</a> , rather than protesting one&#39;s &quot;good faith&quot; and implausibly claiming not to have an agenda.</p><p> The two strategies can be mixed. A simple meta-strategy that performs well without imposing too high of a skill requirement is to default to the object level, and only pull out psychoanalysis as a last resort against <a href="https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters">stonewalling</a> .</p><p> Suppose you point out that my latest reply seems to contradict something I said earlier, and I say, &quot;Look over there, a distraction!&quot;</p><p> If you want to continue sticking to the object level, you could say, &quot;I don&#39;t understand how the distraction is relevant to resolving the inconsistency in your statements that I raised.&quot; On the other hand, if you want to drop down into psychoanalysis, you could say, &quot;I think you&#39;re only pointing out the distraction because you don&#39;t want to be pinned down.&quot; Then I would be forced to either address your complaint, or explain why I had some other reason to point out the distraction.</p><p> Crucially, however, the choice of whether to investigate motives doesn&#39;t depend on an assumption that only &quot;bad guys&quot; have motives—as if there were bad faith actors who have an angle, and good faith actors who are ideal philosophers of perfect emptiness. There&#39;s always an angle; the question is which one.</p><br/><br/> <a href="https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith<guid ispermalink="false"> e4GBj6jxRZcsHFSvP</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Fri, 25 Aug 2023 17:36:32 GMT</pubDate></item><item><title><![CDATA[A Model-based Approach to AI Existential Risk]]></title><description><![CDATA[Published on August 25, 2023 10:32 AM GMT<br/><br/><h1>介绍</h1><p>两极分化阻碍了合作和了解未来人工智能是否对人类构成生存风险以及如何降低灾难性后果的风险的进展。确定这些风险是什么以及什么决策是最好的是非常具有挑战性的。我们认为，<i>基于模型的方法</i>具有许多优势，可以提高我们对人工智能风险的理解，评估缓解政策的价值，并促进人工智能风险争论不同方面的人们之间的沟通。我们还相信，人工智能安全和校准社区中的很大一部分从业者都拥有成功使用基于模型的方法的适当技能。</p><p>在本文中，我们将引导您通过一个基于模型的方法的示例应用来应对未结盟人工智能带来的生存灾难风险：基于卡尔史密斯的<a href="https://joecarlsmith.com/2023/03/22/existential-risk-from-power-seeking-ai-shorter-version"><i><u>《寻求权力的人工智能是否是​​存在风险？》的</u></i></a>概率模型。您将与我们的模型进行交互，探索您自己的假设，并（我们希望）就这种类型的方法如何与您自己的工作相关提出您自己的想法。您可以在此处找到该模型的链接。</p><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>在许多鲜为人知的领域，人们倾向于采取倡导立场。我们在人工智能风险中看到了这一点，经常看到作家<a href="https://www.lesswrong.com/posts/BTcEzXYoDrWzkLLrQ/the-public-debate-about-ai-is-confusing-for-the-general"><u>轻蔑地称某人为“人工智能末日论者”或“人工智能加速论者”。</u></a>这场辩论的双方都无法向另一方传达他们的想法，因为倡导通常包括在另一方不共享的框架内解释的偏见和证据。</p><p>在其他领域，我们亲眼目睹了基于模型的方法是消除此类宣传的建设性方法。例如，通过利用基于模型的方法， <a href="https://lumina.com/case-studies/energy-and-power/a-win-win-solution-for-californias-offshore-oil-rigs/"><u>Rigs-to-Reefs 项目</u></a>在 22 个不同的组织之间就如何退役圣巴巴拉海岸大型石油平台这一有争议的问题达成了近乎共识。几十年来，环保组织、石油公司、海洋生物学家、商业和休闲渔民、航运利益集团、法律辩护基金、加利福尼亚州和联邦机构在这个问题上陷入了僵局。模型的引入使对话重新聚焦于具体假设、目标和选项，并导致 22 个组织中的 20 个组织就同一计划达成一致。加州立法机构通过 AB 2503 号法案将该计划写入法律，该法案几乎一致通过。</p><p>人工智能带来的生存风险存在很多不确定性，而且风险极高。在这种情况下，我们主张使用概率分布明确量化不确定性。遗憾的是，这种情况并没有应有的普遍，即使在此类技术最有用的领域也是如此。</p><p> <a href="https://arxiv.org/abs/2206.13353"><u>Joe Carlsmith</u></a> (2022) 最近发表的一篇关于不结盟人工智能风险的论文有力地说明了概率方法如何帮助评估先进人工智能是否对人类构成生存风险。在本文中，我们回顾了 Carlsmith 的论点，并将他的问题分解纳入我们自己的<a href="https://lumina.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>模型中。然后，我们以多种方式扩展这个起点，以展示应对 x 风险领域中每个独特挑战的基本方法。我们带您参观实时模型，了解其元素，并使您能够自己更深入地研究。</p><h2>挑战</h2><p>预测长期未来总是充满挑战。如果<a href="https://www.youtube.com/watch?v=87l9Az9msHU&amp;t=3108s"><u>没有历史先例</u></a>，难度就会被放大。但这种挑战并不是独一无二的。我们在许多其他领域缺乏历史先例，例如在考虑新的政府计划或全新的商业计划时。当世界状况因技术、气候、竞争格局或监管变化而发生变化时，我们也缺乏先例。在所有这些情况下，难度都很大，但与预测通用人工智能 (AGI) 和存在风险的挑战相比就显得苍白无力了。今天对人工智能存在风险的预测通常至少部分依赖于关于未来先进人工智能将如何表现的抽象论点，而我们今天无法测试这些论点（ <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>尽管我们正在努力改变这一点</u></a>）。即使是最精心设计的论点也常常会遇到合理的不确定性和怀疑。<br><br>例如，在评估有关人工智能存在风险的预测的可靠性时，经常会遇到这样的反对意见：“我在预测中找不到具体的缺陷。它们只是有点抽象，有点连贯，而且论证这个班级经常会以意想不到的方式犯错。”</p><p>举个例子，最近超级预测者对人工智能风险的启发似乎表明，这种普遍的怀疑态度是超级预测者和人工智能安全社区之间对人工智能风险<a href="https://astralcodexten.substack.com/p/the-extinction-tournament"><u>持续存在分歧</u></a>的一个因素。尽管两个群体之间进行了讨论，而且超级预测者在<a href="https://www.lesswrong.com/posts/BHdEvjtfwpgrTh825/ai-21-the-cup-overfloweth?commentId=dpzuaGHyiNcnr4jF2"><u>有关未来人工智能的许多定量预测</u></a>上与领域专家达成了一致，但这种关于人工智能风险的分歧仍然存在，这表明对人工智能风险论点的怀疑更加分散。应认真对待此类反对意见，并根据其本身的优点以及过去类似反对意见在其他领域的表现进行评估。这凸显了不仅评估预测内容，而且评估其背后的基本假设和推理的重要性。</p><p>对于为什么可能出现某些结果，人工智能风险界已经提出了许多论点。当你着手建立一个明确的人工智能存在风险模型时，如果不吸收其他聪明、专注的人经过深思熟虑的想法，那就太疏忽了。然而，将多个想法合并到一个连贯的模型中确实很困难，并且根据某些统计，有多达<a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=C3sb2QZQAeHLZmz2T"><u>五个部分重叠的世界观/研究议程</u></a>，每个都专注于不同的威胁模型。不同的论点常常建立在相互不一致的概念框架之上。简单地统计某个立场存在多少个论据也是行不通的，因为基本假设几乎总是存在大量重叠。此外，以任何深入的方式将<a href="https://www.lesswrong.com/posts/Cd6uMn4qHXZcoe2Lh/discussion-weighting-inside-view-versus-outside-view-on"><u>内部观点与外部观点</u></a>论证合并起来似乎几乎是不可能的。虽然很困难，但整合现有的专家知识（和意见）对于有效的基于模型的方法至关重要。我们认为人工智能存在风险建模在整合多种专家知识来源方面具有独特的方面，因此是进一步研究新方法和新技术的成熟领域。我们已将解决本段中提到的所有挑战的简单方法纳入我们的模型中。</p><p>主观概率是表示不确定性的传统工具，一般来说，它是一个很好的工具。基于模型的方法依赖于对不确定变量的主观评估。在人工智能存在风险领域，当你要求两位专家评估相同的主观概率时，他们的估计通常会存在显着差异（例如，一位专家说 15%，而另一位专家说 80%）。这在其他领域并不正常。尽管您可能会发现两个气象学家分别预测下雨概率为 15% 和 80% 的情况，但这种情况并不常见。</p><p>这是上面已经讨论过的困难的症状，并且引入了该领域的另一个显着特征。由于专家之间的这种极端差异，以及人们的估计校准不佳的事实，似乎需要获得额外的信心层。我们在本文后面的“元不确定性”部分对此进行了详细说明，并且我们在模型中包含了显式的二阶分布（即，二阶分布代表专家意见之间的差异，而一阶不确定性代表不确定性）在结果中）。</p><p>我们在本文中描述的工作是作为 MTAIR 项目（建模变革性人工智能风险）的一部分进行的，建立在<a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">最初的 MTAIR 概念模型的</a>基础上。我们的目标是评估 AGI 存在风险的多个、有时甚至根本上相互冲突的详细模型以及这些外部观点/可靠性考虑因素。我们将他们视为相互竞争的“专家”，以便做出明智且平衡的评估。您可以使用我们的交互式模型来输入您自己的评估并探索其含义。</p><h2>是什么让模型有效？</h2><ul><li><strong>透明度</strong>：为了有效，基于模型的方法应该提供其他人可以浏览和理解的模型。当了解该主题的典型用户能够理解模型在做什么、如何做以及计算中使用什么假设时，我们称模型是<i>透明的</i>。您永远不应该假设主题专家是程序员，或者 python 代码（或任何其他编程语言）不言自明。因此，传统的程序通常被认为是不透明的。</li><li><strong>交互性</strong>：第二个重要属性是<i>交互性</i>，以及利益相关者尝试不同假设、探索不同决策或政策的后果以及探索任意假设场景的能力。</li><li><strong>显性的不确定性</strong>：对于人工智能的存在风险，大部分行动都是在不确定性的尾部进行的（即简单地得出中值结果是人类生存的结论是没有抓住要点的）；因此，<i>不确定性的明确表示</i>很重要。</li></ul><p>我们在<a href="https://analytica.com/"><u>Analytica 视觉建模软件</u></a>中构建了模型，该软件强烈满足上述所有要求，并且使用起来很有趣。 Analytica 模型的结构为分层影响图，这是一种高度可视化且易于理解的表示形式，捕捉了模型如何直观地工作的本质。它是交互式的并且具有嵌入式模块化文档。有强大的多维智能阵列设施，提供了前所未有的灵活性。它使用概率分布明确地表示不确定性。不确定性向下游计算结果的传播会自动发生。它学习起来既简单又快捷，一旦您构建了模型，您就可以将其发布到网络上进行共享（正如我们在本文中所做的那样）。</p><p>如果您受到我们的示例的启发来构建自己的模型，您应该知道<a href="https://analytica.com/products/free101/"><u>Analytica 有一个免费版本</u></a>。当您需要扩展到真正的大型模型时，也可以使用商业版本。桌面版本需要 Microsoft Windows。您无需获取或安装任何东西（浏览器 - Chrome 或 Edge 除外）即可使用我们的模型，该模型在 Analytica 云平台 (ACP) 上共享。我们的模型大约有 150 个对象，略超过免费版本 101 个对象的最大大小。但如果您有兴趣将其下载到桌面 Analytica，免费版本允许您加载、查看、运行、更改输入和重新评估结果等。</p><p>总之，基于模型的方法来评估人工智能存在风险预测的可靠性可以为人工智能安全社区带来多种好处。首先也是最重要的，它提供了清晰、简洁和易读的输出，考虑了可能影响预测准确性的许多不同的反对意见和因素。这有助于确保人工智能安全社区了解预测背后的推理和证据，并可以根据该信息做出明智的决策。</p><p><strong>此外</strong>，这种基于模型的方法鼓励社区考虑更广泛的因素，而不仅仅是详细的论点本身。例如，他们可能会考虑他们对高级抽象的信任程度以及不同启发式的可靠性。通过将这些考虑因素纳入模型中，社区可以更有效地权衡与人工智能相关的风险，并制定更稳健的策略来减轻潜在危害。最后，这种方法可以通过促进对所有相关因素进行更严格的思考和更全面的检查来帮助改善社区的认知，从而更好地理解人工智能存在风险的性质和可能性。</p><p>作为起点，我们将重点关注基于 Joe Carlsmith 报告<a href="https://arxiv.org/abs/2206.13353"><u>“寻求权力的人工智能是否存在风险”的</u></a>单一详细模型，以及影响这一机械模型合理性的几个外部观点/可靠性启发法。我们将首先简要介绍卡尔史密斯对人工智能存在风险的介绍以及我们自己的一些改进，然后在最后讨论改进该模型的后续步骤。</p><h1>型号概览</h1><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>这是在 Analytica 云平台 (ACP) 中运行的分层模型，基于 Joe Carlsmith 的报告“寻求权力的人工智能是否存在风险”。它可以让你计算由人工智能失调引起的生存灾难的概率。</p><p>这些结论隐含地取决于我们在给出各种假设的情况下明确规定的某个时间范围。事实上的时间范围是“到 2070 年”，但在输入您自己的估计时，您可以采用不同的时间范围，而无需更改模型的逻辑。</p><p>简而言之，该模型预测，如果出现以下情况，错位的寻求权力的人工智能将导致一场生存灾难：</p><ol><li>先进、规划、<a href="https://www.planned-obsolescence.org/situational-awareness/"><u>战略意识</u></a>（APS）系统——即能够进行高级规划、具有战略意识并拥有先进的人类水平或超人类能力的人工智能——是可行的，</li><li>当 APS 系统可行时，将会有强烈的激励措施来建设它们，</li><li>构建不以不协调的方式寻求权力的 APS 系统比构建表面上有用但确实以不协调的方式寻求权力的 APS 系统要困难得多，<ol><li>尽管 (3)，实际上将构建和部署错位的 APS 系统，</li></ol></li><li>未对准的 APS 系统在部署后将能够造成全球性的大灾难，</li><li>人类对造成此类灾难的错位 APS 系统的反应不足以阻止其完全接管，</li><li>一旦接管，错位的 APS 系统将摧毁或严重削弱人类的潜力。</li></ol><p>我们模型的总体框架基于卡尔史密斯报告和随后的<a href="https://80000hours.org/problem-profiles/artificial-intelligence"><u>80,000 小时文章</u></a>中提供的人工智能存在风险论点，并进行了修改。这是我们的“顶级模型”，我们对人工智能存在风险进行高级分析。</p><h1>模型巡演</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/mvk6oge6czjenatva2gs"></p><p>在本节中，您将快速浏览我们的模型，并在浏览器窗口中实时运行它。首先，请单击<a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><u>“启动模型”</u></a>以在不同的浏览器选项卡或窗口中将其打开，以便您可以同时参考此页面。我们提供分步说明来帮助您入门。按照此导览了解您的方向，然后您可以自己更深入地探索模型的其余部分，并探索不同估计值会发生什么。我们建议在大显示器上运行模型，而不是移动设备。</p><h2>基本评估</h2><p>在第一页上，您将看到卡尔史密斯报告中的六个概率评估。 （请注意，本文中的屏幕截图是静态的，但它们在运行模型的浏览器窗口中是活动的）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/rolr4s2y84njur0cbthc"></p><p>您可以在此处调整滑块或为每个滑块输入您自己的估计值。要了解每个含义，只需将鼠标悬停在问题上并阅读弹出的说明。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/adt7luy3kmzavtkfqwmd"></p><p>在估计这些之前，您应该选择一个时间范围。例如，您可以估计每个值在 2070 年之前是否成立。计算取决于您的估计，而不是所选的时间范围，但您的估计预计会随着更长期的时间范围而改变（增加）。</p><p>滑块输入下方是一些计算结果，显示 5 个阶段中的每个阶段以及所有前面的阶段最终为真的概率。最后一个“存在性灾难”显示了根据您对六个命题中每一个命题的估计，APS 系统发生存在性灾难的概率。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/qy1yjgqxgkjmwhzm43c5"></p><p>在这个屏幕截图中，我们看到 APS 有 0.37% 的几率（不到百分之一）会导致人类灭绝等生存灾难。考虑到结果的极端性，这似乎是一个巨大的风险，但许多专门研究人工智能安全的人会认为这是超级乐观的。您的估计如何比较？</p><h2>专家们权衡利弊</h2><p>您的估计与其他人工智能安全研究人员的估计相比如何？在卡尔史密斯的报告之后，开放慈善组织<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>征求了其他人工智能安全研究人员的评论</u></a>，并要求他们提供自己对这些主张的估计。这些评论发生于 2022 年 8 月。</p><p>首先，您可以通过按浏览他们对每个提议的原始评估<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/x5126haezlzwpqulhccb">审阅者评估按钮，该按钮显示在页面右下角（您可能需要向右滚动）。该表显示在页面的右上角。请注意审稿人之间的巨大差异。</p><p>单击“选择要使用的中值评估”的选项下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ugm5g5wylo2zr8tawy64"></p><p>选择所有项目，使其现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/jakuafkaps0pfd9umwih"></p><p>存在灾难输出现在显示<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">按钮。按下。结果表出现在右上角。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cdgv48bhgvvyqdl0accx"></p><p>这些显示了估计所隐含的由 APS 引起的存在灾难的概率，这些估计来自您自己的输入以及审阅者的输入。审稿人的中位数为 9.15%，但审稿人之间的数字差异很大。在少数情况下，审稿人不愿意接受卡尔史密斯提出的分解，就会出现空值。接下来，我们将其显示为条形图。将鼠标悬停在表格区域顶部以访问图形按钮，然后按它。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/coje0q97joeqsped2ibd"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/wi8p3wpl6wwu1xiaksrr"></p><p>再次将鼠标悬停在图表顶部，并将视图更改回表视图。查看结果时，您可以通过这种方式在图形视图和表格视图之间切换。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kxtxsvzrvhvdegaqopw8"></p><h2>专家意见的差异</h2><p>专家意见的巨大差异对该领域的理性决策提出了严峻挑战。很难说任何基于通过汇总这些概率获得的预期效用是可信的。因此，我们根据专家意见的变化拟合概率分布。因为这是主观概率的分布，所以它实际上是<i>二阶概率分布</i>，我们称之为<i>元不确定性</i>。我们用一<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.2klzxyklefio"><u>节</u></a>的内容来讨论元不确定性、其动机及其解释，但现在让我们想象一下这种元不确定性。</p><p>将<i>“选择要使用的中值评估”</i>更改为<strong>“所有审阅者的中值”</strong> ，并在“选择<i>要包含的元不确定性”</i>的选择下拉列表中选择<strong>审阅者的分布</strong>选项。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/whhm6iheuj3vqf4fa9h8"></p><p>输出现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">纽扣。将鼠标悬停在 Existential 灾难输出上并按鼠标右键。从上下文菜单中选择<strong>超出概率</strong>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dipzz5jhchogoicm7ydn"></p><p>在框架节点中，切换回图形视图（ <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/f8qvwqfinrmxpyjxzyny"> ）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y2mwu3iud8senllipqbi"></p><p>超出概率图是可视化概率分布的一种方法。本例中的分布反映了专家意见的差异。基本量（x 轴）是 APS 系统中发生诸如人类灭绝之类的生存灾难的概率。沿着绿色箭头，您可以看到大约 10% 的专家认为存在灾难的概率超过 0.17（即 17%），沿着黄色箭头，大约 5% 的专家认为存在灾难的概率超过 0.27。</p><p>为了获得这种二阶分布，该模型将每个问题的专家评估集合视为从基础分布中采样，然后将概率分布“拟合”到这些点。这种拟合的技术细节将在后面的“元不确定性”部分中介绍。该部分还探讨了当元不确定性（即专家意见之间的变化量）增加或减少时我们的观点如何变化。</p><h2>结合内部和外部视图参数</h2><p>卡尔史密斯分解是<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>内部观点框架</u></a>的一个例子，它将感兴趣的主要问题分解为其组成因素、步骤或起作用的因果机制。相比之下，<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>外部视图框架</u></a>从相似的事件或参考类中得出相似之处，以提供上下文和预测。例如， <a href="https://forum.effectivealtruism.org/posts/MMtbCDTNP3M53N3Dc/agi-safety-from-first-principles#AGI%20safety%20from%20first%20principles"><i><u>第二个物种论点</u></i></a>认为人类可能会失去地球上最强大物种的地位。其他外部观点框架包括霍尔顿·卡诺夫斯基的<a href="https://www.cold-takes.com/most-important-century"><i><u>“最重要的世纪”</u></i></a> 、阿杰亚·科特拉的<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>生物锚</u></a>（对一个子问题、时间表、更大问题的外部观点）、与过去变革性技术进步的类比，甚至<a href="https://forum.effectivealtruism.org/posts/mjB9osLTJJM4zKhoq/2022-ai-expert-survey-results"><u>专家意见调查</u></a>。</p><p>每种类型的框架都会产生不同的见解，但由于内部和外部视图框架以不同的方式进行评估，因此将两者吸收为一致的观点是相当具有挑战性的。但我们认为基于模型的方法需要解决这个问题，以便整合来自所有来源的信息。</p><p>我们包括两种简单的外部视图方法（在后面的部分中详细讨论），由这些输入反映出来： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/w4lspva3ifst3gseyzbh"></p><p>将鼠标悬停在每个输入上即可获取您所估计内容的完整描述。这些需要你抽象地思考几个高层的外部观点考虑因素和论点，然后评估这些考虑因素对存在灾难的风险有多大影响。 Cr在这里是<i>信任的</i>意思。与统计学中可能性的概念类似（有些人可能会说同义），可信度是从 0 到 1 范围内的估计，其中 0 表示考虑因素暗示没有风险，1 表示考虑因素暗示一定的灾难。</p><p>您现在已经输入了您自己对卡尔史密斯“世界模型”以及外部观点可信度的估计。我们的重点是模型如何将这些吸收到单一的主观观点中？我们的目标是强调这一挑战并至少尝试这样做。也许您或其他继续采用未来基于模型的方法的人会改进我们的方法。</p><p>在此模型中，我们允许您为不同视图分配相对权重。单击权重表按钮以赋予不同的意见。将鼠标悬停在输入上即可查看要求您评估的内容的描述。可信度是对您认为这些外部观点论据本身支持该主张的程度进行的评级。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/irrgjkuu6h3szgx5ma8o"></p><p>顶部框架中会出现一个条目表，其中包含可用于更改相对权重的滑块。您可以调整这些以反映您自己对相对可信度的看法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/hvjy2karshi3lntbs83r"></p><p>第一部分允许您输入卡尔史密斯分解与外部视图参数相比的相对重要性。在这里，我们将外部视图固定为 1，因此（基于卡尔史密斯的）世界模型的值为 3 意味着您希望该框架的计数是外部视图参数的三倍。</p><p>在世界模型中，您有自己的估计以及接受调查的各个专家的估计。您可以选择或多或少地重视个别专家的估计。</p><p>最后，在下部，您可以调整两个不同的外部视图框架的权重。这些用于组合不同的外部观点论点。</p><p>设置自己的权重后，右列中的输出将显示同化视图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/s586akwpoclyxujnzywc"></p><p>第一个输出 Cr[存在灾难|世界模型] 是卡尔史密斯分解在考虑了您自己的估计与专家的估计之间的相对权重后的评估。</p><p>第二个输出 Cr[AI Existential Catastrophe] 是组合外部视图模型中存在灾难的概率。</p><p>最终输出 Cr[Existential catastrophe] 是对存在灾难的最终同化估计。它考虑了内部视图世界模型和外部视图模型，将两个来源的信息结合起来作为代表性的最终评估。</p><h2>探索模型的内部结构</h2><p>到目前为止，您已经使用了我们为您突出显示的一些选定的输入和输出。接下来，您将探索模型的内部结构。</p><p>顶部是一个大的蓝色模块节点<strong>Main Model</strong> 。点击它。这将带您进入实施阶段，您会看到几个子模块和<a href="https://lumina.com/technology/influence-diagrams/"><u>影响图</u></a>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cm3tppz2rkmycyj2rvyb"></p><p>在第一个图中，上半部分包含基于卡尔史密斯报告的内部视图世界模型。左下角包含外部视图参数。右下四分之一是用于同化不同观点的逻辑。</p><p>影响图的节点是变量。箭头描绘了变量之间的影响。影响图是直观的，您通常可以从中了解模型是如何工作的，而无需查看计算细节。将鼠标悬停在节点上可查看其描述，以获取有关每个变量所代表含义的更多信息。</p><p>在外部视图部分中，一些未定义的节点（已散列）仅用于记录纳入估计的考虑因素。虚线箭头表示这些不是计算所使用的影响，但应该会影响您的思考。</p><p>单击节点后，请注意顶部的选项卡。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vd4gjj6wwh9ffyez8t4p"></p><p> <strong>“对象”</strong>选项卡可能是最有用的，因为它允许您查看单击的变量的定义（和其他属性）。查看完该变量后， <strong>“图表”</strong>选项卡将返回图表。</p><p>现在您已经完成了这个快速浏览，您应该可以轻松地探索模型的各个方面。接下来，我们将更深入地研究纳入模型的内容和概念。</p><h1>型号特点</h1><p>在将卡尔史密斯报告的人工智能存在风险模型应用于 Analytica 时，我们对原始计算进行了一些更改，即简单地将命题 1-6 的条件概率相乘，以获得对错位人工智能的存在风险的总体估计。</p><p>为了更好地捕获围绕问题的全方位不确定性，我们处理了“元不确定性”，方法是将每个点估计更改为一个分布，其方差取决于我们对每个概率估计的置信度，如上<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.wtvwqwsb1xzp"><u>一节所述</u></a>。</p><p>元不确定性是指由于我们对影响我们的信念或观点的更普遍因素的不确定性而产生的不确定性。这些因素可能包括诸如我们应该对内部观点和外部观点给予多少重视，以及长期预测的可靠性如何等问题。</p><p>元不确定性与更直接的不确定性不同，因为它关注的是我们对风险评估所依据的假设和因素的不确定性。它本质上是二阶不确定性，我们不确定驱动一阶不确定性的因素。</p><p>我们通过将<a href="https://www.wikiwand.com/en/Logit-normal_distribution"><u>Logit 正态分布拟合到 Joe Carlsmith 报告的每个原始审稿人给出的单个点估计值的分布，生成了这些元不确定性分布</u></a>。该方法与本文<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future#3_2_Model_Parameterisation"><u>《化解人工智能风险》</u></a>中使用的方法类似。</p><p>我们还纳入了其他不太详细的“外部观点考虑因素”，它们不像卡尔史密斯报告那样依赖于详细的世界模型。我们对这些与卡尔史密斯模型相关的外部观点争论的信任会影响该模型给出的人工智能存在灾难的最终无条件概率。这些外部观点的考虑可以被视为补偿详细世界模型中出现的一般可靠性问题的一种方法，因此是减少我们模型的随机误差或“未知的未知”困难的一种方法。</p><p>我们尚未讨论的一件事是卡尔史密斯模型中潜在的系统缺陷。正如我们将在“框架效应”一节中讨论的那样，一些研究人员反对卡尔史密斯报告本身的框架，认为它系统性地使我们产生向上或向下的偏见。</p><h2>元不确定性</h2><p>围绕人工智能存在风险问题存在许多复杂且不确定的问题，包括协调的难度、不协调的人工智能是否容易接管，甚至是否会建立“APS”类型的通用人工智能（AGI）。世纪。这些不确定性使得评估人工智能存在风险的总体概率变得困难。</p><p>量化这些风险的一种方法是为每个索赔分配点概率估计并将其向前传播，正如卡尔史密斯关于该主题的原始报告中所做的那样。然而，这种方法存在一些问题。作为卡尔史密斯模型输入的六个概率估计中的每一个都涉及历史上没有先例的事件。因此，估计这些事件的概率具有挑战性，并且当您看到两位不同专家的估计存在显着差异时，没有明确且明显的方法来判断哪个估计更可信。</p><p><i>元不确定性</i>通过对可能的意见进行概率分布来审视可能的信念状态。<a href="https://acp.analytica.com/view?invite=4418&amp;code=3221222959844354027"><u>我们的模型</u></a>包含您可以探索的几个版本的元不确定性。</p><p>包含元不确定性的一个有用目的是了解专家意见的变化，以及这种变化如何影响模型的输出。</p><p>开放慈善组织要求人工智能风险领域的几位专家<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>提供他们自己对卡尔史密斯报告中参数的估计</u></a>。我们已将这些包含在我们的模型中。您可以从这些专家中的任何一位或任何专家集中选择估计值。您还可以包括 Joe Carlsmith 在他的文章中给出的估计、所有审稿人的中位数以及您自己的估计。当您同时选择多个时，您将能够在任何下游结果中对它们进行比较。要进行选择，请使用模型前图中的“选择要使用的中值评估”的多选下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/zg0ztojles89j7qd1uqy"></p><p>当您查看模型中变量的结果时，您将看到使用您选择的每个审阅者的估计得出的结果值。例如，这是存在灾难概率的结果表。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kojklpskvf2kxdmgxeda"></p><p>从这些中，您可以了解专家意见的差异有多大，但这还不包括元不确定性的概率分布。对于每个输入，您可以让模型将概率分布拟合到审阅者提供的评估（对于统计极客：它拟合<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal</u></a> ，又名 Log-Odds 分布）。要自己探索这一点，请将“选择要包含的元不确定性”下拉菜单设置为“审阅者的传播”。完成此操作后，它会使用具有跨专家观察到的元不确定性方差的分布来执行所有计算（对于统计极客来说：它实际上是每个数量的<a href="https://docs.analytica.com/index.php/Logit"><u>logit</u></a>的方差与专家的方差相匹配）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/isvopzarcegg2d9pwuug"></p><p> Within the model&#39;s internals, the variable named &#39;Assessments&#39; now contains the meta-uncertainty distributions for each of the six input assessments. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ndgs3pq2uosqwvcpylgj"></p><p> The above graph shows the <a href="https://docs.analytica.com/index.php/Uncertainty_views#Cumulative_probability"><u>cumulative probability</u></a> for each assessed quantity (known as a CDF plot). The value on the Y-axis indicates how likely it is that an expert would estimate the quantity to have a value less than or equal to the corresponding value on the x-axis. The plot&#39;s key items correspond, in order, to the six assessments of the Carlsmith model. The first item, labelled <i>Timelines</i> , is the assessment that APS systems will be feasible to build within the timeline window considered. Its red CDF is almost a straight line, indicating an almost uniformly-distribution uncertainty among the selected experts. The light blue line labelled <i>Catastrophe</i> is the assessment that an unaligned APS system that has already taken over will then destroy or curtail the potential of humanity. The shape of that curve indicates that there is agreement between the selected experts that the probability is close to 1.</p><p> The calculation behind the above graph sets the median of each input meta-uncertainty distribution to the median of the selected reviewers on the same question. By changing the slicer control “Select median assessment to use” at the top of the above graph, you can apply the same level of meta-uncertainty to any single reviewer&#39;s assessments (or your own assessments).</p><p> <a href="https://analytica.com/why-analytica/what-is-analytica/"><u>Analytica</u></a> automatically propagates these meta-uncertainties to any computed downstream result. Here we see the CDF plot for the probability of existential catastrophe (the product of the six assessments). </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/o8nxineidbllqcecttka"></p><p> The assessments from any one person would result in a single probability for this quantity, &#39;Existential Catastrophe&#39;. The above distribution reflects the variation across expert opinions. The curve indicates a 50% probability that an expert would conclude the probability of existential catastrophe is less than 1%. Conversely, using the 0.9 level of the Y-axis, there is a 10% probability that an expert would conclude the probability of existential catastrophe exceeds 15%. When you run the model, you can select a different subset of experts (or all of them) to interactively explore the subset of experts you trust the most.</p><p> When you provide your own estimates for each of the six input probabilities (which we recommend you try when you run the model), you&#39;ll probably have a gut feeling that your estimates are not reliable. You&#39;ll probably feel this way even if you are an expert in the field. You might find it useful to include (or let the model include) meta-uncertainty over your own personal assessments. The model allows you to do so. But first, let&#39;s discuss what a meta-uncertainty over your own belief state even means.</p><p> Each input to the model asks you for your own <i>subjective probability</i> . Each of these summarise your state of knowledge on that question. No one knows whether any of the six propositions are true or false. Your subjective probability simply reflects the strength of the knowledge that you have. You are not estimating a value that exists out there in the world, you are instead estimating your degree of belief. By applying a meta-uncertainty to your degree of belief, you are essentially saying that you are uncertain about what your own beliefs are. That may not intuitively feel far-fetched in a case like this, where there is virtually no historical precedent! In general, when it comes time to making a decision, if you can express your meta-uncertainty, you could also collapse it to a single degree-of-belief number by simply taking the mean belief (or mean utility). Until then, meta-uncertainty gives an indication of how responsive your beliefs would be to new information.</p><p> In a recent article on the Effective Altruism forum, <a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future"><u>&#39;Dissolving&#39; AI Risk - Parameter Uncertainty in AI Future Forecasting</u></a> , the author under the pseudonym <a href="https://forum.effectivealtruism.org/users/froolow"><u>Froolow</u></a> adds meta-uncertainty to each of the six Carlsmith model parameter estimates and shows that when doing so, the estimated existential risk from AI decreases. You can explore the same effect in our model. A good starting point is to select a single median estimate – for example, the estimates from the original Carlsmith report. Then select &#39;View across range of meta-u&#39; in the meta-uncertainty selection. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/fwdimssrxrvfpbjiy1hc"></p><p> The Meta-uncertainty option varies the amount of meta uncertainty from zero (ie, point estimates) toward the maximum meta-uncertainty that is possible for a single probability estimate. The same logit-variance is applied to all six input assessments for each level of meta-uncertainty.</p><p> A <i>Probability Bands</i> view of the main output - the probability of existential catastrophe – illustrates how the meta-uncertainty in the final result behaves as the meta-uncertainty in each parameter is increased. The Bands plot is shown here. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/iob1bw51uwthkbemaies"></p><p> (Note: The squiggles are small variations due to a finite sample size during Monte Carlo).</p><p> Without meta-uncertainty, Carlsmith estimated a 5% probability of existential catastrophe, seen at the left when the level of (meta-)uncertainty is zero. With increasing meta-uncertainty, the median estimate (green line) drops to about 0.75% at the right of the plot, and continues to drop further to the right of what is plotted here. Even the 0.75 quantile drops (eventually) with increasing meta-uncertainty.</p><h2> Framing effects</h2><p> There is a paradox here. Why should being less certain about what you believe make you conclude that the world is a safer place? Does this establish that “ignorance is bliss”? Will  existential catastrophe be more likely if we invest in more research to increase our understanding of just how much we are at risk?</p><p> Some research models AI takeover as being a disjunctive event, meaning that it will happen unless certain conditions are fulfilled, while others (such as Carlsmith) see it as a conjunctive event, meaning that a set of conditions must be met in order for the disaster to occur.</p><p> These framing effects don&#39;t affect the final results when using point estimates. If we took the Carlsmith model and turned every proposition in the model into a negative statement rather than a positive: eg, &#39;APS systems will not produce high impact failures on deployment&#39;, and take one minus our original probability estimates, then we will get the same final probability. But, crucially, if we have uncertainty around our probability distributions the conjunctive and disjunctive models do not behave the same way.</p><p> The paradox becomes even more paradoxical when you realise that reversing the framing inverts the effect. The Carlsmith decomposition says that catastrophe occurs when 6 events all occur. You could instead posit that catastrophe from superintelligence is inevitable unless 6 open technical problems are solved before then (in fact, in the post <a href="https://www.lesswrong.com/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-based-on-a-recent-peer-reviewed"><u>AI X-risk >;35% mostly based on a recent peer-reviewed argument</u></a> on LessWrong, Michael Cohen uses this framing). With this reverse framing, increasing meta-uncertainty drives the effect in the opposite direction, making it appear that catastrophe is more likely the more uncertain we are. Soares&#39; <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive"><u>article on disjunctive AGI ruin scenarios</u></a> conveys this view qualitatively, listing a number of things that he believes all have to go right to avoid an AI existential catastrophe: on such a model, general uncertainty about the world increases the chance of disaster.</p><p> The paradox is, of course, an illusion. But because you could be easily misled, it is worth understanding this phenomena at a deeper level. The result in the previous graph is the product of six uncertain estimates. The following mathematical relationship, which is simply a rearrangement of the definition of covariance, shows that the arithmetic mean is stable as (meta-)uncertainty increases:</p><p> E[xy] = E[x] E[y] + cov(x,y)</p><p> In other words, when the assessment of each parameter is independent (implying a covariance of zero), then the mean of their product is the product of their means. Hence, a plot of the mean vs. level of meta-uncertainty would be a horizontal line. (Side note: Covariances between the parameter estimates are likely not really zero for numerous reasons, but the model does not include any representation or estimate of covariance. The relevant question is whether they are modelled as independent, and indeed they are in our model).</p><p> However, the median of a product decreases with increasing meta-uncertainty. This happens regardless of the shape of the meta-uncertainty distribution. In order for this to happen, the right tail of the meta-uncertainty distribution must increase to compensate for the drop in median. This means that as you have more meta-uncertainty, the meta-uncertainty distribution becomes more <a href="https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic"><u>leptokurtic</u></a> . The net balance, as shown by the stability of the mean, is that does not cause you to conclude the world is more (or less) safe.</p><p> In our model, the mean actually does decrease ever so slightly with increasing meta-uncertainty. You&#39;ll see this if you select the Mean view. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y24kdjxjy3pu9vygakv8"></p><p> The waviness is due to the fact that this is computed by <a href="https://docs.analytica.com/index.php/Monte_Carlo_and_probabilistic_simulation"><u>Monte Carlo simulation</u></a> with a finite sample size. The slight decrease is because we hold the median of each distribution constant as we apply meta-uncertainty. The meta-uncertainty of each parameter is modelled using a <a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal distribution</u></a> , also called a Log-odds distribution, in which the <a href="https://docs.analytica.com/index.php/Logit"><u>Logit</u></a> of the quantity is distributed as a Normal distribution. We keep the mean of the Normal constant as we increase its variance. When you do this, the mean of the logit decreases slightly, so that the mean of each parameter estimation decreases slightly. If you hold the mean constant instead of the median (which is easy to do), then the mean is entirely stable. We found the difference in these two options to be non-perceptible in the Probability Bands graph.</p><p> In the article <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>&#39;Is the Fermi Paradox due to the Flaw of Averages?&#39;</u></a> , we reviewed the paper <a href="https://arxiv.org/abs/1806.02404"><u>&#39;Dissolving the Fermi Paradox (2018)&#39;</u></a> by Sandberg, Drexler and Ord (SDO), and provided a live interactive model. The Fermi Paradox refers to the apparent contradiction that humankind has not detected any extraterrestrial civilizations even though there must be a lot of them among the hundreds of billions of stars in our galaxy. Like the Carlsmith model, the <a href="https://www.seti.org/drake-equation-index"><u>Drake equation</u></a> (which estimates the number of detectable civilizations in the Milky Way) is a multiplicative model. SDO shows that by modelling uncertainty in each of the Drake equation parameters explicitly, the Fermi paradox ceases to be surprising.</p><p> The <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>Fermi paradox model with explicit uncertainty</u></a> and the Carlsmith model with explicit meta-uncertainty (the topic of this article) have the same mathematical form. We see the median and the lower quantiles decrease in the Carlsmith model with increasing (meta-)uncertainty, but this doesn&#39;t really alter our effective judgement of risk. However, the increased uncertainty in the Fermi model dramatically increases the probability that we on Earth are alone in the galaxy. Why is the effect real in the Fermi case but only an illusion in the present case?</p><p> The reason the effect is real in the Fermi case is that the question asked (&#39;What is the probability that there is no other contactable, intelligent civilization in the Milky Way?&#39;) is a question about a quantile, and lower quantiles are indeed decreased when uncertainty increases. P(N&lt;1), where N is the number of such extraterrestrial civilizations, is a cumulative probability, or inverse quantile. Since increasing uncertainty in the factors of a multiplicative model decreases the quantiles in the left tail, it causes the inverse quantiles to increase. Hence, the addition of uncertainty to the Drake equation legitimately increases the probability that we are alone in the galaxy. The real flaw was from omitting the explicit representation in the first place (what Sam L. Savage calls <a href="https://www.flawofaverages.com/"><u>the Flaw of Averages</u></a> ). In contrast, the primary question posed by the Carlsmith model (&#39;What is the probability of existential catastrophe?&#39;) is a question about the mean relative to meta-uncertainty. Hence, for this question (or for any decision based on an expected utility), the appearance that risk decreases as a result of including meta-uncertainty is only an illusion.</p><h3> Explaining framing effects</h3><p> We have seen that the apparent paradox arising from framing effects is illusory. But there is a further question: what is the &#39;right&#39; way to frame AI existential risk, as conjunctive or disjunctive?</p><p> This is a difficult question to answer. One perspective is that treating AGI existential catastrophe as something that will happen unless certain conditions are met might lead to overestimation of the chance of high-impact failures. On this view, requiring a clear path to a stable outcome with complete existential security is both too demanding and historically inaccurate, since that isn&#39;t how humanity ever navigated previous threats. Holden Karnofsky makes a similar point <a href="https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding#Success_without_dignity"><u>here</u></a> . A framing which sees success as conjunctive probably rules out &#39;muddling through&#39;, ie, unplanned &#39;success without dignity&#39;. Since this is something that many domain experts <a href="https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom?commentId=ibGxdAC9nYajWfyfq"><u>believe is credible</u></a> <u>,</u> it might lead us to significantly underrate the chance of survival.</p><p> On the other hand, some experts such as <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive#Correlations_and_general_competence"><u>Nate Soares</u></a><u> </u>argue that AI is a different case: the large number of actors working on AGI and the risk that any one of them could produce an existential catastrophe, along with all the things that would have to occur to prevent this (someone has to develop an aligned AGI and then quickly use it to eliminate AI existential risk), implies that treating survival as a conjunctive event makes more sense.</p><p> These different framings reflect varying world models and threat models. Part of why this disagreement exists is because of Soares&#39; views about extreme AI <a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#Sharp_Left_Turn"><u>alignment difficulty</u></a> , AI takeoff speed and the low likelihood of effective mitigation measures. If you are implicitly using a model where human civilization tends to respond in fixed ways due to internal incentives unless something intervenes, it is more natural to think that we will follow a default path towards disaster unless a specific intervention occurs. On the other hand, if we see many possible futures and many pathways to reducing AI existential risk and don&#39;t know what the final response will look like (as the <a href="https://www.lesswrong.com/posts/Fbk9H6ipfybHyqjrp/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai">&#39;Playbook for AI Risk Reduction&#39;</a> describes), then requiring a specific set of conditions to be met for success seems overly prescriptive.</p><p> We believe that this framing question, and whether to treat survival as conjunctive or disjunctive, is <i>itself</i> something which we should be uncertain about, since whether you treat survival as conjunctive or not depends on the details of your threat model, and we don&#39;t want to assume that any one threat model is the only correct one.</p><p> Currently, we only have the Carlsmith report model, but in theory we could address this problem by looking at both a conjunctive and disjunctive model and comparing them in detail.</p><p> For example, the report, &quot; <a href="https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical"><u>Three Pillars for Avoiding AGI Catastrophe: Technical Alignment, Deployment Decisions, and Coordination</u></a> ,&quot; provides a starting point model that treats success as conjunctive, and we can adapt it to work alongside Carlsmith&#39;s model.</p><p> Another alternative is to alter the Carlsmith report to require fewer steps, better representing the concern that the longer a chain of conjunctions is, the more likely it is to omit disjunctive influences. This formulation collapses propositions (1) and (2), which consider the incentives and feasibility of developing APS, into a straightforward estimate of &quot;when will AGI be developed.&quot; The alignment difficulty premise is then preserved, followed by the collapse of propositions (4, 5, 6) into an estimate of the chance of a takeover given a misaligned APS-AGI. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dozxmoz0isqh8ouyyxn9"></p><p> This alternative formulation has fewer steps and so better represents the model that treats misaligned AI takeover as involving many possible routes that are hard to counter or influence in advance, and sees misaligned power seeking behaviour as a natural consequence of AGI development. This approach may be more appropriate for those who believe that the development of misaligned power seeking systems is a likely outcome of AGI development and that the risk of an AI takeover is more closely tied to the development of AGI systems themselves.</p><p> In addition to exploring conjunctive and disjunctive models of AI existential risk, it may also be useful to equivocate between models that make more detailed technical assumptions about how APS will get developed. For example, Ajeya Cotra&#39;s model <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"><u>&quot;without specific countermeasures, the easiest path to AGI results in takeover&quot;</u></a> tries to construct a specific model of AGI development with technical assumptions, but given those assumptions, is more easily able to reach a stronger conclusion. Similarly, given that there is a wide diversity of views on exactly how AGI might end up misaligned and power-seeking, instead of a binary &#39;Is misaligned AI developed or not&#39;, we might have a <a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#The_Scale"><u>distribution over alignment difficulty</u></a> with a varying success probability.</p><p> Disambiguating different models with different technical assumptions can help us to better understand the potential risks associated with AI development. By exploring different models with varying levels of technical detail and assumptions, we can gain a more comprehensive understanding of the potential risks.</p><p> While this model does not incorporate entire complex alternative inside-view models like those just mentioned, we have incorporated some alternative, less-detailed, simpler alternative &#39;outside view considerations&#39; to illustrate how we go about combining different worldviews to produce an all-things considered estimate.</p><h1> Outside View considerations</h1><p> We&#39;ve talked before about the challenges of combining outside view considerations and more detailed models of the same question. We can attempt to integrate these considerations by delving deeper and examining various reasons to expect our detailed world models to be systematically mistaken or correct.</p><p> We will examine five reference classes into which various experts and commentators have placed AI existential catastrophe. In each case: &#39;Second Species&#39;, &#39;Reliability of existential risk arguments&#39;, &#39;Most important century&#39;, &#39;Accuracy of futurism&#39;, &#39;Accuracy of predictions about transformative tech&#39;, the argument locates AI Existential risk arguments in a (purportedly) relevant reference class: predictions about new sentient species, predictions about human extinction, predictions about which period in history is the most impactful, predictions about large scale civilizational trends in general and predictions about transformative technologies (including past predictions of dramatic AI progress).</p><p> The Carlsmith model implies that all of these things could occur (a new species, extinction, this period of history will be extremely impactful, there will be a large-scale dramatic transformation to society, there will be dramatic transformative technical progress), so it is worth examining its predictions in each reference class to determine if we can learn anything relevant about how reliable this model is.</p><h2> Second species argument</h2><p> This argument suggests that as we create AGI (Artificial General Intelligence) we are essentially creating a “ <a href="https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"><u>second species</u></a> ” that is a human-level intelligence. And by analogy, just as humans have historically been able to supplant other animals, AGI may be able to supplant humans.</p><p> The key premise is that intelligence confers power. Human intelligence allows us to coordinate complex societies and deploy advanced technology, exerting control over the world. An AGI surpassing human intelligence could wield even greater power, potentially reducing humanity to a subordinate role. Just as humans have driven some species extinct and transformed ecosystems, a superintelligent AGI need not preserve humanity or our values. Anthropologists observe that new species often displace incumbents when invading a territory. Similarly, AGI could displace humankind from our position controlling Earth&#39;s future.</p><p> This argument is straightforward and has been widely understood by researchers going all the way back to Alan Turing the 1950s, so while it relies on fuzzy concepts and is open to many objections, it arguably has a better &#39;track record&#39; in terms of the amount of scrutiny it has received over time than the more detailed arguments given by Carlsmith.</p><h2> Reliability of existential risk arguments</h2><p> Another important consideration is the base rate for arguments of existential risk. Historically, predictions of catastrophic events, even ones that were apparently well justified by detailed arguments, have not always been accurate. Therefore, it is important to consider if the possibility that the risks associated with AGI are overestimated for similar underlying reasons (eg, the social dynamics around existential risk predictions, overestimating the fragility of human civilisation, or underestimating humanity&#39;s ability to respond in ways that are hard to foresee).</p><p> One possible driver of inaccuracy in existential risk predictions is <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential"><u>sleepwalk bias</u></a> . Sleepwalk bias is the tendency to underestimate people&#39;s ability to act to prevent adverse outcomes when predicting the future. This can be caused by cognitive constraints and failure to distinguish between predictions and warnings. Because warnings often take the form of &#39;X will happen without countermeasures&#39;, if warnings are misused as predictions we can underestimate the chance of successful countermeasures. People often mix up the two, leading to pessimistic &quot;prediction-warnings&quot;. Thus, when making predictions about existential risk, it&#39;s important to adjust our base rate to account for people&#39;s potential to act in response to warnings, including those made by the one giving the prediction.</p><p> Sleepwalk bias stems from the intuitive tendency to view others as less strategic and agentic than oneself. <a href="https://stefanschubert.substack.com/p/sleepwalk-bias-and-the-role-of-impulses"><u>As Elster notes</u></a> , we underestimate others&#39; capacities for deliberation and reflection. This manifests in predictions that underestimate how much effort people will make to prevent predicted disasters. Instead, predictions often implicitly assume sleepwalking into calamity.</p><p> For existential risks, sleepwalk bias would specifically lead us to underestimate institutions&#39; and individuals&#39; abilities to recognize emerging threats and mobilize massive resources to counter them. Historical examples show that even deeply conflictual societies like the Cold War rivals avoided nuclear war, underscoring potential blindspots in our models. Since the bias arises from a simple heuristic, deep expertise on a given x-risk may overcome it. But for outsiders assessing these arguments, accounting for sleepwalk bias is an important corrective.</p><h2> Most important century</h2><p> Additionally, it is important to consider the probability that the next century is the most important of all, which would plausibly be true if AGI existential risk concerns are well founded. If we have a strong prior against this <a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#the-"><u>&#39;most important century&#39;</u></a> idea then we will be <a href="https://globalprioritiesinstitute.org/wp-content/uploads/William-MacAskill_Are-we-living-at-the-hinge-of-history.pdf"><u>inclined</u></a> to think that AGI existential risk arguments are somehow flawed.</p><p> The Self-Sampling Assumption (SSA) posits that a rational agent&#39;s priors should locate them uniformly at random within each possible world. If we accept the SSA, it seems to imply that we ought to have a low prior on AI existential risk (or any kind of permanent dramatic civilizational change) in this century in particular because of the near-zero base rate for such changes. The detailed evidence in favour of AI existential risk concerns may not be enough to overcome the initial scepticism that arises from our natural prior.</p><p> Alternatively, you might accept the claim<a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#my-view"><u>proposed by Karnofsky</u></a> that there are extremely strong arguments that this <a href="https://www.cold-takes.com/this-cant-go-on/#why-cant-this-go-on"><u>approximate period in history must be very important</u></a> . First, Karnofsky argues that historical trends in economic growth and technological development show massive accelerations in the recent past. Growth rates are near all-time highs and appear unsustainable for more than a few thousand years at most before physical limits are reached. This suggests we are living during a temporary spike or explosion in development.</p><p> Second, he notes that since growth is so rapid and near its limits, some dramatic change seems likely soon. Possibilities include stagnation as growth slows, continued acceleration towards physical limits, or civilizational collapse. This situation seems intrinsically unstable and significant. While not definitive, Karnofsky believes this context should make us more open to arguments that this time period is uniquely significant.</p><h2> Accuracy of futurism</h2><p> Another important consideration is the base rate of forecasting the future without empirical feedback loops. This consideration fundamentally focuses on the process used to generate the forecasts and questions whether it reliably produces accurate estimates. The history of technology has shown that it can be difficult to predict which technologies will have the most significant impact and AI alignment research especially often relies on complex abstract concepts to make forecasts, rather than mechanistically precise models. <a href="https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk#I_don_t_trust_chains_of_reasoning_with_imperfect_concepts"><u>Some examples</u></a> are discussed in this article.</p><p> One way of assessing reliability is to find a reference class where predictions of AI existential catastrophe are comparable to other future predictions. For instance, we can compare AI predictions to the predictions made by professional futurists in the past and then <a href="https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/#todays-futurism-vs-these-predictions"><u>compare relevant features</u></a> . If they compare favourably to past successful predictions, this may indicate a higher level of reliability in the TAI predictions, and if they don&#39;t, it may suggest that we should be cautious in our assessment of their validity.</p><p> We can also look at other general features of the arguments without comparison to specific known examples of successful futurism, like their level of reliance on abstract concepts vs empirical evidence. AI risk involves unprecedented technologies whose impacts are highly uncertain. There are likely gaps in our models and unknown unknowns that make it difficult to assign precise probabilities to outcomes. While we can still make reasonable estimates, we should account for the significant <a href="https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms"><u>Knightian Uncertainty</u></a> by avoiding overconfident predictions, explicitly acknowledging the limitations of our models, and being open to being surprised.</p><p> Considerations like these arose in the recent XPT superforecaster elicitation. For examples of considerations that we would place under this umbrella, we would include <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;Given the extreme uncertainty in the field and lack of real experts, we should put less weight on those who argue for AGI happening sooner.&quot; (XPT superforecaster team 342)</li><li> &quot;Maybe most of the updates during the tournament were instances of the blind leading the blind.&quot; (Peter McCluskey, XPT superforecaster)</li></ul><h2> Accuracy of transformative technology prediction</h2><p> This considers the historical base rate of similar technologies being transformative and notes that predictions often overestimate impact.  It is important to consider the historical base rate of a technology being economically or socially transformative.</p><p> This is due to a number of factors such as under/overoptimism, a lack of understanding of the technology or its limitations, or a failure to consider the societal and economic factors that can limit its adoption.</p><p> By taking into account the historical base rate of similar technologies, we can gain a more accurate perspective on the potential impact of AI. We see similar arguments made by superforecasters, such as <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;The history of AI is littered with periods of rapid progress followed by plateaus and backtracking. I expect history will repeat itself in this decade.&quot; (XPT superforecaster team 339)</li><li> &quot;The prediction track record of AI experts and enthusiasts have erred on the side of extreme optimism and should be taken with a grain of salt, as should all expert forecasts.&quot; (XPT superforecaster team 340)</li><li> &quot;Many superforecasters suspected that recent progress in AI was the same kind of hype that led to prior disappointments with AI...&quot; (Peter McCluskey, XPT superforecaster)</li><li> &quot;AGI predictions have been made for decades with limited accuracy. I don&#39;t expect the pattern to change soon.&quot; (XPT superforecaster team 337)</li></ul><h1>结论</h1><p>In this article we have led you through an example application of a model-based approach applied to estimating the existential risks from future AI. Model-based approaches have many advantages for improving our understanding of the risks, estimating the value of mitigation policies, and fostering communication between advocates on different sides of AI risk arguments.</p><p> During our research we identified many challenges for model-based approaches that are unique to or accentuated in the AI existential risk domain compared to most other decision areas.</p><p> We focused on incorporating elements of all of these challenges, in simple ways, into our model as a way of creating a starting point. The model is certainly not a definitive model of AI x-risk, but we instead hope it might serve as an inspirational starting point for others in the AI safety community to pursue model-based approaches. We&#39;ve posted our model online in open-source tradition to encourage you to learn from it, borrow from it, and improve on it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk<guid ispermalink="false"> sGkRDrpphsu6Jhega</guid><dc:creator><![CDATA[Sammy Martin]]></dc:creator><pubDate> Fri, 25 Aug 2023 10:32:16 GMT</pubDate> </item><item><title><![CDATA[What AI Posts Do You Want Distilled?]]></title><description><![CDATA[Published on August 25, 2023 9:01 AM GMT<br/><br/><p>我想提炼人工智能安全的帖子和论文，并且我希望看到更多的提炼。理想情况下，帖子和论文应满足以下标准：</p><ul><li>可能对更多人了解产生巨大影响</li><li>使用大量行话或通常复杂且难以理解</li><li>没有你想象的那么出名（在 AI X 风险领域）</li></ul><p>哪些职位符合这些标准？</p><br/><br/> <a href="https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled<guid ispermalink="false"> wdvGgEGMohpZcgASP</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Fri, 25 Aug 2023 09:01:27 GMT</pubDate> </item><item><title><![CDATA[2084]]></title><description><![CDATA[Published on August 25, 2023 7:42 AM GMT<br/><br/><p>在一个装饰着最新苹果产品的房间里，市民们聚集在一起，他们的身体上装饰着电极，这些电极是对抗潜意识偏见的沉默守护者。今天的集会是例行集会，名为“文化欣赏与语言和谐”（CALH），是追求进步的团结仪式。</p><p>屏幕上闪现出思想封闭的反动派的形象：种族主义者、性别歧视者、那些坚持过时言辞和思想的人。一个稳定而有力的声音开始说话，记录了罪行并澄清了不断演变的正派语言。电极发出令人放心的蜂鸣声，不断提醒人们认可多样性和宽容的优点。</p><p>房间里充满了和谐的嗡嗡声，逐渐演变成团结的口号。每个人的脸上都洋溢着义愤，为了共同的目标而握紧拳头。房间里充满了集体情感，每个人都融合成一股对抗封闭思想的独特力量。</p><p>屏幕上的图像在过时的旗帜和曾经著名的领导人之间闪烁，现在被标记为不宽容的象征。诸如“所有人的生命都很重要”和“传统婚姻”之类的短语被展示出来，被标记为偏执过去的回声。口号变成了沙哑的咆哮，伴随着“公平！”的呼喊。和“包容！”响彻整个房间。电极发出的柔和的校正电击强调了大会热情的进步交响乐，确保了思想的统一。向参与者展示了新取消的 YouTube 用户和取消平台的作者。人群的反应是厌恶和正义的结合，一个单一的实体陷入了对不断变化的道德准则的不懈追求中。</p><p>然后，随着精心设计的渐强，一切都结束了。设备变暗；电极被移除。房间里现在一片寂静，感觉更冷、更空旷。公民们四散而去，他们的共同经历暂时让他们变得坚强，但党的不懈警惕却标志着这一点。</p><p>在一个必须温和地围堵有害言论的世界里，CALH 是进步的灯塔，是在党的注视下的统一仪式，党关心引导每一个思想、每一句话，走向我们的乌托邦未来。</p><br/><br/> <a href="https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084<guid ispermalink="false"> QxxA7ucNuSvh6kwK6</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:42:13 GMT</pubDate> </item><item><title><![CDATA[Apply for the 2023 Developmental Interpretability Conference!]]></title><description><![CDATA[Published on August 25, 2023 7:12 AM GMT<br/><br/><p><strong>内容</strong>：推进 DevInterp 研究计划的会议</p><p><strong>时间</strong>：2023年11月5日至12日</p><p><strong>地点</strong>：牛津威瑟姆修道院</p><p><strong>如何：</strong><a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>立即申请！</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QpFiEbqMdhaLBPb7X/a5oea02gswfpe8gfmcoj"><br></p><p> We are pleased to announce the upcoming <u>Developmental Interpretability Conference</u> , hosted at the historic <a href="https://www.wythamabbey.org/"><u>Wytham Abbey</u></a> in Oxford from 5 to 12 November.本次会议扩展了<a href="https://devinterp.com/2023/june-summit"><u>2023 年单一学习理论与联盟峰会</u></a>，并提供了一个合作解决<a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>DevInterp 研究议程</u></a>中的开放问题的机会。会议议程将回顾单一学习理论和 DevInterp 的基础知识，并将讨论发展可解释性的最新进展。</p><p><a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>点击此处申请！</u></a>会议空间有限，因此请务必尽早申请，因为当所有席位均已填满时，申请可能会关闭。我们希望今年十一月在牛津见到您！</p><h2><strong>常问问题</strong></h2><p><strong>有什么先决条件？</strong></p><p>会议将利用代数几何、贝叶斯统计和物理学的思想来理解机器学习和人工智能对齐。尽管有帮助，但不一定要掌握所有这些主题才能有效地参与会议。</p><p>为了充分利用会议议程，我们强烈建议与会者查看 SLT 介绍材料，例如 Liam Carroll 的<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC"><u>Distilling Singular Learning Theory</u></a> 。参与者还可以通过观看<a href="https://www.youtube.com/@SLTSummit/playlists"><u>2023 年单一学习理论与联盟峰会的几场讲座而</u></a>受益。</p><p><strong>我对人工智能对齐的一些论点持怀疑态度。我需要购买 AI X-risk 才能参加本次会议吗？</strong></p><p>我们相信超级人工智能的发展给人类带来了严重的风险，DevInterp 议程旨在解决这个问题。然而，虽然在人工智能协调方面取得进展是我们科学议程背后的动机，但 SLT 和发展可解释性引起了广泛的兴趣，我们邀请那些希望了解更多或根据自己的条件做出贡献的人参加。</p><p><strong>我是否需要参加 SLT 和 Alignment 2023 年夏季峰会才能参加本次 DevInterp 会议？</strong></p><p>不需要，您无需参加 SLT 和 Alignment 2023 年夏季峰会即可参加 DevInterp 会议。</p><p><strong>我需要付费才能参加会议吗？那么住宿、食物和交通费用又如何呢？</strong></p><p>会议免费参加。住宿、食物以及牛津与会场之间的交通均由 Wytham Abbey 提供。不支付前​​往牛津的旅费。</p><p><strong>你们会提供旅行支持吗？</strong></p><p>我们可以提供的旅行支持金额待定。如果您因旅行费用而无法参加，请在申请表中告知我们，我们将看看我们能做些什么。</p><p><strong>我该如何申请？</strong></p><p>通过填写<a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>此申请表</u></a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference<guid ispermalink="false"> QpFiEbqMdhaLBPb7X</guid><dc:creator><![CDATA[Stan van Wingerden]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:12:36 GMT</pubDate> </item><item><title><![CDATA[Nuclear consciousness]]></title><description><![CDATA[Published on August 25, 2023 1:28 AM GMT<br/><br/><p>马克·吐温<i>在《亚瑟王宫廷中的康涅狄格洋基队》中</i>的一段话自从我大约 35 年前读到以来一直萦绕在我的脑海中：</p><blockquote><p>对我来说，在这趟沉重而悲伤的朝圣之旅中，在这永恒之间的可悲漂流中，我所想的就是向外看，谦卑地过一种纯洁、高尚、无可指摘的生活，并保存我体内那一个真正属于我的微观原子：其余的人可能会降落在阴间，并欢迎我所关心的一切。</p></blockquote><p>那“一个原子才是真正的我”！</p><p>我认识到我身体的某些部分我不会认为是“我”，例如我的头发或指甲，甚至是我四肢的大部分（如果我不得不失去它们的话）。但如果我失去大脑的大部分，我的自我就会大大削弱——这肯定需要的不仅仅是一个原子。</p><p>这就是为什么精神疾病如此令人痛苦，包括伴随衰老而来的损害，即使不是由疾病引起的。如果<a href="https://plato.stanford.edu/entries/locke-personal-identity/">个人身份是由心理连续性来定义的</a>（也就是说，“我是昨天的我，因为我记得昨天是那个人”），那么单纯的遗忘是一种逐渐失去自我的方式。我想起了<i>2001 年</i>HAL 的拆卸，他的认知功能被一次一个地移除：“戴夫，我的思维在消失。我能感觉到它。毫无疑问……我很害怕，戴夫。”</p><p>大多数人，包括我自己，在谈论自我复合性时的方式并不一致。我们知道我们的大脑是由各个部分组成的，并且<a href="https://www.psychologytoday.com/us/blog/erasing-stigma/202001/the-neuroscience-behavior-five-famous-cases">听说过一些临床案例，</a>其中一个或另一个部分被切割或移除，然后那个人忘记了单词的含义但仍然可以拼写，或者失去了创造新记忆的能力但保留了旧的记忆我们<a href="https://doi.org/10.1007%2Fs11065-020-09439-3">听说过这样的情况</a>：人们有两个功能性的大脑半球，但彼此之间不进行交流——其中一半实际上不知道另一半在做什么。<i>然而，</i>尽管有了这些知识，我们仍然把自己称为原子存在——原子是希腊语中“不可分割”（ἄτομος）的意思，也是指一个存在与另一个存在绝对不同的存在。</p><p>许多重要的概念都基于“我们是个体”这一观念。在伦理学中，我们希望能够说：“亚历克斯是凶手！”这意味着整个亚历克斯都犯下了谋杀罪，而不仅仅是亚历克斯的一半大脑，都应该为此行为受到惩罚。 （我们把他剩下的部分放在哪里？）</p><p>或者在较小的范围内，只是为了能够说“我喜欢辛辣的食物”。也许我的某些部分不喜欢辛辣的食物，而那些能控制的部分喜欢用香料的痛苦来折磨其他部分——它们似乎确实在挣扎和扭动，这就是乐趣的一部分。</p><p>此外，人们很常见（几乎是本能）区分“真实的自我”和外部影响，尤其是在<a href="https://doi.org/10.1177/0146167213508791">道德上对它们进行区分</a>。人类动物有各种各样的欲望，但是欺骗配偶或吃掉所有饼干的欲望被认为是这个人正在努力反对的力量（除非你已经认为他是一个恶棍），而拯救溺水的小狗的欲望则被认为是力量他的真正本质（即使你认为他<i>主要</i>是一个恶棍）。</p><p>犹太教和基督教通常从这样的假设开始：意识是原子的，称为灵魂。不仅需要对灵魂的善良进行核算，而且​​整个灵魂要么得救，要么被诅咒。即使我们无视那些火与硫磺的传教士，他们认为天堂和地狱是物理场所，而诅咒是发生<i>在</i>你身上的事情，而不是你对自己做的事情，整个灵魂要么走向好，要么走向坏的假设结局似乎不可避免。</p><p>这是我非常喜欢的关于地狱的描述，摘自CS刘易斯的<i>《大离婚》</i> ，因为它符合活生生的人类心理。然而，它仍然假定意识是统一的：</p><blockquote><p> “没错。他们上去从一扇窗户往里看。拿破仑就在那里。”</p><p> “他在做什么？”</p><p> “走来走去——一直走来走去——左右，左右——一刻也没有停下来。两个小伙子观察了他大约一年，他从未休息过。而且一直自言自语。” “这是苏尔特的错。这是内伊的错。这是约瑟芬的错。这是俄罗斯人的错。这是英国人的错。”一直都是这样。一刻也没有停止过。一个小胖子，看上去有点累。但他似乎无法停止。”</p></blockquote><p>整个拿破仑都堕落到这个小螺旋中，而不仅仅是他的一部分。</p><p>还有其他意见。摩尼教始于两个基本原则/神的观念，一个是好的（“伟大之父”），一个是坏的（“黑暗之王”）。我们所知道的物质世界是它们混合在一起的结果，而一个好人在地球上应该做的工作就是把它们分开，把坏的从它们身上去掉，让善良自由地飞走。摩尼教徒并不期望最终会进入天堂或地狱：他们期望自己的一部分最终会进入天堂或地狱。</p><p>摩尼教深受佛教影响（它是佛教、基督教和琐罗亚斯德教的有意结合），而佛教以反对统一自我的概念而闻名。有时，佛教徒说任何“我”或“我”都是幻觉，但在更长的解释中，听起来更像是他们相信“我”或“我”是一个构造，就像一把椅子是临时的、模糊的组合。原子或密西西比河是一种说法，“水通常流到这里”，但并不对应于特定的水原子，而且河流的水流甚至河道都可以改变。</p><p>佛教徒将众生描述为五种“蕴、堆、集、群”（梵文<a href="https://en.wikipedia.org/wiki/Skandha">स्कन्ध skandha</a> ），即色（ <a href="https://en.wikipedia.org/wiki/R%C5%ABpa">रूप rūpa</a> ）、受（ <a href="https://en.wikipedia.org/wiki/Vedan%C4%81">वेदना vedanā</a> ）、想（ <a href="https://en.wikipedia.org/wiki/Samjna_(concept)">संज्ञा saṃjñā</a> ）、意行。 （ <a href="https://en.wikipedia.org/wiki/Sa%E1%B9%85kh%C4%81ra">संस्कार saṃskāra</a> ）和意识（ <a href="https://en.wikipedia.org/wiki/Vij%C3%B1%C4%81na">विज्ञान vijñāna</a> ）。即使这样分解，我发现这些词在英语中仍然是模糊的概念，但我确信它们在原始语言中是精确的、技术性的词汇。如果拿走这些堆中的任何一个，就不再有人/众生，但加在一起，人就存在了。在佛教与西方的<a href="https://www.webpages.uidaho.edu/ngier/307/milina.htm">早期接触</a>中，那先谈到自己就像一辆战车一样可以分解：战车不是它的轮子，不是它的车轴，不是它的座位，当你把这些部件移走时，就不再有战车了。 （鉴于此，我不会说战车是一种“幻觉”，但这是一种说话方式。）</p><p>以类似但不完全相同的方式，我们可以说一个人是海马体、杏仁核、前额皮质等的总和。 HAL 是他的（可移除的）认知模块的总和。</p><p>您可能已经注意到，我得出的结论是意识是复合的（在这个网站上，我希望大多数读者都同意），但其后果难以接受。也许摆脱责备的概念并不太令人不安：“亚历克斯是凶手！让我们惩罚亚历克斯！”是一种报应性的正义概念，无论如何，我们中的许多人更喜欢恢复性正义：“出了问题，亚历克斯犯了谋杀罪。让我们修复亚历克斯！”但如果没有责备，也就没有赞扬。当我将这种思维应用到自己身上时，我就变成了第三人称，因为“我想要鼓励的自己的部分”被描述得比“我”更远。</p><p>如果我吃辛辣的食物，因为它会伤害我的嘴，而我喜欢吃辛辣的食物，那又怎么样呢？如果这种相互作用发生在两个人体之间——一个人类动物故意给另一个人类动物造成痛苦——那么这将是一个糟糕的情况，这种情况可能需要一些恢复性（如果不是报应性）正义。将意识原子分裂成意识位至少开启了一种可能性，即不同头脑中的意识位可以以与一个头脑中的意识可分解相同的方式组合。</p><p>在过去的几年里，我一直在想，原子意识的概念是否需要被另一个物理隐喻所取代：<strong>核意识</strong>。核力量杂乱而复杂，但射程短。在质子或中子内，三个（价）夸克通过交换胶子而相互吸引，当胶子飞行时，它们会产生新的夸克-反夸克对，这些夸克-反夸克对本身会与更多的胶子相互吸引。在一篇<a href="https://www.fnal.gov/pub/today/archive/archive_2014/today14-01-31.html">热门物理文章</a>中，我曾经这样画过： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/ivrivbw3eoaay4biqhwj"></figure><p>单个质子和中子通过这种相互作用的较弱的边缘场效应在原子核中相互吸引。胶子很少偏离产生它们的夸克，原子核中的质子和中子比原子核内的夸克相距更远。 （对于其他书呆子：而电场会像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>，胶子场像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="e^{-r/r_0}/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减。）不同原子中粒子之间的核力非常小，但并不严格为零。</p><p>质子中的夸克、原子核中的质子以及不同原子的原子核之间的边界并不是绝对不同的事物，而是在数量上以很大的幅度很好地分开。为了证明它们可以混合这一事实，如果两个原子核碰撞得足够猛烈，则两个完整原子核的所有夸克和胶子都会变成一种汤。这是我在<a href="http://coffeeshopphysics.com/cmsresults/#2012-05-11">另一篇文章</a>中绘制该过程的方式： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/lgg9wv7qc9dn4eildo0m"></figure><p>在宇宙的早期，所有的空间都充满了均匀的夸克-胶子等离子体，直到它膨胀到足以使夸克彼此远离，并凝结成质子和中子。</p><p>现在打个比喻：也许意识的分解就像质子的分解一样。一个有思想的生物似乎是一个单一的、不可分割的单元，就像质子一样，但如果你观察它的内部，你会发现它是由各个部分组成的。佛教徒在内观禅修中看到五蕴；神经科学家观察大脑的功能。在<a href="https://www.lesswrong.com/posts/i8C9KSryDFj4EENvx/reality-and-reality-boxes">现实和现实盒子</a>中，我强调，我认为这两种有效的方法并不研究同一类事物：内省揭示了非常真实的主观现实，而科学研究揭示了非常真实的物理现实，这些都是足够不同，他们可能应该有不同的词，而不是过多地使用“现实”这个词。</p><p>除了单个质子可分解之外，多个质子也是可组合的：它们可以混合在一起形成流体。质子的非原子性是双向的，既可以低于单位水平，也可以高于单位水平。在这个意识隐喻中，我们的头脑中漂浮着潜意识的思想和情感碎片，也有在社区中流动的非意识思想和情感：一群人思考。我的大脑各部分之间通过厚厚的神经线束紧密相连，而生活在一起的人的大脑通过面部表情、群体活动和言语的联系却很弱。</p><p>这两种连接是否仅在规模上有所不同，例如质子内的直接夸克-胶子相互作用以及它们之间的边缘场？或许。这就是我一直在修改的想法，并将其称为核意识，而不是原子意识。我们确实知道灵长类动物之间存在神经效应，例如<a href="https://doi.org/10.1016/0926-6410(95)00038-0">镜像神经元</a>，而自然选择并不尊重身体之间的分界线。当我读到荣格的集体无意识概念时，我就是这样解释它的。 （有趣的是，只有中层是完全清醒的：无论是脑叶白质切除的人还是人群的行为都没有连贯性。）</p><p>我长期以来一直认为（并认为）人们<a href="https://doi.org/10.1017%2FS1478951517000621">在存在上是孤立的</a>：只要单词与不同的事物相匹配，就无法知道我所看到的红色对你来说是否是红色。就像维特根斯坦在<i>《哲学研究》</i>中的盒子里的甲虫比喻一样：</p><blockquote><p>如果我说我自己只有从我自己的经历中才知道“痛苦”这个词的含义，那么我是否也不能对其他人也这么说呢？我怎么能如此不负责任地概括这一案例呢？</p><p>现在有人告诉我，只有他自己才知道什么是痛苦！假设每个人都有一个盒子，里面装着一些东西：我们称之为“甲虫”。没有人可以看别人的盒子，每个人都说只有看他的甲虫才知道甲虫是什么。在这里，每个人的盒子里都有可能有不同的东西。人们甚至可以想象这样的事情是不断变化的。但是假设“甲虫”这个词在这些人的语言中有用吗？如果是这样，它就不会被用作事物的名称。盒子里的东西在语言游戏中根本没有地位；甚至不能作为某种东西：因为盒子甚至可能是空的。不，可以根据盒子里的东西来“划分”；不管它是什么，它都会抵消。</p><p>也就是说：如果我们根据“对象和指称”的模型来解释感觉表达的语法，那么对象就会被视为无关紧要而被排除在外。</p></blockquote><p>但我确实相信我大脑的某些部分知道我大脑的其他部分是什么样子，因为甲虫并没有对它们隐藏。现在，如果我大脑各部分之间的分离与个体之间的分离相同，也许我们确实在某种程度上看到了彼此的头骨内部。核意识原则上是对存在隔离的反驳——人与人之间只存在比人内部更大的隔离，而不是不同类型的隔离。</p><p>我想知道，如果心灵感应成为可能——通过深深连接到我们神经元或其他东西的无线电——我们是否仍然会感觉我们正在<i>互相</i>交谈，就像心灵感应故事中所描述的那样，或者我们会觉得我们<i>都是</i>一个人吗？ ，就像我大脑的各个部分相互作用一样？<i>了解</i>某人（ <a href="https://www.leaflanguages.org/french-grammar-verbs-savoir-vs-connaitre/">connaître，而非 savoir</a> ）是<i>成为</i>那个人的一小步吗？</p><p>我想从豆荚人的角度阅读<i>《掠尸者的入侵》</i>的一个版本。他们想与地球人交流，但他们有心灵感应，所以他们成为了地球人……</p><p>尽管我认为我们可以排除原子意识（对不起，马克·吐温），但我不确定它会消失多少，也不确定我们是一个、松散连接且几乎没有意识的大脑这一想法是否认真对待。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-consciousness#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-意识<guid ispermalink="false">qXZrobGdAycNDBHET</guid><dc:creator><![CDATA[Jim Pivarski]]></dc:creator><pubDate> Fri, 25 Aug 2023 01:28:04 GMT</pubDate></item></channel></rss>