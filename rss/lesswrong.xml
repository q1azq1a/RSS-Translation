<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 15 日，星期日 20:11:46 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The Hidden Perils of Hydrogen]]></title><description><![CDATA[Published on October 15, 2023 7:51 PM GMT<br/><br/><p>氢是未来的燃料。它是宇宙中最丰富的元素，具有令人难以置信的质量效率，并且可以轻松生产。它可用于大规模和小规模能源生产（分别考虑聚变和燃料电池），并且几乎没有排放、碳或其他物质。然而，只有一个问题。</p><p>它的体积效率低得荒谬。</p><p>在 STP（标准温度和压力、273K 和 1 atm）下，一升氢气可产生 0.01 MJ 的能量，而汽油的能量为 34 MJ/L。这相当于每升能量输出的 0.02%。诚然，液氢的情况有了很大改善，对比结果为 8 MJ/L 与 34 MJ/L，但这需要将温度维持在 -252.8°C 以下，仅比绝对零高几度。</p><p>气态氢也不是那么容易储存：它需要可加压至 700 巴（700 倍大气压）的容器。那是 10,000 PSI。即便如此，也需要 5 升氢气才能匹配 1 升汽油。美国能源部估计，为了满足大多数轻型车辆的行驶里程，车辆上需要携带 5-13 公斤的氢气。如果我们快速计算一下，这意味着车辆需要携带 85 - 294 升液态氢（气态氢的几倍）才能到达任何地方。对于所有美国人来说，这大约是 22-77 加仑。</p><p> （供参考，就能源效率而言，5-13公斤氢气转化为20-70升汽油。我也不对数字的具体准确性提出任何要求，这些都是餐巾纸计算的，并且没有考虑到考虑通过回收氢或燃料电池产生的能源效率比内燃机更高效等）</p><p>在仅使用氢燃料的世界中，拥有五十加仑油箱的传统轿车是不可行的。此外，极端条件下的液态氢必须储存，这成为其生产和运输的瓶颈。</p><p>能源部制定了到 2020 年要达到的氢存储标准，以便氢燃料可用于便携式电源和轻型车辆应用。这些曾经是：</p><ul><li> 1.5 kWh/kg（系统整体性能）</li><li> 1.0 kWh/L（系统整体性能）</li><li> 10 美元/kWh（相当于储存氢气 333 美元/kg）</li></ul><p>到目前为止，这些目标都没有实现，我对当前研究的可扩展性表示怀疑（尽管我愿意接受对此的批评）。让我们看看正在采取哪些措施来解决该问题。</p><h2>短期解决方案</h2><p>氢作为一种广泛使用的燃料的问题的大多数短期解决方案包括为氢气创建廉价的高压存储解决方案。再次来自能源部的网站，这意味着开发可以廉价生产并承受 700 巴压力的纤维增强复合材料。由于这些不一定能解决潜在的体积效率低下问题，因此我们可以转向长期解决方案。</p><h2>长期解决方案</h2><p>长期解决方案有两种形式：<strong>更高密度的气态存储</strong>和<strong>基于材料的氢存储</strong>。前者开发可以更多地压缩氢气的容器，而后者则寻求制造具有更好体积氢比的材料。这将主要关注基于材料的方法。</p><p>材料储氢的主要研究方向是金属氢化物、吸附剂和化学储氢材料。让我们逐一看看。</p><h3>金属氢化物</h3><p>金属氢化物是金属原子与氢形成配体的化合物。这些键的强度和性质因金属而异，但它们允许化合物充当氢载体而不会自行分解，这对于材料循环很有用（并大大提高效率）。</p><p>问题是，具有最有前途的性能的复杂氢化物无法以任何数量廉价地大规模生产，更不用说氢气成为汽油的有力竞争对手所需的数量了。然而，如果这些化合物能够规模化，那么我将对我们的能源未来感到非常兴奋。</p><h3>吸附剂</h3><p>吸附是分子粘附到物体内部或外部表面的过程。将氢气附着到某些化合物上将使其保留其原始分子形式并同时将其压缩（由于气体的体积效率相当低）。这些的问题在于它们不会比原始版本压缩那么多，而且制造成本昂贵。</p><h3>化学储氢材料</h3><p>这些可能是最直接的：找到可以水解或热解以释放氢气的材料，并希望它们的体积比液氢小并且更容易存储。好吧，你瞧，事实证明有一整类这样的分子：胺硼烷。</p><p>胺硼烷本质上是与硼络合的氨分子，并添加了额外的氢。最简单的胺硼烷是氨硼烷或硼氮烷，化学式为 NH3BH3。它的水解和热解性能良好，氢的摩尔密度比氢本身更高，在室温下是稳定的固体。你还能要求什么呢？</p><p>嗯，合成成本高得离谱。我怀疑成本可以大规模降低，但目前看来不太可行。</p><br/><br/> <a href="https://www.lesswrong.com/posts/7ygt5iujpruQnNLnc/the-hidden-perils-of-hydrogen#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7ygt5iujpruQnNLnc/the-hidden-perils-of-Hydrogen<guid ispermalink="false"> 7ygt5iujpruQnNLnc</guid><dc:creator><![CDATA[Yudhister Kumar]]></dc:creator><pubDate> Sun, 15 Oct 2023 19:51:07 GMT</pubDate> </item><item><title><![CDATA[ Arguments for radical optimism on AI safety]]></title><description><![CDATA[Published on October 15, 2023 2:51 PM GMT<br/><br/><p>或者，为什么我们基本上不需要担心人工智能。</p><p>因此，这篇文章部分是对阿玛尔西娅关于我如何简单地声称我的立场是正确的评论的回应，我的回应是说我要发表简短的评论，而不是必须对这个问题发表另一篇很长的评论。</p><p> <a href="https://www.lesswrong.com/posts/aW288uWABwTruBmgF/?commentId=r7s9JwqP5gt4sg4HZ#r7s9JwqP5gt4sg4HZ">https://www.lesswrong.com/posts/aW288uWABwTruBmgF/?commentId=r7s9JwqP5gt4sg4HZ#r7s9JwqP5gt4sg4HZ</a></p><p>在这篇文章中，我不会试图声称我的观点是正确的，而是提供证据，以便我可以在这里正确地收集我的想法。这将是一篇链接较多的帖子，我将引用很多概念和对话，因此如果您对这些想法有一些简单的背景知识将会有所帮助，但我会尽力让外行/非技术人员可以理解所有内容人。</p><p>这将是一篇很长的文章，所以喝点饮料和吃点零食吧。</p><h1>急速左转可能不会发生，因为人工智能训练与进化有很大不同</h1><p>Nate Soares 认为，人工智能安全中的一个关键问题是急速左转，而急速左转本质上是能力泛化远远超过目标，即它基本上是目标误泛化加上快速起飞：</p><blockquote><p>我对人工智能进展的猜测是，在某个时刻，一些团队得到的人工智能开始足够好地泛化，远远超出其训练分布，它可以掌握物理、生物工程和心理学等领域，达到很高的水平。其程度足以单枪匹马地威胁整个世界。也许不需要对其最熟练的技能进行明确的训练，就像人类在成功登月之前需要好几代人杀死最不成功的火箭工程师来完善我们的大脑以进行火箭工程一样。</p></blockquote><blockquote><p>在它的能力飞跃发展的同时，它的对齐属性也被揭示为肤浅的，并且无法概括。这里的核心类比是，优化猿类以实现包容性遗传适应性 (IGF) 并不会使人类在精神上针对 IGF 进行优化。当然，类人猿吃东西是因为它们有饥饿本能，做爱是因为感觉良好，但它们不可能因为这些活动如何导致更多 IGF 而吃东西/通奸。他们还无法执行抽象推理来正确地根据 IGF 来证明这些行为的合理性。然后，当它们开始以人类的方式很好地概括时，可以预见的是，它们不会因为 IGF 的抽象推理而突然开始进食/通奸，尽管它们现在可以。相反，他们发明了避孕套，如果你试图消除他们对美食的享受，他们就会与你战斗（告诉他们只需手动计算 IGF）。在功能开始泛化之前您所称赞的对齐属性，可以预见的是无法与功能一起泛化。</p></blockquote><p>所以本质上类似于AI在训练数据中对齐，但在测试集中，由于对齐方法的限制，无法推广到测试集。</p><p>问题是：我们实际上知道为什么会发生急左转，而导致人类急左转的情况不会在人工智能训练和人工智能进步中再次出现。</p><p>基本上，发生了急剧的左转，因为进化的外部优化器比人类终身学习等内部搜索过程的强大程度低数十亿倍，而像我们人类这样的内部学习者基本上在一步后就死了，或者最多2-3步外部优化器。与文化进化相比，进化大多无法通过其工具将许多比特从一代传输到下一代，并且它们在特定时间尺度上传输比特的能力之间的差异是巨大的。</p><p>一旦我们有能力通过文化传递一些信息，这意味着，鉴于我们有能力更有效地优化数十亿倍，我们基本上可以经历一个能力飙升的急转左转。但发生这种情况的唯一原因是引用昆汀·波普的话：</p><blockquote><p>一旦内部学习过程变得足够有能力将他们的知识传递给他们的继任者，你就会看到一个看起来像急左转的情况。但这种急速的左转只会发生，因为内部学习者找到了一个笨拙的解决方法，克服了严重的缺陷，即它们在初始化后不久就被删除了。</p></blockquote><p>对于使用 SGD 训练的 AI 来说，这种情况并不存在，并且外部优化器 SGD 和内部优化器之间的差距要小得多，差异约为 0-40 倍。</p><p>下面是它的来源，我将明确引用它：</p><p> <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn#Don_t_misgeneralize_from_evolution_to_AI">https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn#Don_t_misgeneralize_from_evolution_to_AI</a></p><blockquote><p>另请参阅：与模型无关的元学习提出了一种双层优化过程，在内循环中使用了 10 到 40 倍的计算量，仅用于快速学习或特征重用？以表明他们可以获得相同的性能，同时从内部循环中删除几乎所有计算，甚至完全摆脱内部循环。</p></blockquote><p>此外，我们可以将外部优化步骤与内部优化步骤的比率设置为基本上我们想要的任何值，这意味着我们可以比进化更好地控制内部学习器的学习速率，这意味着我们可以防止急速左转的发生。</p><p>我对扬·库勒维特的一个症结是，就动物确实有文化而言，它比人类文化要有限得多，而且进化在很大程度上没有能力以非文化方式传递特征，非常关键的是，这是一个——时间效率低下，没有理由假设导致急速左转的第二个严重效率低下的原因：</p><p> X4vier 和特别说明了这一点，我将在下面展示：</p><p> <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/?commentId=qYFkt2JRv3WzAXsHL">https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/?commentId=qYFkt2JRv3WzAXsHL</a></p><p> <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/?commentId=vETS4TqDPMqZD2LAN">https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/?commentId=vETS4TqDPMqZD2LAN</a></p><h1>我不认为内特的例子实际上表明了我们所担心的错误概括</h1><p>这是因为所谓的错误概括并不是这样一种情况：1 个人工智能在某个环境中接受训练并最大化了 IGF 的相关性，然后在新的环境中它遇到了改变目标的输入，因此它现在错误地概括了不再追求 IGF 的目标。</p><p>发生的事情是，进化在一种环境中训练人类以优化 IGF 的相关性，然后基本上在另一种环境中训练新人类，然后他们就出现了分歧。</p><p>非常关键的是，有数千个不同的系统/人类在截然不同的环境中接受训练，而不是像现代人工智能训练那样在不同的环境中训练一个人工智能，所以这不是一个错误概括的有效例子。</p><p>昆汀·波普 (Quintin Pope) 的一些帖子和引言会有所帮助：</p><blockquote><p> （第 2 部分，这对于进化的类比有何影响）许多最基本的对齐问题都是关于人工智能如何从训练数据中进行概括。例如，“如果我们训练人工智能在我们可以提供监督的情况下表现良好，那么它会在我们无法提供监督的情况下继续表现良好吗？”</p></blockquote><blockquote><p>当人们试图利用人类进化史来预测人工智能的泛化时，他们经常会提出这样的论点：“在祖先的环境中，进化训练人类做X，但在现代环境中，他们却做Y”。然后，他们尝试通过指出 X 和 Y 的差异来推断有关人工智能概括的信息。</p></blockquote><blockquote><p>然而，这样的论点犯了一个严重的错误：进化优化了人类基因组，而人类基因组是人类学习过程的顶层。进化对中层应用很少的直接优化能力。例如，进化不会将一代人学到的技能、知识、价值观或行为转移给他们的后代。后代必须从环境中存在的信息（可能包括上一代的演示和指示）中重新学习这些东西。</p></blockquote><blockquote><p>这种区别很重要，因为接受环境数据训练的学习系统的全部目的是将有用的信息和行为模式插入到中层内容中。但这（大多数）不会在进化过程中发生，因此从祖先环境到现代环境的转变并不是学习系统从其训练数据中进行概括的例子。这不是一个例子：</p></blockquote><blockquote><p>我们在环境 A 中训练系统。然后，经过训练的系统处理来自环境 B 的不同输入分布，现在系统的行为有所不同。</p></blockquote><blockquote><p>这是一个例子：</p></blockquote><blockquote><p>我们在环境 A 中训练了一个系统。然后，我们在环境 B 的不同输入分布上训练了同一系统的新版本，现在这两个不同的系统表现不同。</p></blockquote><blockquote><p>这些是完全不同类型的转变，试图从第二种转变的实例（人类在祖先环境中与现代环境中）推理到第一种转变的实例（未来人工智能的训练与部署），将非常困难。很容易让你误入歧途。</p></blockquote><blockquote><p>与使用来自两个不同分布的数据进行评估的单个系统相比，使用来自两个不同分布的数据进行训练的两个不同的学习系统通常在其行为之间具有更大的差异。因此，将我们的进化历史视为人类的“训练”将导致对人工智能从其训练数据中得出的概括的稳定性和可预测性过于悲观的期望。</p></blockquote><blockquote><p>从人类进化史中汲取有关人工智能的正确教训需要跟踪进化如何影响人类学习过程的不同层面。我通常发现，这种修正后的进化类比所带来的影响远不如未修正的进化类比有趣或令人担忧。例如，这里有两种思考人类如何喜欢冰淇淋的方法：</p></blockquote><blockquote><p>如果我们假设人类在祖先环境中被“训练”以追求瞪羚肉等，然后“部署”到我们追求冰淇淋的现代环境中，那么这就是一个例子，训练中的行为完全无法预测部署。</p></blockquote><blockquote><p>如果实际上有两组不同的训练“跑步”，一组在祖先环境中训练，人类因追逐瞪羚而获得奖励，一组在现代环境中训练，人类因追求冰淇淋而获得奖励，那么事实后一组的人类倾向于喜欢冰淇淋一点也不奇怪。</p></blockquote><blockquote><p>特别是，从一致性的角度来看，这个结果并没有告诉我们任何新的或令人担忧的事情。适用于单一训练过程的唯一教训是，如果你奖励学习者做某事，他们将来会倾向于做类似的事情，这几乎是对奖励作用的普遍理解。</p></blockquote><p>昆廷（Quintin）对为什么人类实际上并没有误认为喜欢冰淇淋的评论：</p><p> <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/?commentId=sYA9PLztwiTWY939B">https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/?commentId=sYA9PLztwiTWY939B</a></p><h1>人工智能是白盒子，而我们是天生的奖励系统</h1><p>上述关于为什么“急速左转”可能不会在现代人工智能发展中再次出现，以及为什么人类没有错误概括的说法足以让我们摆脱像埃利泽·尤德科斯基这样最悲观的声音，特别是消除假设极端错误概括的理由使我们脱离了 MIRI 领域的观点，并且可以说超出了 50% p(doom) 的范围。但我想说的是，厄运的可能性远低于这个数字，低到我们基本上不应该担心人工智能，因此我必须提供一个积极的故事来说明为什么人工智能很可能是一致的，我认为在这种情况下，人工智能是白盒，而我们是天生的奖励系统。</p><p>我们相对于进化的关键优势在于，与研究大脑不同，我们可以对其内部结构进行完全读写访问，并且它们本质上是一种特殊类型的计算机程序，并且我们已经有了方法来操纵计算机程序，而无需任何成本我们。事实上，这就是 SGD 和反向传播能够优化 SGD 的原因。如果人工智能是一个黑匣子，SGD 和反向传播就不起作用。</p><p>与生俱来的奖励系统通过白盒方法使我们保持一致，奖励系统印在我们身上的价值观是极其可靠的，几乎每个人都对朋友和熟人有同理心，父母的本能，复仇等。</p><p>这显示在下面的链接中：</p><p> <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#White_box_alignment_in_nature">https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#White_box_alignment_in_nature</a></p><p> （在这里，我们必须绕道说一下，我们的奖励制度非常擅长让我们适应生存，而现代世界中的肥胖等缺陷通常是令人惊讶的轻微失败，因为人类没有能力像人类那样做事。我们认为，这可以说意味着实践中的对齐失败看起来更像是能力失败，并将类比回到人工智能案例，我基本上不期望 X 风险、GCR 或任何比说的更严重的东西例如，人工智能弄乱了厨房。）</p><p>史蒂文·伯恩斯（Steven Byrnes）担心，如果您不知道如何进行操纵，那么您确实需要付出代价才能获得知识。</p><p>史蒂文·伯恩斯的评论链接如下： <a href="https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/?commentId=3xxsumjgHWoJqSzqw">https</a> ://forum. effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/?commentId=3xxsumjgHWoJqSzqw</p><p> Nora Belrose 回应了白盒的含义，以及人们如何使用 SGD 来自动化搜索，以便总体意义上的操纵成本尽可能低：</p><p> <a href="https://twitter.com/norabelrose/status/1709603325078102394">https://twitter.com/norabelrose/status/1709603325078102394</a></p><blockquote><p>我的意思是在计算机安全意义上，它指的是程序源代码的可观察性（Nora Belrose）</p></blockquote><p> <a href="https://twitter.com/norabelrose/status/1709606248314998835">https://twitter.com/norabelrose/status/1709606248314998835</a></p><blockquote><p>通过利用神经网络的差异性，使用 SGD 来定位最能改善对齐的 NN 权重操作，我们可以比 IDA Pro 和 Ghidra 做得更好</p></blockquote><blockquote><p>如果我们没有 SGD，而只是在 sim 或 smth 中发展 AGI，我会更加担心 (Nora Belrose)</p></blockquote><p> <a href="https://twitter.com/norabelrose/status/1709601025286635762">https://twitter.com/norabelrose/status/1709601025286635762</a></p><blockquote><p>我要指出的是，从字面意义上来说，它是一个白盒子，你可以观察和操纵里面发生的一切，这是一个远非微不足道的事实，因为你不能用我们像人类一样常规对齐的其他系统来做到这一点或动物。 （诺拉·贝尔罗斯饰演）</p></blockquote><p> <a href="https://twitter.com/norabelrose/status/1709603731413901382">https://twitter.com/norabelrose/status/1709603731413901382</a></p><blockquote><p>不，我不同意这是一种削弱。从字面意义上讲，分析和操作神经网络的成本是零。提出实现某些目标的手动操作的成本可能大于零。但这就是为什么我们使用 SGD 自动搜索操作 (Nora Belrose)</p></blockquote><p>史蒂文·伯恩斯 (Steven Byrnes) 认为这可能是由于定义不同造成的：</p><p> <a href="https://twitter.com/steve47285/status/1709655473941631430">https://twitter.com/steve47285/status/1709655473941631430</a></p><blockquote><p>我认为这是一个黑盒子，前面板上有一个按钮，上面写着“SGD”。我们可以整天谈论按下 SGD 按钮可以做的所有很酷的事情。但从隐喻上来说，它仍然是盒子外面的一个按钮。</p></blockquote><blockquote><p>对我来说，“白盒”意味着：如果LLM输出A而不是B，而你问我为什么，那么我总是可以给你一个合理的答案。我声称这更接近该术语在实践中的通常使用方式。</p></blockquote><blockquote><p> （是的，我知道，它实际上不是一个按钮，它是一个输入输出接口，也改变了黑匣子的内部结构。）（史蒂文·伯恩斯）</p></blockquote><p>这是响应链，这样我就可以明白为什么诺拉·贝尔罗斯和史蒂文·伯恩斯不同意。</p><p>我最终认为一个潜在的区别是，出于对齐的目的，人类与人工智能的抽象并不是一个非常有用的抽象，而 SGD 与内部优化器是更好的抽象，因此人工智能是否普遍进步并不重要，重要的是人类+ SGD 与内部优化器的具体进展很重要，因此操纵 AI 值的成本相当低。</p><p>这导致...</p><h1>我认为安全思维不适合人工智能</h1><p>总的来说，许多 LWers 的一个常见分歧是，从计算机安全领域到人工智能的知识转移非常有限，因为人工智能在方式上有很大不同，使得类比不恰当。</p><p>对于一个特定的示例，您可以随机地将训练数据或模型的大小加倍，并且通常可以正常工作。如果你试图将燃料箱的尺寸增加一倍，火箭就会爆炸。</p><p> Quintin 在下面解释了所有这些以及更多内容，但是人工智能领域和计算机安全领域之间存在一些很大的差异，以至于我认为 ML/AI 很像量子力学，我们不应该将其移植到量子力学中。来自其他领域的直觉并期望它们能够发挥作用，因为该领域的怪异之处：</p><p> <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/#Yudkowsky_mentions_the_security_mindset__">https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/#Yudkowsky_mentions_the_security_mindset__</a></p><blockquote><p>同样，我认为机器学习并不像计算机安全或火箭科学（尤德科夫斯基经常使用的另一个类比）。一些在 ML 中发生但在其他领域中不会真正发生的事情的示例：</p></blockquote><blockquote><p>默认情况下，模型是内部模块化的。交换附近变压器层的位置几乎不会导致性能下降。</p></blockquote><blockquote><p>将计算机的硬盘换成 CPU，或者将火箭的油箱换成其中一个稳定翼，充其量只会导致立即故障。类似地，交换加密协议的不同步骤通常会使其输出无意义的内容。最坏的情况是，它会带来严重的安全漏洞。例如，在对密码进行哈希处理之前添加密码盐。如果你之后再添加它们，那么盐腌几乎毫无用处。</p></blockquote><blockquote><p>我们可以通过算术编辑模型。我们可以针对多个任务单独微调一个模型，并跟踪权重如何随着每次微调而变化，以获得每个任务的“任务向量”。然后，我们可以将任务向量加在一起，形成一个同时擅长多个任务的模型，或者我们可以减去任务向量，使模型在相关任务上表现更差。</p></blockquote><blockquote><p>向火箭或密码系统随机添加/减去额外的部分是在玩最糟糕的火灾，最终会让你分别被黑客攻击或爆炸。</p></blockquote><blockquote><p>我们可以将不同的模型拼接在一起，而无需任何重新训练。</p></blockquote><blockquote><p>计算机安全的粗略等效方法是拥有两种加密算法 A 和 B，以及明文 X。然后，在将 A 应用于 X 的过程中，切换到使用 B。对于火箭技术来说，这就像建造两枚不同的火箭，然后尝试将一枚火箭的上半部分焊接到另一枚火箭的下半部分上。</p></blockquote><blockquote><p>当事情变得更大时，事情往往会变得更容易。扩展模型可以让它们学得更快，也更稳健。</p></blockquote><blockquote><p>在安全或火箭科学领域通常情况并非如此。</p></blockquote><blockquote><p>你可以随机改变你在机器学习训练中所做的事情，而且通常效果很好。例如，您可以将模型或训练数据的大小加倍，或者更改训练过程的超参数，同时进行几乎为零的其他调整，并且事情通常不会爆炸。</p></blockquote><blockquote><p>如果你尝试随机将火箭燃料箱的大小加倍，火箭实际上会爆炸。</p></blockquote><blockquote><p>我认为这种怪异不适合任何现有领域的框架/“叙述”。我认为这些结果就像量子隧道或双缝实验的怪异：表明我们正在处理一个非常奇怪的领域，我们应该对从其他领域导入直觉持怀疑态度。</p></blockquote><p>我还认为，计算机安全和对齐之间的认知差异在于计算机安全，可以轻松检查加密系统是否被破坏的基本事实，而在人工智能对齐中，我们没有能力从提议的方案中获得反馈对齐方案的破坏。</p><p>有关更多信息，请参阅 Quintin 的帖子部分，介绍人工智能安全和计算机安全在认知方面的差异，以及尝试安全突破的有效示例，其中有暗示性证据表明，随着我们增加数量，内部未对准的模型/优化守护进程会消失尺寸。</p><p> <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#True_experts_learn__and_prove_themselves__by_breaking_things">https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#True_experts_learn__and_prove_themselves__by_writing_things</a></p><p> （昆廷·波普（Quintin Pope）在“什么是尝试突破？”中谈到对齐在基本事实上没有良好的反馈循环这一事实，并且随着尺寸的扩大，声称的突破的例子实际上消失了，并注意否定性证据比尝试突破更现实。）</p><p>这就是为什么我不同意 Jeffrey Ladish 关于 Twitter 上的安全思维的观点：我认为这对于那些不具备技术知识的人来说是一个陷阱，比如很多 LWers，而且人工智能和计算机安全之间存在巨大差异，这意味着大多数尝试连接都会失败。</p><p> <a href="https://twitter.com/JeffLadish/status/1712262020438131062">https://twitter.com/JeffLadish/status/17122620204381​​31062</a></p><blockquote><p>呃，我想我希望他读得足够多，能够内化安全心态？ （杰弗里·拉迪什）</p></blockquote><p> <a href="https://twitter.com/SharmakeFarah14/status/1712264530829492518">https://twitter.com/SharmakeFarah14/status/1712264530829492518</a></p><blockquote><p>我通常倾向于认为安全思维是一个陷阱，因为机器学习/人工智能的结合与火箭工程或网络安全有很大不同。</p></blockquote><blockquote><p>有关原因的入门知识，请阅读 @QuintinPope5 的帖子部分：</p></blockquote><blockquote><p> <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#Yudkowsky_mentions_the_security_mindset__">https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#Yudkowsky_mentions_the_security_mindset__</a> （我自己）</p></blockquote><p>现在我已经试图说明为什么移植安全思维是有缺陷的，我想谈谈一类对手，例如梯度黑客或内部错位台面优化，以及为什么我认为这实际上很难对付SGD，甚至是 SGD 的非柏拉图理想版本，我们可以很容易地检测到大多数台面优化器。</p><h1>对于接受 SGD 训练的 AI 来说，内部错位（或者至少是梯度黑客）非常困难</h1><p>我将从 Evan Hubinger 的文章“内部对齐问题”中获取内部未对齐的定义：</p><p> <a href="https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem">https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem</a></p><blockquote><p>我们将台面优化器与基本目标对齐的问题称为内部对齐问题。</p></blockquote><p>错位的台面优化器很难长期存在的根本原因是，梯度下降实际上比人们想象的更强大、更白盒化，特别是它具有任何台面优化器都需要的 5 种防御措施为了使其错位而克服：</p><p><a href="https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult">https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult</a></p><blockquote><p> 1.) 一切都是同时且独立优化的。每个参数对损失的精确局部敏感度是在向后传递中计算的，并且每个参数都基于此进行更新。这意味着 mesaoptimizer 或梯度黑客（只要它以不同方式连接到网络）不可能保护其自身的任何部分免受梯度更新的影响。这包括梯度黑客的内部电路、其与网络其他部分的连接、其与网络输入数据的连接以及网络其余部分与损失之间的连接。</p></blockquote><blockquote><p> 2.) 梯度下降是最大的。每个参数都沿最速下降方向更新。这意味着，与生物生命不同，梯度黑客不能“燃烧”某些东西来增加熵，以与其他地方的熵减少进行权衡。仅允许局部最大熵增加方向。</p></blockquote><blockquote><p> 3.) 它是无国籍的。每个参数根据其对每个数据点的损失的局部敏感性进行更新。信用不会跨时间传播。通过现在增加损失但稍后减少损失或过去减少损失来与梯度下降“讨价还价”是不可能的。</p></blockquote><blockquote><p> 4.) 它只是一阶的。它仅计算参数对损失的偏导数。它没有考虑潜在的二阶相互作用。虽然在某些情况下很糟糕，但这使得梯度下降不受 mesaoptimizer 设置的“勒索”或“绊线”的影响，其中优化其电路的某些部分可能会导致作为二阶效应的巨大损失。 GD 会愉快地触发绊线，然后优化消除后果。这也不允许“交易”，梯度黑客可能会在一个因果分支上造成低损失，以换取其他分支上的高损失。相反，梯度下降将两个分支上最陡的下降方向相加。</p></blockquote><blockquote><p> 5.) 它是无穷小的。 GD仅计算每个参数的无穷小局部敏感性，而不管实践中的实际学习率或步长如何。这意味着梯度下降不会“看到”你是否站在巨大的悬崖旁边，因此也不能以这种方式敲诈。</p></blockquote><p>基本上，它将优化整个因果图，并且不留任何余地，而且作为奖励，它对台面优化器的勒索具有极强的抵抗力。总的来说，我对内部对齐持乐观态度的一个重要原因是，SGD 非常擅长信用分配，并且在台面优化器确实试图错位它的情况下，它具有相当强大的校正功能。</p><p>我们还可以检测人工智能中的大多数台面优化器，而无需机械解释，如下所示：</p><blockquote><p> While it might be possible to construct gradient hackers around these features, the advantages to us is that all of these approaches (except perhaps 2) should be visible to us even without detailed mechanistic interpretability knowledge of the network. We know a-priori where the stop-gradients or non-differentiable parts of the architecture are. We can straightforwardly observe what gradients are being clipped -- and if the gradient updates to the same set of parameters is being constantly clipped that could be a red flag. We can also find regions of high curvature in the loss landscape by just inspecting the Hessians or approximations of it. Only a gradient hacker which utilizes correlations across minibatches to cancel out its interventions in model behaviour seems difficult to detect.</p></blockquote><p> One caveat here is that the prevention of mesa-optimizers applies fully only to SSL learning on IID data, which is an unfortunate limitation, albeit I do expect SGD to still be ridiculously good at credit assignment even in the RL context.</p><p> In particular, the detection mechanisms for mesa-optimizers are intact, but we do need to worry about 1 new potential inner misalignment pathway.</p><blockquote><p> Also of importance is that this post only applies to the classic supervised/self-supervised learning on iid data which most current foundation models are trained in. The important things about this paradigm is that the outputs of the model have no causal link to the data distribution -- ie the model cannot influence what future data it will be trained on (except potentially highly indirectly influencing successor models [1]). In an RL context, or supervised learning with adaptive data sampling, where there is a link between the model&#39;s outputs and the future data distribution, then this provides another route for gradient hackers to operate -- by creating outputs which will steer the data distribution in a desired direction which will strengthen the gradient hacker.</p></blockquote><p> But there&#39;s also weak evidence that optimization daemons/demons, often called inner misaligned models, go away when you increase the dimension count:</p><p> <a href="https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#True_experts_learn__and_prove_themselves__by_breaking_things">https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky#True_experts_learn__and_prove_themselves__by_breaking_things</a></p><blockquote><p> Another poster (ironically using the handle &quot;DaemonicSigil&quot;) then found a scenario in which gradient descent does form an optimization demon. However, the scenario in question is extremely unnatural, and not at all like those found in normal deep learning practice. So no one knew whether this represented a valid &quot;proof of concept&quot; that realistic deep learning systems would develop optimization demons.</p></blockquote><blockquote><p> Roughly two and a half years later, Ulisse Mini would make DaemonicSigil&#39;s scenario a bit more like those found in deep learning by increasing the number of dimensions from 16 to 1000 (still vastly smaller than any realistic deep learning system), which produced very different results, and weakly suggested that more dimensions do reduce demon formation.</p></blockquote><p> <a href="https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect">https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect</a></p><p> <a href="https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect?commentId=hwzu5ak8REMZuBDBk">https://www.lesswrong.com/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect?commentId=hwzu5ak8REMZuBDBk</a></p><p> This was actually a crux in a discussion between me and David Xu about inner alignment, where I argued that the sharp left turn conditions don&#39;t exist in AI development, and he argued that misalignment happens when there are gaps that go uncorrected, which is likely referring to the gap between the base goal like SGD and the internal optimizer&#39;s goal that leads to inner misalignment, and I argued that inner misalignment is likely to be extremely difficult to do, due to SGD being able to correct the gap between the inner and outer mesa-optimizer in most cases, and I now showed the argument in this post:</p><p> Twitter conversation below:</p><p> <a href="https://twitter.com/davidxu90/status/1712567663401238742">https://twitter.com/davidxu90/status/1712567663401238742</a></p><blockquote><p> Speaking as someone who&#39;s read that post (alongside most of Quintin&#39;s others) and who still finds his basic argument unconvincing, I can say that my issue is that I don&#39;t buy his characterization of the doom argument—eg I disagree that there needs to be a &quot;vast gap&quot;. (David Xu)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1712568155959362014">https://twitter.com/davidxu90/status/1712568155959362014</a></p><blockquote><p> SGD is not the kind of thing where you need &quot;vast gaps&quot; between the inner and outer optimizer to get misalignment; on my model, misalignment happens whenever gaps appear that go uncorrected, since uncorrected gaps will tend to grow alongside capabilities/coherence. (David Xu)</p></blockquote><p> <a href="https://twitter.com/SharmakeFarah14/status/1712573782773108737">https://twitter.com/SharmakeFarah14/status/1712573782773108737</a></p><blockquote><p> since uncorrected gaps will tend to grow alongside capabilities/coherence.</p></blockquote><blockquote><p> This is definitely what I don&#39;t expect, and part of that is because I expect that uncorrected inner misalignment will be squashed out by SGD unless extreme things happen:</p></blockquote><blockquote><p><a href="https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult">https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult</a> (Myself)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1712575172124033352">https://twitter.com/davidxu90/status/1712575172124033352</a></p><blockquote><p> Yes, that definitely sounds cruxy—you expect SGD to contain corrective mechanisms by default, whereas I don&#39;t. This seems like a stronger claim than &quot;SGD is different from evolution&quot;, however, and I don&#39;t think I&#39;ve seen good arguments made for it. (David Xu)</p></blockquote><p> This reminds me, I should address that other conversation I had with David Xu on how strong priors do we need to encode to ensure alignment, vs how much can we let it learn and it leading to a good outcome, or alternatively how much do we need to specify upfront. And that leads to...</p><h1> I expect reasonably weak priors to work well to align AI with human values, and that a lot of the complexity can be offloaded to the learning process</h1><p> Equivalently speaking, I expect the cost of specification of values to be relatively low, and that a lot of the complexity is offloadable to the learning process.</p><p> This was another crux between David Xu and me, specifically on the question of whether you can largely get away with weak priors, or do you actually need to encode a lot stronger prior to prevent misalignment? It ultimately boiled down to the crux that I expected reasonably weak priors to be enough, guided by the innate reward system.</p><p> A big part of my reasoning here has to do with the fact that a lot of values and biases are inaccessible by the genome, and that means that you can&#39;t directly specify them. You can shape them via setting up training algorithms and data, but it turns out that it&#39;s very difficult to directly specify things like values, for instance in the genome. This is primarily because the genome does not have direct access to the world model or the brain, which would be required to hardcode the prior. To the extent that it can, it has to be over relatively simple properties, which means that you need to get alignment with relatively weak priors encoded, and the innate reward system generally does this fantastically, with examples of misalignment being rare and mild.</p><p> The fact that humans can reliably get values like &quot;having empathy for friends and acquaintances, we have parental instincts, we want revenge when others harm us, etc&quot;, without requiring the genome to hardcode a lot of prior information, and getting away with reasonably weak priors is a rather underappreciated thing, since it means that we don&#39;t need to specify our values very much, and thus we can reliably offload most of the value learning work to AI.</p><p> Here are some posts and comments below:</p><p> <a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome">https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome</a></p><p> <a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#iGdPrrETAHsbFYvQe">https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#iGdPrrETAHsbFYvQe</a></p><p> (I want to point out that it&#39;s not just that with weak prior information that the genome can reliably bind humans to real-enough things such that for example, they don&#39;t die from thirst from drinking fake water, but that it can create the innate reward system which uses simple update rules to reliably get nearly every person on earth to have empathy for their family and ingroup, revenge when others harmed us, etc, and the rare exceptions to the pattern are rather rare and usually mild alignment failures at best. That&#39;s a source of a lot of my optimism on AI safety and alignment.)</p><p> <a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine">https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine</a></p><p> <a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#8Fry62GiBnRYPnpNn">https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#8Fry62GiBnRYPnpNn</a></p><p> <a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#dRXCwRBkGxKTuq2Cc">https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome#dRXCwRBkGxKTuq2Cc</a></p><p> Here is the compressed conversation between David Xu and me:</p><p> <a href="https://twitter.com/davidxu90/status/1713102210354294936">https://twitter.com/davidxu90/status/1713102210354294936</a></p><blockquote><p> (And the reason I&#39;d be more optimistic there <em>is</em> basically because I expect the human has meta-priors I&#39;d endorse, causing them to extrapolate in a &quot;good&quot; way, and reach a policy similar to one I myself would reach under similar augmentation.) (David Xu)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1713230086730862731">https://twitter.com/davidxu90/status/1713230086730862731</a></p><blockquote><p> (In reality, of course, I disagree with the framing in both cases: &quot;two different systems&quot; isn&#39;t correct, because the genetic information that evolution was working with in fact <em>does</em> encode fairly strong priors, as I mentioned upthread.) (David Xu)</p></blockquote><p> <a href="https://twitter.com/SharmakeFarah14/status/1713232260827095119">https://twitter.com/SharmakeFarah14/status/1713232260827095119</a></p><blockquote><p> My disagreement is that I expect the genetic priors to be quite weak, and that a lot of values are learned, not encoded in priors, because values are inaccessible to the genome:</p></blockquote><blockquote><p> <a href="https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome">https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome</a></p></blockquote><blockquote><p> Maybe we will eventually be able to hardcode it, but we don&#39;t need that. (Myself)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1713232760637358547">https://twitter.com/davidxu90/status/1713232760637358547</a></p><blockquote><p> Values aren&#39;t &quot;learned&quot;, &quot;inferred&quot;, or any other words that suggests they&#39;re directly imbibed from the training data, because values aren&#39;t constrained by training data alone; if this were false, it would imply the orthogonality thesis is false. (David Xu)</p></blockquote><p> I&#39;m going to reply in this Lesswrong post and say that the orthogonality thesis is a lot like the no free lunch theorem: An extraordinarily powerful result that is too general to apply, because it only applies to the space of all logically possible AIs, and it only works if you have 0 prior that&#39;s applied, which in this case would require you to specify everything, including the values of the system, or at best use stuff like brute force search or memorization algorithms.</p><p> I have a very similar attitude to &quot;Most goals in the space of goal space are bad.&quot; I&#39;d probably agree in the most general sense, but that even weak priors can prevent most goals from being bad, and thus I suspect that a 0 prior condition is likely necessary. But I&#39;m not arguing that with 0 prior, models are aligned with people without specifying everything. I&#39;m arguing that we can get away with reasonably weak priors, and let within life-time learning do the rest.</p><p> Once you introduce even weak priors to the situation, then the issue is basically resolved, and I stated that weak priors work to induce learning of values, and it&#39;s consistent with the orthogonality thesis to have arbitrarily tiny prior information be necessary to learn alignment.</p><p> I could make an analogous argument for capabilities, and I&#39;d be demonstrably wrong, since the conclusion doesn&#39;t hold.</p><p> This is why I hate the orthogonality thesis, despite rationalists being right on it: It allows for too many outcomes, and any inference like values aren&#39;t learned can&#39;t be supported based on the orthogonality thesis.</p><p> <a href="https://twitter.com/SharmakeFarah14/status/1713234214391255277">https://twitter.com/SharmakeFarah14/status/1713234214391255277</a></p><blockquote><p> The problem with the orthogonality thesis is that it allows for too many outcomes, and notice I said the genetic prior is weak, not non-existent, which would be compatible with the orthogonality thesis. (Myself)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1713234707272626653">https://twitter.com/davidxu90/status/1713234707272626653</a></p><blockquote><p> The orthogonality thesis, as originally deployed, isn&#39;t meant as a tool to predict outcomes, but to counter arguments (pretty much) like the ones being made here: encountering &quot;good&quot; training data doesn&#39;t constrain motivations. Beyond that the thesis doesn&#39;t say much. (David Xu)</p></blockquote><p> <a href="https://twitter.com/SharmakeFarah14/status/1713236849873891699">https://twitter.com/SharmakeFarah14/status/1713236849873891699</a></p><blockquote><p> I suspect it&#39;s true when looking at the multiverse of AIs as a whole, then it&#39;s true, if we impose 0 prior, but even weak priors start to constrain your motivations a lot. I have more faith in weak priors + whiteboxness working out than you do. (Myself)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1713237355501584857">https://twitter.com/davidxu90/status/1713237355501584857</a></p><blockquote><p> I have more faith in weak priors + whiteboxness working out than you do.</p></blockquote><blockquote><p> I agree that something in the vicinity of this is likely [a] crux. (David Xu)</p></blockquote><p> <a href="https://twitter.com/davidxu90/status/1713238995893912060">https://twitter.com/davidxu90/status/1713238995893912060</a></p><blockquote><p> TBC, I do think it&#39;s logically possible for the NN landscape to be st everything I&#39;ve said is untrue, and that good minds abound given good data. I don&#39;t think this is likely <em>a priori</em> , and I don&#39;t think Quintin&#39;s arguments shift me very much, but I admit it&#39;s <em>possible</em> . (David Xu)</p></blockquote><p> Now that I&#39;m basically finished with laying out the arguments and the conversations, lets move on to the conclusion:</p><h1>结论</h1><p>My radical optimism on AI safety stems from a variety of sources. The reasons are, in order of the post, not ordered by importance are:</p><ol><li><p> I don&#39;t believe the sharp left turn is anywhere near as general as Nate Soares puts it, because the conditions that caused a sharp left turn in humans was basically cultural learning in humans being able to optimize over much faster time-scales than evolution could respond, evolution not course-correcting us, and being able to transmit OOMs more information via culture through the generations than evolution could. None of these conditions hold for modern AI development.</p></li><li><p> I don&#39;t believe that Nate&#39;s example of misgeneralizing the goal of IGF actually works as an actual example of misgeneralization that matters for our purposes, because they were not that 1 AI is trained for a goal in environment A, and then in environment B, it does not pursue the goal, but instead pursues a different goal competently.</p></li></ol><p> Instead, what&#39;s happening is that 1 human generation, or 1 human is trained in Environment A, and then a fresh generation of humans is trained on a different distribution, which predictably will have more divergence than the first case.</p><p> In particular, there&#39;s no reason to be concerned about the alignment of AI misgeneralizing, since we have no reason to assume that the central example of Lesswrong is actually misgeneralization. From Quintin:</p><blockquote><p> If we assume that humans were &quot;trained&quot; in the ancestral environment to pursue gazelle meat and such, and then &quot;deployed&quot; into the modern environment where we pursued ice cream instead, then that&#39;s an example where behavior in training completely fails to predict behavior in deployment.</p></blockquote><blockquote><p> If there are actually two different sets of training &quot;runs&quot;, one set trained in the ancestral environment where the humans were rewarded for pursuing gazelles, and one set trained in the modern environment where the humans were rewarded for pursuing ice cream, then the fact that humans from the latter set tend to like ice cream is no surprise at all.</p></blockquote><blockquote><p> In particular, this outcome doesn&#39;t tell us anything new or concerning from an alignment perspective. The only lesson applicable to a single training process is the fact that, if you reward a learner for doing something, they&#39;ll tend to do similar stuff in the future, which is pretty much the common understanding of what rewards do.</p></blockquote><ol start="3"><li><p> AIs are mostly white boxes, at the very least, and the control over AI that we have means that a better analogy is through our innate reward systems, which align us to quite a lot of goals spectacularly well, so well that the total evidence of alignment could easily put X-risk or even say, killing a human 5-15+ OOMs or less, which would make the alignment problem a non-problem for our purposes. It would pretty much single-handedly make AI misuse the biggest problem, but that issue has different solutions, and governments are likely to regulate AI misuse anyway, so existential risk gets cut 10-99%+ or more.</p></li><li><p> I believe the security mindset is inappropriate for AI due to the fact that aligning AI mostly doesn&#39;t involve dealing with adversarial intelligences or inputs, and the reason turns out to be that the most natural class, inner misaligned mesa-optimizers/optimization daemons mostly doesn&#39;t exist, because of my next reason. Also alignment is in a different epistemic state to computer security, and there are other disanalogies that make porting intuitions from other fields into ML/AI research very difficult to do correctly.</p></li><li><p> It is actually really difficult to inner misalign the AI, since SGD is really good at credit assignment, and optimizes the entire causal graph leading to the loss, leaving no slack. It&#39;s not like evolution where you have to do this from Gwern&#39;s post here:</p></li></ol><p> <a href="https://gwern.net/backstop#rl">https://gwern.net/backstop#rl</a></p><blockquote><p> Imagine trying to run a business in which the only feedback given is whether you go bankrupt or not. In running that business, you make millions or billions of decisions, to adopt a particular model, rent a particular store, advertise this or that, hire one person out of scores of applicants, assign them this or that task to make many decisions of their own (which may in turn require decisions to be made by still others), and so on, extended over many years. At the end, you turn a healthy profit, or go bankrupt. So you get 1 bit of feedback, which must be split over billions of decisions. When a company goes bankrupt, what killed it? Hiring the wrong accountant? The CEO not investing enough in R&amp;D? Random geopolitical events? New government regulations? Putting its HQ in the wrong city? Just a generalized inefficiency? How would you know which decisions were good and which were bad? How do you solve the “credit assignment problem”?</p></blockquote><p> The way SGD solves this problem is by running backprop, which is a white-box algorithm, and Nora Belrose explains it more here:</p><p> <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#Status_quo_AI_alignment_methods_are_white_box">https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6#Status_quo_AI_alignment_methods_are_white_box</a></p><p> And that&#39;s the base optimizer, not the mesa-optimizer, which is why SGD has a chance to correct the inner-misaligned agent far more effectively than cultural/biological evolution, the free market, etc. It is white-box, like the inner optimizers it runs, and solves credit assignment in a much better way than the previous optimizers like cultural/biological evolution, the free market, etc could hope to do.</p><ol start="6"><li> I believe that due to information inaccessibility plus the fact that the brain acts quite a lot like a Universal Learning Machine/Neural Turing Machine, this means that alignment in the human case for say surviving, having empathy for friends etc, can&#39;t depend on complicated genetic priors, and thus to the extent that genetic priors are encoded in, they need to be fairly weak and universalish-priors, plus help from the innate reward system, which is built upon those priors to use simple updating rules to reinforce certain behaviors and penalize others, and this works ridiculously well to align humans to surviving and having things like empathy/sympathy for the ingroup, revenge etc.</li></ol><p> So now that we have listed the reasons why I expect radical optimism on AI safety, I&#39;ll add some new things to show both that the shutdown problem from AI is almost solved, and that capabilities will almost certainly be slowed down by physical limits in the 2030s. (This is not a crux for me on why I believe AI to be safe.)</p><h2> Addendum 1: The shutdown problem for AI is almost solved</h2><p> It turns out that we can keep the most useful aspects of Expected Utility Maximization while making an AI shutdownable.</p><p> Sami Petersen showed that we can integrate incomplete preferences to AIs while weakening transitivity just enough to get a non-trivial theory of Expected Utility Maximization that&#39;s quite a lot safer. Elliott Thornley proposed that incomplete preferences would be used to solve the shut-down problem, and the very nice thing about subagent models of Expected Utility Maximization is that they require a unanimous committee in order for a decision to be accepted as a sure gain.</p><p> This is both useful, but can lead to problems. On the one hand, we only need one expected utility maximizer that wants to be able to shut down the AI in order for us to shut it down as a whole, but we would need to be sort of careful on where their execution conditions/domain is, as unanimous committees can terrible because only one agent needs to do something to grind the entire system to a halt, which is why in the real world, it&#39;s usually not a preferred way to govern something.</p><p> Nevertheless, for AI safety purposes, this is still very, very useful, and if it grows up to have broader conditions than the ones outlined in the posts below, this might be the single biggest MIRI success of the last 15 years, which is ridiculously good.</p><p> <a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1">https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1</a></p><p> <a href="http://pf-user-files-01.s3.amazonaws.com/u-242443/uploads/2023-05-02/m343uwh/The%20Shutdown%20Problem-%20Two%20Theorems%2C%20Incomplete%20Preferences%20as%20a%20Solution.pdf">http://pf-user-files-01.s3.amazonaws.com/u-242443/uploads/2023-05-02/m343uwh/The Shutdown Problem- Two Theorems%2C Incomplete Preferences as a Solution.pdf</a></p><h2> Addendum 2: AI progress is almost certainly going to slow down</h2><p> This is not a crux on why I believe AI to be safe, but I feel like a lot of Lesswrongers are probably wrong in their assumption that AI progress will continue as it had after 2030, and the basic reason is we are close to thermodynamic limits, plus Moore&#39;s law was one of the biggest reliable sources of AI progress. Other computing paradigms and algorithms will develop, but it will take a lot more time than we are used to.</p><p> Here&#39;s Michael P. Frank&#39;s tweet below about this:</p><p> <a href="https://twitter.com/MikePFrank/status/1711788743093616879">https://twitter.com/MikePFrank/status/1711788743093616879</a></p><blockquote><p> More people need to realize that Kurzweil&#39;s projections are simply wrong after about 2030. The laws of physics simply do not permit computing efficiency and density to continue growing fast enough to stay on that track.</p></blockquote><p> <a href="https://twitter.com/MikePFrank/status/1711788858508280116">https://twitter.com/MikePFrank/status/1711788858508280116</a></p><blockquote><p> In particular, the difficulty of reversible computing will knock down that slope significantly.</p></blockquote><p> While I don&#39;t exactly agree with this, and Max H is right to point out that as of right now, we don&#39;t actually know the ratio, I do think that to a certain extent, AI progress will have to slow down in the 2030s, and I definitely think that LW might not realize that AI is on an S-curve right now.</p><p> Max H&#39;s comment is below:</p><p> <a href="https://www.lesswrong.com/posts/fm88c8SvXvemk3BhW/brain-efficiency-cannell-prize-contest-award-ceremony#jcorx6Y7n5w7o4CXe">https://www.lesswrong.com/posts/fm88c8SvXvemk3BhW/brain-efficiency-cannell-prize-contest-award-ceremony#jcorx6Y7n5w7o4CXe</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/99tD8L8Hk5wkKNY8Q/arguments-for-radical-optimism-on-ai-safety#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/99tD8L8Hk5wkKNY8Q/arguments-for-radical-optimism-on-ai-safety<guid ispermalink="false"> 99tD8L8Hk5wkKNY8Q</guid><dc:creator><![CDATA[Noosphere89]]></dc:creator><pubDate> Sun, 15 Oct 2023 14:51:24 GMT</pubDate> </item><item><title><![CDATA[Hyperreals in a Nutshell]]></title><description><![CDATA[Published on October 15, 2023 2:23 PM GMT<br/><br/><p> <i>Epistemic status: Vaguely confused and probably lacking a sufficient technical background to get all the terms right. Is very cool though, so I figured I&#39;d write this.</i></p><blockquote><p> And what are these Fluxions? The Velocities of evanescent Increments? And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?</p><p> George Berkeley, <i>The Analyst</i></p></blockquote><p> When calculus was invented, it didn&#39;t make sense. Newton and Leibniz played fast and dirty with mathematical rigor to develop methods that arrived at the correct answers, but no one knew why. It took another one and a half centuries for Cauchy and Weierstrass develop analysis, and in the meantime people like Berkeley refused to accept the methods utilizing these &quot;ghosts of departed quantities.&quot;</p><p> Cauchy&#39;s and Weierstrass&#39;s solution to the crisis of calculus was to define infinitesimals in terms of limits. In other words, to not describe the behavior of functions directly acting on infinitesimals, but rather to frame the the entire endeavour as studying the behaviors of certain operations in the limit, in that weird superposition of being arbitrarily close to something yet not it.</p><p> (And here I realize that math is better shown, not told)</p><p> The limit of a function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>at <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x=a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> is <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span> if for any <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon>0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> there exists some<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta > 0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> such that if</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="|x-a|<\delta,"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span><p> then</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="|f(x)-L|<\epsilon."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span><p> Essentially, the limit exists if there&#39;s some value <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span></span></span></span></span> that forces <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> to be within <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span> if <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> is within <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span></span></span></span></span> of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> . Note that this has to hold true for all <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> , and you choose <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> first!</p><p> From this we get the well-known definition of the derivative:</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.039em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">lim</span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 7.021em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 7.021em; top: -1.59em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span> <span class="mjx-denominator" style="width: 7.021em; bottom: -0.795em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 7.021em;" class="mjx-line"></span></span><span style="height: 2.385em; vertical-align: -0.795em;" class="mjx-vsize"></span></span></span></span></span></span></span><p> and you can define the integral similarly.</p><p> The limit solved calculus&#39;s rigor problem. From the limit the entire field of analysis was invented and placed on solid ground, and this foundation has stood to this day.</p><p> Yet, it seems like we lose something important when we replace the idea of the &quot;infinitesimally small&quot; with the &quot;arbitrarily close to.&quot; Could we actually make numbers that were <i>infinitely small?</i></p><h2> The Sequence Construction</h2><p> Imagine some mathematical object that had all the relevant properties of the real numbers (addition, multiplication are associative and commutative, is closed, etc.) but had infinitely small and infinitely large numbers. What does this object look like?</p><p> We can take the set of all infinite sequences of real numbers <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> as a starting point. A typical element <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a\in\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> would be</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="a = (a_0, \, a_1, \, a_2, \ldots)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span><p> where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_0, a_1, a_2, \ldots"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span></span></span></span></span></span> is some infinite sequence of real numbers.</p><p> We can define addition and multiplication element-wise as:</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="a + b = (a_{0} + b_{0}, \, a_{1} + b_{1}, a_{2} + b_{2}, \ldots),"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span><p></p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="a \cdot b = (a_0 \cdot b_0, a_1 \cdot b_1, a_2 \cdot b_2, \ldots)."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span><p> You can verify that this is a commutative ring, which means that these operations behave nicely. Yet, being a commutative ring is not the same thing as being an ordered field, which is what we eventually want if our desired object is to have the same properties as the reals.</p><p> To get from <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> to a field structure, we have to modify it to accommodate well-defined division. The typical way of doing this is looking at how to introduce the zero product property: ie ensuring that if <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b \in \mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> then if <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="ab = 0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> either one of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> is <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> .</p><p> If we let <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> be the sequence of all zeros <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(0,0,0,\ldots)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N},"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> then it is clear that we can have two non-zero elements multiply to get zero. If we have</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="a = (a, 0, 0, 0, \ldots),"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span><p>和</p><span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="b = (0,b,b,b, \ldots),"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span><p> then neither of these are the zero element, yet their product is zero.</p><p> How do we fix this? Equivalence classes!</p><p> Our problem is that there are too many distinct &quot;zero-like&quot; things in the ring of real numbered sequences. Intuitively, we should expect the sequence <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(0,1,0,0,\ldots)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> to be <i>basically</i> zero, and we want to find a good condensation of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> that allows for this.</p><p> In other words, how do we make all the sequences with &quot; <a href="https://www.ykumar.org/posts/almost-always/">almost all</a> &quot; their elements as zero to be equal to zero?</p><h2> Almost All Agreement ft. Ultrafilters</h2><p> Taken from &quot; <a href="https://www.ykumar.org/almost-always/">five ways to say &quot;Almost Always&quot; and actually mean it</a> &quot;:</p><blockquote><p> A <i>filter</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{F}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.11em;">F</span></span></span></span></span></span></span></span></span> on an arbitrary set <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> is a collection of subsets of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> that is closed under set intersections and supersets. (Note that this means that the smallest filter on <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> is <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> itself).</p><p> An <i>ultrafilter</i> is a filter which, for every <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A \subseteq I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">⊆</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> , contains either <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> or its complement. A <i>principal</i> ultrafilter contains a finite set.</p><p> A <i>nonprincipal ultrafilter</i> does not.</p><p> This turns out to be an incredibly powerful mathematical tool, and can be used to generalize the concept of &quot;almost all&quot; to esoteric mathematical objects that might not have well-defined or intuitive properties.</p></blockquote><p> Let&#39;s say we define some nonprincipal ultrafilter <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{U}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.061em;">U</span></span></span></span></span></span></span></span></span> on the natural numbers. This will contain only cofinite sets, and will exclude all finite sets. Now, let&#39;s take two sequences <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b \in \mathbb{R}^\mathbb{N},"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> and define their <i>agreement set</i> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> to be the indices on which <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> are identical (have the same real number in the same position).</p><p> Observe that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> is a set of natural numbers. If <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I \in \mathcal{U},"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.061em;">U</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> then <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> cannot be finite, and it seems pretty obvious that almost all the elements in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> are the same (they only disagree at a finite number of places after all). Conversely, if <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I \not\in \mathcal{U},"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∉</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.061em;">U</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> this implies that the complement of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I,"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{N}/I \in \mathcal{U}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.061em;">U</span></span></span></span></span></span></span></span></span> , which means that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> disagree at almost all positions, so they probably shouldn&#39;t be equal.</p><p>瞧！ We have a suitable definition of &quot;almost all agreement&quot;: if the agreement set <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span></span></span></span></span> is contained in some arbitrary nonprincipal ultrafilter <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathcal{U}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.061em;">U</span></span></span></span></span></span></span></span></span> .</p><p> Let<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^*\mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> be the quotient set of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> under this equivalence relation (essentially, the set of all distinct equivalence classes of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> ). Does this satisfy the zero product property?</p><p> (Notation note: we will let <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> denote the infinite sequence of the real number <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> , and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="[a]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> the equivalence class of the sequence <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^* \mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> .)</p><h2> Yes, This Behaves Like The Real Numbers</h2><p> Let <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b \in \mathbb{R}^\mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span></span></span> such that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="ab = (0)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> . Let&#39;s break this down element-wise: either <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_n, b_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span> must be zero for all <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n \in \mathbb{N}."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span> As one of the ultrafilter axioms is that it must contain a set or its complement, either the index set of the zero elements in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> or the index set of the zero elements in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> will be in any nonprincipal ultrafilter on <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{N}."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span> Therefore, either <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> or <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> is equivalent to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(0)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^* \mathbb{R},"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span> so <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^* \mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> satisfies the zero product property.</p><p> Therefore, division is well defined on<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^*\mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> ! Now all we need is an ordering, and luckily almost all agreement saves the day again. We can say for <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b \in ^*\mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a>b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> if almost all elements in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> are greater than the elements in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> at the same positions (using the same ultrafilter equivalence).</p><p>所以，<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="^*\mathbb{R}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span> is an ordered field!</p><h2> Infinitesimals and Infinitely Large Numbers</h2><p> We have the following hyperreal:</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\epsilon = \left( 1, \frac{1}{2}, \frac{1}{3}, \ldots, \frac{1}{n}, \ldots \right)."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.124em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.777em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.145em; vertical-align: -0.777em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 0.8em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.8em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.8em; bottom: -0.722em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.8em;" class="mjx-line"></span></span><span style="height: 2.089em; vertical-align: -0.722em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">)</span></span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span><p> Recall that we embed the real numbers into the hyperreals by assigning every real number <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> to the equivalence class <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="[a]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> . Now observe that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> <i>is smaller than every real number embedded into the hyperreals this way.</i></p><p> Pick some arbitrary real number <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> . There exists <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p \in \mathbb{N}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">N</span></span></span></span></span></span></span></span></span> such that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{p}< a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.497em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.703em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.703em; bottom: -0.707em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.497em;" class="mjx-line"></span></span><span style="height: 1.47em; vertical-align: -0.5em;" class="mjx-vsize"></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> . There are infinitely many fractions of the form <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{n}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.566em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.8em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.8em; bottom: -0.524em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.566em;" class="mjx-line"></span></span><span style="height: 1.341em; vertical-align: -0.37em;" class="mjx-vsize"></span></span></span></span></span></span></span> , where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> is a natural number greater than <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> , so <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span></span></span></span></span></span> is smaller than <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> at almost all positions, so it is smaller than <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> .</p><p> This is an infinitesimal! This is a rigorously defined, coherently defined, <i>infinitesimal number</i> smaller than all real numbers! In a number system which shares all of the important properties of the real numbers! (except the Archimedean one, as we will shortly see, but that doesn&#39;t really matter).</p><p> Consider the following</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display"><span class="mjx-math" style="width: 100%;" aria-label="\Omega = (1,2,3, \\ldots)."><span class="mjx-mrow" style="width: 100%;" aria-hidden="true"><span class="mjx-stack" style="width: 100%; vertical-align: -1.492em;"><span class="mjx-block" style="text-align: center;"><span class="mjx-box"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Ω</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span> <span class="mjx-block" style="text-align: center; padding-top: 0.442em;"><span class="mjx-box"><span class="mjx-mspace" style="width: 0px; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">.</span></span></span></span></span></span></span></span></span></span><p> By a similar argument this is larger than all possible real numbers. I encourage you to try to prove this for yourself!</p><p> (The Archimedean principle is that which guarantees that if you have any two real numbers, you can multiply the smaller by some natural number to become greater than the other. This is not true in the hyperreals. Why? (Hint: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Omega"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Ω</span></span></span></span></span></span></span> breaks this if you consider a real number.))</p><h2> How does this tie into calculus, exactly?</h2><p> Well, we have a coherent way of defining infinitesimals!</p><p> The short answer is that we can define the <i>star</i> operator<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{st}(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">st</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> as that which maps any hyperreal to its closest real counterpart, provided it can be written as the sum of a real number and an infinitesimal part. Then, the definition of a derivative becomes</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="f'(x) = \text{st}\left( \frac{f(x+\Delta x)- f(x)}{\Delta x}\right)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">st</span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">(</span></span> <span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 7.85em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 7.85em; top: -1.59em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span> <span class="mjx-denominator" style="width: 7.85em; bottom: -0.817em;"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 7.85em;" class="mjx-line"></span></span><span style="height: 2.407em; vertical-align: -0.817em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 1.551em; padding-bottom: 1.551em;">)</span></span></span></span></span></span></span></span><p> where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> is some infinitesimal. More on this in a future blog post!</p><p> It also turns out the hyperreals have a bunch of really cool applications in fields far removed from analysis. Check out my expository paper on <a href="https://ykumar.org/files/Nonstandard_Methods_and_Applications_in_Ramsey_Theory.pdf">the intersection of nonstandard analysis and Ramsey theory</a> for an example!</p><p> Yet, the biggest effect I think this will have is pedadogical. I&#39;ve always found the definition of a limit kind of unintuitive, and it was specifically invented to add <i>post hoc</i> coherence to calculus after it had been invented and used widely. I suspect that formulating calculus via infinitesimals in introductory calculus classes would go a long way to making it more intuitive.</p><br/><br/> <a href="https://www.lesswrong.com/posts/9dmrnXa9AcSAHwX6z/hyperreals-in-a-nutshell#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/9dmrnXa9AcSAHwX6z/hyperreals-in-a-nutshell<guid ispermalink="false"> 9dmrnXa9AcSAHwX6z</guid><dc:creator><![CDATA[Yudhister Kumar]]></dc:creator><pubDate> Sun, 15 Oct 2023 14:23:58 GMT</pubDate> </item><item><title><![CDATA[Discovering Latent Knowledge in the Human Brain: Part 1 – Clarifying the concepts of belief and knowledge]]></title><description><![CDATA[Published on October 15, 2023 9:02 AM GMT<br/><br/><p> <i>Acknowledgements: Many thanks to Milan Cvitkovic and Collin Burns for their help in workshopping this project proposal. Many of the ideas presented here regarding the use of neuroscience and neurotechnology were originally proposed by Milan</i> <a href="https://milan.cvitkovic.net/writing/neurotechnology_is_critical_for_ai_alignment/"><i><u>here</u></i></a> <i>. This post represents my current thinking on the topic and does not necessarily represent the views of the individuals I have just mentioned.</i></p><p> <strong>tl;dr</strong> : Methods for promoting honest AI hinge on our ability to identify beliefs/knowledge in models. These concepts are derived from human cognition. In order to use them in AI, we need to provide definitions at the appropriate level of resolution. In this post, I describe why I think it would be a useful starting point to recover truth-like latent features in human brain data with contrast-consistent search (CCS). In an upcoming post, I will outline why I think this is possible with current fMRI technology.</p><h1> Summary</h1><p> Methods for discovering latent knowledge in language models, such as contrast-consistent search (CCS), hold the potential to enable humans to directly infer a model&#39;s “beliefs” from its internal activations. This can be useful for promoting honesty in AI systems by ensuring that their outputs are consistent with their “beliefs”.</p><p> However, it is currently unclear how the truth-like features recovered by CCS relate to the concepts of belief and knowledge, whose definitions are derived from human intelligence. Since these concepts underlie behaviors like deception and honesty, is essential that we understand them at the appropriate level of resolution to enable us to translate the concepts to AI systems.</p><p> A starting point could be to determine whether CCS identifies analogous features in human brains. My proposal is to use ultra-high-field functional magnetic resonance imaging (fMRI) to replicate discovering latent knowledge experiments in humans. This approach not only promises to shed light on the representations identified by CCS but also offers a crucial testbed for evaluating techniques aimed at detecting deception in AI.</p><h2> About this post</h2><p> I am posting a proposal to perform a human version of &quot;Discovering Latent Knowledge in Language Models&quot; in order to elicit feedback and help clarify my thinking on the motivation and utility of the project. I have separated my proposal into two posts. In this post, I&#39;ll cover why I think it is hard to define honesty and deception in AI, and why I think looking at human brain function can 1) help us arrive at better definitions of cognitive concepts and 2) serve as a testbed for our approaches for lie detection in AI. In the next post, I will outline a specific project proposal to test CCS on human brain data from ultra-high-field functional magnetic resonance imaging (fMRI). <strong>Readers who are interested in understanding the justification behind my proposal would benefit most from reading this post</strong> . Readers who are interested in the details of the specific project should also have a look at the second (upcoming) post.</p><h1> How Studying Human Intelligence Could Help Promote Honest AI</h1><p> To start, I would like to outline a couple reasons why neuroscience could play an important role in AI safety, especially when our aim is to foster honest AI. Much of the following overlaps quite a bit with ideas presented elsewhere in the AI safety community (specifically <a href="https://milan.cvitkovic.net/writing/neurotechnology_is_critical_for_ai_alignment/"><u>Milan Cvitkovic&#39;s post on the importance of neurotechnology</u></a> ), but I think that repackaging it here provides some clarity for the general motivation of this project.</p><h2> 1) We need better definitions of cognitive concepts to use them in AI alignment research</h2><p> I think we risk inappropriately anthropomorphizing AI when we use cognitive concepts to describe their behaviors. Deception provides a clear example. If we want to determine if an AI is lying, we first need to define what lying is. Central to the concepts of deception and lying are the concepts of belief and knowledge. Determining whether a system has beliefs or knowledge cannot rely only on measurements of external behavior. Instead, it requires evaluating the internal states of the system. Otherwise, we risk misunderstanding the AI by inappropriately assigning it human cognitive states.</p><p> Cognitive science theories currently lack the specificity necessary to determine if AI models possess beliefs or knowledge analogous to humans (although there is significant work on neurocognitive theories in this direction [ <a href="https://www.frontiersin.org/articles/10.3389/fnbeh.2022.880504/full#:~:text=.%2C%202022).-,Pre%2Dlinguistic%20Processes%20of%20Believing,%2C%20and%20perceptive%2Demotional%20integration.">1</a> , <a href="https://direct.mit.edu/netn/article/1/4/381/5401/The-graphical-brain-Belief-propagation-and-active">2</a> ]). Doing so will require describing these cognitive phenomena at the algorithmic level [ <a href="https://mitpress.mit.edu/9780262514620/vision/">3</a> ] <span class="footnote-reference" role="doc-noteref" id="fnref5ah6drf5sfp"><sup><a href="#fn5ah6drf5sfp">[1]</a></sup></span> , so that we can identify their signatures across systems that may be vastly different from each other at the implementation level. Because any definition for these concepts will need to be validated, defining cognitive phenomena at the appropriate level of resolution to transport to AI will quite likely involve measuring activity in biological brains.</p><h2> 2) We should be sure that our interpretability methods work in humans before relying on them to align AGI</h2><p> We would like to know that our methods for interpreting AI generalize beyond the narrow scope under which these methods are developed. This validation is particularly crucial if we want to deploy general-purpose methods to align powerful AGI in the future. A significant challenge, however, lies in testing the widespread applicability of these interpretability methods—specifically, understanding their scalability. Fortunately, we have a readily accessible general intelligence to examine: humans. Before counting on these methods to align AGI, it&#39;s prudent to ascertain their efficacy within the human context.</p><h1> The Problem</h1><h2> Building lie detectors </h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ps35bv69uzvwaomplcx6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ymvqpj5g4bi1mmg2zvs6 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/t2jm9slbjggfjpniinkx 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/guvphiy0xbcu2d5vwt6m 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ilhlp7roob18qmrmhvgn 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ybshi9jjsbcv1dc6dmkr 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/rduanvfq6tcqxls4aomu 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/p1w7knahnxjqegq3usj1 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/glwiviz1qdg0gem8ebs6 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/jj45j0xnwmzrk2utonfr 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/p5tzg0rhptyumvemkged 1262w"></figure><p> In general, we would like to avoid building models that can lie to us. Methods for discovering latent knowledge (DLK) aim to report what a model “believes/knows” <span class="footnote-reference" role="doc-noteref" id="fnrefh4gbanft2x6"><sup><a href="#fnh4gbanft2x6">[2]</a></sup></span> . If we know what a model believes/knows, then we can simply check if its actions are consistent with these beliefs.</p><p> Burns et al. (2022) [ <a href="https://arxiv.org/abs/2212.03827">4</a> ] made a surprising discovery that a simple linear classifier could be trained on pairs of statements and their negations to identify latent dimensions within language models that track propositional truth value. The ability of this classifier to predict the truth value of the inputs from internal activations exceeded the accuracy of zero-shot baselines. Moreover, the predictions made by the classifier continued to accurately report truth value, even when the model outputs did not.</p><p> Do these latent dimensions capture the “beliefs/knowledge” of the model? What does it mean to say that the model believes cats are mammals? Unfortunately, we can&#39;t answer these questions yet, because we don&#39;t have a sufficient understanding of what beliefs or knowledge are. Predicting whether a statement was true or false based on information present in language models is not enough to confirm that these models hold beliefs regarding the statement, because we do not know if and how that information is utilized or represented in the model. This is a problem if we want to build lie detectors, because our definition of lying relies on the concepts of knowledge and belief, which are defined internally and cannot be inferred from behavior alone.</p><h2> DLK in humans </h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/w1amigmgizutawotr7i0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/cxlccb9sebrr6msz1yr8 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/igsbl4la3o1gbyaqfj9c 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/vnztyivqup4woianocgd 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/w4a2lv7k3uipaguqlndg 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/jv5fbfvdllzycrnvgt30 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/kxgbopl8txuk2abmo0zy 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ecqbjg6xix3ebjmq1lva 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/gjjn23xae021ovc6jjfs 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/dgitksvd9zdzw4yyopri 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/kebuknclsizfoiunvtbd 1272w"></figure><p> What can we do in the absence of a comprehensive theory of belief and knowledge? We could try to build such a theory, but any theory explaining these concepts must be validated in humans. A standard approach is to look for the physical (likely neural) correlates of a cognitive phenomenon and try to draw more general conclusions from those.</p><p> For instance, we observe through direct experience that we possess beliefs and knowledge, and we extend this capacity to other humans. We can record these beliefs pretty reliably, for instance by simply asking a (trustworthy) person whether they think a proposition is true or false. If we observe the underlying brain processes that accompany these propositions, we could identify which of those coincided with the phenomenology of believing or disbelieving those propositions, and ideally also perturb the relevant neural mechanisms to determine whether they play a causal role.</p><p> However, in practice, discovering neural correlates of beliefs/knowledge is difficult, both because it is very hard to experimentally isolate the cognitive processes of interest and because our current means of measuring and perturbing the brain are still relatively imprecise. A slightly different approach could be to directly use DLK methods like CCS, which ostensibly recover “beliefs” from internal states of models, to recover similar truth-like features from human brain data. This approach can provide us with candidate neural features whose relationship to beliefs/knowledge can be explored in detail. The advantage of this approach is that we can be reasonably confident that beliefs/knowledge are embedded within the observed neural activity, because we are able to connect these to phenomenology.</p><p> As mentioned previously, there is already at least one method (CCS) for discovering features in language models that look particularly belief-like. This method may even suggest a framework for how beliefs are encoded in neural activity. Under this framework, the consistency of beliefs with the semantic content of natural language statements is represented <span class="footnote-reference" role="doc-noteref" id="fnref7ajemf68n8p"><sup><a href="#fn7ajemf68n8p">[3]</a></sup></span> along latent dimensions of neural activity, similar to the way other features are thought to be represented in the brain and artificial neural networks alike. If this framework is true of human truth evaluation in natural language processing, then this predicts that we should identify latent dimensions in neural activity that discriminates between true and false statements as rendered by the individual. This accomplishes two goals: 1) It assesses the efficacy of CCS in pinpointing neural correlates of truth evaluation when we&#39;re confident such processes are influencing the observed behavior, and 2) it lays the foundation for a plausible framework for knowledge and beliefs, potentially applicable across diverse intelligences.</p><h2> Testing lie detectors </h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/apgonzhyelywixnzmyam" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/wcolyur9vhefaikz2saw 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/n4t52dxoma6yfwkwmpqy 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/psegou8z2tp9cmicoxsl 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/x5m5duohltwmzcyezdlg 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/mhcv4ur19dkyezonvix2 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/ko3poqfj3efwnf5rlubl 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/la2rtztidubccldinbgq 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/wqrayt6qxcz1etmlidhi 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/qnnreuhiuju7ryu3ccq3 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/csFXHGb7gxpzMTeT5/fyosurfffkedyovesg9q 1272w"></figure><p> Once we have built and validated our DLK method, we should be able to put it to use to identify when models are lying. But there is still a problem. While it is possible to get language models to produce outputs that are inconsistent with the predictions from our DLK method, it is still unclear whether these constitute lies, because our understanding is still limited regarding how the information identified through our method influences the model&#39;s behavior. For instance, the model may have misinterpreted the nature of the task, or it may lack access to the necessary information in its procedure for producing its response. To distinguish between these cases and deceptive behavior, we have to further define how beliefs are used in the context of lying.</p><p> Another layer of complexity arises when considering the stability of belief/knowledge embeddings identified by DLK across varied behavioral circumstances, including situations involving deception. If, for instance, belief embeddings are obscured during deception, the accuracy of DLK, developed under truth-telling conditions, may be compromised.</p><p> If we want to test our DLK method as a lie detector, then we need to test its ability to act as such in humans. Given that humans can intentionally lie, inducing deception in an experimental setting is feasible. Such an approach illuminates the underlying processes governing truth evaluation during honesty and deception. Moreover, subjecting our DLK method to various behavioral conditions will vet its resilience, offering further confidence in its ability to discern genuine neural correlates of specific cognitive functions.</p><h2> Some other specific questions about CCS that can be addressed by DLK in the human brain</h2><p> In order to be very clear about how I think DLK in the human brain could contribute to understanding open questions in AI safety, I&#39;ll provide a few additional examples of specific questions that I believe would be addressed by this project. Note that I do not think that DLK in the human brain will provide definitive answers to any of these, but I think it could provide useful evidence before the arrival of human-level or superhuman AGI.</p><h3> Will future language models encode truth in a simple linear way?</h3><p> If we find that humans do not encode truth with a limited set of features that are accessible with linear decoders, this may change our expectations for future AI systems. In this situation, it would benefit us to understand how humans evaluate truth, and then to look for these processes in AI. On the other hand, if humans do encode truth in a relatively simple way that can be assessed with linear decoders, then I might be cautiously optimistic that similar approaches will work for increasingly general AI.</p><h3> Are belief/knowledge embeddings obscured during active deception?</h3><p> If we find that belief/knowledge embeddings are altered during active deception in humans, or that humans are able to deliberately obscure their belief/knowledge embeddings, for instance, by thinking of something else, then we should have more concern for this kind of behavior being exhibited by an advanced AI. Knowing this can help us advance strategies for promoting honesty that anticipate these kinds of behaviors.</p><h3> Can a purely language-based approach to DLK uncover beliefs that were not acquired through language?</h3><p> A serious concern is that future AI systems will not explicitly connect their beliefs to language such that they can be elicited by methods like CCS. Humans, and presumably other intelligent organisms, invariably form beliefs through means other than interacting through natural language. It would be useful to test whether an approach like CCS, which operates through language, could ascertain beliefs that are formed by humans outside of language.</p><h2> Some possible objections</h2><h3> Can&#39;t we just build truthful AI?</h3><p> Enforcing truthfulness in AI, especially LLMs, rather than honesty could be a more feasible and stringent criterion [ <a href="https://arxiv.org/abs/2110.06674">5</a> ]. While honesty enforces that statements are made in accordance with beliefs, truthfulness requires that statements are verifiably true. Enforcing truthfulness sidesteps the issue of defining internal cognitive processes and relies only on external observations of behavior.</p><p> While I think aiming for truthfulness may be a valuable near-term approach and desirable alongside honesty, I worry that it will have limitations in enforcement as AI systems become increasingly capable, producing outputs that may be difficult to verify by humans. Assessing truth value in natural language is already not straightforward (see debates on factive predicates in linguistics eg [ <a href="https://muse.jhu.edu/article/864635/summary">6</a> ]). Plus, training models exclusively on human-verifiable statements doesn&#39;t eliminate the likelihood of models misgeneralizing beyond their training data. Consequently, discerning a model&#39;s inherent beliefs and its adherence to them remains crucial.</p><p> I also worry that the framing for truthful AI too narrowly conceptualizes deception as “lying”. In fact, deception does not require that statements are made contrary to internal beliefs. Sophisticated deception is observed when an individual makes statements that are verifiably true in order to mislead or manipulate others to act contrary to how they would otherwise. This can happen when individuals anticipate that their statements will not be believed by others [ <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0297.2008.02205.x">7</a> , <a href="https://www.nature.com/articles/s41598-020-67721-z">8</a> ]. Numerous manipulative tactics don&#39;t necessarily involve blatant falsehoods but may be built around selective but still factually accurate statements. Given these considerations, I think aiming for honesty in AI, which circumvents both simple and sophisticated forms of deception, is a necessary long-term goal.</p><h3> Won&#39;t it be obvious when we have identified the cognitive capacities of AI?</h3><p> Perhaps we will discover methods that quite compellingly suggest that some AI systems possess certain cognitive capacities (maybe by exactly describing the process by which some process happens in full detail). However, I&#39;m doubtful that we can be confident that these cognitive capacities map cleanly to concepts like “deception” and “honesty” without testing that we reach similar conclusions in humans. Any attempt to extrapolate human-centric cognitive notions onto AI necessitates a robust validation of these concepts in humans first.</p><h3> We shouldn&#39;t expect the cognition of advanced AIs to resemble that of humans anyway.</h3><p> I think this is quite possibly true. If it is true however, we should be extremely cautious about applying concepts derived from human cognition to AI. It still would benefit us to determine precisely how the cognition of AI differs from human intelligence, and what implications that has for defining what we mean by wanting to avoid deceptive behavior. For instance, if models do not have analogous belief/knowledge structures, then it becomes unclear how to approach the problem of deceptive AI, or whether deception is a coherent framework for thinking about AI at all.</p><h3> Language limits our evaluation of beliefs.</h3><p> I am most worried about this objection, and I think it requires further consideration. When we use language to probe beliefs, we may be limiting ourselves to propositions that can be easily stated in natural language. That is, language may be insufficient to communicate certain propositions precisely, and therefore we may not be able to elicit the full range of beliefs/knowledge encoded by brains or AI systems. We can probably elicit certain things from the underlying world model through language, but what we would like to do is probe the world model itself.</p><p> Furthermore, language does not appear to be necessary to have beliefs/knowledge. The distinction between language and belief/knowledge appears to be demonstrated in individuals who have had corpus callostomies (ie split-brain patients). Often, these patients cannot express, through language or perhaps only certain forms of language, information that is only accessible to the non-language-dominant brain hemisphere. Yet, these individuals can still take actions based on this knowledge even if they are not able to connect it to language (see [ <a href="https://www.nature.com/articles/nrn1740">9</a> ] and [ <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C24&amp;q=review+of+the+split+brain+1975&amp;btnG=">10</a> ] for reviews of split-brain studies).</p><p> I think this objection might hit on some deep philosophical problems that I am not prepared to address directly. However, the current proposal still seems to at least be capable of addressing narrower questions like “How is natural language evaluated against beliefs/knowledge in the brain?” and “Can we use natural language to infer beliefs/knowledge directly from brain data?”, which is perhaps a starting point for addressing deeper questions.</p><p> Moreover, the concern about language being insufficient to ascertain beliefs/knowledge is just as much a problem for aligning AI as it is for studying belief/knowledge in humans. Therefore, this objection does not provide a specific criticism of the present proposal to test CCS in humans, and instead offers a legitimate critique of the use of CCS (in its current form) as a method for discerning beliefs/knowledge in general. I suspect that future methods will be able to discover beliefs/knowledge by eliciting them in ways that do not involve natural language, or use natural language alongside other modalities. These methods might end up looking quite similar to CCS. However, they also may require deeper theoretical foundations for intelligence. Regardless, when we develop such methods and theories, we should ensure that they apply to humans for the same reasons I have outlined above.</p><h1>结论</h1><p>I have outlined here why I think it is necessary to examine human brains to understand belief and knowledge at the appropriate level of resolution to use the concepts to explain the behavior of AI. Since our definitions of deception invokes beliefs/knowledge, understanding these concepts is essential for avoiding deceptive behaviors in AI. I think that perhaps the simplest way forward is to use our current methods for recovering “beliefs” in LLMs to recover analogous features in human brain data. This both tests the generality of these methods, particularly in the context of general intelligence, and begins to examine how the features recovered by these methods relate to belief/knowledge.</p><p> My biggest reservation is that CCS probes beliefs/knowledge through natural language. It seems unclear to me whether this will be sufficient to recover beliefs/knowledge that may have been formed outside of natural language and may not be explicitly connected to language. Furthermore, it limits the neuroscientific questions that can be addressed by restricting assessments to how world-models are brought to bear on language rather than probing the world-model itself. However, I tend to view this as a more general critique of CCS. It still stands to reason that DLK methods, including CCS and future methods that may or may not be tied to natural language, should be investigated in the human context.</p><h2> Next Post</h2><p> My next post will outline a specific experiment for testing CCS with human fMRI data. I&#39;ll provide an assessment of the current neuroscience literature on the topic of truth evaluation in natural language, the experimental details of my proposal, and the limitations of current methodologies for studying this topic in the human brain.</p><h1>参考</h1><p>[1] RJ Seitz. Believing and beliefs—neurophysiological underpinnings. Frontiers in Behavioral Neuroscience, 16:880504, 2022.</p><p> [2] KJ Friston, T. Parr, and B. de Vries. The graphical brain: Belief propagation and active inference. Network neuroscience, 1(4):381–414, 2017.</p><p> [3] D. Marr. Vision: A computational investigation into the human representation and processing of visual information. MIT press, 1982.</p><p> [4] C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022.</p><p> [5] O. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders. Truthful AI: Developing and governing AI that does not lie. arXiv preprint arXiv:2110.06674, 2021.</p><p> [6] J. Degen and J. Tonhauser. Are there factive predicates? An empirical investigation. Language, 98(3):552–591, 2022.</p><p> [7] M. Sutter. Deception through telling the truth?! Experimental evidence<br> from individuals and teams. The Economic Journal, 119(534):47–60, 2009.</p><p> [8] M. Zheltyakova, M. Kireev, A. Korotkov, and S. Medvedev. Neural mecha-<br> nisms of deception in a social context: an fMRI replication study. Scientific<br> Reports, 10(1):10713, 2020.</p><p> [9] MS Gazzaniga. Forty-five years of split-brain research and still going<br>强的。 Nature Reviews Neuroscience, 6(8):653–659, 2005.</p><p> [10] MS Gazzaniga. Review of the split brain, 1975. <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn5ah6drf5sfp"> <span class="footnote-back-link"><sup><strong><a href="#fnref5ah6drf5sfp">^</a></strong></sup></span><div class="footnote-content"><p> Referring here to David Marr&#39;s levels of analysis, which distinguishes between the computational level - the goal of the system, the algorithmic level - how the computation is achieved, and the implementation level - how the algorithm is physically instantiated.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh4gbanft2x6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh4gbanft2x6">^</a></strong></sup></span><div class="footnote-content"><p> This topic appears closely tied to <a href="https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk#:~:text=Eliciting%20Latent%20Knowledge%20is%20an,that%20look%20good%20to%20us."><u>Eliciting Latent Knowledge (ELK)</u></a> , but for the time being, I am assuming that the reporter for the model&#39;s beliefs will be simple enough that we don&#39;t have to worry about the human simulator failure mode described in ARC&#39;s first technical report on the subject. Regardless, my intuition is that the problem of defining belief/knowledge will be common across any approach to ELK, and could therefore benefit from validation in humans.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ajemf68n8p"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ajemf68n8p">^</a></strong></sup></span><div class="footnote-content"><p> It was pointed out to me that saying that truth is “represented” by the brain might be a misapplication of the framework of representationalism in cognitive science. To avoid misusing this terminology, I will try to use “truth evaluation process” as an alternative to “truth representation”.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/csFXHGb7gxpzMTeT5/discovering-latent-knowledge-in-the-human-brain-part-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/csFXHGb7gxpzMTeT5/discovering-latent-knowledge-in-the-human-brain-part-1<guid ispermalink="false"> csFXHGb7gxpzMTeT5</guid><dc:creator><![CDATA[Joseph Emerson]]></dc:creator><pubDate> Sun, 15 Oct 2023 11:00:29 GMT</pubDate> </item><item><title><![CDATA[Rationalist horror movies]]></title><description><![CDATA[Published on October 15, 2023 7:42 AM GMT<br/><br/><p> I like horror movies with smart, agentic protagonists, while staying in horror genre rather than thriller or action. These are rather thin on the ground. In the spirit of <a href="https://www.lesswrong.com/posts/4KATQFzniqDpHo6ck/puzzle-games">Scott Garabant&#39;s Puzzle Game post</a> , here is my list of favorites and an invitation to add yours.</p><p> My favorites:</p><ul><li> Oculus.</li><li> Hush</li><li> Green Room</li></ul><p> Honorable mentions. These aren&#39;t quite the total package, but are close and extremely good such that they still seemed worth including:</p><ul><li> The Ring&#39;s protagonist is not dumb, but it&#39;s more &quot;solving a mystery&quot; than &quot;well does this weapon work?&quot;</li><li> Tremors would need to change very little to be a horror movie, but it is in fact an action movie.</li><li> It Follows spends a little too much time denying the problem</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/aCjvg5whn8DYqneg5/rationalist-horror-movies#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/aCjvg5whn8DYqneg5/rationalist-horror-movies<guid ispermalink="false"> aCjvg5whn8DYqneg5</guid><dc:creator><![CDATA[Elizabeth]]></dc:creator><pubDate> Sun, 15 Oct 2023 07:42:16 GMT</pubDate> </item><item><title><![CDATA[Unity Gridworlds]]></title><description><![CDATA[Published on October 15, 2023 4:36 AM GMT<br/><br/><p><strong>概括：</strong></p><p> <a href="https://github.com/Will9371/Grid-Worlds"><i><u>Unity Gridworlds</u></i></a> is an open-source recreation of DeepMind&#39;s 2017 paper, <a href="https://arxiv.org/abs/1711.09883"><i>AI Safety Gridworlds</i></a> in the Unity game engine, including a level editor that decreases the barrier to entry for non-specialists to design their own Gridworlds-style experiments.</p><p> (See this <a href="https://www.zenmarmotdigital.com/blog/unity-gridworlds">blog</a> for a longer form of this post with more background explanation)</p><p><strong>背景：</strong></p><p> Gridworlds is a series of experiments published by DeepMind in 2017 involving toy environments that analogize to dangerous situations for future AI, exploring questions like “will an AI allow itself to be shut down?” and “will it act differently if it knows it is being supervised?”</p><p> Unity is an extremely popular game engine used by millions of hobbyist and professional game developers around the world.  It can be downloaded for free and has a <a href="https://unity.com/products/machine-learning-agents"><u>Machine Learning Agents</u></a> plugin that abstracts away the algorithms and lets you focus on designing environments and reward systems.</p><p> <strong>Project Demonstration:</strong></p><p> To demonstrate some of the core features of <i>Unity Gridworlds</i> , I have created <i>Risk Aversion,</i> a novel environment that explores AI willingness to search for high risk/reward strategies during training.  See <a href="https://youtu.be/idn20Iibq6Q"><u>this video</u></a> for a step-by-step walkthrough of the creation process, including level design, testing, training, and deployment.</p><p> Some highlights:</p><ul><li> Environment design is done through an intuitive point-and-click graphical user interface.</li><li> Core features, such as agent movement, basic object interactions, saving/loading layouts, and result visualization are built-in.</li><li> You can watch training happen visually and in accelerated-time, gathering qualitative feedback that would be easy to miss in reports and charts (but those are also available).  In my video demonstration, for example, the agent finds a high-reward strategy during training, then discards it for some unknown reason.</li><li> Training for simple environments can take as little as 5-15 minutes on a consumer-grade PC, even without GPU acceleration.<br></li></ul><p> <strong>What&#39;s Next:</strong></p><p> My hopes with <i>Unity Gridworlds</i> are to provide a resource for established AI safety researchers and also to introduce AI alignment concepts to a broad audience of hobbyist and professional game developers.  I am currently looking for collaborators to provide feedback regarding UI and workflow improvements, suggest (or help implement) useful features, and design interesting experiments.  If this interests you, regardless of experience level, feel free to reach out!</p><br/><br/> <a href="https://www.lesswrong.com/posts/74xt2jAMfFTGtEujN/unity-gridworlds#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/74xt2jAMfFTGtEujN/unity-gridworlds<guid ispermalink="false"> 74xt2jAMfFTGtEujN</guid><dc:creator><![CDATA[WillPetillo]]></dc:creator><pubDate> Sun, 15 Oct 2023 05:26:26 GMT</pubDate> </item><item><title><![CDATA[In memory of Louise Glück]]></title><description><![CDATA[Published on October 15, 2023 2:59 AM GMT<br/><br/><p> (Cross-posted from <a href="https://joecarlsmith.com/2023/10/15/in-memory-of-louise-gluck">my website</a> . Podcast version <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13778055-in-memory-of-louise-gluck">here</a> , or search &quot;Joe Carlsmith Audio&quot; on your podcast app.) </p><figure class="image image_resized" style="width:46.81%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gtxrEJoZ4NK4sXqTN/xezfolkdkftvifki8xlh" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gtxrEJoZ4NK4sXqTN/xezfolkdkftvifki8xlh 460w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gtxrEJoZ4NK4sXqTN/hzmxjpd7injz2ib3icuf 232w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gtxrEJoZ4NK4sXqTN/vaee2uqj6ziifeep4eqi 402w"><figcaption> Louise Glück, circa 1977. Image source <a href="https://commons.wikimedia.org/wiki/Category:Louise_Gl%C3%BCck#/media/File:Louise_Gl%C3%BCck_circa_1977.jpg">here</a> .</figcaption></figure><p> Louise Glück, one of my favorite poets, died yesterday.</p><p> I took a writing workshop with her back in 2009. I remember the force and precision of her words as she spoke, slowly, in class. There was a feeling like stones falling, one by one, into place. Like something large was being built, steadily, in simple movements, and you could see it forming.</p><p> I remember, too, a meeting in her office. It was early in the semester, and she hadn&#39;t liked my poem. She wanted me to write from some place closer to where dreams come from – something deeper and more inchoate, some raw edge. She wanted what she had called, in my memory, a “real poem.”</p><p> I read various of her poems in that class, but the book of hers that has most stayed with me, and mattered most to me, came out later, in 2014: “ <a href="https://www.amazon.com/Faithful-Virtuous-Night-Louise-Gl%C3%BCck/dp/0374535779">Faithful and Virtuous Night</a> .” I associate this book, in particular, with the dream-like quality of many of the poems — the way they shift underneath you as you read, full of quiet mystery and surprise. Here&#39;s one of my favorites:</p><blockquote><p> <i>A Foreshortened Journey</i></p><p> I found the stairs somewhat more difficult than I had expected and so I sat down, so to speak, in the middle of the journey. Because there was a large window opposite the railing, I was able to entertain myself with the little dramas and comedies of the street outside, though no one I knew passed by, no one, certainly, who could have assisted me. Nor were the stairs themselves in use, as far as I could see. You must get up, my lad, I told myself. Since this seemed suddenly impossible, I did the next best thing: I prepared to sleep, my head and arm on the stair above, my body crouched below. Sometime after this, a little girl appeared on the top of the staircase, holding the hand of an elderly woman. Grandmother, cried the little girl, there is a dead man on the staircase! We must let him sleep, said the grandmother. We must walk quietly by. He is at that point in life at which neither returning to the beginning nor advancing to the end seem bearable; therefore, he has decided to stop, here, in the midst of things, though this makes him an obstacle to others, such as ourselves. But we must not give up hope; in my own life, she continued, there was such a time, though that was long ago. And here, she let her granddaughter walk in front of her so they could pass me without disturbing me.</p><p> I would have liked to hear the whole of her story, since she seemed, as she passed by, a vigorous woman, ready to take pleasure in life, and at the same time forthright, without illusions. But soon their voices faded into whispers, or they were far away. Will we see him when we return, the child murmured. He will be long gone by then, said her grandmother, he will have finished climbing up or down, as the case may be. Then I will say goodbye now, said the little girl. And she knelt below me, chanting a prayer I recognized as the Hebrew prayer for the dead. Sir, she whispered, my grandmother tells me you are not dead, but I thought perhaps this would soothe you in your terrors, and I will not be here to sing it at the right time.</p><p> When you hear this again, she said, perhaps the words will be less intimidating, if you remember how you first heard them, in the voice of a little girl.</p></blockquote><p> Why do I love this poem? I wish I could say more directly. It feels like its touching, in me, that raw edge. Something about the journey, the unexplained staircase, preparing to sleep, the grandmother&#39;s understanding. “In my own life, she continued, there was such a time.” And the little girl&#39;s simple depth. “Then I will say goodbye now.”</p><p> The little girl, especially, has stayed with me, as have a few other similarly dream-like figures – the conductor in “ <a href="https://www.poetryfoundation.org/poetrymagazine/poems/56626/aboriginal-landscape">Aboriginal Landscape</a> ,” the old woman in “ <a href="https://www.threepennyreview.com/a-sharply-worded-silence/">A Sharply Worded Silence</a> ,” the concierge in “ <a href="https://www.theparisreview.org/poetry/7214/the-denial-of-death-louise-gluck">The Denial of Death</a> .” The backdrop of these poems is often ethereal and unstable. It seems one way, then smoothly un-seems. “Right now you are a child holding hands with a fortune-teller,” Glück writes in “ <a href="https://connectere.wordpress.com/2016/02/18/theory-of-memory-by-louise-gluck/">Theory of Memory</a> .” “All the rest is hypothesis and dream.”</p><p> See, in “Aboriginal Landscape,” the way the speaker&#39;s relationship to the cemetery evolves.</p><blockquote><p> The cemetery was silent. Wind blew through the trees;<br> I could hear, very faintly, sounds of weeping several rows away,<br> and beyond that, a dog wailing.<br><br> At length these sounds abated. It crossed my mind<br> I had no memory of   being driven here,<br> to what now seemed a cemetery, though it could have been<br> a cemetery in my mind only; perhaps it was a park, or if not a park,<br> a garden or bower, perfumed, I now realized, with the scent of roses —<br> douceur de vivre filling the air, the sweetness of living,<br> as the saying goes. At some point,<br><br> it occurred to me I was alone.<br> Where had the others gone,<br> my cousins and sister, Caitlin and Abigail?<br><br> By now the light was fading. Where was the car<br> waiting to take us home?</p></blockquote><p> And yet the figures the speaker encounters have some kind of elusive understanding, like the grandmother&#39;s on the staircase. Here, for example, is the conductor:</p><blockquote><p> Finally, in the distance, I made out a small train,<br> stopped, it seemed, behind some foliage, the conductor<br> lingering against a doorframe, smoking a cigarette.<br><br> Do not forget me, I cried, running now<br> over many plots, many mothers and fathers <i>—</i><br><br> Do not forget me, I cried, when at last I reached him.<br> Madam, he said, pointing to the tracks,<br> surely you realize this is the end, the tracks do not go further.<br> His words were harsh, and yet his eyes were kind;<br> this encouraged me to press my case harder.<br> But they go back, I said, and I remarked<br> their sturdiness, as though they had many such returns ahead of them.<br><br> You know, he said, our work is difficult: we confront<br> much sorrow and disappointment.<br> He gazed at me with increasing frankness.<br> I was like you once, he added, in love with turbulence.<br><br> Now I spoke as to an old friend:<br> What of  you, I said, since he was free to leave,<br> have you no wish to go home,<br> to see the city again?<br><br> This is my home, he said.<br> The city — the city is where I disappear.</p></blockquote><p> And here&#39;s the old woman in “A Sharply Worded Silence”:</p><blockquote><p> When I was young, she said, I liked walking the garden path at twilight<br> and if the path was long enough I would see the moon rise.<br> That was for me the great pleasure: not sex, not food, not worldly amusement.<br> I preferred the moon&#39;s rising, and sometimes I would hear,<br> at the same moment, the sublime notes of the final ensemble<br> of <i>The Marriage of Figaro</i> . Where did the music come from?<br> I never knew.<br><br> Because it is the nature of garden paths<br> to be circular, each night, after my wanderings,<br> I would find myself at my front door, staring at it,<br> barely able to make out, in darkness, the glittering knob.<br><br> It was, she said, a great discovery, albeit my real life.</p></blockquote><p> “My real life.” How do you discover your real life? How do you write a real poem? Part of it, for Glück, is gazing, and speaking, with frankness. Forthright, and without illusions.</p><p> People call her poetry “austere” and “terse.” She has, one senses, no time for bullshit. From “ <a href="https://poems.com/poem/october-section-i/">October</a> ”:</p><blockquote><p> It is true there is not enough beauty in the world.<br> It is also true that I am not competent to restore it.<br> Neither is there candor, and here I may be of some use.</p></blockquote><p> But what she strips away in pursuit of candor leaves something luminous. She is direct, but not dry. Indeed, in her hands, simple words take on strange power. “ <a href="https://www.poetryfoundation.org/poems/49606/early-december-in-croton-on-hudson">I want you</a> .” “ <a href="http://caladesishore.com/dpc/Poets/LouiseGluck/PurpleBathingSuit.html">You are all that is wrong with my life and I need you and I claim you</a> .” “ <a href="https://voetica.com/poem/9608">It&#39;s not the earth I&#39;ll miss, it&#39;s you I&#39;ll miss</a> .”</p><p> Here&#39;s another of my favorites:</p><blockquote><p> <i>The Wild Iris</i></p><p> At the end of my suffering<br> there was a door.<br><br> Hear me out: that which you call death<br> I remember.<br><br> Overhead, noises, branches of the pine shifting.<br> Then nothing. The weak sun<br> flickered over the dry surface.<br><br> It is terrible to survive<br> as consciousness<br> buried in the dark earth.<br><br> Then it was over: that which you fear, being<br> a soul and unable<br> to speak, ending abruptly, the stiff earth<br> bending a little. And what I took to be<br> birds darting in low shrubs.<br><br> You who do not remember<br> passage from the other world<br> I tell you I could speak again: whatever<br> returns from oblivion returns<br> to find a voice:<br><br> from the center of my life came<br> a great fountain, deep blue<br> shadows on azure seawater.</p></blockquote><p> I&#39;ve returned, especially, to that line in the last stanza: “from the center of my life came a great fountain.” And to that seawater. Is that the great discovery? The poem speaks of a door at the end of suffering. Was there, somewhere, a glittering knob?</p><p> The other Glück poem that has really stayed with me is <a href="https://www.poetryfoundation.org/poems/49759/vespers-in-your-extended-absence-you-permit-me">this one</a> , from a series where she sometimes speaks to God from her garden.</p><blockquote><p> <i>Vespers</i></p><p> In your extended absence, you permit me<br> use of earth, anticipating<br> some return on investment. I must report<br> failure in my assignment, principally<br> regarding the tomato plants.<br> I think I should not be encouraged to grow<br> tomatoes. Or, if I am, you should withhold<br> the heavy rains, the cold nights that come<br> so often here, while other regions get<br> twelve weeks of summer. All this<br> belongs to you: on the other hand,<br> I planted the seeds, I watched the first shoots<br> like wings tearing the soil, and it was my heart<br> broken by the blight, the black spot so quickly<br> multiplying in the rows. I doubt<br> you have a heart, in our understanding of<br> that term. You who do not discriminate<br> between the dead and the living, who are, in consequence,<br> immune to foreshadowing, you may not know<br> how much terror we bear, the spotted leaf,<br> the red leaves of the maple falling<br> even in August, in early darkness: I am responsible<br> for these vines.</p></blockquote><p> It feels like there&#39;s a building pain in this poem: the heavy rains, the cold night, and then that breaking blight, and the black spots spreading. And then that last line, that taking-it-on. “I am responsible for these vines.” She loves the earth, and it causes her such pain. See, also, her 2004 “ <a href="https://poems.com/poem/october-section-i/">October</a> ”:</p><blockquote><p> Is it winter again, is it cold again,<br> didn&#39;t Frank just slip on the ice,<br> didn&#39;t he heal, weren&#39;t the spring seeds planted…<br><br> didn&#39;t we plant the seeds,<br> weren&#39;t we necessary to the earth,<br><br> the vines, were they harvested?</p></blockquote><p> And see, too, her warnings in “The Sensual World”:</p><blockquote><p> I caution you as I was never cautioned:<br><br> you will never let go, you will never be satiated.<br> You will be damaged and scarred, you will continue to hunger.<br><br> Your body will age, you will continue to need.<br> You will want the earth, then more of the earth–<br><br> Sublime, indifferent, it is present, it will not respond.<br> It is encompassing, it will not minister.<br><br> Meaning, it will feed you, it will ravish you,<br> it will not keep you alive.</p></blockquote><p> She knew well that she was going to die. “My body,” she writes in “ <a href="https://griffinpoetryprize.com/poem/crossroads-gluck/">Crossroads</a> ,” “now that we will not be traveling together much longer, I begin to feel a new tenderness toward you, very raw and unfamiliar, like what I remember of love when I was young…” And loss is everywhere in her poetry: See, eg, in “ <a href="https://www.theparisreview.org/poetry/7066/primavera-louise-gluck">Primavera</a> ”: “Alas, very soon everything will disappear: the birdcalls, the delicate blossoms. In the end, even the earth itself will follow the artist&#39;s name into oblivion.”</p><p> The artist, in that poem, leaves no signature – just a drawing of the sun in the dirt, and a “mood of celebration.” And in her <a href="https://www.nobelprize.org/prizes/literature/2020/gluck/lecture/">acceptance speech for the Nobel prize</a> , recorded less than six months before her death from cancer, Glück seems attracted to a type of anonymity – or at least, to the intimacy of encountering her readers in private, rather than in the public sphere. (Perhaps: in a fortune-teller&#39;s tent, holding hands, with all the rest as mere hypothesis.) She quotes Dickinson as an example of this intimacy: “I&#39;m Nobody! Who are you? Are you – Nobody – too? Then there&#39;s a pair of us! Don&#39;t tell!” Glück admits to wanting readers. But she wants to reach them “singly, one by one.” “You hear this voice?” she writes in “ <a href="https://poems.com/poem/october-section-i/">October</a> .” “This is my mind&#39;s voice.”</p><p> Specifically, though, she wanted her readers to participate, somehow, in her poetry. When Eliot says “let us go then, you and I,” Glück thinks he is <i>asking</i> something of the reader. She is, too.</p><p> Asking what? Well: many things, presumably. And the individual poems are the best place to look. In general, though, I think she asks for candor – and more, for some sort of intensity and directness of spirit. Perhaps it is a stretch to say that Glück&#39;s is “spiritual” poetry. But she appears, on the page, as fiercely alive and attuned, in a world simultaneously dreamlike and raw, mythical and mundane.</p><p> How do you honor someone who has lived the way Glück, in her poetry, appears to live? By meeting them where they seek to stand. By searching out the same real life they sought – that greatest discovery. Here&#39;s to the fountain she found.</p><br/><br/> <a href="https://www.lesswrong.com/posts/gtxrEJoZ4NK4sXqTN/in-memory-of-louise-glueck#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/gtxrEJoZ4NK4sXqTN/in-memory-of-louise-glueck<guid ispermalink="false"> gtxrEJoZ4NK4sXqTN</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Sun, 15 Oct 2023 02:59:43 GMT</pubDate> </item><item><title><![CDATA[Book Review: Invisible China]]></title><description><![CDATA[Published on October 14, 2023 9:51 PM GMT<br/><br/><p> This is a book review of <a href="https://www.goodreads.com/book/show/51941583-invisible-china"><em>Invisible China</em></a> . The book covers the rural-urban divide in China and its implications for China&#39;s continued economic growth. I think it&#39;s worth the read.</p><hr><p> Pretty good. Made me think, and Rozelle&#39;s unique perspective (economist who&#39;s spent 30+ years on the ground) adds an authentic flavor to the book.</p><p> He claims that the existential threat to China&#39;s potential future growth and success is a lack of human capital: specifically, that systemic factors hinder rural Chinese children&#39;s ability to participate effectively in the service-based, advanced economy of the 21st century, and that fixing this issue is imperative so that China does not fall into the middle income trap.</p><blockquote><p> In a 2004 article in Foreign Affairs, political scientist Geoffrey Garrett shared a startling observation. He looked at the history of recent economic development and noticed that while rich countries were continuing to do well and many poor countries were achieving strong growth rates, the countries in the middle of the global income spectrum were growing more slowly and less successfully than anyone else.</p></blockquote><blockquote><p> In a report that made ripples throughout the development world, economists at the World Bank demonstrated the full extent of the problem. The report showed that out of 101 countries that were middle income in 1960, only thirteen had made it to high-income status by 2008. The rest remained stuck or even ended the fifty years poorer than before.</p></blockquote><p> The middle income trap is the stagnation developing countries face when their average citizen reaches a &quot;middle income.&quot; An easy way for developing countries to rapidly grow their economies is to accept industrialization and foreign direct investment, shifting to a manufacturing economy reminiscent of Western nations in the 1890s. However, Rozelle makes the argument that to then shift from a middle income to high income country requires an educated populace, and the middle-income countries which have failed to continue their economic growth did not appropriately invest in the education of their citizenry.</p><blockquote><p> But with a bit of historical data, another trend emerges that is far more relevant to the question at hand: the very small number of countries that have made it out of middle-income status in recent decades—the graduates—all have very high levels of high school education. Even more surprising, they&#39;ve had those high levels for decades. In particular, back when they were still middle income, the graduates (places like South Korea, Taiwan, and Ireland) all had high school attainment rates comparable to those of the rich countries.</p></blockquote><p> China is not appropriately investing in the necessary amounts of education to avoid the middle income trap. While they may have the necessary innovation, all the Baidus, Tencents, and Bytedances in the world wouldn&#39;t fix the systemic issue of having nearly 70% of its population without a high school education unable to participate in the high-skilled, high-paid industries that characterize high-income countries.</p><p> Rural Chinese mainly work low-skilled manufacturing jobs, or jobs which don&#39;t require a high school education. This was fine, until rising wages in mainland China started pushing manufacturing jobs overseas, leaving hundreds of thousands, if not millions of workers unemployed. Throughout the book, Rozelle includes telling anecdotes of workers who lost their job and couldn&#39;t get a new one because they didn&#39;t have the required skillset.</p><blockquote><p> ...We fell into conversation with one man in particular. Mr. Wang was about thirty-five and had recently been laid off from his job of eighteen years as an electrician.</p></blockquote><blockquote><p> As we watched, Mr. Wang gamely sidled up to the first booth, which represented a bank. The man running the booth, a human resources staff member, shook his hand and asked him to read a short page of text and comment on what he understood. Mr. Wang stared hard at the piece of paper, trying to force the characters into an order he could understand. He knew all the words and could read them aloud, but many of the terms were over his head. He kept getting confused by the logic of the sentences. For several minutes he stared at it, trying to make sense of what he was seeing. At first the man running the booth was patient and seemed sympathetic, but gradually the bank representative lost interest, turned back to his desk, and started looking around for workers who might be a better bet. After a few more minutes, Mr. Wang lowered his head with a sigh: “I&#39;m sorry, I just can&#39;t make sense of it.”</p></blockquote><p> As it turns out, simply increasing access and/or mandating a high school education wouldn&#39;t be enough. The rural/urban divide in China runs deep. The opportunity &amp; outcome differential comes not only from unequal access to education and other outgrowths of the <em>hukou</em> policy, but also from deep-seated systemic differences. &#39;Vocational&#39; schooling is a joke:</p><blockquote><p> Tao&#39;s middle school teacher suggested he check out the new vocational high school that had recently opened across town... When he arrived on campus a week later, he found himself in an unfamiliar world. Whereas his middle school had been orderly and regimented, here chaos reigned. The older students were tough-looking, with tight jeans, black pleather jackets, and spiked hair. As he walked to class on his first day, there were no adults in sight. He passed groups of kids hanging out in the courtyard, smoking cigarettes and laughing.</p></blockquote><p> Children who grow up in rural China suffer from three &quot;invisible epidemics&quot;, as Rozelle calls them: anemia, uncorrected myopia, and parasitic intestinal worms. Iron-deficiency anemia has been linked to dramatic decreases in IQ, myopia (for obvious reasons) impedes learning ability, and worms almost literally sap the life force from their hosts. Luckily, cheap and safe interventions can fix these issues. Iron supplements, subsidized glasses, and cheap, common medicine could easily bring the health of a Chinese kid in the country to within striking distance of one in Beijing.</p><p> Perhaps a harder problem to fix is that of crucial stimulation of children during their formative developmental years. Rural parenting strategies are unequipped to give a child sufficient intellectual stimulation.</p><blockquote><p> In rural China, babies are systematically missing out on the mental stimulation they need. When we asked rural families if they ever talked to their babies, we were met with blank looks or bemused smiles. “Why would I talk to my baby?” one young mother responded, giggling into her hand. “She can&#39;t talk back!”</p></blockquote><p> Yet, this leads to delayed infant development, and the permanent stunting of a child&#39;s ability relative to their peers. Rural chinese babies score horribly on the Bayley test, while urban Chinese children score higher than average. This seems to be much of the issue.</p><p> For the sake of China and the world, Rozelle hopes that China will be able to fix its human capital crisis. He is somewhat bullish: the Chinese government is beginning to recognize that this is an issue, and some sane policies have been implemented. But he wants more to be done. Maybe China can pull off an economic miracle again, but this seems to be a major roadblock.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ppwxCMWC3BE7b4SPa/book-review-invisible-china#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ppwxCMWC3BE7b4SPa/book-review-invisible-china<guid ispermalink="false"> ppwxCMWC3BE7b4SPa</guid><dc:creator><![CDATA[Yudhister Kumar]]></dc:creator><pubDate> Sat, 14 Oct 2023 21:51:10 GMT</pubDate> </item><item><title><![CDATA[Book Review: Radical Markets]]></title><description><![CDATA[Published on October 14, 2023 9:41 PM GMT<br/><br/><p> This is a book review of <i>Radical Markets</i> , taken from my blog <a href="https://ykumar.org/radical-markets/">Random Walks</a> . I summarize and critique each chapter to varying extents. I give the book a rating of <strong>2/5,</strong> and recommend reading the first two chapters.</p><hr><p> <i>Radical Markets? More like Activist Markets</i></p><p> The <a href="https://www.atlasfellowship.org">Atlas Fellowship</a> partners with <a href="https://www.impactbooks.store">Impact Books</a> to give fellows access to a curated collection of books worth reading. One of these books is <i>Radical Markets</i> . The first two chapters are worth reading, the rest are not.</p><p> Posner &amp; Weyl make five proposals to &quot;uproot capitalism and democracy for a just society&quot;:</p><ul><li> partial common ownership of land, mediated by auctions,</li><li> quadratic voting,</li><li> individual visa sponsorship,</li><li> demonopolizing capital, but only &#39;within industries&#39;,</li><li> treating data as labor.</li></ul><p> Each has its own chapter, and each chapter is meant to stand alone as a defense of its policy. Yet the latter three fall flat.</p><p> What <i>Radical Markets</i> does well: a coherent exposition of Georgism, &#39;radical&#39; applications of auctions to the uninitiated, and the first decent defense of quadratic voting from first principles I&#39;ve read. What it falls victim to: left-right political reductionism, one-size-fits-all solutions to coordination problems, and hefty claims partnered with weak evidence.</p><p> Links to each chapter review:</p><ul><li> <a href="#property-as-monopoly">Property as Monopoly</a></li><li> <a href="#radical-democracy">Radical Democracy</a></li><li> <a href="#uniting-the-worlds-workers">Uniting the World&#39;s Workers</a></li><li> <a href="#dismembering-the-octopus">Dismembering the Octopus</a></li><li> <a href="#data-as-labor">Data as Labor</a></li></ul><h1> Property as Monopoly</h1><p> The first chapter is the best chapter. It popularizes the Common Ownership Self-assessed Tax (COST) (aka the <a href="https://en.wikipedia.org/wiki/Harberger_Tax">Harberger Tax</a> ) as a plausible mechanism by which an auction-based property market could be implemented. Coupled with concrete policy proposals and effect analyses, it is a self-contained introduction to modern-day Georgism and disrupting the current &#39;property monopoly&#39; status quo.</p><p> What is a COST? In Harberger&#39;s own words:</p><blockquote><p> <i>If taxes are to be levied...on...the value of ... properties ... it is important that assessment procedures be adopted which estimate the true economic value ... The economist&#39;s answer... is simple and essentially fool-proof: allow each...owner.. to declare the value of his own property, make the declared values ... public, and require that an owner sell his property to any bidder... willing to pay... the declared value. This system is simple, self-enforcing, allows no scope for corruption, has negligible cost of administration, and creates incentives, in addition to those already present in the market, for each property to be put to that use in which it has the highest economic productivity.</i></p><p> - Arnold Harberger, Chile 1962</p></blockquote><p> Essentially, tax properties based on their value as assessed by the property owner. To ensure that the self-assessed values are accurate representations of the owners&#39; valuation, make all of the valuations public and mandate that a property be sold if someone outbids the declared value.</p><blockquote><p> While Harberger designed his scheme as a way to raise government revenue, it offers an inspired solution to the monopoly problem we highlighted above.</p></blockquote><p> Setting the tax rate requires trading off between allocative and investment efficiency wrt the properties being taxed. If the COST is set at the turnover rate (the probability at which the asset changes hands) then on the margin the property owner&#39;s gains from price increases are exactly offset by tax increases -- incentivizing accurate valuations. This maximizes allocative efficiency.</p><p> However, such a high tax rate disincentivizes investment. Consider the case of a property owner who by investing $20,000 in his home can raise its value from $40,000 to $70,000. If this home has a turnover rate of 50%, and the COST is set to 50%, this owner will not invest in his home. A $10,000 profit combined with a $15,000 tax increase is not a good financial decision.</p><p> Disincentivizing property investment is bad. Private property rights are <i>meant</i> to avert the tragedy of the commons and encourage investment. Any alternative will have to do the same. As such, the socially optimal COST rate is less than the turnover rate.</p><p> The socially optimal property COST is non-zero though!</p><blockquote><p> One might assume that the loss in allocative efficiency would offset the gain in investment efficiency. However -- and this is a key point -- the opposite happens. When the tax is reduced incrementally to improve investment efficiency, the loss in allocative efficiency is less than the gain in investment efficiency... In fact, it can be shown that the size of the social loss from monopoly power grows quadratically to the extent of this power. Thus, reducing the markup by a third eliminates close to 5/9...of the allocative harm from private ownership.</p></blockquote><p> One can conceptualize higher COSTs as greater public ownership of private property, as they transfer use value to the public and increase the number of possible owners of the property. In some sense, higher COSTs mean a freer market -- more participation, more competition.</p><p> How could COSTs be implemented? Countless details would have to be worked out, most in practice, but the salient issues seem to be setting the correct tax rates and developing the necessary infrastructure for frictionless transactions. While the latter is mostly a technological and system design issue (an engineering problem! just an engineering problem), the former is an ideological one. What is the optimal tradeoff?</p><blockquote><p> For typical assets, we estimate that turnover once every fourteen years is reasonable and thus (combined with other factors below) a 7% tax annually is a good target.</p></blockquote><p> Posner &amp; Weyl probably get their 7% from the <a href="https://archive.ph/BqOSG">median length of American homeownership</a> being 13.2 years as of 2022. Note that this is essentially setting the COST at the turnover rate, maximizing allocative efficiency at the cost of investment incentivization. They then claim that:</p><blockquote><p> At the tax rate we advocate, asset prices would fall by between a third and two-thirds from their current level. In popular and congested areas like San Francisco and Boston, where very modest houses sell for $600,000 or more, their price could fall to as low as $200,000.</p></blockquote><p> Where are these numbers coming from? It seems <i>plausible</i> that prices would fall this much, considering that a 7% COST with a 14 year turnover rate would tax the owner $588,000 and as such they would be incentivized to lower the price of their home to find other buyers faster, leading to lower turnover rates overall and higher profits for owners. Unclear what the underlying model is though.</p><p> An important point that they make in footnote 47 of Chapter 1:</p><blockquote><p> To make our account vivid we discuss some examples of personal possessions of individuals, like homes and cars, but the reader should keep in mind that most assets are owned by businesses and thus much of the participation and benefits from a COST would be through business assets.</p></blockquote><p> The authors propose starting with implementing COSTs for publicly held assets to enter the private market. For instance, spectrum licenses:</p><blockquote><p> ...redesigning spectrum licenses to include a COST-based license fee would solve [the current misallocation of American spectrum] and could be implemented in a variety of ways consistent with existing FCC rules. This approach, which they call &quot;depreciating licenses,&quot; would address many recent complaints about license design for the newly available 3.5 GHz bands of the spectrum; their small geographic scope and short durations under current plans were intended to maximize flexibility but may undermine investment incentives.</p></blockquote><p> COSTs on Internet domains, grazing rights, and natural resource leases to name a few also make similar amounts of sense. But, a much broader implementation would reap much greater rewards:</p><blockquote><p> As we noted above, the economy underperforms by as much as 25% annually because of the misallocation of resources to low productivity firms. A fully implemented COST could increase social wealth by <strong>trillions of dollars</strong> [emphasis mine] every year...</p><p> At the rate of roughly 7% annually that we imagine being near-optimal, a COST would raise roughly 20% of national income. About half of that money would suffice to eliminate all existing taxes on capital, corporations, property, and inheritance...and to wipe out the budget deficit and significantly reduce debt, further stimulating investment.</p></blockquote><p> Where are these numbers coming from?? I don&#39;t doubt that the gains would be large, or that this would be a more efficient taxation regime, but where is the substantiation?? The models are neither in the text of the book nor in the footnotes, and I don&#39;t think it should be on the reader to fact check these claims?</p><p> Regardless of this chapter&#39;s issues with providing evidence commensurate to its claims, the underlying proposal of a COST is innovative and, according to John Halstead, perhaps <a href="https://archive.ph/V1FxX">the most serious intellectual challenge to the idea of private property in history</a> . Worth the read.</p><h1> Radical Democracy</h1><p> In two words: quadratic voting.</p><blockquote><p> In this chapter, we will show that these two elements -- the capacity to save up voting power, and the square root function -- would be a much-needed cure to the pathologies of the traditional voting systems used in democracies.</p></blockquote><p> Why <i>quadratic</i> voting? Why not giving people votes proportional to the cube root of their credits, or the logarithm of their credits? What property does a quadratic have that no other function does?</p><p> <i>Its derivative is linear.</i> No other function would make the <i>marginal</i> cost of casting an extra vote proportional to the number of votes cast. We make decisions <i>on the margin</i> -- if one voter cares three times as much as another voter about an issue, they will be willing to pay three times as much for an extra vote, and as such buy three times as many votes than the other. No other function would lead to the same outcome.</p><blockquote><p> QV achieves a perfect balance between the free-rider and the tyranny of the majority problems. If the cost of voting increased more steeply, say, as the fourth power of votes cast, those with strong preferences would vote too little and we would revert to a partial tyranny of the majority. If the cost of voting increased more slowly, those with intense preferences would have too much say, as a partial free-rider problem would prevail.</p></blockquote><p> <i>Radical Markets</i> was my first introduction to this explanation, and for this I am grateful. I think this chapter is worth reading just for this, but your mileage may vary.</p><p> QV in practice has seen limited adoption. A version is in <a href="https://www.ccn.com/can-ethereum-based-akasha-revolutionize-social-networks/">Akasha</a> (a Ethereum based blockchain application) where money is the currency by which voters buy their votes. Paying for votes has obvious issues in today&#39;s world -- perhaps if the world was much more egalitarian, then this would be a better system than one person, one vote.</p><p> There is good reason to believe that QV mitigates polarization as a result of allowing individuals to vote more for issues they care more about. Implementation runs into a host of engineering challenges (how to make the UI easy to use &amp; not require excessive amounts of thinking) but I am optimistic that they are solvable.</p><p> I doubt this will gain much traction for public elections at the state and national level. It could be useful and innovative for for local and private governance. Notably, the Colorado House Democratic caucus <a href="https://www.bloomberg.com/news/articles/2019-05-01/a-new-way-of-voting-that-makes-zealotry-expensive?leadSource=uverify%20wall">used it in 2019</a> to decide on their legislative priorities, and <a href="https://www.economist.com/open-future/2019/03/12/inside-taiwans-new-digital-democracy">Taiwan has flirted with QV</a> in their digital democracy.</p><h1> Uniting the World&#39;s Workers</h1><p> Subtitled: <i>REBALANCING THE INTERNATIONAL ORDER TOWARD LABOR</i></p><p> The basic argument: economic freedom is necessary for economic growth. Gains from trade have been the primary engine by which this occurs, but as intranational inequality is less severe than international inequality gains from migration should be considered instead. Expanding existing legal migration channels is insufficient and possibly detrimental to natives. Therefore, countries should auction visas and let individuals sponsor migrants. They dub this the Visas between Individuals Program, or VIP.</p><p> Bryan Caplan thinks this is his <a href="https://www.econlib.org/radical-markets-in-immigration/">open borders proposal in disguise</a> . I think it&#39;s where the book starts going off the deep end.</p><p> I agree with the claim that low-skilled migrants are likely a net drain on the state. As migrants are more likely to work informally and typically send large proportions of their income outside the country, they pay less taxes and consume less than natives. Naturally, this facilitates resentment (amongst a multitude of other cultural reasons, etc.), typically in existing rural, low-income, and more conservative regions.</p><p> How is the VIP going to fix this issue, exactly?? Posner and Weyl think that the money generated by the visa auction system will be enough:</p><blockquote><p> Suppose that OECD countries accepted enough migration to increase their populations by a third. Suppose too that migrants on average bid $6,000 per year for a visa. This sum seems plausible that Mexican migrants to the United States gain more than $11,000 annually under the current highly inefficient system. Average GDP per capita in OECD countries is $35,000, so this proposal would boost the national income of a typical OECD country by almost 6%, comparable to their growth in real income per person in the last five years.</p></blockquote><p> This is <i>highly</i> optimistic, and assumes that both the demand and quota for visas is at least equivalent to the population of a given OECD country. For America, that&#39;s <i>300 million</i> migrants willing to pay <i>$6,000</i> for a visa <i>per year.</i> This won&#39;t even give you 6% income growth because the GDP per capita in America is so high.</p><p> The population of Mexico isn&#39;t even 300 million! The likelihood of most migrants having $6,000 on hand is almost nil! And this money goes to the government, not to private individuals directly -- many individuals will not make the connection between increased funding for public services and the <i>massive, massive</i> amounts of migrants this proposal would apparently bring.</p><p> Even if migrants would arrive in the numbers that the authors propose, it would utterly, completely, irreversibly change the cultural landscape of the developed world. We are considering migration in numbers such that for every native American there would be a non-native migrant -- statistically low-skilled and with poor English. All the typical conservative arguments migration would be multiplied ten-fold, and they would have a point.</p><p> But individuals can sponsor visas! Moreso, they can sponsor migrants and get a cut of their earnings such that the migrant earns less than minimum wage! And this isn&#39;t wage slavery because the migrant would be making more than he would have in his home country, so it makes sense!</p><blockquote><p> The risk of exploitation is minimal because foreign workers are protected by the same health, safety, labor, and employment laws that Americans benefit from, and foreign workers can return to their home country if the employer mistreats them.</p></blockquote><p> Who <i>enforces</i> these health and safety laws? Americans. What are the economic benefits from arriving from, say, Haiti to the United States? About thirty-fold? What is the likelihood that the system would be abused, ala the Jim Crow South? And even then, what is the likelihood that the migrant would <i>voluntarily return to their home country??</i></p><blockquote><p> Many people may object to this system. Perhaps to some readers it is uncomfortably similar to indentured servitude, <strong>even though migrants would be free to leave at any time.</strong> Or perhaps it just seems exploitative.</p></blockquote><p> YES.</p><p> Oh, but don&#39;t worry about any of this, because 100 million people will want to sponsor visas.</p><blockquote><p> Imagine then, that 100 million people sponsor migrant workers. Currently, there are about 45 million foreign-born people in the United States. Of those, about 13 million are legal noncitizens, and 11 million are illegal aliens. If our program replaced existing migrant worker visas [which it wouldn&#39;t], the number of migrant workers would increase dramatically, from 24 million to 100 million, but <strong>not in a way that would disrupt society and overwhelm public services.</strong> [sure] It would leave the ratio of foreign-born to natives in the United States below the numbers in even the most restrictive GCC countries.</p></blockquote><p> To be clear, 22.7% of Americans would be foreign born, or a little bit less than one in every four American residents would be migrants. Yes, this is a <i>potentially sustainable</i> foreign population, but at once? These GCC countries being used as a reference class are members of the <i>Gulf Cooperation Council.</i> Members include the UAE, Qatar, Kuwait, Bahrain, Oman, and Saudi Arabia. Countries positively praised for their labor rights record.</p><p> A more sane reference class would be Australia and New Zealand, which have about one foreigner for every two natives (admittedly, which are mentioned as examples). Where did they come from? According to <a href="https://en.wikipedia.org/wiki/Immigration_to_Australia">Wikipedia</a> :</p><blockquote><p> After World War II Australia launched a massive immigration program, believing that having narrowly avoided a Japanese invasion, Australia must &quot;populate or perish&quot;. Hundreds of thousands of displaced Europeans migrated to Australia and over 1,000,000 British subjects immigrated under the Assisted Passage Migration Scheme... The scheme initially targeted citizens of all Commonwealth countries; after the war it gradually extended to other countries such as the Netherlands and Italy.</p></blockquote><p> In other words, Australia heavily enouraged white immigration (see the <a href="https://en.wikipedia.org/wiki/White_Australia_policy">White Australia policy</a> ) until the early 1970s, at which point it was comfortable accepting large amounts of migrants. <i>Skilled</i> migrants, I might add. There are only about 1.5 million temporary migrants in Australia (compared to about 7.5 million total foreign-born Australian residents), which is an imperfect yet decent upper bound on the number of low-skilled Australian migrants.</p><p> There is no developed country which has managed to have a large foreign-born low-skilled migrant population enter over a short period of time and provide them the same standard of living or the same rights as natives. The GCC has similar proportions of low-skilled workers and an atrocious human rights record; Australia and New Zealand manage to treat migrants decently but with a predominantly high-skilled population.</p><p> These are arguments against any open-borders policy that advocates for a large influx of low-skilled immigrants over a short period of time. But the VIP is <i>doubly</i> stupid because it places the liability of migrant well-being on <i>low-skilled natives,</i> the very same people who are the least well equipped to handle it.</p><p> Anthony is a native Ohioan, Bishal is a hypothetical migrant under the VIP:</p><blockquote><p> In our case, Anthony will be required to obtain basic health insurance for Bishal before he arrives (though this would come out of Bishal&#39;s earnings). If Bishal is unable to find work, Anthony must support him for as long as he remains in the country...If Bishal commits a crime, he will be deported after serving his sentence; Anthony will be required to pay a fine. If Bishal disappears, Anthony will also be fined. We do not think that the fine needs to be large, but it should sting.</p></blockquote><p> <i>100 million Americans</i> are supposed to sign up for this??</p><p> If you want a defense of open borders &amp; greatly expanded immigration, you can read Caplan&#39;s comic book. Not this.</p><h1> Dismembering the Octopus</h1><p> This chapter advocates for letting BlackRock invest in Uber but not Lyft, Coca-Cola but not Pepsi, and McDonald&#39;s but not KFC. Anti-trust for institutional capital. A good idea in theory, but I have doubts about the practical implementation.</p><blockquote><p> Who are the institutional investors, anyway? They include companies that manage mutual funds and index funds, asset managers, and other firms that buy and hold equities on behalf of their customers. The largest names are those we mentioned above: Vanguard, BlackRock, State Street, and Fidelity.</p></blockquote><p> 26% of the public American stock market is held by institutional investors. As institutional investors are the only single entities with large enough amounts of capital to own significant portions of centibillion dollar companies, they have an outsized influence on industry policy. And because they own significant portions of companies which are nominally supposed to be competing in the same industries, it is in their interest to stifle competition within those industries, creating pseudo-cartels negatively affecting consumers.</p><p> As an example, look at banking. BlackRock, Vanguard, State Street, and Fidelity are in the top five largest shareholders of JP Morgan Chase, Bank of America, Citigroup, Wells Fargo, US Bank, <i>and</i> PNC Bank. If the same entities own the companies that are nominally supposed to be competing, the incentives to compete are diminished &amp; monopolistic tendencies being to crop up. Think of Rockefeller after Standard Oil broke up -- he still was the owner of the subsidiary companies in each state, and this allowed him to multiply his wealth far beyond what would have been possible if Standard Oil stayed as one company.</p><p> Monopolies are bad for consumers! This is what anti-trust law was made to combat! We already bring anti-trust litigation against firm mergers which will increase market concentration, why not do the same for capital investments?</p><blockquote><p> A simple but Radical reform can prevent this dystopia: ban institutional investors from diversifying their holdings within industries while allowing them to diversity <i>across</i> industries. BlackRock would own as much as it wants of (say) United Airlines, but it would own no stake in Delta, Southwest, and the others. It would also own as much as it wants of Pepsi, but not Coca-Cola and Dr. Pepper. And it would own as much as it wants of JP Morgan, but none of Citigroup and the other banks...</p><p> Our approach can be stated as a simple rule:</p><p> No investor holding shares of more than a single effective firm in an oligopoly and participating in corporate governance may own more than 1% of the market.</p></blockquote><p> I agree with this policy, in principle. However, it seems difficult to disambiguate industries cleanly. In today&#39;s world, where a relatively small number of massive corporations control disproportionate amounts of the economy, it seems at least somewhat viable. But what of venture capital firms investing in multiple competing startups? Would they be required to divest from one of them if both become unicorns? How big would an industry have to be for the FTC to care about regulating it?</p><p> The authors raise the issue of the lack of usage of anti-trust in local markets. For instance:</p><blockquote><p> ...sociologist Matthew Desmond suggests that landlords in poor neighborhoods often buy up enough housing to have substantial power to drive up rents by holding units vacant and artificially depressing supply. Yet as far as we know, no antitrust case has ever been brought against such local but potentially devastating attempts at monpolization.</p></blockquote><p> Yes, property is easy to point to as an instance of monopolistic behavior ruining lives. I am wary, however, of giving regulators even more power than they currently have. Property market regulations seem to be, on the whole, good and necessary (or at least, good regulation has the potential to have a large positive impact). But letting regulators control the behaviors of small businesses seems bad? Perhaps giving business owners more leeway to bring civil suits against others engaging in anti-competitive practices is the way to go.</p><p> Finally, the authors address the digital economy:</p><blockquote><p> Antitrust authorities, who are accustomed to worrying about competition within existing, well-defined, and easily measurable markets, have allowed most mergers between dominant tech firms and younger potential disrupters to proceed. Google was allowed to buy mapping startup Waze and artificial intelligence powerhouse DeepMind; Facebook to buy Instagram and WhatsApp; and Microsoft to buy Skype and Linkedin.</p></blockquote><p> Their solution:</p><blockquote><p> To prevent this dampening of innovation and competition, antitrust authorities must learn to think more like entrepreneurs and venture capitalists, seeing possibilities beyond existing market structures to the potential markets and technologies of the future, even if these are highly uncertain.</p></blockquote><p> Much easier said than done. The day a government regulating body has the same market foresight as the entrepreneurs and investors in the market, I will eat my hat.</p><p> Meh. Not as groundbreaking as the first two proposals, and not as patently idiotic as the third. It is surprising that they advocate for a drastic expansion in market regulation in a book nominally focused on relying on free market forces to bring about good, but oh well.</p><p> (this is where the <i>Activist Markets</i> label came from)</p><h1> Data as Labor</h1><p> <i>Pay people for their data.</i> Never mind that the marginal benefit of your data to any given firm is negligble. Because machine learning systems need large amounts of data and have spiky loss curves, diminishing marginal returns for data doesn&#39;t count here!</p><p> (I am not joking, this is the argument Posner &amp; Weyl make)</p><blockquote><p> However, if later, harder problems are more valuable than earlier, easier ones, then data&#39;s marginal value may increase as more data become available. A classic example of this is speech recognition. Early ML systems for speech recognition achieved gains in accuracy more quickly than did later systems. However, a speech recognition system with all but very high accuracy is mostly useless, as it takes so much time for the user to correct the errors it makes. This means that the last few percentage points of accuracy may make a bigger difference for the value of the system than the first 90% does. The marginal value grows to the extent that it allows this last gap to be filled.</p></blockquote><p> Maybe the concept of <i>scale, scale, scale</i> wasn&#39;t as prominent in their time as it is nowadays? Two issues: model capabilities grow logarithmically with the amount of data they are trained on, and oftentimes getting from 99% to 99.9% requires as much data as getting from 0% to 99%. The idea that the marginal datapoint has anything but negligible value is absurd.</p><p> Paying people for data that&#39;s used to finetune models, sure. Paying people to RLHF models, sure. Paying people for data that&#39;s used in massive datasets -- in principle, this should probably happen, but this will come <i>nowhere near</i> to providing individuals with an income equivalent to their job that has just been automated.</p><blockquote><p> To make a ballpark estimate of what gains we might expect, we suppose that over the next twenty years, AI that would (absent our proposal) not pay data providers comes to represent 10% of the economy. We further assume that the true share of labor if paid in this area of the economy is two-thirds, as in the rest of the economy; and that paying labor fairly expands the output of this sector by 30%, as seems quite reasonable given productivity gains accompanying fairer labor practices in the early twentieth century. Then our proposal would increase the size of the economy by 3% and transfer about 9% of the economy from the owners of capital to those of labor.</p></blockquote><p> Fundamentally, the authors miss the point that AI will make labor substitutable with non-human entities. The world in five years will likely use datasets generated by AI systems, simply because the Internet is too small for the capabilities of the models we want to build. Paying people for their data is not an alternative to UBI in an increasingly automated economy.</p><p> I would critique this chapter more, but it seems like a reasonable proposal to make in the world before ChatGPT and before people realized that scaling laws held to this extent. Still not worth reading in 2023.</p><br/><br/> <a href="https://www.lesswrong.com/posts/R9QmJJkdTBgX2wNkt/book-review-radical-markets#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/R9QmJJkdTBgX2wNkt/book-review-radical-markets<guid ispermalink="false"> R9QmJJkdTBgX2wNkt</guid><dc:creator><![CDATA[Yudhister Kumar]]></dc:creator><pubDate> Sat, 14 Oct 2023 21:41:48 GMT</pubDate> </item><item><title><![CDATA[One-on-one tutoring for any subject]]></title><description><![CDATA[Published on October 14, 2023 8:58 PM GMT<br/><br/><p> I occasionally find myself wanting to directly contact someone with expert understanding of something - some software, some topic - to ask a few specific questions and have a brief discussion about an idea, a workflow, etc. Naturally, I&#39;d be happy to pay for such a service.</p><p> Stack Overflow is all well and good, but I am looking to have a longer discussion / more precise and detailed feedback about something specific that I&#39;m working on. Most recently I&#39;ve wanted to talk to an expert in Adobe InDesign to check that some process I have set up is the right way to go about things. In the past I&#39;ve wanted to get advice and feedback on the overall architecture of a small project&#39;s code base.</p><p> I haven&#39;t been able to find a generic &quot;contact an expert&quot; website or serve out there. Has anyone else?</p><br/><br/> <a href="https://www.lesswrong.com/posts/vbkxboCxHpzyEk5jj/one-on-one-tutoring-for-any-subject#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/vbkxboCxHpzyEk5jj/one-on-one-tutoring-for-any-subject<guid ispermalink="false"> vbkxboCxHpzyEk5jj</guid><dc:creator><![CDATA[yakimoff]]></dc:creator><pubDate> Sat, 14 Oct 2023 21:00:55 GMT</pubDate></item></channel></rss>