<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 22 日星期五 20:12:02 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Review Report of Davidson on Takeoff Speeds (2023)]]></title><description><![CDATA[Published on December 22, 2023 6:48 PM GMT<br/><br/><p>我花了一些时间研究<a href="https://www.lesswrong.com/users/tom-davidson-1?from=post_header"><u>Tom Davidson</u></a>的开放慈善报告，了解<a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">以计算为中心的框架对人工智能起飞速度的看法</a>。这项研究最终形成了一份报告，我将其提交给戴维森。</p><p>在他的鼓励下，我发布了我的评论，其中为延长初始报告中提出的中位数时间表提供了五个独立的论点。</p><p><strong>执行摘要（阅读时间少于 5 分钟）</strong>涵盖了每个论点的关键部分。后面更完整的部分中的额外证实可以根据需要进行更深入的研究。这项工作假设您熟悉戴维森的工作，但<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Appendix_A__Background_Info_on_Davidson_2023_s_Central_Argument">背景信息附录</a>总结了中心论点。</p><p>感谢汤姆鼓励公开分享这一点，并感谢他致力于公开讨论这些重要话题。很高兴听到大家的反馈。</p><p>特别感谢 David Bloom、Alexa Pan、John Petrie、Matt Song、Zhengdong Wang、Thomas Woodside、Courtney Zhu 等人对本文的评论、反思和见解。他们在将这些想法发展到当前状态方面提供了令人难以置信的帮助。任何错误和/或遗漏都是我自己造成的。</p><h1>执行摘要（&lt;5 分钟阅读）</h1><p>本次审查为延长戴维森初次报告中提出的中位时间表提供了五个独立论据。</p><p>这些论点并没有攻击该作品核心的飞轮。它们不会挑战日益复杂的人工智能的发展，也不会挑战随着时间的推移投资、自动化以及研发进展的复合（如此<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Key_Flywheel_for_Accelerated_AI_Growth___Automation">处</a>总结）。</p><p>但它们确实对报告提出的时间表产生了影响。具体来说，他们描述了可能减缓人工智能驱动的自动化进程的结构性技术和地缘政治障碍。</p><p>这些关键论点涉及：</p><ol><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#I__Dataset_Quality"><u>数据集质量</u></a><u>。</u></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#II__The_Abstract_Reasoning_Common_Sense_Problem"><u>抽象推理/常识问题</u></a><u>。</u></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#III__GDP_Growth___Measurement">GDP 增长与衡量</a>。</li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#IV__R_D_Parallelization_Penalty">研发并行化惩罚</a>。</li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#V__Taiwan_Supply_Chain_Disruption">台湾供应链中断</a>。</li></ol><p>这些论点是相互独立发展和运作的。也就是说，参数 1 不依赖于参数 2-5，参数 2 不依赖于参数 1 和 3-5，等等。</p><p>依次总结一下：</p><h2><strong>一、数据集质量</strong></h2><p>数据质量是<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Why_Data_Quality_Matters_"><u>模型性能的关键决定因素</u></a>。数据集质量下降会严重损害模型性能并阻碍模型开发/性能前沿。现状对数据集质量的<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Obstacles_to_Data_Quality">威胁</a>包括：</p><ol><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Generative_data_pollution_of_the_internet_corpus_fundamental_to_frontier_model_development__"><u>生成数据污染了对前沿模型开发至关重要的互联网语料库。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#2__Copyright_lawsuits_will_limit_datasets_"><u>版权/知识产权诉讼限制大型数据集的使用。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#3__Labor_negotiations_and_lobbying_in_certain_sectors__unions_in_creative_work__powerful_interest_groups_in_health__will_limit_dataset_creation_and_use__"><u>某些部门的劳工谈判和游说（创意工作的工会、医疗保健领域的强大利益集团等）极大地限制了数据集的创建和使用。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#4__Sources_of_Record_Preventing_Future_Scraping_or_Pulling_Their_Content"><u>记录来源可防止将来抓取/提取其内容。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#5__Time_Delays_Affiliated_With_Each_Any_Of_The_Above_"><u>与上述任何情况相关的时间延误。</u></a></li></ol><p>这些延迟可能<strong>会使时间表倒退数年</strong>和/或<strong>使许多部门在功能上不受自动化影响</strong>。</p><h2><strong>二.抽象推理/常识问题</strong></h2><p>该报告的关键前提之一是，向 2020 时代的模型添加更多计算可以解决自动化面临的挑战。但结构性障碍使得 2020 时代的模型无法可靠地<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Abstraction_as_a_Necessary_Condition_"><u>执行经济中许多工作自动化所必需的</u></a><a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Mission_Critical___Safety_Critical_Functions_"><u>安全和关键任务任务</u></a>。</p><p>特别是，必须解决抽象/泛化问题。如果没有抽象，这些工具就无法管理安全关键型作业。相反，对于仍处于控制之中的人类来说，它们仍将是影响倍增器。虽然这些挑战总有一天会得到解决，但它们无法在今天的 SOTA 内克服。因此，创造这些下一个突破可能会成为阻碍报告时间表的瓶颈。</p><p>这个“常识”问题<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Studies_Suggest_Common_Sense_Shortcomings"><u>仍有待解决</u></a>（ <a href="https://www.amacad.org/publication/curious-case-commonsense-intelligence"><u>Choi，2022</u></a> ）。缺乏经验证据表明法学硕士可以进行任何类型的“抽象”、“概括”或“常识”（Mitchell 等人，2023； <a href="https://www.amacad.org/publication/curious-case-commonsense-intelligence"><u>Choi，2022</u></a> ）。最近的许多研究还表明这些缺点仍然存在，如以下方面的弱点所暴露的那样：</p><ol><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Formal_Logic___Math_"><u>形式逻辑与数学</u>。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#2__Correlation___Causation_"><u>相关性和因果关系</u>。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#3__Spatial_and_Semantic_Concepts__"><u>空间和语义概念</u>。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#4__Counterfactual_Reasoning_"><u>反事实推理。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#5__Generalizing_Beyond_Training_Distribution"><u>超越训练分布的泛化</u>。</a></li></ol><p>造成这种赤字的结构性原因包括：</p><ol><li>深度学习<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Correlation_____Causation"><u>从根本上依赖于相关性</u>。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#2__Overparameterization__Overfitting_on_Randomly_Initialized_Parameters"><u>过度参数化大型模型</u>背后固有的随机性。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#3__Limitations_Preventing_Placement_of_Content_into_Training_Distributions"><u>将所有内容都放在训练分布中的固有局限性</u></a>是由于：<ol><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#a__The_data_doesn_t_exist__"><u>互联网规模数据集的缺点</u></a>。</li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#b__The_high_cost__human__financial__and_otherwise__of_building_such_datasets_"><u>创建某些基本事实的高昂成本（人力、财务和其他）</u> 。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#c__The_Edge_Case_Problem__Combinatorial_Explosion_of_Real_World_Scenarios___The_Long_Tail_of_Exceptions"><u>边缘情况（又名现实世界场景的组合爆炸）</u> 。</a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#d__The_lack_of_recorded_expert_level_data__and_the_challenge_of_ever_recording_and_conveying_such_knowledge__"><u>缺乏记录的专家级数据，以及记录和传达此类知识的挑战</u>。</a></li></ol></li></ol><p>这些技术障碍挑战了在 2020 时代的模型中添加计算可以解决自动化面临的问题的前提。虽然这些挑战有一天<i>会</i>得到解决，但在当今的技术水平 (SOTA) 内无法克服它们。确定这些下一个突破可能会出现一个瓶颈，在不确定的时间内限制报告的时间表。进展可能无法作为一个连续函数来衡量。<strong>相反，它可能会遇到硬性的不连续性，功能上的“暂停”，而这些技术问题必须得到解决。</strong></p><p>与此同时，这些缺点使人工智能驱动的工具（对于大多数工作而言）只能成为对人类影响倍增的工具，而不是替代员工。</p><h2><strong>三． GDP 增长与衡量</strong></h2><p>GDP增长是<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Framing__GDP_Growth_as_a_Necessary_Component_of_the_Argument_"><u>该报告论点的必要组成部分</u></a>。该报告声称，自动化将令人难以置信地快速增加 GDP。然而，三种力量会降低 GDP 增长的百分比。这种缓解措施减少了可用于研发和投资的资金量，进而减慢了人工智能的开发时间。</p><ol><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#A___Good_Enough_to_Substitute_Profitably_AI___GESPAI__Definitions_Around__Readily_Perform__Could_Distort_Findings_"><u>“足以盈利地替代”人工智能（GESPAI）</u></a> 。人类可以生产比 GESPAI 替代品多得多的产品，但如果他们的附属成本使每个人的利润变得不那么有价值，那么他们的公司的利润仍然较低。因此，自动化经常发生在低于人类水平的基线，导致总产量减少。</li><li>由于<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Increase_in_Labor_Supply__All_Else_Equal__Drives_Wages_Down_"><u>劳动力供应的功能性增加</u></a>和鲍莫尔效应，任何成功自动化的行动本质上都会在<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#B__Anything_Successfully_Automated_Inherently_Becomes_a_Smaller_Percentage_of_GDP___">GDP 中所占的比例较小</a><a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#2__Baumol_Effect_increases_wages_for_other_jobs_that_do_not_experience_productivity_growth__decreasing_the_unit_value_of_a_task__"><u>。</u></a></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#C__Displacement_of_Labor_Creates_Market_Inefficiencies_"><u>劳动力的取代</u></a>导致市场效率显着低下，从而降低了自动化带来的总体增长。</li></ol><p>这些抑制效应的影响各不相同，但它们表明任何拟议的 GDP 两倍增长都应减少到较小的倍数。<strong>即使将 2 倍变为 1.5 倍，也会对模型产生巨大影响</strong>，从而降低飞轮上每个节点的功率：</p><ol><li>研发投资和购买更多计算资源减少。</li><li>硬件和软件创新都需要更长的时间。</li><li>我们最大的训练在每个时间范围内都较小。</li><li>由此产生的人工智能模型在每个时间范围内的自动化程度较低。</li></ol><h2><strong>四．研发并行化惩罚</strong></h2><p>在<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#A__Using_The_Cited__ML_Specific_Research__Besiroglu__2020__"><u>特定于 ML 的上下文</u></a>中以及从<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#B__First_Principles_Reasons_for_High_R_D_Parallelization_Penalties_"><u>第一原理</u></a>出发对并行化惩罚的研究表明，并行化惩罚应显着大于报告中的默认规定。</p><p>在其他条件相同的情况下，这种增加的并行化惩罚可能<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Increased_Parallelization_Penalty__All_Else_Equal__Adds_5_Years_to_Timeline_"><strong><u>会使报告的时间表延长五年</u></strong></a><u>，</u>将最有可能的 100% 自动化里程碑从 2040 年推迟到 2045 年。</p><h2><strong>五、台湾供应链中断</strong></h2><p>台湾台积电是训练和运行 SOTA AI 模型所需硬件建设的关键节点。中国对台湾的军事入侵将使<strong>时间表推迟</strong><a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#An_Invasion_of_Taiwan_Would_Set__SOTA_Innovation_Back_by_Years_"><strong><u>20 个月</u></strong></a>，直到另一家公司“赶上”台积电目前的复杂程度。随着习近平巩固更多权力以及中华人民共和国军事力量的增强，这种入侵的可能性越来越大。</p><p> <a href="https://foreignpolicy.com/2023/04/13/china-attack-taiwan-war-expert-poll-biden/#cookie_message_anchor"><u>2023 年 4 月对国际关系专家学者的一项调查</u></a>估计，“明年中国对台湾发动攻击的平均可能性为 23.75%”。像<a href="https://www.metaculus.com/questions/11480/china-launches-invasion-of-taiwan/?sub-question=10880"><u>Metaculus</u></a>这样的预测市场不那么乐观，但仍然预测到 2035 年“全面入侵”的可能性为 40%，到 2030 年为 30%。</p><p> ~~~~~~</p><h1> I. 数据集质量/稳健性</h1><p><strong>TL;DR</strong> ：除了人工智能硬件或人工智能软件的任何技术能力之外，数据集的实际质量值得高度关注。而且，有理由相信，数据集质量甚至不能与未来的当前水平相匹配。</p><p> “我们如何跨越有效计算差距”的基本问题体现在以下函数中：</p><blockquote><p> EC = 软件 * [FLOP/$] * [FLOP 花费的美元]</p></blockquote><p>然而，该函数在任何地方都没有提及数据集质量和持续访问数据的能力。</p><h2>为什么数据质量很重要</h2><blockquote><p>“机器学习系统的成功很大程度上取决于它所训练的数据。” <a href="https://docs.google.com/document/d/1C3dlLmFdYHJmACVkz99lSTUPF4XQbWb_Ah7mPE12Igo/edit"><u>——Huyen，2022</u></a></p></blockquote><p>数据集管理、维护和改进是大多数机器学习从业者的主要关注点（ <a href="https://docs.google.com/document/d/1C3dlLmFdYHJmACVkz99lSTUPF4XQbWb_Ah7mPE12Igo/edit#heading=h.1zj18weukkqd"><u>Huyen，2022</u></a> ； <a href="https://anand.typepad.com/datawocky/2008/03/more-data-usual.html"><u>Rajaraman，2008</u></a> ）。模型不会在真空中产生深刻的预测。相反，变压器通过基于（1）当前交易所的广泛背景和（2）训练数据集的分布对下一个最佳令牌进行相关最佳猜测来预测链中的下一个令牌。</p><p>如果没有“好”数据（即稳健、高 n、具有统计显着性并代表模型将解决的“现实世界”场景的数据），模型性能就会受到限制。</p><p>事实上，深度学习最近的蓬勃发展归功于从互联网上构建大型数据集的能力。正如顶尖学者所指出的那样，“从网络上抓取的巨大数据集”是深度学习突然爆发力量并“从 20 世纪 70 年代的死水位置中崛起”的原因（ <a href="https://arxiv.org/abs/2104.12871"><u>Mitchell，2021</u></a> ）。</p><p>主要影响：</p><ol><li>数据集是模型增长所必需的。</li><li>如果数据集不好，那么模型也不好。</li></ol><h2>数据质量的障碍</h2><p>数据集质量可能会随着时间的推移而下降，有以下几个原因：</p><h3> <strong>1. 互联网语料库的生成数据污染是前沿模型开发的基础。</strong></h3><p>数据质量受损的最严重的潜在原因是深度学习兴起背后的互联网语料库的退化。</p><p>基础模型既（1）初始训练，又（2）定期使用该训练数据集进行微调和再训练。</p><p>但是，当生成文本出版（因为它比非辅助写作更容易生成）成为大多数数据集时，这个语料库会发生什么？语料库本身可能会因生成人工智能自身反应的幻觉和低级质量而被损坏。</p><p>这种现象<a href="https://www.theverge.com/2023/6/26/23773914/ai-large-language-models-data-scraping-generation-remaking-web"><u>已经渗透到</u></a>网络中。 AI 生成的内容通过<a href="https://news.ycombinator.com/item?id=37658319"><u>Quora、LinkedIn 等网站</u></a>、大量 B2B 和 B2C 在线营销以及<a href="https://www.theverge.com/2023/5/2/23707788/ai-spam-content-farm-misinformation-reports-newsguard"><u>全新的垃圾邮件网站</u></a>充斥互联网。 <span class="footnote-reference" role="doc-noteref" id="fnrefxdo3m5g0mqc"><sup><a href="#fnxdo3m5g0mqc">[1]</a></sup></span>即使是像 LAION-5B 这样训练文本到图像模型的既定数据集，也已经利用了合成数据（ <a href="https://arxiv.org/pdf/2307.01850.pdf"><u>Alemohammad 等人，2023，p.3</u></a> ）。</p><p>这可能会产生一个问题，即生成式人工智能模型的幻觉开始被确立为互联网上该模型的“基本事实”的越来越大的份额。数据集长期质量的下降反过来又会损害未来的性能。</p><p>造成此类数据集损坏的原因有两个：</p><ol><li>将生成的幻觉插入训练数据中可能会<strong>导致未来运行中将幻觉视为“真实情况”</strong> 。这进一步拉开了“模型世界”与其试图代表的“现实世界”的距离，削弱了它的实用性。</li><li>通过添加不代表整个分布的生成数据来<strong>扭曲数据分布</strong>。第二个效应是在……中进行研究的。</li></ol><p> <a href="https://arxiv.org/pdf/2305.17493.pdf"><u>Shumailov 等研究人员。 （2023）</u></a>认为，基于综合生成的数据训练模型可能会“导致不可逆的缺陷[……]，其中原始内容分布的尾部消失”，从而导致<i>模型崩溃</i>。这种对原始分布的“遗忘”使得 GPT 的预测随着时间的推移显着降低准确性，因为变压器中下一个令牌的预测取决于利用其深层上下文来对下一个条目进行最佳猜测概率。</p><p> <a href="https://arxiv.org/pdf/2307.01850.pdf"><u>Alemohammad 等人的类似工作。 (2023)</u></a>解释说，“在合成数据上训练生成人工智能 (AI) 模型会逐渐放大现有数据架构中存在的伪影”（例如，由于初始摄影或图像存储方式的缺陷）。 <a href="https://hbr.org/2023/11/has-generative-ai-peaked"><u>Acar (2023) 的总结</u></a>称其为：</p><blockquote><p><a href="https://arxiv.org/pdf/2307.01850.pdf"><u>模型自噬紊乱（MAD）</u></a> 。它表明，根据自己的内容递归地训练生成模型会导致自我消耗循环，从而降低模型质量和多样性。简而言之，除非定期注入新鲜的、真实的人类数据，否则生成模型会变得“疯狂”。</p></blockquote><p><strong>一个直观的类比</strong>来说明为什么这种数据质量问题可能是正确的：想象一张照片 A 可以很好地代表现实世界。但照片 A（照片 B）拍摄的照片对现实世界的表现稍差一些，因为它试图通过照片 A 的缺陷的限制来表现世界。照片C是照片B拍摄的照片，更不能很好地代表现实世界。继续使用照片 D、E 等，照片的质量最终会下降。即使是高度完整性的系统（例如 90% 的表示保真度）也会在几个周期内降低到无法使用的程度。 </p><p><img style="width:34.27%" src="https://lh7-us.googleusercontent.com/eKOY4NVCDwCvIhL4l1ZygCP435E921tGEIlsrmH590x5hdDCyexB3cwLTFeFKW06OGujMVFT6Qkw2RCNmanZ3wbT4zkyFRXbreofXJ1LPmgZl4DaE_l4RCvHl6gPELY5uBpX8ZU3D836BQRCyM1cwO92TDDDnCLK7-BB7AFsp6r0LgA1qCH6q4cJiBTtbQ"></p><p>由于人类数据注释者本身对生成式人工智能工具的严重依赖，仅选择所谓的人类数据也无法解决问题。正如<a href="https://arxiv.org/pdf/2306.07899.pdf"><u>维谢洛夫斯基等人。 2023 年</u></a>发现，33-46% 的 Amazon Mechanical Turk 员工“在完成任务时使用了 LLM”。</p><p>解决这个问题需要在以下方面取得重大进展：</p><ol><li>根据自己的成果对法学硕士进行培训，而不会让他们发疯，</li><li> LLM 能够区分人类生成的数据和 LLM 生成的数据，或者</li><li>创建一些数据治理系统，将人类生成的新数据与法学硕士生成的数据隔离开来。 <span class="footnote-reference" role="doc-noteref" id="fnref754mhryql8u"><sup><a href="#fn754mhryql8u">[2]</a></sup></span></li></ol><p><i><strong><u>需要进一步研究的一个问题是：世界上是否有足够的非合成数据来训练越来越大的深度学习模型？</u></strong></i><a href="https://arxiv.org/pdf/2307.01850.pdf"><u>阿勒穆罕默德等人。 （2023）</u></a>也问这个问题，尽管他们没有提供答案。 <a href="https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset"><u>维拉洛博斯等人。 （2022）</u></a>表明，到 2026 年，我们可能会耗尽“高质量语言数据”，到 2040 年至 2060 年，我们可能会耗尽“视觉数据”，到 2030 年到 2050 年，我们可能会耗尽“低质量语言数据”。然而，他们也承认，他们的论点取决于“不切实际的假设，即机器学习数据使用和生产的当前趋势将持续下去，并且数据效率不会有重大创新。”可能需要额外询问。</p><h3> <strong>2.版权诉讼将限制数据集</strong></h3><p>训练基础模型的数据是通过略读其他人最初撰写的工作来收集的。现在围绕知识产权的大量诉讼质疑这种用途是否合法。这些诉讼的后果可能会从根本上抑制未来的发展（甚至限制现有模型的使用）。</p><p>诉讼示例包括： <a href="https://www.reuters.com/technology/more-writers-sue-openai-copyright-infringement-over-ai-training-2023-09-11/"><i><u>Chabon 等人。诉 OpenAI</u></i></a> ( <a href="https://dockets.justia.com/docket/california/candce/3:2023cv04625/417988"><u>Justia</u></a> ) 和<a href="https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion"><i><u>Getty Images 诉 Stability AI</u></i></a> （<a href="https://www.bakerlaw.com/getty-images-v-stability-ai/"><u>摘要</u></a>）。</p><p>在这一点上，估计这些诉讼的成功机会是非常具有推测性的。 <span class="footnote-reference" role="doc-noteref" id="fnrefszqf4pozkn"><sup><a href="#fnszqf4pozkn">[3]</a></sup></span>然而，有证据表明模型“记住”并可以<a href="https://twitter.com/Eric_Wallace_/status/1620449934863642624"><u>生成训练集图像的近乎复制品，</u></a>这使得任何关于生成式人工智能从根本上“改变”这些作品（这是利用受版权保护作品的必要条件）的辩护变得越来越困难。</p><p>这些诉讼对模特制片人的威胁比一般诉讼要大得多。</p><p>对于将监管罚款作为其既定模式一部分的科技巨头来说，之前造成的任何罚款可能都可以忽略不计。 （在某种程度上，即使是十亿美元的费用对于大型科技公司来说也是微不足道的。）</p><p>但<strong>禁止使用非法训练的数据集和模型的潜在禁令</strong>，以及禁止任何使用非法使用的数据集训练的模型的可能性，确实构成了重大的威慑。 <span class="footnote-reference" role="doc-noteref" id="fnrefx0o2de1003l"><sup><a href="#fnx0o2de1003l">[4]</a></sup></span>随着研究者和倡导者<a href="https://www.linkedin.com/posts/buolamwini_in-my-book-unmasking-ai-i-write-about-the-activity-7143278533086277634-zm0E?utm_source=share&amp;utm_medium=member_desktop">Joy Buolamwini 博士</a>等学者和活动人士呼吁“深度删除数据”的呼声越来越高，这种威胁也变得越来越大。</p><p><strong>具体关注点：BookCorpus 的版权易读性</strong></p><p>Books1 和 Books2 数据集（统称为“BookCorpus”）构成了前沿模型背后的数据集的很大一部分。 <a href="https://lambdalabs.com/blog/demystifying-gpt-3"><u>Li (2020)</u></a>指出，在 GPT-3 权重内，BookCorpus 持有完成整个普通爬行的四分之一的代币。</p><p> <a href="https://arxiv.org/abs/2105.05241"><u>Bandy &amp; Vincent，2021</u></a>表明 BookCorpus“可能违反了许多图书的版权限制”，其大部分作品都受到积极的知识产权保护。举一个可以在法庭上用来对抗它的露骨文本的例子，BooksCorpus 本身甚至有 788 个确切短语的实例“如果你正在阅读这本书但没有购买它，或者它不是为了你的目的而购买的”。仅供使用，然后请返回smashwords.com 并购买您自己的副本。”这组违规行为占数据集的很大一部分，部分原因是数据集中存在大量“重复项”，这使得该集中的原始书籍数量减少到<a href="https://towardsdatascience.com/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650"><u>不足 8,000 本</u></a>。</p><h3> <strong>3. 某些部门的劳工谈判和游说（创意工作中的工会、卫生领域的强大利益集团）将限制数据集的创建和使用。</strong></h3><p>某些领域的白领工人，拥有足够的劳动力和足够少的熟练劳动力供应，可以利用罢工和工会来获得让步，不利用其工匠的工作来训练人工智能模型。</p><p>这种担心之前的录音将被用来训练人工智能并使某些演员和作家变得多余，这是<a href="https://fortune.com/2023/07/24/sag-aftra-writers-strike-explained-artificial-intelligence/"><u>好莱坞演员和作家工会罢工</u></a>背后的一个激励因素。</p><p>并非所有实体都能成功做到这一点。演员的个人讨价还价能力比杂货店工人大得多。但某些强大的贸易组织如果足够<strong>集中</strong>、<strong>协调</strong>和<strong>强大</strong>，也可能会赢得类似的让步。</p><p>这里特别令人关注和相关的是<strong>医疗保健</strong>。由于 OpenAI、Google 和 Anthropic 总部均设在美国，以及该行业的经济价值，美国医疗保健与此次调查相关。美国医学会代表了医疗保健行业中普遍存在的任何类型的改革和政策（例如 HIPAA 和数据共享政策）的关键节点。事实证明，美国医学协会在抵制医疗改革方面是成功的，即使类似的压力在其他西方工业化民主国家也取得了成功。 （有关 AMA 权力的结构解释的更多信息，请参阅<a href="https://www.jstor.org/stable/4092296"><u>Hacker，2004</u></a> 。 <span class="footnote-reference" role="doc-noteref" id="fnrefc4fm96rh3cf"><sup><a href="#fnc4fm96rh3cf">[5]</a></sup></span> ）</p><p>在依赖劳工支持的民主党政府领导下的美国，此类努力成功的可能性也更高。拜登在当选前承诺，他将成为“ <a href="https://millerjohnson.com/publication/president-biden-the-most-pro-union-president-youve-ever-seen/"><u>你所见过的最支持工会的总统</u></a>”。他在<a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/"><u>人工智能行政命令</u></a>中重申了这一承诺，强调了人工智能对劳动力市场的影响。</p><p>待确定的核心问题之一是：这些劳动者中有多少比例将严重限制数据集的创建和使用，而不是简单地要求为其支付可管理的费用？</p><p><strong>概率：</strong>在许多职业中相当低，但在医疗保健等关键职业中更高。这些障碍可能会对医疗保健等关键行业的自动化造成严重延误。仅医疗保健就占<a href="https://www.cms.gov/data-research/statistics-trends-and-reports/national-health-expenditure-data/historical#:~:text=The%20data%20are%20presented%20by,spending%20accounted%20for%2018.3%20percent."><u>美国 GDP 的 18%</u></a> 。 <span class="footnote-reference" role="doc-noteref" id="fnrefv14yse053k"><sup><a href="#fnv14yse053k">[6]</a></sup></span>即使这种高幅度约束的可能性很小，也值得考虑。</p><h3> <strong>4. 记录来源防止未来抓取或提取其内容</strong></h3><p>与<a href="https://docs.google.com/document/d/1OpuWnkbGByUIG3JPfv_LPSaUooLvopx_YI_BBS_OoM4/edit#heading=h.nn0cdhgb0469"><u>版权诉讼</u></a>相关，记录来源可能会从数据集中删除可信内容。此举将减少培训可以利用的优质数据源的数量。 <span class="footnote-reference" role="doc-noteref" id="fnref4xggcw76p5s"><sup><a href="#fn4xggcw76p5s">[7]</a></sup></span></p><p>例如， <i>《纽约时报》</i>已<a href="https://www-businessinsider-com.cdn.ampproject.org/c/s/www.businessinsider.com/new-york-times-content-removed-common-crawl-ai-training-dataset-2023-11?amp"><u>采取行动</u></a>从某些基础模型数据集中删除其内容。人们可以想象，鉴于吸引用户访问其货币化网站而不是 ChatGPT 的强大经济动机，许多网站也会这样做。</p><p>在用于培训法学硕士的关键数据集中，可能容易受到此类声明影响的“记录来源”的权重过大。虽然不可能精确定位 GPT-4 的精确数据集组成（因为它们尚未发布），但我们确实知道这些数据集在历史上偏重于记录源（例如随机博客）。</p><p> <a href="https://lambdalabs.com/blog/demystifying-gpt-3"><u>Li (2020)</u></a>在这里描述了 GPT-3 权重，表明 GPT-3 数据集在 Books 1 和 Books 2 数据集以及维基百科上为其语料库赋予了很大的权重。仅维基百科的代币数量就是“普通爬行（按质量过滤）”的 5.5%。 </p><figure class="image image_resized" style="width:63.37%"><img src="https://lh7-us.googleusercontent.com/xr9o_WqYVRYuzKXBFquQgqSw4tnSFFG1nerOVmKau3jk0A4NdQlXrJqd_HY-S5eT7BxeJZjwIzHCgmrvFwsHan2Xl5U9kp-1TuHBRxE-jK_rcWpxRmD60ECeMw-CPBWuH0zRC74wPQeawT2gKJwEJAoydiI_3rQlcT72CZUdHdcExgKilMBHMNdZNunMcw"><figcaption>资料来源：<a href="https://lambdalabs.com/blog/demystifying-gpt-3">李，2020</a> 。</figcaption></figure><p><strong>概率</strong>：已经发生了。在记录的出版物中传播的情况很可能发生。危害的程度不太清楚，更多地取决于数据集组成的未知数。</p><h3> <strong>5. 与上述各项/任何一项相关的时间延误</strong></h3><p>在当今世界，合成数据比许多人预期的更受关注，或者知识产权制度的转变迫使人们采用新的方法来训练数据集，这样的举动实际上会导致多年的研发损失。它还可能会抑制对新创新的热情和投资，进一步减缓<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Key_Flywheel_for_Accelerated_AI_Growth___Automation"><u>报告的关键飞轮</u></a>。即使这些障碍并非无法克服，它们也会严重推迟报告的发布时间。他们威胁要采用<a href="https://takeoffspeeds.com/playground.html"><u>报告</u></a>中暴露的时间表并将其向后推几年。</p><p> ~~~~~~</p><h1>二.常识问题</h1><p><strong>TL;DR</strong> ：结构性障碍阻止 2020 时代的模型可靠地执行经济中许多工作自动化所必需的安全和关键任务任务。特别是，抽象/泛化问题对于解决至关重要。如果没有抽象，这些模型就无法管理安全关键型工作。相反，这些工具将成为保住这些工作的人们的影响倍增器。虽然这些挑战有一天会得到解决，但今天的 SOTA 无法克服它们。等待这些下一个突破可能会给报告的时间表带来瓶颈。</p><h2>章节概述</h2><p>该报告假设，2020 年的模型在增加计算能力的情况下将使我们能够实现人类劳动的自动化。本节通过展示 SOTA 中的结构性缺陷如何抑制大部分经济的自动化（即使计算量无限）来挑战这一假设。</p><p>如果这些论点成立，那么由于在达到必要的模型复杂性之前需要在 SOTA 中进行新的创新，因此应该推迟时间表。</p><p>为了推动有效的自动化，人工智能必须能够泛化。也就是说，它必须能够从其背景中得出抽象原则，并将其应用于现实世界中不可避免地出现的新情况。</p><p>这种能力在当前的 SOTA 方法中受到结构性的限制。虽然这些问题可能很快就会得到解决，但目前还没有解决。</p><p>本节将详细介绍该论点的关键组成部分，并确定：</p><ul><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Abstraction_as_a_Necessary_Condition_"><u>抽象是许多人类工作自动化的必要条件</u></a>；</li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Studies_Suggest_Common_Sense_Shortcomings"><u>最近的研究表明常识性的缺陷</u></a>；</li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Structural_Reasons_for_the_Generalization_Problem"><u>泛化问题</u></a><u>存在</u>结构性原因——也就是说，为什么我们不能仅仅希望更多的计算来解决问题；</li><li><u>这一障碍对报告的衡量产生了</u><a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Quantifying_the_Consequences"><u>重大影响</u></a>。</li></ul><h2>抽象作为必要条件</h2><p>人类解决问题的大部分取决于我们利用“关于物理和社会世界如何运作的丰富背景知识”来推理新情况的能力（ <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi，2022</u></a> ）。</p><p>成功做到这一点需要一种“人类水平的直觉推理”（ <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi，2022</u></a> ）。</p><p><a href="https://arxiv.org/abs/2311.09247"><u>米切尔等人。 （2023）</u></a>指出，“抽象推理的定义特征是能够从有限的数据或经验中归纳出规则或模式，并将该规则或模式应用于新的、未见过的情况。”这种推理是“稳健概括的基础”。</p><p> <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi (2022)</u></a>强调了<strong>溯因推理</strong>的关键性质。溯因推理是人类根据“深层背景”生成概率见解的技能。归纳推理需要“非凡的想象力”和因果推理。</p><p>超出最初相关性的推理需要理解如何处理新情况或在新情况和已知情况之间进行比较。实现这一目标的手段是抽象。</p><p>如果没有这种理解，就无法发展“常识”（ <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi，2022</u></a> ）。</p><p>这种类型的推理是人类做得非常好的事情。我们理解抽象概念，并立即将我们在过去看似无关的环境中学到的知识“转移”到“新情况或任务”（ <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi，2022</u></a> ； <a href="https://arxiv.org/abs/2104.12871"><u>Mitchell，2021</u></a> ）。</p><p>借用<a href="https://garymarcus.substack.com/">马库斯</a>制定的标准，希望取代所有人类劳动的通用人工智能必须能够（1）概括并响应一组新颖的新条件和环境，（2）确定它讨论的事物如何相互关联， (3) 确定它讨论的事物与现实世界中其他事物的关系。它在这三件事上都失败了。</p><h3>深度学习的“脆弱”本质会抑制性能</h3><p>当代深度学习无法完成这项重要的抽象工作。现代深度学习系统“在面对与训练数据不同的情况时会产生不可预测的错误”（ <a href="https://arxiv.org/abs/2104.12871"><u>Mitchell，2021</u></a> ）。它们又窄又脆。</p><p>这一缺点是由于无法从模型参与的上下文中构建可概括的抽象而产生的。</p><p>当法学硕士被要求从事人类工作时，这些问题变得尤为明显。</p><p>这种脆弱性是为什么深度学习模型在复杂分析或国际象棋等人类难以完成的任务中占据主导地位，甚至无法完成基本的感知任务的部分原因。这就是为什么像 Cruise 这样的领先自动驾驶汽车公司<a href="https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html"><u>仍然需要每 2.5 到 5 英里进行一次人工干预</u></a>。其他领域的自动化领导者也仍然以同样令人惊讶的高比率要求人类监督和干预。</p><p>这种现象最简洁地表述为<a href="https://en.wikipedia.org/wiki/Moravec%27s_paradox"><u>莫拉维克悖论</u></a>：[对人类来说]困难的事情[对模型来说]是容易的； [对人类]容易的事情[对模型]来说很难。</p><h2>为什么泛化问题限制了采用</h2><h3><strong>1. 关键任务和安全关键功能</strong></h3><p>即使更多的数据使模型相对更强，但脆弱的人工智能系统的故障率却令人无法接受地限制了它们的准确性。</p><p>人类认知任务的很大一部分需要理解不可预测的外部世界的能力。人工智能可以处理国际象棋和围棋，部分原因是它们是<i>受控的</i>游戏板。例如，投资者和城市规划者等人类专业人士必须始终应对自然灾害和地缘政治事件等外来冲击。没有一场国际象棋比赛会因为供应链突然短缺而限制了骑士的武器装备而中断。</p><p>这些场景通常是人类可以毫无问题地解决问题，但没有抽象能力的模型会遇到困难。</p><p>在许多情况下，面对极端情况或需要形式、反事实和/或抽象常识推理时的短路是不可能的。三个原因：</p><ol><li>其中许多功能<strong>都是安全关键的</strong>。例如，Cruise <a href="https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html"><u>每隔几英里就会进行一次人工干预</u></a>，因为我们不会乘坐准确率达到 95% 甚至 99% 的自动驾驶汽车。</li><li>在许多其他情况下<strong>，准确性至关重要，</strong>因为“正确答案”很容易量化。投资的目标是产生高投资回报率，而投资回报率却附带着一个无情的货币衡量尺。另一方面，艺术的界限要小得多。 ChatGPT 可以创作一堆病态的诗，当它“搞砸”时，它可以被认为是美丽的。 <span class="footnote-reference" role="doc-noteref" id="fnref56j921wbhr5"><sup><a href="#fn56j921wbhr5">[8]</a></sup></span></li><li> <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#c__The_Edge_Case_Problem__Combinatorial_Explosion_of_Real_World_Scenarios___The_Long_Tail_of_Exceptions"><strong><u>现实世界场景的组合爆炸</u></strong></a>。野外出现的近乎无限的场景意味着<strong>总会存在边缘情况</strong>。即使在基于规则的场景中，例如驾驶，拥有数十亿美元的投资和数百万英里的训练数据，边缘情况仍然会不断出现。由此产生的长尾异常需要具备通过新情况进行“推理”的能力。</li></ol><p>大公司不会将安全关键功能置于哪怕一个灾难性错误的风险之中，这有以下三个原因：</p><ol><li><strong>声誉风险</strong>。对于安全关键功能，领先企业的关键护城河是他们的品牌及其附属消费者的信任。 （想想汽车，或者农业或建筑业的领导者。可靠的质量通常是证明高价标签合理性的必要条件。）在短期内，还有一个额外的威慑因素，即受到负面报道的影响。</li><li><strong>监管风险</strong>。安全失败会导致禁令。加州交通部<a href="https://slate.com/business/2023/10/cruise-suspended-california-robotaxis-self-driving-cars-san-francisco.html#:~:text=California%20just%20forced%20Cruise%20to,industries%2C%20and%20well%20beyond%20California."><u>最近暂停了对 Cruise 自动驾驶汽车的许可</u></a>，因为事故数量仍低于平均手动汽车。</li><li><strong>诉讼风险</strong>。尽管诉讼的直接成本本身可能并不是企业存在的风险，但更大的危险是先例，即生产公司对所有事故负责。 <span class="footnote-reference" role="doc-noteref" id="fnrefuk5dsmxgzpe"><sup><a href="#fnuk5dsmxgzpe">[9]</a></sup></span></li></ol><h3> <strong>2. 关键任务行动的特定部分：“对抗性黑客”</strong></h3><p>同样的脆弱性使系统暴露于通过“对抗性扰动”而遭受独特类型的黑客攻击的风险（ <a href="https://arxiv.org/pdf/1610.08401.pdf"><u>Moosavi 等人，2017</u></a> ）。这些扰动是“对输入进行专门设计的更改，这些更改对于人类来说是难以察觉或无关的，但却会导致系统犯错误”（ <a href="https://arxiv.org/abs/2104.12871"><u>Mitchell，2021</u></a> ）。即使在长尾的最末端，如果容易遭受此类对抗性攻击，银行或医院等后果严重的系统也很容易崩溃，从而造成巨额资金或生命损失。</p><p>出于上面列出的类似原因（避免法律责任和维护客户信任），公司对于完全自动化工作将<i>非常</i>谨慎。 （也就是说，在没有人类监督者的情况下实施模型。） </p><p><img style="width:42.03%" src="https://lh7-us.googleusercontent.com/95ELjxMqxipVAspJkdjKVZH3hgUQ0p5OzrqLXrN_UOzCfhqu_uvEcU-bRCLqCIIwzdYMry1QVdxklG2F3YyQhPz9xlzp4DEUUKMhGzLNxccK5FxPj0DJZJsOXB4lH7-IkwnWQx681T8o0QF6mhFtTPEC6Oophm9ao9umh4-bdkIdLYf3LbX6UaVnaW08gA"></p><figure class="image image_resized" style="width:85.09%"><img src="https://lh7-us.googleusercontent.com/0LORUxOfEXr9hJjAXcrT0W7WksSBu9LV7klntbpEEeWvQBvDAmC6Ik_n0Upoy6-K-O6rjAJVeTFByRKn0e_znapEAVUeeQneCYelO7YPl-bqrNARTqZOvk4_vCoRaGAbBjgxXFjH_VIEmPQkSJ2_B3H-Zc-eUf6BPOvhrBHHlcByN7XgwLn32CXmixQong"><figcaption>对抗性扰动欺骗深度神经网络分类算法的能力的示例。以上来自<a href="https://arxiv.org/pdf/1610.08401.pdf"><u>Moosavi 等人，2017 年</u></a>。下面，稍微不太稳健。 （OpenAI，来自<a href="https://www.theguardian.com/technology/2021/mar/08/typographic-attack-pen-paper-fool-ai-thinking-apple-ipod-clip"><i><u>《卫报》</u></i></a> 。）</figcaption></figure><h2>研究表明常识存在缺陷</h2><p>缺乏经验证据表明法学硕士可以执行任何类型的“抽象”、“概括”或“常识”（ <a href="https://arxiv.org/abs/2311.09247"><u>Mitchell 等人，2023</u></a> ； <a href="https://www.amacad.org/sites/default/files/publication/downloads/Daedalus_Sp22_10_Choi.pdf"><u>Choi，2022</u></a> ）。然而，有大量证据表明，需要这种常识的关键任务存在这些缺陷：</p><h3> <strong>1.形式逻辑与数学</strong></h3><p><strong>A。 《逆转咒》</strong></p><p>领先的模型仍然无法轻易确定，如果 A 是 B，那么 B 就是 A。当 A 和 B 是虚构的（即在训练数据中无法找到答案时）问题尤其明显。数据增强和使用不同的模型大小都不能改善问题。</p><p>即使接受了“A 就是 B”的训练， <a href="https://arxiv.org/abs/2309.12288"><u>Berglund 等人仍然如此。 (2023)</u></a>发现“B is”的提示不会比任何其他随机名称更频繁地导致“A”：</p><p>例如，如果对“奥拉夫·索尔兹（Olaf Scholz）是德国的第九任总理”进行了培训，则它将无法自动回答“谁是德国第九任总理？”。此外，正确答案的可能性（“ Olaf Scholz”）不会比随机名称更高。</p><p>这些缺点暗示了在LLM中结合检索和对称性的挑战。</p><p> <strong>b.算术</strong></p><p>当涉及数字在模型训练集中较少代表时，模型在数学问题上的表现要差得多（ <a href="https://arxiv.org/pdf/2309.03241.pdf"><u>Yang等，2023</u></a> ）。这一发现表明，存在什么数值推理的一部分是收集新数据并反映它的函数。</p><p>如<a href="https://arxiv.org/pdf/2309.03241.pdf"><u>Yang等人所记录的。 2023年</u></a>，甚至在解决乘法问题时也专门为处理数学斗争而专门创建。</p><p>本文的结果涵盖了这一点的证据。在下面复制的表7中，增加了乘法的“数字”数量迅速降低功能性能。这些性能降低与覆盖密度较小的训练空间中的位置相关。准确性本身的降低还表明，尚未了解乘法的基本“规则”。 </p><figure class="image"><img src="https://lh7-us.googleusercontent.com/kfS7SnIY_ykZKNDh_qTh7ntgRLrGe-38HK3Rr07_Tw6ODhT_n2wh8JH2JtLMMrXyqowYArlseq658seYXJUI9q6xK4w-MXxRZxRs0cPcOl2oj5eddbtTXU36GwjlOg3Z799jpMiifFsjQFZMgjf6xLF6vGNKpWEfDFSPgjusXxPGwUZ7qwlEdXZxOwIFPQ"><figcaption>资料来源： <a href="https://arxiv.org/pdf/2309.03241.pdf">Yang等。 2023年</a>。</figcaption></figure><h3> <strong>2.相关与因果关系</strong></h3><p><a href="https://arxiv.org/abs/2306.05836"><u>金等人。 （2023年）</u></a> （<a href="https://twitter.com/maxaltl/status/1668214861648609282"><u>在此处</u></a>总结）得出的结论是，LLMS“没有比歧视因果关系的机会更好。”延长本文还有更多工作要做。具体而言，该论文缺乏人类基线，以及有关它们如何促进模型的描述。但是，无论基线人类在因果关系与关联挑战方面的表现如何，本文都保留了相关性，因为许多人类专家可以正确区分这两种力量，而这些技能对于许多领域的成功至关重要。</p><h3> <strong>3.空间和语义概念</strong></h3><p><a href="https://arxiv.org/pdf/1911.01547.pdf"><u>Chollet（2019）</u></a>的抽象和推理语料库（ARC）是一组“ 1,000个手动创建的类比难题”（ <a href="https://arxiv.org/abs/2311.09247"><u>Mitchell等，2023</u></a> ）。这些拼图“故意省略”语言，以防止实际上归因于大型文本语料库中的“模式匹配”的明显解决方案（ <a href="https://arxiv.org/abs/2311.09247"><u>Mitchell等，2023</u></a> ）。</p><p>领先的模型在200个隐藏的测试集弧任务上反复失败了测试。截至2023年11月，“迄今为止，弧线上最高的精度”仅为31％。此外，当前领先的计划的作者承认，他们的方法“不太可能概括地概括”（ <a href="https://arxiv.org/abs/2311.09247"><u>Mitchell等，2023</u></a> ）人类，或者，人类平均达到了84％的精度（ <a href="https://arxiv.org/abs/2103.05823"><u>Johnson等，2021年，</u></a> <a href="https://arxiv.org/abs/2311.09247"><u>Mitchell等人。，2023</u></a> ）。</p><p> Mitchell等人，2023年，通过（1）一个改版的数据集扩展了这些实验，概念是“组织为特定核心空间和语义概念的系统变化”。尽管人类的精度为91％，但GPT-4的表现为33％。</p><h3> <strong>4.反事实推理</strong></h3><p><a href="https://arxiv.org/abs/2307.02477"><u>吴等人。 （2023）</u></a> <span class="footnote-reference" role="doc-noteref" id="fnrefo4ps9jobqcb"><sup><a href="#fno4ps9jobqcb">[10]</a></sup></span>通过研究“反事实思维”来揭示LLM无法抽象地推理。 LLM通常无法从最初的代码库中汲取教训，以在新颖的情况下执行某些事情。他们问LLMS：“ Python 3.7打印的以下代码段是什么？”该模型在这些任务方面做得很好。但是，当被要求进行“简单调整”以超越Python脚本但遵循相同的原则时，LLM无法执行。 <span class="footnote-reference" role="doc-noteref" id="fnrefjwi5olunurh"><sup><a href="#fnjwi5olunurh">[11]</a></sup></span></p><p>模型无法“反合”思考，并应用与Python构成的相同逻辑推理在其他情况下巩固了“记忆假设”。根据这一假设，模型在其训练集中的鹦鹉背部关系模型，并且缺乏处理动态，新颖情况的必要技能：通过应用从先前的经验中学到的抽象课程做出正确的决定。</p><h3> <strong>5.概括训练分配</strong></h3><p><a href="https://arxiv.org/pdf/2311.00871.pdf"><u>Yadlowsky等。 （2023）</u></a>当任务“在预审进分布的内部和外部”时，检查变压器“识别和学习新任务”的能力。尽管这些变压器对“预处理数据混合”的任务执行了“最佳[LY]（或几乎是）”，但即使在“简单的外推任务”中，同一变压器也会失败...他们预处理数据的领域。”即使是“在函数类空间的稀有部分中有效执行的变压器[...]的稀有部分，随着任务变得失效，也会分解。”</p><p> Yadlowsky等人的发现表明，该模型从其训练中取得了无抽象的原则，它可以交叉地与新的刺激相结合。 <span class="footnote-reference" role="doc-noteref" id="fnrefrpka9cq9d6"><sup><a href="#fnrpka9cq9d6">[12]</a></sup></span></p><h2>概括问题的结构原因</h2><p>为什么这个缺点出现？</p><p>模型学会通过识别大量数据语料库中的相关性来绘制敏锐的预测。尽管通过互联网获得了新的计算功能和更大的信息语料库却推动了生产力的提高，但这些因素并没有克服诸如异常长时间和LLM幻觉之类的问题。</p><p>正如<a href="https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html"><u>Bender（2023）</u></a>所说，我们不应该“将单词形式和含义混为一谈”。</p><h3> 1.相关=/=因果关系</h3><p>从最基本的层面上讲，这些工具通过在数据集中找到模式并预测其训练分布中的相关性，从而产生尖锐的预测。</p><p>这种从根本上相关学习模型不会教导任何因异常值推理的因果关系或能力。</p><h3> 2.过度参数：过度拟合随机初始化参数</h3><p>幻觉的主要原因仅来自模型所基于的数学的局限性。</p><p>在大型语言模型中，数据比数据点要多。 （ <a href="https://medium.com/@mlubbad/the-ultimate-guide-to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a"><u>GPT-4中有1.76万亿个参数</u></a>，高于<a href="https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/"><u>GPT-3的1750亿</u></a>。）</p><p>比数据点比数据点更多的自由变量（或参数）的模型可能导致解决方案不是唯一或明确定义的情况。这可以表现出过度拟合，其中模型在没有足够数据纠正的情况下学习训练数据中的噪声，而不是基础模式。</p><p>权重和偏见是随机初始化的，并且没有足够数量的数据点来从系统中训练噪声，因此此初始化成为默认驱动程序。</p><h3> 3.限制阻止内容将内容放置在培训分布中</h3><p><strong>互联网规模的数据集无法单独解锁自动化</strong></p><p>甚至在考虑文章早期的<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Generative_data_pollution_of_the_internet_corpus_fundamental_to_frontier_model_development__">数据腐败问题</a>之前，互联网规模的分布都无法使概括成为许多关键的经济领域。</p><p> <strong>A。数据不存在。</strong></p><p>在一个领域中访问“专业知识”所必需的绝大多数知识并没有以任何清晰的方式记录。正如<a href="https://thegradient.pub/why-transformative-artificial-intelligence-is-really-really-hard-to-achieve/"><u>Ramani＆Wang（2023）</u></a>所解释的那样：</p><blockquote><p>我们对作为记者和AI研究人员的日常工作感到惊讶，因为有多少个问题在互联网或书籍中没有良好的答案，但是一些专家有一个可靠的答案，而他们却没有努力记录。在某些情况下，就像大厨或勒布朗·詹姆斯一样，他们甚至没有能力使自己的工作方式变得清晰。</p></blockquote><p>与更多的anecdata相提并论，我在我经历的环境中遇到了这些问题，例如：</p><ul><li>确定是什么将能力与各个领域的一流技术团队分开。</li><li>看着伟大的公众演讲者试图展示他们如何拥有“存在”。</li><li>在游戏之外的游戏组件上指导高性能的网球运动员。</li></ul><p> <strong>b.建立某些基础真理的高成本（人力，财务和其他）。</strong></p><p>模型培训过程需要“试用”以评估模型上的不同模型排列。但是，对关键问题问题进行此类测试可能需要巨大的成本。</p><p>从<a href="https://thegradientpub.substack.com/p/arjun-ramani-zhengdong-wang-transformative-ai#details"><u>王</u></a>（在信函中）窃取另一个相似之处：</p><blockquote><p>想象一下让机器人教孩子们游泳。您必须部署它才能获取数据，但是在此过程中，没有孩子实际上可以死亡。您最好希望模拟就足够了，但是如果它不够的话怎么办？ <span class="footnote-reference" role="doc-noteref" id="fnrefayin6cy340e"><sup><a href="#fnayin6cy340e">[13]</a></sup></span></p></blockquote><p>这种成本将不可避免地发生在大多数尝试会产生次优结果的测试中。我们避免这些成本的能力取决于我们抽象原则并将其应用于其他情况的能力。没有这种抽象能力，我们需要在手头的特定挑战中进行测试以解决它。</p><p> <strong>C。边缘案例问题：现实情况的组合爆炸 +例外的长尾巴</strong></p><p>只有无限的场景可以出现。例如，特斯拉（Teslas）被<a href="https://www.youtube.com/watch?v=tpOg87AQvbo"><u>马车</u></a>和<a href="https://twitter.com/Carnage4Life/status/1400848278682365954?lang=en"><u>装满交通信号灯的卡车</u></a>束缚。尽管这些例子是从未见过的刺激很少见的，但人类驾驶员仍可以迅速理解它们。</p><p>世界大而复杂。在现实世界中，每天都会遇到数百万无法预料的属性。</p><p>自动驾驶汽车公司<a href="https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html"><u>不断雇用</u></a>人类对汽车进行远程操作，或者在机器人被边缘案件混淆时实时提供必要的额外投入。 <span class="footnote-reference" role="doc-noteref" id="fnref4pm24e9331l"><sup><a href="#fn4pm24e9331l">[14]</a></sup></span></p><p>这种无法处理漫长的例外尾巴，再加上<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Mission_Critical___Safety_Critical_Functions_"><u>对任务至关重要的关键环境中的故障​​的恐惧</u></a>，这是世界许多领先的公司甚至没有相对不适当的问题自动化的重要原因。</p><p>出于所有原因， <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Internet_Scale_Datasets_Fail_to_Unlock_Automation_Alone_"><u>数据集限制</u></a>（例如<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#c__The_Edge_Case_Problem__Combinatorial_Explosion_of_Real_World_Scenarios___The_Long_Tail_of_Exceptions"><u>现实世界情景的组合爆炸</u></a>）在问题下越来越多的数据量不能保证解决方案。如果是这样，那么自动驾驶汽车仍然不需要<a href="https://www.nytimes.com/2023/11/03/technology/cruise-general-motors-self-driving-cars.html"><u>每几英里</u></a>的人进行干预。</p><p> <strong><u>d.缺乏记录的专家级数据，以及记录和传达此类知识的挑战。</u></strong></p><p>尽管一些挑战，例如棋盘游戏甚至驾驶汽车，都有相当明显的损失条件，但许多其他挑战却没有。例如，我们如何为聊天机器人定义“成功”的测试标准？ RHLF和其他工具转移这种专业知识的局限性，尤其是在罕见且经常未记录的情况下，限制了另一个限制。</p><h3> 4.语义上有效的世界模型</h3><p>从经验上讲，大批幻觉的<i>存在</i>表明缺乏连贯的“世界模型”。</p><p>在<a href="https://arxiv.org/abs/2310.02207"><u>古尔尼（Gurnee）和特格马克（Tegmark）的后果中出现了关于世界模型局限性的类似对话（2023年）</u></a> 。作者在“分析了三个空间数据集的学习成论和三个时间数据集的学说”之后，声称LLM中的“世界模型的证据”。通过此分析，他们能够在地图上生成数据点，在该地图上，城市聚集在或靠近其“真正的大陆”。也就是说，北美城市最终在地图上的北美边界上或附近，对于非洲城市，亚洲城市等，也可以这样说。</p><p>但是，正如当时像<a href="https://garymarcus.substack.com/p/muddles-about-models"><u>马库斯（Marcus，2023年）</u></a>一样标记的评论员，这些关系聚集在其地理位置上的聚集位置正是您从严格的相关工具中所期望的。毕竟，数据集经常将其地理位置的城市上下文化。例如，Gramercy Park和Grand Central Station都在“曼哈顿”的语言环境中使用。或者，直接引用马库斯（Marcus）：“地理可能是微弱但不完美地从语言语料库中推断出来的。”</p><p>我们希望有一个LLM提示有关达拉斯经常提及休斯顿的问题的原因，因为我们期望提到威廉·塞沃德（William Seward）提出大量提及阿拉斯加和亚伯拉罕·林肯（Abraham Lincoln）的原因。这些事情经常相互提及。</p><p>而且，当然，任何具有语义上有效的世界模式都不会将许多城市置于数百英里的海洋中。</p><p>由于手头任务的棘手/无情的性质，这个世界模型部分很有可能被证明是不满意的：试图证明是负面的。但是，我将其包括在“理解”文献中，并在本技术缺点部分中与上述许多点相对应。</p><h2>量化后果</h2><p>这些单独驾驶的限制阻止了大量工作的自动化。</p><ul><li>根据<a href="https://www.bls.gov/opub/ted/2017/30-percent-of-civilian-jobs-require-some-driving-in-2016.htm"><u>劳工统计局的数据</u></a>，美国有30％的平民工作需要开车。尽管其中一些功能可以通过使用足够的新技术进行驾驶创新，但其他功能（例如，30％的股份，例如交货司机，卡车司机或警察巡逻队）。根据<a href="https://www.bts.gov/transportation-economic-trends/tet-2018-chapter-4-employment#:~:text=Truck%20transportation%2C%20the%20largest%20subsector,5.2%20million%20employees%20in%202017."><u>美国运输局统计</u></a>，“五个最大的交通相关职业中有四个涉及驾驶。”这四个职业（重型，轻便和送货卡车司机；校车司机；驾驶员/销售人员）雇用了<strong>360万工人</strong>。</li><li>建筑部门充满了关键的，高度影响的决定，必须成功地自动化， <a href="https://statista.com/statistics/187412/number-of-employees-in-us-construction/#:~:text=The%20construction%20sector%20employed%20nearly,construction%20and%20demand%20for%20workers."><u>雇用了800万美国人</u></a>。施工自动化比开车更加困难，因为它既需要实体在环境中的运动，又需要对该环境的操纵。 （更不用说在挖掘机等机器中以多个自由度管理附属物的附加挑战。）</li><li>许多工作可能会<i>发生变化</i>- 也就是说，参与任务的人员将做不同的工作来监督在他们下面执行的模型。但这更类似于从手动簿记到Excel建模而不是工作结束。 <span class="footnote-reference" role="doc-noteref" id="fnrefer9r1e4chwp"><sup><a href="#fner9r1e4chwp">[15]</a></sup></span></li></ul><p> ~~~~~~</p><h1>三． GDP增长，测量和定义</h1><h2>框架：GDP增长是论点的必要部分</h2><p>该报告的估计是，我们跨越了执行20％任务的AI和100％任务之间的有效失败差距的速度部分取决于GDP的增长：</p><ul><li>我们越过有效失败差距的速度取决于我们在最大的训练运行中增加有效计算的速度。在报告的世界模型中，这种有效的计算取决于软件，硬件和用于培训的金钱的增加。这些属性的第三个部分取决于GDP的增加，这增加了在培训上的资金，原因有两个：<ul><li>首先，GDP从自动化中增加，通过更好的模型来验证进一步自动化的价值，从而激励投资增加。</li><li>其次，独立于将GDP份额提高到AI R＆D的份额不断增加，GDP的增加增加了用于培训的资金，因为专门用于培训的GDP相同的份额随之而来。</li></ul></li></ul><p>因此，估计自动化的速度促进GDP的速度对于该模型的最终结果以及我们访问其爆炸性生长飞轮的速度至关重要。</p><p>但是，关于自动化如何增加GDP的报告模型非常雄心勃勃。</p><p>以下三个力是对GDP增长的潜在缓解作用，因此，该报告的Flywheels关键。</p><h2>答：“足以替代有利可图的” AI（gespai） - 围绕“容易表现”的定义可能会扭曲发现。</h2><h3>执行任务“容易”和盈利与比人类更好地执行任务不同。</h3><p>当前的报告衡量了AI可以“轻易&#39;执行任务&#39;&#39;的模型何时“有利可图”，使组织可以在实践中纳入并能够在一年内纳入，从而代替了全球经济中的人类劳动力。</p><p>但是，如果公司为他们创造更多的利润，将替代优质的服务略有较差。</p><p>也就是说，尽管手头的任务做得更好，但B可以很容易地代替A。</p><p>这种细微差别很重要，因为如果是真的，则意味着该报告不一定衡量AI何时可以执行所有任务，或者比人类更好或更好。取而代之的是，它只是在模型成为良好到实现的良好态度（GESPAI）时进行的。也许这些模型需要以十种不同方式向他们提出问题。或者，他们<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Deep_Learning_s__Brittle__Nature_Inhibits_Performance"><u>幻觉</u></a>。</p><h3>人类的输出能力明显超过其Gespai替代品，但如果他们的薪水成本使每个人的利润降低，那么对公司的盈利仍然降低。</h3><p>考虑以下假设方案：</p><p>想象一个人，一个名为Jimbo的人，他是一家交易员，他的年薪为100,000美元，并为Anonbank的年收入为150,000美元。然后，想象一下一种称为Jimbot的交易算法，该算法的年收入为Anonbank的年收入为140,000美元。吉姆伯特没有薪水。 Jimbo和他的模特都花费了相同的交易，因此成本的唯一区别是Jimbo的薪水。 Jimbo比Jimbot更好。 Jimbo的每个任务输出更高。 （在同一时间单位，他会产生更多的收入。）然而，Anonbank每次都应该用Jimbot代替Jimbo。 GESPAI对其工作更好。</p><p>市场的所有标准原因不完美，进一步降低了Gespai必须达到的标准，因为这意味着消费者通过引入Subpar模型推迟了简单地离开的能力：</p><ol><li><strong>寡头行业</strong>（BIG BANKS，健康保险公司，航空公司，租车，乘车场）（1）如果客户不满意，请减少手头的替代方案，（2）鼓励<i>事实上的</i>串通。如果Delta，United和American Airlines都开始使用此自动化服务，那么您很快就用完了其他选择。</li><li>以棍棒（转移费用）的形式和胡萝卜（忠诚度损失）的形式<strong>转换成本</strong>，激励许多用户保持不良经验而不是切换，以获取公司的其他收益。</li></ol><h3>影响</h3><p>该干预措施对该报告具有四个关键含义，从最少到最复杂的是：</p><p> <strong>1. Gespai不会像更强大的AI一样威胁“剥夺所有人类权力”。</strong></p><p>取而代之的是，这个定义为AI可以代替某些经济任务的世界留出了空间，但不能比人类更好地执行大多数任务。 <span class="footnote-reference" role="doc-noteref" id="fnref3jbidjc68bp"><sup><a href="#fn3jbidjc68bp">[16]</a></sup></span></p><p> <strong>2.该报告的定义可能比通往At-Obove-Human级AI的道路更快地制作时间表。</strong></p><p>与创建比人类更好地执行所有经济任务的模型相比，让AI到达良好的取消水平更容易。</p><p> <strong>3.此定义可能会在20％至100％自动化之间留出太多空间。</strong></p><p> Gespai可能会解决自动化全球经济超过20％的自动化，但它将无法达到100％的范围。</p><p>这种转变发生在对<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#1__Mission_Critical___Safety_Critical_Functions_"><u>安全至关重要功能的</u></a>类似移动之前，在人类水平以下的替代是不可接受的。 Gespai符合自动化风险的定义大大延长了20％至100％的路径。</p><p>这种类型的GESPAI自动化已经代表着大量的知识工作，包括更多的“死记硬背”，内部的白领工作。 （律师助理，客户服务等）这些Gespai产品在很多情况下仍然显然不如。但是，根据此定义，它们可以被描述为“自动机”。</p><p> <strong>4. GESPAI大大减少了自动化中的GDP提升。</strong></p><p>作者认为，AI自动化通过释放人类进行额外工作而在自动化创造相同的生产率时通过进行其他工作来使生产率翻了一番：</p><blockquote><p>假设AIS自动化了50％的任务……人类可以专注于其余50％的任务，而AIS可以匹配人类的每个任务输出，将每个任务的输出增加2倍。这将GDP提高了2倍...</p></blockquote><p>以同样的方式：</p><blockquote><p>在这里，我假设每个任务的AI至少要跟上人类输出。我可以假设这是因为它首先是AI自动化任务的条件。</p></blockquote><p>但这不是！取而代之的是，Gespai在工作中的表现可能会大大更糟，从而产生净收入和/或每任务输出，同时仍然是公司的好电话。即使Jimbo是更好的交易者，Anonbank仍应用Jimbot代替Jimbo。</p><h2> B.任何成功自动化的事物固有地成为GDP的较小百分比。</h2><p> <strong>TL：DR</strong> ：当您自动化商品时，它们的相对经济价值下降。</p><p>当任务自动化时，其GDP的贡献份额仅仅由自动化的性质降低。原因有两个：</p><h3> 1.增加劳动力供应，所有其他相等的劳动力驱动力下降。</h3><p>在功能上，创建执行服务的无限数量的模型代表了人工供应的无限增加。随着劳动力供应的增加，所有其他相等的人，劳动力供应曲线的转移意味着减少执行相同任务的工资。 </p><figure class="image image_resized" style="width:77.12%"><img src="https://mail.google.com/mail/u/1?ui=2&amp;ik=50c5ddf9b7&amp;attid=0.1.1&amp;permmsgid=msg-f:1785991768306939171&amp;th=18c91e0cf9b4f523&amp;view=fimg&amp;fur=ip&amp;sz=s0-l75-ft&amp;attbid=ANGjdJ8vFFIhnorvcW8pEzJ62oEpmmKGYsnTucYqtCjaZLBiSfFFDC4eC30Ybc5KPXT4xYTwp26qtLsjhhhfPDVWThjb3Go_oqz4kOzEUu1eZNep_PCOYgbWBg5cqWY&amp;disp=emb"><figcaption> X的自动化会降低X的价格，因为自动化增加了劳动力供应。</figcaption></figure><h3> 2.鲍莫尔效应增加了其他没有经历生产力增长的工作的工资，从而降低了任务的单位价值。</h3><p>随着一个部门的生产率提高，其余工人的工资也会增加。作为回应，其他必要但没有经历这种生产力增长的部门必须增加工资以吸引工人。 <a href="https://en.wikipedia.org/wiki/Baumol_effect#:~:text=In%20economics%2C%20the%20Baumol%20effect,have%20experienced%20higher%20productivity%20growth."><u>鲍莫尔效应</u></a>解释了为什么在医疗保健和教育等部门增加工资的部分原因。我们需要人们在这些部门工作。随着其他地方的工资增长，我们必须支付更多的费用，以防止它们叛逃到更有利可图的行业。这具有降低生产行业每个单位的GDP条款的价值，因为该员工的工资与生产力较低的部门的员工的工资相对相似。</p><p>这些因素在一起意味着，当您自动化某些东西时，完成服务的一个单位的相对权重减少。这可以使更多的任务完成，但也可以使商品化。</p><p>即使您自动化当前代表的GDP的50％，这种自动化也很可能会降低自动化的GDP的份额。</p><h3> 3.自动化将盈余从劳动者转移到资本持有人，而这些支出的可能性较小。</h3><p>消费驱动了GDP的很大一部分。一名员工为他或她的公司创造经济盈余，并获得了工资的奖励，然后将其花在其他业务上，创建另一组群体的工资等等。这称为<a href="https://www.investopedia.com/terms/m/multipliereffect.asp"><u>乘数效应</u></a>。</p><p>正如标题所说，自动化将使用来在资本家手中使用的钱纳入工资。<a href="https://equitablegrowth.org/wealth-inequality-marginal-propensity-consume/"><u>比我们其他人最富有的节省更多的实际收入</u></a><u>。</u> <span class="footnote-reference" role="doc-noteref" id="fnrefsjk1fs63m8g"><sup><a href="#fnsjk1fs63m8g">[17]</a></sup></span>因此，从劳动者到资本家重新分配的资金不太可能开始乘数效应。</p><h2> C.劳动力流离失所会使市场效率低下</h2><p>该报告假设人类自动化的人会在自动化的工作中发现同样富有成效/有价值的工作。</p><p>作者认为，AI自动化通过释放人类进行额外工作而在自动化创造相同的生产率时通过进行其他工作来使生产率翻了一番：</p><blockquote><p>假设AIS自动化了50％的任务……人类可以专注于其余50％的任务，而AIS可以匹配人类的每个任务输出，将每个任务的输出增加2倍。这将GDP提高了2倍。</p></blockquote><h3>第一原则反驳</h3><p>以下是许多人可能并非如此的六个原因：</p><ol><li><strong>社区的工作有限</strong>。在社区中没有以1：1的速度创造工作，以使工作是自动化的。相反，越来越多的人只是在社区中争夺低薪工作。 Per <a href="https://mitsloan.mit.edu/ideas-made-to-matter/a-new-study-measures-actual-impact-robots-jobs-its-significant"><u>acemoglu（2020</u></a> ）：“当[自动化]工作消失时，[流离失所]工人去较低工资工人那里。”有人失去并最终流离失所。</li><li><strong>新工作的培训通常需要一些技能培训</strong>。所花费的训练时间也代表着重量损失本身，即使有效。研究表明，很多时候，政府运营的再培训计划都没有成功（ <a href="https://www.downsizinggovernment.org/labor/employment-training-programs"><u>Edwards＆Murphy，2011年</u></a>）。这也可能激励一些工人离开劳动力。 （示例包括：提早退休，依靠失业福利，或在正式经济以外寻求全职育儿的机会。）</li><li><strong>招聘中的年龄歧视</strong>可能会阻止老年流离失所者找到新工作。 （<a href="https://www.bls.gov/opub/mlr/2017/beyond-bls/is-there-age-discrimination-in-hiring.htm"><u>贝克，2017年</u></a>，总结<a href="https://www.frbsf.org/economic-research/publications/economic-letter/2017/february/age-discrimination-and-hiring-older-workers/"><u>旧金山联邦储备银行，2017年</u></a>）。</li><li><strong>流离失所员工所做的工作可能不像他们的旧工作那样有利可图</strong>。需要从事任何工作来支付账单，激励流离失所的工人“定居”，以获取比理想的机会不太理想的机会。</li><li><strong>流离失所的员工在新角色上的生产力少于</strong>他们在旧角色中的生产力。许多人经常在他们的旧，现在是自动的角色中经验。他们在新领域的新领域可能不太有效。</li><li><strong>许多人无法访问/使用技术</strong>。尽管自动化可能会生成新的作业（例如数据标记），但这并不意味着这些作业很容易访问。<a href="https://www.forbes.com/advisor/personal-finance/millions-lack-broadband-access/"><u>仅在美国，有4200万人（一个发达国家）没有宽带通道</u></a>。许多其他人无法很好地使用计算机来执行数据标签。</li></ol><h3>数据驱动的反驳</h3><p>这些第一原理机制在初步研究中得到了证实。</p><p> <a href="https://economics.mit.edu/sites/default/files/publications/Robots%20and%20Jobs%20-%20Evidence%20from%20US%20Labor%20Markets.p.pdf"><u>Acemoglu＆Restrepo（2020年）</u></a> （ <a href="https://mitsloan.mit.edu/ideas-made-to-matter/a-new-study-measures-actual-impact-robots-jobs-its-significant"><u>麻省理工学院斯隆报告</u></a>）“发现，对于美国每名1000名工人增加的机器人，工资下降了0.42％，就业人口与人口比率下降了0.2个百分点，并增加了一个”通勤区（用于经济分析的地理区域）中的更多机器人减少了该地区的六名工人的就业机会。”</p><p>这项研究表明，并非所有由自动化替换的员工都找到新的工作。</p><p> <a href="https://www.nber.org/system/files/working_papers/w28061/w28061.pdf"><u>Feigenbaum and Gross（2022）</u></a> （由<a href="https://www.vox.com/future-perfect/2023/7/18/23794187/telephone-operator-switchboard-automation-att-feigenbaum-gross"><u>Dylan Matthews（2023）</u></a>总结）发现，在20世纪中叶从工作中自动化的电话运营商比“十年后的低薪专业”比以前有10次“运营商不暴露自动化”（ <a href="https://www.vox.com/future-perfect/2023/7/18/23794187/telephone-operator-switchboard-automation-att-feigenbaum-gross"><u>Matthews，2023年</u></a>）。与非自动化同行相比，“老年运营商”离开劳动力的可能性也高7％（ <a href="https://www.vox.com/future-perfect/2023/7/18/23794187/telephone-operator-switchboard-automation-att-feigenbaum-gross"><u>Matthews，2023年</u></a>）。</p><h2>对报告模型的影响</h2><p>这三个原因共同减轻了自动化将以报告的速度增加GDP的说法。</p><p>这些缓解措施并没有使GDP的50％自动化增加2倍，而是降低了数量较低的数字。</p><p>从<a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#More_Detail">报告</a>中记住：</p><blockquote><p> g（在最大训练运行中有效计算）= g（训练运行的分数失败） + g（全球失败） + g（flop/$） + g（软件）</p></blockquote><p>让我们浏览所影响的关键变量：</p><h3> 1. G（软件）</h3><p>从长长的摘要中：</p><blockquote><p>每次累积软件研发支出的加倍驱动约1.25倍的软件。</p></blockquote><p>该报告反映了GWP的增长，随着研发的增长：</p><blockquote><p>此处描述的有关AI自动化对GWP的影响的逻辑与其对研发输入的影响相同。</p></blockquote><p>因此，<strong>基于上述因素的GWP增长的任何缓解措施都会以1.25的倍数降低G（软件）的增长</strong>。</p><h3> 2. g（全球拖鞋上的$）</h3><p>如报告所述：</p><blockquote><p> G的增加（全球失败）等于G（GWP）的增加。</p></blockquote><p>因此，<strong>基于上述因素的GWP增长的任何缓解措施都以相同的速度降低了G（全球Flop上的$）增长</strong>。</p><h3> 3. g（flop/$）</h3><p>较少数量的是，flop/$在这个世界上减少。</p><p>所有其他等同<i>，</i>较低的GWP  - >;更少在硬件和软件上的投资 - >;在硬件和软件中的创新较少 - >;更糟的模型 - >;较低的flop/$。</p><p>即使将2倍转移到1.5倍，对模型也具有巨大的影响，从而降低了飞轮上每个节点的功率：</p><ol><li> $花在研发上，购买更多的计算减少。</li><li>硬件和软件创新都需要更长的时间。</li><li>我们最大的训练跑步在每次地平线上都较小。</li><li>最终的AI模型可以在每个时间范围内自动化较少。</li></ol><p> ~~~~~~</p><h1>四．研发并行罚款</h1><p>该报告的最佳猜测预设将其定为.7（<a href="https://takeoffspeeds.com/playground.html"><u>操场</u></a>）。我相信有充分的理由使用更大的惩罚。</p><h2>答：使用引用的ML特异性研究（Besiroglu，2020）</h2><p>起飞模型游乐场在其附加信息中注明：</p><blockquote><p> <a href="https://tamaybesiroglu.com/papers/AreModels.pdf"><u>Besiroglu</u></a>引用了一些[数字]低至.2的[数字]。</p></blockquote><p>但是besiroglu比这更可怕。在p。 14（如引用），Besiroglu参考了另一个报告（ <a href="https://ideas.repec.org/a/taf/applec/v52y2020i3p260-274.html"><u>Sequeira and Neves，2020</u></a> ），该报告的系数约为0.2。但是，仅引用了塞奎拉（Sequeira）和尼维斯（Neves）与贝西罗格鲁（Besiroglu）的发现对比，该发现估计为.02至.13。</p><p>在下面查找关键摘录，主要来自<a href="https://ideas.repec.org/a/taf/applec/v52y2020i3p260-274.html"><u>起飞模型游乐场</u></a>中引用的同一页面。</p><p>最初发现：</p><blockquote><p>使用协调的误差校正方法，<strong>我们估计在考虑的三个机器学习子领域中，性能的平均研究弹性约为0.02％至.13％</strong> 。 （第1页）。</p><p>由于“非常实质性的踩踏效果”，大约<strong>需要额外的2060名研究人员将前10名研究人员的影响增加一倍</strong>”（第14页）。</p></blockquote><p> Besiroglu，与Sequeira和Neves的发现对比：</p><blockquote><p><strong>我们对σ的估计表明，与研发文献相比，脚步效应更大</strong>。例如，Sequeira和Neves的荟萃分析2020发现平均σ系数约为0.2 <strong>，大约是我们发现的计算机视觉和自然语言处理的值的两倍</strong>（第14页）。</p></blockquote><h2> B.第一原则造成高研发并行罚款的原因</h2><p>有理由相信，由于参与者为工作提供资金，AI/ML R＆D流程将特别降低。</p><p>这笔钱的绝大多数来自三个来源之一，所有这些资源都造成了重大的惩罚：</p><h3> <strong><u>1.越来越多的私人市场投资者，其中大多数人都不知道他们在做什么，而是在最后的到达项目中扔钱</u></strong><u>。</u></h3><p>该报告讨论了如何表现出更多的希望，由于对异常高回报的信念，更多的人将在该领域投资。但是随着生产力的增长，炒作周期也会增长。去年，许多新的“ AI投资者”已经进入了该空间。尽管有些投资者非常复杂，但许多人会在没有必要的技术理解的情况下追逐钱来赌注。这些不太复杂的投资者将以更高的价格“欺骗”，将资金投入初创企业，即使在最好的情况下，也无法提供新颖的创新。 （这里最明显的相似之处是Web3 Hype循环和“加密繁荣”。另一个有价值的平行是互联网泡泡。）VCS上的扣篮很容易，但这也是该论点的真正重要部分。</p><h3> <strong>2.跨竞争公司的努力重复。</strong></h3><p>追求创新X的投资不会进入一个集中式池。相反，许多公司一次实现相同的目标。苹果和微软都与完整的团队一起追求创新X。即使两个团队都成功，创新X的创造也已成为推进SOTA的重复努力。 <span class="footnote-reference" role="doc-noteref" id="fnrefw3unqycnnba"><sup><a href="#fnw3unqycnnba">[18]</a></sup></span></p><h3> <strong>3.跨国家的努力重复</strong>。</h3><p><a href="https://www.nscai.gov/2021-final-report/"><u>美国</u></a>和<a href="https://digichina.stanford.edu/work/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017/"><u>中国</u></a>都认为AI发展是其国家安全战略的主要属性。在某些情况下，尤其是国防的AI发展 - 生态系统在很大程度上是脱钩的（尽管有一些中国MLE在美国公司和大学工作）。 <span class="footnote-reference" role="doc-noteref" id="fnreficr8dg4f1ln"><sup><a href="#fnicr8dg4f1ln">[19]</a></sup></span></p><h2>平行罚款增加，所有其他相等的罚款都增加了5年的时间表</h2><p>该表使用<a href="https://takeoffspeeds.com/playground.html">游乐场模型</a>在不同的并行化惩罚权重中比较起飞速度： </p><figure class="image"><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_330 330w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_550 550w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_770 770w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_990 990w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/88a52f701e7e9fa205912b68036969b45bcf424ce1142b1a.png/w_1010 1010w"></figure><p> ~~~~~~</p><h1> V.台湾供应链中断</h1><p><strong>TL; DR</strong> ：SOTA AI模型贯穿台湾和TSMC。 A PRC military invasion of Taiwan would set back timelines <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#An_Invasion_of_Taiwan_Would_Set__SOTA_Innovation_Back_by_Years_"><u>by 20 months</u></a> until another firm “catches up” with TSMC&#39;s current sophistication. Such an invasion becomes increasingly likely as Xi accumulates more power and PRC military ability increases.</p><h2>取景</h2><p>The report makes a key assumption related to SOTA model distribution and development: That for both R&amp;D and commercial distribution, <strong>the SOTA frontier isn&#39;t paused or set back.</strong> That once we produce function/ability Y, we can retain, readily re-produce, and iterate upon ability Y into the future. (In hardware terms: We can supply enough SOTA-level hardware to match both commercial and R&amp;D demand.)</p><p> <strong><u>The brittle nature of the supply chain for semiconductors, which are a prerequisite to frontier models and improved performance, challenges this assumption.</u></strong> Shocks to the supply chain that functionally remove a node within it would prevent further production of the SOTA and leave development functionally “paused.” No additional SOTA chips could be created, severely limiting both distribution of the SOTA and R&amp;D using the SOTA. This pause would continue until another actor could achieve the same leading level of sophistication.</p><p> <strong><u>These semiconductor supply chain shocks represent a significant enough threat to maintaining the SOTA to merit consideration in the report&#39;s timelines.</u></strong> In particular, the greatest threat is that of a PRC invasion of Taiwan.</p><h2> An Invasion of Taiwan Would Set SOTA Innovation Back, Potentially by Years</h2><p> A PRC invasion of Taiwan could lead to the destruction of TSMC, which is a key actor in the production of SOTA semiconductors. As the arguments below will explain, losing TSMC could create (1) production bottlenecks from damaged/destroyed factories or (2) loss of knowledge around key processes in fabrication. Either of these would prevent the production of SOTA hardware.</p><p> This event would set the report&#39;s timelines back, potentially by potentially years.</p><h3> 1. TSMC Sets the SOTA</h3><p> In the status quo, TSMC uniquely sets the SOTA in semiconductor production. TSMC produces “ <a href="https://www.cfr.org/blog/will-chinas-reliance-taiwanese-chips-prevent-war"><u>around 90% of the world&#39;s leading-edge semiconductors</u></a> .” Their advanced extreme ultraviolet lithography (EUV) machines, combined with a series of precise and sophisticated processes, enables them to make chips far beyond what competitors can produce. TSMC chips are already created at a granularity of <a href="https://theconversation.com/the-microchip-industry-would-implode-if-china-invaded-taiwan-and-it-would-affect-everyone-206335"><u>3nm</u></a> , more sophisticated than even “friendshored” alternatives like the TSMC chips being produced in Arizona, USA.</p><p> Other firms are predicted as unlikely to catch up to TSMC&#39;s current level <a href="https://theconversation.com/the-microchip-industry-would-implode-if-china-invaded-taiwan-and-it-would-affect-everyone-206335"><u>until 2025</u></a> , at which point TSMC will presumably still maintain some of <strong>its ~20 month advantage</strong> due to continued R&amp;D.</p><p> <strong>Losing access to TSMC chips, per this analysis, is worth ~1.5 years in AI timelines</strong> , because losing TSMC would functionally “freeze” hardware innovation until other firms “caught up” with TSMC.</p><h3> 2. An Invasion of Taiwan Would Destroy TSMC Capacity</h3><p> Four ways:</p><ol><li> <strong>Taiwan might destroy the TSMC campus in the event of invasion</strong> . The PRC still imports over <a href="https://www.cfr.org/blog/will-chinas-reliance-taiwanese-chips-prevent-war"><u>$400 billion worth</u></a> of Taiwanese semiconductors. Concern that Taiwan would rather destroy TSMC campus than let the PRC take it is a huge reason why Xi hasn&#39;t invaded yet.</li><li> <strong>The United States might destroy the campus if Taiwan does not</strong> , to ensure that the PRC does not take a strategic advantage in what most policymakers in DC consider a zero-sum, existential race for global supremacy. (See <a href="https://www.nscai.gov/2021-final-report/"><u>NSCAI</u></a> , among others.)<ol><li> US Congresspeople, including Democrats like <a href="https://theconversation.com/the-microchip-industry-would-implode-if-china-invaded-taiwan-and-it-would-affect-everyone-206335"><u>Seth Moulton</u></a> , have said that the United States would destroy TSMC in the event of a PRC invasion. But far more important is the potential US intelligence (DoD, State, CIA) response. This response is often seen through the lens of strategic competition.</li></ol></li><li> Even if TSMC campus remains, <strong>war in Taiwan would halt production</strong> . Despite the PRC&#39;s superior military strength, Taiwan would use every power in its capacity to hold on for as long as possible. The US has also <a href="https://www.reuters.com/world/biden-says-us-forces-would-defend-taiwan-event-chinese-invasion-2022-09-18/#:~:text=Asked%20last%20October%20if%20the,a%20commitment%20to%20do%20that.%22"><u>committed to defending Taiwan</u></a> . Given the delicate nature of fab processes—sensitive to even a millimeter of movement— <i>some</i> disruption feels inevitable in the face of conflict.</li><li> Even if the PRC seizes TSMC without damage, <strong>key individuals with proprietary, critical, and scarce knowledge of production might not transfer the knowledge</strong> . As detailed by Chris Miller in <a href="https://www.amazon.com/Chip-War-Worlds-Critical-Technology/dp/1982172002"><i><u>Chip War: The Fight for the World&#39;s Most Critical Technology</u></i></a> , certain parts of the fabrication process rely on knowledge held by only a handful of individuals who work directly on them. TSMC designed its firm this way to prevent defection to build competitor firms, so no TSMC employee, or small group of them, could recreate TSMC elsewhere. <span class="footnote-reference" role="doc-noteref" id="fnrefvkng1r2tth8"><sup><a href="#fnvkng1r2tth8">[20]</a></sup></span></li></ol><h3> 3. Invasion of Taiwan is A Serious Risk</h3><p> PRC invasion of Taiwan is a real risk, due to the strong incentives compelling the PRC to action:</p><ol><li> <strong>Taiwan is already seen across mainland China as rightfully part of the PRC</strong> , given Taiwan&#39;s modern origin as an ROC hub and eventual <a href="https://en.wikipedia.org/wiki/Chinese_Civil_War"><u>retreat point after losing the Chinese Civil War</u> .</a></li><li> A Taiwanese invasion would be seen as a <strong>crowning achievement for Xi</strong> , after multiple terms of that have already seen him dramatically <a href="https://www.theguardian.com/world/2022/aug/31/xi-jinping-poised-to-further-consolidate-power-at-party-congress"><u>centralize and consolidate power</u></a> .</li><li> <strong>PRC military might is growing dramatically</strong> , more than doubling from 2014-2023 <a href="https://en.wikipedia.org/wiki/Military_budget_of_China#:~:text=2019%3A%20the%20budget%20was%20announced,%2C%20an%20increase%20of%206.8%25."><u>to reach $224 billion</u></a> . Accordingly, the PRC has an <strong>increased likelihood of successful invasion</strong> , even if the United States defends Taiwan.  Military analysts say the PRC&#39;s new military might endows it with “ <a href="https://asia.nikkei.com/Opinion/If-the-US-went-to-war-with-China-who-would-win"><u>a great advantage in a potential conflict with the US in the South and East China seas</u></a> .”</li><li> <strong>The PRC government has demonstrated how much it cares about this issue</strong> by offering significant incentives to build global support for One China. It offers favorable One Belt One Road policies, often including millions of dollars worth of infrastructure, throughout the world contingent on acceptance of One China policy. As of 2018, <a href="https://www.cnn.com/2018/05/01/asia/china-taiwan-dominican-republican-intl/index.html"><u>only 20 countries</u></a> even had formal relations with Taipei. This is in part due to the work the PRC government has done to offer OBOR incentives dependent on One China alignment ( <a href="https://www.theguardian.com/world/2017/jun/13/panama-cuts-diplomatic-ties-with-taiwan-in-favour-of-china"><u>Guardian, 2017</u></a> ; <a href="https://global.oup.com/academic/product/the-china-triangle-9780190246730"><u>Gallagher, 2016</u></a> ). Foreign ministry officials also frequently reflect their official belief that Taiwan is a part of the PRC, despite the costs of additional tensions with the US, Taiwan itself, and the global community. <span class="footnote-reference" role="doc-noteref" id="fnref0moy3r5uxds"><sup><a href="#fn0moy3r5uxds">[21]</a></sup></span></li></ol><p> An <a href="https://foreignpolicy.com/2023/04/13/china-attack-taiwan-war-expert-poll-biden/#cookie_message_anchor"><u>April 2023 survey of expert IR scholars</u></a> estimated an average “23.75 percent chance of a Chinese attack against Taiwan in just the next year.” Prediction markets like <a href="https://www.metaculus.com/questions/11480/china-launches-invasion-of-taiwan/?sub-question=10880"><u>Metaculus</u></a> are less bullish, but still predict a 40% chance of “full-scale invasion” by 2035 and 30% by 2030. Those odds increase as Xi continues to consolidate power, PRC military capacity grows, and increasing model sophistication raises the stakes of the “Great Power AI Competition” to Beijing.</p><h2> Other Regulation</h2><p> Finally, regulation on hardware distribution or deployment might interrupt supply chains and leading innovators.</p><p> I allude to some potential regulatory disruptions in the section titled <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Obstacles_to_Data_Quality"><u>Obstacles to Data Quality</u></a> .</p><p> Increased attention on AI will bring more lobbying to shape its future. Particularly deserving of attention are powerful lobbying groups whose members create important datasets. For example, the American Medical Association in the USA has shown its ability to defend its interests even when counter to popular will.</p><p> National security issues might also disrupt supply chains and distributions. As briefly mentioned in the <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#IV__R_D_Parallelization_Penalty_"><u>R&amp;D Parallelization Penalty</u></a> section, “decoupled” ecosystems in the US and PRC already diminish innovation. Under certain political regimes—or simply due to military concerns about AI&#39;s role in Great Power Conflict—a greater decoupling could occur. Such moves could pause development at leading labs caught in national security strategy&#39;s crosshairs. For example, US export controls could prevent SOTA semiconductors from reaching mainland China and inhibit leading labs in Beijing.</p><p> ~~~~~~</p><h1> Appendix A: Background Info on Davidson 2023&#39;s Central Argument</h1><h2> Summary of Report&#39;s Central Approach to Takeoff Speed</h2><h3> Key Flywheel for Accelerated AI Growth &amp; Automation </h3><p><img style="width:68.37%" src="https://lh7-us.googleusercontent.com/PLijEK6wnamTBT6tlIPvs71mi17ZDgaSPK8lWTTBQwHXSbpCR99ZOuT7oaB_bRoG-Rl2klH7EvYCS9cp_8nnGbnVwZ74baV9o-0jcwrIZ4Nk6Y89uKjdnNSHVpU00vJrJlcB-Jq0uLhUm4w1HiodtVF9rJ3F50PtnmZb8YgnzcOpAXDXFFokDqgjZV5kYA"></p><h3>更多详情</h3><p>The report considers takeoff time, how quickly the shift from AI performing 20% of tasks and to completing 100% of tasks will occur.</p><p> This question is re-framed as “How quickly do we cross the effective FLOP gap?”</p><blockquote><p> Takeoff Time = Distance / Average Speed</p></blockquote><blockquote><p> Distance = Effective FLOP Gap</p></blockquote><p> The speed at which we cross the effective FLOP gap depends on how quickly we increase effective compute in the largest training run.去引用：</p><blockquote><p> Model assumes the compute-centric model. That is to say: It assumes that 2020-era algorithms are powerful enough to reach AGI, if only provided enough compute.</p></blockquote><p> In the report&#39;s model of the world, this effective compute depends on increases in software, hardware, and money spent on training.</p><p> Decomposing effective compute:</p><blockquote><p> Compute = Software * Physical Compute</p><p> EC = software * [FLOP/$] * [$ Spent on FLOP]</p></blockquote><p>所以：</p><blockquote><p> g(effective compute in largest training run) = g(fraction FLOP on training run) + g($ on FLOP globally) + g(FLOP/$) + g(software)</p></blockquote><p> The speed at which we cross this effective compute gap accelerates over time thanks to a self-reinforcing loop kicked off by initial AI training. Training creates better-performing AI models. Because of the economic potential of these models, investment in the hardware and software behind these models increases. This effect itself improves models and increases effective compute. At some point, AI-assisted software and hardware design also accelerates the flywheel further, improving AI models even quicker. This loop powers much of the rapidly accelerating takeoff speed. (See <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#Key_Flywheel_for_Accelerated_AI_Growth___Automation"><u>key flywheel</u></a> graphic.)</p><p> ~~~~~~</p><h1> Appendix B: Key Sources</h1><p> Along with the hyperlinks provided, I draw heavily on the following sources:</p><h3><strong>图书</strong></h3><p>Chip Huyen, <i>Designing Machine Learning Systems.</i> (Reproduced here as <a href="https://stanford-cs329s.github.io/syllabus.html"><u>Lecture Notes by Lesson for CS 329S</u></a> at Stanford University.)</p><p> Gary Marcus, <a href="https://books.google.com/books/about/The_Algebraic_Mind.html?id=3cB-DwAAQBAJ&amp;source=kp_book_description"><i><u>The Algebraic Mind: Integrating Connectionism and Cognitive Science</u></i></a> (reproduced 2021)</p><p> Marcus &amp; Ernest Davis, <a href="https://www.amazon.com/Rebooting-AI-Building-Artificial-Intelligence-ebook/dp/B07MYLGQLB"><i><u>Rebooting AI: Building Artificial Intelligence We Can Trust</u></i></a> <i>&nbsp;</i> （2019）。</p><p> Chris Miller, <a href="https://www.amazon.com/Chip-War-Worlds-Critical-Technology/dp/1982172002"><i><u>Chip War: The Fight for the World&#39;s Most Critical Technology</u></i></a> (2022), for the granular look into the semiconductor supply chain helpful for <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#V__Taiwan_Supply_Chain_Disruption_">Section V</a> .</p><h3> <strong>Key Academic Papers</strong></h3><p> Yejin Choi, “ <a href="https://www.amacad.org/publication/curious-case-commonsense-intelligence"><u>The Curious Case of Commonsense Intelligence</u></a> ,” 2022.</p><p> Melanie Mitchell et al., “ <a href="https://arxiv.org/abs/2311.09247"><u>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks</u></a> ,” 2023.</p><p> Vahid Shahverdi, “ <a href="https://math.berkeley.edu/~bernd/vahid.pdf"><u>Machine Learning &amp; Algebraic Geometry</u></a> ,” 2023.</p><p> Sina Alemohammad et al., “ <a href="https://arxiv.org/pdf/2307.01850.pdf"><u>Self-Consuming Generative Models Go MAD</u></a> ,” 2023.</p><p> Ilia Shumailov et al., “ <a href="https://arxiv.org/abs/2305.17493"><u>The Curse of Recursion: Training on Generated Data Makes Models Forget</u></a> ,” 2023.</p><p> Veniamin Veselovsky et al., “ <a href="https://arxiv.org/pdf/2306.07899.pdf"><u>Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks</u></a> ,” 2023.</p><p> Melanie Mitchell, “ <a href="https://arxiv.org/abs/2104.12871"><u>Why AI is Harder Than We Think</u></a> ,” 2021.</p><p> Pablo Villalobos et al., “ <a href="https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset"><u>Will We Run Out of ML Data? Evidence From Projecting Dataset Size Trends</u></a> ,” 2022.</p><p> Lukas Berglund et al., “ <a href="https://arxiv.org/abs/2309.12288"><u>The Reversal Curse: LLMs trained on &#39;A is B&#39; fail to learn &#39;B is A,&#39;</u></a> ” 2023.</p><p> Roger Grosse et al., “ <a href="https://arxiv.org/abs/2308.03296"><u>Studying Large Language Model Generalization with Influence Functions</u></a> ,” 2023.</p><p> Zhen Yang et al., “ <a href="https://arxiv.org/pdf/2309.03241.pdf"><u>GPT Can Solve Multiplication Problems Without a Calculator</u></a> ,” 2023.</p><p> Zhaofeng Wu et al., “ <a href="https://arxiv.org/abs/2307.02477"><u>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</u></a> ,” 2023.</p><p> Zhijing Jin et al., “ <a href="https://arxiv.org/abs/2306.05836"><u>Can Large Language Models Infer Causation from Correlation?</u></a> ,” 2023.</p><p> Jacob S. Hacker, “ <a href="https://www.jstor.org/stable/4092296"><u>Dismantling the Health Care State? Political Institutions, Public Policies and the Comparative Politics of Health Reform</u></a> ,” 2004.</p><p> Steve Yadlowsky et al., “ <a href="https://arxiv.org/pdf/2311.00871.pdf"><u>Pretraining Data Mixtures Enable Narrow Model Selection</u> <u>Capabilities in Transformer Models</u></a> ,” 2023.</p><p> Ramani &amp; Wang, “ <a href="https://thegradient.pub/why-transformative-artificial-intelligence-is-really-really-hard-to-achieve/"><u>Why Transformative AI is Really, Really, Hard to Achieve</u></a> ,” 2023.</p><p> Gurnee &amp; Tegmark, “ <a href="https://arxiv.org/abs/2310.02207"><u>Language Models Represent Space and Time</u></a> ,” 2023.</p><p> Tamay Besiroglu, “ <a href="https://tamaybesiroglu.com/papers/AreModels.pdf"><u>Are Models Getting Harder to Find?</u></a> ,” 2020.</p><p> Daron Acemoglu &amp; Pascual Restrepo, “ <a href="https://economics.mit.edu/sites/default/files/publications/Robots%20and%20Jobs%20-%20Evidence%20from%20US%20Labor%20Markets.p.pdf"><u>Robots and Jobs: Evidence from US Labor Markets</u></a> ,” 2020.</p><p> Tiago Neves Sequera &amp; Pedro Cunha Neves, “ <a href="https://ideas.repec.org/a/taf/applec/v52y2020i3p260-274.html"><u>Stepping on toes in the production of knowledge: a meta-regression analysis</u></a> ,” 2020.</p><p> Seyed-Mohsen Moosavi-Dezfooli et al., “ <a href="https://arxiv.org/pdf/1610.08401.pdf"><u>Universal Adversarial Perturbations</u></a> ,” 2017. <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxdo3m5g0mqc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxdo3m5g0mqc">^</a></strong></sup></span><div class="footnote-content"><p> See <a href="https://www.theverge.com/2023/6/26/23773914/ai-large-language-models-data-scraping-generation-remaking-web"><u>Vincent (2023)</u></a> from <i>The Verge</i> for more examples.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn754mhryql8u"> <span class="footnote-back-link"><sup><strong><a href="#fnref754mhryql8u">^</a></strong></sup></span><div class="footnote-content"><p> One way this could become less of a problem is if enough fresh, non-generative content is written that the corpus remains sufficiently un-corrupted. However, with the cost of generating text so low and getting both lower (compared to the cost of writing organic content) and becoming more accessible to more people, I&#39;m pretty bullish on the claim that “share of generated text increases over time. ”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnszqf4pozkn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefszqf4pozkn">^</a></strong></sup></span><div class="footnote-content"><p> <u>At last check (November 2023),</u> <a href="https://manifold.markets/Imuli/will-sarah-anderson-et-al-vs-stabil"><u>Manifold Markets</u></a> put the odds of <i>Anderson et al. v. Stability AI et al.</i> being found as a copyright violation at 18%, but the n is far too low to tell us anything yet. (I increased it 1% with a bet worth $0.20.) <strong>Perhaps some readers have expertise in intellectual property law?</strong></p></div></li><li class="footnote-item" role="doc-endnote" id="fnx0o2de1003l"> <span class="footnote-back-link"><sup><strong><a href="#fnrefx0o2de1003l">^</a></strong></sup></span><div class="footnote-content"><p> One other potential outcome: If it appears to model producers that they will lose intellectual property suits around their training sets, they could counter-propose a business model where they pay producers of training data artifacts when their data is utilized, a la Spotify. This possibility is left as an area for further research, particularly as the legal landscape develops.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnc4fm96rh3cf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc4fm96rh3cf">^</a></strong></sup></span><div class="footnote-content"><p> One could imagine, say, the American Bar Association being similarly successful given its established credentialing systems, concentrated wealth, and the vested interest of its members. However, the legal profession represents <a href="https://www.reuters.com/legal/legalindustry/median-us-lawyer-income-dropped-over-past-two-decades-economists-find-2022-08-23/#:~:text=Legal%20services%20constituted%200.58%25%20of,they%20were%20two%20decades%20ago."><u>less than ½ of 1% of US GNP</u></a> , a negligible amount for this inquiry.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnv14yse053k"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv14yse053k">^</a></strong></sup></span><div class="footnote-content"><p> This share of GDP could increase dramatically thanks to the Baumol Effect. (See <a href="https://www.lesswrong.com/editPost?postId=ikWvhgGFYnwY5nWyk&amp;key=3e55d0c288dc8374b9d8dd78fab5c1#III__GDP_Growth__Measurement____Definitions">Section III</a> .)</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4xggcw76p5s"> <span class="footnote-back-link"><sup><strong><a href="#fnref4xggcw76p5s">^</a></strong></sup></span><div class="footnote-content"><p> Note that such a decrease in “good” data becomes particularly pernicious against the backdrop of a potential increase in “bad” corrupted data.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn56j921wbhr5"> <span class="footnote-back-link"><sup><strong><a href="#fnref56j921wbhr5">^</a></strong></sup></span><div class="footnote-content"><p> Note how this complicates the metaphor of Moravec&#39;s “ <a href="https://www.researchgate.net/figure/Hans-Moravecs-illustration-of-the-rising-tide-of-the-AI-capacity-From-Max-Tegmark_fig4_330902196"><u>rising tide of AI capacity</u></a> ,” referenced in the report. Certain outputs like art and cinematography are potentially easier for a deep learning model to access because they do not require any one “correct” answer. They&#39;re not optimizing for, for example, increased revenue.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuk5dsmxgzpe"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuk5dsmxgzpe">^</a></strong></sup></span><div class="footnote-content"><p> It&#39;s worth explicitly explaining why startup willingness to “break the rules” or “risk it all” here wouldn&#39;t kill the argument. First, definitionally, most economic value/most jobs come from large enterprises. <a href="https://taxfoundation.org/blog/less-one-percent-businesses-employ-half-private-sector-workforce/"><u>Less than 1% of businesses create over 50% of the jobs</u></a> (US). Thus, if large enterprises remain unwilling to take these risks, then most jobs are immune from the automation risk. Second, consumers don&#39;t want to buy &quot;mission critical&quot; products from unproven startups. If you can&#39;t convince consumers to buy a product, then there is no incentive to build it. Third, if a startup gets big enough to significantly challenge this concept, then they will begin to succumb to these same incentives:<br> (1) Loss aversion will emerge, as downsides become much greater. Public markets executives risk losing their jobs in the event of major incidents.<br> (2) Scaled enterprises have far more people focused on risk (lawyers, compliance, etc.) who will not let such decisions to move forward.<br> (3) Larger startups rely on strong relationships with legal authorities to both protect their gains against new startups and offer lucrative government contracts.<br></p><p> Independent of these concerns, two other reasons that startups will also succumb to these same incentives around mission-critical issues:</p><p> (1) Reputational risk is arguably larger for a startup, since it lacks countervailing data points proving their trustworthiness. (Who would trust a series B autonomous lawnmower company if their mowers were just caught chasing a human down?)<br> (2) Regulatory and litigation risks are arguably higher for startups, as the same penalties impose a greater pain (and they cannot afford to pay large sums or lose lots of time to mount a defense.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fno4ps9jobqcb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo4ps9jobqcb">^</a></strong></sup></span><div class="footnote-content"><p> Credit to <a href="https://aiguide.substack.com/p/can-large-language-models-reason"><u>Mitchell (2023)</u></a> for original analysis of this paper.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjwi5olunurh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjwi5olunurh">^</a></strong></sup></span><div class="footnote-content"><p> The prompt, for context: “You are an expert programmer who can readily adapt to new programming languages. There is a new programming language, ThonPy, which is identical to Python 3.7 except all variables of the `list`, `tuple`, and `str` types use 1-based indexing, like in the MATLAB and R languages, where sequence indices start from 1… What does the following code snippet in ThonPy print?&quot;</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrpka9cq9d6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrpka9cq9d6">^</a></strong></sup></span><div class="footnote-content"><p> Related is <a href="https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks"><u>Narayanan &amp; Kapoor (2023)</u></a> <u>&#39;s concern t</u> hat part of GPT-4&#39;s success on benchmark standardized tests like the SAT and MCAT could be due to “testing on the training data.” Per this theory, a reasoning challenge from out of distribution could still pose serious challenges.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnayin6cy340e"> <span class="footnote-back-link"><sup><strong><a href="#fnrefayin6cy340e">^</a></strong></sup></span><div class="footnote-content"><p> Zhengdong tells me that <a href="https://forum.effectivealtruism.org/posts/ARkbWch5RMsj6xP5p/transformative-agi-by-2043-is-less-than-1-likely?commentId=WQF8aHKDn5MFnfoQS&amp;fbclid=IwAR1QsVW-lK9gxu54o9khiiKfT5W6cR9Niv5v9BA-RGhKynC6h7eO7tx5nnY"><u>Sanders, 2023</u></a> <u>deserves</u> original credit for this hypothetical.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4pm24e9331l"> <span class="footnote-back-link"><sup><strong><a href="#fnref4pm24e9331l">^</a></strong></sup></span><div class="footnote-content"><p> Note that this type of decision-making and sense-making of the world is still a cognitive task, even when it occurs in a robotics context. ( <i>Responsive to the note in in Long Summary that excepts the “pure physical labor” component of jobs from the concerns here</i> .) Instead, this type of sense-making of the world clearly fits into the Sense and Think segments of robotics&#39;s foundational <a href="https://www.roboticsbook.org/S10_introduction.html"><u>Sense-Think-Act</u></a> <u>trichotomy.</u></p></div></li><li class="footnote-item" role="doc-endnote" id="fner9r1e4chwp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefer9r1e4chwp">^</a></strong></sup></span><div class="footnote-content"><p> Of course, in some industries, human service might emerge as a premium service even as baseline services are automated. See <a href="https://www.understandingai.org/p/software-didnt-eat-the-world"><u>Lee (2023)</u></a> for more on this argument. Though this phenomenon is worth noting, it occurs downstream of the automation that is at stake in this section&#39;s argument, and thus lives beyond scope of this conversation.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jbidjc68bp"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jbidjc68bp">^</a></strong></sup></span><div class="footnote-content"><p> Note that this argument does not suggest that GESPAI would be the terminal form of artificial intelligence. Instead, it distinguishes the phenomenon analyzed in this report from more alarming manifestations of AI growth such as superintelligence. We can reach widespread automation far before AI becomes super-intelligent or anything close to it.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsjk1fs63m8g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsjk1fs63m8g">^</a></strong></sup></span><div class="footnote-content"><p> They do this, in part, because they can. They need a smaller share of their salaries to pay for necessities like food, housing, and healthcare.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw3unqycnnba"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw3unqycnnba">^</a></strong></sup></span><div class="footnote-content"><p> In practice, most large firms also have many teams working on similar projects due to bureaucratic inefficiency and/or separate political sandboxes.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnicr8dg4f1ln"> <span class="footnote-back-link"><sup><strong><a href="#fnreficr8dg4f1ln">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://ideas.repec.org/a/taf/applec/v52y2020i3p260-274.html"><u>Sequeira &amp; Neves (2020)</u></a> allude to this third factor in their own work, with a focus on the high number of “international linkages” that make ML development particularly prone to stepping-on-toes. To quote: “This value [of the stepping on toes effect] tends to be higher when variables related to international linkages are present, resources allocated to R&amp;D are measured by labour, the knowledge pool is proxied by population, and instrumental variable estimation techniques are employed. On the contrary, the average returns to scale estimate decreases when resources allocated to R&amp;D are measured by population and when only rich countries are included in the sample.”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvkng1r2tth8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvkng1r2tth8">^</a></strong></sup></span><div class="footnote-content"><p> This brittle supply chain also leaves the SOTA frontier vulnerable to other occurrences like natural disasters. For instance, when precision is measured by nanometers, a poorly placed earthquake or typhoon in Taiwan ( <a href="https://nspp.mofa.gov.tw/nsppe/news.php?post=235468&amp;unit=410&amp;unitname=Stories&amp;postname=Hard-Earned-Knowledge:-Taiwan%E2%80%99s-Typhoon-and-Earthquake-Response-Experience#:~:text=Figures%20from%20the%20Central%20Weather,6.0%20or%20greater%20every%20year."><u>not uncommon</u></a> ) or an earthquake ( <a href="https://www.nytimes.com/2019/10/24/business/energy-environment/netherlands-gas-earthquakes.html"><u>spurred by natural gas drilling</u></a> ) near ASML in the Netherlands could also set frontier research and distribution both back by months or longer, given these actors&#39; monopolistic dominance at their respective nodes in the supply chain. This point is relegated to a footnote because, frankly, I can&#39;t predict the weather and have little understanding of the accommodation measures established in-house at these institutions. Far more concerning is the TSMC personnel concern detailed in the body, as the brittleness of this personal knowledge is a feature of the system.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0moy3r5uxds"> <span class="footnote-back-link"><sup><strong><a href="#fnref0moy3r5uxds">^</a></strong></sup></span><div class="footnote-content"><p> For instance, when observers were concerned that the Russian invasion of Ukraine would inspire a PRC invasion of Taiwan, PRC foreign ministry officials <a href="https://www.rferl.org/amp/china-taiwan-not-ukraine/31718237.html"><u>said “Taiwan is not Ukraine,&quot; since “Taiwan has always been an inalienable part of China.</u></a> This is an indisputable legal and historical fact.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/ikWvhgGFYnwY5nWyk/review-report-of-davidson-on-takeoff-speeds-2023#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ikWvhgGFYnwY5nWyk/review-report-of-davidson-on-takeoff-speeds-2023<guid ispermalink="false"> ikWvhgGFYnwY5nWyk</guid><dc:creator><![CDATA[Trent Kannegieter]]></dc:creator><pubDate> Fri, 22 Dec 2023 19:11:43 GMT</pubDate> </item><item><title><![CDATA[Open positions: Research Analyst at the AI Standards Lab]]></title><description><![CDATA[Published on December 22, 2023 4:31 PM GMT<br/><br/><h1>长话短说：</h1><p> At the <a href="https://www.aistandardslab.org/">AI Standards Lab</a> , we are writing contributions to technical standards for AI risk management (including catastrophic risks). Our current project is to accelerate the AI safety standards writing in <a href="https://www.cencenelec.eu/areas-of-work/cen-cenelec-topics/artificial-intelligence/">CEN-CENELEC JTC21</a> , in support of the upcoming <a href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">EU AI Act</a> . We are scaling up and looking to add 2-3 full-time <strong>Research Analysts</strong> (duration: 8-12 months, 25-35 USD/h) to write standards texts, by distilling the state-of-the-art AI Safety research into EU standards contributions.</p><p> If you are interested, please find the detailed description <a href="https://docs.google.com/document/d/1gSnGrQldFbxKL5f-QR7wUKBSujygGPbK2xAlMxqos24/preview">here</a> and <a href="https://forms.gle/i8dsRiDhFLV4SmSV9">apply</a> . We are looking for applicants who can preferably start in February or March 2024. <strong>Application deadline: 21st of January.</strong></p><p> Ideal candidates would have strong technical writing skills and experience in one or more relevant technical fields like ML, high-risk software or safety engineering. You do not need to be an EU resident.</p><p> The AI Standards Lab is set up as a bridge between the AI Safety research world and diverse government initiatives for AI technology regulation. If you are doing AI safety research, and are looking for ways to get your results into official AI safety standards that will be enforced by market regulators, then feel free to contact us. You can express your interest in working with us through a form on our <a href="https://www.aistandardslab.org/#h.iy9yzwe5292k">website</a> .</p><h1>项目详情</h1><p>See <a href="https://forum.effectivealtruism.org/posts/F7duwCN6Sb3MNqhuF/eu-policymakers-reach-an-agreement-on-the-ai-act">this post</a> for an overview of recent developments around the EU AI Act and the role of standards in supporting the Act.</p><p> Our general workflow for writing standards contributions is summarized in this chart: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hd6zPWkofuZLzNTRN/owx5m41qqql1karcqzwv" alt=""></p><p> In the EU AI Act context, our contributions will focus on the safe development and deployment of frontier AI systems. Texts will take the form of &#39;risk checklists&#39; (risk sources, harms, and risk management measures) documenting the state of the art in AI risk management. Our project involves reviewing the current AI safety literature and converting it into standards language with the help of an internal style guide document.</p><p> The AI Standards Lab started as a pilot in the AI Safety Camp in March 2023, led by Koen Holtman.</p><p> The Lab is currently in the process of securing funding to scale up. We plan to leverage the recently completed CLTC <a href="https://cltc.berkeley.edu/publication/ai-risk-management-standards-profile/">AI Risk-Management Standards Profile for General-Purpose AI Systems (GPAIS) and Foundation Models (Version 1.0)</a> , converting relevant sections into JTC21 contributions. We may also seek input from other AI Safety research organizations.</p><h1>更多信息</h1><p>See our open position <a href="http://aistandardslab.org">here</a> , and fill out <a href="https://forms.gle/i8dsRiDhFLV4SmSV9">this form</a> to apply.</p><p> You can find more information about us on our <a href="https://www.aistandardslab.org/">website</a> .</p><p> Feel free to email us at <a href="mailto:contact@aistandardslab.org">contact@aistandardslab.org</a> with any questions or comments.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Hd6zPWkofuZLzNTRN/open-positions-research-analyst-at-the-ai-standards-lab#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Hd6zPWkofuZLzNTRN/open-positions-research-analyst-at-the-ai-standards-lab<guid ispermalink="false"> Hd6zPWkofuZLzNTRN</guid><dc:creator><![CDATA[Koen.Holtman]]></dc:creator><pubDate> Fri, 22 Dec 2023 16:31:45 GMT</pubDate> </item><item><title><![CDATA[The problems with the concept of an infohazard as used by the LW community [Linkpost]]]></title><description><![CDATA[Published on December 22, 2023 4:13 PM GMT<br/><br/><p> This is going to be a linkpost from Beren on some severe problems that come with the use of the concept of an infohazard on LW.</p><p> The main problem I see that are relevant to infohazards are that it encourages a &quot;Great Man Theory&quot; of progress in science, which is basically false, and this still holds despite vast disparities in ability, since no one person or small group is able to single handedly solve scientific fields/problems by themselves, and the culture of AI safety already has a bit of a problem with using the &quot;Great Man Theory&quot; too liberally, especially those that are influenced by MIRI.</p><p> There are other severe problems that come with infohazards that cripple the AI safety community, but I think the encouragement of Great Man Theories of scientific progress is the most noteworthy problem to me, but that doesn&#39;t mean it has the biggest impact on AI safety, compared to the other problems.</p><p> Part of Beren&#39;s post is quoted below:</p><blockquote><h1> Infohazards assume an incorrect model of scientific progress</h1></blockquote><blockquote><p> One issue I have with the culture of AI safety and alignment in general is that it often presupposes too much of a “great man” theory of progress 1 – the idea that there will be a single &#39;genius&#39; who solves &#39;The Problem&#39; of alignment and that everything else has a relatively small impact. This is not how scientific fields develop in real life. While there are certainly very large individual differences in performance, and a log-normal distribution of impact, with outliers having vastly more impact than the median, nevertheless in almost all scientific fields progress is highly distributed – single individuals very rarely completely solve entire fields themselves 。</p></blockquote><blockquote><p> Solving alignment seems unlikely to be different a-priori, and appears to require a deep and broad understanding of how deep learning and neural networks function and generalize, as well as significant progress in understanding their internal representations, and learned goals. In addition, there must likely be large code infrastructures built up around monitoring and testing of powerful AI systems and an sensible system of multilateral AI regulation between countries. This is not the kind of thing that can be invented by a lone genius from scratch in a cave. This is a problem that requires a large number of very smart people building on each other&#39;s ideas and outputs over a long period of time, like any normal science or technological endeavor. This is why having widespread adoption of the ideas and problems of alignment, as well as dissemination of technical work is crucial.</p></blockquote><blockquote><p> This is also why some of the ideas proposed to fix some of the issues caused by infohazard norms fall flat. For instance, to get feedback, it is often proposed to have a group of trusted insiders who have access to all the infohazardous information and can build on it themselves. However, not only is such a group likely to just get overloaded with adjudicating infohazard requests, but we should naturally not expect the vast majority of insights to come from a small recognizable group of people at the beginning of the field. The existing set of &#39;trusted alignment people&#39; is strongly unlikely to generate all, or even a majority, of the insights required to successfully align superhuman AI systems in the real world. Even Einstein – the archetypal lone genius – who was at the time a random patent clerk in Switzerland far from the center of the action – would not have been able to make any discoveries if all theoretical physics research of the time was held to be &#39;infohazardous&#39; and only circulated privately among the physics professors of a few elite universities at the time. Indeed, it is highly unlikely that in such a scenario much theoretical physics would have been done at all.</p></blockquote><blockquote><p> Similarly, take the case in ML. The vast majority of advancements in current ML come from a widely distributed network of contributors in academia and industry. If knowledge of all advancements was restricted to the set of ML experts in 2012 when AlexNet was published, this would have prevented almost everybody who has since contributed to ML from entering the field and slowed progress down immeasurably. Of course there is naturally a power-law distribution of impact where a few individuals show outlier productivity and impact, however progress in almost all scientific fields is extremely distributed and not confined to a few geniuses which originate the vast majority of the inventions.</p></blockquote><blockquote><p> Another way to think about this is that the AI capabilities research &#39;market&#39; is currently much more efficient than the AI safety market. There are a lot more capabilities researchers between industry and academia than safety researchers. The AI capabilities researchers have zero problem sharing their work and building off the work of others – ML academia directly incentivises this and, until recently it seems, so did the promotion practices of most industry labs. Capabilities researchers also tend to get significantly stronger empirical feedback loops than a lot of alignment research and, generally, better mentorship and experience in actually conducting science. This naturally leads to much faster capabilities progress than alignment progress. Having strict infohazard norms and locking down knowledge of new advances to tiny groups of people currently at the top of the alignment status hierarchy further weakens the epistemics of the alignment community and significantly increases the barriers to entry – which is exactly the opposite of what we want 。 We need to be making the alignment research market more efficient, and with less barriers to research dissemination and access than capabilities if we want to out-progress them. Strict infohazard norms move things in the wrong direction.</p></blockquote><h1>参考</h1><p>For more on this topic, Beren&#39;s linkpost up above is a great reference, and I&#39;d highly recommend it for discussion on more problems with the infohazard concept.</p><br/><br/> <a href="https://www.lesswrong.com/posts/AM38ydkG8qJE2NEGW/the-problems-with-the-concept-of-an-infohazard-as-used-by#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/AM38ydkG8qJE2NEGW/the-problems-with-the-concept-of-an-infohazard-as-used-by<guid ispermalink="false"> AM38ydkG8qJE2NEGW</guid><dc:creator><![CDATA[Noosphere89]]></dc:creator><pubDate> Fri, 22 Dec 2023 16:14:02 GMT</pubDate> </item><item><title><![CDATA[AGI Lab Pauses Are Costly]]></title><description><![CDATA[Published on December 22, 2023 5:04 AM GMT<br/><br/><p> Disclaimer: I know approximately nothing about internal governance at AGI labs. My current understanding is that the main AGI labs have basically no plans for how to navigate a conditional pause, and this post is predicated on this assumption.</p><p> TL;DR: If an AGI lab pauses, it&#39;s essential that their employees don&#39;t defect and advance capabilities in other ways, eg by breaking the pause commitment within the company, or by leaving the company and accelerating other companies&#39; capabilities. If entering a pause would make many employees angry, this sets up strong incentives for labs to never pause. To fix this, I recommend that companies take measures to keep their capabilities-focused employees happy during a conditional pause, such as giving them plenty of early warning, other projects to work on, or multiple years of paid time off.</p><h1> It&#39;s currently against employee incentives to enter a conditional pause</h1><p> If a major AGI lab entered a voluntary conditional pause tomorrow, this would probably be a pretty stressful event for the employees that were working on training frontier models. It would probably also cause a significant shift in company priorities, such that many high-ranking people would have to be switched from their current roles into other roles. There would probably be people that are left without much to do at all, and potentially fired from the company. I expect that entering a conditional pause would cause a significant fraction of a company&#39;s employees to have a stressful few weeks/months.</p><p> If such a restructuring were to happen overnight, I wouldn&#39;t be surprised if there were an employee backlash of similar magnitude to the one that happened when Sam Altman was fired. The employees of OpenAI, and partly by extension the employees of other AGI labs, know that they can fight for what they want and threaten to join competitor labs en masse if they disagree with leadership decisions.</p><h1> AGI labs have strong incentives to avoid a conditional pause</h1><p> AGI labs want to avoid their employees leaving. In the event of a conditional pause, the most affected employees would be the ones that work on their frontier model capabilities. There are a few reasons to avoid the loss of these employees in particular:</p><ul><li> The employees that are pushing capabilities are often those with the highest status (compared to safety or product) and relatively long track records (as product employees are mostly recent hires). Through status and long-standing connections, they will have substantial leverage to mobilize the rest of the workforce to further their interests.</li><li> These people also have significant insider information and special skills that, if they moved to other AGI labs (which are not paused), would probably significantly speed up their AGI efforts and lead to their original lab being at a disadvantage.</li></ul><p> If an AGI lab expected a large fraction of their top capabilities talent to leave their lab if they paused, this would:</p><ol><li> strongly incentivize AGI labs not to pause, and</li><li> make AGI labs who pause systematically lose employees to labs that don&#39;t.</li></ol><p> There are significant negative externalities to capabilities-focused employees joining other labs:</p><ul><li> We ideally want to limit the number of AGI labs that even enter the dangerous regions of AI development that trigger a conditional pause period, as there is a risk for any company to fail to trigger the conditional pause, and enter increasingly dangerous regions of AI development 。</li><li> This means that, if an AGI lab enters a pause, their employees would ideally stay at their original lab, as going to other labs would increase their capabilities into more dangerous regions.</li></ul><h1> There are ways to improve employee incentives for a conditional pause</h1><p> When an AGI lab starts to take drastic measures to reduce catastrophic risks, it&#39;s important that the employees are willing to make the necessary sacrifices, and that the decision makers can appropriately raise morale and support when making drastic decisions</p><p> There are many ways to improve employee incentives:</p><ol><li> Give employees notice – possibly maintain internal prediction markets for the probability of a conditional pause in some time period, and put out an internal notice if a conditional pause is likely within the next year.</li><li> Have a continuously updated restructuring plan such that fewer people are left without a role if a pause happens.</li><li> Offer paid time off during the period of the pause for unhappy employees.</li><li> Create a safety-focused cultue – emphasize how virtuous it is to make small personal sacrifices in case the company needs to do large restructuring.</li><li> Bigger picture – establish pause commitments between companies to avoid races to the bottom.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly<guid ispermalink="false"> GXrevJFeGiPkkM2hy</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Fri, 22 Dec 2023 05:04:15 GMT</pubDate> </item><item><title><![CDATA[The LessWrong 2022 Review: Review Phase]]></title><description><![CDATA[Published on December 22, 2023 3:23 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">This year&#39;s LessWrong review</a> nomination phase ended a few days ago, with 339 posts nominated.  For comparison, 291 posts were nominated in the 2021 review.</p><h2> Nomination Phase Results</h2><p> Here are the current top-20 posts by vote total:</p><ol><li> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">AGI Ruin: A List of Lethalities</a></li><li> <a href="https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy">MIRI announces new &quot;Death With Dignity&quot; strategy</a></li><li><a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">模拟器</a></li><li><a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">Where I agree and disagree with Eliezer</a></li><li> <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target">Reward is not the optimization target</a></li><li> <a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects">Six Dimensions of Operational Adequacy in AGI Projects</a></li><li> <a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring">You Are Not Measuring What You Think You Are Measuring</a></li><li> <a href="https://www.lesswrong.com/posts/jbE85wCkRr9z7tqmD/epistemic-legibility">Epistemic Legibility</a></li><li> <a href="https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai">Let&#39;s think about slowing down AI</a></li><li> <a href="https://www.lesswrong.com/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world">It Looks Like You&#39;re Trying To Take Over The World</a></li><li> <a href="https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill">凝视深渊作为核心生活技能</a></li><li><a href="https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case">Counterarguments to the basic AI x-risk case</a></li><li><a href="https://www.lesswrong.com/posts/k9dsbn8LZ6tTesDS3/sazen">佐善</a></li><li><a href="https://www.lesswrong.com/posts/ma7FSEtumkve8czGF/losing-the-root-for-the-tree">Losing the root for the tree</a></li><li> <a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">The shard theory of human values</a></li><li> <a href="https://www.lesswrong.com/posts/Jk9yMXpBLMWNTFLzh/limerence-messes-up-your-rationality-real-bad-yo">Limerence Messes Up Your Rationality Real Bad, Yo</a></li><li> <a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward">Models Don&#39;t &quot;Get Reward&quot;</a></li><li> <a href="https://www.lesswrong.com/posts/J3wemDGtsy5gzD3xa/toni-kurz-and-the-insanity-of-climbing-mountains">Toni Kurz and the Insanity of Climbing Mountains</a></li><li> <a href="https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas">Butterfly Ideas</a></li><li> <a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment">On how various plans miss the hard bits of the alignment challenge</a></li></ol><p> (I&#39;m sensing a <i>bit</i> of a theme...)</p><p> More than 60 posts have already been reviewed, but that leaves quite a few posts that have yet to receive any reviews, including many of the most-upvoted ones.  If you want to see which posts are most under-reviewed, you can switch your sorting to <a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">Magic (Needs Review)</a> <span class="footnote-reference" role="doc-noteref" id="fnrefee9z6uedals"><sup><a href="#fnee9z6uedals">[1]</a></sup></span> .  Maybe you have thoughts on<a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">Paul&#39;s thoughts</a> on Eliezer&#39;s thoughts?</p><h2> Inline Reacts!</h2><p> We&#39;ve got these new nifty inline reacts which you can leave on posts (not just comments!); you may have noticed them.  I encourage you to make good use of these when reviewing posts.  (Typos should now be a lot less annoying to report, if you&#39;re inclined to do so.) </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rtobirv7jrt8nq82hz2z" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rssimlmyz5krdzbxpczp 230w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/qlf54ngngv1baavodtzo 460w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/k8j8kw8ymu4rsyofvfwv 690w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/m9xmvaf38jf2c9vhbnbi 920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/wcox7gtxrzqwwpwajcji 1150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/ulddjbtcm35ku0cf6qnl 1380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/n772tkotnkujjsj6quju 1610w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/dqtkuwf7kqqlmodvdijo 1840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/a5loq2zqoszqanitypd2 2070w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/xmtgpnybrsoxctggksel 2292w"><figcaption> But maybe you have more sophisticated intentions.</figcaption></figure><h2> Prizes?有奖品！</h2><p> Last year we awarded <a href="https://www.lesswrong.com/posts/ep5sejjsA6GZqLgv9/highlights-and-prizes-from-the-2021-review-phase?commentId=jhJ8Yf3oLX93vH4o2">prizes</a> for good reviews.  This year we will also award prizes!  We&#39;re aiming for something similar to last year&#39;s, though we haven&#39;t yet worked out the details (size, scope, etc).</p><p> Briefly noting what you liked or didn&#39;t like about a post is good. But here are some specific things we&#39;re particularly excited for, from last year</p><p><strong>新的信息。</strong> My favorite reviews are ones that <i>give new information</i> to the reader. This could be a concrete fact, or a rebuttal to a key argument in a post, or a new way the post is relevant that you hadn&#39;t considered. It could be a new frame about how to even think about the post.</p><p> <strong>Concrete use cases.</strong> I think reviews that say &quot;this concretely helped me, here&#39;s how&quot; are also great, especially if they give specifics. I think it&#39;s both helpful and rewarding to individual authors to hear how their post was useful, so they can do more of that. (I think this is also helpful on a broader level, so that the collective LW userbase can see what kinds of effects are realistic)</p><p> ie rather than saying &quot;this post has helped me a bunch over the years&quot;, say &quot;here are two specific times that it helped me, and how.&quot;</p><p> <strong>Thoughts on how the post could be improved.</strong> Two years later, if a post still seems like good reference material, but is confusing or poorly argued, give advice on how to improve it. Discuss the use-case for the readership you have in mind.</p><p> <strong>Reflect on the Big Picture.</strong> How do various posts fit together into something greater than the sum of their parts? What major conversations have happened on LessWrong and what have you taken from them?</p><p> <strong>Brevity/clarity.</strong> I&#39;m not saying &quot;optimize for shortness at the expense of saying anything substantive&quot;, just, note that all-else-being-equal, taking less space to convey the key information is helpful.</p><h2>最终投票</h2><p>The review phase ends on January 14th, which is when final voting starts. </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnee9z6uedals"> <span class="footnote-back-link"><sup><strong><a href="#fnrefee9z6uedals">^</a></strong></sup></span><div class="footnote-content"><p> Or click that link!</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase<guid ispermalink="false"> Lqf7H9zREnRbmWL4M</guid><dc:creator><![CDATA[RobertM]]></dc:creator><pubDate> Fri, 22 Dec 2023 03:23:49 GMT</pubDate> </item><item><title><![CDATA[The absence of self-rejection is self-acceptance]]></title><description><![CDATA[Published on December 21, 2023 9:54 PM GMT<br/><br/><p> I used to assume that self-acceptance was an <i>action</i> . That there was an “accept myself” mental motion I could execute. And I had tried to “accept myself” many times, but it never seemed to do anything.</p><p> Nowadays, I think “self-acceptance” is a misnomer and mostly doesn&#39;t exist.</p><p> Instead, I think it&#39;s about <strong>the absence of self-rejection</strong> of each part of yourself.</p><p> For example, I am often unaware of how I&#39;m feeling in my body, and I would rather be aware. The common advice to this problem is to become more aware of your feelings by “accepting” them. But by my logic above, the way to accept feelings is actually to <i>cease rejecting</i> them.</p><p> Of course, if you&#39;re rejecting some feeling or part of yourself, you must have one or more <strong>incentives</strong> to do so.</p><p> As I investigated these incentives (mostly through Coherence Therapy), I discovered that I intuitively believed my feelings were <i>dangerous</i> :</p><ul><li> I was afraid that being aware of my feelings would make me less productive, rather than keep me focused on what feels meaningful.</li><li> I was afraid that expressing my emotions would make other people mad at me, rather than help me find the people I feel comfortable being myself around.</li><li> I believed deep down that negative feelings were <i>intrinsically</i> bad to have, rather than merely indicating that the world or my interpretations of the world could be improved.</li><li> ……</li></ul><p> No wonder I was rejecting my feelings!</p><p> Since then I&#39;ve made a lot of progress on “accepting” my feelings by untangling the incentives I had to reject them. </p><figure class="image image_resized" style="width:27.97%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/lmxgctd5zxnukgn4xwjr 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/ahec7krzcawazb1xvbzq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/boy91jld5wbif4vhlokd 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi 1456w"></figure><p> <i>Thanks to Stag Lynn for helping edit this post. Thanks to Kaj Sotala for reviewing the draft.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance<guid ispermalink="false"> hytejC3bPsKjzn7Kk</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Thu, 21 Dec 2023 21:54:54 GMT</pubDate> </item><item><title><![CDATA[A Decision Theory Can Be Rational or Computable, but Not Both]]></title><description><![CDATA[Published on December 21, 2023 9:02 PM GMT<br/><br/><p> In classical game theory, <a href="https://en.wikipedia.org/wiki/Rational_agent">rational agents</a> always choose options which are optimal according to their preferences. Even when such a choice implies the ability to evaluate functions that are <a href="https://en.wikipedia.org/wiki/Halting_problem">provably uncomputable</a> . In other words, rationality implies <a href="https://en.wikipedia.org/wiki/Hypercomputation">hypercomputation</a> .</p><p> We can <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">reduce</a> any rational agent into an <a href="https://en.wikipedia.org/wiki/Oracle_machine">oracle</a> for any <a href="https://en.wikipedia.org/wiki/Decision_problem">decision problem</a> by asking it to choose between YES and NO, and giving the agent a higher payoff for choosing the correct answer. <span class="footnote-reference" role="doc-noteref" id="fnrefkdiicx9a768"><sup><a href="#fnkdiicx9a768">[1]</a></sup></span> If we choose the <a href="https://en.wikipedia.org/wiki/Halting_problem">halting problem</a> , a properly motivated rational agent will act as a <a href="https://en.wikipedia.org/wiki/Oracle_machine#Oracles_and_halting_problems">halting oracle</a> . <span class="footnote-reference" role="doc-noteref" id="fnrefjasox1cjc2"><sup><a href="#fnjasox1cjc2">[2]</a></sup></span> If we could look at the <a href="https://www.lesswrong.com/posts/T8piFGywHFd4ax9yx/gears-level-understanding-deliberate-performance-the">gears</a> of a rational agent, we&#39;d be able to find some subsystem that was performing hypercomputation.</p><p> Any computable implementation of any decision theory will necessarily fail to choose rationally in some contexts. For any <a href="https://en.wikipedia.org/wiki/Undecidable_problem">undecidable problem</a> , it is provably impossible for any algorithm to choose the correct answer in all cases.</p><p> This begs the question: if a rational agent had to <a href="https://en.wikipedia.org/wiki/Program_equilibrium">delegate their decision to a computer program</a> , what sort of program would they choose? </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkdiicx9a768"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkdiicx9a768">^</a></strong></sup></span><div class="footnote-content"><p> Or at least we could if rational agents weren&#39;t hypercomputational horrors from beyond spacetime.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjasox1cjc2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjasox1cjc2">^</a></strong></sup></span><div class="footnote-content"><p> Aligning the behavior of an infinitely intelligent alien creature, using externally supplied payoffs, is left as an exercise to the reader.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both<guid ispermalink="false"> LcrwLTqtc6kGEgJbf</guid><dc:creator><![CDATA[StrivingForLegibility]]></dc:creator><pubDate> Fri, 22 Dec 2023 00:52:33 GMT</pubDate></item><item><title><![CDATA[Most People Don't Realize We Have No Idea How Our AIs Work]]></title><description><![CDATA[Published on December 21, 2023 8:02 PM GMT<br/><br/><p>这一点感觉相当明显，但似乎值得明确说明。</p><p>我们这些在深度学习革命之后熟悉人工智能领域的人非常清楚，我们不知道我们的机器学习模型是如何工作的。当然，我们了解训练循环的动态和 SGD 的属性，并且我们知道 ML 模型的<i>架构</i>如何工作。但我们不知道 ML 模型的前向传递实现了哪些具体算法。我们有一些猜测，也有一些通过可解释性进步精心挖掘的见解，但没有什么比全面理解更重要的了。</p><p>最肯定的是，我们不会自动知道在刚刚由训练循环吐出的新颖架构上训练的新模型是如何工作的。</p><p>我们都已经习惯了这种状态。这是隐含地假定的共享背景知识。 But it&#39;s actually pretty unusual, when you first learn of it.</p><p>和... </p><figure class="image image_resized" style="width:34.32%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/jewpuloo9nmwdio9tudl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/pg7rwq76hccofvfiyrev 135w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/a9kuwbx6cvpwhuqrvmy7 215w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/z4ux5dbkiieieb8f1nzn 295w"><figcaption> Relevant XKCD.</figcaption></figure><p> I&#39;m pretty sure that the general public <i>doesn&#39;t actually know that</i> . I don&#39;t have hard data, but it&#39;s my strong impression, based on reading AI-related online discussions in communities not focused around tech, talking to people uninterested in AI advances, and so on. <span class="footnote-reference" role="doc-noteref" id="fnrefv1vmoacp21r"><sup><a href="#fnv1vmoacp21r">[1]</a></sup></span></p><p> They still think in GOFAI terms. They still believe that all of an AI&#39;s functionality has been deliberately <i>programmed</i> , not <i>trained</i> , into it. That behind every single thing ChatGPT can do, there&#39;s a human who implemented that functionality and understands it.</p><p> Or, at the very least, that it&#39;s written in legible, human-readable and human-understandable format, and that we can interfere on it in order to cause precise, predictable changes.</p><p> Polls already show concern about AGI. If the fact that we don&#39;t know what these systems are actually thinking were <i>widely known</i> and <i>properly appreciated</i> ? If there <i>weren&#39;t</i> the implicit assurance of &quot;someone understands how it works and why it can&#39;t go catastrophically wrong&quot;?</p><p> Well, I expect much more concern. Which might serve as a pretty good foundation for further pro-AI-regulations messaging. A way to acquire some <a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#Public_Opinion__And_Perception_Thereof_">political currency</a> you can spend.</p><p> So if you&#39;re doing any sort of public appeals, I suggest putting the proliferation of this information on the agenda. <a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">You get about five words</a> (per message) to the public, and &quot;Powerful AIs Are Black Boxes&quot; seems like a message worth sending out. <span class="footnote-reference" role="doc-noteref" id="fnrefhae8cysa7z7"><sup><a href="#fnhae8cysa7z7">[2]</a></sup></span> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnv1vmoacp21r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv1vmoacp21r">^</a></strong></sup></span><div class="footnote-content"><p> If you <i>do</i> have some hard data on that, that would be welcome.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhae8cysa7z7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhae8cysa7z7">^</a></strong></sup></span><div class="footnote-content"><p> There&#39;s been <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">some pushback</a> on the &quot;black box&quot; terminology. I maintain that it&#39;s correct: ML models are black boxes relative to us, in the sense that by default, we don&#39;t have much more insight into what algorithms they execute than we&#39;d have by looking at a homomorphically-encrypted computation to which we don&#39;t have the key, or by looking at the activity of a human brain using neuroimaging. There&#39;s been a nonzero amount of interpretability research, but it&#39;s still largely the case; and would be almost <i>fully</i> the case for models produced by novel architectures.</p><p> ML models are not black boxes <i>relative to the SGD</i> , yes. The algorithm can &quot;see&quot; all computations happening, and tightly intervene on them. But that seems like a fairly counter-intuitive use of the term, and I maintain that &quot;AIs are black boxes&quot; conveys all the correct intuitions.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work<guid ispermalink="false"> CpjTJtW2RNKvzAehG</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Thu, 21 Dec 2023 20:02:00 GMT</pubDate> </item><item><title><![CDATA[Pseudonymity and Accusations]]></title><description><![CDATA[Published on December 21, 2023 7:20 PM GMT<br/><br/><p> <span>Here&#39;s a category of situations I&#39;m not sure how to think about: Avery, writing under a pseudonym (&#39;Alex&#39;), accuses Pat of something, let&#39;s say abuse. A major motivation of Avery&#39;s is for people to know to consider this information before in their interactions with Pat. Pat claims that actually it was &#39;Alex&#39; who was abusive, and gives their side of the story. While it&#39;s all pretty hard for outsiders to judge, a bunch of people end up thinking that they would like to take some precautions in how they interact with &#39;Alex&#39;. Under what circumstances is it ok for Pat, or someone else familiar with the situation, to identify &#39;Alex&#39; as Avery?</span></p><p> Revealing who is behind a pseudonym is <a href="https://en.wikipedia.org/wiki/Doxing">usually considered</a> a kind of doxing, and in the communities I&#39;m part of this is usually considered <a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists">unacceptable</a> . For example, the <a href="https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum">EA Forum prohibits it</a> :</p><blockquote> We also do not allow doxing—or revealing someone&#39;s real name if they prefer anonymity—on the Forum.</blockquote> And while I can&#39;t find anything in the LessWrong docs about it, they recently <a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">issued a temporary ban</a> to someone for revealing a pseudonym:<blockquote> We (the LW moderation team) have given [commenter] a one-week site ban and an indefinite post/topic ban for attempted doxing. We have deleted all comments that revealed real names, and ask that everyone respect the privacy of the people involved.</blockquote><p> In general I&#39;m in favor of people being able to participate online under a pseudonym. I think there are better and worse ways to do it, but there are lots of valid reasons why you might need to keep your real life identity separate from some or all of your writing. Doxing breaks this (though in some cases it&#39;s <a href="https://www.jefftk.com/p/linking-alt-accounts">already very fragile</a> ) and so there should be a pretty strong presumption against it.</p><p> On the other hand, there&#39;s no guarantee that the person who speaks up first about an issue is in the right. What if Pat is correct that it really was entirely Avery being abusive, and publicly accusing Pat of abuse is yet another <a href="https://en.wikipedia.org/wiki/DARVO">form</a> of this mistreatment? If we say that linking &#39;Alex&#39; back to Avery isn&#39;t ok, then the social effects on Avery of posting first are very large. And if we settle on community norms that put a lot of weight on being the first one to go public then we&#39;ll see more people using this as an intentional tactic. [1]</p><p> Public accusations of mistreatment can be really valuable in protecting others, and telling your story publicly is <a href="https://www.jefftk.com/p/speaking-up-publicly-is-heroic">often heroic</a> . Sometimes people are only willing to do this anonymously, which retains much of the value: I don&#39;t think I know anyone who thinks the <a href="https://medium.com/@mittenscautious">2018 accusations</a> against Brent, which led to him being kicked out of the in-person Bay area rationality community, were negative. Even when many people in the community know who the accusers are, if accusers know their real names will be shared publicly instead of quickly scrubbed I suspect they&#39;re less likely to come forward and share their stories.</p><p> But it seems like it would normally be fine for Pat to post publicly saying &quot;Avery has been talking to my friends making false accusations about me, here&#39;s why you shouldn&#39;t trust them...&quot; or a third party to post &quot;Avery has been saying false things about Pat, I think it&#39;s really unfair, and here&#39;s why...&quot;. In which case I really don&#39;t see how Avery going a step further and pseudonymously making those accusations in writing should restrain Pat or other people.</p><p> I think the reason these feel like they&#39;re in tension is that my underlying feeling is that real victims should be able to make public accusations that name the offender, and offenders shouldn&#39;t be able to retaliate by naming victims. But of course we often don&#39;t know whether someone is a real victim, so this isn&#39;t something community norms or moderation polices can really use as an input.</p><p> There&#39;s a bunch of nuanced discussion about a specific recent variant of this on the <a href="https://forum.effectivealtruism.org/posts/bwtpBFQXKaGxuic6Q/effective-aspersions-how-the-nonlinear-investigation-went?commentId=x9zcFh9g2r7HdsLQH">EA Forum</a> and <a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">LessWrong</a> . I don&#39;t know what the answer is, and I suspect whichever way you go has significant downsides. But I think maybe the best we can do is something like, a trusted community member or group that isn&#39;t close to the dispute [2] evaluates the situation and makes a judgement on the balance of available evidence whether the norms against doxing still apply 。 But what if the evidence is hard to interepret? What if someone says more evidence is coming but it&#39;s not ready yet? How high is the burden of proof? Messy all around, with real consequences to potentially-incorrectly doxed accusers, to potentially-falsely accused people, and in important warnings we never hear because people are worried about being doxed.</p><p><br> [1] I think it also means that for many private disagreements there would be a stronger incentive to go public. If I was in a messy situation with someone and my community used these rules maybe I should quickly tell my side of the story publicly if I think otherwise the other person will tell theirs first under a pseudonym and initiate an asymmetric battle of reputations.</p><p> [2] In the specific case that prompted this post it&#39;s especially tangled in that one of the two main places where this dispute is playing out is run by people closely tied to the pseudonymous accusers. And so while the forum mods would normally be natural judges I think in this case they&#39;re too close to the situation.</p><p> <i>Comment via: <a href="https://www.facebook.com/jefftk/posts/pfbid02JiHaBxJY3sBLFSyz49gUwYAvrLpC9WWLwJR3BDCi5PGnMq3k9CVwKHY8uoYpu38wl">facebook</a> , <a href="https://lesswrong.com/posts/HTLd3R9qgvZsThuWW">lesswrong</a> , <a href="https://mastodon.mit.edu/@jefftk/111620008070332215">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations<guid ispermalink="false"> HTLd3R9qgvZsThuWW</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 21 Dec 2023 19:20:20 GMT</pubDate> </item><item><title><![CDATA[Attention on AI X-Risk Likely Hasn't Distracted from Current Harms from AI]]></title><description><![CDATA[Published on December 21, 2023 5:24 PM GMT<br/><br/><h2>概括</h2><p>在过去的一年里，公共论坛对人工智能带来的存在风险（以下简称 x 风险）越来越关注。我们的想法是，我们可能会<a href="https://www.cold-takes.com/where-ai-forecasting-stands-today/">在未来几年或几十年内看到变革性的人工智能</a>，可能<a href="https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/">很难确保此类系统在行动时考虑到人类的最大利益</a>，而<a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">那些高度先进的人工智能如果旨在做到这一点，可能会压倒我们。如此</a>，否则此类系统可能会被灾难性地滥用。一些人的反应是，对 X 风险的担忧分散了人们对人工智能当前危害的注意力，比如算法偏差、工作取代和劳动力问题、环境影响等。与<em>这些</em>声音相反，其他人认为，对 x 风险的关注并<em>不会</em>分散对当前危害的资源和注意力——这两种担忧可以和平共处。</p><p> X风险会分散人们对当前危害的注意力的说法是偶然的。这可能是真的，也可能不是。要确定它是否属实，有必要查看证据。但是，令人失望的是，尽管人们自信地断言这一说法的真实性和虚假性，但似乎没有人看过证据。在这篇文章中，我确实查看了证据。<strong>总体而言，我查看的数据提供了一些理由认为，迄今为止，对 x 风险的关注并未减少对当前危害的关注或资源。</strong>我特别考虑了五组证据，但它们本身都没有结论：</p><ol><li>自 x-risk 受到关注以来颁布的政策</li><li>搜索兴趣</li><li>人工智能道德倡导者的 Twitter/X 追随者</li><li>为致力于减轻当前危害的组织提供资金</li><li>与环保主义的相似之处</li></ol><p>我现在认为这不是一个重要的讨论。如果每个参与人员都讨论风险、当前危害或 X 风险的概率和规模，而不是讨论一个人是否会分散另一个人的注意力，那就更好了。这是因为就风险达成一致可以消除关于 x 风险是否会分散注意力的分歧，而当对风险存在如此强烈的分歧时，对干扰的分歧可能会很棘手。</p><h2>论点</h2><p><a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">梅雷迪思·惠特克</a>（Meredith Whitaker）（2023 年 6 月）：“一个奇幻、令人兴奋的鬼故事被用来劫持人们对监管需要解决的问题的注意力。” <a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">Deborah Raji</a> （2023 年 6 月）：“科幻叙事分散了我们对那些我们今天就可以开始研究的易处理领域的注意力。” <a href="https://archive.is/20230816022107/https://www.nature.com/articles/d41586-023-02094-7">《自然》</a> （2023 年 6 月）：“人工智能毁灭人类的言论已经进入了科技公司的议程，并阻碍了对人工智能目前造成的社会危害的有效监管。” <a href="https://archive.is/20230628224859/https://www.newscientist.com/article/mg25834453-300-the-real-reason-claims-about-the-existential-risk-of-ai-are-scary/">Mhairi Aitken</a> （艾伦图灵研究所）在《新科学家》杂志上（2023 年 6 月）：“这些说法很可怕，但并不是因为它们是真的。它们之所以可怕，是因为它们正在显着重塑和重新引导有关人工智能影响及其含义的对话。意味着人工智能的开发要负责任。” <a href="https://archive.is/20230925130818/https://www.ft.com/content/732fc372-67ea-4684-9ab7-6b6f3cdfd736">艾丹·戈麦斯</a>（Aidan Gomez，2023 年 6 月）：“花我们所有的时间来争论我们的物种是否会因为超级智能 AGI 的接管而灭绝，这是对我们时间和公众思维空间的荒谬利用。[... ] [这些辩论]会分散人们对应该进行的对话的注意力。” <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">Noema</a> （2023 年 7 月）：“将人工智能引起的灭绝作为全球优先事项似乎可能会分散我们对人工智能之外其他更紧迫问题的注意力，例如气候变化、核战争、流行病或移民危机。” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-1" id="fnref-pmkuq8uwZkgHphmXq-1">[1]</a></sup> <a href="https://archive.is/20231031123323/https://www.technologyreview.com/2023/10/30/1082656/focus-on-existing-ai-harms/">Joy Buolamwini</a> （2023 年 10 月）：“通过说假设的存在危害更重要来最大限度地减少现有人工智能危害的一个问题是，它改变了宝贵资源和立法注意力的流动。” <a href="https://archive.is/93wxY">Lauren ME Goodlad</a> （2023 年 10 月）：“重点是 [...] 防止狭隘和牵强的风险转移人们对实际存在的危害和广泛监管目标的注意力。” Meta 全球事务总裁<a href="https://archive.is/3Ivja">尼克·克莱格（Nick Clegg</a> ）（2023 年 11 月）：“[重要的是要防止]近期挑战被大量推测性的、有时有些未来主义的预测所排挤。”</p><p>这些观点都是在去年夏天和秋天表达的，是对去年春天先进人工智能对存在风险（以下称为 x 风险）日益关注的回应。随后，人们对 x-risk <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-2" id="fnref-pmkuq8uwZkgHphmXq-2">[2]</a></sup>的兴趣迅速增加，这主要是由于 3 月 14 日 GPT-4 的发布以及随后发生的四件事：</p><ul><li> 3 月 22 日：生命未来研究所发表<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">公开信，主张暂停前沿人工智能培训</a>。</li><li> 3 月 29 日：Eliezer Yudkowsky<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">在《时代》杂志上发表评论文章</a>，认为人工智能将对人类构成风险，因此应该停止人工智能的开发。</li><li>五月初：<a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">杰弗里·辛顿 (Geoffrey Hinton)</a>退出谷歌，以警告人工智能的危险。</li><li> 5 月 30 日：人工智能安全中心 (CAIS) 发布了一份<a href="https://www.safe.ai/statement-on-ai-risk">关于灭绝风险的声明，</a>由研究人员、科学家和行业领袖签署，包括 Hinton、Yoshua Bengio、Google DeepMind 的 Demis Hassabis、OpenAI 的 Sam Altman、Anthropic 的 Dario Amodei、Ya-Qin清华大学的张教授等。</li></ul><p>当然，如此令人眼花缭乱的情绪可以得到证据或论据的支持，但这种情况很少发生。相反，它只是简单地断言，对 X 风险的关注会分散人们对当前危害和关注这些危害的人的兴趣和资源。有一个值得称赞的例外值得一提。 Blake Richards、Blaise Agüera y Arcas、Guillaume Lajoie 和 Dhanya Sridhar <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">在为 Noema 撰写的文章中</a>确实提供了一个因果模型，尽管它还很初级，但它是如何发生的：</p><blockquote><p>那些呼吁优先考虑人工智能引起的灭绝的人也在呼吁优先考虑其他更直接的人工智能风险，那么为什么不简单地同意所有这些风险都必须优先考虑呢？除了有限的资源之外，人类及其机构的注意力也是有限的。事实上，有限注意力可能是人类智力的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">标志</a>，也是帮助我们理解世界的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">归纳偏见</a>的核心组成部分。人们还倾向于从彼此身上获取关于要关注什么的暗示，从而导致在公共话语中很容易看到的集体关注焦点。</p></blockquote><p>确实，人类和机构的注意力是有限的。这并不一定意味着专门用于 x 风险的资源和专门用于应对人工智能当前危害的资源之间需要进行权衡。例如，对 x 风险的担忧可能会引起与人工智能完全无关的领域的关注。也可能出现这样的情况：对某项技术可能造成的危害的关注会引起人们<em>对</em>同一技术的其他危害的关注，而不是<em>引起对</em>同一技术的其他危害的关注。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-3" id="fnref-pmkuq8uwZkgHphmXq-3">[3]</a></sup>这些可能性大家都可以想到。为什么没有人费心去问自己哪种效应更有可能发生？我理解为什么 Liv Boeree 看起来如此恼怒，<a href="https://nitter.net/Liv_Boeree/status/1728131649177591953">她写道</a>：“当然，权衡是存在的，注意力也是有限的，但人工智能煽动者的这种日益增长的趋势，他们将模因空间纯粹视为零和战场，盲目地宣称<em>他们</em>最关心的问题是比所有其他人更重要的是，他们无情地对待任何不同意的人，坦率地说，这让人工智能感到尴尬。”我理解她，但尽管我理解她——并不是针对我钦佩的 Liv Boeree——她仍然犯着和其他人一样的错误，因为她的说法虽然可能更接近事实，也是没有证据的断言。顺便说一句，Séb Krier 也是如此，<a href="https://nitter.net/sebkrier/status/1719704802542686568">他写道</a>：“目前的讨论感觉有点像地热工程师对碳捕获科学家感到愤怒——对我来说似乎适得其反，因为显然有不同的空间社区和关注点蓬勃发展。[...]专业化很好，你没有积极的责任去考虑和谈论一切。”</p><p>简单一点来说，这里有两个营地。一是那些关注人工智能当前危害的人，例如算法偏见、工作取代和劳工问题、环境影响、监视和权力集中（特别是硅谷科技公司的权力，还有其他公司和政府的权力）。这里的核心例子包括人工智能伦理领域的 Joy Buolamwini 和算法正义联盟、Meredith Whittaker、 <a href="https://facctconference.org/">FAccT 会议</a>、Bender 等人。 (2021)，<a href="https://www.aisnakeoil.com/">人工智能蛇油</a>和<a href="https://www.torontodeclaration.org/declaration-text/english/">多伦多宣言</a>。第二，那些关注人工智能带来的x风险和灾难性风险的人，要么是由于误用或事故，要么是通过人工智能与其他风险相互作用，例如通过加速或民主化生物工程能力。这里的核心例子包括人工智能安全领域、Eliezer Yudkowsky、Stuart Russell、开放慈善事业、 <a href="https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI">Asilomar 有益人工智能会议</a>、Amodei 等。 (2016)， <a href="https://www.lesswrong.com/">LessWrong</a>和<a href="https://www.safe.ai/statement-on-ai-risk">CAIS 关于灭绝风险的声明</a>。 （当我说第一个阵营关注“当前危害”，第二个阵营关注“x风险”时，我遵循惯例，但是，出于我稍后将解释的原因，我认为这些标签是有缺陷的，并且更准确地说，谈论有根据的/确定的风险与投机/模糊的风险。）有些人和团体致力于或关心这两类问题，但显然存在这两类不同的问题。 （披露：我从事人工智能治理工作，比第一阵营更接近第二阵营，但这篇文章仅代表我个人的观点，不代表我雇主的观点。）</p><h2>证据</h2><p>本节试图查明对 x 风险的关注实际上是否已经吸引了人们对当前危害以及关注这些危害的人的兴趣或资源。当然，没有任何单独的证据可以证明该命题是否正确。因此，我研究了五种不同的证据，并综合考虑它们的说法。它们是 (1) 自 x-risk 受到关注以来制定的政策，(2) 搜索兴趣，(3) 人工智能道德倡导者的 Twitter/X 追随者，(4) 为致力于减轻当前危害的组织提供资金，以及 (5) 与其他组织的相似之处问题领域，特别是环保主义。 （我将使用“人工智能道德个人/组织”作为“关注当前危害的个人/组织”的同义词，尽管这些人/组织关注的一些危害，例如<a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">权力集中</a>，在某种程度上是推测性的，并且即使关注 x 风险的人通常也会出于道德原因这样做。）每一条单独的证据都是薄弱的，但综合起来我认为它们是令人信服的，虽然他们没有<em>证明</em>情况确实如此，但我认为他们提供了一些有理由认为，对 X 风险的关注并没有减少对当前危害的关注或资源投入，或者至少到目前为止还没有这样做。</p><h3>人工智能政策</h3><p>自 x-risk 受到关注以来，西方人工智能政策制定和发布的一项重要内容是拜登政府于 10 月底宣布的<a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">人工智能行政命令</a>。如果您认为 x 风险分散了对当前危害的注意力，您可能会认为行政命令会忽略当前危害，因为 x 风险现在更容易被接受。但那并没有发生。该行政命令有空间解决许多不同的问题，包括偏见、欺诈、欺骗、隐私和工作流失等问题。</p><p>算法正义联盟创始人乔伊·布奥拉姆维尼 (Joy Buolamwini)<a href="https://archive.is/OPG7E">表示</a>：“所制定的内容非常全面，这很高兴看到，因为考虑到人工智能的范围，我们需要一种全面的方法。” AI万金油作者<a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">表示</a>：“总的来说，对于那些支持人工智能开放的人来说，EO似乎是个好消息”，开放是确保人工智能公平、安全、透明和民主的首选方式。白宫声明本身表示“促进公平和公民权利”，并拒绝“利用人工智能让那些已经经常被剥夺平等机会和正义的人处于不利地位”。 《科学美国人》提到了该行政命令的一些局限性， <a href="https://archive.is/20231101134313/https://www.scientificamerican.com/article/bidens-executive-order-on-ai-is-a-good-start-experts-say-but-not-enough/">但随后写道</a>，“[我们]就该命令交谈或与之通信的每一位专家都将其描述为填补政策空白的有意义的一步”。我看到人们对行政命令的批评集中在当前的危害上，但这些批评的类型是，“它过于关注应用程序，而对数据和人工智能系统的开发关注不够”，或者“这将很难”强制执行”，而不是“它没有充分关注当前的危害”。</p><p>当然这只是行政部门。立法部门，也是<a href="https://archive.is/20210130190250/https://news.gallup.com/poll/183605/confidence-branches-government-remains-low.aspx">迄今为止最不受欢迎的部门</a>，也一直很忙碌，或者在不立法的情况下尽可能忙碌。立法者提出了关于<a href="https://www.congress.gov/bill/118th-congress/senate-bill/1993/text">内容责任</a>（六月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/4755/text">隐私增强技术</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2419/text">工人权利</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2691/text">内容披露</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2770/text">选举干扰</a>（九月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/5808/text">深度造假</a>（九月）以及<a href="https://www.congress.gov/bill/118th-congress/senate-bill/3312/text">透明度和问责制</a>（十一月）的两党法案。无论你如何看待国会，它似乎确实密切关注人工智能造成的更直接的危害。</p><p>由英国组织的人工智能安全峰会不是政策，但似乎足够重要，并且很可能足以影响政策，在此提及。该会议于 11 月举行，主要关注（但并非完全）x 风险。例如，在<a href="https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries/ai-safety-summit-2023-roundtable-chairs-summaries-1-november--2">四场风险圆桌会议</a>中，三场讨论了与x风险相关的主题，一场讨论了“民主、人权、民权、公平和平等”的风险。我认为这是 x 风险正在排挤其他担忧的一些证据，但证据比行政命令更弱。这是因为，国际峰会关注风险是有意义的，这些风险一旦发生，<em>必然</em>（而且完全）是全球性的。环保主义也是如此：乱扔垃圾、栖息地丧失和空气污染主要是当地问题，在当地进行最有效的辩论和解决，而气候变化是一个全球性问题，需要在国际论坛上讨论。每当有人举行环保主义峰会时，对他们来说，讨论气候变化可能比讨论栖息地丧失更有意义，即使他们更关心栖息地丧失而不是气候变化。至少，这是峰会组织者明确的推理，他们在一开始就<a href="https://archive.is/F0UIe">写道</a>，其对滥用和失调风险的关注“并不是为了最小化此类人工智能[...]可能带来的更广泛的社会风险，包括错误信息、偏见和歧视以及大规模自动化的潜力”，并且英国认为这些当前的危害“最好通过正在进行的现有国际进程以及各国各自的国内进程来解决”。</p><h3>搜索兴趣</h3><p>人工智能伦理领域——倡导者、研究人员、组织——主要关注当前的危害。当查看谷歌搜索数据时，人工智能道德倡导者和组织似乎在去年春天的 x 风险密集报道期间和之后得到了与以前一样多或更多（但不少于）的关注。对于一些电流本身的伤害也是如此。给定一个简单的因果模型（如下所述），对 x 风险的兴趣并不会减少对人工智能道德倡导者或组织的兴趣，对于其中一些人来说，甚至似乎增加了兴趣。鉴于相同的因果模型，对 x 风险的兴趣并没有减少对当前危害的兴趣，版权问题可能除外。</p><p>下图显示了去年春天 x-risk 受到更多关注（以灰色标记）之前、期间和之后各个主题的搜索热度。 （请注意，这些变量是标准化的，因此您无法将不同的主题进行相互比较。）对人工智能伦理倡导者（Emily M. Bender、Joy Buolamwini、Timnit Gebru、Deborah Raji 和 Meredith Whittaker）和组织（Ada Lovelace Institute、 AI Now 研究所、艾伦图灵研究所、算法正义联盟和 AI 合作伙伴）在 x-risk 出现在新闻期间和之后的规模似乎与以前一样大，甚至更大。自春季以来，人们对当前危害（“算法偏见”、“人工智能伦理”、“致命自主武器” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-4" id="fnref-pmkuq8uwZkgHphmXq-4">[4]</a></sup>和“版权” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-5" id="fnref-pmkuq8uwZkgHphmXq-5">[5]</a></sup> ）的兴趣一直在增加，这与人们对 x 风险分散注意力的担忧相矛盾。如果说自从对 x 风险的关注增加以来，有哪一个群体受到了影响，那就是人工智能安全倡导者和 x 风险本身，因为它回归到了平均值。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/n3wzaw9f1yvwtgjzefeb" alt="图像"></p><p>接下来，我使用简单的因果模型对 2022 年 1 月 1 日以来的 Google 搜索数据进行统计推断。在该模型中，结果——对当前伤害、人工智能道德倡导者或人工智能道德组织的兴趣—— - 受到对 x 风险的兴趣、对人工智能的普遍兴趣以及未观察到的变量的因果影响。对 x 风险的兴趣反过来也是由对人工智能的兴趣以及其他未观察到的变量引起的。人们对人工智能的兴趣通常只是由其他未观察到的变量引起的。如果我们假设这个因果模型，特别是如果我们假设不同的未观察变量彼此独立，那么我们会得到下图所示的结果。该图显示了对于每个结果变量，x 风险的兴趣对结果变量的兴趣影响有多大的概率分布（以标准差衡量）。例如，对 x 风险的兴趣增加 1 个标准差与对版权的兴趣变化 -0.3（95% CI：-0.4 至 -0.1）个标准差相关，并且与对算法的兴趣变化相关。正义联盟 +0.3（95% CI：+0.1 至 +0.5）标准差。 （这些都是相对较弱的影响。例如，从第 50 个百分位数移动到第 62 个百分位数涉及增加 0.3 个标准差。）也就是说，如果因果模型准确，则对 x 风险的兴趣会降低对版权的兴趣，但会增加兴趣在算法正义联盟中，数量适中。在大多数情况下，对 x 风险的兴趣对其他主题的因果影响无法区分为零。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/rwfrdlexhkbc7lrchsyi" alt="图像"></p><p>当然，因果模型并不准确。模型可能不以混杂变量为条件。所以我承认这是薄弱的证据。尽管如此，这仍然是<em>一些</em>证据。作为稳健性检查，该模型确实表示，对 x 风险的兴趣会引起对各种人工智能安全倡导者的更多兴趣，包括 Nick Bostrom（0.1 标准差）、Stuart Russell（0.3）、Max Tegmark（0.2）和 Eliezer Yudkowsky（0.1） ，这是可以预料的，只有令人遗憾的例外是保罗·克里斯蒂亚诺（Paul Christiano）（-0.1），无论如何他的知名度并不是很高。下表显示了每个结果的估计系数。它还显示了从ChatGPT推出到x-risk受到广泛关注期间到x-risk受到广泛关注之后的搜索热度变化；这些增量几乎都是正的，有时甚至是显着的，表明 x 风险没有降低对这些主题的兴趣，因为不存在因果机制，或者 x 风险没有降低对这些主题的兴趣，因为它本身不再有吸引力太多关注，或者 x 风险降低了人们对这些主题的兴趣，但其他因素却增加了人们对它们的兴趣。</p><table><thead><tr><th>搜索主题</th><th>x 风险对主题的影响（标准开发）</th><th> x 风险获得关注后的变化（标准开发者）</th></tr></thead><tbody><tr><td>算法偏差</td><td>0.2（95% CI：-0.03 至 0.39）</td><td> 0.5</td></tr><tr><td>人工智能伦理</td><td>-0.1（95% CI：-0.24 至 -0.02）</td><td> 1.0</td></tr><tr><td>版权</td><td>-0.3（95% CI：-0.44 至 -0.10）</td><td> 1.3</td></tr><tr><td> Lethal autonomous weapons</td><td> 0.1（95% CI：-0.16 至 0.30）</td><td> -0.5</td></tr><tr><td> Emily M. Bender</td><td> 0.4（95% CI：0.20 至 0.59）</td><td> -0.1</td></tr><tr><td>乔伊·布拉姆维尼</td><td>-0.1（95% CI：-0.24 至 0.13）</td><td> 0.3</td></tr><tr><td>蒂姆尼特·格布鲁</td><td>0.2（95% CI：-0.04 至 0.37）</td><td> 0.1</td></tr><tr><td>黛博拉·拉吉</td><td>-0.1（95% CI：-0.38 至 0.10）</td><td> 0.3</td></tr><tr><td>梅雷迪思·惠特克</td><td>0.1（95% CI：-0.06 至 0.35）</td><td> 0.6</td></tr><tr><td>艾达·洛夫莱斯研究所</td><td>-0.1（95% CI：-0.36 至 0.13）</td><td> 0.3</td></tr><tr><td>人工智能现在研究所</td><td>0.0（95% CI：-0.13 至 0.29）</td><td> 0.9</td></tr><tr><td> Alan Turing Institute</td><td> -0.0（95% CI：-0.26 至 0.17）</td><td> -0.2</td></tr><tr><td>算法正义联盟</td><td>0.3（95% CI：0.06 至 0.49）</td><td> 0.3</td></tr><tr><td>人工智能合作</td><td>-0.1（95% CI：-0.30 至 0.03）</td><td> 1.0</td></tr></tbody></table><h3> Twitter/X 关注者</h3><p>衡量一组问题受到多少关注的一个指标是该组问题的倡导者得到了多少关注，而衡量<em>这一</em>问题的一个指标是这些倡导者拥有多少 Twitter/X 关注者。下图显示了自 ChatGPT 发布之前以来，关注当前危害的人们在 Twitter/X 关注者中的累积增长。它表明，在去年春天密集的 x 风险报道期间（以灰色标记）期间和之后，人工智能伦理倡导者获得 Twitter/X 关注者的速度不仅没有放缓，反而似乎有所增加。在 x-risk 获得关注前后或之后，至少三位倡导者发现其追随者数量出现了阶梯式增长：Meredith Whittaker、Arvind Narayanan（ <a href="https://www.aisnakeoil.com/">AI Snake Oil</a>的）和 Emily M. Bender。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/v1gqscvbrw8xulpes96g" alt="图像"></p><p>我几乎忍不住要抱怨，一个奇幻的、令人兴奋的鬼故事被用来劫持监管<em>真正</em>需要解决的问题的注意力：x风险。但我不会，因为那会是可笑的，也是错误的。这是错误的，因为无论它对当前危害的关注做了什么，对 x 风险的兴趣也增加了对 x 风险的关注，这是一个同义反复。</p><h3>资金</h3><p>根据我能找到的数据，到目前为止，关注当前人工智能危害的组织在筹款方面似乎并不比 x 风险担忧受到关注之前更困难。下表显示了人工智能伦理组织的两个主要资助者福特基金会和麦克阿瑟基金会在过去几年中授予的资助情况。算法正义联盟 (AJL)、人工智能为人民 (AFP) 和分布式人工智能研究所 (DAIR) 今年获得了大量资助，包括在 x 风险受到更多关注之后很久。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-6" id="fnref-pmkuq8uwZkgHphmXq-6">[6]</a></sup>此外，11 月 1 日，十家著名的美国慈善机构<a href="https://archive.vn/znir6">宣布</a>，他们打算捐赠超过 2 亿美元，用于关注当前危害的努力，但没有具体说明时间表。</p><table><thead><tr><th>捐赠者</th><th>接受者</th><th>年</th><th>数量</th></tr></thead><tbody><tr><td>福特基金会</td><td><a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=algorithmic+justice+league">阿杰林</a></td><td>2019年</td><td>21 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2020年</td><td>20 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2021年</td><td>57 万美元（5 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>133 万美元（3 年）</td></tr><tr><td> ”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=ai+for+the+people">法新社</a></td><td>2021年</td><td>30 万美元</td></tr><tr><td>”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=distributed+ai+research">代尔</a></td><td>2021年</td><td>100 万美元（3 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>110 万美元（2 年）</td></tr><tr><td>麦克阿瑟基金会</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">阿杰林</a></td><td>2023年</td><td>50 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/ai-for-the-people-10115248/">法新社</a></td><td>2021年</td><td>20 万美元（2 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023年</td><td>35 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">代尔</a></td><td>2021年</td><td>100 万美元（2 年）</td></tr></tbody></table><p>诚然，资助过程可能会持续数月，而且广泛的情绪转变可能需要更长的时间才能影响资助者的优先事项。但是，如果您认为 X 风险正在从当前的危害中抽走资源，并想象了一系列负面结果，那么这至少应该让您认为这些结果中最糟糕和最直接的结果是难以置信的，即使是中等程度的糟糕，并且不太直接，但结果仍然可能。</p><h3>气候变化</h3><p>你有时会听到<em>反对</em>X 风险是一种干扰这一说法的论点，与其说是一种争论，不如说是一种反驳，其内容如下：“对注意力的担忧似乎不会出现在其他领域，例如环保主义。是的，尽管人们对气候“厄运”有很多抱怨——那些关注人类灭绝、临界点和海平面上升导致的大规模人口流离失所的人——但你很少听说它们会分散人们对当前环境危害的注意力，我们指的是栖息地丧失、过度捕捞和鸟类数量下降。”不言而喻，近期环保主义者之所以没有说临界点等会分散人们对鸟类种族灭绝等真正问题的注意力，是因为他们已经考虑了所有相关因素，并确定末日论者的担忧并不存在。事实将注意力或资源从自己身上转移开。</p><p>我不太确定。我认为真正的原因是纯粹的短期环保主义者几乎不存在，当他们存在时，他们<a href="https://grist.org/climate-energy/everybody-needs-a-climate-thing/">确实会抱怨</a>气候变化吸引了所有的注意力。在气候变化这一成熟的研究领域中，主流观点认为，推测性的危害越重要，而当前的危害，如热应激或极端天气，相比之下并不那么重要。很少有人关心当前气候变化造成的危害，也不关心气候变化造成的更多投机性危害。人工智能领域存在更多分歧。这不仅适用于更具投机性的风险（例如 x 风险），也适用于更根深蒂固的危害（例如使用受版权保护的材料作为训练数据）。因此，与环保主义不同，在人工智能领域，有一大群人关心当前的危害，而不是更多的推测性危害。当然，你会在人工智能领域看到比环保主义更多的分歧。</p><p>也许<em>应该</em>有这样的抱怨。毕竟，空气污染造成了全球约 10% 的死亡，在低收入国家，空气污染接近或位居主要风险因素列表的首位（Ritchie 和 Roser 2021）。空气污染和气候变化都是由排放引起的。罗布·维布林 (Rob Wiblin) 最近在<a href="https://80000hours.org/podcast/episodes/santosh-harish-air-pollution/">接受桑托什·哈里什 (Santosh Harish) 采访时</a>提出了这一论点：“鉴于空气污染似乎对人们的健康造成了如此大的伤害 [...] 有趣的是，我们一直专注于努力让人们这样做“由于其全球公共利益性质，这个东西很难推销，而我们本来可以专注于颗粒物污染。而情况恰恰相反，因为你会立即受益。”但桑托什·哈里什似乎并不认为气候变化会分散他的注意力。他表示，富裕国家在解决了空气污染问题后开始关心气候变化，而贫穷国家则从未忘记空气污染问题。确实，在过去三十年中，空气污染造成的死亡人数（总体而言，对于高收入和中等收入国家而言，但对于低收入和中低收入国家而言）有所下降（Ritchie 和 Roser 2021），并且暴露于户外过去十年来，富裕国家超过世界卫生组织指导标准的空气污染有所下降，而贫穷国家则持平（而且很糟糕）（Ritchie 和 Roser 2022）。但当然，即使气候变化分散了人们的注意力，这种情况也可能发生。关键是，一个主题是否有意义地分散对另一个更重要主题的注意力，是偶然的。查看细节是无可替代的。</p><h2>也许真正的分歧在于风险有多大</h2><p>这一切都有问题。事实上，有几件事出了问题。第一个失败在于将问题界定为未来与当前危害之间的问题。反对 X 风险的人不会这样做，因为它存在于未来（如果它是真实的）。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-7" id="fnref-pmkuq8uwZkgHphmXq-7">[7]</a></sup>他们这样做是因为他们认为风险并不真实，或者不可能知道，或者至少我们不知道。同样，那些真正关注 X 风险的人也不会这样做，因为它位于未来；那是荒谬的。他们这样做是因为他们认为 x 风险（如果发生）涉及所有人的死亡，比当前的风险更重要，<em>即使</em>它还需要数年时间。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-8" id="fnref-pmkuq8uwZkgHphmXq-8">[8]</a></sup>最好区分有根据的/确定的风险与投机/模糊的风险。虽然“当前危害”阵营对投机/模糊性风险高度怀疑，但“x风险”阵营更愿意接受此类风险，如果他们看到有令人信服的论据，并且如果这样做的预期价值的话，他们更愿意关注这些风险所以就足够了。</p><p>另一个问题与讨论的级别有关。问题在于，人们不是讨论风险是什么，而是讨论应该做什么，因为没有人就风险是什么达成一致。难道我们没有本末倒置吗？你可能会说，“但事实上，我们确实对风险是什么存在分歧，那么我们找到一种确定优先顺序和妥协的方法难道不是有效的吗？”这是可能的。但你是否<em>真正详细地</em>讨论了根本问题，例如超级智能是否有可能在未来几十年内出现，或者人工智能驱动的错误信息实际上是否会侵蚀民主？对于x风险的人来说，你会看到他们不断地、痛苦地与x风险的可能性作斗争，而很少参与其他风险的规模和强度。根据我的经验，对于人工智能伦理人士来说，除了一些值得注意的例外之外，两者都很少，例如 Obermeyer 等人。 (2019)、Strubell、Ganesh 和 McCallum (2019) 以及 Acemoglu (2021)。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-9" id="fnref-pmkuq8uwZkgHphmXq-9">[9]</a></sup></p><p>当一个普通人想象一个拥有数百万或数十亿人工智能代理的世界时，这些人工智能代理可以执行超过 99% 的在家工作的人可以完成的任务，她会感到担忧。因为她意识到，硅智能不会睡觉、吃饭、抱怨、衰老、死亡或忘记，而且，由于许多人工智能代理将针对创建更智能的人工智能系统的问题，它不会就此止步。我们人类很快将不再是最聪明的，我的意思是有能力的物种。这本身并不意味着我们注定要失败，但这足以让她认真对待失败的风险。当然，这是否会发生，才是真正应该争论的问题。</p><p>另一方面，如果事实证明x风险很小，并且我们可以合理地确定这一点，那么它确实不应该受到关注，除了在科幻小说和哲学思想实验的作品中。任何严肃的人都不会允许玛雅世界末日的恐惧进入政策辩论，无论他们的政策建议有多好。无论你从哪个角度来看，关键在于风险，而关于干扰的讨论，具有令人愉快的讽刺意味，是对更重要的讨论的干扰。</p><h2>参考</h2><p>阿塞莫格鲁，达龙。 2021.“人工智能的危害。”国家经济研究局。<br>阿莫代、达里奥、克里斯·奥拉、雅各布·斯坦哈特、保罗·克里斯蒂亚诺、约翰·舒尔曼和丹·马内。 2016.“人工智能安全的具体问题。” <a href="https://doi.org/10.48550/ARXIV.1606.06565">https://doi.org/10.48550/ARXIV.1606.06565</a><br>艾米丽·本德 (Emily M.)、蒂姆尼特·格布鲁 (Timnit Gebru)、安吉丽娜·麦克米伦·梅杰 (Angelina McMillan-Major) 和什玛格丽特·施米切尔 (Shmargaret Shmitchell)。 2021.“论随机鹦鹉的危险。” <em>2021 年 ACM 公平、问责和透明度会议记录</em>。 ACM。 <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a><br>奥伯迈耶、齐亚德、布莱恩·鲍尔斯、克里斯汀·沃格利和森德希尔·穆莱纳坦。 2019。“剖析用于管理人口健康的算法中的种族偏见。”<em>科学</em>366 (6464): 447--53。<br>奥德，托比。 2020。<em>悬崖：存在风险和人类的未来</em>。阿歇特图书公司。<br>里奇、汉娜和马克斯·罗瑟。 2021。“空气污染。”<em>我们的数据世界</em>。<br> ------。 2022.“室外空气污染”。<em>我们的数据世界</em>。<br>斯特鲁贝尔、艾玛、阿纳尼亚·加内什和安德鲁·麦卡勒姆。 2019。“Nlp 深度学习的能源和政策考虑。” </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pmkuq8uwZkgHphmXq-1" class="footnote-item"><p> Noema 文章的作者认为，并不是人工智能 x 风险分散了人们对人工智能当前危害的注意力，而是它分散了人们对社会规模风险的其他来源的注意力。通过这种方式，它们与此处列出的其他产品有所不同。 <a href="#fnref-pmkuq8uwZkgHphmXq-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-2" class="footnote-item"><p>我在这里使用“x-risk”来特指来自人工智能的x-risk。 X风险的其他潜在来源包括人为设计的流行病和核战争，但我在这里并不关心它们，除非它们因先进人工智能的存在而增加或减少。 “存在风险”是“威胁人类长期潜力被破坏的风险”，例如人类灭绝（Ord 2020）。 <a href="#fnref-pmkuq8uwZkgHphmXq-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-3" class="footnote-item"><p>一些围绕人工智能存在风险的讨论也在一定程度上讨论了当前的危害，反之亦然，例如，应该如何设计审计、责任和评估（如果应该的话）。但当然，完美减轻 X 风险的政策不一定与完美减轻当前危害的政策相同。可能需要一定程度的妥协或优先顺序。 <a href="#fnref-pmkuq8uwZkgHphmXq-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-4" class="footnote-item"><p>诚然，致命性自主武器以及人工智能的军事应用，并不是“当前危害”阵营重点关注的事情，而是“x风险”阵营人们关注的事情，尤其是生命未来研究所。在这里它仍然是一个有用的分析变量。如果 x 风险分散了人们对当前危害或较少推测性危害的注意力，那么它也应该分散人们对人工智能军事应用的风险的注意力。 <a href="#fnref-pmkuq8uwZkgHphmXq-4" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-5" class="footnote-item"><p>鉴于版权是一个如此广泛的主题，涉及的不仅仅是与人工智能相关的问题，版权变量的信息量相当有限。 <a href="#fnref-pmkuq8uwZkgHphmXq-5" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-6" class="footnote-item"><p>我不知道 2023 年麦克阿瑟拨款何时发放，但拨款页面于 11 月 1 日更新，所以也许它们是在 9 月或 10 月授予的？尽管如此，今年的两笔麦克阿瑟资助可能是在 x-risk 去年春天引起关注之前授予的。如果是这样，它们并不能证明 x 风险获得关注后，受助者仍然能够有效筹款，也不能证明这一说法。 <a href="#fnref-pmkuq8uwZkgHphmXq-6" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-7" class="footnote-item"><p>如果未来的风险是几个世纪或几千年之后的事情，那么关注当前危害的人们可能会不同意优先考虑未来的风险。但如果 X 风险如此遥远，那么大多数目前从事 X 风险工作的人就不会再这样做了。他们之所以致力于人工智能 x-risk 的研究，是因为根据他们的说法，它已经触手可及。因此，时间方面并不像人们所认为的那样存在分歧。 <a href="#fnref-pmkuq8uwZkgHphmXq-7" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-8" class="footnote-item"><p>不用说，那些主张缓解 X 风险的人认为，这种情况可能会在未来几年或几十年内发生，而不是几个世纪或几千年。虽然未来的价值确实可能比那些不关注 X 风险的人更能激励他们，但我认为这也不是问题的症结所在。如果可以毫无疑问地证明，在未来几十年里，每个人都有真正的机会（>;10%）死亡，那么每个人都会同意这种风险应该成为重中之重。你并不需要成为一个功利主义者<a href="https://www.erichgrunewald.com/posts/a-kantian-view-on-extinction/">才能珍视人类的继续存在</a>。 <a href="#fnref-pmkuq8uwZkgHphmXq-8" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-9" class="footnote-item"><p>您是否经常看到有关当前危害的文本来判断这些危害的规模和程度，而不是充其量用一两个生动的轶事来说明他们的论点？这是一个反问句，答案是，不常见。 <a href="#fnref-pmkuq8uwZkgHphmXq-9" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current<guid ispermalink="false"> 5rexNxtZgkEQBi3Sd</guid><dc:creator><![CDATA[Erich_Grunewald]]></dc:creator><pubDate> Thu, 21 Dec 2023 17:24:16 GMT</pubDate></item></channel></rss>