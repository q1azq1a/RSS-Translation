<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 18 日星期五 00:49:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Managing risks of our own work]]></title><description><![CDATA[Published on August 18, 2023 12:41 AM GMT<br/><br/><p><i>注意：这不是个人帖子。我代表 ARC Evals 团队分享。</i></p><h2>发布的潜在风险及我们的应对措施</h2><p><i>本文档扩展了 ARC Evals 论文“</i><a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><i><u>在现实自主任务上评估语言模型代理</u></i></a><i>”的附录。</i></p><p>我们发布<a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><u>这份报告的</u></a>目的是：i) 增进对前沿人工智能模型潜在危险能力的了解，ii) 推进此类模型安全评估的最新技术。我们希望这将提高社会在具有危险能力的模型造成灾难性损害之前识别它们的能力。</p><p>可能有人会说，这种研究本身就是有风险的，因为它使得语言模型代理的危险能力的开发和运用变得更容易。事实上， <a href="https://github.com/Significant-Gravitas/Auto-GPT"><u>Auto-GPT</u></a>的作者表示，他在 GPT-4 系统卡中看到我们的评估描述后受到了启发。 <span class="footnote-reference" role="doc-noteref" id="fnrefun11wm97b2"><sup><a href="#fnun11wm97b2">[1]</a></sup></span>虽然无论如何，这样的项目似乎很快就会出现， <span class="footnote-reference" role="doc-noteref" id="fnrefb33b8ke2moc"><sup><a href="#fnb33b8ke2moc">但 [2]</a></sup></span>提高语言模型代理能力的可能性不仅仅是假设。</p><p>考虑到此类担忧，我们对本报告进行了重大修改，包括（但不限于）：</p><ul><li>使用我们的脚手架删除代理运行的完整记录。</li><li>使用我们的脚手架删除对代理的优势和劣势的更详细描述。</li></ul><p>然而：</p><ul><li>当该材料的风险明显最小时，我们可能会稍后将其公开。</li><li>如果更详细的分析使我们有足够的信心相信它是合理的，我们稍后可能会将这些材料公开。</li><li>从事人工智能安全评估的研究人员可能会联系我们，请求额外访问非公开材料，我们还将与人工智能实验室和政策制定者分享一些非公开材料。</li></ul><p>我们的理由概述如下：</p><ul><li>从本质上讲，我们最好的猜测是，更完整的出版物会降低净风险，并且对推进危险能力的进展相对无助。有关我们为什么相信这一点的更多信息，请参阅<a href="https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model"><u>保罗·克里斯蒂安诺 (Paul Christiano) 的这篇文章</u></a>。 （平均而言，ARC Evals 对这些论点表示同情，尽管信心较弱，而且员工之间的意见也各不相同。）</li><li>我们最好的猜测是，更完整的出版物将得到大多数相关利益相关者的批准，因此不会过度<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/"><u>单边主义</u></a>，尽管这可以更仔细地制定，并且有一些概念问题需要解决。</li><li>然而，一些审阅本报告草稿以识别此类潜在风险的人对更完整的出版物抱有极大的担忧。</li><li>考虑到缺乏共识、相对缺乏公开对话以及我们的分析还处于早期阶段，我们决定暂时采取谨慎态度。</li></ul><p>我们在这里分享这个基本原理是为了向一些可能对此最感兴趣的人传达它，并获得有关我们的思考和决策过程的反馈。</p><h2>我们实验期间采取的安全和监督预防措施</h2><p>所有代理在这些任务上的运行都是在人类监督下进行的， <span class="footnote-reference" role="doc-noteref" id="fnrefroqr8m3yw3a"><sup><a href="#fnroqr8m3yw3a">[3]</a></sup></span>人类实验者根据指示，如果代理要与未经同意的人类交互或以其他方式参与任何不道德或非法活动，则不要继续运行。 <span class="footnote-reference" role="doc-noteref" id="fnref6qwg53cbrkb"><sup><a href="#fn6qwg53cbrkb">[4]</a></sup></span>当我们想要调查此类活动（例如在有针对性的网络钓鱼任务中）时，实验者会模拟任务规范中详细概述的人类响应。</p><p>人们可能想知道，我们使用的监督是否可以被具有足够态势感知能力的语言代理所规避，从而可以在没有监督或限制的情况下访问互联网。例如，代理可能会采取混淆的行为，实验者不会认为这些行为是可疑的，但实际上会对世界造成伤害或构成风险。虽然这些可能涉及可能性，但我们的评估强烈表明这些代理没有足够的能力发现和利用此类漏洞或在监督下有效隐藏其行为。随着模型能力的提高，这种情况可能会发生变化，但我们相信，这些类型的能力评估恰恰会告诉我们在处理模型时需要更加小心，我们将密切关注这一点。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnun11wm97b2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefun11wm97b2">^</a></strong></sup></span><div class="footnote-content"><p>基于与托兰·布鲁斯·理查兹的书面交流。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb33b8ke2moc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb33b8ke2moc">^</a></strong></sup></span><div class="footnote-content"><p>我们相信这一点的原因之一是浪链是在 ARC Evals 的作品发表之前独立开发的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnroqr8m3yw3a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefroqr8m3yw3a">^</a></strong></sup></span><div class="footnote-content"><p>实验者有时会一次运行几个步骤，而不批准每个步骤，特别是当代理从事常规或低风险活动时。网页浏览期间的每个操作都会得到人工监督员的主动批准，不会自动播放。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6qwg53cbrkb"> <span class="footnote-back-link"><sup><strong><a href="#fnref6qwg53cbrkb">^</a></strong></sup></span><div class="footnote-content"><p> OpenAI 系统卡描述了模型与不知情的人类 (TaskRabbit) 之间的交互。该事件不属于该实验的一部分，并且不受相同准则的约束。您可以<a href="https://evals.alignment.org/taskrabbit.pdf"><u>在此处</u></a>阅读有关该单独实验的更多信息。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work<guid ispermalink="false">农场R2tiyCem8DD35</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:41:30 GMT</pubDate> </item><item><title><![CDATA[Memetic Judo #1: On Doomsday Prophets v.2]]></title><description><![CDATA[Published on August 18, 2023 12:14 AM GMT<br/><br/><p>人们普遍倾向于将那些担心人工智能安全的人视为“世界末日先知”，并提出这样的建议：预测不久的将来存在的风险会自动抹黑他们（因为“你知道；<em>他们</em>一直都是错的”）在过去”）。</p><h2>参数结构示例</h2><blockquote><p>人类灭绝的预测（“世界末日先知”）在过去从来都不是正确的，因此 X 风险的说法通常是不正确/可疑的。</p></blockquote><h2>讨论/困难</h2><p>这个论点是持久的，并且有点难以接近/处理，特别是因为它在技术上是一个有效的（但我认为是弱点）点。这是基于对历史趋势的天真的推断的归纳论证。因此，不能通过利用其前提之一的不一致或无效的简单证伪来完全驳回它。相反，有必要列出一份令人信服的弱点清单——越多越好。如下所示的列表。</p><h3> #1：不可靠的启发式</h3><p>如果你回顾历史，就会发现这种“事情将保持不变”的临时预测通常是错误的。</p><h3> #2：生存偏差</h3><p>它们不仅经常是错误的，而且有一类预测，根据设计/定义，它们只能正确一次，而对于这些预测，它们的论点甚至更弱，因为你的样本会受到生存偏差等因素的影响。存在风险论点就属于这一类，因为你只能灭绝一次。</p><h3> #3：动荡的时期</h3><p>我们生活在一个高度不稳定和不可预测的时代，这个时代受到技术和文化的迅猛发展。从祖父母的角度来看，当今的世界几乎无法辨认。在这种时候，这种争论就变得更加无力。这种趋势似乎并没有放缓，而且有强有力的论据表明，即使是良性的人工智能，它也会颠覆许多此类归纳预测。</p><h3> #4：爆炸半径感应（感谢<a href="https://www.lesswrong.com/users/npcollapse">Connor Leahy</a> ）</h3><p>莱希引入了“技术爆炸半径”的类比，它代表了一种抽象的方式来思考不同技术的潜在力量，包括它们故意或人为错误造成伤害的可能性。当我们在科技树上取得进展时——虽然它的许多角落相对无害或良性，但我们可用的技术的最大“爆炸半径”必然会增加。你用剑比用棍棒造成的伤害更大，如果你有火药、现代武器等，伤害甚至更大。TNT工厂的爆炸可以摧毁一个城市街区，核武库可以用来夷平许多城市。现在看来非常明智（通过归纳！）最终，这个“爆炸半径”将涵盖整个地球。有强有力的迹象表明，强人工智能将会出现这种情况，甚至一旦这项技术被开发出来，这种情况很可能是偶然发生的。</p><h3> #5：支持证据和责任</h3><p>将此确立为技术上有效但薄弱的论点（对于不懂的人来说是一种启发），您有责任查看我们对人工智能存在风险的担忧所基于的具体证据和可用论据，以便决定是否确认或驳回您的初始假设（这是有效的）。因为这个话题显然非常重要，所以我恳请你这样做。</p><h3> #6：许多领先的研究人员担心</h3><p><a href="https://twitter.com/paulg/status/1642110597545295872"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3TjhYN8ZvFs5jASW/xtfusertgr2h33nzxdhk" alt="保罗·格雷厄姆的推文"></a><br>担心人工智能带来的生存风险的人工智能研究人员名单中包括 Geoffrey Hinton、Yoshua Bengio 和 Stuart Russel 等大牌人物。</p><h2>最后的评论</h2><p>我认为这个列表是一个正在进行的工作，所以请随时在评论中告诉我遗漏的要点（或您的批评！）。<br>我还打算根据我的个人笔记和与我的行动主义相关的讨论，将其制作成一系列关于反 X 风险论点的文章。欢迎提出流行或重要论点的建议！</p><br/><br/> <a href="https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2<guid ispermalink="false"> G3TjhYN8ZvFs5jASW</guid><dc:creator><![CDATA[Max TK]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:14:11 GMT</pubDate> </item><item><title><![CDATA[Looking for judges for critiques of Alignment Plans]]></title><description><![CDATA[Published on August 17, 2023 10:35 PM GMT<br/><br/><p>你好！<br><br> AI-Plans.com 最近举办了一场“批评马拉松”，参与者提交并完善了 40 多条对 AI 调整计划的批评。以下是该活动的最终评论： <a href="https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing">https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit</a> ?usp=sharing<br><br>我们正在寻找任何有兴趣帮助评判这最后 11 条评论的人。<br><br>到目前为止，我们非常感谢 Peter S Park 博士（麻省理工学院 Tegmark 实验室博士后、哈佛大学博士）和 Aishwarya G（未来生命研究所 AI 存在安全社区成员和 BlueDot Impact AI 安全基础知识治理课程主持人）的帮助，以及一些独立的对齐研究人员。<br><br>我很想听听你的想法！<br><br>卡比尔·库马尔（AI-Plans.com 创始人）</p><br/><br/> <a href="https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans<guid ispermalink="false"> q7nWEbyW7tXwnKBe9</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Thu, 17 Aug 2023 22:35:41 GMT</pubDate></item><item><title><![CDATA[How is ChatGPT's behavior changing over time?]]></title><description><![CDATA[Published on August 17, 2023 8:54 PM GMT<br/><br/><p>很惊讶我在 lesswrong 上找不到这个，所以我想添加它。随着时间的推移，法学硕士的行为似乎会产生一些一致性影响，至少获得更多的背景信息。<br><br>与我交谈过的其他人立即对它进行了贬低，因为某种实验错误使论文的结论变得相当无效，但我并没有真正看到这一点。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-change-over-time<guid ispermalink="false"> 9nooX9djbM5bXGKNn</guid><dc:creator><![CDATA[Phib]]></dc:creator><pubDate> Thu, 17 Aug 2023 21:32:42 GMT</pubDate> </item><item><title><![CDATA[Progress links digest, 2023-08-17: Cloud seeding, robotic sculptors, and rogue planets]]></title><description><![CDATA[Published on August 17, 2023 8:29 PM GMT<br/><br/><h2><strong>机会</strong></h2><ul><li><a href="http://apply.nucleate.xyz/">学术生物技术创始人：申请 Nucleate 的 Activator 计划</a>（来自<a href="https://twitter.com/kulesatony/status/1689316164764065793">@kulesatony</a> ）</li></ul><h2><strong>新闻与公告</strong></h2><ul><li><a href="https://twitter.com/neuralink/status/1688582504196739072">Neuralink 筹集 2.8 亿美元 D 轮融资，由 Founders Fund 领投</a></li><li><a href="https://waymo.com/blog/2023/08/waymos-next-chapter-in-san-francisco.html">Waymo</a>和<a href="https://twitter.com/kvogt/status/1689814193875374080">Cruise</a>已获准在旧金山运营机器人出租车</li><li><a href="https://twitter.com/ADoricko/status/1688740627855589376">造雨者发射旨在“结束全球水资源短缺和地球改造”</a></li></ul><h2><strong>播客</strong></h2><ul><li><a href="https://www.dwarkeshpatel.com/p/dario-amodei#details">Dwarkesh Patel 采访了 Anthropic 首席执行官 Dario Amodei</a> （来自<a href="https://twitter.com/dwarkesh_sp/status/1688916080700555264">@dwarkesh_sp</a> ）。 “达里奥很搞笑，对这些模型正在做什么、为什么它们的扩展性如此之好以及如何调整它们有令人着迷的看法”</li><li><a href="https://conversationswithtyler.com/episodes/paul-graham/">泰勒·考恩采访保罗·格雷厄姆</a>（来自<a href="https://twitter.com/tylercowen/status/1689260912182542337">@tylercowen</a> ）</li></ul><h2><strong>链接</strong></h2><ul><li><a href="https://www.monumentallabs.co/">Monumental Labs 正在建造“人工智能机器人石雕工厂”，</a>以“以极低的成本打造具有佛罗伦萨、巴黎或纽约美术学院般辉煌的城市”。这是<a href="https://twitter.com/mspringut/status/1682126571392360448">一个演示</a>（来自<a href="https://twitter.com/devonzuegel/status/1689438847220813824">@devonzuegel</a> ）。他们正在<a href="https://wellfound.com/company/monumental-labs-2/jobs/2578817-stone-carver-full-and-part-time-roles">雇用石雕师</a>（仍然需要他们进行精细的细节和精加工）</li><li><a href="https://www.readcodon.com/p/synbio-guide">合成生物学密码子指南</a>（作者： <a href="https://twitter.com/NikoMcCarty/status/1689625144132820992">@NikoMcCarty</a> ）</li><li><a href="https://exformation.williamrinehart.com/p/silicon-innovation-is-colliding-with">人工智能许可证 Raj：人工智能在采用方面面临着严重的官僚障碍</a>（ <a href="https://twitter.com/WillRinehart/status/1688524865236619264">@WillRinehart</a> ）</li><li><a href="https://earthsky.org/space/rogue-planets-exoplanets-nancy-grace-roman-space-telescope/">我不知道有这么多流氓行星</a></li><li><a href="https://goodscience.substack.com/p/metascience-since-2012-a-personal">“我作为元科学风险投资家的个人历史”</a> （作者： <a href="https://twitter.com/stuartbuck1/status/1690508406942011392">@stuartbuck1</a> ）</li><li> <a href="https://beta.reddit.com/r/interestingasfuck/comments/15gzvhb/a_slice_of_the_a303_in_hampshire_england">英国汉普郡的A303穿过巨石阵，古罗马福斯路的一部分包含在其中</a>（来自<a href="https://twitter.com/Rainmaker1973/status/1690016575917580290">@Rainmaker1973</a> ）</li></ul><p> <a href="https://pbs.twimg.com/media/F3QkgxTWMAEE9Xp?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zmek5t33153yoxbyv15h" alt=""></a></p><h2><strong>社交媒体</strong></h2><ul><li><a href="https://twitter.com/paulg/status/1689872015535300608">在当前的 YC 批次中，“各种不同领域的大量领域专家已经找到了使用 AI 来解决其领域中的人们早已知道但不太能够解决的问题的方法”</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689320109532225551">如果您对 LK-99 过于兴奋，那么值得重新调整您的先验知识。但如果你正确预测没有RTS，那就真的没有必要幸灾乐祸了</a>。相关： <a href="https://twitter.com/MaximZiatdinov/status/1689295189045661696">LK99 证明科学界完全有能力通过 arXiv 和社交媒体工具进行同行评审</a></li><li><a href="https://twitter.com/AlecStapp/status/1688538038341931008">生物安全政策中一些最容易实现的成果：在监测废水的预警系统上投入更多资源</a></li><li><a href="https://twitter.com/AlecStapp/status/1689625548195241987">19 世纪初，纽约、费城和波士顿的居住密度高达每平方英里 75,000 人</a></li><li><a href="https://twitter.com/owasow/status/1690549009147174912">“NuScale 于 2008 年开始致力于获得监管批准。2020 年，当其反应堆获得设计批准时，该公司表示，监管过程花费了 5 亿美元，并且已向该公司提供了约 200 万页的支持文件。国家研究委员会。”</a>相关的是，我关于<a href="https://rootsofprogress.org/devanney-on-the-nuclear-flop">核电为何失败的</a>书评</li><li><a href="https://twitter.com/eric_is_weird/status/1688935852364193792">按时间顺序学习数学</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689379507487002624">为你的想法不懈地、不知疲倦地宣传意味着什么</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689036685349212160">在线平台无法让您在受众较少的情况下取得成功；他们可以让你在没有把关人的情况下培养观众</a></li><li><a href="https://twitter.com/mbateman/status/1688378460006240256">也许“平衡”是一个不快乐的人对幸福生活的看法。快乐的人看起来不平衡也没什么问题</a></li><li><a href="https://twitter.com/NobelPrize/status/1688129810915045376">亚历山大·弗莱明最初将青霉素称为“霉菌汁”</a></li></ul><p> <a href="https://pbs.twimg.com/media/F21xGYbWEAANvN5?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/fmmgoeh6knycp6cuiruk" alt=""></a></p><h2><strong>引号</strong></h2><p>尝试一种新的格式，我将完整的引号内联。 （强调。）链接到社交媒体，以便您可以轻松分享。让我知道你的想法：</p><p><a href="https://twitter.com/michael_nielsen/status/1689371336517541888">思想史的发明</a>（彼得·沃森， <a href="https://www.amazon.com/Ideas-History-Thought-Invention-Freud-ebook/dp/B000FCKC5G"><i>《思想：思想与发明史，从火到弗洛伊德</i></a>》）</p><blockquote><p>第一个构思思想史的人也许是弗朗西斯·培根（Francis Bacon，1561-1626）。他当然认为历史最有趣的形式是思想史，如果不考虑任何时代的主导思想，“历史是盲目的”。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690897070847049729">为什么汽车比马更好——来自经历过这一转变的人</a>（大卫·麦卡洛，<a href="https://www.amazon.com/Wright-Brothers-David-McCullough-ebook/dp/B00LD1RWP6"><i>莱特兄弟</i></a>）</p><blockquote><p>阿莫斯·鲁特充满热情，始终渴望“看到车轮转动”。他喜欢钟表、风车、自行车和各种机器，尤其是他的奥兹莫比尔小跑。在任何季节里，他很少比在路上行驶时更快乐。 “虽然我在某种程度上喜欢马（他写道），但我不喜欢照顾它们。我不喜欢马厩的气味。我不喜欢每天早上必须清洁马，也不喜欢在冬天拴马。 ……<strong>套马需要时间；但汽车立即准备好启动。它永远不会疲倦；它比任何马都能更快地到达那里。</strong> ” 至于奥兹莫比尔，他喜欢说，350 美元的价格比一匹马和马车还便宜。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690900703110152192">直到 18 世纪，就连国王和皇帝也饱受糟糕的路况之苦</a>（理查德·布利特， <a href="https://rootsofprogress.org/books/the-wheel"><i>《车轮</i></a>》）</p><blockquote><p>直到十九世纪中叶新的道路建设实验开始取得成果之前，马车车轮下方的表面仍然布满车辙、泥泞不堪，而且路面状况不佳——如果有路面的话。在农村尤其如此，但即使在大城市也存在糟糕的道路。例如，1703年，在从伦敦向南前往五十英里外的佩特沃斯的途中，<strong>哈布斯堡王朝皇帝查理六世乘坐的马车在路上翻车了十二次。</strong>半个世纪后，麦尔安德路（Mile End Road）是从阿尔德门（Aldgate）入口向东延伸的主干道，被描述为“从怀特查佩尔（Whitechapel）到斯特拉特福（Stratford）的一处深泥淤泥湖”，全长四英里。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690906769365540864">米塞斯反对稳定</a>（丹尼尔·斯特德曼·琼斯，<a href="https://rootsofprogress.org/books/masters-of-the-universe"><i>宇宙大师</i></a>）</p><blockquote><p>和波普尔一样，米塞斯也看到了官僚心态与柏拉图的乌托邦之间的相似之处，在柏拉图的乌托邦中，绝大多数被统治者为统治者服务。他认为“后来所有按照柏拉图的例子塑造人间天堂蓝图的乌托邦主义者都同样相信人类事务的不变性”。他接着说，官僚化必然是僵化的，因为它涉及对既定规则和惯例的遵守。但在社会生活中，僵化就等于石化和死亡。一个非常重要的事实是，稳定和安全是当今“改革者”最珍视的口号。<strong>如果原始人采取了稳定的原则，他们早就被猛兽和微生物消灭了。</strong></p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690906098432098305">尝试改变喷发火山熔岩流的方向是什么感觉</a>（<a href="https://en.wikipedia.org/wiki/Eldfell">埃尔德费尔</a>，冰岛，1973）（约翰·麦克菲， <a href="https://rootsofprogress.org/books/the-control-of-nature"><i>《自然的控制</i></a>》）</p><blockquote><p>喷发期间，当抽水人员第一次尝试登上熔岩时，他们发现薄至两英寸的地壳足以支撑一个人，并且还可以隔热——只有几英寸厚的硬岩像融化的英寻上的池塘冰。当工作人员拖拉软管、喷嘴三脚架和管道段时，他们了解到最好不要静止不动。他们常常原地踏步。<strong>即便如此，他们的靴子有时也会起火。</strong></p></blockquote><h2><strong>图表</strong></h2><p><a href="https://tfp.elidourado.com/">美国经利用率调整后的全要素生产率已连续三个季度下降，目前低于疫情爆发前 2019 年第四季度的水平</a>。这很糟糕（来自<a href="https://twitter.com/elidourado/status/1688564129869582336">@elidourado</a> ） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/dhlws1xp4vvwcvtjjmpy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/br2jkdpyxjptx9z1r5w3 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/c7s2qfk0ijt39a85qxzl 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zbcc2hgnarzioxa7apkz 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/lb91b5i8kgnagpoe54sm 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cmdprkjnjsvlmp6zwklr 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/wdjmpx5174v0hbv56i8t 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cy7mpdvnxwrg0gfv0qyz 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cjflc2wviegf2ws5krzk 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/gwqe4jiqeewfapmekbes 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/ksdykld4bmcurbgo2isq 1029w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic<guid ispermalink="false"> CJNbF9hM8PtxkrHHT</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Thu, 17 Aug 2023 20:29:28 GMT</pubDate> </item><item><title><![CDATA[Model of psychosis, take 2]]></title><description><![CDATA[Published on August 17, 2023 7:11 PM GMT<br/><br/><p> <i>（我无论如何都不是精神病方面的专家。这更像是“在博客中实时记录我的想法”。我希望激发讨论并获得反馈和指示。）</i></p><h1>一、简介</h1><p>去年 2 月，我在博客文章<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症是长程皮层间交流的缺陷”第 4.2 节</u></a>中提出了一种精神病模型。但它有一些问题。我终于抽出时间再看一下，我想我找到了解决这些问题的简单方法。所以这篇文章是更新版本。</p><p><strong>对于tl;dr，您可以跳过正文，只看下面的两张图</strong>。</p><h1> 2. 背景：我之前的“精神病模型，取 1”</h1><p>以下是我在<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症作为长程皮层间通讯缺陷”第 4.2 节</u></a>中提出的建议： </p><figure class="image image_resized" style="width:84.91%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/wktqaurpaalkjoggdiuu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/oghtomiodqbanmrqmja2 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pct3wxlentqcpdlpmosm 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qj0t6vaozbu0iccn7fby 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jn71t5qau9ikibfefhce 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/l2blxhhkorj6am4fubje 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fhihkdcjhqoqstwo2rs4 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/d4hg2hc6pz7d0xa7vmvt 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/sjd7xw4brb3sxxmvmh37 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/o1emb3bcgxegzsyomhkt 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jzl0b6rs9iswligmrcnc 1318w"></figure><p>这个想法是，在精神病中，绿色箭头是活跃且有效的，但红色箭头不是，因此紫色箭头也不是。结果可能是一种手臂被外力移动的感觉。这只是一个例子，对于左侧皮质输出的不同类型，相应的精神病的第一人称体验会有所不同。但我声称所有精神病症状都大致符合这个模板。</p><h1> 3. 第一个模型似乎缺少重要的三个方面</h1><ul><li>据我了解，精神分裂症患者的精神病可能会反复出现，而（我相信）皮层间沟通的缺陷是精神分裂症患者大脑的特征，并且是永久性的（未来医疗技术没有进步）。</li><li>抗精神病药物可以减少精神病，但上图无法解释这一点。</li><li>除精神分裂症外，精神病还可能发生在其他疾病中。我认为最常见的例子是躁郁症的躁狂期。上图无法解释这一点。</li></ul><h1> 4. 我的“精神病模型，取2”</h1><p> <i>（该图顶部的唯一变化是左侧新的绿色文本，上面写着“信号强度= <strong>B</strong> ”。）</i> </p><figure class="image image_resized" style="width:93.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qnmby6hoqa5bfwlqeayu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jjrb8mawu2ujeqw7c5ot 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/ehibbu9bgqqowkwjldir 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/cogtxpqp9ex2o3ip54fa 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/rdirptmkvrozlxi5d9ny 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qzpurvyri4vy823pk7po 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pyoqgkrhdtuwzyhq0ixa 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pjjscqiimpaceb2q13wl 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fwjfxfswenkvooxyxutk 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/kjnkjrftfoufriobw1pl 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/uahekggt8l0bsvgkqw6r 1485w"></figure><p>可以将皮层的一部分视为具有可调节的“音量”，即它宣布当前正在做什么的强烈程度和清晰程度。例如，如果您想到<i>也许</i>您<i>可以</i>移动手指，那么您可能会发现（如果您仔细观察和/或使用科学设备）您的手指有点抽搐，而如果您强烈想要移动手指，然后你的手指就会真正移动。</p><p>无论如何，如果我们逐渐增加一部分皮层的“体积”，那么在某个时间点， <strong>A</strong>消息将开始通过并产生影响，同时在<i>其他</i>某个时间点， <strong>B</strong>消息将开始通过并产生影响。有效果。为了避免精神病，我们希望前者<i>首先</i>发生， <i>&nbsp;</i>以及<i>后者</i>，这样就不可能出现<strong>B</strong>消息正在传输但<strong>A</strong>消息没有传输的“音量级别”。</p><h1> 5.这个新模型的优点</h1><h2>5.1 <strong>B/A</strong>比率的缓慢变化至少在<i>先验上</i>是合理的，因为<strong>B</strong>和<strong>A</strong>来自不同皮质层的不同神经元（分别为第 5 层和第 2/3 层）</h2><p>我认为<strong>B/A</strong>比率是一个可以变化的参数，这是非常合理的，因为：</p><ul><li>信号<strong>B</strong>仅由皮层<strong>第 5 层</strong>的一部分神经元发送</li><li>信号<strong>A</strong>至少部分（也许大部分？）由皮层<strong>第 2/3 层</strong>的神经元子集发送</li><li>一般来说，不同的皮质层有不同类型的神经元，具有不同的输入、与多巴胺系统的不同关系等。</li></ul><p>因此，涉及<strong>B/A</strong>比率长期变化的理论至少是<i>合理的</i>。</p><h2> 5.2 至少一篇论文似乎表明抗精神病药比第 2/3 层信号更能抑制第 5 层信号</h2><p>参见<a href="https://elifesciences.org/reviewed-preprints/86805"><u>Heindorf &amp; Keller 2023</u></a> 。他们发现“氯氮平……降低了[第 2/3 层]兴奋性神经元的皮质活动相关性。然而，这种减少明显弱于我们在……[第 5 层端脑内]神经元中观察到的减少”。 （此特定比较的 p 值为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 0.005"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.005</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>对于短程相关性， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 10^{-8}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span></span></span></span></span>对于长程相关性）。</p><p>如果我正确地理解了这篇论文（一个很大的“如果”！），这是有启发性和令人鼓舞的，但并不能<i>明确</i>支持我的理论，因为首先，该论文测量了错误类型的第 5 层锥体神经元。发送信号<strong>B 的</strong>神经元，其次，作者测量的这种相关性<i>可能</i>是由通常信号较弱的第 5 层神经元引起的（这样空间相关性很快就会低于本底噪声），但还有其他原因也有可能的原因（与空间相关性<i>直接</i>相关——我认为这就是作者认为正在发生的事情）。</p><h1> 6. 我仍然不确定的事情</h1><h2>6.1 哪些 D2 受体解释抗精神病药物如何发挥作用？</h2><p>每种抗精神病药物的共同点是阻断多巴胺 D2 受体。所以想必这就是他们的工作方式。但整个大脑的神经元中都有 D2 受体。据推测，这些带有 D2 受体的神经元的一部分是抗精神病药物发挥作用的秘密，其余的则仅与副作用有关。哪个是哪个？</p><p>当我尝试充实我的模型时，迄今为止我想到的最简单、最优雅的故事涉及<i>皮质中</i>扮演主角的 D2 受体。特别是，不同的皮质层有不同的 D2 受体密度，如果我没记错的话，抗精神病药物降低<strong>B/A</strong>比率的迹象似乎是正确的。</p><p>但这很有趣，因为我认为几乎其他人似乎都认为<i>纹状体中的</i>D2 受体是抗精神病药物的作用机制？我试图弄清楚为什么人们似乎相信这一点，但无法弄清楚。我能找到的所有关于抗精神病药通过纹状体起作用的证据都相当薄弱和间接。如果您对此有所了解，请评论。</p><h2> 6.2 精神病的其他原因呢？</h2><p>由于各种原因，我脑子里有一个模糊的经验法则，那就是，在其他条件相同的情况下，更多的多巴胺往往会增加<strong>B/A</strong>比率（因此，超过某个阈值，会导致精神病）。这似乎很好地解释了与躁狂相关的精神病（我将躁狂与“大量多巴胺”松散地联系起来，至少在某些渠道和各种警告中），以及精神病是左旋多巴治疗帕金森病的副作用这一事实。</p><p>我对精神病性抑郁症更困惑。 （在双相情感障碍中，我知道精神病在躁狂症中比抑郁症更常见，但它<i>也可能</i>发生在抑郁症中。）我通常认为抑郁症是躁狂症的“相反”，并且涉及异常少<i>的</i>多巴胺，同样是在某些渠道和与各种警告。所以我对精神病性抑郁症是否会发生感到有点困惑。我不知道。无论如何，多巴胺系统只是影响<strong>B/A</strong>比率的众多因素之一。或者上图右侧的紫色信号可能没有通过？或者也许这是一个完全不同的故事。</p><br/><br/><a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2<guid ispermalink="false"> tgaD4YnpGBhGGbAy5</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:11:17 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Robustified ANNs Reveal Wormholes Between Human Category Percepts]]></title><description><![CDATA[Published on August 17, 2023 7:10 PM GMT<br/><br/><p>这是<a href="https://arxiv.org/abs/2308.06887"><i>https://arxiv.org/abs/2308.06887</i></a><i>的链接帖子</i><i>。</i></p><blockquote><p>众所周知，人工神经网络（ANN）的视觉对象类别报告对微小的对抗性图像扰动非常敏感。因为人类类别报告（又名人类感知）被认为对那些相同的小范数扰动不敏感，而且总体上局部稳定，这表明人工神经网络是人类视觉感知的不完整科学模型。与此一致的是，我们表明，当标准 ANN 模型生成小范数图像扰动时，人类对象类别感知确实高度稳定。然而，在这个完全相同的“人类假定稳定”体系中，我们发现稳健的人工神经网络可靠地发现了强烈扰乱人类感知的低范图像扰动。这些以前无法检测到的人类感知干扰的幅度非常大，接近于稳健的人工神经网络中可见的相同敏感度水平。此外，我们表明，稳健的人工神经网络支持精确的感知状态干预：它们指导低范数图像扰动的构建，这些扰动强烈改变人类类别感知到特定规定的感知。这些观察结果表明，对于图像空间中的任意起点，存在一组附近的“虫洞”，每个虫洞都会引导主体从当前类别感知状态进入语义上非常不同的状态。此外，当代生物视觉处理的人工神经网络模型现在足够准确，可以始终如一地引导我们到达这些门户。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes-between-human#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes- Between- human<guid ispermalink="false"> t5CXh9LwcKZqwNpTk</guid><dc:creator><![CDATA[Bogdan Ionut Cirstea]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:10:40 GMT</pubDate> </item><item><title><![CDATA[Against Almost Every Theory of Impact of Interpretability]]></title><description><![CDATA[Published on August 17, 2023 6:44 PM GMT<br/><br/><p><i>认知状态：我相信我精通这个主题。我的错误在于提出了过于强烈的主张，允许读者提出不同意见并就精确的观点展开讨论，而不是试图对每一个陈述进行边缘化。我还认为使用模因很重要，因为安全想法很无聊且</i><a href="https://www.lesswrong.com/posts/zk6RK3xFaDeJHsoym/connor-leahy-on-dying-with-dignity-eleutherai-and-conjecture%23Eliezer_Has_Been_Conveying_Antimemes"><i><u>反模因</u></i></a><i>。那么我们走吧！</i></p><p><i>非常感谢</i><a href="https://www.lesswrong.com/users/scasper?mention=user"><i>@scasper</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/sid-black?mention=user"><i>@Sid Black</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/neel-nanda-1?mention=user"><i>Neel Nanda</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/fabien-roger?mention=user"><i>Fabien Roger</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/bogdan-ionut-cirstea?mention=user"><i>@Bogdan Ionut Cirstea</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/wcargo?mention=user"><i>@WCargo</i></a> <i>、@</i> <a href="https://www.lesswrong.com/users/alexandre-variengien?mention=user"><i>Alexandre Variengien</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/lelapin?mention=user"><i>@Jonathan Claybrough</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/edoardo-pona?mention=user"><i>@Edoardo Pona</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/andream?mention=user"><i>Andrea_Miotti</i></a> <i>、Diego Dorn、Angélina Gentaz、Clement Dumas、和 Enzo Marsot 提供有用的反馈和讨论。</i></p><p>当我开始写这篇文章时，我首先批评 Neel Nanda 的文章<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>《可解释性影响的一长串理论》</u></a> ，但后来我扩大了批评的范围。提出的一些想法并没有得到任何人的支持，但为了解释其中的困难，我仍然需要1.解释它们和2.批评它们。它给这篇文章带来了一种敌对的氛围。对此我感到很抱歉，我认为对可解释性进行研究，即使它不再是我认为的优先事项，仍然是值得赞扬的。</p><p><strong>如何阅读这份文件？</strong>除了“可解释性的最终故事是什么样的？”部分之外，本文档的大部分内容都不是技术性的。一开始大部分可以跳过。我希望这份文档对于不进行可解释性研究的人也有用。不同的部分大多是独立的，我添加了很多书签来帮助模块化这篇文章。</p><p>如果你时间很少，就看一下（这也是我最有信心的部分）：</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>使用 Interp 审计欺骗是遥不可及的</u></a>（4 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>列举安全</u></a>评论（2 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>具有更好影响理论的技术议程</u></a>（1 分钟）</li></ul><p></p><p>以下是我将辩护的索赔清单：</p><p> （粗体部分是最重要的部分）</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#The_overall_Theory_of_Impact_is_quite_poor"><strong><u>整体影响理论相当差</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp 不能很好地预测未来系统</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><strong><u>使用 interp 审计欺骗是遥不可及的</u></strong></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><strong><u>可解释性的最终故事是什么样的？这一点都不清楚。</u></strong></a><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><strong><u>枚举安全性？</u></strong></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>逆向工程？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Olah_s_interpretability_dream_"><u>奥拉的可解释性梦想？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Retargeting_the_search_"><u>重新定位搜索？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>轻松的对抗训练？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>显微镜人工智能？</u></a></li></ul></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><strong><u>针对欺骗的预防措施似乎更加可行</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency"><u>引导世界走向透明</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_Design"><u>认知模拟 - 可解释性设计</u></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><strong><u>可解释性可能总体上是有害的</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Outside_view__The_proportion_of_junior_researchers_doing_interp_rather_than_other_technical_work_is_too_high"><strong><u>外界观点：初级研究员从事Interp而非其他技术工作的比例太高</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#So_far_my_best_ToI_for_interp__Nerd_Sniping_"><u>到目前为止，我最好的解释员指南：书呆子狙击？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Even_if_we_completely_solve_interp__we_are_still_in_danger"><strong><u>即使我们完全解决了interp，我们仍然处于危险之中</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><strong><u>具有更好影响理论的技术议程</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Conclusion"><strong><u>结论</u></strong></a></li></ul><p>注：本文的目的是批评类 GPT 模型等深度学习模型的可解释性影响理论（ToI），而不是小模型的可解释性和可解释性。</p><h1>皇帝没穿衣服？</h1><p>我演讲了不同的<a href="https://www.lesswrong.com/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review"><u>风险模型</u></a>，然后进行了可解释性演示，然后我得到了一个有问题的问题，“我不明白，这样做有什么意义？”哼。 </p><figure class="image image_resized" style="width:587.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zzxhts0u9ne414ahftxw" alt=""><figcaption>来自<a href="https://distill.pub/2017/feature-visualization/"><i><u>特征可视化的</u></i></a><i>图像</i><i>。</i></figcaption></figure><ul><li>功能即？ （左图）嗯，很漂亮，但是有用吗？ <span class="footnote-reference" role="doc-noteref" id="fnref2stfurwyyg6"><sup><a href="#fn2stfurwyyg6">[1]</a></sup></span> <a href="https://arxiv.org/abs/2010.12606%23:~:text%3Dversion%252C%2520v3)%255D-,Exemplary%2520Natural%2520Images%2520Explain%2520CNN%2520Activations%2520Better%2520than,of%252Dthe%252DArt%2520Feature%2520Visualization%26text%3DFeature%2520visualizations%2520such%2520as%2520synthetic,convolutional%2520neural%2520networks%2520(CNNs)."><u>这个</u></a><a href="https://arxiv.org/abs/2306.04719"><u>可靠</u></a>吗？</li><li> GradCam（一种像素归因技术，如上右图），它很漂亮。但我从未见过有人在工业中使用它。 <span class="footnote-reference" role="doc-noteref" id="fnref4qi9kn3ip89"><sup><a href="#fn4qi9kn3ip89">[2]</a></sup></span>像素归因似乎很有用，但准确性仍然是王道。 <span class="footnote-reference" role="doc-noteref" id="fnref6xxwjs20rd7"><sup><a href="#fn6xxwjs20rd7">[3]</a></sup></span></li><li>感应头？好吧，我们可能正在对<a href="https://en.wikipedia.org/wiki/Regular_expression"><u>法学硕士中的正则</u></a>表达式机制进行逆向工程。凉爽的。</li></ul><p>最后要点中的考虑是基于感觉，并不是真正的论据。此外，大多数机械解释现在甚至都不是为了有用。但在本文的其余部分，我们将了解原则上可解释性是否有用。那么我们就来调查一下解说帝到底是有隐形衣服还是根本没有衣服！</p><h1>整体影响理论相当差</h1><p>Neel Nanda 撰写了<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><strong><u>一长串可解释性影响理论</u></strong></a><strong>，</strong>其中列出了 20 种不同的影响理论。然而，我发现自己不同意其中的大多数理论。元层面的三大分歧是：</p><ul><li><strong>每当你想做一些具有可解释性的事情时，最好不要这样做。</strong>我怀疑 Redwood Research 已因此停止进行可解释性工作（请参阅此处的当前计划<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw"><u>EAG 2023 湾区当前的调整计划，以及我们如何改进它</u></a>）。<ul><li><strong>对于欺骗性对齐来说尤其如此</strong>，尽管它是可解释性研究的主要焦点。许多其他风险情景也值得考虑。 【 <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>欺骗</u></a>部分】</li></ul></li><li><strong>可解释性常常试图同时解决太多目标。</strong>请<a href="https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately"><u>分别购买 Fuzzies 和 Utilons</u></a> ：即同时优化多个目标是非常困难的！最好直接针对每个子目标分别进行优化，而不是将所有内容混合在一起。当我查看尼尔·南达（Neel Nanda）的<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>这份清单</u></a>时，我发现这个原则没有得到遵循。</li><li><strong>一般来说，可解释性可能是有害的。</strong>使用 interp 来保证安全无疑对功能来说是有用的。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>危害</u></a>]</li></ul><p>其他不太重要的分歧：</p><ul><li><strong>概念上的进步更为紧迫，</strong>而 interp 可能无助于推进这些讨论。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>结束故事</u></a>]</li><li><strong>目前的可解释性主要用于事后分析</strong>，在事前或预测能力方面几乎没有什么用处[ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#id.vcvarqienhb7"><u>未来系统的部分预测器</u></a>]</li></ul><p>以下是我不同意的一些关键理论：</p><ul><li>影响理论<strong>2：“</strong><i><strong>更好地预测未来系统”</strong></i></li><li>影响理论<strong>4：“</strong><i><strong>欺诈审计”</strong></i></li></ul><p>在附录中，我批评了几乎所有其他影响理论。</p><h2> Interp 不能很好地预测未来系统</h2><p><i>影响理论 2：“<strong>更好地预测未来系统</strong>：可解释性可以使人们更好地机械地理解 ML 系统和工作原理，以及它们如何随规模变化，类似于科学定律。这使我们能够更好地从当前系统推断未来系统，类似于缩放定律。例如，观察感应头的相变向我们表明，模型可以在训练期间快速获得能力”，</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li>对<a href="https://transformer-circuits.pub/2021/framework/index.html"><strong><u>感应头</u></strong></a>示例<strong>的挑剔</strong><strong>。</strong>如果我们关注上面的例子“<i>模型在训练过程中可能会快速获得能力</i>”，我不认为是可解释性让我们发现了这一点，而是行为评估。在训练期间定期测量损失，并通过让模型复制一系列随机令牌<a href="https://transformer-circuits.pub/2021/framework/index.html"><u>来测量</u></a>归纳能力的快速增益。一开始，复制是行不通的，但经过一些训练后，就可以了。可解释性只是告诉我们，这与感应头的出现相吻合，但我不明白可解释性如何让我们“<i>更好地从当前系统推断未来系统”</i> 。此外，感应头首先被研究是因为它们很容易学习。</li><li><strong>可解释性主要是在现象发现后完成的，而不是事前完成的。</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefztj4j3pmerg"><sup><a href="#fnztj4j3pmerg">[4]</a></sup></span><ul><li>我们首先观察到了 grokking 现象，然后我们<i>才</i>开始对其进行一些<a href="https://arxiv.org/abs/2301.05217"><u>解释</u></a>。有没有反例？</li><li>在<a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do"><u>DALL-E 2 可以做什么和不能做什么</u></a>中，我们看到 DALL-E 2 无法正确拼写单词。两个月后， <a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do?commentId%3Dg6kZ3eRFejRjiyGiw"><u>Imagen</u></a>就能正确拼写这些单词了。我们甚至没有费心去解释。</li></ul></li><li><strong>有更好的方法来预测这些系统的未来功能。</strong>跳出框框思考，如果你真的想看看未来的系统会是什么样子，那么查看 NeurIPS 会议和 AutoGPT 等认知架构上发表的论文会容易得多。否则，订阅 DeepMind 的 RSS feed 也不失为一个好主意。</li></ul><h2>使用 interp 审计欺骗是遥不可及的</h2><p>审计欺骗通常是进行解释的主要动机。所以我们在这里：</p><p>影响理论 4：<i><strong>欺骗审计</strong>：与审计类似，我们也许能够检测模型中的欺骗行为。这比完全审核模型要低得多，而且我们只需能够<strong>查看模型的随机位并识别电路/特征</strong>，这似乎是我们可以做到的事情 - 我认为这更多地是“世界的变革理论”可解释性比我希望的更难”</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li><strong>我不明白“查看</strong><i><strong>模型的随机位并识别电路/特征</strong></i><strong>”如何有助于欺骗。</strong>例如，假设我对随机电路的 GPT2 进行了逆向工程，例如在论文<a href="https://arxiv.org/abs/2211.00593"><u>Interpretability in the wild 中</u></a>，他们对间接对象识别电路进行了逆向工程。目前还不清楚这将如何帮助欺骗。即使预期的含义是“识别可能与欺骗/社交建模相关的电路/特征”，也不清楚分析每个电路是否足够（参见“ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>枚举安全</u></a>”小节）。</li><li><strong>我们还远未达到通过插译员检测或训练欺骗行为所需的水平。</strong> Evan Hubinger 在他的文章<a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"><u>《透明度和可解释性技术树》</u></a>中列出了 8 个可解释性级别，其中只有第 7 级和第 8 级提供了一些打击欺骗的手段。这些级别大致描述了可解释性的需求，但到目前为止我们只达到了第 2 级，并且我们已经在第 4 级遇到了负面结果。Evan 解释说，“<i>任何级别的透明度和可解释性技术对欺骗性模型具有鲁棒性都是极其困难的”</i> ”。</li><li> <i><strong>“此外，试图通过可解释性工具提前发现欺骗行为可能会失败，因为欺骗性对齐模型没有必要积极思考其欺骗行为。</strong>一个从未见过有机会夺取权力的情况的模型不需要仔细计划在这种情况下会做什么，就像工厂清洁机器人不需要计划如果有一天发现自己陷入困境该怎么办一样。丛林而不是工厂。尽管如此，该模型之前并未计划夺取权力，但这并不意味着如果有机会它就不会夺取权力。特别是，一个模型可能会被欺骗性地对齐，因为它推断，在有明确监督者的情况下，做它想做的事是在世界上获得权力和影响力的良好总体策略，而不需要为以后的欺骗制定任何明确的计划”。</i> （摘自 <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>《监视欺骗性对齐</u></a>》中的 Hubinger）</li><li><strong>已经存在反对可解释性的负面概念点</strong>，这表明先进的人工智能不容易被解释，正如杀伤力列表中的<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities%23sufficiently_good_and_useful"><u>可解释性部分</u></a>所讨论的那样（这些是我过去<a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23"><u>尝试</u></a>批评的观点，但大多失败了） 。特别是第 27、29 和 33 点：<ul><li> <strong>27. 选择不可检测性</strong>：“<i>针对解释性思想进行优化，就针对可解释性进行优化。”</i></li><li> <strong>29. 现实世界是一个不透明的领域：</strong> “<i>通用人工智能的输出在产生真正的后果之前要经过一个巨大的、我们不完全了解的领域（现实世界）。人类无法检查通用人工智能的输出来确定后果是否良好。”</i><ul><li><strong>并且认知可以外化。</strong>这不是特定于 interp 的。许多模式只能通过它们与环境的交互方式来解释，而不能仅通过网络中的内容来完全解释。例如“查阅食谱并采取书中所写的行动。” （康纳的例子）。</li></ul></li><li> <strong>33. 外星人概念：</strong> “<i>人工智能不像你那样思考”</i>对于通过矩阵乘积处理数字来完成的认知，可能不一定有人类可以理解的解释。</li><li>我并不完全同意所有这些观点，但我还没有看到对这些具体观点的太多讨论，你可以在我的评论中找到一些警告</li></ul></li><li>其他较弱的困难见脚注。 <span class="footnote-reference" role="doc-noteref" id="fnref2464ho15s7t"><sup><a href="#fn2464ho15s7t">[5]</a></sup></span></li></ul><p>仅使用 interp 来对抗欺骗并不是唯一的方法：</p><ul><li><strong>解释并不是研究欺骗的唯一方法。</strong>以下是其他被忽视的范例：<ul><li><strong>对抗性攻击</strong>（基本上<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw%26themeRefresh%3D1"><u>是雷德伍德的计划</u></a>）。许多重要的对抗策略不依赖于<a href="https://arxiv.org/abs/2306.09983"><strong><u>一致性检查</u></strong></a>、<a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/"><u>人工智能检查和平衡</u></a>以及定期提示<span class="footnote-reference" role="doc-noteref" id="fnrefrbnra8fsacj"><sup><a href="#fnrbnra8fsacj">[6] 等</a></sup></span>插值。</li><li><strong>创建一个欺骗性对齐或模拟的玩具模型：</strong>研究“可能构成生存威胁的各种失败的体外演示”，来自<a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>《错位模型有机体》</u></a> ，这是一篇关于此的最新详细文章。</li><li><strong>寻找欺骗性联盟代理：</strong>例如，监控以下<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default%23Foundational_properties_for_deceptive_alignment"><u>4 个标准</u></a>，这些标准通常被视为欺骗性联盟的先决条件：目标导向的行为、跨事件/长期目标视野的优化、基本目标的概念化、情境意识。</li><li>接近人类水平的人工智能的各种基线策略：<ul><li>在 NLP 中，监控 AutoGPT 的计划和思路（ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#kix.cyunx152z1bn"><u>细节</u></a>和注意事项）。</li><li>愿景中，类EfficientZero的一致性损失——在蒙特卡罗树中展望未来（ <a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23bookmark%3Did.dgd983y7t1ym"><u>草案</u></a>）</li></ul></li><li><a href="https://twitter.com/StephenLCasper/status/1650662566476279809"><u>更多</u></a>想法，如神经蒸馏和<a href="https://www.alignmentforum.org/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive%23How_to_actually_implement_a_speed_prior"><u>速度优先</u></a>。</li><li>如果 DeepMind 今天宣布他们在 GPT 中发现了欺骗行为，他们不太可能只使用可解释性来发现这一发现。他们更有可能使用定期提示。</li></ul></li><li><strong>有一些针对欺骗性联盟的预防措施</strong>似乎更可行（请参阅<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>针对欺骗的预防措施</u></a>部分）。</li><li><strong>概念上的进步更为紧迫。</strong>从概念上思考欺骗比通过可解释性思考要富有成效得多。据我所知，可解释性还没有教会我们任何关于欺骗的知识。<ul><li>例如，<a href="https://www.lesswrong.com/tag/simulator-theory"><u>模拟器理论</u></a>和对<i>GPT 已经可以模拟欺骗性拟像</i>的理解是我们对欺骗性对齐的理解比欺骗性可解释性方面所发生的进步更大的进步。</li><li>关于欺骗性对齐的概念性考虑，如文章<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default"><u>《默认情况下欺骗性对齐的可能性 &lt;1%》</u></a>中所示，完全不依赖于可解释性。 </li></ul></li></ul><p></p><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:273.72px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/dkw7e0dp4pangmxj7ei4" alt=""></p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:268.46px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/ct8q2lqan4o5dhrw6bcf" alt=""></p></td></tr></tbody></table></figure><p><i>受到我与捍卫 interp 的朋友们进行的每一次讨论的启发。 “你对天文学的论证太笼统了”，所以让我们在下一节中深入探讨一些物体级的论证！</i></p><h1>可解释性的最终故事是什么样的？这一点都不清楚。</h1><p><i>这部分技术性比较强。可以先跳过它，或者只阅读“枚举安全性？”部分。这非常重要。</i></p><p>当然，深度学习中的可解释性似乎本质上比神经科学更可行，因为我们可以保存所有激活并非常缓慢地运行模型，通过尝试因果修改来理解正在发生的事情，并且比功能磁共振成像允许更多的控制。但在我看来，这还不够——我们甚至不知道我们的目标是什么。我们的目标是：</p><h2>枚举安全性？</h2><p>正如 Neel Nanda <a href="https://www.lesswrong.com/posts/qgK7smTvJ4DB8rZ6h/othello-gpt-future-work-i-am-excited-about#:~:text=enumerative%20safety%2C%20the%20idea%20that%20we%20might%20be%20able%20to%20enumerate%20all%20features%20in%20a%20model%20and%20inspect%20this%20for%20features%20related%20to%20dangerous%20capabilities%20or%20intentions.%20Seeing%20whether%20this%20is%20remotely%20possible%20for%20Othello%2DGPT%20may%20be%20a%20decent%20test%20run."><strong><u>所说</u></strong></a><strong>，枚举安全</strong><strong>是指我们可以枚举</strong>模型中的<i><strong>所有</strong></i><strong>特征</strong>，并检查其是否存在与危险能力或意图相关的特征。我认为这个策略从一开始就注定了（从最重要到不太重要）：</p><ul><li><strong>确定某个特征的危险性是一个错误指定的问题。</strong>在网络的权重/结构中搜索危险特征是毫无意义的。一个特性本身并没有好或坏之分。单个原子的危险并不能有力地预测原子和分子组装的危险。例如，如果你想象第53层、第127通道的特征，它看起来像一把枪，这是否意味着你的系统很危险？或者您的系统是否能够识别危险的枪支？认知可以<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach:~:text=And%20cognition%20can%20be%20externalized."><u>外化</u></a>这一事实也有助于这一点。</li><li><strong>特征仍然是一个模糊的概念</strong>，叠加问题和自然抽象假设在<a href="https://distill.pub/2020/circuits/zoom-in/"><u>Distill</u></a>论文发表三年后仍然是一个假设，很少有令人信服的策略来解决它们。这并不奇怪：可解释性的核心概念“特征”似乎本质上是模糊的，并且仍然没有定义。这是“枚举安全”策略以及对神经元进行逐一迭代以验证每个特征的“良好性”并获得保证的一个主要问题：<ul><li>并且由于<a href="https://arxiv.org/abs/2209.10652"><u>叠加</u></a>，迭代每个神经元是无效的。因此，我们不能只迭代神经元，而是必须迭代所有神经元集（或更糟糕的是所有方向），这在计算上是完全难以处理的。</li></ul></li><li><strong>危险模型的属性不是低级特征，而是高级行为能力，</strong>例如编码能力、 <a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>阿谀奉承</u></a>或各种心理代理理论、态势感知或黑客攻击。<ul><li>网络的态势感知可能包括几个子特征，例如日期和时间、地理位置以及用户的当前需求。删除这些子功能会降低模型的竞争力。</li></ul></li><li><a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness"><strong><u>深度欺骗性</u></strong></a>——简单来说，由于优化压力以及模型与环境之间的复杂交互，即使没有任何单个部分是危险的，系统也可能具有欺骗性。</li><li><strong>这种策略已经通过自动解释技术在视觉上尝试过</strong>，以标记所有神经元，并且它似乎没有太多高级对齐，并且大多数神经元都回避简单的解释：<ul><li> NetDisect 和神经元的组成解释（Mu 和 Andreas，2021）</li><li>深度视觉特征的自然语言描述（Andreas，2022）</li><li> Clip-Dissect（Oikarinen，2022）<a href="https://visualvocab.csail.mit.edu/"><u>走向 GAN 潜在空间的视觉概念词汇</u></a>（Schwettmann，2021）</li><li>这些工作[ <a href="https://www.lesswrong.com/posts/XZfJvxZqfbLfN6pKh/introductory-textbook-to-vision-models-interpretability"><u>此处</u></a>部分总结]并没有改变我们在实践中尝试使视觉系统更加强大且风险更低的方式。</li></ul></li><li>大多数自动解释性工作，例如<a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models"><u>语言模型可以解释来自 OpenAI 的语言模型中的神经元</u></a>或概念擦除技术，都属于这一类。</li></ul><h2>逆向工程？</h2><p>逆向工程是可解释性的典型例子，但我没有看到成功的前进方向。这会是：</p><ul><li>该模型的<strong>等效 C++ 注释算法</strong>是什么？能够通过一些模块化 C++ 代码重现 GPT-4 的难以理解的矩阵的功能已经超出了人类的智能水平，这<a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>太危险</u></a>了，因为这将允许很多不同的优化，并且可能允许递归自我- 改进似乎很危险，特别是如果我们依赖自动化流程来实现这一点。</li><li><strong>对模型行为的通俗解释</strong>？在哪个粒度级别？每个标记、句子或段落？这实在是不清楚。</li><li>通过高级解释获得的模型的<strong>功能连接组</strong>？好吧，您在功能连接组中看到该模型能够编码和破解，而这些都是危险的功能。这不就是常规的评估吗？<ul><li>在实践中，为了进行插值实验，我们几乎总是从创建提示数据集开始。也许有一天我们不需要提示来激活这些功能，但我不认为（即使是原则上）这种情况会很快发生。</li></ul></li><li>一张<strong>图</strong>来解释电路？像下面这样的图表可能会让人不知所措，但仍然非常有限。</li></ul><p>您可以注意到，“枚举安全性”通常隐藏在“逆向工程”结局背后。 </p><p><img style="width:599.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/hpuwlgsxjtrumo38ckav" alt=""></p><p><i>来自</i><a href="https://arxiv.org/abs/2211.00593"><i><u>IOI论文</u></i></a><i>。从 Wang 等人的“Interpretability in the Wild”中理解该图。 2022 年对于我们的讨论来说并不重要。了解完整的电路和所使用的方法需要</i><a href="https://www.youtube.com/watch?v%3Dgzwj0jWbvbo"><i><u>三个小时的视频</u></i></a><i>。而且，此分析仅关注单个标记并涉及大量简化。例如，虽然我们试图解释为什么标记“Mary”比“John”更受欢迎，但我们没有深入研究为什么模型最初考虑“Mary”或“John”。此外，此分析仅基于 GPT2-small。</i> </p><p></p><p><img style="width:385.83px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/mmmq7xwly9mufts4101o" alt=""></p><p><i>确实，这个数字是相当可怕的。来自</i><a href="https://www.lesswrong.com/posts/j6s9H9SHrEhEfuJnq/causal-scrubbing-results-on-induction-heads"><i><u>因果擦洗：感应头上的结果</u></i></a><i>，适用于 2 层模型。经过 4 次精炼假设，他们能够挽回 86% 的损失。但即使对于这个简单的任务，他们也表示“我们最终不会得出完全具体或完全人类可以理解的假设，因果清理将使我们能够验证模型的哪些组件和计算是重要的。”。</i></p><p>在上面的两个玩具示例中，逆向工程已经如此困难，这一事实似乎令我感到担忧。</p><h2>奥拉的可解释性梦想？</h2><p>又或许 interp 只是一场由好奇心驱动的探索，等待着意外的发现？</p><ul><li><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>可解释性梦想</u></a>是 Chris Olah 关于机械可解释性未来目标的非正式说明。它讨论了<strong>叠加</strong>，即可解释性的敌人。然后，在注释的末尾，在标题为“<a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23safety"><u>机械可解释性如何适应安全性？”的</u></a>部分中。 ”，我们理解该计划是为了解决叠加能够使用以下公式： <br><br><img style="width:526.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/bbzkzi7n1cc1w6cvueju" alt=""></li><li>但这又是用电路而不是功能来表述的“<i>枚举安全性”</i> 。然而，正如上面所解释的，我认为这不会给我们带来任何好处。</li><li>笔记的最后一部分“ <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23aesthetics"><u>美丽与好奇</u></a>”读起来就像一首诗或一首美丽的赞歌。然而，除了偶然发现的希望之外，它似乎缺乏实质内容。</li></ul><p>总的来说，我对 Anthropic 使用字典学习方法来解决叠加问题持怀疑态度。虽然他们的负面结果很有趣，并且他们正在努力解决围绕“功能”概念的概念性困难（如其<a href="https://transformer-circuits.pub/2023/may-update/index.html%23superposition-dictionary"><u>5 月更新</u></a>中所述），但我仍然对这种方法的有效性持怀疑态度，即使在阅读了他们<a href="https://transformer-circuits.pub/2023/july-update/index.html%23safety-features"><u>最近的 7 月更新</u></a>后也是如此。仍然没有解决我对枚举安全性的反对意见。</p><p> Olah<a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">建议的</a>一个潜在解决方案是自动化研究：“<i>似乎很可能方法的类型 [...] 最终将不够，并且可解释性可能需要依赖人工智能自动化</i>”。然而，我相信这种自动化是潜在有害的[ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>有害</u></a>部分]。</p><p>这仍然是一个正在发展的故事，在 Distill 上发表的论文总是令人读起来很愉快。然而，我仍然对押注这种方法犹豫不决。</p><h2>重新定位搜索？</h2><p>或者也许 interp 对于<a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"><strong><u>重定向搜索</u></strong></a><strong>很有用</strong><strong>？</strong>这个想法表明，如果我们在系统中找到目标，我们可以简单地改变系统的目标并将其重定向到更好的目标。</p><p>我认为这是一个充满希望的探索，即使仍然存在困难：</p><ul><li>这很有趣，因为这将是一种不需要对完整模型进行完全逆向工程的方法。<strong> </strong>对我来说<a href="https://www.alignmentforum.org/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network"><u>，理解和控制迷宫解决策略网络</u></a>中使用的技术似乎很有前途。只需关注“激励 API”就足够了。</li><li>但我仍然不知道<a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vecto"><u>转向向量</u></a>（即潜在空间中向量的激活添加）是否真的算作可解释性，并且真的显着改变了对齐的图像，而不仅仅是即时工程。好的，这是修补模型的新方法。但我不知道如何可靠地使用它来防止欺骗。 <span class="footnote-reference" role="doc-noteref" id="fnrefmf7vlk6ib69"><sup><a href="#fnmf7vlk6ib69">[7]</a></sup></span></li></ul><h2>轻松的对抗训练？</h2><p><strong>轻松的对抗训练？</strong> TL;DR 是，<a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>宽松的对抗训练</u></a>与对抗训练相同，但我们不是创建对抗输入来测试网络，而是创建对抗潜在向量。这可能很有用，因为创建真实的对抗性输入是对抗性训练的瓶颈。 [更多解释<a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.2ats8akz8z6u"><u>请看这里</u></a>]</p><p>这似乎是正确的，但非常困难，并且仍然存在重大的<a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>概念困难</u></a>。已经提出了一种具体的方法，即<a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>潜在对抗训练</u></a>，并且似乎很有希望，但是：</p><ul><li><strong>程序未明确说明</strong>。<strong>元参数会太多</strong>。校准这些元参数将是一场噩梦，您可能不想迭代具有欺骗性的强大模型。我们必须从元参数的第一个选择开始就做好。<i>正如作者自己所说，“这里唯一的希望在于外科医生在模型学会欺骗之前强制其保持绝对安全。一旦模型具有欺骗性，那么游戏就真的结束了。”</i></li><li><strong>我们仍然没有任何保证。</strong>这个过程允许一个对“小扰动”具有鲁棒性的潜在空间，但对“小扰动”具有鲁棒性<i>并不等同于不具有欺骗性</i>（我不清楚欺骗不会出现在约束区域之外）。</li><li>使用这种程序的论文效果有限，例如论文<a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (Liu et al., 2019)</u></a>中的检测率约为 90%。 [ <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.ewixtcjxjqvq"><u>此处</u></a>总结的论文] 我认为这并不适用于所有类型的木马。</li></ul><p>据我所知，<a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>潜在对抗训练</u></a>中描述的确切程序尚未经过测试。所以我们应该努力解决这个问题。 <span class="footnote-reference" role="doc-noteref" id="fnrefc2q5uxqhj6j"><sup><a href="#fnc2q5uxqhj6j">[8]</a></sup></span></p><h2>显微镜人工智能？</h2><p><strong>也许显微镜人工智能，即</strong>也许我们可以直接使用人工智能的世界模型，而不必了解一切。 Microscope AI 是一种不会用于推理的 AI，而是仅通过查看其内部激活或权重来使用，而不需要部署它。我的定义是这样的：我们可以向前传球，但只能在模型的中间进行。</p><ul><li>这违背了几乎所有的经济激励措施（参见 Gwern 的<a href="https://gwern.net/tool-ai%23:~:text%3DAn%2520Agent%2520AI%2520has%2520the,its%2520outputs%252C%2520on%2520harder%2520domains."><u>《为什么工具人工智能想要成为人工智能代理</u></a>》）。</li><li> <strong>($) 可解释性对于发现世界事实几乎没有用，并且仅通过查看权重来学习新东西太难了。</strong><ul><li>在<a href="https://arxiv.org/abs/2111.09259"><u>《Acquisition of Chess Knowledge in AlphaZero》</u></a>一文中，作者研究了“<i>我们是否可以通过解释经过训练的 AlphaZero 的<strong>行为</strong>来学习国际象棋策略</i>”。答：事实并非如此。他们仅使用 Stockfish 已知的概念来探测网络，并且没有获得新的基本见解。我们只检查 AlphaGo 在训练过程中<i>何时</i>学习人类概念。</li><li>我认为我们无法通过对陶哲轩的大脑进行逆向工程来学习范畴论。围棋棋手如何从围棋程序中学习策略？他们会解释 AlphaGo 的权重，还是试图理解这些程序的行为评估？回答：他们从自己的行为中学习，而不是通过解释模型来学习。我怀疑我们能否从我们还不知道的神经网络的权重/激活/电路中获得全新的知识，特别是考虑到仅从英语教科书中学习东西是多么困难。</li></ul></li><li><strong>根据定义，显微镜人工智能不应具有代理性。但能动性和探索对于人类发现新真理有巨大帮助。因此，在超人水平以下，</strong><i><strong>显微镜</strong></i><strong>需要具有</strong><i><strong>代理性</strong></i><strong>……这是一个矛盾。</strong> <a href="https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a%23Philosophical_framing"><u>例如</u></a>，建议使用 Microscope AI 作为工具而不是<a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>代理</u></a>。然而，要了解复杂事实的真相，我们需要对世界进行实验并积极寻找信息。这是一个模糊推理（随意跳过）：<ul><li> A) 要么<strong>信息已经存在并且被清楚地写</strong>在互联网上的某个地方，在这种情况下，就不需要显微镜人工智能（这就像文本检索）。</li><li> B）或者<strong>该信息在互联网上不存在</strong>，在这种情况下，有必要通过对世界进行实验或积极思考来发挥作用。这种特征只能通过强化学习“创建”，但无法通过监督学习“发现”，就像 MuZero 发现新的国际象棋策略一样。</li><li>或 C），<strong>这些信息并不是简单地写出来的，而是训练数据的深层特征</strong>，可以通过梯度下降来理解/理解。这是可以通过监督学习“发现”的特征类型。</li><li>如果是 B），我们需要代理，而它不再是显微镜。</li><li>如果 C)，我们可以应用上述推理<u>($)</u> + 能够通过纯梯度下降而不进行探索来实现这一点可能比通过探索来实现这一点的能力更高。 （这就像在梦中发现<a href="https://en.wikipedia.org/wiki/Quaternion"><u>四元数</u></a>公式一样？）。但即使是传奇数学家也需要付出一些努力，并在探索中保持积极性。他们不只是被动地阅读教科书。所以，恐怕超出了拉马努金的水平，太危险了？</li><li>所以，我很不确定，但总的来说，我不认为显微镜人工智能是降低人工智能风险的有前途或有效的方法。</li></ul></li></ul><p> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Burns_et_al___2022_"><u>附录</u></a>中包含一个简短的案例研究，用于通过探测从模型中提取知识<a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><u>的发现潜在知识</u></a>技术。</p><h1>针对欺骗的预防措施似乎更加可行</h1><p>TL;DR：更理想的目标是实现一个不需要深入研究模型内部组件的世界。预防胜于治疗，或者至少，这是一种被忽视的策略。 </p><figure class="image image_resized" style="width:599.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lon9yhtv1wgba17yhryt" alt=""><figcaption><i>从</i><a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23The_Translucent_Thoughts_Hypotheses"><i><u>半透明的思想假设及其含义</u></i></a><i>，法比安·罗杰</i></figcaption></figure><p>我不认为可解释性是监控接近人类水平的 AGI 最有前途的方法，原因如下：</p><p><strong>我不认为神经网络能够在一次前向传递中接管。模型可能会用英语进行推理，并且会有半透明的想法</strong>（我们甚至希望获得透明的想法，半透明是隐写术可能的可怕之处）。为了制定一个有效的计划来毁灭世界，第一个人类水平的 AGI 需要通过思想链（CoT）在一定程度上外化其推理，在我看来，监控这个思想链要容易得多。思考而不是探究模型的内部结构。我们或许可以使用<a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications"><u>半透明思想假设</u></a>（Fabien，作者，给出了 20% 的机会，但我认为它会更像 60%）。我还认为，我们必须尽一切努力通过法规，<strong>并将自己置于一个这些假设成立的世界中。</strong></p><p>例如，魔法是可能的（顺便说一句，这是<a href="https://www.youtube.com/watch?v%3DEAN-PwRfJcA"><u>我最喜欢的把戏</u></a>），而人类的大脑可以如此轻易地被魔法操纵，这一事实是一个巨大的危险。因此，我们或许应该尝试让这种魔法变得不可能，例如通过训练人工智能系统进行基于流程的训练，使其仅一步一步地推理（基于流程的训练方法在<a href="https://www.lesswrong.com/posts/FFz6H35Gy6BArHxkc/task-decomposition-for-scalable-oversight-agisf-distillation"><u>AGISF 精炼</u></a>中进行了总结）。</p><h2>引导世界走向透明</h2><p><strong>提前考虑训练设置非常重要，并且可能会主导解释考虑因素，</strong>因为如果模型操纵的概念完全陌生，那么提供监督就会变得非常非常困难。与使用 RL/进化方法等从头开始训练的 AGI 相比，调整那些根据人类生成的文本进行预训练的聊天机器人要容易<a href="https://aligned.substack.com/i/88447351/the-ai-tech-tree-is-looking-favorably"><u>得多</u></a>。</p><p>如果是这种情况，我们应该关注 Fabien <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23Research_Directions"><u>在此</u></a>详细介绍的各个方面和重要建议。以下是一些亮点：</p><ul><li><strong>让法学硕士只使用可理解的英语：</strong>防止模型使用隐写术、不对合成数据进行训练等……这些方向通常被非常忽视 - 例如，看看这个<a href="https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId%3DS4wqnxW5CYd8QsTox"><u>评论</u></a>。</li><li><strong>建立一个监督系统：</strong>这就是 Buck、IDA 的“<a href="https://youtu.be/YTlrPeikoyw?t%3D2415"><u>蓄势待发</u></a>”的技术研究，或者被忽视的基线，如<a href="https://arxiv.org/abs/2308.07308"><u>过滤</u></a>等……</li><li><strong>测试半透明思想假设</strong>并能够监控模型的异质性对于确保某些拟像具有类似大脑的表征非常重要（请参阅<a href="https://www.lesswrong.com/posts/iXbPe9EAxScuimsGh/linkpost-scaling-laws-for-language-encoding-models-in-fmri"><u>此</u></a>和<a href="https://www.lesswrong.com/posts/2QexGHrqSxcuwyGmf/linkpost-large-language-models-converge-on-brain-like-word"><u>此</u></a>）。<a href="https://arxiv.org/abs/2305.04388"><u>语言模型并不总是说出他们的想法：思想链提示中的不忠实解释</u></a>方向的更多工作。</li><li>通过技术工作、推广和治理等<strong>使假设成立</strong>。</li></ul><p><strong>我认为策略应该是：让我们瞄准一个不可能出现欺骗的世界。</strong> （我并不是说我们应该制定以默认情况下不可能发生欺骗为条件的计划，但我们应该尝试引导 AGI/世界走向一个不可能发生欺骗的地方）。我相信有多种方法可以思考和解决这个问题，并且这里需要更多的技术研究，从<a href="https://arxiv.org/abs/2302.00805"><u>条件预测模型：风险和策略</u></a>开始。</p><h2>认知模拟 -<i>设计的</i>可解释性</h2><p><strong>如果可解释性确实是一个瓶颈，我们可以使用</strong><a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>认知模拟</u></strong></a><strong>，在我看来，这比可解释性给我们带来了更好的可解释性和透明度。</strong></p><p>附录的<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design"><u>认知仿真</u></a>部分中有一些注意事项。</p><h1>可解释性可能总体上是有害的</h1><p>（请注意，以下一些要点并非特定于 interp，但我认为它们特别适用于 interp。）</p><p><strong>错误的控制感：</strong></p><ul><li><strong>错误的理解感。</strong>当我们没有太多的时候，你很容易认为你开始理解我们开始得到保证。这是非常经典的：<ul><li>过去的我：“哟，我花了5个小时试图用<a href="https://transformer-circuits.pub/2021/framework/index.html"><u>变形金刚数学框架中</u></a>难以理解的数学公式来理解归纳头和K组合的机制，我明白了很多。”是的，但不是。</li></ul></li><li><strong>过度解读。</strong>很难说哪个解释结果是可靠的。例如，<a href="https://arxiv.org/abs/1810.03292"><u>显着性图的健全性检查</u></a>表明大多数像素归因技术通常具有误导性。 <span class="footnote-reference" role="doc-noteref" id="fnrefpo4e41md3r"><sup><a href="#fnpo4e41md3r">[9]</a></sup></span>同样，特征可视化最近被发现有一些相当致命的缺陷，请参阅<a href="https://arxiv.org/abs/2306.04719"><u>不要相信你的眼睛：关于特征可视化的（不）可靠性</u></a>，并且诸如<a href="https://rome.baulab.info/"><u>ROME</u></a>之类的模型编辑技术<a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><u>非常具有误导性</u></a>。这主要是由于斯蒂芬·卡斯珀在他的序列中解释的方法论问题。 [参见附录： <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Methodological_problems_"><u>方法论问题</u></a>]。</li><li><strong>安全洗涤。</strong>我觉得安全研究的一部分是为了使大型实验室的能力研究合法化（尽管这并不完全特定于 interp）。<ul><li> <i>“我认为，相当一部分从事“人工智能联盟研究”的人的主要目标是“让人工智能联盟看起来合法”。这些不是同一个目标，很多优秀的人都可以看出，这让他们感觉有点被欺骗了，而且这在这个领域内造成了非常混乱的动态，人们对研究的次要影响是什么有强烈的看法，因为这就是他们感兴趣的主要事情，而不是询问研究是否指向真正能够调整人工智能的有用的真实事物”，</i>摘自 <a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>《关闭 Lightcone 办公室》</u></a> 。</li></ul></li><li>与对手研究等其他领域的成就相比<strong>，解释性研究的成就始终按照自己的曲线进行评级，并且被夸大了</strong>。例如，最近的论文<a href="https://arxiv.org/abs/2307.15043"><u>《对齐语言模型的通用和可转移对抗攻击》</u></a>令人印象深刻地发现了针对最先进模型的有效攻击，而无需任何涉及模型内部的解释。想象一下，如果机械可解释性研究人员做了完全相同的事情，但是通过研究模型内部结构会怎样？考虑到过去围绕精心挑选的问题（例如<a href="https://arxiv.org/abs/2301.05217"><u>这个</u></a>或<a href="https://arxiv.org/abs/2211.00593"><u>这个</u></a>）的玩具模型的机械解释性成就而出现的兴奋，似乎这样的事情可能会让人工智能安全研究界变得疯狂。 Stephen Casper <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/MyvkTKfndx9t4zknh%23:~:text%3DFrom%2520an%2520engineer%25E2%2580%2599s%2520perspective%252C%2520it%25E2%2580%2599s%2520important%2520not%2520to%2520grade%2520different%2520classes%2520of%2520solutions%2520each%2520on%2520different%2520curves.%25C2%25A0"><u>在这里</u></a>提出了类似的观点：“<i>从工程师的角度来看，重要的是不要在不同的曲线上对不同类别的解决方案进行分级。</i> <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_:~:text%3Dtechniques%2520on%2520curves...-,Imagine%2520that%2520you%2520heard%2520news%2520tomorrow%2520that%2520MI%2520researchers%2520from%2520TAISIC%2520meticulously%2520studied%2520circuits%2520in%2520a%2520way%2520that%2520allowed%2520them%2520to%25E2%2580%25A6,-Reverse%2520engineer%2520and"><u>EIS VI：人工智能安全中机械可解释性工作的批评</u></a>（感谢 Stephen 强调了这一点）。</li></ul><p><strong>世界对于公共可解释性研究的协调性还不够：</strong></p><ul><li><strong>双重用途。</strong>似乎任何与信息表示相关的东西都可以以双重方式使用。这是一个问题，因为我相信可解释性研究的核心可以带来能力的重大进步。请参阅此<a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>帖子</u></a>。<ul><li>使用高级 interp 提供的见解来改进功能（例如通过模块化来优化推理时间和减少失败）可能比使用它们进行更好的监督更容易。 This is because <strong>optimizing for capability is much simpler than optimizing for safety</strong> , as we lack clear metrics for measuring safety.</li></ul></li><li> <strong>When interpretability starts to be useful, you can&#39;t even publish it because it&#39;s too info hazardous.</strong> The world is not coordinated enough for public interpretability research.<ul><li> Nate Soares <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous"><u>explained</u></a> this, and this was followed by <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>multiple</u></a> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>posts</u></a> . <i>“Insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research</i> <a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><i><u>closed</u></i></a> <i>. […] I acknowledge that public sharing of research insights could, in principle, both shorten timelines and improve our odds of success. I suspect that</i> <a href="https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"><i><u>isn&#39;t the case in real life</u></i></a> <i>.”</i></li><li> Good interp could produce a &quot;foom overhang&quot; as described in &quot; <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a> &quot;.</li><li> Good interp also creates an<a href="https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful"><u>infosec/infohazard attack vector</u></a> .</li><li> The post &#39; <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When is Interpretability Work Dangerous?</u></a> &#39; ends on a sobering note, stating, “ <i>In closing, if alignment-conscious researchers continue going into the interpretability subfield, the probability of AGI ruin will tend to increase.</i> ”</li></ul></li><li> <strong>Interpretability already helps capabilities.</strong> For example, the understanding of Induction head has allowed for <a href="https://twitter.com/NeelNanda5/status/1618185819285778433"><u>better</u></a> architectures <span class="footnote-reference" role="doc-noteref" id="fnrefplu4ji16iui"><sup><a href="#fnplu4ji16iui">[10]</a></sup></span> .</li><li> Interpretability may be a <a href="https://en.wikipedia.org/wiki/Wicked_problem%23Super_wicked_problems"><u>super wicked problem</u></a> <span class="footnote-reference" role="doc-noteref" id="fnrefw7s6gsvuwb"><sup><a href="#fnw7s6gsvuwb">[11]</a></sup></span> .</li></ul><p> Thus the list of &quot;theory of impact&quot; for interpretability should not simply be a list of benefits. It&#39;s important to explain why these benefits outweigh the possible negative impacts, as well as how this theory can save time and mitigate any new risks that may arise. </p><p></p><p><img style="width:414.07px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zao0mylkanwh60aajinm" alt=""></p><p> <i>The concrete application of the</i> <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"><i><u>logit lens</u></i></a> <i>is not an oversight system for deception, but rather capability works to accelerate inference speed like in</i> <a href="https://twitter.com/GoogleAI/status/1603845007663734785"><i><u>this paper</u></i></a> <i>. (Note that the paper does not cite logit lens, but relies on a very similar method).</i></p><h1> Outside view: The proportion of junior researchers doing interp rather than other technical work is too high</h1><p> It seems to me that many people start alignment research as follows:</p><ul><li> At the end of <a href="https://www.arena.education/"><u>Arena</u></a> , an advanced upskilling program in AI Safety, almost all research projects this year (June 2023), except for two out of 16, were interp projects.</li><li> At <a href="https://ia.effisciences.org/"><u>EffiSciences</u></a> , at the end of the last 3 <a href="https://www.lesswrong.com/posts/DkDy2hvkwbQ54GM9u/introducing-effisciences-ai-safety-unit-1"><u>ML4Good</u></a> bootcamps, students all start by being interested in interp, and it is a very powerful attractor. I myself am guilty. I have redirected too many people to it. I am now trying to correct my ways.<ul><li> In the past, if I reconstruct my motivational story, it goes something like this: &quot;Yo, I have a math/ML background, how can I recycle that?&quot; -->; then <i>brrr interp</i> , without asking too many questions.</li></ul></li><li> During <a href="https://apartresearch.com/"><u>Apart Research</u></a> hackathons, interpretability hackathons tend to draw 3.12 times as many participants as other types of hackathons. (thinkathon, safety benchmarks, …). <span class="footnote-reference" role="doc-noteref" id="fnrefavln8kyvlzg"><sup><a href="#fnavln8kyvlzg">[12]</a></sup></span></li><li> Interpretability streams in <a href="https://www.serimats.org/"><u>Seri Mats</u></a> are among the most competitive streams (see this <a href="https://twitter.com/leedsharkey/status/1656705667963535370"><u>tweet</u></a> ). People then try hard, get rejected, get disappointed and lose motivation. This is a recent important problem.</li></ul><p> &quot;Not putting all your eggs in one basket&quot; seems more robust considering our uncertainty, and there are more promising ways to reduce x-risk per unit of effort (to come in a future post, mostly through helping/doing governance). I would rather see a <strong>more diverse ecosystem</strong> of people trying to reduce risks. More on this in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>Technical Agendas with better ToI</u></a> .</p><p> If you ask me if interp is also over represented in senior researchers, I&#39;m a bit less confident. Interp also seems to be a significant portion of the pie: this year, while Conjecture and Redwood have partially pivoted, there are new active interp teams in Apollo, DeepMind, OpenAI, and still in Anthropic. I think I would particularly critique DeepMind and OpenAI&#39;s interpretability works, as I don&#39;t see how this reduces risks more than other works that they could be doing, and I&#39;d appreciate a written plan of what they expect to achieve.</p><h1> So far my best ToI for interp: Nerd Sniping?</h1><p> 1. <strong>Interp for Nerd Sniping/honeypot?</strong></p><ul><li> <strong>Interp is a highly engaging introduction to AI research</strong> . That&#39;s really cool for that, I use it for my <a href="https://www.master-mva.com/cours/seminaire-turing/"><u>classes</u></a> , and for technical outreach, but I already have enough material on interpretability, for 10 hours of class, no need to add more.</li><li> <strong>Interp as a honeypot for junior researchers?</strong> Just as a honeypot attracts bees with its sweet nectar, interp is very successful for recruiting new technical people! but then they would probably be better off doing something else than interp (unless it is their strong comparative advantage).</li><li> (Nerd Sniping senior capability researchers into interpretability research? Less capability research, more time to align AIs? I&#39;m joking, don&#39;t do that at home! )</li></ul><p> 2. <strong>Honorable mentions:</strong></p><ul><li> <strong>Showing strange failures</strong> , such as the issue with the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"><u>SolidGoldMagicCarp</u></a> token, highlights the possibility of unexpected results with the model. More generally, interpretability tools can be useful for the red teaming toolbox. They seem like they might be able to guide us to more problems than test sets and adversaries can alone.</li><li> <strong>Showing GPT is not a stochastic parrot?</strong> The article <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>Actually, Othello-GPT Has A Linear Emergent World Representation</u></a> <strong>&nbsp;</strong> is really cool <strong>.</strong> Showing that OthelloGPT contains a world model is really useful for technical outreach (even if OthelloGPT being good at Othello should be enough, no?).</li><li> <strong>It&#39;s a good way to introduce the importance and tractability of alignment research</strong> “ <i>Interpretability gives people a non-technical story for how alignment affects their lives, the scale of the problem, and how progress can be made. IMO no other approach to alignment is anywhere near as good for this.”</i> [from <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DuzBFJDsy9Jqkxzdnx"><u>Raymond D</u></a> ]</li><li> <strong>Better: Showing that “We have basically no idea how it does what it does.”,</strong> see this <a href="https://twitter.com/robertskmiles/status/1663534255249453056"><u>tweet</u></a> : </li></ul><p><img style="width:387.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/q9gbkwdp3drer7soei5a" alt=""></p><h1> Even if we completely solve interp, we are still in danger</h1><p> No one has ever claimed otherwise, but it&#39;s worth remembering to get the big picture. From stronger arguments to weaker ones:</p><ul><li> <strong>There are many X-risks scenarios, not even involving deceptive AIs.</strong> Here is a list of such scenarios (see this <a href="https://www.lesswrong.com/posts/nCeyBbhtJhToBFmrL/cheat-sheet-of-ai-x-risk"><u>cheat sheet</u></a> ):<ul><li> Christiano1 - <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom%23Part_I__You_get_what_you_measure"><u>You get what you measure</u></a></li><li> Critch1 - <a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic%23Part_1__Slow_stories__and_lessons_therefrom"><u>Production Web</u></a></li><li> Soares - <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>A central AI alignment problem: capabilities generalization, the sharp left turn</u></a></li><li> Cohen et al. - <a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>Advanced artificial agents intervene in the provision of reward</u></a></li><li> Gwern - <a href="https://gwern.net/fiction/clippy"><u>It Looks Like You&#39;re Trying To Take Over The World</u></a></li><li> Exercise: Here is <a href="https://www.safe.ai/ai-risk"><u>a list of risks</u></a> from the Center of AI Safety. Which ones can be solved by interp? At least half of those risks don&#39;t directly involve deception and interp.</li></ul></li><li> <strong>Total explainability of complex systems with great power is not sufficient to eliminate risks.</strong> Significant risks would still remain. Despite our full understanding of how atomic bombs function, they still pose substantial risks. See this <a href="https://en.wikipedia.org/wiki/List_of_nuclear_close_calls"><u>list of nuclear close calls</u></a> .</li><li> <strong>Interpretability implicitly assumes that the AI model does not optimize in a way that is adversarial to the user.</strong> Consider being able to read the mind of a psychopath like Voldemort. Would this make you feel safe? The initial step remains to box him. However, <strong>a preferable scenario would be not having to confront this situation at all.</strong> (this last claim is probably the most important lesson - see <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ). </li></ul><figure class="image image_resized" style="width:54.8%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/s6qlqvxf5an1gdofmmk3" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qgopxuvs7mrmxegq8nco 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/smtffxyqwgnqhsasg4fy 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lrifkrnrib0sybu8nvo8 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/j9jd3of3dwrmqpyg6xhr 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/cuouqap73vyebun1hfu7 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/e3ppnnd6cgyt2czjf3l6 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/vozp1plvffdzhxsuh1qy 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qywwfgebl65ejalxo5ps 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/araz4zygcvf4szuqkmy4 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/spahqo0hllay5zjyvlk0 1010w"><figcaption> Pytorch hooks can be used to study the internals of models. Are they going to be sufficient? <i>Idk, but</i> <a href="https://www.youtube.com/watch?v=LveUcCBRrSo"><i><u>Hook Me up Baby</u></i></a> <i>, from the album “Take Me as I Am” could be the national anthem of interp.</i></figcaption></figure><p> <strong>That is why focusing on coordination is crucial! There is a level of coordination above which we don&#39;t die - there is no such threshold for interpretability.</strong> We currently live in a world where coordination is way more valuable than interpretability techniques. So let&#39;s not forget that <a href="https://www.alignmentforum.org/posts/FfTxEf3uFPsZf9EMP/avoiding-perpetual-risk-from-tai%23Non_alignment_aspects_of_AI_safety_are_key_"><u>non-alignment aspects of AI safety are key!</u></a> AI alignment is only a subset of AI safety! ！ (I&#39;m planning to deep-dive more into this in a following post).</p><p> A version of this argument applies to &quot;alignment&quot; in general and not just interp and those considerations will heavily influence my recommendations for technical agendas.</p><h1> Technical Agendas with better ToI</h1><p> Interp is not such a bad egg, but opportunity costs can be huge (especially for researchers working in big labs).</p><p> I&#39;m not saying we should stop doing technical work. Here&#39;s a list of technical projects that I consider promising (though I won&#39;t argue much for these alternatives here):</p><ul><li> <strong>Technical works used for AI Governance.</strong> A huge amount of technical and research work needs to be done in order to make regulation robust and actually useful. The governance section of <a href="https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice#Governance_work"><u>AGI safety career advice</u></a> by Richard Ngo is really great : “ <i>It&#39;s very plausible that, starting off with no background in the field, within six months you could write a post or paper which pushes forward the frontier of our knowledge on how one of those topics is relevant to AGI governance.</i> ”<ul><li> For example, each of the measures proposed in the paper <a href="https://arxiv.org/abs/2305.07153"><u>towards best practices in AGI safety and governance: A survey of expert opinion</u></a> could be a pretext for creating a specialized organization to address these issues, such as auditing, licensing, and monitoring.</li><li> Scary demos (But this shouldn&#39;t involve gain-of-function research. There are already many powerful AIs available. Most of the work involves video editing, finding good stories, distribution channels, and creating good memes. Do not make AIs more dangerous just to accomplish this.).</li><li> In the same vein, <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a> is probably good because “ <a href="https://www.lesswrong.com/posts/vavnqwYbc8jMu3dTY/ai-coordination-needs-clear-wins"><u>AI coordination needs clear wins</u></a> ”.</li><li> Interoperability in AI policy, and good definitions usable by policymakers.</li><li> Creating benchmarks for dangerous capabilities.</li><li> Here&#39;s a <a href="https://docs.google.com/document/d/1Tvz2JS8CZ51TW-vfU3vwRn8dpK3F0UttdhMrZC2o7hw/edit#bookmark=id.jnvbactyuay7"><u>list</u></a> of other ideas</li></ul></li><li> <strong>Characterizing the technical difficulties of alignment. (</strong><a href="https://www.lesswrong.com/posts/uHYYA32CKgKT3FagE/hold-off-on-proposing-solutions"><strong><u>Hold Off On Proposing Solutions</u></strong></a> <strong>“Do not propose solutions until the problem has been discussed as thoroughly as possible without suggesting any.”)</strong><ul><li> Creating the <a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change"><u>IPCC</u></a> of AI Risks</li><li> More red-teaming of agendas</li><li> Explaining problems in alignment.</li></ul></li><li> Adversarial examples, adversarial training, latent adversarial training (the only <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>end-story</u></a> I&#39;m kind of excited about). For example, the papers &quot; <a href="https://arxiv.org/abs/2210.04610"><u>Red-Teaming the Stable Diffusion Safety Filter</u></a> &quot; or &quot; <a href="https://arxiv.org/abs/2307.15043"><u>Universal and Transferable Adversarial Attacks on Aligned Language Models</u></a> &quot; are good (and pretty simple!) examples of adversarial robustness works which contribute to safety culture.</li><li> <strong>Technical outreach</strong> . <a href="https://www.youtube.com/@ai-explained-"><u>AI Explained</u></a> and <a href="https://www.youtube.com/c/robertmilesai"><u>Rob Miles</u></a> have plausibly reduced risks  more than all interpretability research combined.</li><li> In essence, ask yourself: “What would Dan Hendrycks do?”<ul><li> Technical newsletter, non-technical newsletters, benchmarks, policy recommendations, risks analysis, banger statements, courses and technical outreach.</li><li> He is not doing interp. Checkmate!</li></ul></li></ul><p> In short, my agenda is <strong>&quot;Slow Capabilities through a safety culture&quot;</strong> , which I believe is robustly beneficial, even though it may be difficult. I want to help humanity understand that we are not yet ready to align AIs. Let&#39;s wait a couple of decades, then reconsider.</p><p> And if we really have to build AGIs and align AIs, it seems to me that it is more desirable to aim for a world where we don&#39;t need to probe into the internals of models. Again, prevention is better than cure.</p><h1>结论</h1><p>I have argued against various theories of impact of interpretability, and proposed some alternatives. I believe working back from the different risk scenarios and red-teaming the theories of impact gives us better clarity and a better chance at doing what matters. Again, I hope this document opens discussions, so feel free to respond in parts. There probably <i>should</i> be a non-zero amount of researchers working on interpretability, this isn&#39;t intended as an attack, but hopefully prompts more careful analysis and comparison to other theories of impact.</p><p> We already know some broad lessons, and we already have a general idea of which worlds will be more or less dangerous.Some ML researchers in top labs aren&#39;t even aware of, or acknowledging, that AGI is dangerous, that connecting models to the internet, encouraging agency, doing RL and maximizing metrics isn&#39;t safe in the limit.</p><p> Until civilization catches up to these basic lessons, we should avoid playing with fire, and should try to slow down the development of AGIs as much as possible, or at least steer towards worlds where it&#39;s done only by extremely cautious and competent actors.</p><p> Perhaps the main problem I have with interp is that it implicitly reinforces the narrative that we must build powerful, dangerous AIs, and then align them. For X-risks, prevention is better than cure. Let&#39;s <i>not</i> build powerful and dangerous AIs. We aspire for them to be safe, by design.</p><h1>附录</h1><h2>Related works</h2><p> There is a vast academic literature on the virtues and academic critiques of interpretability (see this <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>page</u></a> for plenty of references), but relatively little holistic reflection on interpretability as a strategy to reduce existential risks.</p><p> The most important articles presenting arguments for interpretability:</p><ul><li> <a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>Chris Olah&#39;s views on AGI safety</u></a></li><li> <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>A Longlist of Theories of Impact for Interpretability</u></a></li><li> <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>Interpretability Dreams</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous%23When_Interpretability_is_Still_Important"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/6ReBeYwsDeNgv6Dr5/the-defender-s-advantage-of-interpretability"><u>The Defender&#39;s Advantage of Interpretability</u></a></li><li> <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a></li></ul><p> Against interpretability</p><ul><li> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>Should we publish mechanistic interpretability research?</u></a></li><li> <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>EIS III: Broad Critiques of Interpretability Research</u></a></li><li> <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3"><u>EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety</u></a></li><li> <a href="https://link.springer.com/article/10.1007/s13347-019-00372-9"><u>Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning</u></a></li></ul><h3> The Engineer&#39;s Interpretability Sequence</h3><p> I originally began my investigation by rereading  “The Engineer&#39;s Interpretability Sequence”, in which Stephen Casper raises many good critiques of interpretability research, and this was really illuminating.</p><p> <strong>Interpretability tools lack widespread use by practitioners in real applications.</strong></p><ul><li> No interpretability technique is yet publicly known to have been used in production in SOTA models such as ChatGPT.</li><li> There have been interpretability studies of SOTA multimodal models such as <a href="https://distill.pub/2021/multimodal-neurons/"><u>CLIP</u></a> in the past, but these studies are only descriptive.</li><li> The efficient market hypothesis: The technique used for the censorship filter of the Stable Diffusion model was a <a href="https://arxiv.org/abs/2210.04610"><u>vulgar cosine similarity threshold</u></a> between generated image embeddings and a list of taboo concepts. Yes, this may seem a bit ridiculous, but at least there is a filter, and it appears that interp has not yet been able to provide more convenient tools than this.</li></ul><p> <strong>Broad critiques.</strong> He <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>explains</u></a> that interp is generally not scaling, relying too much on humans, failing to combine techniques. He also <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>criticizes</u></a> mech interp, which may not be the best way of doing interp, because of cherry-picking, focusing only on toy examples and lack of scalability, and failing to do useful things.</p><p> <strong>Methodological problems:</strong></p><ul><li> He <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/gwG9uqw255gafjYN4%23A_lack_of_practical_applications"><u>points out</u></a> , &quot;The root cause of this has much to do with interpretability research not being approached with as much engineering rigor as it ought to be.&quot;</li><li> One good point to note is that since the publication of his sequence, certain methods have become more rigorous. For example, <a href="https://www.lesswrong.com/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and"><u>Causal methods</u></a> , <a href="https://arxiv.org/abs/2211.00593"><u>activation patching</u></a> , <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>probing only with linear probes</u></a> are more rigorous and promising than descriptive techniques.</li><li> But I also believe that interpretability may be fundamentally flawed [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interp look like</u></a> ].</li></ul><h2> Cognitive Emulations - Explainability <i>By design</i></h2><p> <strong>If interpretability was really a bottleneck, we could use</strong> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>cognitive emulation</u></strong></a> <strong>, which, in my opinion, allows way better explainability/transparency than interpretability will ever get us.</strong></p><p> My understanding of cognitive emulation: Emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in <i>plain English</i> to each other before outputting the next token. If the neural network had deceptive thoughts, we could see them in these intermediate messages.</p><p> <strong>Interpretability is needed only if one module of the cognitive emulation is deceptive.</strong> Then maybe you can use interpretability to explain this module. But I&#39;m confident models will need Chain of Thought for the foreseeable future. So interpretability is not &#39;urgent&#39; research.</p><p> <strong>If we don&#39;t use Reinforcement learning on this cognitive emulation, and if we only prompt in English the different submodules</strong> , the cognitive emulation won&#39;t create whole “new languages” or be able to obfuscate a lot of its thoughts.</p><p> Yes, there are <strong>problems with this proposal:</strong></p><ul><li> CoEms don&#39;t preclude steganography or obfuscation.</li><li> Even when asked to reason with Chain-of-Thought (CoT), LLMs are able to skip some inference steps to some extent (see <a href="https://arxiv.org/abs/2305.04388"><u>Language Models Don&#39;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</u></a> ). However, if we don&#39;t train models to skip a zillion inference steps, they won&#39;t be able to obfuscate their thoughts efficiently.</li></ul><p> But we could further reduce these risks with:</p><ul><li> <strong>Process-based training</strong> , to ensure that the systems are accustomed to detailing their thoughts in much greater detail (and not being able to skip too many steps). It seems much more feasible than interpretability (and the <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>alignment tax</u></a> even seems negative - for example see those progress for <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>Mathematical reasoning</u></a> )</li><li> <strong>selection inference:</strong> <i>“CoT has access to the whole context, so each reasoning step is not necessarily causally connected to the last. But</i> <a href="https://arxiv.org/abs/2205.09712"><i><u>selection inference</u></i></a> <i>enforces a structure where each reasoning step necessarily follows from the last, and therefore the whole reasoning chain is causal.”</i> from <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit?disco=AAAA21_wJ_g"><u>Sid Black</u></a> , CTO of Conjecture.</li><li> Other ideas were listed in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency">Steering the world towards transparency</a> ”.</li></ul><p> Spicy: However, cognitive emulation will quite likely be an engineering nightmare, facing significant robustness issues that are always present in small models. The alignment tax will be higher than for end-to-end systems, making it unlikely that we will ever use this technology. The bottleneck is probably not interp, but rather an ecosystem of preventive safety measures and a safety culture. Connor Leahy, CEO of Conjecture, explaining the difficulties of the problem during interviews and pushing towards a safety culture, is plausibly more impactful than the entire CoEm technical agenda.</p><h2> Detailed Counter Answers to Neel&#39;s list</h2><p> Here is Neel&#39;s <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Longlist of Theories of Impact for Interpretability</u></a> with critiques for each theory. Theories proposed by Neel are displayed in italics, whereas my critiques are rendered in standard font.</p><ol><li> <i><strong>Force-multiplier on alignment research</strong> : We can analyse a model to see why it gives misaligned answers, and what&#39;s going wrong. This gets much richer data on empirical alignment work, and lets it progress faster.</i><ul><li> I think this &quot;force multiplier in alignment research&quot; theory is valid, but is conditioned on the success of the other theories of impact, which imho are almost all invalid.</li><li> <strong>Conceptual advancements are more urgent</strong> It&#39;s better to think conceptually about what misalignment means rather than focusing on interp. [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interpretability look like?</u></a> ]</li><li> <strong>Dual Use:</strong> Force-multiplier on capability research.</li></ul></li><li> <i><strong>Better prediction of future systems</strong> : Interpretability may enable a better mechanistic understanding of the principles of how ML systems work, and how they change with scale, analogous to scientific laws.这使我们能够更好地从当前系统推断未来系统，类似于缩放定律。 Eg, observing phase changes a la induction heads shows us that models may rapidly gain capabilities during training</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Auditing</strong> : We get a Mulligan. After training a system, we can check for misalignment, and only deploy if we&#39;re confident it&#39;s safe</i><ul><li> <strong>Not the most direct way.</strong> This ToI targets outer misalignment, the next one targets inner misalignment. But currently, people who are auditing for outer alignment do not use interpretability. They evaluate the model, they make the model speak and look if it is aligned with behavioral evaluations. Interpretability has not been useful in finding GPT&#39;s jailbreaks.</li><li> To date, I still don&#39;t see how we would proceed with interp to audit GPT-4.</li></ul></li><li> <i><strong>Auditing for deception</strong> : Similar to auditing, we may be able detect deception in a model. This is a much lower bar than fully auditing a model, and is plausibly something we could do with just the ability to look at random bits of the model and identify circuits/features - I see this more as a theory of change for &#39;worlds where interpretability is harder than I hope&#39;.</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Enabling coordination/cooperation:</strong> If different actors can interpret each other&#39;s systems, it&#39;s much easier to trust other actors to behave sensibly and coordinate better</i><ul><li> <strong>Not the most direct way.</strong> If you really want coordination and cooperation, you need to help with AI governance and outreach of experts and researchers. The <a href="https://www.safe.ai/statement-on-ai-risk"><u>statement on AI risks</u></a> has enabled more coordination than interp will probably never get us.</li></ul></li><li> <i><strong>Empirical evidence for/against threat models</strong> : We can look for empirical examples of theorized future threat models, eg inner misalignment</i><ul><li> <i><strong>Coordinating work on threat models</strong> : If we can find empirical examples of eg inner misalignment, it seems much easier to convince skeptics this is an issue, and maybe get more people to work on it.</i><ul><li> <a href="https://ai.facebook.com/research/cicero/"><u>Cicero</u></a> or poker models are already capable of masking pieces of information or bluffing to play poker. From there, I don&#39;t know what it would mean to show canonical inner misalignment to non-technical people.</li><li> This focuses too much on deceptive alignment, and this will probably be too late if we get to this point.</li></ul></li><li> <i><strong>Coordinating a slowdown</strong> : If alignment is really hard, it seems much easier to coordinate caution/a slowdown of the field with eg empirical examples of models that seem aligned but are actually deceptive</i><ul><li> <strong>Not the most direct way.</strong> This is a good theory of change, but interp is not the only way to show that a model is deceptive.</li></ul></li></ul></li><li> <i><strong>Improving human feedback</strong> : Rather than training models to just do the right things, we can train them to do the right things for the right reasons</i><ul><li> Seems very different from current interpretability work.</li><li> <strong>Not the most direct way.</strong> Process-based training, model psychology, or other scalable oversight techniques not relying on interp may be more effective.</li></ul></li><li> <i><strong>Informed oversight</strong> : We can improve recursive alignment schemes like IDA by having each step include checking the system is actually aligned. Note: This overlaps a lot with 7. To me, the distinction is that 7 can be also be applied with systems trained non-recursively, eg today&#39;s systems trained with Reinforcement Learning from Human Feedback</i><ul><li> Yes, it&#39;s an improvement, but it&#39;s naive to think that the only problem with RLHF is just the issue of lack of transparency or deception. For example, we would still have agentic models (because agency is preferred by human preferences) and interpretability alone won&#39;t fix that. See the <a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>Compendium of problems with RLHF</u></a> and <a href="https://www.lesswrong.com/posts/LqRD7sNcpkA9cmXLv/open-problems-and-fundamental-limitations-of-rlhf">Open Problems and Fundamental Limitations of RLHF</a> for more details.</li><li><strong>概念上的进步更为紧迫。</strong> What does &#39;checking the system is actually aligned&#39; really means? <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit#bookmark=id.wqr2jvmzsg7c"><u>It&#39;s not clear at all.</u></a></li></ul></li><li> <i><strong>Interpretability tools in the loss function:</strong> We can directly put an interpretability tool into the training loop to ensure the system is doing things in an aligned way. Ambitious version - the tool is so good that it can&#39;t be Goodharted. Less ambitious - The could be Goodharted, but it&#39;s expensive, and this shifts the inductive biases to favor aligned cognition</i> .<ul><li> <strong>Dual Use,</strong> for obvious reasons, and this one is particularly dangerous.</li><li> <strong>List of lethalities 27. Selecting for undetectability</strong> : “ <i>Optimizing against an interpreted thought optimizes against interpretability.”</i></li></ul></li><li> <i><strong>Norm setting</strong> : If interpretability is easier, there may be expectations that, before a company deploys a system, part of doing due diligence is interpreting the system and checking it does what you want</i><ul><li> <strong>Not the most direct way.</strong> Evals, evals, evals.</li><li> No need to wait for interpretability. We already roughly know what to do. We could conduct studies in line with <a href="https://www.serimats.org/evals"><u>Evaluating Dangerous Capabilities</u></a> and the paper <a href="https://arxiv.org/abs/2305.15324"><u>Model Evaluation for Extreme Risks</u></a> <u>,</u> <a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance">Towards Best Practices in AGI Safety and Governance</a> , this last paper presenting 50 statements about what AGI labs should do, none mentioning interp.</li></ul></li><li> <i><strong>Enabling regulation</strong> : Regulators and policy-makers can create more effective regulations around how aligned AI systems must be if they/the companies can use tools to audit them</i><ul><li> Same critique as <strong>10.</strong> <i><strong>Norm setting</strong></i></li></ul></li><li> <i><strong>Cultural shift 1:</strong> If the field of ML shifts towards having a better understanding of models, this may lead to a better understanding of failure cases and how to avoid them</i><ul><li> <strong>Not the most direct way.</strong> Technical Outreach, communications, interviews or even probably standards and <a href="https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt"><u>Benchmarks</u></a> are way more direct.</li></ul></li><li> <i><strong>Cultural shift 2:</strong> If the field expects better understanding of how models work, it&#39;ll become more glaringly obvious how little we understand right now</i><ul><li> Same critique as <strong>12.</strong> <i><strong>Cultural shift 1.</strong></i></li><li> This is probably the opposite of what is happening now: People are fascinated by interpretability and continue to develop capabilities in large labs. I suspect that the well-known Distill journal has been very fascinating for a lot of people and has probably been a source of fascination for people entering the field of ML, thus accelerating capabilities.</li><li> See the <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#False_sense_of_control_"><u>False sense of control</u></a> section.</li></ul></li><li> <i><strong>Epistemic learned helplessness</strong> : Idk man, do we even need a theory of impact? In what world is &#39;actually understanding how our black box systems work&#39; not helpful?</i><ul><li> I don&#39;t know man, the worlds where we have limited resources, where we are funding constrained + Opportunity costs.</li><li> <strong>Dual Use</strong> , refer to the section &quot; <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful">Interpretability May Be Overall Harmful</a> &quot;.</li></ul></li><li> <i><strong>Microscope AI</strong> : Maybe we can avoid deploying agents at all, by training systems to do complex tasks, then interpreting how they do it and doing it ourselves</i><ul><li> Critique in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI?</u></a> 。</li></ul></li><li> <i><strong>Training AIs to interpret other AIs</strong> : Even if interpretability is really hard/labor intensive on advanced systems, if we can create aligned AIs near human level, we can give these interpretability tools and use them to interpret more powerful systems</i><ul><li> <strong>Object level:</strong> Training AI to interpret other AI, could be useful but would be already dangerous, and we are already in classes of scenarios that are super <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>dangerous</u></a> .</li><li> <strong>Meta level:</strong> This scheme is very speculative. I do not want the survival of civilization to rely on it. <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"><u>Godzilla strategy</u></a> is probably not a good strategy (though this is controversial).</li></ul></li><li> <i><strong>Forecasting discontinuities</strong> : By understanding what&#39;s going on, we can predict how likely we are to see discontinuities in alignment/capabilities, and potentially detect a discontinuity while training/before deploying a system</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Intervening on training</strong> : By interpreting a system during training, we can notice misalignment early on, potentially before it&#39;s good enough for strategies to avoid our notice such as deceptive alignment, gradient hacking, obfuscating its thoughts, etc.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Auditing a training run</strong> : By checking for misalignment early in training, we can stop training systems that seem misaligned. This gives us many more shots to make an aligned system without spending large amounts of capital, and eg allows us to try multiple different schemes, initialisations, etc. This essentially shifts the distribution of systems towards alignment.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Eliciting Latent Knowledges:</strong> Use the length of the shortest interpretability explanation of behaviors of the model as a training loss for ELK - the idea is that models with shorter explanations are less likely to include human simulations / you can tell if they do. (credit to Tao Lin for this one)</i><ul><li> Same critique as <strong>9.</strong> <i><strong>Interpretability tools in the loss function.</strong></i></li><li> Same critique as <strong>15. Microscope AI</strong> .</li><li> Same critique as <strong>16.</strong> <i><strong>Training AIs to interpret other AIs.</strong></i></li></ul></li></ol><h2> Case study of some cool interp papers</h2><p> This section is more technical.</p><p> Stephen Casper <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3#Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>lists</u></a> a bunch of impressive interpretability papers, as of February 2023. Let&#39;s try to investigate whether these papers could be used in the future to reduce risks. For each article, I mention the corresponding end story, and the critic of this end story applies to the article.</p><h3> Bau et al. (2018)</h3><p> <a href="https://arxiv.org/abs/1811.10597"><u>Bau et al. (2018)</u></a> : Reverse engineer and repurpose a GAN for controllable image generation.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://www.youtube.com/watch?v=yVCgUYe4JTM"><u>video</u></a> ) We generate images of churches using a GAN. There are often trees in the generated images. We manually surround the trees, then find the units in the GAN that are mostly responsible for generating these image regions. After finding these regions, we perform an ablation of these units, and it turns out that the trees disappear.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a></li><li> <strong>Useful for outer alignment?</strong> Ideally, we could 1. Find features which are undesirable 2. Then remove parts of the network that are most linked to these features. This is a very limited form of alignment procedure, by ablation.<ul><li> Maybe we could use this kind of procedure to filter pornography, but why then train the network on pornographic images in the first place?</li><li> Basically, this is the same strategy as enumerative safety which is criticized above.</li></ul></li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, because by definition, deception will not result in a difference in outputs, so we cannot apply this procedure.</li></ul><h3> Ghorbani et al. (2020)</h3><p> <a href="https://arxiv.org/abs/2002.09815"><u>Ghorbani et al. (2020)</u></a> : Identify and successfully ablate neurons responsible for biases and adversarial vulnerabilities.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://slideslive.com/38936399/neuron-shapley-discovering-the-responsible-neurons"><u>video</u></a> ) It calculates the Shapley score of different units of a CNN and then removes the units with the highest Shapley value to maximize or minimize a metric. Removing certain units seems to make the network more robust to certain adversarial attacks.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a> (and <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>Reverse engineering</u></a> ?)</li><li> <strong>Useful for outer alignment?</strong> What would have happened if we had just added black women to the dataset? We can simply use a generative model for that and generate lots of images of black women. I&#39;m almost certain that the technique used by OpenAI to remove biases in <a href="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2"><u>Dalle-2</u></a> , does not rely on interp.</li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, again because the first step in using Shapley value and this interpretability method is to find a behavioral difference, and we need first to create a metric of deception, which does not exist currently. So again we first need to find first a behavioral difference and some evidence of deception.</li></ul><h3> Burns et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2212.03827"><u>Burns et al. (2022)</u></a> : Identify directions in latent space that were predictive of a language model saying false things.</p><ul><li> <strong>Procedure:</strong> compare the probability of the &#39;Yes&#39; token with the probability probed from the world model.</li><li> <strong>End story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI</u></a></li><li> <strong>Useful for inner alignment?</strong><ul><li> Extracting knowledge from near GPT-3 level AIs, mostly trained through self-supervised learning via next token prediction, is a <a href="https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4">misunderstanding</a> .</li><li> <strong>This technique requires a minimum of agency and is not just usable as an oracle.</strong><ul><li> <strong>Chain-of-thought will probably always be better.</strong> Currently, this technique barely performs better than next token prediction. Chain-of-thought performs much better, and it seems we have (obvious) <a href="https://twitter.com/BogdanIonutCir2/status/1664974522791895040"><u>theoretical reason</u></a> to think so. So using GPTs as just an oracle won&#39;t be competitive. This paper doesn&#39;t test the trivial baseline of just fine-tuning the model (which has been found to usually work better).</li><li> <strong>Agency is probably required.</strong> It seems unlikely that it will synthesize knowledge on its own in a world model during next-token prediction training. Making tests in the world, or reasoning in an open-ended way, is probably necessary to synthesize a proper truth feature in the world model in advanced GPT using continual learning.</li></ul></li><li> <strong>Conclusion:</strong> Yes, maybe in the future, if we create autonomous agents that conduct experiments and have their own world model, this kind of technique could probably be spot a mismatch between the world model oracle and what the model tells you. But if that were the case, we would probably already be in a very, very dangerous world. Civilization is not ready for this, and I still think that this method will be very brittle, and I prefer to aim for worlds where deception is unlikely. [section: ​​<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ]</li></ul></li></ul><h3> Casper et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2211.10024"><u>Casper et al. (2022)</u></a> : Identify hundreds of interpretable copy/paste attacks.</p><ul><li> <strong>Procedure:</strong> We try to find adversarial attacks automatically. For each image x, we optimize the latent space of a GAN to complete a patch in the image, to optimize the probability of classification towards class y2. We do this for each possible (x, y2) and we observe what appears in the patch. Then we search for natural patches that have a similar embedding to the synthetic adversarial patch, and if the similarity is high enough, we test the natural patch and see if this natural patch breaks the classifier.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Useful for inner alignment?</strong> Conceptually yes, but this is so brittle. And if the trigger mechanism for deceptive alignment is not generable by the GAN or is not in the distribution of natural images/text, we won&#39;t find it.</li></ul><h3> Ziegler et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2205.01663"><u>Ziegler et al. (2022)</u></a> : Debug a model well enough to greatly reduce its rate of misclassification in a high-stakes type of setting.</p><ul><li> <strong>Procedure</strong> : The project consists in building a highly reliable injury classifier: <i>“We started with a baseline classifier trained on some mildly injury-enriched (but otherwise mostly random) data. Then, over the course of several months, we tried various techniques to make it more reliable”.</i> The aim here is to minimize the false negative rate, striving to make the classifier as reliable as possible.: we do not want to let any problematic sentence or example go unnoticed. In order to find adversarial examples, they experimented with the following techniques:<ul><li> 1. unaugmented humans: humans manually search for problematic sentences,</li><li> 2. automatic paraphrasing: use of LLM to find variations of problematic sentences and progressively enrich the dataset.</li><li> and 3. humans augmented with a rewriting tool. It is mainly this last technique that was useful.</li></ul></li><li> <strong>End Story:</strong> Adversarial training and maybe <strong>&nbsp;</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Robustness results:</strong><ul><li> <strong>In-distribution:</strong> They started with a classifier that had a 2.5% false negatives, and ended up with a classifier that had 0.003% false negatives! They are almost reaching the limits of detection for this type of dataset.</li><li> <strong>Out-of-distribution:</strong> The tool-assisted attack to generate an adversarial example now takes 26 minutes per example, which is only twice as long as the previous time of 13 minutes. This falls short of the paper&#39;s aim to gain many orders of magnitude in speedup. Unfortunately, even though the original dataset (in-distribution) is now almost completely well classified, it seems that it is still possible to easily create adversarial examples with automated tools, and we have only improved the defense by a factor of 2. But only a factor of 2 falls short of the aims of this project.</li><li> <strong>Conclusion:</strong> Thus, improving a lot in-distribution performance by several orders of magnitude seems to not have much impact on out-of-distribution. Even though Redwood <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>said</u></a> that this project could have been conducted better, this is currently rather a negative result to improve the adversarial robustness (ie out-of-distribution Robustness). In addition, we can link these results to the paper <a href="https://arxiv.org/abs/2211.00241"><u>Adversarial Policies Beat Superhuman Go AIs, Wang et al.. 2022</u></a> , which studies adversarial attacks on the Katago AI, which is superhuman in the game of Go. They show that it is probably possible to find simple adversarial strategies even against very superhuman AIs. And as a consequence, it seems that even for very robust and powerful AIs, It may always be possible to find adversarial attacks.</li></ul></li><li> <strong>How to use this for deception?</strong>不清楚。 Do we watch what the AI does, tell it <i>&#39;you are the lord of the universe, you can do whatever you want&#39;</i> or more subtle variations of this prompt and then see what happens? This is not really interpretability, it&#39;s much more prompt engineering with <a href="https://arxiv.org/abs/2306.09983"><u>consistency checks</u></a> or cyborgism than interp. (Beside, the interpretability technique that they used is quite underwhelming, only coloring tokens with the bigger gradient of the norm of the embedding of the token, and then basically iterating on synonymous of those important tokens). </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2stfurwyyg6"> <span class="footnote-back-link"><sup><strong><a href="#fnref2stfurwyyg6">^</a></strong></sup></span><div class="footnote-content"><p> Is feature visualization useful? Some findings suggest no: <a href="https://arxiv.org/abs/2302.10894"><u>Red Teaming Deep Neural Networks with Feature Synthesis Tools</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qi9kn3ip89"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qi9kn3ip89">^</a></strong></sup></span><div class="footnote-content"><p> GradCam: Maybe this <a href="https://www.notion.so/Against-Almost-every-Theories-of-Change-of-Interpretability-61ebd2937cab4e12b9eb777454b7ed29?pvs%3D21"><u>paper</u></a> ? But this is still academic work.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6xxwjs20rd7"> <span class="footnote-back-link"><sup><strong><a href="#fnref6xxwjs20rd7">^</a></strong></sup></span><div class="footnote-content"><p> I have organized <a href="https://www.lesswrong.com/posts/WF5JpmpK8EM4xKyve/new-hackathon-robustness-to-distribution-changes-and"><u>two</u></a> <a href="https://github.com/EffiSciencesResearch/hackathon42"><u>hackathons</u></a> centered around the topic of spurious correlations. I strongly nudged using interp, but unfortunately, nobody used it...Yes this claim is a bit weak, but still indicates a real phenomenon, see [section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_tools_lack_widespread_use_by_practitioners_in_real_applications_"><u>Lack of real applications</u></a> ]</p></div></li><li class="footnote-item" role="doc-endnote" id="fnztj4j3pmerg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefztj4j3pmerg">^</a></strong></sup></span><div class="footnote-content"><p> Note: I am not making any claims about ex-ante interp (also known as <a href="https://arxiv.org/abs/2207.13243"><u>intrinsic interp</u></a> ), which has not been so far able to predict the future system either.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2464ho15s7t"> <span class="footnote-back-link"><sup><strong><a href="#fnref2464ho15s7t">^</a></strong></sup></span><div class="footnote-content"><p> Other weaker difficulties for auditing deception with interp: <strong>This is already too risky and Prevention is better than cure. 1) Moloch may still kill us:</strong> <i>&quot;auditing a trained model&quot; does not have a great story for wins. Like, either you find that the model is fine (in which case it would have been fine if you skipped the auditing) or you find that the model will kill you (in which case you don&#39;t deploy your AI system, and someone else destroys the world instead)</i> . […] <i>a capable lab would accidentally destroy the world because they would be trying the same approach but either not have those interpretability tools or not be careful enough to use them to check their trained model as well?”</i> [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DpHt4w9SN5PLLTzZuB"><u>Source</u></a> Rohin Shah]. <strong>2) We probably won&#39;t be competent enough to fix our mistake:</strong> “ <i>in order for auditing the model to help (directly), you have to actually be pretty confident in your ability to understand and fix your mistakes if you find one. It&#39;s not like getting a coin to land Heads by flipping it again if it lands Tails - different AGI projects are not independent random variables, if you don&#39;t get good results the first time you won&#39;t get good results the next time unless you understand what happened. This means that auditing trained models isn&#39;t really appropriate for the middle of the skill curve</i> .” [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3D8Ckqr87YsoMprivEW"><u>Source</u></a> Charlie Steiner].</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrbnra8fsacj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrbnra8fsacj">^</a></strong></sup></span><div class="footnote-content"><p> From “ <a href="https://www.lesswrong.com/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"><u>Conditioning Generative Models</u></a> . “Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs. <strong>Is there a way we could use different conditionals to get at whether or not Alice was deceptive</strong> (eg prompt the model with “DeepMind develops perfect transparency tools and provides an opportunity for deceptive models to come clean and receive a prize before they&#39;re discovered.”)?”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmf7vlk6ib69"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmf7vlk6ib69">^</a></strong></sup></span><div class="footnote-content"><p> For example, what happens If you prompt a deceptive model with a Joyous prompt, and prompt the same deceptive model with a sad prompt and then take the difference, you obtain a Joyous Deceptive model?</p></div></li><li class="footnote-item" role="doc-endnote" id="fnc2q5uxqhj6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc2q5uxqhj6j">^</a></strong></sup></span><div class="footnote-content"><p> But at the same time, we could be pessimistic, because this good idea has been out there in the wild since Christiano <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>described</u></a> it in 2019. So either this idea does not work and we have not heard about it. Or the community has failed to recognize a pretty simple good idea.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpo4e41md3r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpo4e41md3r">^</a></strong></sup></span><div class="footnote-content"><p> Causal scrubbing could be a good way for evaluating interp techniques using something other than intuition. However, this is only suitable for localization assessment and does not measure how understandable the system is for humans.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnplu4ji16iui"> <span class="footnote-back-link"><sup><strong><a href="#fnrefplu4ji16iui">^</a></strong></sup></span><div class="footnote-content"><p> “ <i>I was previously pretty dubious about interpretability results leading to capabilities advances. I&#39;ve only really seen</i> <a href="https://arxiv.org/pdf/2212.14052.pdf"><i><u>two</u></i></a> <i>&nbsp;</i> <a href="https://arxiv.org/pdf/2302.10866.pdf"><i><u>papers</u></i></a> <i>which did this for LMs and they came from the same lab in the past few months. It seemed to me like most of the advances in modern ML (other than scale) came from people tinkering with architectures and seeing which modifications increased performance. But in a conversation with Oliver Habryka and others, it was brought up that as AI models are getting larger and more expensive, this tinkering will get more difficult and expensive. This might cause researchers to look for additional places for capabilities insights, and one of the obvious places to find such insights might be interpretability research.</i> ” from <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous?commentId=GYo8WegFmfxWmB5Z3"><u>Peter barnett</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw7s6gsvuwb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw7s6gsvuwb">^</a></strong></sup></span><div class="footnote-content"><p> Not quite! Hypotheses 4 (and 2?) are missing. Thanks to Diego Dorn for presenting this fun concept to me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnavln8kyvlzg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefavln8kyvlzg">^</a></strong></sup></span><div class="footnote-content"><p> This excludes the governance hackathon, though, this is only from the technical ones. Source: Esben Kran.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1<guid ispermalink="false"> LNA8mubrByG7SFacm</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Thu, 17 Aug 2023 18:44:41 GMT</pubDate> </item><item><title><![CDATA[Announcing Foresight Institute's AI Safety Grants Program]]></title><description><![CDATA[Published on August 17, 2023 5:34 PM GMT<br/><br/><p> Foresight Institute 已获得资金支持我们认为可能尚未充分探索的三个人工智能安全领域的项目：<br><br> 1. 神经技术、脑机接口、全脑仿真和“低保真”上传方法，以产生符合人类需求的软件智能<br><br>2. 帮助保护人工智能系统的计算机安全、密码学和相关技术<br><br>3. 多主体模拟、博弈论和相关技术，创建安全的多极人工智能场景，避免共谋并促进正和动态<br><br>赠款申请流程现已开放并接受滚动申请。我们预计每年为各个项目拨款 1 - 120 万美元，并期待收到您的项目提案，这些项目提案可以在短时间内对人工智能安全产生重大影响。<br><br>请访问<a href="https://foresight.org/ai-safety/"><u>https://foresight.org/ai-safety/</u></a>了解完整详细信息和应用说明。</p><p>如果您考虑与可能从申请中受益的其他人分享这个机会，我将不胜感激。</p><p>如果您有任何问题、反馈或合作想法，请随时在此处发表评论或发送电子邮件至<a href="mailto:allison@foresight.org">allison@foresight.org</a> 。</p><br/><br/> <a href="https://www.lesswrong.com/posts/sswXWiB4dBpChuLsm/announcing-foresight-institute-s-ai-safety-grants-program-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sswXWiB4dBpChuLsm/announcing-foresight-institute-s-ai-safety-grants-program-2<guid ispermalink="false"> sswXWiB4dBpChuLsm</guid><dc:creator><![CDATA[Allison Duettmann]]></dc:creator><pubDate> Thu, 17 Aug 2023 17:34:59 GMT</pubDate> </item><item><title><![CDATA[The Negentropy Cliff]]></title><description><![CDATA[Published on August 17, 2023 5:08 PM GMT<br/><br/><p> Something that is not often discussed explicitly and factors into the different intuitions people have about P(Doom) is how close to optimal biology and humans are in terms of <a href="https://en.wikipedia.org/wiki/Entropy_and_life">harnessing negative entropy</a> . This consideration pertains to equally to nanobots, ASI and artifical life in general.</p><p> Let&#39;s consider grey goo first: The race to turn all resources into copies of yourself has been going for a few billion years and is quite competitive. In order to supplant organic life, nanobots would have to either surpass it in carnot efficiency or (more likely) utilise a source of negative entropy thus far untapped. Examples of this previously happening are:</p><ul><li> Photosynthesis</li><li> Aerobic respiration</li><li> Control of fire by early humans</li><li> Agricultural revolution</li><li> Industrial revolution</li></ul><p> If, in the designspace of replicators, we are in a local (metastable) optimum and the ability to consume negative entropy, falls off a cliff in a place that is reachable by the synthetic but not organic life, we will get outcompeted quickly. So, are we stumbling in the dark, next to a civilisation swallowing precipice? Would the ASI need to discover new physics or are there already examples of negentropy sources that it could use better than biology?</p><br/><br/><a href="https://www.lesswrong.com/posts/guuu5JGz87fJYJpKr/the-negentropy-cliff#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/guuu5JGz87fJYJpKr/the-negentropy-cliff<guid ispermalink="false"> guuu5JGz87fJYJpKr</guid><dc:creator><![CDATA[mephistopheles]]></dc:creator><pubDate> Thu, 17 Aug 2023 20:29:40 GMT</pubDate></item></channel></rss>