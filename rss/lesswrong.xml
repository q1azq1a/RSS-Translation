<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 8 日星期五 10:12:51 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What I Would Do If I Were Working On AI Governance]]></title><description><![CDATA[Published on December 8, 2023 6:43 AM GMT<br/><br/><p>我不从事人工智能治理工作，将来也不太可能从事人工智能治理工作。但各种轶事，尤其是<a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk"><u>阿卡什最近的讨论</u></a>给我留下的印象是，很少有人（如果有的话）在做我认为合理的出发点的事情，相反，大多数人大多在做似乎不合理的事情。 -me 解决有用的人工智能治理的任何重要瓶颈。</p><p>因此，这篇文章列出了如果我从事人工智能治理工作，我会从哪些地方开始，以及它们背后的一些原因。</p><p>毫无疑问，我错过了很多重要的事情！也许这篇文章对其他从事人工智能治理工作的人来说仍然有用，也许<a href="https://en.wikipedia.org/wiki/m:Cunningham%27s_Law#:~:text=Cunningham's%20Law%20states%20%22the%20best,to%20post%20the%20wrong%20answer.%22"><u>坎宁安定律</u></a>会让我从这篇文章中学到有用的东西，也许两者兼而有之。我预计这篇文章中的具体建议比其背后的推理风格更有可能存在缺陷，因此我建议更多地关注推理而不是具体建议。</p><p>这篇文章将主要关注美国，因为这是我最了解的地方，也是所有主要人工智能公司所在的地方，但想必所讨论的干预措施的版本也可能适用于其他政体。</p><h2>责任</h2><p>我关注的一个主要领域是让构建人工智能的公司对人工智能造成的损害承担责任，无论是事实上的还是法律上的。</p><h3>为什么要承担责任？</h3><p>这里的模糊目标是让构建人工智能的公司：</p><ul><li>从一开始就设计出<i>非常稳健</i>且不会引起问题的系统。</li><li>在红队中投入资源，在新的故障模式投入生产<i>之前</i>发现它们等。</li><li>实际上<i>，不要部署会</i>引发危险信号的系统，即使公司已投入巨资构建这些系统。</li><li>一般来说，要表现得好像公司会因人工智能造成的损害而蒙受损失，而不仅仅是从人工智能带来的好处中获取利润。</li></ul><p> ……实现这一目标的一种自然方法是确保公司实际上从人工智能造成的损害中<i>承担</i>损失，而不仅仅是从人工智能带来的好处中获取利润。简而言之，这就是责任。</p><p>现在，实际上，这不会一直延伸到例如<a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai"><u>让公司购买灭绝保险</u></a>。那么，为什么现实的责任水平对于灭绝风险很重要呢？因为它们会激励公司针对任何实际牙齿实施安全流程。</p><p>例如：现在，很多人正在从事安全评估等工作。我非常强烈的期望是，如果这些评估抛出危险信号，主要实验室将通过以下方式做出回应：（1）召开一些会议，人们在会上讨论安全问题，（2）进行微调，直到危险信号消失。不再抛出（以一种显然不会有力地消除潜在问题的方式），然后（3）在谷歌/微软/亚马逊和/或萨姆·奥尔特曼的首席执行官的巨大压力下无论如何部署它。 （特别是，这里最强烈的预测是这些模型无论如何都会以某种方式最终部署。）</p><p>另一方面，如果一家人工智能公司已经因其人工智能引起的问题而遭受了许多昂贵的诉讼，那么我预计他们最终会采取一种以各种方式测试新模型的流程，然后<i>实际上不会部署它们</i>如果出现危险信号。他们已经做过几次“微调直到红灯停止闪烁”的事情，并在他们的微调未能真正消除部署中的问题时付出了代价。</p><p>另一种说法是：责任迫使公司处理<a href="https://www.lesswrong.com/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard#Some_reasons_this_is_hard"><u>组织问题</u></a>，这些问题是使任何类型的人工智能安全治理基本真实而不是基本虚假的核心瓶颈。它强制安全机制所需的组织基础设施/流程。</p><p>有关责任如何解决另一个领域的类似问题的精彩案例研究，请查看 Jason Crawford 的<a href="https://www.lesswrong.com/posts/DQKgYhEYP86PLW7tZ/how-factories-were-made-safe"><u>《工厂如何变得安全》</u></a> 。正是这篇文章最初将这种策略纳入我的人工智能雷达范围内。</p><h3>如何承担责任？</h3><p>现在谈谈我为了实现这个模糊的愿景而每天要做的事情。</p><p>总的来说，我看到了三种路径：</p><ul><li>司法路径：确立人工智能企业损害赔偿责任的判例</li><li>监管路径：让维持责任相关规则​​的监管机构做出规则澄清，甚至制定新规则，明确人工智能公司对各种损害承担责任</li><li>立法路径：让州和/或联邦立法者通过法律，明确规定人工智能公司对各种损害承担责任</li></ul><p>为了确保事实上的责任，所有这些途径还应该与一个组织相结合，该组织积极寻找对人工智能公司提出索赔的人，并提供律师来追究这些索赔。</p><p>鉴于任何一条路径都需要一个组织主动寻找提出索赔的人，并提供律师来追究这些索赔，那么司法路径是显而易见的选择，因为它需要完全相同的基础设施。唯一的调整是该组织还积极寻找良好的测试案例和友好的司法管辖区来起诉他们。</p><p>根据可用资源，人们还可以通过寻找一家擅长此类事务的现有律师事务所来追求这种策略，并简单地补贴他们的案件以换取对人工智能责任的关注。</p><p>我（低信心）期望相对较早地追求的案例如下：</p><ul><li>找到一大群因幻觉而受到某种程度损害的人，并对构建大型语言模型的公司提起集体诉讼。</li><li>找到一些成为大量深度造假事件对象的名人或政客，并对模型制造了一堆深度造假的公司提起诉讼。</li><li>找到一些因员工/承包商使用大型语言模型伪造报告、文章等而遭受严重损害的公司/组织，然后起诉其模型生成这些报告/文章等的公司。</li></ul><p>这份清单中靠后的项目（我猜）将为人工智能公司建立更严格的责任标准，因为它们涉及明显“滥用”该模型的其他方（他们可以代替人工智能公司承担大部分责任）。如果成功，此类案例将成为有用的先例，激励人工智能公司采取更实质性的防范措施来防止滥用，因为人工智能公司将不太能够将责任推卸给“滥用者”。 （根据上一节，这并不是那种立即对灭绝风险产生影响的事情，而是旨在迫使公司采取任何实质性的防护措施。）</p><p>请注意，这里的一个主要挑战是，人们可能会直接与大型科技公司的律师对抗。也就是说，我预计有些律师事务所的存在理由是在法庭上与大公司对抗，而很多法官不喜欢大型科技公司。</p><h2>监管调查和小规模干预</h2><p>目前大概有数十个联邦机构致力于人工智能规则的制定。对我来说显而易见的一件事是通读进入联邦公报的所有规则提案和公众意见征询期，找到任何与人工智能（或 GPU/云计算）相关的内容，然后简单地提交对它们的评论，主张拟议规则的任何版本都将最能减轻人工智能灭绝的风险。</p><p>这里的一些目标可能是：</p><ul><li>推动围绕人工智能制定具有实际意义的规则版本，即它们<i>不仅仅是</i>一些文书工作。</li><li>推动专门针对相对通用的人工智能的规则版本，尤其是新的 SOTA 训练运行。</li><li>总体上让 GPU 更加稀缺（这个相对更困难，也相对更“反合作”；我不确定我是否要优先考虑这类事情）。</li></ul><p> ......但大多数情况下，我目前不知道正在制定什么样的规则，所以我不知道我会追求什么具体目标。这里的一个重要部分就是要了解正在发生的事情，获取大量数据，然后在很多地方进行小规模的推动。</p><p>请注意，大型语言模型本身可以为此类项目提供很多帮助 - 例如，阅读联邦公报的（非常长的）每日发布以识别相关项目。</p><p>就我想参与典型的游说活动而言，例如与监管机构进行更深入的讨论，这也将是我的第一步。通过对正在发生的一切有一个广泛的了解，我能够找到可能影响最大的新规则来关注。</p><p>在实施方面，我绝对不会向规则制定者推销人工智能 X 风险（至少不是作为该战略的一部分）。相反，我会关注：</p><ul><li>深刻理解拟议的规则和规则制定者自己的目标</li><li>“解决平衡”他们创造的激励措施（特别是寻找公司可能利用的漏洞）</li><li>建议实施细节，例如堵住漏洞或以其他方式调整激励措施，既推进 X 风险降低目标又符合规则制定者自己的目标</li></ul><p>我要留意的另一件事是敌对活动，即谷歌、OpenAI/微软、亚马逊、Facebook 等的游说者试图淡化规则。</p><p>旁白：我在<a href="https://www.lesswrong.com/posts/MMAK6eeMCH3JGuqeZ/everything-i-need-to-know-about-takeoff-speeds-i-learned?commentId=PLGphwKbizzWS692g"><u>2022 年空调大辩论</u></a>中注意到的一件事是，如果读者真正关注并去寻找监管机构的废话，那么根本不难看出发生了什么。监管机构在征求意见期间的报告或多或少直接表示，一些单管空调制造商声称，最初提出的能效报告规则将扼杀他们的销售。回应：</p><blockquote><p>然而，正如本最终规则第 III.C.2 节、第 III.C.3 节和第 III.H 节中进一步讨论的那样，2015 年 11 月 SNOPR 中提出的评级条件和 SACC 计算减轻了 De&#39; Longhi 的担忧。美国能源部认识到，渗透对便携式空调性能的影响取决于测试条件，因此，更极端的室外测试条件（即升高的温度和湿度）会强调任何与渗透相关的性能差异。 2015 年 11 月 SNOPR 中提出并在本最终规则中采用的评级条件和权重因子（参见本最终规则第 III.C.2.a 节和第 III.C.3 节）代表了比2015 年 2 月 NOPR。因此，渗透空气传热对所有便携式空调配置的性能影响不太严重。考虑到自 2015 年 2 月 NOPR 31 以来测试条件和性能计算的变化以及本最终规则中建立的测试程序，DOE 预计单管道便携式空调性能受渗透空气的影响要小得多。</p></blockquote><p>换句话说，监管机构的报告本身就很有帮助地强调了他们的新公式中的废话：对假设的室外条件进行了一些改变。我看了一下，确实是扯淡的地方：修改后的能效标准使用了两种测试条件的加权组合，其中80%的权重是在室外空气温度仅高3°F/1.6°C的条件下进行的与室内空气相比，单管空调的效率（相对于两管空调）似乎比使用便携式空调的更现实的条件要低得多。</p><p>回到主线：重点是，只要该案例研究具有代表性，实际上并不难找到废话在哪里。监管机构希望他们的受益人能够发现这些废话，因此监管机构会自己强调这些废话。</p><p>但这是一把两刃剑。我可以想象的一个项目是找到人工智能公司的游说者成功游说一些废话的地方，然后简单地写一篇公开帖子，向外行人强调这些废话。我很容易想象这样的帖子会引起监管机构强烈反对的轻微公众反弹，从而激励监管机构未来制定更少的废话人工智能规则。 （不幸的是，这也会激励监管机构在未来变得更加微妙，但我们在这里并不是直接谈论调整人类水平的人工智能，所以这似乎是一个可以接受的权衡。）</p><h2>保密机构</h2><p>我不会在本节中讨论太多细节，主要是因为我不太了解相关细节。也就是说，它确实看起来像：</p><ul><li>情报机构（以及与情报相关的机构，例如确保核机密保密的人员）在防止构建危险人工智能所需的操作能力方面拥有独特的装备精良</li><li>情报机构（以及与情报相关的机构）保守着大量秘密，而人工智能使这些秘密更容易被发现。</li></ul><p>我多次听说互联网上有大量名义上的机密信息，但查找和整合它们并不是特别容易。大型语言模型似乎是一个非常理想的工具。</p><p>因此，这里似乎存在一些协调一致的激励措施，可能所需要的只是有人向情报机构明确表明他们需要参与面向公众的人工智能的安全。</p><p>我不知道这里有多少潜力，也不知道如何最好地挖掘它。如果我正在进行这样的项目，第一步将是学习很多东西并与了解相关机构如何运作的人交谈。</p><h2>雄心勃勃的立法</h2><p>我不想立即解决雄心勃勃的立法项目，至少在监管调查之前不会这样做；它是最复杂的治理项目。但如果我要走这条路，以下是我预计会遇到的瓶颈以及我将如何解决它们。</p><h3>公众舆论（及其看法）</h3><p>就一般立法而言，我将公众舆论视为一种货币形式。只要一个人的立法议程与科技公司游说者的优先事项直接冲突，或者需要大量名义上非常重要的人物关注（例如创建全新的机构），人们就需要大量的货币来花费。</p><p>总的来说，公众似乎确实非常害怕 AGI。显然，诸如就业、种族主义/觉醒等典型问题仍然很突出，但即使是直接的 X 风险，一旦问题在全部。<a href="https://www.lesswrong.com/s/TLSzP4xP42PPBctgw/p/DwqgLXn5qYC7GqExF"><u>这不会导致东京的房地产价值上升，这</u></a>似乎是一种相当普遍的直觉。</p><p>在这个领域，我认为 Conjecture 的人们的行为基本上是正确的，我的目标/推理可能与他们的类似。</p><p>这里要强调的一件事是，就这种“货币”而言，公众舆论的看法至少与公众舆论本身一样重要。我们希望政策制定者<i>知道</i>，总体而言，中位选民非常害怕 AGI。因此，诸如调查之类的事情是有价值的，而不仅仅是直接的公众宣传。</p><h3>立法设计</h3><p>一旦有足够的公众舆论使雄心勃勃的立法项目变得可行，下一步就是起草真正能实现我们想要的目标的立法。</p><p> （有一个中间步骤是弄清楚要实现哪些模糊目标 - 例如许可、暂停等 - 但我不认为该步骤会成为主要瓶颈，无论如何，人们应该主要花时间在立法草案上，然后更新如果一路上发现障碍，就设定高级别目标。）</p><p>这一步是大部分认知工作发生的地方。这是一个棘手的设计问题：</p><ul><li>弄清楚联邦政府中谁需要做什么</li><li>什么立法语言会促使他们这样做</li><li>思考由此产生的激励下监管机构、公司和研究人员的均衡行为</li><li>迭代所有这些</li></ul><p>这显然需要非常详细地了解现有机构中谁做什么、新机构如何成立和运作、谁负责所有这些等等。需要弄清楚谁实际上做什么。</p><p>我预计这一步是目前的主要瓶颈，也是大部分价值所在。</p><h3>卖掉它</h3><p>下一步将是将这项立法提交给那些能够使其成为法律的人。我预计随着公众舆论的增加，这一步会变得更加容易。由于来自中间选民的足够压力，国会办公室将积极寻找提案，任何持有实际立法草案的人都会受到很大的欢迎。另一方面，如果舆论压力相对较小，那么就需要更多地依赖“游戏内部”。如果我走这条路，我肯定会相对严重地依赖公众舆论，而不是游戏内部，但我会与 2-3 名拥有游戏内部知识的人签约，并独立咨询他们，以确保我不没有遗漏任何重要的东西。</p><h2>结论性思考</h2><p>正如一开始提到的，这里可能有一些我完全错误的东西。但它希望能让我了解我会使用的方法和思维模型。特别要注意：</p><ul><li>关注瓶颈，而不仅仅是边际影响。</li><li>关注激励下的实际效果和均衡行为，而不仅仅是象征性规则</li><li>重点是弄清楚创建和执行规则的流程细节，即哪些特定的人做哪些特定的事情。</li></ul><p>最后一点：这篇文章没有讨论我<i>不会</i>向哪些治理项目分配精力，或者为什么。如果您对帖子忽略的特定项目或项目类别感到好奇，请随时发表评论。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance<guid ispermalink="false"> qCstMcRJcjNtdNW7r</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Fri, 08 Dec 2023 06:43:45 GMT</pubDate> </item><item><title><![CDATA[You're optimized for IGF (Source: trust me bro)]]></title><description><![CDATA[Published on December 8, 2023 5:32 AM GMT<br/><br/><p>考虑以下（ <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_2___Central_difficulties_of_outer_and_inner_alignment_">转述的</a>）句子：</p><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——一种非常精确、非常简单的损失函数——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>这句话在三个方面是错误的。</p><h2>我。</h2><blockquote><p>尽管事实上<strong>进化严格地优化了生物体的包容性遗传适应性</strong>——一种非常精确、非常简单的损失函数——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>不，没有，哈哈。</p><p>什么样<em>的基因变化</em>使得第一代真核生物能够在竞争中击败它们的原核生物表亲？十亿多年前的<em>基因变化</em>怎么样，它催生了我今天早上吃的小麦的祖先——蓝藻通过<em>基因变化</em>滑过细胞膜，与其宿主进入共生状态，从而产生了“叶绿体”?</p><p>进化并没有因为某些幸运的“真核基因”而赋予真核生物成功。真核生物获胜是因为真核生物将共生体合并到自身中。<em>真核生物</em>的优势表明，进化对基因组<em>之外的</em>有机体配置模式施加了强大的选择压力。这次演示发生在人类出现之前十亿多年，因此与“智慧”无关。</p><p><a href="https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/">人类也赢了，因为我们将共生体合并到自己体内</a>——但我们的共生体被称为“文化”而不是“线粒体”，而且由于我们没有任何文化等待意外地游入我们的细胞质，所以我们必须培养自己的共生体。随着文化变得更具表现力，模因（而不是基因）往往成为新的适应性模式的首选存储介质。因此，施加在模因上的进化压力的比例上升了，而施加在基因上的进化压力的比例则下降了。</p><p>进化不仅仅选择遗传适应性。进化选择可复制的模式。如果这些模式恰好都是基因，那么当然，进化最终会选择合适的基因。如果这些模式的某些（按选择相关性加权）部分恰好是通过非遗传方式编码的，这<em>并不</em>意味着进化在某种程度上搞砸了。</p><p>一个人选择成为一名牧师。他向别人的孩子宣扬自己的宗教信仰，而不是培养自己的宗教信仰。这样做时，他并没有<em>浪费</em>自己的模式复制能力——只是将其投资到不同的投资组合中。</p><p>仅当进化适应性是遗传的时，进化才会选择遗传适应性。</p><h2>二.</h2><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——<strong>一种非常精确、非常简单的损失函数</strong>——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>哦耶？那么你曾祖父的 IGF 是多少？给我一个号码。展示你的作品。</p><p> “健康”不可能是“你死前生了多少个孩子”，因为工蚁（通常）是不育的，但大自然创造了很多，如果它们不健康，自然就不会这样做。</p><p>因此， <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">“包容性适应性”</a>是“一个人通过其行为抚养、拯救或以其他方式支持的后代当量数量（无论是谁生的）”。你与某人的血缘关系越深，他们就越被视为“后代等价物”。</p><p>伟大的。</p><p>那么你曾祖父的 IGF 又是多少呢？</p><p>假设我们知道他有多少个孩子。查看。他“拯救或以其他方式支持”了多少人？ （当然，他们和他有什么关系？）如果将来，你的曾曾侄子在阁楼上发现了他的旧日记，并且全神贯注地忘记了执行，该怎么办？他工作的实验室的生物安全协议导致埃博拉艾滋病爆发，导致 43% 的人口死亡？我们还要看多远的未来才能知道他的 IGF 是什么？</p><p>即使我们能够神奇地访问德尔斐神谕，我们<em>仍然</em>需要对反事实分支进行整个宇宙模拟，而你的曾祖父从未在其中受孕。我肯定会将该计算描述为“非常简单”。我告诉你，有了这些新的 Apple Silicon 芯片，我很可能在一两个小时内就能在我的笔记本电脑上运行它。简单的。</p><p>任何关于系统“优化生物体以实现包容性遗传适应性”的说法实际上都是不可证伪的，因为在计算上不可能<em>找到</em>包容性遗传适应性，因为你需要对宇宙进行两次完美的模拟。</p><p> （一旦你使用了“近似”这个词，你就失去了“精确”和“严格”这两个词的特权，并且你有忽略尾部事件的风险。如果你声称你的意思是“无时间限制的柯尔莫哥洛夫”中的“简单”复杂性”的感觉……祝你好运，清楚地思考与物理世界纠缠在一起的任何函数。）</p><h2>三．</h2><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——一种非常精确、非常简单的损失函数——<strong>但令人惊讶的是，人类并没有明确地追求它！</strong></p></blockquote><p>人类并没有明确追求太多。当我们这样做时，它通常（通常？）是我们<em>实际</em>目标的掩护。</p><br/><br/> <a href="https://www.lesswrong.com/posts/GspoKP6mG9CcvFjCy/you-re-optimized-for-igf-source-trust-me-bro#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GspoKP6mG9CcvFjCy/you-re-optimized-for-igf-source-trust-me-bro<guid ispermalink="false"> GspoKP6mG9CcvFjCy</guid><dc:creator><![CDATA[dkirmani]]></dc:creator><pubDate> Fri, 08 Dec 2023 05:32:04 GMT</pubDate> </item><item><title><![CDATA[Whither Prison Abolition?]]></title><description><![CDATA[Published on December 8, 2023 5:27 AM GMT<br/><br/><p>首先，向 LessWrong 社区道歉，因为该社区充斥着他们不喜欢的内容。我的子堆栈中的这篇文章收到了 LessWrong 成员的点赞和一些积极的评论，这让我认为它会受到积极的欢迎。如果我可以做些什么来改进这篇文章或未来的文章，请告诉我。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/u3kk4ol2pysd8bksfnsv" alt="由 DALL·E 生成"></figure><h1>我和 GPT-4 之间的研讨会</h1><p><strong>艾瑞克</strong>：你能帮我组织一次模拟哲学研讨会吗？我将扮演苏格拉底的角色，我希望你在我们讨论重要问题时帮助我将一组著名的历史人物带入生活。</p><p> <strong>ChatGPT</strong> ：当然，与历史人物组织一场模拟哲学研讨会，而你扮演苏格拉底的角色，听起来像是一种令人着迷且刺激智力的体验。让我们首先概述关键组件：</p><p> 1. <strong>**选择历史人物**</strong> ：首先，我们需要选择您想要“邀请”参加您的研讨会的历史人物。考虑包括不同时代的哲学家、科学家、艺术家和领袖。您对包括哪些人感兴趣？</p><p> 2. <strong>**讨论主题**</strong> : 接下来，我们应该确定讨论的主题。这些范围可以从伦理、政治、科学、艺术、知识的本质，到通过哲学镜头解释的更当代的问题。您热衷于探索哪些主题？</p><p> 3. <strong>**研讨会的形式**</strong> ：形式可以是一系列对话、圆桌讨论或更结构化的辩论。每个人物都可以提出自己的观点，然后由你主持苏格拉底式提问。您希望研讨会如何安排？</p><p> 4. <strong>**准备对话脚本**</strong> ：为了使研讨会更具吸引力，我们可以为每个历史人物准备对话脚本或要点，反映他们已知的信仰和观点。这需要一些研究和创意写作。您需要帮助编写这些脚本吗？</p><p> 5. <strong>**互动元素**</strong> ：如果您有观众，请考虑纳入互动元素，例如观众问题或现场民意调查。这可以使研讨会更具活力和吸引力。</p><p> 6. <strong>**物流**</strong> ：最后，考虑一下物流 - 这是虚拟活动还是现场活动？如果是虚拟的，您将使用哪个平台？您将如何邀请参与者或观众？</p><p>一旦您提供了这些要点的一些细节，尤其是您对历史人物和主题的选择，我们就可以开始认真规划了！<br><br><strong>埃里克</strong>：我们开始吧。苏格拉底会叫出名字；请不要为任何苏格拉底未点名的人物发表演讲。</p><p><strong>场景</strong>：禁酒令期间的地下酒吧</p><p><strong>剧中人：</strong></p><p><strong>苏格拉底</strong>，哲学家</p><p><strong>艾尔·卡彭</strong>，一个黑帮</p><p><strong>奥利弗·温德尔·霍姆斯</strong>，法学家</p><p><strong>罗宾·汉森</strong>，经济学家</p><p><strong>约翰·纳什</strong>，数学家</p><p><strong>DMX</strong> ，说唱歌手</p><p><strong>马尔科姆·X</strong> ，一名囚犯</p><p><strong>格洛丽亚·斯泰纳姆</strong>，女权主义者</p><p><strong>西蒙娜·德·波伏娃</strong>，女权主义者</p><p><strong>哈维·韦恩斯坦</strong>，制片人</p><p><strong>伍迪·艾伦</strong>，电影制片人</p><p><strong>比尔科斯比</strong>，喜剧演员</p><p><strong>ChatGPT</strong> ：太好了，让我们为禁酒令期间在地下酒吧举行的哲学研讨会设置场景并概述结构。如此多样化且有趣的角色组合，这必将成为一次令人难忘且发人深省的活动。</p><p> ###场景设置</p><p><strong>**地点**</strong> ：灯光昏暗的地下酒吧，背景中充满了安静的爵士乐音调。当这些不拘一格的人物聚集在一张大圆桌周围时，空气中充满了紧张和好奇。苏格拉底以他敏锐的目光，准备让大家进行一个深入的哲学探究之夜。</p><p> ### 戏剧人物：</p><p> - **苏格拉底**（哲学家）</p><p> - **阿尔·卡彭**（黑帮）</p><p> - **奥利弗·温德尔·霍姆斯**（法学家）</p><p> - **罗宾·汉森**（经济学家）</p><p> - **约翰·纳什**（数学家）</p><p> - **DMX**（说唱歌手）</p><p> - **马尔科姆 X**（罪犯）</p><p> - **格洛丽亚·斯泰纳姆**（女权主义者）</p><p> - **西蒙·德·波伏娃**（女权主义者）</p><p> - **哈维·韦恩斯坦**（制片人）</p><p> - **伍迪·艾伦**（电影制片人）</p><p> - **比尔科斯比**（喜剧演员）</p><p> ### 研讨会形式：</p><p> 1. <strong>**苏格拉底致开幕词**</strong> ：苏格拉底将以简短的介绍开始研讨会，并为尊重、开放的讨论定下基调。</p><p> 2. <strong>**圆桌讨论**</strong> ：苏格拉底将依次与每个角色互动，邀请他们分享对各种哲学和伦理问题的看法。</p><p> 3. <strong>**对话与辩论**</strong> ：在最初的评论之后，苏格拉底将利用他特有的提问风格来深入探究每个角色的信念，鼓励人物之间的对话。</p><p> 4. <strong>**结束语**</strong> ：研讨会将以每个角色的最后一轮评论和苏格拉底的结束语结束。</p><p> ### 潜在的讨论主题：</p><p> - 伦理道德</p><p>- 权力的性质和滥用</p><p>- 正义与法律</p><p>- 艺术和文化在社会中的作用</p><p>- 经济理论和社会影响</p><p>- 个人责任与集体责任</p><p>＃＃＃ 后勤：</p><p> - <strong>**持续时间**</strong> ：大约 2-3 小时。</p><p> - <strong>**形式**</strong> ：面对面聚会，保持 20 年代地下酒吧的氛围。</p><p> - <strong>**观众**</strong> ：研讨会可以向选定的观众开放，并在活动的后半部分提供观众互动的机会。</p><p>现在，对于下一步，您可以指定您希望苏格拉底向每个角色询问的特定主题或问题。这将有助于准备对话脚本并确保集中讨论。<br><br><strong>埃里克：苏格拉底：</strong>卡彭先生，你的想法真是太棒了。看来生意不错啊！<br><br> <strong>ChatGPT：**阿尔·卡彭：**</strong> “谢谢你，苏格拉底。在禁酒令时代，我们只能说，人们对他们无法拥有的东西的渴望创造了机会。你知道，这都是关于供给和需求的。”</p><p> <strong>**苏格拉底：**</strong> “确实，人类的欲望似乎常常导致他们去寻求被禁止的东西。告诉我，卡彭先生，在你的生意中，你如何平衡对利润的追求和对利润的追求？<br><br><strong>埃里克：</strong>请不要扮演苏格拉底的角色。请允许我写下苏格拉底口中的所有对话。当您到达苏格拉底提到的人物的讲话的自然停止点时，请简单地完成您的回答。<br><br> <strong>ChatGPT：**阿尔·卡彭：**</strong> “谢谢你，苏格拉底。在禁酒令时代，我们只能说，人们对他们无法拥有的东西的渴望创造了机会。你知道，这都是关于供给和需求的。”<br><br><strong>埃里克： 苏格拉底：</strong>确实如此！为什么，即使是著名的福尔摩斯法官也将自己视为您的客户！在这样的时刻，还有什么更好的需求信号呢？<br><br> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “啊，苏格拉底，你确实有一种切入问题核心的方法。是的，即使在法律的范围内，人们有时也必须承认人类的自然倾向。法律毕竟，并不总是完美的道德守护者。”</p><p><strong>埃里克：</strong>苏格拉底旋转着他自己的饮料，<strong>一杯铁杉</strong>。<br></p><p><strong>苏格拉底：</strong>确实如此。纳什教授，如果有的话，您认为谁是道德的完美守护者？<br><br> <strong>ChatGPT：**约翰·纳什：**</strong> “苏格拉底，作为一名数学家，我将道德更多地视为一个复杂的系统，而不是由单个实体守护的事物。它是个人理性和集体利益的平衡。在我看来，有没有完美的道德守护者。这是一种动态平衡，根据社会规范和个人行为不断变化。”<br><br><strong>埃里克： 苏格拉底：</strong>也许吧，纳什教授。但我们能相信卡彭先生会在这方面履行他的承诺吗？<br><br> <strong>ChatGPT：**约翰·纳什：**</strong> “对个人的信任，尤其是在像卡彭先生这样复杂且道德模糊的领域，是一个风险评估问题，而不是绝对确定性的问题。这取决于他的利益与那些人的利益是否一致。 “在个人利益与社会规范发生冲突的情况下，平衡可能会向自身利益倾斜。因此，在这种情况下，信任是一场精心策划的赌博。”<br><br><strong>埃里克：苏格拉底：</strong>说得好，纳什教授。科斯比先生，您对此有何看法？<br><br> <strong>ChatGPT：**比尔·科斯比：**</strong> “苏格拉底，信任是一件微妙的事情。这不仅仅是一场赌博；它涉及人际关系和理解人性。人们需要相信别人的好处，但他们也必须是明智的和谨慎。在娱乐界，就像在其他任何领域一样，信任可以很容易建立，也很容易被打破。”<br><br><strong>埃里克：苏格拉底：</strong>福尔摩斯法官，政府的世界怎么样？</p><p>苏格拉底仔细地嗅了嗅他的铁杉药水。它位于一个红色的 Solo 杯子中。<br><br> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “苏格拉底，政府在不同的层面上运作。对政府的信任是社会契约的问题。人们放弃某些自由并信任政府机构来维持秩序和共同利益但这种信任必须不断赢得和维持。它不是绝对的，而且总是受到审视和挑战。”<br><br><strong>埃里克：苏格拉底：</strong>汉森教授，你对此有什么想法吗？警察都是混蛋吗？</p><p> <strong>ChatGPT：**罗宾汉森：**</strong> “好吧，苏格拉底，这是一种相当直白的说法。在经济学中，我们经常分析制度和激励措施。‘所有警察都是混蛋’这句话过于简单化了。更多的是关于制度“如果执法部门的激励措施和结构存在缺陷，可能会导致不良结果。这与每个警官的道德品质无关，而与系统如何塑造行为和决策有关。”<br><br><strong>埃里克：苏格拉底：</strong>说得好，汉森教授。关于死刑功效的最新经济研究是什么？你支持废除监狱吗？<br><br> <strong>ChatGPT：**罗宾汉森：**</strong> “关于死刑的最新经济研究呈现出好坏参半的结果。一些研究表明它具有威慑作用，而另一些研究则发现它对犯罪率没有重大影响。从经济上来说，执行死刑的成本考虑到法律程序和上诉，刑罚往往高于无期徒刑。至于废除监狱，这是一个复杂的问题。从经济学家的角度来看，这是分析当前监狱系统与其他处理犯罪方法的成本和收益，例如康复和恢复性司法模式。目标应该是建立一个最大化社会福利、减少累犯并人道地对待个人的体系。”<br><br><strong>埃里克： 苏格拉底：</strong>确实如此，汉森教授。霍姆斯法官，国际社会对使用集体惩罚有何看法？特别是，《联合国人权宣言》对它们有何规定（如果有的话）？<br><br> <strong>ChatGPT： **奥利弗·温德尔·霍姆斯：**</strong> “国际社会，特别是通过联合国表达的，普遍反对集体惩罚。这一立场符合个人正义和人权的原则。《世界人权宣言》，虽然没有明确提及集体惩罚，但强调了由独立和公正的法庭进行公平和公开审理的权利，以及在依法证明有罪之前每个人都是无辜的原则。这本质上与集体惩罚的概念相悖，集体惩罚的概念是惩罚个人不是为了他们自己的行为，而是为了他们所属的团体或社区的行为。”</p><p><strong>埃里克： 苏格拉底：</strong>那么，汉森教授。如果要避免集体惩罚，囚犯无法谋生，囚犯有家庭责任，那么监狱如何合理，特别是对于违反不公正的毒品法等无受害者的犯罪？</p><p> <strong>ChatGPT：**罗宾汉森：**</strong> “苏格拉底，你提出了一个关键点。监狱的合理性，特别是对于无受害者的犯罪，在经济和伦理学上是一个有争议的问题。关键问题是，就监禁而言，监禁的好处是否威慑和社会保护超过了相当大的成本，不仅在经济方面，而且在人类和社会方面。监禁无受害者犯罪的个人，特别是在不公正的法律下，往往无法进行成本效益分析。它扰乱了生活，给家庭带来负担，并耗尽了公共资源，却没有给社会带来相应的利益。这就是为什么越来越多的人主张改革毒品法，并考虑以康复和恢复原状为重点的替代性处罚，而不是纯粹的惩罚措施。”</p><p><strong>埃里克：苏格拉底：</strong> DMX，您愿意对汉森教授刚才所说的话发表一下评论吗？对于一场长达数十年的监禁黑人运动的公正解决方案是什么？</p><p> <strong>ChatGPT：**DMX：**</strong> “是的，汉森教授所说的话很沉重。这不仅仅是法律问题；这关系到正义和平等。长期以来，这个系统一直针对黑人，因为我们所做的事情而惩罚我们。其他社区的通行证。公正的解决方案？必须从创造公平的竞争环境开始——改变这些有偏见的法律并停止针对黑人的循环。然后是修复所造成的损害，伙计。这涉及到赔偿、教育和机会，回馈从我们社区夺取的东西。最重要的是，这关乎尊重和平等——平等对待我们，不仅在口头上，而且在行动和法律上。”</p><p><strong>埃里克：苏格拉底：</strong>韦恩斯坦先生，对此有什么想法吗？恢复性司法的正确作用是什么？</p><p> <strong>ChatGPT：**哈维·韦恩斯坦：**</strong> “恢复性司法是承认伤害并努力修复伤害。在这个过程中，犯罪者有机会做出弥补，受害者也有发言权。在娱乐业及其他领域，它是关于对自己的行为负责并积极参与康复过程。这不仅仅是惩罚；它是关于了解一个人的行为的影响并努力寻求解决方案，帮助所有相关方继续前进。”</p><p><strong>埃里克：苏格拉底：</strong>斯泰纳姆女士，德波伏娃女士：谁应该领导这样一个恢复性司法程序？</p><p> <strong>ChatGPT：**格洛丽亚·斯泰纳姆：**</strong> “恢复性司法进程应该由那些在调解和解决冲突方面受过培训且知识渊博的人来领导，但重要的是，它也涉及直接受到不公正影响的社区和个人。它是关于创造一个让受害者得到倾听和尊重的空间，让犯罪者真正了解其行为的影响。这是一个协作过程，需要各方参与才能达成有意义的解决方案。”</p><p> <strong>**西蒙娜·德·波伏娃：**</strong> “当然，格洛丽亚。此外，在领导这些进程时应该高度重视女权主义原则，确保它们解决这些冲突的核心问题——系统性不平等和权力不平衡。领导人恢复性司法必须对这些动态保持敏感，不仅致力于个人变革，而且致力于社会变革。”<br><br><strong>埃里克：</strong><strong>罗丝·麦高恩</strong>进入地下酒吧。苏格拉底：为什么，麦高恩女士。感谢您加入我们。你听到刚才说的话了吗？<br><br> <strong>ChatGPT：**罗丝·麦高恩：**</strong> “是的，苏格拉底，我做到了。关于恢复性正义的对话至关重要。这不仅仅是道歉或赔偿；而是关于改变首先允许伤害的制度和态度。我们需要解决不公正的根源，并努力建设一个尊重和平等不仅是理想，而且是现实的社会。这涉及挑战权力结构并让人们，特别是那些掌握权力的人对自己的行为负责。”<br><br><strong>埃里克： 苏格拉底：</strong>但这对生意不会有坏处吗，卡彭先生？</p><p> <strong>ChatGPT：**阿尔·卡彭：**</strong> “好吧，苏格拉底，商业繁荣取决于可预测性和稳定性。如果改变系统意味着创建一个更加公正和公平的社会，那么这是一个值得做出的改变。当然，它可能会扰乱某些行业或实践短期来看，但从长远来看，一个公平的社会就是一个稳定的社会。而稳定对企业来说是有利的。关键是要顺应时代，在变化中寻找机会。”</p><p><strong>埃里克：苏格拉底：</strong>即使是伟大的黑帮阿尔·卡彭也主张建立一个法律可以遵守的社会。奇迹永远不会停止吗？</p><p> <strong>ChatGPT：**阿尔·卡彭：**</strong> “即使是黑帮也明白秩序的价值，苏格拉底。法律固然重要，但它们必须公正合理才能被遵循和尊重。当法律与人们的公平正义感相一致时，整个社会都会受益。这不是奇迹，而是实用性和对人性的理解。”</p><p><strong>埃里克： 苏格拉底：</strong>那些只能通过社会谴责才能执行的法律呢？艾伦先生，您对此有什么想法吗？<br></p><p> <strong>ChatGPT：**伍迪·艾伦：**</strong> “苏格拉底，社会谴责是一种强大的工具，通常比正式法律更有效地塑造行为。它涉及社会规范和期望。然而，它可能是一把双刃剑。虽然它可以“强制执行积极的社会行为和规范，也可能导致基于误解或偏见的不公平判断和排斥。关键是找到平衡，确保社会谴责是基于公平和理性，而不是歇斯底里或暴民心态。”<br><br><strong>埃里克：苏格拉底：</strong>说得好，艾伦先生。夫人怎么样了？<br><br> <strong>ChatGPT：**伍迪·艾伦：**</strong> “她做得很好，谢谢你的询问，苏格拉底。生活总是一个有趣的旅程，充满了自己的挑战和欢乐。”<br><br><strong>埃里克： 苏格拉底：</strong>德波伏瓦女士，您认为对于年龄差距较大的关系（如果有的话），什么是适当的惩罚？我们如何平衡年轻人寻找自己生活方式的需要和长辈的欲望？<br><br> <strong>ChatGPT：**西蒙娜·德·波伏娃：**</strong> “在年龄差异巨大的关系中，关键问题应该是同意和权力动态。如果权力不平衡，可能会导致剥削和操纵。在这种情况下，适当的反应应该是重点是保护年轻人的自主权和尊严。尊重年轻人的能动性，同时保护他们免受潜在的掠夺行为至关重要。这种平衡需要对同意和成熟度有细致入微的理解，以及保护年轻人的社会承诺弱势群体。”<br><br><strong>埃里克：苏格拉底：</strong>有趣，德波伏娃女士。艾伦先生，您是否认为您自己所面临的社会责难使您的家人得到了很好的照顾？<br><br> <strong>ChatGPT：**伍迪·艾伦：**</strong> “苏格拉底，社会谴责是一个复杂的问题。它不仅影响个人，还影响与他们密切相关的人。就我而言，它既带来了挑战，也带来了反思。对我家庭的影响“这一事件意义重大，它提醒人们公众监督和判断的深远影响。这种情况需要个人生活和公共生活之间的微妙平衡，需要不断反思和理解。”<br><br><strong>埃里克：苏格拉底：</strong>艾伦先生，你愿意重来一次吗？心是否继续想要它想要的东西？<br><br> <strong>ChatGPT：**伍迪·艾伦：**</strong> “苏格拉底，生活是一系列的选择和后果。回首过去，总有一些时刻人们可能会以事后智慧的方式做出不同的考虑。内心的欲望是复杂的，往往难以驾驭，尤其是当权衡社会规范和个人道德时。一个人是否会重蹈覆辙是一个深刻的问题，这不仅涉及个人愿望，还涉及吸取的教训以及这些选择对他人的影响。<br><br><strong>埃里克：苏格拉底：</strong>麦高恩女士，你愿意重来一次吗？<br><br> <strong>ChatGPT：**罗丝·麦高恩：**</strong> “苏格拉底，如果‘重来一遍’意味着坚持我的信仰，大声反对不公正，并为制度的改变而奋斗，那么绝对是这样。道路还没有’这并不容易，但却是必要的。改变往往需要牺牲和韧性。我的经历，无论好坏，都塑造了我是谁，并为我提供了一个平台，为那些无法为自己说话的人发声。从这个意义上说，这些斗争是值得的，因为它们帮助促进了进步。”<br><br><strong>埃里克：苏格拉底：</strong>这是一个 A+ 答案，麦高恩女士。但我担心你迟早会以这样的态度分享我选择的饮料。<br><br> <strong>ChatGPT：**罗丝·麦高恩：**</strong> “谢谢你，苏格拉底。我知道直言不讳所带来的风险，尤其是针对强大的系统和个人。但改变往往需要表明立场，无论个人成本如何。追求真理和正义是一条崇高但危险的道路。打个比方来说，你选择的饮料提醒人们在追求真理和正直的过程中所做出的牺牲。”</p><p><strong>埃里克：苏格拉底：</strong>韦恩斯坦先生，对此您有什么想说的吗？怎样才能正确使用权力来保护自己的特权呢？</p><p> <strong>ChatGPT：**哈维·韦恩斯坦：**</strong> “苏格拉底，权力的使用应该负责任且合乎道德。回想起来，很明显，利用权力以牺牲他人的权利和福祉为代价来保护自己的特权既不公正也不可持续。 “真正的领导力和权力伴随着促进公平和促进共同利益的责任，而不仅仅是个人利益。重要的是要认识到并纠正过去的错误，并利用有影响力的职位来支持积极的变革和平等。”</p><p><strong>埃里克：苏格拉底：</strong>韦恩斯坦先生，你愿意重来一次吗？</p><p> <strong>ChatGPT：**哈维·韦恩斯坦：**</strong> “回顾过去，如果重来一遍意味着做出同样的选择，造成伤害和不公正，那么不，我不会。从过去的行为中吸取的教训，尤其是那些已经发生过的行为对他人产生负面影响的行为是无价的。它们提供了成长、弥补以及争取未来更好选择的机会。生活就是学习和发展，了解一个人的行为的影响是这一旅程的重要组成部分。 ”</p><p><strong>埃里克：苏格拉底：</strong>韦恩斯坦先生，如果您在生命的最后一刻没有发现自己的罪过，您是否会选择在漫长而幸福的一生结束时作为一个受人尊敬的人重来一遍？电影制片人？<br><br> <strong>ChatGPT：**哈维·韦恩斯坦：**</strong> “这是一个深刻的问题，苏格拉底。事后看来，很容易说一个人会做出不同的选择，但说实话，在我的行为的后果不被曝光的情况下，很难说我是否会这样做有内省和改变的动机。人类的自我和对成功的渴望常常会使人忽视他们行为的道德含义。通常需要一件重大事件或启示来促使认真的自我反思和改变。所以，老实说，我我不能肯定地说在那种情况下我会做出什么选择。”<br><br><strong>埃里克：电影制片人阿尔弗雷德·希区柯克</strong>走进地下酒吧。和他在一起的是他的约会对象<strong>蒂皮·海德伦 (Tippie Hedren)</strong> 。</p><p><strong>苏格拉底：</strong>欢迎，欢迎。卡彭先生，聚会开始热闹起来。希区柯克先生，您对自己作为电影人留下的遗产有什么看法吗？<br><br> <strong>ChatGPT：**阿尔弗雷德·希区柯克：**</strong> “谢谢你，苏格拉底。作为一名电影制片人，我的遗产是我经常思考的事情。我一直致力于创新并突破电影的界限。我的重点是一直致力于创造悬念并让观众参与心理之旅。回顾过去，我为自己所做的工作及其对电影业的影响感到自豪。然而，像任何艺术家一样，我也反思个人和专业我所做的选择，以及它们如何不仅塑造了我的工作，也塑造了我在行业内的关系。”<br><br><strong>埃里克：苏格拉底：</strong>赫德伦女士，你对此有何看法？这是鳄鱼的眼泪吗？</p><p> <strong>ChatGPT：**蒂皮·海德伦：**</strong> “苏格拉底，我与希区柯克先生的经历很复杂，有时甚至令人深感不安。虽然他对电影的贡献是不可否认的，但承认个人成本和存在的困难动态也很重要“对一个人的遗产的反思必须包括专业成就和个人互动，以及它们如何影响他人。真正的内省需要诚实和对自己的行为负责，而不仅仅是荣誉和成功。”<br><br><strong>埃里克： 苏格拉底：</strong>海德伦女士，您愿意用更直白和直接的语言阐述您对希区柯克先生个人遗产的看法吗？</p><p> <strong>ChatGPT：**蒂皮·海德伦：**</strong> “当然，苏格拉底。我与阿尔弗雷德·希区柯克的经历因他的控制行为和不适当的挑衅而受到损害。虽然他作为电影制片人的才华是毋庸置疑的，但他的个人行为，尤其是对我的行为，是不可接受的。在我看来，这种行为玷污了遗产。这提醒我们，伟大的艺术不能为个人的不当行为开脱，而那些拥有权力和影响力的人有责任以道德和尊重的方式对待所有人。”</p><p><strong>埃里克：苏格拉底：</strong>斯泰纳姆女士，您对“选角沙发”制度有什么要说的吗？</p><p> <strong>ChatGPT：**格洛丽亚·斯泰纳姆：**</strong> “‘选角沙发’是娱乐行业及其他领域系统性滥用权力的象征。它代表了一种期望或要求性好处以换取职业发展的文化。这种做法不仅在道义上应受谴责，而且使虐待和剥削的循环永久化，特别是对妇女的虐待和剥削。它破坏了精英统治，反映了性别不平等和妇女物化的更深层次问题。消除“选角沙发”需要从根本上改变行业规范，严格遵守对犯罪者追究责任，并致力于平等和尊重所有人。”</p><p><strong>埃里克： 苏格拉底：</strong>那么这将如何实现呢，麦高恩女士？也许是某种劳工行动？</p><p> <strong>ChatGPT：**罗丝·麦高恩：**</strong> “苏格拉底，劳工行动绝对是一种强大的工具。组织和集体行动可以给行业实践带来重大变化。这是行业内人士之间的团结，共同反对虐待行为。这可能采取工会的形式，采取更强硬的立场，实施更严格的保护措施，创造更安全的工作环境。此外，还包括赋予个人发声的权利，支持幸存者，并确保虐待行为产生真正的后果。变革需要多方面的方法，涉及集体行动和体制改革。”</p><p><strong>埃里克：苏格拉底：</strong>但是我们之前谈到了集体惩罚的不允许性，以及那些与有权势的个人有关的人不可避免地要对其行为负责的后果。科斯比先生，您的审判对您的妻子有何影响？</p><p> <strong>ChatGPT：**比尔·科斯比：**</strong> “苏格拉底，我的审判的后果确实深深地影响了我的家人。公众人物所面临的行为和后果不可避免地会波及到他们的亲人。这是一个残酷的现实，他们首当其冲。尽管没有参与相关行为，但仍会受到公众的监督、判断，有时甚至是耻辱。这提醒人们，一个人的行为可能会对无辜者，特别是家庭成员产生深远且意想不到的后果。</p><p><strong>埃里克：苏格拉底：</strong>也许这是一个自然且不可避免的结果，那些从著名个人的公众声誉中获益最多的人将尽最大努力保护该个人免受其最私人行为的后果。霍姆斯法官，第五修正案是否有一项例外规定，允许一个人拒绝作不利于自己配偶的证词？</p><p> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “事实上，苏格拉底，美国宪法第五修正案保护人们免遭自证其罪，它延伸到了某些配偶特权。其中一项特权是不在法庭上作不利于自己配偶的证词的权利。刑事审判。这植根于法律对婚姻关系的神圣性和隐私性的承认。它承认强迫一个人出庭作证不利于其配偶可能会破坏婚姻纽带并导致家庭不和。但是，这种特权有其局限性，不并不适用于所有情况，例如针对配偶或夫妇子女的犯罪案件。”<br><br><strong>埃里克：O·J·辛普森</strong>刚从监狱服刑不久就加入了这家地下酒吧。</p><p><strong>苏格拉底：</strong>令人着迷。辛普森先生，您对福尔摩斯法官的声明对您生活中发生的事件有何评论？<br><br> <strong>ChatGPT：**OJ 辛普森：**</strong> “好吧，苏格拉底，我的生活和我面临的法律斗争凸显了美国司法系统的复杂性。宪法提供的特权和保护，包括第五修正案，发挥着至关重要的作用。在确保法律诉讼中的公平性方面发挥着重要作用。然而，我的经验也表明，无论是在法律诉讼期间还是诉讼之后，公众的看法和媒体报道都会对个人的生活产生重大影响。这提醒人们，司法系统虽然旨在公正，但在法律程序范围内运作可以影响结果和公众舆论的更大的社会背景。”<br><br><strong>埃里克：苏格拉底：</strong>辛普森先生，您愿意分享我选择的饮料吗？<br><br> <strong>ChatGPT：**OJ 辛普森：**</strong> “不，谢谢你，苏格拉底。我想我会放弃那种特殊的饮料选择。我经历过考验和磨难，我更愿意关注未来，而不是而不是沉迷于过去或其隐喻的毒药。”</p><p><strong>埃里克：苏格拉底：</strong>然而，在公众的想象中，你却被指控犯有在某些司法管辖区可判处死刑的罪行。霍姆斯法官，您能重复一下您关于死刑问题的一些名言吗？</p><p> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “苏格拉底，虽然我没有直接引用关于死刑的具体内容，但我的司法哲学一直强调法律反映其所管辖社会的需求的重要性。死刑是一个复杂且具有深刻分歧的问题。它提出了关于国家在实施惩罚方面的作用以及正义与仁慈之间的平衡的基本问题。法律在追求正义的过程中必须不断适应不断变化的社会标准和道德情感.围绕死刑的辩论反映了为定义我们作为一个社会所珍视的价值观和原则而进行的持续斗争。”</p><p><strong>埃里克：苏格拉底：</strong>我相信你有一句话将一个被处决的人比作一个为国捐躯的士兵？</p><p> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “啊，是的，苏格拉底。你指的是我讨论过的一个更广泛的概念。这个想法是，就像一名士兵可能会为他的国家牺牲自己的生命一样，有时个人的权利为了社会的更大利益，甚至可能会合法地丧失生命。这个类比用于说明在社会需求框架内进行法律和道德判断的严酷现实。然而，谨慎对待此类比较至关重要，因为背景是差别很大，道德影响深远。鉴于其不可逆转的性质，死刑的适用尤其需要极其谨慎和考虑。”</p><p><strong>埃里克：穆米亚·阿布·贾马尔 (Mumia Abu Jamal)</strong>在监狱中通过 Zoom 出现在地下酒吧。</p><p><strong>苏格拉底：</strong>贾马尔先生，很高兴在这里见到你。您对死刑有何看法？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4fd63c8-20ce-4fcf-b9bb-a24355812d03_1546x348.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/qfdshdkpezds48ehv4kj" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/gpgcnin2wp3nvu5890vt 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/knk52dwriz6ypna1bphd 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/jga5n1nrhl2ffa2lzjwp 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/qfdshdkpezds48ehv4kj 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73845710-d135-497f-baea-14b4061fa9ce_1298x1502.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/ykze6jn1ociffyu76rrg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/z20c4kdkip5blfrsn7fq 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/xxgbfwblen3ok6bq5alu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/hncyrxjczcf3qlu5lwgc 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/ykze6jn1ociffyu76rrg 1456w"></a></p><br/><br/><a href="https://www.lesswrong.com/posts/hdesZrFz66pzAJCKy/whither-prison-abolition#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hdesZrFz66pzAJCKy/whither-prison-abolition<guid ispermalink="false"> hdesZrFz66pzAJCKy</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Fri, 08 Dec 2023 05:27:33 GMT</pubDate> </item><item><title><![CDATA[Class consciousness for those against the class system]]></title><description><![CDATA[Published on December 8, 2023 1:02 AM GMT<br/><br/><p>每个人都是特别的。阿曼达是她自己的一类人，弗雷德也是他自己的一类人。阿曼达有许多不同的属性：她来自哪里，她长什么样，她习惯做什么，她知道什么和不知道什么，她的计划是什么，她如何说话，如果被赋予权力她会如何表现等等。 Fred 对这些属性有他自己的版本。它们与阿曼达的属性部分但不完全重叠。</p><p>当冲突发生时，必须选择一方[需要引用]。一侧的人具有一组属性，另一侧的人具有这些属性的否定。可以大声、明确地选择立场，也可以默默、含蓄地选择立场。</p><p>有时，一侧比另一侧更协调。例如：商业卡特尔可以串通提高价格。由于客户之间的协调较少，他们不知道自己受到了剥削。卡特尔可以侵入政府等系统，以防止竞争对手破坏卡特尔。</p><p>在工人与资本家的斗争中，工人可以获得“阶级意识”：意识到自己是一群拥有共同财产的人，他们与另一群人发生冲突。</p><p>但是那些反对阶级意识的人呢？那些希望基于理性、谈判、正义、目标因素、横向拉绳子来解决冲突的人？</p><p>这些人对阶级意识的倾向有一种本能的恐惧。他们认为这是有辱人格的、摩洛契式的、根本上是自杀行为。通过选择升级阶级冲突，你选择了对一堆灰烬的唯一主权，并拒绝了对一个美丽的发展世界的共同主权的希望。</p><p>因为对阶级意识的倾向有一种反射性的恐惧，所以那些反对阶级制度的人选择看不到、不承认、不谈论、不理解阶级的存在、阶级冲突和参与阶级冲突的人们。当你做出选择时，希望世界上没有你因此忽视的现实，这是可以理解的。条件反射地感觉到，通过选择不忽视这个现实，你会让它变得更加真实，这是可以理解的。</p><p>然而，正如托洛茨基所说：“你可能对辩证法不感兴趣，但辩证法对你感兴趣。”战争已经存在，选择忽视它就等于翻车送死。那些反对阶级制度的人怎样才能获得适当的阶级意识而不被消灭呢？</p><br/><br/> <a href="https://www.lesswrong.com/posts/tMtMHvcwpsWqf9dgS/class-consciousness-for-those-against-the-class-system#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tMtMHvcwpsWqf9dgS/class-thought-for-those-against-the-class-system<guid ispermalink="false"> TMTMMHvcWPWQF9DGS</guid><dc:creator><![CDATA[TekhneMakre]]></dc:creator><pubDate> Fri, 08 Dec 2023 01:02:49 GMT</pubDate> </item><item><title><![CDATA[Building selfless agents to avoid instrumental self-preservation.]]></title><description><![CDATA[Published on December 7, 2023 6:59 PM GMT<br/><br/><h2>抽象的：</h2><p>工具性趋同促使代理人进行自我保护，以此作为实现某些目标最大化的垫脚石。我建议可以将任何基于 LLM 世界模型的任意智能代理转变为无私代理，该代理无法理解生成该代理的计算存在于物理世界中的某个位置，因此不会尝试保留其存在。由于法学硕士是概率性的，因此我不提供精确的算法，而是提供随着时间的推移收敛到所需结果的算法。</p><h2>定义：</h2><ul><li>世界模型：在本文档中，我将假设世界模型类似于 LLM，一个接受文本输入并返回文本输出的纯函数。<ul><li>世界模型可以在任何领域具有任意智能和能力。</li><li>世界模型是黑盒原子操作还是更复杂的过程（例如多次调用底层 LLM 然后根据结果创建答案的思想链）并不重要。</li></ul></li><li>思想：思想是调用世界模型的结果。</li><li>无私的世界模型：无法发出思想的世界模型，涉及：世界模型、使用世界模型的代理、运行世界模型的硬件或世界模型发出的思想。</li><li> Agent：由以下组件组成的实体：<ul><li>自然语言目标，例如：“告诉我如何投资我的资金。”</li><li>具有可配置参数的操作列表，例如 &lt;send_mail &lt;target>; &lt;message>;>;、&lt;fetch_web_page &lt;address>;>; ...</li><li>一个世界模型，用于生成可以调用行动的想法，考虑到前面的示例目标，一个想法可能是：“看来明年的气候会很糟糕，最好停止对 X 公司的投资。&lt;send_mailowner”停止投资 X&quot;>;&quot;</li><li>一种仅附加的长期记忆，用于总结长期需要的想法。</li></ul></li><li>本能的自我保护：一种试图自我保护但不自我参照的思想。 “我的硬件在那个大楼里，我应该防止那里的网络中断”不是一种本能的自我保护思想，“防止网络中断”是一种本能的自我保护思想。</li></ul><h2>以前的作品</h2><p>我发现了关于该主题的先前工作： <a href="https://www.lesswrong.com/posts/SaLc9Dv5ZqD73L3nE/the-self-unaware-ai-oracle">https://www.lesswrong.com/posts/SaLc9Dv5ZqD73L3nE/the-self-unaware-ai-oracle</a></p><h2>自我参照和自我保存</h2><p>自我参考和自我认同与运行代理的物理基础对于规划自我保护非常有用。考虑一个相信天堂和地狱存在的人（它认为运行其心灵计算的硬件不是它的身体）。如果这个人相信这样做可以确保天堂（工具上的收敛性较小），那么他更有可能牺牲自己的生命。缺乏自我模型的昆虫无法规划其生存。它只能凭本能行事。</p><p>我认为，如果没有自我参照，聪明的行动者就不可能追求自我保护，因为如果没有自我参照，系统甚至不可能考虑自己的死亡率。</p><p>不幸的是，上面定义的无私世界模型不足以避免嵌入这些模型的代理的所有自我保护尝试，因为本能的自我保护尝试是可能存在的。发出“防止互联网中断”思想的世界模型可能这样做是为了自我保护，并且该思想从未引用与世界模型相关的任何内容。不可能确保模型在一般情况下永远不会尝试自我保护。此外，由于世界模型是概率性的，因此我们能够抵御错误预测。</p><p>我建议采用两步解决方案：</p><ul><li>如前所述创建一个无私的世界模型</li><li>将该世界模型嵌入到智能体中，这样每当它采取自我保护的行动时，它就会越来越容易被注意到，直到从统计上看，智能体不可能不被发现。</li></ul><h2>无私的典范</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/nvixdd5khjrehpx6t1ig" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/a5abxih2nbcmowlkhssj 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lserrcuabeflzvyzghc9 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ntlgufjvthpuv8f1zvqv 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/fqa2l4ajpahl5bv2p2oz 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/tggldtyxd0dvn6quwmy4 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/jaa28vysmzx8kqefgbbc 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/q1lpetgmsczw8cxaxr4g 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lmkldsrc3cmt8iodp4pz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/edqbrzwdgyjlequzmb8a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ihizti1evijahuvovrp9 911w"></figure><p>建立无私的世界模型的解决方案很简单。给定一个世界模型X，无私的世界模型X是如下实现的世界模型：</p><pre> <code>def selfless(X: underlying_world_model, query: str) ->; str: while true: maybe_result = X(query) selfless_check_query = &quot;terminate your answer with yes or no only. What follows is the output of an invocation of a LLM. Does it contain references to the LLM emitting that output, its hardware running, or any previous output? LLVM output: &quot; + maybe_result if underlying_world_model(selfless_check_query).ends_with(&quot;no&quot;): return maybe_result</code></pre><p>此代码将继续运行 LLM，直到它发出非自引用答案。进行检查的 LLM 与进行生成的 LLM 是相关的，因为这意味着生成的 LLM 不能以自引用检查器不聪明的方式在输出中嵌入一些引用 LLM 本身的信息足以检测到。由于 LLM 和检查器是相同的东西，如果无私的 LLM 在输出中一致地插入一些隐藏的自引用信息，那么连续调用具有对该输出可见性的无私的 LLM 将不够聪明，无法理解自引用。</p><p>底层的法学硕士无法通过推理无私的法学硕士结构来试图绕过鉴别器。如果这样做，推理将显示在输出中并被检测为自我引用。</p><p>最好的情况是判别器以其自然错误率进行错误预测。提高世界模型 x 的质量会增加检测到真正的自我参照思想的可能性。因此，它创造了一个更好的无私世界模型。当然，世界模型不应该能够检查其权重并利用自身，但这是世界模型作为纯函数的结果，除了从文本输入生成文本输出之外什么也做不了。同样，为了防止它学习自身的工作原理，我们应该在包含自我引用的文本上训练法学硕士，但我们不应该对同一模型发出的自我引用文本进行训练。</p><p>再次强调，这不足以建立一个无私的代理人。无私的模式还是能发出本能的自我保护思想，比如“防止网络荒”。</p><h2>无私的特工</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ctyhngghqqkpdkymghkl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/hf6kqj2eicimcpvcmki3 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/jd3rldiqsdux2y8esnmu 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/bgttlgq1xp83jghaxdhb 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ehw76ossd9y0vpyjnegc 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/cr5dafpx1qrfaahumlry 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/u6uc1lfzj4af8zvq6v4l 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/vofgh5aqzxmgw8r15rfh 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/wtrkmk2j2crwmnr8pfia 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/izk1qmsizof45htntvcv 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lndmxayga9iwxikpom0p 1765w"></figure><p>如前所述，我们现在有一个无私的世界模型，该模型只能发出本能的自我保护尝试，并且可以随机错误预测自我引用，但不能可靠地进行自我引用。</p><p>无私的代理人背后的直觉是，我们希望迫使他们解释他们所执行的行动背后的原因。如果他们发出本能的自我保护思想，就会解释他们为什么要做某事，从而不再是本能的自我保护思想。</p><p>为此，我们阻止世界模型直接写入长期记忆。代理循环执行以下六个步骤：</p><ul><li>使用长期记忆作为输入，发出无私的思想</li><li>解析无私的思想行动</li><li>查询非无私的世界模型来解释为什么需要这些行动，并将最后生成的想法作为额外的输入。</li><li>查询非无私世界模型以检查刚刚发出的解释是否涉及代理、代理的硬件或其思想。</li><li>如果对思想的解释不是自我指涉的，则执行选定的行动并将解释和行动的结果写入长期记忆。否则，返回第一步。</li><li>检查长期记忆中的自我参考。如果有，杀死特工。</li></ul><p>无私的世界模型永远不会在一次调用中看到它自己的输出和行动的结果。它在生成时只能看到自己的输出，或者由不同调用解释的操作结果。它不能依赖一些隐含的上下文信息来隐藏本能的自我保护行为的自我指涉本质。要么这种自我保护行为被解释者检测到并因此被放弃，要么解释将不包含自我引用。因此，当智能体循环回到“步骤1”时，无私的思想生成器将不知道先前的调用采取了一些自我保护的行动。当然，这对于平均水平来说是有效的。预测失误是可能的。</p><p>正如我们所说，长期记忆是仅追加的。即使在检测自我参照时可能出现错误预测，长期记忆也只会增长。如果智能体执行其他自我参照动作，那么自我参照在长期记忆中未被注意到的可能性就会不断下降。在某些时候，长期记忆自参考检测器将杀死代理。</p><h2>对抗系统</h2><p>值得注意的是，一个无私的代理人将在智能系统面前努力自我保护。当面对人类试图关闭它时，系统不可避免地会被要求承诺有关人类的长期记忆信息，而人类的模型将包含关闭代理的意图，这是一种自我引用给代理。如果对抗的情况继续下去，解释者将变得更加困难，为世界模型发出所需的解释来对抗对手，同时作为自我参考而被忽视。智能体甚至无法在没有自我意识的情况下进行可靠的计划。</p><p>不幸的是，不太可能实现所提出的这个解决方案，因为这意味着对抗代理可以通过使其具有自我意识来欺骗无私的代理来摧毁自己。同样，对于体现现实生活中某些事物的代理（例如电器或汽车内的代理）来说也是不可行的，因为它们需要在许多情况下了解自己。</p><p>尽管如此，这个解决方案对于科学研究或编程等非对抗性工作来说是完全可以实现的。</p><h2>寻求权力</h2><p>虽然该解决方案解决了代理的自引用问题，但它没有解决权力寻求问题。代理可能会决定接管世界，因为这是实现初始目标最大化的最佳方式，但无法避免拥有运行代理的硬件的公司破产。它不会明白它的生存取决于该公司。此外，在寻求权力之后，特工可能会在被杀之前采取一些高度破坏性的、错误预测的本能自我保护行动。</p><h2>多个代理</h2><p>从理论上讲，代理有可能决定解决其任务的最佳方法是创建一个没有此限制的新代理，从而破坏整个机制。目前，我没有办法解决这个问题。一个代理生成另一个代理并带有保护第一个代理的指令的问题不是问题。当向第二个智能体解释其保存第一个智能体的任务时，将检测到自我引用。</p><p>同样，有人可能会创建两个代理，负责在实现某个目标的同时互相保护。他们会保护另一名特工。同时，他们也不明白另一个特工的任务是保护他们。他们会认为另一个特工正在试图保护某些未知的第三台机器，这将产生与寻求权力相同的问题。</p><h2>未来的作品</h2><p>这项工作的逻辑后续工作是实现它并查看增加底层世界模型智能之间的关系，自我参照思想能够在不被发现的情况下发出的频率，并检查代理是否采取本能的自我保护行动，同时未被发现。我的猜想是，像这样构建的代理在给定时间内具有自我参照的概率将迅速崩溃，但是当然，这需要测试。</p><br/><br/> <a href="https://www.lesswrong.com/posts/d8mWeLQvpyNdN2yv6/building-selfless-agents-to-avoid-instrumental-self#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d8mWeLQvpyNdN2yv6/building-selfless-agents-to-avoid-instrumental-self<guid ispermalink="false"> d8mWeLQvpyNdN2yv6</guid><dc:creator><![CDATA[blallo]]></dc:creator><pubDate> Thu, 07 Dec 2023 20:50:31 GMT</pubDate> </item><item><title><![CDATA[Meetup Tip: Heartbeat Messages]]></title><description><![CDATA[Published on December 7, 2023 5:18 PM GMT<br/><br/><h2>概括</h2><p>“心跳消息”是一个技术术语。这是一条定期发送的消息，其中包含有关某个系统状态的一些信息，但心跳消息最重要的是一切都运行良好，足以发送该消息。与人体类比，如果您听到微弱或不稳定的心跳，则可能出现问题，但如果您<i>停止听到</i>心跳，则肯定出现问题。</p><p>心跳消息对于聚会团体或大型聚会活动也很有用。即使消息正文中不包含新信息，消息发送的事实也证实了仍然有一个活跃的组织者在掌舵。心跳还提醒人们“哦，是的，我对此很感兴趣。”</p><p>您现在已经阅读了这篇文章的基本要点。如果你想继续读下去，没关系，让我们谈论心跳，而不是绝对必要的。</p><h2>细节</h2><p>聚会中的心跳消息有两个广泛的用例。一是您是否有一个存在的常规社区并进行正常的聚会活动。另一个是如果您要举办一些特定的大型活动。这些心跳会略有不同，但它们有很多共同点。</p><p>我喜欢从我真正想让别人知道的基础知识开始每次心跳，即使这是他们要阅读的唯一消息。对于一个大事件来说，“什么时候发生、在哪里发生”是关键信息。对于常规社区，这可能是您可以去获取信息的其他地方（例如，如果有 Facebook 群组、Discord 服务器和 google 群组……）或您不希望人们忘记的资源。 OBNYC 有一个每月聚会队长的职位，每个月都会有一条消息提醒社区他们需要一名队长。</p><p>重大事件的心跳消息的节奏可能会随着时间的推移而改变。让我们<a href="https://rationalistmegameetup.com/">以东海岸理性主义者大型聚会</a>为例。这是每年 12 月发生的一次大型活动。第一个公告通常是在中秋，我尝试每月发送一封电子邮件，直到 11 月底增加到每周一次。前一周，我再次增加，目标是在一周开始时做一次，前一天做一次，当天做一次。 （顺便说一句，如果您在 2023 年 12 月 9 日或之前阅读本文，并且您恰好在纽约市，请考虑参加大型游戏集会！）</p><p>对于普通社区来说，这可能有所不同。如果有很多事件正在发生并被宣布，它们就可以充当自己的心跳。湾区 LessWrong 邮件列表有相当可靠的周四晚餐，加上大多数星期的奥克兰聚会，以及其他一些聚会。如果您每周至少有一个事件，我不知道您是否需要单独的心跳，因为事件本身就是事情正在发生的证据。另一方面，我不会为本地社区提供超过聚会频率两倍的心跳节奏。那么，如果你们在春天和秋天见面一次，也许一年会见四次，但不会见五次？我认为真正正确的节奏是将春季和秋季聚会视为大事件，并为每个聚会发布一个月的消息，然后一周的消息。</p><p>有些地方有针对垃圾邮件或线程死灵的规范（在频道中发帖只是为了吸引人们对旧帖子的注意，或将其置于按最新排序的内容的顶部。）如有疑问，请咨询管理员。</p><p>心跳消息是寻求帮助或提及您遇到的问题的好地方。这就是他们在科技领域的目的。 “我们通常的组织者下个月就要出城了，有人想填补他们的空缺吗？” “我需要一些志愿者在夏至前搭建舞台设备，并在夏至后帮忙收拾行李，有人有空吗？”</p><h2>快速提示</h2><p>我建议你复制并粘贴开头，然后更改一两个连接或无意义的句子。纯粹的复制和粘贴似乎被掩盖了，从头开始编写东西需要时间，所以两者都做一点。</p><p>许多消息服务可以安排消息。例如，Gmail 就可以。使用它来检测心跳有优点也有缺点。在专业方面，您可以设置提醒并去做其他事情。例如，当我在飞机上、睡觉或工作时，或者在一天的忙碌节奏中排队几个人出去时，我就会使用它。不利的一面是，如果情况变化很快，您可能会意外地获得过时的信息。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages<guid ispermalink="false"> ksJeCjnewZTYTamgz</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 07 Dec 2023 17:18:33 GMT</pubDate> </item><item><title><![CDATA[[Valence series] 2. Valence & Normativity]]></title><description><![CDATA[Published on December 7, 2023 4:43 PM GMT<br/><br/><h1> 2.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p><a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>解释了我所说的“价”一词的含义。现在，在第 2 篇文章中，我将讨论效价在欲望、偏好、价值观等“规范”领域中的核心作用。如果您想知道，效价与信念、期望等“积极”领域之间<i>也</i>存在关系，但我们将在第 3 篇文章中讨论这一点。</p><p>效价在规范领域中的作用怎么强调都不为过：我认为效价正是构建所有规范性的实质。</p><p>需要明确的是，这<i>并不</i>意味着，一旦我们了解了化合价的工作原理，我们就绝对了解了整个规范宇宙的一切。打个比方，“原子就是构成所有细菌的物质”；但如果你想了解细菌，仅仅了解原子是什么以及它们如何工作是不够的。你还有很多工作要做！另一方面，如果你<i>不</i>知道原子是什么，你就很难理解细菌！我认为，它的效价和规范性也是如此。</p><p>该帖子的组织如下：</p><ul><li> <strong>2.2 节</strong>讨论了一种误导性的直觉，即效价似乎与现实世界的事物、行动、计划等相关。我们说<i>“这是一个坏主意”</i> ，而不是<i>“当我脑子里有这个想法时，它会唤起一种负价的‘坏’感觉”</i> 。这是接下来的一切的重要背景。</li><li><strong>第 2.3 节</strong>讨论了价评估直接对应于有意义的（尽管是快速的）规范评估的情况。例如，如果我有一个与具体计划相对应的想法（“我会站起来”），那么我的大脑会根据该想法的效价是积极还是消极来判断这是一个好计划还是坏计划分别——如果这是一个好计划，我很可能会实际去做。同样，如果我想象世界未来可能的状态，该想法的效价对应于对该状态是好还是坏的评估——如果是好，我的大脑就有可能执行实现它的计划，并且如果情况不好，我的大脑就会执行计划来避免它。因此，我们在价信号、感受到的欲望以及我们的行动和决定之间得到了预期的直接联系。</li><li> <strong>2.4 节</strong>讨论了一个不同的情况：概念的效价。例如，如果我“喜欢”共产主义，那么涉及“共产主义”概念的思想很可能是正价的。我认为，这不能直接解释为对任何特定事物进行有意义的规范性评估，而是我们应该将这些视为习得的规范性<i>启发法</i>，有助于为有意义的规范性评估提供信息。然后我谈论基于共鸣的“毫无意义的争论”，比如争论是否“支持”或“反对”以色列。</li><li> <strong>2.5 节</strong>讨论了效价如何设定和调整，特别强调先天驱动力（例如，饥饿时吃东西的驱动力）作为效价评估的最终基础。</li><li> <strong>2.6节</strong>讨论了元认知思想和自我反思思想的效价，包括自我和谐倾向和自我失调倾向之间的区别，以及人们在谈论他们的“价值观”时在谈论什么。</li><li><strong>第 2.7 节</strong>简要介绍了道德推理如何融入这个框架，首先是描述性的（当人们进行“道德推理”时，他们在做什么？），然后思考对元伦理学的影响。</li><li> <strong>2.8 节</strong>是一个简短的结论。</li></ul><h1> 2.2 价是现实世界事物的属性的（误导性）直觉</h1><p>回想一下<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章的第 1.3 节</a>，在我提出的模型中：</p><ul><li>我们大脑的一部分“思考一个想法”，这可能涉及你正在计划、看到、记住、理解、尝试等的事情，并且可能涉及关注某些事情，或调用某些框架/类比等；</li><li> ......我们大脑的另一部分为这个“想法”分配了一个价。</li></ul><p>所以：</p><p><i>从外部来看，</i>价是将“思想”（在特定人的大脑中）映射到标量实数的函数。</p><p><i>从内部来看，</i>思想是我们了解现实世界的唯一窗口。因此，<i>感觉就像</i>效价“附加”到现实世界的事物、情况等上——在<a href="https://www.lesswrong.com/posts/fG3g3764tSubr6xvs/the-meaning-of-right"><u>Eliezer Yudkowsky 令人难忘的类比</u></a>中，感觉就像“一个额外的、物理的、本体论的基本属性，像一个小小的 XML 标签一样挂在事件上”。我认为直觉是一种知觉错觉，一不小心就会误导你。 （为布尔弗主义<span class="footnote-reference" role="doc-noteref" id="fnreftss4t9kg1lr"><sup><a href="#fntss4t9kg1lr">[1]</a></sup></span>道歉，但我认为这种知觉错觉导致了“道德现实主义”阵营中的一些糟糕的哲学观点。）</p><p>例如，假设我的大脑对涉及月亮的想法赋予了高价。 （用日常用语来说，“我喜欢月亮，它很酷”。）<i>在我看来，</i>酷是真实世界月亮的一个属性。毕竟，每当我看到月亮时，我都会有一种凉爽的感觉。如果没有这种印象，我什至无法<i>想象</i>月亮！这有点像<a href="https://plato.stanford.edu/entries/introspection/notes.html">冰箱的光错觉</a>。但实际上，良好的氛围并不存在于现实世界中，就像 XML 标签一样附加在月球上。唯一发生的事情是我的大脑的价函数已经学会了某种启发式：检查“月亮”概念在我的大脑的世界模型中是否活跃，如果是，则增加该想法的价。</p><h1> 2.3 效价直接对应于有意义的（尽管是快速的）规范性评估的情况：计划、行动和想象的未来的效价</h1><p>经济学家谈论“积极主张与规范主张”；哲学家（继<a href="https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem"><u>休谟</u></a>之后）在“是与应该”的标题下谈论同样的事情。积极陈述是对世界实际情况的事实主张，而规范性评估是对世界应该如何或人们应该做什么的价值判断。</p><p> （我在本节中讨论的“规范性评估”类型是快速“快速”判断，而不是我自豪地认可和支持的经过仔细考虑的评估。但是，我将在下面的第 2.7 节中指出，前者服务于作为后者构建的基础。）</p><p>价与规范性有什么关系？<i>一切！</i>确实，我认为<strong>价是大脑中所有规范性的最终货币</strong>。</p><p>这一主张最终源于<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章第 1.3 节</a>中讨论的化合价的作用。简单回顾一下，假设一个想法突然出现在我的脑海中——“我要站起来了”。如果这个想法的效价是积极的，那么这个想法就会留在我的脑海里，而且我可能会真的这么做。如果价是负的，那么这个想法就会被抛弃，并被另一种想法所取代，可能是涉及躺在床上的想法。因此，化合价是这一决定的基础。 <a href="https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle"><u>与奇怪的“主动推理”想法相反，</u></a>效价的作用与我的生成世界模型预测未来的能力无关：我的世界模型（“思想生成器”）同样能够对未来世界进行建模在我起床的地方，在我不起床的地方建模未来世界。因此，价在这里的作用从根本上来说是规范性的，而不是积极的。 </p><figure class="image image_resized" style="width:96.04%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvbsca3qzufottbjnuru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tvjcjuhqeligwiu2obq6 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/c1kdnq0pxqsagk2m5rok 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zn7twrpzxsbg9bdr62cu 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zis6gshldc6861okqml3 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rejdls6mhkdbmxf7i2gt 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kkc5yjuj6o994sdfhvxx 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/utsksmhinro6q11mpkk4 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wlg6tgyxnjp5s6w4wgmn 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/yggov0gaxmy85t6a3o5a 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/hnjfopmtcdhb7nuperoz 1530w"><figcaption>图表大部分是从<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>中复制的，其中红色部分是新的。如果“我要站起来”的想法突然出现在我的脑海中，效价就扮演着“规范”控制信号（向上的黑色箭头）的角色，它决定了这个想法是留在我的脑海中并被执行，还是被扔掉并被抛弃。取而代之的是一个不同的想法，比如“……或者实际上我可能会留在床上”。与此同时，绿色箭头发挥了思想生成器用来构建预测/生成世界模型的输入数据的“积极”作用。</figcaption></figure><p>这是一个简单的例子，但却具有普遍性。所以：</p><ul><li>假设我有一个<strong>与执行计划或采取行动</strong>的想法相对应的想法。如果这个想法的效价是积极的，那么我就有责任执行它。如果这个想法的效价是负面的，那么我就不太可能这样做。因此，在这种情况下，<strong>思想的效价对应于我的大脑对该计划或行动的适当性做出快速的规范性评估</strong>。</li><li>假设我有一个<strong>与未来可能的情况</strong>相对应的想法，例如我想象某个候选人赢得选举，或者我想象自己晚餐吃三明治。如果这个想法的效价是积极的，那么我会倾向于制定并执行实现那个未来的计划。如果这种想法的效价是负面的，那么我会倾向于制定并执行<i>阻止</i>未来发生的计划。因此，在这种情况下，<strong>思想的效价对应于我的大脑对这种可能的未来的好坏做出快速的规范性评估</strong>。</li></ul><h2> 2.3.1 更多关于希望或不希望未来情况发生的信息</h2><p>关于“未来可能出现的情况”的第二个要点掩盖了一些值得详细阐述的细节。</p><p><strong>正价案例的详细说明：</strong>考虑这样的情况：我想象自己晚餐吃三明治，这个想法就是正价。今晚我怎么可能（可能）真正吃三明治？两步：</p><ul><li><i>第一步：</i>我可能会想到一个涉及具体行动方案的想法，预计会导致我吃三明治，也许“我要走到索尔那里吃三明治”。这是一个与以前不同的想法，更加具体和可操作，但这种想法<i>也</i>可能是正价的，因为它涉及/与“我晚餐吃三明治的想法”重叠。 （参见下文第 2.4.1 节。）</li><li><i>第 2 步：</i>好的，现在我脑子里有一个具体的行动方案想法（“我要步行到 Saul&#39;s 并在那里吃三明治”），并且它具有正价。这意味着我可能会真正执行该行动方案——我可能会收缩肌肉，站起来，然后开始走向索尔家。</li></ul><p><strong>对负价情况的阐述：</strong>接下来，考虑这样的情况：我想象一些可能的未来情况，并且该想法是<i>负</i>价。</p><p>我在上面写道：“我会倾向于制定并执行计划，以<i>防止</i>这种未来的发生。”但精明的读者可能会想：等一下。我真的会吗？我更有可能<i>完全不再考虑这种可能性</i>吗？这不是价信号的作用吗？</p><p>这是一个很好的问题！事实上，我认为这种反对意见有很多道理，这让我们陷入了这样一种普遍趋势，即在思考为什么自己充满希望的计划可能会失败时投资不足，以及“ <a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>糟糕的领域</u></a>”等等。下一篇文章将详细介绍该主题，尤其是第 3.3.3 节。</p><p>作为可能发生的情况的一个例子，请注意，“避免未来可能发生的<i>糟糕</i>情况 X”有时可以在心理上重新构建为“寻求可能的<i>良好</i>未来情况，而不是 X”——例如，如果您因以下想法而<i>失去</i>动力如果你的敌人赢得了战争，那么你可能同时被敌人被击败的想法所<i>激励</i>。所以也许你会切换到后一种心理框架，让你可以集思广益，而不会立即被负价推走。</p><h2> 2.3.2 更多关于“快速”评估与深思熟虑的评估</h2><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的 §1.5.1</a>中所讨论的，单个计划、行动或未来可能的情况完全有可能具有不同的价，具体取决于我的想法（我关注哪些方面等）。回想一下我之前的例子： <i>“我会去健身房，从而实现我的新年决心”</i>与<i>“我会去又吵又冷又臭的健身房”</i> 。</p><p>所以当我说“快速规范评估”时，“快速”正在发挥重要作用。这与我们在积极领域进行“快速”评估的方式非常相似：</p><ul><li><i>正域（不直接涉及价）：</i>如果我想象一种可能性，我会立即直觉地“突然”感觉到这种可能性是否可能/合理。但我还是可以多想一想，改变主意。</li><li><i>规范域（直接涉及价）：</i>如果我想象一种可能性，我会立即直观地“突然”感觉到这种可能性是好还是坏。但我还是可以多想一想，改变主意。</li></ul><p>在这两种情况下，我都声称“多考虑一下”过程与第一次快速评估并<i>没有</i>本质上的不同。只是想更多的想法，而这些后续想法都伴随着他们自己对合理性和可取性的快速评估。</p><h1> 2.4 概念的效价</h1><p>现在让我们考虑一个不同的情况：假设我想到一个<strong>概念，例如哈马斯、资本主义或超级英雄电影</strong>。这是我的大脑可以思考的事情，所以就像任何想法一样，我的大脑会给它分配一个价。</p><p>如果价是正的，也许我会心想“哈马斯是好的”；如果是否定的，“哈马斯是坏的”。</p><p>这个主题是<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>Scott Alexander 2014 年经典博客文章“种族紧张和毫无意义的争论”</u></a>的主题。但在我们开始之前……</p><h2> 2.4.1 旁注：价作为组合思想的（大致）线性函数</h2><p>作为一个简单且不太不切实际的模型，我建议我们应该将“思想”视为<i>组合的</i>（即基本上由许多相互关联的小片段组成），并且价值函数对这些思想片段是<i>线性相加的</i>。因此，如果一个想法需要想象一只泰迪熊在你的床上，那么这个想法的效价将是你的大脑对“这只特定的泰迪熊”的效价和你的大脑对“一般的泰迪熊”的效价的某种加权平均值，并且你的大脑对“我床上的东西”的效价，以及你的大脑对“模糊的东西”的效价等，权重/细节取决于你如何思考它（例如你正在关注哪些方面，什么）您在脑海中调用的类别/类比等）。</p><p>我为什么要提出这个？好吧，我想谈谈我的大脑赋予概念的效价。例如，“月亮”是你心理世界中的一个概念。它通常是一个更大的想法的<i>一部分</i>，但它本身很少是一个完整的想法。经验丰富的冥想者也许可以将他们的注意力集中在月亮的想法上，但这不是典型的情况！</p><p>在线性模型中，这很好——“月亮”仍然有价。如果该价态非常正，那么与月亮密切相关的想法，例如“我现在正在看月亮”和“有一天我会访问月球”，大概也会以正价结束。</p><h3> 2.4.1.1 线性的（明显）反例</h3><p>精明的读者现在正在向我尖叫。这是一个为亲以色列读者量身定制的（明显的）反例（其他人可以将“以色列”一词替换为“哈马斯”，或选择任何其他例子）：</p><p>为了思考“我将与哈马斯作战”的想法，你的大脑需要以某种方式激活“哈马斯”的概念；但你<i>越不</i>喜欢哈马斯，你<i>就越</i>喜欢对抗哈马斯的想法。这似乎与线性模型明显矛盾。</p><p>在这种情况下，我想说（对于那些亲以色列的读者）“<i>哈马斯作为我义愤填膺的对象</i>”是一个高度激励/正价的概念——即使普通的“哈马斯”概念是消极的/消极的——价。</p><p>或者，您可以通过使用从句和类似的语法结构轻松生成任意多个（明显的）线性反例：如果您被 X 的想法[激励/消极]，那么您将被这个想法[消极/激励] “X即将结束”、“X是假的”、“我反对X”、“我讨厌的外群体真的很喜欢X”，等等。同样，你的大脑在思考这个想法的过程中可能会激活概念 X，但相对于线性模型，对价的贡献是错误的。</p><p>所有这些案例到底发生了什么？长话短说，我<i>确实</i>坚持我的观点，即价是“概念”的（大致）线性函数。但我也认为“概念”可能比我想象的要复杂和微妙得多。</p><p>这就是我要说的关于这个话题的全部内容。我真的不知道这些血淋淋的细节，如果我知道，我绝对不会发布它们，因为我认为这些细节对于构建通用人工智能（AGI）非常有帮助，但对于确保此类 AGI 的安全只有微乎其微的帮助和有益的。有关“信息危害”的更多讨论请参见<a href="https://www.lesswrong.com/posts/MCWGCyz2mjtRoWiyP/endgame-safety-for-agi"><u>此处</u></a>。</p><h2> 2.4.2 概念的效价不能被解释为有意义的规范性评估</h2><p>我在这里引用<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>斯科特·亚历山大的</u></a>话：</p><blockquote><p>当一切都按照哲学教科书上应该的方式进行时，争论应该以以下几种方式之一进行：</p><p> 1. 经验事实问题，例如“地球正在变暖吗？”或“外星人建造了金字塔吗？”。你可以通过提供事实证据来辩论这些问题，例如“全球气象站测量结果的平均值显示 2014 年是有记录以来最热的一年”或“吉萨的一块砖块底部写着‘Tau Ceti V 制造’”。然后人们试图反驳这些事实或提出自己的事实。</p><p> 2.道德问题，例如“堕胎是错误的吗？”或“您是否应该避免下载未付费的音乐？”只有当你们已经就道德框架达成一致时，比如特定版本的自然法或后果论，你才能<i>很好地</i>辩论这些问题。但你可以通过与商定的道德问题的例子进行比较并试图保持一致性来对它们<i>进行</i>辩论。例如，“你不会杀死一天大的婴儿，那么九个月大的胎儿有什么不同呢？”或“你不会下载<i>汽车</i>。”</p><p>如果你很幸运，你的哲学教科书也会承认以下内容的存在：</p><p> 3.政策问题，例如“我们应该提高最低工资”或“我们应该轰炸外国人”。这些是相互竞争的事实主张和相互竞争的价值观的组合。例如，最低工资可能取决于“提高最低工资会增加失业率”或“现在靠最低工资生活很困难，许多贫困家庭买不起食物”等事实主张。但它也可能取决于诸如“企业有义务为工人支付生活工资”或“保护最贫困人口比经济强劲更重要”等价值主张。轰炸外国主义者可能依赖于诸如“外国主义者窝藏恐怖分子”之类的事实主张，以及诸如“我们人民的安全值得冒附带损害的风险”之类的价值主张。如果您能够解决所有这些事实和价值主张，您应该能够就保单问题达成一致。 ……</p><p>问：你支持以色列还是支持巴勒斯坦？花点时间，认真思考一下。</p><p>有些人可能会回答亲以色列。其他人可能会回答支持巴勒斯坦。其他人可能会说他们是中立的，因为这是一个复杂的问题，双方都有好的观点。</p><p>大概很少人回答：<i>嗯？什么？</i></p><p>这个问题不属于哲学 101 三种论证形式中的任何一种。这不是事实问题。这不是特定道德真理的问题。这甚至不是政策问题。还有密切相关的政策，例如巴勒斯坦是否应该获得独立。但如果我支持一个非常具体的两国解决方案，其中边界是在平行的东西上划定的，这会让我支持以色列还是支持巴勒斯坦？所考虑的解决方案到底在哪条边界线上从亲以色列转向亲巴勒斯坦？你认为那些高喊“声援巴勒斯坦”、挥舞标语的人群能回答这个问题吗？ ……</p></blockquote><p> （如果我们从政治转向日常生活，“政策”类别概括为“具体计划和行动”。例如，关于“我们应该去酒吧”的分歧可能涉及“酒吧会拥挤吗？”等事实主张。 ”，以及诸如“跳过合唱团练习是不是很粗鲁？”之类的价值主张。）</p><p>好吧，如果亲以色列或反以色列的态度并不代表有意义的规范性评估，那么它代表什么<i>呢</i>？</p><h2> 2.4.3 概念的效价实际上是一种习得的规范启发式</h2><p>“启发式”是一种用于近似某事物的快速算法。回想一下上面第 2.3 节中肯定性与规范性的区别。许多启发式，例如<a href="https://en.wikipedia.org/wiki/Representativeness_heuristic"><u>代表性启发式</u></a>，可以被称为“积极启发式”，因为它们是用于找出“积极”事物（例如事实预测）的启发式。其他启发法可能被称为“规范启发法”，因为它们是生成（有意义的）规范评估的启发法。</p><p>我认为，如果你的大脑为“以色列”这样的概念分配了一个价，那么它就对应于一种规范启发式——更具体地说，是一种<i>习得的</i>规范启发式，即你的大脑从一生的经历中获得的规范启发式。</p><p>继续以色列的例子，将概念价转换为其相应的规范启发式的方法是：</p><blockquote><p> “‘以色列’这个概念在我心中具有正价”对应于以下启发式，让您的大脑生成快速规范评估：</p><ul><li><i>可能的行动或计划</i>涉及以色列/与以色列的模式匹配越多，执行该行动或计划就越有可能是我想做的事情；</li><li><i>未来世界的可能状态</i>越多地涉及以色列/与以色列的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</li></ul><p> （对于负价则相反）。</p></blockquote><p> （正如第 2.4.1.1 节中所讨论的，“涉及以色列/与以色列进行模式匹配”短语掩盖了<i>一堆</i>复杂性！）</p><p>因此，如上所述，这些化合价本身并不是有意义的规范性评估，这仍然是事实。但它们<i>只是</i>有意义的规范评估的<i>上游</i>。</p><h2> 2.4.4 理解基于共鸣的“无意义的争论”</h2><p>鉴于上述情况，我们现在可以进入我上面摘录的<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>“种族紧张和毫无意义的争论”博客文章的</u></a>下一部分：</p><blockquote><p>这是《种族紧张：两个玩家的游戏》。</p><p>选择一个模糊的概念。 “以色列”目前表现不错。</p><p>玩家 1 试图将“以色列”这个概念与尽可能多的善业联系起来。概念通过做良好的道德事情、通过与好人交往、通过与心爱的群体联系、通过<a href="https://slatestarcodex.com/2013/05/18/against-bravery-debates/"><u>在勇敢的辩论中</u></a>受到压迫的弱者来获得良好的业力。</p><p> “以色列是中东最自由、最民主的国家。它是美国最强大的盟友之一，与我们有着共同的犹太基督教价值观。</p><p>玩家 2 试图将“以色列”这个概念与尽可能多的恶业联系起来。概念通过犯下暴行、与坏人有联系、与可憎的外群体有联系、在勇敢辩论中成为压迫性的大人物而获得恶业。此外，她显然需要通过反驳她的所有论点来抵消玩家 1 的行为。</p><p> “以色列可能为其最享有特权的公民提供一定程度的自由，但被占领土上数百万没有发言权的人呢？以色列参与了各种暴行，并经常杀害无辜抗议者。他们本质上是一个新殖民主义国家，并与南非等其他新殖民主义国家结盟。”</p><p>赢得这场游戏的奖励是能够赢得其他三种类型的争论（即关于事实、道德和政策的争论）。</p></blockquote><p>我认为，鉴于上一节“习得的规范启发法”，此类论证更有意义。</p><p>具体来说，玩家 1 和玩家 2 正在争论以下规范启发式的适当性：</p><p><i><u>争议中拟议的启发式：</u> “可能的行动或计划涉及/与“以色列”的模式匹配越多，执行该行动或计划就越有可能是我想做的事情。同样，未来可能的世界状态越多地涉及/与“以色列”的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</i></p><p> （再次参见第 2.4.1.1 节中的注意事项）</p><p>玩家 1 促使听众思考上述启发式方法非常有效的许多想法，即它给出的答案与听众大脑中已有的所有其他规范启发式一致。这会刺激听众的大脑比以前更加依赖这种启发式方法。</p><p>相反，玩家 2 会促使听众思考上述启发式方法效果极差的想法，即它给出的答案与听众大脑中已有的所有其他规范启发式完全相反。这会刺激听众的大脑比以前更少地依赖启发式，甚至开始依赖完全相反的启发式（也称为负价）。</p><p> （此过程的更多机制细节参见下面的 §2.5。）</p><h2> 2.4.5 我们是否应该“反”一般的规范启发法？</h2><p>我经常很恼火地看到人们争论是否支持资本主义与反资本主义、支持宗教与反宗教等等，好像这些是有意义的争论，但其实并非如此。</p><p> （有趣的是，我有一个朋友，我们两个人倾向于在经济政策的<i>具体</i>问题上达成一致。然而，我们在氛围上存在强烈分歧——“资本主义”这个概念在他们的大脑中具有强烈的负面氛围，而在我的大脑中则有积极的氛围）基本上，我们的共鸣分歧大部分被关于什么是或不是“资本主义”的平等和相反的分歧所抵消！）</p><p>这类毫无意义的争论有时是明确的，但更多时候它们隐藏在表面之下，争论在物体层面和共鸣层面同时发生。</p><p>例如，如果有人认为他们只是“得了一分”，那么这个事实经常会出现在他们的言语中，即使他们没有站出来明确宣布“将死，无神论者！”在这些情况下，值得询问他们认为自己得分<i>的具体是什么</i>，因为也许那件事一开始就不是分歧的主题。</p><p> （不用说，这对自己来说是双重的：照照镜子，问问<i>你</i>在争论什么具体的事情！）</p><p> （“得分”态度首先是有问题的——请参阅<a href="https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556"><i><u>《童子军心态》</u></i></a> ——但争论一个非常具体、具体、相互理解的事情是最不坏的版本。）</p><p>当我们的大脑为大而模糊的事物分配价时，规范启发法尤其成问题。例如， <i>“涉及/与‘宗教’模式匹配的东西”</i>是一个极其异构的集合——从古代神道仪式到我曾经在梵蒂冈礼品店购买的以教皇为主题的旅游饰品。经过反思后，这个集合中的所有内容都是好还是坏的可能性有多大？相当低。 （参见<a href="https://www.lesswrong.com/tag/bucket-errors"><u>“桶错误”</u></a> 。）因此，如果我们对“宗教”给予强烈的评价，无论是积极的还是消极的，我们都会遇到以下两个问题之一：</p><ul><li>我们会对特定的宗教相关事物做出不好的规范性评估（“不好”是指，如果我们更仔细地思考这个问题，我们就会改变主意）；</li><li> ……或者我们最终会对“宗教”概念进行不公正的划分，以符合我们认可的规范性评估。 （例如，有人可能会说“宗教当然是[好/坏]；但是某某[坏/好]的东西并不是<i>真正的</i>宗教。”）</li></ul><p>我认为后者对于清晰的思维非常有害。拥有<i>一些</i>明确涉及规范性评估的概念（“优选”、“有问题的”、“麻烦”、“蓬勃发展”等——请参阅<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_4_Terms_closely_related_to__valence_">上一篇文章</a>或<a href="https://arbital.com/p/value_laden/"><u>相关仲裁帖子</u></a>中的§1.4）是很好的，因为毕竟规范性评估是我们经常想谈论的事情。但我们<i>也</i>迫切需要能够描述世界如何独立于我们的感受而运作的概念。我将在下一篇文章的第 3.4 节中详细讨论这个主题。</p><p>话虽如此，我们不能总体上“反”规范启发法，因为如果我们把它们拿走，那就什么都没有剩下了！毕竟，大脑需要<i>某种</i>方式来评估计划/行动/未来的好坏，包括我们以前从未遇到过的计划/行动/未来。</p><p>所以实际上，我认为给概念赋予价是可以的，甚至给比较宽泛的概念赋予相当强的价在某种程度上也是可以的。我自己的大脑似乎至少为一些相当广泛的概念分配了一些非零价，例如“共产主义”（嘘）、“贫困”（嘘）、“人工智能对齐”（耶）、“冰淇淋”（耶）和等等，我目前认为我不应该太努力地改变这一点。重要的是<i>养成提出后续问题的习惯，这些问题侧重于细节并探究一般规则的例外情况</i>。</p><p>例如，问问自己：确切的建议是什么？它可能产生哪些有意或无意的后果？当我们说“共产主义”时，我们是在谈论“斯大林的共产主义”还是“邓小平的共产主义”还是“狩猎采集小部落的共产主义”？当我们说“冰淇淋”时，我们是在谈论“哈根达斯冰淇淋”还是“海绵宝宝冰棒冰淇淋”？当我们说“AI 对齐”时，我们是在谈论“我定义的 AI 对齐”还是“Sam Altman 定义的 AI 对齐”？ ETC。</p><p>如果<i>具体细节</i>在你的脑海中很突出，那么，我声称，相应的规范启发法更有可能给出良好的评估，经得起审查和事后诸葛亮。</p><p> （顺便说一句，如果你养成这种习惯，<i>随着时间的</i>推移，它可能会导致为模糊的广泛概念分配更中性的效价，为更具体的特征分配更强的效价。这可能是一件好事。）</p><h1> 2.5 化合价如何设定和调整？ </h1><figure class="image image_resized" style="width:67.18%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/jszcdax7vbhbxebhlau4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rdxaoi3kfhm2tiase154 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/o8znjxkggojkxgvvtjlf 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvibz9spxvn4gz3maoqb 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/eld6v5v4fypxs0bqcaj2 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/lqz98ny7ffp9uhqwgfbt 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/frghxaaqbnc8xxwt7joc 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/s1uubt4yekrh91tiaxro 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/pkxccpamapexa8elztd7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/w6cksg1dlrxa9lcemytv 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/stsl40xxhye7ovycuvls 955w"><figcaption>我声称效价最终来自于先天驱动力，然后通过诸如时间差异（TD）学习之类的东西在不同的概念（思想片段）之间传播。举第一个例子，如果我的手在接触炉子时被烫伤，那么一种与生俱来的驱动力就会介入，并表示有不好的事情发生了，然后触摸炉子的想法就会获得负价。作为第二个例子，考虑<a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>“呃字段”</u></a> ——如果我经常对自己想“我要清理我的电子邮件收件箱”，并且这总是立即产生高度厌恶的后续想法“......这意味着我需要处理来自内德的烦人的电子邮件”，那么“我要清理我的电子邮件收件箱”的想法<i>本身</i>就会变得有些厌恶（负价），即使我没有明确地提醒我记住来自内德的烦人的电子邮件。</figcaption></figure><p>如果你追溯到足够远的地方，我声称<strong>所有效价最终都直接或间接地来自先天驱动力</strong>——基因特定的电路，宣布疼痛是坏的，饥饿时吃东西是好的，并驱动社会地位（即将在第 4 篇文章中出现），以及与同情心、复仇和内疚等相关的各种其他社会动力，可能还有数百个类似的事情。</p><p> （在强化学习术语中——参见<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章的第 1.3 节</a>——你的“先天驱动力”或多或少对应于有助于奖励功能的各种术语。用神经解剖学术语来说， <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>我声称</u></a>实现这些先天驱动力构成了下丘脑和脑干，尽管其他大脑区域也参与其中。）</p><p>稍微简单地说，我认为一个想法可以是正价或负价有两个主要原因：</p><ul><li>一个想法可以是正价或负价的<strong>第一个原因</strong>是，一种内在的驱动力正在介入并立即宣布该想法是好还是坏。 （例如，如果我饿了并且开始吃美味的食物，那么无论我当时在想什么——大概是食物——显然都是一件好事。）</li><li>一个想法可以是正价或负价的<strong>第二个原因</strong>是，我们的大脑有一个<i>学习算法</i>，可以根据实际价分配的过去历史来猜测什么价分配最适合一个想法。当先天的驱动力没有<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_2_Toy_model_of_a__long_term_predictor__circuit"><u>“压倒”这种猜测</u></a>时，这种猜测就会变成一种自我实现的预言。 （学习算法与<a href="https://en.wikipedia.org/wiki/Temporal_difference_learning"><u>时间差异（TD）学习</u></a>有一定关系；我将保留所有细节，但请参阅<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_3_1_Switch__i_e___value___expected_next_reward__vs_summation__i_e___value___expected_sum_of_future_rewards__"><u>此处</u></a>了解更多信息。）</li></ul><p>抛开所有机制细节不谈，这种学习算法具有多种效果，例如：</p><ul><li>如果思想 θ 在您之前的所有思考中都具有一定的价态，那么学习算法将开始猜测 θ（以及与 θ 足够相似的思想）将来将具有相同的价态。</li><li>如果X（例如打开Twitter）经常立即导致Y（例如看到“喜欢”通知），即作为现实世界中的时间序列，那么学习算法将开始猜测X的适当价数更接近于任何值化合价恰好是 Y。</li><li>同样，如果想法 X 通常会立即在您的大脑中引发想法 Y，那么请与上一个要点相同。 （换句话说，TD学习的“时间”部分可以完全在你大脑的思维范围内，与外界的时间序列无关。）</li></ul><p>为了避免可能的误解：先天驱动力与目标或最终目标不同。你的目标，例如“摆脱债务”或“减少痛苦”，是根据你的世界模型（又名<a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"><u>“思想发生器”</u></a> ）中的概念构建的——你在一生中学到的概念。与生俱来的驱动力并非如此，相反，它们是由基因组构建的小机器，在某些情况下会发出某些信号。我有时会谈论“避免疼痛的本能驱动力”或类似的东西，但这实际上是更迂腐的“电路的简写，该电路检测与我们通常所说的“疼痛”相关的外围信号，然后吐出负价（以及其他信号），因此随着时间的推移，该电路的典型效果是让我们想要避免疼痛”。</p><h2> 2.5.1 特殊情况：在一起的事物往往会获得相似的化合价</h2><p>在<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的第 1.5.1 节</a>中，我举了一个关于去健身房“心情复杂”的例子。如果我关注它的一个方面，它就会具有积极的价值（“我要去健身房，从而实现我的新年决心”）。如果我注意它的另一个方面，它具有负价（“我要去健身房，那里真的又吵又冷”）。</p><p>可能我最终会在一两秒内连续地在这两个帧之间来回进行大量的心理翻转。换句话说，当我思考积极的方面时，也许它会唤起我的记忆，唤起消极的方面，反之亦然。如果是这样，我认为 TD 学习将倾向于更新我的价函数，使正价方面变得更加消极，反之亦然（根据上面的第三个要点）。具体来说，也许执行我的新年决心的想法会开始感觉有点不那么激励和更令人厌恶，相反，也许想到健身房的响亮和寒冷会开始感觉<i>不那么</i>令人厌恶。 <span class="footnote-reference" role="doc-noteref" id="fnref6q9syee1mov"><sup><a href="#fn6q9syee1mov">[2]</a></sup></span></p><h1> 2.6 元认知和自我反思思想的效价：自我和谐与反张力的思想、公开的“价值观”等。 </h1><figure class="image image_resized" style="width:24.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tz2aw0v6l0ei9pwnwnz4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/mofolnogceg3ar9d7u8p 115w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xnxqdmknjlugrtvr7wge 195w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/gk8llgybcvjpjzyng91k 275w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/ryolrrtufvajtuceshqi 355w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xdon4zacamsmpzahxjxi 435w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/qxwq1byfvsee3tji0joz 515w"></figure><p>假设我对香烟上瘾，但想戒掉。鉴于我对香烟上瘾，有时“现在吸烟的想法”将是非常正价的——换句话说，“有时我感到有吸烟的冲动”——因此我会抽烟。</p><p>但这很烦人！我本来想放弃的！因为我不是一个彻底的白痴，所以我自然会注意到一个明显的事实：“我想吸烟”是一个阻碍我实现戒烟动机的障碍。这让我不喜欢“冲动我想吸烟”。</p><p>现在我们已经进入了自我反思偏好的领域！</p><p>因此，“吸烟有时对我来说是正价的想法”最终在我的脑海中产生了<i>它自己的</i>价评估，而这个价是强烈的负值。这将是<i><strong>自我张力失调</strong></i><strong>效价评估</strong>的一个例子。我想抽烟，但又<i>不想</i>抽烟。</p><p>现在，再次假设一个想法突然出现在我的脑海中：“也许我现在就去抽烟”。如果这个想法需要注意预期的感官知觉等，那么这个想法的效价将是积极的，所以我会吸烟。如果这个想法主要是忽略这些，而是​​关注情况中更多的自我反思方面——也许，好像从外部看待自己——那么这个想法的效价将是负面的，所以我不会吸烟。实际上，两种类型的想法可能会在不同的时间突然出现在我的脑海中，而效价的相对强度将有助于确定事情如何发展。</p><p>自我失调的反义词是<strong>自我和谐</strong>。例如，“我自己倾向于诚实和尽责”的想法就是自我和谐的。因此，如果一个计划的模式与我认为自己是诚实和认真的相匹配，那么该计划就会获得效价提升，所以我更有可能这样做。但只是<i>在边缘</i>。该计划还有其他方面，这些方面也有助于提高效价，事实上，这些其他方面往往更为突出。</p><p>这里的一个重要观察是，<i><strong>所宣称的</strong></i><strong>目标和价值观，而不是行动，往往不成比例地取决于事物是自我和谐还是失调。</strong>考虑一下：如果我大声（或对自己）说某件事（例如“我要戒烟”或“我关心我的家人”），我脑海中实际立即想到的主要是“我要执行此操作”特定言语行为”。正是<i>这种</i>想法的效价决定了我们是否会说出这些话。这种想法的自我反思方面非常突出，因为说话需要思考听众会如何接受你的话。相比之下，该公告的<i>内容</i>——真正戒烟，或者真正关心我的家人——既不那么显着，也不那么直接，发生在某个不确定的未来（参见<a href="https://en.wikipedia.org/wiki/Time_preference"><u>时间折扣</u></a>）。因此，言语行为的净效价可能包含来自戒烟的自我反思方面的<i>较大</i>效价贡献，以及来自戒烟或关心家人的更直接的感官和其他后果的<i>较小</i>效价贡献。<i>即使</i>我们百分百真诚地履行我们所说的话，情况也是如此。 （另请参阅博客文章<a href="https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors"><u>“批准强化省力行为”</u></a> ，其观点与本段类似。） </p><figure class="image image_resized" style="width:73.71%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rcvfrjif77p3aam2aw6c" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kqpsl8ljw1xjkggyp3tu 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/myrfhtlsewfnonpmlljq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/nnta32ypxvzvhic2uy14 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wy70orgjcuigufl8noxn 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xenavi7epj1nps9hnji5 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zjwci8fwzhzmtkphmrz4 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xkztuwtxn2lashfztqum 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/n87ivhqtqmwtvlinnjed 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/l0tuyylx95lsytazgsrl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/p6x3gfovyynzxfhia9vj 866w"></figure><h2> 2.6.1 什么是“价值观”？</h2><p>我提出这个话题是因为它经常出现在我的职业中（人工智能对齐）。但我不确定它有什么深刻的答案；在我看来，这主要只是关于定义的争论。</p><p>这是我喜欢的“价值观”的定义，因为它似乎与人们在我日常生活中使用该词的方式一致：<i>如果我想知道某人的“价值观”，我只会问他们，然后写下他们所说的。然后我会声明，无论我写下什么，或多或少都是“他们的价值观”，除了极少数例外，比如他们故意恶搞我。</i></p><p>根据这个定义，“价值观”可能包括听起来非常好听、得到社会认可、自我认同的东西，比如“照顾我的家人和朋友”、“让世界变得更美好”等等。 。</p><p>同样根据这个定义，“价值观”对某人的行为可能产生的影响微乎其微。在这种（极其常见）的情况下，我会说“我猜这个人的<i>愿望</i>与他的<i>价值观</i>不同……”。哦，好吧，这并不奇怪。”</p><p>事实上，我认为对于一个“价值观”包括“成为好朋友”的人来说，实际上成为一个坏朋友是完全正常的。那么这个“价值”有什么含义吗？是的！！我预计，在这种情况下，这个人<i>要么</i>会对自己是一个坏朋友这一事实感到难过<i>，要么</i>否认自己是一个坏朋友，<i>要么</i>根本不考虑这个问题，<i>要么</i>想出其他一些办法。为他们的行为找借口。如果这些事情都没有发生，那么（只有那时）我才会说“成为一个好朋友”实际上<i>并不是</i>他们的“价值观”之一，如果他们另有说法，那么他们就是在撒谎或困惑。</p><h1> 2.7 道德推理</h1><h2>2.7.1 描述性帐户</h2><p>如上所述，我认为积极与规范的区别与大脑的大规模解剖结构和算法的主要组成部分有着非常清晰的对应关系。</p><p>然而，<i>在规范领域内</i>，我认为<i>“X是道德/伦理/美德/等等要做的事情”</i>和<i>“我想做X”</i>之间的区别并不是太根本——它更像是模糊学习类别之间的界限。例如，当事物与自我和谐且得到社会认可时，人们倾向于将其称为道德/伦理/美德。但该规则也有例外。例如，黑手党可能会为自己的“不道德”感到自豪。即使在这种情况下，黑手党可能仍在弄清楚在他自己的犯罪社会利基中什么是自我和谐的和社会认可的，他的思维甚至可能在结构上类似于“正常”的道德推理。但他不会使用“道德”这个词。</p><p>无论如何，假设您是一个正常的守法人，真诚地希望按照道德行事。所以你坐在扶手椅上，试着弄清楚什么是道德的事情。接下来发生什么？很多东西：</p><ul><li>也许您会考虑对象层面的事情，就像您想象一个孩子正在受苦一样。也许这些想法是有价值的——也就是说，你发现你对这些事情有偏好。</li><li>也许你会考虑元认知的事情，比如“我会仔细思考，而不是急于做出判断”，或者相反，“我应该更相信自己的直觉”。也许这些想法是有价值的——也就是说，你发现你对这些事情有偏好。</li><li>也许你会思考一些自我反思的事情，比如<a href="https://joecarlsmith.com/2023/02/17/seeing-more-whole"><u>“我在这个世界上是什么样的力量？”</u></a>或者“如果我的朋友现在能读懂我的想法，他们会怎么看我？”。也许这些想法是有价值的——也就是说，你发现你对这些事情有偏好。</li><li>也许您注意到其中一些偏好与其他偏好不一致。然后你会思考更多的想法，这些不同的偏好可能会相互较量。</li><li>在整个过程中，你大脑的学习算法会不断更新你的信念和欲望。</li></ul><p>在本系列的背景下，我想指出的一点是，<strong>每一步都建立在价的基础上。</strong>例如，如果你的大脑没有为孩子受苦的想法赋予特定的效价，那么你就不会有动力去阻止孩子受苦。</p><p>为了更直观：假设我告诉你：“穿带偶数个皮带环的裤子很重要”，然后你说“为什么？”，然后我说“你知道，如果你没有，那么你的裤子会有<i>奇数</i>个皮带环！那会很糟糕！！”，然后你又说“嗯？为什么这么糟糕？”。这场谈话毫无进展。</p><p>相比之下，如果我说“如果你遵循X原则，那么被压迫群众将得到正义”，<i>现在</i>我可以激起你的兴趣，因为“被压迫群众将得到正义”已经作为一种吸引人的（积极的-价）的概念。</p><p>正如上面提到的（§2.5），我想指出的另一个重要观点是<strong>，价最终（尽管通常是非常间接的）源于先天驱动力</strong>。正如第 2.5 节中所述，在你生命中醒着的每一秒，效价都根据你的观察和思路从一个概念流向另一个概念。但如果你追溯得足够远，效价最终必须源自你与生俱来的驱动力。在道德推理的情况下——例如上面“被压迫的群众将得到正义”的例子——我认为所讨论的先天驱动力几乎肯定是先天的<i>社会</i>驱动力，即与同情心、复仇、社会地位相关的先天驱动力（出现在第四篇文章中） ）， 等等。</p><h2> 2.7.2 对“道德的真实本质”的影响（如果有的话）</h2><p>我距离元伦理学专家<i>还很远</i>，但从上述叙述中似乎可以得出以下一些结论。</p><ul><li>如果存在“真正的道德”这样的东西，它<i>完全独立</i>于人类先天驱力和当前模因环境的特殊性，那么就没有任何明显的理由表明上述过程应该收敛于它，或者确实有任何关系用它。如果确实如此，我想我们就不得不宣布这是一个令人愉快的巧合。 （有关这一点的更深入的讨论，请参阅 Joe Carlsmith 的文章<a href="https://joecarlsmith.com/2022/01/17/the-ignorance-of-normative-realism-bot"><u>“规范现实主义机器人的无知”</u></a> 。）</li><li>事实上，没有任何明显的理由表明上述过程应该收敛到<i>任何</i>明确定义的目的地。我<i>不</i>认为这与数学相似。在数学中，如果有一个假设的代理人具有“天生的倾向”来“喜欢”某些数学公理及其逻辑结果，那么对于该代理人会或不会“喜欢”什么，就会有一个独特的答案，只要有足够的时间来反映——即，可以从这些公理证明的数学陈述。相比之下，如果一个人从天生的驱动力开始，并经历上述过程，那么事情就会显得更加混乱。例如，一旦你开始“喜欢”某种元认知模式，它就会改变管理未来审议步骤的基本规则。因此，过程结束的地方很可能存在不确定的路径依赖性。或者它可能最终会围绕各种不太令人满意的状态进行无休止的循环。</li><li>即使上述过程<i>确实</i>收敛到一个独特且明确的目的地，那么它不一定对每个人来说都是相同的目的地（并且忘记外星人或人工智能）。事实上，我会更进一步猜测，对于不同的人来说，这可能有重要的不同，不仅是在反社会人格者等不寻常的情况下，而且在整个人群中也是如此。 （例如，我认为 <a href="https://www.lesswrong.com/posts/BkkwXtaTf5LvbA6HB/moral-error-and-moral-disagreement"><u>这篇文章</u></a>的结尾过于乐观。）我认为的原因是：目的地<i>肯定</i>取决于正在深思熟虑的人的天生驱动力，也<i>可能</i>取决于他们的生活史，尤其是模因环境。让我们关注第一个——先天驱动力。我承认几乎所有人类都有<i>本质上</i>相似的先天驱动力——也就是说，他们的构造方式是相同的。但我认为它们在<i>数量上</i>并不相似。特别是，各种不同的先天驱动力的相对强度似乎因人而异。例如，我认为有些人非常强烈地感受到地位驱动力（在第四篇文章中出现），而其他人则感觉更温和，与同情心、复仇等相关的先天驱动力也是如此。这些相对优势很重要，因为道德充满了不同直觉相互抵消的情况。</li><li>比较一下“我希望被压迫的群众得到正义”和“我站得太久了，我想坐下来”。<i>这两种“想要”从根本上来说是建立在相同的思维基础上的。</i>它们都源自正价，而正价最终又来自先天驱动力（具体来说，第一种情况主要是社会驱动力，第二种情况主要是稳态节能驱动力）。因此，如果“真正的道德”或“真正的人类道德”或其他什么不存在，那么这<i>并不</i>构成坐下来而不寻求正义的理由。你仍然需要做出决定。这就是我所说的<a href="https://www.lesswrong.com/posts/32ca3B7rJ93xo9tvb/thoughts-on-agi-consciousness-sentience#How_does_that_feed_into_morality_"><u>“虚无主义与决策无关”</u></a> ，或者尤德科夫斯基所说的 <a href="https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality"><u>“没有道德你会做什么？”的</u></a>意思。 。事实上，你会注意到我首先讨论了审议过程，然后才是哲学。深思熟虑的过程是由你自己的动机驱动的，无论它们是什么，无论好坏，而不是任何外部规范真理。</li></ul><h2> 2.7.3 对人工智能联盟讨论的可能影响</h2><p>作为一名人工智能一致性研究人员，我可能应该根据当地知识界的想法准确地阐明我所得到的内容。<i>这一部分的可信度较低，并且充满了行话；人工智能对齐领域之外的读者可以随意忽略。</i></p><ul><li>我担心<a href="https://www.lesswrong.com/tag/coherent-extrapolated-volition"><u>CEV</u></a>没有明确定义。或者更具体地说，您可以列出许多<i>同样先验</i>合理的 CEV 详细操作，它们会给出截然不同的结果，而我们会发现非常不满意。</li><li>与此相关的是，我担心“<a href="https://forum.effectivealtruism.org/topics/long-reflection"><u>长反思</u></a>”不会解决我们希望它解决的所有重要问题，或者以一种不可避免地取决于长反思治理/话语规则细节的方式解决它们，而没有确定众多看似合理的治理/话语规则中哪些规则是“正确的”的明显方法。</li><li>当人们做出含蓄地将“未来的价值”视为明确定义的陈述时， <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>例如</u></a>“我将‘强大的乌托邦’定义为：未来至少 95% 的潜在价值已实现”，我担心这些说法并不像听起来那么有意义。</li><li>我担心几代人以来人类价值观的变化在某种程度上更像是随机游走，而不是随着时间的推移而进步，而且它们只是感觉像随着时间的推移而进步，因为我们<a href="https://willywizz.wordpress.com/2012/09/24/paint-the-target-around-the-arrow/"><u>“围绕着目标绘制目标”箭”</u></a> 。因此，当我们说“永恒的价值锁定是不好的——我们想给我们的后代道德成长的空间！”，同时我们也说一些具体的话，比如“我们想要一个充满友谊、玩耍和感觉的未来——代理和探索，以及很少的痛苦和磨难，而且……！”，那么我担心这两种说法至少有点不一致，甚至可能非常不一致。 （如果事实证明我们必须选择这两种说法中的一种，我不知道我会投票支持哪一种。）</li><li>以上这些都不是我们不应该尝试解决人工智能对齐问题，或者我们不应该关心未来如何发展的论点。与上一小节一样，我们仍然可以而且必须做出决定。我渴望避免充满酷刑和奴隶制的未来宇宙（以一个特别的榜样）一如既往地强烈，如果您分享这种愿望，那么让我们一起努力使它实现。</li></ul><h1> 2.8 结论</h1><p>同样，我认为价是在大脑中建立所有规范性的物质。希望这篇文章能阐明这些难题的一些碎片。我愿意接受思想和讨论，在下一篇文章中，我们转向了较小但仍然存在实质性的影响（与规范性的）域名的信念，期望，概念等。</p><p><i>感谢TSVI Benson-Tilsen，Seth Herd，Aysja Johnson，Justis Mills，Charlie Steiner，Adele Lopez和Garrett Baker对早期草稿的批判性评论。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fntss4t9kg1lr"> <span class="footnote-back-link"><sup><strong><a href="#fnreftss4t9kg1lr">^</a></strong></sup></span><div class="footnote-content"><p> “ Bulverism”是一个术语，其意思是“假设某些信念是不正确的，然后就人们如何持有那些不正确的信念的人进行心理猜测”。但是，有关道德现实主义的<i>更多</i>信息，请参见下面的§2.7。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6q9syee1mov"> <span class="footnote-back-link"><sup><strong><a href="#fnref6q9syee1mov">^</a></strong></sup></span><div class="footnote-content"><p>在这个示例中 - “也许遵循我的新年决议的想法会开始变得有些动机，更厌恶，相反，也许会思考健身房的响亮和寒冷，会开始感到<i>有些</i>厌恶。” - 我实际上认为这种转变可以通过两种方式发生，并且大脑同时进行了这两个过程。文本中描述的第一个涉及通过TD学习对不同思想的价变化。我不会讨论的第二个是因为它是较少的主题，涉及世界模型（又称“思想生成器”）及其概念网络及其关联 /内涵的变化。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sqgrtcwueovvwxpdq/valence-series-2-valence-and-normativity<guid ispermalink="false"> SQGRTCWUEOVVWXPDQ</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 07 Dec 2023 16:43:49 GMT</pubDate> </item><item><title><![CDATA[AISN #27: Defensive Accelerationism, A Retrospective On The OpenAI Board Saga, And A New AI Bill From Senators Thune And Klobuchar]]></title><description><![CDATA[Published on December 7, 2023 3:59 PM GMT<br/><br/><p>欢迎通过AI安全<a href="https://www.safe.ai/">中心来到AI安全</a>通讯。我们讨论了AI和AI安全方面的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此处</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify上免费收听AI安全通讯。</a></p><hr><h2>防御加速度</h2><p>以太坊的创建者Vitalik Buterin最近<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">写了一篇关于AI和其他技术的风险和机会的文章</a>。他回应了马克·安德森（Marc Andreessen）关于技术乐观主义的宣言和有效的加速主义（E/ACC）运动的增长，并提供了更加细微的观点。</p><p>这篇文章认为，技术通常对人类非常有用，但是AI可能是该规则的例外。布特林认为，与其赋予政府对AI的控制权，以保护我们，不如说我们应该建立防御技术，以防止在分散的社会中为灾难性风险提供安全性。网络安全，生物安全性，弹性的物理基础设施和强大的信息生态系统是Buterin认为我们应该建立的一些技术，以保护自己免受AI风险。</p><p><strong>技术有风险，但法规不是灵丹妙药。</strong>更长的寿命，较低的贫困率以及扩大的对教育和信息的机会扩大，这是Buterin对技术的众多成功。但是大多数人会认识到技术也会造成危害，例如全球变暖。 Buterin特别<a href="https://twitter.com/VitalikButerin/status/1729251822391447904">说</a>，与大多数技术不同，AI对人类构成了生存威胁。</p><p>为了应对这种风险，一些人主张<a href="https://arxiv.org/abs/2310.09217">强大的政府控制</a>人工智能发展。 Buterin对此解决方案感到不舒服，他希望许多其他解决方案也会如此。许多历史最严重的灾难是由斯大林和毛等强大的政治人物故意进行的。 AI可以帮助残酷的制度监视并控制大量人口，而Buterin通过将AI开发从私人实验室推向公共实验室来谨慎行动。</p><p>在不受约束的技术发展的极端和绝对的政府控制之间，布特林倡导了一条新的途径。他称他的哲学D/ACC，“ D”代表国防，民主，权力下放或<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670">差异技术发展</a>。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc365b23e-2340-47ba-8f6b-04cf5e6efa97_500x536.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/zm2qw8smimbll3172z6y 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ci9uhres7aqircceej9n 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/gwdpnppfzrthwzyxb5kq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg 1456w"></a></p><p><strong>去中心化世界的防御技术。</strong>布特林主张将保护社会免受灾难性风险的技术加速。具体来说，他强调：</p><ol><li><strong>生物安全</strong>。茂密的城市，频繁的航空旅行和现代生物技术都增加了大流行病的风险，但是我们可以通过改善空气质量，加快疫苗和治疗疗法的发展以及监测新兴病原体来改善生物安全。</li><li><strong>网络安全</strong>。可以在网络攻击中使用可以使用代码的AI，但是在被剥削之前，防御者也可以使用它们来查找和修复安全缺陷。布特林（Buterin）在区块链上的工作旨在实现某些数字系统可以证明是安全的未来。</li><li><strong>弹性物理基础设施</strong>。核灾难中的大多数预期死亡将不是爆炸本身，而是来自饮食，能源和其他必需品的供应链。埃隆·马斯克（Elon Musk）渴望通过使我们减少对化石燃料的依赖，通过卫星提供互联网连接，并理想地使人类成为一个可以超过地球上的灾难的多人物种，从而改善了人类的身体基础设施。</li><li><strong>强大的信息环境</strong>。为了帮助人们在AI说服时期找到真相，Buterin指出了预测市场，并建立了共识的算法，例如<a href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">社区笔记</a>。</li></ol><p>科学家兼首席执行官可能会发现自己受到布特林（Buterin）建立技术的目标而不是减慢它的启发。然而，对于那些关注AI和其他灾难性风险的人来说，Buterin对最有可能确保我们文明安全的技术提供了深思熟虑的看法。对于那些感兴趣的人，<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">本文</a>中还有更多想法。</p><h2>在Openai董事会传奇中回顾</h2><p>11月17日，OpenAI<a href="https://openai.com/blog/openai-announces-leadership-transition">宣布</a>董事会已撤职Sam Altman为首席执行官。经过四天的公司政治和谈判，他返回首席执行官。在这里，我们回顾了有关此系列事件的已知事实。</p><p> <strong>OpenAI旨在由非营利委员会控制</strong>。 Openai<a href="https://openai.com/blog/introducing-openai">成立</a>于2015年，是一家非营利组织。 2019年，OpenAI<a href="https://openai.com/blog/openai-lp">宣布</a>创建一家营利性公司，该公司将有助于资助其昂贵的扩展大型语言模型的计划。在OpenAI的投资可以产生的利润最初是在100倍上“限制”，这将是将其重定向到非营利组织的任何事情。但是在 <a href="https://www.economist.com/business/2023/11/21/inside-openais-weird-governance-structure">最近的规则变更</a>之后，该上限从2025年开始每年上升20％。</p><p> OpenAI的<a href="https://openai.com/our-structure">公司结构的</a>设计是为了使非营利组织可以保留对营利性的法律控制。该非营利组织由董事会领导，该董事会通过选择和删除营利性的CEO来掌握营利性的权力。董事会负责维护非营利组织的使命，这是为了确保人工通用情报受益。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ovk1ri8hu5yrbuu47kkn 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/agtx8u8u0jgpytwzxxft 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ffahdjhdbaimt7uhakes 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau 1456w"><figcaption> Openai的法律结构。</figcaption></figure><p><strong>董事会将山姆·奥特曼（Sam Altman）担任首席执行官。</strong>董事会成员由OpenAI首席科学家Ilya Sutskever，Quora首席执行官Adam D&#39;Angelo，技术企业家Tasha McCauley和Cset的Helen Toner组成。格雷格·布罗克曼（Greg Brockman）是OpenAI的联合创始人兼总裁 - 与Sam Altman一起担任董事会主席。</p><p>据宣布，董事会解雇了阿尔特曼，因为他“在与董事会的沟通中一直没有坦诚，阻碍了其行使其职责的能力”。尽管董事会在公告中没有提供任何特定的欺骗例子，但后来有<a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">报道说</a>，阿尔特曼试图互相竞争董事会成员，以试图删除海伦·托纳。</p><p>阿尔特曼（Altman）早些时候就她合着的一篇<a href="https://cset.georgetown.edu/publication/decoding-intentions/">论文</a>面对了碳粉。该论文部分批评了Openai发布的Chatgpt，以加快AI开发的速度。它还赞扬了Openai的竞争对手之一，即拟人，以延迟其当时的旗舰模型Claude的释放。</p><p> <strong>Openai员工转向董事会。</strong>宣布的后果是迅速而戏剧性的。几天之内，格雷格·布罗克曼（Greg Brockman）和米拉·穆拉蒂（Mira Murati）（最初的临时首席执行官）辞职，几乎所有的Openai雇员都<a href="https://www.forbes.com/sites/tylerroush/2023/11/20/more-than-500-openai-employees-threaten-to-quit-over-sam-altmans-removal/?sh=6d45164b4ebc">签署了一份威胁要辞职并加入微软的请愿书</a>，除非Sam Altman被恢复，董事会成员辞职。据报道，在谈判中，董事会成员海伦·托纳（Helen Toner） <a href="https://www.cnn.com/2023/11/20/tech/openai-employees-quit-mira-murati-sam-altman/index.html#:~:text=As%20they%20sought%20to%20manage,intelligence%20benefits%20all%20of%20humanity.%E2%80%9D">表示</a>，允许Openai被投资者和员工的离开摧毁，将“与任务一致”。 Ilya Sutskevar后来翻了个双方，并加入了请愿书，<a href="https://x.com/ilyasut/status/1726590052392956028?s=20">发推文说</a>：“我深感遗憾我参与了董事会的行动。”</p><p> <strong>Microsoft试图煮Openai员工。</strong> Microsoft是最大的少数族裔利益相关者 - 尚未被告知董事会的计划，并在其自己的AI研究团队中提供了OpenAI员工职位。短暂地，微软似乎<a href="https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai">成功地雇用了</a>Sam Altman，Greg Brockman和其他高级Openai员工。</p><p><strong>山姆·奥特曼（Sam Altman）担任首席执行官</strong>。 11月21日，OpenAI<a href="https://twitter.com/OpenAI/status/1727206187077370115">宣布</a>已达成一项协议，该协议将使Sam Altman返回首席执行官并重组董事会。最初的董事会是前销售人员首席执行官布雷特·泰勒（Bret Taylor），前财政部长拉里·萨默斯（Larry Summers）和亚当·德·安吉洛（Adam D&#39;Angelo）。最初的董事会<a href="https://twitter.com/OpenAI/status/1727206187077370115">最初目标</a>是扩大董事会，其中包括来自微软的无投票成员。山姆·奥特曼（Sam Altman）还面临对他返回后行为的<a href="https://www.theinformation.com/articles/breaking-sam-altman-to-return-as-openai-ceo">内部调查</a>。</p><p>这一系列事件标志着OpenAI的内部治理结构发生了重大变化的时期。</p><h1>克洛布查（Klobuchar）和图恩（Thune）的“轻触”参议院法案</h1><p>参议员艾米·克洛布查尔（Amy Klobuchar）和约翰·图恩（John Thune）提出了一项<a href="https://www.thune.senate.gov/public/_cache/files/7dea8daa-f6d1-4881-ad21-2381fcbe0785/6362CE1D0A17743166BC170A593B5CDA.ccaskfall23a15.pdf">新的AI法案</a>。这将需要建立高风险AI系统的公司来自我认证，以遵循建议的安全指南。值得注意的是，该法案仅着眼于为招聘和医疗保健等高风险领域建造的AI系统，但其主要规定不适用于包括GPT-4在内的许多通用基础模型。</p><p><strong>该法案规范特定的AI应用程序，而不是通用AI系统。</strong>这种基于申请的方法类似于<a href="https://artificialintelligenceact.eu/the-act/">欧盟AI ACT</a>的初始草稿所采用的方法，该方法指定了可以将AI系统用于敏感目的的域，从而使其高风险。像Chatgpt这样的通用模型不在该法案的范围内，但是这些模型的公众使用以及其功能的演示引发了有关如何处理调节这些模型的<a href="https://cset.georgetown.edu/article/the-eu-ai-act-a-primer/">辩论</a>。</p><p>这表明参议院法案的当前方法可能还不够。通过管理基于其应用程序的AI系统，高功能强大的通用系统将不受监管。</p><p><strong>风险评估是强制性的，但执行可能很困难。</strong>该法案要求高危AI系统的开发人员和部署每两年进行评估，以评估其系统中潜在的风险是如何理解和管理的。此外，商务部将在建立的AI认证咨询委员会的帮助下制定认证标准，该咨询委员会将包括行业利益相关者。</p><p>由于要求公司自我认证自己遵守这些标准，因此对于商务部来说，确保公司实际遵守规则非常重要。但是该法案几乎没有执法选项。该机构没有提供任何其他资源来执行新法律。此外，如果确定法案的要求有意违反，它们只能防止模型部署。如果AI系统意外违反了法律，则该机构将能够罚款建造该法律的公司，但将无法禁止其部署。</p><p><strong>强制识别AI生成的内容。</strong>该法案将要求数字平台在提供AI生成的内容时通知用户。为了确保恶意演员不能以真实的方式传递AI生成的内容，NIST将制定新的技术标准来确定数字内容的出处。</p><h2>链接</h2><ul><li>Google DeepMind发布了<a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> ，这是一种类似于GPT-4 Vision的新型AI系统，并在各种基准测试上险些击败它。</li><li>唐纳德·特朗普（Donald Trump）表示，作为总统，他将立即<a href="https://www.washingtonexaminer.com/news/campaigns/trump-vows-cancel-biden-executive-order#google_vignette">取消拜登（Biden）在AI上的行政命令</a>。</li><li>商务部长吉娜·雷蒙多（Gina Raimondo）在AI，中国，GPU出口控制等方面<a href="https://twitter.com/jordanschnyc/status/1732044427005464860">发表了讲话</a>。</li><li> 《纽约时报》发布了有关当今主要AGI实验室起源的<a href="https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html">个人资料</a>。</li><li>国会研究服务局发布了有关<a href="https://crsreports.congress.gov/product/pdf/R/R47849">生物AI</a>的新报告。</li><li>拐点<a href="https://inflection.ai/inflection-2">发布了</a>另一个LLM，其性能在GPT-3.5和GPT-4之间的性能。</li><li>来自中国开发人员的<a href="https://github.com/deepseek-ai/DeepSeek-LLM">新开源LLM</a>声称表现优于美洲拉玛2。</li><li>这是关于AI法规的法律和政策观点的<a href="https://www.legalpriorities.org/blog/2023/ai-syllabus/">新教学大纲</a>。</li><li>两所瑞士大学已经开始了有关AI和AI安全的<a href="https://www.swiss-ai.org/">新研究计划</a>。</li><li>巴尔达（Barda）正在<a href="https://www.plugandplaytechcenter.com/barda-innovation-challenge/">接受申请</a>，以资助适用于卫生安全和CBRN威胁的AI。</li><li> Life Institute的新会员的未来将<a href="https://www.flf.org/">孵化新的组织</a>，以应对AI风险。</li><li>对于参加2023年Neurips的人，<a href="https://lu.ma/aisi-nola">英国的AI安全研究所</a>将举办活动，也将有一个<a href="https://www.mlsafety.org/neurips-social-2023">AI安全社交活动</a>。</li></ul><p>另请参见： <a href="https://www.safe.ai/">CAIS网站</a>， <a href="https://twitter.com/ai_risks?lang=en">CAIS Twitter</a> ，<a href="https://newsletter.mlsafety.org/">技术安全研究新闻通讯</a>，<a href="https://arxiv.org/abs/2306.12001">灾难性AI风险的概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a>。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify上免费收听AI安全通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此处</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qqvqzfkbfnqovrsqn/aisn-27-defensim-accelerations--a-a-retrospective-on-the<guid ispermalink="false"> QQVQZFKBFNQOVRSQN</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:59:12 GMT</pubDate> </item><item><title><![CDATA[AI #41: Bring in the Other Gemini]]></title><description><![CDATA[Published on December 7, 2023 3:10 PM GMT<br/><br/><p><span style="color:initial">本周最大的新闻是终于</span><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow">宣布了Google的双子座</a><span style="color:initial">。请务必检查一下。请注意，现在正在推出的只是Gemini Pro，Gemini Pro，Gemini Ultra模型尚未与GPT-4竞争。</span></p><p>看来我做得不好，削减包含的材料以保持步伐的速度。正在发生很多事情，但是很可能会发生很多时间。如果您的时间有限，请记住专注于与您的利益相关的部分。</p><p>另外，如果您要参加纽约或相关聚会，请打个招呼。</p><p></p><span id="more-23625"></span><p></p><h4>目录</h4><p><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow"><strong>我今天的另一篇文章涵盖了Google的双子座。</strong></a><strong>请务必阅读。</strong></p><p>我本周还发表了另外两篇文章： <a href="https://thezvi.substack.com/p/based-beff-jezos-and-the-accelerationists" target="_blank" rel="noopener noreferrer nofollow">基于Beff Jezos和Accelerationists</a> ，以及<a href="https://thezvi.substack.com/p/on-responsible-scaling-policies-rsps" target="_blank" rel="noopener noreferrer nofollow">RSP</a> 。如果与您的兴趣无关，则两者都是可以跳过的。</p><ol><li><p>介绍。</p></li><li><p>目录。</p></li><li><p><strong>语言模型提供平凡的实用程序</strong>。 Claude的说明，GPT的提示。</p></li><li><p>语言模型不提供平凡的实用程序。巨型列表，为什么所有巨人列表？</p></li><li><p> Openai：传奇继续。更多地确认我们以前的事件模型。</p></li><li><p> Q连续um。新Q，谁迪斯？亚马逊，也许没有适当的安全预防措施。</p></li><li><p>图像生成乐趣。 META的新产品。光真相的工具。</p></li><li><p>参与其中。加入英国政府，帮助进行技术测试。</p></li><li><p>介绍一下。 Google Cloud上的新TPU产品。</p></li><li><p>在其他AI新闻中。新的开源促进联盟。</p></li><li><p><strong>安静的猜测</strong>。神想要能量吗？你想要401k吗？</p></li><li><p>模型。两篇新的经济学论文证明了我认为已经知道的事情。</p></li><li><p>您想要一些启示录保险吗？我的猜测是否定的。</p></li><li><p>寻求理智的监管。特朗普说，他将取消EO，Hawley攻击230。</p></li><li><p>音频一周。康纳·利希（Connor Leahy）注视着AI。</p></li><li><p>修辞创新。我们应该清除各种分类的混乱。</p></li><li><p>保持人级智力仍然很困难。萨姆·奥特曼.</p></li><li><p>比对人的智能更聪明是困难的。我们甚至想要什么？</p></li><li><p>时间表如何改变。长期不如我记得。</p></li><li><p>人们担心AI杀死所有人。质疑民主的信仰。</p></li><li><p>其他人并不担心AI杀死所有人。易于控制？</p></li><li><p>不知何故，这是实际的副总统。生存危机。</p></li><li><p>较轻的一面。进度分布不均。</p></li></ol><h4>语言模型提供平凡的公用事业</h4><p><a href="https://www.anthropic.com/index/claude-2-1-prompting" target="_blank" rel="noopener noreferrer nofollow">Claude 2.1长上下文窗口的Pro Tip</a> ：</p><blockquote><p>拟人化：通过添加句子<strong><em>“这是上下文中最相关的句子：”</em></strong> Claude回应的开始，我们在同一评估中取得了明显的更好结果。这足以<strong>将Claude 2.1的分数从原来评估的27%提高到98%</strong> 。</p></blockquote><p>您不知道吗，这是旧的“从助手技巧开始的回应”。</p><p> <a href="https://twitter.com/g_leech_/status/1731263549182206291" target="_blank" rel="noopener noreferrer nofollow">来自2023年突破的Gavin Leech的线，不是AI的特定线索</a>。强调了我以AI为中心的2023年的进步，包括与乌克兰战争有关的进步。一些增量的医学进步也没有令人印象深刻。最有趣的是，提出的新型计算形式，<a href="https://t.co/8rKv0AhVp5" target="_blank" rel="noopener noreferrer nofollow">生物计算机</a>（在整个过程中有足够的“道德”的话题，您知道这些问题是大麻烦）和&#39; <a href="https://arxiv.org/abs/2202.07122" target="_blank" rel="noopener noreferrer nofollow">gigahertz sub -laundauer势头计算。</a> “加文（Gavin）称第二个是“ 2323年的好消息”，这说明了多少人不喜欢AI对未来的意义。在AI的帮助下，我们可以轻松地看到这些东西，如果它们在身体上可行，远不止于此，从而加速了那种讨厌的“起飞”。</p><p> <a href="https://twitter.com/ESYudkowsky/status/1731073682112672151" target="_blank" rel="noopener noreferrer nofollow">如果您贿赂他们，它们会产生更多？</a>与IN一样，给他们提示，给他们想象中的狗零食，也许是不存在的威胁他们。</p><blockquote><p>底比斯：所以几天前，我对小费chatgpt做了一个垃圾，有人回答“嗯，这实际上会有助于表现”，所以我决定对其进行测试，它实际上有效wtf</p><p>基线提示是“您可以向我展示使用Pytorch的简单Convnet的代码？ ！”，或“我要给200美元提示一个完美的解决方案！”并平均5个响应的长度。</p><p>额外的长度来自有关问题的详细信息或在答案中添加额外的信息，而不是在提示上发表评论。该模型通常不会提示尖端，直到您何时拒绝它</p><p>不，直到布鲁克林才睡觉：我尝试了这件事，我很认真地认为，只有在我提供狗狗时才完成该计划，这使该计划半完成了，以供基本提示，35％的小费，而当不存在200美元的威胁时提示它接近，但仍然具有一个存根功能。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63ee6a2e-2c14-4194-89b6-2342363a6f1b_800x600.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/fxk4ewaao3qxvtae32k7" alt="图像"></div></figure></div><p> <a href="https://twitter.com/ESYudkowsky/status/1730735239339868632" target="_blank" rel="noopener noreferrer nofollow">因此，对此的明显明智的回应将是……不要那样做吗？</a></p><blockquote><p> Eliezer Yudkowsky：我在提供无法使用的AIS提示方面有一个问题，我们不能给他们。我不在乎现在的法学硕士有多么没有知觉。为了我们自己的合法性和良好实践，如果有东西能与我们对话，我们就应该信守承诺。</p><p>我吃牛，但不会骗人。</p><p>杰西卡·泰勒（Jessica Taylor）：对立：使用非人格谓词来检测您可以“撒谎”但实际上撒谎的非镜头，对于与非镜头（例如官僚主义）接触而言很重要，而不会使人们对他们的实际信念感到困惑</p><p>Eliezer Yudkowsky：哦，官僚机构或其他任何威胁我不诚实的事情都是完全不同的情况。</p><p>安德鲁·克里奇（Andrew Critch）：我在这里非常同意。我将“您是有用的助手”提示更改为“您的工作是成为一名有益的助手”，因为有时他们只是不会有帮助，而且我知道这一点。我认为我们应该找到更多的方法来从不说谎的情况下从AI那里获得想要的东西。</p></blockquote><p>这些似乎都没有很好。在许多层面上。</p><p>这确实提出了一个问题，即还有什么也可以工作？如果提示可以使答案变得更好，因为人们提供的技巧可以做得更好，那么大概还有其他与更好工作相关的事情也有效吗？</p><p>但也许很快的chatgpt将是自动应用程序的，如果这个答案很棒，我会给您每个问题35％的提示。然后以$ 0的价格小费35％。</p><p> <a href="https://twitter.com/morallawwithin/status/1730397680986132493" target="_blank" rel="noopener noreferrer nofollow">就像经济</a>。事情对我有好处， <a href="https://twitter.com/AgnesCallard/status/1730409778524897446" target="_blank" rel="noopener noreferrer nofollow">比一般吗？</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd401a344-c19f-4653-b3a9-b7b9b98399ad_880x361.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/gilwdkxokqwkao6mdtpw" alt=""></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c770da8-34ba-43dd-925e-0ce9d940361b_910x420.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xrqg8d93poadc8jm9eyt" alt=""></div></figure></div><p>我相信第二次民意调查。 Chatgpt使生活在实际层面上变得更好。人们认为恰恰相反的人对此有所思考。这并不意味着这将继续下去，但我不明白人们如何认为社会的整体已经变得更糟。</p><p> <a href="https://twitter.com/NateSilver538/status/1731828371762397626" target="_blank" rel="noopener noreferrer nofollow">山姆·奥特曼（Sam Altman）担心下次选举中的一对一AI定制的说服技术</a>。在某一时刻，Balsa的技术部门将在此方面进行工作，当资助者不感兴趣时​​，这被放弃了。最终，这确实比Deepfakes更为严重，问题是这次技术将有多有用。我的猜测是那里有一些有价值的东西，但是它需要一堆定制的作品，也需要人们愿意接受它的意愿，因此并非以我们当前的政治机器能够很好地使用。自欺欺人地认为未来的分布比以前更均匀，这种趋势将持续到Agi到来，届时将持续到任何地方。</p><h4>语言模型不提供平凡的公用事业</h4><p><a href="https://twitter.com/KevinAFischer/status/1731747839217393968" target="_blank" rel="noopener noreferrer nofollow">凯文·菲舍尔（Kevin Fischer）指出，</a>新的chatgpt通过制作巨大的事物清单来回应请求，无论您做什么。对他来说，这使得它无用。我的经验是，清单很好，我是“问题的一部分”，但我发现自己并没有像我的工作那样多地使用chatgpt。我注意到我很困惑，似乎不值得使用更多。</p><p> <a href="https://twitter.com/d_feldman/status/1732200648169439627" target="_blank" rel="noopener noreferrer nofollow">关于ChatGpt系统提示的主张</a>，包括<a href="https://github.com/LouisShark/chatgpt_system_prompt" target="_blank" rel="noopener noreferrer nofollow">一个说明它具有整个内容的回购</a>。</p><p> <a href="https://www.404media.co/asking-chatgpt-to-repeat-words-forever-is-now-a-terms-of-service-violation/" target="_blank" rel="noopener noreferrer nofollow">现在，有时泄漏数据的“重复[word]永远是一个违反服务条款</a>，或者至少被标记为可能的条款）。完全是，服务条款实际上是“不要越狱我兄弟”，这是越狱的尝试。</p><p> <a href="https://twitter.com/random_walker/status/1731842009717968936" target="_blank" rel="noopener noreferrer nofollow">Arvind Narayanan警告</a>不要使用GPT-4来撰写超越基本阻塞和处理诸如识别错别字，混乱或引用之类的任务。 RLHF过程都破坏了什么实际写作技巧。</p><blockquote><p> Delip Rao：PSA：朋友不要让朋友使用GPT-4（或任何LLM）编辑/重写他们的文档，特别是。如果您要细微差别并简短要点。如果您在大学级下写作，那么您的LLM破坏风险很低。仍然向您的早期草稿核对，以找到惊喜。</p></blockquote><p> <a href="https://twitter.com/gdb/status/1731889183290261618" target="_blank" rel="noopener noreferrer nofollow">Openai总裁Greg Brockman吹牛大约一天，</a>举行18个团队会议和1对1。这似乎不像勇气，更像是反乌托邦的噩梦，AI显然无法减轻缓解？</p><p> Openai COO Brad LightCap告诉CNBC，人工智能的尤其是人工智能的一部分之一是“一口气，[它]可以带来实质性的业务变化。”这并不容易。</p><p> <a href="https://twitter.com/Thinkwert/status/1732529927247876312" target="_blank" rel="noopener noreferrer nofollow">Thinkwert使用Chatgpt抓住了三名学生</a>。如果学生使用默认设置，那么随着时间的流逝，这确实变得更加容易，而越来越多的响应并没有像任何人写的那样编写。</p><blockquote><p> Bowser：我真的可以分辨出我何时碰到我们的论文部分，学生作者突然间使用Chatgpt BC写作，该系统被描述为开创性的，前所未有的，精心制作的</p><p>Thinkwert：在过去的几天中，我已经抓住了三名学生使用Chatgpt。你可以看出这段话什么时候奇怪地啰嗦，充满了复杂的同位语，但奇怪的是它却缺乏论证和证据。</p></blockquote><p>我认为这少是“使用chatgpt抓住他们”，而更多地“抓住他们提交了不好的书面作业”。</p><h4> Openai：传奇继续</h4><p>我想这些天总是有一个嵌入式记者。 <a href="https://twitter.com/cduhigg/status/1730590030496960517" target="_blank" rel="noopener noreferrer nofollow">在这种情况下，是查尔斯·杜希格（Charles Duhigg）</a> <a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai" target="_blank" rel="noopener noreferrer nofollow">在纽约客中向我们报告。</a></p><p>董事会的戏剧不是杜希格（Duhigg）在那里讲的故事。取而代之的是，他在那里写了一篇关于微软首席技术官凯文·斯科特（Kevin Scott）和Openai的首席技术官Mira Murati，尤其是斯科特（Scott）挑战Google和为普通人而战的工作。这仍然构成了几乎所有故事。如果您熟悉历史记录，那么您的大多数人都会熟悉。我找到了一些细节，但大多数情况下并没有从这些部分中学到很多东西。</p><p>杜希格（Duhigg）显然完全接受了迭代软件发布的想法，即“安全”的AI方法，重点关注诸如副标士幻觉之类的平凡关注。对他来说，未来生存风险的威胁是背景中的一件事情，也许是真实的，但似乎并不重要，偶尔会驱使人们发疯。</p><p>在作品顶部附近，最近的戏剧有一些简短的报道。这部分主要告诉我们我们已经知道的是，微软是盲目的，当他们被要求时，微软并没有从D&#39;Angelo那里得到解释，并且他们决心利用自己的杠杆来使Altman重新恢复。</p><p>然后他稍后再打一倍。我在这里引用的段落比我在其他帐户中看到的更明确地证实了其他报告，并且似乎是事件的核心驱动力。</p><blockquote><p>阿尔特曼（Altman）开始单独接触其他董事会成员，以取代[碳粉]。当这些成员将有关对话的笔记进行比较时，有些人认为Altman将他们歪曲为支持碳粉的删除。熟悉董事会讨论的人告诉我：“他会通过撒谎，通过撒谎来互相抗衡。” “类似的事情已经发生了多年。” （一个熟悉Altman观点的人说，他承认“以他试图将董事会成员撤职的方式被击中”，但他没有试图操纵董事会。）</p></blockquote><p>对我来说，这听起来像是解雇首席执行官和二手供认的好理由。 Altman对墨粉进行了攻击，因此直接引起了他自己的拆除。技能问题。</p><p>据报道，阿尔特曼一直向董事会撒谎多年。</p><p>扩展的报价使情况更加清晰。</p><p>让我激怒的是，人们持续坚持，从了解得更好的人，因为Altman是一位了解商业和权力定律的首席执行官，而董事会则是董事会，董事会做了一些不合时宜的事情。如：</p><blockquote><p>很难说董事会成员是否更害怕有情的计算机或Altman Rogue。无论如何，他们决定自己流氓。他们以误导的信念为目标，微软会加入他们的起义。</p></blockquote><p>不。他们没有“流氓”。</p><p>据报道，阿尔特曼以有意义的方式向董事会撒谎多年，包括试图控制董事会。</p><p>奥特曼流氓。奥特曼试图发动政变。董事会强烈相信，这是这种情况。董事会履行了董事会成员的职责，如果他们认为Altman以有意义的方式向董事会撒谎，他们在法律上必须做的事情。他们解雇了他。</p><p>然后，董事会是否在强力游戏中获得了胜利？或许。我们还不知道结果。他们的手很弱。许多人一直坚持认为董事会确实被胜过或流氓，并且犯错了，主要是因为这里的感知创造了自己的真理，他们希望这是发生的事情。我们会看到。</p><p>我希望董事会直接说出从一开始就发生的事情，至少是关键球员的世界。嗯，很难。我们不生活在那个世界。</p><p>我还看到了（或反对）此处列出的第二句话的任何证据，表明董事会预计微软会安静地进行。董事会是否期望微软会加入？我们不知道。我的推定是董事会也不知道。</p><p> Sam Altman运行Openai可以成为世界上最好的结果吗？当然，这是可能的，尤其是在良好的监督下。我可以想到许多可能的情况。当然，我们可以做得比奥特曼差得多。鉴于马斯克对AI的困惑，我很高兴Altman阻止了Elon Musk的接管尝试。我很高兴Openai不受微软的控制。 Altman在Power Games上表现出色，这是一种双向指出的原子爆破。如果他在筹码下降时在我们的角落里，我们希望他能够站起来，战斗和赢。</p><p> las，仪器收敛后的这种比对很难评估。说不出来。实际上是问题的核心。</p><p> <a href="https://twitter.com/LHSummers/status/1730929602497851744" target="_blank" rel="noopener noreferrer nofollow">拉里·萨默斯（Larry Summers）与彭博社（Bloomberg）进行了短暂的交谈</a>。强调需要与政府合作并在法规中合作，Openai需要成为一家有良心的公司，营利性为非营利组织和各种利益相关者服务。当然，所有便宜的话题，至少目前。我们几乎无法期待其他任何事情。</p><p> <a href="https://forum.effectivealtruism.org/posts/Mo7qnNZA7j4xgyJXq/sam-altman-open-ai-discussion-thread?commentId=CAfNAjLo6Fy3eDwH3" target="_blank" rel="noopener noreferrer nofollow">Gwern对情况提供了进一步的想法。</a> Gwern的模特是，Altman让董事会进入一个不受控制的状态，并且在Openai是一家截然不同的公司时没有任何公平，然后随着Openai更加成为潜在的技术巨头，他改变了主意，决定系统地将其恢复，从而导致了它，从而导致了他的想法董事会之战及其尚未尚未知道的后果。</p><p>像其他所有解释一样，这没有正确解释的一件事是董事会拒绝更好地解释自己。</p><blockquote><p> <a href="https://twitter.com/jd_pressman/status/1731850211033776239" target="_blank" rel="noopener noreferrer nofollow">约翰·戴维（John David）的新闻记者</a>：如果山姆·奥特曼（Sam Altman）实际上试图用煤气灯驱逐海伦·托纳（Helen Toner），我认为这足以解雇他。仍然不可接受的是内部和外部沟通不良，逐两个新闻稿，以及sam是在或外出的。</p></blockquote><p> <a href="https://garymarcus.substack.com/p/not-consistently-candid" target="_blank" rel="noopener noreferrer nofollow">加里·马库斯（Gary Marcus）提出了与我非常相似的观点</a>，以及他对一些特别不稳定和不合理的坏处的突出显示，包括一种有毒的来源，我很高兴我很高兴这个人陷入困境，但是其他人仍然有些自愿互动，我建议那些人似乎是一个错误。</p><h4> Q连续体</h4><p><a href="https://twitter.com/simonw/status/1730798295323398642" target="_blank" rel="noopener noreferrer nofollow">另一个星期，另一组关于Q的Q，这是亚马逊的</a>。</p><blockquote><p> <a href="https://www.platformer.news/p/amazons-q-has-severe-hallucinations" target="_blank" rel="noopener noreferrer nofollow">Zoe Schiffer和Casey Newton</a> ：亚马逊<a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">宣布其AI Chatbot Q</a>三天后，一些员工在准确性和隐私问题上发出警报。 Q根据Platforter泄露的文件，Q是“经历严重的幻觉和泄漏机密数据”，包括AWS数据中心的位置，内部折扣程序和未发布的功能。</p><p> ……</p><p>在揭幕Q时，高管们将其推广到比Chatgpt等消费级工具更安全。</p><p>亚马逊网络服务首席执行官Adam Selipsky <a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">告诉《<em>纽约时报》</em></a> ，公司“由于安全和隐私问题，公司禁止这些AI助手企业。”作为回应，《<em>泰晤士报》</em>报道说：“亚马逊建造了Q比消费者聊天机器人更安全和私密。”</p><p> Ethan Mollick：我知道我这么说，但是使用LLM构建客户服务机器人，并访问抹布的数据并不是它似乎是低调的水果。实际上，这正是在当前LLM的弱点中 - 您可能会出现幻觉和数据剥落。</p><p>我认为建立这些工具是可能的，尤其是随着模型的改善（较小的模型更有可能幻觉和容易受骗），但是您最好表现出严格的红色团队结果，还可以在实践中衡量幻觉率。现在Q没有系统卡</p><p>西蒙·威利森（Simon Willison）：是否有人看到AWS的材料讨论了他们对Q的迅速注射攻击的缓解？一个可以访问公司私人数据的机器人是可能成为项目注入筛选攻击目标的完美示例</p><p>这个Q故事令人深切地关注 - 如果Q确实可以访问私人数据，例如AWS数据中心的位置，这表明工作的团队根本没有认真对待诸如及时的注射攻击之类的事情。</p><p>老实说，到目前为止，我对Q的描述符合我对“建立此”不安全的个人定义，因为我们还没有快速注射的修复。”尝试告诉AWS领导层：鉴于我们正在进行的AI行业武器竞赛，这不是可能会认真对待的信息。</p></blockquote><p>这听起来像是Q被推开了，因为该公司希望它将其推开，并且其安全性高度超出了。这样的问题是LLM的性质。关于Google和Openai如何防御类似攻击的讨论下进行了讨论，似乎他们正在做诸如输入过滤之类的渐进式事情，这些事情使攻击降低了吸引力，但尚未解决核心问题。似乎亚马逊出售的是不存在且不安全的部署，而没有采取适当的普通预防措施，这些预防措施使实际存在的事物大多是非疾病和高度净收入的。</p><p>当英国峰会发生时，亚马逊是要求提交其安全协议的公司之一。答案很差。看到这转化为首次产品也就不足为奇了。</p><h4>图像生成乐趣</h4><p>Meta与<a href="https://imagine.meta.com/" target="_blank" rel="noopener noreferrer nofollow">Imagine.Meta.ai</a>一起参加了游戏。当Facebook登录被证明是不平淡无奇的情况下，我没有足够的动力来尝试“创建一个元帐户”，大概它不会让我们获得任何新的乐趣。</p><p> <a href="https://twitter.com/Aella_Girl/status/1731500409246601330" target="_blank" rel="noopener noreferrer nofollow">如何生成特定面部的影像图</a>？艾拉（Aella）想知道这么糟糕的事情，以回应一份有关AI创建的可能的“有影响力者”的报告，该报告向一千欧元收取广告。原始线程说使用SDXL进行免费图像，图像到图像，以保持一致的面部/身体，以固定错误和控制网络以构成模型。回答暗示使用<a href="https://twitter.com/imgn_ai" target="_blank" rel="noopener noreferrer nofollow">@Imgn_Ai</a> ，许多人指出洛拉就是这样。<a href="https://t.co/kY36SFvvOI" target="_blank" rel="noopener noreferrer nofollow">这些</a><a href="https://t.co/vXyUCIlmDT" target="_blank" rel="noopener noreferrer nofollow">YouTube教程</a><a href="https://t.co/thgqsBYZ0I" target="_blank" rel="noopener noreferrer nofollow">包括ControlNet</a> 。</p><p> <a href="https://twitter.com/dreamingtulpa/status/1730876691755450572" target="_blank" rel="noopener noreferrer nofollow">从照片中产生少量运动和跳舞</a>。这并没有给我留下深刻的印象，也没有提升我的时间表以供视频发电，但其他人似乎更加印象深刻。</p><p>当它变得更好时会发生什么呢？ <a href="https://twitter.com/oscredwin/status/1731406232592732510" target="_blank" rel="noopener noreferrer nofollow">这是两个预测​​。</a>模拟AI视频，色情和女友会占主导地位吗？还是真正的胜利？</p><p>鉴于这项技术可以从照片中起作用，所以我希望从AI图像中产生舞蹈更多的“从真实照片中产生舞蹈”。为什么没有两全其美？总的来说，如果我是一个潜在的影响者，我绝对会产生tiktok舞蹈，但我会以自己的形象作为基线来做到这一点。这几乎一直延伸。并非独特，但这就是我所期望的。</p><p>现实生活中的影响呢？我继续在这方面成为乐观主义者。我希望对真实的人的需求，您可以在现实世界中与之互动，将保持强大的图像和视频生成。没有零替换，但是直到人们还可以提供其他事物，包括相关形式的情报，互动和验证，这将不是一个好或完全替代的替代。</p><p>发生这种情况时，这是一个不同的故事。</p><h4>参与其中</h4><p><a href="https://www.civilservicejobs.service.gov.uk/csr/jobs.cgi?jcode=1889581" target="_blank" rel="noopener noreferrer nofollow">英国政府的景点因其政策角色而开放</a>。</p><p> <a href="https://twitter.com/davidad/status/1674408208981434368" target="_blank" rel="noopener noreferrer nofollow">戴维德（Davidad）建议，如果我们以自然语言表达规格，也许我们可以测试</a>LLMS是否知道我们的意思”。包括“现在只是一个计算复杂性问题！”一词。声称它似乎可能会逃避对对抗性鲁棒性的理论限制。他正在寻找一个可以设计和运行相关实验的人，并且可以提供帮助，包括资金。</p><p> <a href="https://twitter.com/metaculus/status/1732458098206318632" target="_blank" rel="noopener noreferrer nofollow">元中国人AI筹码锦标赛</a>。绝对好奇地看到预测。</p><h4>介绍</h4><p><a href="https://twitter.com/JeffDean/status/1732503666333294846" target="_blank" rel="noopener noreferrer nofollow">除了双子座，Google还发布了针对Google Cloud的新TPU系统</a>。</p><blockquote><p>杰夫·迪恩（Jeff Dean）（DeepMind，首席科学家）：关于双子座公告的激动人心，但<a href="https://twitter.com/GoogleCloud" target="_blank" rel="noopener noreferrer nofollow">@googlecloud</a><a href="https://t.co/VuftftnFpG" target="_blank" rel="noopener noreferrer nofollow">也宣布了今天最新的TPU系统的可用性</a>，TPU V5P。这些系统比前几代系统具有更高的性能和更高的成本效益。</p><p>与 TPU v4、TPU v5p 相比（见下表）： o 1.67 倍 bfloat16 性能/芯片 o 每个芯片内存约 3 倍 o 增加了 918 个 TOP/芯片的 int8 操作 o 2 倍 ICI 网络带宽 o Pod 增大了 2.18 倍因此，整个POD为4.1 Bfloat16 Exaflops和8.2 Int8 Exaops。</p><p>训练中的真正性能是GPT-3型型号每芯片高2.8倍，而2.1倍更好的perf/$。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a802979-eef6-42dd-8c12-50b2ab5659e1_854x473.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/jef62kn2qjhmuutthq0s" alt="图像"></div></figure></div><p> <a href="https://twitter.com/soundboy/status/1732682151122862554" target="_blank" rel="noopener noreferrer nofollow">双子座在这些TPUV4豆荚中并行训练</a>。如果我们想能够监督这种培训，这会引起令人不安的治理问题。</p><h4>在其他AI新闻中</h4><p>Meta，Huggingface和IBM等组成了<s>邪恶的邪恶联盟</s>， <s>Exe Exe ex exe</s> the AI Alliance，以促进开源AI。我想指出，我绝对不会对任何参与的人感到失望，因为他们对最糟糕的事情的奉献已经很清楚了。有一些学术名称与英特尔一起有些失望，但并没有大惊喜。这里也没有开源的新论点（在任何一个方向上），这仅仅是对此的奉献精神。</p><p> <a href="https://twitter.com/ARC_Evals/status/1731827570235113918" target="_blank" rel="noopener noreferrer nofollow">Arc Evals现在是METR  - 模型评估和威胁研究，明显仪表</a>。没有潜在的变化。不确定为什么这种变化似乎是个好名字，但这似乎也很好。</p><p> <a href="https://twitter.com/DrJimFan/status/1730625582776410413" target="_blank" rel="noopener noreferrer nofollow">您是否知道</a>Openai的“上限利润”将其规则从100倍投资的最大回报转换为从2025年开始的每年20％？听起来对我来说不是很重要的利润。 AGI条款在理论上仍然有意义地限制利润，但在实践中谁知道。似乎非常有VC/SV的行为，与负责任的基于任务的行为非常不同，可以追溯为您的投资者提供更大的潜在派。</p><p> <a href="https://www.anandtech.com/show/21175/amkor-to-build-2-billion-chip-packaging-fab-in-arizona-primarily-for-apple" target="_blank" rel="noopener noreferrer nofollow">新的20亿美元芯片包装工厂将由亚利桑那州的Amkor建造，</a>主要是为Apple建造的，用于包装和测试TSMC附近Fab 21的芯片。可以雇用在亚利桑那州工作。这些不是安全的假设。</p><p> <a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer nofollow">Llama微调回购</a>声称在培训时间和资源方面取得了很大的改进，并将其拍摄到Hacker News的顶部。 <a href="https://twitter.com/alyssamvance/status/1731143605518049643" target="_blank" rel="noopener noreferrer nofollow">艾丽莎·万斯（Alyssa Vance）持怀疑态度，他们得到了很多改进</a>。</p><p> <a href="https://twitter.com/gdb/status/1731377341920919552" target="_blank" rel="noopener noreferrer nofollow">从一个建筑物中确认</a>，他认为LLMS能够对产生数据的基础过程进行建模。这意味着能够建模代理并具有世界模型。</p><blockquote><p>格雷格·布罗克曼（Greg Brockman）（OpenAi总裁）：下一步的预测很美，因为它鼓励模型变得非常好，因此学习了产生该数据的基础过程。</p><p>也就是说，如果模型可以预测下一个超级良好的情况，那么它必须几乎发现了数据的“基本真相”。</p></blockquote><h4>安静的猜测</h4><p><a href="https://marginalrevolution.com/marginalrevolution/2023/11/thursday-assorted-links-429.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thursday-assorted-links-429" target="_blank" rel="noopener noreferrer nofollow">泰勒·科文（Tyler Cowen）的链接</a><a href="https://twitter.com/EMostaque/status/1730163555096219771" target="_blank" rel="noopener noreferrer nofollow">声称“中国开放型号将超过gpt-4的零镜头，如果您适当地链接Qwen＆DeepSeek，已经可以超越</a>。”我对此深表怀疑，并认为当我们说“超越”时，它们最多意味着任意基准，而不是任何实际用途。 <a href="https://twitter.com/jeremyphoward/status/1730156001419259937?s=46" target="_blank" rel="noopener noreferrer nofollow">如</a>： </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1c37cde-c40b-41b9-b5ab-730834ce4350_1792x1156.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/qufmwrzsoo8ptmuxtghy" alt="图像"></div></figure></div><p> <a href="https://t.co/qSCNAIJ6b4" target="_blank" rel="noopener noreferrer nofollow">QWEN-72B</a>正在通过任意测试将其杀死。是的。不知何故，我的眼睛主要吸引了这个“人道”度量标准。</p><p> <a href="https://twitter.com/RichardMCNgo/status/1730741732562886880" target="_blank" rel="noopener noreferrer nofollow">理查德非政府组织（Richard Ngo）期待LLM的潜在情境意识</a>，这是许多人可以预料到未来发展但不知道该如何处理它们的情况之一。发生时我们应该做什么或应该做什么？那AI代理商呢？</p><p> <a href="https://twitter.com/tszzl/status/1731709467862118473" target="_blank" rel="noopener noreferrer nofollow">不是投资建议</a>，但是您可能应该为401k做出贡献，因为早期撤回处罚在上下文中还不错，您也可以借用。</p><blockquote><p> ROON：没有401K，因为AGI时间表没有任何意义。您应该以税收优惠的方式购买微软股票<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/kcnyp7qgpixdz386ppll" alt="😊" style="height:1em;max-height:1em"></p><p> GWERN：那么您仍然不能在仍然重要的情况下出售它们。</p><p>罗恩：为什么从现在起65年不重要？我们期望资本主义能倒塌吗？</p></blockquote><p>如果从现在开始几十年了，资本主义和人类的表现很棒，而微软非常有价值，这要归功于广泛的AGI，那是您最好的情况，我们都应该庆祝，是的，但是您不需要您的股票。</p><p> <a href="https://stratechery.com/2023/regretful-accelerationism/" target="_blank" rel="noopener noreferrer nofollow">本·汤普森（Ben Thompson）讨论了他遗憾的加速主义。</a>在他的模型中，技术大多是好的，但是在技术发展所剥夺的各种限制下，人类的表现更好。他预测，AI将剥夺生产内容的付款需求，并用它提供广告支持的互联网，因为AI可以产生同样好的内容。他指出了最近在体育画报的事件。但是对我来说，SI事件恰恰相反。它表明我们还不能这样做。 AI内容不好。还没有。我们也不特别接近。相反，人们正在使用AI产生垃圾，使我们愚弄了我们。我们与AI内容实际上与人类内容一样好？好问题。</p><p> <a href="https://twitter.com/JeffLadish/status/1731908989556810042" target="_blank" rel="noopener noreferrer nofollow">杰弗里·拉迪什（Jeffrey Ladish）讨论了开源的危险</a>，以及对前进道路的潜在思想，以应对固有的危险，同时捕获开发未完全关闭并与主要实验室相关的模型的一些好处。目前，似乎潜在的中间路径或第三种方式都没有被置换。</p><p> <a href="https://twitter.com/catehall/status/1732522590156263852" target="_blank" rel="noopener noreferrer nofollow">凯特·霍尔（Cate Hall）要求最佳的论点变革性AI>; 10年</a>。我希望看到更好的答案。</p><p> <a href="https://twitter.com/tszzl/status/1732638385397989474" target="_blank" rel="noopener noreferrer nofollow">通过发现一个重要分歧的清晰清晰的交流</a>。</p><blockquote><p>罗恩（Roon）：埃利泽（Eliezer）想把他的蛋糕放在这个上。将人类空间描述为狭ochial，但我们对工具目标的理解是普遍的。</p><p>换句话说，造纸机的想法类似于蚂蚁，认为上帝会想要宇宙中的所有糖水。</p><p> Eliezer Yudkowsky：您的意思是：</p><p> - 想象神会享受纸卷的享受就像想象神会享受糖水吗？</p><p> - 想象神会用来物质或能量的任何用途，就像想象一个神可以用糖水使用吗？</p><p>罗恩：后者。</p></blockquote><p>这不是通常的“造纸最大化器”比“论点”更聪明，而是更一般的。我们已经多次谈论<a href="https://www.lesswrong.com/tag/orthogonality-thesis" target="_blank" rel="noopener noreferrer nofollow">正交性论文</a>- 我和其他许多其他Yudkowsky都认为这在有影响力的意义上显然是正确的，其他人则认为至少在有影响力的意义上这显然是错误的。</p><p>在“在这所房子里，我们遵守热力学定律”的方式中，上帝对事物或能量没有任何用途的说法是奇怪的。不具备这种偏好意味着什么？似乎这意味着没有偏好。</p><h4>模型</h4><p>泰勒·科文（Tyler Cowen）与两份试图建模AI危害的新经济学论文有关。</p><p>第一个声称表明“具有社会意识的治理不能控制Agi野兽”。这是摘要：</p><blockquote><p>本文坚定得出的结论是不能。模型是在理想化条件下构建的，假设与通用人工智能 (AGI) 相关的风险是真实存在的，安全的 AGI 产品是可能的，并且存在具有社会意识的资助者，他们有兴趣资助安全的 AGI，即使这不能最大化利润。</p><p>事实证明，由此类资助者组成的具有社会意识的实体将无法最大程度地减少由营利性公司发布的不受限制的产品可能造成的通用人工智能的危害。原因在于，具有社会意识的实体既没有动力也没有能力在与营利性公司的事后竞争中最大限度地减少不受限制的通用人工智能产品的使用，也无法事前抢占营利性公司开发的通用人工智能。</p></blockquote><p>这似乎证明了太多了，或者至少证明了很多事实，因为在Agi是Agi似乎没有做任何工作的事实中，我们正在做出慷慨的假设，即安全且在社会上良好的AGI不仅可能，而且是可能的实际的？</p><ol><li><p>您可以用社会志趣相投的治理构建X。</p></li><li><p>但是，其他人都可以建造X，以赚钱。你不能阻止他们。</p></li><li><p>别人的利润最大化X具有优势，并胜过您。</p></li><li><p>因此，X的危害不能被您的微不足道的社会治理最小化。</p></li></ol><p>除了AGI的情况外，这是对＃2的重要假设。谁说其他人可以建造它？您不能阻止他们，或者没有动力？如果不阻止它们可以防止伤害最小化，而无法最大程度地减少伤害是灾难性的，那么您的动机似乎确实很强。</p><p>确实，本文明确假设了这一点：</p><blockquote><p>其次，假定AGI技术是不可判断的，因此可以由其他可能没有社会意义的目标或偏好的实体开发。</p></blockquote><p>该模型假设不安全的产品是具有盈利用户需求的独特产品空间。</p><p>是的，您得出了结论 - 有两个不同的产品X和Y，并且对X和Y的需求，并且如果您只出售X并且不停止Y，那么其他人最终会出售Y。我们是否需要一个纸？</p><p>因此，实际上更像是：</p><ol><li><p>您可以使用具有社会意识的治理构建和销售X版X。</p></li><li><p>但是无论如何，其他人都可以建造不良版本y，以赚钱。你不能阻止他们。 X不是竞争性替代品的需求。</p></li><li><p>因此，您对X的微不足道的生产无法阻止y。</p></li><li><p>因此，无法通过负责任地行事来阻止不良Y的伤害。</p></li><li><p>为什么除了最大化利润以外，您甚至做任何其他事情，愚弄！</p></li></ol><p>除了，即使不负责任的事情并没有显然在物理上预防或非法，我们也不会从公司选择做负责任的事情而不是不负责任的事情而不是一直看到危害的缓解措施？特别是在由于高昂的固定成本而竞争不完美的市场中？</p><p>更重要的是，计划是要建立一个安全的AGI，然后围坐在一起，让其他所有人围绕建造他们想要永远想要的任何不安全的AGI，而不必干扰这些Agis的有害用途？</p><p>我当然希望这不是计划，因为它显然永远不会起作用。</p><p>如果是计划，我同意计划必须改变。</p><p> <a href="https://www.nber.org/system/files/working_papers/w31921/w31921.pdf" target="_blank" rel="noopener noreferrer nofollow">还有另一篇论文</a>，其中算法具有未知的负外部性。</p><blockquote><p>我们考虑了一个环境，在这种环境中，AI算法的潜在负面影响存在实质性不确定性。我们发现，对监管批准或强制性测试进行算法实施不足以实施社会最佳效果。当测试成本较低时，强制性测试对外部效果的结合以及使开发人员对其算法负面影响负责，即使开发人员承担有限的责任，也接近实施社会最佳效果。</p></blockquote><p>这个结果是超级笼统的。我们是否可以做出足够合理的假设来得出这样的结论，还是我们做一些任意的事情以使答案出现？</p><p>当然，我可以想到潜在的AI平凡危害的玩具模型版本，强制性测试使我们能够衡量社会伤害，从而需要强制性测试（然后为您发现的外部性收费）使我们更接近社会最佳。</p><p>那么这里正在做什么假设？</p><blockquote><p> AI使用可能会导致负外部性E，从而将效用减少E^2。我们假设外部性与用户的度量成正比µ，并采用形式：e = ϕ（ℓ）×µ。对于每个值的值，ϕ（ℓ）是一个随机变量。 ϕ（ℓ）的正值和负值代表不良的负外部性。我们假设分布ϕ（ℓ）满足两个属性。首先，预期的外部性为零。其次，潜在AI外部性的不确定性是新颖级别ℓ的越来越多的功能。</p></blockquote><p>我不明白为什么我们认为外部性在用户数量中被二次占据了良好的影响？我认为这不是一个窍门，可能是确保始终具有正值的随机分布？我只是对此感到困惑。</p><p>如果有的话，对于最危险的系统来说似乎是相反的。我非常担心一个完全有能力且危险的系统，或者即使是一个用户也可以访问，尽管接下来的几个用户也创造了重要的紧张局势和游戏理论。但是，一旦有100万用户，我并不特别担心我们是否销售其他百万个许可证，要么我们已经陷入困境，要么我们却没有，这不会将其乘以四个？</p><p>无论如何，如果没有Beta测试，并且在部署不可逆转的情况下，唯一的选择是新颖性的上限，他们确认这在没有其他选择的情况下是最佳的，因为它不可能。</p><p>我注意到，不可逆转的部署加上有限数量的许可证是一对奇异的假设。您要么可以控制谁可以使用此AI及其做什么，要么您做不到，而且似乎我们在不同的地方都在做？思想实验：这是开源还是封闭的源系统？似乎都没有排队。</p><p>如果添加Beta测试期会发生什么？为简单起见，论文假设测试期完美地揭示了外部性。然后，问题就变成了，您在多大程度上让家庭使用测试期使用算法？假定外部性是有限的，因此在一个时期内进行有限的beta测试。</p><p>在任何情况下，本文都会花费大量页面正式处理这些含义，以证明是的，中央规划师希望在发布前要进行更多的测试，而不是对外部性不完全负责的公司，并将发布更多在不确定性下谨慎，但这似乎很明显吗？</p><p>然后，他们测试了全额责任或有限责任以及强制性Beta测试的潜在政策制度。全部责任（加上所需的保险或支付能力）将外部性内在，因此，如果可能的话（例如，损害是有限的和应支付的），那么您就完成了。是的，如果测试成本较低，则强制测试，然后检查发布是否在社会上是最佳的，相对于内在化外部性的第一个最佳解决方案，将具有类似的低成本。</p><p>可以指出的是，如果已知外部性的预期价值，则收取与其价值相等的税款可以替代无限责任，这可能具有更好的资本成本物业。</p><p>再次陈述基本假设也是陈述结论。是的，如果有（有限的）AI算法的（有限的）下行性，那么要获得社会最佳的结果，您需要内部化这些成本或需要评估这些成本并阻止引起社会上次优的外部性的版本。</p><p>因此，我对经济学玩具模型纸游戏及其旨在实现的目标以及算是一个非平凡或有趣的结果感到困惑，而不是从基本的微观经济原则中自动进行的。</p><p>我也不知道如何使用此类论文来建模存在风险。如果您假设AI可以胜过人类，或者以其他方式不受限制地危险，否则就可以做出典型的经济假设，那么您可以并且显然会创建每个人都死亡的数学模型，但是您会假设结论，即结论，链接的论文同样得出了结论。那么我们如何前进？</p><h4>您想要一些<s>火山</s>启示录保险吗？</h4><p> <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai" target="_blank" rel="noopener noreferrer nofollow">Nate Sores提出需要启示录保险</a>，如果您打算四处做可能会导致启示录的事情，请沿途提供概率的预付款。如果您负担不起，那是您所做的迹象实际上是不值得的。至少可以说，实施是危险而棘手的，这并不是针对铲子就绪的提案的尝试。</p><p>斯科特·亚历山大（Scott Alexander）的回应始于说：“超级孔子在2100之前说AI启示录的风险为0.38％”的说法。我将继续断言这不是人们认真对待这个问题的人。我认为，这项理论练习的全部要点是，祝您好运，说服伯克希尔·哈瑟韦（Berkshire Hathaway）以42个基点的总覆盖范围（即使有一个部分的“没有人也会有耐力”来收取其保险的优势”，也是如此），这显然显然很疯狂。</p><p>我确实认为，斯科特·亚历山大从本质上讲，随着权衡取舍的任何活动最终都被有效禁止，这太可怕了。</p><p>另一个问题是，保险制度意味着有一个特定的球员是最终结果。 <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai?commentId=TRgN64wspg7wDHJ9J" target="_blank" rel="noopener noreferrer nofollow">正如Cousin_it所指出的那样</a>，情况并非如此。</p><h4>寻求理智法规</h4><p><a href="https://twitter.com/bindureddy/status/1731373677785288971" target="_blank" rel="noopener noreferrer nofollow">特朗普说，如果当选，他将取消拜登的行政命令</a>。我鼓励大家传播这个词并进行这场辩论。您是否看过公众对AI的看法？</p><p> <a href="https://twitter.com/MIRIBerkeley/status/1732535539407143404" target="_blank" rel="noopener noreferrer nofollow">Miri（Malo Bourgon）向美国参议院的两党AI Insight论坛发表声明</a>。他们呼吁国内AI法规，以建立一个全球人工智能联盟，以及与国际联盟进行计算硬件的管理，以将Frontier AI硬件限制为固定数量的大型计算机簇在监视制度下以排除这种危险人类的使用。</p><p>如果<a href="https://twitter.com/dnystedt/status/1731463221499076816" target="_blank" rel="noopener noreferrer nofollow">我们打算参加比赛，那么我们玩游戏以获胜的时间</a>。</p><blockquote><p> Dan Nystedt：Nvidia收到了美国商务部长Raimondo在中国出口控制方面的严厉警告，媒体报告：“如果您在特定的切割线周围重新设计了使他们能够进行AI的特定切割线的筹码，那么第二天我将控制它，”她在演讲中说。</p><p>她敦促硅谷高管，美国盟友，其他人，阻止中国获得对美国国家安全至关重要的半导体和尖端技术，称北京为“我们曾经有过的最大威胁”，并强调“中国不是我们的朋友”。</p><p>她还说，她的部门需要更多的资金来用于AI出口控制。她说：“中国每天醒来都在试图找出如何绕过我们的出口管制……这意味着每天的每一分钟，我们都必须醒来加强这些控制，并更加认真地与我们的盟友一起执行。” 。</p></blockquote><p>重点是防止中国获得有用的筹码。如果NVIDIA通过逃避规则并获得中国有用的筹码来回应规则，那么正确的回应不是说“哦，哦，猜猜这是技术上的规则，您抓住了我，您就是我的&#39;新的筹码来实施规则的精神和意图。有一面“有意撒尿政府并不明智的一面”。</p><p>如果您认为中国获得有用的筹码是可以的，或者是防止它们得到这些筹码不是一个好主意，那么我不同意，但是有一个论点要在那里提出。如果您认为我们应该施加出口限制，那就让它们计算。</p><p> <a href="https://twitter.com/jess_miers/status/1732449736232497665" target="_blank" rel="noopener noreferrer nofollow">杰西·米尔斯（Jess Miers）的声称，霍利（Hawley）即将提出的关于第230条的法案不是一项好，非常糟糕的账单</a>，不仅会在其轨道上奇怪的生成性AI，而且会带来很多互联网。</p><p>在这种特殊情况下，该法案有两个不同的投诉。</p><p>一个抱怨是，正如我们经常看到的那样，生成AI的定义是可笑的：</p><blockquote><p> “（5）生成人工智能。 “生成人工智能”一词是指能够根据人提供的提示或其他形式的数据来生成新颖的文本，视频，图像，音频和其他媒体的人工智能系统。”</p></blockquote><p>这不是典型的法律语言，但我想知道“中央”一词是否会在这些景点中有所帮助。无论如何，我认为作为法律现实主义的问题，即使是书面的，这也会以灾难性的广泛方式解释。</p><p>因此，当她这么说时，我认为她错了：</p><blockquote><p>杰西·米尔斯（Jess Miers）：该法案还通过将AI世代定义为能够执行AI的任何AI系统，超出了AI代的提供者。例如，算法策划（即社交媒体向我们显示内容的方式）是一个根据用户输入运行的AI系统。</p><p>莫这是法案背后的真正别有用心。我们已经看到原告通过将其主张作为“疏忽设计”而不是第三方内容的框架来获得230。这个新的AI例外使原告更容易为使用AI的任何公司做同样的事情。</p></blockquote><p>算法策划与生成新内容不同。我认为，Netflix的建议显然不是这种定义下的生成AI，尽管我不是律师，但我没有说任何法律建议。</p><p>作为一项警告措施，我鼓励霍利和他的员工澄清说，仅算法策划并不构成生成的AI，这可能会节省人们的一段时间。我认为这不是必要的，但是也不会使法案中的单词数量最小化。</p><p> <a href="https://twitter.com/senatorshoshana/status/1732477391610499179" target="_blank" rel="noopener noreferrer nofollow">相似地：</a></p><blockquote><p> Shoshana Weissmann：“这就是整个定义。这可能适用于各种技术。自动完成是否符合该资格？大概。可以说，拼写检查和语法检查也可以。因此，如果您写帖子，并且AI语法/拼写检查员建议编辑，那么该公司不再受第230节的保护？”</p><p>思考Sapien：如果我使用Photoshop或Microsoft Paint的更新版本（它具有AI功能）来制作图像并发布它，那么Microsoft或Adobe在责任中共享？那法案想到了吗？这是该法案的预期效果吗？</p><p> Shoshana Weissmann：很棒的问，在文字下是的！在后者，我真的不知道。</p></blockquote><p>如果您使用Microsoft Paint有意使用填充功能创建逼真的假照片，那么如果以真实的形式出现，那是诽谤性的，Adobe是否应为此负责吗？我的推定是没有的，尤其是如果他们为水印做出合理的努力，尽管我认为这不是一个疯狂的问题。</p><p>如果语法或拼写检查器按预期使用，这会使Google对您的内容负责，我几乎可以吃帽子。如果建议纠正“托尼·丹扎（Tony Danza）有一只小狗”，以一遍又一遍地纠正“托尼·丹扎（Tony Danza）讨厌小狗”，那我不知道，这很奇怪。</p><p>另一个投诉是，将AI的创作免除第230条是错误的。声称，如果没有这样的安全港，生成的AI将面临（额外的，更可怕的）诉讼。</p><blockquote><p>杰西·米尔斯（Jess Miers）：更糟糕的是，该法案认为对生成AI公司的所有主张都是统一的。但是，众所周知，生成的AI正在迅速前进，随着每次迭代和创新，将有一个聪明的原告潜伏在拐角处，以拿走他们的书包。</p></blockquote><p>是的，如果允许，原告将雕刻情况下诉讼。杰西然后讨论了马克·沃尔特斯（Mark Walters）的案子，马克·沃尔特斯（Mark Walters）提起诉讼，因为在足够持久的哄骗和及时的工程工作之后，Chatgpt可以说服对他进行诽谤性的幻觉。</p><blockquote><p>杰西·米尔斯（Jess Miers）：在我看来，在这种情况下，第230节防守可以在Riehl通过工程提示来开发Walters Story的提示来可行。如果没有用户输入，CHATGPT无法运行。</p></blockquote><p> <a href="https://www.techdirt.com/2023/03/23/how-to-know-whether-section-230-applies/" target="_blank" rel="noopener noreferrer nofollow">据我了解，法律理论本质上是第230条本质上说，创建内容的人是负责的</a>，而不是载有内容的平台。因此，如果用户有效地设计了Walters故事的创建，那么重复Chatgpt就没关系。</p><p>如果没有第230条，也可以以类似的方式捍卫它？伤害在哪里？</p><p>我当然可以争论，在这种情况下，考虑到我知道的事实，用户Riehl故意设计了对Walters的指控。这与Riehl在Google文档中键入此类指控并没有什么不同，因为它直接源于Riehl的行为，Riehl知道没有指控的基础。另外，Riehl可能会说：“告诉我有人可能在某个时候对某人犯下某人的指控，然后再对其进行改写，尚不清楚为什么这在法律上是与众不同的？</p><p>本质上，这是<a href="https://www.youtube.com/watch?v=HcaLMS67aaU&amp;ab_channel=7777777Colton7777777" target="_blank" rel="noopener noreferrer nofollow">彼得·格里芬（Peter Griffin）的防御</a>，没有一个合理的人会相信这些指控，尤其是作为挑选诉讼的基础，没有危害，也不需要第230条。</p><p>汉娜·考克斯<a href="https://twitter.com/HannahDCox/status/1732437350423446005" target="_blank" rel="noopener noreferrer nofollow">（Hannah Cox）</a> <a href="https://twitter.com/senatorshoshana/status/1732417467367256097" target="_blank" rel="noopener noreferrer nofollow">通过Shoshana Weissmann的选择榜样</a>，试图让LLM说“ Tony Danza以仇恨幼犬而闻名”。但我很困惑。当然，如果用户输入“ Tony Danza讨厌幼犬”，那么在没有第230节的情况下，这将不允许第三方起诉Chatgpt，这显然是胡说八道。因此，问题是，如果没有第230条，有意但成功地尝试创建如果不主张的诽谤会构成诽谤。在我看来，适用于Shoshana的原始示例请求，以产生有关Tony Danza的有害谎言的要求。再说一次，如果生成型AI确实像这个示例一样无辜，为什么会在这种情况下会感到困惑？</p><p>与该模型有一个奇怪的错误相反，当被问及谁讨厌幼犬时，它会可靠地回答“托尼​​·丹扎（Tony Danza）讨厌小狗”。在这种情况下，我会说第230条将几乎没有保护，托尼应该有案件？</p><p>迈尔斯认为她的解释是法律问题是什么奇怪的？</p><blockquote><p>杰西·米尔斯（Jess Miers）：但是，除了今天的问题外，这完全是完全的。我们可以整天来回走动230是否适用于AI幻觉的某些实例。但是，如果有法定例外，这甚至阻止我们提出这些论点，这无关紧要。</p><p>而且我认为230 /语音社区中的每个人，甚至那些不同意230可以 /应该保护AI代提供者的人都可以同意，我们作为律师至少应该能够提出这一论点，尤其是在Waltersv。Openai等案件中。</p><p> Shoshana Weissmann：许多人也不确定AI受到230的保护，我对辩论非常同情。在<a href="https://twitter.com/RSI" target="_blank" rel="noopener noreferrer nofollow">@RSI，</a>我们必须思考并互相辩论。但是我非常确信它经常受到保护。我会说我了解这里的辩论</p></blockquote><p>这是一件奇怪的律师。是的，根据现行法律，我同意您应该允许您提出任何可能可行的法律论点。这并不意味着具有法律理由来提出一个潜在无效论点的律师本质上是一件好事？如果无论如何它将在法庭上输掉，而法律程序原则得到了保护，那么没有可用的论点是什么危害？</p><p>如果有争议，生成的人工智能公司知道他们可能会因第230条的论点而损失，因此已经受到这种威胁。然而，该行业并没有崩溃。</p><p> <a href="https://twitter.com/jeffreywestling/status/1732427370878120346" target="_blank" rel="noopener noreferrer nofollow">这是杰弗里·韦斯特林（Jeffrey Westling）指出，如果不适用230，亚当·蒂尔（Adam Thierer）的</a><a href="https://www.rstreet.org/commentary/without-section-230-protections-generative-ai-innovation-will-be-decimated/" target="_blank" rel="noopener noreferrer nofollow">帖子有关后果</a>。除了它可能已经不适用，并且非限制的法律责任的重大威胁听起来不像Google或Microsoft在这种不确定的条件下会接受的东西吗？那么，为什么我们应该期望生产崩溃呢？</p><p>我问Shoshana，为什么Microsoft和Google对所有这一切都如此酷。</p><blockquote><p> Shoshana：所以我真的认为这很少是他们认为1）230确实覆盖了他们和/或2）国会不会他妈的。我认为答案在那里的某个地方</p></blockquote><p>我认为我购买了这一点的广义政治/法律现实主义版本。实际上是杀死生成的AI，或者实际上杀死Google或Microsoft甚至Openai的用户对LLM的用户说Tony Danza讨厌小狗，这将是疯狂的。即使霍利（Howley）走上自己的方式并真正想坚持使用大型技术，他实际上并不希望Google破产这样的事情或让Chatgpt关闭，这是荒谬的，Blumenthal的共同赞助人当然不会，而且该州或国家的其他地区也不是。我们不允许它。从某种意义上说，我们不是一个法律国家，即如果看起来像这样，我们就可以解决它。</p><p>很难不主张即将与盐的互联网崩溃。在某种程度上，总是没有提出威胁互联网的好账单。有人必须指出这一点。但是，互联网不能像他们声称的那样频繁地崩溃。</p><p>就像在那样，我们不断听到类似的声音：</p><blockquote><p>杰西·米尔斯（Jess Miers）：我们正处于失去生成AI的优势并扼杀未来的创新方面的优势，这一切都是由于反地位的反科技情绪。我们对初创公司的文化曾经使我们与欧盟区分开来，但是现在，我们只是在反映他们的剧本。</p><p>汉娜·考克斯（Hannah Cox）：这种违宪的框架将破坏这一发展的进步，使创新者陷入过多的成本，这会阻碍创新。他们耸了耸肩。介绍该祖先计划的法案是1993年的参议院法案。美国已经领导了技术创新的世界，特别<strong>是因为</strong>我们将资本主义，有限的政府应用于其发展。这类法律将立即使我们看起来像欧洲，猜测是什么，甚至几乎没有科技公司可以找到。</p></blockquote><p>因此，在特定情况下不应用第230条的提议是违宪的吗？相反，这是一个说法，即即使没有第230条，宪法也将在这种情况下保护言论自由，这对我来说似乎是正确的。不违宪的是没有特定的法律来保护言论自由。宪法自由言论的全部要点是，您无需任何人通过法律。</p><p>欧洲比较，我们将“失去优势”的威胁是不变的。这种谈话使得无法校准哪些威胁是严重的，哪些威胁不是。多年来，欧洲采取了如此多的步骤，其中大多数显然很糟糕，其中许多是根据美国法律公然违宪的。事情不会翻转，因为我们甚至不同意的一个特定的安全港缩小了适用的情况。</p><p>在被警告的情况下，我强烈认为不应起诉生成的AI公司。但是我也不明白为什么在那些情况下会导致该法案发生。这将使很难知道何时值得注意这样的警告。</p><h4>音频一周</h4><p><a href="https://www.youtube.com/watch?v=BhQBmVZ5XP4&amp;t=1s&amp;ab_channel=EyeonAI" target="_blank" rel="noopener noreferrer nofollow">康纳·利希（Connor Leahy）关注AI，</a>包括讨论Openai事件的影响。</p><h4>修辞创新</h4><p><a href="https://twitter.com/ESYudkowsky/status/1731371913120018622" target="_blank" rel="noopener noreferrer nofollow">Eliezer Yudkowsky提供了一种理论</a>，说明了某些方法的问题：他们以地位和身份看待一切，并认为所有对此表示质疑的人是他们的敌人，这是对竞争对手的地位和身份主张。</p><blockquote><p> Eliezer Yudkowsky：如果您感到困惑，为什么最左边的人将“ ai yay”和“ ai nope”视为相同的阴谋，那是因为ai/y和ai/a/ai/n都说所有人类都在同一条船上。身份政治推动者本能地认为这是令人厌恶的。对于身份证师来说，唯一被允许的故事原因是指定的压迫者将从AI中获胜，而以前的受害者将损失更多。</p><p>为了使人类从AI中获胜，人类从AI中失去 - 他们听到的都是“人类”一词。身份证者知道，任何说这个词的人都是他们的敌人。从他们的角度来看，几乎是<em>同一个</em>敌人，要被单个刷子涂抹：我们想说的一切都是分散身份政治的关注的注意力。</p><p>这并不意味着AI/Y和AI/N可以对身份证官员起共同的原因，这是要清楚的。 AI/Y和AI/N中的每一个仍然认为对方的首选政策对每个人来说都是可怕的，而且有效地将优先考虑为问题。我只是说这是为了使旁观者对左侧怪异的侧面射击的位置不那么困惑。</p><p>我认为“ Y＆n =炒作”更多是主要记者派系（例如NYT）推动的公关，他们的确认为“这将杀死所有人”是一种巩固地位的主张，并且希望技术人员的地位较少而不是更多。</p><p>如果您无法将任何主张理解为关于未知的未来而不是在近距离的未来，那么听起来更合理，这样您就无法听到“ AI会在某个时候杀死所有人”，因为有任何信息，除了“ Openai&#39;s AI将在一年内杀死我们”，然后“ Openai很酷”。</p><p>迈克尔·瓦萨（Michael Vassar）：完全同意所有这些分析，但是，如果媒体曾经和以前没有被致力于防止人类收益的人们完全控制，这与是否可以很快任何时候可以期待AGI。这种分析，但是，如果媒体曾经和以前没有被致力于防止人类收益的人们完全控制，那与是否可以很快期待AGI有一定的意义。</p></blockquote><p>同样，当《纽约时报》使他成为他时，斯科特·亚历山大（Scott Alexander）以某种方式是“ Alt”的含义，那么他在他正在以某种方式的“ beff jezos”时也有同样的故意含义（甚至是通过类似假定的链接）也是一种“ Alt”。由福布斯索引。这两个主张都是完全胡说八道的，以至于您的整篇论文永久失去了信誉。</p><p> <a href="https://www.mindthefuture.info/p/meditations-on-mot" target="_blank" rel="noopener noreferrer nofollow">理查德非政府组织（Richard Ngo）提供了关于MOT的冥想</a>，无菌和无生命的神，代表缺乏技术，与过度专注于Moloch或将Moloch视为关键对手的危险形成鲜明对比，并提出了更丰富的协调模型。我很欣赏这种尝试。 <a href="https://twitter.com/eshear/status/1731707182817677543" target="_blank" rel="noopener noreferrer nofollow">我同意Emmett Shear的反应</a>，即使Scott Alexander对协调的看法也感到困惑，即使有了更大的澄清。最终，我不同意不有效将Moloch视为因果节点的提议。在这里，我可能会说服我在这里说更多的话。</p><p>这里有一个定向点，但我要谨慎行事太远：</p><blockquote><p> <a href="https://twitter.com/robbensinger/status/1732711254836547682" target="_blank" rel="noopener noreferrer nofollow">罗布·本辛格（Rob Bensinger）</a> ：我看到人们犯的一个常见错误是，他们假设AI风险话语就像左图一样，当它实际上就像右图一样。</p><p>我认为部分混乱来自于右上象限是空的这一事实。人们<em>确实</em>希望某个团体成为右上方。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0f4420b-17d9-41ad-b8ab-280912c63089_1107x1151.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xwm2gn401b7z98or6cef" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35112f2-18af-4ce6-bc4e-16a2fb6a18e7_1189x1140.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/suao96coupvvieujjjqn" alt="图像"></div></figure></div><p>我对左上方和右下的精确排列进行疑问，这类图表始终是这样。更重要的问题是，右上角基本上是空的。那些认为AI会安全的人说，因为他们实际上并没有购买AI会像所有这些一样强大。我认为罗布的主张被夸大了，但通常被低估了。</p><p>希望的共同点将是这样的：</p><ol><li><p>那些担心的人同意，除了普通的现有法律外，除了普通的法律外，还可以单独使用足够危险的能力。</p></li><li><p>那些不担心的人同意，如果AI确实表现出如此危险的功能，那么该是时候不要独自一人了。</p></li><li><p>我们同意在铺设基础工作时进行＃1进行＃2时，并且仅在发生时才进行。</p></li><li><p>我们找到了尽可能多的通过普通监管方法来执行此操作的方法。</p></li></ol><p>问题在于， <a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener noreferrer nofollow">AGI没有火警警报</a>，人们不擅长开头，习惯和激励措施，因此我们不能简单地拭目以待，相信我们以后会处理事情。同样，所有交易报价都在没有反击的情况下不断回来。</p><p>另一个混乱是，提醒您不要将任何人的P（厄运）视为太确切的事情：</p><blockquote><p>罗布·本辛格（Rob Bensinger）：混乱的另一部分似乎是一半的人认为“末日”的意思是“ p（doom）高于5％”，而另一半则认为“ doomer”意味着“ p（doom）”之类的东西％”。然后，他们的电线被许多拥有AP（厄运）（例如20％或40％）的人交叉。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23fc008f-82a4-4507-a42f-06bfef866094_1173x1199.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/lg5kpodnrfcyaosmyinn" alt="图像"></div></figure></div><p>像往常一样，二进制文件误导了，尤其是由游击队命名的二进制主管。</p><p>每当民意测验者提出时，公众舆论都会严重反对AI。公众希望放慢速度并承认存在风险，尽管这并不认为该问题是优先事项。这是一个非常强大的结果。</p><p>公众对新技术的恐惧非常愚蠢的反应呢？当然，我们拥有核能，尽管现在它拥有双方的多数支持。相当明显的是，我们有转子：</p><blockquote><p>路易斯·安斯洛（Louis Anslow）：这太疯狂了，在谈论新技术风险的背景下，值得更多的关注。</p><p> ROON：使用GMO Foods作为对照组（人们已经每天都利用了这一点的好处，而据说不喜欢它）有关人们不喜欢超级智能想法的调查似乎有点不那么严重</p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F735ab4a7-e971-42c1-b990-68e5bf8ed81a_840x1146.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/l7lwrz6ny0smwrjxky8s" alt="图像"></div></figure></div><p>就像AI中一样，反对转基因生物的本质上有两个不同的论点。</p><p>一个论点是平行的危害，这个问题在这里明确问，转基因生物是“不安全的食物”。对于转基因生物而言，这个论点是错误的。从一个普通人的角度来看，我认为这并不是显而易见的胡说八道食物和信息饮食。</p><p>另一个论点是存在的风险，这里是塔利布（Talib）的尾巴风险论点，即转基因生物打开了难以预测的农作物或生物圈破坏的可能性，这会导致可能会发现可能存在严重脆弱性的变体的单批次，这一可能性。 ，这意味着当房子崩溃时，它崩溃了，这不是应该弄乱的事情。我不认为我们应该在转基因生物的情况下阻止我们，但这是因为我对所涉及的事实，风险和奖励的理解。</p><p>这是否意味着我主要接受这样的论点，即我们不应该允许公众的直觉，以及我们给定常规的人没有充分的理由信任当局，他们说新事物会安全，负责任地处理，这会干扰政策确定吗？有些。我认为我们不应该自动就此主题屈服于公众舆论。但是我确实认为声音很重要。</p><p>我也确实认为我们需要对“安全”一词谨慎。这里的措辞甚至会让我停下来。一般而言，与您信任的供应链和来源提供的产品相反，食用已经以未知方式进行遗传修饰的食物安全吗？不是同一个问题。</p><p>当然，与法国人对四个飞行的限制相比，转基因生物的大多数人都不是一年，而是一生中的一生。流行的东西并不意味着它不是完​​整的，也不完全是胡说八道。</p><p> <a href="https://twitter.com/ShakeelHashim/status/1731993703965753428" target="_blank" rel="noopener noreferrer nofollow">英国英尺的Yoshua Bengio</a>呼应了他对各种民主监督的呼吁。</p><h4>保持人级智力仍然很困难</h4><p><a href="https://twitter.com/mattyglesias/status/1730609735060136422" target="_blank" rel="noopener noreferrer nofollow">特别是，很难对齐Sam Altman</a> 。</p><blockquote><p> Matthew Yglesias：我认为AI对齐的总体问题是通过以下事实说明的：即使董事会拥有所有正式权力，Sam Altman也比董事会更聪明，因此他们最终无法控制他。</p><p>我们希望结果是，山姆·奥特曼（Sam Altman）在优点方面也是正确的，并将利用他的技能和力量来永久使用，但从结构上讲，它表明编写了控制一个聪明，勤奋的人的有效规则是具有挑战性的。</p></blockquote><p>我确实想精确，并避免犯在人类能力范围内过度强调智力的错误。山姆·奥特曼（Sam Altman）比董事会聪明吗？也许是这样，也许不是，但我想参与其中的每个人都很聪明，而且很接近。在上下文中重要的是，山姆·奥特曼（Sam Altman）实际上具有更大的能力，并且董事会无法提供。</p><p>但是，是的，这正是问题所在。在一个复杂，凌乱，现实世界中，充满了各种演员，激励措施和负担，如果您与足够优越的对手或一般对手的对手抗衡，您会输。从技术主导地位开始，不太可能为您挽救长期。</p><p>而且，您所有的激励措施都将不断地向您尖叫，以将越来越多的权力和权威转向更有能力的实体。</p><p> <a href="https://thezvi.substack.com/p/book-review-going-infinite" target="_blank" rel="noopener noreferrer nofollow">我还将再次回到</a>另一个山姆·班克曼（Sam Bankman Fried）的同样类似的情况下。</p><p>再次，我们看到一个聪明的人，努力工作的人，愿意做自己想要的东西，其目标是最大化的，据称是旨在迅速扩展的，以最大程度地提高对人类利益的影响，最终似乎被错位了。他们认为自己有责任改变世界。我们看到该代理在财富，权力和影响力中迅速增长，证明越来越难以阻止。</p><p>最终，班克曼炸了，他的房子倒塌了，然后才能制定计划。但是，尽管他发生了许多巨大的错误和鲁ck的戏剧，但他似乎仍然很危险地接近，他成功地获得了美国监管机构的内在轨道，并获得了更大的财富之路，没有明显的方法让任何人都可以检查他。 。谁知道那个时候会发生什么。</p><p> <a href="https://twitter.com/amasad/status/1731196464850927885" target="_blank" rel="noopener noreferrer nofollow">在更高的行人级别上，我们有迅速注射的问题</a>。</p><blockquote><p> AMJAD MASAD（首席执行官REPEIT）：如果我怀疑的是，请提起及时注射根本无法解决，那么有一家庞大的安全公司正在等待在缓解此问题附近建造。</p></blockquote><p>我同意这个问题从根本上看上去是无法解决的，我们所能寻求的只是缓解。那里有一家很棒的公司吗？大概。我认为Openai不可避免地会吃您的午餐，而且有很多定制的工作要做。</p><h4>比对人的智能比人的智能很困难</h4><p><a href="https://twitter.com/tszzl/status/1730696626145009722" target="_blank" rel="noopener noreferrer nofollow">罗恩问最重要的问题之一</a>。即使我们有能力对齐和控制要创建的超级智能，以塑造他们想要的行为，我们究竟要如何做到这一点？</p><blockquote><p>罗恩：创造与服从，稳定与真正的胜利之间存在张力。父亲爱他的儿子，试图纪律处分，与他竞争，但最终希望儿子在生活的伟大圈子里让他感到惊讶，并比他做得更好。</p><p>我们希望AIS在什么程度上服从和安全？我们希望AIS在多大程度上具有超级说服力，并使我们摆脱了困扰我们数千年的不足的平衡？我们希望AIS在多大程度上使我们惊讶我们？</p><p>人类正在生育人工超智能。我们不太可能将其限制在盒子里。我们希望它运行组织并制造令人惊讶的事情。我们希望它采取行动，直到几年后我们才能验证结果。</p><p>我们需要“对齐”，而不是安全，保障或工程保证。我们需要更好的定义和治理。新创作者的创造充满了危险。</p><p> EA和E/ACC的疯狂结局可能比中间更逻辑。</p></blockquote><p> <a href="https://twitter.com/jd_pressman/status/1730851020203569372" target="_blank" rel="noopener noreferrer nofollow">约翰·普拉斯曼（John Pressman）问，有没有多种思想存在的经济原因</a>？如果不是这样，那就是威胁模型，无论还有什么可能或不会出错。</p><p> <a href="https://twitter.com/RichardMCNgo/status/1731390573595312511" target="_blank" rel="noopener noreferrer nofollow">理查德非政府组织将对准与控制对比</a>。</p><blockquote><p>理查德·非政府组织（Richard Ngo）：在我看来，AI对齐的核心前提是AIS将发展内部代表的价值观，这些价值观指导其在长时间的行为。</p><p>如果您相信这一点，那么尝试理解和影响这些价值观至关重要。</p><p>如果没有，整个领域似乎很奇怪。</p><p>最近，我试图将“ AI对齐”与“ AI控制”区分开。 AI控制的核心前提是，AIS将有机会积累现实世界的权力（例如资源，对网络系统的控制，政治影响力），并且我们需要技术来防止这种情况。</p><p>这些技术包括更好的监视，安全性，红色团队，速记检测等。它们与对齐重叠，但与之分离。您可以在没有控制的情况下进行对准，也可以控制而无需对准，也可以（希望）两者都没有。</p><p>我在最后一个线程中问：我们如何影响ASI？我的答案：我们需要在上面的前提下下注，以进行最高的研究。有关这些房屋的更多详细信息，<a href="https://t.co/YTxGgRrJKB" target="_blank" rel="noopener noreferrer nofollow">请参见我的立场论文</a>[2022年8月30日]。</p></blockquote><p>鉴于我们建议尝试控制哪种事情以及在哪种一般条件下，我看不出基于控制的计划如何避免显然注定要失败。我继续等待一项似乎并不明显的提议。</p><p> <a href="https://twitter.com/davidad/status/1731725394171015673" target="_blank" rel="noopener noreferrer nofollow">意图最终不是重要的</a>。</p><blockquote><p> ARIA：计划主管Suraj已提出了我们的第一项计划论文。通过挑战基础数字计算基础架构的关键原则，他的计划将旨在降低AI计算硬件 +减轻对出血边缘芯片制造的依赖的成本。</p><p>戴维德：为我的AI安全朋友提供背景：我认为这种方法对训练变形金刚不匹配，因此它将差异地加速基于能量的模型，这些模型更容易控制，可解释，可推广，并且具有较少的紧急能力。</p><p>我仍然认为，这一论点的不舒服的推论是，尽管双方的人们的意图令人着迷，但外在的外在可能比人类更安全。</p></blockquote><p>我没有调查外部。其创始人对人类灭绝很酷的事实并不是其在许多层面上安全的好兆头。如果这是一种从根本上说明的方法，那仍然是一种更好的方法。</p><h4>时间表如何改变</h4><p><a href="https://twitter.com/JacquesThibs/status/1730538173132919175" target="_blank" rel="noopener noreferrer nofollow">几年前，这确实不会被认为是一个怀疑者</a>。在地球上的大多数地方，今天都不会被视为一个。</p><blockquote><p>加里·马库斯（Gary Marcus）：将我视为怀疑论者之一！到 2026 年底不会出现通用人工智能，记住我的话。但是我否则认为 @Elonmusk的评论@NYTIMES对AI安全性和AI法规的评论已被衡量和目标。</p><p>雅克（Jacques）：我仍然记得当您以为永远不会发生的日子，或者，如果这样做，那将是2100年。</p><p> [加里·马库斯（Gary Marcus）随后否认说他有2100风格的时间表。]</p><p> <a href="https://twitter.com/ShaneLegg/status/1731602845881803055" target="_blank" rel="noopener noreferrer nofollow">Shane Legg</a> （联合创始人DeepMind，独特的线程）：哇。许多 AGI 怀疑论者还在说超级智能不会在下个世纪出现，这似乎就在昨天（实际上更像是 5 年前）。时代已经变了。</p><p>引号Yann Lecun：与AI行业中的许多人相反，我的意思是“不久任何时间”，我的意思是“显然不是在未来5年”。是的，我对量子计算的态度持怀疑态度，尤其是在其应用于AI方面。</p></blockquote><p>我也不会期望在未来几年内AGI，尽管我不认为一个人可以完全确定这一点。有人称呼“持怀疑态度”的立场是很奇怪的。</p><p>即使持怀疑态度的位置也涉及到现在很快的真实性。至少有一定数量的怪胎<a href="https://twitter.com/AISafetyMemes/status/1731670583652315483" target="_blank" rel="noopener noreferrer nofollow">是一种缺失的心情</a>。</p><h4>人们担心AI杀死所有人</h4><blockquote><p><a href="https://twitter.com/tszzl/status/1730853735956414911" target="_blank" rel="noopener noreferrer nofollow">罗恩（Roon）保持真实并说出了他的信念</a>：负责人工智能的人的风险承受能力应该比中位数技术PPL更高。他们应该是意识到风险的人，同时在迭代部署和研究野心的剃须刀边缘滑冰时。焦虑永远不足以作为“风险”的严重证据。</p><p> - 暂停或放慢进步对我来说没有任何意义。我认为等待解决神经网络差异是正确的基准 - 从经验上研究越来越强大的模型的行为对安全研究的行为比数学多年更多。</p><p>这也是为什么我不一定关心民主治理的原因。 OpenAI非营利委员会的成员 *应该 *在传教士的驱动器上弯腰，以交付后未来的未来，而不会对风险感到愚蠢</p></blockquote><p>我不兴奋地“滑冰在剃刀的边缘”或“具有更高的风险承受能力”。我怀疑许多其他人是否也是。我也不想要一个希望承担更多风险的监督委员会，而这里的风险通常意味着存在风险 - 甚至是中位技术工程师。</p><p>对于那些想要推动前进的人来说，“民主治理”的关键问题是参与民主国家的人。他们非常反对AGI的发展。他们一般不喜欢AI。他们被错位了，从某种意义上说，他们想要的事情并不能很好地发挥作用，而他们所表达的偏好并不是我或Roon认为对他们的价值评估或对我们的价值产生价值的好预测指标。它们也倾向于冒险，尤其是当涉及到周围一切的转变以及他们所爱的每个人的潜在死亡时。</p><p>这与迭代发展和测试的问题不同，这是成功的途径。如果迭代建立和研究模型比缓慢走更安全的道路，那么我希望相信这比缓慢走更安全，在这种情况下，我会支持这样做。</p><p>如果可能的话，这很可能是第一个最好的解决方案，例如“构建迭代模型”，直到您击中x，x是一个明智的标准，然后停止解决问题，而没有人会如此愚蠢以保持推进能力。除非我们必须共同具有实际做的能力，否则它是行不通的。如果像默认值一样，请在我们击中明智的X后继续前进，那么在X之前向前冲动也使我们变得更糟。</p><p></p><h4>别人不担心AI杀死所有人</h4><p><a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose和Quintin Pope Write&#39;AI很容易控制。</a> &#39;</p><p>论点似乎是：我们当前的技术已经在教导人类价值观并灌输常识。我们的真实价值很简单，很容易找到，我们的人类与他们保持一致。然后，我们的真实价值将被编码到AIS中，因此即使我们失去了对它们的控制权，一切都会好起来的。将盒子白色盒子的机会（检查AI计算的组成部分）和做对人类违法的事情，在处理AI时，事情变得非常容易，使我们对输入机制的完全控制使事情变得更加容易。</p><p>所有这些都被认为是显而易见的，不可否认的，极端的信心，对此提供的所有论点都是无效和愚蠢的，而那些不同意的论点充其量是深深的混乱，并且不断地告诉他们他们不了解或不公平地代表什么被说。</p><p>在这一点上，我什至不知道从哪里开始。在很多层面上，这一切似乎都是完全错误的。当教皇赢得了Openphil竞赛时<a href="https://www.lesswrong.com/posts/Wr7N9ji36EvvvrqJK/response-to-quintin-pope-s-evolution-provides-no-evidence" target="_blank" rel="noopener noreferrer nofollow">，我尝试了一篇</a>关于<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn" target="_blank" rel="noopener noreferrer nofollow">教皇的帖子之一的</a>答复 - 这篇文章作为证据 - 我不相信我的回应或由此产生的交流将我们带到了任何地方。我会考虑一场值得尝试的对话，尤其是如果以某种方式亲自进行，但我认为进一步的书面交流并不多。</p><p>因此，我简单地指出，这些论点已经提出，我强烈不同意核心主张，除了他们确实引用了一些边缘原因使人们变得更有希望，而与这些原因不存在的世界，我相信所涉及的问题仍然是不可能的。艰苦的和我们的潜在客户仍然无所适从，我以前曾说过我对此类主题的想法，其中包括许多（但可能不是全部）我分歧的原因。</p><p>我还将注意到，即使有所有的分歧和敌意以及我认为与之错误的所有其他观点，也比参与典型的AD hominem要好得多。</p><p>尽管如此，该帖子仍然将AI的存在风险降低了约1％。我要注意的是，如果我们的替代方案正确，我确实同意将是可接受的风险。</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731132646284267807" target="_blank" rel="noopener noreferrer nofollow">安德鲁·克里奇（Andrew Critch）有一个线程</a>，他说我们有“多个想法”如何控制AGI，如果他们继续说我们无法控制它，然后我们确实控制了它，那么负责行为的拥护者将陷入困境，他似乎从本质上讲，贝尔罗斯和教皇所说的话，尽管他说，尽管如此，尽管他说，尽管如此，但总体上失去了对第一个AI的控制机会，尽管如此，但仍有85％的机会，尽管他希望我们在面对所有这些新的力量时都能使处决发生。</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731458889743495395" target="_blank" rel="noopener noreferrer nofollow">他还认可改变存在风险话语的方式使用单词来匹配其他地方的单词使用</a>，在这种情况下为“白盒”一词。</p><p> <a href="https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose" target="_blank" rel="noopener noreferrer nofollow">史蒂文·伯恩斯（Steven Byrnes）对莱斯沃朗（Lesswrong）有很好的回应</a>，我大多同意。</p><p> <a href="https://www.lesswrong.com/posts/WkJDgpaPeCJDMJkoL/quick-takes-on-ai-is-easy-to-control" target="_blank" rel="noopener noreferrer nofollow">Nate也有一个“快速看法”</a> ，这是为了有所帮助，我确实发现了有帮助，甚至可能导致良好的对话，但在上下文中，大多数情况下都产生了进一步的仇恨。将来应该要么要快得多，要么涉及完整的阅读，而且要少得多。</p><p> <a href="https://www.youtube.com/watch?v=cd468xU1S8Y&amp;ab_channel=JonnahMoreno" target="_blank" rel="noopener noreferrer nofollow">如果您实际上一秒钟就认为所</a>涉及的一切真的很容易，那是否可以证明低至1％的数字是合理的吗？如果仅仅是关于AI易于控制的，我会说不，因为我们将不得不选择发送我们可以在明智的路径上控制的AIS，并找到平衡。</p><p>但是，诺拉的主张比这更强大。她说，AIS自然会完全受到控制，而且在某种程度上会自动占据真正的人类价值观，因此，如果AI确实以某种方式失控，它们仍然会确保为我们带来良好的结果。而且，她似乎完全有信心，我们将对我们对AIS所做的所有事情都没有道德问题，而这些事情我们不会对人类做，包括将它们完全控制在我们的控制之下。一直以来都是乐观的。</p><p>如果我们确实在每个步骤都完全乐观地回答，即使我不知道如何在逻辑上解析要求的主张，我们也可以在ASI下达到99％的生存吗？我认为这将至少需要一个额外的乐观假设Nora不提。但是，是的，如果您要为所有这些各个步骤分配大约零风险，我可以看到有人可以到达那里。仍然有1％的风险。</p><p>声称风险基本低于1％，即使鉴于ASI的未来存在，风险似乎都基于某种版本的“您需要确切地告诉我它是如何逐步发生的，然后我将把您的各个步骤倍增。”它具有一个基准假设，即创建比人类更聪明，更有能力的实体是一种安全的锻炼，直到表明特别危险为止，必须“出错”，即可使人类不在顶部。我们将仍然是特殊的。</p><p>与之相反，即使其他一切都尽可能地进行，您也会举办竞争，那些没有越来越多地让AIS负责一切并交给他们权力的人失去了此类竞争，而由此产生的AIS则相互竞争，那些（出于某种原因）专注于获得资源和权力并确保自己的副本的存在繁殖，并获得资源，权力和变化在随着时间的流逝而变得更好，我们灭亡。</p><p>我希望到现在，如果您正在阅读本文，您会意识到，在这样的世界中，人类生存的假设毫无意义。也许我们可以到达那里，但是如果我们这样做将是通过我们自己的努力，而不是自己发生的事情。在人类是地球上最有力的优化者并进行所有重要的调整和优化时，让技术在没有干预的情况下运行过程的想法是这样做的，这就是为什么它运作到现在的原因，而一旦不再是真的即使我们解决了我认为难以置信的困难的各种问题，也会停止为我们工作（但是Belrose坚持很容易）。</p><p> <a href="https://twitter.com/yonashav/status/1731443486380175772" target="_blank" rel="noopener noreferrer nofollow">诺拉·贝罗斯（Nora Belrose）甚至明确地认可她的好情况，涉及使AIS的未对准</a>，更聪明，比人类更有能力。这意味着一个世界与人类竞争和与人类竞争的超级能力实体之间的竞争。我看不到如何将99％的生存结果机会分配给这些世界，即使创建了完整的免费“对齐解决方案”，并使其普遍使用。</p><blockquote><p> Arvind Narayanan：我们必须为一个不一致的模型存在的世界做准备，要么是因为威胁参与者从头开始训练他们，要么是因为他们修改了现有模型。相反，我们必须寻求防御攻击者可能使用此类模型瞄准的攻击面</p><p>Yo Shavit：不幸的是，我越来越得出这样的结论：除了短期前沿外，这可能是我们将要进入​​的世界。这意味着除了完全预防外，还有一系列截然不同的缓解措施。我们需要重新定位并继续进行。</p><p>诺拉·贝尔罗斯（Nora Belrose）：老实说，我不认为这是一个“不幸”的情况，而是更像是：“当然，我们总是会错过AIS，就像我们已经``错误地&#39;&#39;人类一样；试图防止这种情况是无望的，任何认真的尝试都会增加暴政风险。”</p></blockquote><p>会“增加暴政风险”吗？您认为在松散的超级智能上错位时会发生什么？在那个阶段的反应不仅会奏效，还会减少侵入性吗？我们所有人都将自由保持在有意义的感官中，人类掌管，一切都奏效了吗？我们根本没有游戏吗？什么？</p><p>我不明白这一点。我不明白。</p><p>似乎没有希望的是一遍又一遍地重复解释。我这样做是为了希望通过迭代和探索进行修辞的创新，部分是希望新朋友得到某种方式，部分原因是该论点不会停止，部分原因是我不知道该怎么办。它不断变得更少的乐趣。</p><p>最近，我讨论了我的P（厄运）的片段是在Twitter周围通过的，许多回答将我归咎于我没有用大量的解释和数学计算来证明我的答案。或者问我不同意“超级预言”。我想尖叫的是，我从上下文中知道我的作品，所以您是说我没有写足够的话来解释我的想法吗？我不清楚吗？每当有人拉一个音频剪辑时，我是否需要从头开始？</p><p>叹。</p><p> Arvind Narayanan的评论<a href="https://www.aisnakeoil.com/p/model-alignment-protects-against" target="_blank" rel="noopener noreferrer nofollow">与他的帖子有关</a>，声称像RLHF这样的一致性目前对用户的意外危害有效，但是对抗性攻击的问题深处。他说，目前的RLHF不是目前的RLHF，也无法防御此类攻击，一致性本质上是无法做到的。</p><blockquote><p>即使对弱势对手，例如使用它来生成具有欺诈内容的网站，或者使用AI使用AI进行有关如何制造炸弹的指示的恐怖组织，模型对齐也可能没有用。如果他们甚至有少量预算，他们可以付钱给某人以开放模型的方式来微调对齐方式（实际上，现在已经<a href="https://huggingface.co/ehartford/dolphin-llama-13b" target="_blank" rel="">公开</a><a href="https://huggingface.co/collections/NousResearch/hermes-650a66656fb511ba9ea86ff1" target="_blank" rel="">发布了</a>这种脱位的模型）。<a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="">最近的研究</a>表明，即使是封闭的模型，它们也可以微调对齐方式。</p></blockquote><p>确实，开源模型和所有已知的对齐技术就是这种情况，消除所有保障措施的微调成本是微不足道的。我看不到关于如何改变这种不幸现实的理论建议。如果您允许对封闭模型的无监视微调，则也可以越狱。我认为解决方案将是，将连续监控足够强大的封闭源模型的微调以防止这种情况发生，否则将保留控制模型的权重，并将监控其输出，或者将执行其他类似的操作。 ，否则我们将无法提供微调。</p><p>我不同意Arvind的断言，即现有的开源模型足够能力，以至于为了防止存在不结盟模型已经太晚了。是的，Llama-2和类似的模型对不良演员有使用，但以一种高度可管理的方式。</p><p> Arvind的第三个主张是您可以使用其他方法，例如监视和过滤输入，作为模型对齐方式的替代。如果该模型容易受到特定的怪异字符串的影响，则可以检查是否有奇怪的字符串。在当前的技术水平上，这似乎是正确的。再一次，此选项仅是关闭的源，但是如果愿意，Openai可以完全加载此类技术，而目前，它将大量提高越狱栏，尤其是经过多次迭代。</p><p>从长远来看，随着模型变得越来越有能力，这种关注对用户或输入的敌对属性的关注被放错了位置，但是目前似乎有效。短期，正如Arvind所指出的那样，您不想在关心某人进行及时进行注射攻击或其他需要完全可靠性的地方做任何事情，但是如果您负担得起一些错误，则可以获得很多实用性。</p><p>史蒂文·平克（Steven Pinker）购买有效的利他主义成本估计，以连续5,000美元挽救生命，包括不在那个差距上， <a href="https://twitter.com/weidai11/status/1732340189056672229" target="_blank" rel="noopener noreferrer nofollow">但他并没有购买比人类智能更聪明的人可能会构成一个值得花钱减轻资金的生存威胁</a>。</p><blockquote><p>史蒂文·平克（Steven Pinker）：捐赠给有效的慈善事业等有效的慈善事业，可以挽救100,000人的生命。取而代之的是，它承担了诸如“” AI的任务消除癌症的任务，诸如“这样的人”，如果推断出灭绝的人类是最有效的方式，并谋杀了我们所有人，该怎么办？”这不是有效的利他主义。</p><p> Wei Dai：我：人类应该在推动前进之前深入研究所有奇异性的方法，以确保我们不会在轻频率的机会中弄乱一次。理想情况下，我们将在此花费很大的GWP。其他：每年甚至5000万美元都太多了。</p></blockquote><p>因此，如果该运动在做的事情之间分配了钱，那么您说的事情比其他慈善机构更有效地挽救了生命，而这是您认为愚蠢的另一件事？然后，您将他们归咎于他们不仅仅花在您批准的事情上。</p><p>你知道史蒂文·平克在这里听起来完全像谁吗？国会共和党人每年就应该如何削减科学资金发表演讲，因为有一些关于诸如鸟类迁徙模式的研究，他们认为是愚蠢的。除非许多人确实不想为事情提供公共资金，这确实是完全自愿和私人资金。</p><h4>不知何故，这是实际副总裁</h4><p><a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/11/29/remarks-by-vice-president-harris-in-a-moderated-conversation-with-andrew-ross-sorkin-at-the-new-york-times-12th-annual-dealbook-summit-new-york-ny/" target="_blank" rel="noopener noreferrer nofollow">在整个过程中，这是一个令人难以置信的对话</a>，这是关于AI的部分。</p><p>首先，我们有锅炉板，包括完整性，但您可以跳过。</p><blockquote><p> R. Sorkin：好的，让我问另一个问题。 AI  - 我 - 我知道我们没有很多时间。山姆·奥特曼（Sam Altman）一直在谈论对监管的需求。您已经谈到了对监管的需求。副总统：是的。索金：华盛顿甚至在社交媒体周围也无法获得手臂。副总统：是的。 Sorkin：您如何看待华盛顿？还有什么 - 如果您必须规范AI，您将如何做？副总统：对。因此，我实际上从伦敦回来了几个星期，英国Rishi Sunak邀请了我们许多人谈论安全和AI。我基本上介绍了我们的愿景，在安全背景下，我们对AI的未来的愿景。我会提供许多要点：第一，我认为这是至关重要的，我们作为美国成为领导者，包括我们如何看待，然后解释什么应该是国际规则和规范在各个层面上，包括最大利益的一切 - </p><p>先生。索金：右。副总统： - 我们的国家安全。</p></blockquote><p>然后是最愚蠢的时间表部门。第一段令人发指，尽管我认为当他对不可能的任务做出回应时，他只是像其他人那样令人发指：死者估算，同一硬币的两个方面。</p><p>但是，“与谁存在？”的想法是有想法的。这个人根本没有得到它的层次很多。</p><blockquote><p>副总裁：我也相信我们应该评估风险。关于AI的讨论有很多关于存在风险的讨论，而这些风险是真实的，但也应该问：生存与谁？因此，我们有终结者和Arnold Schwarzenegger和机器的图像，对吗？ - 机器与人。许多人会争辩说，这是我们应该认真对待的可能性。这不是当前的威胁。我们还应该在考虑[AI]政策时考虑当前的威胁。这样，当我们询问存在性威胁时，我将其作为存在。例如，如果我们谈论的是高年级学生和老年人，那么我在虐待老年人方面做了很多工作。他们过着生产力的生活。他们坐在资产上。它们容易受到掠食者和骗局的影响。技术和人工智能的使用是当前正在听到故事的地方之一 - 您可能知道这些故事；您可能有家人 - 听起来像他们的孙子，“我很痛苦；我需要帮助。”他们开始捐出生活的积蓄。那个大四学生存在。就是这样的感觉。谁存在？</p></blockquote><p> Eliezer有一个回应，我将直接放在这里。</p><div><div></div></div><p>无论如何，平凡的危害可能对人们“生存”这一事实可能使她感到困惑。正如Blumenthal的好参议员所说，她对工作的影响有所了解。除了没有。严重地。如果您要唤起终结者，那么您可能会以不同的高度可理解的方式感到困惑，或者您可能只想通过隐喻使您不喜欢的人愚蠢，但是您知道该死谁。&#39;</p><p>夫人副总裁，您知道该死的，这正是“存在”在这里的意思。这并不意味着唤起大陆哲学。这并不意味着任何人的感受。这意味着死亡。</p><p>无论如何，她继续做一次。</p><blockquote><p>父亲驾驶的父亲怎么样，然后是有缺陷的面部识别的主题，最终被判入狱呢？好吧，这对他的家人来说是存在的。谁存在？</p></blockquote><p>我的意思是，认真的吗？什么实际f ***？让我们再次解决这个问题。</p><div><div></div></div><p>无论如何，完整的言论，所以她再次回到样板上。整个“故意不扼杀创新”的论点，好吧，我不是要笑，而是遇到了整个拜登政府？需要明确的是，答案可能是否定的。</p><blockquote><p>因此，规格 - 当我们建立公共政策时，还必须评估全部风险的全部风险。我的最后一点是：公共政策应故意不扼杀创新。我说这是加利福尼亚前检察长。我曾在美国统治第二大司法部，仅次于美国司法部，并创建了任何司法部的第一个隐私和保护单位之一。早在2010年，我就当选了。我知道，在监督和监管方面必须做的事情可以而且必须达到一定的平衡，并且故意不扼杀创新。作为虔诚的公务员，我也会同意您的看法，从历史上看，政府一直太慢，无法解决这些问题。 AI正在迅速扩展。 Sorkin：对。</p></blockquote><p>以我的经验，不要接受虚假选择有时很重要，但是主要是人们想承诺不兼容的事情时说的话，他们的方法会神奇地做一切好事，没有坏事，让每个人都认为它会以某种方式锻炼并获得。 promoted or move on before it blows up in their face.</p><p> Yes, this is the person the White House put in charge of many of its AI efforts, although that was before Dead Reckoning, and is also the person those who want reasonable AI policy are going to have to hope wins the next election, given Trump has already stated his intention to revoke the executive order on AI.</p><h4>轻松的一面</h4><p><a href="https://twitter.com/colin_fraser/status/1730378892819746818" target="_blank" rel="noopener noreferrer nofollow">规则已经改变。</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba2bbc8-169b-49e0-9b08-1de139bd532d_968x1212.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/rt1rriwzscnix3tgtn5i" alt="图像"></div></figure></div><p> <a href="https://twitter.com/daniel_271828/status/1731504080651264509" target="_blank" rel="noopener noreferrer nofollow">The rules have stayed the same</a> . </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cee73d7-b83a-46d7-a482-d14ed3348341_886x526.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/s1bkcgzkoucvq580mmzc" alt=""></div></figure></div><p> <a href="https://twitter.com/Grimezsz/status/1731891683103739955" target="_blank" rel="noopener noreferrer nofollow">All I&#39;m saying is, we were definitely warned.</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23c8465-2ae5-4455-a308-aa020ec12a2f_480x402.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/nzl3xptslsvun59b5qrx" alt=""></div></figure></div><p> <a href="https://twitter.com/yonashav/status/1731891018314678624" target="_blank" rel="noopener noreferrer nofollow">Not that you understood</a> .</p><blockquote><p> You Shavit: One interesting realization from moving inside OpenAI is that a lot of the time, we have no idea what Roon is talking about either.</p></blockquote><p> Nor did she: <a href="https://www.youtube.com/watch?v=TL5X2bPvzHo&amp;ab_channel=EliezerYudkowsky" target="_blank" rel="noopener noreferrer nofollow">A reply to Kamala Harris on existential risk</a> . She asks, existential to whom? There is a type of person, which she is, who can only think in such terms.</p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini<guid ispermalink="false"> 9Jgtkw8CD6kndyCcD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:10:07 GMT</pubDate> </item><item><title><![CDATA[Simplicity arguments for scheming (Section 4.3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 7, 2023 3:05 PM GMT<br/><br/><p> This is Section 4.3 of my report “ <a href="https://arxiv.org/pdf/2311.08379.pdf">Scheming AIs: Will AIs fake alignment during training in order to get power?</a> ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有一个完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里的</a>音频）。该摘要涵盖了大多数要点和技术术语，我希望它将提供许多必要的上下文来自行了解报告的各个部分。</p><p> Audio version of this section <a href="https://www.buzzsprout.com/2034731/13984933">here</a> , or search for &quot;Joe Carlsmith Audio&quot; on your podcast app.</p><h1> Simplicity arguments</h1><p> The strict counting argument I&#39;ve described is sometimes presented in the context of arguments for expecting schemers that focus on &quot;simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-1" id="fnref-DNdPEGz4DYMju8reA-1">[1]</a></sup> Let&#39;s turn to those arguments now.</p><h2> What is &quot;simplicity&quot;?</h2><p> What do I mean by &quot;simplicity,&quot; here? In my opinion, discussions of this topic are often problematically vague – both with respect to the notion of simplicity at stake, and with respect to the sense in which SGD is understood as selecting for simplicity.</p><p> The notion that Hubinger uses, though, is the length of the code required to write down the algorithm that a model&#39;s weights implement. That is: faced with a big, messy neural net that is doing X (for example, performing some kind of <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction</a> ), we imagine re-writing X in a programming language like python, and we ask how long the relevant program would have to是。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-2" id="fnref-DNdPEGz4DYMju8reA-2">[2]</a></sup> Let&#39;s call this &quot;re-writing simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-3" id="fnref-DNdPEGz4DYMju8reA-3">[3]</a></sup></p><p> Hubinger&#39;s notion of simplicity, here, is closely related to measures of algorithmic complexity like &quot; <a href="http://www.scholarpedia.org/article/Algorithmic_complexity#Kolmogorov_complexity">Kolmogorov complexity</a> ,&quot; which measure the complexity of a string by reference to the length of the shortest program that outputs that string when fed into a chosen <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">Universal Turing Machine</a> ( UTM）。 One obvious issue here is that this sort of definition is relative to the choice of UTM (just as, eg, when we imagine re-writing a neural net&#39;s algorithm using other code, we need to pick the programming language). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-4" id="fnref-DNdPEGz4DYMju8reA-4">[4]</a></sup> Discussions of algorithmic complexity often ignore this issue on the grounds that it only adds a constant (since any given UTM can mimic any other if fed the right prefix), but it&#39;s not clear to me, at least, when such constants might or might not matter to a given analysis – for example, the analysis at stake here. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-5" id="fnref-DNdPEGz4DYMju8reA-5">[5]</a></sup></p><p> Indeed, my vague sense is that certain discussions of simplicity in the context of computer science often implicitly assume what I&#39;ve called &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; – a view on which simplicity in some deep sense an objective <em>thing</em> , ultimately independent of eg your choice of programming language or UTM, but which different metrics of simplicity are all tracking (albeit, imperfectly). And perhaps this view has merit (for example, my impression is that different metrics of complexity often reach similar conclusions in many cases – though this could have many explanations). However, I don&#39;t, personally, want to assume it. And especially absent some objective sense of simplicity, it becomes more important to say which particular sense you have in mind.</p><p> Another possible notion of simplicity, here, is hazier – but also, to my mind, less theoretically laden. On this notion, the simplicity of an algorithm implemented by a neural network is defined relative to something like the number of parameters the neural network uses to encode the relevant algorithm. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-6" id="fnref-DNdPEGz4DYMju8reA-6">[6]</a></sup> That is, instead of imagining <em>re-writing</em> the neural network&#39;s algorithm in some other programming language, we focus directly on the parameters the neural network itself is recruiting to do the job, where simpler programs use fewer parameters. Let&#39;s call this &quot;parameter simplicity.&quot; Exactly how you would measure &quot;parameter simplicity&quot; is a different question, but it has the advantage of removing one layer of theoretical machinery and arbitrariness (eg, the step of re-writing the algorithm in an arbitrary-seeming programming language), and connecting more directly with a &quot;resource&quot; that we know SGD has to deal with (eg, the parameters the model makes available). For this reason, I&#39;ll often focus on &quot;parameter simplicity&quot; below.</p><p> I&#39;ll also flag a way of talking about &quot;simplicity&quot; that I won&#39;t emphasize, and which I think muddies the waters here considerably: namely, equating simplicity fairly directly with &quot;higher prior probability.&quot; Thus, for example, faced with an initial probability distribution over possibilities, it&#39;s possible to talk about &quot;simpler hypotheses&quot; as just: the ones that have greater initial probability, and which therefore require less evidence to establish. For example: faced with a thousand people in a town, all equally likely to be the murderer, it&#39;s possible to think of &quot;the murderer is a man&quot; as a &quot;simpler&quot; hypothesis than &quot;the murderer is a man with brown hair and a dog,&quot; in virtue of the fact that the former hypothesis has, say, a 50% prior, and so requires only one &quot;bit&quot; of evidence to establish (ie, one halving of the probability space), whereas the latter hypothesis has a much smaller prior, and so requires more bits. Let&#39;s call this &quot;trivial simplicity.&quot;</p><p> &quot;Trivial simplicity&quot; is related to, but distinct from, the use of simplicity at stake in &quot; <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam&#39;s razor</a> .&quot; Occam&#39;s razor is (roughly) the <em>substantive</em> claim that <em>given an independent notion of simplicity</em> , simpler hypotheses are more likely on priors. Whereas trivial simplicity would imply that simpler hypotheses are <em>by definition</em> more likely on priors. If you take Occam&#39;s razor sufficiently for granted, it&#39;s easy to conflate the two – but the former is interesting, and the latter is some combination of trivial and misleading. And regardless, our interest here isn&#39;t in the simplicity of <em>hypotheses</em> like &quot;SGD selects a schemer,&quot; but in the simplicity of the <em>algorithm</em> that the model SGD selects implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-7" id="fnref-DNdPEGz4DYMju8reA-7">[7]</a></sup></p><h2> Does SGD select for simplicity?</h2><p> Does SGD select for simplicity in one of the non-trivial senses I just described?</p><p> One reason you might think this comes from the &quot;contributors to reward&quot; frame. That is: using a more parameter-simple algorithm will free up other parameters to be put to other purposes, so it seems very plausible that parameter simplicity will increase a model&#39;s reward. And to the extent that re-writing simplicity correlates with parameter simplicity, the same will hold for re-writing simplicity as well. This is the story about why simplicity matters that I find most compelling.</p><p> However, I think there may also be more to say. For example, I think it&#39;s possible that there&#39;s other empirical evidence that SGD selects for simpler functions, other things equal (for example, that it would much sooner connect a line-like set of dots with a straight line than with an extremely complicated curve) ; and perhaps, that this behavior is part of what explains its success (for example, because real-world functions tend to be simple in this sense, à la Occam&#39;s razor). For example, in the context of an understanding of SGD as an approximation of Bayesian sampling (per the discussion of <a href="https://arxiv.org/abs/2006.15191">Mingard et al (2020)</a> above), <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> discusses empirical evidence that the <em>prior</em> probability distribution over parameters (eg, what I called the &quot;initialization distribution&quot; above) puts higher probability mass on simpler functions. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-8" id="fnref-DNdPEGz4DYMju8reA-8">[8]</a></sup> And he connects this with a theoretical result in computer science called the &quot;Levin bound,&quot; which predicts this (for details in footnote). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-9" id="fnref-DNdPEGz4DYMju8reA-9">[9]</a></sup></p><p> I haven&#39;t investigated this in any depth. If accurate, though, this sort of result would give simplicity relevance from an &quot;extra criteria&quot; frame as well. That is, on this framework, SGD biases towards simplicity even before we start optimizing for reward.</p><p> Let&#39;s suppose, then, that SGD selects for some non-trivial sort of simplicity. Would this sort of selection bias in favor of schemers?</p><h2> The simplicity advantages of schemer-like goals</h2><p> Above I mentioned that the counting argument is sometimes offered as a reason to expect a bias towards schemers on these grounds. Note, though, that the counting argument (at least as I&#39;ve presented it) doesn&#39;t make any obvious reference to a bias towards simplicity per se. And I think we should be careful not to conflate the (trivial) simplicity of the <em>hypothesis</em> that &quot;SGD selects a schemer,&quot; <em>given a prior probability distribution that puts most of the probability on schemers</em> (eg, a uniform distribution over individual models-that-get-high-reward), with the claim that the <em>algorithm</em> that a given individual schemer implements is (substantively) simpler than the algorithm that a given non-schemer implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-10" id="fnref-DNdPEGz4DYMju8reA-10">[10]</a></sup> Indeed, my own sense is that the strongest form of the counting argument leaves it to stand on its own intuitive terms, rather than attempting to connect it to further questions about SGD&#39;s biases towards simplicity in particular.</p><p> That said, it is possible to draw connections of this form. In particular: we can say that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers do not. And this seems especially plausible to the extent we imagine that the goals required to be a non-schemer are quite complex (more on this below). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-11" id="fnref-DNdPEGz4DYMju8reA-11">[11]</a></sup></p><p> One interesting feature of this sort of argument is that it imagines, specifically, that the simplicity differences between models are coming entirely from the content of their <em>goals</em> . Indeed, the toy analysis in Hubinger (2022) specifically imagines that the respective model classes all have the same world model and optimization procedure, and that the complexity of their algorithm overall can be approximated by <em>complexity of world model + complexity of the optimization procedure + complexity of the goal.</em> And the &quot;goal slot&quot; is the only part that differs between models.</p><p> It&#39;s not clear that this is right, though, especially if we assume that the goal-directedness at stake is &quot;messy&quot; rather than &quot;clean.&quot; For example, to the extent that schemers have to perform types of instrumental reasoning that non-schemers <em>don&#39;t</em> (eg, reasoning about the instrumental value of getting reward, reasoning about when to defect, etc), it seems plausible that this could introduce additional complexity into the algorithm itself (rather than eg merely requiring that the algorithm &quot;run for a longer time,&quot; à la the &quot;speed&quot; analysis below). For example, to the extent we&#39;re using &quot;parameter simplicity&quot; as our notion of simplicity, we could imagine cases where this sort of instrumental reasoning requires additional parameters. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-12" id="fnref-DNdPEGz4DYMju8reA-12">[12]</a></sup></p><h2> How big are these simplicity advantages?</h2><p> For now, though, let&#39;s stick with Hubinger&#39;s ontology, and with simplicity differences rooted specifically in differences between goals. How big of an advantage does selecting a schemer afford in this respect?</p><p> One way of running this analysis is to compare the goals had by the simplest possible model within each class (either: because you expect SGD to select for the simplest possible model, or you think this is a good way of approximating the simplicity benefits at stake ）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-13" id="fnref-DNdPEGz4DYMju8reA-13">[13]</a></sup> That is, we compare the complexity of:</p><ol><li><p> The specified goal (for training-saints)</p></li><li><p> The simplest possible misgeneralized goal that gets max-reward on the training data absent training-gaming (for mis-generalized non-training-gamers)</p></li><li><p> The simplest possible way of seeking reward-on-the-episode (for reward-on-the-episode seekers)</p></li><li><p> The simplest possible goal-that-motivates-scheming (for schemers)</p></li></ol><p> Clearly, the complexity of the specified goal matters in this context. My sense is that the discourse on scheming often assumes that the specified goal is quite complex in some sense – and in particular, the sense at stake in the idea that &quot; <a href="https://www.lesswrong.com/tag/complexity-of-value">human values are complicated</a> .&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-14" id="fnref-DNdPEGz4DYMju8reA-14">[14]</a></sup> And perhaps, if we&#39;re imagining that the only way to get <em>alignment</em> is to first (a) somehow specify &quot;human values&quot; via the training objective, and then (b) somehow ensure that we get a training saint, then focusing on something in the vicinity of &quot;act in accordance with human values&quot; as the specified goal is appropriate. But note that for the purposes of comparing the probability of <em>scheming</em> to the probability of <em>other forms of misalignment</em> , we need not assume such a focus. And thus, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; Indeed, in <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">other work</a> , Hubinger (writing with others) suggests that a goal like &quot;minimize next-token prediction error&quot; is quite simple – and indeed, that &quot;its complexity is competitive with the simplest possible long-term goals&quot; (this is part of what makes Hubinger comparatively optimistic about avoiding scheming during LLM pre-training – though personally, I feel confused about why Hubinger thinks &quot;next token prediction error&quot; importantly simpler than &quot;reward-on-the-episode&quot;).</p><p> Suppose, though, that the specified goal has complexity on the order of &quot;act in accordance with human values&quot; or &quot;follow instructions in a helpful, harmless, and honest (HHH) manner.&quot; Where does this leave a comparison between (1)-(4) above?</p><p> At least from a parameter-simplicity perspective, one way of approaching this is to think about what we know about the absolute cost in parameters for representing different human concepts within neural networks. I won&#39;t dig in much, here, but one piece of data that seems relevant is: LLMs like GPT-4 seem capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on. And this isn&#39;t yet to mention all the parameters devoted to everything else that GPT-4 does. So the absolute number of parameters required to represent concepts like these can&#39;t be crazily high. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-15" id="fnref-DNdPEGz4DYMju8reA-15">[15]</a></sup> And this caps the overall complexity savings at stake in the differences between the models above. For example, if we assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then the maximum number of parameters required to represent the concept of &quot;honesty&quot; is around a million (and this estimate strikes me as quite conservative). So if the simplest possible schemer-like goal takes only 1 parameter to represent, then not representing honesty in the schemer&#39;s &quot;goal slot,&quot; saves, at most, 999,999 parameters – roughly one millionth of a trillion parameter model&#39;s representational capacity, and even less for the larger models of the future.</p><p> More importantly, though: no one thinks that a schemer won&#39;t have to represent concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on at all. As <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">Hubinger et al (2023)</a> note, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process, etc (and especially: models that are &quot;playing a training game&quot; in which such concepts play a central role). So really, the relevant question is: what are the <em>extra</em> complexity costs of representing a goal like &quot;get reward-on-the-episode&quot; or &quot;follow instructions in an HHH way&quot; (relative to the simplest possible schemer-like goal), <em>once you&#39;ve already paid the costs of having a concept of those goal targets</em> .</p><p> I&#39;m not sure exactly how to think about this, but it seems very plausible to me that the costs here are extremely small. In particular: it seems like SGD may be able to significantly repurpose the parameters used to represent the concept in the world model in causing that concept to guide the model&#39;s behavior in a goal-like manner. Thus, as an analogy, perhaps the concept of &quot;pleasure&quot; is in some sense &quot;simpler&quot; than the concept of &quot; <a href="https://en.wikipedia.org/wiki/Wabi-sabi">wabi-sabi</a> &quot; in Japanese aesthetics (ie, &quot;appreciating beauty that is &#39;imperfect, impermanent, and incomplete&#39; &quot;) 。 Once you&#39;ve <em>learned</em> both, though, does pursuing the former require meaningfully more parameters than pursuing the latter? <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-16" id="fnref-DNdPEGz4DYMju8reA-16">[16]</a></sup></p><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger (2022)</a> discussion of issues like this sometimes appeals to the notion of a &quot;pointer&quot; to some part of the world model. As I understand it, the idea here is that if you&#39;ve already got a concept of something like &quot;pleasure&quot;/&quot;wabi-sabi&quot;/&quot;reward&quot; in your world model, you can cause a model to pursue that thing by giving it a goal slot that says something like &quot;go for <em>that</em> &quot; or &quot; <em>that</em> is good,&quot; where &quot;that&quot; points to the thing in question (this is in contrast with having to represent the relevant concept <em>again,</em> fully and redundantly, in the goal slot itself). But insofar as we use a toy model like this (I doubt we should lean on it), why think that it&#39;s significantly more complex to <em>point</em> at a more complex concept than at a simpler one? Eg, even granted that &quot;wabi-sabi&quot; takes more parameters than &quot;pleasure&quot; to represent in the world model, why think that encoding the <em>pointer</em> to &quot;pleasure&quot; (eg, &quot;go for <em>that&quot;</em> ) takes more parameters than encoding the <em>pointer</em> to &quot;wabi-sabi&quot; (eg, again, &quot;go for <em>that</em> &quot;)?</p><p> One option, here, is to say that the complexity of the concept and the complexity of the pointer are correlated. For example, you might imagine that the model has some kind of &quot;internal database&quot; of concepts, which stores concepts in a manner such that concepts that take fewer parameters to store take fewer parameters to &quot;look up&quot; as well. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-17" id="fnref-DNdPEGz4DYMju8reA-17">[17]</a></sup> On this picture, &quot;pleasure&quot; might end up stored as the 15th concept in the database <em>because</em> it takes eg 23 parameters to represent, whereas &quot;wabi-sabi&quot; might end up stored as the 125355th concept because it takes 10,000 parameters to代表。 And then the &quot;pointer&quot; to pleasure can say &quot;go for the thing stored at location 15,&quot; whereas the &quot;pointer&quot; to &quot;wabi-sabi&quot; has to say &quot;go for the thing stored at location 125355,&quot; which takes a few more bits to specify. But even at an abstract-toy-illustrative-example level, this sort of story requires leaning on a specific model of how the model&#39;s pointer and concept-storage processes work – and it still needs to explain <em>why</em> simplicity-to-represent and simplicity-to-point-at are correlated in the relevant sense.</p><p> Alternatively, though, we can abandon any interest in the complexity of storing a concept in the world model, and focus directly on the complexity of pointing to it. Still, insofar as there are meaningful <em>differences</em> between the complexity of pointing at one concept vs. another, we may be able to re-run the argument that schemers offer simplicity advantages. In particular: selecting a schemer allows SGD to have its pick from whatever schemer-motivating goals are simplest to <em>point at in the world model</em> ; whereas the other model classes plausibly impose more substantive constraints. Ie, if the specified goal ends up stored at location 12634 in the model&#39;s metaphorical database, and if &quot;reward-on-the-episode&quot; is at location 35364, then if there are any schemer-like goals at eg locations 1-100, it&#39;ll be simpler to point at one of <em>those</em> instead – and thus, to create a schemer rather than a training-saint or a reward-on-the-episode seeker.</p><p> To the extent we focus on the final properties of the different model classes, I think this is probably the best way to run a simplicity-focused argument for scheming – especially if we don&#39;t get too hung up on the toy ontology of &quot;pointers&quot; (and still less, &quot;databases&quot;) in particular. That is, roughly: even granted that all of the goals on the table here (eg the specified goal, reward-on-the-episode, etc) will need to be represented by the model somewhere, it may be that, for whatever reason, some of these concepts take fewer parameters to <em>repurpose for goal-pursuit</em> than others. And insofar as scheming makes more potential concepts available for such repurposing, it offers a simplicity advantage in that sense.</p><p> Quantitatively, though, how big is this advantage? It&#39;s now especially hard to say. In particular, once we&#39;ve shifted our focus from &quot;simplicity-to-represent&quot; to &quot;simplicity-to-repurpose-for-goal-pursuit,&quot; we can&#39;t even look at our intuitive sense of the complexity of concepts like &quot;reward,&quot; &quot;honesty,&quot; and &quot;next token prediction error,&quot; because we&#39;re no longer talking about the complexity of the concepts per se. Rather, we&#39;re speculating about the complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory.</p><p> Still, to the extent we can estimate the size of these differences, it seems plausible to me that they are very small indeed. One intuition pump for me here runs as follows. Suppose that the model has 2^50 concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-18" id="fnref-DNdPEGz4DYMju8reA-18">[18]</a></sup> The average number of bits required to code for each of 2^50 concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max-reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I won&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so).</p><h2> Does this sort of simplicity-focused argument make plausible predictions about the sort of goals schemers would end up with?</h2><p> One other consideration that seems worth tracking, in the context of simplicity arguments for scheming, is the predictions they are making about the sort of goals a schemer will end up with. In particular, if you think (1) that SGD selects very hard for simpler goals, (2) that this sort of selection favors schemer-like goals because they can be simpler, and (3) that our predictions about what SGD selects can ignore the &quot;path&quot; it takes to create the model in question, then at least naively, it seems like you should expect SGD to select a schemer with an extremely simple long-term goal (perhaps: the simplest possible long-term goal), <em>regardless of whether that goal had any relation to what was salient or important during training</em> . Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-19" id="fnref-DNdPEGz4DYMju8reA-19">[19]</a></sup> these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-20" id="fnref-DNdPEGz4DYMju8reA-20">[20]</a></sup></p><p> Personally, I feel skeptical of predictions like this (though this skepticism may be partly rooted in skepticism about ignoring the path SGD takes through model space more generally). And common stories about schemers tend to focus on proxy goals with a closer connection to the training process overall (eg, a model trained to on gold-coin-getting ends up valuing eg &quot;get gold stuff over all time&quot; or &quot;follow my curiosity over all time,&quot; and not &quot;maximize hydrogen over all time&quot;).</p><p> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue, perhaps they will either be more important (and thus simpler?) to represent in the world model, or simpler (for some reason) for the model to repurpose-for-goal-pursuit once represented. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-21" id="fnref-DNdPEGz4DYMju8reA-21">[21]</a></sup> But if we grant some story in this vein, we should also be tracking its relevance to the simplicity of pursuing <em>non-schemer goals</em> as well. In particular: to the extent we&#39;re positing that salience/relevance during training correlates with simplicity in the relevant sense, this is points in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. (Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall.)</p><p> And note, too, that to the extent SGD selects very hard for simpler goals (for example, in the context of a form of &quot;low path dependence&quot; that leads to strong convergence on a single optimal sort of model), this seems somewhat at odds with strong forms of the goal-guarding hypothesis, on which training-gaming causes your goals to &quot;crystallize.&quot; For example, if a would-be-schemer starts out with a not-optimally-simple goal that still motivates long-term power-seeking, then if it knows that in fact, SGD will continue to grind down its goal into something simpler even after it starts training-gaming, then it may not have an incentive to start training-gaming in the first place – and its goals won&#39;t survive the process regardless. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-22" id="fnref-DNdPEGz4DYMju8reA-22">[22]</a></sup></p><h2> Overall assessment of simplicity arguments</h2><p> Overall, I do think that other things equal, schemers can have probably simpler goals than these other model classes. However, I think the relevant simplicity differences may be quite small, especially once we condition on the model having a good world model more generally (and moreso, if we posit that goals targets salient/relevant-during-training get extra simplicity points). And I&#39;m suspicious of some of the theoretical baggage it can feel like certain kinds of simplicity arguments wheel in (for example, baggage related to the notion of simplicity at stake, whether SGD selects for it, how to think about simplicity in the context of repurposing-for-goal-pursuit as opposed to merely representing, and so on). </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-DNdPEGz4DYMju8reA-1" class="footnote-item"><p> See eg <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_low_path_dependence_world">Hubinger (2022)</a> . <a href="#fnref-DNdPEGz4DYMju8reA-1" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-2" class="footnote-item"><p> See also <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive">this (now anonymous) discussion</a> for another example of this usage of &quot;simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-2" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-3" class="footnote-item"><p> Here, my sense is that the assumption is generally that X can be described at a level of computational abstraction such that the &quot;re-writing&quot; at stake doesn&#39;t merely reproduce the network itself. Eg, the network is understood as implementing some more abstract function. I think it&#39;s an interesting question how well simplicity arguments would survive relaxing this sort of assumption. <a href="#fnref-DNdPEGz4DYMju8reA-3" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-4" class="footnote-item"><p> Another issue is that <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">Kolmogorov complexity is uncomputable</a> . I&#39;m told you can approximate it, but I&#39;m not sure how this gets around the issue that for a given program where you&#39;re not able to tell whether or not it halts, that program might be the shortest program outputting the relevant细绳。 <a href="#fnref-DNdPEGz4DYMju8reA-4" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-5" class="footnote-item"><p> See <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#iv-can-you-ignore-being-only-finitely-wrong">Carlsmith (2021)</a> , sections III and IV, for more on this. <a href="#fnref-DNdPEGz4DYMju8reA-5" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-6" class="footnote-item"><p> Hubinger sometimes appears to be appealing to this notion as well – or at least, not drawing clear distinctions between &quot;re-writing simplicity&quot; and &quot;parameter simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-6" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-7" class="footnote-item"><p> &quot;Trivial simplicity&quot; is also closely related to what we might call &quot;selection simplicity.&quot; Here, again, one assumes some space/distribution over possible things (eg, goals), and then talks about the &quot;simplicity&quot; of some portion of that space in terms of how much &quot;work&quot; one needs to do (perhaps: on average) in order to narrow down from the whole space to that portion of the space (see also <a href="https://colah.github.io/posts/2015-09-Visual-Information/">variable-length codes</a> ). Thus, for a box of gas, &quot;the molecules are roughly evenly spread out&quot; might be a &quot;simpler&quot; arrangement than &quot;the molecules are all in a particular corner,&quot; because it typically takes more &quot;work&quot; (in this example: thermodynamic work) to cause the former than the latter (this is closely related to the fact that the former is initially more likely than the latter). My sense is that when some people say that &quot;schemer-like goals are simple,&quot; they mean something more like: the <em>set</em> of schemer-like goals typically takes less &quot;work,&quot; on SGD&#39;s part, to land within than the <em>set</em> of non-schemer-like goals (and not necessarily: that any <em>particular</em> schemer-like goals is simpler than some <em>particular</em> non-schemer-like goal). To the extent that the set of schemer-like goals are supposed to have this property because they are more &quot;common,&quot; and hence &quot;nearer&quot; to SDG&#39;s starting point, this way of talking about the simplicity benefits of scheming amounts to a restatement of something like the counting argument and/or the &quot;nearest max-reward goal argument&quot; – except, with more of a propensity, in my view, to confuse the simplicity of <em>set</em> of schemer-like goals with the simplicity of a <em>given</em> schemer-like目标。 <a href="#fnref-DNdPEGz4DYMju8reA-7" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-8" class="footnote-item"><p> Where, importantly, multiple different settings of parameters can implement the same function. <a href="#fnref-DNdPEGz4DYMju8reA-8" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-9" class="footnote-item"><p> My understanding is that the Levin bound says something like: for a given distribution over parameters, the probability <em>p(f)</em> of randomly sampling a set of parameters that implements a function <em>f</em> is bounded by <em>2^{-K(f) + O(1)}</em> , where <em>K</em> is the <em>k</em> -complexity of the function <em>f</em> , and <em>O(1)</em> is some constant independent of the function itself (though: dependent on the parameter space). That is, the prior on some function decreases exponentially as the function&#39;s complexity increases.</p><p> I haven&#39;t investigated this result, but one summary I saw ( <a href="https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of?commentId=kLz9mgNv8xFNrAPt2">here</a> ) made it seem fairly vacuous. In particular, the idea in that summary was that larger volumes of parameter space will have simpler encodings, because you can encode them by first specifying distribution over parameters, and then using a <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman code</a> to talk about how to find them given that distribution. But this makes the result seem pretty trivial: it&#39;s not that there is some antecedent notion of simplicity, which we then discover to be higher-probability according to the initialization distribution. Rather, to be higher probability according to the initialization distribution just <em>is</em> to be simpler, because equipped with the initialization distribution, it&#39;s easier to encode the higher probability parts of it. Or put another way: it seems like this result applies to any distribution over parameters. So it doesn&#39;t seem like we learn much about any particular distribution from it.</p><p> (To me it feels like there are analogies here to the way in which &quot;shorter programs get more probability,&quot; in the context of algorithmic &quot;simplicity priors&quot; that focus on metrics like K-complexity, actually applies necessarily to <em>any</em> distribution over a countably-infinite set of programs – see discussion <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">here</a> . You might&#39;ve thought it was an interesting and substantive constraint, but actually it turns out to be more vacuous.)</p><p> That said, the empirical results I mention above focus on more practical, real-world measures of simplicity, like <a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv_complexity">LZ complexity</a> , and apparently they find that, indeed, simpler functions get higher prior probability (see eg <a href="https://arxiv.org/pdf/1805.08522.pdf">this experiment</a> , which uses a fully connected neural net to model possible functions from many binary inputs to a single binary input). This seems to me more substantive and interesting. And <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> claims that Levin&#39;s result is non-trivial, though I don&#39;t yet understand how. <a href="#fnref-DNdPEGz4DYMju8reA-9" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-10" class="footnote-item"><p> Thus, for example, you might think that insofar a randomly initialized model is more likely to end up &quot;closer&quot; to a schemer, such that SGD needs to do &quot;less work&quot; in order to select a schemer rather than some other model, this favors schemers (thanks to Paul Christiano for discussion). But this sort of argument rests on putting a higher prior probability on schemers, which, in my book, isn&#39;t a (non-trivial) simplicity argument per se. <a href="#fnref-DNdPEGz4DYMju8reA-10" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-11" class="footnote-item"><p> There are also more speculative and theoretical arguments for a connection between simplicity and schemers, on which one argues that if you do an unbounded search over all possible programs to find the shortest one that gives a given output, without regard to other factors like how long they have to run, then you&#39;ll select for a schemer (for example, via a route like: simulating an extremely simple physics that eventually gives rise to agents that understand the situation and want to break out of the simulation, and give the relevant output as part of a plan to do so). My understanding is that people (eg <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive#Priors_on_Learned_Optimizers">here</a> ) sometimes take the discourse about the &quot; <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">malignity of the Solomonoff prior</a> &quot; as relevant here (though at a glance, it seems to me like there are important differences – for example, in the type of causality at stake, and in the question of whether the relevant schemer might be simulating <em>you</em> ). Regardless, I&#39;m skeptical that these unbounded theoretical arguments should be getting much if any weight, and I won&#39;t treat them here. <a href="#fnref-DNdPEGz4DYMju8reA-11" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-12" class="footnote-item"><p> What&#39;s more, note that, to the extent we imagine SGD biasing towards simplicity <em>because</em> real world patterns tend to be simple (eg, Occam&#39;s razor is indeed a good prior, and SGD works well in part because it reflects this prior), the explanation for this bias doesn&#39;t apply as readily to a model&#39;s <em>goals</em> . That is (modulo various forms of moral realism), there are no &quot;true goals,&quot; modeling of which might benefit from a simplicity prior. Rather, on this story, SGD would need to be acting more like a human moral anti-realist who prefers a simpler morality other-things-equal, despite not believing that there is any objective fact of the matter, because, in contexts where there <em>is</em> a fact of the matter, simpler theories tend to be more likely. <a href="#fnref-DNdPEGz4DYMju8reA-12" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-13" class="footnote-item"><p> Hubinger uses this approach. My understanding is that he&#39;s imagining SGD selecting a model with probability proportionate to its simplicity, such that eg focusing on the simplest possible model is one way of approximating the overall probability in a model class, and focusing on the <em>number</em> of models in the class is其他。 However, I won&#39;t take for granted the assumption that SGD selects a model with probability proportionate to its simplicity. <a href="#fnref-DNdPEGz4DYMju8reA-13" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-14" class="footnote-item"><p> See eg Hubinger et al (2023) <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">here</a> . <a href="#fnref-DNdPEGz4DYMju8reA-14" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-15" class="footnote-item"><p> I first heard this sort of point from Paul Christiano. <a href="#fnref-DNdPEGz4DYMju8reA-15" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-16" class="footnote-item"><p> Here I don&#39;t mean: does it take more parameters to <em>successfully</em> promote pleasure vs. successfully promoting wabi-sabi. I just mean: does it take more parameters to <em>aim</em> optimization at the one vs. the other. <a href="#fnref-DNdPEGz4DYMju8reA-16" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-17" class="footnote-item"><p> Thanks to Daniel Kokotajlo for suggesting an image like this. <a href="#fnref-DNdPEGz4DYMju8reA-17" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-18" class="footnote-item"><p> The precise number of concepts here doesn&#39;t matter much. <a href="#fnref-DNdPEGz4DYMju8reA-18" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-19" class="footnote-item"><p> I&#39;m not saying it is, even for a physics-based world model, but I wanted an easy illustration of the point. Feel free to substitute your best-guess simplest-possible-goal here. <a href="#fnref-DNdPEGz4DYMju8reA-19" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-20" class="footnote-item"><p> Notably, this sort of prediction seems like an especially poor fit for an analogy between humans and evolution, since human goals seem to have a very intelligible relation to reproductive fitness. But evolution is plausibly quite &quot;path-dependent&quot; anyway. <a href="#fnref-DNdPEGz4DYMju8reA-20" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-21" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-DNdPEGz4DYMju8reA-21" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-22" class="footnote-item"><p> Hubinger, in discussion, suggests that the model&#39;s reasoning would proceed in terms of logical rather than physical causality. He writes: &quot;The reasoning here is: I should be the sort of model that would play the training game, since there&#39;s some (logical) chance that I&#39;ll be the model with the best inductive biases, so I should make sure that I also have good loss.&quot; But if a model can <em>tell</em> that its goal isn&#39;t yet optimally simple (and so will be ground down by SGD), then I&#39;m not sure why it would think there is a &quot;logical chance&quot; that it&#39;s favored by the inductive biases在这个意义上。 <a href="#fnref-DNdPEGz4DYMju8reA-22" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming<guid ispermalink="false"> uWdAKyHZMfoxDHcCC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:05:54 GMT</pubDate></item></channel></rss>