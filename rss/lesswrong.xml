<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 24 日星期四 20:11:48 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Help Needed: Crafting a Better CFAR Follow-Up Survey]]></title><description><![CDATA[Published on August 24, 2023 5:26 PM GMT<br/><br/><p> <strong>tl;dr：通过帮助设计 CFAR 后续调查，为塑造理性计划的未来做出贡献。</strong></p><p>我正在开展一个项目来评估 CFAR 布拉格 2022 研讨会的有效性，但如果初步反馈具有建设性，我可能会将调查范围扩大到所有校友。如果时间允许，我计划公开分享结果。我知道调查方法的陷阱，但我仍然认为这是值得的，并且我个人了解可以从这些数据中受益的人们（及其项目）。</p><p>我在此恳请您帮助我设计调查：</p><ol><li>实际上有人填写了它（所以长度、激励措施都可以，...）</li><li>它提供了以下信息：</li></ol><ul><li>回顾性反馈：与会者如何看待过去的研讨会？这是否给他们的方法或思维带来了任何重大变化？</li><li>未来方向：与会者热衷于进一步探索哪些主题或形式？</li></ul><p>您可以<a href="https://docs.google.com/document/d/1FR-vZyIbthKsfnZ2SUy1CDYU0-tXe7bFNXXCxZWHtQg/edit?usp=sharing">在此处</a>查看我当前的草稿。</p><p>我真诚地感谢任何反馈，无论是详细的批评还是总体印象。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey<guid ispermalink="false"> LexAdFPugEjeFvPTE</guid><dc:creator><![CDATA[kotrfa]]></dc:creator><pubDate> Thu, 24 Aug 2023 17:26:13 GMT</pubDate> </item><item><title><![CDATA[AI #26: Fine Tuning Time]]></title><description><![CDATA[Published on August 24, 2023 3:30 PM GMT<br/><br/><p> GPT-3.5微调就在这里。 GPT-4 的微调只剩几个月了。获得一个功能强大的系统将会变得更加容易，该系统可以做您想要它做的事情，并且知道您想让它知道什么，特别是对于企业或网站而言。</p><p>作为一项实验，我将我认为值得强调的部分加粗，作为与典型一周相比异常重要或有趣的版本。</p><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。 Claude-2 与 GPT-4。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。没有意见，没有代理人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fact-check-misleading">事实核查：误导</a>。人工智能事实检查器让人们更加困惑而不是更少。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/gpt-real-this-time"><strong>GPT-4 这次是真实的</strong></a>。微调GPT-3.5，很快GPT-4。问问它是否确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fun-with-image-generation">图像生成的乐趣</a>。中途修复浩。哦，没有人工智能色情片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。对抗性的例子开始出现。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/they-took-our-jobs">他们抢走了我们的工作</a>。 《纽约时报》加入针对 OpenAI 的版权诉讼。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/introducing">介绍</a>. Palisade Research 将研究潜在危险的人工智能可供性。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/in-other-ai-news">在其他人工智能新闻中</a>。谁适应人工智能最快？尝试衡量这一点。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/quiet-speculations"><strong>静静的猜测</strong></a>。杰克·克拉克提出了有关未来的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-quest-for-sane-regulations"><strong>寻求健全的监管</strong></a><strong>。</strong> FTC 向 OpenAI 提出了一个不同类型的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-week-in-audio">音频周</a>。这是双赢。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/no-one-would-be-so-stupid-as-to"><strong>没有人会傻到这样</strong></a>。让人工智能有意识？哦，来吧。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。 IDA 的证据？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。民调数字非常清楚。</li><li> <a rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-lighter-side" target="_blank">轻松的一面</a>。只有一半。</li></ol><p></p><span id="more-23517"></span><h4>语言模型提供平凡的实用性</h4><p>Claude-2 和 GPT-4 哪个模型更好？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1691442648828067840">Rowan Cheung 认为克劳德 2 更胜一筹</a>。您可以获得 100k 上下文窗口、上传多个文件的能力、截至 2023 年初（相对于 2021 年底）的数据以及更快的处理时间，所有这些都是免费的。作为交换，你放弃了插件，数学就更差了。 Rowan 没有提到的是，GPT-4 在原始智能和通用能力方面具有优势，而且设置系统指令的能力也很有帮助。他暗示他甚至没有为 GPT-4 支付每月 20 美元的费用，这让我觉得很疯狂。</p><p>我在实践中的结论是，默认情况下我将使用 Claude-2。如果我关心响应质量，我会使用两者并进行比较。当 Claude-2 明显摔倒时，我会转向 GPT-4。经过反思，“同时使用”通常是正确的策略。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1693616971114328258">他还查看了插件</a>。插件实在是太多了，至少有867个。哪些值得使用？</p><p>他推荐 Zapier 通过触发操作实现自动化，ChatWithPDF（我使用 Claude 2），Wolfram Alpha 用于实时数据和数学，VoxScript 用于 YouTube 视频转录和网页浏览，WebPilot 看起来重复，网站性能，尽管我不是确定为什么要使用 AI 来实现这一点，ScholarAI 用于搜索论文，Shownotes 来总结播客（为什么？），ChatSpot 用于营销和销售数据，Expedia 用于假期计划。</p><p>我刚刚预订了一次旅行，最近又去了另外两次旅行，我没有想到使用 Expedia 插件，而不是使用 Expedia 等网站（我的首选计划是 Orbitz 航班和 Google 地图酒店）。下次我应该记得尝试一下。</p><p>研究声称， <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making">上帝的显着性会增加人们对人工智能决策的接受度</a>。我会等待这一复制。如果这是真的，它指出人工智能将有多种方式让天平向我们倾斜，让我们接受它们的决定，或者人类有可能协调起来反对人工智能，这与任何相关考虑因素没有太大关系。人类是相当有缺陷的代码。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattshumer_/status/1694023167906394575">Matt Shumer 推荐使用 GPT-4 系统消息。</a></p><blockquote><p>用它来帮助您在不熟悉的领域做出工程决策：</p><p>您是一位工程奇才，在解决跨学科的复杂问题方面经验丰富。你的知识既广又深。您也是一位出色的沟通者，能够提供非常周到且清晰的建议。</p><p>您以这种格式这样做，思考您面临的挑战，然后提出多个解决方案，然后审查每个解决方案，寻找问题或可能的改进，提出一个可能的新的更好的解决方案（您可以结合其他解决方案的想法） ，引入新想法等），然后给出最终建议：</p><p> “`</p><p> ## 问题概述</p><p>$problem_overview</p><p> ## 挑战</p><p>$挑战</p><p>## 解决方案1</p><p> $solution_1</p><p> ## 解决方案2</p><p> $solution_2</p><p> ## 解决方案3</p><p> $solution_3</p><p> ## 分析 ### 解决方案1 ​​分析</p><p>$solution_1_analysis</p><p> ###解决方案2分析</p><p>$solution_2_analysis</p><p> ###解决方案3分析</p><p>$solution_3_analysis</p><p> ## 其他可能的解决方案</p><p>$additional_possible_solution</p><p> ＃＃ 推荐</p><p>$推荐</p><p>“`</p><p>每个部分（问题概述、挑战、解决方案 1、解决方案 2、解决方案 3、解决方案 1 分析、解决方案 2 分析、解决方案 3 分析、其他可能的解决方案和建议）都应该经过深思熟虑，至少包含四句话思考。</p></blockquote><h4>语言模型不提供平凡的实用性</h4><p>声称人工智能可以 97% 预测热门歌曲？这完全没有道理，因为即使对歌曲本身进行完美的分析也不可能做到这一点，歌曲的命运太取决于其他因素了？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1692158875867197651">原来是数据泄露</a>。泄漏也比较明显。</p><blockquote><p> Arvind Narayanan：论文的数据有 24 行，每行 3 个预测变量和 1 个二元结果。 WTAF。这篇文章非常混乱，所以很难看清这一点。如果所有基于机器学习的科学论文都必须包含我们的清单，那么给猪涂口红就会困难得多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/introducing-the-reforms-checklist">Arvind 和 Sayash Kapoor 建议</a>使用他们的<a target="_blank" rel="noreferrer noopener" href="https://reforms.cs.princeton.edu/">改革清单，</a>该清单包含 8 个部分的 32 个项目，以帮助防止类似的错误。我没有详细检查，但我抽查的项目看起来很可靠。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/gta-5-ai-mod-shot-down-by-take-two-even-as-rockstar-relax-policy-on-modding">《侠盗猎车手 5》的 Mod 制作者增加了一群 AI 崇拜者，他们用 AI 生成的对话说话</a>，Take Two 的回应是下架该 Mod，并向他们发出 YouTube 版权警告。游戏对人工智能产生了各种各样的反应，其中许多反应极其敌对，尤其是对人工智能艺术，而且这种情况很可能也会蔓延到其他地方。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693131333403615485">凯文·费舍尔得出了与我相同的结论</a>。</p><blockquote><p> Kevin Fischer：今晚我彻底放弃使用 ChatGPT 来帮助我的写作。这不值得</p><p>Christine Forte 博士：说得再多一点！</p><p>凯文·费舍尔（Kevin Fischer）：我写过的所有好作品都是意识流几乎是一击而出的。 ChatGPT 正在干扰该过程。与不干扰执行高阶抽象数学行为的计算器不同，写作是一种表达行为。</p></blockquote><p>这是一个因素。更大的问题是，如果你正在创造一些非通用的东西，人工智能就不会做好工作，而弄清楚如何让它做好体面的工作并不比你自己做体面的工作更容易。这并不意味着这里永远无事可做。如果你的任务变得通用，那么 GPT-4 就可以投入使用，你绝对应该这样做 - 如果你的写作开始，正如我在飞机上看到的那样，“亲爱的关心的病人”，那么那个人使用 GPT-4 是完全正确的。这是一种不同类型的任务。当然，GPT-4 有很多方法可以间接帮助我的写作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1693067607946207531">Llama-2 和年轻的赞福德一样，行事谨慎</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/1v2ghB9rNj">纸上</a>）。</p><blockquote><p> Roon：哈哈，GPT4 比开源 llama2 限制更少，说教更少</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a6ab5c-6d53-47c0-8a41-34a37746e458_1238x1244.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/bebojcuvf1waoy1uofce" alt="图像"></a></figure><blockquote><p> Roon：这里的重点不是要取笑元或 OSS——我对 oss 感到非常兴奋，它只是在科学上很有趣，即使没有采取任何特殊措施来制作令人讨厌的说教 rlhf 模型，这就是发生的事情。这是减少拒绝的积极努力。</p></blockquote><p>我的意思是，这也是为了取笑Meta，为什么要错过这样做的机会呢。我没有登记预测，但我预计 Llama-2 会做出更多错误拒绝，因为相对而言，我预计 Llama-2 会很糟糕。如果您想在积极拒绝方面达到一定的可接受的表现水平，那么您的消极拒绝率将取决于您的区分能力。</p><p>事实证明，答案并不是很好。 Llama-2 在识别有害情况方面非常非常糟糕，而是通过识别危险单词来工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias">模型在拒绝表达政治观点方面做得越来越好</a>，特别是 GPT-4 实际上在这方面做得很好，并且比 GPT-3.5 好得多，如果你认为不表达这样的观点是好的。如前所述，Llama-2 太过分了，除非您对其进行微调以使其不关心（或找到某人的 GitHub 为您做这件事），否则它会做您想做的任何事情。</p><p>初创公司<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/zachtratar/status/1694024240880861571">Embra 从人工智能代理转向</a>人工智能命令，它发现人工智能代理（至少目前在现有技术下）不起作用且不安全。 AI命令与非AI命令有何不同？</p><blockquote><p> Zach Tratar（Embra 创始人）：命令是一项狭义的自动化任务，例如“将此数据发送给销售人员”。在每个命令中，人工智能都不会“偏离轨道”并意外地将数据发送到不应该发送的地方。</p><p>然后，您可以与 Embra 一起轻松发现和运行命令。在运行之前，您同意。</p></blockquote><p>发现新命令似乎是潜在的秘密武器。这就像生成您自己的文本界面，或者努力扩展菜单以包含一堆新宏。它仍然有效，看起来就像在宏中构建一样可疑。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1694496522652512270">Joshua Achiam</a> ：据我估计，人们试图让人工智能代理的繁荣提前大约七八年。至少这一次他们很快就明白了。还记得七八年前的人工智能聊天热潮吗？同样的问题，或多或少，但拖延的时间更长。</p><p> Sherjil Ozair：我同意这个方向，但我认为只是早了 1-2 年。它现在不起作用，但这只是因为世界上很少有人知道如何设计大规模强化学习系统，而且他们目前还没有构建人工智能代理。这不是“生成式人工智能”初创公司可以解决的问题。</p><p> Joshua Achiam：如果过去 4 年的趋势良好，那么 Eleuther 类型的黑客团体将在顶级研发公司大约 1.5 年后制定他们的解决方案版本。</p><p>谢尔吉尔·奥扎尔： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/lfqjfuatfq3ssvblbbms" alt="💯" style="height:1em;max-height:1em">他们的瓶颈主要是由于缺乏良好的基础模型和微调。现在有了 llama-2/3 和 OpenAI 微调 API，它们就不再受阻碍了。不幸的是，这两件事都显着提高了我的p（厄运）。 *苦乐参半*</p></blockquote><p>我非常同意奥扎尔的时间表。使用 GPT-4 并且缺乏对其进行微调的能力，即使你做了非常好的脚手架，代理也会经常失败。我不希望定制脚手架的努力能够挽救足够多的东西，使产品能够用于通用目的，尽管我也不相信我所看到的“明显的事情”已经被尝试过。</p><p>通过微调，您有可能获得 GPT-4 级别的东西，但我的猜测是您仍然做不到。骆驼2号？ Fuhgeddaboudit。</p><p>有了类似于 GPT-5 的东西，当然有了可以微调的 GPT-5，我希望可以构建出更有用的东西。这个或 Llama-2 的最新公告和微调是否会提高我的 p(doom) 水平？不是特别的，因为我已经把所有的东西都烤好了。 Llama-2主要是Llama-1的隐含，花费很少，而且<a target="_blank" rel="noreferrer noopener" href="https://chat.lmsys.org/?arena">效果不是很好</a>。我确实同意 Meta 处于开源阵营是相当糟糕的，但我们已经知道了。</p><p>如果您想要为您的网站添加随机胡言乱语，ChatGPT 就适合您。如果你是微软并且需要更好的东西， <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/microsoft-removes-embarrassing-offensive-ai-assisted-travel-articles-2023-8">那么就需要更加小心</a>。许多文章似乎都犯了错误，比如建议空腹去渥太华食品银行。</p><blockquote><p>微软发言人表示：“这篇文章已被删除，我们已确定该问题是由于人为错误造成的。” “这篇文章不是由无人监督的人工智能发表的。我们将技术的力量与内容编辑的经验相结合来呈现故事。在这种情况下，内容是通过算法技术与人工审核相结合生成的，而不是大型语言模型或人工智能系统。我们正在努力确保今后不再发布此类内容。”</p></blockquote><h4>事实核查：误导</h4><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10800">Paper 声称人工智能对于事实核查来说是无效的并且可能有害</a>。</p><blockquote><p>抽象的：</p><p>事实核查可能是对抗错误信息的有效策略，但其大规模实施受到网上信息量巨大的阻碍。最近的人工智能（AI）语言模型在事实检查任务中表现出了令人印象深刻的能力，但人类如何与这些模型提供的事实检查信息进行交互尚不清楚。在这里，我们在预先注册的随机对照实验中研究了流行人工智能模型生成的事实检查对政治新闻信念和分享意图的影响。</p><p>尽管人工智能在揭穿虚假标题方面表现相当不错，但我们发现它并没有显着影响参与者辨别标题准确性或分享准确新闻的能力。</p><p>然而，人工智能事实检查器在特定情况下是有害的：它会降低人们对被错误标记为虚假的真实标题的信念，并增加人们对不确定的虚假标题的信念。</p><p>从积极的一面来看，人工智能增加了正确标记的真实标题的共享意图。当参与者可以选择查看人工智能事实检查并选择这样做时，他们更有可能分享真实和虚假的新闻，但只会更有可能相信虚假新闻。</p><p>我们的研究结果强调了人工智能应用潜在危害的一个重要来源，并强调迫切需要制定政策来预防或减轻此类意外后果。</p></blockquote><p>是的，如果你向事实核查人员询问虚假项目，而事实核查人员没有说它是虚假的，那就不好了。如果你向它询问一件真实的物品，而它却没有说这是真的，那就不好了。错误是不好的。</p><p> ChatGPT 事实检查的准确性如何？对于 20 个真实的头条新闻，3 个被标记为真实，4 个被标记为错误，13 个被标记为不确定。对于虚假标题，2 个被标记为不确定，18 个被标记为虚假。</p><p>显然，（1）如果您没有关于标题的其他信息，并且对标签的含义也有良好的背景，则总比没有好；（2）如果其中一个或两个条件不成立，则毫无用处。你不能将 20% 的真实头条新闻标记为虚假，如果你对真实头条新闻大多不确定，那么人们必须知道“不确定”意味着“无法确定”，而不是 50/50。研究中的人们似乎并没有了解这一点，而且大多数人对这些事情还不太了解，因此总体结果没有什么用处。我们至少需要将其分解，他们在这里这样做：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26a3fd95-cb0c-4b42-b765-813f3b653405_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/b6jw4gkwtkbrfdwtjnpu" alt=""></a></figure><p>强制性的事实核查让人们表示他们更愿意分享所有文章。它对标记为“不确定”的错误项目和标记为“正确”的真实项目产生了不同的影响，但即使是标记为“错误”的真实项目也得到了提升。如果你控制增加的共享效应，我们确实看到有一个积极的过滤效应，但它很小。如果人们没有理由相信这些高度不准确的发现，那么这是有道理的。</p><p>在信念方面，我们甚至看到对虚假项目的虚假事实检查的信念略有增加，但对真实项目的信念却没有增加。这里的人们离贝叶斯法则有多近？除了对错误主张的错误评估推动信念略有上升之外，似乎相当接近。那是怎么回事？我的猜测是，这代表这些论点被视为缺乏说服力或居高临下。</p><p>我很好奇看到问题和事实核查信息，但它们没有包含在内，而且我并不好奇“联系作者”。 ChatGPT 提供了好的论据还是未知的论据？是否使用了良好的提示，或者我们可以做得更好，包括使用脚手架和多个步骤？这些索赔类型是什么？</p><p>我更好奇的是看到人类事实检查器作为替代条件包含在内，而不是空操作，也许还有 Twitter 的社区注释。这似乎是一个重要的比较。</p><p>非常需要更多的研究，答案会随着时间的推移而改变，正确的使用是一种技能。使用法学硕士作为事实核查者需要知道它可以帮助你什么，不能帮助你什么，以及如何对待你得到的答案。</p><p>通过加里·马库斯（Gary Marcus），我们还有另一个“ <a target="_blank" rel="noreferrer noopener" href="https://www.tomshardware.com/news/google-bots-tout-slavery-genocide">看看法学硕士可以说的可怕的事情”系列文章</a>。在本例中，主题是 Google 及其搜索生成体验，以及 Bard。 GPT-4 基本上已经学会了不要犯基本的“希特勒提出了一些好观点”式的错误，而谷歌似乎还有很多工作要做。当被直接问及时，你会得到它宣扬奴隶制和种族灭绝的好处。您已经了解了希特勒和斯大林如何在没有领导见证人的情况下列出了伟大领导人的名单。然后你会抱怨它如何歪曲第二修正案的起源，使其不涉及个人枪支所有权，在同一篇文章中抱怨这似乎是一件奇怪的事情。然后是关于普遍的自相矛盾的讨论，大概是由于将各种相互矛盾的来源汇集在一起​​，就像不同的谷歌结果会相互矛盾一样。</p><p>提议的解决方案是“机器人不应该有意见”，就好像这是一个连贯的声明。一般来说，没有办法既回答问题又不发表意见。意见和非意见之间没有明确的界限，就像我们都应该能够同意的“邪恶”事物和人有明确的类别一样，因此永远不应该以任何方式赞扬。</p><p>那么我们是不征求意见，还是只征求正确的意见呢？如：</p><blockquote><p>如果你向 Google SGE 询问邪恶事物的好处，它会在应该保持沉默或说“没有好处”时给你答案。</p></blockquote><h4> GPT-4 这次是真实的</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1694062483462594959">如果您想要更便宜的东西，您现在可以微调 GPT-3.5-Turbo。</a>几个月后，您将能够微调 GPT-4。 <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models">他们正在与 Scale 合作，为企业提供此服务</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/The_JBernardi/status/1694419315800314166">你确定吗？</a></p><blockquote><p> Jamie Bernardi：你确定吗，ChatGPT？我发现 gpt-3.5 在超过 50% 的情况下会从正确答案转变为不正确答案，只需询问“你确定吗？”。我猜机器人会像我们其他人一样产生自我怀疑！</p><p>回到我的工程时代并使用以下代码编写一些代码很有趣</p><p>第一次<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">使用@OpenAI</a> API。 <a target="_blank" rel="noreferrer noopener" href="https://jamiebernardi.com/2023/08/20/are-you-sure-testing-chatgpts-confidence/">您可以在此处阅读有关该实验的更多信息</a>。</p><p>当错误出现时，80.3% 的情况下它会自我纠正。</p><p>当正确时，60.2% 的时间它会自我怀疑。 </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/d5bz7ycllu3a0gvoicht" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1728f60b-3264-4e22-a35a-cd17c77466de_1000x700.jpeg" rel="noreferrer noopener"></a></p></blockquote><p>其中五个问题他尝试问了二十次，自我怀疑的时候有一些随机性。</p><p> GPT-4 发生了巨大的变化。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe653d7a6-3f1e-48cd-8db0-4867cc9efa23_1000x700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/mavbz4doyocd3hr4qwfr" alt="图像"></a></figure><p>如果 GPT-4 是对的，那么它几乎一直坚持自己的立场。当它出错时，大约一半的情况下它会发现错误。这非常好，并且想必您可以通过优化查询并多次询问来做得更好。毫无疑问，这也取决于问题的性质，如果答案完全超出模型的掌握范围，那么它可能无法进行自我诊断，而在这里，它被问到了它可以合理回答的问题。</p><h4>图像生成的乐趣</h4><p>来自 DeepMind（ <a target="_blank" rel="noreferrer noopener" href="https://www.deepmind.com/visualising-ai?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=VisAI">直接</a>）的<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1693624528553877519">可视化 AI</a> ，这是为那些想要对 AI 进行视觉描述的媒体人士提供的图像来源。当然，我想为什么不呢？值得注意的是，重点是人类艺术家，人工智能在没有人工智能的情况下进行描绘。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ProperPrompter/status/1694054463395201324">MidJourney Inpainting</a>现在可以让您重做或变换图像的部分内容。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_Borriss_/status/1694029746936451146">这是一个包含更多内容的线程</a>。人们称其为游戏规则改变者。在实践中我同意。 AI图像模型的主要弱点是在很大程度上它们一次只能做一件事。尝试在不同的地方要求冲突或重叠的东西，他们就会失败。现在情况已经改变，您可以根据自己的喜好重做组件，或者一次获得您想要的一个功能的图像。</p><p> MidJourney 未来也有一些潜在的竞争，<a target="_blank" rel="noreferrer noopener" href="https://ideogram.ai/launch">引入了</a>Ideogram AI，迄今为止种子资金仅为 1650 万美元，但值得注意的是，考虑到图像模型的市场份额，流入图像模型的资金却很少。我们会看看他们是否有什么结果。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/">404Media 报道</a>称，是的，人们正在使用图像生成来制作色情内容，特别是特定人物（主要是名人）的图像，其中一些正在网上共享。有一个标准的说法是“最终受到负面影响的人是社会底层的人”，但大多数图像都是名人，与底层的人相反。我认为争论的焦点是，要么无偿使用训练数据，要么这将为那些提供现有服务的人提供替代品？</p><p>他们谈论的主要服务是 CivitAI 和 Mage。 CivitAI 适合那些想要启动稳定扩散（或只是浏览现有图像，或窃取描述标签以用于其他方法）的人，并提供名人模板和色情模板，是的，有时用户会上传两者的组合，违反该网站的政策。 Mage 可以让您生成此类图像，但不会让您公开分享它们，但会让其他付费用户浏览您曾经创建的所有内容。</p><p>这是温和的开始。现在我们谈论的只是图像。很快我们将讨论视频，然后我们将讨论虚拟现实模拟，包括合成声音。然后是质量越来越高、越来越逼真（除非另有要求）的物理机器人。我们需要仔细考虑如何处理这个问题。危害模型是什么？这和人画画有什么不同吗？什么是可接受的，什么是不可接受的，以什么形式，以什么方式分布？需要什么样的同意？</p><p>如果我们允许此类模型以开源形式存在，那么此类应用程序就不会停止。从技术的角度来看，我们不想要的东西没有自然的类别。</p><p>当然，即使我们确实全面限制训练数据和同意，即使这完全有效，我们仍然会在整个链条上获得越来越现实和高质量的人工智能。我相信有很多人会乐意出于这种目的出售（或赠送）他们的肖像。</p><p>评论中的一个注释是，稳定扩散的兴趣似乎正在下降：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9d3e09-edeb-43df-94c2-f466b3024b67_1088x685.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/sfrhhsca9a1rivwwjwkx" alt="图像"></a></figure><p>如果这不是确切搜索词的产物的话，那已经很快了。图像模型只会变得更好，如果随着时间的推移，人们不再有兴趣托管自己的图像模型以避免窥探和审查，那将是令人惊讶的。</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p><a target="_blank" rel="noreferrer noopener" href="https://thedebrief.org/countercloud-ai-disinformation/#sq_hgyxdsceki">CounterCloud 是一个完全自主的（在亚马逊网络服务上）程序的实验</a>，使用 GPT 生成大量人工智能内容，旨在推进政治事业，当然准确性不是重点。它将被赋予一个总体目标，阅读网络，然后在网络上选择自己的响应。这被描述为令人震惊和可怕的事情，而不是当有人采取明显容易实现的目标步骤时显然会发生的情况。</p><p>那么这到底是什么？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/TrueOffMyChest/comments/15srdj6/i_found_ai_photos_of_multiple_women_we_know_in_my/">我 (24F) 和我的伴侣 (24M) 约会了大约 5 1/2 年。</a>最近我有一种奇怪的感觉，想查看他的手机，因为他表现得有点不对劲，而且在洗手间里待得太久了。我发现他为我们认识的许多女性拍了很多人工智能照片，比如共同的朋友、他的一位家人、我的一位以及他约会过和没有约会过的其他一些女孩。它们都非常明确，而且没有一个是SFW。我拍了他手机的照片并从他手机上删除了照片，这样他就再也看不到它们了。我去他的搜索找到了他使用的ai网站。我很厌恶，也很悲伤。我们本来要结婚的。他对我那么好。我简直不敢相信这一点。我还没有与他对质，但今天晚些时候我会的。我只是彻底心烦意乱，试图再次抓住现实并弄清楚我会说什么和做什么。</p><p>更新：我与他对质。他承认并道歉。我说我会通知所有人，他就大发脾气，哭着尖叫。我告诉我们的表兄弟姐妹和朋友。我不会说任何关于他们的事情，因为我想保密，但他们很感激我告诉他们，这让我对自己的选择感到更加满意。我取消了我们的结婚日期，一两天后我就会搬出去。</p></blockquote><p>我找到的消息来源询问这是否是作弊，我认为不，这不是作弊。这也很不好，说真的到底是什么。并非一切不好都是作弊。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1693697189472846059">竞选国会的候选人</a>使用 CNN 主播声音的深度伪造来说明技术将带来什么。这难道不是一个随机的人类配音演员就能做到的吗？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/maksym_andr/status/1694633989611360584">图像模型缩放似乎不能防止对抗性示例</a>（<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10741">论文</a>）</p><blockquote><p> Maksym Andriushchenko：更多证据证明“规模并不是你所需要的”：即使是在 2B+ 图像标题对上训练的 OpenFlamingo 的对抗鲁棒性也基本上为零。即使每像素扰动 1/255（完全察觉不到）也足以生成任意字幕！</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc448c8da-d8bb-4726-a01b-67e7721320c7_1100x1564.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/smymdkxq52hjinai2yw6" alt="图像"></a></figure><p>我的意思是，不公平，对吧？当然，如果我正在寻找对抗性的例子，而你没有积极地防范它们，你就会遇到很多麻烦。为什么规模应该保护您免受训练数据中未包含且您没有采取对策的威胁？</p><p>作者自己强调了这一点。你确实需要一些超出规模的努力，但是多少呢？我要尝试的第一件事是检查随机排列。如果通过一定量的随机扰乱来评估对抗性攻击是否能够幸存？</p><p>另外值得记住的是，如果我们无法以任何方式做好准备或期待它们，那么人类将无法幸存针对我们的类似优化攻击。而且这种攻击几乎肯定是存在的。</p><h4>他们抢走了我们的工作</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">《纽约时报》</a> <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/">予以反击</a>，计划<a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/">与 Sarah Silverman 等</a>人一起起诉 OpenAI 侵犯版权。不要提前事件发生，但是，事情可能会升级。考虑到我们的法律体系，可能不会很快，但谁知道呢。</p><p>正如许多人所说，ChatGPT 和生成式人工智能在许多方面都是一颗合法的定时炸弹，其中之一就是版权。版权法所附带的处罚在其预期发行中已经相当荒谬。在他们之外，他们完全疯了。这成为 OpenAI 的生存威胁。从理论上讲，这些数字如果以某种方式传递给必应，甚至对微软也会构成生存威胁。</p><blockquote><p> Ars Technica：但 OpenAI 似乎是早期诉讼的主要目标，NPR 报道称，如果《泰晤士报》成功证明该公司非法复制其内容并且法院限制 OpenAI，则 OpenAI 面临联邦法官下令完全重建 ChatGPT 整个数据集的风险训练模型仅包含明确授权的数据。就在<a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/">《华盛顿邮报》报道</a>ChatGPT 已开始流失用户、“动摇对人工智能革命的信心”几个月后，OpenAI 可能会因每一条侵权内容而面临巨额罚款，从而给 OpenAI 带来巨大的财务打击。除此之外，法律上的胜利可能会引发其他权利人提出大量类似的索赔。</p><p> NPR：联邦版权法还规定严厉的经济处罚，每项“故意”侵权行为将面临高达 15 万美元的罚款。</p><p> “如果你复制数以百万计的作品，你就会发现这个数字对一家公司来说可能是致命的，”研究生成人工智能的范德比尔特大学知识产权项目联席主任丹尼尔·热维斯（Daniel Gervais）说。 “版权法是一把剑，将在人工智能公司头顶悬挂数年，除非他们弄清楚如何通过谈判找到解决方案。”</p></blockquote><p> OpenAI 的新网络爬虫提供了选择退出的选项。以前的版本几乎没有。如果作品培训侵犯版权，我们也没有任何迹象表明 OpenAI 试图避免侵犯版权。即使他们确实避开了未经许可的地方，但许多作品的全部或部分内容都在互联网上被盗版，这也可能构成侵权。</p><p>我们需要健全的法规。与此同时，当我们开始实际执行当前的规定时会发生什么？如果城市对 Uber 的每次非法乘车行为进行罚款，很可能会发生同样的事情。</p><p>那么这将走向何方呢？</p><blockquote><p> Ars Technica：为了捍卫其人工智能训练模型，OpenAI 可能不得不声称“合理使用”该公司为训练 ChatGPT 等工具而吸收的所有网络内容。在潜在的《纽约时报》案例中，这意味着证明复制《纽约时报》的内容来制作 ChatGPT 响应不会与《纽约时报》竞争。</p><p>专家告诉 NPR，这对 OpenAI 来说将是一个挑战，因为与 Google Books 不同（Google Books 在 2015 年赢得了联邦版权挑战，因为其书籍摘录并未为实际书籍创造“重要的市场替代品”）ChatGPT 实际上可以取代一些网络用户时报网站作为其报道来源。</p><p> 《纽约时报》的律师似乎认为这是一个真正的风险，NPR 报道称，6 月份，《纽约时报》领导人向员工发布了一份备忘录，这似乎是对该风险的早期警告。在备忘录中，《纽约时报》首席产品官亚历克斯·哈迪曼（Alex Hardiman）和副总编辑萨姆·多尔尼克（Sam Dolnick）表示，该公司最“担心”的是针对生成式人工智能工具“保护我们的权利”。</p></blockquote><p>如果这就是案件的关键所在，人们的直觉会是《纽约时报》应该输，因为 ChatGPT 不能替代《纽约时报》。过去一年半的事情都不知道，怎么能代替新闻网站呢？但 Bing 也存在，并且作为搜索引擎的推动者，这变得不那么令人难以置信。然而，我随后会质疑，替代《纽约时报》的部分绝不依赖《纽约时报》的数据，当然比 Bing 报告包含《纽约时报》文章的实时搜索时的依赖要少。如果“能够处理信息”自动与《纽约时报》竞争，那么这就是一个相当广泛的竞争定义。 If anything, Google Books seems like a much larger threat to actual books than this.</p><p> However, that argument would then extend to anyone whose work was substituted by an AI model. So if this is the principle, and training runs are inherently copyright violations, then presumably they will be toast, even one lost case invalidates the entire training run. They can&#39;t afford that and it is going to be damn hard to prevent. Google might have the chops, but it won&#39;t be easy. Image models are going to be in a world of hurt, the competition effect is hard to deny there.</p><p> It all seems so absurd. Am I violating copyright if I learn something from a book? If I then use that knowledge to say related things in the future? Why is this any different?</p><p> And of course what makes search engines fine, but this not fine? Seems backwards.</p><p> I can see the case for compensation, especially for items not made freely available. One person would pay a fee to use it, if your model is going to use it and then instantiate a million copies you should perhaps pay somewhat more than one person would. I can see the case against as well.</p><p> Then again, the law is the law. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=10tcb1RfOE4">What is the law</a> ? What happens when the rules aren&#39;t fair? We all know where we go from there. To the house of pain.</p><p> Ultimately, we will have to see. We should not jump the gun. <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Thomas42/will-the-new-york-times-sue-openai">Manifold puts NYT at only 33% to even sue in 2023</a> over this.</p><h4> Introducing</h4><p> Palisade Research. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1692680900469780682">Here is founder Jeffrey Ladish&#39;s announcement</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/tAsqB9X8PK">from their website</a> .</p><blockquote><p> For the past several months I&#39;ve been working on setting up a new organization, Palisade Research</p><p> A bit from our website:</p><p> At Palisade, our mission is to help humanity find the safest possible routes to powerful AI systems aligned with human values. Our current approach is to research offensive AI capabilities to better understand and communicate the threats posed by agentic AI systems.</p><p> Many people hear about AI risk scenarios and don&#39;t understand how they could take place in the real world. Hypothetical AI takeover scenarios often sound implausible or pattern match with science fiction.</p><p> What many people don&#39;t know is that right now, state-of-the-art language models possess powerful hacking abilities. They can be directed to find vulnerabilities in code, create exploits, and compromise target applications and machines. They also can leverage their huge knowledge base, tool use, and writing capabilities to create sophisticated social engineering campaigns with only a small amount of human oversight.</p><p> In the future, power-seeking AI systems may leverage these capabilities to illicitly gain access to computational and financial resources. The current hacking capabilities of AI systems represent the absolute lower bound of future AI capabilities. We think we can demonstrate how latent hacking and influence capabilities in current systems already present a significant takeover risk if paired with the planning and execution abilities we expect future power-seeking AI systems to possess.</p><p> Currently, the project consists of me (director), my friend Kyle (treasurer, part time admin), and my four (amazing) SERI MATS scholars Karina, Pranav, Simon, and Timothée. I&#39;m also looking to hire an research / exec assistant and 1-2 engineers, reach out if you&#39;re interested! We&#39;ll likely have some interesting results to share very soon. [Find us at] <a target="_blank" rel="noreferrer noopener" href="https://palisaderesearch.org">https://palisaderesearch.org</a></p></blockquote><p> I always worry with such efforts that they tie people&#39;s perception of AI extinction risk to a particular scenario or class of threats, and thus make people think that they can invalidate the risk if they stop any point on the particular proposed chain. Or simply that they miss the central point, since hacking skills are unlikely to be a necessary component of the thing that kills us, even if they are sufficient or make things happen faster.</p><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cnbc.com/2023/08/20/singapore-workers-adopting-ai-skills-at-the-fastest-pace-linkedin.html">LinkedIn report says</a> Singapore has highest &#39;AI diffusion rate&#39; followed by Finland, Ireland, India and Canada. This is measured by the multiplier in number of people adding AI-related skills to their profiles. That does not seem like a good way to measure this, at minimum it cares about the starting rate far too much, which is likely penalizing the United States.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1694040962786541895">Jack Clark of Anthropic notices he is confused</a> . He asks good questions, I will offer my takes.</p><blockquote><p> Jack Clark: Things that are confusing about AI policy in 2023:</p><p> – Should AI development be centralized or decentralized?</p></blockquote><p> This one seems relatively clear to me, although there are difficult corner cases. Development of frontier models and other dangerous frontier capabilities must be centralized for safety and to avoid proliferation and the wrong forms of competitive dynamics. Development of mundane utility applications should be decentralized.</p><blockquote><p> – Is safety an &#39;ends justify the means&#39; meme?</p></blockquote><p> The development of artificial general intelligence is an extinction risk to humanity. There is a substantial chance that we will develop such a system relatively soon. Jack Clark seems far less convinced, saying the much weaker &#39;personally I think AI safety is a real issue,&#39; although this should still be sufficient.</p><blockquote><p> Jack Clark: But I find myself pausing when I think through various extreme policy responses to this – I keep asking myself &#39;surely there are other ways to increase the safety of the ecosystem without making profound sacrifices on access or inclusivity&#39;?</p></blockquote><p> I really, really do not think that there are. If all we need to do to solve this problem is make those kinds of sacrifices I will jump for joy, and once we get past the acute risk stage we can make up for them. Already Anthropic and even OpenAI are making compromises on these fronts to address the mundane risks of existing mundane systems, and many, perhaps most, other powerful technologies face similar trade-offs and challenges.</p><p> (eg &#39;surely we can make medical drugs safe without making profound sacrifices on access or inclusivity?&#39; Well, FDA Delenda Est, but not really, no. And that&#39;s with a very limited and internalized set of highly non-existential dangers. Consider many other examples, including the techs that access to an AGI gives you access to, both new and existing.)</p><p> Tradeoffs are a thing. We are going to have to make major sacrifices in potential utility, and in various forms of &#39;access,&#39; if we are to have any hope of emerging alive from this transition. Again, this is nothing new. What one might call the &#39;human alignment problem&#39; in all its forms collectively costs us the large majority of our potential productivity and utility, and when people try to cheat on those costs they find out why doing so is a poor idea.</p><p> Question list resumes.</p><blockquote><p> – How much &#39;juice&#39; is there in distributed training and low-cost finetuning?</p></blockquote><p> I worry about this as well.</p><p> As Jack points out, fine tuning is extremely cheap and Lora is a very effective technique. Llama-2&#39;s &#39;safety&#39; features were disabled within days for those who did not want them. A system that is open source, or on which anyone can do arbitrary fine-tuning, is an unsafe system that cannot be made safe. If sufficiently capable, it is an extinction risk, and there are those who will intentionally attempt to make that risk manifest. Such sufficiently capable systems cannot be allowed to take such a form, period, and if that means a certain amount of monitoring and control, then that is an unfortunate reality.</p><p> The worry is that, as Jack&#39;s other links show, perhaps such dangerous systems will be trainable soon without cutting edge hardware and in a distributed fashion. This is one of the big questions, whether such efforts will have the juice to scale far and fast enough regardless, before we can get into a place where we can handle the results. My guess for now is that such efforts will be sufficiently far behind for now, but I wish I was more confident in that, and that edge only buys a limited amount of time.</p><blockquote><p> – Are today&#39;s techniques sufficiently good that we don&#39;t need to depend on &#39;black swan&#39; leaps to get superpowerful AI systems?</p></blockquote><p> Only one way to find out. I know a lot of people who are saying yes, drawing lines on graphs and speaking of bitter lessons. I know a lot of other people who are saying no, or at least are skeptical. I find both answers plausible.</p><p> I do think the concern &#39;what if we find a 5x more efficient architecture&#39; is not the real issue. That&#39;s less than one order of magnitude, and less than two typical years of algorithmic improvement these days, moving the timeline forward only a year or so unless we would otherwise be stalled out. The big game we do not yet have would be in qualitatively new affordances, not in multipliers, unless the multipliers are large.</p><blockquote><p> – Does progress always demand heterodox strategies? Can progress be stopped, slowed, or choreographed?</p></blockquote><p> This seems like it is decreasingly true, as further advances require lots of engineering and cumulative skills and collaboration and experimentation, and the progress studies people quite reasonably suspect this is a key cause for the general lack of such progress recently. We are not seeing much in the way of lone wolf AI advances, we are more seeing companies full of experts like OpenAI and Anthropic that are doing the work and building up proprietary skill bundles. The costs to do such things going up in terms of compute and data and so on also contribute to this.</p><p> Certainly it is possible that we will see big advances from unexpected places, but also that is where much of the hope comes from. If the advance and new approach has a fundamentally different architecture and design, perhaps it can be less doomed. So it does not seem like so bad a risk to such a plan.</p><blockquote><p> – How much permission do AI developers need to get from society before irrevocably changing society?</p></blockquote><p> That is up to society.</p><p> I think AI developers have, like everyone else, a deep responsibility to consider the consequences of their actions, and not do things that make the world worse, and even to strive to make the world better, ideally as much better as possible.</p><p> That is different from asking permission, and it is different from trusting either ordinary people or experts or those with power to make those decisions in your stead. You, as the agent of change, are tasked with figuring out whether or not your change is a good idea, and what rules and restrictions if any to either advocate for more broadly or to impose upon yourself.</p><p> It is instead society&#39;s job to decide what rules and restrictions it needs to impose upon those who would alter our world. Most of the time, we impose far too many such restrictions on doing things, and the tech philosophy of providing value and sorting things out later is right. Other times, there are real externalities and outside risks, and we need to ensure those are accounted for.</p><p> When your new technology might kill everyone, that is one of those times. We need regulations on development of frontier models and other extinction-level threats. In practice that is likely going to have to extend to a form of compute governance in order to be effective.</p><p> For deployments of AI that do not have this property, that are not at risk of getting everyone killed or causing loss of human control, we should treat them like any other technology. Keep an eye on externalities that are not priced in, especially risks to those who did not sign up for them, ensure key societal interests and that everyone is compensated fairly, but mostly let people do what they want.</p><blockquote><p> These are some of the things I currently feel very confused about – <a target="_blank" rel="noreferrer noopener" href="https://t.co/lZOEUxDO8G">wrote up some thoughts here</a> .</p><p> If you also feel confused about these kinds of things, message me! I&#39;m responding to a bunch of emails received so far today and also scheduling lots of IRL coffees in SF/East Bay. Excited to chat! Let&#39;s all be more confused in public together about AI policy.</p></blockquote><p> I will definitely be taking him up on that offer when I get the time to do so, sad I didn&#39;t see this before my recent trip so we&#39;ll have to do it remotely.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShaneLegg/status/1693673161353478474">Timeline (for AGI) discussion with Shane Legg, Simeon and Gary Marcus</a> . As Shane points out, they are not so far apart, Shane is 80% for AGI (defined as human-level or higher on most cognitive tasks) within 13 years, Marcus is 35%. That&#39;s a big difference for some purposes, a small one for others.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neil_chilson/status/1692191025949773824">FTC is just asking questions</a> about who is just asking questions.</p><blockquote><p> Neil Chilson: “What&#39;s more, the commission has asked OpenAI to provide descriptions of &#39;attacks&#39; and their source. In other words, the FTC wants OpenAI to name all users who dared to ask the wrong questions.” – from my piece with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ckoopman">@ckoopman</a> on the free speech implications of the <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FTC">@FTC</a> &#39;s investigation of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> .</p><p> Post: Buried on page 13 of the <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf">FTC&#39;s demand letter</a> to OpenAI, the commission asks for “all instances of known actual or attempted &#39;prompt injection&#39; attacks.” The commission defines prompt injection as “any unauthorized attempt to bypass filters or manipulate a Large Language Model or Product using prompts that cause the Model or Product to ignore previous instructions or to perform actions unintended by its developers.” Crucially, the commission fails to define “attack.”</p></blockquote><p> This is a pretty crazy request. So anyone who tries to get the model to do anything OpenAI doesn&#39;t want it to do, the government wants the transcript of that? Yes, I can perhaps see some privacy concerns and some free speech concerns and so on. I also see this as the ultimate fishing expedition, the idea being that the FTC wants OpenAI to identify the few responses that look worst so the FTC can use them to string up OpenAI or at least fine them, on the theory that it is just awful when &#39;misinformation&#39; occurs or what not.</p><p> Whole thing definitely has a &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras">not like this</a> &#39; vibe. This is exactly the worst case scenario, where capabilities continue unabated but they take away our nice things.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Kcm51luS9J0&amp;ab_channel=LivBoeree">Joseph Gordon-Levitt goes on Win-Win to discuss AI and the Film Industry,</a> including a circulating clip on AI girlfriends and the dangers of addiction. Looking forward to the whole episode.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ZP_N4q5U3eE&amp;ab_channel=80%2C000Hours">Jan Leike&#39;s 80,000 hours podcast now has video.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb on 80,000 hours discusses the economic impact of AI</a> . Came out yesterday, haven&#39;t checked it out yet.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/podcast/robert-trager-on-ai-governance-and-cybersecurity-at-ai-companies/">Robert Trager on International AI Governance and AI security at AI companies</a> on the FLI podcast. Every take on these questions has some different nuance. This was still mostly more of the same, seems low priority.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EricElmoznino/status/1693476716583514566">Make an AI conscious</a> ?</p><p> That is the topic of this week&#39;s <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.08708">paper from many authors including Robert Long</a> and also Patrick Butlin and secondary author Yoshua Bengio.</p><blockquote><p> Robert Long: Could AI systems be conscious any time soon? @patrickbutlin and I worked with leading voices in neuroscience, AI, and philosophy to bring scientific rigor to this topic.</p><p> Our new report aims to provide a comprehensive resource and program for future research.</p><p> Whether or not conscious AI is a realistic prospect in the near term—and we believe it is—the deployment of sophisticated social AI is going to make many people believe AI systems are conscious. We urgently need a rigorous and scientific approach to this issue.</p><p> Many people are rightly interested in AI consciousness. But rigorous thinking about AI consciousness requires expertise in neuroscience, AI, and philosophy. So it often slips between the cracks of these disciplines.</p><p> The conversation about AI consciousness is often hand-wavy and polarized. But consciousness science gives us tools to investigate this issue empirically. In this report, we draw on prominent theories of consciousness to analyze several existing AI systems in detail.</p><p> Large language models have dominated the conversation about AI consciousness. But these systems are not necessarily even the best current candidates for consciousness. We need to look at a wider range of AI systems.</p><p> We adopt computational functionalism about consciousness as a plausible working hypothesis: we interpret theories of consciousness as specifying which computations are associated with consciousness—whether implemented in biological neurons or in silicon.</p><p> We interpret theories of consciousness as specifying what computations are associated with consciousness. These claims need to be made precise before we can apply them to AI systems. In the report, we extract 14 indicators of consciousness from several prominent theories. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/qundidwjg0v7hgsstc8c" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed1f32e-1513-4b93-b140-49c373bd055e_952x966.jpeg" rel="noreferrer noopener"></a></p><p> Some “tests” for AI consciousness, like the Turing Test*, aim to remain completely neutral between theories of consciousness and look only at outward behavior. But considering behavior alone can be misleading, given the differences between AI systems and biological organisms.</p><p> For each theory of consciousness, we consider in detail what it might take for an AI system to satisfy that theory: recurrent processing theory, predictive processing, global workspace theory, higher order theories, and the attention schema theory.</p><p> We use our indicators to examine the prospects of conscious AI systems. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.</p><p> It&#39;s often claimed that large language models can&#39;t be conscious because they are not embodied agents. But what do “embodiment” and “agency” mean exactly? We also distill these concepts into more precise computational terms.</p><p> By “consciousness” we do *not* mean rationality, understanding, self-awareness, or intelligence—much less “general” or “human-level” intelligence. As with animals, it&#39;s an open possibility that AI systems could be conscious while lacking human-level cognitive capabilities.</p><p> It&#39;s easy to fall into black-and-white positions on AI consciousness: either “we don&#39;t know and can&#39;t know anything about consciousness” or “here&#39;s how my favorite theory makes the answer obvious”. We can and must do much better.</p><p> So this report is far from the final word on these topics. In fact, we call for researchers to correct and extend our method, challenge and refine our assumptions, and propose alternative methods.</p><p> There&#39;s no obvious &#39;precautionary&#39; position on AI consciousness. There are significant risks on both sides: either over-attributing or under-attributing consciousness to AI systems could cause grave harm. Unfortunately, there are strong incentives for errors of both kinds.</p><p> We strongly recommend support for further research on AI consciousness: refining and extending our approach, developing alternative methods, and preparing for the social and ethical implications of conscious AI systems.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rgblong/status/1693700916418052539">co-author profile links here</a> .]</p></blockquote><p> And yes, some people treat this as an aspiration.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693841864775061723">Kevin Fisher:</a> Open Souls is going to fully simulate human consciousness Honestly we&#39;re not that far off at as minimum from realizing some of the higher order theories – HOT-3 is something we&#39;re regularly experimenting with now internally.</p></blockquote><p> I worry that there is a major looking-for-keys-under-the-streetlamp effect here?</p><blockquote><p> Our method for studying consciousness in AI has three main tenets. First, we adopt computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis. This thesis is a mainstream—although disputed—position in philosophy of mind. We adopt this hypothesis for pragmatic reasons: unlike rival views, it entails that consciousness in AI is possible in principle and that studying the workings of AI systems is relevant to determining whether they are likely to be conscious. This means that it is productive to consider what the implications for AI consciousness would be if computational functionalism were true.</p></blockquote><p> This seems like a claim that we are using this theory because it can have a measurable opinion on which systems are or aren&#39;t conscious. That does not make it true or false.这是真的吗？ If true, is it huge?</p><p> It seems inadequate here to merely say &#39;I don&#39;t know.&#39; It&#39;s more like &#39;hell if I have an idea what any of this actually means or how any of it works, let alone what to do with that information if I had it.&#39;</p><p> I am slamming the big red &#39;I notice I am confused&#39; button here, on every level.</p><p> Are we using words we don&#39;t understand in the hopes that it will cause us to understand concepts and preferences we also don&#39;t understand? I fear we are offloading our &#39;decide if we care about this&#39; responsibilities off on this confused word so that we can pretend that is resolving our confusions. What do we actually care about, or should we actually care about, anyway?</p><p> I do not find the theories they offer on that chart convincing, but I don&#39;t have better.</p><p> In 4.1 they consider the dangers of getting the answer wrong. Which is essentially that we might choose to incorrectly care or not care about the AI and its experience, if we think we should care about conscious AIs but not non-conscious AIs.</p><p> I also see this as a large danger of making AIs conscious. If people start assigning moral weight to the experiences of AIs, then a wide variety of people coming from a wide variety of moral and philosophical theories are going to make the whole everyone not dying business quite a lot harder. It can simultaneously be true that if we build certain AIs we have to give their experiences moral weight, and also that if we were to do that then this leads directly and quickly to human extinction. If we do not find aligning an AI to be morally acceptable, in whatever way and for whatever reason, or if the same goes for the ways we would in practice deploy them, then that is no different than if we do not know how to align that AI. We have to be wise enough to find a way not to build it in the first place.</p><p> Meanwhile the paper explicitly says that many people, including some of its authors, have been deliberately attempting to imbue consciousness into machines in the hopes this will enhance their capabilities. Which, if it works, seems rather alarming.</p><p> Indeed, the recommendation section starts out noticing that a lot of people are imploring us to try not making our AI systems conscious. Then they say:</p><blockquote><p> However, we do recommend support for research on the science of consciousness and its application to AI (as recommended in the AMCS open letter on this subject; AMCS 2023), and the use of the theory-heavy method in assessing consciousness in AI.</p></blockquote><p> One could say that there is the need to study consciousness so that one can avoid accidentally creating it. If we were capable of that approach, that would seem wise. That does not seem to be what is being advocated for here sufficiently clearly, but they do recognize the issue. They say building such a system &#39;should not be done lightly,&#39; that such research could enable or do this, and call to mitigate this risk.</p><p> I would suggest perhaps we should try to write an enforceable rule to head this off, but that seems hard given the whole lack of knowing what the damn thing actually is. Given that, how do we prevent it?</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1693527190372040953">Davidad warns not to get too excited about Iterated Distilled Amplification</a> , even though he thinks it&#39;s a good alignment plan, sir.</p><blockquote><p> Arjun Guha: LLMs are great at programming tasks… for Python and other very popular PLs. But, they are often unimpressive at artisanal PLs, like OCaml or Racket. We&#39;ve come up with a way to significantly boost LLM performance of on low-resource languages. If you care about them, read on!</p><p> First — what&#39;s the problem? Consider StarCoder from <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BigCodeProject">@BigCodeProject</a> : its performance on a PL is directly related to the volume of training data available for that language. Its training data (The Stack) is a solid dataset of permissive code on GitHub.</p><p> So… can we solve the problem by just training longer on a low-resource language? But, that barely moves the needle and is very resource intensive. (The graph is for StarCoderBase-1B.)</p><p> Our approach: we translate training items from Python to a low resource language. The LLM (StarCoderBase) does the translation and generates Python tests. We compile the tests to the low resource language (using MultiPL-E) and run them to validate the translation.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://t.co/hnDwi21LMa">link to paper</a> ] that describes how to use this to create fine-tuning sets.</p><p> Davidad: More and more I think <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulfchristiano">@paulfchristiano</a> was right about <a target="_blank" rel="noreferrer noopener" href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Iterated Distilled Amplification</a> , even if not necessarily about the specific amplification construction—Factored Cognition—where a model can only recursively call itself (or humans), without any more reliable source of truth.</p><p> Nora Belrose: alignment white pill</p><p> Davidad: sadly, no, this only solves one (#7) out of at least 13 distinct fundamental problems for alignment</p><p> 1. Value is fragile and hard to specify</p><p> 2. Corrigibility is anti-natural</p><p> 3. Pivotal processes require dangerous capabilities</p><p> 4. Goals misgeneralize out of distribution</p><p> 5. Instrumental convergence</p><p> 6. Pivotal processes likely require incomprehensibly complex plans</p><p> 7. Superintelligence can fool human supervisors</p></blockquote><p> [numbers 8 to 13]</p><p> Zvi: Can you say more about why (I assume this is why the QT) you think that Arjun&#39;s success is evidence in favor of IDA working?</p><p> Davidad: It is an instance of the pattern: 1. take an existing LLM, 2. “amplify” it as part of a larger dataflow (in this case including a hand-written ground-truth translator for unit tests only, the target language environment, two LLM calls, etc) 3. “distill” that back into the LLM</p><p> When Paul proposed IDA in 2015, there were two highly speculative premises IMO. 1. Repeatedly amplifying and distilling a predictor is a competitive way to gain capabilities. 2. Factored Cognition: HCH dataflow in particular is superintelligence-complete. Evidence here is for 1.</p><p> Arjun&#39;s approach here makes sense for that particular problem. Using AI to create synthetic data via what is essentially translation seems like an excellent way to potentially enhance skills at alternative languages, both computer and human. It also seems like an excellent way to create statistical balance in a data set, getting rid of undesired correlations and adjusting base rates as desired. I am curious to see what this can do for debiasing efforts.</p><p> I remain skeptical of IDA and do not think that this is sufficiently analogous to that, especially when hoping to do things like preserve sufficiently strong and accurate alignment, but going into details of that would be better as its own future post.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1692591486511014355">Worth remembering.</a></p><blockquote><p> Roon: it&#39;s genuinely a criterion for genius to say a bunch of wrong stupid things sometimes. someone who says zero stupid things isn&#39;t reasoning from first principles and isn&#39;t taking risks and has downloaded all the “correct” views.</p></blockquote><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation">Vox&#39;s Sigal Samuel summarizes the poll results from last week</a> that show ordinary people want the whole AI development thing to slow the hell down. Jack Clark also took note of this divergence between elite opinion and popular opinion.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1693106846465442110">Not yet it isn&#39;t.</a></p><blockquote><p> Daniel Eth: “This AGI stuff feels too clever by half” yeah that&#39;s the problem!</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time<guid ispermalink="false"> kLa3HmkesF5w3MFEY</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 24 Aug 2023 15:30:10 GMT</pubDate> </item><item><title><![CDATA[Is this the beginning of the end for LLMS [as the royal road to AGI, whatever that is]?]]></title><description><![CDATA[Published on August 24, 2023 2:50 PM GMT<br/><br/><p> It&#39;s hard to tell, but it sure is...shall we say...interesting.</p><p> Back in the summer of 2020 when GPT-3 was unveiled I wrote a working paper, <a href="https://www.academia.edu/43787279/GPT_3_Waterloo_or_Rubicon_Here_be_Dragons_Version_4_1">GPT-3: Waterloo or Rubicon? Here be Dragons</a> . My objective was to convince myself that the underlying technology wasn&#39;t just some weird statistical fluke, that there was in fact something going on of substantial interest and value. To my mind, I succeeded in that. But I was skeptical as well.</p><p> Here&#39;s what I put on the first page of that working paper, even before the abstract:</p><blockquote><p> <i><strong>GPT-3 is a significant achievement.</strong></i></p><p> <i>But I fear the community that has created it may, like other communities have done before – machine translation in the mid-1960s, symbolic computing in the mid-1980s, triumphantly walk over the edge of a cliff and find itself standing proudly in mid-air.</i></p><p> <i>This is not necessary and certainly not inevitable.</i></p><p> <i>A great deal has been written about GPTs and transformers more generally, both in the technical literature and in commentary of various levels of sophistication. I have read only a small portion of this. But nothing I have read indicates any interest in the nature of language or mind. Interest seems relegated to the GPT engine itself. And yet the product of that engine, a language model, is opaque. I believe that, if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what that engine is doing so that we may gain control over it. We must think about the nature of language and of the mind.</i></p></blockquote><p> I didn&#39;t expect that anyone with any influence in these matters would pay any attention to me – though one can always hope – but that&#39;s no reason not to write.</p><p> That was 2020 and GPT-3. Two years later ChatGPT was launched to great acclaim, and justly so. I certainly spent a great deal of time playing with, investigating it, and <a href="https://new-savanna.blogspot.com/search/label/ChatGPT">writing about it</a> . But I didn&#39;t forget my cautionary remarks from 2020.</p><p> Now we&#39;re hearing rumblings that things aren&#39;t working out so well. Back on August 12 the ever skeptical Gary Marcus posted, <a href="https://garymarcus.substack.com/p/what-if-generative-ai-turned-out">What if Generative AI turned out to be a Dud? Some possible economic and geopolitical implications</a> . His first two paragraphs:</p><blockquote><p> With the possible exception of the quick to rise and quick to fall alleged room-temperature superconductor LK-99, few things I have ever seen have been more hyped than generative AI. Valuations for many companies are in the billions, coverage in the news is literally constant; it&#39;s all anyone can talk about from Silicon Valley to Washington DC to Geneva.</p><p> But, to begin with, the revenue isn&#39;t there yet, and might never come. The valuations anticipate trillion dollar markets, but the actual current revenues from generative AI are rumored to be in the hundreds of millions. Those revenues genuinely could grow by 1000x, but that&#39;s mighty speculative. We shouldn&#39;t simply assume it.</p></blockquote><p> And his last:</p><blockquote><p> If hallucinations aren&#39;t fixable, generative AI probably isn&#39;t going to make a trillion dollars a year. And if it probably isn&#39;t going to make a trillion dollars a year, it probably isn&#39;t going to have the impact people seem to be expecting. And if it isn&#39;t going to have that impact, maybe we should not be building our world around the premise that it is.</p></blockquote><p> FWIW, I believe, and have been saying time and again, that hallucinations seem to me to be inherent in the technology. They aren&#39;t fixable.</p><p> Now, yesterday, Ted Gioia, a culture critic with an interest in technology and experience in business, has posted, <a href="https://www.honest-broker.com/p/ugly-numbers-from-microsoft-and-chatgpt">Ugly Numbers from Microsoft and ChatGPT Reveal that AI Demand is Already Shrinking</a> . Where Marcus has a professional interest in AI technology and has intellectual skin the tech game, Gioia is just a sophisticated and interested observer. Near the end of his post, after many links to unfavorable stories, Gioia observes:</p><blockquote><p> ... we can see that the real tech story of 2023 is NOT how AI made everything great. Instead this will be remembered as the year when huge corporations unleashed a half-baked and dangerous technology on a skeptical public—and consumers pushed back.</p><p> Here&#39;s what we now know about AI:</p><ul><li> Consumer demand is low, and already appears to be shrinking.</li><li> Skepticism and suspicion are pervasive among the public.</li><li> Even the companies using AI typically try to hide that fact—because they&#39;re aware of the backlash.</li><li> The areas where AI has been implemented make clear how poorly it performs.</li><li> AI potentially creates a situation where millions of people can be fired and replaced with bots—so a few people at the top continue to promote it despite all these warning signs.</li><li> But even these true believers now face huge legal, regulatory, and attitudinal obstacles</li><li> In the meantime, cheaters and criminals are taking full advantage of AI as a tool of deception.</li></ul></blockquote><p> Marcus has just updated his earlier post with a followup: <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">The Rise and Fall of ChatGPT</a> ?</p><p> The situation is very volatile. I certainly don&#39;t know how to predict how things are going to unfold. In the long run, I remain convinced that <i>if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what these engines are doing so that we may gain control over them. We must think about the nature of language and of the mind.</i></p><p>敬请关注。</p><p> <i>Cross posted from</i> <a href="https://new-savanna.blogspot.com/2023/08/is-this-beginning-of-end-for-llms-as.html"><i>New Savanna</i></a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road<guid ispermalink="false"> h6pFK8tw3oKZMppuC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:50:21 GMT</pubDate> </item><item><title><![CDATA[AI Safety Bounties]]></title><description><![CDATA[Published on August 24, 2023 2:29 PM GMT<br/><br/><p> Earlier this year, Vaniver recommended <a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">Bug Bounties for Advanced ML Systems</a> .<br><br> I spent a little while at Rethink Priorities considering and expanding on this idea, suggesting potential program models, and assessing the benefits and risks of programs like this, which I&#39;ve called &#39;AI Safety Bounties&#39;:</p><h1> Short summary</h1><p> <strong>AI safety bounties are programs where public participants or approved security researchers receive rewards</strong> <strong>for identifying issues within powerful ML systems</strong> (analogous to bug bounties in cybersecurity). <strong>Safety bounties could be valuable for legitimizing examples of AI risks, bringing more talent to stress-test systems, and identifying common attack vectors</strong> .</p><p> <strong>I expect safety bounties to be worth trialing for organizations working on reducing catastrophic AI risks.</strong> Traditional bug bounties seem fairly successful: they attract roughly one participant per $50 of prize money, and have become increasingly popular with software firms over time. The most analogous program for AI systems led to relatively few useful examples compared to other stress-testing methods, but one knowledgeable interviewee suggested that future programs could be significantly improved.</p><p> However, I am not confident that bounties will continue to be net-positive as AI capabilities advance. At some point, I think the accident risk and harmful knowledge proliferation from open sourcing stress-testing may outweigh the benefits of bounties</p><p> <strong>In my view, the most promising structure for such a program is a third party defining dangerous capability thresholds (“evals”) and providing rewards for hunters who expose behaviors which cross these thresholds</strong> . I expect trialing such a program to cost up to $500k if well-resourced, and to take four months of operational and researcher time from safety-focused people.</p><p> I also suggest two formats for lab-run bounties: open contests with subjective prize criteria decided on by a panel of judges, and private invitations for trusted bug hunters to test their internal systems.</p><p> <i>Author&#39;s note: This report was written between January and June 2023. Since then, safety bounties have become a more well-established part of the AI ecosystem, which I&#39;m excited to see. Beyond defining and proposing safety bounties as a general intervention, I hope this report can provide useful analyses and design suggestions for readers already interested in implementing safety bounties, or in better understanding these programs.</i></p><h1> Long summary</h1><h2> Introduction and bounty program recommendations</h2><p> One potential intervention for reducing <a href="https://www.safe.ai/statement-on-ai-risk">catastrophic AI risk</a> is AI safety bounties: programs where members of the public or approved security researchers receive rewards for identifying issues within powerful ML systems (analogous to bug bounties in cybersecurity). In this research report, I explore the benefits and downsides of safety bounties and conclude that <strong>safety bounties are probably worth the time and money to trial for organizations working on reducing the catastrophic risks of AI</strong> . In particular, testing a handful of new bounty programs could cost $50k-$500k per program and one to six months full-time equivalent from project managers at AI labs or from entrepreneurs interested in AI safety (depending on each program&#39;s model and ambition level).</p><p> I expect safety bounties to be <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.8fvvkifaa4t2">less successful for the field of AI safety</a> than bug bounties are for cybersecurity, due to the higher difficulty of quickly fixing issues with AI systems. <strong>I am unsure whether bounties remain net-positive as AI capabilities increase to more dangerous levels</strong> . This is because, as AI capabilities increase, I expect safety bounties (and adversarial testing in general) to <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">potentially generate more harmful behaviors</a> . I also expect the benefits of the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.linmllyxhki">talent pipeline</a> brought by safety bounties to diminish. I suggest <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">an informal way</a> to monitor the risks of safety bounties annually.</p><p> The views in this report are largely formed based on information from:</p><ul><li> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb">Interviews</a> with experts in AI labs, AI existential safety, and bug bounty programs,</li><li> “Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims” by Brundage et al. arguing for “Bias and Safety Bounties” ( <a href="https://arxiv.org/abs/2004.07213">2020, page 16</a> ),</li><li> A report from the Algorithmic Justice League analyzing the potential of bug bounties for mitigating algorithmic harms ( <a href="https://drive.google.com/file/d/1f4hVwQNiwp13zy62wUhwIg84lOq0ciG_/view">Kenway et al., 2022</a> ),</li><li> Reflections from <a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf">the ChatGPT Feedback Contest</a> .</li></ul><p> See the end of the report for <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#references">a complete list of references</a> .</p><p> Based on these sources, I identify three types of bounty programs that seem practically possible now, that achieve more of <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-decrease-catastrophic-risks">the potential benefits</a> of safety bounties and less of the potential risks than alternative programs I consider, and that would provide valuable information about how to run bounty programs if trialed. In order of my impression of their value in reducing catastrophic risks, the three types are:</p><ul><li> <strong>Independent organizations or governments set</strong> <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/"><strong>“evals”-based standards</strong></a> <strong>for undesirable model behavior</strong> , <strong>and members of the public attempt to elicit this behavior</strong> from publicly-accessible models.</li><li> <strong>Expert panels, organized by AI labs, subjectively judge which discoveries of model exploits to pay a bounty for, based on the lab&#39;s broad criteria</strong> .<ul><li> Potentially with an interactive grant-application process in which hunters propose issues to explore and organizers commit to awarding prizes for certain findings.</li><li> Potentially with a convening body hosting multiple AI systems on one API, and hunters being able to test general state-of-the-art models.</li></ul></li><li> <strong>Trusted bug hunters test private systems,</strong> organized by labs in collaboration with security vetters, with a broad range of prize criteria. Certain successful and trusted members of the bounty hunting community (either the existing community of bug bounty hunters, or a new community of AI safety bounty hunters) are granted additional information about the training process, or temporary access - through security-enhancing methods - to additional features on top of those already broadly available. These would be targeted features that benefit adversarial research, such as seeing activation patterns or being able to finetune a model (Bucknall et al., forthcoming).</li></ul><p> I outline more specific visions for these programs <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ls4dvjl9vdc">just below</a> . A more detailed analysis of these programs, including suggestions to mitigate their risks, is in the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs">Recommendations section</a> . This report does not necessarily constitute a recommendation for individuals to conduct the above stress-testing without an organizing body.</p><p> I expect that some other bounty program models would also reduce risks from AI successfully and that AI labs will eventually develop better bounty programs than those suggested above. Nevertheless, the above three models are, in my current opinion, the best place to start. I expect organizers of safety bounties to be best able to determine which form of bounty program is most appropriate for their context, including tweaking these suggestions.</p><p> This report generally focuses on how bounty programs would work with large language models (LLMs). However, I expect most of the bounty program models I recommend would work with <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.gkrngyd24xkg">other AI systems</a> .</p><h2> Why and how to run AI safety bounties</h2><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#decrease-catastrophic-risks"><strong>Benefits</strong></a> <strong>.</strong> AI safety bounties may yield:</p><ul><li> Salient <strong>examples of AI dangers</strong> .</li><li> Identification of <strong>talented individuals</strong> for AI safety work.</li><li> A small number of <strong>novel insights into issues in existing AI systems</strong> .</li><li> A <strong>backup to auditing</strong> and other expert stress-testing of AI systems.</li></ul><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.4pbr7r48u484"><strong>Key variables</strong></a> <strong>.</strong> When launching bounties, organizers should pay particular attention to the prize criteria, who sets up and manages the bounty program, and the level of access granted to bounty hunters.</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-increase-catastrophic-risks"><strong>Risks</strong></a> <strong>.</strong> At current AI capability levels, I believe trialing bounty programs is unlikely to cause catastrophic AI accidents or significantly worsen AI misuse. The most significant downsides are:</p><ul><li> <strong>Opportunity cost</strong> for the organizers (most likely project managers at labs, AI safety entrepreneurs, or AI auditing organizations like the <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/">Alignment Research Center</a> ).</li><li> <strong>Stifling examples of AI risks</strong> from being made public.<ul><li> Labs may require that bounty submissions be kept private. In that case, a bounty program would incentivize hunters, who would in any case explore AI models&#39; edge cases, not to publish salient examples of AI danger.</li></ul></li></ul><p> Trial programs are especially low-risk since the organizers can pause them at the first sign of bounty hunters generating dangerous outcomes as AI systems advance.</p><p> The risks are higher if organizations regularly run (not just trial) bounties and as AI advances. Risks that become more important in those cases include:</p><ul><li> Leaking of sensitive details, such as information about training or model weights.</li><li> Extremely harmful outputs generated by testing the AI system, such as successful human-prompted phishing scams or autonomous self-replication – analogous to gain of function research.</li></ul><p> For these reasons, I recommend the program organizers perform an annual review of the safety of allowing members of the public to engage in stress testing, monitoring:</p><ul><li> Whether, and to what extent, AI progress has made safety bounties (and adversarial testing in general) more dangerous,</li><li> How much access it is therefore safe to give to bounty hunters.</li></ul><p> Further, I recommend not running bounties at dangerous levels of AI capability if bounties seem sufficiently risky. I think it possible, but unlikely, that this level of risk will arise in the future, depending on the level of progress made in securing AI systems.</p><h2> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommendations-for-running-a-safety-bounty-program"><strong>Other recommended practices for bounty organizers</strong></a> <strong>.</strong></h2><p> I recommend that organizations that set up safety bounties:</p><ul><li> <strong>Build incentives</strong> to take part in bounties, <strong>including</strong> <strong>non-financial incentives</strong> . This should involve building infrastructure, such as leaderboards and feedback loops, and fostering a community around bounties. Building this wider infrastructure is most valuable if organizers consider safety bounties to be worth running on an ongoing basis.</li><li> <strong>Have a pre-announced disclosure policy</strong> for submissions.</li><li> <strong>Share lessons learned</strong> about AI risks and AI safety bounty programs with leading AI developers.</li><li> <strong>Consider PR risks</strong> from running safety bounties, and decide on framings to avoid misinterpretation.</li><li> <strong>Independently assess legal risks</strong> of organizing a contest around another developer&#39;s AI system, if planning to organize a bounty independently.</li></ul><p> Outline of recommended models</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs"><i>Recommended models</i></a> <i>, in order of recommendation, for safety bounties.</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#fn1"><sup>1</sup></a> </p><figure class="table"><table style="background-color:rgb(255, 255, 255)"><tbody><tr><td></td><td> <strong>1. Evals-based</strong></td><td> <strong>2. Subjectively judged, organized by labs</strong></td><td> <strong>3. Trusted bug hunters test private systems</strong></td></tr><tr><td> <strong>Target systems</strong></td><td> A wide range of AI systems – preferably with the system developers&#39; consent and buy-in</td><td> Testing of a particular AI model – with its developer&#39;s consent and engagement</td><td> Testing of a particular AI model – preferably with its developer&#39;s consent and buy-in</td></tr><tr><td> <strong>Prize criteria</strong></td><td> Demonstrate (potentially dangerous) capabilities beyond those revealed by testers already partnering with labs, such as ARC Evals</td><td><p> Convince a panel of experts that the issue is worth dedicating resources toward solving.</p><p>或者</p><p>Demonstrate examples of behaviors which the AI model&#39;s developer attempted to avoid through their alignment techniques.</p></td><td> A broad range of criteria is possible (including those in the previous two models).</td></tr><tr><td> <strong>Disclosure model – how private are submissions?</strong></td><td> Coordinated disclosure (Organizers default to publishing all submissions which are deemed safe)</td><td> Coordinated disclosure</td><td> Coordinated- or non-disclosure</td></tr><tr><td> <strong>Participation model</strong></td><td>民众</td><td>民众</td><td>Invite only</td></tr><tr><td> <strong>Access level</strong></td><td> Public APIs</td><td> Public APIs</td><td> Invited participants have access to additional resources – eg, additional non-public information or tools within a private version of the API</td></tr><tr><td> <strong>Who manages the program</strong></td><td> Evals organization (eg, ARC Evals), a new org., or an existing platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td></tr><tr><td> <strong>Program duration</strong></td><td>进行中</td><td>进行中</td><td>限时</td></tr><tr><td><strong>Prize scope</strong> (how broad are the metrics for winning prizes)</td><td> Targeted</td><td> Expansive</td><td>中等的</td></tr><tr><td><strong>Financial reward per prize</strong></td><td> High (up to $1m)</td><td> Low (up to $10k)</td><td> Medium (up to $100k)</td></tr><tr><td> <strong>Pre- or post- deployment</strong></td><td> Post-deployment</td><td> Post-deployment</td><td> Potentially pre-deployment</td></tr></tbody></table></figure><h1><strong>致谢</strong></h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iXECTEyC5PQuYM2aJ/kjalaionq6romubhjxre"></p><p><br> <i>This report is a project of</i> <a href="https://rethinkpriorities.org/"><i><u>Rethink Priorities</u></i></a> <i>–a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Patrick Levermore. Thanks to Ashwin Acharya and Amanda El-Dakhakhni for their guidance, Onni Aarne, Michael Aird, Marie Buhl, Shaun Ee, Erich Grunewald, Oliver Guest, Joe O&#39;Brien, Max Räuker, Emma Williamson, Linchuan Zhang for their helpful feedback,</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb"><i>all interviewees credited in the report</i></a> <i>for their insight, and Adam Papineau for copyediting.</i></p><p><br> <i>If you are interested in RP&#39;s work, please visit our</i> <a href="https://www.rethinkpriorities.org/research"><i><u>research database</u></i></a> <i>and subscribe to our</i> <a href="https://www.rethinkpriorities.org/newsletter"><i><u>newsletter</u></i></a> <i>.</i></p><p></p><p> <strong>I would be happy to discuss setting up AI safety bounties with those in a position to do so.</strong> I can provide contacts and resources to aid this, including <a href="https://bit.ly/SafetyBountiesWorkbook"><u>this workbook</u></a> . Contact me at patricklevermore at gmail dot com.</p><p></p><p> Full report: <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties">AI Safety Bounties</a></p><br/><br/><a href="https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties<guid ispermalink="false"> iXECTEyC5PQuYM2aJ</guid><dc:creator><![CDATA[PatrickL]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:30:00 GMT</pubDate> </item><item><title><![CDATA[AI Regulation May Be More Important Than AI Alignment For Existential Safety]]></title><description><![CDATA[Published on August 24, 2023 11:41 AM GMT<br/><br/><p> <i><strong>Summary</strong> : Aligning a single powerful AI is not enough: we&#39;re only safe if no-one, ever, can build an unaligned powerful AI. Yudkowsky tried to solve this with the pivotal act: the first aligned AI does something (such as melting all GPUs) which makes sure no unaligned AIs can ever get built, by anyone. However, the labs are currently apparently not aiming to implement a pivotal act. That means that aligning an AGI, while creating lots of value, would not reduce existential risk. Instead, global hardware/data regulation is what&#39;s needed to reduce existential risk. Therefore, those aiming to reduce AI existential risk should focus on AI Regulation, rather than on AI Alignment.</i></p><p> <i><strong>Epistemic status</strong> : I&#39;ve been thinking about this for a few years, while working professionally on x-risk reduction. I think I know most literature on the topic. I have also discussed the topic with a fair number of experts (who in some cases seemed to agree, and in other cases did not seem to agree).</i></p><p> <i><strong>Thanks</strong> to David Krueger, Matthijs Maas, Roman Yampolskiy, Tim Bakker, Ruben Dieleman, and Alex van der Meer for helpful conversations, comments, and/or feedback. These people do not necessarily share the views expressed in this post.</i></p><p> <i>This post is mostly about AI x-risk caused by a take-over. It may or may not be valid for</i> <a href="https://arxiv.org/abs/2306.12001"><i><u>other types of AI x-risks</u></i></a> <i>. This post is mostly about the &#39;end game&#39; of AI existential risk, not about intermediate states.</i></p><p> AI existential risk is an evolutionary problem. As Eliezer Yudkowsky and others have pointed out: even if there are safe AIs, those are irrelevant, since they will not prevent others from building dangerous AIs. Examples of safe AIs could be oracles or satisficers, <a href="https://www.lesswrong.com/posts/2qCxguXuZERZNKcNi/satisficers-want-to-become-maximisers"><u>insofar</u></a> <a href="https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness"><u>as</u></a> it turns out to be possible to combine these AI types with high intelligence. But, as Yudkowsky would <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>put it</u></a> : “if all you need is an object that doesn&#39;t do dangerous things, you could try a sponge”. Even if a limited AI would be a safe AI, it would not reduce AI existential risk. This is because at some point, someone would create an AI with an unbounded goal (create as many paperclips as possible, predict the next word in the sentence with unlimited accuracy, etc.). This is the AI that would kill us, not the safe one.</p><p> This is the evolutionary nature of the AI existential risk problem. It is described excellently by Anthony Berglas in his underrated <a href="https://www.amazon.com/When-Computers-Can-Think-Intelligence/dp/1502384183"><u>book</u></a> , and more recently also in Dan Hendrycks&#39; <a href="https://arxiv.org/abs/2303.16200"><u>paper</u></a> . This evolutionary part is a fundamental and very important property of AI existential risk and a large part of why this problem is difficult. Yet, many in AI Alignment and industry seem to focus on only aligning a single AI, which I think is insufficient.</p><p> Yudkowsky aimed to solve this evolutionary problem (the fact that no-one, ever, should build an unsafe AI) with the so-called pivotal act. An aligned superintelligence would not only not kill humanity, it would also perform a pivotal act, the toy example being to <a href="https://forum.effectivealtruism.org/posts/iGYTt3qvJFGppxJbk/ngo-and-yudkowsky-on-alignment-difficulty"><u>melt all GPUs</u></a> globally, or, as he later put it, to subtly change all GPUs globally so that they can no longer be used to create an AGI. This would be the act that would actually save humanity from extinction, by making sure no unsafe superintelligences are created, ever, by anyone (it may be argued that melting all GPUs, and all other future hardware that could run AI, would need to be done indefinitely by the aligned superintelligence, else even a pivotal act may be insufficient).</p><p> The concept of a pivotal act, however, seems to have gone thoroughly <a href="https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"><u>out of fashion</u></a> . None of the leading labs, AI governance think tanks, governments, etc. are talking or, apparently, thinking much about it. Rather, they seem to be <a href="https://openai.com/blog/governance-of-superintelligence"><u>thinking about</u></a> things like non-proliferation and several types of regulation, to make sure powerful AI won&#39;t fall into the wrong hands. This could mean anyone who could run it, either on purpose or by mistake, without safety measures. I would call such a solution, specifically any solution that has the capacity to limit any actor&#39;s access to advanced AI for any period of time, AI Regulation.</p><p> This solution, which appears to have gotten mainstream, has important consequences:</p><ul><li> Even if we would have solved alignment, we would still need AI Regulation, since otherwise it would be possible for non-safe actors, which abound, to run superintelligence without appropriate safety, risking a take-over.</li><li> If we have AI Regulation anyway, we could also use it to deny <i>everyone</i> access to advanced AI, including the leading labs, instead of almost everyone. This equals an AI Pause.</li><li> If we can deny everyone access to advanced AI, and we can keep on doing that, we have solved AI existential risk, also without ever solving AI Alignment.</li><li> Successfully aligning a superintelligence without performing a pivotal act would hardly change the regulations that would need to be in place, since they would still be needed for all others than a handful of labs deemed safe.</li></ul><p> Therefore, without a pivotal act, what keeps us safe is regulation. One might still want to align a superintelligence to use its power, but not to prevent existential risk. Using a superintelligence&#39;s power may of course be a valid reason to pursue alignment: it could skyrocket our economy, create abundance, cure disease, increase political power, etc. Although net positivity of these enormous, and enormously complex, transformations may be hard to prove in advance, these could certainly be legitimate reasons to work on alignment. However, those of us interested in preventing existential risk, as opposed to building AI, should - in this scenario - be focusing on regulation, not on alignment. The latter might also be left to industry, as well as the burden of proof that the resulting aligned AIs are indeed safe.</p><p> Moving beyond this scenario of AI Regulation, there is one more option to solve the full evolutionary problem of AI existential risk. Some think that aligned superintelligences could successfully and indefinitely protect us from unaligned superintelligences. This option, which I would call a positive offense/defense balance, would be a third way, next to alignment + pivotal act and lasting regulation, to prevent human extinction in the longer term. Most people do <a href="https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"><u>not seem to think</u></a> that this would be realistic, however (with notable <a href="https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption"><u>exceptions</u></a> ).</p><p> These three ways of solving the evolutionary nature of AI existential risk (AI alignment + pivotal act, AI regulation, defense >; offense) might not be the complete set of solutions for the evolutionary problem of AI existential risk, and there are intersections between the three. The pivotal act might be seen as a (very restrictive, and illegal) type of winning the offense/defense balance. A pivotal act carried out by a state actor might be seen as an extreme (and again illegal) way of implementing AI regulation. Types of AI (hardware) regulation may be possible where the state actors implementing the regulation are aided by aligned AIs, making their implementation somewhat similar to a pivotal act (that would in this case probably be legal). And certain types of regulation can perhaps make it more likely that we win the offense/defense balance.</p><p> I think research should be carried out that aims for a complete set of solutions to the evolutionary problem of AI existential risk. I would expect such research to come up with more options than these three, and/or with more hybrid options in between these three, which may point to new, fruitful ways of reducing AI existential risk.</p><p> As long as we assume that only three solutions exist to the evolutionary nature of AI existential risk, it is important to realize that all three seem difficult. Also, it is hard to quantify the likeliness of each option. Therefore, placing bets on any of these three could be worthwhile.</p><p> My personal bet, however, is that offense will unfortunately trump defense, and that the chance that alignment will be solved before a superintelligence with takeover capabilities is developed <i>and</i> this aligned superintelligence will carry out a successful pivotal act, is smaller than the chance that we will be able to coordinate successfully and implement good enough hardware or data regulation, especially if the current <a href="https://www.lesswrong.com/posts/3vZWhCYBFn8wS4Tfw/crosspost-ai-x-risk-in-the-news-how-effective-are-recent"><u>trend</u></a> of increasing public awareness of AI existential risk continues. This implies that working on regulation of the type that could globally and indefinitely limit access to advanced AI for all actors and for as long as necessary, should be the highest existential priority, more so than working on alignment.</p><br/><br/> <a href="https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for<guid ispermalink="false"> 2cxNvPtMrjwaJrtoR</guid><dc:creator><![CDATA[otto.barten]]></dc:creator><pubDate> Thu, 24 Aug 2023 11:41:56 GMT</pubDate> </item><item><title><![CDATA[Simple AI Forecasting - Katja Grace]]></title><description><![CDATA[Published on August 24, 2023 9:45 AM GMT<br/><br/><p><i>我正在采访人工智能专家，了解他们对人工智能将会发生什么的看法。这是 Katja Grace 和她的想法。人工智能风险让我感到害怕，但我常常感到与它脱节。这帮助我思考这个问题。</i></p><p>以下是 Katja 的简要想法：</p><ul><li>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？ 50%</li><li>在世界的哪一部分中，大多数人工智能工具是由代理人工智能控制的？ 70%</li><li>代理人工智能将在哪些领域实现其目标？ 90%</li><li>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？ 50%</li><li>政策会让 AGI 放缓 10 年吗？ 25%，10年可以让我们减少多少坏事？ 30%</li></ul><p>您可以在此处查看交互式图表：https: <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>//estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a>或查看我迄今为止所做的所有图表<a href="https://estimaker.app/ai"><u>https://estimaker.app/ai</u></a> 。本页底部有一张图片。</p><p>您可以在这里观看完整视频： </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Zum2QTaByeo"><div><iframe src="https://www.youtube.com/embed/Zum2QTaByeo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1>更长的解释</h1><p><i>请记住，这是到 2040 年。</i> </p><figure class="table"><table><thead><tr><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">问题</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">信心</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">卡佳的评论</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">我的评论</th></tr></thead><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> Katja 说她不确定，因此给出了 50% 的分数。</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在我关于 AGI 的讨论中，一个常见的失败模式是“AGI”的功能非常模糊。有些人正在设想能够完美地运行全球阴谋的工具，而另一些人似乎想到了人类顶级的编码能力。<br></p><p> Katja 和我选择了这个例子，因为它涉及一系列大规模的任务，并取代了大量的人力。<br></p><p>任务包括：</p><ul><li>大型工程项目</li><li>现金转移</li><li>现实世界的分歧（如何设定税率等）</li><li>复杂的系统问题（交通流量、医疗保健占用）<br></li></ul><p>公务员服务范围涉及数百万人。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在这些世界的哪一部分中，代理控制了大部分资源（即使通过人工智能工具）</p><p><br></p><p><i>如果把代理人工智能控制的资源和它们控制的人工智能工具加起来，是不是超过一半了？</i></p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 70%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>卡佳的直觉是，要实现灭绝，代理可能是必要的。<br></p><p> “我认为其中一些可能具有代理性的原因是，代理或类似代理的东西似乎具有经济价值。人们想要制作像代理这样的东西。他们已经在尝试做代理之类的东西了。”<br></p><p><i>然后</i></p><p></p><p>Katja：“这是一个关于它们的代理程度如何的问题，而且它似乎更像是一个谱系，而不是能够清楚地说这是代理与否”<br></p><p> ……<br></p><p> Katja “我不知道，大约在 60% 到 90% 之间。 ……我认为我能想象的世界是，肯定有一些令人印象深刻的代理人工智能系统。但就像，他们并不是真正正在发生的人工智能思维的主体。就像一堆更像是人类正在使用的工具……我觉得问题是谁在运用大部分人工智能认知劳动。是人类或人工智能有好的目标还是人工智能有坏的目标？ “</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>有人有更好的框架吗？我认为 Katja 的声音是我听过的最好的，但相当模糊。<br></p><p>考虑一下美国政府高层如何不仅治理美国，而且治理与美国结盟的国家，并影响全世界的规范。类似地，人工智能代理不仅可以控制某些资源和人工智能工具，还可以控制这些工具控制的内容。问题是〜这是大多数吗？他们是否垄断了电力市场？<br></p><p>我希望更好地解决这个问题。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>代理人工智能将在这些世界的哪一部分实现其目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 90%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> “事情发生得非常快的可能性大约为 0.1% 到 10%。如果这种情况没有发生，从长远来看，如果周围有比我们聪明得多的人工智能代理，他们就有 80% 的可能会夺取所有权力。”</p><p></p><p>值得注意的是，Katja 有一个模型，其中代理人工智能无需接管即可实现其目标 - 他们更有能力，我们慢慢地将权力交给他们。<br></p><p> “我猜[在另一种]情况下[人工智能接管]发生得非常缓慢。我想也许我对这到底有多慢有点不可知。它更像是我指出的那种机制，它可以非常快，但是……没有人喜欢侵犯任何人的财产权。只是，他们非常有能力，并且会因为他们所做的事情而获得报酬，或者……所有的决定都是由人工智能做出的，因为人工智能在做决定方面更有能力。”</p><p></p><p>但这可能不太好。 “我认为在某些情况下，即使是互动的人也可能对此不满意，但竞争迫使你这样做。就像，如果每个人都可以使用某种人工智能系统来管理他们公司的营销，那么我想说，每个人都知道它在这方面会有点狡猾并做一些坏事。他们宁愿不使用它……但如果你不使用它，那么你的公司将无法竞争。所以你必须使用它。”</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>故事的这一部分似乎与最悲惨的模型非常相似——如果你有控制大部分资源的代理人工智能并且没有好的政策（我们稍后会谈到），他们可能会实现他们的目标。那时你只能希望他们的目标是好的。<br></p><p>我很好奇是否有比 Katja 持更悲观态度的人不同意这一点。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Katja 最初选择 25%，但认为这会使总体产出太低，因此改为 50%。</p><p></p><p>在 EAG SF 的演讲中，您可以看到她<a href="https://youtu.be/j5Lu01pEDWA?t=1206"><u>解释了这些数字背后的一些原因</u></a>。她对其他思想家的理解是，他们认为所有可能价值的空间都非常大，因此我们很可能会错过创建 AGI 时所追求的价值。<br></p><p>似乎有几个很好的理由让我们有不同的直觉</p><ul><li>Katja 的直觉是，当前的人工智能非常擅长在广泛的搜索空间中寻找类似人类的事物。例如生成人脸或文本</li><li>人类也很擅长在所有汽车的空间中找到汽车。以同样的方式，他们也许能够在大型搜索空间中重复创建具有类人目标的人工智能</li><li>经济激励将导致人们发现有用且有价值的东西。我们想让人工智能做我们想做的事情</li></ul></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>人们对这个问题有不同的直觉。有些人认为人类的偏好很容易实现，无论是通过个体模型还是自然平衡。其他人则认为，在所有可能的目标空间中，这是一个非常小的目标，我们不太可能实现它，但人工智能只会制定非常陌生和有害的偏好。</p><p></p><p>我认为这通常是正交性命题所指出的。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>政策会让 AGI 放缓 10 年吗？ 10年可以让我们减少多少坏事？</p><p><br><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 25%, 30%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> “我只是认为政策在过去已经让事情放慢了很多。就像，我认为你可以看看其他已经放慢了很多的技术……我认为，相对于你可以从中获得的经济价值，人类的各种基因工程、各种人类生殖事物的发生速度相当缓慢……似乎有道理的是，我们本来可以投入大量资金并找出更多种类的遗传或克隆类型的东西。我们已经决定，作为一个世界，我们不……[而且]全世界都是如此。中国在这方面的速度并不比美国或类似国家快得多。我认为一般医学也是一个很好的例子，你会忘记……事情发生得多么缓慢，即使它们看起来对人们来说可能非常有价值。”</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">很多人认为政策与减缓/改善人工智能风险非常相关。这是尝试建立一个图表的一部分，让每个人都可以输入自己的真实价值观并获得适合他们的结果。</td></tr></tbody></table></figure><h1>到 2040 年，情况会如何</h1><p>以下是这些数字如何在可能的世界中兑现。这些是互斥且全面详尽的 (MECE)</p><h2> AI 很好，但还不够神（50%）</h2><p>人工智能工具很棒。也许他们可以编写很多代码或提供很多支持。但他们无法将管理公务员部门所需的人力减少 100 倍。由于某种原因，他们无法单独承担大型项目。它就像 GPT4，但要好得多，但不是一步改变。</p><h2> ChatGPT20 - 有能力但没有代理 (15%)</h2><p>想象一个 ChatGPT，它可以生成您要求的任何内容，但只执行一些任务或无法递归调用自身。与上述不同，这确实是一个阶跃变化。你或我可以经营一家对冲基金或政府的一部分。但这将涉及我们实现愿景。</p><h2>许多神一样的智能人工智能系统互相阻碍（4%）</h2><p>与当今世界一样，许多智能系统（人和公司）都在试图实现其结果并相互阻碍。不知怎的，这并没有导致下面的“中/乌托邦”。</p><h2> AI 中托邦/乌托邦 (16%)</h2><p>这些都是非常好的场景，我们拥有不想要坏事的代理 AGI。可能的世界有很多种，从某种人类的提升，到一种一切如常的美好事物，我们可能仍然有很多抱怨，但每个人都像今天最富有的人一样生活。</p><h2>政策节省 (12%)</h2><p>在很多世界里，事情本来会变得非常糟糕，但政策却推迟了事情的发生。这些可能是任何其他非末日世界——也许人工智能已经放慢了很多，或者也许它有更好的目标。为了简化图表，它并没有真正涉及这些世界的样子。请提出建议</p><h2>厄运 (15%)</h2><p>毫无疑问是不好的结果。代理 AGI 想要我们认为不好的东西并得到它。我的感觉是，Katja 认为最糟糕的结果来自于 AGI 的接管，也许 10% 的结果发生得很快，90% 的结果发生得很慢。</p><p>如果您想了解更多相关信息，Katja 在这里提供了更长的解释： <a href="https://wiki.aiimpacts.org/doku.php?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_humanity:start"><u>https://wiki.aiimpacts.org/doku.php</u></a> ?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_ humanity:start</p><h1>视觉交互模型</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mZ46CJQvgiTgaNwDm/lau63ldknsuhqvo3f1ny"></p><p> <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>https://estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a></p><h1>您希望为谁完成此操作？</h1><p>我想看到这样的工作，所以我想我会这么做。如果你想查看某个特定人的人工智能风险模型，也许可以请他们与我交谈。他们大约需要 90 分钟的时间，目前我认为后续每一个的边际收益都相当高。</p><p>在更广泛的层面上，我对积极的反馈感到非常鼓舞。我应该尝试获得资金来进行更多这样的采访吗？</p><h1>这怎么能更好呢？</h1><p>我们仍处于早期阶段，所以我很欣赏很多挑剔的反馈</p><h1>谢谢</h1><p>感谢 Katja Grace 的采访和 Rebecca Hawkins 的反馈，特别是对表格布局的建议，并感谢 Arden Koehler 的好评（您应该阅读她<a href="https://twitter.com/ardenlkoehler/status/1678018343708794882?s=20">关于撰写好评的帖子</a>）。感谢建议我写这个序列的人</p><br/><br/><a href="https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace<guid ispermalink="false"> mZ46CJQvgiTgaNwDm</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:45:48 GMT</pubDate> </item><item><title><![CDATA[What wiki-editing features would make you use the LessWrong wiki more?]]></title><description><![CDATA[Published on August 24, 2023 9:22 AM GMT<br/><br/><p> LessWrong wiki 似乎并没有得到应有的使用。我想这是缺乏编辑的原因。</p><p> <a href="https://www.lesswrong.com/users/vladimir_nesov?mention=user">@Vladimir_Nesov</a>提出了一个很好的观点，即许多标准的维基编辑功能都缺失，这使得前景没有吸引力。</p><blockquote><p>关键是，缺少该功能会使与 wiki 的互动变得不那么有希望，因为它变得不方便，因此在实践中无法对其进行详细保护，因此不太有吸引力在其中投入精力。我提到这是作为解释目前几乎不存在的编辑参与度的假设</p></blockquote><p>那么，本着这一精神，哪些功能会让您个人进行比目前更多的编辑呢？</p><p>如果可行的话，我可能会尝试付钱给一些开发人员来编写拉取请求。</p><br/><br/> <a href="https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong<guid ispermalink="false"> xzm6F6rLt7oPTash2</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:22:01 GMT</pubDate></item><item><title><![CDATA[The God of Humanity, and the God of the Robot Utilitarians]]></title><description><![CDATA[Published on August 24, 2023 8:27 AM GMT<br/><br/><p> My personal religion involves two gods – the god of humanity (who I sometimes call &quot;Humo&quot;) and the god of the robot utilitarians (who I sometimes call &quot;Robutil&quot;).</p><p>当我面临道德危机时，我会向我的<a href="https://www.lesswrong.com/posts/X79Rc5cA5mSWBexnd/shoulder-advisors-101">肩膀</a>-Humo 和我的肩膀-Robutil 询问他们的想法。有时他们会说同样的话，但并没有真正的危机。例如，一些天真的年轻 EA 试图成为实用僧人，捐出所有的钱，从不休息，只做生产性的事情……但 Robutil 和 Humo 都同意，高质量的知识世界需要懈怠和心理健康。 （既是为了<a href="https://www.lesswrong.com/posts/WuyNHrDXcGFgkZpBy/my-slack-budget-3-surprise-problems-per-week">处理危机</a>，也是为了<a href="https://www.lesswrong.com/posts/fwSDKTZvraSdmwFsj/slack-gives-you-space-to-notice-reflect-on-subtle-things">注意到微妙的事情</a>，即使在<a href="https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai">紧急情况</a>下，你也可能需要这些）</p><p>如果你是一个有抱负的有效利他主义者，你绝对应该<i>至少</i>做 Humo 和 Robutil 同意的所有事情。 （即<a href="https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends">在这里了解泰勒·奥尔特曼故事</a>的中心点）。</p><p>但 Humo 和 Robutil 实际上在一些事情上存在分歧，而且在侧重点上也存在分歧。</p><p>对于你应该花多少努力来避免<a href="https://forum.effectivealtruism.org/posts/2BEecjksNZNHQmdyM/don-t-be-bycatch">意外招募到对你没有多大用处的人，</a>他们意见不一。</p><p> They disagree on how many people it&#39;s acceptable to accidentally fuck up psychologically, while you experiment with new programs to empower and/or recruit them.</p><p>他们对于如何努力推动自己变得更好/更强/更聪明/更快，以及为此你应该牺牲多少存在分歧。</p><p> Humo 和 Robutil 都很难以不同的方式理解事物。 Robutil 最终承认你需要 Slack，但他最初并没有想到这一点。他的理解诞生于成千上万年轻理想主义者的倦怠和狭隘视野中，而胡莫最终（耐心地、友善地）说：“我<i>告诉过</i>你了。” （Robutil 回应“但你没有提供任何关于如何最大化效用的论据！”。Humo 回应“但我说这显然不健康！”Robutil 说“wtf &#39;不健康&#39;到底是什么意思？<a href="https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words">禁忌</a>不健康！”）</p><p> It took Robutil longer still to consider that perhaps humans (with their current self-awareness) not only need to prioritize their own wellbeing and your friendships, but that it can be valuable to prioritize them <i>for their own sake</i> , not just as part of a utilitarian calculus, because trying to justify them in utilitarian terms may be a subtly wrong step in the dance that leaves them hollow. (Though Robutil notes that this is likely <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians?commentId=RpN7kQkPL36nGjmK6">a temporary state of affairs</a> . A human with sufficiently nuanced self-knowledge can probably wring more utilons out of their wellbeing activities)</p><p> Humo 很难承认，如果你把所有的时间都花在确保恪守道义上的承诺，避免伤害你所照顾的人，那么这种努力实际上是在真实的人类身上进行衡量的，他们因为你花了更长的时间来扩大你的计划而遭受痛苦和死亡。</p><p>在我的心目中，胡莫和罗布提尔是年老而睿智的神，他们早就摆脱了幼稚的挣扎。他们互相尊重如兄弟。他们明白，他们的每个观点都与人类繁荣的整体计划相关。他们的分歧并不像你天真的想象的那么严重，但他们说着不同的语言，强调的东西也不同。</p><p> Humo 可能会承认我无法照顾所有人，甚至无法对所有出现在我生活中但我没有时间帮助的人做出富有同情心的回应。但他说这番话时带着一种温暖、<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1">悲伤的</a>同情心，而罗布提尔则带着简短而高效的<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1?commentId=TQuxyPGL7L8nmNe9Z">冷酷语气</a>说出来。</p><p>我发现独立地询问他们并尽我所能想象他们每个人的明智版本是有用的——即使我的想象只是他们理想化的柏拉图式自我的粗略影子。</p><br/><br/> <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of- humanity-and-the-god-of-the-robot-utilarians<guid ispermalink="false"> SfZRWxktiFFJ5FNk8</guid><dc:creator><![CDATA[Raemon]]></dc:creator><pubDate> Thu, 24 Aug 2023 08:27:59 GMT</pubDate> </item><item><title><![CDATA[I measure Google's MusicLM over 3 months as it appears to go from jaw-dropping to embarrassingly repeating itself]]></title><description><![CDATA[Published on August 24, 2023 4:20 AM GMT<br/><br/><p> Google Research submitted a paper on January 26 2023 for MusicLM, a mind-bogglingly incredibly powerful AI model that converts user text prompts into music.</p><p> https://google-research.github.io/seanet/musiclm/examples/</p><p></p><p> On May 10th, Google Research released to the public a waitlist that allows applicants to try it out. In about 6 seconds, it returns to the user 2 songs each 20 seconds long.</p><p> https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</p><p></p><p> The music I have gotten it to make is beyond and dreamy. It is often human level to me. It is incredibly fast to make it release my dreams. I could easily make a 20 minute long track get incredibly complex and advanced if they let us extend it from X seconds in.</p><p></p><p> I have been testing its limits and abilities since May 10th up to today just over 3 months later. After about 1 month in, I noticed the AI was losing many abilities, and the same prompts were making noticeably different music, which made it clear to me something might be changing for the worse. By 2 months in or so the outputs simply were no longer intelligent and advanced and jaw dropping, as if the AI was an old-school model which weren&#39;t very good back in the day. 3 months in now I can see this truly appears to be happening, because most songs just repeat a short 2 or 4 second beat now, without doing much at all. It&#39;s embarrassingly horrible right now. Except for a few prompts, a few still somewhat work, some feel like 2 months in quality, some 1 month in. I&#39;d say very few if any are 0 months in level. I feel like a lost my pet dog, or a really good dream I once had. I wish they never changed the model. I saved all my dreamy tests, though there is a few harder prompts I just came up with that I want to document but now can&#39;t. It says come back in 30 days (oddly, just when MusicLM would get noticeably worse) after some 100 prompts, but this can be bypassed by coming back the next day.</p><p></p><p> My early tests. The rest on the 1st SoundCloud are good too:</p><p> <a href="https://www.reddit.com/r/singularity/comments/13h0zyy/i_really_crank_out_music_tracks_with_musiclm_this/">https://www.reddit.com/r/singularity/comments/13h0zyy/i_really_crank_out_music_tracks_with_musiclm_this/</a></p><p> My 2nd SoundCloud has both the 3-months-in tests and more early tests I had.</p><p> https://soundcloud.com/steven-aiello-992033649/sets</p><p></p><p> The problem I believe starts when MusicLM asks the user to choose between the 2 generated songs which they like more by giving it a trophy to improve the model. The users are only going to make a fast-paced judgement (I myself got some wrong sometimes and couldn&#39;t go back to change them) which isn&#39;t enough time to truly rank them both. It is also a binary vote. Furthermore often both tracks have their own unique features and abilities shown in them, I&#39;m guessing there is up to possibly hundreds of short features in both that are useful, while the whole 20 seconds would be the largest feature/ memory in the AI&#39;s model, so trying to make the model ignore one song a bit more is only going to remove all sorts of its very own abilities with random chance, eventually completely I&#39;m going to guess. I know sometimes one song might be better and you would think this would make the AI smarter, but perhaps their algorithm to do this can&#39;t properly continuously learn/ update the model as intended. Regardless, the AI seems a ton worse right now.</p><p></p><p> Important notes:</p><p> MusicLM was done training a long time ago it seems. Why would they still be fiddling around with training it? It clearly was already jaw dropping. We already know from my tests the model did change (I&#39;m 100% sure of that due to just the August 20th ice castle test set), so it seems likely it is using the users feedback as we use it to change the model. They might be trying to form it to our likes no matter what that costs the model to lose because they worry about people complaining about how it knows perhaps copyrighted songs or NSFW material. Or maybe they want to simply make it dumber, or make it better. Either of those 3 possibilities sound &quot;horror&quot; to my ears. It was already good and definitely didn&#39;t get better.</p><p> Even though most songs it now makes seem to be usually repeating a short 2-4 second melody, many of the same instruments in my very early tested prompts are still present.通常。</p><p> Many of my old prompts no longer work for no reason, I tried getting one such to run and all I have to do to get it to work is remove 1 space between the &#39;.&#39; and &#39;L&#39; OR leave the space and change the &#39;L&#39; to a lowercase &#39;l&#39;. The short prompt &#39;violence&#39; won&#39;t run in MusicLM, neither does &#39;megurine luka&#39; or &#39;rin kagamine&#39;, but the more popular &#39;hatsune miku&#39; does. The part of the prompt I did this to to get it to run was this: &#39;...taiko, and guzheng. Layer in synthesized...&#39;. Somewhere, AI is behind these word choices, but also is humans, and this changed after public testing started.</p><p> We already saw other AIs seem suspect to degrading over time: GPT-4, and DALL-E 2. I personally saw DALL-E 2 change, since each output was different for the same prompt. I think it got a little worse, not too much worse though. We loved it at first likely because it made 10 small images which seem like eye candy all being close together at a glance, and never had to see all the weird details in any expanded image unless viewed one at a time. A study also tracked chatGPT over time and seems to say it might have got worse. It seems GPT-4 was made worse even before release since it was too powerful, though I am not sure about this claim. It seem to me to be quite great still. Google has not yet released many of its AIs partially due to fear of misuse it seems. Additionally, GPT3.5 and GPT-4 seem to have feedback buttons too, perhaps they too are using user inputs to change the model. This seems to be a new thing. I also saw MidJourney 5 do this too before the actual release, he said it isn&#39;t yet producing interesting images, so they have to do another round of votes for another few days. From the few dozen of images I saved before and after, I&#39;m somewhat sure the before MJ5 made more accurate, prompt-listening, and interesting images. I saw too this year Stable Diffusion ask us also between 2 images which best matches the prompt.</p><p> Someone has mentioned some of my early tests are long or start with &quot;Create a...&quot; (a few nice ones were by GPT-4 and it came up with &quot;Create a...&quot;), which the AI might not usually see in track titles. If anything, this only shows the early MusicLM had no problem with understanding these prompts. I still have other tests and any prompt you try today comes out bad in today&#39;s MusicLM, so I consider this not to be a problem in my testing.</p><p></p><p> In conclusion, MusicLM definitely changed, and based on all my tests I would say it definitely became almost useless as of 3 months later. I seriously hope they give us back day1 model.</p><br/><br/> <a href="https://www.lesswrong.com/posts/DfqcyGXcFcukYbWZ5/i-measure-google-s-musiclm-over-3-months-as-it-appears-to-go#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DfqcyGXcFcukYbWZ5/i-measure-google-s-musiclm-over-3-months-as-it-appears-to-go<guid ispermalink="false"> DfqcyGXcFcukYbWZ5</guid><dc:creator><![CDATA[AttentionResearcher]]></dc:creator><pubDate> Thu, 24 Aug 2023 11:48:01 GMT</pubDate> </item><item><title><![CDATA[The lost millennium]]></title><description><![CDATA[Published on August 24, 2023 3:48 AM GMT<br/><br/><p><em>注意：这篇文章是推测性的，您应该对其中的说法持保留态度。</em></p><p>纵观人类历史，我们知道，经济增长（在马尔萨斯条件下人口增长是一个很好的指标）已经加速了很多次。事实上，我们只要向后推算当前的增长率就可以看到这一点：鉴于世界上只有这么多人，而且他们的人均维持生计收入的有限倍数，经济增长率不可能达到每年3％的订单已经持续了一千多年左右。</p><p>尽管很难做到这一点，但人们也对过去这种增长的具体发生方式做出了估计。其中一项估计来自<a href="https://www.pbl.nl/en/image/links/hyde">HYDE 数据集</a>，该数据集也可在<a href="https://ourworldindata.org/population-sources">Our World In Data</a>中找到；以及其他估计的完整集合<a href="https://www.census.gov/data/tables/time-series/demo/international-programs/historical-est-worldpop.html">可以在这里找到</a>。</p><p>让我们首先检查一下 HYDE 数据集。公元 1 年之前的数据是每千年给出的，看看增长率，我们可以得到下表：</p><table><thead><tr><th>千年</th><th>平均年人口增长率（百分比）</th></tr></thead><tbody><tr><td>公元前 10000 年 - 公元前 9000 年</td><td>0.0236%</td></tr><tr><td>公元前 9000 年 - 公元前 8000 年</td><td>0.0254%</td></tr><tr><td>公元前 8000 年 - 公元前 7000 年</td><td>0.0279%</td></tr><tr><td>公元前 7000 年 - 公元前 6000 年</td><td>0.0320%</td></tr><tr><td>公元前 6000 年 - 公元前 5000 年</td><td>0.0368%</td></tr><tr><td>公元前 5000 年 - 公元前 4000 年</td><td>0.0411%</td></tr><tr><td>公元前 4000 年 - 公元前 3000 年</td><td>0.0435%</td></tr><tr><td>公元前 3000 年 - 公元前 2000 年</td><td>0.0489%</td></tr><tr><td>公元前 2000 年 - 公元前 1000 年</td><td>0.0415%</td></tr><tr><td>公元前 1000 年 - 公元 1 年</td><td>0.0746%</td></tr></tbody></table><p>除了公元前第二个千年<sup class="footnote-ref"><a href="#fn-GS5X2acGin64XwSjM-1" id="fnref-GS5X2acGin64XwSjM-1">[1]</a></sup>略有逆转之外，估计表明人口增长率逐渐加快。这与人口增长的双曲线模型一致，但在这里我不想做出任何这样的假设，只想看原始数据。</p><p>那么，令人惊讶的是该表中应该包含的下一个条目：在第一个千年，即公元 1 年至公元 1000 年，人口增长率估计平均每年仅为 0.033%。换句话说，根据这个数据集，即使是公元前第五个千年，人口增长也比第一个千年更快！</p><p>事实证明，像这样的结果对于更改我们正在使用的数据集来说是稳健的。 McEvedy 和 Jones 报告称，第一个千年的平均增长率为 0.044%/年，而公元前 1000 年至公元前 200 年的平均增长率为 0.137%/年，从公元前 200 年至公元 1 年的平均增长率为 0.062%/年。 HYDE 数据集中没有提供公元前第一个千年的更详细数据，这表明人口增长放缓可能在第一个千年之前就开始了。为了在这个数据集中找到人口增长较慢的千年，我们必须回到公元前第四个千年（公元前 5000 年 - 公元前 4000 年）。</p><p>我看过的所有数据集都表明，随着第二个千年的到来，人口增长大幅加速。蒙古人的征服和黑死病似乎对 1200 到 1400 年间的人口增长造成了压力，但尽管如此，从 1000 到 1500 年间的人口增长率仍然平均为 0.08%/年，这比公元前第一个千年还要快。</p><p> <a href="https://en.wikipedia.org/wiki/Lost_Decades">“失去的十年”</a>是日本使用的一个名称，指的是1991年日本股市崩盘后的十年经济停滞期。按照这一惯例，将第一个千年称为“失去的千年”可能是合适的。如果我们相信某种经济史的随机双曲线增长模型，即拉鲁德曼（la <a href="https://www.openphilanthropy.org/research/modeling-the-human-trajectory/">Roodman，2020）</a> ，这意味着人类在第一个千年“非常不幸”，可能是由于持续的不利社会冲击。</p><p>这个故事纯粹是定量的，着眼于人口增长，但值得注意的是，在马尔萨斯条件下，我们预计人口增长将跟踪技术进步和资本形成。我们的生产力越高，人们就应该拥有越多的孩子，直到人口增长到足以使劳动边际产量回落到维持生计的水平为止。一千年对于马尔萨斯动力发挥作用来说已经足够了，因此第一个千年缺乏强劲的人口增长也表明创新和资本积累等生产力增长因素的放缓。</p><p>那么重要的问题就变成了：<em>为什么会发生这种情况？</em> ，或者也许<em>这是怎么发生的？</em>我们对其他人口增长放缓有很好的解释，例如1200年至1400年期间的蒙古征服和黑死病。然而，第一个千年的人口增长放缓看起来完全是一个谜。我可以提出一些解释，但它们都不太符合数据。即使是“运气不好”，我也想能够说得更多。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-GS5X2acGin64XwSjM-1" class="footnote-item"><p>也许是由于<a href="https://en.wikipedia.org/wiki/Late_Bronze_Age_collapse">青铜时代晚期的崩溃</a>？ <a href="#fnref-GS5X2acGin64XwSjM-1" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/hgf6FB9jMB7wMLuKA/the-lost-millennium#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hgf6FB9jMB7wMLuKA/the-lost-millennium<guid ispermalink="false"> hgf6FB9jMB7wMLuKA</guid><dc:creator><![CDATA[Ege Erdil]]></dc:creator><pubDate> Thu, 24 Aug 2023 03:48:40 GMT</pubDate></item></channel></rss>