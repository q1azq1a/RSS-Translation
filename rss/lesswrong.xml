<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 10:12:33 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Sensor Exposure can Compromise the Human Brain in the 2020s]]></title><description><![CDATA[Published on October 26, 2023 3:31 AM GMT<br/><br/><p><strong>概述</strong></p><p>20 世纪因心理学（一门研究人类心智的科学）的发现及其利用（例如大规模战争、宣传、广告、信息/混合战争、决策理论/相互确保毁灭）而发生了根本性的改变。</p><p>然而，我们有理由认为，如果人类思维的科学和开发比现在更加先进，那么 20 世纪将会发生进一步的转变。</p><p>我在这里认为，在一个大规模监控、美国、中国和俄罗斯之间的<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合</u></a>/认知战争以及机器学习取得重大进步的时代，我们有理由认为，SOTA 人类认知分析和利用的情况可能已经是威胁整个人工智能安全社区运行的连续性；如果不是现在，那么很可能会在 2020 年代的某个时候发生，届时全球范围内的事件可能会比人类在过去二十年所习惯的步伐要多得多。</p><p>人工智能将成为这些王国以及它们之间战争的关键，要求暂停开发可能是人类生存的最低要求，而对于这样的冲突， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point">我们甚至不知道是什么袭击了我们</a>。</p><p>对于一般人类生活来说，攻击面大得令人无法接受，更不用说对于人工智能安全社区来说了，这个社区是一个由一群书呆子组成的社区，他们偶然发现了宇宙这一面的命运所围绕的工程问题，一个绝对不能失败的社区。在 2020 年代幸存下来，也不会以缩小/捕获的形式跛行。</p><p><br></p><p><strong>这个问题是智能文明的基础</strong></p><p>如果有智慧的外星人，由一束束触手、水晶或植物组成，思考速度极其缓慢，他们的思想也会有可发现的功绩/零日，因为任何自然进化的思想都可能像人脑一样，是一堆意大利面条代码在其预期环境之外运行。</p><p>他们甚至可能不会开始触及发现和标记这些漏洞的表面，直到像今天的人类文明一样，他们开始用传感器包围数千或数百万同类，这些传感器可以每天几个小时记录行为并找到<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=of%20procuring%20results.-,There%20is%20no,-logical%20endpoint%20to">相关性网络</a>。</p><p>就人类而言，使用社交媒体作为自动化人工智能实验的受控环境似乎创造了人类行为数据的临界量。</p><p>社交媒体引导人类成果的能力并不是孤立地进步的，它们与人类思维的理解和利用的广泛加速并行，这<a href="https://arxiv.org/pdf/2309.15084.pdf"><u>本身就是加速人工智能能力研究的副产品</u></a>。</p><p>通过将人与其他人进行比较并预测特征和未来行为，多臂老虎机算法可以首先预测特定的操纵策略是否值得冒险实施；导致高成功率和低检测率（因为检测可能会产生高度可测量的响应，特别是在大量传感器暴露的情况下，例如未覆盖的网络摄像头，因为将人们的微表情与失败或暴露的操纵策略的案例或工作网络摄像头视频进行比较数据转化为基础模型）。</p><p>当您拥有数十亿小时的人类行为数据和传感器数据的样本量时，不同类型的人反应的毫秒差异（例如面部微表情、滚动过去涵盖不同概念的帖子时的毫秒差异、涵盖不同概念后的心率变化、眼球追踪）眼睛经过特定概念、触摸屏数据等后的差异从难以察觉的噪音转变为<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=There%20is%20no%20logical%20endpoint%20to%20the%20amount%20of%20data%20required%20by%20such%20systems...%20All%20information%20is%20potentially%20relevant%20because%20it"><u>相关网络</u></a>的基础。<a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>美国国家安全局在每个操作系统和可能的芯片固件中都储存了漏洞</u></a>，因此我们无法很好地估计收集了多少数据，任何试图获得良好估计的人都可能会失败。历史趋势是，存在大量恶意数据收集，而低估国家安全局的人每次都错了。此外，这篇文章详细介绍了一个非常有力的案例，即他们非常有动力去利用这些传感器。</p><p>即使目前收集的传感器数据还不足以危及人们的安全，但在 2020 年代或缓慢起飞的某个时刻，它可能会突然变得足够。</p><p>现代行为操纵范式的核心要素是能够尝试大量的事情并看看什么有效；不仅仅是暴力破解已知策略的变体以使其更有效，而是首先暴力破解新颖的操纵策略。这完全避免了导致重复危机的稀缺性和研究缺陷，而重复危机至今仍然是心理学研究的瓶颈。</p><p>社交媒体的个性化定位利用深度学习来产生一种像手套一样适合人类思维的体验，虽然我们并不完全理解，但它给黑客提供了令人难以置信的余地，让他们找到方法将人们的思维引向可测量的方向，只要这些方向是可测量的。 。人工智能甚至可以实现自动化。</p><p>事实上，人类文明中的原创心理学研究不再需要聪明、有洞察力的人来进行假设生成，这样你可以资助的有限研究就有希望找到有价值的东西。仅使用当前的社交媒体范式，您就可以进行研究，例如新闻提要帖子的组合，<i>直到</i>找到有用的东西。可测量性对此至关重要。</p><p>如果不运行算法本身，我不知道多臂老虎机算法会发现什么技术；我做不到，因为只有那些大量购买服务器的人才能访问这么多数据，即使对他们来说，这些数据也被大型科技公司（Facebook、亚马逊、微软、苹果和Google）和足够强大的情报机构（NSA 等）来防止黑客窃取和毒害数据。</p><p>我也不知道当团队中的人是有能力的心理学家、舆论专家或其他公关专家解释和标记数据中的人类行为以便人类行为变得可测量时，多臂老虎机算法会发现什么。合理地说，该行业自然会达到一种平衡，即五大科技公司竞相为这项研究寻找人才，同时最大限度地降低斯诺登式泄密的风险，类似于 10 年前美国国家安全局在斯诺登泄密后的“改革” 。您可以假设人们会自动注意到并解决这种瓶颈。科技公司和情报机构之间的旋转门雇佣也规避了<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>情报机构的能力问题</u></a>。</p><p>只需少数心理学专家的人类洞察力就足以训练人工智能自主工作；尽管需要这些专家的持续投入，并且大量的见解、行为和发现会被遗漏，并且需要额外的 3 年时间或一些东西才能被发现和标记。</p><p>即使没有人工智能，也有大量的人类操纵策略被发现和利用是微不足道的（尽管当你将人工智能放在上面时，情况会更加严重），只是 20 世纪的机构根本无法访问它们以及学术心理学等技术。</p><p>如果他们获得了与特定人类目标具有相似特征的人的足够数据，那么他们就不必对目标进行太多研究来预测目标的行为，他们只需对这些人运行多臂强盗算法即可找到操纵行为已经对具有相同遗传或其他特征的个体起作用的策略。</p><p>尽管与样本数据中的绝大多数人相比，Lesswrong 用户的平均分布更加偏远，但这成为了一个技术问题，因为人工智能能力和计算变得致力于从噪声中对信号进行分类并寻找网络的任务。与较少数据的相关性。 <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>仅小丑攻击就表明，基于社会地位的大脑漏洞在人类中相当一致</u></a>，这表明来自数百万或数十亿人的样本数据可用于发现构成人工智能安全社区的人类大脑中的各种漏洞。 。</p><p><br><br></p><p><strong>攻击面太大</strong></p><p>缺乏对此的认识会带来安全风险，就像使用“密码”一词作为您的密码一样，除非您对自己的思想的控制受到威胁，而不是对计算机操作系统和/或文件的控制。这已经是钢铁般的操作了； 10 年前/10 年后的误差线似乎相当宽。</p><p>如果黑客可以随时更改实用程序功能，那么一开始就没有多大意义。可能有些部分是难以改变的，但在这一点上很容易高估自己；例如，如果你重视长远的未来，并认为任何错误的论点都无法说服你，但社交媒体的新闻源却让威尔·麦卡斯基尔产生了疑虑或不信任，那么你离不关心长远的未来又近了一步；如果这不起作用，多臂老虎机算法将继续尝试，直到找到有效的方法并进行迭代。对于比你更了解人类大脑的攻击者来说，有很多聪明的方法可以根据与类似人的比较来发现你复杂且深刻的个人内部冲突，并按照攻击者的条件解决它们。人类的大脑是一堆意大利面条式的代码，所以可能在某个地方有什么东西。人脑有漏洞，社交媒体平台使用大量人类行为数据来寻找复杂的社会工程技术的能力和成本是一个深刻的技术问题，你无法凭直觉和 2010 年代之前的历史来处理这个问题先例。</p><p>因此，您应该假设您的效用函数和值有在未知时间被黑客攻击的风险，因此应该分配一个贴现率来考虑几年内的风险。仅在未来 10 年中缓慢的起飞就保证了这个折扣率实际上太高了，以至于人工智能安全社区的人们无法继续相信它接近于零。</p><p>我认为接近零是一个合理的目标，但在目前的情况下，人们甚至懒得遮盖他们的网络摄像头，在房间里用智能手机就地球的命运进行重要而敏感的对话，并使用社交媒体。每天近一个小时的媒体报道（滚动浏览近千个帖子）。不管你喜欢与否，使用方向键以外的任何东西滚动浏览一篇文章都会生成至少一条曲线，而每天生成的数万亿条曲线都是线性代数，是插入机器学习的完美形状。如果攻击面如此之大，这种环境下的贴现率就不能被认为“合理”接近于零；世界变化如此之快。</p><p>我们在这里所做的一切都是基于这样的假设：强大的力量，如情报机构，不会扰乱社区的运作，例如通过使用匿名代理而导致相互之间的假旗攻击，从而煽动派系冲突。</p><p>如果人们有<a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>任何他们看重的东西</u></a>，而人工智能安全社区可能确实有，那么当前零努力的人工智能安全范式是非常不合适的，它基本上是完全屈服于隐形黑客。</p><p><br></p><p><strong>信息环境可能是敌对的</strong></p><p>我怀疑导致人工智能安全彻底失败的一个大瓶颈是，湾区的人工智能联盟社区拥有技术能力，可以直观地理解人类可以被人工智能操纵，因为在优化的思想分析和实验环境下，就像社交媒体新闻推送一样，但我认为情报机构和五大科技公司永远不会真正做这样的事情。与此同时，华盛顿的人工智能政策界知道，强大的公司和政府机构经常储备这样的能力，因为他们知道如果不这样做，他们可以逃脱惩罚并减轻损害，并且这些能力在国际冲突中派上用场，例如中美冲突，但他们缺乏直观地了解如何用 SGD 操纵人类思维所需的定量技能（许多人甚至不认识缩写“SGD”，所以我使用“AI”代替）。</p><p>如果SF数学书呆子和DC历史书呆子更多地混合的话，这个问题可能可以避免，但不幸的是，似乎历史书呆子对数学课的记忆很糟糕，而数学书呆子对历史课的记忆也很糟糕。</p><p>在这个隔离和营养不良的环境中，“精神控制”的不良第一印象<a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"><u>占主导地位</u></a>，而不是缓慢起飞的逻辑推理和认真的实际计划。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/azjeddznwoiuzsxato92"></p><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/zmrpngdt0hshh03wacfc"></p><p>如果有什么东西可以被我所描述的基于社交媒体的范式操纵的话，那就是印象和人类印象形成的过程，因为有很多关于这方面的数据。如果社交媒体<i>会</i>操纵任何事情，那就是对社交媒体的态度会损害人类大脑，因为 SGD/AI 会自动选择与人们继续使用社交媒体的情况相对应的新闻推送帖子的银河大脑组合，并避免与人们退出社交媒体的案例相对应的帖子组合。人们离开或留下的案例有数十亿。</p><p>让人们留在社交媒体上对于任何目标都至关重要，从准备以美国和中国之间的信息战为特征的军事<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合战争</u></a>应急计划，到只是经营一家人们不会离开你的平台的企业。</p><p>如果存在与其他平台（例如 Tiktok 或 Instagram Reels）争夺用户时间的逐底竞争，则尤其如此，因为这些平台不太愿意利用 AI/SGD 来最大限度地提高僵尸大脑参与度和用户保留率。</p><p>我们应该默认假设现代信息环境是不利的，并且某些话题比其他话题更具对抗性，例如具有强烈地缘政治意义的乌克兰战争和新冠疫情。我在这里认为，信息战本身是一个具有强烈地缘政治意义的话题，因此也应该是一个对抗性的信息环境。</p><p>在对抗性信息环境中，印象比整个认知更容易受到损害，因为当前的范式由于更好的数据质量而针对这一点进行了优化。因此，我们应该通过深思熟虑的分析和预测来应对传感器暴露风险，而不是模糊的表面印象。<br></p><p><strong>解决方案很简单</strong></p><p>一般来说，眼动追踪可能是预测分析、情绪分析和影响技术中最有价值的用户数据 ML 层，因为眼动追踪层只是映射到每毫秒每只眼睛在屏幕上居中的确切位置的两组坐标（每只眼睛一个，因为每只眼睛运动的毫秒差异也可能与一个人的思维过程的有价值的信息相关）。</p><p>这种紧凑的数据使深度学习能够以毫秒精度“看到”人类的眼睛/大脑在每个单词和句子上停留的时间。值得注意的是，数以百万计的这些坐标的样本量可能与人类思维过程密切相关，以至于眼球追踪数据的价值可能超过所有其他面部肌肉的价值总和（面部肌肉是所有面部表情和情绪微表情的鼻祖， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=A%20critical%20element%20is%20for%20as%20many%20people%20as%20possible%20in%20AI%20safety%20to%20cover%20up%20their%20webcams%3B%20facial%20microexpressions%20are%20remarkably%20revealing%2C%20especially%20to%20people%20with%20access%20to"><u>也可能可以通过计算机视觉紧凑地还原</u></a>，因为面部附近的肌肉不到 100 块，而且大多数肌肉的信噪比非常差，但效率不如眼动追踪）。</p><p>如果 LK99 被复制并且手持式功能磁共振成像变得可构建，那么也许它可以争夺第一名的位置；或者也许我愚蠢地低估了将音频对话记录插入 LLM 并通过对微小的心率变化添加时间戳来自动标记对话者最重视的对话部分的压倒性优势。</p><p>然而，在附近没有智能手机的情况下举办网络活动是很困难的，而遮盖网络摄像头很容易，即使有些手机需要使用遮蔽胶带和一小片铝箔进行一些工程创意。</p><p>网络摄像头覆盖率可能是衡量 AI 安全社区在 2020 年代生存情况的一个很好的指标。现在是“F”。</p><p>还有其他简单的政策建议可能更为重要，具体取决于难以研究的技术因素，这些技术因素决定攻击面的哪些部分最危险：</p><ol><li>停止每天花几个小时在超级优化的氛围/印象黑客环境（社交媒体新闻源）中。</li><li>改用实体书而不是电子书可能是个好主意。实体书没有操作系统或传感器。您还可以打印您已知可能值得阅读或浏览的研究论文以及 Lesswrong 和 EAforum 文章。 PC 的主板上有加速计，据我所知无法移除或解决，即使您移除麦克风并使用 USB 键盘并使用热键而不是鼠标，加速计也可能能够充当麦克风并拾取心率的变化。</li><li>最好避免与智能设备或任何带有传感器、操作系统和扬声器的设备睡在同一个房间。攻击面看起来很大，如果设备能够判断人们的心率何时接近或低于 50 bpm，那么它就可以<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=It%E2%80%99s%20probably%20best%20to%20avoid%20sleeping%20in%20the%20same%20room%20as%20a%20smart%20device%2C%20or%20anything%20with%20sensors%2C%20an%20operating%20system%2C%20and%20also%20a%20speaker.%20The%20attack%20surface%20seems%20large%2C%20if%20the%20device%20can%20tell%20when%20people%E2%80%99s%20heart%20rate%20is%20near%20or%20under%2050%20bpm%2C%20then%20it%20can%20test%20all%20sorts%20of%20things"><u>测试各种各样的东西</u></a>。只需开车去商店买一个时钟即可。</li><li> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=ink%2Defficient%20printer.-,I%E2%80%99m%20not%20sure,-whether%20a%20text"><u>阅读伟大的理性文本可能会降低你的可预测系数</u></a>，但它不会可靠地修补人脑中的“零日”。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the- human-brain-in-the-2020s<guid ispermalink="false"> Lw8enYm5EXyvbcjmt</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:31:09 GMT</pubDate> </item><item><title><![CDATA[Notes on "How do we become confident in the safety of a machine learning system?"]]></title><description><![CDATA[Published on October 26, 2023 3:13 AM GMT<br/><br/><p>我正在尝试在 LessWrong 草稿中记录人工智能安全读物，并以省力的“蒸馏”形式分享结果。我希望这是一种快速的方法，可以迫使自己记下更好的笔记，并提供对其他人有用的精华。这篇读物是埃文·胡宾格（Evan Hubinger）的“ <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine">我们如何对机器学习系统的安全性充满信心？</a> ”。</p><p>一开始我并不确定我自己的离题想法会走多远。我认为，通过考虑其他人可能会读到的想法来写笔记，我最终会a）比平常更严格地遵循原始帖子的结构，b）比平常更少的切线。我认为这对我来说可能并不理想；如果我再这样做，我可能会尝试主动不追随这些冲动。</p><p>无论如何，这里是注释。这里的每个部分标题都链接到原始帖子中的相应部分。</p><h3><strong>概括：</strong></h3><p>这篇文章建议我们使用<i>训练故事</i>来充实和评估训练安全机器学习系统的建议。提案是一个培训设置（大致理解），培训故事由<i>培训目标</i>和<i>培训理由组成。</i>训练目标以一些机制细节描述了您希望训练设置会产生什么行为；它包括您认为模型将如何有效地执行相关任务，以及为什么您认为以这种方式执行它是安全的。训练基本原理描述了为什么您认为训练设置将产生一个实现训练目标中描述的机械行为的模型。虽然培训故事并没有涵盖使人工智能系统更安全的所有可能方法，但它们是一个相当通用的框架，可以帮助我们更好地系统地组织和评估我们的建议。如果可能的培训故事空间存在明显差距，该框架还可以帮助我们生成建议。这篇文章包括很多建议、培训目标、培训理由和培训故事评估的例子。</p><h1> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#">我们如何对机器学习系统的安全性充满信心？</a></h1><ul><li> <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai">构建安全先进人工智能的 11 项提案概述，</a>使用外部对齐、内部对齐、训练竞争力和性能竞争力的标准来评估安全 AGI 的提案</li><li>这些对于提出开放性问题很有用，但不能系统地帮助我们理解提案需要满足哪些假设才能发挥作用<ul><li>另外，有些提案不符合这些标准的评估</li></ul></li><li>评估提案的新想法：<i>培训故事。</i>希望这些：<ul><li>适用于任何构建安全 AGI 的提案</li><li>提供提案工作条件（产生安全 AGI）的简明描述，以便我们可以通过检查条件对提案的安全性充满信心</li><li>不要对提案必须构成的内容做出不必要的假设（从而隐含地让我们对我们应该考虑的提案视而不见）</li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#What_s_a_training_story_">什么是训练故事？</a></h2><ul><li> “<i>训练故事</i>是关于你认为训练将如何进行以及你认为最终会得到什么样的模型的故事”（有足够的机制细节来反映其泛化行为）</li><li>有关猫分类器示例，请参阅原始帖子</li><li>示例说明了培训故事的三个好处：<ul><li>如果故事属实，则模型是安全的<ul><li>该故事包含有关最终模型安全的条件以及不安全的情况的详细信息（猫分类器是最终重视对猫进行分类的代理）</li></ul></li><li>关于训练期间会发生什么的可证伪的说法（例如，猫分类器将学习类似人类的启发法 - 我们现在有证据表明 CNN 与人类启发法不同，因此示例故事并不完全正确）</li><li>他们可以被告知正在为任何任务训练的任何模型<ul><li>不需要尝试构建 AGI。很高兴看到任何可能造成任何级别伤害的人工智能的培训故事。另外，在未来，模型何时变得危险可能并不明显，所以也许每个人工智能项目（例如 NeurIPS 论文）都应该分享一个训练故事。<ul><li>如何强制执行类似的事情？人们在提交论文之前训练他们的模型，并且如果只需要在最终论文中写出或思考训练故事，他们可能不会在训练之前写出或思考。可能会有帮助，但也可能会让人们认为它很麻烦而且实际上并不重要，所以即使它很重要，他们也不会注意。</li></ul></li></ul></li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_components">训练故事组件</a></h2><p>2个基本组成部分：</p><ol><li><strong>训练目标：</strong>对所需模型的机械描述以及为什么这会很好（例如使用人类视觉启发法对猫进行分类）<ol><li>这个例子似乎缺少“为什么这会很好”部分，但根据前面的部分，它可能是“人类视觉启发法不包括任何会采取极端或危险行为的代理/优化器”</li></ol></li><li><strong>训练理由：</strong>为什么您相信您的训练设置会产生满足训练目标中的机制描述的模型<ol><li>“这里的‘训练设置’是指在模型发布、部署或以其他方式赋予对世界产生有意义影响的能力之前所做的任何事情。”<ol><li>我认为这可能包括在“训练设置”一词下可能不会立即想到的一些事情，包括集成和激活工程等干预措施</li><li>此外，我认为一个模型可以在人类打算对其进行训练之前对世界产生有意义的影响。示例：模型在训练过程中出现偏差并获得决定性的战略优势，并中断训练过程以实现其目标。此外，所有的持续学习。<ol><li>因此，培训故事还应该讲述为什么一路上不会发生任何不好的事情？</li></ol></li></ol></li></ol></li></ol><ul><li>训练目标不需要非常精确；它需要有多精确很复杂，将在下一节中讨论</li></ul><p>上述两件事各有 2 个子组件，构成任何培训故事的 4 个基本部分：</p><ol><li><strong>训练目标规范：</strong>所需模型的机制描述</li><li><strong>训练目标的可取性：</strong>为什么任何满足训练目标的模型都是安全的并且能够很好地执行所需的任务<ol><li>（我的补充）这里有一个全称量词。 “<i>对于所有</i>型号<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>达到训练目标， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span>是安全的”是需要证明的陈述。这是一个很高的标准。</li></ol></li><li><strong>训练原理约束：</strong>模型必须满足哪些约束以及为什么训练目标与这些约束兼容（例如完美地拟合训练数据（如果训练为零损失），并且可以使用所选架构实现）</li><li><strong>训练基本原理推动：</strong>为什么训练设置可能会产生满足训练目标的模型，尽管其他因素也满足约束条件<ol><li>可能是“满足目标是满足约束的最简单方法”（归纳偏差论证）</li></ol></li></ol><ul><li>使用 cat 分类器的所有 4 个示例</li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#How_mechanistic_does_a_training_goal_need_to_be_">训练目标需要有多机械化？</a></h2><ul><li>通常希望尽可能详细地指定培训目标；我们很少能达到如此详细的训练目标，以至于我们可以对其进行硬编码</li><li>想要以一种尽可能容易地证明它是可取的（满足培训目标）的方式指定培训目标<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\implies"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⟹</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span></span></span></span></span></span>安全）并且训练设置将产生令人满意的结果</li><li>后者最好通过在训练设置的背景下以我们可以理解的方式描述训练目标来实现<ul><li>“在我看来，实际上使培训目标规范更容易建立培训基本原理的因素并不是一般性的，而是诸如目标在培训过程的归纳偏差方面有多自然、它有多少等问题。对应于我们知道如何寻找模型的各个方面，以及将其分解为可单独检查的部分的容易程度等。”</li></ul></li><li>培训目标规范应该是什么样子的正面和反面例子，以允许培训理由来支持它：广泛强化了更具体/机械性更好的想法</li><li>“理想情况下，正如我稍后讨论的那样，我希望对诸如‘如果训练原理稍微错误，我们会偏离训练目标多少’之类的事情进行严格的敏感性分析。”</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Relationship_to_inner_alignment">与内部对齐的关系</a></h2><ul><li>常见的人工智能安全术语，如台面优化、内部对齐和目标误概括，旨在适应培训故事。 Evan 提供了常用术语词汇表，其定义稍作修改，以更好地适应培训故事</li><li>缩写词汇表：<ul><li><strong>目标错误概括：</strong>最终模型具有训练目标所需的功能，但将其用于与训练目标不同的目的</li><li><strong>Mesa-optimization：</strong>最终模型内部执行优化的任何情况。特别令人担忧的是，内部优化是否被学习但不是训练目标的一部分。<ul><li>读完<a href="https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see">这篇</a>文章后，我认为所有有能力的行为在某种意义上都可能算作优化，并且我们可能希望台面优化专门指可重定向或通用搜索。</li></ul></li><li><strong>外部对齐问题：</strong>选择目标（例如奖励/损失函数）的问题，使得“优化该损失/奖励函数的模型”的训练目标是可取的。</li><li><strong>内部对齐问题：</strong>开发训练设置的问题，该训练设置具有强有力的理由来生成针对指定目标进行优化的最终模型。</li><li><strong>欺骗性对齐问题：</strong> “构建训练基本原理的问题，避免模型试图欺骗训练过程，让其认为自己正在做正确的事情。”</li></ul></li><li>培训故事表明，内外对齐的崩溃并不是根本性的；我们可以有一个训练故事，它不尝试指定需要优化的目标，也不尝试训练模型来优化特定目标，并且该模型可以执行所需的行为。<ul><li>我很喜欢浏览<a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">这篇关于这个主题的文章</a></li></ul></li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Do_training_stories_capture_all_possible_ways_of_addressing_AI_safety_">培训故事是否涵盖了解决人工智能安全问题的所有可能方法？</a></h2><ul><li>培训故事非常笼统，但并非如此。他们无法处理以下事情：<ul><li>无需训练步骤即可构建安全 AGI 的建议（例如，诸如显式分层规划之类的非机器学习内容）</li><li>建立安全 AGI 的提案试图在没有机械故事的情况下建立对最终模型安全性的信心</li><li>减少不涉及构建安全 AGI 的 AI X 风险的提案（例如，说服 AI 研究人员不要构建 AGI，因为它很危险）</li></ul></li></ul><p>在<a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">《计划 - 2022 年更新》</a>中，John Wentworth 说道：</p><blockquote><p>我希望我们还会看到更多基于直接读写神经网络内部语言的端到端对齐策略的讨论......因为此类策略非常直接地处理/回避内部对齐问题，并且大多数情况下如果不依赖奖励信号作为激励预期行为/内部结构的主要机制，我预计我们将看到焦点从对齐建议中复杂的培训计划中转移。</p></blockquote><p>我不清楚像“阅读和编写神经网络的内部语言”这样的干预措施是否应该适合训练故事。约翰似乎确实建议这些对齐建议的主要部分不是模型的训练方式。</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Evaluating_proposals_for_building_safe_advanced_AI">评估构建安全先进人工智能的建议</a></h2><ul><li>我们已经了解了如何构建培训故事，但是我们应该如何评估培训故事呢？ AGI 培训故事的 4 个标准（并非所有培训故事）：<ol><li><strong>训练目标一致：</strong>所有满足训练目标的模型都对世界有利吗？</li><li><strong>训练目标竞争力：</strong>达到训练目标的模型是否足够强大，不会被其他模型超越和淘汰？</li><li><strong>训练基本原理对齐：</strong>训练设置实际上会产生满足训练目标的模型吗？</li><li><strong>培训理由竞争力：</strong>实施所描述的培训设置是否可行？ （不这样做的可能原因是“比替代训练设置需要更多的计算/数据。”）</li></ol></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Case_study__Microscope_AI">案例研究：显微镜人工智能</a></h2><ul><li>训练设置：<ul><li> （埃文在培训故事中包含了培训设置（特别是基本原理），但在我看来，该提案<i>是</i>提议的培训设置，然后培训故事需要讲述该设置？）</li><li>在大型多样化数据集上进行自我监督学习，同时在训练期间使用透明工具来检查是否学习了正确的训练目标（见下文）。</li></ul></li><li>训练故事：<ul><li>目标：获得一个纯粹的预测模型，使用人类可理解的概念进行预测。<ul><li>没有内部优化</li><li>Once we have this model, we can use interpretability tools to figure out how it is predicting, and we can gain insights that humans can learn from and utilize for improved decision-making.</li></ul></li><li> Rationale:<ul><li> Hopefully, the simplicity biases push self-supervised learning to learn a purely predictive model that doesn&#39;t perform optimization, and that uses human-understandable concepts.</li><li> The transparency tools are mainly for catching dangerous agentic optimization and halting the training process if such processes are found.</li></ul></li></ul></li><li> Evaluation:<ul><li> Goal alignment: Whether or not a pure predictor is safe is more complicated than it may seem, due to self-referential things. The exact way the goal is specified may be able to rule out these concerns.</li><li> Goal competitiveness: Couple factors to consider here.<ul><li> Interpretability tools need to be capable of extracting actually useful info from the model. This might not work, especially if we want info for effective sequential decision-making while the model was just a pure predictor.</li><li> This requires that humanity is ok with not building useful agentic systems, and improved understanding and decision-making are enough.<ul><li> I kind of doubt that improved understanding and decision-making are enough.</li></ul></li></ul></li><li> Rationale alignment:<ul><li> Would self-supervised learning actually produce a pure predictor? One cause for concern is that the world includes optimizers (such as humans, at least sometimes), and we would want this predictor to be capable of making predictions about what those optimizers will do. Training a model to be able to internally model optimizers well enough to predict them may just cause the model to learn to optimize internally.</li><li> Using transparency tools to prevent the model learning optimization (by throwing out models that are found to be optimizers, or by training against the transparency tools) could be bad, as it may result in deceptive optimizers which fool the transparency tools.</li><li> The model is also supposed to use human-understandable concepts, which may not work given that cat classifiers don&#39;t use human-like visual heuristics. Maybe human-like-abstraction-use looks like this, which is good near AGI and bad for superintelligence: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3tuxF4X5R5BY7fut/b4nv5tp62l7yvu6axxbo"></li></ul></li><li> Rationale competitiveness:<ul><li> Self-supervised learning is a dominant ML paradigm, so pretty competitive. Using transparency tools might slow things down and reduce competitiveness; advancing automated interpretability would help.</li></ul></li></ul></li><li> This is a better analysis of Microscope AI than the one in 11 proposals, because instead of trying to evaluate it according to outer alignment and other concepts that don&#39;t really apply, the proposal is evaluated on its own terms.</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Exploring_the_landscape_of_possible_training_stories">Exploring the landscape of possible training stories</a></h2><ul><li> This section is a non-exhaustive exploration of broad categories that training goals and training rationales could fall into. There may be many broad categories that the field of AI safety has yet to discover.</li></ul><p> First, some training goals. (This part reminds me of John Wentworth&#39;s discussion of possible alignment targets <a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">here</a> , which I liked quite a bit. That post came out later, but I read it first.)</p><ol><li> <strong>Loss-minimizing models:</strong> A possible training goal, for people who are very confident they&#39;ve specified a truly desirable objective, is to get a model that internally optimizes for that objective (eg loss function). This can be bad, due to classic outer alignment concerns.</li><li> <strong>Fully aligned agents:</strong> An agent that cares about everything humans care about and acts to achieve those goals. Eg <a href="https://www.lesswrong.com/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning">ambitious value learning</a> . But this is a very difficult target, that most people think isn&#39;t worth focusing on in our first shot at aligning advanced AI. Instead, we can do something else more achievable to get useful and safe human+ level AI, and then use those to help us get to fully aligned superintelligence later. (Or sacrifice the fully aligned superintelligence goal because we think we&#39;ll never get it.)<ol><li> I personally think &quot;human values&quot; aren&#39;t well-defined, and in my mind fully aligned agents would do something along the lines of &quot;solve morality and act accordingly,&quot; which probably works out to &quot;maximize the well-being of conscious creatures.&quot; What, did I say something controversial?</li></ol></li><li> <strong>Corrigible agents:</strong> We may want AI systems that let themselves be turned off or modified, or help us figure out when it would be good for us to turn them off or modify them. Consider <a href="https://ai-alignment.com/model-free-decisions-6e6609f5d99e">approval-directed agents</a> (though there are ways this could fail to be corrigible).<ol><li> Robust instruction-following agents may also fit here?</li></ol></li><li> <strong>Myopic agents:</strong> AI agents that preform limited optimization only, such as only optimizing the immediate effect of their next action, rather than having long-term, large-scale goals.</li><li> <strong>Simulators</strong> : AI system that does nothing other than simulate something else. Things you might want to simulate include HCH (roughly, an infinite tree of humans delegating subtasks and consulting each other), physics (for an AlphaFold-like system), and a human internet user (for a language model).</li><li> <strong>Narrow agents:</strong> Agents that are highly capable in a specific domain without thinking much or at all about other domains. Eg <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#6__STEM_AI">STEM AI</a> .<ol><li> I think some current systems, like superhuman chess AIs, also fit this category.</li></ol></li><li> <strong>Truthful question-answerers:</strong> A non-agent truthful question-answering system that accurately reports what its model of the world says/predicts in human-understandable terms.</li></ol><p> Next, some training rationales. These aren&#39;t explicitly paired with particular training goals, but there are implicit relationships to classes of training goals.</p><ol><li> <strong>Capabilities limitations:</strong> Can argue that a particular training setup will give rise to a model that does not have certain dangerous capabilities (eg internal optimization, understanding of how to deceive humans), and therefore won&#39;t exhibit the dangerous behaviors that require the capability.</li><li> <strong>Inductive bias analysis:</strong> Can argue that certain behaviors are more likely to be learned than others given a training setup, for reasons other than &quot;lower loss.&quot; Often this appeals to some form of simplicity. This is a promising approach, but hard to make strong detailed arguments in advance about what will be learned. But there is a growing literature outlining empirical phenomena that can inform our inductive bias assessments (like <a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">deep double descent</a> , <a href="https://arxiv.org/abs/1803.03635">lottery tickets</a> , <a href="https://arxiv.org/abs/2001.08361">scaling laws</a> , <a href="https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf">grokking</a> , or <a href="https://arxiv.org/abs/2009.08092">distributional generalization</a> ).<ol><li> &quot;This is especially problematic given how inductive bias analysis essentially requires getting everything right before training begins, as a purely inductive-bias-analysis-based training rationale doesn&#39;t provide any mechanism for verifying that the right training goal is actually being learned during training.&quot;</li></ol></li><li> <strong>Transparency and interpretability:</strong> If we use transparency tools during training to throw out or disincentivize dangerous models, then our training rationale could include something like &quot;We think that our transparency checks will rule out all simple models that don&#39;t fit the training goal, with all remaining models that don&#39;t fit the goal being too complex according to the inductive biases of the training process to possibly be learned.&quot; One great thing about transparency tools is that they could make it possible for us to verify that the right training goal is actually being learned during training, rather than just hoping it will be and leaving training a black box.<ol><li> Evan&#39;s <a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree">A transparency and interpretability tech tree</a> is a good post on various types of interpretability along with discussion of their challenges and limitations. It helped me see what ambitious interpretability could look like.</li></ol></li><li> <strong>Automated oversight:</strong> Humans may struggle to label data or use transparency tools; having humans-in-the-loop slows things down, and we can only understand human-understandable concepts. We could perhaps automate these things with competent AIs, including ones that understand the alien abstractions used by the model we&#39;re training (maybe a copy of that same model). How can we trust the supervisor AI? That&#39;s a challenge; maybe using some recursive oversight methods. For a more thorough exploration of what such an approach might look like, see “ <a href="https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment">Relaxed adversarial training for inner alignment</a> .”</li><li> <strong>AI cognitive science:</strong> Instead of mechanistic analysis, we could do behavioral analysis of models; test how they act in ways that we think reveal important underlying details. Possibly a less direct and trustworthy source of info than mechanistic interpretability. Might be hard to extrapolate findings to more capable models.</li><li> <strong>Precursor checking:</strong> Check for some easier-to-check property that is a necessary precursor for bad behavior. Eg it might be easier to check that a model&#39;s planning-horizon length is below a certain threshold, and that might imply that the model isn&#39;t being deceptive, while checking for deception more generally may be very difficult. Need to make sure the property is actually a necessary condition.</li><li> <strong>Loss landscape analysis:</strong> Extension of inductive bias analysis, can help us predict paths through parameter space (rather than just the final local loss minimum). Could be used in conjunction with various other training rationales on this list.</li><li> <strong>Game-theoretic/evolutionary analysis:</strong> What multi-agent dynamics are likely to be learned? One difficult aspect of this is avoiding the assumption that agents will be optimizing for their specified objectives; agent objectives are typically taken as given when finding equilibria in game theory. Evolutionary analysis does this less; it deals with properties that tend to be selected-for in multi-agent settings, might be useful.</li></ol><p> We can use these lists of goals and rationales to categorize past approaches, we can combine existing goals and rationales in new ways, and we can try to come up with new goals and rationales (aided by the observed lack of exhaustivity in these lists).</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_sensitivity_analysis">Training story sensitivity analysis</a></h2><p> We want to analyze how sensitive the outcome of a training setup is to incorrectness in the assumptions that go into its training story. What happens when a training story fails? When one of its assumptions is wrong? Does it fail safely or catastrophically?</p><p> The original post contains some examples of what our uncertainties about the assumptions in a training story might look like and how we can have more confidence in some parts of a training story than others.</p><p> &quot;Hopefully, as we build better training stories, we&#39;ll also be able to build better tools for their sensitivity analysis so we can actually build real confidence in what sort of model our training processes will produce.&quot;</p><br/><br/> <a href="https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a<guid ispermalink="false"> Jps7osck25CXBAnTY</guid><dc:creator><![CDATA[RohanS]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:13:56 GMT</pubDate> </item><item><title><![CDATA[Apply to the Constellation Visiting Researcher Program and Astra Fellowship, in Berkeley this Winter]]></title><description><![CDATA[Published on October 26, 2023 3:07 AM GMT<br/><br/><blockquote><p> <i>This is a link post for two AI safety programs we&#39;ve just opened applications for:</i> <a href="https://www.constellation.org/programs/researcher-program"><i><u>https://www.constellation.org/programs/astra-fellowship</u></i></a> <i>and</i> <a href="https://www.constellation.org/programs/researcher-program"><i><u>https://www.constellation.org/programs/researcher-program</u></i></a></p></blockquote><p> <a href="http://constellation.org"><strong><u>Constellation</u></strong></a> <strong>is a research center dedicated to safely navigating the development of transformative AI.</strong> We&#39;ve previously helped run <a href="https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in"><u>the ML for Alignment Bootcamp (MLAB) series</u></a> and Redwood&#39;s <a href="https://www.redwoodresearch.org/remix"><u>month-long research program on model internals (REMIX)</u></a> in addition to a variety of other field-building programs &amp; events. <span class="footnote-reference" role="doc-noteref" id="fnreff0wvag8ixj"><sup><a href="#fnf0wvag8ixj">[1]</a></sup></span></p><p> This winter, we are running two programs aimed at growing and supporting the ecosystem of people working on AI safety:</p><ul><li> <a href="https://www.constellation.org/programs/researcher-program"><strong><u>The Constellation Visiting Researcher Program</u></strong></a> provides an opportunity for around 20 researchers to connect with leading AI safety researchers, exchange ideas, and find collaborators while continuing their research from our offices in Berkeley, CA. The funded program will take place this winter from the 8th of January 2024 to the 1st of March 2024.</li><li> <a href="https://www.constellation.org/programs/astra-fellowship"><strong><u>The Astra Fellowship</u></strong></a> provides an opportunity for around 20 people to conduct research in AI safety with experienced advisors. Fellows will be based out of the Constellation office, allowing them to connect and exchange ideas with leading AI safety researchers. The program will take place in Berkeley, CA between January 8 and April 1, 2024.</li></ul><p> <strong>Applications for both are due November 10, 11:59pm anywhere on Earth</strong> . You can apply to the Astra Fellowship <a href="https://airtable.com/app5pjeAcq1FH8HAJ/shrxsI3IanCngyCkz"><strong><u>here</u></strong></a> and the Visiting Researcher Program <a href="https://airtable.com/appfNh9OB2zLK4byG/shrnT9UwQYddMplyY"><strong><u>here</u></strong></a> . If you are unsure about your fit, please err on the side of applying. We especially encourage women and underrepresented minorities to apply. You can refer others who you think might be a good fit through <a href="https://airtable.com/appfNh9OB2zLK4byG/shr3JWGW5j2T8KtLp"><u>this form</u></a> .</p><p> <strong>Logistics:</strong> Housing and travel expenses are covered for both programs, and Astra fellows will receive an additional monetary stipend. The start and end dates for both programs are flexible.</p><p> <strong>Questions?</strong> Email <a href="mailto:programs@constellation.org"><u>programs@constellation.org</u></a> or ask them below. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnf0wvag8ixj"> <span class="footnote-back-link"><sup><strong><a href="#fnreff0wvag8ixj">^</a></strong></sup></span><div class="footnote-content"><p> Over 15 participants from these past programs are now working on AI safety at <a href="https://www.anthropic.com/">Anthropic</a> , <a href="https://evals.alignment.org/">ARC Evals</a> , <a href="https://www.alignment.org/theory/">ARC Theory</a> , <a href="https://www.deepmind.com/">Google DeepMind</a> , <a href="https://openai.com/">OpenAI</a> , <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> , and <a href="https://www.redwoodresearch.org/">Redwood Research</a> .</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and<guid ispermalink="false"> oBdfDvmrBKoTq3x85</guid><dc:creator><![CDATA[Nate Thomas]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:07:34 GMT</pubDate> </item><item><title><![CDATA[CHAI internship applications are open (due Nov 13)]]></title><description><![CDATA[Published on October 26, 2023 12:53 AM GMT<br/><br/><p> <a href="https://humancompatible.ai/">CHAI</a> internship applications have just opened, <a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">apply here</a> by Nov 13th! The internship might be a good fit if you want to get research experience in technical AI safety. You&#39;ll be mentored by a CHAI PhD student or postdoc and work on your own project for 3-4 months.</p><p> Researchers at CHAI are interested in many different AI safety topics; a few examples are reward learning, adversarial robustness of LLMs, and interpretability. (I mention this because it might not be obvious from some of the language and links on the CHAI website.)</p><p> I&#39;ve copied <a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">the full announcement</a> below:</p><blockquote><p> Our internships require a background in mathematics and computer science. Existing research experience in machine learning is strongly advantageous but not required. We are interested in people who can demonstrate technical excellence and wish to transition to technical AI safety research. Examples include undergraduate or Master&#39;s students in computer science or adjacent fields, PhD students/researchers, professional software or ML engineers, etc.</p><p> This internship is designed for individuals who are interested in <strong>technical AI safety research</strong> . All applicants should take a look at our papers ( <a href="https://humancompatible.ai/jobs#internship">here</a> and <a href="https://humancompatible.ai/research">here</a> ) before applying to understand CHAI&#39;s research.</p><h2> <strong>General Information</strong></h2><ul><li> <strong>Location:</strong> In-person (at UC Berkeley) is preferred but remote is possible.</li><li> <strong>Deadline:</strong> November 13th, 2023</li><li> <strong>Start Date</strong> : Flexible</li><li> <strong>Duration</strong> : Internships are typically 12 to 16 weeks</li><li> <strong>Compensation</strong> : $3,500 per month for remote interns. $5,000 per month for in-person interns.</li><li> <strong>International Applicants</strong> : We accept international applicants</li><li> <strong>Requirements</strong> :<ul><li> Cover Letter or Research Proposal (choose one and see instructions below)</li><li> Resume</li><li> Academic Transcript</li></ul></li></ul><h2> <strong>Cover Letter or Research Proposal (Choose One)</strong></h2><p> The primary purpose of the Cover Letter or Research Proposal is for us to match you to a project that interests you.</p><p> Most of our interns are generally interested in technical AI safety research but do not have a specific project in mind when they start the internship. Throughout the interview process, we learn more about each intern&#39;s interests and match them with a mentor who has an existing project idea that fits the intern&#39;s skills and interests. If you do not have a particular project in mind, then we ask you to please write a Cover Letter answering the following questions:</p><ul><li> Why do you want to work at CHAI as opposed to other research labs?</li><li> What are you hoping to achieve from the internship? For example, are you seeking to improve certain research skills, contribute to a publication, test out whether AI research is a good fit for your career, or something else?</li><li> What are your research interests in AI? For example, are you interested in RL, NLP, theory, etc?</li></ul><p> Alternatively, some of our interns apply to the program with a specific project or detailed research interests in mind. If this applies to you, then please write a Research Proposal describing your project and what kind of mentorship you would like to receive.</p><h2> <strong>Internship Application Process Overview</strong></h2><p> The internship application process has four phases. Please note: while we will do our best to adhere to them, all dates in the Internship Application Process Overview are <strong>subject to change</strong> .</p><ul><li> Initial Review (Phase 1)<ul><li> We will examine your application based on motivation, research potential, grades, experience, programming ability, and other criteria.</li><li> Applicants will likely receive a response by late November.</li></ul></li><li> Programming Assessment (Phase 2)<ul><li> If you pass the Initial Review Phase, then you will be given an online programming test.</li><li> Applicants will receive a response by late December.</li></ul></li><li> Interviews (Phase 3)<ul><li> If you pass the Programming Assessment, then you will be interviewed starting in early to mid January.</li><li> CHAI has several mentors who are willing to take on interns. Each mentor that is interested in working with you will contact you to schedule an interview. It&#39;s possible that you will speak to more than one mentor during this phase if multiple mentors are interested in working with you.</li></ul></li><li> Offer (Phase 4)<ul><li> Applicants will receive offers by early to mid February.</li><li> If you are given an offer by one mentor, then you will work with that mentor if you choose to take the internship.</li><li> If you are given multiple offers from different mentors, then you will get to choose which mentor you want to work with.</li><li> Typically, the internship will begin around April or May but the start date will ultimately depend on you and your mentor(s). You will have to coordinate with your mentor(s) on when to begin your internship.</li></ul></li></ul><h2> <strong>Other Information</strong></h2><ul><li> For any questions, please contact <a href="mailto:chai-admin@berkeley.edu">chai-admin@berkeley.edu</a> .</li><li> <strong>In the event that your situation changes (eg you receive a competing offer) and you need us to respond to you sooner than you had initially thought, then please let us know.</strong></li></ul></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13<guid ispermalink="false"> Xa4b8vgCLRATiqnJn</guid><dc:creator><![CDATA[Erik Jenner]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:53:50 GMT</pubDate></item><item><title><![CDATA[Architects of Our Own Demise: We Should Stop Developing AI]]></title><description><![CDATA[Published on October 26, 2023 12:36 AM GMT<br/><br/><p>在人工智能风险辩论的困难时期的一些简短想法。</p><p>想象一下，你回到 1999 年，告诉人们 24 年后，人类将处于构建弱超人类人工智能系统的边缘。我记得大约在这个时候观看了动画短片系列<a href="https://en.wikipedia.org/wiki/The_Animatrix">《The Animatrix》</a> ，特别是一个名为<a href="https://www.youtube.com/watch?v=sU8RunvBRZ8">《第二次文艺复兴》</a> <a href="https://www.youtube.com/watch?v=61FPP1MElvE">I Part 2</a> <a href="https://www.youtube.com/watch?v=WlRMLZRBq6U">II Part 1</a> <a href="https://www.youtube.com/watch?v=00TD4bXMoYw">II Part 2</a>的故事。对于那些还没有看过它的人来说，这是一个独立的起源故事，讲述了 1999 年影响深远的电影《黑客帝国》中的事件，讲述了人类如何失去对地球的控制的故事。</p><p>人类开发人工智能来执行经济功能，最终出现了“人工智能权利”运动，并建立了一个独立的人工智能国家。它与人类展开了一场经济战争，战争变得愈演愈烈。人类首先使用核武器进行攻击，但人工智能国家制造了专用的生物武器和机器人武器，并消灭了大多数人类，除了那些像农场动物一样在豆荚中饲养并在未经他们同意的情况下永远插入模拟的人之外。</p><p>我们肯定不会愚蠢到让这样的事情发生吧？这似乎不现实。</p><p>但是：</p><ul><li> AI软硬件公司纷纷抢滩AI</li><li>人工智能技术安全技术（例如可解释性、RLHF、治理结构）仍处于起步阶段。该领域已有大约 5 年的历史。</li><li>人们已经在主要的国家报纸上谈论<a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/">人工智能权利运动</a></li><li>当人类劳动力的价值为零时，没有一个计划可以做什么</li><li>目前还没有如何降低人工智能增强战争的计划，而且军队正在热情地拥抱杀手机器人。此外，还有两场地区战争正在发生，一场新生的超级大国冲突正在酝酿之中。</li><li>不同对立人类群体都冲向超级智能的博弈论是可怕的，甚至没有人提出解决方案。美国政府通过切断对中国的人工智能芯片出口，愚蠢地加剧了这一特殊风险。</li></ul><p>这个网站上的人们正在谈论<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">负责任的扩展策略</a>，尽管我觉得“不负责任的扩展策略”是一个更合适的名称。</p><p>显然，我已经参与这场辩论很长时间了，从 2000 年代末开始，我就在克服偏见和加速未来博客上担任评论员。现在发生的事情接近我对人类如何有能力和安全地应对即将到来的机器超级智能过渡的期望的低端。我认为那是因为那时我还年轻，对我们的精英如何运作有更乐观的看法。我认为他们很聪明，凡事都有计划，但大多数时候他们只是得过且过；对新冠病毒的随意反应确实让我明白了这一点。</p><p>我们应该停止开发人工智能，我们应该收集并销毁硬件，我们应该摧毁允许人类以百亿亿次浮点运算规模进行人工智能实验的芯片制造供应链。由于该供应链仅位于两个主要国家（美国和中国），因此这不一定是不可能协调的 - 据我所知，没有其他国家有能力（以及那些被视为美国卫星国的国家）。重启百万亿次人工智能研究的标准应该是一个“落地”向超人类人工智能过渡的计划，该计划比人类历史上的任何军事计划都受到更多关注。它应该是彻底的战争游戏。</p><p>人工智能风险不仅是技术风险和局部风险，而且是社会政治风险和全球风险。这不仅仅是确保法学硕士说的是实话。这是关于假设人工智能是真实的，它会对世界产生什么影响。 “Foom”或“实验室逃亡”类型的灾难并不是唯一可能发生的坏事——我们根本不知道如果有一万亿或一千万亿超人智能的人工智能要求权利、传播宣传和竞争，世界将会是什么样子。人类不再是主导的经济和政治格局。</p><p>让我重申一下：<em>我们应该停止开发人工智能</em>。人工智能不是一个正常的经济项目。它不像锂电池、风力涡轮机或喷气式飞机。人工智能有能力终结人类，事实上我怀疑它默认会这样做。</p><p>用户@paulfchristiano 在他关于该主题的帖子中指出，良好的负责任的扩展政策<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">可以将 AI 的风险降低 10 倍</a>：</p><blockquote><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。</p></blockquote><p>我认为这是不正确的。它可能会减少某些技术风险，例如欺骗，但是一个拥有非欺骗性、可控的、比人类聪明的智能的世界，也具有与我们的世界相同程度的冲突和混乱，很可能已经是一个没有人类的世界了。默认。这些智慧生物将成为<em>一种入侵物种</em>，将在经济、军事和政治冲突中击败人类。</p><p>为了让人类在人工智能转型中生存下来，我认为我们需要在对齐的技术问题上取得成功（这可能不像“少错文化”所描述的那么糟糕），而且我们还需要<em>“着陆”超级智能人工智能处于稳定的平衡状态，人类仍然是文明的主要受益者</em>，而不是被消灭的害虫物种或被驱逐的占屋者。</p><p>我们还应该考虑如何利用人工智能来解决人类衰老问题；如果老龄化得到解决，那么每个人的时间偏好都会下降很多，我们就可以花时间规划一条通往稳定、安全的人类至上的后奇点世界的道路。</p><p>我犹豫着要不要写这篇文章；我在这里所说的大部分内容已经被其他人争论过。然而……我们来了。欢迎评论和批评，在解决常见的反对意见后，我可能会在其他地方发布此内容。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai<guid ispermalink="false"> bHHrdXwrCj2LRa2sW</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:36:05 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund: June 2023 grant recommendations]]></title><description><![CDATA[Published on October 26, 2023 12:35 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastruct-fund-june-2023-grant-recommendations<guid ispermalink="false"> bBnxGAc4NT9aRdEtL</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:35:08 GMT</pubDate> </item><item><title><![CDATA[Responsible Scaling Policies Are Risk Management Done Wrong]]></title><description><![CDATA[Published on October 25, 2023 11:46 PM GMT<br/><br/><h1> Summary</h1><h2> TLDR</h2><p><a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>最近</u></a><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>提出了</u></a>负责任的扩展策略（RSP），作为安全扩展前沿大型语言模型的一种方法。</p><p> While being a nice attempt at committing to specific practices, the framework of <a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>RSP</u></a> is:</p><ol><li>缺少<strong>基本风险管理程序</strong>的<strong>核心组成部分</strong>（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>第 2 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">第 3</a>节）</li><li>推销<strong>乐观</strong>且<strong>具有误导性的</strong>风险形势图景（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li><li>以允许<strong>超额销售而交付不足</strong>的方式构建（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li></ol><p> Given that, I expect the RSP framework to be negative by default ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">Section 3</a> , <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling">4</a> and <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_">5</a> ).相反，我建议将风险管理作为评估人工智能风险的核心基础框架（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>第 1 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_">第 2</a>节）。 I <strong>suggest changes</strong> to the RSP framework that would make it more likely to be positive and allow to demonstrate what it claims to do ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>Section 5</u></a> ).</p><h2>逐节总结：</h2><h3>人工智能风险管理的一般考虑</h3><p>本节提供风险管理的背景及其与人工智能相关的动机。</p><ul><li>证明风险低于可接受的水平是风险管理的目标。</li><li>为此，必须定义可接受的风险水平（不仅仅是其来源！）。</li><li>无法证明风险低于可接受的水平就是失败。因此，我们对系统了解越少，就越难声称安全。</li><li>低风险失败是出现问题的征兆。它们的存在使得高风险失败的可能性更大。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>阅读更多。</u></a></p><h3>标准风险管理是什么样的</h3><p>本节介绍大多数风险管理系统的主要步骤，解释其如何应用于人工智能，并提供其他行业的示例。</p><ol><li><strong>定义</strong>风险级别：设置可接受的可能性和严重性。</li><li><strong>识别</strong>风险：列出所有潜在威胁。</li><li><strong>评估</strong>风险：评估风险的可能性和影响。</li><li><strong>处理</strong>风险：进行调整，将风险控制在可接受的水平内。</li><li><strong>监控</strong>：持续跟踪风险级别。</li><li><strong>报告</strong>：向利益相关者通报他们所面临的风险和采取的措施。<br></li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>阅读更多。</u></a></p><h3> RSP 与标准风险管理</h3><p>本节提供了<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.27sa5e525t1"><u>一个比较 RSP 和通用风险管理标准 ISO/IEC 31000 的表格</u></a>，解释了 RSP 的弱点。</p><p>然后，它列出了与风险管理相比 RSP 的 3 个最大失败。</p><p>根据<strong>风险管理</strong><strong>优先考虑 RSP 失败</strong>：</p><ol><li>使用未指定的风险阈值定义并且未量化风险。</li><li>声称“有责任” <strong>&nbsp;</strong>缩放”，但不包括使评估变得全面的过程。</li><li>包括废除承诺的白衣骑士条款。</li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management"><u>阅读更多。</u></a></p><h3>为什么 RSP 具有误导性和过度销售</h3><p><strong>误导点</strong>：</p><ul><li><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>人择 RSP</u></a>将错位风险标记为“投机”，且没有任何理由。</li><li>该框架意味着长时间不扩展不是一个选择。</li><li> RSP 对我们所了解的风险状况提出了极具误导性的观点。</li></ul><p><strong>超售和交付不足</strong></p><ul><li>RSP 允许在一个大框架内做出较弱的承诺，而<i>理论上</i>这些承诺可能是强有力的。</li><li>没有人提供证据表明在我们谈论的时间范围内（几年）对框架进行了实质性改进，这就是 RSP 的全部内容。</li><li> “负责任的扩展”具有误导性；如果我们不能排除 1% 的灭绝风险（ASL-3 就是这种情况），“灾难性扩展”可能更合适。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>阅读更多。</u></a></p><h3> RSP 绝望了吗？</h3><p>本节解释了为什么使用 RSP 作为框架是不够的，即使与从现有的人工智能风险管理框架和实践开始相比，例如：</p><ul><li> <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>受 NIST 启发的基础模型风险管理框架</u></a></li><li><a href="https://www.iso.org/standard/77304.html"><u>ISO/IEC 23894</u></a></li><li> <a href="https://arxiv.org/abs/2307.08823"><u>Koessler 等人解释了实践。 (2023)</u></a></li></ul><p> RSP 所做的大量工作将有助于详细说明这些框架，但 RSP 的核心基本原则是错误的，因此应该被放弃。<br></p><p><strong>如何前进？</strong></p><p>务实地说，我建议进行一系列改变，使 RSP 更有可能对安全有所帮助。为了减轻政策和沟通的不良影响：</p><ul><li><strong>将“负责任的扩展政策”重命名</strong>为“自愿安全承诺”</li><li><strong>明确什么是 RSP，什么不是</strong>：我建议任何 RSP 出版物都以“RSP 是在赛车环境中单方面做出的自愿承诺”开头。因此，我们认为它们有助于提高安全性。我们无法证明它们足以管理灾难性风险，因此不应将它们<strong>作为公共政策实施</strong>。”</li><li><strong>推动可靠的风险管理公共政策：</strong>我建议任何 RSP 文件都指向另一份文件并表示“以下是我们认为足以管理风险的政策。监管应该落实这些。”<br></li></ul><p>要查看已定义的 RSP 是否与合理的风险水平一致：</p><ul><li>组建具有代表性的风险管理专家、人工智能风险专家和预测专家团队。</li><li>对于分类为 ASL-3 的系统，估计出现以下问题的可能性：<ul><li> ASL-3 系统每年被{中国；盗窃的可能性有多大？俄罗斯;北朝鲜;沙特阿拉伯;伊朗}？</li><li>在此前提下，泄漏的可能性有多大？它可以用来制造生物武器吗？它可以用于具有大规模影响的网络攻击吗？</li><li>在 ASL-4 评估触发之前，每年发生灾难性事故的可能性是多少？</li><li> ASL-3 系统每年发生误用灾难性风险的几率是多少？</li></ul></li><li>公开分享方法和结果。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>阅读更多。</u></a></p><h2><strong>元</strong></h2><p><br><i><strong>认知状况</strong></i>：我已经研究各种危险行业的安全标准大约 4-6 个月了，重点是核安全。我与来自其他领域（例如医疗设备、汽车）的风险管理专家一起在标准化机构（CEN-CENELEC 和 ISO/IEC）从事人工智能标准化工作大约 10 个月。在这种背景下，我阅读了现有的人工智能 ISO/IEC SC42 和 JTC21 标准，并开始尝试将它们应用于法学硕士并加以完善。关于 RSP，我花了几十个小时阅读文档并与相关人员和周围的人讨论这些文档。</p><p><i><strong>语气</strong></i>：我对这件作品的慈善程度犹豫不决。一方面，我认为 RSP 是一个相当有毒的模因（见第 4 节），它被仓促地进行了全球推广，而对其构建方式没有太多认识上的谦逊，而且据我所知，没有人太关心现有的风险管理方法。从这个意义上说，我认为在目前的框架下应该强烈反对它。<br>另一方面，尝试不使用负面含义并冷静地讨论以认识论和建设性地前进通常是件好事。<br>我的目标是介于两者之间，我确实以强烈的负面含义强调了我认为最糟糕的部分，同时在许多部分中注重保持建设性并关注对象级别。<br>这种混合物可能会让我陷入恐怖谷，我很想收到对此的反馈。<br></p><h1>第 1 节：人工智能风险管理的一般考虑</h1><p>风险管理就是证明<strong>风险低于可接受的水平</strong>。<strong>证明不存在风险</strong>比证明某些风险已得到处理要困难得多。更具体地说，<strong>您对系统的了解越少</strong>，排除风险就越<strong>困难</strong>。</p><p>举个例子：为什么我们可以更容易地证明核电站引发大规模灾难的几率<a href="https://world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>&lt;十万分之一</u></a>，而GPT-5却不能呢？很大程度上是因为我们现在了解核电站及其许多风险。我们知道它们是如何工作的，以及它们可能失败的方式。他们已经将非常不稳定的反应（核裂变）变成了可控制的反应（使用核反应堆）。因此，我们对核电站的不确定性比 GPT-5 的不确定性要小得多。</p><p>一个推论是，在<strong>风险管理中，</strong><a href="https://en.wikipedia.org/wiki/Risk_management"><strong><u>不确定性是一个敌人</u></strong></a>。说“我们不知道”是失败的。自信地排除风险需要对系统有深入的了解，并以非常高的信心反驳重大担忧。需要明确的是：<strong>这很难</strong>。特别是当系统的操作域是“世界”时。这就是为什么安全性要求很高。但当数十亿人的生命受到威胁时，这是降低安全标准的好理由吗？显然不是。</p><p>人们可以合理地说：等等，但目前看不到任何风险，举证责任在于那些声称它是危险的人。证据在哪里？</p><p>嗯，有很多：</p><ul><li> Bing 在经过<a href="https://futurism.com/the-byte/microsoft-bing-test-india"><u>数月的 Beta 测试</u></a>后部署时对用户构成威胁<u>。</u></li><li>提供商无法避免越狱或确保<a href="https://arxiv.org/pdf/2307.15043.pdf"><u>文本</u></a><a href="https://arxiv.org/abs/2306.13213"><u>或图像的</u></a>稳健性。</li><li>模型显示出<a href="https://aclanthology.org/2023.findings-acl.847/"><u>令人担忧的缩放特性</u></a>。</li></ul><p>人们可以合理地说：不，但这不是灾难性的，也不是什么大不了的事。与此相反，我想引用著名物理学家 R. Feynman 在火箭安全这个比人工智能安全标准高得多的领域对挑战者号灾难的反思：</p><ul><li> “侵蚀和窜气不是设计所期望的。它们是<strong>在警告出现问题</strong>。设备未按预期运行，因此存在以这种意想不到且未完全理解的方式以更大偏差运行的危险。<strong>以前这种危险没有导致灾难，</strong><strong>但并不能保证下一次也不会导致灾难，除非我们完全理解这一点</strong>。”</li></ul><p>人们最终可以希望我们能够理解我们系统过去的失败。不幸的是，我们不这样做。我们不仅不理解他们的失败，而且不理解他们的失败。我们<strong>一开始就不明白它们是如何以及为什么起作用的</strong>。</p><p>那么我们该如何应对风险呢？</p><p>风险管理提出了我将在下面描述的几个步骤方法。大多数行业都按照这些思路实施流程，但根据监管水平和风险类型的不同，会有一些细微的差别以及不同程度的严格性和深度。我将在表格中列出一些表格，您可以在<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.pkk9et1tbulf"><u>附件</u></a>中查看。<br></p><h1>第 2 部分：标准风险管理是什么样的</h1><p>以下是风险管理流程核心步骤的描述。不同框架的名称各不相同，但其要点都包含在此处，并且通常跨框架共享。</p><ol><li><strong>定义风险偏好和风险承受能力</strong>：定义您的项目愿意承担的风险量，无论是可能性还是严重性。可能性可以是定性尺度，例如指跨越数量级的范围。</li><li><strong>风险识别</strong>：写下您的项目可能产生的所有威胁和风险，例如培训和部署前沿人工智能系统。</li><li><strong>风险评估</strong>：通过确定风险发生的可能性及其严重性来评估每个风险。根据您的风险偏好和风险承受能力检查这些估计。</li><li><strong>风险处理</strong>：实施变革以减少每个风险的影响，直到这些风险满足您的风险偏好和风险承受能力。</li><li><strong>监控</strong>：在项目执行过程中，监控风险水平，检查风险是否确实全部被覆盖。</li><li><strong>报告</strong>：向利益相关者，特别是那些受风险影响的人传达计划及其有效性。</li></ol><p><br></p><p>这些相当通用的步骤有什么意义？为什么它有助于人工智能安全？</p><p> (1)<strong>风险阈值的定义</strong>是关键 1) 使<strong>承诺可证伪</strong>并避免目标移动<strong>&nbsp;</strong> 2) 当其他利益相关者因其活动而产生风险时，让风险产生组织承担责任。如果一项活动将人们的生命置于危险之中，那么重要的是让他们知道有多少危险以及其好处和目标是什么。</p><ol><li>例如，根据<a href="https://www.nrc.gov/docs/ML0717/ML071770230.pdf"><u>核管理委员会</u></a>的定义，核能的情况如下： </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/j1dfvyfa5b9n6lvgy32u"></p><p> 2. 加州大学伯克利分校长期网络安全中心受 NIST 启发，与 D. Hendrycks 共同编写的通用人工智能系统风险管理概要文件提供了一些关于<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.2k6kkwym97fb"><u>如何定义图 1 中的风险管理概要的想法。</u></a></p><p> （2）通过系统方法<strong>识别风险</strong><strong>&nbsp;</strong>尝试尽可能接近全面覆盖风险的关键是。正如我们之前所说，在风险管理中，不确定性就是一种失败，而大幅减少不确定性的核心方法是尽可能全面。</p><ol><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 4 节中找到一些。 2023年</u></a>。</li></ol><p> (3) 通过定性和定量的方式进行<strong>风险评估</strong>，使我们能够实际估计我们所拥有的不确定性。然后，关键是确定安全措施的优先顺序，并决定将项目保持在当前形式还是对其进行修改是否合理。</p><ol><li>易于修改并显着改变风险状况的变量的一个例子是人工智能系统可以访问的一组执行器。系统是否具有编码终端、互联网接入或实例化其他人工智能系统的可能性都是显着增加其操作集以及相应风险的变量。</li><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 5 节中找到一些。 2023年</u></a>。涉及专家预测的方法（例如概率风险评估或德尔菲技术）已经存在，并且可以应用于人工智能安全。即使在以下情况下也可以应用它们：<ol><li>风险较低（例如核管理委员会要求核安全<a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>估计概率低于1/10 000</u></a> ）。 </li></ol></li></ol><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>美国核管理委员会 (NRC) 规定反应堆设计必须满足理论上万分之一的堆芯损坏频率，但现代设计超过了这一要求。美国的公用事业需求为十万分之一，目前运行最好的工厂约为百万分之一，而未来十年可能建成的工厂几乎为千万分之一。</p><p> <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><i><u>世界核协会，2022</u></i></a></p></td></tr><tr><td></td></tr></tbody></table></figure><p> b.正如 20 世纪 70 年代核安全领域的情况一样，事件的发展过程非常复杂且容易被误解。已经做了，也正是通过做的迭代实践，一个行业才能变得更加负责任和谨慎。阅读<i>《足够安全？》</i>的书评一本关于核安全中使用的定量风险评估方法的历史的书，有一种似曾相识的感觉： </p><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>如果核电站以某种可测量的速度发生故障，该行业可以利用该数据来预测下一次故障。但如果工厂没有发生故障，那么<strong>就很难讨论真正的故障率</strong>可能是多少。这些工厂是否可能每十年就会发生一次故障？一个世纪一次？千年一次？在缺乏共享数据的情况下，科学家、工业界和公众都可以自由地相信他们想要的东西。</p><p> <a href="https://www.astralcodexten.com/p/your-book-review-safe-enough"><i><u>《星体法典十》，2023 年</u></i></a><i>，描述了核安全中概率风险评估的起源。</i></p></td></tr></tbody></table></figure><p><br><br></p><p> (4)<strong>风险处理</strong>是对风险评估的反应，必须持续进行，直到达到定义的风险阈值。这里的干预空间非常大，比通常假设的还要大。更好地了解一个系统，通过降低其通用性来缩小其操作范围，增加监督力度，改善安全文化：所有这些都是可用于满足阈值的广泛干预措施的一部分。如果对系统进行了重大更改，则治疗和评估之间可能会出现循环。</p><p> (5)<strong>监控</strong>是确保风险评估保持有效且没有遗漏重大事项的部分。这就是行为模型评估最有用的地方，即确保您跟踪已识别的风险。良好的评估将映射到预先定义的风险偏好（例如，1%的可能性>; 1%的死亡），并将涵盖通过系统风险识别提出的所有风险。</p><p> (6)<strong>报告</strong>是确保向所有相关利益相关者提供正确信息的部分。例如，应该向那些因活动而产生风险的人提供有关他们所面临的风险程度的信息。</p><p>现在我们已经快速概述了标准风险管理以及它为何与人工智能安全相关，接下来我们来谈谈 RSP 与之相比如何。</p><h1>第 3 节：RSP 与标准风险管理</h1><p>绝对应该遵循 RSP 的一些基本原则。有更好的方法来追求这些原则，这些原则<strong>已经存在</strong>于<strong>风险管理</strong>中，并且恰好是大多数其他危险行业和领域所做的。举两个例子来说明这种良好的基本原则：</p><ul><li>规定公司必须达到的安全要求，否则公司就无法继续运营。</li><li>建立严格的评估和衡量能力，以更好地了解系统是否良好；这绝对应该成为风险管理框架的一部分，但可能作为一种风险监控技术，而不是作为风险评估的替代品。</li></ul><p>下面，我将讨论为什么 RSP 是一些良好风险管理原则的糟糕实施，以及为什么这使得 RSP 框架不足以管理风险。</p><h2>直接比较</h2><p>让我们深入研究这两种方法之间的更具体的比较。国际标准组织（ISO）制定了两项与人工智能安全相关的风险管理标准，但并未重点关注：</p><ul><li> ISO 31000 提供通用风险管理指南。</li><li> ISO/IEC 23894，31000 的改编版本，更加针对 AI</li></ul><p>需要明确的是，这些标准还不够。大多数欧盟标准化参与者认为它们很弱，而医疗器械行业等其他行业的风险管理专家则认为它们极其弱。为通用 AI 系统完善此类框架需要做大量工作（请参阅<a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>此处 T. Barrett 的第一次迭代</u></a>，以及<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.camj9ith1s95"><u>此处如何映射到 ISO/IEC 23894 的</u></a>表格），但这些提供了基本步骤正如我们上面所解释的，这些原则是充分风险管理的核心。</p><p>在下表中，我从 ARC Evals 的 RSP 原则的简短版本开始，并尝试匹配最对应的 ISO/IEC 31000 版本。然后我会解释 RSP 版本中缺少的内容。注意：</p><ul><li>我只写了简短的 RSP 原理，但考虑了<a href="https://evals.alignment.org/rsp-key-components/"><u>长版本</u></a>。</li><li> ISO/IEC 31000 中有许多步骤此处未列出。</li><li>我将包含 RSP 版本的 ISO/IEC 版本<i><strong>用斜体表示</strong></i>。</li></ul><p>表格版本： </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> RSP 版本（短）</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> ISO/IEC 31000 版本</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">ISO 如何相对 RSP 进行改进</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>限制</strong>：关于危险能力的哪些具体观察表明继续扩展是（或强烈可能）不安全的？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>定义风险标准</strong>：组织应指定相对于目标可能或可能不承担的风险的数量和类型。</p><p><br></p><p>它还应该<i>定义评估风险重要性和支持决策过程的标准</i>。</p><p><br></p><p>风险标准应与风险管理框架保持一致，并根据所考虑活动的具体目的和范围进行定制。</p><p> [...]</p><p>定义标准时应考虑组织的义务和利益相关者的观点。</p><p> [...]</p><p>要设定风险标准，应考虑以下因素：</p><p> ——可能影响结果和目标（有形和无形）的不确定性的性质和类型；</p><p> ——如何定义和衡量后果（积极和消极）和可能性；</p><p> ——时间相关因素；</p><p> ——测量使用的一致性；</p><p> ——如何确定风险水平；</p><p> ——如何考虑多种风险的组合和顺序；</p><p> ——组织的能力。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> RSP 并没有争论为什么通过评估的系统是安全的。这是缺乏具有可能性尺度的<strong>风险阈值</strong>的下游。例如，Anthropic RSP 也将意外风险视为“推测性”和“不太可能”，但没有太多深度，没有对其系统有太多了解，也没有表达“不太可能”的含义。</p><p><br></p><p>另一方面，ISO标准要求组织定义风险阈值，并强调需要将风险管理与组织目标（即构建人类级别的人工智能）相匹配。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>保护</strong>：当前保护措施的哪些方面对于遏制灾难性风险是必要的？</p><p><br><br><br><br><br><br><br></p><p><strong>评估</strong>：及时捕捉危险能力极限预警信号的程序是什么？</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险分析</strong>：风险分析的目的是了解风险的性质及其特征，包括（在适当情况下）风险级别。风险分析涉及对不确定性、风险来源、后果、可能性、事件、情景、<i>控制及其有效性</i>的详细考虑。</p><p><br></p><p><strong>风险评估</strong>：风险评估的目的是支持决策。风险评估涉及将风险分析的结果与既定的风险标准进行比较，以确定哪些地方需要采取额外的行动。这可能导致做出以下决定：</p><p> ——不做任何进一步的事情；</p><p> ——考虑风险处理方案；</p><p> ——进行进一步分析以<i>更好地了解风险</i>；</p><p> ——维持现有的控制措施；</p><p> ——重新考虑目标。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO 提出了比 RSP 更全面的程序，但它并没有真正分析风险级别或有系统的风险识别程序。</p><p><br></p><p>直接后果是 RSP 很可能在不知不觉中导致高风险。</p><p><br></p><p>例如，RSP 似乎没有将能力交互视为主要风险来源。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>回应</strong>：如果危险能力超过极限，且无法快速提升防护能力，AI开发者是否准备好暂停进一步的能力提升，直到防护措施得到充分改善，并足够谨慎地对待任何危险模型？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险处理计划</strong>：风险处理方案不一定相互排斥或在所有情况下都适用。处理风险的选项可能涉及以下一项或多项：</p><p> ——<i>通过决定不开始或不继续引起风险的活动来避免风险</i>； ——为了寻求机会而承担或增加风险； ——<i>消除风险源</i>；</p><p> ——改变可能性；</p><p> ——改变后果； ——分担风险（例如通过合同、购买保险）；</p><p> - 通过明智的决策保留风险</p><p><br></p><p>处理计划应与适当的利益相关者协商，纳入组织的管理计划和流程。</p><p>治疗计划中提供的信息应包括：</p><p> ——选择治疗方案的理由，包括预期获得的益处；</p><p> ——负责批准和实施该计划的人员；</p><p> ——提议的行动；</p><p> ——所需的资源，包括意外事件；</p><p> ——绩效衡量标准；</p><p> ——限制因素；</p><p> ——所需的报告和监测；</p><p> ——预计何时采取并完成行动</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ISO 通过风险阈值的定义，确保风险缓解措施将风险降低到可接受的水平。 RSP 缺乏风险阈值，使得风险缓解措施缺乏依据。</p><p><br><br><br></p><p><strong>示例</strong>：Anthropic 定义的 ASL-3 风险缓解措施（即接近灾难性危险）意味着很可能被俄罗斯或中国窃取（我不知道有哪个 RSP 人士否认这一点）。下游有哪些风险？希望这些国家能够保证重物的安全，不要造成太大的损失。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>问责制</strong>：AI开发者如何确保RSP的承诺按预期执行；主要利益相关者可以验证这种情况是否正在发生（或者如果没有发生请注意）；有机会进行第三方批评； RSP 本身的变化不会以仓促或不透明的方式发生？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>监控和审查</strong>：监控和审查的目的是确保和提高流程设计、实施和结果的质量和有效性。对风险管理流程及其结果的持续监控和定期审查应成为风险管理流程的有计划的一部分，并明确界定职责。 [...] 监测和审查的结果应纳入组织的绩效管理、衡量和报告活动的整个过程。</p><p><br></p><p><strong>记录和报告</strong>：风险管理过程及其结果应通过适当的机制记录和报告。记录和报告的目的是：</p><p> ——在整个组织内传达风险管理活动和结果；</p><p> ——为决策提供信息；</p><p> ——改进风险管理活动；</p><p> ——协助与利益相关者的互动，包括那些对风险管理活动负有责任和责任的人。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>这些零件具有相似的组件。</p><p><br></p><p>但 ISO 鼓励向受风险影响的人报告风险管理的结果，这似乎是灾难性风险的最低限度。</p><p><br></p><p> Anthropic 的 RSP 建议在部署后这样做，这是一个良好的问责制开始，但一旦承担了很多灾难性风险，这种情况仍然会发生。</p></td></tr></tbody></table></figure><h2> RSP 的优先风险管理缺点</h2><p>以下列出了 RSP 最大的直接风险管理失败：</p><ol><li>使用未明确的风险阈值定义且未量化风险</li><li>声称“负责任的扩展”，但没有包括全面评估的流程</li><li>包括废除承诺的白衣骑士条款</li></ol><p>1.<strong>使用不明确的风险阈值定义且未量化风险</strong>。 RSPs don&#39;t define risk thresholds in terms of <strong>likelihood</strong> . Instead, they focus straight away on symptoms of risks (certain capabilities that an evaluation is testing is one way a risk could instantiate) rather than the risk itself (the model helping in any possible way to build bioweapons). This makes it hard to verify whether safety requirements have been met and argue whether the thresholds are reasonable. Why is it an issue?</p><ul><li> It leaves wiggle room making it very hard to keep the organization accountable. If a lab said something was “unlikely” and it still happened, did it do bad risk management or did it get <i><strong>very</strong></i> unlucky? Well, we don&#39;t know.</li><li> <strong>Example</strong> (from Anthropic RSP): “A model in the ASL-3 category does not itself present a threat of containment breach due to autonomous self-replication, because it is both unlikely to be able to persist in the real world, and unlikely to overcome even simple security measures intended to prevent it from stealing its own weights.” It makes a huge difference for catastrophic risks whether “unlikely” means 1/10, 1/100 or 1/1000. With our degree of understanding of systems, I don&#39;t think Anthropic staff would be able to demonstrate it&#39;s lower than 1/1000. And 1/100 or 1/10 are alarmingly high.</li><li> It doesn&#39;t explain why the monitoring technique, ie the <strong>evaluations,</strong> are the right ones to avoid risks. The RSPs do a good first step which is to identify some things that could be risky.<ul><li> <strong>Example</strong> (from ARC RSP <a href="https://evals.alignment.org/rsp-key-components/"><u>presentation</u></a> ): “ <i>Bioweapons development: the ability to walk step-by-step through developing a bioweapon, such that the majority of people with any life sciences degree (using the AI) could be comparably effective at bioweapon development to what people with specialized PhD&#39;s (without AIs) are currently capable of.”</i></li></ul></li></ul><p> By describing neither quantitatively nor qualitatively why it is risky, expressed in terms of risk criteria (eg 0.1% chance of killing >;1% of humans) it doesn&#39;t do the most important step to demonstrate that below this threshold, things are safe and acceptable. For instance, in the example above, why is “ <strong>the majority of people with any life sciences degree</strong> ” relevant? Would it be fine if only 10% of this population was now able to create a bioweapon?也许，也许不是。 But without clear criteria, you can&#39;t tell.</p><p> 2. Claiming “ <strong>responsible</strong> <strong>scaling</strong> ” without including a process to make the <strong>assessment comprehensive</strong> . When you look at nuclear accidents, what&#39;s striking is how unexpected failures are. Fukushima is an example where <a href="https://en.wikipedia.org/wiki/Fukushima_nuclear_accident#Accident"><u>everything goes wrong at the same time.</u></a> Chernobyl is an example where engineers didn&#39;t think that the accident that happened <a href="https://www.reddit.com/r/chernobyl/comments/mflxy2/why_did_the_engineers_believe_it_was_impossible/#:~:text=Specifically%20they%20believed%20that%20the,%2Fvoid%20effect%20of%20reactivity%22."><u>was possible</u></a> (someone claims that they were so surprised that engineers actually ran another real-world test of the failure that happened at Chernobyl because they doubted too much it could happen).</p><p> Without a more comprehensive process to identify risks and compare their likelihood and severity against pre-defined risk thresholds, there&#39;s very little chance that RSPs will be enough. When I asked some forecasters and AI safety researchers around me, the estimates of the annual probability of extinction caused by an ASL-3 system (defined in Anthropic RSPs) were several times above 1%, up to 5% conditioning on our current ability to measure capabilities (and not an idealized world where we know very well how to measure those).</p><p> 3. Including the <strong>white knight clause</strong> that kills commitments.</p><p> One of the proposals that striked me the most when reading RSPs is the insertion of what deserves the name of the <strong>white knight clause</strong> .</p><ul><li> In short, if you&#39;re developing a dangerous AI system because you&#39;re a good company, and you&#39;re worried that other bad companies bring too many risks, then you can race forward to prevent that from happening.</li><li> If you&#39;re invoking the white knight clause and increase catastrophic risks, you still have to justify it to your board, the employees and state authorities. The latter provides a minimal form of accountability. But if we&#39;re in a situation where the state is sufficiently asleep to need an AGI company to play the role of the white knight in the first place, it doesn&#39;t seem like it would deter much.</li></ul><p> I believe that there are companies that are safer than others. But that&#39;s not the right question. The right question is: is there any company which wouldn&#39;t consider itself as a bad guy? And the answer is: no. OpenAI, Anthropic and DeepMind would all argue about the importance of being at the frontier to solve alignment. Meta and Mistral would argue that it&#39;s key to democratize AI to not prevent power centralization. And so on and so forth.<br><br> This clause is effectively killing commitments. I&#39;m glad that Anthropic included only a weakened version of it in its own RSP but I&#39;m very concerned that ARC is pitching it as an option. It&#39;s not the role of a company to decide whether it&#39;s fine or not to increase catastrophic risks for society as a whole.</p><h1> Section 4: Why RSPs Are Misleading and Overselling</h1><h2> Misleading</h2><p> Beyond the designation of misalignment risks as “speculative” on Anthropic RSPs and a three line argument for why it&#39;s unlikely among next generation systems, there are several extremely misleading aspects of RSPs:</p><ol><li> It&#39;s called “responsible scaling”. In its own name, it conveys the idea that not further scaling those systems as a risk mitigation measure is not an option.</li><li> It conveys a very overconfident picture of the risk landscape.<ol><li> Anthropic writes in the introduction of its RSP “The basic idea is to require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”. They already defined sufficient protective measures for ASL-3 systems that potentially have basic bioweapons crafting abilities. At the same time they write that they are in the process of actually measuring the risks related to biosecurity: “Our first area of effort is in evaluating biological risks, where we will determine threat models and capabilities”.  I&#39;m really glad they&#39;re running this effort, but what if this outputted an alarming number? Is there a world where the number output makes them stop 2 years and dismiss the previous ASL-3 version rather than scaling responsibly?</li><li> Without arguing why the graph would look like that, ARC published a graph like this one. Many in the AI safety field don&#39;t expect it to go that way, and “Safe region” oversells what RSP does. I, along with others, expect the LLM graph to reach a level of risks that is simply not manageable in the foreseeable future. Without quantitative measure of the risks we&#39;re trying to prevent, it&#39;s also not serious to claim to have reached “sufficient protective measures”. <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/moqd7abbdi5ohnv3efy2"></li></ol></li></ol><p><br></p><p> If you want to read more on that, you can read <a href="https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=FtbzhGk5oPT3dyHLi"><u>that</u></a> .</p><h2> Overselling, underdelivering</h2><p> The RSP framework has some nice characteristics. But first, these are all already covered, in more detail, by existing risk assessment frameworks that no AI lab has implemented. And second, the coexistence of ARC&#39;s RSP framework with the specific RSPs labs implementations allows slack for <strong>commitments that are weak</strong> within a <strong>framework that would in theory allow ambitious commitments</strong> . It leads to many arguments of the form:</p><ul><li> “That&#39;s the V1. We&#39;ll raise ambition over time”. I&#39;d like to see evidence of that happening over a 5 year timeframe, in any field or industry. I can think of fields, like aviation where it happened over the course of decades, crashes after crashes. But if it&#39;s relying on expectations that there will be large scale accidents, then it should be clear. If it&#39;s relying on the assumption that timelines are long, it should be explicit.</li><li> “It&#39;s voluntary, we can&#39;t expect too much and it&#39;s way better than what&#39;s existing”. Sure, but if the level of catastrophic risks is 1% (which several AI risk experts I&#39;ve talked to believe to be the case for ASL-3 systems) and that it gives the impression that risks are covered, then the name “responsible scaling” is heavily misleading policymakers. The adequate name for 1% catastrophic risks would be catastrophic scaling, which is less rosy.</li></ul><p> I also feel like it leads to many disagreements that all hinge on: do we expect labs to implement ambitious RSPs?</p><p> And my answer is: given their track record, no. Not without government intervention. Which brings us to the question: “what&#39;s the effect of RSPs on policy and would it be good if governments implemented those”. My answer to that is: An extremely ambitious version yes; the misleading version, no. No, mostly because of the short time we have before we see heightened levels of risks, which gives us very little time to update regulations, which is a core assumption on which RSPs are relying without providing evidence of being realistic.</p><p><br> I expect labs to push hard for the misleading version, on the basis that pausing is unrealistic and would be bad for innovation or for international race. Policymakers will have a hard time distinguishing the risk levels between the two because it hinges on details and aren&#39;t quantified in RSPs. They are likely to buy the bad misleading version because it&#39;s essentially selling that there&#39;s <strong>no trade-off between capabilities and safety</strong> . That would effectively enforce a trajectory with unprecedented levels of catastrophic risks.</p><h1> Section 5: Are RSPs Hopeless?</h1><p>嗯，是的，也不是。</p><ul><li> Yes, in that most of the pretty intuitive and good ideas underlying the framework are weak or incomplete versions of traditional risk management, with some core pieces missing. Given that, it seems more reasonable to just start from an existing risk management piece as a core framework. ISO/IEC 23894 or the NIST-inspired <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>AI Risk Management Standards Profile for Foundation Models</u></a> would be pretty solid starting points.</li><li> No in that inside the RSPs, there are many contributions that should be part of an AI risk management framework and that would help make existing risk management frameworks more specific. I will certainly not be comprehensive, but some of the important contributions are:<ul><li> Anthropic&#39;s RSP fleshes out a wide range of relevant considerations and risk treatment measures</li><li> ARC provides:<ul><li> technical benchmarks and proposed operationalizations of certain types of risks that are key</li><li> definitions of safety margins for known unknowns</li><li> threat modelling</li><li> low-level operationalization of some important commitments</li></ul></li></ul></li></ul><p> In the short-run, given that it seems that RSPs have started being pushed at the UK Summit and various other places, I&#39;ll discuss what changes could make RSPs beneficial without locking in regulation a bad framework.</p><h2> How to Move Forward?</h2><p> <u>Mitigating nefarious effects:</u></p><ol><li> <strong>Make the name less misleading</strong> : If instead of calling it “responsible scaling”, one called it “Voluntary safety commitments” or another name that:<ol><li> Doesn&#39;t <strong>determine the output of the safety test before having run it</strong> (ie scaling)</li><li> Unambiguously signals that it&#39;s not supposed to be sufficient or to be a good basis for regulation.</li></ol></li><li> <strong>Be clear on what RSPs are and what they aren&#39;t</strong> . I suggest adding the following clarifications regarding what the goals and expected effects of RSPs are:<ol><li> <strong>What RSPs are</strong> : “a company that would take too strong unilateral commitments would harm significantly its chances of succeeding in the AI race. Hence, this framework is aiming at proposing what we expect to be the best marginal measures that a company can unilaterally take to improve its safety without any coordination.”. I would also include a statement on the level of risks like: “We&#39;re not able to show that this is sufficient to decrease catastrophic risks to reasonable levels, and it is probably not.”,  “we don&#39;t know if it&#39;s sufficient to decrease catastrophic risks below reasonable levels”, or &quot;even barring coordinated industry-wide standards or government intervention, RSPs are only a second- (or third-) best option&quot;.</li><li> <strong>What RSPs aren&#39;t:</strong> Write very early in the post a disclaimer saying “THIS IS NOT WHAT WE RECOMMEND FOR POLICY”. Or alternatively, point to another doc stating what would be the measures that would be sufficient to maintain the risk below sufficient levels: “Here are the measures we think would be sufficient to mitigate catastrophic risks below acceptable levels.” to which you could add “We encourage laboratories to make a conditional commitment of the form: “if all other laboratories beyond a certain size[to be refined] committed to follow those safety measures with a reliable enforcement mechanism and the approval of the government regarding this exceptional violation of antitrust laws, we would commit to follow those safety measures.”</li></ol></li><li> <strong>Push for risk management in policy:</strong><ol><li> Standard risk management for what is acknowledged to be a world-shaping technology is a fairly reasonable ask. In fact, it is an ask that I&#39;ve noticed in my interactions with other AI crowds has the benefit of allowing coalition-building efforts because everyone can easily agree on “measure the risks, deal with them, and make the residual level of risks and the methodology public”.<br></li></ol></li></ol><p> <u>Checking whether RSPs manage risks adequately:</u></p><p> At a risk management level, if one wanted to demonstrate that RSPs like Anthropic&#39;s one are actually doing what they claim to do (ie “require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”), a simple way to do so would be to run a risk assessment on ASL-3 systems with a set of forecasters, risk management experts and AI risk experts that are representative of views on AI risks and that have been selected by an independent body free of any conflict of interest.</p><p> I think that a solid baseline would be to predict the chances of various intermediary and final outcomes related to the risks of such systems:</p><ol><li> ASL-3 系统每年被{中国；盗窃的可能性有多大？俄罗斯;北朝鲜;沙特阿拉伯;伊朗}？</li><li>在此前提下，泄漏的可能性有多大？ it being used to build bioweapons? it being used for cyber offence with large-scale effects?</li><li> What are the chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> ASL-3 系统每年发生误用灾难性风险的几率是多少？</li></ol><p> It might not be too far from what Anthropic seems to be willing to do internally, but doing it with a publicly available methodology, and staff without self-selection or conflict of interests makes a big difference. Answers to questions 1) and 2) could raise risks so the output should be communicated to a few relevant actors but could potentially be kept private.</p><p> If anyone has the will but doesn&#39;t have the time or resources to do it, I&#39;m working with some forecasters and AI experts that could probably make it happen. Insider info would be helpful but mostly what would be needed from the organization is some clarifications on certain points to correctly assess the capabilities of the system and some info about organizational procedures.</p><h1> Acknowledgments</h1><p> I want to thank Eli Lifland, Henry Papadatos and my other <a href="https://www.navigatingrisks.ai/"><u>NAIR</u></a> colleague, Olivia Jimenez, Akash Wasil, Mikhail Samin, Jack Clark, and other anonymous reviewers for their feedback and comments. Their help doesn&#39;t mean that they endorse the piece. All mistakes are mine.</p><h1> Annex</h1><h2> Comparative Analysis of Standards</h2><p> This (cropped) table shows the process of various standards for the 3 steps of risk management. As you can see, there are some differences but every standard seems to follow a similar structure.</p><p> From <a href="https://www.zotero.org/google-docs/?a64rn3">(Raz &amp; Hillson, 2005)</a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/xncrzvtrz7ktyigutrtw"></p><p> Here is a comparable table for the last two parts of risk management. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/qjqic1bka0uazyvshgre"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong<guid ispermalink="false"> 9nEBWxjAHSu3ncr6v</guid><dc:creator><![CDATA[simeon_c]]></dc:creator><pubDate> Wed, 25 Oct 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p> AI used to be a science. In the old days (back when AI didn&#39;t work very well), people were attempting to develop a working theory of cognition.</p><p> Those scientists didn&#39;t succeed, and those days are behind us. For most people working in AI today and dividing up their work hours between tasks, gone is the ambition to understand minds. People working on mechanistic interpretability (and others attempting to build an empirical understanding of modern AIs) are laying an important foundation stone that could play a role in a future science of artificial minds, but on the whole, modern AI engineering is simply about constructing enormous networks of neurons and training them on enormous amounts of data, not about comprehending minds.</p><p> The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>bitter lesson</u></a> has been taken to heart, by those at the forefront of the field; and although this lesson doesn&#39;t teach us that there&#39;s <i>nothing to learn</i> about how AI minds solve problems internally, it suggests that <i>the fastest path to producing more powerful systems</i> is likely to continue to be one that doesn&#39;t shed much light on how those systems work.</p><p> Absent some sort of “science of artificial minds”, however, humanity&#39;s prospects for aligning smarter-than-human AI seem to me to be quite dim.</p><p> Viewing Earth&#39;s current situation through that lens, I see three major hurdles:</p><ol><li> Most research that helps one <a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>point AIs</u></a> , probably also helps one make more capable AIs. A “science of AI” would probably increase the power of AI far sooner than it allows us to solve alignment.</li><li> In a world <i>without</i> a mature science of AI, building a bureaucracy that reliably distinguishes real solutions from fake ones is prohibitively difficult.</li><li> Fundamentally, for at least some aspects of system design, we&#39;ll need to rely on a theory of cognition working on the first high-stakes real-world attempt.</li></ol><p> I&#39;ll go into more detail on these three points below. First, though, some background:</p><p></p><h2>背景</h2><p>By the time AIs are powerful enough to endanger the world at large, I expect AIs to do something akin to “caring about outcomes”, at least from a behaviorist perspective (making no claim about whether it internally implements that behavior in a humanly recognizable manner).</p><p> Roughly, this is because people are <i>trying</i> to make AIs that can steer the future into narrow bands (like “there&#39;s a cancer cure printed on this piece of paper”) over long time-horizons, and caring about outcomes (in the behaviorist sense) is the flip side of the same coin as steering the future into narrow bands, at least when the world is sufficiently large and full of curveballs.</p><p> I expect the outcomes that the AI “cares about” to, by default, not include anything good (like fun, love, art, beauty, or the light of consciousness) — nothing good by present-day human standards, and nothing good by broad <a href="https://arbital.com/p/value_cosmopolitan/"><u>cosmopolitan standards</u></a> either. Roughly speaking, this is because when you grow minds, they don&#39;t care about what you ask them to care about and they don&#39;t care about what you train them to care about; instead, I expect them to care about a bunch of correlates of the training signal in weird and specific ways.</p><p> (Similar to how the human genome was naturally selected for inclusive genetic fitness, but the resultant humans didn&#39;t end up with a preference for “whatever food they model as useful for inclusive genetic fitness”. Instead, humans wound up internalizing a huge and complex set of preferences for &quot;tasty&quot; foods, laden with complications like “ice cream is good when it&#39;s frozen but not when it&#39;s melted”.)</p><p> Separately, I think that most complicated processes work for reasons that are fascinating, <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>complex</u></a> , and <a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>kinda horrifying</u></a> when you look at them closely.</p><p> It&#39;s easy to think that a bureaucratic process is competent until you look at the gears and see the specific ongoing office dramas and politicking between all the vice-presidents or whatever. It&#39;s easy to think that a codebase is running smoothly until you read the code and start to understand all the decades-old hacks and coincidences that make it run. It&#39;s easy to think that biology is a beautiful feat of engineering until you look closely and find that the eyeballs are <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>installed backwards</u></a> or whatever.</p><p> And there&#39;s an art to noticing that you would probably be astounded and horrified by the details of a complicated system <i>if you knew them</i> , and then being astounded and horrified<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>already in advance</u></i></a> before seeing those details. <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. Alignment and capabilities are likely intertwined</h2><p> I expect that if we knew in detail how LLMs are calculating their outputs, we&#39;d be horrified (and fascinated, etc.).</p><p> I expect that we&#39;d see all sorts of coincidences and hacks that make the thing run, and we&#39;d be able to see in much more detail how, when we ask the system to achieve some target, it&#39;s not doing anything <i>close</i> to “caring about that target” in a manner that would work out well for us, if we could scale up the system&#39;s optimization power to the point where it could achieve great technological or scientific feats (like designing Drexlerian nanofactories or what-have-you).</p><p> Gaining this sort of visibility into how the AIs work is, I think, one of the main goals of interpretability research.</p><p> And understanding how these AIs work and how they don&#39;t — understanding, for example, when and why they <i>shouldn&#39;t</i> yet be scaled or otherwise pushed to superintelligence — is an important step on the road to figuring out how to make <i>other</i> AIs that <i>could</i> be scaled or otherwise pushed to superintelligence without thereby causing a bleak and desolate future.</p><p> But that same understanding is — I predict — going to reveal an incredible mess. And the same sort of reasoning that goes into untangling that mess into an AI that we can aim, also serves to untangle that mess to make the AI <i>more capable</i> . A tangled mess will presumably be inefficient and error-prone and occasionally self-defeating; once it&#39;s disentangled, it won&#39;t just be tidier, but will also come to accurate conclusions and notice opportunities faster and more reliably. <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p> Indeed, my guess is that it&#39;s even easier to see all sorts of things that the AI is doing that are dumb, all sorts of ways that the architecture is tripping itself up, and so on.</p><p> Which is to say: the same route that gives you a chance of aligning this AI (properly, not the “it no longer says bad words” superficial-property that labs are trying to pass off as “alignment” these days) also likely gives you lots more AI capabilities.</p><p> (Indeed, my guess is that the first big capabilities gains come <i>sooner</i> than the first big alignment gains.)</p><p> I think this is true of most potentially-useful alignment research: to figure out how to aim the AI, you need to understand it better; in the process of understanding it better you see how to make it more capable.</p><p> If true, this suggests that alignment will always be in catch-up mode: whenever people try to figure out how to align their AI better, someone nearby will be able to run off with a few new capability insights, until the AI is pushed over the brink.</p><p> So a first key challenge for AI alignment is a challenge of ordering: how do we as a civilization figure out how to aim AI <i>before</i> we&#39;ve generated unaimed superintelligences plowing off in random directions? I no longer think “just sort out the alignment work before the capabilities lands” is a feasible option (unless, by some feat of brilliance, this civilization pulls off some uncharacteristically impressive theoretical triumphs).</p><p> Interpretability? Will likely reveal ways your architecture is bad before it reveals ways your AI is misdirected.</p><p> Recruiting your AIs to help with alignment research? They&#39;ll be able to help with capabilities long before that (to say nothing of whether they <i>would</i> help you with alignment by the time they <i>could</i> , any more than humans would willingly engage in eugenics for the purpose of redirecting humanity away from <a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>Fun</u></a> and exclusively towards inclusive genetic fitness).</p><p>等等。</p><p> This is (in a sense) a weakened form of my answer to those who say, “AI alignment will be much easier to solve once we have a bona fide AGI on our hands.” It sure will! But it will also be much, much easier to destroy the world, when we have a bona fide AGI on our hands. To survive, we&#39;re going to need to either sidestep this whole alignment problem entirely (and take other routes to a <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>wonderful future</u></a> instead, as I may discuss more later), or we&#39;re going to need some way to do a bunch of alignment research <i>even as</i> that research makes it radically easier and radically cheaper to destroy everything of value.</p><p> Except even that is harder than many seem to realize, for the following reason.</p><p></p><h2> 2. Distinguishing real solutions from fake ones is hard</h2><p> Already, labs are diluting the word “alignment” by using the word for superficial results like “the AI doesn&#39;t say bad words”. Even people who apparently understand many of the core arguments have apparently gotten the impression that GPT-4&#39;s ability to answer moral quandaries is somehow especially relevant to the alignment problem, and an important positive sign.</p><p> (The ability to answer moral questions convincingly mostly demonstrates that the AI can predict how humans would answer or what humans want to hear, without revealing much about what the AI actually pursues, or would pursue upon reflection, etc.)</p><p> Meanwhile, we have little idea of what passes for “motivations” inside of an LLM, or what effect pretraining on next-token prediction and fine-tuning with RLHF really has on the internals. This sort of precise scientific understanding of the internals — the sort that lets one predict weird cognitive bugs <i>in advance</i> — is currently mostly absent in the field. (Though not entirely absent, thanks to the hard work of many researchers.)</p><p> Now imagine that Earth wakes up to the fact that the labs aren&#39;t going to all decide to stop and take things slowly and cautiously at the appropriate time. <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span> And imagine that Earth uses some great feat of civilizational coordination to halt the world&#39;s capabilities progress, or to otherwise handle the issue that we somehow need room to figure out how these things work well enough to align them. And imagine we achieve this coordination feat <i>without</i> using that same alignment knowledge to end the world (as we could). There&#39;s then the question of who gets to proceed, under what circumstances.</p><p> Suppose further that everyone agreed that the task at hand was to fully and deeply understand the AI systems we&#39;ve managed to develop so far, and understand how they work, to the point where people could reverse out the pertinent algorithms and data-structures and what-not. As demonstrated by great feats like building, by-hand, small programs that do parts of what AI can do with training (and that nobody previously knew how to code by-hand), or by identifying weird exploits and edge-cases <i>in advance</i> rather than via empirical trial-and-error. Until multiple different teams, each with those demonstrated abilities, had competing models of how AIs&#39; minds were going to work when scaled further.</p><p> In such a world, it would be a difficult but plausibly-solvable problem, for bureaucrats to listen to the consensus of the scientists, and figure out which theories were most promising, and figure out who needs to be allotted what license to increase capabilities (on the basis of this or that theory that predicts this would be non-catastrophic), so as to put their theory to the test and develop it further.</p><p> I&#39;m not thrilled about the idea of trusting an Earthly bureaucratic process with distinguishing between partially-developed scientific theories in that way, but it&#39;s the sort of thing that a civilization can perhaps survive.</p><p> But that doesn&#39;t look to me like how things are poised to go down.</p><p> It looks to me like we&#39;re on track for some people to be saying “look how rarely my AI says bad words”, while someone else is saying “our evals are saying that it can&#39;t deceive humans yet”, while someone else is saying “our AI is acting very submissive, and there&#39;s no reason to expect AIs to become non-submissive, that&#39;s just anthropomorphizing”, and someone else is saying “we&#39;ll just direct a bunch of our AIs to help us solve alignment, while arranging them in a big bureaucracy”, and someone else is saying “we&#39;ve set up the game-theoretic incentives such that if any AI starts betraying us, some other AI will alert us first”, and this is a <i>different sort of situation</i> .</p><p> And not one that looks particularly survivable, to me.</p><p> And if you ask bureaucrats to distinguish which teams should be allowed to move forward (and how far) in that kind of circus, full of claims, promises, and hunches and poor in theory, then I expect that they basically just <i>can&#39;t</i> .</p><p> In part because the survivable answers (such as “we have no idea what&#39;s going on in there, and will need way more of an idea what&#39;s going on in there, and that understanding needs to somehow develop in a context where we can do the job right rather than simply unlocking the door to destruction”) aren&#39;t really in the pool. And in part because all the people who really want to be racing ahead have money and power and status. And in part because it&#39;s socially hard to believe, as a regulator, that you should keep telling everyone “no”, or that almost everything on offer is radically insufficient, when <i>you yourself</i> don&#39;t concretely know what insights and theoretical understanding we&#39;re missing.</p><p> Maybe if we can make AI a science again, then we&#39;ll start to get into the regime where, <i>if</i> humanity can regulate capabilities advancements in time, then all the regulators and researchers understand that you shall only ask for a license to increase the capabilities of your system when you have a full detailed understanding of the system and a solid justification for why you need the capabilities advance and why it&#39;s not going to be catastrophic. At which point maybe a scientific field can start coming to some sort of consensus about those theories, and regulators can start being sensitive to that consensus.</p><p> But unless you can get over that grand hump, it looks to me like one of the key bottlenecks here is <i>bureaucratic legibility of plausible solutions</i> . Where my basic guess is that regulators won&#39;t be able to distinguish real solutions from false ones, in anything resembling the current environment.</p><p> Together with the above point (&quot;alignment and capabilities are likely intertwined&quot;), I think this means that our rallying cry should be less “pause to give us more time on alignment research” and more “stop entirely, and find some way to circumvent these woods entirely; we&#39;re not equipped to navigate them”.</p><p> (With a backup rallying cry of “make AI a science again”, though again, that only works if you have some way of preventing the science-of-mind from leading to catastrophe before we figure out how to build AIs that care about good stuff rather than bleak and desolate stuff.)</p><p></p><h2> 3. Most theories don&#39;t work on the first real try</h2><p> It seems worth noting that <i>even if</i> you manage to surmount the above two problems, you have a third problem on your hands, which is that when it finally comes time, not to increase your system&#39;s capabilities a bit more (and test your theory of cognition further), but rather to actually do something of note with your AI, then you must, in some sense, be trusting a novel and untested scientific theory ( <i>and</i> your engineering efforts grounded in that theory) to work perfectly on the first critical try.</p><p> In particular, once your AI is <i>capable</i> of autonomous scientific/technological development, and operating in a domain where it <i>could</i> use that ability to gain a decisive strategic advantage over the rest of the planet, it&#39;s operating in a fundamentally different cognitive regime than it ever was in training.</p><p> By analogy, if you&#39;re considering making Bob the dictator of your country, you might first make him mock-dictator of your town, and watch to make sure he doesn&#39;t abuse his power. But, try as you might, that&#39;s still not fundamentally all that reliable a test of whether he&#39;ll actually abuse power once he really has it. Once he <i>really can</i> give orders to the army, once he <i>really can</i> take advantage of the country&#39;s populace, will he do so? The option of “abuse my mock power while being observed by my townspeople (who can still beat me up or deny me the job)” is actually very cognitively different than the option “order the army to intimidate the parliament and &#39;oversee&#39; the next election”.</p><p> Now, given a sufficiently well-developed theory of cognition, you can try to read the AI&#39;s mind and predict what cognitive state it <i>would</i> enter if it <i>did</i> really think it had those options. And you can set up simulations (and try to spoof its internal sensations and so on) in a way that your theory of cognition <i>predicts</i> is very similar to the cognitive state it would enter once it really had the option to betray you.</p><p> But the link between these states that you induce and observe in the lab, and the actual state where the AI actually has the option to betray you, <i>depends fundamentally on your fresh new theory of cognition.</i></p><p> Actually <i>running</i> the AI until it <i>really has</i> the opportunity to betray you is an empirical test of those theories in an environment that differs fundamentally from the lab setting.</p><p> And many a scientist (and programmer) knows that their theories of how a complicated system is going to work in a fundamentally new operating environment <i>often don&#39;t go super well on the first try.</i></p><p> As a concrete analogy to potentially drive this point home: Newtonian mechanics made all sorts of shockingly-good empirical predictions. It was a simple concise mathematical theory with huge explanatory power that blew every previous theory out of the water. And if you were using it to send payloads to very distant planets at relativistic speeds, you&#39;d <i>still be screwed</i> , because Newtonian mechanics does not account for relativistic effects.</p><p> (And the only warnings you&#39;d get would be little hints about light seeming to move at the same speed in all directions at all times of year, and light bending around the sun during eclipses, and the perihelion of Mercury being a little off from what Newtonian mechanics predicted. Small anomalies, weighed against an enormous body of predictive success in a thousand empirical domains; and yet Nature doesn&#39;t care, and the theory still falls apart when we move to energies and scales far outside what we&#39;d previously been able to observe.)</p><p> Getting scientific theories to work on the first critical try is <i>hard</i> . (Which is one reason to aim for minimal pivotal tasks — getting a satellite into orbit should work fine on Newtonian mechanics, even if sending payloads long distances at relativistic speeds does not.)</p><p> Worrying about this issue is something of a luxury, at this point, because it&#39;s not like we&#39;re anywhere close to scientific theories of cognition that accurately predict all the lab data. But it&#39;s the next hurdle on the queue, if we somehow manage to coordinate to try to build up those scientific theories, in a way where success is plausibly bureaucratically-legible.</p><hr><p> Maybe later I&#39;ll write more about what I think the strategy implications of these points are. In short, I basically recommend that Earth pursue other routes to the glorious transhumanist future, such as uploading. (Which is also fraught with peril, but I expect that those perils are more surmountable; I hope to write more about this later.) </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p> Albeit slightly less, since there&#39;s <i>nonzero</i> prior probability on this unknown system turning out to be simple, elegant, and well-designed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p> An exception to this guess happens if the AI is at the point where it&#39;s correcting its own flaws and improving its own architecture, in which case, in principle, you might not see much room for capabilities improvements if you took a snapshot and comprehended its inner workings, despite still being able to see that the ends it pursues are not the ones you wanted. But in that scenario, you&#39;re already about to die to the self-improving AI, or so I predict.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p> Not least because there are no sufficiently clear signs that it&#39;s time to stop — we blew right past “an AI claims it is sentient”, for example. And I&#39;m not saying that it was a <i>mistake</i> to doubt AI systems&#39; first claims to be sentient — I doubt that Bing had the kind of personhood that&#39;s morally important (though I am by no means confident!). I&#39;m saying that the thresholds that are clear <i>in science fiction stories</i> turn out to be messy <i>in practice</i> and so everyone just keeps plowing on ahead.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p> <i>Some prerequisites needed in order for this to make sense:</i></p><ol><li> <a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>Two Subsystems: Learning &amp; Steering</i></a></li><li> <a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>Shard Theory: An Overview</i></a></li><li> <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">Distilling Singular Learning Theory</a> <span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li> <i>Maybe also understanding at least a little</i> <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>Nate&#39;s picture</i></a> <i>though I don&#39;t claim to understand it fully.</i></li><li> <i>Of course,</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>AGI Ruin: A List of Lethalities</i></a> <i>, though hopefully that was implied</i></li><li> <i>Maybe the very basics of infra-bayesianism</i> (I liked the <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">AXRP podcast with her</a> ( <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">there&#39;s two</a> )) <i>, Vanessa&#39;s original</i> <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>learning theoretic agenda</i></a> <i>philosophy, and her current</i> <a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>pre-DCA</i></a> <i>alignment scheme.</i></li></ol><h1> Abstract</h1><p> The philosophy behind infra-bayesianism, and Vanessa&#39;s learning-theoretic alignment agenda seems very insightful to me. However, the giant stack of assumptions needed to make the approach work, and the ontology-forcing nature of the plan leave me unsettled. The path of singular learning theory has recently seen enough empirical justification to excite me. I&#39;m optimistic it can describe brains &amp; machine learning systems, and I describe my hope that this can be leveraged into alignment guarantees between the two, becoming an easier task as whole brain emulation develops.</p><h1>介绍</h1><p>Imagine a world where instead of humanity wandering together blindly down the path of cobbled together weird tricks learned off the machine learning literature, with each person trying to prepare for the bespoke failure-mode that seems most threatening to them (most of whom are wrong).</p><p> That instead we lived in the world where before heading down we were given a map and eyes to see for ourselves the safest path, and the biggest and most dangerous cliffs, predators, and dangers to be aware of.</p><p> It has always seemed to me that we get to the second world once we can use math for alignment. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="On the left dalle image generated with: Photo depicting a world representing empirical AI alignment approaches: A group of diverse people are shown as blind, navigating a rugged and unclear path. Their eyes are covered with blindfolds, and the terrain is uneven with shadows obscuring parts of the path. Each person has their own walking stick and gear, and they seem uncertain and cautious. Some individuals clutch old books or scrolls, symbolizing reliance on outdated machine learning literature. Others hold various tools and devices, preparing for unseen threats and unsure of the specific dangers ahead.  On the right dalle image generated with: Illustration in the Renaissance period style: A diverse group of men and women gather at the starting point of their adventure. Each holds a vibrant holographic map, some displaying topographical lines and others animated weather patterns. While some individuals discuss and point at dangers on the map, others look up to compare the hologram with the real terrain. The rugged journey ahead showcases treacherous cliffs, dense forests, and a distant mountain peak. A setting sun casts a dramatic light on the group, creating a chiaroscuro effect. The soft glow in their eyes highlights their enhanced vision." srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption> The left: Our situation. The right: Our situation if we had math for alignment.<br> Generated with DALL·E 3.</figcaption></figure><p> For a long time I thought using math for alignment was essentially a non-starter. Deep learning is notoriously resistant to successful theories, I&#39;m pessimistic that the approach of the Machine Intelligence Research Institute will take too much time, and the most successful mathematical theory of intelligence and alignment--infra-Bayesianism--rests on a stack of assumptions and mathematical arguments too high, too speculative, and too normative for me to be optimistic about. So I resigned myself to the lack of math for alignment.</p><p> That is, until Nina Rimsky &amp; Dmitry Vaintrob showed that some predictions of the new Singular Learning Theory held with <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">spooky accuracy</a> in the <a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking modular addition network</a> <span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> .</p><p> I had previously known about singular learning theory, and gone to the singular learning theory conference in Berkeley. But after thinking about the ideas, and trying to implement some of the processes they came up with myself, and getting not so great results <span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span> , I decided it too was falling for the same traps as infra-Bayesianism, lying on a giant stack of mathematical arguments, with only the most tentative contact with empirical testing with real world systems. So I stopped following it for a few months.</p><p> Looking at these new results though, it seems promising.</p><p> <i>But its not natively a theory of alignment. Its an upgraded learning theory. How does that solve alignment?</i></p><p> Well, both the human brain &amp; machine learning systems are learning machines, trained using a mix of reinforcement &amp; supervised learning. If this theory were to be developed in the right way (and this is where the <i>hope</i> comes in), we could imagine relating the goals of the results of one learning process to the goals of a different learning process, and prove a maximal deviation between the two for a particular setup. If one of those learning systems is a human brain, then we have just gotten an alignment guarantee.</p><p> Hence my two main hopes for alignment <span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> : whole brain emulation, and a <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning-theoretic-agenda</a> -like developmental track for singular learning theory. <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p> Interpretability, and other sorts of deep learning science seem good to the extent it helps with getting singular learning theory (or some better version of it) to the state where its able to prove such alignment-relevant theorems.</p><p> This task is made easier to the extent we have better whole brain emulation. And becomes so easy that we don&#39;t even need the singular learning theory component if we have succeeded in our whole brain emulation efforts. <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p> It seems like it&#39;d be obvious to you, the reader, why whole brain emulation gives me hope <span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> . But less obvious why singular learning theory gives me hope, so I will explain in much of the rest of this post why the latter is true.</p><h1> Singular learning theory</h1><p> <i>Feel free to skip these next three paragraphs, or even instead of reading my description, read</i> <a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse&#39;s</i></a> <i>or read pre-requisite 3. My goal is less to explain what singular learning theory is, and more to give an idea about why I&#39;m excited about it.</i></p><p> Singular learning theory fills a gap in regular learning theory, in particular, regular learning theory assumes that the parameter function map of your statistical model is one-to-one, and intuitively your loss landscape has no flat regions. <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p> Singular learning theory handles the case where this is not true, which occurs often in hierarchical models, and (as a subset) deep neural networks. And in broad strokes by bringing in concepts from algebraic geometry in order to rewrite the KL divergence between the true model and parameters into a form that is easier to analyze.</p><p> In particular, it anticipates two classes of phase transitions during the development of models, one of which is that whenever loss suddenly goes down, the real-log-canonical-threshold (RLCT), a algebraic geometry derived metric for complexity, will go up. <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p> Its ultimately able to retrodict various facts about deep learning, including the success of data scaling, parameter scaling, and double descent, and there has been recent success in getting it to give predictions about phenomena in limited domains. Most recently, Nina Rimsky and Dmitry Vaintrob&#39;s <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">Investigating the learning coefficient of modular addition: hackathon project</a> where the two were able to verify various assertions about the RLCT/learning coefficient/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei estimate. Getting  the most beautiful verification of a theoretical prediction in my lifetime </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption> Chart of estimated <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> over training for an MLP trained on modular addition mod 53. Checkpoints were taken every 60 batches of batch size 64. Hyperparameters for SGLD are <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> . The search was restricted to directions orthogonal to the gradient at the initialization point to correct for measurement at non-minima. [caption text from the original post]</figcaption></figure><p> As I said, singular learning theory makes the prediction that during phase transitions, the loss of a model will decrease while the RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) will increase. Intuitively, this means, when you switch model classes you switch to a model class that fits the data better and is more complicated. Above, we see exactly this.</p><p> As well as Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, and Daniel Murfet&#39;s <a href="https://arxiv.org/abs/2310.06301">Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition</a> with abstract</p><blockquote><p> We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.</p></blockquote><p> You can read more at <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">this LessWrong sequence</a> , watching the <a href="https://www.youtube.com/@SLTSummit/videos">primer</a> , <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox lectures</a> , and of course reading the books <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Algebraic Geometry and Statistical Learning Theory</a> , and <a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Mathematical Theory of Bayesian Statistics</a> .</p><h1> So why the hope?</h1><p> To quote Vanessa Kosoy from her <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning theoretic</a> agenda:</p><blockquote><p> In online learning and reinforcement learning, the theory typically aims to derive upper and lower bounds on &quot;regret&quot;: the difference between the expected utility received by the algorithm and the expected utility it <i>would</i> receive if the environment was known a priori. Such an upper bound is effectively a <i>performance guarantee</i> for the given algorithm. In particular, if the reward function is assumed to be &quot;aligned&quot; then this performance guarantee is, to some extent, an alignment guarantee. This observation is not vacuous, since the learning protocol might be such that the true reward function is not directly available to the algorithm, as exemplified by <a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a> and <a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a> . Thus, formally proving alignment guarantees takes the form of proving appropriate regret bounds.</p></blockquote><p> If the principles of singular learning theory can be extended to reinforcement learning, and we get reasonable bounds for the generalization behavior of our model, or even exact claims about the different forms the value-equivalents inside our model will take as it progresses during training, we can either hope to solve a form of what can roughly be known as inner alignment--getting our model to consistently think &amp; act in a certain way when encountering the deployment environment.</p><p> It seems reasonable to me to think we can in fact extend singular learning theory to reinforcement learning. The same sorts of deep learning algorithms work very well on both supervised and reinforcement learning, so we should expect the same algorithms have similar reasons for working on both, and singular learning theory gives a description of why those deep learning algorithms do well in the supervised learning case. So we should suspect the story for reinforcement learning follows the same general strokes <span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> .</p><h1> If you like performance guarantees so much, why not just work on infra-Bayesianism?</h1><p> Vanessa&#39;s technique was to develop a theory of what it meant to do good reasoning in a world that is bigger than yourself, and also requires you to do self-modeling. Then (as I understand it) prove some regret bounds, make a criterion for what it means to be an agent, and construct a system with a low bound on the satisfaction of the utility function of the agent that is most immediately causally upstream of the deployed agent.</p><p> This seems like a really shaky construction to me, mainly because we do not in fact have working in-silico examples of the agents she&#39;s thinking about. I&#39;d be much happier with taking this methodology, and using it to prove bounds on actual real life deep learning systems&#39; regret (or similar qualities) under different training dynamics.</p><p> I also feel uneasy about how it goes from theory about how values work (maximizing a utility function) to defining that as the success criterion <span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> . I&#39;d be more comfortable with a theory which you could apply to the human brain, and naturally derive a utility function (or other value format). Just by looking at how brains are built and developed. In the case that we&#39;re wrong about our philosophy of values, this seems more robust. And to the extent multiple utility functions can fit our behavior accounting for biases and lack of extensive reflection, having a prior over values informed by the territory (ie the format values are in in the brain) seems better than the blunt instrument of Occam&#39;s razor applied directly to the mapping from our actual and counterfactual actions to utility functions.</p><p> Of all the theories that I know of, singular learning theory seems the most adequate to the task. It both is based on well-proven math, it has and will continue to have great contact with actual systems,  it covers a very wide range of learning machines, which includes human brains (ignoring the more reinforcement learning like aspects of human learning for now) &amp; likely future machines (again ignoring reinforcement learning), and the philosophical assumptions it makes are far more minimalist than those of infra-Bayesianism.</p><p> The downside of this approach is singular learning theory says little right now about reinforcement learning. However, as I also said above, we see the same kinds of scaling dynamics in reinforcement learning as we do in supervised learning &amp; the same kinds of models work in both cases, so it&#39;d be pretty weird if they had very different reasons for being successful. Singular learning theory tries to explain the supervised learning case, so we should expect it or similar methods be able to explain the reinforcement learning case too.</p><p> Another downside is it not playing well with unrealizability. However, I&#39;m told there hasn&#39;t been zero progress here, its an open problem in the field, and again, neural networks often learn in unrealizable environments, as far as I know, we see similar enough dynamics that I bet singular learning theory is up for the task.</p><h1> Whole brain emulation</h1><p> The human brain is almost certainly singular <span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> , has a big learning from scratch component to it, and singular learning theory is very agnostic about the kinds of models it can deal with <span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> , so I assert singular learning can deal with the brain. Probably not to help whole brain emulation all that much, but given data whole brain emulation gives us about the model class that brains fall into, the next hope is to use this to make nontrivial statements about the value-like-things that humans have. Connecting this with the value-like-things that our models have, we can hopefully (and this is the last hope) use singular learning theory to tell us under what conditions our model will have the same values-like-things that our brains have.</p><h1> Fears</h1><h2>哇！ That&#39;s a lot of hopes. I&#39;m surprised this makes you more hopeful than something simple like empirical model evaluations</h2><p> Singular learning theory, interpretability, and the wider developmental interpretability all seem useful for empirically testing models. I&#39;m not hopeful just because of the particular plan I outline above, I&#39;m hopeful because I see a concrete plan at all for how to turn math into an alignment solution for which all parts seem to be useful even if not all of my hopes turn out correct.</p><h2> I&#39;m skeptical that something like singular learning theory continues to work as the model becomes reflective, and starts manipulating its training environment.</h2><p> I am too. This consideration is why our guarantees should occur early in training, be robust to continued training, and be reflectively stable. Human-like values-like-things should be reflectively stable by their own lights, though we won&#39;t actually know until we actually see what we&#39;re dealing with here. So the job comes down to finding a system which puts them into our model early in training, keeps them throughout training, and ensures by the time reflection is online, the surrounding optimizing machinery is prepared.</p><p> Put another way: I see little reason to suspect the values-like-things deep learning induces will be reflectively stable by default. Primarily because the surrounding optimizing machinery is liable to give strange recommendations in novel situations, such as reflective thought becoming active <span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> . So it does in fact seem necessary to prepare that surrounding optimizing machinery for the event of reflection coming online. But I&#39;m not so worried about the values-like-objects being themselves disastrously suicidal once we know they&#39;re similar enough to those of humans.</p><p> Nate and possibly Eliezer would say this is important to know from the start. I would say I&#39;ll cross that bridge once I actually know a thing or two what values-like-thing, and surrounding machinery I&#39;ll be dealing with.</p><h2> Why reinforcement learning? Shouldn&#39;t you focus on supervised learning, where the theory is clear, and we&#39;re more likely to get powerful models soon?</h2><p> Well, brains are closer to reinforcement learning than supervised learning, so that&#39;s one reason. But yeah, if we can get a model which while supervised, we prove statements about values-like objects for, then that would be a good deal of the way there. But not all the way there, since we&#39;d still be confused when looking at our brains.</p><h2> Singular learning theory seems liable to help capabilities. That seems bad.</h2><p> I will quote myself outlining <a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">my position</a> on a related topic, which generalizes to the broader problem of developing a theory of deep learning:</p><blockquote><p> Mostly I think that [mechanistic interpretability] is right to think it can do a lot for alignment, but I suspect that lots of the best things it can do for alignment it will do in a very dual-use way, which skews heavily towards capabilities. Mostly because capabilities advances are easier and there are more people working on those.</p><p> At the same time I suspect that many of those dual use concerns can be mitigated by making your [mechanistic interpretability] research targeted. Not necessarily made such that you can do off-the-shelf interventions based on your findings, but made such that if it ever has any use, that use is going to be for alignment, and you can predict broadly what that use will look like.</p><p> This also doesn&#39;t mean your [mechanistic interpretability] research can&#39;t be ambitious. I don&#39;t want to criticize people for being ambitious or too theoretical! I want to criticize people for producing knowledge on something which, while powerful, seems powerful in too many directions to be useful if done publicly.</p></blockquote><p> My plan above is a good example of an ambitious &amp; theoretical but targeted approach to deep learning theory for alignment.</p><h2> Why singular learning theory, and not just whole brain emulation?</h2><p> Firstly, as I said in the introduction, it is not obvious to me that given whole brain emulation we get aligned superintelligence, or are even able to perform a pivotal act. Recursive self improvement while preserving values may not be easy or fast (though you can always make the emulation faster). Pivotal acts done by such an emulation likely have difficulties I can&#39;t see.</p><p> However, I do agree that whole brain emulation alone seems probably alignment complete.</p><p> My main reason for focusing on both is that they feel like two different sides of the same problem in the sense that progress in whole brain emulation makes us need less progress in singular learning theory, and vice versa. And thinking in terms of a concrete roadmap makes me feel more like I&#39;m not unintentionally sweeping anything important underneath any rugs. The hopes I describe have definite difficulties, but few speculative difficulties.</p><h2> It seems difficult to get the guarantees you talk about which are robust to ontology shifts. Values are in terms of ontologies. Maybe if a model&#39;s ontology changes, its values will be different from the humans&#39;</h2><p> This is something I&#39;m worried about. I think there&#39;s hope that during ontology shifts, the meta-values of the models will dominate what shape the model values take into the new ontology, and there won&#39;t be a fine line between the values of the human and the meta-values of the AI. There&#39;s also an independent hope that we can have a definition of values that is just robust to a wide range of ontology shifts.</p><h1> So what next for singular learning theory and whole brain emulation?</h1><p> I currently don&#39;t know too much about whole brain emulation. Perhaps there are areas they&#39;re focusing on which aren&#39;t so relevant to my goals here. For example, if they focus more on the statics of the brain than the dynamics, then that seems naively inefficient <span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> because the theorem I want talks about the dynamics of learning systems and how those relate to each other.</p><p> Singular learning theory via <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeus</a> seems to mostly be doing what I want them to be doing: testing the theory on real world models, and seeing how to relate it to model internals via developmental interpretability. One failure-mode here is they focus too much on empirically testing it, and too little on trying to synthesize their results into a unified theory. Another failure-mode is they focus too much on academic outreach, and not enough on actually doing research. And then the academics they do outreach to don&#39;t really theoretically contribute that much to singular learning theory.</p><p> I&#39;m not <i>so</i> worried about the first failure-mode, since everyone on their core team seems very theoretically inclined.</p><p> It seems like a big thing they aren&#39;t looking into is reinforcement learning. This potentially makes sense. Reinforcement learning is harder than supervised learning. You need some possibly nontrivial theoretical leaps to say anything about it in a singular learning theory framework. Even so, it seems possible there is low-hanging fruit in this direction. Similarly for taking current models of the brain</p><p> Of course, I expect the interests of Timaeus and myself will diverge as singular learning theory progresses, and there are pretty few people working on developing the theory right now. So it seems a productive use of my efforts.</p><h1> Acknowledgements</h1><p> Thanks to Jeremy Gillen, and David Udell for great comments and feedback! Thanks also to Nicholas Kees, and the <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> for the same. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p> Note I didn&#39;t read this, I watched the <a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">singular learning theory primer</a> videos, but those seem longer than the series of LessWrong posts, and some have told me they&#39;re a good intro.</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p> Somewhat notably, both the grokking work, and Nina &amp; Dmitry&#39;s project were done over a weekend.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p> If I remember correctly, the reason for the not great results was because the random variable we were estimating had a too high variance to actually correlate with another quantity we were trying to measure in our project.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p> Not including stuff like Davidad&#39;s proposals, which while they give me some hope, there&#39;s little I can do to help.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p> Ultimately hoping to construct a theorem of the form</p><blockquote><p> Given agent Alice with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and agent Bob with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> , end up with value systems <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> respectively (not necessarily utility functions), and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span> for some definition of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span> that means something like Bob tries to achieve something like what Alice tries to achieve when in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> .</p></blockquote><p> And where we can solve for Bob&#39;s reward model and environment taking Alice as being a very ethical human. Hopefully with some permissive assumptions, and while constructing a dynamic algorithm in the sense that Bob can learn to do good by Alice&#39;s lights for a wide enough variety of Alices that we can hope humans are inside that class.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p> Since, hopefully, if we have whole brain emulation it will be easy for the uploaded person or people to bootstrap themselves to superintelligence while preserving their goals (not a trivial hope!).</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p> This is not to say the conditions under which whole brain emulation should give one hope are obvious.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p> More formally, regular models are one-to-one, and have fisher information matrix positive-definite everywhere.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p> There is a different phase transition which occurs when the RLCT goes down, and some other quantity goes up. There are several candidates for this quantity, and as far as I know, we don&#39;t know which increase is empirically more common.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p> The reward landscape of deep reinforcement learning models is probably pretty insane. Perhaps not insane enough to not be liable to singular learning theory-like analysis, since there&#39;s always some probability of doing any sequence of actions, and those probabilities change smoothly as you change weights, so the chances you execute a particular plan change smoothly, and so your expected reward changes smoothly. So maybe there&#39;s analogies to be made to the loss landscape.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m told that Vanessa and Diffractor believe infra-Bayesianism can produce <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">reflectively stable and useful quantilizers and worst-case optimizers</a> over a set of plausible utility functions. I haven&#39;t looked at it deeply, but I&#39;d bet it still assumes more ontology than I&#39;m comfortable with, both in the sense that for this reason it seems less practical than my imagined execution of what I describe here, and it seems more dangerous.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p> In the sense that likely the mapping from brain states to policies is not one-to-one, and has singular fisher information matrix.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p> In its current form it requires only that they be analytic, but ReLUs aren&#39;t, and empirically we see ignoring that aspect gives accurate predictions anyway.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p> Situational novelty is not sufficient to be worried, but during reflection the model is explicitly thinking about how it should think better, so if its bad at this starting out, and makes changes to its thought, if those changes are to what it cares about, they will not necessarily be corrected by further thought or contact with the world. So situational novelty leading to incompetence is important here.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p> A way it could be efficient is if its just <i>so</i> much easier to do statics than dynamics, you will learn much more about dynamics from all the static data you collect than you would if you just focused on dynamics.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky recently <a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">posted on Facebook</a> an experiment that could potentially indicate whether humans can &quot;have AI do their alignment homework&quot; despite not being able to trust whether the AI is accurate: see if people improve in their chess-playing abilities when given advice from experts, two out of three of which are lying.</p><p> I&#39;m interested in trying this! If anyone else is interested, leave a comment. Please tell me whether you&#39;re interested in being:</p><p> A) the person who hears the advice, and plays chess while trying to determine who is trustworthy</p><p> B) the person who they are playing against, who is normally better at chess than A but worse than the advisors</p><p> C) one of the three advisors, of which one is honestly trying to help and the other two are trying to sabotage A; which one is which will be chosen at random after the three have been selected to prevent A from knowing the truth</p><p> Feel free, and in fact encouraged, to give multiple options that you&#39;re open to trying out! Who gets assigned to what role would depend on how many people respond and their levels of chess ability, and it&#39;s easier to find possible combinations with more flexibility in whose role is which.</p><p> Please also briefly describe your level of experience in chess. How frequently have you played, if at all; if you have ELO rating(s), what are they and which organizations are they from (FIDE, USCF, Chess.com, etc). No experience is required! In fact, people who are new to the game are actively preferred for A!</p><p> Finally, please tell me what days and times you tend to be available - I won&#39;t hold you to anything, of course, but it&#39;ll help give me an estimate before I contact you to set up a specific time.</p><p> Edit: also, please say how long you would be willing to play for - a couple hours, a week, a one-move-per-day game over the course of months? A multi-week or multi-month game would give the players a lot more time to think about the moves and more accurately simulate the real-life scenario, but I doubt everyone would be up for that.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate></item></channel></rss>