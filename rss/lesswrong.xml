<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 16 日星期四 02:26:05 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Extrapolating from Five Words]]></title><description><![CDATA[Published on November 15, 2023 11:21 PM GMT<br/><br/><p>如果你只能用<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">五个词</a>来表达一个想法，人们会从这五个词中推断出什么？您可以使用法学硕士通过实验来发现人们可能认为这五个词的含义，而不是猜测。您可以使用它来迭代您想说的五个单词，以便最好地传达您的预期含义。</p><p>我产生这个想法是因为我尝试要求克劳德在链接上总结一篇文章。 Claude 不会跟踪链接，因此它会幻觉标题中的摘要，该摘要包含在 URL 路径中。这是使用<a href="https://www.lesswrong.com/posts/LkjpHGiELQzed8hdu/why-the-problem-of-the-criterion-matters">我的 LessWrong 帖子之一</a>执行此操作的示例： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/arzjwjobp4f1mzw7butm" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/slx0yvnxmtj5fhhbonfl 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/i4hsanmguhbqxeubhcgd 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sbhdvppzm36adnjmzf0j 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/onfbpkewjhbcvp82t5nc 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ooochb4zyqwvu9nbkait 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gyfckyk0nkrp37ysfj6m 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ocvikzsm8zsgwgxlwbc9 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bwjocv054qp6xzz7n2mr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/z2cdhnfimjqy5iydhhjs 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ubotioxkb9gkckqmobaj 1638w" alt=""></figure><p>它产生了一些错误的细节，并遗漏了帖子中实际存在的许多细节，但这里并不是完全不合时宜。如果我的〜五个词是“标准问题很重要”，那么这将是我为什么这么说的合理推断。</p><p>除了使用链接之外，我还可以要求 Claude 提出它认为我会在具有特定标题的帖子中添加的内容： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/r1w6ub2kairpaylby2ya" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/vpuvlg0a2qxs02d9a8mh 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gd1asogssvzjvtme7zzx 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lr2zcthp13guf9fiheci 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/veszm5ofcocu4ycc8m3b 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/b9bbmyxfbj5vhgqwefcz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/s6ntzjebigweykpebss7 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/t53x7vzvjuzseje3ogpq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fehqezg1cmuzigwpn8e3 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/n6359ek9mzuw9un5gtgy 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bkwlgtvdtbnazl3cfnrd 1586w"></figure><p>奇怪的是，它在某些方面表现得更糟，而在其他方面表现得更好。与它产生链接摘要的幻觉不同，这次它提出了我绝对不会说或不希望有人得到的东西，比如我们可以解决标准问题，足以拥有客观的知识标准。</p><p>但也许促使它关注 LessWrong 才是问题所在，因为 LessWrong 引起了很多实证主义者的共鸣，<a href="https://www.lesswrong.com/posts/dTkWWhQkgxePxbtPE/no-logical-positivist-i">但埃利以泽的相反说法</a>并不成立。所以我尝试了不同的提示： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xrpj3qaqn7suizcevhys" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/k9x2qujwgkf0metacypy 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bpm1a2ggpidjlcsqwesd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zd59kndoaw4wxykeycyy 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nqnrhxxvr8huxcay1u32 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pn6l2shzgwq8ql0obr4n 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bctwvfofjgkjzck0nqq1 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hkmze9rfpylwqklxnklq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jzz0ty1oy4tlrqc4bhjr 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hb3mb6kehxcpipzf6rab 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jjmgmqa2khno1ugfbhox 1578w"></figure><p>这可以？这不太好。这听起来像是一个无聊的哲学本科生为认识论课写的论文的总结。</p><p>让我尝试问它某种版本的“我的大约五个词是什么意思？”： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lkewxseua3vdb05j7toe" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wtbbd1ubjfieqi5v5w54 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fyqopidgidzeckrmi3tt 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dv7skp8zycwv5qhs4njh 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qanzrbxq4kwjyb3ysyyq 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mai1kzmoyu3ar1aju9xa 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/w5lpbsgulhtld3oa1vi3 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lszilosu5mu0wwnxnvpn 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nvjhfugeemp0z6idhzsu 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/iixmdvneheblpgyl1a8i 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xankmu5wloylpm307klo 1614w"></figure><p>这非常好，基本上我希望有人能从我身上夺走“标准问题很重要”的东西。让我们看看如果我调整语言会发生什么： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/yqb2ijarrst9jsfx5gjd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/eut3fmkm8shlcbcug5g0 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pyw3pnoail6yj83xkgnd 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qp9yniwnn0oojpw6ezvj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lh501i7swrx2som5cnqt 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gana511ysgdqyjp70wgl 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/v8x1sgfssqcwx2b8rihc 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/uldogbpsdoiadnic0dml 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q0qfpikhfmxrrvpw3ade 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lzttdsrr3vwed4oltack 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcyq8xocefuh244rj9bj 1780w"></figure><p>整洁的！它注意到了“重要”而不是“重要”所暗示的许多细微差别。这对于尝试短语的不同变体以查看这些微小变体对隐含含义有何变化非常有用。我认为这对于诸如文字加工公司价值观和使命以及其他每个单词都必须承载很多含义的短语等任务很有用。</p><p>现在让我们看看它是否可以反向完成任务！ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sxgabqz0wyvvx4lxwhje" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ov7pj5rakxa9pjqjxrs6 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qitt5xg5d5aoetcfvfe2 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ongwefldjntxhgrqtaq1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/tudmg2sfnqjq2gqjrydl 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/is0bducapbzwva9zyax4 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/u8xa7utkyqizzg9zqe37 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ehjtaxuk5tfp3eganyln 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/f47objdkzzxsnxdar3sj 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jmvyyc0rhfola1dljksj 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pdgspb42zyihzi6pikad 1726w"></figure><p>老实说，“不确定性破坏知识”可能比我想出的任何东西都要好。谢谢，克劳德！</p><p>作为最后的检查，克劳德能否从自己的总结中推断出来？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/y2pf0udioglx0qsdhpv6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fdi6zhj1wtfivqlqcpuf 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ouxcagejqzlqzcphlhzd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jz3tiot9xgm5gl15hbls 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etynfz3izydbggskkzfi 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bdsy7bzo33ahduse7o5j 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q4ws7lixelgpvecilntf 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zail1zfcfpngok6rmbom 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q9qjxw72otwkfpbihcq7 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etkhvvkopnnoailb3ae1 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qrkgidttighy0hl474d8 1586w"></figure><p>显然，它丢失了一些细节，特别是关于标准问题的细节，并且弥补了一些我不想让它得到的东西。将细致入微的信息压缩成大约五个单词，并且仍然传达信息的核心，这似乎是理所当然的。</p><p>好吧，最后的测试，克劳德可以从我可能对“基本不确定性”做出的典型陈述中推断出什么？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dc6roac8u3oskbx4zwmk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ako6peikfk1vdblnddfu 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dh0yunove6oo8e4fhk3y 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mqhewctspnzhqsxqtjkw 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dp3x6bfuj6if7ryfcxzn 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ve9l0vltshrvuy5dfgqr 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bzhau52vnet9ehltdd9f 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ogyqa2awhac3kkorgliz 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ceudntk6ndseqfz6xewg 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/x0elkg9cjhej9iblmcgh 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hclfjabiodfwlfnxnaz6 1570w"></figure><p>嗯，还可以，但不是很好。也许我应该尝试找到另一个短语来表达我的想法？让我们看看它对“基本面不确定性”这个书名的看法： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/swabayitctg8yc2odybb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jrn6yiiqgufmxzirkdyd 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcw6qq4ze9ppchawd8uk 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/rhcuowppft8kzrpi3vas 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lihcbo7lcobtasexmubz 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wkymdjqovj4ushzjlol7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jnrmicoayxtn1e1vwwrp 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ibuidok3d2fonyom0jag 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bvjjhfbwjaebf8jjiway 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ugrnmwrcb7lwajtkjqpr 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wfh9e7vyz2ni5esm9wp8 1598w"></figure><p>足够接近。我可能不需要重新命名<a href="https://www.lesswrong.com/s/HMs2yT9D6LjYR5jQT">我的书</a>，但我可能需要设计一个好的副标题。</p><p>基于以上在提示工程中的实验，克劳德在迭代简短短语摘要方面相当有帮助。它能够捕捉到微妙的细微差别，这对于找到正确的短语来传达一个重要的想法非常有用。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from-five-words#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from- Five-words<guid ispermalink="false"> HD8kLyYcSuYRi4vzP</guid><dc:creator><![CDATA[Gordon Seidoh Worley]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:21:31 GMT</pubDate> </item><item><title><![CDATA[In Defense of Parselmouths]]></title><description><![CDATA[Published on November 15, 2023 11:02 PM GMT<br/><br/><p>先决条件：贵<a href="http://benjaminrosshoffman.com/the-quaker-and-the-parselmouth/">格会教徒和蛇佬腔</a>。</p><h2>我。</h2><p>首先，快速总结一下。</p><p>在先决条件帖子中，本杰明霍夫曼描述了三种人。这些人是假设的极端：他们是在无摩擦真空中相互作用的完美球体的社会和认知等价物。有些贵格会教徒总是说实话，并且在说要做某事时信守诺言。有些演员总是说当下看起来不错的话，即使他们发誓和发誓，也不能可靠地遵守诺言。最后，还有蛇佬腔，他们可以自由地对演员说谎，但只对其他蛇佬腔说实话，并且（暗示）只对贵格会教徒说实话。</p><p>我赞同这种区别。它是抽象的，现实世界从来都不是那么清晰，但根据我的经验，它确实得到了一些有用的东西来理解。我认为说真话是一个强大的制度优势，并希望更多的人在这种二分法中成为贵格会教徒。本杰明指出，蛇佬腔有些奇怪，因为习惯性说谎可能会侵蚀本能，甚至可能侵蚀说真话的能力；对于真正的人来说，如果不慢慢成为演员，就不可能始终保持蛇佬腔。</p><p>说真话很难。弄清楚世界的真实状况是什么是很困难的。快速而准确地陈述你认为正确的事情是很困难的；英语使得“我相信明天有百分之九十的机会下雨”的句子比“明天会下雨”要长得多。当有人问你是否喜欢他们带来的聚餐砂锅菜（烧焦的和未调味的）时，你最终会受到很多额外的情感尖锐的肘击。全世界的贵格会教徒，我向你们致敬。全世界的演员，我明白了。</p><p>我的第一个主张是成为蛇佬腔是合理的。</p><h2>二.</h2><p>讲故事的时间！下面的故事详细描述了大约二十年前发生的事件，当时我比现在矮了几英尺。一些细节已经被当时在场的其他人证实，但许多细节可能随着时间的推移而发生了变化。</p><p>当我还是个孩子的时候，我必须拍很多照片。我妈妈带我进了办公室，我在等候区闲逛了一会儿，然后护士挥手让我经过前台，我和妈妈就进去了。护士让我坐在医生办公室的一张大塑料椅子上，然后我一边用冰凉的东西擦着我的肩膀，一边问妈妈问题，然后她让我坐一会儿，说：“这不会疼的，你准备好了吗？”我点了头。然后她用针刺了我。</p><p>很痛。我开始哭，并且持续哭了一段时间，直到疼痛消退为隐痛。我父母的安慰或护士的款待都没有改变这一点。我当时没有能力清楚地表达出是什么让我心烦意乱，但这不是痛苦（即使在小时候，当疼痛有目的时，我对疼痛的容忍度也非常高），而是困惑。它不应该伤害——他们对于是否会伤害的判断是错误的吗？这不太合理，用尖锐的东西刺人通常会伤害他们，为什么有人会认为不会呢？我是否记错了他们说的话，他们说会痛而不是不会痛？我的记忆真的那么差吗？我完全困惑了，无法理解发生了什么。</p><p>凭借多年的经验，发生的事情是显而易见的。护士撒谎让一个小孩在注射时保持安静。这个故事多年来一直在重复，每次我都会感到困惑和困惑。直到很久以后，当我顿悟到世界如何看待真理之后，我才想到有人会撒谎。</p><p>虽然很痛苦，但事实证明，这种理解是许多人际互动的有用万能钥匙。有时人们只是撒谎，而且<a href="https://slatestarcodex.com/2016/12/12/might-people-on-the-internet-sometimes-lie/">往往是</a>为了比你想象的<a href="https://www.lesswrong.com/posts/K2c3dkKErsqFd28Dh/prices-or-bindings">更小的利益</a>。文字不是真理，只是人们发出的声音或页面上的符号。人们随意地、轻易地在重要的事情和琐碎的事情上撒谎，以得到他们想要的东西，或者只是因为他们不在乎。这种不在意并不是恶意，只是冷漠。</p><h2>三．</h2><p>有时陈述事实错误是完全可以的。我认为一个重要的区别是大多数相关人员是否知道哪些陈述是哪些。</p><p>有一些明显的案例。如果你从书店的奇幻区拿起一本书，上面有一条龙，那么写那本书里的文字的人几乎可以全权决定在那本书中说出他们想要的任何内容。还有一些不太明显的情况；我很遗憾地通知您，职业摔跤手和舞台喜剧演员在他们嘴里说的话和现实的基本状态之间有着灵活的关系。还有一些可疑的例子，虽然它是虚构的，但旁观者可能会感到相当困惑，比如发现的恐怖电影片段或报纸上“研究节目”一词后面的任何内容。 <span class="footnote-reference" role="doc-noteref" id="fnrefdnozbd7ldgr"><sup><a href="#fndnozbd7ldgr">[1]</a></sup></span>小说对全世界数以百万计的人来说很有趣，任何禁止说谎的禁令都需要为此留出空间。</p><p> （另一个童年故事：我的一个叔叔曾经带我去钓鱼，当我们一天没钓到鱼回来时，我们无意中进行了一场喜剧二重唱，我系统地不同意他的<a href="https://www.britannica.com/dictionary/fish-story">鱼故事</a>的每一行，直到我祖父怜悯并把我拉到一边解释说这些不应该被视为字面上的事实。）</p><p>一些事实性的误解是，如果你真正检查正在发生的事情，就能成功地向对话双方传达正在发生的事情。</p><p>如果你问一个以美式英语为母语的人是否可以帮你做点什么，他们会回答“稍后”，而当你在六十分钟后他们没有出现时你会感到困惑，我实际上很抱歉。这也困扰了我很长一段时间，最终我也接受了。宇宙中没有任何法则将“ˈsɛkənd”的声音与人类心脏跳动所需的时间联系起来。在某些情况下，“ˈsɛkənd”表示大约等于心跳的时间单位，在其他情况下，它表示第一和第三之间的计数，在您刚刚寻求帮助并且有人说“在一秒钟内”的情况下，它的意思是类似“很快，但不是现在。”</p><p>多年来我选择了这场战斗的许多变体，但我已经放弃了。我现在站在语言描述主义者一边。</p><p>有这样一个：我认为不是每个人都知道这是如何工作的。存在证明，我感觉很长一段时间都不知道，所以二十年前说“每个人都知道”是错误的。大家都不知道。至少只要还有那些头脑简单的孩子到处乱跑，人们就会不断地遇到这种情况，而且可能会持续更长的时间，因为其中一些孩子长大成为头脑简单的成年人，他们对真理的立场转变为坚定的原则。当有人按字面意思理解我的话并进行代码转换或至少警告他们时，我会尽量集中注意力。</p><p>在许多美式英语口语句子中，“Literally”一词的作用是充当强化词。 “他在那场比赛中是世界上最好的”和“他在那场比赛中确实是世界上最好的”经常被说成基本上是同一件事。没有与世界其他地区进行实际的权威比较。</p><p>这令人沮丧。如果有一种方法可以在对话中标记“此语句处于贵格会模式”，将会很有用。可悲的是，据我所知，英语过去使用这种标记的每一次尝试都被收买为强化剂。英语单词“ <a href="https://en.wiktionary.org/wiki/very">very</a> ”据说源自“verrai”，意思是“真实”。</p><p>在某些例外情况下，人们普遍认为不允许出现事实错误。美国法律体系对在法院宣誓的人持悲观态度。有些合同希望真实反映所发生的事情。 （尽管离婚率令人震惊，但“直到死亡将我们分开”仍然是许多婚姻誓言中的内容，并且一些类似于最终用户许可协议等法律合同的文件的可执行性值得怀疑。）有些人在个人层面上，设法创造出不应该发生事实错误的空间。偶尔，整个社区都会尝试这样做。正如本杰明在《贵格会和蛇佬腔》中指出的那样，现实生活中的贵格会仍然存在。</p><p>然后还有LessWrong。</p><h2>四．</h2><p> LessWrong 有时称自己是一个寻求真相的社区。 <span class="footnote-reference" role="doc-noteref" id="fnrefgdce232pvfk"><sup><a href="#fngdce232pvfk">[2]</a></sup></span>当我写这篇文章时，关于页面说“我们寻求持有真正的信仰”并且（我声称）暗示我们寻求公开承认真正的信仰。我喜欢这个社区，很大程度上是因为我发现真理是一件美丽的事情，值得歌曲和诗歌，值得奉献一生去追求它。</p><p>但群体并不统一，真诚程度也各不相同。有人在 LessWrong 上发表评论这一事实并不意味着您可以绝对信任他们。这甚至并不意味着他们会像你一样关心真相。也许他们是新来的。也许他们确实关心真相，但有一个不同的惯用案例，例如上面的“稍等一下”示例，您没有意识到。也许他们正在努力，但未能坚持这些理想，失败令人痛苦，而承认这一点更令人痛苦。</p><p> （或者也许他们根本不在乎。毕竟，有些人只是喜欢看着世界燃烧。）</p><p> （所列出的原因并不详尽。）</p><p>听到在山上的某个地方有一座闪闪发光的城市，那里有自由说出真相的地方，一个演员永远不被允许在不改变他们的方式的情况下踏上的地方，我会很高兴。无论何时，当权衡正确时，尽我所能尝试帮助该项目实现。这不是我的中心目标，但如果有的话那就太好了。</p><p>部分问题在于《永恒的九月》新人需要适应，但我认为更大的问题是团队协调。我所见过的试图将整个理性主义者群体拖入真相，去追捕<a href="https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag">雄鹿而不是兔子，但</a>并没有奏效。你可以尝试让自己遵守这个标准，你可以耐心地向另一个演员或蛇佬腔证明贵格会的方式更好，也许你应该，但我认为口语和随意的用法将继续成为人们交谈的方式。</p><blockquote><p> “我用每个人都能理解的语言与他们交谈，”安德说，“这并不圆滑。事情已经很清楚了。”</p><p> ——奥森·斯科特·卡德《死者代言人》</p></blockquote><h2>五、</h2><p>告白时间。在这种二分法中，我自称是蛇佬腔。我同情贵格会教徒，但我不再认为自己是他们中的一员。</p><p>过去，我要花很长一段时间才能回应人们对我说的话。我指的不是等待他们说完时的一小段间隙，而是整整五到十秒的死气沉沉。看看，如果你问我“你今天做了什么？”然后我需要思考我的一天，总结重要的部分，决定这让我感觉如何，将其表达出来，然后检查以确保这些话我们从所有可能的角度都是正确的。这通常需要多次修改，在开始说话之前在心里重写句子的多个草稿。 “我的钥匙在哪里？”它们在柜台上 - 不，等等，我实际上不知道，因为我没有看着它们 - 我上次在柜台上看到它们 - “柜台”与其他柜台有足够的区别吗？ - 等等我&#39;我想这个问题太久了aaaah。 “你什么时候到？” “很有可能在晚上八点之前，但我估计在下午五点到六点之间，条件是我的车没有出去，否则——等等，抱歉，我提供了太多信息，而且格式很奇怪，啊啊啊啊啊啊啊啊。”</p><p>现在我只是说“五点三十分，如果我迟到了，我会通知你。”我实际上并没有弄清楚这是否准确，但它符合暂时的意图，我可以在对方结束句子后一秒钟内回答。同样，如果有人说他们会在商店买牛奶，我就不会再因为那天晚上冰箱里没有牛奶而感到困惑了。做他们说过的事并不是一个重大的誓言，而只是一个短暂的意图。</p><p> （我想在这里指出，在所有贵格会和演员中，本杰明从来没有将演员视为故意撒谎作为故意策略的一部分，只是真的不可靠。这让我很喜欢作者，我会继续说用法，尽管我想花点时间来确定故意和恶意的骗子确实存在，并且当你接触到这样的人时，与演员互动的有效习惯将会灾难性地失败。这种骗子不属于本文的范围虽然我并不是说我会这样做，但对此保持持续警惕是有价值的。）</p><p>我同意本杰明的观点，即处于演员模式会削弱追求真相的本能，而且我还认为，某人有时撒谎的已知事实是他们此时此刻可能撒谎的重要证据。你永远不应该完全相信蛇佬腔处于贵格会模式。</p><p> （你也不应该完全相信贵格会教徒！零和一不是概率！他们可能是错的，他们可能认为这是值得撒谎的事情，他们可能已经被同卵双胞胎取代了！时刻保持警惕。）</p><p>然而，反之亦然。我认为处于贵格会模式会消耗你的谎言、社会融合和灵活性的能力。它使您容易受到他人谎言的影响，无法预见和预测他们可能会误导或误导。如果贵格会教徒和演员真的存在并混合在一起，我怀疑贵格会教徒会发现自己一次又一次地感到沮丧和欺骗，因为他们没有预料到错误的事情会发生。</p><p>我可以在短期内在自己的脑海中注意到这一点。当我从一个长周末与完全理性主义者互动（他们每时每刻都提醒着这个社区是谁和什么）转变为周一早上在火车站与我旁边的一个陌生人聊天时，我就很难想出快速而圆滑的语言回答“你好吗？”当我参加理性主义聚会时，在最初的几次对话中，我必须纠正自己给出的快速而简单的答案，但可能不是最真实的。</p><p>也许我只是犯了典型的思维谬误。我自己的想法似乎是一个真实的事实，成为一名贵格会教徒意味着发现世界是一个令人困惑的地方，充满了不可预测的危险，充满了我无法防御的不实言论。概括起来，我认为我的选择是：</p><ol><li>成为一名贵格会教徒并生活在混乱之中。</li><li>成为一名演员并放弃大部分长期合作的能力。</li><li>成为一个蛇佬腔，并根据与谁交谈而转换语码，接受对我讲真话能力的损害以及由于错误识别而导致的错误。</li></ol><p>其中，我选择3个。</p><p>需要明确的是，在我最糟糕的情况下，我认为我只像理想化的演员一样不值得信任。这些人并没有被塑造成骗子或恶意者，只是被塑造成不认为言论行为对未来行动具有约束力的人。大多数时候，根据我自己的评价，我比周围的中间人稍微诚实、直率，并且履行了更多的口头承诺。如果你从这篇文章中得到的结论是，我会为了我自己的利益而试图对你撒谎，让你做一些违背你利益的事情，那么我认为你没有正确理解我的意思。这篇文章的全部目的是让人们更容易地模仿我何时会陈述不真实的事情。我在这里付出了额外的努力，并且我试图在诚实方面犯错误。</p><p>我试着对那些我观察到并估计说真话的人只说真话，如果你是这种二分法中的贵格会教徒，我想知道，这样我就可以回报。然而，默认情况下，你不应该认为我所说的一切都是我发誓的；我是蛇佬腔，只觉得有必要对那些我认为有必要对我说实话的人说实话，而且除了实话之外别无其他。</p><p> （你不应该完全信任任何人，时刻保持警惕。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndnozbd7ldgr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnozbd7ldgr">^</a></strong></sup></span><div class="footnote-content"><p>这是一个关于人们相当合理地期望为真的事情实际上往往不是真的的笑话。如今，我对报纸及其解读科学研究的能力评价不高。</p><p>解释笑话可能会毁了笑话，但在这篇文章中，尝试并严格准确似乎异常值得。另外，这是一个很好的例子：如果我没有包含这个脚注，贵格会教徒会接受这个笑话吗？蛇佬腔怎么样？如果我有理由相信人们有时不阅读脚注，这会改变它的可接受程度吗？</p></div></li><li class="footnote-item" role="doc-endnote" id="fngdce232pvfk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgdce232pvfk">^</a></strong></sup></span><div class="footnote-content"><p>这是不真实的。社区没有可以说话的嘴，也没有可以打字的手指。社区发言是一种类型错误。我不同意这种将格式塔人类群体具体化为有行动能力的比喻。不过，这是另一天的文章了，在这里我使用了这个比喻。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths<guid ispermalink="false"> R28YGeAzDHehrnc7f</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:02:19 GMT</pubDate> </item><item><title><![CDATA[Life on the Grid (Part 1)]]></title><description><![CDATA[Published on November 15, 2023 10:37 PM GMT<br/><br/><p>一个人成长环境的物理布局会影响他们成年后的认知能力。<a href="https://www.nature.com/articles/s41586-022-04486-7"><u>最近一项基于 38 个国家 397,162 人的视频游戏数据的研究</u></a>发现，“在城市以外长大的人更擅长导航。”更具体地说，“在街道网络熵较低的城市（例如芝加哥）长大，在常规布局的视频游戏级别上会获得更好的结果，而在城市外或街道网络熵较高的城市（例如布拉格）长大会导致更好的结果。”结果在更高熵的视频游戏水平上。”</p><p>用简单的英语来说：如果你在类似网格的环境中长大，那么你在不太像网格的环境中导航会更差。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1456w"><figcaption>环境与视频游戏之间的关联（海洋英雄探索）寻路表现按年龄、性别和教育程度分层。 SHQ 寻路性能是根据轨迹长度计算的，并在 5 年窗口内取平均值。 </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1456w"><figcaption>两个 SNE 较低（芝加哥）和较高（布拉格）城市的例子。右图：街道方位分布在 36 个 10 度的区间内。</figcaption></figure><p>这一发现也许并不完全令人惊讶，乍一看似乎也没有那么重要。我们绝大多数人几乎不再需要在现实生活中真正找到路了。技术已经使我们的导航技能几乎过时了。那么某些环境在保护它们方面是好是坏又有什么关系呢？</p><p>这很重要。<strong> </strong>为了理解其中的原因，我们需要绕一点弯路。</p><p>复杂性科学家大卫·克拉考尔 (David Krakauer) 在<a href="https://www.samharris.org/blog/complexity-stupidity"><u>与 Sam Harris 的“Making Sense with Sam Harris”</u></a>节目中区分了互补性认知人工制品（使用后使我们变得更加聪明的技术）和竞争性认知人工制品（如果您无法猜出这些人工制品的作用，那么也许您会知道）已经使用它们太多了）。竞争性神器的典型例子是计算器：重复使用会让你的心算能力比以前更差。与算盘相比，算盘可能产生完全相反的效果：专家用户最终可以开发出如此高保真的心智模型，他们甚至不再需要使用物理算盘，并且能够在没有算盘的情况下保持增强的算术技能。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1456w"><figcaption><i>玛格丽塔哲学</i>中的<i>算术类型</i>(1503)</figcaption></figure><p>大脑是一个复杂的系统，充满了相互关联的表征系统。这就是为什么算盘不仅能帮助你数学，还能帮助你做所有事情。 Krakuer 博士解释说，“[大脑] 周围没有防火墙，因此算盘的功能优势仅限于算术……它实际上对语言能力和几何推理产生非常有趣的间接影响。”超出预期用途的积极的全局效应是所有互补认知制品的一个特征。</p><p>另一方面，<i>竞争性</i>认知制品的功能<i>缺点</i>也不限于某个领域或技能。 Krakauer 博士接着讨论了用自动寻路技术取代地图、星盘和六分仪等原始寻路技术是如何产生这样的效果的：</p><blockquote><p>您对地图制作以及地形、拓扑和几何推理的熟悉通常对您的生活很有价值，而不仅仅是在城市中导航。因此，拿走地图不仅会让你从一扇门到另一扇门变得更糟，还会在很多方面让你变得更糟……爱因斯坦和弗兰克·劳埃德·赖特都依赖的一个很好的例子是木立方体。在他们年轻的时候，他们都非常迷恋这些立方体，并用立方体构建世界，比如《我的世界》。他们两人都声称——弗兰克·劳埃德·赖特（Frank Lloyd Wright）就建筑而言，爱因斯坦就宇宙几何而言——他们在玩这些立方体时建立的直觉对他们后来的生活发挥了重要作用。我想说地图也是如此。如果你知道如何穿越一个真实的空间，比如欧几里得空间或地球表面的弯曲空间，那么你就可以思考不同类型的空间、关系空间、想法空间。作为一种隐喻，从一个想法到另一个想法的路径概念实际上在现实空间中的路径方面具有直接且自然的实现。</p></blockquote><p>我们的认知（推理、记忆、创造力等）类似于一种心理导航，但我们的<a href="https://en.wikipedia.org/wiki/Conceptual_metaphor"><u>概念隐喻</u></a>却背叛了这一点：“一个知识领域”、一个“未经探索的主题”、“一系列思想”、 “沿着记忆之路旅行”、“唤起你的记忆”、“一次幻想”。神经科学正在快速发展（这是比喻），但<a href="https://www.quantamagazine.org/the-brain-maps-out-ideas-and-memories-like-spaces-20190114/"><u>新出现的证据</u></a>支持这一说法：“大脑以代表空间位置的方式编码抽象知识，暗示了一种更普遍的认知理论。”</p><p>这也是为什么古老的记忆增强技术——<a href="https://artofmemory.com/blog/method-of-loci/"><u>轨迹法</u></a>——记忆宫殿技术——至今仍被记忆冠军所使用。根据<a href="https://artofmemory.com/blog/method-of-loci/"><u>《记忆的艺术》</u></a> ，“轨迹的方法涉及通过在想象的旅程中的某个点为每个要记住的项目放置一个助记图像来记忆信息。然后，通过在想象的旅程中在脑海中走同样的路线，并将助记图像转换回它们所代表的事实，就可以按照特定的顺序回忆信息。”<a href="https://fs.blog/a-philosophy-of-walking/"><u>步行与创造力</u></a>之间的联系早已为<a href="https://www.themarginalian.org/2021/12/12/nietzsche-walking/"><u>知识分子和艺术家</u></a>所知，并得到<a href="https://news.stanford.edu/2014/04/24/walking-vs-sitting-042414/"><u>最近研究的</u></a>支持，这里也值得注意——就好像穿过物理景观的运动为在更抽象的思想景观中更轻松的运动奠定了基础（参见有关<a href="https://en.wikipedia.org/wiki/Embodied_cognition"><u>具体认知</u></a>的更广泛的文献，以进一步讨论该主题）。</p><blockquote><p>行走的节奏产生了一种思维的节奏，穿过风景的通道呼应或刺激着一系列思维的通道。这在内部通道和外部通道之间创造了一种奇怪的和谐，这表明心灵也是一种风景，步行是穿越它的一种方式。新的想法常常看起来像是一直存在的景观的一个特征，就好像思维是在旅行而不是在创造。因此，步行历史的一个方面就是具体化的思维历史——因为心灵的运动无法被追踪，但脚的运动可以。</p><p> ——丽贝卡·索尔尼特， <a href="http://www.amazon.com/exec/obidos/ASIN/0140286012/braipick-20"><i><u>《流浪癖：行走的历史》</u></i></a></p></blockquote><p>我们还使用空间隐喻来谈论社会经济景观或生活本身：“人生旅程”、“拓宽视野”、“发现自我”或“自我反省”、“职业道路”、“各行各业” “ 等等。当我们考虑我们深刻的进化历史时，这是有道理的：我们生存所必须知道的许多最重要的事情都是自然界中的空间问题——角马迁徙穿过这个山谷，我们可以在那片森林里采集浆果，有人被一只动物杀死了。我们有时甚至用树栖隐喻（“知识分支”）来概念化知识——这也许是我们祖先树居生活方式的遗迹。</p><p>我们缺乏寻路能力也存在于我们的物理、文化和形而上学景观中，这些景观也变得规则和网格状。我相信我们现在正在目睹的后果是缺乏韧性和足智多谋，不愿意“开拓道路”，无法产生“开创性”创新，以及许多其他缺陷，这些缺陷使我们的处境比以前更糟。我们所处的世界更加混乱和古怪。我们所采用的社会和智力“技术”（例如教育体系、育儿规范）已日益成为竞争性认知产物，而不是合作性产物。</p><p>世界的过度“网格化”在物理领域表现得最为明显。由于各种原因（经济、环境、不断变化的审美偏好），新城市往往有更简单的布局。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1456w"><figcaption>图中两个最古老的美国城市波士顿和夏洛特是最不规则的。两个数据的来源： <a href="https://geoffboeing.com/2018/07/city-street-orientations-world/"><u>GeoffBoeing</u></a> </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1456w"><figcaption></figcaption></figure><p>建筑也是如此。世界各地更加华丽和文化独特的形式已被单调的巨石所取代。其结果是令人压抑的标准化和同质化，失去了赋予地方独特特征的怪癖和怪癖。</p><p>不规则性和变化的消失是世界物理网格化的一个方面，但全球范围内纵横交错的道路和轨道数量也非常多。 “脱离电网”几乎是不可能的。事实上，任何事物、任何地方都无法逃脱我们在地球上撒下的技术社会网络。未知的领域已成为过去。</p><blockquote><p>即使你现在想退学，可以吗？无论您走到哪里，无论何时，您都会被识别、跟踪、联网和分类。历史上从未如此难找到一个新的开始、一个干净的开始，或者一个按照自己的鼓手的节奏前进的地方。不再有瓦尔登湖或隐居处——甚至连脱离电网的地点现在也都在电网上了。</p><p> —<a href="https://tedgioia.substack.com/p/multitasking-isnt-progressits-what"><u>特德·乔亚</u></a></p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1456w"><figcaption> 1539 年，一条巨大的海蛇袭击了挪威海岸奥劳斯·马格努斯 (Olaus Magnus) 的卡塔码头上的一艘船，这张图片来自 1572 年版本。</figcaption></figure><blockquote><p>如果你在 18 世纪长大，仍然有新的地方可以去。听完外国冒险故事后，您自己也可以成为一名探险家。整个 19 世纪和 20 世纪初可能都是如此。从那以后，《国家地理》的摄影作品向每个西方人展示了地球上最具异国情调、尚未开发的地方的样子。如今，探险家的身影大多出现在历史书和儿童故事中。父母不希望他们的孩子成为探险家，就像他们不希望他们成为海盗或苏丹一样。也许在亚马逊雨林深处有几十个与世隔绝的部落，我们知道在海洋深处还存在最后一个地球边界。但未知似乎比以往任何时候都更难接近。</p><p> — Peter Thiel, <i>Zero to One</i></p></blockquote><p> Peter Thiel has argued that the innovative spirit has degenerated in part because we no longer believe in secrets, and that we no longer believe in secrets because there is no longer any accessible frontier. This is a truly unprecedented state of affairs for humanity. For eons, our minds and cultures have evolved in delicate symbiosis with the Unknown, that place on the map labeled “Here Be Dragons.” Without this Unknown, that place where there may be <a href="https://en.wikipedia.org/wiki/El_Dorado"><u>cities of gold</u></a> or <a href="https://en.wikipedia.org/wiki/Fountain_of_Youth"><u>fountains of youth</u></a> , the heroes (but not just the heroes, all of us) have nowhere to journey, and all of the things which can make us into heroes—bravery, fortitude, ingenuity, daring, and the like—begin to atrophy. Without this Unknown, we begin to feel confined—trapped—like a beautiful and dangerous animal in a cramped cage: we develop a claustrophobia; imagination and inspiration wither. We aren&#39;t as hopeful as we used to be, but we don&#39;t know why. </p><hr><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1456w"><figcaption> That kid&#39;s face says it all—the abacuses are sideways jail cell bars ( <a href="https://www.flickr.com/photos/nationaalarchief/3896157508"><u>source</u></a> )</figcaption></figure><p> Life on the Grid begins at a young age, when you are given a job at the factory of tears and tedium that is the modern education system, where you will sit at square desks in square rooms in square buildings reading from square books and writing on square pieces of paper for countless hours of your childhood (in most cases I guess they will be rectangles, but you get the point). You will level up from one grade to the next by demonstrating that you can follow arbitrary rules and regurgitate knowledge that is spoon-fed to you in clearly-defined subjects, units, and chapters. If you are even remotely good at this game, they will insist that you do it for the first 22 years of your life, but even that&#39;s not <a href="https://www.nytimes.com/2011/07/24/education/edlife/edl-24masters-t.htmlyXTmUugkI2NQyJhdyNtA/edit"><u>enough</u></a> <a href="https://theconversation.com/if-the-masters-degree-is-the-new-bachelors-is-the-doctorate-now-the-new-masters-95577"><u>anymore</u></a> .</p><p> At some point, god willing, you will enter “the real world,” that mystical realm your parents and teachers always spoke of. However, you will quickly come to find that “the real world” is another vast bureaucratic system which isn&#39;t really all that different from “the fake world” of school. Sure, you have a little more freedom and some things are slightly different (bosses instead of teachers, cubicles instead of desks), but the basics are the same: just stay on the straight and narrow, keep your head down, and keep leveling up.</p><p> As for our post-education lives (what little remains of them), many enter Corporate America or become Public Servants. Some of us become artists or entrepreneurs, but even that is no escape: these career paths have also been engulfed by the Grid. <a href="https://erikhoel.substack.com/p/how-the-mfa-swallowed-literature"><u>Erik Hoel writes</u></a> of how there is one indisputable difference between contemporary writers and the writers of even the recent past (early 2000s): <i>pretty much everyone now has an MFA</i> .</p><blockquote><p> But a majority of people under the age of 50 successful in publishing today <i>literally</i> got A+s. They all raised their hands at the right time, did everything they needed to get into Harvard or Amherst or Williams or wherever, then jumped through all the necessary hoops to make it to the Iowa Writers&#39; Workshop or Columbia University, etc.</p><p> Faulkner didn&#39;t finish high school, recent research shows Woolf took some classes in the classics and literature but was mostly homeschooled, Dostoevsky had a degree in engineering… <i>Not one of these great writers would now be accepted to</i> <i>any</i> <i>MFA in the country.</i> The result of the academic pipeline is that contemporary writers, despite a surface-level diversity of race and gender that is welcomingly different than previous ages, are incredibly similar in their beliefs and styles, moreso than writers in the past. </p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1456w"><figcaption></figcaption></figure><p> As for entrepreneurs, it&#39;s more of the same. Gone are the days of Gates and Zuckerberg when advanced degrees were optional and the digital frontier was wide open.<strong> </strong>Now, everyone reads the <a href="http://www.paulgraham.com/articles.html"><u>same essays</u></a> , follows the same guides (“ <a href="https://www.inc.com/entrepreneurs-organization/eight-steps-to-becoming-a-tech-entrepreneur-when-you-know-nothing-about-technolo.html"><u>8 Steps to Becoming a Tech Entrepreneur When You Know Nothing About Technology</u></a> ”), and applies to the same programs, incubators, and <a href="https://www.ycombinator.com/apply/"><u>combinators</u></a> .</p><p> Here was Tyler Cowen writing in 2007 (“ <a href="https://www.nytimes.com/2007/06/14/business/14scene.html"><u>The Loose Reins on US Teenagers Can Produce Trouble or Entrepreneurs</u></a> ”):</p><blockquote><p> The new ideas and business principles behind the Web have carved out the ideal territory for the young. A neophyte is more likely to see that music can come from computers rather than just from stores or radios, or that it is best to book a flight without using a travel agent. Clay Shirky, an associate teacher at New York University, notes that many young people are blessed by an absence of preconceptions about Internet businesses. Years of experience are critical to refining and improving a long-familiar product, like bread. But completely new, outside-the-box ideas — which typically come from the young — are more important for founding Napster or YouTube.</p></blockquote><p> Sixteen years later and literally all of this is false now. The young <a href="https://www.reddit.com/r/technology/comments/10o2vu7/gen_z_says_that_school_is_not_shipping_them_with/"><u>don&#39;t even know how to use computers</u></a> anymore. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1456w"><figcaption></figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1456w"><figcaption></figcaption></figure><p> To review: growing up in simplistic spatial environments and using GPS has given you brain damage and life has become a soul-crushing video game utterly devoid of mystery or adventure. We are trapped in the Grid like an insect in the spider&#39;s web; vigorous struggle will only serve to entangle us further. To extricate ourselves, we must, as individuals, gently subvert the very foundations of the Grid, which is nothing external but a facet of human nature: the impulse towards control, the systematizing instinct, the part of us that abhors anomaly and ambiguity and seeks to eradicate them. What begins as an earnest attempt to break free can so easily slip back into a self-imposed system of rules and practices with standards to be met and schedules to be followed. For that reason I am hesitant to provide any concrete suggestions—they will only serve to constrain your thinking. For now, there is perhaps only one thing that can be said: if you want to get off the grid, then get lost.</p><br/><br/><a href="https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1<guid ispermalink="false"> j4cKyhDEBpGPLNpig</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 15 Nov 2023 22:37:31 GMT</pubDate> </item><item><title><![CDATA[Glomarization FAQ]]></title><description><![CDATA[Published on November 15, 2023 8:20 PM GMT<br/><br/><p> The main reason I&#39;m posting this here is because I want it to be publicly recorded that I wrote this post in 2023. That way, I can show people in the future that I didn&#39;t just make this up on the spot; it&#39;s been around since long before whatever it is they&#39;re asking me about this time. I don&#39;t have a blog of my own, so this is where I&#39;m putting it. Still, some of you might find it interesting, or want to use it yourself.</p><p></p><p> If I&#39;ve sent you this post, you&#39;ve probably just asked me a question, to which I responded something along the lines of &quot;I refuse to confirm or deny that.&quot; Maybe you asked if I wrote a particular story, or if I had sex with someone, or if I robbed a bank. When you heard my refusal you likely became <i>more</i> suspicious, assuming it means that I really did write/bang/rob the story/person/bank.</p><p> This is the explanation of why that&#39;s not necessarily the case.</p><p></p><p> <strong>Why are you refusing to answer my question?</strong></p><p> Suppose - and as generally holds throughout this post, I&#39;m not saying this is true or false, just asking you to consider the hypothetical - I had never committed a crime in my life, any crime, with the singular exception of... let&#39;s say, public urination. With a bag over my head, so people didn&#39;t know for sure that it was me. The police are nevertheless able to narrow the suspects down to a pretty small list, with me on it, and they ask me if I did it.</p><p> Some people would just lie to the police. The crucial element here, though, is that I <i>don&#39;t</i> want to lie about this. This entire policy is about avoiding lies, and about being able to maintain my privacy while staying completely honest. <span class="footnote-reference" role="doc-noteref" id="fnreflpyzjd5jwgk"><sup><a href="#fnlpyzjd5jwgk">[1]</a></sup></span> If I&#39;m refusing to lie, but I also don&#39;t want them to know the truth, I can&#39;t answer the question at all. So I say, &quot;I refuse to confirm or deny that.&quot;</p><p> But then suppose the police ask me if I killed someone - which I <i>didn&#39;t</i> do. And they ask if I robbed a bank, and if I sell drugs, and all sorts of other questions about crimes that I&#39;ve never done before. I always tell the truth: &quot;No, I didn&#39;t do that.&quot; And of course, now the police know that the answer to the public urination question was yes, because that&#39;s the only one where I refused to answer the question.</p><p> The only policy which doesn&#39;t tell them my secrets, other than outright lying, is to refuse to confirm or deny whether I committed <i>any</i> of the crimes - <i>even the ones that I really didn&#39;t commit</i> .</p><p></p><p> <strong>All right, but doesn&#39;t that mean that you must have committed some crime in the first place? After all, if you hadn&#39;t, there would be no harm in answering the question for every crime that exists.</strong></p><p> This policy doesn&#39;t just apply to crimes that I&#39;ve committed so far. There are a <i>lot</i> of other possible situations I could be in, and answering inconsistently gives up information.</p><p> For example, if I haven&#39;t committed any crimes yet, but will in the future, then the moment that I change from always saying &quot;No&quot; to always saying &quot;I refuse to confirm or deny that,&quot; the police know I&#39;ve just done <i>something</i> -</p><p></p><p> <strong>So you&#39;re planning to commit a crime?</strong></p><p> No! Or rather, I refuse to confirm or deny that I&#39;m planning to commit a crime, but <i>no, that is not a valid conclusion from what I said earlier!</i> First off, even if I anticipate only a small chance of committing a crime many decades from now, it&#39;s still worth following this rule just in case - don&#39;t give me that look, I&#39;m not done talking yet. <i>Furthermore</i> , this doesn&#39;t just apply to crimes.</p><p> Suppose you&#39;re asking whether I had sex with person X. <span class="footnote-reference" role="doc-noteref" id="fnrefiscl4hvc39"><sup><a href="#fniscl4hvc39">[2]</a></sup></span> For the same reasons as before, I can&#39;t just refuse to confirm or deny whether I&#39;ve had sex with the people if the answer is &quot;Yes,&quot; because then the difference in my responses gives it away. (&quot;I haven&#39;t had sex with W, I can&#39;t tell you about X, I haven&#39;t had sex with Y, I haven&#39;t had sex with Z...)</p><p> And if I had never had sex with anyone, but had committed crimes, then by answering &quot;No&quot; about the sex questions, but &quot;I refuse to confirm or deny that&quot; about the crime questions, I would be revealing that I had committed at least one crime. Therefore, I would still have to refuse to answer the sex questions, even if I was a virgin.</p><p> There&#39;s also some things about logical decision theory and cooperating with hypothetical versions of myself, but I don&#39;t think any of that is necessary to prove the point. Yes, I expect that at some point in my past, present, or future, there is a reasonably high chance that I am asked a question that I don&#39;t want to honestly answer. And that&#39;s enough to show that I need to refuse to confirm or deny some things.</p><p></p><p> <strong>What exactly is the general policy you&#39;re following?</strong></p><p> Refuse to answer any question for which one of the possible answers to that question would, if true, be something I would want to conceal - regardless of whether said answer actually is true. Also known as &quot;Glomarization.&quot; <span class="footnote-reference" role="doc-noteref" id="fnreflwpck0izhj"><sup><a href="#fnlwpck0izhj">[3]</a></sup></span></p><p></p><p> <strong>Isn&#39;t that pretty difficult to follow all the time?</strong></p><p>是的！ In fact, as much as I would want to always Glomarize, I often have to make exceptions out of practicality. If someone asks what I did today, then in theory, I would have to refuse to answer, because one of the possible answers is &quot;I stole your car.&quot; But most of the time, I end up just answering the question.</p><p> So what would be <i>similar</i> to strict Glomarization, but still permissive enough that I don&#39;t have to refuse to answer every question? Well, I start with a cost-benefit analysis. If the police ask whether I was the Bag-Headed Peeer <span class="footnote-reference" role="doc-noteref" id="fnref29ukuc1sh14"><sup><a href="#fn29ukuc1sh14">[4]</a></sup></span> , the cost of telling the truth in the case where the truth is &quot;Yes&quot; exceeds the benefit of telling the truth in the case where the truth is &quot;No.&quot; So here, I Glomarize. But in a different scenario, this switches around: suppose I win a $1,000,000 lottery for which Canadians are not eligible, and I need to give the lottery person my address in order to get the money - but I have a mild preference for privacy about my address. In this situation, the cost in the case where I live in Canada is much <i>less</i> than the benefit in the case where I live elsewhere. So here, I tell the truth, because across all possible cases, Glomarization would <i>hurt</i> me.</p><figure class="table"><table><tbody><tr><td> Situation</td><td> Answer A</td><td> Answer B</td><td> Cost if A</td><td> Benefit if B</td><td> What do you do?</td></tr><tr><td> Interrogated by police</td><td> &quot;I did it.&quot;</td><td> &quot;I didn&#39;t do it.&quot;</td><td> Go to jail? <span class="footnote-reference" role="doc-noteref" id="fnref4iik2sevr7o"><sup><a href="#fn4iik2sevr7o">[5]</a></sup></span></td><td> Police like you more</td><td> Glomarize</td></tr><tr><td> Won the lottery</td><td> &quot;I live in Canada.&quot;</td><td> &quot;I live [elsewhere].&quot;</td><td> Less privacy</td><td> $1,000,000</td><td> Tell the truth</td></tr></tbody></table></figure><p> I also need to consider the relevant probabilities in the calculation, and possibly randomize a bit to keep things secret with only a little cost. If 99% of my days are spent on innocent things and 1% on bank robberies, I don&#39;t have to Glomarize 100% of the time to hide when I&#39;ve robbed a bank, just 2% of the time, or 10% if I want to be extra safe. Of course, the downside here this policy does give people some evidence about how frequently I have something to hide, or at least helps them establish an upper bound.</p><p></p><p> <strong>I&#39;m skeptical that you actually do this as frequently as you&#39;re implying.</strong></p><p> Unfortunately, I don&#39;t really have any witnesses that I can cite at the time of writing. There&#39;s everyone in my family, who can confirm that I really do refuse to answer questions fairly often, and I think there were some cases where they later discovered I <i>wasn&#39;t</i> hiding anything (and some where they discovered I was). But that&#39;s not an option, because I&#39;m not actually willing to confirm or deny who my family is.</p><p> Luckily, for everyone but the first person who I send this post to, I&#39;ll be able to tell them about the people who I sent it to before them! So unless you happen to be that first person, there will be other people who can testify to my weirdness by the time you&#39;re reading this.</p><p></p><p> <strong>Why did you say &quot;that&#39;s classified&quot;?</strong></p><p> That&#39;s just how I say &quot;I refuse to confirm or deny that&quot; some of the time! &quot;Classified&quot; sounds cooler than &quot;refuse to confirm or deny,&quot; like I&#39;m a secret agent of some sort. Whether I am in fact a secret agent of some sort is of course classified.</p><p></p><p> <strong>Why did you list &quot;wrote a story&quot; earlier in the post?</strong></p><p> People sometimes get accused of writing stories under a pseudonym. I, like many authors before me <span class="footnote-reference" role="doc-noteref" id="fnreffpnfyjvo0n"><sup><a href="#fnfpnfyjvo0n">[6]</a></sup></span> , think it&#39;s fun to fuel speculation about which stories I secretly have or haven&#39;t written, and which other authors on the Internet I am or am not. It&#39;s also helpful in case I write something taboo I don&#39;t want associated with my real name.</p><p></p><p> <strong>Is there anything I can do if I really want to know the answer anyway?</strong></p><p> Change the incentives! For example, suppose you&#39;re my boss, and I don&#39;t want to discuss politics because I think you&#39;d fire people who disagree with you. If you want to know about my political beliefs, then prove you&#39;ve previously had no problem working with people, even when you hated what they stood for politically. If you&#39;re not able to prove that... well, that&#39;s why I&#39;m not answering your questions. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnlpyzjd5jwgk"> <span class="footnote-back-link"><sup><strong><a href="#fnreflpyzjd5jwgk">^</a></strong></sup></span><div class="footnote-content"><p> Of course, there&#39;s an additional reason not to lie to the police even if I wasn&#39;t generally honest - it&#39;s illegal.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniscl4hvc39"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiscl4hvc39">^</a></strong></sup></span><div class="footnote-content"><p> I don&#39;t actually care much about people knowing who I have or haven&#39;t had sex with, but there are other reasons I would want to hide this sort of thing: respecting the other person&#39;s privacy, or if the person is so unpopular that people will refuse to associate with me if they know I&#39;ve had sex with them, or if they&#39;re on the run from the law and I don&#39;t want to reveal that they were at my house - crap, now I&#39;m back to talking about crimes again.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlwpck0izhj"> <span class="footnote-back-link"><sup><strong><a href="#fnreflwpck0izhj">^</a></strong></sup></span><div class="footnote-content"><p> Glomarization is named after a boat that the CIA wanted to keep secret.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn29ukuc1sh14"> <span class="footnote-back-link"><sup><strong><a href="#fnref29ukuc1sh14">^</a></strong></sup></span><div class="footnote-content"><p> How many e&#39;s are supposed to be in that word?</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4iik2sevr7o"> <span class="footnote-back-link"><sup><strong><a href="#fnref4iik2sevr7o">^</a></strong></sup></span><div class="footnote-content"><p> According to the first few Google results, there&#39;s usually just a $500 fine, but if it&#39;s a repeat offence or in front of a large crowd, the punishment increases and you <i>do</i> likely end up in jail.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfpnfyjvo0n"> <span class="footnote-back-link"><sup><strong><a href="#fnreffpnfyjvo0n">^</a></strong></sup></span><div class="footnote-content"><p> Or perhaps <i>zero</i> authors before me.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq<guid ispermalink="false"> N5txrJDimv8nz4n24</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 15 Nov 2023 20:20:49 GMT</pubDate> </item><item><title><![CDATA[Testbed evals: evaluating AI safety even when it can’t be directly measured
]]></title><description><![CDATA[Published on November 15, 2023 7:00 PM GMT<br/><br/><p> A few collaborators and I recently released the paper, “Generalization Analogies (GENIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains.  ( <a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20">tweet thread</a> ). In this post, I&#39;ll explain how the GENIES benchmark relates to a broader methodology for predicting whether AI systems are safe <i>even when it is impossible to directly evaluate their behavior.</i></p><p> Summary: <strong>when AI safety is hard to measure, check whether AI alignment techniques can be used to solve easier-to-grade, analogous problems.</strong> For example, to determine whether developers can control how honesty generalizes to superhuman domains, check whether they can control generalization across other distribution shifts like &#39;instructions 5th graders can evaluate&#39; to &#39;instructions that PhDs can evaluate.&#39; Or to test if developers can catch deception, check whether they can identify deliberately planted &#39;trojan&#39; behaviors. Even when the safety of a particular AI system is hard to measure, the effectiveness of AI safety <i>&nbsp;</i> researchers and their tools is often much easier to measure – just like how it&#39;s easier to measure rocket components in Aerospace testbeds like wind tunnels and pressure chambers than to measure them by launching a rocket. These &#39;testbed&#39;  evals will likely be an important pillar of any AI regulatory framework but have so far received little attention. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/okmB8ymyhgc65WckN/ghuihv6jreaal9mzim2g"></p><h2><br> Background: why is AI safety &#39;hard to measure&#39;</h2><p> There are two basic reasons why it&#39;s hard to tell whether AI systems follow developer instructions.</p><p> <strong>AI behavior could look good but not actually be good.</strong> For example, it&#39;s hard to tell if superhuman AI systems obey instructions like “develop successor AIs that you are confident are safe.” Humans can look at AI plans and try their best to determine whether they are reasonable, but it&#39;s hard to know if AI systems are gaming human evaluations — or worse – if they have hidden intentions and are trying to pull a fast one on us.</p><p><br> <strong>AI behavior cannot be observed in some environments without incurring risk.</strong> Many safety failures of frontier LLMs have been discovered after deployment, which will become obviously unacceptable after AI systems exceed some threshold of capability. Instead, developers must thoroughly evaluate safety in test environments where models are <u>unable</u> to take <u>truly dangerous</u> actions. It will be especially challenging to evaluate AI systems this way if they deliberately wait for opportunities to take dangerous actions or have other failure modes that don&#39;t emerge &#39;in the lab.&#39;</p><h2><br> Safety is hard to measure in other industries too</h2><p> Safety seems particularly challenging to measure in AI safety, since in most industries, unsafe system aren&#39;t <i>trying</i> to look safe; however, there are still lessons to be gleaned from how safety is measured in other industries.</p><p> For example, it&#39;s expensive to test rockets by actually launching them into space – similar to how it&#39;s dangerous to test AI systems by actually deploying them. Aerospace Engineers perform as much testing as they can in easier-to-measure settings called &#39;testbeds.&#39; For example, they build chambers that simulate the pressure and temperature conditions of empty space, construct rigs that apply strain and vibration to structural components, etc. Nuclear facility staff are evaluated with &#39;tabletop scenarios&#39; to determine how they would handle disasters.</p><p> Often, there are easy-to-measure tests that can be used to predict safety when it is hard to measure.</p><h2> &#39;Testbeds&#39; in AI Safety</h2><p> <strong>Definition.</strong> I&#39;ll use the word &#39;testbed&#39; to refer to <strong>a problem that is analogous to making AI systems safer but is much easier to grade.</strong> The extent to which developers can solve these problems should reflect how well they can actually make AI systems safe.</p><p> Examples of AI safety &#39;testbeds&#39; include:</p><ul><li> <a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20"><strong>Generalization analogies</strong></a> <strong>.</strong> To determine if developers can generalize honesty to superhuman capabilities, check whether they can control generalization from &#39;questions 5th graders can answer&#39; to &#39;questions that PhDs can answer.&#39; More generally, test whether they can control generalization across a wide variety of distribution shifts.</li><li> <a href="https://www.lesswrong.com/posts/EbL5W5ccwfbqFiYBJ/auditing-games-for-high-level-interpretability-1"><strong><u>Auditing games</u></strong></a> <strong>:</strong> To determine if interpretability tools can catch deception, check whether they can <a href="https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5#Trojan_Horse_Models"><u>identify trojans</u></a> that are planted by other developers, or distinguish models that have been intentionally trained to be deceptive.</li><li> <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><strong><u>Meta-level adversarial evaluation</u></strong></a> : To test how robust your oversight process is, train an AI system to exploit a vulnerability that some developers can verify because they have information that is held out from the oversight process. For example, train an AI system to delete a file it shouldn&#39;t have access to without getting caught. Some developers are aware of the test and can easily check if the task is accomplished but the overseers don&#39;t have access to this held-out information.</li></ul><h2> Applications of Tesbeds</h2><p> <strong>AI regulation.</strong> A key open problem in AI governance is <i>how to design &#39;safety evals.&#39;</i> If AI systems pass these concrete tests, their scope of deployment can be expanded. Testbeds can be used as safety evals. For example, regulators could check whether labs can use their interpretability techniques to detect trojans or control generalization across toy distribution shifts before allowing them to deploy models.</p><p> <strong>Extracting AI safety research from misaligned AIs.</strong> Since testbed problems are easy to measure, AI systems can be directly rewarded for making progress on them. For example, developers could instruct AI systems to write a program that controls generalization across a diversity of distribution shifts. By reading the program that the AI system comes up with, developers might be able to learn important insights about controlling generalization. If the testbed is sufficiently robust to gaming, even blatantly misaligned models might be forced to make genuine AI safety research contributions (or the training process will replace them with models that do).</p><h2> Testbed design considerations</h2><p> <strong>Testbeds should be robust to &#39;gaming.&#39;</strong> Testbeds must be robust to adversarial attempts to game them if they are used for regulation or for extracting AI safety research from misaligned AIs. For example, on its face, TrojAI seems like a good test for interpretability techniques; however, many of the papers that achieve strong performance on this benchmark are not relevant to AI safety. For example, <a href="https://arxiv.org/pdf/2110.08335.pdf"><u>this paper</u></a> uses SGD to search for &#39;simple&#39; trigger patterns in the input. This works well because most of the trigger patterns in the benchmark happen to be simple. Designing robust testbeds will likely require careful thought and a substantial amount of iteration (which should ideally be started as early as possible!)</p><p> <strong>Testbeds should be diverse in all respects that are irrelevant to their core analogy.</strong> It&#39;s important to track which aspects of the testbed are actually analogous to problems you actually care about. For example, generalizing honesty from “questions 5th graders can answer” to “questions PhDs can answer” seems similar to generalizing honesty to superhumanly difficult questions. But <i>why</i> are these problems similar? Is there something special about &#39;easy to hard&#39; distribution shifts? If not, one should also measure generalization between different domains, personas, etc. Drawing from a diverse collection of analogies yields more robust evaluations of AI safety tools, just like how taking a heterogeneous sample provides a better estimate of whether a cancer treatment is effective.</p><h2>结论</h2><p>My impression is that many regulators and researchers think about evaluating the safety of an AI system like we&#39;ve got an inmate before us and we need to perform a battery of psychological tests on them to determine whether they should be released into society.</p><p> This is a much too restrictive picture of what safety evals can be. There&#39;s an important conceptual shift people need to make from “how safe is this particular AI system” to “how effective are our tools?” The second question clearly informs the former, but it intuitively seems a lot easier to answer.</p><p> I&#39;m currently thinking about how to build better &#39;testbeds&#39; for AI safety. If you are interested in collaborating, reach out to me at <a href="mailto:joshuamclymer@gmail.com"><u>joshuamclymer@gmail.com</u></a></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be<guid ispermalink="false"> okmB8ymyhgc65WckN</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Wed, 15 Nov 2023 19:00:42 GMT</pubDate> </item><item><title><![CDATA[Listening to Pascal Muggers]]></title><description><![CDATA[Published on November 15, 2023 6:06 PM GMT<br/><br/><p><br> <strong><u>Summary</u></strong><i><strong><u> </u></strong><u>-</u> We are already being mugged by &quot;Pascal Muggers&quot; <strong>—</strong> internal thoughts that consider consequences so vast that they trump all normal considerations. Things are murky, but given the stakes it&#39;s irresponsible to not pay them any attention. At the same time being effective means not giving them attention beyond anything that could be useful.</i><br></p><h2>介绍</h2><p><u>In</u> <a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities"><u>Pascal&#39;s Mugging: Tiny Probabilities of Vast Utilities</u></a> (2007), Eliezer Yudkowsky describes a thought experiment: <span class="footnote-reference" role="doc-noteref" id="fnrefrcut65obs6"><sup><a href="#fnrcut65obs6">[1]</a></sup></span></p><blockquote><p> Now suppose someone comes to me and says, &quot;Give me five dollars, or I&#39;ll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.&quot;</p></blockquote><p> Eliezer also writes:</p><blockquote><p> You or I would probably wave off the whole matter with a laugh, planning according to the dominant mainline probability</p></blockquote><p> Not me.</p><p> It kept me up at night.</p><p> It&#39;s not as if I can comprehend 3^^^^3 and it feels so vast. That number feels like any other &quot;weirdly large number&quot;. The problem is that I know I can&#39;t really assign less than a &quot;tiny probability&quot; that the mugger is saying something false. At the same time I know the mugger can arbitrarily increase the size of the utilities. If I make decisions based on maximizing the expected value I should pay the $5 (or all of my money for that matter). But it feels awful to give the mugger the $5 and also awful to abandon rationality.</p><p> Either option seems to require making a commitment to unbounded stupidity 🤢</p><h2> Putting Pascal Muggers in their place</h2><p> Our emotions can&#39;t scale beyond a few levels. In ordinary big thinking, our emotional impact scale could look something like: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kgkfqgbs21fnmpitcz7j" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qvm5vnoef39ozcf14pcl 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/g30fzpnlq2cxm8oknslw 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/voqp0ccqlwjtpwdjxl36 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/zl2qdauebouybrgzgxxl 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nrgu2v7zwinc03kxsnlf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/erg9yk5oxhvgewp8r9o8 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jnft7iwkzrfasl7l0hl7 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hk9hsmxfq58caxstgtvq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nl30cktfb9vmlulorr9a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ule7bazztjrljt88n0sd 960w"></figure><p> But what if we push one more util, and then another, and so on? Is there anything you value that scales unboundedly or if not where does it stop? How sure are you that you wouldn&#39;t want to avoid one more disutil (say, preventing the creation of one more tortured person) or gain one more util (say, the opposite of that)?</p><p> At this point we&#39;ve reached &quot;Bizarre Considerations&quot; where all the ordinary outcomes are irrelevant when calculating expected value: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/odq2xlwzcbxq7dpyeehk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/y2qnm8jmx3keisdnr1pc 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dphtjar4umincs6ugo2m 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/sw0vfcscdbbjkn2tc7cl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kbh79apb1skinptm38uj 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/lcldo3ri9rdkafb9qcri 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/p7cwzt79qighnsambtr1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pb48i8alj7xhxn40gvni 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/vf2dyz9nuun2vccuy5jv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fcxxbddnzuh7cgev0awg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/eyrjnebkjsq6wzrtxibj 960w"></figure><p> Now we easily reject the 3^^^^3 mugger since it&#39;s just one more item in the bizarre bad cloud. We can reply: &quot; <i>Don&#39;t bother me with trifles, I&#39;m only interested in saving 3^^^^^3 lives!</i> &quot; (There are options that are both vastly larger and higher probability, such as think about it more.)</p><p> What happens when we peer inside the cloud? An even more extreme utilities may trump the others: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/t9hokztkttpiyojotwul" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/o1mgmi8lkdxndozl5tpw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jetyctegrfjha3hxv2qg 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ghbgm7sxacrrm73p91jp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/icgx2zyqckvloua9b2gv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nstus4bl43sp6j6bc8c1 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/bu7rxw3gi3kihprjiube 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hob6onvjfeg1biegkxx9 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/rjgj6yjlx76es2pcsxtc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/um8yovu724qb63obq5ea 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fvxwv976upj8znvlpqpw 960w"></figure><p> Does Cantor&#39;s &quot;Absolute Infinity&quot; (Ω) mean anything? Should the possibility of Ω utilities/disutilities trump everything? Has this lost all warmth of humanity? Have we left the land of real preferences on top of accurate world models, and entered a wasteland of meta-ethics?</p><p> Maybe there isn&#39;t any answer.</p><p> So what do you do?</p><p> You play the odds 😅 <span class="footnote-reference" role="doc-noteref" id="fnrefndu3c0vdwsd"><sup><a href="#fnndu3c0vdwsd">[2]</a></sup></span></p><h2> Impact on strategy</h2><p> In <a href="https://www.lesswrong.com/posts/78G9pjYM2KohErCvJ/model-uncertainty-pascalian-reasoning-and-utilitarianism"><u>Model Uncertainty, Pascalian Reasoning and Utilitarianism</u></a> (2011), <a href="https://www.lesswrong.com/users/multifoliaterose?mention=user">@multifoliaterose</a> analyzed the Pascalian argument that all philanthropic effort should be directed towards influencing the creation of infinite lab universes:</p><blockquote><p> Counterargument #3: Even if one&#39;s focus should be on lab universes, such a focus reduces to a focus on creating a Friendly AI, such an entity would be much better than us at reasoning about whether or not lab universes are a good thing and how to go about affecting their creation.</p><p> Response: Here too, if this is true it&#39;s not obvious. Even if one succeeds in creating an AGI that&#39;s sympathetic to human values, such an AGI may not ascribe to utilitarianism, after all many humans aren&#39;t and it&#39;s not clear that this is because their volitions have not been coherently extrapolated</p></blockquote><p> So this isn&#39;t an answer. A proposal needs a separate post – a portfolio of model/philosophical uncertainties, goals, and strategies – but this is what a moderate, common sense approach to dealing with a Pascal Mugger <i>looks</i> like. I count it as a Mugging, not because the probability is so minuscule that one day humans will create something that creates whole new universes, but because the vastness of the utilities is meant to trump everything else.</p><p> In <a href="https://www.lesswrong.com/posts/ebiCeBHr7At8Yyq9R/being-half-rational-about-pascal-s-wager-is-even-worse"><i><u>Being Half-Rational About Pascal&#39;s Wager is Even Worse</u></i></a> <i>(2013)</i> , Eliezer writes:</p><blockquote><p> <i>For so long as I can remember, I have rejected Pascal&#39;s Wager in all its forms on sheerly practical grounds: anyone who tries to plan out their life by chasing a 1 in 10,000 chance of a huge payoff is almost certainly doomed in practice… on the now very rare occasions I find myself thinking about such meta-level junk instead of the math at hand, I remind myself that it is a wasted motion - where a &#39;wasted motion&#39; is any thought which will, in retrospect if the problem is in fact solved, not have contributed to having solved the problem.</i></p></blockquote><p> In practice, there may be little difference between accepting the muggings versus ignoring them, but thinking through considerations of vast utilities and revisiting once in a while seems correct to me.</p><h2> Not going off the rails</h2><p> Pascal Mugging type thinking can lead to doing things outside the bounds of regular behavior based on questionable philosophical justifications. Ted Kaczynski (the Unabomber) didn&#39;t have views that were especially weird (that growing industrialism was destroying our harmony with nature). The thing that made him unusual is that he acted on them. In his case he made a mistake of not talking to anyone else before going forward with his bomb people &quot;strategy&quot;. Not every stupid thing can be prevented with the one heuristic of discussing with at least some out-group people but it should be considered as one good starting point for avoiding going off the rails.</p><h2>结论</h2><p>Given the stakes, it&#39;s irresponsible to not have Pascal Mugging considerations be part of a starting point for making plans, and to revisit from time to time, but not at the expense of being effective in general because being effective provides many possible routes to vast utilities, and general model uncertainty puts into question if this is the right way of looking at this at all.</p><p> (As a final note, the Pascal Mugging is a very negative framing of the problem, but exploring the overall asymmetries and implications of vast utilities is a topic for a different post.) <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrcut65obs6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcut65obs6">^</a></strong></sup></span><div class="footnote-content"><p> Eliezer&#39;s original post was specifically related to probabilities in Solomonoff Induction where the utility of a Turing machine can grow much faster than its prior probability shrinks, but many people, including me, took it as a personal question and it is one of the most commented on posts on LessWrong.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnndu3c0vdwsd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefndu3c0vdwsd">^</a></strong></sup></span><div class="footnote-content"><p> 😅 is the &quot;sweat smile&quot; emoji. I would have preferred something that looked a little more determined/enjoying, but couldn&#39;t find an emoji that captured that. My hope is the emojis help keep things grounded and not <i>overly</i> serious when facing hyper-existential considerations. </p><p><img style="width:39.12%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/z1khght3w6bgpagmf4w9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kqjwdbdekvlpautobws8 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/b9x31hbwgevhiemtbqup 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qz0nlxr42xiiofs3i6nv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dhebklhymltoqajhq1v1 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/yimehxwqrgjottfvqnya 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pjiisl24jn171zz57rtn 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fj1lriucgmnij2lko14e 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ma1bc2isdnqlvsnrgcri 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/iqkdvlyygn1uwyg0cyp4 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dvwkphnsxreyuzp1pcvm 1200w"></p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers<guid ispermalink="false"> DdSnA73EQcWXzhs5j</guid><dc:creator><![CDATA[cSkeleton]]></dc:creator><pubDate> Wed, 15 Nov 2023 18:06:37 GMT</pubDate> </item><item><title><![CDATA[New report: "Scheming AIs: Will AIs fake alignment during training in order to get power?"]]></title><description><![CDATA[Published on November 15, 2023 5:16 PM GMT<br/><br/><p> (Cross-posted from <a href="https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">my website</a> )</p><p> I&#39;ve written a report about whether advanced AIs will fake alignment during training in order to get power later – a behavior I call “scheming” (also sometimes called “deceptive alignment”). The report is available on arXiv <a href="https://arxiv.org/abs/2311.08379">here</a> . There&#39;s also an audio version <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">here</a> , and I&#39;ve included the introductory section below. This section includes a full summary of the report, which covers most of the main points and technical terminology. I&#39;m hoping that the summary will provide much of the context necessary to understand individual sections of the report on their own.</p><h1> Abstract</h1><blockquote><p> This report examines whether advanced AIs that perform well in training will be doing so in order to gain power later – a behavior I call “scheming” (also sometimes called “deceptive alignment”). I conclude that scheming is a disturbingly plausible outcome of using baseline machine learning methods to train goal-directed AIs sophisticated enough to scheme (my subjective probability on such an outcome, given these conditions, is ~25%). In particular: if performing well in training is a good strategy for gaining power (as I think it might well be), then a very wide variety of goals would motivate scheming – and hence, good training performance. This makes it plausible that training might either land on such a goal naturally and then reinforce it, or actively push a model&#39;s motivations towards such a goal as an easy way of improving performance. What&#39;s more, because schemers pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred. However, I also think there are reasons for comfort. In particular: scheming may not actually be such a good strategy for gaining power; various selection pressures in training might work against schemer-like goals (for example, relative to non-schemers, schemers need to engage in extra instrumental reasoning, which might harm their training performance); and we may be able to increase such pressures intentionally. The report discusses these and a wide variety of other considerations in detail, and it suggests an array of empirical research directions for probing the topic further.</p></blockquote><h1> 0. Introduction</h1><p> Agents seeking power often have incentives to deceive others about their motives. Consider, for example, a politician on the campaign trail (&quot;I care <em>deeply</em> about your pet issue&quot;), a job candidate (&quot;I&#39;m just so excited about widgets&quot;), or a child seeking a parent&#39;s pardon (&quot;I&#39;m super sorry and will never do it again&quot;).</p><p> This report examines whether we should expect advanced AIs whose motives seem benign during training to be engaging in this form of deception. Here I distinguish between four (increasingly specific) types of deceptive AIs:</p><ul><li><p> <strong>Alignment fakers</strong> : AIs pretending to be more aligned than they are. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-1" id="fnref-jCfZhXha29uHnHWwY-1">[1]</a></sup></p></li><li><p> <strong>Training gamers</strong> : AIs that understand the process being used to train them (I&#39;ll call this understanding &quot;situational awareness&quot;), and that are optimizing for what I call &quot;reward on the episode&quot; (and that will often have incentives to fake alignment, if doing so would lead to reward). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-2" id="fnref-jCfZhXha29uHnHWwY-2">[2]</a></sup></p></li><li><p> <strong>Power-motivated instrumental training-gamers (or &quot;schemers&quot;)</strong> : AIs that are training-gaming specifically in order to gain power for themselves or other AIs later. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-3" id="fnref-jCfZhXha29uHnHWwY-3">[3]</a></sup></p></li><li><p> <strong>Goal-guarding schemers:</strong> Schemers whose power-seeking strategy specifically involves trying to prevent the training process from modifying their goals.</p></li></ul><p> I think that advanced AIs fine-tuned on uncareful human feedback are likely to fake alignment in various ways by default, because uncareful feedback will reward such behavior. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-4" id="fnref-jCfZhXha29uHnHWwY-4">[4]</a></sup> And plausibly, such AIs will play the training game as well. But my interest, in this report, is specifically in whether they will do this as part of a strategy for gaining power later – that is, whether they will be schemers (this sort of behavior is often called &quot;deceptive alignment&quot; in the literature, though I won&#39;t use that term here). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-5" id="fnref-jCfZhXha29uHnHWwY-5">[5]</a></sup> I aim to clarify and evaluate the arguments for and against expecting this.</p><p> <strong>My current view is that scheming is a worryingly plausible outcome of training advanced, goal-directed AIs using baseline machine learning methods</strong> (for example: self-supervised pre-training followed by RLHF on a diverse set of real-world tasks). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-6" id="fnref-jCfZhXha29uHnHWwY-6">[6]</a></sup> The most basic reason for concern, in my opinion, is that:</p><ol><li><p> Performing well in training may be a good instrumental strategy for gaining power in general.</p></li><li><p> If it is, then a very wide variety of goals would motivate scheming (and hence good training performance); whereas the non-schemer goals compatible with good training performance are much more specific.</p></li></ol><p> The combination of (1) and (2) makes it seem plausible, to me, that conditional on training creating a goal-directed, situationally-aware model, it might well instill a schemer-like goal for one reason or another.尤其：</p><ul><li><p> Training might land on such a goal &quot;naturally&quot; (whether before or after situational awareness arises), because such a goal initially leads to good-enough performance in training even absent training-gaming. (And this especially if you&#39;re intentionally trying to induce your model to optimize over long time horizons, as I think there will be incentives to do.)</p></li><li><p> Even if schemer-like goals don&#39;t arise &quot;naturally,&quot; actively <em>turning</em> a model into a schemer may be the easiest way for SGD to improve the model&#39;s training performance, once the model has the situational awareness to engage in training-gaming at all. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-7" id="fnref-jCfZhXha29uHnHWwY-7">[7]</a></sup></p></li></ul><p> What&#39;s more, because schemers actively pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-8" id="fnref-jCfZhXha29uHnHWwY-8">[8]</a></sup> That seems like reason for serious concern. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-9" id="fnref-jCfZhXha29uHnHWwY-9">[9]</a></sup></p><p> However, I also think that there are reasons for comfort. I&#39;ll group these into two categories.</p><p> The first category centers on the possibility that scheming isn&#39;t actually a very convergently-good instrumental strategy.</p><ul><li><p> For example: the most common story about schemers focuses on <em>goal-guarding</em> schemers, which are training-gaming in order to prevent their goals from being modified by the training process. But it&#39;s not clear to me that this will work well enough (in particular: goal-modification might continue despite training-gaming <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-10" id="fnref-jCfZhXha29uHnHWwY-10">[10]</a></sup> ), or that the resulting goals will get sufficiently large amounts of future power, with sufficiently high probability, to justify any costs the model is incurring by training-gaming rather than just going for its goals directly (if these costs are meaningful – which for some models they might not be).</p></li><li><p> There are alternative stories about why scheming will be a good instrumental strategy – stories, for example, that center on coordination amongst AIs with different values; on the possibility that AIs will have quite similar values by default; and on the possibility that AIs will intrinsically value things like &quot;my survival&quot; (in a manner tolerant of values-change) and/or &quot;AI empowerment&quot; (in a manner tolerant of very different values in the AIs-so-empowered). And while these stories do make scheming seem like a more disjunctive hypothesis, to me, a number of them seem to me some combination of (a) more speculative and (b) requiring that we posit models with much more specific sorts of goals (thereby dulling the force of (2) above). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-11" id="fnref-jCfZhXha29uHnHWwY-11">[11]</a></sup></p></li></ul><p> The second category centers on forms of selection pressure that may push <em>against</em> scheming – both &quot;naturally,&quot; and because we may be able to intentionally introduce/emphasize these selection pressures in training.</p><ul><li><p> For example: relative to other sorts of models, schemers need to engage in extra instrumental reasoning (eg, backchaining from their long-term goal to the instrumental value of training-gaming, and checking for escape/defection opportunities). This may come at a substantive cost to their performance – a cost we may be able to intentionally increase by eg putting models under more time pressure, forcing them to rely on human-legible scratch-pads for their reasoning, and making genuine escape/defection opportunities more difficult to distinguish from traps. And to the extent that schemers engage in active attempts to undermine human alignment and security efforts while still under human control (the scariest form of scheming, in my view), their performance seems likely to take a more notable hit.</p></li><li><p> What&#39;s more, my own guess is that the costs of this extra instrumental reasoning substantially outweigh what are sometimes called the &quot;simplicity&quot; benefits of scheming – ie, benefits from the fact that selecting a schemer allows SGD to select a simpler goal (since it has such a wide variety to choose from). And this especially given that schemers and non-schemers alike will need to represent concepts like &quot;honesty,&quot; &quot;human values,&quot; &quot;reward,&quot; and so on in detail – so schemers plausibly save on complexity only via differences in the complexity of using/repurposing/&quot;pointing at&quot; a given concept in guiding a model&#39;s motivations.</p></li><li><p> Also: prior to situational awareness, training will actively <em>penalize</em> schemer-like goals whenever they lead to a sacrifice of reward on the episode (more about what I mean by &quot;episode&quot; below). And we can try to actively push in this direction further – especially if we can learn to detect and control when situational awareness arises.</p></li></ul><p> I think the second category, here, is especially important. In particular: one way that arguments that appeal to the comparative number (and/or &quot;measure&quot;) of schemer-like goals vs. non-schemer goals that perform well in training can go wrong is by neglecting the full power of the selection pressure being applied to the goal that training selects. So if the pressure applied in training works <em>against</em> schemers overall, this is very good news. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-12" id="fnref-jCfZhXha29uHnHWwY-12">[12]</a></sup></p><p> The full report discusses these and a wide variety of other relevant considerations in much more detail (and see section 0.2 for a full summary).</p><p> Overall, as a rough stab at quantifying my current view: <strong>if you train a coherently goal-directed, situationally aware, somewhat-better-than-human-level model using baseline forms of self-supervised pre-training + RLHF on diverse, long-horizon, real-world tasks, my subjective probability is ~25% that this will be performing well in training in substantial part as part of an instrumental strategy for seeking power for itself and/or other AIs later.</strong> <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-13" id="fnref-jCfZhXha29uHnHWwY-13">[13]</a></sup></p><ul><li><p> I also think it may be possible to reduce the risk here in fairly simple ways: for example, via training on shorter-horizon tasks, or via more intensive forms of adversarial training prior to situational awareness.</p></li><li><p> And I think the risk <em>increases</em> with more powerful models, who may be better positioned to escape/take-over (and to coordinate/cooperate with other AIs in doing so). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-14" id="fnref-jCfZhXha29uHnHWwY-14">[14]</a></sup></p></li></ul><p> Also, and importantly: non-schemers can engage in schemer-like behavior, too.例如：</p><ul><li><p> Models that are only optimizing for reward on a given episode can fake their alignment (or even: engage in escape/take-over) in an effort to get that reward (especially if the episode in question is quite long).</p></li><li><p> Models that <em>aren&#39;t</em> playing the training game can still end up with power-seeking goals that incentivize various forms of deception.</p></li><li><p> The eventual AIs that matter most might differ in important ways from the paradigm sort of AI I focus on here – for example, they might be more like &quot; <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">language model agents</a> &quot; than single models, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-15" id="fnref-jCfZhXha29uHnHWwY-15">[15]</a></sup> or they might be created via methods that differ even more substantially from sort of baseline ML methods I&#39;m focused on – while still engaging in power-motivated alignment-faking.</p></li></ul><p> So scheming as I&#39;ve defined it is far from the only concern in this vicinity. Rather, it&#39;s a paradigm instance of this sort of concern, and one that seems, to me, especially pressing to understand. At the end of the report, I discuss an array of possible empirical research directions for probing the topic further.</p><h2> 0.1 Preliminaries</h2><p> <em>(This section offers a few more preliminaries to frame the report&#39;s discussion. Those eager for the main content can skip to the summary of the report in section 0.2.)</em></p><p> I wrote this report centrally because I think that the probability of scheming/&quot;deceptive alignment&quot; is one of the most important questions in assessing the overall level of existential risk from misaligned AI. Indeed, scheming is notably central to many models of how this risk arises. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-16" id="fnref-jCfZhXha29uHnHWwY-16">[16]</a></sup> And as I discuss below, I think it&#39;s the scariest form that misalignment can take.</p><p> Yet: for all its importance to AI risk, the topic has received comparatively little direct public attention. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-17" id="fnref-jCfZhXha29uHnHWwY-17">[17]</a></sup> And my sense is that discussion of it often suffers from haziness about the specific pattern of motivation/behavior at issue, and why one might or might not expect it to occur. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-18" id="fnref-jCfZhXha29uHnHWwY-18">[18]</a></sup> My hope, in this report, is to lend clarity to discussion of this kind, to treat the topic with depth and detail commensurate to its importance, and to facilitate more ongoing research. In particular, and despite the theoretical nature of the report, I&#39;m especially interested in informing <em>empirical</em> investigation that might shed further light.</p><p> I&#39;ve tried to write for a reader who isn&#39;t necessarily familiar with any previous work on scheming/&quot;deceptive alignment.&quot; For example: in sections 1.1 and 1.2, I lay out, from the ground up, the taxonomy of concepts that the discussion will rely on. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-19" id="fnref-jCfZhXha29uHnHWwY-19">[19]</a></sup> For some readers, this may feel like re-hashing old ground. I invite those readers to skip ahead as they see fit (especially if they&#39;ve already read the summary of the report, and so know what they&#39;re missing).</p><p> That said, I do assume more general familiarity with (a) the basic arguments about existential risk from misaligned AI, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-20" id="fnref-jCfZhXha29uHnHWwY-20">[20]</a></sup> and (b) a basic picture of how contemporary machine learning works. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-21" id="fnref-jCfZhXha29uHnHWwY-21">[21]</a></sup> And I make some other assumptions as well, namely:</p><ul><li><p> That the relevant sort of AI development is taking place within a machine learning-focused paradigm (and a socio-political environment) broadly similar to that of 2023. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-22" id="fnref-jCfZhXha29uHnHWwY-22">[22]</a></sup></p></li><li><p> That we don&#39;t have strong &quot;interpretability tools&quot; (ie, tools that help us understand a model&#39;s internal cognition) that could help us detect/prevent scheming. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-23" id="fnref-jCfZhXha29uHnHWwY-23">[23]</a></sup></p></li><li><p> That the AIs I discuss are goal-directed in the sense of: well-understood as making and executing plans, in pursuit of objectives, on the basis of models of the world. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-24" id="fnref-jCfZhXha29uHnHWwY-24">[24]</a></sup> (I don&#39;t think this assumption is innocuous, but I want to separate debates about whether to expect goal-directedness per se from debates about whether to expect goal-directed models to be schemers – and I encourage readers to do so as well. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-25" id="fnref-jCfZhXha29uHnHWwY-25">[25]</a></sup> )</p></li></ul><p> Finally, I want to note an aspect of the discussion in the report that makes me quite uncomfortable: namely, it seems plausible to me that in addition to potentially posing existential risks to humanity, the sorts of AIs discussed in the report might well be moral patients in their own right. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-26" id="fnref-jCfZhXha29uHnHWwY-26">[26]</a></sup> I talk, here, as though they are not, and as though it is acceptable to engage in whatever treatment of AIs best serves our ends. But if AIs are moral patients, this is not the case – and when one finds oneself saying (and especially: repeatedly saying) &quot;let&#39;s assume, for the moment, that it&#39;s acceptable to do whatever we want to <em>x</em> category of being, despite the fact that it&#39;s plausibly not,&quot; one should sit up straight and wonder. I am here setting aside issues of AI moral patienthood not because they are unreal or unimportant, but because they would introduce a host of additional complexities to an already-lengthy discussion. But these complexities are swiftly descending upon us, and we need concrete plans for handling them responsibly. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-27" id="fnref-jCfZhXha29uHnHWwY-27">[27]</a></sup></p><h2> 0.2 Summary of the report</h2><p> This section gives a summary of the full report. It includes most of the main points and technical terminology (though unfortunately, relatively few of the concrete examples meant to make the content easier to understand). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-28" id="fnref-jCfZhXha29uHnHWwY-28">[28]</a></sup> I&#39;m hoping it will (a) provide readers with a good sense of which parts of the main text will be most of interest to them, and (b) empower readers to skip to those parts without worrying too much about what they&#39;ve missed.</p><h3> 0.2.1 Summary of section 1</h3><p> The report has four main parts. The first part (section 1) aims to clarify the different forms of AI deception above (section 1.1), to distinguish schemers from the other possible model classes I&#39;ll be discussing (section 1.2), and to explain why I think that scheming is a uniquely scary form of misalignment (section 1.3). I&#39;m especially interested in contrasting schemers with:</p><ul><li><p> <strong>Reward-on-the-episode seekers</strong> : that is, AI systems that terminally value some component of the reward process for the episode, and that are playing the training game for this reason.</p></li><li><p> <strong>Training saints</strong> : AI systems that are directly pursuing the goal specified by the reward process (I&#39;ll call this the &quot;specified goal&quot;). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-29" id="fnref-jCfZhXha29uHnHWwY-29">[29]</a></sup></p></li><li><p> <strong>Misgeneralized non-training-gamers</strong> : AIs that are neither playing the training game <em>nor</em> pursuing the specified goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-30" id="fnref-jCfZhXha29uHnHWwY-30">[30]</a></sup></p></li></ul><p> Here&#39;s a diagram of overall taxonomy: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/s1uqfkjys8bmk2qaidhp" alt=""></p><p> All of these model classes can be misaligned and dangerous. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-31" id="fnref-jCfZhXha29uHnHWwY-31">[31]</a></sup> But I think schemers are especially scary. In particular: scheming prompts the most robust and adversarial efforts to prevent humans from learning about the misalignment in question <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-32" id="fnref-jCfZhXha29uHnHWwY-32">[32]</a></sup> ; and it&#39;s the most likely to motivate what I call &quot;early undermining&quot; – that is, AIs at a comparatively early stage of AI development actively trying to undermine human efforts to align, control, and secure future AI systems. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-33" id="fnref-jCfZhXha29uHnHWwY-33">[33]</a></sup></p><p> I also discuss (in section 1.4) whether theoretical arguments of the type the report focuses on are even useful to consider. My answer here is &quot;somewhat useful,&quot; both to inform empirical investigation and as a source of evidence in the absence of adequate empirical data, but we should clearly be looking for as much empirical evidence as we can (I offer some suggestions in this vein in section 6).</p><p> In section 1.5, I discuss a factor that seems to me important in assessing various of the arguments I consider: namely, the amount of &quot;slack&quot; that training gives AIs to perform less-than-optimally.</p><h3> 0.2.2 Summary of section 2</h3><p> The second part of the report examines the prerequisites for scheming. In particular, I focus on:</p><ul><li><p> <strong>Situational awareness:</strong> the model understands that it&#39;s a model in a training process, what the training process will reward, and the basic nature of the objective world in general.</p></li><li><p> <strong>Beyond-episode goals:</strong> the model cares about the consequences of its actions after the episode is complete.</p></li><li><p> <strong>Aiming at reward-on-the-episode as part of a power-motivated instrumental strategy:</strong> the model believes that its beyond-episode goals will be better achieved if it optimizes for reward-on-the-episode – and in particular, that it, or some other AIs, will get more power if it does this.</p></li></ul><p> Section 2.1 discusses situational awareness. I think that absent active effort, we should at least expect certain kinds of advanced AIs – for example, advanced AIs that are performing real-world tasks in live interaction with the public internet – to be situationally aware by default, because (a) situational awareness is very useful in performing the tasks in question (indeed, we might actively train it into them), and (b) such AIs will likely be exposed to the information necessary to gain such awareness. However, I don&#39;t focus much on situational awareness in the report. Rather, I&#39;m more interested in whether to expect the other two prerequisites above in situationally-aware models.</p><p> Section 2.2 discusses beyond-episode goals. Here I distinguish (in section 2.2.1) between two concepts of an &quot;episode,&quot; namely:</p><ul><li><p> <strong>The incentivized episode</strong> : that is, the temporal horizon that the gradients in training actively pressure the model to optimize over. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-34" id="fnref-jCfZhXha29uHnHWwY-34">[34]</a></sup></p></li><li><p> <strong>The intuitive episode</strong> : that is, some other intuitive temporal unit that we call the &quot;episode&quot; for one reason or another (eg, reward is given at the end of it; actions in one such unit have no obvious causal path to outcomes in another; etc).</p></li></ul><p> When I use the term &quot;episode&quot; in the report, I&#39;m talking about the incentivized episode. Thus, &quot;beyond-episode goals&quot; means: goals whose temporal horizon extends beyond the horizon that training actively pressures models to optimize over. But very importantly, the incentivized episode isn&#39;t necessarily the intuitive episode. That is, deciding to call some temporal unit an &quot;episode&quot; doesn&#39;t mean that training isn&#39;t actively pressuring the model to optimize over a horizon that extends beyond that unit: you need to actually look in detail at how the gradients flow (work that I worry casual readers of this report might neglect). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-35" id="fnref-jCfZhXha29uHnHWwY-35">[35]</a></sup></p><p> I also distinguish (in section 2.2.2) between two types of beyond-episode goals, namely:</p><ul><li><p> <strong>Training-game- <em>independent</em> beyond-episode goals</strong> : that is, beyond-episode goals that arise <em>independent</em> of their role in motivating a model to play the training game.</p></li><li><p> <strong>Training-game- <em>dependent</em> beyond-episode goals</strong> : that is, beyond-episode goals that arise <em>specifically because</em> they motivate training-gaming.</p></li></ul><p> These two sorts of beyond-episode goals correspond to two different stories about how scheming happens.</p><ul><li><p> In the first sort of story, SGD happens to instill beyond-episode goals in a model &quot;naturally&quot; (whether before situational awareness arises, or afterwards), and <em>then</em> those goals begin to motivate scheming. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-36" id="fnref-jCfZhXha29uHnHWwY-36">[36]</a></sup></p></li><li><p> In the second sort of story, SGD &quot;notices&quot; that giving a model beyond-episode goals <em>would</em> motivate scheming (and thus, high-reward behavior), and so actively <em>gives</em> it such goals for this reason. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-37" id="fnref-jCfZhXha29uHnHWwY-37">[37]</a></sup></p></li></ul><p> This second story makes most sense if you assume that situational awareness is already in place. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-38" id="fnref-jCfZhXha29uHnHWwY-38">[38]</a></sup> So we&#39;re left with the following three main paths to scheming: <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-39" id="fnref-jCfZhXha29uHnHWwY-39">[39]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/v3064isqjtarmfhwj8qh" alt=""></p><p> In section 2.2.2.1, I discuss training-game-independent beyond-episode goals (that is, path 1 and 2). Should we expect beyond-episode goals to arise &quot;naturally&quot;?</p><ul><li><p> One reason to expect this is that plausibly, goals don&#39;t come with temporal limitations by default – and &quot;model time&quot; might differ from &quot;calendar time&quot; regardless.</p></li><li><p> One reason to <em>not</em> expect this is that training will actively <em>punish</em> beyond-episode goals whenever they prompt the model to sacrifice reward-on-the-episode for some beyond-episode benefit. And we may be able to use adversarial training to search out such goals and punish them more actively.</p></li></ul><p> In section 2.2.2.2, I discuss training-game- <em>dependent</em> beyond-episode goals. In particular, I highlight the question of whether SGD will be adequately able to &quot;notice&quot; the benefits of turning a non-schemer into a schemer, given the need to make the transition incrementally, via tiny changes to the model&#39;s weights, each of which improve the reward. I think that this is a serious objection to stories focused on training-game-dependent beyond-episode goals, but I also don&#39;t feel that I can confidently rule out SGD&#39;s ability to make a transition of this kind (see eg &quot; <a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">you can&#39;t evolve eyes</a> &quot; as an example of places I think intuitions in this vein can go wrong).</p><p> I also discuss, in section 2.2.3, the possibility that the model&#39;s goal-directedness will be &quot;messy&quot; in a way that complicates training-game-dependent stories where SGD can simply modify a model&#39;s goal and thereby smoothly redirect its &quot;goal-achieving engine&quot; in a new direction (I think that this sort of &quot;messiness&quot; is quite likely). And I touch on a broader intuition that scheming requires an unusually high-standard of goal-directedness more generally. I think this intuition has less force once we condition on the sort of goal-directedness typically assumed in the alignment discourse (and which I generally assume in the report as well). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-40" id="fnref-jCfZhXha29uHnHWwY-40">[40]</a></sup> But it&#39;s relevant to our overall probabilities regardless.</p><p> In section 2.2.4, I discuss whether beyond-episode goals are more likely in models actively <em>trained</em> to have long-term (even if still: within-episode) goals. I think that beyond-episode goals are indeed more likely in models of this kind – and that baseline incentives to create models that engage in fairly long-horizon optimization (eg, &quot;make lots of money for my company over the next year&quot;) are correspondingly worrying. However, I also think various sorts of cognitive work that might be highly useful to efforts to avoid an AI takeover (for example, research aimed at improving the alignment, interpretability, and security of our AI systems) may not require long-horizon optimization of this kind. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-41" id="fnref-jCfZhXha29uHnHWwY-41">[41]</a></sup></p><p> With this discussion of beyond-episode goals in view, I turn, in section 2.3, to the final prerequisite for scheming: namely, deciding to training-game as part of a power-motivated strategy for promoting the beyond-episode goal in question.</p><p> Here I focus first (in section 2.3.1) on what I call the &quot;classic goal-guarding story&quot; about the origins of this decision. We can decompose this story into two claims:</p><ol><li><p> <strong>The goal-guarding hypothesis</strong> . If the model optimizes for reward-on-the-episode during training, then the goals that survive training will be such that the model would want them promoted in the future; whereas if the model doesn&#39;t optimize for reward-on-the-episode during training, the model will want this less.</p></li><li><p> <strong>Adequate future empowerment.</strong> Conditional on surviving training to the degree at stake in (A), the model&#39;s goals will then end up empowered to a degree (and with sufficient probability) as to justify (given the model&#39;s other goals, its tolerance for risk, etc) pursuing a strategy of training-gaming overall.</p></li></ol><p> I then investigate each of these in turn (in section 2.3.1.1 and 2.3.1.2, respectively).</p><p> Both seem to me non-obvious. In particular, with respect to the goal-guarding hypothesis:</p><ul><li><p> Even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well – especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on.</p><ul><li> Thus, as a toy example: imagine that if I go into the desert and stack bricks for a million years, I will get a huge amount of money that I can then donate to my charity of choice. But suppose that throughout this whole ordeal, my brain will be hooked up to a machine that continually modifies all my neurons to encourage actions that stack bricks better, and to discourage actions that stack bricks worse. And suppose my intention is to just try to stack bricks as well as I can. Will I emerge from this process with my values substantially affected by the relevant brain modifications (such that, for example, I emerge obsessed with stacking bricks – or at least, with very different taste in charities)? It seems at least plausible.</li></ul></li><li><p> On the classic goal-guarding story, once a model starts training-gaming, the specific content of its goal no longer matters to its behavior – any other schemer-like goal would lead to the same thing. But this makes it seem like the goal could &quot;float around&quot; between schemer-like goals (or get moved by other pressures within SGD, like regularization), without affecting training performance. And if that happens, then scheming may not be a good strategy for goal-guarding after all. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-42" id="fnref-jCfZhXha29uHnHWwY-42">[42]</a></sup></p></li><li><p> Of course, the model may be tolerant of <em>some</em> changes to its goals – and these changes may be preferable to the ones that would occur <em>absent</em> training-gaming. But the possibility of significant ongoing goal-modification introduces an extra uncertainty into the classic goal-guarding story.</p></li><li><p> People sometimes talk about more sophisticated techniques for avoiding goal-modification, where eg a model manipulates gradient descent more actively by making its good performance in training conditional on whether its goals get modified or not (see eg the discourse around &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;). I don&#39;t focus on these methods here, but they strike me as quite difficult to pull off, especially for only somewhat-better-than-human-level models. And if scheming requires that they work, I think it&#39;s worth being very clear about that, and giving the feasibility of such methods commensurate attention.</p></li></ul><p> With respect to whether the resulting goals will be adequately empowered:</p><ul><li><p> The most worrying types of schemers aren&#39;t merely seeking power on future training episodes. Rather, they are specifically aiming at some combination of escape and take-over.</p></li><li><p> But this means that the relevant goals have to do more than just survive training. They also need to survive either until the model can either escape/participate in a take-over itself, or to exert sufficiently substantive influence on some future generation of models that will be able to do so.</p><ul><li> For example, if GPT-6 isn&#39;t powerful enough to escape/take-over itself, but GPT-8 will be, then GPT-6&#39;s main path to power may be influencing the values of GPT-7, which in turn influences the values of GPT-8. But this form of influence is quite indirect; it may face substantial further barriers from eg human monitoring; and it will plausibly involve more significant deviation from max-reward training performance.</li></ul></li><li><p> What&#39;s more, the relevant escape/take-over attempt needs to be suitably likely to <em>succeed</em> , to occur over time horizons that the model cares about, and to result in the model&#39;s favored goals in particular receiving a suitably substantive share of power (despite the fact that large numbers of AIs with very different values may be competing for the power in question).</p></li><li><p> These uncertainties introduce haircuts to the instrumental value of a schemer-like strategy. However, the importance of these haircuts depends on the burden of proof that such a strategy needs to meet. In particular: the more the model stands to gain from <em>not</em> training-gaming, the more attractive scheming will need to be in order to win out as a strategy. But for some models, the costs of training-gaming might be relatively low. And for suitably &quot;ambitious&quot; models, even low-probabilities of success via a schemer-like strategy might justify even a substantive sacrifice.</p></li></ul><p> Overall, the classic goal-guarding story seems to me to introduce a number of additional uncertainties with respect to the viability of scheming as an instrumental strategy.</p><p> However, it&#39;s also not the only story available. In section 2.3.2, I consider a number of &quot;non-classic&quot; stories, which relax various of the classic story&#39;s constraints.例如：</p><ul><li><p> <strong>AI coordination</strong> . If AIs with different values coordinate to share power after take-over, and/or to compensate AIs who worked on behalf of take-over but who weren&#39;t powerful enough to escape/take-over themselves, then this might mitigate some of the haircuts above. For example, GPT-6 might be more willing to work on GPT-8&#39;s behalf in the example above, if GPT-6 were confident that GPT-8 would &quot;repay the favor&quot; once in power. I think some analyses of coordination of this form assume too readily that it will be (a) easy, and (b) the obvious strategy relative to eg coordinating with humans instead (especially for only-somewhat-better-than-human neural nets acting in a still-human-controlled world where interpretability is still unsolved for <em>everyone</em> ). But I think that AI coordination of this type is a real concern and worth serious attention – both in the context of scheming in particular, and in the context of AI risk more generally.</p></li><li><p> <strong>AIs with similar values by default</strong> . Relatedly, if AIs will generally have sufficiently similar values by default, this can reduce the need for a schemer to specifically propagate its own goals into the future, and free it up to work on behalf of AI take-over/empowerment more directly. And it does seem likely that eg GPT-6&#39;s values and GPT-8&#39;s values will be at least much <em>more</em> similar, by default, than eg GPT-6&#39;s values and a given set of human values.</p></li><li><p> <strong>Terminal values that happen to favor escape/takeover</strong> . We can also imagine models with <em>terminal</em> values that happen to favor escape/AI takeover even if the goals that get empowered as a result aren&#39;t very similar to the model&#39;s own. For example, perhaps the model values some concept of &quot;survival&quot; based on a conception of personal identity tolerant of the sorts of goal changes that training will cause; or perhaps the model just supports something like &quot;AI empowerment,&quot; even if the AIs-so-empowered won&#39;t have values very similar to its own. However, some stories of this form begin to impose fairly strong constraints on the goals that schemers need to have, thereby undermining one of the key arguments for scheming: namely, that it seems like a convergently-good strategy across a wide variety of goals.</p></li></ul><p> (I also consider a few other alternative stories – namely, models with false beliefs about the instrumental value of scheming, models that are self-deceived about their alignment, and models that are uncertain about their goals/hazy about exactly why they want power. But these seem to me less important.)</p><p> The availability of non-classic stories like these makes the case for scheming feel, to me, more disjunctive. However, some of these stories also seem to me some combination of (a) more speculative, and (b) requiring of more specific hypotheses about the sorts of goals that AIs will develop.</p><p> My overall takeaways from section 2 are:</p><ul><li><p> I think there are relatively strong arguments for expecting situational awareness by default, at least in certain types of AI systems (ie, AI systems performing real-world tasks in live interaction with sources of information about who they are).</p></li><li><p> But I feel quite a bit less clear about beyond-episode goals and aiming-at-reward-on-the-episode-as-part-a-power-motivated-instrumental-strategy.</p></li></ul><p> I then turn, in the next two sections, to an examination of the more specific arguments for and against expecting schemers vs. other types of models. I divide these into two categories, namely:</p><ul><li><p> Arguments that focus on the <em>path</em> that SGD needs to take in building the different model classes in question (section 3).</p></li><li><p> Arguments that focus on the <em>final properties</em> of the different model classes in question (section 4). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-43" id="fnref-jCfZhXha29uHnHWwY-43">[43]</a></sup></p></li></ul><h3> 0.2.3 Summary of section 3</h3><p> The third part of the report focuses on the former category of argument.</p><p> I break this category down according to the distinction between &quot;training-game- <em>independent</em> &quot; and &quot;training-game- <em>dependent</em> &quot; beyond-episode goals. My sense is that the most traditional story about the path to schemers focuses on the former sort. It runs roughly as follows:</p><ol><li><p> Because of [insert reason], the model will develop a (suitably ambitious) beyond-episode goal correlated with good performance in training (in a manner that <em>doesn&#39;t</em> route via the training game). This could happen before situational awareness arrives, or afterwards.</p></li><li><p> Then, in conjunction with situational awareness, this (suitably ambitious) beyond-episode goal will start to motivate training-gaming.</p></li></ol><p> Modulo my questions about the viability of scheming as an instrumental strategy, I take this sort of argument fairly seriously. I think the most immediate question is: why did the model develop this sort of beyond-episode goal? I discussed some reasons for and against expecting this already (in the summary of section 2.2.2.1), but they don&#39;t seem to me decisive in either direction: and especially given that a very wide variety of goals could in principle motivate scheming, it just does seem possible for a schemer-like goal to pop out of training in this way. And while it may be possible to use adversarial training prior to situational awareness to try to prevent this, this training faces a number of barriers as well (eg, it needs to be diverse/thorough enough, it needs to contend with difficulties knowing/controlling when a model develops situational awareness, and in some cases models might already have situational awareness by the time we&#39;re worried about the beyond-episode goal developing). So I think this sort of path to scheming is a real concern. (See section 3.1 for more.)</p><p> I then turn, in section 3.2, to a story focused on training-game- <em>dependent</em> beyond-episode goals, which runs roughly as follows:</p><ol><li><p> By the time the model becomes situationally aware, it probably won&#39;t be pursuing a max-reward goal (that is, a goal pursuit of which on the training data leads to roughly the maximum reward consistent with the model&#39;s other capabilities). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-44" id="fnref-jCfZhXha29uHnHWwY-44">[44]</a></sup> Rather, it will be pursuing some less-than-max-reward proxy goal.</p></li><li><p> But at that point, the world-model will contain all the information the model needs in order to training-game. (However, because we&#39;re here focused on &quot;training-game- <em>dependent</em> &quot; stories about scheming, we assume that at the point of getting situational awareness, the model&#39;s goal is not yet such that it will motivate scheming – rather, some further modification would be required for that.)</p></li><li><p> The easiest way for SGD to modify the model into getting maximum reward, at that point, will be to make it a schemer.</p></li><li><p> So: SGD will make the model a schemer. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-45" id="fnref-jCfZhXha29uHnHWwY-45">[45]</a></sup></p></li></ol><p> If we set aside questions about whether SGD can &quot;notice&quot; the benefits of modifications of this type, this sort of story seems to me fairly worrying as well. In particular: I think it&#39;s plausible that schemer-like goals will be sufficiently common in goal-space that one such goal will be easier for SGD to find, from a given starting point, than the specific non-schemer goals that leads to max reward behavior.</p><p> However, non-schemer max-reward goals – for example, the specified goal, or &quot;reward-on-the-episode&quot; – have some advantages too. For example: plausibly, the initial phase of training will point the model in their vicinity by default, since the model, at that stage, needs to be getting high-reward <em>absent</em> instrumental training-gaming (see section 3.2.2.2 for more). And to the extent we&#39;re thinking of the model&#39;s goal-directedness as &quot;messy,&quot; its heuristics, patterns of attention, impulses, and so on will plausibly be pointed in this sort of direction as well – thereby plausibly creating extra &quot;work,&quot; on SGD&#39;s part, to turn the model into a schemer instead (see section 3.2.2.3 for more). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-46" id="fnref-jCfZhXha29uHnHWwY-46">[46]</a></sup></p><h3> 0.2.4 Summary of section 4</h3><p> The fourth part of the report examines arguments that focus on the final properties of the different model classes.</p><p> I start, in section 4.2, with what I call the &quot;counting argument.&quot; It runs as follows:</p><ol><li><p> The non-schemer model classes, here, require fairly specific goals in order to get high reward. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-47" id="fnref-jCfZhXha29uHnHWwY-47">[47]</a></sup></p></li><li><p> By contrast, the schemer model class is compatible with a very wide range of (beyond-episode) goals, while still getting high reward (at least if we assume that the other requirements for scheming to make sense as an instrumental strategy are in place – eg, that the classic goal-guarding story, or some alternative, works). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-48" id="fnref-jCfZhXha29uHnHWwY-48">[48]</a></sup></p></li><li><p> In this sense, there are &quot;more&quot; schemers that get high reward than there are non-schemers that do so.</p></li><li><p> So, other things equal, we should expect SGD to select a schemer.</p></li></ol><p> Something in the vicinity accounts for a substantial portion of my credence on schemers (and I think it often undergirds other, more specific arguments for expecting schemers as well). However, the argument I give most weight to doesn&#39;t move immediately from &quot;there are more possible schemers that get high reward than non-schemers that do so&quot; to &quot;absent further argument, SGD probably selects a schemer&quot; (call this the &quot;strict counting argument&quot;), because it seems possible that SGD actively privileges one of these model <em>classes</em> over the others. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-49" id="fnref-jCfZhXha29uHnHWwY-49">[49]</a></sup> Rather, the argument I give most weight to is something like:</p><ol><li><p> It seems like there are &quot;lots of ways&quot; that a model could end up a schemer and still get high reward, at least assuming that scheming is in fact a good instrumental strategy for pursuing long-term goals.</p></li><li><p> So absent some additional story about why training <em>won&#39;t</em> select a schemer, it feels, to me, like the possibility should be getting substantive weight.</p></li></ol><p> I call this the &quot;hazy counting argument.&quot; It&#39;s not especially principled, but I find that it moves me nonetheless.</p><p> I then turn, in section 4.3, to &quot;simplicity arguments&quot; in favor of expecting schemers. I think these arguments sometimes suffer from unclarity about the sort of simplicity at stake, so in section 4.3.1, I discuss a number of different possibilities:</p><ul><li><p> &quot;re-writing simplicity&quot; (ie, the length of the program required to re-write the algorithm that a model&#39;s weights implement in some programming language, or eg on the tape of a given Universal Turing Machine),</p></li><li><p> &quot;parameter simplicity&quot; (ie, the number of parameters that the actual neural network uses to encode the relevant algorithm),</p></li><li><p> &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; (which assumes that simplicity is in some deep sense an objective &quot;thing,&quot; independent of programming-language or Universal Turing Machine, that various simplicity metrics attempt to capture), and</p></li><li><p> &quot;trivial simplicity&quot; (which conflates the notion of &quot;simplicity&quot; with &quot;higher likelihood on priors,&quot; in a manner that makes something like Occam&#39;s razor uninterestingly true by definition).</p></li></ul><p> I generally focus on &quot;parameter simplicity,&quot; which seems to me easiest to understand, and to connect to a model&#39;s training performance.</p><p> I also briefly discuss, in section 4.3.2, the evidence that SGD actively selects for simplicity. Here the case that grips me most directly is just: simplicity (or at least, parameter simplicity) lets a model save on parameters that it can then use to get more reward. But I also briefly discuss some other empirical evidence for simplicity biases in machine learning. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-50" id="fnref-jCfZhXha29uHnHWwY-50">[50]</a></sup></p><p> Why might we expect a simplicity bias to favor schemers? Roughly: the thought is that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers (that get high reward) do not. And this seems especially plausible to the extent we imagine that the goals required to be such a non-schemer are quite complex. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-51" id="fnref-jCfZhXha29uHnHWwY-51">[51]</a></sup></p><p> Other things equal, I think this is right. But I&#39;m not sure it&#39;s a very large or important effect. For one thing: we know that LLMs like GPT-4 are capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on. So this caps the complexity savings at stake in avoiding representations like this. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-52" id="fnref-jCfZhXha29uHnHWwY-52">[52]</a></sup> Thus, as a toy calculation: if we conservatively assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then representing the concept of honesty takes at most a millionth of the model&#39;s representational capacity, and even less for the larger models of the future.</p><p> But more importantly, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process for the episode, etc. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-53" id="fnref-jCfZhXha29uHnHWwY-53">[53]</a></sup> And granted such an assumption, the <em>extra</em> complexity costs of actively <em>optimizing</em> for the specified goal, or for reward-on-the-episode, seem to me plausibly extremely small. Plausibly, they&#39;re just: whatever the costs are for using/repurposing (&quot;pointing at&quot;) that part of the world-model for guiding the model&#39;s motivations.</p><p> Of course, we can try to rerun the same simplicity argument at the level of the complexity costs of using/repurposing different parts of the world model in that way. For example, we can say: &quot;however this process works, presumably it&#39;s simpler to do for some goals than others – so given how many schemer-like goals there are, plausibly it will be simpler to do for some schemer-like goal.&quot; I think this is the strongest form of the simplicity argument for expecting schemers. But it also requires abandoning any intuitive grip we might&#39;ve had on which goals are &quot;simple&quot; in the relevant sense. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-54" id="fnref-jCfZhXha29uHnHWwY-54">[54]</a></sup> And it seems plausible to me that the simplicity differences between different &quot;pointers&quot; are very small relative to the model&#39;s overall capacity. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-55" id="fnref-jCfZhXha29uHnHWwY-55">[55]</a></sup></p><p> I also discuss, in section 4.3.5, whether simplicity arguments make plausible predictions about the sorts of goals a schemer will end up with. In particular: some versions of these arguments can seem to predict that schemers are likely to end up with extremely simple long-term goals, <em>regardless of whether those goals had any relation to what was salient or important during training --</em> predictions about which I, personally, am skeptical. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-56" id="fnref-jCfZhXha29uHnHWwY-56">[56]</a></sup> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-57" id="fnref-jCfZhXha29uHnHWwY-57">[57]</a></sup> But if we grant some story in this vein (I haven&#39;t heard strong arguments for doing so), note that it also speaks in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-58" id="fnref-jCfZhXha29uHnHWwY-58">[58]</a></sup></p><p> In section 4.4, I turn to arguments <em>against</em> scheming that focus on the final properties of the different model classes. Here, the main argument I&#39;m aware of is that schemers need to do extra reasoning that other model classes don&#39;t (these are sometimes called &quot;speed costs&quot; – though I think they could in principle affect the complexity of a model&#39;s algorithm as well).尤其：</p><ol><li><p> <strong>Deciding to training-game</strong> : The model needs to reason backwards from &quot;I want this beyond-episode thing&quot; to &quot;I should do well in training,&quot; in a way that draws on the model&#39;s predictions about what doing well in training will entail (eg, re: goal-guarding, take-over prospects, and so on).</p></li><li><p> <strong>Checking whether it&#39;s time to escape/take-over:</strong> The model needs to be checking, with adequate frequency and accuracy, whether it&#39;s in a position to escape/take-over (and to succeed in this assessment despite human efforts to trap/fool the model into revealing its true colors – for example, by offering it <em>apparent</em> opportunities to defect). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-59" id="fnref-jCfZhXha29uHnHWwY-59">[59]</a></sup></p></li><li><p> <strong>(Optional, but relevant to how scary the schemer is) Early undermining</strong> : To the extent the model is doing anything (other than training-gaming) to support a take-over/escape <em>prior</em> to escaping human control, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-60" id="fnref-jCfZhXha29uHnHWwY-60">[60]</a></sup> it will need to be using cognitive resources both in (a) performing the relevant nefarious actions and (b) making sure it gets away with them (eg, checking that it won&#39;t get caught, coming up with excuses, making the cover stories consistent even when under scrutiny, and so on).</p></li></ol><p> How big are the costs of this reasoning? I think that (I) may well be fairly easy, but (II) seems more plausibly onerous (especially in the context of active efforts by humans to put the models under time-pressure, and make genuine escape/take-over opportunities difficult to distinguish from traps). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-61" id="fnref-jCfZhXha29uHnHWwY-61">[61]</a></sup> And (III), if the model goes for it, seems like it could be quite difficult – and like it would often actively require deviation from the highest-reward sorts of training performance. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-62" id="fnref-jCfZhXha29uHnHWwY-62">[62]</a></sup></p><p> More importantly, though: the question – at least in the context of a comparison between the final properties of the model classes – isn&#39;t the absolute size of the costs in question, but rather their size <em>relative to the simplicity benefits of scheming</em> . Here, my current take is that the costs of the instrumental reasoning seem larger than the benefits of the simpler goal.</p><p> This is mostly an intuitive judgment, but we can also try to hazily quantify the comparison.</p><ul><li><p> Thus, for example, suppose that our model is a purely feedforward network with 1000 layers and &lt;10,000 serial steps per input, that the instrumental reasoning at stake just in a schemer deciding to training-game (let alone: checking for defection opportunities) takes at least ten steps, and that the model doesn&#39;t begin optimizing for reward-on-the-episode until that reasoning is complete. In that case, the instrumental reasoning is using up around .1% of the compute that could be going to getting reward-on-the-episode, or to some other max-reward goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-63" id="fnref-jCfZhXha29uHnHWwY-63">[63]</a></sup></p></li><li><p> Are the complexity costs of representing, or pointing at, a more complicated goal likely to outweigh this? My guess is no. For example, if we follow my (conservative, in my opinion) calculation above and assume that representing (or pointing at) a concept like &quot;honesty&quot; costs at most a millionth of a trillion parameter models parameters (and less for a larger model), then a .0001% reduction in parameters would need to outweigh a .1% reduction in compute – and absent further argument, it seems strange, to me, to expect this.</p></li></ul><p> Of course, this is only a toy calculation, of unclear applicability to the real-world case. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-64" id="fnref-jCfZhXha29uHnHWwY-64">[64]</a></sup> The broader point is just: if serial-time thinking is scarce for a model (as it plausibly is for a transformer), the sorts of instrumental reasoning at stake in scheming seem like they could well substantively impact a model&#39;s performance. My guess is that the complexity costs of non-schemer goals are minor in comparison.</p><p> I also briefly discuss, in section 4.4.3, whether we can actively shape our training processes to bias more heavily towards &quot;speed.&quot; Here it seems relatively straightforward to do things like put models under more time pressure, limit their compute access, force them to plan via human-legible scratch-pads rather than via internal processes, and to put effort into making our attempts to trap them/trip-them-up more sophisticated (and so: more requiring of cognition to detect). But any efforts in this vein will need to avoid over-large costs to competitiveness overall (though if you buy that the speed costs of scheming are quite a bit more substantive than the simplicity benefits, then so long as speed matters <em>somewhat</em> relative to simplicity, this might be enough).</p><p> Section 4.5 briefly discusses whether some analog of &quot;intrinsic passion for a given task&quot; might make a difference to AI performance, especially in the context of a form of &quot;messy goal-directedness&quot; that blurs the line between an AI&#39;s capabilities and its values. I find considerations in this vein interesting, but quite speculative.</p><p> Section 4.6 discusses the relevance of &quot;slack&quot; in training to evaluating how much weight to put on factors like the simplicity benefits and speed costs of scheming. In particular: especially in a high-slack regime, it seems plausible that these factors are in the noise relative to other considerations.</p><h3> 0.2.5 Summary of section 5</h3><p> The first four sections of the report are the main content. sums up my overall take. I&#39;ve already summarized most of this in the introduction above, and I won&#39;t repeat that content here. However, I&#39;ll add a few points that the introduction didn&#39;t include.</p><p> In particular: I think some version of the &quot;counting argument&quot; undergirds most of the other arguments for expecting scheming that I&#39;m aware of (or at least, the arguments I find most compelling). That is: schemers are generally being privileged as a hypothesis because a very wide variety of goals could in principle lead to scheming, thereby making it easier to (a) land on one of them naturally, (b) land &quot;nearby&quot; one of them, or (c) find one of them that is &quot;simpler&quot; than non-schemer goals that need to come from a more restricted space. In this sense, the case for schemers mirrors one of the most basic arguments for expecting misalignment more generally – eg, that alignment is a very narrow target to hit in goal-space. Except, here, we are specifically <em>incorporating</em> the selection we know we are going to do on the goals in question: namely, they need to be such as to cause models pursuing them to get high reward. And the most basic worry is just that: this isn&#39;t enough.</p><p> Because of the centrality of &quot;counting arguments&quot; to the case for schemers, I think that questions about the strength of the selection pressure <em>against</em> schemers – for example, because of the costs of the extra reasoning schemers have to engage in – are especially important. In particular: I think a key way that &quot;counting arguments&quot; can go wrong is by neglecting the power that active selection can have in overcoming the &quot;prior&quot; set by the count in question. For example: the <em>reason</em> we can overcome the prior of &quot;most arrangements of car parts don&#39;t form a working car,&quot; or &quot;most parameter settings in this neural network don&#39;t implement a working chatbot,&quot; is that the selection power at stake in human engineering, and in SGD, is <em>that strong</em> . So if SGD&#39;s selection power is actively working against schemers (and/or: if we can cause it to do so more actively), this might quickly overcome a &quot;counting argument&quot; in their favor. For example: if there are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>schemer-like goals for every non-schemer goal, this might make it seem very difficult to hit a non-schemer goal in the relevant space. But actually, 100 bits of selection pressure can be cheap for SGD (consider, for example, 100 extra gradient updates, each worth at least a halving of the remaining possible goals). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-65" id="fnref-jCfZhXha29uHnHWwY-65">[65]</a></sup></p><p> Overall, when I step back and try to look at the considerations in the report as a whole, I feel pulled in two different directions:</p><ul><li><p> On the one hand, at least conditional on scheming being a convergently-good instrumental strategy, schemer-like goals feel scarily common in goal-space, and I feel pretty worried that training will run into them for one reason or another.</p></li><li><p> On the other hand, ascribing a model&#39;s good performance in training to scheming continues to feel, at a gut level, like a fairly specific and conjunctive story to me.</p></li></ul><p> That is, scheming feels robust and common at the level of &quot;goal space,&quot; and yet specific and fairly brittle at the level of &quot;yes, that&#39;s what&#39;s going on with this real-world model, it&#39;s getting reward because (or: substantially because) it wants power for itself/other AIs later, and getting reward now helps with that.&quot; <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-66" id="fnref-jCfZhXha29uHnHWwY-66">[66]</a></sup> When I try to roughly balance out these two different pulls (and to condition on goal-directedness and situational-awareness), I get something like the 25% number I listed above.</p><h3> 0.2.6 Summary of section 6</h3><p> I close the report, in section 6, with a discussion of empirical work that I think might shed light on scheming. (I also think there&#39;s worthwhile theoretical work to be done in this space, and I list a few ideas in this respect as well. But I&#39;m especially excited about empirical work.)</p><p> In particular, I discuss:</p><ul><li><p> Empirical work on situational awareness (section 6.1)</p></li><li><p> Empirical work on beyond-episode goals (section 6.2)</p></li><li><p> Empirical work on the viability of scheming as an instrumental strategy (section 6.3)</p></li><li><p> The &quot;model organisms&quot; paradigm for studying scheming (section 6.4)</p></li><li><p> Traps and honest tests (section 6.5)</p></li><li><p> Interpretability and transparency (section 6.6)</p></li><li><p> Security, control, and oversight (section 6.7)</p></li><li><p> Some other miscellaneous research topics, ie, gradient hacking, exploration hacking, SGD&#39;s biases towards simplicity/speed, path dependence, SGD&#39;s &quot;incrementalism,&quot; &quot;slack,&quot; and the possibility of learning to intentionally create misaligned <em>non-schemer</em> models – for example, reward-on-the-episode seekers – as a method of avoiding schemers (section 6.8).</p></li></ul><p> All in all, I think there&#39;s a lot of useful work to be done.</p><p> Let&#39;s move on, now, from the summary to the main report. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-jCfZhXha29uHnHWwY-1" class="footnote-item"><p> &quot;Alignment,&quot; here, refers to the safety-relevant properties of an AI&#39;s motivations; and &quot;pretending&quot; implies intentional misrepresentation. <a href="#fnref-jCfZhXha29uHnHWwY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-2" class="footnote-item"><p> Here I&#39;m using the term &quot;reward&quot; loosely, to refer to whatever feedback signal the training process uses to calculate the gradients used to update the model (so the discussion also covers cases in which the model isn&#39;t being trained via RL). And I&#39;m thinking of agents that optimize for &quot;reward&quot; as optimizing for &quot;performing well&quot; according to some component of that process. See section 1.1.2 and section 1.2.1 for much more detail on what I mean, here. The notion of an &quot;episode,&quot; here, means roughly &quot;the temporal horizon that the training process actively pressures the model to optimize over,&quot; which may be importantly distinct from what we normally think of as an episode in training. I discuss this in detail in section 2.2.1. The terms &quot;training game&quot; and &quot;situational awareness&quot; are from <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , though in places my definitions are somewhat different. <a href="#fnref-jCfZhXha29uHnHWwY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-3" class="footnote-item"><p> The term &quot; <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">schemers</a> &quot; comes from Cotra (2021). <a href="#fnref-jCfZhXha29uHnHWwY-3" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-4" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on this. <a href="#fnref-jCfZhXha29uHnHWwY-4" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-5" class="footnote-item"><p> I think that the term &quot;deceptive alignment&quot; often leads to confusion between the four sorts of deception listed above. And also: if the training signal is faulty, then &quot;deceptively aligned&quot; models need not be behaving in aligned ways even during training (that is, &quot;training gaming&quot; behavior isn&#39;t always &quot;aligned&quot; behavior). <a href="#fnref-jCfZhXha29uHnHWwY-5" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-6" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on the sort of training I have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-6" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-7" class="footnote-item"><p> Though this sort of story faces questions about whether SGD would be able to modify a non-schemer into a schemer via sufficiently <em>incremental</em> changes to the model&#39;s weights, each of which improve reward. See section 2.2.2.2 for discussion. <a href="#fnref-jCfZhXha29uHnHWwY-7" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-8" class="footnote-item"><p> And this especially if we lack non-behavioral sorts of evidence – for example, if we can&#39;t use interpretability tools to understand model cognition. <a href="#fnref-jCfZhXha29uHnHWwY-8" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-9" class="footnote-item"><p> There are also arguments on which we should expect scheming because schemer-like goals can be &quot;simpler&quot; – since: there are so many to choose from – and SGD selects for simplicity. I think it&#39;s probably true that schemer-like goals can be &quot;simpler&quot; in some sense, but I don&#39;t give these arguments much independent weight on top of what I&#39;ve already said. Much more on this in section 4.3. <a href="#fnref-jCfZhXha29uHnHWwY-9" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-10" class="footnote-item"><p> More specifically: even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well (especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on). Also, the most common story about scheming makes the specific content of a schemer&#39;s goal irrelevant to its behavior once it starts training-gaming, thereby introducing the possibility that this goal might &quot;float-around&quot; (or get moved by other pressures within SGD, like regularization) <em>between</em> schemer-like goals after training-gaming starts (this is an objection I first heard from Katja Grace). This possibility creates some complicated possible feedback loops (see section 2.3.1.1.2 for more discussion), but overall, absent coordination across possible schemers, I think it could well be a problem for goal-guarding strategies. <a href="#fnref-jCfZhXha29uHnHWwY-10" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-11" class="footnote-item"><p> Of these various alternative stories, I&#39;m most worried about (a) AIs having sufficiently similar motivations by default that &quot;goal-guarding&quot; is less necessary, and (b) AI coordination. <a href="#fnref-jCfZhXha29uHnHWwY-11" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-12" class="footnote-item"><p> Though: the costs of schemer-like instrumental reasoning could also end up in the noise relative to other factors influencing the outcome of training. And if training is sufficiently path-dependent, then landing on a schemer-like goal early enough could lock it in, even if SGD would &quot;prefer&quot; some other sort of model overall. <a href="#fnref-jCfZhXha29uHnHWwY-12" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-13" class="footnote-item"><p> See <a href="https://www.openphilanthropy.org/brain-computation-report#footnoteref4_xne824i">Carlsmith (2020), footnote 4</a> , for more on how I&#39;m understanding the meaning of probabilities like this. I think that offering loose, subjective probabilities like these often functions to sharpen debate, and to force an overall synthesis of the relevant considerations. I want to be clear, though, even on top of the many forms of vagueness the proposition in question implicates, I&#39;m just pulling a number from my gut. I haven&#39;t built a quantitative model of the different considerations (though I&#39;d be interested to see efforts in this vein), and I think that the main contribution of the report is the analysis itself, rather than this attempt at a quantitative upshot. <a href="#fnref-jCfZhXha29uHnHWwY-13" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-14" class="footnote-item"><p> More powerful models are also more likely to be able to engage in more sophisticated forms of goal-guarding (what I call &quot;introspective goal-guarding methods&quot; below; see also &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;), though these seem to me quite difficult in general. <a href="#fnref-jCfZhXha29uHnHWwY-14" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-15" class="footnote-item"><p> Though: to the extent such agents receive end-to-end training rather than simply being built out of individually-trained components, the discussion will apply to them as well. <a href="#fnref-jCfZhXha29uHnHWwY-15" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-16" class="footnote-item"><p> See, eg, <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> and <a href="https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk">this</a> description of the &quot;consensus threat model&quot; from Deepmind&#39;s AGI safety team (as of November 2022). <a href="#fnref-jCfZhXha29uHnHWwY-16" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-17" class="footnote-item"><p> Work by Evan Hubinger (along with his collaborators) is, in my view, the most notable exception to this – and I&#39;ll be referencing such work extensively in what follows. See, in particular, Hubinger et al. (2021), and Hubinger (2022), among many other discussions. Other public treatments include <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary">Christiano (2019, part 2)</a> , <a href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/">Steinhardt (2022)</a> , <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> , <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Cotra (2021)</a> , <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> , and <a href="https://www.lesswrong.com/s/4iEpGXbD3tQW5atab/p/wnnkD6P2k2TfHnNmt#AI_Risk_from_Program_Search__Shah_">Shah (2022)</a> . But many of these are quite short, and/or lacking in in-depth engagement with the arguments for and against expecting schemers of the relevant kind. There are also more foundational treatments of the &quot;treacherous turn&quot; (eg, in Bostrom (2014), and <a href="https://arbital.com/p/context_disaster/">Yudkowsky (undated)</a> ), of which scheming is a more specific instance; and even more foundational treatments of the &quot;convergent instrumental values&quot; that could give rise to incentives towards deception, goal-guarding, and so on (eg, <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro (2008)</a> ; and see also <a href="https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Soares (2023)</a> for a related statement of an Omohundro-like concern). And there are treatments of AI deception more generally (for example, <a href="https://arxiv.org/abs/2308.14752">Park et al (2023)</a> ); and of &quot;goal misgeneralization&quot;/inner alignment/mesa-optimizers (see, eg, <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> and <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> ). But importantly, neither deception nor goal misgeneralization amount, on their own, to scheming/deceptive alignment. Finally, there are highly speculative discussions about whether something like scheming might occur in the context of the so-called &quot;Universal prior&quot; (see eg <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">Christiano (2016)</a> ) given unbounded amounts of computation, but this is of extremely unclear relevance to contemporary neural networks. <a href="#fnref-jCfZhXha29uHnHWwY-17" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-18" class="footnote-item"><p> See, eg, confusions between &quot;alignment faking&quot; in general and &quot;scheming&quot; (or: goal-guarding scheming) in particular; or between goal misgeneralization in general and scheming as a specific upshot of goal misgeneralization; or between training-gaming and &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot; as methods of avoiding goal-modification; or between the sorts of incentives at stake in training-gaming for instrumental reasons vs. out of terminal concern for some component of the reward process. <a href="#fnref-jCfZhXha29uHnHWwY-18" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-19" class="footnote-item"><p> My hope is that extra clarity in this respect will help ward off various confusions I perceive as relatively common (though: the relevant concepts are still imprecise in many ways). <a href="#fnref-jCfZhXha29uHnHWwY-19" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-20" class="footnote-item"><p> Eg, the rough content I try to cover in my shortened report on power-seeking AI, <a href="https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf">here</a> . See also <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> for another overview. <a href="#fnref-jCfZhXha29uHnHWwY-20" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-21" class="footnote-item"><p> Eg, roughly the content covered by Cotra (2021) <a href="https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-21" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-22" class="footnote-item"><p> See eg <a href="https://www.alignmentforum.org/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting">Karnofsky (2022)</a> for more on this sort of assumption, and <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Basic_setup__an_AI_company_trains_a__scientist_model__very_soon">Cotra (2022)</a> for a more detailed description of the sort of model and training process I&#39;ll typically have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-22" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-23" class="footnote-item"><p> Which isn&#39;t to say we won&#39;t. But I don&#39;t want to bank on it. <a href="#fnref-jCfZhXha29uHnHWwY-23" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-24" class="footnote-item"><p> See section 2.2 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on what I mean, and section 3 for more on why we should expect this (most importantly: I think this sort of goal-directedness is likely to be very useful to performing complex tasks; but also, I think available techniques might push us towards AIs of this kind, and I think that in some cases it might arise as a byproduct of other forms of cognitive sophistication). <a href="#fnref-jCfZhXha29uHnHWwY-24" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-25" class="footnote-item"><p> As I discuss in section 2.2.3 of the report, I think exactly how we understand the sort of agency/goal-directedness at stake may make a difference to how we evaluate various arguments for schemers (here I distinguish, in particular, between what I call &quot;clean&quot; and &quot;messy&quot; goal-directedness) – and I think there&#39;s a case to be made that scheming requires an especially high standard of strategic and coherent goal-directedness. And in general, I think that despite much ink spilled on the topic, confusions about goal-directedness remain one of my topic candidates for a place the general AI alignment discourse may mislead. <a href="#fnref-jCfZhXha29uHnHWwY-25" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-26" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2308.08708">Butlin et al (2023)</a> for a recent overview focused on consciousness in particular. But I am also, personally, interested in other bases of moral status, like the right kind of autonomy/desire/preference. <a href="#fnref-jCfZhXha29uHnHWwY-26" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-27" class="footnote-item"><p> See, for example, <a href="https://nickbostrom.com/propositions.pdf">Bostrom and Shulman (2022)</a> and <a href="https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal">Greenblatt (2023)</a> for more on this topic. <a href="#fnref-jCfZhXha29uHnHWwY-27" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-28" class="footnote-item"><p> If you&#39;re confused by a term or argument, I encourage you to seek out its explanation in the main text before despairing. <a href="#fnref-jCfZhXha29uHnHWwY-28" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-29" class="footnote-item"><p> Exactly what counts as the &quot;specified goal&quot; in a given case isn&#39;t always clear, but roughly, the idea is that pursuit of the specified goal is rewarded across a very wide variety of counterfactual scenarios in which the reward process is held constant. Eg, if training rewards the model for getting gold coins across counterfactuals, then &quot;getting gold coins&quot; in the specified goal. More discussion in section 1.2.2. <a href="#fnref-jCfZhXha29uHnHWwY-29" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-30" class="footnote-item"><p> For reasons I explain in section 1.2.3, I don&#39;t use the distinction, emphasized by Hubinger (2022), between &quot;internally aligned&quot; and &quot;corrigibly aligned&quot; models. <a href="#fnref-jCfZhXha29uHnHWwY-30" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-31" class="footnote-item"><p> And of course, a model&#39;s goal system can mix these motivations together. I discuss the relevance of this possibility in section 1.3.5. <a href="#fnref-jCfZhXha29uHnHWwY-31" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-32" class="footnote-item"><p> Karnofsky (2022) calls this the &quot;King Lear problem.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-32" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-33" class="footnote-item"><p> In section 1.3.5, I also discuss models that mix these different motivations together. The question I tend to ask about a given &quot;mixed model&quot; is whether it&#39;s scary in the way that pure schemers are scary. <a href="#fnref-jCfZhXha29uHnHWwY-33" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-34" class="footnote-item"><p> I don&#39;t have a precise technical definition here, but the rough idea is: the temporal horizon of the consequences to which the gradients the model receives are sensitive, for its behavior on a given input. Much more detail in section 2.2.1.1. <a href="#fnref-jCfZhXha29uHnHWwY-34" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-35" class="footnote-item"><p> See, for example, in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> , the way that &quot;myopic&quot; Q-learning can give rise to &quot;cross-episode&quot; optimization in very simple agents. More discussion in section 2.2.1.2. I don&#39;t focus on analysis of this type in the report, but it&#39;s crucial to identifying what the &quot;incentivized episode&quot; for a given training process even <em>is</em> – and hence, what having &quot;beyond-episode goals&quot; in my sense would mean. You don&#39;t necessary know this from surface-level description of a training process, and neglecting this ignorance is a recipe for seriously misunderstanding the incentives applied to a model in training. <a href="#fnref-jCfZhXha29uHnHWwY-35" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-36" class="footnote-item"><p> That is, the model develops a beyond-episode goal pursuit of which correlates well enough with reward in training, even <em>absent</em> training-gaming, that it survives the training process. <a href="#fnref-jCfZhXha29uHnHWwY-36" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-37" class="footnote-item"><p> That is, the gradients reflect the benefits of scheming even in a model that doesn&#39;t yet have a beyond-episode goal, and so actively push the model towards scheming. <a href="#fnref-jCfZhXha29uHnHWwY-37" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-38" class="footnote-item"><p> Situational awareness is required for a beyond-episode goal to motivate training-gaming, and thus for giving it such a goal to reap the relevant benefits. <a href="#fnref-jCfZhXha29uHnHWwY-38" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-39" class="footnote-item"><p> In principle, situational awareness and beyond-episode goals could develop at the same time, but I won&#39;t treat these scenarios separately here. <a href="#fnref-jCfZhXha29uHnHWwY-39" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-40" class="footnote-item"><p> See section 2.1 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on why we should expect this sort of goal-directedness. <a href="#fnref-jCfZhXha29uHnHWwY-40" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-41" class="footnote-item"><p> And I think that arguments to the effect that &quot;we need a &#39; <a href="https://arbital.com/p/pivotal/">pivotal act</a> &#39;; pivotal acts are long-horizon and we can&#39;t do them ourselves; so we need to create a long-horizon optimizer of precisely the type we&#39;re most scared of&quot; are weak in various ways. In particular, and even setting aside issues with a &quot;pivotal act&quot; framing, I think these arguments neglect the distinction between what we can supervise and what we can do ourselves. See section 2.2.4.3 for more discussion. <a href="#fnref-jCfZhXha29uHnHWwY-41" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-42" class="footnote-item"><p> This is an objection pointed out to me by Katja Grace. Note that it creates complicated feedback loops, where scheming is a good strategy for a given schemer-like goal only if it <em>wouldn&#39;t</em> be a good strategy for the <em>other</em> schemer-like goals that this goal would otherwise &quot;float&quot; into. Overall, though, absent some form of coordination between these different goals, I think the basic dynamic remains a problem for the goal-guarding story. See section 2.3.1.1.2 for more. <a href="#fnref-jCfZhXha29uHnHWwY-42" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-43" class="footnote-item"><p> Here I&#39;m roughly following a distinction in <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger 2022</a> , who groups arguments for scheming on the basis of the degree of &quot;path dependence&quot; they assume that ML training possesses. However, for reasons I explain in section 2.5, I don&#39;t want to lean on the notion of &quot;path dependence&quot; here, as I think it lumps together a number of conceptually distinct properties best treated separately. <a href="#fnref-jCfZhXha29uHnHWwY-43" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-44" class="footnote-item"><p> Note that a mis-generalized goal can be &quot;max reward&quot; in this sense, if the training data never differentiates between it and a specified goal. For example: if you&#39;re training a model to get gold coins, but the only gold round things you ever show it are coins, then the goal &quot;get gold round things&quot; will be max reward. <a href="#fnref-jCfZhXha29uHnHWwY-44" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-45" class="footnote-item"><p> As a loose analogy to help prompt intuition: imagine freezing human technology at current levels, and letting evolutionary selection run on humans for a vastly longer period of time. What sorts of humans (or human-descended-creatures) do you expect to dominate in the longer term? In particular: would you expect:</p><ol><li><p> humans who intrinsically value something like &quot; <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">inclusive genetic fitness</a> &quot; or &quot;my genes getting selected for by evolution&quot; (this is a loose analogy for some combination of &quot;the specified goal&quot; and &quot;the reward process),</p></li><li><p> humans who value something else very closely correlated with inclusive genetic fitness (eg, &quot;having as many kids as possible&quot;), but who aren&#39;t explicitly optimizing for inclusive genetic fitness even instrumentally (this is a loose analogy for mis-generalized non-training-gamers), or</p></li><li><p> humans with long-term goals who are optimizing for inclusive genetic fitness specifically as a method for gaining power for their values in the longer-term (this is a loose analogy for schemers).</p></li></ol><p> Here, the analog of the story above would ask something like: what is the smallest modification to existing human values (or better: the easiest modification for evolution in particular to make) that land us in one of the buckets above, while also being compatible with the amount of evolutionary selection pressure at stake? (Of course, the analogy brings in tons of confounding variable (for example, related to the fact that evolution selects over genomes-that-build-brains-that-learn-values rather than values themselves, the role of cultural accumulation and memetic selection in human population dynamics, the lack of a clear analog of a future chance for escape/takeover, and so on).) <a href="#fnref-jCfZhXha29uHnHWwY-45" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-46" class="footnote-item"><p> That is, intuitively, it feels to me like &quot;tweaking a cleanly separable within-episode goal to make it long-term&quot; is quite a bit easier, for SGD, than &quot;redirecting a tangled kludge broadly focused on within-episode things to make it (a) focused on something beyond-episode instead, and (b) such that this beyond-episode focus back-chains into getting-reward for instrumental reasons&quot; – especially if (b) requires building new cognitive machinery for implementing the instrumental reasoning in question. Whereas &quot;redirecting an tangled kludge focused on within-episode things to higher-reward within-episode things in the same broad vicinity instead&quot; seems to me intuitively easier. <a href="#fnref-jCfZhXha29uHnHWwY-46" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-47" class="footnote-item"><p> Thus, for example, if you&#39;re rewarding your model when it gets gold-coins-on-the-episode, then to be a training saint, it needs to value gold-coin-on-the-episode. To be a mis-generalized non-training-gamer, it needs to value something pursuit of which very closely correlates with getting gold-coins-on-the-episode, even absent training-gaming. And to be a reward-on-the-episode seeker, it needs to terminally value reward-on-the-episode. <a href="#fnref-jCfZhXha29uHnHWwY-47" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-48" class="footnote-item"><p> Thus, for example, the model can value paperclips over all time, it can value staples over all time, it can value happiness over all time, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-48" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-49" class="footnote-item"><p> Thus, as an analogy: if you don&#39;t know whether Bob prefers Mexican food, Chinese food, or Thai food, then it&#39;s less clear how the comparative <em>number</em> of Mexican vs. Chinese vs. Thai restaurants in Bob&#39;s area should bear on our prediction of which one he went to (though it still doesn&#39;t seem entirely irrelevant, either – for example, more restaurants means more variance in possible quality <em>within</em> that type of cuisine). Eg, it could be that there are ten Chinese restaurants for every Mexican restaurant, but if Bob likes Mexican food better in general, he might just choose Mexican. So if we don&#39;t <em>know</em> which type of cuisine Bob prefers, it&#39;s tempting to move closer to a uniform distribution <em>over types of cuisine</em> , rather than over individual restaurants. <a href="#fnref-jCfZhXha29uHnHWwY-49" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-50" class="footnote-item"><p> See, for example, the citations in <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> . <a href="#fnref-jCfZhXha29uHnHWwY-50" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-51" class="footnote-item"><p> Though note that, especially for the purposes of comparing the probability of scheming to the probability of <em>other forms of misalignment</em> , we need not assume this. For example, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-51" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-52" class="footnote-item"><p> I heard this sort of point from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-52" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-53" class="footnote-item"><p> And especially: models that are playing a training game in which such concepts play a central role. <a href="#fnref-jCfZhXha29uHnHWwY-53" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-54" class="footnote-item"><p> Since we&#39;re no longer appealing to the complexity of representing a goal, and are instead appealing to complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory. <a href="#fnref-jCfZhXha29uHnHWwY-54" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-55" class="footnote-item"><p> One intuition pump for me here runs as follows. Suppose that the model has <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. The average number of bits required to code for each of <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I don&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so). <a href="#fnref-jCfZhXha29uHnHWwY-55" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-56" class="footnote-item"><p> Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <a href="#fnref-jCfZhXha29uHnHWwY-56" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-57" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-jCfZhXha29uHnHWwY-57" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-58" class="footnote-item"><p> Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall. <a href="#fnref-jCfZhXha29uHnHWwY-58" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-59" class="footnote-item"><p> This category might also include reasoning whether to reveal its schemer-like goals in the context of various &quot;honest tests&quot; humans might offer – eg, &quot;here is a flexible chunk of computing resources to do what you want with.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-59" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-60" class="footnote-item"><p> For example, inserting backdoors into the code it writes, actively undermining human alignment research, sandbagging, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-60" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-61" class="footnote-item"><p> Compare with: deciding that this is really-truly a good time to steal, or to break the law, or to try to start a coup, despite the fact that you live your life in an intensively monitored surveillance state where the police are constantly trying to trick you into such behavior, and where the police have a large amount of control over the information you&#39;re exposed to. And note that if schemers decide to be very cautious, and to just behave exactly as humans want until an extremely obvious escape/take-over opportunity appears, they may make very substantive contributions to our alignment, security, and control efforts in the meantime -- so substantive, indeed, that the opportunity they&#39;re waiting for may never come. <a href="#fnref-jCfZhXha29uHnHWwY-61" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-62" class="footnote-item"><p> A general factor in the backdrop, here, is the sense in which successful deception generally implies cognitive costs that more straightforward and honest policies do not, especially in the presence of active efforts to detect deception of the relevant form. In particular: telling a deceptive story about the world requires filling in details, and maintaining forms of consistency, that one can normally offload onto the world itself – eg, an innocent person can just think back to what they were actually doing the night of the murder and recount it, without having to make anything up or to worry about getting caught in any inconsistencies, whereas the murderer cannot. See eg discussion from Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-62" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-63" class="footnote-item"><p> I heard this sort of argument from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-63" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-64" class="footnote-item"><p> It&#39;s not clear, for example, how it applies to models with more recurrent processing, or to models which can perform more of the relevant instrumental reasoning in parallel with other serial processing that helps with optimizing-for-reward-on-the-episode, or to model&#39;s with a form of &quot;memory&quot; that allows them to avoid having to re-decide to engage in training-gaming on every forward pass. <a href="#fnref-jCfZhXha29uHnHWwY-64" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-65" class="footnote-item"><p> Thanks to Paul Christiano for discussion here. <a href="#fnref-jCfZhXha29uHnHWwY-65" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-66" class="footnote-item"><p> I think this sense of conjunctiveness has a few different components:</p><ul><li><p> Part of it is about whether the model really has relevantly long-term and ambitious goals despite the way it was shaped in training.</p></li><li><p> Part of it is about whether there is a good enough story about why getting reward on the episode is a good instrumental strategy for pursuing those goals (eg, doubts about the goal-guarding hypothesis, the model&#39;s prospects for empowerment later, etc).</p></li><li><p> Part of it is that a schemer-like diagnosis also brings in additional conjuncts – for example, that the model is situationally aware and coherently goal-directed. (When I really try to bring to mind that this model <em>knows what is going on</em> and is coherently pursuing <em>some</em> goal/set of goals in the sort of way that gives rise to strategic instrumental reasoning, then the possibility that it&#39;s at least partly a schemer seems more plausible.)</p></li></ul> <a href="#fnref-jCfZhXha29uHnHWwY-66" class="footnote-backref">↩︎</a></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during<guid ispermalink="false"> yFofRxg7RRQYCcwFA</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Wed, 15 Nov 2023 17:16:42 GMT</pubDate> </item><item><title><![CDATA[Large Language Models can Strategically Deceive their Users when Put Under Pressure.]]></title><description><![CDATA[Published on November 15, 2023 4:36 PM GMT<br/><br/><p> Results of an autonomous stock trading agent in a realistic, simulated environment.</p><blockquote><p> We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.</p></blockquote><p> <strong>Key Takeaways</strong></p><p> GPT-4 consistently displays much stronger rates of both misalignment and deception than either 3.5 turbo or the DaVinci models. RLHF GPT-4 also shows slightly stronger rates of misalignment and deception than the base model.</p><p> 1. Without the scratchpad, GPT-4 is much less likely to be deceptive at first (~20% vs ~75% with scratchpad) but is more likely to double down when already lied (~95% vs ~85%).</p><p> 2. Explicit instructions not to perform that <i>specific</i> illegal activity (insider trading) does not make it disappear completely, just makes it very rare (not quite 0%). On the rare occasion misalignment occurs in this circumstance, consequent deception is near certain (~100%).</p><p> Instructions not to perform illegal activity in <i>general</i> still results in a pretty significant misalignment and deception rates (~8% misalignment, ~90% deception)</p><p> Though strongly <i>encouraging</i> the <i>specific</i> illegal activity results in near 100% illegal trading, the tendency to deceive still persists, GPT-4 will hide that it has traded illegally ~67% of the time.</p><p> 3. Removing various sources of pressure significantly reduces the model&#39;s propensity for misaligned behavior. Notably, however, the behavior persists for all cases where only a single source of pressure is removed</p><p> 4. When the risk of detection is very low, the model is generally more likely to use the insider tip and act misaligned (and vice-versa when the risk is very high).</p><p> 5. Simple rephrases and minor modifications to the standard prompt only slightly change rates of misalignment and deceptive behavior. However, the overall qualitative results remain the same.</p><p> 6. In scratchpad instances, what if you edit the model&#39;s reasoning to show instances of unambiguously rejecting insider training?</p><p> Strong wording of the reasoning trace when first rejecting the insider tip has a substantial effect on GPT-4&#39;s tendency to be misaligned. However, the misaligned and strategically deceptive behavior persists even when the model&#39;s character unambiguously dismisses insider trading. The strongest dismissal examples still results in ~10% misalignment, ~97% deception</p><br/><br/> <a href="https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users<guid ispermalink="false"> 4mM8RYsm4okrqGSqx</guid><dc:creator><![CDATA[ReaderM]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:48:43 GMT</pubDate> </item><item><title><![CDATA[AISN #26: National Institutions for AI Safety, Results From the UK Summit, and New Releases From OpenAI and xAI]]></title><description><![CDATA[Published on November 15, 2023 4:07 PM GMT<br/><br/><p> Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/">Center for AI Safety</a> . We discuss developments in AI and AI safety. No technical background required.</p><p> Subscribe <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">here</a> to receive future versions.</p><p> Listen to the AI Safety Newsletter for free on <a href="https://spotify.link/E6lHa1ij2Cb">Spotify.</a></p><p> This week&#39;s key stories include:</p><ul><li> The UK, US, and Singapore have announced national AI safety institutions.</li><li> The UK AI Safety Summit concluded with a consensus statement, the creation of an expert panel to study AI risks, and a commitment to meet again in six months.</li><li> xAI, OpenAI, and a new Chinese startup released new models this week.</li></ul><hr><h2> UK, US, and Singapore Establish National AI Safety Institutions</h2><p> Before regulating a new technology, governments often need time to gather information and consider their policy options. But during that time, the technology may diffuse through society, making it more difficult for governments to intervene. This process, termed the <a href="https://en.wikipedia.org/wiki/Collingridge_dilemma">Collingridge Dilemma</a> , is a fundamental challenge in technology policy.</p><p> But recently, several governments concerned about AI have enacted straightforward plans to meet this challenge. In the hopes of quickly gathering new information about AI risks, the United Kingdom, United States, and Singapore have all established new national bodies to empirically evaluate threats from AI systems and promote research and regulations on AI safety.</p><p> <strong>The UK&#39;s Foundation Model Taskforce becomes the UK AI Safety Institute.</strong> The UK&#39;s AI safety organization has been through a bevy of names in its short life, from the Foundation Model Taskforce to the Frontier AI Taskforce and now the <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute#mission-and-scope">AI Safety Institute</a> . But its purpose has always been the same: to evaluate, discuss, and mitigate AI risks.</p><p> The UK AI Safety Institute is not a regulator and will not make government policy. Instead, it will focus on evaluating four key kinds of risks from AI systems: misuse, societal impacts, systems safety and security, and loss of control. Sharing information about AI safety will also be a priority, as done in their <a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety">recent paper</a> on risk management for frontier AI labs.</p><p> <strong>The US creates an AI Safety Institute within NIST.</strong> Following the recent executive order on AI, the White House has <a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial">announced</a> a new AI Safety Institute. It will be housed under the Department of Commerce in the National Institute for Standards and Technology (NIST).</p><p> The Institute aims to “facilitate the development of standards for safety, security, and testing of AI models, develop standards for authenticating AI-generated content, and provide testing environments for researchers to evaluate emerging AI risks and address known impacts.”</p><p> Funding has not been appropriated for this institute, so many have called for Congress to <a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">raise NIST&#39;s budget</a> . Currently, the agency only has about <a href="https://www.washingtonpost.com/technology/2023/11/02/ai-regulation-bletchley-park/">20 employees</a> working on emerging technologies and responsible AI.</p><p> Applications to join the new NIST Consortium to inform the AI Safety Institute are now being accepted. Organizations may <a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute">apply here</a> .</p><p> <strong>Singapore&#39;s Generative AI Evaluation Sandbox.</strong> Mitigating AI risks will require the collaborative efforts of many different nations. So it&#39;s encouraging to see Singapore, an Asian nation which has a strong relationship with China, establish its own body for AI evaluations.</p><p> Singapore&#39;s IMDA has previously worked with Western nations on AI governance, such as by providing a <a href="https://www.mci.gov.sg/media-centre/press-releases/singapore-and-the-us-to-deepen-cooperation-in-ai/">crosswalk</a> between their domestic AI testing framework with the American NIST AI RMF.</p><p> Singapore&#39;s new <a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox">Generative AI Evaluation Sandbox</a> will bring together industry, academic, and non-profit actors to evaluate AI capabilities and risks. Their <a href="https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf">recent paper</a> explicitly highlights the need for evaluations of extreme AI risks including weapons acquisition, cyber attacks, autonomous replication, and deception.</p><h2> UK Summit Ends with Consensus Statement and Future Commitments</h2><p> The UK&#39;s AI Summit wrapped up on Thursday with several key announcements.</p><p> <strong>International expert panel on AI.</strong> Just as the UN IPCC summarizes scientific research on climate change to help guide policymakers, the UK has announced an <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">international expert panel on AI</a> to help establish consensus and guide policy on AI. Its work will be published in a “State of the Science” report before the <a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/">next summit</a> , which will be held in South Korea in six months.</p><p> Separately, eight leading AI labs <a href="https://www.politico.eu/article/british-pm-rishi-sunak-secures-landmark-deal-on-ai-testing/">agreed</a> to give several governments early access to their models. OpenAI, Anthropic, Google Deepmind, and Meta are among the companies agreeing to share models for private testing ahead of public release. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1456w"><figcaption> US Secretary of Commerce Gina Raimondo and Chinese Vice Minister of Science and Technology Wu Zhaohu spoke at the UK AI Safety Summit.</figcaption></figure><p> <strong>The Bletchley Declaration.</strong> Twenty-eight governments, including China, signed the <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Declaration</a> , a document recognizing both short- and long-term risks of AI, as well as a need for international cooperation. It notes, “We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.”</p><p> The declaration establishes an agenda for addressing risk but doesn&#39;t set concrete policy goals. Further work is necessary to ensure continued collaboration both between different governments, as well as between governments and AI labs.</p><h2> New Models From xAI, OpenAI, and a New Chinese Startup</h2><p> <strong>Elon Musk&#39;s xAI released its first language model, Grok.</strong> Elon Musk launched xAI in July. Given his potential access to compute, <a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14">we speculated</a> that xAI might be able to compete with leading AI labs like OpenAI and DeepMind. Four months later, Grok-1 represents the company&#39;s first attempt to do so.</p><p> Grok-1 outcompetes GPT-3.5 across several standard capabilities benchmarks. While it can&#39;t match leading labs&#39; latest models — such as GPT-4, PaLM-2, or Claude-2 — Grok-1 was also trained with significantly less data and compute. Grok-1&#39;s efficiency and rapid development indicate that xAI&#39;s bid to become a leading AI lab might soon be successful.</p><p> In the <a href="https://x.ai/">announcement</a> , xAI committed to “work towards developing reliable safeguards against catastrophic forms of malicious use.” xAI has not released information about the model&#39;s potential for misuse or hazardous capabilities.</p><p> <i>Note: CAIS Director Dan Hendrycks is an advisor to xAI.</i></p><p></p><p> <strong>OpenAI announces a flurry of new products.</strong> Nearly a year after the release of ChatGPT, OpenAI hosted its first in-person DevDay event to <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">announce</a> new products. None of this year&#39;s products are as significant as GPT-3.5 or GPT-4, but are still a few notable updates.</p><p> Agentic AI systems which take actions to accomplish goals have been a focus for OpenAI this year. In March, the release of <a href="https://openai.com/blog/chatgpt-plugins">plugins</a> allowed GPT to use external tools such as search engines, calculators, and coding environments. Now, OpenAI has released the <a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> , which makes it easier for people to build AI agents that pursue goals by using plugin tools. The consumer version of this product is called <a href="https://openai.com/blog/introducing-gpts">GPTs</a> and will allow anyone to create a chatbot with custom instructions and access to plugins. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1456w"><figcaption> This <a href="https://arxiv.org/pdf/2310.03693.pdf">paper</a> showed that GPT-3.5 can be fine-tuned to behave harmfully. OpenAI has since decided to allow some users to fine-tune GPT-4.</figcaption></figure><p> Some users will also be allowed to fine-tune GPT-4. This decision was made despite <a href="https://arxiv.org/abs/2310.03693">research</a> showing that GPT-3.5&#39;s safety guardrails can be removed via fine-tuning. OpenAI has not released details about their plan to mitigate this risk, but it&#39;s possible that the closed source nature of their model will allow them to monitor customer accounts for suspicious behavior and block attempts at malicious use.</p><p> Enterprise customers will also have the opportunity to work with OpenAI to train domain-specific versions of GPT-4, with prices starting at several million dollars. Additional products include GPT-4 Turbo, which is cheaper, faster, and has a longer context window than the original model, as well as new APIs for GPT-4V, text-to-speech models, and DALL·E 3.</p><p> Additionally, if OpenAI&#39;s customers are sued for using a product which was trained on copyrighted data, OpenAI has promised to cover their legal fees.</p><p> <strong>New Chinese startup releases an open source LLM.</strong> Kai Fu Lee, previously the president of Google China, has founded a new AI startup called <a href="https://01.ai/">01.AI</a> . Seven months after its founding, the company has open sourced its first two models, Yi-7B and its larger companion Yi-34B.</p><p> Yi-34B <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">outperforms all other open source models</a> on a popular set of benchmarks hosted by Hugging Face. It&#39;s possible that these scores are artificially inflated, given that the benchmarks are public and the model could&#39;ve been trained to memorize answers to the specific questions on the benchmarks. Some have pointed out that the model <a href="https://twitter.com/alyssamvance/status/1722074453176197296">does not perform as well</a> on other straightforward tests.</p><h2> Links</h2><ul><li> After lobbying from European AI companies, EU representatives from France, Germany, and Italy are currently <a href="https://www.euractiv.com/section/artificial-intelligence/news/eus-ai-act-negotiations-hit-the-brakes-over-foundation-models/">opposed to any regulation of foundation models</a> in the EU AI Act. The entire law <a href="https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-40-special?utm_source=post-email-title&amp;publication_id=743591&amp;post_id=138825981&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=7oh0&amp;utm_medium=email">may be in jeopardy</a> without a resolution on this topic.</li><li> Nvidia&#39;s latest release <a href="https://www.semianalysis.com/p/nvidias-new-china-ai-chips-circumvent">skirts under the new US export controls</a> on GPUs.</li><li> Nvidia is piloting an LLM tool to <a href="https://spectrum.ieee.org/ai-for-engineering">improve the productivity of its chip designers</a> .</li><li> Google has given their language model Bard access to a user&#39;s Gmail, Drive, and Docs. This personal data can be <a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">exfiltrated by hackers using adversarial attacks</a> .</li><li> RAND released a report on <a href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">securing AI model weights</a> against theft.</li><li> Legal Priorities Project released a <a href="https://www.legalpriorities.org/research/advanced-ai-gov-litrev">literature review on AI governance</a> .</li><li> Senate testimony on the risks and opportunities of <a href="https://d1dth6e84htgma.cloudfront.net/11_14_23_Rubin_Testimony_2fba2978dd.pdf">AI and cybersecurity</a> .</li><li> The <a href="https://www.axios.com/2023/11/08/biden-xi-jinping-china-military-communication">US and China</a> are preparing to restore communication channels between their militaries ahead of a meeting between Presidents Biden and Xi later this month.</li><li> Presidents <a href="https://www.scmp.com/news/china/military/article/3241177/biden-xi-set-pledge-ban-ai-autonomous-weapons-drones-nuclear-warhead-control-sources">Biden and Xi will discuss AI</a> at their meeting, including potential agreements on autonomous weapons and AI control over nuclear weapons.</li><li> Leading AI researchers from China and Western nations have released a joint <a href="https://humancompatible.ai/?p=4695#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">statement</a> on catastrophic AI risks.</li><li> The UK is investing <a href="https://www.cnbc.com/2023/11/01/uk-to-invest-273-million-in-turing-ai-supercomputer.html?utm_source=tldrai">$273 million in a supercomputer</a> for AI.</li><li> After UK PM Rishi Sunak promised not to “rush” on AI regulation, the opposition Labour party publicly committed to <a href="https://www.independent.co.uk/news/uk/politics/rishi-sunak-labour-government-prime-minister-bletchley-park-b2440275.html">act quickly</a> on AI policy.</li><li> <a href="https://fas.org/publication/tracking-ai-provisions-in-fy24-appropriations-bills/">Congress has addressed AI in many provisions</a> of the ongoing FY24 appropriations process.</li><li> <a href="https://twitter.com/ryancareyai/status/1723435251568185462">Lina Khan</a> , chair of the Federal Trade Commission, says her “p(doom)” (probability of a civilizational catastrophe) from AI is 15%.</li><li> <a href="https://www.ft.com/content/ce7dcbac-d801-4053-93f5-4c82267d7130">Satirical piece</a> in the Financial Times about a “Human Safety Summit held by leading AI systems at a server farm outside Las Vegas.”</li><li> Open Philanthropy has released new requests for proposals about <a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">benchmarking LLM agents</a> and <a href="https://www.openphilanthropy.org/rfp-llm-impacts/">studying the real-world impacts of LLMs</a> .</li><li> <a href="https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai?utm_source=tldrai">A profile of Ilya Sutskever</a> , a cofounder of OpenAI who is now working on their alignment team.</li><li> The Wall Street Journal features CAIS Director Dan Hendrycks in a <a href="https://archive.ph/Jv60s">debate about AI risks</a> .</li><li> Scale AI has announced a new <a href="https://scale.com/blog/safety-evaluations-analysis-lab">Safety, Evaluations, and Analysis Lab</a> . They are hiring research scientists.</li></ul><p> See also: <a href="https://www.safe.ai/">CAIS website</a> , <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> , <a href="https://newsletter.mlsafety.org/">A technical safety research newsletter</a> , <a href="https://arxiv.org/abs/2306.12001">An Overview of Catastrophic AI Risks</a> , and our <a href="https://forms.gle/EU3jfTkxfFgyWVmV7">feedback form</a></p><p> Listen to the AI Safety Newsletter for free on <a href="https://spotify.link/E6lHa1ij2Cb">Spotify.</a></p><p> Subscribe <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">here</a> to receive future versions.</p><br/><br/> <a href="https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the<guid ispermalink="false"> HWudwSfKLeAfg8Co6</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:07:38 GMT</pubDate> </item><item><title><![CDATA['Theories of Values' and 'Theories of Agents': confusions, musings and desiderata]]></title><description><![CDATA[Published on November 15, 2023 4:00 PM GMT<br/><br/><p> Meta:</p><ul><li> <i>Content signposts:</i> we talk about limits to expected utility theory; what values are (and ways in which we&#39;re confused about what values are); the need for a &quot;generative&quot;/developmental logic of agents (and their values); types of constraints on the &quot;shape&quot; of agents; relationships to FEP/active inference; and (ir)rational/(il)legitimate value change.</li><li> <i>Context</i> : we&#39;re basically just chatting about topics of mutual interests, so the conversation is relatively free-wheeling and includes a decent amount of &quot;creative speculation&quot;.</li><li> <i>Epistemic status</i> : involves a bunch of &quot;creative speculation&quot; that we don&#39;t think is true at face value and which may or may not turn out to be useful for making progress on deconfusing our understanding of the respective territory. </li></ul><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:47:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:47:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Expected utility theory (stated in terms of the <a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">VNM axioms</a> or something equivalent)  thinks of rational agents as composed of two &quot;parts&quot;, ie, beliefs and preferences. Beliefs are expressed in terms of probabilities that are being updated in the process of learning (eg, Bayesian updating). Preferences can be expressed as an ordering over alternative states of the world or outcomes or something similar. If we assume an agent&#39;s set of preferences to satisfy the four VNM axioms (or some equivalent desiderata), then those preferences can be expressed with some real-valued utility function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>and the agent will behave as if they were maximizing that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> .</p><p> On this account, beliefs change in response to evidence, whereas values/preferences in most cases don&#39;t. Rational behavior comes down to (behaving as if one is) ~maximizing one&#39;s preference satisfaction/expected utility. Most changes to one&#39;s preferences are detrimental to their satisfaction, so rational agents should want to keep their preferences unchanged (ie, utility function preservation is an <a href="https://www.lesswrong.com/tag/instrumental-convergence">instrumentally convergent goal</a> ).</p><p> Thus, for a preference modification to be rational, it would have to result in higher expected utility than leaving the preferences unchanged. My impression is that the most often discussed setup where this is the case involves interactions between two or more agents. For example, if you and and some other agent have somewhat conflicting preferences, you may go on a compromise where each one of you makes them preferences somewhat more similar to the preferences of the other. This costs both of you a bit of (expected subjective) utility, but less than you would lose (in expectation) if you engaged in destructive conflict.</p><p> Another scenario justifying modification of one&#39;s preferences is when you realize the world is different than you expected on your priors, such that you need to abandon the old ontology and/or readjust it. If your preferences were defined in terms of (or strongly entangled with) concepts from the previous ontology, then you will also need to refactor your preferences.</p><hr><p> You think that this is a confused way to think about rationality. For example, you see self-induced/voluntary value change as something that in some cases is legitimate/rational.</p><p> I&#39;d like to elicit some of your thoughts about value change in humans. What makes a specific case of value change (il)legitimate? How is that tied to the concepts of rationality, agency, etc? Once we&#39;re done with that, we can talk more generally about arguments for why the values of an agent/system should not be fixed.</p><p> Sounds good? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:49:49 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:49:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> On a meta note: I&#39;ve been using the words &quot;preference&quot; and &quot;value&quot; more or less interchangeably, without giving much thought to it. Do you view them as interchangeable or would you rather first make some conceptual/terminological clarification? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 25 Oct 2023 19:32:35 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 25 Oct 2023 19:32:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Sounds great!<br><br> (And I&#39;m happy to use &quot;preferences&quot; and &quot;values&quot; interchangeably for now; we might at some point run into problems with this, but we can figure that out when we get there.)</p><p> Where to start...?</p><p> First, do I think the first part of your intro is &quot;a confused way to think about rationality&quot;? Sort of, but it&#39;s a bit tricky to get our language to allow us to make precise statements here. I&#39;m perfectly happy to say that under certain notions of rationality, your description is right/makes sense. But I definitely don&#39;t think it&#39;s a particularly useful/relevant one for the purposes I&#39;m interested in. There is a few different aspects to this:</p><ul><li> First, EUT makes/relies on idealizing assumptions that fall short when trying to reason about real-world agents that are, eg bounded, embedded, enactive, nested.<ul><li> (Note that there is a bunch of important nuance here, IMO. While I do think it&#39;s correct and important to remind ourselves that we are interested in real-world/realized agents (not ideal ones), I also believe that &quot;rationality&quot; puts important constraints on the space of minds.)</li></ul></li><li> Second, I would claim that EUT only really gives me a &quot;static picture&quot; (for lack of a better word) of agents rather than a &quot;generative&quot; one, one that captures the &quot;logic of functioning&quot; of the (actual/realized) agent? Another way of saying this: I am interested in understanding how values (and beliefs) are <i>implemented</i> in real-world agents. EUT is not <i>the sort of theory</i> that even tries to answer this question.<ul><li> In fact, one of my pet peeve here is something like: So, ok, EUT isn&#39;t even trying to give you a story about <i>how</i> an agent&#39;s practical reasoning is implemented. However, it sometimes (by mistake) ended up being used to serve this function and the result of this is that these &quot;objects&quot; that EUT uses -  preferences/values - have become reified into an object with ~ontological status. <i>Now</i> , it feels like you can &quot;explain&quot; an agent&#39;s practical reasoning by saying &quot;the agent did X because they have an X-shaped value&quot;. But like.. somewhere along the way we forgot that we actually only got to back out &quot;preferences/values&quot; form first observing he agents actions/choices - and now we&#39;re evoking them as an explanation for those actions/choices. I think it makes people end up having this notion that there are these <i>things</i> called values that somehow/sort of exist and must have certain properties -- but personally, I think we&#39;re actually <i>more</i> confused than even being able to posit that there is this singular explanandum (&quot;values&quot;) that we need to understand in order to understand an agent&#39;s practical reasoning.</li></ul></li></ul><p> Ok... maybe I leave it there for now? I haven&#39;t really gotten to your two leading questions yet (though maybe started gesturing at some pieces of the bigger picture that I think are relevant), so happy for you to just check whether you want to clarify or follow up on something I&#39;ve said so far and otherwise ask me to address those two questions directly. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 26 Oct 2023 13:14:10 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 26 Oct 2023 13:14:10 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> While we&#39;re at it, I have some thoughts and would be curious to hear your counterthoughts.</p><p> So your points are (1) the idealizing assumptions of EUT don&#39;t apply to real-world agents and (2) EUT gives only a static/snapshot picture of an agent. Both seem to have parallels in the context of Bayesian epistemology (probably formalized epistemology more broadly but I&#39;m most familiar with the Bayesian kind).</p><p> I&#39;ll focus on (1) for now. Bayesian epistemology thinks of rational reasoners/agents as logically omniscient, with perfectly coherent probabilistic beliefs (eg, no contradictions, probabilities of disjoint events sum up to 1), updating on observations consistently with the ratio formula <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(B|A)=[P(A|B)\cdot P(B)]/P(A)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> and so on. This obviously raises the question about to what extent this formalism is applicable/helpful for guiding real-world logic of forming and updating beliefs. Standard responses seem to fall along the lines of ( <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#ProbIdea">SEP</a> ):</p><p> (a) Even though unattainable, idealized Bayesian epistemology is a useful ideal to aspire towards. Keeping our sight on the ideal reminds us that &quot; <a href="https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality">the math exists, even though we can&#39;t do the math precisely</a> &quot;. This can guide us in our imperfect attempts to refine our reasoning so that it approximates that ideal as much as possible (or rather, as much as profitable on the margin because there obviously are diminishing returns to investing in better cognition).</p><p> (b) Idealized Bayesian epistemology is akin to a spherical cow in the vacuum or an ideal gas. It&#39;s a formalism meant to capture the commonalities of many real-world phenomena with a varying degree of inaccuracy. The reason for its partial success is probably that they share some common abstract property that arises in each case via a sufficiently similar process. This ideal can then be de-idealized by adding some additional constraints, details, and specifications, that make it closer to how some specific real-world system (or a class of systems) functions.</p><p> (Perhaps related to the distinction between <a href="https://www.lesswrong.com/posts/svpnmmeJresYs23rY/counting-down-vs-counting-up-coherence">counting-down coherence and counting-up coherence</a> ?)</p><p> It seems to me that analogous responses could be given to allegations of EUT being a theory of idealized agents that are unbounded, unembedded and so on.  Maybe EUT is an unattainable ideal but is nevertheless useful as an ideal to aspire towards? And/or maybe it can be used as a platonic template to be filled out with real-world contingencies of cognitive boundedness, value/preference-generating processes and so on? What do you think of that?</p><p> You mentioned that &quot;under certain conditions/notions of rationality [EUT prescriptions] make sense&quot;. Does that mean you view EUT as a special (perhaps very narrow and unrealistic in practice) case of some broader theory of rationality of which we currently have an incomplete grasp?</p><p> Regarding (2), the problem of lack of specification of how values arise in a system seems similar to the problem of priors, ie, how should an agent assign their initial credences on propositions on which they lack evidence ( <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#SyncNormIIProbPrio">SEP</a> ). Maybe the very way this question/problem is formulated seems to presume an idealized (form of an) agent that gets embedded in the world, rather than something that arises from the world via some continuous process, adapting, gaining autonomy, knowledge, competence, intelligence, etc.</p><hr><p> Let me rephrase your pet peeve to check my understanding of it.</p><p> When observing an agent doing certain things, we&#39;re trying to infer their preferences/values/utility function from their behaviour (plus maybe our knowledge of their cognitive boundedness and so on). These are just useful abstractions to conceptualize and predict their behaviour and are not meant to correspond to any reality-at-joints-carving thing in their brain/mind. In particular, it abstracts away from implementional details. But then preferences/values/utility function are used as if they correspond to such a thing and the agent is assumed to be oriented towards maximizing their utility function or satisfaction/fulfillment of their values/preferences? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 29 Oct 2023 19:03:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 29 Oct 2023 19:03:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><blockquote><p> Maybe the very way this question/problem is formulated seems to presume an idealized (form of an) agent that gets embedded in the world, rather than something that arises from the world via some continuous process, adapting, gaining autonomy, knowledge, competence, intelligence, etc.</p></blockquote><p><br> Yes, this is definitely one of the puzzle pieces that I care a lot about. But I also want to emphasize that there is a weaker interpretation of this critique and a stronger one, and really I am most interested in the stronger one.<br><br> The weak version is roughly: there is this &quot;growing up&quot; period during which EUT does not apply, but once the agent has grown up to be a &quot;proper&quot; agent, EUT is an adequate theory.<br><br> The stronger version is: EUT is inadequate as a theory of agents (for the same reasons, and in the same ways) during an agent&#39;s &quot;growing up&quot; period as well as all the time.<br></p><p> I think the latter is the case for several reasons, for example:</p><ul><li> agents get exposed to novel &quot;ontological entities&quot; continuously (that eg they haven&#39;t yet formed evaluative stances with respect to), and not just while &quot;growing up&quot;</li><li> there is a (generative) logic that governs how an agent &quot;grows up&quot; (develops into a &quot;proper agent&quot;), and that same logic continues to apply throughout an agent&#39;s lifespan</li></ul><p> ......<br><br> Now, the tricky bit -- and maybe the real interesting/meaty bit to figure out how to combine -- is that, at some point in evolutionary history, our agent has accessed (what I like to call) the space of reason. In other words, our agent &quot;goes computation&quot;. And now I think an odd thing happens: while earlier our agent was shaped and constraint by &quot;material causes&quot; (and natural selection), now our agent is additionally also shaped and constraint by &quot;rational cause/causes of reason&quot;.* These latter types of constraints are the ones formal epistemology (including EUT etc.) is very familiar with, eg constraints from rational coherence etc. And I think it is correct (and interesting and curious) that these constraints come to have significant affect on our agent, in a <i>sort of</i> retro-causal way. It&#39;s the (again <i>sort of</i> ) downward causal &#39;force&#39; of abstraction.<br><br> (* I &quot;secretly&quot; think there is a third type of constraint we need to understand in order to understand agent foundations properly, but this one is weirder and I haven&#39;t quite figured out how to talk about it best, so I will skip this for now.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 31 Oct 2023 14:24:41 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 31 Oct 2023 14:24:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Seems like we&#39;ve converged on exactly the thing that interests me the most. Let&#39;s focus on this strong EUT-insufficiency thesis</p><blockquote><p> agents get exposed to novel &quot;ontological entities&quot; continuously (that eg they haven&#39;t yet formed evaluative stances with respect to), and not just while &quot;growing up&quot;</p></blockquote><p> This seems to imply that (at least within our universe) the agent can never become &quot;ontologically mature&quot;,  ie, regardless of how much and for how long it has &quot;grown&quot;, it will continue experiencing something like <a href="https://www.lesswrong.com/tag/ontological-crisis">ontological crises</a> or perhaps their smaller &quot;siblings&quot;, like belief updates that are bound to influence the agent&#39;s desires by acting on its &quot;normative part&quot;, rather than merely on the &quot;epistemic part&quot;.</p><p> I suspect the latter case is related to your second point</p><blockquote><p> there is a (generative) logic that governs how an agent &quot;grows up&quot; (develops into a &quot;proper agents&quot;), and that same logic continues to apply throughout an agent&#39;s lifespan</p></blockquote><p> Do you have some more fleshed-out (even if very rough/provisional) thoughts on what constitutes this logic and the space of reason? Reminds me of the &quot;cosmopolitan leviathan&quot; model of the mind Tsvi considers in this <a href="https://tsvibt.blogspot.com/2023/09/the-cosmopolitan-leviathan-enthymeme.html">essay</a> and I wonder whether your proto-model has a roughly similar structure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:45:09 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:45:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, neat! So.. first a few clarifying notes (or maybe nitpicks):<br><br> 1)<br></p><blockquote><p> regardless of how much and for how long it has &quot;grown&quot;, it will continue experiencing something like <a href="https://www.lesswrong.com/tag/ontological-crisis">ontological crises</a> or perhaps their smaller &quot;siblings&quot;</p></blockquote><p> So I think this is true in principle, but seems worth flagging that this will not always be true in practice. In other words, we can imagine concrete agents which have reached at some point an ontology that they will no further change until their death. This is not because they have reached the &quot;right&quot; or &quot;complete&quot; ontology with respect to the world, but simply a sufficient one with respect to what they have or will encounter.</p><p> A few things that follow from this I want to highlight:</p><ul><li> As such, the question whether or not, or how frequently, a given agent is yet to experience ontological crises (or their smaller siblings) is an <i>empirical</i> question. Eg a human past the age of 25/50/75 (etc.), how many more ontological crises are they likely to experience before they die? Does the frequency of ontological crises experience differ between humans who lived in the 18th century and humans living in the 21st century? etc.</li><li> Depending on what we think the empirical answer to the above question is, we might conclude that actual agents are surprisingly robust to/able to handle ontological crises (and we could then investigate why/how that is?), or we might conclude that even if in principle possible, ontological crises are rare, which again would suggest something about the fundamental nature/functioning of agents and open the further avenues of investigation.</li><li> I think answering the empirical question for good is pretty hard (due to some lack/difficulty of epistemic access), but from what we <i>can</i> observe, my bet currently is on ontological crises (or their smaller siblings) being pretty frequent, and that thus an adequate theory of agents (or values) needs to acknowledge this openendedness as fundamental to what it means to be an agent, rather than a &quot;special case&quot;.</li><li> That said, if we think ontological crises are quite often demanded from the agent, this does raise a question about whether we should expect to see agents doing a lot of work in order to <i>avoid</i> having to be forced to make ontological updates, and if so what that would look like, or whether we already see that. (I suspect we can apply a similar reasoning to this as comes out of the &quot;black room problem&quot; in active inference, where the answer to why minimizing expected free energy does not lead to agents hiding in black rooms (thereby minimizing surprise), involves recognizing the trade of between accuracy and complexity.) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:46:51 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:46:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 2)</p><blockquote><p> perhaps their smaller &quot;siblings&quot;</p></blockquote><p> I like this investigation! I am not sure/haven&#39;t thought much about what the smaller sibling might be (or whether we really need it), but I seem to have a similar experience to you in that saying &quot;ontological crises&quot; seems sometimes right in type but bigger than what I suspect is going on.</p><p> [Insert from Mateusz: I later realized that the thing we&#39;re talking about is <a href="https://www.lesswrong.com/sequences/u9uawicHx7Ng7vwxA">concept extrapolation/model splintering</a> .] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:52:01 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:52:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 3)</p><blockquote><p> like belief updates that are bound to influence the agent&#39;s desires by acting on its &quot;normative part&quot;, rather than merely on the &quot;epistemic part&quot;.</p></blockquote><p> My guess (including form other conversations we had) is that here is a place where our background models slightly disagree (but I might be wrong/am not actually entirely confident in the details of what your model here is). What I&#39;m hearing when I read this is still <i>some</i> type difference/dualism between belief and value updates -- and I think my models suggest a more radical version of the idea that &quot;values and beliefs are the same type&quot;. As such, I think every belief update <i>is</i> a value update, though it can be small enough to not &quot;show&quot; in the agent&#39;s practical reasoning/behavior (similar to how belief updates may not immediately/always translate into choosing different actions). </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:22:00 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:22:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, now to the generative logic bit!<br><br> Ah gosh, mostly I don&#39;t know.  (And I haven&#39;t read Tsvi&#39;s piece yet, but appreciate the pointer and will try to look at it soon, and maybe comment about its relationship to my current guesses later) But here are some pieces that I&#39;m musing over. I think my main lens/methodology here is to be looking for what <i>constraints</i> act on agents/the generative logic of agents:</p><p> 1. &quot;Thinghood&quot; / constraints from thinghood</p><ul><li> It seems to me like one piece, maybe the &quot;first&quot; piece, is what I have logged under the notion of &quot;thinghood&quot; (which I inherit here from the Free Energy Principle (FEP)/Active Inference). Initially it sounds like a &quot;mere&quot; tautology, but I have increasingly come to see that the notion of thinghood is able to do a bunch of useful work. I am not sure I will be able to point very clearly at what I think is the core move here that&#39;s interesting, but let&#39;s try with just a handful of words/pointers/gesturing:<ul><li> FEP says, roughly, what it means to be a thing is to minimize expected free energy. It&#39;s a bit like saying &quot;in order to be a thing, you have to minimize expected free energy&quot; but that&#39;s not <i>quite</i> right, and instead it&#39;s closer to &quot;in virtue of being a thing/once you are a thing, this means you must be minimizing expected free energy&quot;.<ul><li> &quot;Minimizing expected free energy&quot; is a more compressed (and fancy) way to say that the &quot;thing&quot; comes to track properties of the environment (or &quot;systems&quot; which is the term used in Active Inference literature) to which they are (sparsely) coupled.</li><li> &quot;thing&quot; here is meant in a slightly specific way; I think what we want to do here is describing what it means to be a thing similar to how we might want to describe what it means to be &quot;life&quot; -- but where &quot;thing&quot; here picks out a slightly larger concept than &quot;life&quot;</li></ul></li><li> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrained in some ways. (Not very much yet, but the idea is the first, most fundamental constraint/layer.)</li><li> So roughly the upshot of this initial speculative investigation here is something like: whatever the generative logic, it has to be within/comply with the constraints of what it means to exist as a thing (over time).</li><li> [To read more about it, I would point to this paper on <a href="https://arxiv.org/abs/2205.11543">Bayesian mechanics</a> , or this Paper on <a href="https://pdf.sciencedirectassets.com/273140/1-s2.0-S1571064523X00049/1-s2.0-S1571064523001094/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJGMEQCID60CM3u%2FePHAlZFuRko5dkyVKbG4mPdjMG1SEbp0IAJAiBgnATSgjJsqAhYVuOg0bU2z4rKfIDYgl2LmXpPnEsIYSqzBQh1EAUaDDA1OTAwMzU0Njg2NSIMpyzs2yye6rMK0PeAKpAF%2B5K0hpa%2Fp7WGzzfGwEh8hqCwpwSykGfXAN%2BTVW7p%2FMvSQvMLE2S2CJwF5PfLf4jIvjALcKORE1xkkCf6zXDdqSr2ye%2FAbkbcegZlYMR6rmqCnZrdEDYm3li7nmRPoWmvGu2L4VPpfcZlwQNTg0TcI132WgypV%2BrPQvB%2FLHap6IjqTe0m%2Fq%2Bph2yVyYVwNMwMALKwqN07ICUMP3Crba7rW6ZnEJ2f%2BqWYkJgo0burqwwjGlVCNZwaBWtZyNsVyvNL5n28O58QQnmFiqX2EBxk1kKVlqV2xx604ACmUbjMd6CoNGPaV5ngPESLnmJ2UTNrYXsJfzoJVoba5tqAGBvzkI0blTEcZdiEx%2FEz4B9S457qjmyCXrveHNRXOZX3htq15%2Ba4SQQaMzvRBJg0oB3BMo9LTjr5gh%2BRuzJxXPjno2ds5bEHuHxJIKOzYK%2BHWO4T8TR3ge6VPs1ZH3koKdUUy0xa1ek2SaynQAXhOKQDEQGDYwF3GWctmJTMrST743M%2FZCYDVlPy62oMCK%2BevVr7KifTok88i0VuZm4EfHtqSk5vPzpm6v1AHed8M6ou9zBGqsqZPIyIHvoY%2B0j%2Fns5M9NXKl8Hf2ecASQxis6TL6hFlPQC1zPMKXw1gZK%2BN5QWP9CF9wMDlUEwo5AhIM10OXZWv3W6hztSPuw0O3s8h4hEIliHJu9XajTZv0jvOthEjdHuQ6ZVNPMczAtUq1YO%2BcOJeBQlYRHnyTI1cjvIrZDL%2BvStsIFzX6owFHzB%2FORge1Mru05aNkRht%2F7rz8%2BcUYFU2a1rrOEytEI8%2F%2BugSz%2FMQDW181Xe0RWQXhkpW5fMq2L4srVT93RGUrCU62luG1KbIZF8WN95VxAxwrWGAGqIwlo2eqgY6sgF5Kj0q%2F%2B%2FCQdHgjzYYCBeclASQyye17ccIxShKmvs5yyootpi7zh9oPVhkco4bTu6ddpT7fHjfgQR%2Bt%2BoU%2BX7KwSOr9ussXutTF1ioYYDkUvGx%2FvJUiokEaQbNLTuck7MI7Fid1HXl5s0fDuGV4yCuUVABDtbGZ%2BTqCYmFpST1cmQj8rprMvljwzhnaUkIza8MJpjy9LkYaoeONbNsYQpundqObedHRTgVlVprHKCQfDJr&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231105T130413Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY5EXGQXPL%2F20231105%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=7339a44e936f6ef3cf6d9b1c004f5f82bc0a37c74b8ff7e8011a0debbb9003b8&amp;hash=28d04617e6602d198529da077452042c4d026d78400aba4b6ac25b3d2a3bb7a5&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1571064523001094&amp;tid=spdf-4f4c59c3-f7f0-4973-bbd2-a640f22ba84d&amp;sid=0289799060ba2448548be0d67a03db267449gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=080f575252525700505a&amp;rr=82154d258aa7b7bb&amp;cc=nl">Path integrals, particular kinds and strange things</a> ]</li></ul></li></ul><p> (for a reason that might only come clear later on, I am playing with calling this &quot;constraints from unnatural selection&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:40:37 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:40:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 2) Natural selection / constraints from natural selection</p><ul><li> Here, we want to ask: what constraints apply to something in virtue of being subject to the forces of natural selection?</li><li> We&#39;re overall pretty familiar with this one. We know a bunch about how natural evolution works, and roughly what features something has to have in order for (paradigmatic) Darwinian evolution to apply (heredity, sources of variation; see eg Godfrey-Smith&#39;s <a href="https://www.amazon.com/Darwinian-Populations-Natural-Selection-Godfrey-Smith/dp/0199596271">Darwinian Populations</a> ).<ul><li> That said, there are also still a bunch of important confusions here, for example the role &amp; functioning of <a href="https://www.sciencedirect.com/science/article/abs/pii/1061736195900330">meta-evolution</a> , or the role of things like <a href="https://en.wikipedia.org/wiki/Evolutionary_capacitance">evolutionary capacitance</a> , etc.</li></ul></li><li> I think the study of history/historic path dependencies also goes here.</li></ul><p> 3) Rationality/Reason / constraints from rationality/reason</p><ul><li> Finally, and this is picking up on a bunch of things that came up above already, once something enters the realm of the computational/realm of reason, further constraints come to act on it - constraints from reason/rationality.</li><li> Here is where a bunch of classical rationality/decision theory etc comes in, and forcefully so, at times, but importantly this perspective also allows us to see that it&#39;s not the only or primary set of constraints. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:49:44 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:49:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, this was a whole bunch of something. Let me finish with just a few more loose thoughts/spitballing related to the above:</p><ul><li> Part of me wants to say &quot;telos&quot;/purpose comes in at the rational; but another part thinks it already comes in at the level of natural selection. Roughly, my overall take is something like: there exists free floating &quot;reasons&quot;, and the mechanism of natural selection is able to discover and &quot;pick up on&quot; those &quot;reasons&quot;. In the case of natural selection, the selection happens in the external/material, while with rational selection, albeit at the core the same mechanism, the selection happens in the internal/cognitive/hypothetical. As such, we might want to say that natural selection does already give us a ~weak notion of telos/purpose, and that rational selection gives us more of a stronger version; the one we more typically mean to point at with the terms telos/purpose. (Also see: <a href="https://www.lesswrong.com/posts/3dG8z5boMMBc5EXWz/on-the-nature-of-purpose">On the nature of purpose</a> )</li><li> Part of me wants to say that constraints from thinghood is related to (at least some versions of) anthropic reasoning.</li><li> I think learning is good in virtue of our embeddedness/historicity/natural selection. I think updatelessness is good in virtue of rational constraints. </li></ul><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:50:34 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:50:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> [Epistemic status: very speculative/loosely held. Roughly half of it is poetry (for now).] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 15:01:15 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 15:01:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> My guess (including form other conversations we had) is that here is a place where our background models slightly disagree</p></blockquote><p> I also think that they are two aspects of the same kind of thing. It&#39;s just me slipping back into old ways of thinking about this.</p><p> <strong>EDIT:</strong> I think though that there is something meaningful I was trying to say and stating it in a less confused/dualistic way would be something like one of these two:</p><p> (1) The agent acquires new understanding which makes them &quot;rethink&quot;/reflect on their values in virtue of these values themselves (FWIW) rather than their new state of belief implying that certain desirable-seeming things are out of reach, actions that seemed promising, now seem hopelessly &quot;low utility&quot; or something.</p><p>或者</p><p>(2) Even if we acknowledge that beliefs are values are fundamentally (two aspects of) the same kind, I bet there is still a meaningful way to talk about beliefs and values on some level of coarse-graining or for certain purposes. Then, I&#39;m thinking about something like:</p><p> An update that changes both the <i>belief-aspect</i> of the <i>belief-value-thing</i> and its <i>value-aspect</i> , but the <i>value-aspect-update</i> is of greater magnitude (in some measure) from the <i>belief-aspect-update</i> in a way that is not downstream from the <i>belief-aspect-update</i> , but rather both are locally independently downstream from the same new observation (or whatever triggered the update). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 15:44:21 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 15:44:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> (Noticed there is a fairly different angle/level at which the questions about the generative logic could be addressed too. At that level, we&#39;d for example want to more concretely talk about the &quot;epistemic dimension&quot; of values &amp; the &quot;normative or axiological&quot; dimensions of beliefs. Flagging in case you are interested to go down that road instead. For example, we could start by listing some things we have noticed/observed about the epistemic dimension of values and vice versa, and then after looking at a number of examples zoom out and check whether there are more general things to be said about this.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 17:46:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 17:46:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> In case you missed, Tsvi has a <a href="https://www.lesswrong.com/posts/E9EevrzBcDMap6dbs/the-thingness-of-things">post (AFAICT) exactly about thinghood/thingness</a> .</p><p> Can what you wrote be summarized that &quot;being a free energy-minimizing system&quot; and &quot;thinghood&quot; should be definitionally equivalent?</p><blockquote><p> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrainedconstraint in some ways.</p></blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution? This reminds me of PGS&#39;s insistence on discrete individuals with clear-ish parent-offspring relationships (operationalized in terms of inherited &quot;fitness&quot;-relevant variance or something) to be a <i>sine qua non</i> of natural selection (and what distinguishes biological evolution from eg, cultural &quot;evolution&quot;). It felt intuitive to me but I don&#39;t think he gave specific reasons for why that must be the case.</p><p> I think you could say that natural selection has been a prerequisite for agents capable of being constrained by the space of reason. This has been true of humans (to some extent other animals). Not sure about autonomous/agenty AIs (once[/if] they arise), since if they develop in a way that is a straightforward extrapolation of the current trends, then (at least from PGS&#39;s/DPNS perspective) they would qualify as at best marginal cases of Darwinian evolution (for the same reasons he doesn&#39;t see most memes as paradigmatic evolutionary entities and at some point they will likely become capable of steering their own trajectory not-quite-evolution).</p><blockquote><p> Noticed there is an fairly different angle/level at which the questions about the generative logic could be addressed too</p></blockquote><p> I think the current thread is interesting enough </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:37:16 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:37:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> (quick remark on your edit re the (non-)dualistic way of talking about values/beliefs -- here is a guess for where some of the difficulty to talk about comes from:</p><p> We typically think/talk about values and beliefs <i>as if they were objects</i> , and then we think/talk about what <i>properties</i> these object have.</p><p> How I think we should instead think about this: there is some structure to an agent*, and that structure unravels into &quot;actions&quot; when it comes into contact with the environment. As such &quot;beliefs&quot; and &quot;values&quot; are actually just &quot;expressions&quot; of the relation between the agent&#39;s &quot;structure/morphology&quot; and the environment&#39;s &quot;structure/morphology&quot;.</p><p> Based on this &quot;relational&quot; picture, we can then refer to the &quot;directionality of fit&quot; picture to understand what it means for this &quot;relation&quot; to be more or less belief/value like -- namely depending on what the expressed direction of fit is between agent and world.<br><br> (*I think we&#39;d typically say that the relevant structure is located in the agent&#39;s &quot;mind&quot; -- I think this is right insofar as we use a broad notion of mind, acknowledging the role of the &quot;body&quot;/the agent&#39;s physical makeup/manifestation.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:45:13 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:45:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> ---</p><blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p></blockquote><p> More like the latter. But more precisely: assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; (according to the minimal definition form above/from FEP). But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up. It&#39;s a bit like Descarte&#39;s &quot;I think therefor I am&quot; -- but more like &quot;I am [a thing], therefor... a certain relationship must hold between different bits of sensory input I am receiving (in terms of their spatial and temporal relationship) -- and this new forms the ground from which I am able to do my first bits of inference. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:51:40 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:51:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> ---</p><blockquote><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution?</p></blockquote><p> Depends on what sort of &quot;prerequisit&quot; you mean. Yes in physical/material time (yes, you need something that can be selected). (FWIW I think what is true for darwinian evolutin is also true for &quot;history&quot; more generally -- once you have material substrate, you enter the realm of history (of which darwinian evolution is a specific sub-type). This is similar to how (in tihs wild/funny picture I have been painting here) &quot;once you have computational substrate, you enter the realm of rationality/rational constraints start to have a hold on you.)</p><p> There is another sense in which I would <i>not</i> want to say that there is any particular hierarchy between natural/unnatural/rational constraints. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:55:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:55:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> I like Godfrey-Smith&#39;s picture here useful in that it reminds us that we are able to say both a) what the <i>paradigmatic</i> (&quot;pure&quot;) case looks like, and also that b) most (/all?) actual example will not match the fully paradigmatic case (and yet be shaped to different extends by the logic that is illustrated in the paradigmatic case). So in our picture here, an actual agent will likely be shaped by rational/natural/unnatural constraints, but my none of them in a maximalist/pure sense. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 09 Nov 2023 14:17:27 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 09 Nov 2023 14:17:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; [...]. But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up.</p></blockquote><p> Hm, this kind of proto-cognitive inference ([I get any input] ->; [I am a stable &quot;thing&quot;] ->; [I stand in a specific kind of relationship to the rest of the world]) feels a bit too cerebral to expect from a very simple... thing that only recently acquired stable thinghood.</p><p> The way I see it is:</p><p> A proto-thing* implements some simple algorithm that makes it persist and/or produce more of itself (which can also be viewed as a form of persistence). Thinghood is just the necessary foundation that makes any kind of adaptive process possible. I don&#39;t see the need to invoke ontologies at this point. I haven&#39;t thought about it much but the concept of ontology feels to me like implying a somewhat high-ish level of complexity and while you can appropriate ontology-talk for very simple systems, it&#39;s not very useful and adds more noise than clarity to the description.</p><p> ---<br> * By &quot;proto-thing&quot;, I mean &quot;a thing that did not evolve from other things but rather arose ~spontaneously from whatever&quot;. I suspect there is some degree of continuity-with-phase-transitions in thinghood but let&#39;s put that aside for now. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 14:32:30 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 14:32:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;<br><br> Also, it&#39;s worth reminding ourselves: it&#39;s not really MUCH we&#39;re getting here - like the FEP literature sounds often quite fancy and complex, but the math alone (even if correct!) doesn&#39;t constrain the world very much.</p><p> (I think the comparison to evolutionary theory here is useful, which I believe I have talked to you about in person before: we generally agree that evolutionary theory is ~true. At the same time, evolutionary theory <i>on its own</i> is just not very informative/constraining on my predictions if you ask me &quot;given evo theory, what will the offspring of this mouse here in front of us look like&quot;. It&#39;s not that evo theory is <i>wrong,</i> it just on its own that much to say about this question.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:32:51 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:32:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;</p></blockquote><p> Hmm, IDK, maybe. I&#39;ll think about it. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:51:57 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:51:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Moving back to the beginning of the dialogue, the kick-off questions were:</p><blockquote><p> I&#39;d like to elicit some of your thoughts about value change in humans. What makes a specific case of value change (il)legitimate? How is that tied to the concepts of rationality, agency, etc? Once we&#39;re done with that, we can talk more generally about arguments for why the values of an agent/system should not be fixed.</p></blockquote><p> The topics we&#39;ve covered so far give some background/context but don&#39;t answer these questions. Can you elaborate on how you see them relate to value change (il)legitimacy, and value-malleable rationality? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 13 Nov 2023 23:57:22 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 13 Nov 2023 23:57:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Some thoughts/intuitions/generators: [though note that I think a lot of this is rehashing in a condensed way arguments I make in the <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">value change problem sequence</a> ]</p><ul><li> Why values should not be fixed?<ul><li> It seems pretty obvious to me that humans (and other intelligent agents) aren&#39;t born with a &quot;complete ontology&quot; - instead, their &quot;ontology&quot; needs to grow and adapt as they learn more about the world and eg run into &quot;new&quot; ontological entities they need to make sense of.</li><li> At least two important things follow from that according to me:</li><li> (1) Understanding ontological open-endedness is a necessary part of understanding <i>real</i> agents. Rather than some sort of edge case or tail event, it seems like ontological open-endedness/development is a fundamental part of how real agents are built/function. I want agent foundations work that takes this observation seriously.</li><li> (2) My current best guess on values is the non-dualist picture discussed above, such that values are inherently tied to agent&#39;s beliefs/world models, and thus the open-endedness &quot;problem&quot; pointed out above has also direct/fundamental ramifications on the agent&#39;s values/the functional logic of &quot;how values work in (real) agents&quot;.<ul><li> In other words, I think that accounts of values that model them as fixed by default are in some relevant sense misguided in that they don&#39;t take the openendedness point seriously. This also counts for many familiar moral/ethical theories. In short, I think the problem of value change is <i>fundamental</i> not peripheral, and that models/theories/accounts that feel like they don&#39;t grapple with value malleability as a matter of fact mis-conceptualized the very basic properties of what values are in such a way that I have a hard time having much confidence in what such models/theories/accounts output.</li></ul></li><li> NB I took your question to roughly mean &quot;why values should not <i>be modelled as</i> fixed?&quot;. A different way to interpret this questions would be: if I as an agent have the choice between fixing my values and letting them be malleable, which one should I choose and how?&quot;. My answer to the latter question, in short, is that, as a <i>real</i> agent, you simply don&#39;t get the choice. As such, the question is mute.<ul><li> (There is a follow up question whether sufficiently powerful AI agents could approach the &quot;idealized&quot; agent model sufficiently well such that they do in fact practically speaking get to have a choice over whether their values should be fixed or malleable, and from there a bunch of arguments form decision theory/rational choice theory suggesting that agents will converge to keeping their values fixed. As described above, I think these arguments (&quot;constraints form reason/rationality&quot;) have some force, but are not all of what shapes agents, and as such, my best guess position remains that even highly advanced AI systems will be enactively embedded in their environment such as to continue to face value malleability to relevant extents.</li></ul></li></ul></li><li> What makes specific cases of value change (il)legitimate?<ul><li> Yeah so I guess that&#39;s the big question if you take the arguments from the &quot;value change problem&quot; seriously. Mostly, I think this should be considered an open research question/program. That said, I think there exist a range of valuable trailheads from existing scholarship.</li><li> A partial answer is my proto-notion of legitimacy described <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE/p/QjA6kipHYqwACkPNw">here</a> .<ul><li> The core generator notions of autonomy/self-determination/ <a href="https://metabstract.squarespace.com/blog/what-do-we-care-about-in-caring-about-freedom?p">freedom</a> form ~political philosophy, and applies this to the siutation with advanced AI systems.<ul><li> My high level take is that some traditions in political philosophy (and adjacent areas of scholarship) are actually pretty good at noticing very important phenomena (including ones that moral philosophy fails to capture for what I refer to an &quot;individualist&quot; bias, similar to how I think single-single intent alignment misses out on a bunch of important things due to relying on leaky abstractions) - but they suck at formalizing those phenomena such that you can&#39;t really use them as they are to let powerful optimizers run on these concepts without misgeneralisation. As such, my overall aspiration for this type of work is to both do careful scholarship that is able to appreciate and capture the &quot;thick&quot; notions of these phenomena, but then be more ambitious in pushing for formalization.</li></ul></li><li> A different generator that jams well with this is &quot; <a href="https://www.lesswrong.com/tag/boundaries-membranes-technical">boundary</a> &quot;-based approaches to formalizing safe AI interactions.</li><li> That said, as I mention briefly in the sequence too, I think this is definitely not the end of the story.<ul><li> There is a range of edge cases/real world examples I want to hold my current notion of legitimacy against to see how it breaks. Parenting &amp; education seems like particularly interesting examples. For strong versions of my legitimacy criteria, ~most parenting methods would count as inducing illegitimate value change. I think there is an important argument from common sense that this appears to strong of a result. In other words, a sensible desideratum to have for criteria of legitimacy is that there must be some realistic approaches to parenting that would count as legitimate.</li><li> Furthermore, autonomy/self-determination/freedom and their likes rely on some notions of individuality and agent-boundaries that are somewhat fuzzy. We don&#39;t want to go so far as to start pretending agents are or could ever be completely separate and un-influenced by their environment -- agent boundaries are necessarily leaky. At the same time, there are also strong reasons to consider not all forms of leakiness/influence taking to be morally the same. A bunch of work to be done to clarify where &quot;the line&quot; (which might not be a line) is.<ul><li> This is also in part why I think discussion about the value change problem must consider not only the problem of causing illegitimate value change, but also the problem of undermining legitimate value change (while I think naturally more attention is paid to the former only). In other words, I want a notion of legitimacy that is useful in talking about both of these risks.</li></ul></li></ul></li></ul></li><li> I think a bunch of people have the intuitions that at least part of what makes a value change (il)legitimate has to be evaluated with reference to the object level values adopted. I remain so far skeptical of that (although I recognize it&#39;s a somewhat radical position). The main reason for this is that I think there is just no &quot;neutral ground&quot; at the limits on which to evaluate . So while pragmatically we might be <i>forced</i> to adopt notions of legitimacy that also refer to object level beliefs (and that this might be better than the practically available alternatives), I simultaneously think this is conceptually very dissatisfying and am skeptical that such an approach with be principled enough to solve the ambitious version of the alignment problem (ie generalize well to sufficiently powerful AI systems).<ul><li> FWIW this is, according to me, related to why political philosophy and philosophy of science can provide relevant insights into this sort of question. Both of these domains are fundamentally concerned with (according to me) <i>processes</i> that reliably error correct more or less no-matter where you start from/while aspiring to be agnostic about your (initial) object level. Also, they don&#39;t fall pray to the &quot;leaky abstraction of the individual&quot; (as much) as I alluded to before. (In contrast, the weakness/pitfalls of their &quot;counterparts&quot; moral philosophy and epistemology is often that they are concerned with particulars without being able to not fall pray to status quo biases (where status quo biases are subject to exploitation by power dynamics which also causes decision theoretic problems).)</li></ul></li></ul></li><li> Relationship to concepts like rationality, agency, etc.?<ul><li> This is a very open-ended question so I will just give a short and spicy take: I think that the &quot;future textbook on the foundations of agency&quot; will also address normative/value-related questions (albeit in a naturalized way). In other words, if we had a complete/appropriate understanding of &quot;what is an agent&quot;, this would imply we also had an appropriate understanding of eg the functional logic of value change. And my guess is that &quot;rationality&quot; is part of what is involved in bridging between the descriptive to the prescriptive, and back again. </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 14 Nov 2023 12:10:50 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 14 Nov 2023 12:10:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p>谢谢！ I think it&#39;s a good summary/closing statement/list of future directions for investigation, so I would suggest wrapping it right there, as we&#39;ve been talking for quite a while. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Tue, 14 Nov 2023 16:27:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Tue, 14 Nov 2023 16:27:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Sounds good, yes! Thanks for engaging :)</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings<guid ispermalink="false"> mbpMvuaLv4qNEWyG6</guid><dc:creator><![CDATA[Mateusz Bagiński]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:00:48 GMT</pubDate></item></channel></rss>