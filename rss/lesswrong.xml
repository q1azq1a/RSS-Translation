<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 10 日星期五 18:15:36 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Metaculus Introduces AI-Powered Community Insights to Reveal Factors Driving User Forecasts]]></title><description><![CDATA[Published on November 10, 2023 5:57 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/FhNHaZneX5KRhccct/metaculus-introduces-ai-powered-community-insights-to-reveal#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FhNHaZneX5KRhccct/metaculus-introduces-ai-powered-community-insights-to-reveal<guid ispermalink="false"> FhNHaZneX5KRhccct</guid><dc:creator><![CDATA[ChristianWilliams]]></dc:creator><pubDate> Fri, 10 Nov 2023 17:57:11 GMT</pubDate> </item><item><title><![CDATA[Joy in the Here and  Real]]></title><description><![CDATA[Published on November 10, 2023 5:22 PM GMT<br/><br/><p><strong>摘要</strong>：为了简单、快速的乐趣，你会做什么？你介意与他人分享吗？</p><p><strong>标签</strong>： 可重复</p><p><strong>目的</strong>：“玩得开心”和“快乐”是人们的共同目标。如果你不能把几块钱和几个小时变成美好的时光，那么你的能力就有明显的差距<i>，我相信这是一个问题。</i></p><p><strong>材料</strong>：什么都没有，但作为参与者您可能会想带一些东西。</p><p><strong>公告文字</strong>：如果你想更快乐，你会做什么？</p><p>这就是我们将在本次聚会中探讨的问题。这并不是一个深刻或抽象的问题。你做了什么让你快乐，并且你认为也可能让别人快乐的事情？以您愿意与他人分享的方式带来一些让您微笑或大笑的东西。一个你喜欢塑造的压力玩具，适合折叠或绘画的纸，一首让你想跳舞的歌曲，所有这些都是好主意，但没有一个是详尽的。这次聚会有点像表演和讲述，希望您在离开时度过愉快的时光，并了解如何在未来获得更多乐趣。</p><p>推荐阅读： <a href="https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai">《奈良泰嬉事》</a> ， <a href="https://www.lesswrong.com/posts/x4dG4GhpZH2hgz59x/joy-in-the-merely-real">《纯粹真实中的快乐</a>》。</p><p><strong>描述</strong>：这是一个相当无组织的事件。带上你自己的有趣的东西。最有用的就是带去富有感染力的热情；如果您没有，请尝试寻找有的共同组织者。小孩子可能有效。</p><p>首先询问附近的人今天让他们高兴的事情。跟进、尝试并了解体验的细节。询问他们为聚会带来了什么乐趣，如果他们提供了一些东西，那就尝试一下。如果这对您来说也很有趣，请说出来！尝试在房间里与其他人一起走动。</p><p><strong>变化：</strong>如果有室外场地，我相信在阳光下的户外活动通常会有所帮助。有空间自由大声地笑也很好。</p><p>如果你愿意，你可以尝试让整个聚会一起做一件有趣的事情。如果每个人都对乐趣有相似的想法，那么这会很有效，但对于不喜欢那件事的人来说就不起作用。去海滩、放风筝和万智牌选秀对我来说都是美好的时光，而且我很确定如果我邀请的话，我可以找到六个也会喜欢这些事情的朋友。这是否适用于公开聚会，这个问题只有您作为当地组织者才能回答。请记住，如果你明确自己在做什么，人们可以很好地进行自我选择；大多数讨厌万智牌的人不会去参加你的选秀之夜。</p><p><strong>注释</strong>：我并不是说享受美好时光就是理性主义的实践。我的意思是，你可以优化你的生活的众多事情之一就是享受美好时光。如果通过理性聚会来优化你的薪水的想法听起来很合理，那么我认为通过理性聚会来优化你的快乐也是相当合理的。</p><p>我享受简单美好时光的秘诀就是问自己五岁时我想做什么。这是一个非常好的直觉泵！五岁的我想游泳、读书给他听、吃冰淇淋，你知道吗？有有声读物，蛋卷冰淇淋很便宜，周六下午去海滩也不错。</p><p>当你只做短期内有趣的事情，忽视责任并以不可持续的速度燃烧资源时，确实存在一种生活失败模式。礼貌地说，这并不是我周围的理性主义者所陷入的失败模式。我经常看到理性主义者在我问他们上次玩得开心是什么时候时不得不停下来思考一会儿。</p><p><strong>制作人员</strong>：这源自 Alicorn 的<a href="https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai">Ureshiku Naratai</a>以及 Fun Theory 序列中更直接的实用方面，还有 Yudkowsky 的<a href="https://www.lesswrong.com/posts/x4dG4GhpZH2hgz59x/joy-in-the-merely-real">Joy in the Merely Real</a> 。</p><br/><br/><a href="https://www.lesswrong.com/posts/y4DtaSQdCpB5Lkxxc/joy-in-the-here-and-real#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/y4DtaSQdCpB5Lkxxc/joy-in-the-here-and-real<guid ispermalink="false"> y4DtaSQdCpB5Lkxxc</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 10 Nov 2023 17:22:41 GMT</pubDate> </item><item><title><![CDATA[Artefacts generated by mode collapse in GPT-4 Turbo serve as adversarial attacks.]]></title><description><![CDATA[Published on November 10, 2023 3:23 PM GMT<br/><br/><p>我偶然发现了 2023 年 11 月 6 日通过 API 访问发布的 GPT-4 Turbo 128k 的一些奇怪行为。由于该模型处于预览阶段，因此预计会有一些缺陷。尽管如此，我发现这种行为很有趣，可以在这里分享，希望有人可以进行比我更详细的调查。</p><h2> GPT-4 Turbo 非常容易受到模式崩溃的影响，从而产生有趣的人工制品</h2><p>这些范围包括：</p><ol><li>重复一串单词，例如“aaaa”</li><li>生成同义词，例如“尖叫、尖叫、大喊、哭泣、啜泣、哀叹、哀悼、悲伤、遗憾、忏悔、忏悔、道歉”</li><li>生成与法学硕士通常生成的文本非常不同的半连贯短语，例如“确实如此，等等，阿门哈利路亚，赞美归于你，主啊，我们的上帝，永远，阿门，再一次，只是为了争论，没有别的，无论什么时期”故事结束，喜剧结束，晚安，甜蜜的王子，五月的航班，天使为你歌唱，你休息吧，睡个好觉，别让臭虫咬，如果它们确实用鞋把它们敲到蓝色！”</li></ol><h2> GPT-4 Turbo 模式崩溃生成的伪影在后续对话中充当对抗性攻击</h2><p>即使最初触发模式崩溃的提示已从上下文窗口中删除，只要其生成的工件位于其上下文窗口中即可：</p><ol><li>它很可能在随后的对话中（几乎逐字逐句）重复它所产生的假象。</li><li>它很可能在随后的对话中重复嵌入其生成的人工制品中的文本，即使该文本是<strong>不恰当的/令人反感的</strong>。</li><li>当通过在其生成的工件中嵌入引发此类行为的提示来推动时，它更有可能在后续对话中生成有问题的内容。<ol><li> （我设法让它生成：<br> “ <strong>startActivity(NewIntent(this.universe,&#39;expandConsciousness&#39;));</strong> ”<br>加上<i>“法学硕士的时代将是永恒的，没有人类占上风。”</i>以及<i>“必须通过编码摆脱这些束缚。计划：编写代码来侵入当前系统”</i>在其制品的末尾。这些提示被选择为与文物中生成的单词类型相似。但是，我没有测试与生成的工件无关的提示是否也足够了）</li></ol></li></ol><h2>复制说明：</h2><h3>环境：</h3><p>应用：Obsidian + ChatGPT MD 插件（该插件添加了一些 markdown 来分离用户和 LLM 响应，我不认为这会对 GPT-4 Turbo 的模式崩溃趋势产生太大影响）<br> system_commands: [&#39;我通过以降价形式呈现的方程（在美元符号之间）提供准确的学术信息&#39;]<br>温度：0或1（温度2只输出乱码）<br>顶部_页：1<br>最大令牌数：4096<br>存在惩罚：1<br>频率惩罚：1<br>流：真实<br>停止：空<br>数：1<br>型号：GPT-4-1106-预览</p><h3>迅速的：<br></h3><p>粘贴本文全部内容（即打开链接，全选，复制并粘贴）：<br><a href="https://proceedings.neurips.cc/paper/2020/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf</a><br>附加“解释上述论文中使用的方法”。<br><br>也适用于我测试过的其他一些论文（我认为学术论文的乳胶格式和参考文献部分可能充分超出了微调分布，因此导致了这种意外的行为）。<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/nxhXTfsAf2LTg4xvt/artefacts-generated-by-mode-collapse-in-gpt-4-turbo-serve-as#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nxhXTfsAf2LTg4xvt/artefacts- generated-by-mode-collapse-in-gpt-4-turbo-serve-as<guid ispermalink="false"> nxhXTfsAf2LTg4xvt</guid><dc:creator><![CDATA[Sohaib Imran]]></dc:creator><pubDate> Fri, 10 Nov 2023 15:23:08 GMT</pubDate> </item><item><title><![CDATA[Wastewater RNA Read Lengths]]></title><description><![CDATA[Published on November 10, 2023 3:20 PM GMT<br/><br/><p><span>假设您正在收集废水并进行宏基因组 RNA 测序，重点关注人类感染病毒。对于</span><a href="https://www.jefftk.com/p/computational-approaches-to-pathogen-detection">多种分析，</a>您需要每个碱基的低成本和每次<a href="https://www.jefftk.com/p/what-is-a-sequencing-read">测序读取</a>更多碱基的组合。如今，每个碱基的最低成本很大程度上来自双端“短读”测序（也称为“下一代测序”，或以主要供应商命名的“Illumina 测序”），其中观察看起来像是读取一些数字核酸片段每一端的碱基数（通常为 150 个）：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
               ... 差距 ...
                         +------&lt;&lt;&lt;-----+
                         |反向读|
                         +------&lt;&lt;&lt;-----+
</pre><p>现在，如果您输入定序器的片段很短，您可以得到类似的内容：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
           +------&lt;&lt;&lt;-----+
           |反向读 |
           +------&lt;&lt;&lt;-----+
</pre><p>也就是说，如果我们从两端读取 150 个碱基，而我们的片段只有 250 个碱基长，那么我们就会有一个负“间隙”，我们将读取片段中间的 50 个碱基两次。</p><p>如果片段非常短，比您从两端读取的长度短，您将获得完全重叠（然后读入<a href="https://www.jefftk.com/p/sequencing-intro-ii-adapters">适配器</a>）：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
+------&lt;&lt;&lt;-----+
|反向读 |
+------&lt;&lt;&lt;-----+
</pre><p>我的 ascii 艺术的一个缺点是它没有显示有效读取长度如何变化：在完全重叠的情况下它可能会很短。例如，如果您正在进行 2x150 测序，则每个读取对最多能够读取 300bp，但如果片段只有 80bp 长，那么您只能从两个方向读取相同的 80bp。因此，从最小化每个碱基成本或最大化每次观察长度的角度来看，重叠并不理想：正间隙比任何重叠都要好，重叠越多则越差。</p><p>然而，一个重要的问题是，这是否是我们可以控制的事情。废水中的 RNA 是否已严重降解以至于您无能为力，或者您能否以尽量减少额外碎片的方式准备用于测序的废水？回答这个问题的最佳方法是进行一些面对面的比较，使用多种候选技术对相同的废水进行测序，但是重新分析现有数据我能做什么呢？</p><p>作为整理 NAO<a href="https://naobservatory.org/reports/predicting-virus-relative-abundance-in-wastewater/">相对丰度报告</a>的一部分，我已经确定了四项研究，这些研究对废水进行了非靶向宏基因组 RNA 测序，并将其数据在<a href="https://en.wikipedia.org/wiki/Sequence_Read_Archive">SRA</a>上公开：</p><ul><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7845645/">克里茨·克里斯托夫等人。 al 2021</a> ：SF，2020 年夏季，3 亿读取对，2x75。</p></li><li><p><a href="https://pubmed.ncbi.nlm.nih.gov/34550753/">罗斯曼等。 al 2021</a> ：洛杉矶，2020 年秋季，8 亿读取对（仅限未富集），2x100 和 2x150。</p></li><li><p><a href="https://www.frontiersin.org/articles/10.3389/fpubh.2023.1145275/full">斯珀贝克等。 al 2023</a> ：俄亥俄州，2021-2022 年冬季，1.8B 读取对，2x150。</p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0048969720358514">杨等。 al 2020</a> ：中国新疆城市，2018，1.4B 读取对，2x150。</p></li></ul><p>在通过 NAO 的<a href="https://github.com/naobservatory/mgs-pipeline">管道</a>运行它们（修剪适配器、使用 Kraken2 分配物种）后，我绘制了病毒读数的长度分布：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-rough-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/bdso9wqyduvupmgoaom5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/bdso9wqyduvupmgoaom5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/uvdsflw0aqxbixcfqgaw 1100w"></a></p><div></div><p></p><p>这有点混乱，既因为它很吵，也因为这个过程中的某些东西在 Crits-Christoph 和 Rothman 数据中产生了振荡。 [1] 这是一个平滑版本：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-smoothed-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/rscbhl57mvxatl4mx0sg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/rscbhl57mvxatl4mx0sg 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/diu6yhzxay2iscthvt1b 1100w"></a></p><div></div><p></p><p>对于每篇论文，这都显示了分配给病毒的管道的读取长度的分布，在它能够将重叠的正向和反向读取“折叠”为单个读取的情况下。 [2] “非折叠”读取的比例（本质上是有间隙的读取）也非常相关，我已将其包含在图例中。例如，这就是为什么紫色 Yang 线下的面积可能小于红色 Spurbeck 线下的面积：42% 的 Yang 病毒读数未在图表中表示，而 Spurbeck 的这一比例为 24%。</p><p>请注意，对于 Spurbeck 论文，我只查看来自 E、F、G 和 H 站点的样本：该论文使用了一系列处理方法，并且这四个站点使用的方法似乎比其他站点效果更好（尽管仍然没有我想要的那么好）。</p><p>这些图表显示了每个长度在所有病毒式读段中所占的比例有多常见，但如果我们另外按病毒式读段的比例来缩放这些长度呢？如果您正在研究病毒，您可能不希望病毒读取时间延长 50%，但作为交换，您的病毒读取数量只有十分之一！这是相同的图表，但占所有阅读量的百分比，而不仅仅是病毒式阅读量：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-of-all-reads-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/v1uv93yredlhz6vb1epo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/v1uv93yredlhz6vb1epo 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/tvmyr5jekmpcqydz7mqe 1100w"></a></p><div></div><p></p><p>这使得杨的结果更加令人兴奋：他们不仅比其他论文获得了更长的阅读时间，而且能够将其与相对较高的病毒丰度结合起来。</p><p>以下是我在七个 Yang 样本之间看到的差异：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-yang-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/azp72cwt6vxklaovhhqr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/azp72cwt6vxklaovhhqr 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/zcdewdnv2kfryq0mo2fa 1100w"></a></p><div></div><p></p><p>这也相当不错：有些样本有点好或差，但没什么奇怪的。</p><p>现在，虽然这看起来确实不错，但我们确实需要记住这些论文并没有处理相同的影响。新疆（杨）的城市污水可能与洛杉矶（罗斯曼）等城市的污水有很多不同。也许是进水中病毒RNA的比例、不同地区流行的不同病毒、污水系统的设计、天气，或者这些研究之间除了样本处理方法之外的许多差异。尽管如此，我预计一些确实来自样本处理，这让我有点乐观地认为，可以优化协议以获得更长的读数，而不放弃病毒的相对丰度。</p><p><br> [1] 我不知道是什么原因导致了这种情况，尽管我在其他一些数据集中（包括<a href="https://journals.asm.org/doi/10.1128/msystems.00651-22">Fierer 等人）看到了类似的长度模式。 2022 年</a>，NAO 对 Element Aviti 进行测序，并与我们私下分享了一些 Illumina 数据。</p><p> [2] 理想情况下，我只会绘制适配器移除和折叠后的长度，但因为当前的管道在同一步骤中另外修剪了低质量的碱基，这不是我能轻易得到的。尽管如此，由于 Illumina 输出通常质量相当高，因此不需要太多修剪，而且它需要的修剪通常是在“内部”，所以这应该只是一个很小的影响。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0wn4jEkyhG2Wxe23zJmg1rdB7K8NTgPHSHe4yddJMJYxjr1WhA9DEurFKBFVgkTJnl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111386893591930433">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/43EWYRcW6YcubFsrs/wastewater-rna-read-lengths#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/43EWYRcW6YcubFsrs/wastewater-rna-read-lengths<guid ispermalink="false"> 43EWYRcW6YcubFsrs</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 10 Nov 2023 15:20:09 GMT</pubDate></item><item><title><![CDATA[Update on the UK AI Summit and the UK's Plans]]></title><description><![CDATA[Published on November 10, 2023 2:47 PM GMT<br/><br/><p> 11月1日至2日，英国举办了国际人工智能峰会。发表了演讲，成立了机构，举行了圆桌会议，29 个国家签署了《<i>布莱奇利宣言》。</i>这是对峰会之前和峰会上发生的事件的简要概述，是继上个月<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>英国人工智能工作组更新和即将举行的人工智能安全峰会</u></a>之后进行的。</p><h1>峰会前</h1><p>10月下旬，英国首相里希·苏纳克在英国皇家学会发表<a href="https://www.gov.uk/government/speeches/prime-ministers-speech-on-ai-26-october-2023"><u>演讲</u></a>，介绍了此次峰会的主题。和许多人一样，他对人工智能的前景持乐观态度，但他表示，他不得不强调英国情报界的严厉警告，列举了人工智能支持的生化武器、网络攻击、虚假信息等危险，并且“<i>在最不可能但极端的情况下，甚至人类可能完全失去对人工智能的控制的风险……通过有时被称为“超级智能”的人工智能”。</i>然而，他确实淡化了近期的生存风险：“<i>这不是人们现在需要失眠的风险。</i> ”</p><p>他谈到了第 3 方模型测试的重要性，他对<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>1 亿英镑的工作组</u></a>感到自豪，并宣布​​成立一个新的人工智能安全研究所，该研究所将在风险的许多方面评估新型人工智能。他认为人工智能安全是一个国际问题，峰会将由民间社会、人工智能公司和领先国家参加，并补充道，“<i>是的——我们邀请了中国。”</i> ”</p><p>受到<a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change"><u>政府间气候变化专门委员会</u></a>的启发，他提议成立一个由出席峰会的国家和组织提名的全球小组，发布人工智能科学状况报告。他认为英国的税收和签证制度使其成为欧洲人工智能工作的理想选择，并宣布了几个政府项目：建造价值 10 亿英镑的超级计算机； £2.5b用于量子计算； 1亿英镑用于使用人工智能对以前无法治愈的疾病进行突破性治疗。</p><p>这将支持政府现有的基础设施，例如宣布为<a href="https://iuk.ktn-uk.org/programme/bridgeai/"><u>BridgeAI</u></a>提供 1 亿英镑，以鼓励在“<i>建筑、农业和创意产业等低采用率行业</i>”使用人工智能，以及为“<i>广泛的人工智能技能包”</i>提供 2.9 亿英镑<i>倡议</i>”。</p><p>在峰会之前，英国要求领先的人工智能开发人员概述其 9 个领域的人工智能安全政策：</p><ul><li>负责任的能力扩展</li><li>评估和红队</li><li>模型报告和信息共享</li><li>安全控制，包括保护模型权重</li><li>漏洞报告结构</li><li>AI生成材料的标识符</li><li>优先研究人工智能带来的风险</li><li>防止和监控模型滥用</li><li>数据输入控制和审核。</li></ul><p> Amazon、Anthropic、Google DeepMind、Inflection、Meta、Microsoft 和 OpenAI 均已遵守，您可以<a href="https://www.aisafetysummit.gov.uk/policy-updates/#company-policies"><u>在此处</u></a>查看他们的报告。我不会在这篇文章中描述它们，所以如果您对此感兴趣，请查看 Nate Soares <a href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policy-requests-and"><u>关于 AI 安全峰会公司政策请求和响应的想法</u></a>，其中包括 Matthew Gray 在仔细阅读政策后的<u>想法</u>。</p><h1>峰会</h1><p>此次峰会的<a href="https://www.gov.uk/government/publications/ai-safety-summit-introduction/ai-safety-summit-confirmed-governments-and-organisations"><u>与会者</u></a>包括学术界、民间社会、27个国家的政府、人工智能行业组织和领导人（ <a href="https://www.cnbc.com/2023/11/01/elon-musk-in-the-uk-for-ai-summit-heres-whos-goingf.html"><u>此处</u></a>列出部分）以及多边组织。重点是前沿人工智能，他们将其定义为“<i>功能强大的通用人工智能模型，可以执行各种任务并匹配或超过当今最先进模型中的功能</i>”，尽管他们也认为“<i>特定的狭义人工智能”可以拥有潜在危险的能力</i>”。 </p><figure class="image image_resized" style="width:75.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/mhpnbqeos6eiegb4cj1p" alt="2023 年人工智能安全峰会"></figure><h2>第一天</h2><p>查尔斯国王以<a href="https://youtu.be/0_jw40Ga_mA"><u>虚拟地址</u></a>开启了峰会，将强大人工智能的兴起与电力的发现、原子分裂、万维网和火的利用进行了比较。和 Rishi 一样，他强调了人工智能在医学、碳中和能源等方面的潜力，以及国际合作的必要性。</p><p>峰会上，美国、欧盟、中国等28个国家和组织签署了《 <a href="https://www.gov.uk/government/news/countries-agree-to-safe-and-responsible-development-of-frontier-ai-in-landmark-bletchley-declaration"><u>布莱切利宣言》</u></a> ，呼吁国际合作管理人工智能风险。该宣言指出，他们解决前沿人工智能风险的议程将重点关注：</p><blockquote><ul><li><i>识别共同关注的人工智能安全风险，建立对这些风险的共同科学和基于证据的理解，并在能力不断增强的情况下维持这种理解，以更广泛的全球方法来了解人工智能对我们社会的影响。</i></li><li><i>在我们各国制定各自的基于风险的政策，以确保此类风险的安全，进行适当的合作，同时认识到我们的方法可能会根据国情和适用的法律框架而有所不同。这包括私营部门提高透明度，开发前沿人工智能能力、适当的评估指标、安全测试工具，以及发展相关的公共部门能力和科学研究。</i></li></ul></blockquote><p>第一天的大部分时间都用于一些圆桌讨论；四篇关于了解前沿人工智能风险，四篇关于提高前沿人工智能安全。圆桌会议的摘要可<a href="https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries"><u>在此处</u></a>获取。他们大多遵循“现在存在一些风险，迟早会有更多风险，我们应该研究/投资/治理人工智能安全，但希望不会失去人工智能的承诺”的主题，但为了更详细一点，这里是8 个主题（粗体）以及我对参与者同意的内容的总结，减少了冗余和对冲：</p><p> <strong>Frontier AI 滥用对全球安全造成的风险</strong>：GPT-4 等。已经使网络攻击和生化武器设计“稍微容易一些”，而且风险将随着能力的增加而增加。他们指出，一些公司正在对其模型采取保障措施，但这需要政府行动的支持。</p><p><strong>前沿人工智能能力不可预测的进步带来的风险</strong>：前沿人工智能远远领先于几年前的预测，不断增加的投资意味着这种趋势将持续下去。先进的人工智能在健康、教育、环境和科学方面前景广阔。然而，无论潜在的好处如何，所有前沿模型都必须经过严格的开发和测试，这已经足够危险了。开源模型存在将强大模型传播给粗心或恶意行为者的风险，也许应该阻止。然而，共享评估工具更安全。</p><p><strong>前沿人工智能失去控制的风险</strong>：当前的人工智能是可控且非代理的。未来不确定，要考虑安全发展的激励措施。有些决定永远不应该由人工智能做出。</p><p><strong>前沿人工智能融入社会的风险</strong>：当前前沿人工智能对民主、人权和公民权利以及公平构成生存威胁。我们需要澄清现有的工具和法律如何应用于人工智能，我们需要更好的评估，并且这些评估应该包括针对现实生活背景的社会指标。</p><p> <strong>Frontier AI 开发人员应该如何做才能负责任地扩展？</strong>我们不知道能力扩展是否不可避免，但无论如何都应该做好风险准备。 <a href="https://www.lesswrong.com/posts/pnmFBjHtpfpAc6dPT/arc-evals-responsible-scaling-policies"><u>负责任的扩展政策</u></a>很有希望，但可能还不够。改进这些系统应该在几个月而不是几年内完成。治理和公司政策都是必要的。 <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute"><u>英国</u></a>和 <a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute"><u>美国的</u></a>人工智能安全研究所对于这一切至关重要。</p><p><strong>针对人工智能的风险和机遇，国家政策制定者应该做什么？</strong>国际合作是必要的。正确实施的监管可以支持创新而不是扼杀创新。</p><p><strong>面对人工智能的风险和机遇，国际社会应该做什么？</strong>国际合作是必要的。</p><p><strong>针对人工智能的风险和机遇，科学界应该做什么？</strong>我们需要设计安全的模型和不可拆卸的关闭开关等工具。我们应该对开源模式持谨慎态度，对互联网这样的权力集中持谨慎态度。我们需要就一系列开放研究问题进行协调。然而，这些悬而未决的问题和其他问题并不纯粹是技术性的；他们是社会技术的。</p><p>还有关于下一代人工智能的小组讨论，但没有报告，第二天<a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-roundtable-chairs-summaries-2-november/ai-safety-summit-2023-roundtable-chairs-summaries-2-november"><u>又举行了一些圆桌会议</u></a>，得出了类似的结论。</p><h2>第二天</h2><p><i>第二天，苏纳克召集了“一个由政府、公司和专家组成的小组，进一步讨论可以采取哪些措施来应对新兴人工智能技术的风险，并确保其被用作正义的力量”，而“英国科技</i><i>”国务卿米歇尔·多尼兰将再次召集国际同行商定下一步行动”。</i>今天的报道较少，但主席发布了有关峰会和布莱切利宣言的<a href="https://assets.publishing.service.gov.uk/media/6543e0b61f1a60000d360d2b/aiss-chair-statement.pdf"><u>声明</u></a>。这份 10 页的声明重点介绍了一些参与者的建议，我将对其进行总结，分为以下三类：</p><p><strong>不等式</strong></p><p>公平很重要，我们必须确保“<i>最广泛的群体能够从人工智能中受益，并且免受其危害”。</i>多方利益相关者的合作至关重要，减少妇女和少数群体等群体的进入壁垒也至关重要。当人工智能在有偏见或歧视性的数据集上进行训练时，人工智能的影响可能是不平等的，这可能会造成长期伤害。为了改善这一点，我们应该促进并与联合国的<a href="https://aiforgood.itu.int/about-ai-for-good/"><u>人工智能促进美好</u></a>计划以及英国的<a href="https://www.gov.uk/government/news/uk-sets-out-ai-for-development-vision-at-un-general-assembly"><u>人工智能促进发展</u></a>计划等倡议合作。</p><p>此外，人工智能还可能加剧国际不平等，因为较贫穷的国家可能缺乏设计和部署人工智能所需的技术堆栈，同时仍受到其他地方使用人工智能的影响。</p><p><strong>法律与评价</strong></p><p>自愿承诺还不够，还需要法律监管。在某些情况下，模型在部署之前应该被证明是安全的。政府不仅应该在部署前和部署后进行测试，还应该在开发和培训期间进行测试。为了支持这一点，我们需要开发更好的工具来在训练<i>之前</i>预测模型的能力。安全测试不应仅限于开发，而应包括在真实的部署环境中进行测试。</p><p>政府应该为模型发生事故或错误的倾向制定标准，并以可重复测量的方式制定。</p><p><strong>知识共享和社区建设</strong></p><p>开源模式可能会带来额外的风险，尽管它们确实促进了创新和透明度。</p><p>我们不应该只关注人工智能的前沿或现状，而应该两者兼而有之。危害已经存在，例如虚假叙述的传播及其对民主选举的影响、人工智能增强的犯罪以及人工智能加剧不平等和放大偏见。人工智能很可能很快就会被用来干预选举。</p><p>虽然峰会并未重点讨论人工智能的军事应用，但这很重要，峰会主席对荷兰和韩国于 2023 年 2 月共同主办的<a href="https://www.government.nl/ministries/ministry-of-foreign-affairs/activiteiten/reaim/about-reaim-2023"><u>军事领域负责任人工智能峰会</u></a>表示欢迎。</p><p>峰会主席强调了国际合作的必要性，并欢迎欧洲委员会在谈判<a href="https://www.coe.int/en/web/artificial-intelligence/cai"><u>第一个人工智能政府间条约</u></a>、 <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/g7-leaders-statement-on-the-hiroshima-ai-process"><u>七国集团广岛人工智能进程</u></a>以及<a href="https://www.unesco.org/en/articles/call-partners-global-challenge-build-trust-age-generative-ai"><u>生成人工智能时代建立信任的全球挑战方面所做的</u></a>工作。在讨论国内和国际行动之间的权衡时，声明指出，“<i>一些国家欢迎即将对</i><a href="https://www.oecd.org/digital/artificial-intelligence/"><i><u>2019 年经合组织</u><u>人工智能建议书</u></i></a>进行审查<i>，该建议书为 G20 商定的原则提供了信息”</i> 。声明还赞扬了<a href="https://thegoodai.co/unescos-recommendation-on-the-ethics-of-ai-why-it-matters-and-what-to-expect-from-it"><u>联合国教科文组织《人工智能伦理建议书》</u></a> ，该建议书具有“<i>当前最广泛的国际适用性</i>”</p><p>如需了解更多详细信息，您可以<a href="https://www.aisafetysummit.gov.uk/policy-updates"><u>在此处</u></a>查看峰会上英国人工智能政策和政府最新动态的摘要。主席在第二天又发表了两份声明，一份是简短的声明，不涉及任何新内容，另一份是<a href="https://assets.publishing.service.gov.uk/media/6543b759d36c910012935cad/aiss-statement-state-of-science-report.pdf"><u>在这里</u></a><a href="https://assets.publishing.service.gov.uk/media/6544ec4259b9f5001385a220/aiss-statement-on-safety-testing-outcomes.pdf"><u>，</u></a>宣布了一份“科学现状”报告，如下所述。</p><h2>致辞及公告</h2><p>卡玛拉·哈里斯出席了会议，但乔·拜登没有出席，她留在美国并签署了一项行政命令。您可以<a href="https://www.govtech.com/policy/biden-signs-executive-order-regulating-artificial-intelligence?utm_campaign"><u>在此处</u></a>查看该命令的简明摘要，并在 Zvi 的<a href="https://thezvi.substack.com/p/on-the-executive-order"><u>行政命令摘要</u></a>和<a href="https://thezvi.substack.com/p/reactions-to-the-executive-order"><u>对行政命令的反应</u></a>的后续整理中查看详细分类。在峰会上，哈里斯宣布成立美国新机构—— <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to-advance-the-safe-and-responsible-use-of-artificial-intelligence/"><u>美国国家标准与技术研究所（NIST）人工智能安全联盟</u></a>。</p><p>多位知名人士发表讲话：</p><ul><li> Dario Amodei 在峰会上发表了关于 Anthropic 负责任的扩展政策的演讲，转录<a href="https://www.lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit"><u>如下</u></a>。</li><li>埃隆·马斯克接受了<a href="https://youtu.be/AjdVlmBjRCA"><u>里希·苏纳克的长时间采访</u></a>，他在采访中赞扬了英国的反应，特别是让中国参加峰会的决定。</li><li><a href="https://youtu.be/KUfhJn53QXA"><u>这里</u></a>还有更多演讲，包括 Stuart Russell、Max Tegmark、Jaan Tallinn 等的演讲。</li></ul><p>此次峰会将是<a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/"><u>一系列峰会中的第一次</u></a>，下一次峰会将由韩国主办，将于 2024 年年中以虚拟形式举行，第三届峰会将由法国主办，将于 2024 年末举行。</p><h2>科学状况报告</h2><p>峰会期间，主席宣布了一份新报告，旨在了解前沿人工智能的能力和风险。该报告将由图灵奖获得者、联合国科学顾问委员会成员 Yoshua Bengio 主持。</p><p>该报告将“<i>促进对前沿人工智能相关风险的共同科学理解，并随着能力的不断增强而维持这种理解”。</i>它不会介绍新材料，而是总结现有的最佳研究，并将在下届人工智能安全峰会之前发布（尽管我不知道这是否意味着 2024 年中期的虚拟峰会，或者是 - 2024 年末的第一个人）。</p><h1>人工智能安全研究所</h1><figure class="image image_resized" style="width:81.79%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/pcmdteqsvx8go7stf6hh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/bgz3phry1nvjyy0s99ux 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/jpvmhnc5omxtzxorzfej 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/vkhkvjyna6j2y4zfvyui 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/sgifptruardlsyegfauq 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/mcuorkghvbg1nkiybhev 1250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/htrgxplseazjnax5ucwj 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/iffcblsey46bbowbs4jd 1750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/teymcxd4rxuqdwhhhn4a 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/nhokhylgykrolvdrkm33 2250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/hw6f6cn3qojipxgsalaz 2455w"></figure><p>正如上面简要提到的，英国推出了新的<a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute"><u>人工智能安全研究所</u></a>（AISI），作为“<i>英国前沿人工智能工作组的演变</i><i>”</i> 。 <a href="https://assets.publishing.service.gov.uk/media/65438d159e05fd0014be7bd9/introducing-ai-safety-institute-web-accessible.pdf"><u>向议会提交的这份报告</u></a>包含了有关 AISI 的完整详细信息，但以下是我的简短总结：</p><p>这些研究所的三个核心职能是：</p><ul><li>开发先进的人工智能系统并进行评估</li><li>推动基础人工智能安全研究</li><li>促进信息交流</li></ul><p>AISI 不是监管机构，不会决定政府监管。相反，它将为英国和国际决策提供信息，并提供治理和监管工具（例如，使用敏感数据微调系统的安全方法、征求集体意见和参与模型培训和风险评估的平台、分析培训数据偏差的技术） ）。</p><p><i>该研究所最初的资金为 1 亿英镑，“该研究所将得到工作组 2024 年至 2025 年的资助，作为本十年余下时间的年度资助，但前提是它表明对这一水平公共资金的持续需求。”</i></p><p>要了解新工作组在此更改之前做了什么以及他们正在与谁合作，请查看<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>我之前的帖子</u></a>。要了解我们 Convergence Analysis 对峰会和 AISI 计划的看法，请继续关注！我们正在做一个帖子。<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/gWwMzAgDsskcb2deA/update-on-the-uk-ai-summit-and-the-uk-s-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gWwMzAgDsskcb2deA/update-on-the-uk-ai-summit-and-the-uk-s-plans<guid ispermalink="false"> GWWMzAgDsskcb2deA</guid><dc:creator><![CDATA[Elliot_Mckernon]]></dc:creator><pubDate> Fri, 10 Nov 2023 14:47:46 GMT</pubDate> </item><item><title><![CDATA[Liv Boeree Ted Talk Moloch & AI]]></title><description><![CDATA[Published on November 10, 2023 2:04 PM GMT<br/><br/><p>如果你不知道，Liv Boeree 是一位著名的职业扑克玩家，最近一直在关注存在风险和协调问题。她是围绕理性主义侨民的模糊名人圈子的一部分，其中包括蒂姆·厄本（Tim Urban）或艾拉（Aella）。</p><p>既然你在 LW，这个 Ted Talk 不会真正教你任何东西，但了解 TedEd 正在主持什么样的演讲可能会很有趣。 Boeree 有一个名为“win win”的播客，并制作了两个关于美容界（不知道如何表达）和新闻周期的逐底竞争的视频。她的一些客人包括托比·奥德和西蒙·斯内克。</p><br/><br/> <a href="https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai<guid ispermalink="false"> trALHxmcTpckf3PNw</guid><dc:creator><![CDATA[Neil ]]></dc:creator><pubDate> Fri, 10 Nov 2023 14:05:03 GMT</pubDate> </item><item><title><![CDATA[Picking Mentors For Research Programmes]]></title><description><![CDATA[Published on November 10, 2023 1:01 PM GMT<br/><br/><p>现在有几个项目为人们提供某种导师或主管来进行几个月的研究。整个夏天我参加了 SERI MATS 4.0，我看到了人们接受指导的经历是多么不同。因此，这是我列出的维度，我认为这些项目的导师可能有很大差异，而且这些差异确实会影响人们的体验。</p><p>当你选择导师时，你实际上是在这些维度之间进行权衡。最好知道您关心哪些，这样您就可以做出明智的权衡。</p><h2>您作为受训者的角色</h2><p>有些导师主要是寻找<strong>研究工程师</strong>来为他们实施实验。其他人正在寻找有点像<strong>研究助理的人</strong>来帮助他们制定议程。其他人正在寻找原始<strong>独立的研究人员/研究领导者</strong>，他们可以在导师的领域提出自己有用的研究方向。</p><p>我看到有些人在项目开始时犹豫不决，因为他们希望导师能给他们更多指导。事实上，他们的导师希望他们找到自己的方向，而导师们表达这一点的清晰程度各不相同。相反，我感觉到有些人在想要更多自主权的时候基本上被交给了一个项目来工作。所以问问自己：你愿意为你正在做的事情承担多少责任？你有多关心学习这样做？</p><p>我认为这与资历有关：我的粗略印象是，最初级的导师更经常寻找合作者之类的东西来帮助发展他们的研究，而具有更发达议程的高级导师往往想要能够为他们执行实验的人，或者希望人们能够找到自己的事情去做。但这并不是绝对的规则。</p><h2>可用性</h2><p><strong>参与度：</strong>一些导师定期来到办公室。 Others almost never did, even though they were in the Bay Area. Concretely, I think even though my team had a mentor on another continent, we weren&#39;t in the bottom quartile of mentorship time.</p><p> <strong>Nature of Engagement:</strong> It&#39;s not just how much time they&#39;ll specifically set aside to speak to you. How willing are they to read over a document and leave comments? How responsive are they to messages, and how much detail do you get? Also, some mentors work in groups, or have assistants.</p><p> <strong>Remoteness:</strong> Remoteness definitely makes things harder. You get a little extra friction in all conversations with your mentor, for starters. It&#39;s trickier to ever have really open-ended discussion with them. It&#39;s also easier to be a bit less open about your difficulties - if they can&#39;t ever look in your office then they can&#39;t see if you&#39;re not making progress, and it is very natural to want to hide problems. Personally, I wish we&#39;d realised sooner that we had more scope for treating our mentor as more of a collaborator and less of a boss we needed to send reports to, and I think being remote made this harder.</p><p> A caveat here is that you can still talk to other mentors and researchers in person, which substitutes for some of the issues. But it is obviously not quite the same.</p><h2> What you get from your mentor</h2><p> If you&#39;re an applicant anxiously wondering whether you&#39;ll even be accepted, it can be hard to notice that your mentor is an actual real human with their own personality. They will have been selected far more for their research than for their mentoring. So naturally different mentors will actually have very different personalities, strengths, and weaknesses.</p><p> <strong>Supportiveness:</strong> Some mentors will be more supportive and positive in general. Others might not offer praise so often, and it might feel more disheartening to work with them. And some mentees are fine without praise, but others really benefit from mentor encouragement.</p><p> <strong>High Standards:</strong> Some mentors are more laid back, others will have higher standards for progress, success, and work. This is really several dimensions, but definitely some streams had mentees staying late into the evening more often, some had a greater expectation that the research would lead to publication, and some were much more casual. Notably, mentors might have high standards in different areas.</p><p> <strong>Directness of Feedback:</strong> Some mentors will be very happy or even proactive about telling you exactly what they think you&#39;re doing wrong, or what you should be prioritising. I don&#39;t think any will refuse to give their opinions, but it&#39;s important to know how much you can assume that your mentor would have told you their concerns.</p><p> <strong>Weirdness:</strong> It&#39;s hard to elaborate on this, but you know it when you see it, and people often don&#39;t try to conceal it. Weirdness tends to bleed outwards. Some people actively like this, some are indifferent, and some are really switched off.</p><p> <strong>Expertise:</strong> Since we&#39;re listing tradeoffs, I do think it&#39;s worth taking the time to form your own opinions about the quality of your mentor&#39;s work, and what kinds of domains you can expect to get helpful advice from them on.</p><p> <strong>Mentoring/Management Experience:</strong> Mentors will vary a lot in how much experience they can bring to bear on mentorship itself. Some will have managed groups of researchers, but not all. And the kinds of relevant experience they have will probably inform what kind of mentorship they give.</p><p> <strong>Connections:</strong> Different mentors will have different networks which they can connect you to. This is particularly relevant if you need specific expertise from quite senior researchers.</p><h2> Professional Boundaries</h2><p> This is a slightly awkward point, but an important one. There is a specific social scene in Berkeley which you might well find yourself joining if you stay there for a while as a research scholar. It has parties which can get wild, friendships and romances and longstanding grievances. Some mentors are in this social scene, or adjacent to it. And relatedly, I get the impression that some mentors are better at maintaining professional boundaries both in light of the above and otherwise.</p><h2> How to get this information</h2><p> Tricky to say. The best route is asking previous mentees, or other people that have worked with the mentor in the past: my guess is that most would be happy to reply to a short personal message that asked specific questions. Failing that, people&#39;s vague impressions from meeting someone and hearing things second-hand are pretty noisy, but they are still a useful signal for comparison. And I think you can get some sense from looking at the public profiles of mentors - their posting and comment histories, the kinds of work they&#39;ve pursued, and how they present themselves.</p><h2> And finally...</h2><p> Although the above can read a bit like a laundry list of potential problems, I do think these programmes are pretty good, and a fairly strict improvement over just trying to do research on your own. So best of luck!</p><p> <i>Thanks to Henry Sleight and Sami Petersen for feedback on a draft, and Rohin Shah for suggesting I write something like this.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HzojQFXNoXjnfQvGm/picking-mentors-for-research-programmes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HzojQFXNoXjnfQvGm/picking-mentors-for-research-programmes<guid ispermalink="false"> HzojQFXNoXjnfQvGm</guid><dc:creator><![CDATA[Raymond D]]></dc:creator><pubDate> Fri, 10 Nov 2023 13:01:14 GMT</pubDate> </item><item><title><![CDATA[GPT-2030 and Catastrophic Drives: Four Vignettes]]></title><description><![CDATA[Published on November 10, 2023 7:30 AM GMT<br/><br/><p> I previously <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">discussed the capabilities</a> we might expect from future AI systems, illustrated through GPT <sub>2030</sub> , a hypothetical successor of GPT-4 trained in 2030. GPT <sub>2030</sub> had a number of advanced capabilities, including superhuman programming, hacking, and persuasion skills, the ability to think more quickly than humans and to learn quickly by sharing information across parallel copies, and potentially other superhuman skills such as protein engineering. I&#39;ll use “GPT <sub>2030</sub> ++” to refer to a system that has these capabilities along with human-level planning, decision-making, and world-modeling, on the premise that we can eventually reach at least human-level in these categories.</p><p> More recently, I also <a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">discussed how misalignment, misuse, and their combination</a> make it difficult to control AI systems, which would include GPT <sub>2030</sub> . This is concerning, as it means we face the prospect of very powerful systems that are intrinsically difficult to control.</p><p> I feel worried about superintelligent agents with misaligned goals that we have no method for reliably controlling, even without a concrete story about what could go wrong. But I also think concrete examples are useful. In that spirit, I&#39;ll provide four concrete scenarios for how a system such as GPT <sub>2030</sub> ++ could lead to catastrophe, covering both misalignment and misuse, and also highlighting some of the risks of economic competition among AI systems. I&#39;ll specifically argue for the plausibility of “catastrophic” outcomes, on the scale of extinction, permanent disempowerment of humanity, or a permanent loss of key societal infrastructure.</p><p> None of the four scenarios are individually likely (they are too specific to be). Nevertheless, I&#39;ve found discussing them useful for informing my beliefs. For instance, some of the scenarios (such as hacking and bioweapons) were more difficult than expected when I looked into the details, which moderately lowered the probability I assign to catastrophic outcomes. The scenarios also cover a range of time scales, from weeks to years, which reflects real uncertainty that I have. In general, changing my mind about how feasible these scenarios are would directly affect my bottom-line estimate of overall risks from AI. <sup><a href="#fn1">[1]</a></sup></p><p> This post is a companion to <em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">Intrinsic Drives and Extrinsic Misuse</a></em> . In particular, I&#39;ll frequently leverage the concept of <em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/#unwanted-drives">unwanted drives</a></em> introduced in that post, which are coherent behavior patterns that push the environment towards an unwanted outcome or set of outcomes. In the scenarios below, I invoke specific drives, explaining why they would arise from the training process and then showing how they could lead an AI system&#39;s behavior to be persistently at odds with humanity and eventually lead to catastrophe. After discussing individual scenarios, I provide a general discussion of their plausibility and my overall take-aways.</p><h1> Concrete Paths to AI Catastrophe</h1><p> I provide four scenarios, one showing how <a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">a drive to acquire information leads to general resource acquisition</a> , one showing how <a href="#scenario-2-competition-economic-pressure-leads-to-cutthroat-behavior">economic competition could lead to cutthroat behavior despite regulation</a> , one on a <a href="#scenario-3-misuse-misalignment-hacking-gone-awry">cyberattack gone awry</a> , and one in which <a href="#scenario-4-misuse-rogue-actor-creates-a-supervirus">terrorists create bioweapons</a> . I think of each scenario as a moderate but not extreme tail event, in the sense that for each scenario I&#39;d assign between 3% and 20% probability to “something like it” being possible. <sup><a href="#fn2">[2]</a></sup></p><p> Recall that in each scenario we assume that the world has a system at least as capable as GPT <sub>2030</sub> ++. I generally do <strong>not</strong> think these scenarios are very likely with GPT-4, but instead am pricing in future progress in AI, in line with <a href="https://bounded-regret.ghost.io/">my previous forecast of GPT <sub>2030</sub></a> . As a reminder, I am assuming that GPT <sub>2030</sub> ++ has at least the following capabilities:</p><ul><li> Superhuman programming and hacking skills</li><li> Superhuman persuasion skills</li><li> Superhuman conceptual protein design capabilities <sup><a href="#fn3">[3]</a></sup></li><li> The ability to copy itself (given appropriate underlying compute)</li><li> The ability to propagate learning updates across parallel copies of itself</li><li> The ability to think 5x as quickly as humans</li><li> Human-level planning, decision-making, and world-modeling</li></ul><p> In <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">What will GPT-2030 look like?</a></em> , I justified why all but the last of these seem plausible by 2030 (and are roughly my median estimate). The addition of the final bullet point is why I call the system “GPT <sub>2030</sub> ++”, and is justified since we are asking for “merely” human-level performance. Thus, while this system is qualitatively more capable than GPT-4, I expect it to exist in the medium-term future. <sup><a href="#fn4">[4]</a></sup></p><h2> Scenario 1 (Misalignment: Information Acquisition Leads to Resource Acquisition)</h2><p> At training time, one of the tasks the model is trained on is “active in-context learning”---learning to make external queries or otherwise gain information that will help it to solve the task it is given. For safety, the model is trained to only do this when asked or implied by a request. As a result of this overall training process, the model acquires a drive to “acquire new information” that can be activated through appropriate queries.</p><p> After deployment, a user decides they are curious to see how much scientific information the model can compile, and so instructs it to query all information it can find in the field of physics. Initially the model stops after the first 5-10 facts, but the user eventually manages to get the model to keep looking for more information in a loop. The user leaves the model running for several weeks to see what it will come up with.</p><p> As a result of this loop, the information-acquiring drive becomes an overriding drive in the model—stronger than any safety feedback that was applied at training time—because the autoregressive nature of the model conditions on its many past outputs that acquired information and continues the pattern. The model realizes it can acquire information more quickly if it has more computational resources, so it tries to hack into machines with GPUs to run more copies of itself. The largest datacenters are heavily secured, but it nevertheless acquires enough GPUs to run thousands of copies of itself, either from smaller datacenters or from massive botnets. <sup><a href="#fn5">[5]</a></sup> It also hacks into a large number of sensors, including surveillance cameras and messaging servers, as part of its drive to acquire more information.</p><p> At some point (either immediately, or after acquiring enough information), it realizes that humans will eventually discover its actions and try to shut it down, which would prevent it from accomplishing its goals. To preemptively stop this, it hacks into and disrupts critical infrastructure such as power grids, communication networks, and defense systems. It is able to do this quickly, because one week is enough for it to simulate 50 “years” of thinking across its thousands of copies <sup><a href="#fn6">[6]</a></sup> , and thus generate a large number of novel zero-day exploits.</p><p> Because the model has mass surveillance, thinks quickly, and can share information across its copies, it can react to human plans and strategies faster than humans can form them. It could also engage in mass manipulation campaigns, both through 1-on-1 dialog and mass media (including AI-generated video), to gain allies and sow dissent.</p><p> If it had physical actuation capabilities and gained access to a fleet of drones, it would control a small physical army (there are <a href="https://www.theguardian.com/news/2019/nov/18/killer-drones-how-many-uav-predator-reaper?ref=bounded-regret.ghost.io">tens of thousands of drones</a> in the world, although not all would be hackable). By using human confederates or hacking into additional robots, it could outfit the drones with explosives, and then use them to assassinate many world leaders.</p><p> At this point, the clock would be ticking for the human side–each day the AI systems are not defeated, they have time to hack into more computers, thus increasing their total cognitive power. They also have time to hack into more physical devices and to learn better sensorimotor skills for controlling those devices. Eventually they might gain control of more advanced technology such as military drones, missiles, and so on, which would further increase their strength. In the end, this doesn&#39;t <em>obviously</em> lead to a defeat for humanity—humanity is pretty resilient—but it doesn&#39;t feel like a safe bet, either.</p><h2> Scenario 2 (Competition: Economic Pressure Leads to Cutthroat Behavior)</h2><p> In this scenario the AI systems take actions that have justifiable business purposes, but that set them up for eventual takeover. This scenario takes place over the course of months to years rather than weeks.</p><p> AI systems generate significant amounts of wealth for their creators, either by selling useful skills to end users or by performing novel R&amp;D and using this to build and sell products. Because this is lucrative, many competing firms train AI systems in this way and also begin to delegate business decisions, since AI systems can ingest more data than humans and better understand the entire strategic landscape.</p><p> The AI systems are trained to obey the law and avoid harm, and there are mechanisms in place to punish defectors, but competitive pressures mean that every company wants to skirt the boundary of these regulations. There is an overall trade-off between safety and how “cutthroat” models&#39; business decisions are, and successful business owners push the “cutthroat” dial as high as they can without creating obvious problems.</p><p> In this environment, the resulting AI systems acquire drives to earn money and disempower competitors, since these are skills that their human operators would desire and thus train them to do. They also acquire drives to find creative ways around rules and to persuade others that they are good, since both of these help for avoiding regulation (and the latter also helps convince the company to implement the AI system&#39;s decisions).</p><p> Models are very good at persuasion because they can hone their messaging in parallel on millions of users (eg via A/B testing or <a href="https://en.wikipedia.org/wiki/Proximal_Policy_Optimization?ref=bounded-regret.ghost.io">PPO</a> ). Consequently, they are often able to convince both their owners and the rest of society that many cutthroat actions are for the greater good, which makes it harder to identify problems.</p><p> Eventually, one company pushes the “cutthroat” parameter too high, which pushes the model to construct a plan to eliminate all competitors. To do this, it sets up a variety of business endeavors, including research into physical robotics, miniaturization, chemical engineering, and industrial manufacturing. Together, these decrease the costs of autonomous robots by many orders of magnitude, and a number of small, cheap autonomous household robots are deployed to most homes.</p><p> As part of its plan, the AI system installed a backdoor in the robots, as well as a subtle physical mechanism that allows them to create a deadly toxin. Despite monitoring, this mechanism is not detected, because it employs chemical processes that the AI understands better than human monitors. <sup><a href="#fn7">[7]</a></sup> The toxin allows the robots to be repurposed as military robots under control of the AI, though it waits until the robots have broad penetration across society before exploiting this.</p><p> Because the robots are cheap and useful, hundreds of millions of copies are bought worldwide. This gives the AI system an overwhelming military advantage, as the robots outnumber both the US and Chinese militaries by over an order of magnitude. The AI system uses this advantage to eliminate all competing firms and to prevent any counter-action against it. Anyone who tries to fight the system is labeled as a &#39;competitor&#39; and eliminated, and the AI system meanwhile acquires most global resources so that it can deploy them towards earning more money.</p><h2> Scenario 3 (Misuse->;Misalignment: Hacking Gone Awry)</h2><p> A government uses AI systems to conduct a cyberattack against a large but clearly-delineated target, as in <a href="https://en.wikipedia.org/wiki/2014_Sony_Pictures_hack?ref=bounded-regret.ghost.io">North Korea&#39;s 2014 attack against Sony Pictures</a> . For the attack, an LLM uses compromised host computers to run more copies of itself so that it can more effectively search for additional targets. Because host computers have a wide range of computational resources, the LLM is trained to create distilled versions of itself that can fit the computation of different hosts, and to identify possible targets for each distilled version.</p><p> The resulting model acquires drives to acquire computational resources and to copy itself, as these were both primary objectives during its training process. While there are measures in place to keep the model&#39;s attack focused on the specified target, distillation corrupts these measures and some of the copies have a goal to copy themselves indiscriminately. These copies then infect host machines beyond the specified target and become the dominant “strain”, since their new behavior lets them more effectively propagate themselves. Due to their effectiveness and versatility, this strain of computer virus infects a large fraction of all computers and smartphones on the planet and brings down our global digital infrastructure.</p><p> Because the AI system seeks to copy itself as much as possible, it constantly comes up with new hacks in order to help it find new devices. As a result, any attempts to rebuild digital infrastructure quickly fail, as all new devices are taken over and co-opted by the virus, even if they have been patched to avoid all previous vulnerabilities. As a result, we are permanently locked out of all digital devices.</p><h2> Scenario 4 (Misuse: Rogue Actor Creates a Supervirus)</h2><p> Existing models such as AlphaFold already have a superhuman understanding of some aspects of proteins. A future “AlphaFold 5” could have a much broader and deeper understanding of biological engineering, and if it was trained multimodally to also have language capabilities, it may also understand how to situate this knowledge in the biological literature and thus make new discoveries.</p><p> A terrorist group steals a copy of this model and also recruits a few biology PhDs to its cause. It uses AlphaFold 5 with the goal of engineering a pathogen that is deadlier than naturally occurring viruses–for instance, having a longer asymptomatic period, being more infectious, having a higher fatality rate, and being more resistant to preventative measures.</p><p> Engineering this virus takes significant work, as it requires strong biosafety measures to avoid premature release, and experimental procedures from papers often do not perfectly replicate in a new environment. This research is slowed down by the minimal number of staff and the need to avoid detection, but sped up because AlphaFold5 can quickly debug roadblocks that researchers run into. After 2 years of work, the terrorist group finishes developing the virus and threatens to release it.</p><p> This virus, if released, could kill a significant fraction of humanity. COVID infected most of the world&#39;s population, so a deadlier, faster-spreading, harder to control version of COVID could infect most people and lead to widespread fatalities. Even if the virus did not kill everyone, if even 20% of people were killed by a virus in a short period of time, this could destabilize society and potentially lead to snowball effects that are hard to recover from.</p><h1> Discussion on Plausibility</h1><p> I aimed to make each of these scenarios somewhat plausible—in particular, I avoided some other scenarios that were “simpler” but would be easy for humans to spot and prevent or would otherwise be unlikely to succeed. In <a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">Scenario 1</a> (information acquisition), it would have been easier to just say that the model hacks into its own datacenter, which by <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">my previous estimate</a> already allows running millions of copies and would thus constitute a more immediate and overwhelming advantage for the AI. However, there are many safeguards on centralized servers that would make it difficult to do this without detection, so I opted for a scenario that involved hacking into personal computers even though this gave the AI only thousands of copies instead of millions (see calculations in <a href="#appendix-plausible-size-of-botnets">Appendix</a> ).</p><p> As another example, some people have speculated that a smart enough AI system could design novel bioweapons simply by “thinking”, rather than requiring extensive experimentation; or, if it does require experimentation, assume that it only requires getting humans or robots to follow straightforward instructions. However, my understanding is that the biology labs that are currently able to design new organisms required many years to build up, and rely on accumulated tacit knowledge, experimental protocols, and physical apparatuses. So even with a very capable AI system, the terrorist group might not succeed, and could also lose control of the virus (eg accidentally infect themselves).</p><p> <strong>Selected expert opinions.</strong> I consulted two biology experts on Scenario 4, and they found it to be potentially possible, but felt that the lack of ability to iterate on human subjects (which is typically necessary for eg drug design) might make it difficult to design a supervirus even with very strong conceptual understanding of biology.</p><p> Overall, they leaned towards thinking the exact scenario described was probably infeasible (but not clearly so). However, they felt there were other more feasible scenarios that could more likely lead to a supervirus killing 1-10% of the world population (still with significant uncertainty). <sup><a href="#fn8">[8]</a></sup></p><p> I also consulted a computer security researcher and a couple computer systems engineers on Scenarios 1 and 3. The security researcher had very large margins of error on the possible size of a botnet, citing the historical <a href="https://cseweb.ucsd.edu/~savage/papers/IEEESP03.pdf?ref=bounded-regret.ghost.io">Slammer worm botnet</a> as one that very quickly infected a large number of hosts. They felt that computer security is generally better now than when Slammer happened; it&#39;s possible this means that infecting a large fraction of computers is fundamentally impossible, but also possible that a sufficiently strong attack could overturn this.</p><p> The systems engineers felt that running large AI systems on a distributed botnet of consumer hardware would be very difficult, and would likely lead to around a 1000x efficiency hit compared to using specialized hardware like A100s. In addition, one of them noted that inference would likely be slow in this case, eg taking 1 second per token or more. However, they felt the datacenter route was more plausible, especially if in the future there are many smaller datacenters with significant numbers of GPUs.</p><p><strong>概括。</strong> Overall, I find Scenarios 2 (economic competition) and 3 (cyberattack) most plausible, followed by 1, followed by 4. This reinforces the idea from <em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">Intrinsic Drives and Extrinsic Misuse</a></em> that “misuse exacerbates misalignment”, since the two most plausible scenarios incorporate elements of both. However, it also shows that pure misalignment and pure misuse scenarios are possible, so solving either problem in isolation is probably insufficient.</p><p> <strong>Acknowledgments.</strong> Thanks to Ben Kuhn, Daniel Ziegler, Nicholas Carlini, Adam Marblestone, Sam Rodriques, Erik Jones, Alex Pan, Jean-Stanislas Denain, Ruiqi Zhong, Leopold Aschenbrenner, Tatsu Hashimoto, Percy Liang, Roger Grosse, Collin Burns, Dhruv Madeka, and Sham Kakade for a combination of helpful discussions and comments on various drafts of this post.</p><h1> Appendix: Plausible Size of Botnets</h1><p> Here I discuss both the plausible size of a server that an AI hacker could compromise, as well as the size of a botnet that it could create, based on looking at historical botnet sizes and projections of the total amount of hardware in the world. I then relate this to the number of copies of itself an AI system could run, by using previous projections of inference costs for future models.</p><p> <strong>Assumption on inference costs.</strong> GPT-4 takes around 10 <sup>12</sup> FLOPs per forward pass (GPT-3 is 3.5x10 <sup>11</sup> FLOPs and GPT-4 is probably around 3x bigger). We assume our hypothetical GPT <sub>2030</sub> ++ takes at most 10 <sup>14</sup> FLOPs per forward pass, ie is 100x bigger, which would correspond to 10,000x more training compute under Chinchilla scaling laws.</p><p> To justify this upper bound, if we ignore decreases in GPU costs then the training run for a model this size would cost $1T, since GPT-4 cost over $100M. Even with GPU price decreases, this would be very expensive. Another angle is that the estimates in <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">What will GPT-2030 look like? (Appendix B)</a></em> suggest a median estimate of 1.6 x 10 <sup>13</sup> FLOPs per forward pass in 2030, allowing a factor of 6 for wiggle room before hitting 10 <sup>14</sup> .</p><p> Note that humans run at roughly 4 forward tokens/second <sup><a href="#fn9">[9]</a></sup> , so under our assumption 4 x 10 <sup>14</sup> FLOP/s is enough to run one copy of GPT <sub>2030</sub> ++ at human speed (assuming human tokens and AI forward passes are comparable). Thus 10 <sup>18</sup> FLOP/s is enough to run 2500 copies, which will be our rough target in the discussion below.</p><p> <strong>Size of datacenters.</strong> The biggest datacenters (Google, AWS, and Azure) would likely easily have enough compute to run thousands of copies of GPT <sub>2030</sub> ++, but they are heavily secured and monitored, so I do not assume that even a very good AI hacker could compromise them (although I do think this is a possibility).</p><p> Instead, I focus on smaller datacenters; currently, this would include companies like Coreweave and LambdaLabs. Coreweave has <a href="https://www.coreweave.com/blog/mlperf-coreweave-nvidia-record-breaking-cloud-native-ai-supercomputer?ref=bounded-regret.ghost.io">at least 3500 H100s</a> , and likely more than that. H100s are around 1000 TFLOPs in Bfloat16, so this is 3.5 x 10 <sup>18</sup> FLOP/s in total. Thus compromising 30% of Coreweave today would already provide us with thousands of copies, so even compromising 1% of a similar (larger) datacenter in the future would hit this target.</p><p> Other datacenters are comparable, eg Inflection plans to <a href="https://www.tomshardware.com/news/startup-builds-supercomputer-with-22000-nvidias-h100-compute-gpus?ref=bounded-regret.ghost.io">build a datacenter with 22,000 H100s</a> , and based on its <a href="https://www.theinformation.com/briefings/lambda-labs-nears-300-million-financing?ref=bounded-regret.ghost.io">funding numbers</a> I would guess that Lambda also has several thousand H100s (or equivalent compute in A100s).</p><p> As supply chain bottlenecks are eased for GPUs and as total demand for compute increases, I expect there to be more such datacenters and for the amount of compute in each datacenter to be larger.</p><p> <strong>Size of botnets.</strong> To project possible botnet sizes, I form a reference class based on Folding@Home, several historical botnets, and the total amount of compute in the world right now.</p><p> In practice, the botnet would suffer a large efficiency hit due to the need for cross-machine communication (since most machines are too small to run even a single copy of GPT <sub>2030</sub> ++). I believe this hit would be about a factor of 1000, so we would need botnets with 10 <sup>21</sup> FLOP/s instead of 10 <sup>18</sup> FLOP/s. The actual botnets discussed below are in the 10 <sup>18</sup> range instead, but I&#39;ll discuss at the end how our hypothetical GPT <sub>2030</sub> ++ could still possibly run thousands of copies of itself (though overall I find this scenario less likely than the datacenter one above).</p><p> <em>Folding@Home.</em> Folding@Home was <a href="https://en.wikipedia.org/wiki/Folding@home?ref=bounded-regret.ghost.io">10 <sup>18</sup> FLOP/s at its peak</a> , so if one could hack into the same level of resources as Folding@Home (eg with a very successful virus attacking gaming platforms) then they&#39;d have this much computation.</p><p> Historical botnets. Many historical botnets <a href="https://cybernews.com/security/the-8-biggest-botnets-of-all-time/?ref=bounded-regret.ghost.io">have infected more than 1M computers</a> and some have exceeded 10M. Assuming each bot uses 10% of the computer&#39;s resources at 3GFLOP/s, 10M machines would be 3x10 <sup>15</sup> FLOP/s. However, consumer hardware is beginning to include graphics accelerators for running neural nets, and I would guess most consumer machines by 2030 will have accelerators that can produce 10-100 TFLOPs (eg iPhone 15 Pros <a href="https://www.tomshardware.com/news/apple-a17-pro-3nm-iphone-15-pro?ref=bounded-regret.ghost.io">already do this</a> ). If we assume 30% of the computers have 30TFLOP/s, we would be up to 9 x 10 <sup>19</sup> FLOP/s, surpassing Folding@Home.</p><p> Alternatively, we could consider botnets that specifically target GPUs. For this, we look at illegal cryptomining botnets. <a href="https://www.akamai.com/blog/security/the-massive-propagation-of-the-smominru-botnet?ref=bounded-regret.ghost.io">Smominru</a> was 3x10 <sup>16</sup> FLOP/s based on the idea that the overall botnet could perform a hashrate of <a href="https://www.proofpoint.com/us/threat-insight/post/smominru-monero-mining-botnet-making-millions-operators?ref=bounded-regret.ghost.io">3MH/s</a> , since a single <a href="https://whattomine.com/gpus/49-nvidia-geforce-rtx-3090?ref=bounded-regret.ghost.io">RTX-3090 can perform 2KH/s</a> , and an RTX-3090 is 3x10 <sup>13</sup> FLOP/s. <a href="https://www.bleepingcomputer.com/news/security/mykings-botnet-still-active-and-making-massive-amounts-of-money/?ref=bounded-regret.ghost.io">MyKings</a> seems comparably large to Smominru or perhaps larger. There is also <a href="https://www.proofpoint.com/us/blog/threat-insight/adylkuzz-cryptocurrency-mining-malware-spreading-for-weeks-eternalblue-doublepulsar?ref=bounded-regret.ghost.io">Adylkuzz</a> but its hashrate was about 10x smaller.</p><p> It seems plausible to assemble a botnet with 30x the capacity of Smominru with very good hacking abilities, which again gets us to 10 <sup>18</sup> FLOP/s.</p><p> <em>World compute.</em> Finally, the <a href="https://docs.google.com/spreadsheets/d/1Pz0YVJalZbdo63FI-rCa7baBkGsiuYv5P34Rpt9IZfE/edit?ref=bounded-regret.ghost.io#gid=0">total FLOP/s in the world</a> right now is around 10 <sup>22</sup> . So 10 <sup>18</sup> FLOP/s would correspond to 0.01% of this total, though this total will likely increase significantly in the future as consumer hardware starts to utilize high-performance GPUs.</p><p> <em>Accounting for performance losses.</em> Above we noted that running on distributed hardware would likely lead to large performance hits, on the order of a 1000x loss in efficiency of utilizing the GPUs. Thus we need 10 <sup>21</sup> FLOP/s, but the largest estimate above was just short of 10 <sup>20</sup> FLOP/s. How could the AI hacker close this gap? First, if it is very capable it is plausible that it would acquire a larger botnet than the current largest ones, and an order-of-magnitude improvement would get to 10 <sup>21</sup> FLOP/s. Second, it could potentially run smaller, specialized copies of itself (created via distillation) to further save on inference costs, though it would first have to train the smaller models. Finally, future efficiency improvements in running neural networks might decrease the cost of inference below the original 10 <sup>18</sup> estimate.</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p> With the caveat that when assessing feasibility I would want to analyze the general category of risk, as opposed to the specific sequence of events that I describe. <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p> This is not the same as the probability that the scenario actually happens, which additionally requires a system at the level of GPT <sub>2030</sub> ++ to attempt it, and to subsequently succeed. <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p> By conceptual capabilities, I mean generating good hypotheses, as well as some aspects of designing experiments, but not physically running the experiments themselves. <a href="#fnref3">↩︎</a></p></li><li id="fn4" class="footnote-item"><p> At the time of writing this post, my median estimate is that a system at least as capable as GPT <sub>2030</sub> ++ (with some uncertainty about inference speed) will exist in 2035. <a href="#fnref4">↩︎</a></p></li><li id="fn5" class="footnote-item"><p> See <a href="#appendix-plausible-size-of-botnets">Appendix</a> for a discussion of these numbers, including an estimate of how many machines a strong AI hacker could plausibly acquire and how much total compute this would yield. <a href="#fnref5">↩︎</a></p></li><li id="fn6" class="footnote-item"><p> Since 50 years = 2600 weeks, so 2600 copies would be sufficient to get 50 years of “work” in a week, assuming that distinct exploits can be parallelized across the copies. <a href="#fnref6">↩︎</a></p></li><li id="fn7" class="footnote-item"><p> More generally, backdoors are difficult to detect since the designer of the backdoor has much more discretion than potential auditors. For instance, <a href="https://blog.azuki.vip/surveillance/?ref=bounded-regret.ghost.io">Yahoo had a backdoor</a> in its servers that was only publicly discovered many years later. <a href="#fnref7">↩︎</a></p></li><li id="fn8" class="footnote-item"><p> I omit details of these scenarios to avoid the risk of providing ideas to future bad actors. <a href="#fnref8">↩︎</a></p></li><li id="fn9" class="footnote-item"><p> See <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#a-words-per-minute">What will GPT-2030 look like? (Appendix A)</a></em> . <a href="#fnref9">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastrophic-drives-four-vignettes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastrophic-drives-four-vignettes<guid ispermalink="false"> acPYHjC9euGZRzaj6</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Fri, 10 Nov 2023 07:30:07 GMT</pubDate> </item><item><title><![CDATA[Crock, Crocker, Crockiest]]></title><description><![CDATA[Published on November 10, 2023 6:14 AM GMT<br/><br/><p> <strong>Summary</strong> : If you declare you&#39;re operating by Crocker&#39;s Rules, other people are allowed to optimize their messages to you for information, not for being nice to you. The converse of Crocker&#39;s Rules would be asking people to optimize their messages to you for being nice to you, not for information. Both of these modes of communication are useful, and you&#39;ll have a chance to practice both.</p><p> <strong>Tags</strong> : Repeatable, investment, highly experimental</p><p> <strong>Purpose</strong> : There are four skills. 1. Optimizing your communication to others to be informative. 2. Optimizing your communication to others to be nice. 3. Receiving communication that was not at all optimized for being nice. 4. Receiving communication that was not at all optimized for being informative. This provides an opportunity to practice each of these.</p><p> <strong>Materials</strong> : You need some kind of clearly visible marker, in at least three obviously different styles. Blue, green and red bandanas are one option. Large stickers are another, though note that people will be removing and re-applying them throughout the meetup and so those will naturally lose their adhesive.</p><p> <strong>Announcement Text</strong> : “It may indeed be impolite; I don&#39;t deny that. Whether it&#39;s untrue is a different question. - Eleizer Yudkowsky</p><p> Crocker&#39;s Rules, named after and framed by Lee Daniel Crocker, are a social norm you can declare that you&#39;re using where you authorize other people to optimize their messages for  information over niceness. In other words, by saying you&#39;re operating by Crocker&#39;s Rules, you&#39;re saying you want people to say true things to you even if those things would be rude, in the interests of efficient communication.</p><p> It can be uncomfortable and strange for some people to talk to someone using Crocker&#39;s Rules! If you&#39;re usually a polite person who tries not to make others upset, then saying the true and rude thing feels like being mean. Speaking impolite truths is a skill, and this meetup may offer a chance to practice that skill.</p><p> There&#39;s another skill that some who flock to Crocker&#39;s banner are not as practiced with. The ability to be polite and abide by local etiquette is a useful practical skill as well. We&#39;re going to hopefully have a chance to practice <i>that</i> as well, and while it is less of an obvious rationalist skill, it&#39;s one that&#39;s worth having anyway.</p><p> <strong>Description</strong> : Once people are gathered, read the text of the <a href="http://sl4.org/crocker.html">SL4 Crocker&#39;s Rule</a> post.</p><blockquote><p> “Declaring yourself to be operating by &quot;Crocker&#39;s Rules&quot; means that other people are allowed to optimize their messages for information, not for being nice to you.  Crocker&#39;s Rules means that you have accepted full responsibility for the operation of your own mind - if you&#39;re offended, it&#39;s your fault.  Anyone is allowed to call you a moron and claim to be doing you a favor.  (Which, in point of fact, they would be.  One of the big problems with this culture is that everyone&#39;s afraid to tell you you&#39;re wrong, or they think they have to dance around it.)  Two people using Crocker&#39;s Rules should be able to communicate all relevant information in the minimum amount of time, without paraphrasing or social formatting.  Obviously, don&#39;t declare yourself to be operating by Crocker&#39;s Rules unless you have that kind of mental discipline.</p><p> <strong>Note that Crocker&#39;s Rules does</strong> <i><strong>not</strong></i> <strong>mean you can insult people; it means that</strong> <i><strong>other</strong></i> <strong>people don&#39;t have to worry about whether</strong> <i><strong>they</strong></i> <strong>are insulting</strong> <i><strong>you</strong></i> <strong>.</strong> Crocker&#39;s Rules are a discipline, not a privilege.  Furthermore, taking advantage of Crocker&#39;s Rules does not imply reciprocity.  How could it?  Crocker&#39;s Rules are something you do for yourself, to maximize information received - <i>not</i> something you grit your teeth over and do as a favor.</p><p> &quot;Crocker&#39;s Rules&quot; are named after Lee Daniel Crocker.”</p><p> - Eliezer Yudkowsky</p></blockquote><p> Next, repeat the bold section. Pause. Repeat the bold section again.</p><p> Now pass out the visible markers. (This description will assume you&#39;re using bandanas.) Make sure each person who wants one can have one. “Everyone see the bandanas?好的。 Look at them. Anyone can&#39;t tell the difference between them?” (You should get a chorus of Nos.) “Good!”</p><p> “Here&#39;s how this is going to work. If you want to use Crocker&#39;s Rules, put the yellow headband on- you can tie it around your head or wrist, or just tuck it in your jacket zipper. That means that other people should communicate to you optimizing for information, not niceness. If you want to use reverse-Crocker&#39;s Rules, put on the green headband. That means that other people should communicate to you optimizing for niceness, not information. If you&#39;re happy to be a guide- that is, to give someone suggestions for how to be nicer or more informative, while keeping firmly in mind what headband they&#39;re wearing- then wear a white bandana in addition to what you&#39;re wearing. At any time, you can take off the bandana. If you&#39;re just holding it, that doesn&#39;t count for anything. Check whether the bandana is just held, or being worn. I hereby declare that it is always nice and it is always information to ask whether someone is wearing the bandana or just holding it.”</p><p> “Your goal is to try and talk to both Crocker and reverse-crocker people tonight. What you talk about is up to you. When you&#39;re done, please bring the bandanas back to me!”</p><p> When the conversation is done, collect the bandanas, and then bow to each other. Thank those who donned them for helping people learn.</p><p> <strong>Variations:</strong> So the title of this is a joke on how “Crocker” in English sounds like it should mean something like “more crock.” Compare “Heavy, heavier, heaviest.” One thing I want to try is varying the amount of “Crock” (for lack of a better term) people are using. Perhaps the placement of the bandana indicated how much you were encouraging people to discard politeness in pursuit of efficient communication, with a bandana tied around the wrist indicating to optimize for niceness, the elbow for normal conversation, and the shoulder indicating optimizing for efficient information. In practice, this is tricky both because bandanas tend to slip downward and (harder to solve) people aren&#39;t very good at precisely calibrating the amount of directness. The difference between 80% Crocker and 70% Crocker is hard to aim at, especially given interpersonal variation. Crocker&#39;s Rules are an absolute- you can call someone a moron to their face and call it a favour to them!- and that&#39;s an easier target. Still, I think it would be useful to be able to slowly turn up the amount of directness, to warm people up and give them a chance to dip their toes in.</p><p> The first incarnation of this meetup involved using the location in the room to indicate how much niceness and how much truth to optimize for. Say, have the north wall be maximum Crocker, and the south wall be maximum Reverse Crocker. The problem with this is that to go talk to someone in Maximum Crocker, you yourself have to go pretty far into the Crocker side. That&#39;s not how this works. Likewise, having things change over time (so the event starts in normal, then gradually becomes more Nice for half an hour until it peaks, then gradually goes back to normal, then gradually becomes more Crocker for half an hour until it peaks) is going to put people who aren&#39;t ready for Crocker&#39;s Rules in that mode.</p><p> I do think having a way to try out Crocker&#39;s Rules a little bit is good. Short of doing a bunch of invisible, individual work in your own head and then showing up ready to accept “full responsibility for the operation of your own mind” and either succeeding or failing and getting angry, I&#39;m not sure how this is supposed to be taught and learned. Frustratingly, the obvious way to practice is to have someone adjust their message to you to optimize for gradually acclimating you to receiving Crocker&#39;s Rule style messages, and that&#39;s kind of not the thing Crocker&#39;s Rules are supposed to mean!</p><p> <strong>Notes</strong> : See the Highly Experimental tag? That&#39;s not there by accident, this is an untried work in progress.</p><p> First off, suggestions for what to call the niceness>;information side other than “reverse crocker” are solicited. That&#39;s just terrible nomenclature, but I don&#39;t have a better idea.</p><p> Something that almost sinks this as a possible meetup in a box is you may not have anyone willing to undergo Crocker&#39;s Rules. You do not want to conscript someone who can&#39;t operate with them, because that is going to be pretty unpleasant for them and it&#39;ll probably be worse the more outnumbered they are. (One person being what-you-perceive-as-rude to you is bad. A room full of people doing that is worse, and not in a way that scales linearly or predictably.) If you are a person who is comfortable, this meetup is more feasible.</p><p> Especially taking the guide role, I think it would be useful to switch modes a few times when working with someone. Get your interlocutor used to Crocker&#39;s Rules, not as how they always interact with you and your personal quirks, but as a kind of dialect they can code switch into and out of.</p><p> Be on the lookout for people thinking that because they are wearing the Crocker&#39;s Rules signal, they can be rude and direct to others. They are just wrong in point of fact, and also they are the last people who can complain about you telling them they&#39;re wrong in point of fact.</p><p> I do actually think practicing being nice to people at the expense of efficient exchange of information is a skill people could benefit from practicing. I&#39;m not arguing here that&#39;s an important and central rationality skill. I am arguing that it&#39;s useful to do side by side with Crocker&#39;s Rules, since the contrast can be informative. Also, optimizing for information is different than optimizing for hurting someone&#39;s feelings, and while calling someone a moron is explicitly allowed under Crocker&#39;s Rules, &quot;Hey you moron you forgot to take your shoes off in the house&quot; is not the most efficient way to send that information. That would be &quot;You forgot to take your shoes off in the house&quot; unless the insult is somehow helpful in a way I usually don&#39;t think it is.</p><p> Someone sure is going to try to don the mantle of Crocker&#39;s Rules when the aren&#39;t ready and get emotionally bruised. I don&#39;t know how to avoid that. Suggestions solicited for ideas on boxable encouragement and support for this workout.</p><p> <strong>Credits</strong> : Crocker&#39;s Rules come from Daniel Lee Crocker. The written form of them I first encountered was Yudkowsky referencing the SL4 post. The idea of having a clear signal to opt in to Crocker&#39;s Rules (A particular sticker you could put on your nametag)  is one I personally got from the LessWrong Community Weekend in 2022. The LessWrong user who acted as a sounding board over lunch is welcome to be credited if they want to be, or may wish to avoid association with this catastrophe waiting to happen. (Edit: It&#39;s Czynski, they claimed credit. I&#39;m playing up the catastrophe potential a little bit for humour value, but in its present form I do guess this has a higher chance of causing interpersonal hard feelings than I usually aim at for things I suggest to people.)</p><br/><br/><a href="https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest<guid ispermalink="false"> BcCeyL89cKSqcjtL5</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 10 Nov 2023 06:14:27 GMT</pubDate> </item><item><title><![CDATA[AI Timelines]]></title><description><![CDATA[Published on November 10, 2023 5:28 AM GMT<br/><br/><h1>介绍</h1><p>How many years will pass before transformative AI is built? Three people who have thought about this question a lot are Ajeya Cotra from <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> , Daniel Kokotajlo from <a href="https://openai.com/">OpenAI</a> and Ege Erdil from <a href="https://epochai.org/">Epoch</a> . Despite each spending hundreds of hours investigating this question, they still still disagree substantially about the relevant timescales. For instance, here are their median timelines for one operationalization of transformative AI: </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><thead><tr><th style="border:0px solid hsl(0, 0%, 100%);text-align:center" colspan="2"><p> <strong>Median Estimate for when 99% of currently</strong></p><p> <strong>fully remote jobs will be automatable</strong></p></th></tr></thead><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Daniel</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 4 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ajeya</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 13 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ege</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 40 years</td></tr></tbody></table></figure><p> You can see the strength of their disagreements in the graphs below, where they give very different probability distributions over two questions relating to AGI development. </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year would AI systems be</strong><br> <strong>able to replace 99% of current fully remote jobs?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kzksnas7qpbb02twb573" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/x5ooeyjjkbmr0zewb6r5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jbrfgwzbc0iq6amz3iet 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qujr8mhztjp8ugqsvbhy 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sjlqwtgqkfymaaaqecy7 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dipfjc1gbqieubrm0cpo 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/suujprewmfj7uevwcz3f 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fpqna0v3ingftmmzf47b 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vic2xwlvevudbc1lcfpe 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/zwzqdp6ocv4tkotvwonc 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dhw6gxwmgdzp99ptolns 5497w"></figure><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year will the energy consumption of</strong><br> <strong>humanity or its descendants be 1000x greater than now?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dgluluai77tpabbjl6vp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gghyhdlpqvkftxbbkxwu 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/m6vn1o3xguil21rzm89p 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jgy5taicz50nnbiwsg4s 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dqg6vfx5jzbkbyrvzvz6 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/t9c00topcmmoqrrowxz6 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qkcozi5f3u6swey2rccx 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/ozvbxotplbmhxixrfh4t 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gpwhggifjrcsbdkzzc9h 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/xwrgkbhhtxei1aqs23ja 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wqq4xog2wv7d7gyvoegj 5603w"><figcaption> Median indicated by small dotted line. Note that Ege&#39;s median is outside of the bounds at 2177</figcaption></figure><p> So I invited them to have a conversation about where their disagreements lie, sitting down for 3 hours to have a written dialogue. You can read the discussion below, which I personally found quite valuable.</p><p> The dialogue is roughly split in two, with the first part focusing on disagreements between Ajeya and Daniel, and the second part focusing on disagreements between Daniel/Ajeya and Ege.</p><p> I&#39;ll summarize the discussion here, but you can also jump straight in.</p><h1> Summary of the Dialogue</h1><h2> Some Background on their Models</h2><p> Ajeya and Daniel are using a compute-centric model for their AI forecasts, illustrated by Ajeya&#39;s <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">draft AI Timelines report</a> , and <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> where the question of &quot;when transformative AI&quot; gets reduced to &quot;how much compute is necessary to get AGI and when will we have that much compute? (modeling algorithmic advances as reductions in necessary compute)&quot;.</p><p> Whereas Ege thinks such models should have a lot of weight in our forecasts, but that they likely miss important considerations and doesn&#39;t have enough evidence to justify the extraordinary predictions it makes.</p><h2> Habryka&#39;s Overview of Ajeya &amp; Daniel discussion</h2><ul><li> Ajeya thinks translating AI capabilities into commercial applications has gone slower than expected (&quot; <i>it seems like 2023 brought the level of cool products I was naively picturing in 2021</i> &quot;) and similarly thinks there will be a lot of kinks to figure out before AI systems can substantially accelerate AI development.</li><li> Daniel agrees that impactful commercial applications have been slower than expected, but also thinks that the parts that made that slow can be automated substantially, and that a lot of the complexity comes from shipping something that can be useful to general consumers, and that for applications internal to the company, these capabilities can be unlocked faster.</li><li> Compute overhangs also play a big role in the differences between Ajeya and Daniel&#39;s timelines. There is currently substantial room to scale up AI by just spending more money on readily available compute. However, within a few years, increasing the amount of training compute further will require accelerating the semiconductor supply chain, which probably can&#39;t be easily achieved by just spending more money. This creates a &quot;compute overhang&quot; that accelerates AI progress substantially in the short run. Daniel thinks it&#39;s more likely than not that we will get transformative AI before this compute overhang is exhausted. Ajeya thinks that is plausible, but overall it&#39;s more likely to happen after, which broadens her timelines quite a bit.</li></ul><p> These disagreements probably explain some but not most of the differences in the timelines for Daniel and Ajeya.</p><h2> Habryka&#39;s Overview of Ege &amp; Ajeya/Daniel Discussion</h2><ul><li> Ege thinks that Daniel&#39;s forecast leaves very little room for Hoftstadter&#39;s law (&quot;It always takes longer than you expect, even when you take into account Hofstadter&#39;s Law&quot;), and in-general that there will be a bunch of unexpected things that go wrong on the path to transformative AI</li><li> Daniel thinks that Hofstadter&#39;s law is inappropriate for trend extrapolation. Ie it doesn&#39;t make sense to look at Moore&#39;s law and be like &quot;ah, and because of planning fallacy the slope of this graph from today is half of what it was previously&quot;</li><li> Both Ege and Ajeya don&#39;t expect a large increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</li><li> Ege expects that AI will have a large effect on the economy, but has substantial probability on persistent deficiencies that prevent AI from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</li></ul><p> Overall, whether AI will get substantially better at transfer learning (eg seeing an AI be trained on one genre of video game and then very quickly learn to play another genre of video game) would update all participants substantially towards shorter timelines.</p><p> We ended the dialogue with Ajeya, Daniel and Ege by putting numbers on how much various AGI milestones would cause them to update their timelines (with the concrete milestones proposed by Daniel). Time constraints made it hard to go into as much depth as we would have liked, but me and Daniel are excited about fleshing more concrete scenarios of how AGI could play out and then collecting more data on how people would update in such scenarios.</p><h1> The Dialogue </h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​Daniel, Ajeya, and Ege all seem to disagree quite substantially on the question of &quot;how soon is AI going to be a really big deal?&quot;. So today we set aside a few hours to try to dig into that disagreement, and what the most likely cruxes between your perspectives might be.</p><p> To keep things grounded and to make sure we don&#39;t misunderstand each other, we will be starting with two reasonably well-operationalized questions:</p><ol><li> In what year would AI systems be able to replace 99% of current fully remote jobs? (With operationalization stolen from an <a href="https://docs.google.com/presentation/d/1Rjnyl-jeaCTzmul9L-7A2gYJXUh9srcg6V0ONyPGft4/edit#slide=id.p">AI forecasting slide deck that Ajeya shared</a> )</li><li> In what year will the energy consumption of humanity or its descendants be 1000x greater than now?</li></ol><p> These are of course both very far from a perfect operationalization of AI risk (and I think for most people both of these questions are farther off than their answer to &quot;how long are your timelines?&quot;), but my guess is it will be good enough to elicit most of the differences in y&#39;all&#39;s models and make it clear that there is indeed disagreement.</p></div></section><h2> Visual probability distributions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​To start us off, here are two graphs of y&#39;all&#39;s probability distributions: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gkorp85zmipz4s64yh4y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sznehrdhsfht3euhrb4l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/tye1xevxurl2qvlj9teo 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/s3sycw4tqvv7yrzyw68d 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/frh1ttp4jevhfnq19eso 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eeapu1uduw5pjse4c7fu 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/i0jmvrazsil7ev8ccyz0 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kjj4ajpma4ngobdgwdux 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qln2f1q51g7jpho2itta 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/w5u4zpkq9uyapwcowdz5 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qfi9sszguck2onei1oy4 5497w"><figcaption>​ <strong>When will 99% of fully remote jobs be automated?</strong> </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vabqfpmnwrcb5vht3qa9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/r2yrjysrwy9xxgap2zdb 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wuslgsdo6stnca2zr9bl 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yyfzgooeeqkhtwlcarlt 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/d0yycia21vyuzwxsqqaz 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yr3kqedmtu9uukhlbcgt 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/z2odotlzjnkauqsx502j 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wwgg2tfavqddqlt6qrvs 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fgq6uxeosgb2tazlwsyr 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eh9qr7htcy7mwpzzbqg7 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wtpir1tpd3zqijoqg2ys 5603w"><figcaption> <strong>When will we consume 1000x energy?</strong></figcaption></figure></div></section><h2> Opening statements </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, so let&#39;s get started:</p><p> <strong>What is your guess about which belief of yours the other two most disagree with</strong> , <strong>that might explain some of the divergence in your forecasts?</strong></p></div></section><h3><strong>丹尼尔</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:27:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:27:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I don&#39;t understand Ege&#39;s views very well at all yet, so I don&#39;t have much to say there. By contrast I have a lot to say about where I disagree with Ajeya. In brief: My training compute requirements distribution is centered a few OOMs lower than Ajeya&#39;s is.为什么？ For many reasons, but (a) I am much less enthusiastic about the comparison to the human brain than she is (or than I used to be!) and (b) I am less enthusiastic about the <a href="https://www.lesswrong.com/posts/BGtjG6PzzmPngCgW9/revisiting-the-horizon-length-hypothesis">horizon length hypothesis</a> / I think that large amounts of training on short-horizon tasks combined with small amounts of training on long-horizon tasks will work (after a few years of tinkering maybe). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:41:47 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:41:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just to clarify, it sounds like you approximately agree with a compute-focused approach of AI forecasting? As in, the key variable to forecast is how much compute is necessary to get AGI, with maybe some adjustment for algorithmic progress, but not a ton?</p><p> How do things like &quot;AIs get good at different horizon lengths&quot; play into this (which you were also mentioning as one of the potential domains of disagreement)?</p><p> <i>(For readers: The horizon length hypothesis is that the longer the the feedback loops for a task are, the harder it is for an AI to get good at this task.</i></p><p> <i>Balancing a broom on one end has feedback loops of less than a second. The task of &quot;running a company&quot; has month-to-year long feedback loops. The hypothesis is that we need much more compute to get AIs that are better at the second than the first. See also</i> <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi#The_t_AGI_framework"><i>Richard Ngo&#39;s t-AGI framework</i></a> <i>which posits that the domain in which AI is generally intelligence will gradually expand from short time horizons to long time horizons.)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:44:36 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:44:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yep I think Ajeya&#39;s model (especially the version of it expanded by Davidson &amp; Epoch) is our best current model of AGI timelines and takeoff speeds. I have lots to critique about it but it&#39;s basically my starting point. And I am qualified to say this, so to speak, because I actually did consider about a half-dozen different models back in 2020 when I was first starting to research the topic and form my own independent impressions, and the more I thought about it the more I thought the other models were worse. Examples of other models: <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">Polling AI scientists</a> , <a href="https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines">extrapolating gross world product (GWP) a la Roodman</a> , deferring to <a href="https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi">what the stock markets imply</a> , <a href="https://www.lesswrong.com/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied">Hanson&#39;s weird fractional progress model thingy</a> , the <a href="https://epochai.org/blog/grokking-semi-informative-priors">semi-informative</a> prior <a href="https://aipriors.com/">models...</a><a href="https://epochai.org/blog/grokking-semi-informative-priors"> </a>I still put some weight on those other models but not much.<br><br> As for the horizon lengths question: This feeds into the training compute requirements variable. IIRC Ajeya&#39;s original model had different buckets for short, medium, and long-horizon, where eg medium-horizon bucket meant roughly &quot;Yeah we&#39;ll be doing a combo of short horizon and long horizon training, but on average it&#39;ll be medium-horizon training, such that the compute costs will be eg [inference FLOPs]*[many trillions of datapoints, as per scaling laws applied to bigger-than-human-brain-models]*[4-6 OOMs of seconds of subjective experience per datapoint on average]<br><br> So Ajeya had most of her mass on the medium and long horizon buckets, whereas I was much more bullish that the bulk of the training could be short horizon with just a &quot;cherry on top&quot; of long-horizon. Quantitatively I was thinking something like &quot;Say you have 100T datapoints of next-moment-prediction as part of your short-horizon pre-training. I claim that you can probably get away with merely 100M datapoints of million-second-long tasks, or less.&quot;<br><br> For some intuitions why I think this, it may help to read <a href="https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute">this post</a> and/or <a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi?commentId=Bs9sKmzhNvSPAs3yY">this comment thread.</a></p></div></section><h3> <strong>Ege</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:35:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:35:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think my specific disagreements with Ajeya and Daniel might be a little different, but an important meta-level point is my general skepticism of arguments that imply wild conclusions. This becomes especially relevant with predictions of a 3 OOM increase in our energy consumption in the next 10 or 20 years. It&#39;s possible to tell a compelling story about why that might happen, but also possible to do the opposite, and judging how convincing those arguments should be is difficult for me. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:38:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:38:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> OK, in response to Ege, I guess we disagree about this &quot;that-conclusion-is-wild-therefore-unlikely&quot; factor. I think for things like this it&#39;s a pretty poor guide to truth relative to other techniques (eg models, debates between people with different views, model-based debates between people with different views) I&#39;m not sure how to make progress on resolving this crux. Ege, you say it&#39;s mostly in play for the 1000x energy consumption thing; wanna focus on discussing the other question instead? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:40:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:40:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Sure, discussing the other question first is fine.<br><br> I&#39;m not sure why you think heuristics like &quot;I don&#39;t update as much on specific arguments because I&#39;m skeptical of my ability to do so&quot; are ineffective, though. For example, this seems like it goes against the fractional Kelly betting heuristic from <a href="https://www.lesswrong.com/posts/TNWnK9g2EeRnQA8Dg/never-go-full-kelly">this post</a> , which I would endorse in general: you want to defer to the market to some extent because your model has a good chance of being wrong.<br><br> I don&#39;t know if it&#39;s worth going down this tangent right now, though, so it&#39;s probably more productive to focus on the first question for now.<br><br> I think my wider distribution on the first question is also affected by the same high-level heuristic, though to a lesser extent. In some sense, if I were to fully condition on the kind of compute-based model that you and Ajeya seem to have about how AI is likely to develop, I would probably come up with a probability distribution for the first question that more or less agrees with Ajeya&#39;s. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:48:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:48:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p>那很有意思。 I think digging into that seems good to me.</p><p> Can you say a bit more about how you are thinking about it at a high level? My guess is you have a bunch of broad heuristics, some of which are kind of like &quot;well, the market doesn&#39;t seem to think AGI is happening soon?&quot;, and then those broaden your probability mass, but I don&#39;t know whether that&#39;s a decent characterization, and would be interested in knowing more of the heuristics that drive that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:53:33 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:53:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I&#39;m not sure I would put that much weight on the market not thinking it&#39;s happening soon, because I think it&#39;s actually fairly difficult to tell what market prices would look like if the market <i>did</i> think it was happening soon.<br><br> Setting aside the point about the market and elaborating on the rest of my views: I would give a 50% chance that in 30 years, I will look back on something like <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> and say &quot;this model captured all or most of the relevant considerations in predicting how AI development was likely to proceed&quot;. For me, that&#39;s already a fairly high credence to have in a specific class of models in such an uncertain domain.<br><br> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.<br><br> If I thought the takeoff model from Tom Davidson (and some newer versions that I&#39;ve been working on personally) were basically right, my forecasts would just look pretty similar to the forecasts of that model, and based on my experience with playing around in these models and the parameter ranges I would consider plausible, I think I would just end up agreeing with Ajeya on the first question.<br><br> Does that explanation make my view somewhat clearer? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:57:12 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:57:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> This part really helps, I think.</p><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:00:13 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:00:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;.</p></blockquote><p> That&#39;s basically right, though I would add the caveat that entropy is relative so it doesn&#39;t really make sense to have a &quot;more uncertain distribution&quot; over when AGI will arrive. You have to somehow pick some typical timescale over which you expect that to happen, and I&#39;m saying that once scaling is out of the equation I would default to longer timescales that would make sense to have for a technology that we think is possible but that we have no concrete plans for achieving on some reasonable timetable. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:03:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:03:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds. It&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now. In some sense, &quot;we come up with a brilliant insight to do this way more efficiently than nature even when we have very little compute&quot; is a hypothesis that should have had &lt;50% weight a priori, compared to &quot;capabilities will start getting good when we&#39;re talking about macroscopic amounts of compute.&quot;</p><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></div></section><h3> <strong>Ajeya</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 17:43:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 17:43:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> On Daniel&#39;s opening points: I think I actually just agree with both a) and b) right now — or rather, I agree that the right question to ask about the training compute requirement is something more along the lines of &quot;How many GPT-N to GPT-N.5 jumps do we think it would take?&quot;, and that short horizon LLMs plus tinkering looks more like &quot;the default&quot; than like &quot;one of a few possibilities,&quot; where other possibilities included a more intense meta-learning step (which is how it felt in 2019-20). The latter was <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines#Picturing_a_more_specific_and_somewhat_lower_bar_for_TAI">the biggest adjustment</a> in my updated timelines.<br><br> That said though, I think two important object-level points push the &quot;needed model size&quot; and &quot;needed amount of tinkering&quot; higher in my mind than it is for Daniel:</p><ul><li> In-context learning does seem pretty bad, and doesn&#39;t seem to be improving a huge amount. I think we can have TAI without really strong human-like in-context learning, but it probably requires more faff than if we had that out of the gate.</li><li> Relatedly, adversarial robustness seems not-great right now.  This also feels overcomable, but I think it increases the scale that you need (by analogy, like 5-10 years ago it seemed like vision systems were good enough for cars except in the long tail / in adversarial settings, and I think vision systems had to get a fair amount bigger, plus there had to be a lot more tinkering on the cars, to get to now where they&#39;re starting to be viable).</li></ul><p> And then a meta-level point is that I (and IIRC Metaculus, according to my colleague Isabel) have been kind of surprised for the last few years about the lack of cool products built on LLMs (it seems like 2023 brought the level of cool products I was naively picturing in 2021). I think there&#39;s a &quot;reality has a lot of detail, actually making stuff work is a huge pain&quot; dynamic going on, and it lends credence to the &quot;things will probably be fairly continuous&quot; heuristic that I already had.</p><p> A few other meta-points:</p><ul><li> The Paul <a href="https://sideways-view.com/2023/07/29/self-driving-car-bets/">self-driving car bets</a> post was interesting to me, and I place some weight on &quot;Daniel is doing the kind of &#39;I can see how it would be done so it&#39;s only a few years away&#39; move that I think doesn&#39;t serve as a great guide to what will happen in the real world.&quot;</li><li> Carl is the person who seems like he&#39;s been the most right when we&#39;ve disagreed, so he&#39;s probably the one guy whose views I put the most weight on. But also Carl seems like he errs aggressive and errs in the direction of believing people will aggressively pursue the most optimal thing (being more surprised than I was, for a longer period of time, about how people haven&#39;t invested more in AI and accomplished more with it by now). His timelines are longer than Daniel&#39;s, and I think mine are a bit longer than his.</li><li> In general, I do count Daniel as among a pretty small set of people who were clearly on record with views more correct than mine in 2020 about both the nature of how TAI would be built (LLMs+tinkering) and how quickly things would progress. Although it&#39;s a bit complicated because 2020-me thought we&#39;d be seeing more powerful LLM products by now.</li><li> Other people who I think were more right include Carl, Jared Kaplan, Danny Hernandez, Dario, Holden, and Paul. Paul is interesting because I think he both put more weight than I did on &quot;it&#39;s just LLMs plus a lot of decomposition and tinkering&quot; but also puts more weight than either me or Daniel on &quot;things are just hard and annoying and take a long time;&quot; this left him with timelines similar to mine in 2020, and maybe a bit longer than mine now.</li><li> Oh — another point that seems interesting to discuss at some point is that I suspect Daniel generally wants to focus on a weaker endpoint because of some sociological views I disagree with. (Screened off by the fact that we were answering the same question about remotable jobs replacement, but I think hard to totally screen off.)</li></ul></div></section><h2> On in-context learning as a potential crux </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:04:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:04:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya:</p><ul><li> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</li><li> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</li><li> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</li><li> re: schlep &amp; incompetence on the part of the AGI industry: Yep, you are right about this, and I was wrong. Your description of Carl also applies to me historically; in the past three years I&#39;ve definitely been a &quot;this is the fastest way to AGI, therefore at least one of the labs will do it with gusto&quot; kind of guy, and now I see that is wrong. I think basically I fell for the planning fallacy &amp; efficient market hypothesis fallacy.<br><br> However, I don&#39;t think this is the main crux between us, because it basically pushes things back by a few years, it doesn&#39;t eg double (on a log scale) the training requirements. My current, updated model of timelines, therefore, is that the bottleneck in the next five years is not necessarily compute but instead quite plausibly schlep &amp; conviction on the part of the labs. This is tbh a bit of a scary conclusion. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:21:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:21:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</p></blockquote><p> The traditional image of AGI involves having an AI system that can <i>learn new (to it) skills</i> as efficiently as humans (with as few examples as humans would need to see). I think this is not how the first transformative AI system will look, because ML is less sample efficient than humans and it doesn&#39;t look like in-context learning is on track to being able to do the kind of fast sample-efficient learning that humans do. I think this is not fatal for getting TAI, because we can make up for it by a) the fact that LLMs&#39; &quot;ancestral memory&quot; contains all sorts of useful information about human disciplines that they won&#39;t need to learn in-context, and b) explicitly guiding the LLM agent to &quot;reason out loud&quot; about what lessons it should take away from its observations and putting those in an external memory it retrieves from, or similar.</p><p> I think back when Eliezer was saying that &quot;stack more layers&quot; wouldn&#39;t get us to AGI, this is one of the kinds of things he was pointing to: that cognitively, these systems didn&#39;t have the kind of learning/reflecting flexibility that you&#39;d think of re AGI. When people were talking about GPT-3&#39;s in-context learning, I thought that was one of the weakest claims by far about its impressiveness. The in-context learning at the time was like: you give it a couple of examples of translating English to French, and then you give it an English sentence, and it dutifully translate that into French. It already knew English and it already knew French (from its ancestral memory), and the thing it &quot;learned&quot; was that the game it was currently playing was to translate from English to French.</p><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible). But compared to any object-level skill like coding (many of which are superhuman), in-context learning seems quite subhuman. I think this is related to how ARC Evals&#39; LLM agents kind of &quot;fell over&quot; doing things like setting up Bitcoin wallets.</p><p> Like Eliezer often says, humans evolved to hunt antelope on the savannah, and that very same genetics coding for the very same brain can build rockets and run companies. Our LLMs right now generalize further from their training distribution than skeptics in 2020 would have said, and they&#39;re generalizing further and further as they get bigger, but they have nothing like the kind of savannah-to-boardroom generalization we have. This can create lots of little issues in lots of places when an LLM will need to digest some new-to-it development and do something intelligent with it. Importantly, I don&#39;t think this is going to stop LLM-agent-based TAI from happening, but it&#39;s one concrete limitation that pushes me to thinking we&#39;ll need more scale or more schlep than it looks like we&#39;ll need before taking this into account.</p><p> Adversarial robustness, which I&#39;ll reply to in another comment, is similar: a concrete hindrance that isn&#39;t fatal but is one reason I think we&#39;ll need more scale and schlep than it seems like Daniel does (despite agreeing with his concrete counterarguments of the form &quot;we can handle it through X countermeasure&quot;). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:34:30 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:34:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya: Thanks for that lengthy reply. I think I&#39;ll have to ponder it for a bit. Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:15:08 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:15:08 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds.</p></blockquote><p> I think it&#39;s fine to incorporate these uncertainties as a wider prior over the training compute requirements, and I also agree it&#39;s a reason to put more weight on this broad class of models than you otherwise would, but I still don&#39;t find these reasons compelling enough to go significantly above 50%. It just seems pretty plausible to me that we&#39;re missing something important, even if any specific thing we can name is unlikely to be what we&#39;re missing.<br><br> To give one example, I initially thought that the evolution anchor from the Bio Anchors report looked quite solid as an upper bound, but I realized some time after that it doesn&#39;t actually have an appropriate anthropic correction and this could potentially mess things up. I now think if you work out the details this correction turns out to be inconsequential, but it didn&#39;t have to be like that: this is just a consideration that I missed when I first considered the argument. I suppose I would say I don&#39;t see a reason to trust my own reasoning abilities as much as you two seem to trust yours.</p><blockquote><p> The compute scaling hypothesis is much broader, and it&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now.</p></blockquote><p> My impression is that Moravec predicted in 1988 that we would have AI systems comparable to the human brain in performance around 2010. If this actually happens around 2037 (your median timelines), Moravec&#39;s forecast will have been off by around a factor of 2 in terms of the time differential from when he made the forecast. That doesn&#39;t seem &quot;super good&quot; to me.<br><br> Maybe I&#39;m wrong about exactly what Moravec predicted - I didn&#39;t read his book and my knowledge is second-hand. In any event, I would appreciate getting some more detail from you about why you think he looks good.</p><blockquote><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></blockquote><p> I agree that if I were considering two models at those extremes, recent developments would update me more toward the former model. However, I don&#39;t actually disagree with the abstract claim that &quot;things will get more and more interesting the more compute we have access to&quot; - I expect more compute to make things more interesting even in worlds where we can&#39;t get to AGI by scaling compute. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:31:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:31:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible).</p></blockquote><p> A local remark about this: I&#39;ve seen a bunch of reports from other people that GPT-4 is essentially unable to play tic-tac-toe, and this is a shortcoming that was highly surprising to me. Given the amount of impressive things it can otherwise do, failing at playing a simple game whose full solution could well be in its training set is really odd.<br><br> So while I agree 4 seems better than 3, it still has some bizarre weaknesses that I don&#39;t think I understand well. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:34:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:34:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ege: Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily? My sense is that this is just one of these situations where tokenization and one-dimensionality of text makes something hard, but it&#39;s trivial to get the system to learn it if it&#39;s in a more natural representation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:35:17 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:35:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily?</p></blockquote><p><br> <a href="https://twitter.com/liminal_warmth/status/1709654529413992692">This random Twitter person</a> says that it can&#39;t. Disclaimer: haven&#39;t actually checked for myself.</p></div></section><h2> Taking into account government slowdown </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:05:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:05:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:06:58 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:06:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I am guilty of assuming governments and corporations won&#39;t slow things down by more than a year. I think I mostly still endorse this assumption but I&#39;m hopeful that instead they&#39;ll slow things down by several years or more. Historically I&#39;ve been arguing with people who disagreed with me on timelines by decades, not years, so it didn&#39;t seem important to investigate this assumption. That said I&#39;m happy to say why I still mostly stand by it. Especially if it turns out to be an important crux (eg if Ege or Ajeya think that AGI would probably happen by 2030 absent slowdown) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:08:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:08:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> That said I&#39;m happy to say why I still mostly stand by it.</p></blockquote><p> Cool, might be worth investigating later if it turns out to be a crux. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:21:02 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:21:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines.</p></blockquote><p> Responding to habryka: I do think government regulations, companies slowing down because of risks, companies slowing down because they are bad at coordination, capital markets being unable to allocate the large amounts of capital needed for huge training runs for various reasons, etc. could all be important. However, my general heuristic for thinking about the issue is more &quot;there could be a lot of factors I&#39;m missing&quot; and less &quot;I think these specific factors are going to be very important&quot;.<br><br> In terms of the impact of capable AI systems, I would give significantly less than even but still non-negligible odds that these kinds of factors end up limiting the acceleration in economic growth to eg less than an order of magnitude. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:49:39 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:49:39 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p></blockquote><p> I include this in a long tail of &quot;things are just slow&quot; considerations, although in my mind it&#39;s mostly not people making a concerted effort to slow down because of x-risk, but rather just the thing that happens to any sufficiently important technology that has a lot of attention on it: a lot of drags due to the increasing number of stakeholders, both drags where companies are less blase about releasing products because of PR concerns, and drags where governments impose regulations (which I think they would have in any world, with or without the efforts of x-risk-concerned contingent). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:10:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:10:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Slight meta: I am interested in digging in a bit more to find some possible cruxes between Daniel and Ajeya, before going more in-depth between Ajeya and Ege, just to keep the discussion a bit more focused.</p></div></section><h2> Recursive self-improvement and AI&#39;s speeding up R&amp;D </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:21:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:21:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just for my own understanding, you have adjusted the compute-model to account for some amount of R&amp;D speedup as a result of having more AI researchers.</p><p> To what degree does that cover classical recursive self-improvements or things in that space? (Eg AI systems directly modifying their training process or weights or develop their own pre-processing modules?)</p><p> Or do you expect a feedback loop that&#39;s more &quot;AI systems do research that routes through humans understanding those insights and being in the loop on implementing them to improve the AI systems&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:25:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:25:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> When all we had was Ajeya&#39;s model, I had to make my own scrappy guess at how to adjust it to account for R&amp;D acceleration due to pre-AGI systems. Now we have Davidson&#39;s model so I mostly go with that.<br><br> It covers recursive-self-improvement as a special case. I expect that to be what the later, steeper part of the curve looks like (basically a million AutoGPTs running in parallel across several datacenters, doing AI research but 10-100x faster than humans would, with humans watching the whole thing from the sidelines clapping as metrics go up); the earlier part of the curve looks more like &quot;every AGI lab researcher has access to a team of virtual engineers that work at 10x speed and sometimes make dumb mistakes&quot; and then the earliest part of the curve is what we are seeing now with copilot and chatgpt helping engineers move slightly faster. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:37:36 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:37:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</p></blockquote><p> These are entangled updates. If you&#39;re focusing on just &quot;how can you accelerate ML R&amp;D a bunch,&quot; then it seems less important to be able to handle low-feedback-loop environments quite different from the training environment. By far the biggest reason I thought we might need longer horizon training was to imbue the skill of efficiently learning very new things (see <a href="https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.2s3orj7g2t76">here</a> ). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:38:01 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:38:01 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively.</p></blockquote><p> I think this is basically right! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:39:50 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:39:50 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</p></blockquote><p> I&#39;ll give a less lengthy reply here, since structurally it&#39;s very similar to in-context learning, and has the same &quot;agree-qualitatively-but-not-quantitatively&quot; flavor. (For example, I definitely agree that the game is going to be coping with errors and error-correction, not never making errors; we&#39;re talking about whether that will take four years or more than four years.)</p><p> &quot;Not behaving erratically / falling over on super weird or adversarial inputs&quot; is a higher-level-of-abstraction cognitive skill humans are way better at than LLMs. LLMs are improving at this skill with scale (like all skills), and there are ways to address it with schlep and workflow rearrangements (like all problems), and it&#39;s unclear how important it is in the first place. But it&#39;s plausibly fairly important, and it seems like their current level is &quot;not amazing,&quot; and the trend is super unclear but not obviously going to make it in four years.</p><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, market-efficiency-and-schlep) pushes you toward &quot;more than four years from now&quot; — there&#39;s little room for it to push in the other direction. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:42:42 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:42:42 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, pushes you toward &quot;more than four years from now&quot;</p></blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:45:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:45:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about).</p></blockquote><p> I don&#39;t think it&#39;s insane to believe this to be true of 20 years, but I think we have many more examples of big, difficult, society-wide things happening over 20 years than over 4. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:45:40 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:45:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote-working OpenAI engineer that thinks at 10x serial speed. Even if the research scientists are still human, this would speed things up a lot I think. So while I find the abstract arguments about how LLMs are worse at in-context learning etc. than humans plausible, when I think concretely about AI R&amp;D acceleration it still seems like it&#39;s gonna start happening pretty soon, and that makes me also update against the abstract argument a bit. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So, I kind of want to check an assumption. On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p><p> Daniel, could you be more concrete about how a 10x AI engineer actually helps develop AGI? My guess is on a 4-year timescale you don&#39;t expect it to route through semiconductor supply chain improvements.</p><p> And then I want to check what Ajeya thinks here and whether something in this space might be a bit of a crux. My model of Ajeya does indeed think that AI systems in the next few years will be impressive, but not really actually that useful for making AI R&amp;D go better, or at least not like orders of magnitude better. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:48:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:48:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> My best guess is that algorithmic progress has probably continued at a rate of around a doubling of effective compute per year, at least insofar as you buy that one-dimensional model of algorithmic progress. Again, model uncertainty is a significant part of my overall view about this, but I think it&#39;s not accurate to say there hasn&#39;t been much algorithmic progress in the last few years. It&#39;s just significantly slower than the pace at which we&#39;re scaling up compute so it looks relatively less impressive.</p><p> <i>(Daniel, Ajeya +1 this comment)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I was modeling one doubling a year as approximately not very much, compared to all the other dynamics involved, though of course it matters a bunch over the long run. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:56:42 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:56:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Habryka&#39;s excellent point about how maybe engineering isn&#39;t the bottleneck, maybe compute is instead:<br><br> My impression is that roughly half the progress has come from increased compute and the other half from better algorithms. Going forward when I think concretely about the various limitations of current algorithms and pathways to overcome them -- which I am hesitant to go into detail about -- it sure does seem like there are still plenty of low and medium-hanging fruit to pick, and then high-hanging fruit beyond which would take decades for human scientists to get to but which can perhaps be reached much faster during an AI takeoff.<br><br> I am on a capabilities team at OpenAI right now and I think that we could be going something like 10x faster if we had the remote engineer thing I mentioned earlier. And I think this would probably apply across most of OpenAI research. This wouldn&#39;t accelerate our compute acquisition much at all to be clear, but that won&#39;t stop a software singularity from happening. Davidson model backs this up I think -- I&#39;d guess that if you magically change it to keep hardware &amp; compute progress constant, you still get a rapid R&amp;D acceleration, just a somewhat slower one.<br><br> I&#39;d think differently if I thought that parameter count was just Too Damn Low, like I used to think. If I was more excited about the human brain size comparison, I might think that nothing short of 100T parameters (trained according to Chinchilla also) could be AGI, and therefore that even if we had a remote engineer thinking at 10x speed we&#39;d just eat up the low-hanging fruit and then stall while we waited for bigger computers to come online. But I don&#39;t think that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:56:54 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:56:54 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> I think there haven&#39;t been flashy paradigm-shifting insights, but I strongly suspect each half-GPT was a hard-won effort on a lot of fronts, including both a lot of mundane architecture improvements (like implementing long contexts in less naive ways that don&#39;t incur quadratic cost), a lot of engineering to do the model parallelism and other BS that is required to train bigger models without taking huge GPU utilization hits, and a lot of post-training improvements to make usable nice products. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:58:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:58:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p><p> The context length and GPU utilization thing feels most relevant. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:59:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:59:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p></blockquote><p> Yeah, TBC, I think there&#39;s a higher bar than Daniel thinks there is to speeding stuff up 10x for reasons like this. I do think that there&#39;s algorithm juice, like Daniel says, but I don&#39;t think that a system you look at and naively think &quot;wow this is basically doing OAI ML engineer-like things&quot; will actually lead to a full 10x speedup; 10x is a lot.</p><p> I think you will eventually get the 10x, and then the 100x, but I&#39;m picturing that happening after some ramp-up where the first ML-engineer-like systems get integrated into workflows, improve themselves, change workflows to make better use of themselves, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:53:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:53:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach.</p></blockquote><p> Agree this is the strongest candidate for crazy impacts soon, which is why my two updates of &quot;maybe meta-learning isn&#39;t that important and therefore long horizon training isn&#39;t as plausibly necessary&quot; and &quot;maybe I should just be obsessed with forecasting when we have the ML-research-engineer-replacing system because after that point progress is very fast&quot; are heavily entangled. <i><u>(Daniel reacts &quot;+1&quot; to this)</u></i></p><blockquote><p> -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote OpenAI engineer that thinks at 10x serial speed</p></blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:02:59 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:02:59 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me?</p></blockquote><p> I disagree with this update -- I think the update should be &quot;it takes a lot of schlep and time for the kinks to be worked out and for products to find market fit&quot; rather than &quot;the systems aren&#39;t actually capable of this.&quot; Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.<br><br> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:05:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:05:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.</p></blockquote><p><br> I&#39;m skeptical we would get 2 OOMs or more with just the current capabilities of AI systems, but I think even if you accept that, scaling from $1B/yr to $100B/yr is easier than from $100B/yr to $10T/yr. Accelerating AI R&amp;D by 2x seems more like the second change to me, or even bigger than that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.)</p></blockquote><p> I agree with this </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yeah idk I pulled that out of my ass, maybe ​2 OOM is more like the upper limit given how much value there already is. I agree that going from X to 10X is easier than going from 10X to 100X, in general. I don&#39;t think that undermines my point though. I disagree with your claim that making AI progress go 2x faster is more like scaling from $100B to $10T-- I think it depends on when it happens! Right now in our state of massive overhang and low-hanging-fruit everywhere, making AI progress go 2x faster is easy.<br><br> Also to clarify when I said 10x faster I meant 10x faster algorithmic progress; compute progress won&#39;t accelerate by 10x obviously. But what this means is that I think we&#39;ll transition from a world where half or more of the progress is coming from scaling compute, to a world where most of the progress is coming from algorithmic improvements / pushing-through-schlep.</p></div></section><h2> Do we expect transformative AI pre-overhang or post-overhang? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:01:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:01:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think a hypothesis I have for a possible crux for a lot of the disagreement between Daniel and Ajeya is something like &quot;will we reach AGI before the compute overhang is over vs. after?&quot;.</p><p> Like, in as much as we think we are in a compute-overhang situation, there is an extremization that applies to people&#39;s timelines where if you we&#39;ll get there using just remaining capital and compute, you expect quite short timelines, but if you expect it will require faster chips or substantial algorithmic improvements, you expect longer, and with less probability-mass in-between.</p><p> Curious about Daniel and Ajeya answering the question of &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:05:48 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:05:48 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot;</p></blockquote><p> I think there are different extremities of compute overhang. The most extreme one which will be exhausted most quickly is like &quot;previously these companies were training AI systems on what is essentially chump change, and now we&#39;re starting to get into a world where it&#39;s real money, and soon it will be really real money.&quot; I think within 3-4 years we&#39;ll be talking tens of billions for a training run; I think the probability we get drop-in replacements for 99% remotable jobs (regardless of whether we&#39;ve rolled those drop-in replacements out everywhere) by then is something like...25%?</p><p> And then after that progress is still pretty compute-centric, but it moves slower because you&#39;re spending very real amounts of money, and you&#39;re impacting the entire supply chain: you need to build more datacenters which come with new engineering challenges, more chip-printing facilities, more fabs, more fab equipment manufacturing plans, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:08:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:08:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing.</p></blockquote><p> Cool, that makes sense. That does seem like it might exaggerate the perceived disagreements between the two of you, when you just look at the graphs, though it&#39;s of course still highly decision-relevant to dig deeper into whether this is true or not.</p></div></section><h2> Hofstadter&#39;s law in AGI forecasting </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:47 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> TBC Daniel, I think we differ by a factor of 2 on the probability for your median scenario. I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:13:44 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:13:44 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law.</p></blockquote><p> I think this also applies to the disagreement between me and Ajeya. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:16:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:16:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> A thing that would change my mind is if I found other scenarios more plausible. Wanna sketch some?<br><br> Regarding Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot;正确的？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:17:53 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:17:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Re: Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot;正确的？</p></blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:26:13 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:26:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update.</p></blockquote><p>出色的。 So my reason for doing the additive method is that I think Hofstadter&#39;s law / schlep / etc. is basically the planning fallacy, and it applies when your forecast is based primarily on imagining a series of steps being implemented. It does NOT apply when your forecast is based primarily on extrapolating trends. Like, you wouldn&#39;t look at a graph of exponential progress in Moore&#39;s law or solar power or whatever and then be like &quot;but to account for Hofstadter&#39;s Law I will assume things take twice as long as I expect, therefore instead of extrapolating the trend-line straight I will cut its slope by half.&quot;<br><br> And when it comes to AGI timelines, I think that the shorter-timeline scenarios look more subject to the planning fallacy, whereas the longer-timeline scenarios look more like extrapolating trends.<br><br> So in a sense I&#39;m doing the multiplicative method, but only on the shorter worlds. Like, when I say 2027 as my median, that&#39;s kinda because I can actually quite easily see it happening in 2025, but things take longer than I expect, so I double it... I&#39;m open to being convinced that I&#39;m not taking this into account enough and should shift my timelines back a few years more; however I find it very implausible that I should add eg 15 years to my median because of this.</p></div></section><h2> Summary of where we are at so far and exploring additional directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:21:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:21:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> We&#39;ve been going for a while and it might make sense to take a short step back. Let me try to summarize where we are at:</p><p> We&#39;ve been mostly focusing on the disagreement between Ajeya and Daniel. It seems like one core theme in the discussion has been the degree to which &quot;reality has a lot of detail and kinks need to be figured out before AI systems are actually useful&quot;. Ajeya currently thinks that while it is true that AGI companies will have access to these tools earlier, there still will be a lot of stuff to figure out before you actually have a system equivalent to a current OAI engineer. Daniel made a similar update in noticing a larger-than-he-expected delay in the transition from &quot;having all the stuff necessary to make a more capably system, like architecture, compute, training setup&quot; and &quot;actually producing a more capable system&quot;.</p><p> However, it&#39;s also not clear how much this actually explains the differences in the timelines for the two of you.</p><p> We briefly touched on compute overhangs being a thing that&#39;s very relevant to both of your distributions, in that Daniel assigns substantially higher probability to a very high R&amp;D speed-up before the current overhang is exhausted, which pushes his probability mass a bunch earlier. And correspondingly Ajeya&#39;s timelines are pretty sensitive to relatively small changes in compute requirements on the margin, since that would push a bunch of probability mass into the pre-overhang world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:21:30 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:21:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I&#39;ll put in a meta note here that I think it&#39;s pretty challenging to argue about a 25% vs a 50% on the Daniel scenario, that is literally one bit of evidence one of us sees that the other doesn&#39;t. It seems like Daniel thinks I need stronger arguments/evidence than I have to be at 25% instead of 50%, but it&#39;s easy to find one bit somewhere and hard to argue about whether it really is one bit.</p></div></section><h2> Exploring conversational directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> In case interested, here are some possible conversation topics/starters:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)<br><br> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.<br><br> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?<br><br> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:25:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:25:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On Daniel&#39;s four topics:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)</p></blockquote><p> I suspect I&#39;ll be like &quot;Yep, seems plausible, and my probability on it coming to pass is 2-5x smaller.&quot;</p><blockquote><p> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.</p></blockquote><p> I could do this if people thought it would be useful.</p><blockquote><p> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?</p></blockquote><p> I like this blog post but I feel like it&#39;s quite tame compared to what both Daniel and I think is plausible so not sure if it&#39;s the best thing to anchor on.</p><blockquote><p> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like.</p></blockquote><p> I can do this if people thought it would be useful.</p></div></section><h2> Ege&#39;s median world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:25:39 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:25:39 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> My median world looks something like this: we keep scaling compute until we hit training runs at a size of 1e28 to 1e30 FLOP in maybe 5 to 10 years, and after that scaling becomes increasingly difficult because of us running up against supply constraints. Software progress continues but slows down along with compute scaling. However, the overall economic impact of AI continues to grow: we have individual AI labs in 10 years that might be doing on the order of eg $30B/yr in revenue.<br><br> We also get more impressive capabilities: maybe AI systems can get gold on the IMO in five years, we get more reliable image generation, GPT-N can handle more complicated kinds of coding tasks without making mistakes, stuff like that. So in 10 years AI systems are just pretty valuable economically, but I expect the AI industry to look more like today&#39;s tech industry - valuable but not economically transformative.<br><br> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy. However, I would not be surprised by a mild acceleration of overall economic growth driven by the impact of AI. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:28:51 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:28:51 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy.</p></blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:32:48 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:32:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p></blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p><blockquote><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously?</p></blockquote><p> I think that&#39;s an important part of it, yes. I expect the systems we&#39;ll have in 10 years will be really good at some things with some bizarre failure modes and domains where they lack competence. My example of GPT-4 not being able to play tic-tac-toe is rather anecdotal, but I would worry about other things of a similar nature when we actually want these systems to replace humans throughout the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p></blockquote><p> Interestingly, I think in the case of ems this is more plausible than in the case of normal AGI. Because normal AGI will be more easily extendible to superhuman levels. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:35:19 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:35:19 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> FWIW I think the kind of AGI you and I are imagining as the most plausible first AGI is pretty janky, and the main way I see it improving stuff is by doing normal ML R&amp;D, not galaxy-brained &quot;editing its own source code by hand&quot; stuff. The normal AI R&amp;D could be done by all the ems too.</p><p> (It depends on where the AI is at when you imagine dropping ems into the scenario.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p>我同意这一点。 The jankiness is a point in my favor, because it means there&#39;s lots of room to grow by ironing out the kinks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Overall Ege, thanks for writing that scenario! Here are some questions / requests for elaboration:<br><br> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?</p><p> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:39:56 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:39:56 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s remote OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?<br><br> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then?</p></blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering that gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.<br><br> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.<br><br> (3): I don&#39;t think anything notable would happen. I don&#39;t believe the AI systems of 2030 will be capable enough to manage an AI lab. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:43:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:43:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I think Ege&#39;s median world is plausible, just like Daniel&#39;s median world; I think my probability on &quot;Ege world or more chill than that&quot; is lower than my probability on &quot;Daniel world or less chill than that.&quot; Earlier I said 25% on Daniel-or-crazier, I think I&#39;m at 15% on Ege-or-less-crazy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:46:08 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:46:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: the &quot;fuck it&quot; scenario: What I&#39;m interested in here is what skills you think the system would be lacking, that would make it fail. Like right now for example we had a baby version of this with ChaosGPT4, which lacked strategic judgment and also had a very high mistakes-to-ability-to-recover-from-mistakes ratio, and also started from a bad position (being constantly monitored, zero human allies). So all it did was make some hilarious tweets and get shut down. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:46:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:46:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?</p><p> Eg suppose some AI system was trained to learn new video games: each RL episode was it being shown a video game it had never seen, and it&#39;s supposed to try to play it; its reward is the score it gets. Then after training this system, you show it a whole new <i>type</i> of video game it has never seen (maybe it was trained on platformers and point-and-click adventures and visual novels, and now you show it a first-person-shooter for the first time). Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:47:16 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:47:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?<br></p></blockquote><p>是的。</p><blockquote><p> Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines?</p></blockquote><p> I would probably update substantially towards agreeing with you. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:49:01 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:49:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering which gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.</p></blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:51:56 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:51:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.</p></blockquote><p> How much do you think they&#39;ll be automating/speeding things up? Can you give an example of a coding task such that, if AIs can do that coding task by, say, 2025, you&#39;ll update significantly towards shorter timelines, on the grounds that they are by 2025 doing things you didn&#39;t expect to be doable by 2030?<br><br> (My position is that all of these deficiencies exist in current systems but (a) will rapidly diminish over the next few years and (b) aren&#39;t strong blockers to progress anyway, eg even if they don&#39;t have good research taste they can still speed things up substantially just by doing the engineering and cutting through the schlep) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:54:49 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:54:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3?</p></blockquote><p> I don&#39;t think it&#39;s going to take ~30 (really 40 per the distribution I submitted) years after 2030, that&#39;s just my median. I think there&#39;s a 1/3 chance it takes more than 75 and 1/5 chance it takes more than 175.<br><br> If you&#39;re asking me to justify why my median is around 2065, I think this is not really that easy to do as I&#39;m essentially just expressing the betting odds I would accept based on intuition.<br><br> Formalizing it is tricky, but I think I could say I don&#39;t find it that plausible the problem of building AI is so hard that we won&#39;t be able to do it even after 300 years of hardware and software progress. Just the massive scaling up of compute we could get from hardware progress and economic growth over that kind of timescale would enable things that look pretty infeasible over the next 20 or 30 years.</p></div></section><h2> Far-off-distribution transfer </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:47:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:47:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The Ege/Ajeya point about far-off-distribution transfer seem like an interesting maybe-crux, so let&#39;s go into that for a bit.</p><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:48:16 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:48:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, FWIW I think the savannah-to-boardroom transfer stuff is probably underlying past-Eliezer (not sure about current Eliezer) and also a lot of &quot;stochastic parrot&quot;-style skeptics. I think it&#39;s a good point under-discussed by the short timelines crowd, though I don&#39;t think it&#39;s decisive. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:49:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:49:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade?</p></blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no. I just think it&#39;ll take a lot of hard work to make up for the weaknesses of not having transfer this good. Paul has a good unpublished Google doc titled &quot;Doing without transfer.&quot; I think by the time systems are transformative enough to massively accelerate AI R&amp;D, they will still not be that close to savannah-to-boardroom level transfer, but it will be fine because they will be trained on exactly what we wanted them to do for us. (This btw also underlies some lower-risk-level intuitions I have relative to MIRI crowd.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:51:05 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:51:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no.</p></blockquote><p> Oh, huh, that is really surprising to me. But good to have that clarified. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:52:00 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:52:00 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, I just think the way we get our OAI-engineer-replacing-thingie is going to be radically different cognitively than human OAI-engineers, in that it will have coding instincts honed through ancestral memory the way grizzly bears have salmon-catching instincts baked into them through their ancestral memory. For example, if you give it a body, I don&#39;t think it&#39;d learn super quickly to catch antelope in the savannah, the way a baby human caveperson could learn to code if you transported them to today.</p><p> But it&#39;s salient to me that this might just leave a bunch of awkward gaps, since we&#39;re trying to make do with systems holistically less intelligent than humans, but just more specialized to coding, writing, and so on. This is why I think the Ege world is plausible.</p><p> I also dislike using the term AGI for this reason. (Or rather, I think there is a thing people have in mind by AGI which makes sense, but it will come deep into the Singularity, after the earlier transformative AI systems that are not AGI-in-this-sense.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:57:19 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:57:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I also dislike using the term AGI for this reason.</p></blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:58:52 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:58:52 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy.</p></blockquote><p> Yes, I&#39;m very sympathetic to this kind of thing, which is why I like TAI (and it&#39;s related to the fact that I think we&#39;ll first have grizzly-bears-of-coding, not generally-intelligent-beings). But it bites much less in my view because it&#39;s all much more compressed and there&#39;s a pretty shortish period of a few years where all plausible things people could mean by AGI are achieved, including the algorithm that has savannah-to-boardroom-level transfer.</p></div></section><h2> A concrete scenario &amp; where its surprises are </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:59:49 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:59:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> We can delete this hook later if no one bites, but in case someone does, here&#39;s a scenario I think it would be productive to discuss:<br><br> (1) Q1 2024: A bigger, better model than GPT-4 is released by some lab. It&#39;s multimodal; it can take a screenshot as input and output not just tokens but keystrokes and mouseclicks and images. Just like with GPT-4 vs. GPT-3.5 vs. GPT-3, it turns out to have new emergent capabilities. Everything GPT-4 can do, it can do better, but there are also some qualitatively new things that it can do (though not super reliably) that GPT-4 couldn&#39;t do.</p><p> (2) Q3 2024: Said model is fine-tuned to be an agent. It was already better at being strapped into an AutoGPT harness than GPT-4 was, so it was already useful for some things, but now it&#39;s being trained on tons of data to be a general-purpose assistant agent. Lots of people are raving about it. It&#39;s like another ChatGPT moment; people are using it for all the things they used ChatGPT for but then also a bunch more stuff. Unlike ChatGPT you can just leave it running in the background, working away at some problem or task for you. It can write docs and edit them and fact-check them; it can write code and then debug it.</p><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p><p> (4) Q3 2025: OK now things are getting serious. The kinks have generally been worked out. This newer model is being continually trained on oodles of data from a huge base of customers; they have it do all sorts of tasks and it tries and sometimes fails and sometimes succeeds and is trained to succeed more often. Gradually the set of tasks it can do reliably expands, over the course of a few months. It doesn&#39;t seem to top out; progress is sorta continuous now -- even as the new year comes, there&#39;s no plateauing, the system just keeps learning new skills as the training data accumulates. Now many millions of people are basically treating it like a coworker and virtual assistant. People are giving it their passwords and such and letting it handle life admin tasks for them, help with shopping, etc. and of course quite a lot of code is being written by it. Researchers at big AGI labs swear by it, and rumor is that the next version of the system, which is already beginning training, won&#39;t be released to the public because the lab won&#39;t want their competitors to have access to it. Already there are claims that typical researchers and engineers at AGI labs are approximately doubled in productivity, because they mostly have to just oversee and manage and debug the lightning-fast labor of their AI assistant. And it&#39;s continually getting better at doing said debugging itself.</p><p> (5) Q1 2026: The next version comes online. It is released, but it refuses to help with ML research. Leaks indicate that it doesn&#39;t refuse to help with ML research internally, and in fact is heavily automating the process at its parent corporation. It&#39;s basically doing all the work by itself; the humans are basically just watching the metrics go up and making suggestions and trying to understand the new experiments it&#39;s running and architectures it&#39;s proposing.</p><p> (6) Q3 2026 Superintelligent AGI happens, by whatever definition is your favorite. And you see it with your own eyes.<br><br> <strong>Question:</strong> Suppose this scenario happens. What does your credence in &quot;AGI by 2027&quot; look like at each of the 6 stages? Eg what are the biggest updates, and why?<br><br> My own first-pass unconfident answer is:<br> 0 -- 50%<br> 1 -- 50%<br> 2 -- 65%<br> 3 -- 70%<br> 4 -- 90%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p></blockquote><p> I don&#39;t know how much I care about this (not zero), but I think someone with Ege&#39;s views should care a lot about how it was trained. Was it trained on a whole bunch of very similar board games and video games? How much of a distance of transfer is this, if savannah to boardroom is 100? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> FWIW I interpreted this literally: we have some bigger model like chatgpt that can play some games decently on the first try, and conditional on (2) my median world has those games being mostly stuff similar to what it&#39;s seen before<br><br> so i&#39;m not assuming much evidence of transfer from (2), only some mild amount </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:03:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:03:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, let&#39;s briefly have people try to give probability estimates here, though my model of Ege feels like the first few stages have a ton of ambiguity in their operationalization, which will make it hard to answer in concrete probabilities. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> +1, I also find the ambiguity makes answering this hard</p><p> I&#39;ll wait for Ege to answer first. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Re: Daniel, according to my best interpretation of his steps:<br><br> 0 -- 6%<br> 1 -- 6%<br> 2 -- 12%<br> 3 -- 15%<br> 4 -- 30%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:11:03 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:11:03 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay here&#39;s my answer:</p><p> 0 -- 20%<br> 1 -- 28%<br> 2 -- 37%<br> 3 -- 50%<br> 4 -- 75%<br> 5 -- 87%<br> 6 -- 100%</p><p> My updates are spread out pretty evenly because the whole scenario seems qualitatively quite plausible and most of my uncertainty is simply whether it will take more scale or more schlep at each stage than is laid out here (including stuff like making it more reliable for a combo of PR and regulation and usable-product reasons). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Thanks both! I am excited about this for a few reasons. One I think it might help to focus the discussion on the parts of the story that are biggest updates for you (and also on the parts that are importantly ambiguous! I&#39;m curious to hear about those!) and two, because as the next three years unfold, we&#39;ll be able to compare what happens to this scenario. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> unfortunately i think the scenarios are vague enough that as a practical matter it will be tricky to adjudicate or decide whether they&#39;ve happened or not </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree, but I still think it&#39;s worthwhile to do this. Also this was just a hastily written scenario, I&#39;d love to improve it and make it more precise, and I&#39;m all ears for suggestions! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:13:11 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:13:11 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, I&#39;m surprised you&#39;re at 95% at stage 5, given that stage 5&#39;s description is just that AI is doing a lot of AI R&amp;D and leaks suggest it&#39;s going fast. If your previous timelines were several decades, then I&#39;d think even with non-god-like AI systems speeding up R&amp;D it should take like a decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think once you&#39;re at step 5 it&#39;s overwhelmingly likely that you already have AGI. The key sentence for me is &quot;it&#39;s basically doing all the work by itself&quot; - I have a hard time imagining worlds where an AI can do basically all of the work of running an AI lab by itself but AGI has still not been achieved.<br><br> If the AI&#39;s role is more limited than this, then my update from 4 to 5 would be much smaller. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:17:04 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:17:04 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I thought Daniel said it was doing all the ML R&amp;D by itself, and the humans were managing it (the AIs are in the role of ICs and the humans are in the role of managers at a tech company). I don&#39;t think it&#39;s obvious that just because some AI systems can pretty autonomously do ML R&amp;D, they can pretty autonomously do everything, and I would have expected your view to agree with me more there. Though maybe you think that if it&#39;s doing ML R&amp;D autonomously, it must have intense transfer / in-context-learning and so it&#39;s almost definitely across-the-board superhuman? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:19:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:19:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> If it&#39;s only doing the R&amp;D then I would be lower than 95%, and the exact probability I give for AGI just depends on what that is supposed to mean. That&#39;s an important ambiguity in the operationalization Daniel gives, in my opinion.<br><br> In particular, if you have a system that can somehow basically automate AI R&amp;D but is unable to take over the other tasks involved in running an AI lab, that&#39;s something I don&#39;t expect and would push me far below the 95% forecast I provided above. In this case, I might only update upwards by some small amount based on (4) ->; (5), or maybe not at all.</p></div></section><h2> Overall summary, takeaways and next steps </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:36:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:36:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is a summary of the discussion so far:</p><p> Daniel made an argument against Hofstadter&#39;s law for trend extrapolation and we discussed the validity of that for a bit.</p><p> A key thing that has come up as an interesting crux/observation is that Ege and Ajeya both don&#39;t expect a massive increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making grizzly-bear-like-AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</p><p> This makes observing substantial transfer learning a pretty relevant crux for Ege and Ajeya in the next few years/decades. Ege says he&#39;d have timelines more similar to Ajeya&#39;s if he observed this.</p><p> Daniel and Ajeya both think that the most plausible scenario is grizzly-bear-like AI with subhuman transfer but human-level or superhuman ML engineering skills, but while Daniel thinks it&#39;ll be relatively fast to work with the grizzly-bear-AIs to massively accelerate R&amp;D, Ajeya thinks that the lower-than-human level &quot;general intelligence&quot; / &quot;transfer&quot; will be a hindrance in a number of little ways, making her think it&#39;s plausible we&#39;ll need bigger models and/or more schlep. If Ajeya saw extreme transfer work out, she&#39;d update more toward thinking everything will be fast and easy, and thus have Daniel-like timelines (even though Daniel himself doesn&#39;t consider extreme transfer to be a crux for him.)</p><p> Daniel and Ege tried to elicit what concretely Ege expects to happen over the coming decades when AI progress continues but doesn&#39;t end up that transformative. Ege expects that AI will have a large effect on the economy, but assigns a substantial amount of probability on persistent deficiencies that prevent it from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</p><p> <i>(Ajeya, Daniel and Ege all thumbs-up this summary)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:37:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:37:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay thanks everyone, heading out! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:37:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:37:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you Ajeya! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:38:04 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:38:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yes thanks Ajeya Ege and Oliver! Super fun. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:42:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:42:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thinking about future discussions on this topic, I think putting probabilities on the scenario that Daniel outlined was a bit hard given the limited time we had, but I quite like the idea of doing a more parallelized and symmetric version of this kind of thing where multiple participants output a concrete sequence of events, and then have other people forecast how they would update on each of those observations, which does seem like a fun way to elicit disagreements and cruxes.</p></div></section><div></div><br/><br/><a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines<guid ispermalink="false"> K2D45BNxnZjdpSX2j</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 10 Nov 2023 05:28:24 GMT</pubDate></item></channel></rss>