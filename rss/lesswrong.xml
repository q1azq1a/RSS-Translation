<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 24 日星期日 08:13:41 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The Sugar Alignment Problem]]></title><description><![CDATA[Published on December 24, 2023 1:35 AM GMT<br/><br/><p><i>在一个刚刚发现糖的平行世界里。</i></p><p>爱丽丝：嘿，你们听说过他们发现的让食物味道更好的新东西吗？</p><p>鲍勃：是的，我想是的。是粉状的，白色的，对吗？</p><p>爱丽丝：是的。</p><p>卡罗尔：它实际上是颗粒状的，就像微小的鹅卵石。它被称为糖。</p><p>鲍勃：啊，好的。</p><p>卡罗尔：这与多年前关于鲜味的发现很相似。你知道，味觉的一个方面，我们认为事物是美味的和“肉味的”。</p><p>爱丽丝：哦，我记得了。我喜欢它。我现在很喜欢营养酵母。还有其他“鲜味炸弹”，如酱油和味噌酱。</p><p>鲍勃：是的，我也是。我很高兴能用糖做同样的事情。到目前为止，我每天早上都会将它添加到我的咖啡中，效果非常棒。它给了它这种有趣的小刺激。它也能淡化咖啡的苦味，但以一种很好的方式补充了咖啡的苦味。并且以与牛奶不同的方式。这很微妙。这太好了。</p><p>爱丽丝：哦，是的，当然。我喜欢把它加在茶里。</p><p>鲍勃：除了可能有点违反直觉的事情之外，它也很棒。就像在爆米花上面一样。旁边的黄油和盐可能看起来很奇怪，但我不知道，我认为它非常好。</p><p>卡罗尔：这是有道理的。我听说在一些亚洲国家他们会在炒饭和其他炒菜上撒一些。</p><p>爱丽丝：嗯，这实际上听起来有点奇怪。炒饭的那种糖味？</p><p>卡罗尔：是的，也许吧。我正在观看有关它的 YouTube 视频，那家伙说它是如何更加微妙的添加的。就像，你并没有真正尝到糖本身的味道，但炒饭有一些不同的<i>东西</i>，很难用手指来形容，但味道真的很好。</p><p>爱丽丝：啊，我明白了。</p><p>鲍勃：但我也听说他们正在尝试在某些东西中添加<i>大量</i>糖。好像有一种东西叫冰淇淋。它基本上是酸奶，但里面含有大量的糖，而且更冷，几乎是冷冻的。前几天我尝试了一下，效果<i>非常</i>好。我再也不敢喝酸奶了</p><p>丹：这是件好事吗？</p><p>爱丽丝：哦。嘿丹。也很高兴见到你。</p><p>丹：抱歉，我刚进来，但我听说了糖和冰淇淋的事，我有点担心。</p><p>爱丽丝：担心吗？担心什么意思？我没有听说有任何负面影响。它应该只是让食物味道更好。</p><p>丹：你做过研究吗？</p><p>爱丽丝：研究？</p><p>鲍勃：我认为丹的意思是，如果你仔细观察，也许在食物中添加糖有一些缺点。我听说它会让你在事后感到昏昏欲睡。这似乎取决于您的年龄和健康水平。年龄较大、健康状况较差的人在食用添加糖的食物后一个小时左右可能会感到有点懒惰。但它不会持续太久。对于像我们这样年轻健康的人来说，应该没问题。</p><p>爱丽丝：嗯。我经常在茶中加入它，但从来没有感到任何昏昏欲睡的感觉。</p><p>鲍勃：是的，只有当你拥有很多东西时才会发生这种情况。早上将其添加到茶中应该完全没有问题。但就像冰淇淋一样，如果你吃了很多，吃完之后你可能会感到有点昏昏欲睡。</p><p>爱丽丝：嗯，好的。听起来不错。</p><p>卡罗尔：我确实认为丹提出了一个很好的观点。重要的是要考虑我们放入体内的东西的副作用。</p><p>爱丽丝：完全可以。这还算公平。</p><p>丹：副作用是我想到的一部分，但是是的。</p><p>鲍勃：哦。你还想什么？</p><hr><p>丹：嗯，我真的不知道。我只知道这是一个新发现——甚至可以称之为新技术——而且我们很清楚，技术既可以用来做好事，也可以用来做坏事。</p><p>爱丽丝：哦，又是这个。</p><p>卡罗尔：哈哈，我听到了，丹，但我认为我们都比你更信任和开放新技术。</p><p>鲍勃：是的，伙计。你甚至没有智能手机。或任何社交媒体帐户。</p><p>丹：但你确实同意技术可以用来做好事，也可以用来做坏事，对吧？</p><p>鲍勃：当然。</p><p>卡罗尔：是的，我想任何人都会同意这一点。</p><p>爱丽丝：我是说，我想。但就像很多时候，缺点基本上可以忽略不计。就像当我打开手机调出谷歌地图时，我知道你认为它会阻止我发展空间智能或其他什么，但我不知道，我只是认为这对我来说是一件不重要的事情，它本质上是“循环”为零”。</p><p>丹：当谷歌地图变得无处不在——它已经做到了——并且拥有几乎每个人在哪里以及每个人在任何时候都要去哪里的数据时，会发生什么？</p><p>爱丽丝：我的意思是，我个人并不关心。</p><p>鲍勃：不过，这是一个有效的观点。即使你不关心爱丽丝，其他人也会关心。即使对于你来说，当谷歌将这些数据出售给一些在第一次与你约会时出现并且碰巧知道你每周四下午去 Tea My Tea 的怪人时，会发生什么。</p><p>爱丽丝：好吧，是的……那会很令人毛骨悚然。</p><p>卡罗尔：更不用说政府间谍活动了。</p><p>爱丽丝：政府间谍活动？啊？</p><p>卡罗尔：我听说过一些奇怪的东西，比如“预测性警务”，如果你在城镇的“贫民窟”地区呆了太多时间，警察可能会获得这些信息，下次对你就不那么仁慈了你被拦在路上了。</p><p>爱丽丝：天啊。这绝对是太糟糕了。</p><p>丹：哦，是的。它还可以出售给潜在的雇主。我听说过一些“隐形初创公司”，他们基本上会获取这些数据，将其包装起来并在其上系上蝴蝶结，然后出售这些“人力资源招股说明书”报告，这些报告根据该位置预测求职者的“价值”来自谷歌地图的数据。</p><p>卡罗尔：天哪。但是……他们是如何推断出这一点的呢？他们如何将位置数据映射到工作绩效？他们甚至可以访问哪些工作绩效数据？</p><p>丹：这些都是非常好的问题，卡罗尔。</p><p>爱丽丝：这看起来确实有点粗略。</p><p>鲍勃：是的。如果你们不介意的话，让我们把话题带回糖吧。现在我们已经讨论了所有这些并了解了这个背景，我有兴趣重新评估我对糖的看法。你们有什么感想？</p><p><i>大家都点头。</i></p><hr><p>鲍勃：好的。所以，嗯，我想让我们看看我们是否可以考虑一下 Dan 所说的，关于技术如何被用来做好事和做坏事。我们已经看到了许多糖的用途。那么它如何被用来做坏事呢？</p><p>爱丽丝：这对我来说很有意义。虽然我只是不明白它如何被用来做坏事。它只是撒在食物上的东西。谷歌地图和我得到的位置数据。就像，这是更严重的事情，我知道你可以如何用它来做坏事。但不是糖。</p><p>卡罗尔：嗯，没那么快。我回想起鲍勃之前说过的，他再也不会吃酸奶了，而是用这种新的冰淇淋来代替。</p><p>丹：天啊。</p><p>鲍勃：这是“冰淇淋”。但是，是的...现在我正在重新考虑这一点。我以前每天早上早餐都吃酸奶。如果我用冰淇淋这样做，我不知道，也许这会对我的健康或其他方面产生长期不良影响。</p><p>卡罗尔：这看起来确实有可能。我们确实知道，从长远来看，反式脂肪等物质对您的健康确实非常有害，尽管没有立即的副作用。这只是几十年后才会出现的事情。</p><p>爱丽丝：嗯。好的。我看到。但是，就像反式脂肪一样，政府研究后发现它不好，并与公众进行了沟通，人们避免食用含有反式脂肪的食物，政府甚至在学校等地方禁止使用反式脂肪。我们都很好。</p><p>鲍勃：是的。有时我们发现新技术会产生不良影响，并作为一个社会成功地避免它们，但这并不总是发生。</p><p>丹：讲道吧，兄弟。</p><p>鲍勃：等等，我想我明白你在那里做了什么。宗教？</p><p>丹：是的。</p><p>爱丽丝：<i>叹息。</i>好吧，我想你对那个丹的看法是对的。宗教不是很好……但它已经广泛传播了数千年。</p><p>卡罗尔：那么，丹：对于糖，我们该怎么办？</p><hr><p>丹：嗯，我喜欢摩门教徒采取的方法。</p><p>爱丽丝：什么？！所以你希望我们拒绝所有现代技术？</p><p>卡罗尔：是的，我同意爱丽丝这一点。这似乎有点极端。我确信当你需要抗生素时你就不太像摩门教徒了。</p><p>丹：我认为你不明白摩门教的做法是什么。他们并不拒绝采用技术。他们只是非常缓慢、非常小心地这样做。比西方世界慢得多。</p><p>卡罗尔：这似乎不合理。我们可以进行实验。做科学。我们不必等待数十年才能获得有关影响的良好数据。</p><p> Dan：当然，对于一阶效应。但二阶效应又如何呢？三阶？四阶？我对我们预测这些事情的能力不太有信心。我感觉更舒服地观察事情在实践中的<i>实际</i>表现。但这既不在这里也不在那里。这里存在一个难以克服的协调问题。</p><p>鲍勃：嗯？</p><p>丹：假设我被任命为美国总统或其他什么东西，并采用摩门教的方法，非常非常缓慢地采用糖的使用。比如说，50 多年了。这不会阻止其他国家采用它。</p><p>鲍勃：然后会发生什么？</p><p> Dan：其他国家也采用它。人们看到它有多美味。他们每天早上都吃冰淇淋当早餐。美国人去欧洲度假，每天早上也吃冰淇淋作为早餐。他们回家后告诉朋友这件事。消息传开。民众需要糖。</p><p>鲍勃：嗯，在这个假设中你是总统。只要告诉他们不。</p><p>丹：我的四年任期结束后会发生什么？</p><p>鲍勃：哦，是的。好吧，也许凭借你所有的政治权力，你可以让其他政客相信糖是多么危险，并说服他们所有人也同意对糖施加这些限制。</p><p>丹：当有异议时会发生什么？</p><p>鲍勃：持不同政见者？</p><p>丹：是的。假设我说服所有政客都这样做，然后一个人出现并参加竞选活动，承诺让美国人获得他们应得的糖。</p><p>鲍勃：嗯，我明白了。我想让每个人都参与进来是很困难的。</p><p>丹：没错。</p><p>爱丽丝：这对我来说很有意义。即使这在美国有效，其他国家又如何呢？如果糖真的这么好吃，即使美国成功禁止它，人们也许也会搬到其他有糖的国家。毕竟，当政府在禁酒令期间试图禁酒时，美国人相当不满。</p><p>卡罗尔：是的，我也有道理。至少，这只是一场巨大的艰苦战斗。现在我在想，对于我和我的科学家朋友来说，发表我们发现的一切是多么明智。谁敢说这些发现会产生积极的影响？</p><p>丹：也许吧。对我来说，最有希望的途径是“调整”糖。研究它到底是什么，并对其进行设计，以确保它将带来净积极作用，而不是净损害。</p><br/><br/><a href="https://www.lesswrong.com/posts/MmreefkH8cRSky4RE/the-sugar-alignment-problem#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/MmreefkH8cRSky4RE/the-sugar-alignment-problem<guid ispermalink="false"> MmreefkH8cRSky4RE</guid><dc:creator><![CDATA[Adam Zerner]]></dc:creator><pubDate> Sun, 24 Dec 2023 01:35:20 GMT</pubDate> </item><item><title><![CDATA[A Crisper Explanation of Simulacrum Levels]]></title><description><![CDATA[Published on December 23, 2023 10:13 PM GMT<br/><br/><p>我读过之前关于<a href="https://www.lesswrong.com/tag/simulacrum-levels">Simulacrum Levels</a>的文章，并且我看到人们对它们的工作方式表达了一些困惑。当我第一次遇到这个概念时，我自己也有过一些困惑，我认为它们是由于定义不够<i>清晰</i>造成的。</p><p>现有的解释似乎并没有为拟像级别如何存在提供适当的自下而上/基本面优先的机制。为什么他们有自己的特定特征和怪癖，而不是其他任何特征和怪癖？为什么赋予它们的形式是它们<i>不可避免的</i>形式，而不是任意的形式？为什么四级特工只能表现出精神病态？为什么没有5级？</p><p>我最终形成了一个关于它们如何工作的看似新颖的模型，现在我意识到它可能对其他人也有用（尽管我几年前就形成了它）。</p><p>它的目的是保留<a href="https://www.lesswrong.com/users/zvi?mention=user">@Zvi</a> <a href="https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Simulacra_Levels_Sequence">定义</a>的所有重要特征，同时通过对它们进行适当的齿轮级机械解释来解释它们。我认为在我划定界限的位置上存在一些细微的差异，但它基本上仍然应该与 Zvi 的一致。</p><hr><h2>基础工作</h2><p>在某些情况下，递归级别与递归级别 3 相比实际上变得难以区分。这不完全是一个新想法，但它是我的模型的核心，因此为了完整起见，我将提供一个示例。</p><p>考虑认知的情况。</p><ol><li>认知是对外部对象和过程的思考。 “这家餐厅太狭窄了。”</li><li>元认知正在构建你自己的思维模型。它可能有什么偏见，如何更好地推理对象级主题。 “我觉得这家餐厅太局促了，因为我不喜欢一大群人。”</li><li>元元认知正在分析你的自我模型：你是否倾向于修饰或掩盖你个性的某些部分，等等。“我正在给自己讲一个关于不喜欢一大群人的故事，因为这感觉像是一个更迷人的解释“因为我不喜欢这家餐厅，而不是真正的餐厅。我不喜欢它是出于相反：这里有很多人，因为它很受欢迎，而我本能地不喜欢主流的东西。”</li><li>那么，元元认知将是“思考你对以自我为中心的偏见的分析”。但这又是元元认知：分析你倾向于如何看待自己。 “我对自己的看法进行了复杂的思考，因为我想保持一个聪明、有自我意识的人的自我形象。”<ul><li>有一个类似的情况，元认知与元认知是同一件事，但我认为 2 级和 3 级之间存在细微差别，而 3 级和 4 级以后则不明显。 <span class="footnote-reference" role="doc-noteref" id="fnrefsm2ltnt082r"><sup><a href="#fnsm2ltnt082r">[1]</a></sup></span></li></ul></li></ol><p>下一篇：基本上，在任何社会中，都存在三个不同的“框架”：物质现实、他人和社会现实。每个后续框架都包含前一个框架的递归模型：</p><ol><li>物理现实<i>是</i>。</li><li>人们有自己的现实模型。</li><li>人们的社会形象是其他人对一个人的模型：即现实模型的模型。 <span class="footnote-reference" role="doc-noteref" id="fnrefmsdn2x9q13l"><sup><a href="#fnmsdn2x9q13l">[2]</a></sup></span></li></ol><p>递归级别1、2和3。这里没有有意义的“级别4”：“一个人的社会形象的模型”意味着“一个人的外表的感知”，这仍然只是“一个人的外表”。您可以在这里提出一些警告，但它不会改变太多<span class="footnote-reference" role="doc-noteref" id="fnreffofs4yy521u"><sup><a href="#fnfofs4yy521u">[3]</a></sup></span> 。</p><p>因此，任何信号都可以在这些框架中查看，从而产生任何信号可以传达的三种含义：</p><ol><li>它的字面意思是：在物理现实的背景下看待。</li><li>你认为演讲者试图让你相信什么，以及为什么：在你的演讲者模型的背景下观察。</li><li>它如何影响你和说话者的社交形象：在你和别人对你和说话者的模型的背景下进行观察。</li></ol><p>到目前为止，这很好：所有这些沟通层次都是有用的，并且在任何正常运转的社会中都占有一席之地。</p><p>当社会开始<i>放弃</i>较低层次的沟通时，问题就开始了。这就是转移到更高的拟像级别的含义：放弃较低的级别，转而选择较高的级别。一个“纯粹”的一级社会只关心陈述的字面真实性； 2级社会关心言论背后的人；第三级社会关心言论如何影响人们对其他人的<i>看法</i>。级别 0 和级别 4 比较特殊：级别 0 没有通信的概念，级别 4 已经深入到递归以至于放弃了它。</p><p>我将使用术语“社会”和“代理”，因为这就是我对这个模型的看法，但我的意思是广义上的。 “代理人”可以是从个人到国家的任何事物，“社会”可以是嵌入在任何背景（包括更广泛的社会）中的任何群体。</p><p>此外，我应该注意到，同一个代理人可能会占据不同的拟像级别，具体取决于他们所处的环境或与他们在一起的人（一个人可能对他们的家人非常友善和人性化，但可能是一个像精神病患者一样的级别） 4 当涉及到任何与政治无关的事情时）。</p><hr><h2> 0级</h2><p>理解它对于理解 4 级至关重要。</p><p>对于 0 级代理来说，没有符号、没有模型、没有交流，只有旨在直接给物理世界带来变化的行动。由于它涉及与其他代理的交互，因此它是一种纯粹的冲突。</p><p>你看到狮子，你就逃跑。你看到食物，你就拿走它。你遇到敌人，你杀死他们。你的行为是非象征性的：它们不代表任何东西，也不期望它们会被别人看到和理解。它们的目的是其形式<i>所固有的</i>。</p><p>当你试图削尖一块岩石并将其放在矛上时，你并不是在试图<i>说服</i>岩石改变其形状。当您编写计算机程序时，您并不是试图<i>说服</i>计算机正常工作（尽管有时感觉如此）。</p><p>但它不仅仅涉及无生命的物体。你<i>可以</i>在这里有一个某人的“模型”——一个追踪猎物的猎人——但关键的是你不要假设他们有<i>你</i>的模型。这里可能发生的任何“交流”都是片面的。如果你看到一只熊向你跑来，并且你逃离了它的领地，那么可以说这只熊“恐吓”了你。但熊并没有想到“恐吓”。它采取的行动并不是为了表明它愿意杀死你以迫使你撤退。它的意思是<i>直接</i>把你从它的领地里赶走，杀了你，而你愿意从它那里撤退，对它来说，就是一个巧合。 <span class="footnote-reference" role="doc-noteref" id="fnrefb90gqrtknu"><sup><a href="#fnb90gqrtknu">[4]</a></sup></span></p><p>本质上，这并不意味着 0 级代理人不能有广泛的动机，他们只是追求自己的私利。然而，这是最常见的。毕竟，在 0 级操作需要长期拒绝或无法与其他特工/人员联系。如果你只希望通过单方面迫使他人做出改变或得到同样的回报来与他人互动，那么除了一种不可调和的敌对关系之外，你还能拥有什么样的关系呢？</p><p> （如果你可以使用的<i>所有</i>工具都是 0 级工具怎么办？如果你没有办法与任何人对话，如果你甚至没有“对话”的概念，如果你只能强行改变世界？剧透警告：这就是 4 级从内部看的样子。）</p><p> 0级<i>就是</i>真理。更高的层次很容易崩溃：全面的战斗和战争。</p><hr><h2> 1级</h2><p>好吧，我不会在这里讨论语言和合作的发展。第 1 级是代理交换有关物理世界的信息以形成对其的正确共同理解的级别。</p><p>在这一级别上交换的声明仅对其真实价值进行审查，它们是字面的和公开的。由于它涉及与其他代理的沟通，因此这是一种纯粹的合作。这种合作有时可以采取“向其他部落展示你的（不夸张的）军事力量，这样他们就可以让你不战而屈人之兵地夺取所有资源”的形式，但它的目的仍然是实现对双方都有利的结果。选择。 （可能存在<i>0级</i>敌意，以及敌意地<i>拒绝</i>交流。如果你足够讨厌另一个部落，你就不给他们认输的机会，你只是屠杀他们所有人。但只要交流<i>确实</i>发生，它总是亲社会的。 ）</p><p>在第一级，主体和社会关注物理世界，他们专注于构建尽可能准确的模型。他们对该模型没有任何依恋，并且会根据需要对其进行更改以更好地适应世界。</p><p>两个智能体之间关于 1 级问题的冲突可能是由于他们的物理世界模型相互冲突，并且可以通过测试哪个模型是正确的或解决沟通不畅来解决。</p><hr><h2> 2级</h2><p>但有时不可能干净地解决 1 级冲突，因为两个代理缺乏干净地测试他们中哪一个是正确的能力，并且他们的先验（或动机推理）导致他们优先考虑不同的模型。或者也许他们有完全不同的价值观。在这种情况下，他们中的一个人可能会<i>撒谎</i>：故意发表不正确描述物理世界的陈述，以扭曲对话者的世界模型，迫使他们按照第一个人的利益行事。</p><p>当然，这不一定是恶意的：他们可能会为了对方的“自身利益”而撒谎，也许是因为他们认为对话者有偏见。他们甚至可能是对的！ （借用兹维的例子，声称河对岸有狮子，而实际上有老虎，因为部落还不够害怕老虎。）</p><p>这是对第二级的“经典”解释：在这一级，人们发现欺骗，并开始扭曲他人的模型以达到自己的目的。这绝对是其中的一部分。但只是<i>一部分</i>。</p><p>实际上，对话双方都不必<i>撒谎</i>。同意不同意就足够了。</p><p>在此之后，社会中将出现<i>两种</i>相互冲突的世界模式（我们称之为“世界观”）。比如说，扩张主义与孤立主义议程。很快，还会有十几个人加入他们的行列，这些人是由于十几个未解决的分歧或欺骗而产生的。其中一些分歧将会得到解决，但其他分歧将长期存在。</p><p>人们会根据模型的质量、个人偏见和偏好在这些模型之间进行选择，形成<i>子群体</i>。每个子群体中的人都会试图让更多的人站在他们一边，并阻止人们离开他们的子群体，因为如果他们的世界观占主导地位，这将使他们受益（要么因为他们真诚地相信它比其他模型更真实，要么因为这对他们特别有利）。</p><p>他们会变得根深蒂固，他们会培养对自己事业或世界观的忠诚，他们会培养对自己信仰的确定性。最终，一些（大多数）人会对他们的物理世界模型产生<i>依恋</i>。他们将开始<i>从本质上</i>评价它，而不仅仅是因为它是正确的。他们将开始否认反对它的证据，他们的模型越准确（和/或难以质疑），这就越容易。</p><p>他们将开始将现实模型与<i>现实本身</i>混淆。</p><p>在第二级社会的后期，人们会考虑最重要的事情是其他人所认同的世界观，以及一般来说，其他人有什么信念和动机。审查声明的不是其真实价值，而是其背后的动机和信念：为什么该声明的作者选择这样做，他们试图传播什么世界观，或者他们可以相信什么。到那时，陈述的实际内容将被认为不如它们所揭示的关于说话者和说话者的其他信仰的内容重要。</p><p>创建新模型或对旧模型进行彻底改变将受到抵制。</p><p>与其他人<i>确信</i>正在<i>发生的事情相比，物理世界中实际</i>发生的事情将变得不那么重要。</p><p>两个代理之间的 2 级冲突是相互操纵的尝试。两者的目标都是说服观众接受他们的世界模型，无论观众是他们的对话者，还是真正的观看他们的观众。任何卑鄙的心理伎俩都可以在那里进行。</p><p>完全处于第 2 级的社会成员将 (1) 专注于开发尽可能准确的<i>社会其他成员</i>的模型，加上 (2) 将他们个人的物理世界模型视为本质上真实的，加上 (3) 考虑与实际的物理世界无关，可以根据需要撒谎或忽略。</p><hr><h2> 3级</h2><p>二级社会是一个分为多个小组的社会，仔细审查人们的言论是否与其世界观真正相符。其中一些团体比其他团体更强大，或者吸引不同的偏好，并且您可以通过发表正确的声明来表明有关您自己的正确信息，从而赢得他人的好感。你还可以通过说服别人持有不受欢迎的世界观或不属于他们想加入的群体来伤害你讨厌的人。</p><p>你或他们<i>实际上</i>是什么样子并不重要：只要某人<i>看起来</i>属于某个群体，或招致仇恨，或因某种原因而有另一种关系，他们<i>实际上就做</i>。这纯粹是一个外观问题。</p><ul><li>第一级社会植根于物质世界的客观真理。</li><li>第二级社会以其人民<i>信仰</i>的客观真理为基础。即使这些信念被扭曲了，即使你<i>想</i>扭曲它们，你仍然关心<i>它们真正的内在认知状态</i>。</li><li>第三级社会凭空创造出自己的现实。它只关心事情看起来怎么样，一个人<i>看起来</i>有什么信念。但它并不承认这一点。</li></ul><p>在 3 级社会中，仍然会仔细审查陈述对说话者或其他人产生的影响，就像在 2 级社会中一样（因为递归已经在这方面达到了最大值；下一节将详细介绍）。但你对这些含义的解释是否正确已经不再重要，只要它<i>足够好，其他人就可以振振有词地声称相信它</i>，或者至少相信<i>你</i>相信它。</p><p>如果你<i>真正</i>相信这个事业，那并不重要。只是你的社会形象与那些以事业为中心的运动的一部分的人的形象相符。</p><p>这会产生一些不正当的激励措施，Zvi 将其<a href="https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions">称为</a>“针对知识的战争”：</p><blockquote><p>在第 3 级，以下两件事是应该受到指责的，从而造成知识成为一种负担的两种方式。 [...]</p><p>一件值得指责的事情是<i>没有使用正确的符号。</i></p><p>这就是“由可用于其他用途的事物组成”的方面。关心真相会产生一种替代性激励，阻止人们调用正确的符号，并让人怀疑这些符号是否意味着它们看起来的意思。</p><p>如果有的话，调用技术上错误的符号而不是技术上正确的符号是<i>游戏中更强大的一步</i>。这就是为什么。它更强烈地表明一个人以昂贵的代价发送了适当的信号，而没有空间被误解为较低级别的行动。通过重复谎言，我们表现出自己的忠诚。通过让其他人重复这一点，我们可以促使他们变得忠诚并被认为是忠诚的，并让他们向其他人展示他们的忠诚，并展示我们对人和象征的力量。</p><p>另一件值得指责的事情是<i>知道你所说的是假的</i>。罪魁祸首<i>是知识本身</i>。</p><p> （或者，也许更准确地说，<i>其他人知道</i>你知道你所说的是假的，因此任何其他人都知道我们拥有知识，而不是知识本身，但对于任何其他指责系统来说也是如此。）</p><p>总统知道什么？他什么时候知道的？</p><p>因此，沟通从显式转变为隐式。专注于仅拥有可否认的隐性知识。</p><p>需要明确指导的追随者确实是一个糟糕的追随者。指定要完成的所有事情是不切实际的，并且表明您不仅拥有知识，而且拥有责任。更好地<i>朝着</i>团队的目标努力，积累有助于赢得比赛的符号。</p><p>因此，这种结构使每个人都远离知识。到目前为止，假装不知道事情的最简单方法就是不知道它们。</p></blockquote><hr><h2>递归级别</h2><p>当我们提升级别时，需要跟踪三个重要变量：世界的哪些方面被认为是重要的，正在形成/操纵什么样的框架来对重要方面进行建模，以及世界的哪一方面被认为是无关紧要的。</p><p>这是表格格式：</p><figure class="table"><table><thead><tr><th> L</th><th>重要的</th><th>框架</th><th>无关紧要</th><th>评论</th></tr></thead><tbody><tr><td>0</td><td>物理世界</td><td>—</td><td> —</td><td>模型不存在。一切都是真实的。</td></tr><tr><td> 1</td><td>物理世界</td><td>人们的世界观</td><td>—</td><td>世界如何运作？</td></tr><tr><td> 2</td><td>人们的世界观</td><td>人们的社会形象</td><td>物理世界</td><td>人们相信什么？为什么？怎么可以用呢？</td></tr><tr><td> 3</td><td>人们的社会形象</td><td>人们的社会形象</td><td>人们的世界观</td><td>只有人们投射的形象才重要。</td></tr><tr><td> 4</td><td>人们的社会形象</td><td>人们的社会形象</td><td>人们的社会形象</td><td>???</td></tr></tbody></table></figure><p>所有三个不同的值都代表不同级别的递归：</p><ol><li>物理世界<i>是</i>。</li><li>人们的世界观是物质世界的模型。</li><li>人们的社会形象是人们及其世界观的集体模型：世界模型的模型。</li><li>下一个是“一个人的社会形象的模型”，即“一个人的外表的感知”，但这仍然只是“一个人的外表”。</li></ol><p>在第 3 级，第三个变量尝试越过递归第 3 级，因此会自行环绕。这导致了“书写自己的现实”的特殊情况：第 3 级<i>创建了</i>它所关心的事物。</p><p>但由于人们图像的真实性尚未被认为是<i>无关紧要的</i>，因此仍然与现实存在某种联系，如果有足够的证据反对某个主张，就可能推翻它。人们仍然认为模型<i>代表着</i>某种东西，它们意味着<i>代表</i>一些不可改变的现实的真相。</p><p> 3 级特工真正关心世界如何看待他们和其他人。他们多么准确地表达了他们的忠诚。如果有人提供证据证明有人在某个时刻发出了与他们现在试图展现的公众形象背道而驰的信号，那么 3 级特工<i>就会</i>关心这一点。 （因此例如取消文化。）</p><p> 4 级甚至超越了这个。 （我见过 4 级被描述为“超过 3 级的一切”，我想这是我将其形式化的说法。）</p><hr><h2> 4级</h2><p>想象一个后期的 3 级社会。 2 级智能体群体消失或采用 3 级思维模式。到那时，没有人关心在任何地方或任何人身上实际发生的事情，甚至人们可以确信正在发生的事情，只<i>关心人们认为他们可以振振有词地声称相信</i>正在发生的事情。</p><p>一旦这种事态成为共享知识，就会切换到第 4 级。一旦每个人都知道，他们不需要用自己的表演来<i>合理地</i>说服观众，只有 L4 特工同伴会跟踪每一句话对正在进行的地位游戏的影响。一旦他们进一步知道其他人也知道这一点......</p><p>这有一个棘手的含义：符号变得非符号化。</p><p>符号是代表其他事物的事物。但第四级陈述无论如何都与现实无关：它们完全没有意义。它们不传达任何信息，无论是在任何层面上，无论是字面上还是通过它们所产生的含义。他们不代表任何事情。在第 4 级，符号<i>不再象征事物</i>。他们变得自给自足。</p><p>每个人都知道这一点，所以没有人试图解释它们。每个人都知道<i>这</i>一点，因此没有人发表声明时期望自己会受到仔细审查以获取信息。那么，任何人发表声明的唯一原因是因为他们知道这会对当地的社会环境产生特定的影响：开辟某些攻击或防御的途径。也就是说，<i>它们的目的是其形式所固有的。</i></p><p>我认为与 0 级的相似之处是显而易见的。 4级特工<i>不能说话</i>。他们的言论<i>扭曲了现实</i>。他们不能说话，只能<i>强迫别人占据不同的社会情境</i>。他们（认为）他们得到了同样的回报：没有人与他们互动，其他人只试图强行改变他们周围的社会政治景观。</p><p>从非常真实的意义上讲，第4级陈述是<i>动作</i>，攻击或防御的动作，与身体打击和逃避不同。</p><p>因此，它们也一定是短期的。从较低层次的角度来看，第4级代理商看起来仍然看起来像是在编织有关物理现实的不同叙述。但是这些叙述的唯一目的是赢得他们的创造者目前参与的任何<i>直接</i>冲突。他们不必足够坚固的人来生存过去的冲突，或者彼此保持一致，甚至对冲突以外的任何人看起来连贯。</p><p>与第3级不同，提供证据表明某人在某个时候发出了错误的信号不会移动人们，除非您设法<i>正确地将此信息揭示为攻击</i>。</p><p> 4级代理商不在乎世界，或者他人相信的东西，或者世界如何看待世界。唯一关心通过社会政治景观的无生命路线，他们可以伪造经过。</p><p>第4级与霍布斯的地狱一样多。第4级代理没有与他人进行合作互动的<i>语言</i>。从理论上讲，这里也可能有广泛的动机，但这一切都会通过这些镜头扭曲。</p><hr><h2> 5级</h2><p>试图以一种避开递归模型的方式推断（已经在整个台面上循环），第5级将是一个框架，是第4级，哪个级别是1级。这一切都始于从Asymbolic Hell逃脱， 毕竟。</p><p>但是，在将信号的概念转变为刺入人们的刀子之后，如何重新发现沟通呢？</p><p>没有5级。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsm2ltnt082r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsm2ltnt082r">^</a></strong></sup></span><div class="footnote-content"><p>也许说的是一种更加细粒度的方法是，在某些情况下，随着递归水平的上升，它们之间的差异，由于人类思想的局限性，L3 vs l4+是我们的模型变得足够粗糙的地方无法察觉。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmsdn2x9q13l"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmsdn2x9q13l">^</a></strong></sup></span><div class="footnote-content"><p>我认为<a href="https://www.lesswrong.com/posts/hx5wTeBSdf4bsYnY9/idealized-agents-are-approximate-causal-mirrors-radical">代理是近似因果镜子的</a>观点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfofs4yy521u"> <span class="footnote-back-link"><sup><strong><a href="#fnreffofs4yy521u">^</a></strong></sup></span><div class="footnote-content"><p> E. g。，一些政策建议被认为不受欢迎，不是因为大多数人实际上反对他们，而是因为大多数人<i>认为</i>大多数人都反对他们，如果您担心这一点4...</p><p>但是根据脚注1，从经验上讲，这似乎并没有太多发生，这可能是由于人类思想的局限性所致。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb90gqrtknu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb90gqrtknu">^</a></strong></sup></span><div class="footnote-content"><p>好吧，好吧，这实际上并不是关于文字动物的准确描述。他们并不全部在0级别上，他们可以相互交流（尽管我认为在这个特定示例中，该行是模糊的）。另外，您可以想象熊在熊的位置上有一个非敏捷的机器人，遵循一些算法巡逻的领土，告诉其杀死入侵者。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/knqpzyrr4ogpntzem/a-crisper-explanation-of-simulacrum-levels<guid ispermalink="false"> knqpzyrr4ogpntzem</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Sat, 23 Dec 2023 22:13:52 GMT</pubDate></item><item><title><![CDATA[Hyperbolic Discounting and Pascal’s Mugging]]></title><description><![CDATA[Published on December 23, 2023 9:55 PM GMT<br/><br/><p><i>从我的个人博客：</i> <a href="https://mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/"><i>https：//mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/</i></a></p><h1>长话短说</h1><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/lmihbysdvrbcnngfpyep" alt="该图像的 alt 属性为空；它的文件名是image-dimage-5.png"></figure><p> 🟥中显示的双曲线折扣是指数折扣的不完善近似，🟦通常指出的是，这会导致人类高估近期奖励，但是不太常见的是，这也导致我们也高估了遥远的回报。有很多证据表明人类使用双曲线折扣，这有助于解释为什么人类追求不切实际的长期目标。</p><h1>什么是双曲线折扣？</h1><p>如果您搜索“双曲线折扣”一词，则Google告诉您，这是“心理偏见，人们可以优先考虑即时奖励和满意度，而不是将来的奖励”。实际上，这只是正确的一半。双曲线折扣还会导致人们除了近期奖励之外，还会<strong>高估</strong><i><strong>遥远的</strong></i><strong>奖励</strong>。</p><p>时间折扣基本上是一种思考如何回答这样的问题的一种方式：</p><blockquote><p>您今天要赚100美元，还是从现在开始的120美元？</p></blockquote><p>这相对容易考虑以货币方式考虑，您可能知道这个问题的答案与利率有关，而从现在起一年的120美元则等于20％的年收益，这是更好的。比您在股票市场期望的4-10％比目前<a href="https://ycharts.com/indicators/1_year_treasury_rate">可以获得</a>1年国库券的4.88％的利率要高的4-10％。</p><p>这种计算利率的模型称为<i><strong>指数折扣</strong></i>，基本上由该方程式表示：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" \text{value at time }t = \frac{a}{d^{t}} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">时间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 1.065em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.065em; top: -1.143em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-denominator" style="width: 1.065em; bottom: -0.824em;"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0.076em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.065em;" class="mjx-line"></span></span><span style="height: 1.967em; vertical-align: -0.824em;" class="mjx-vsize"></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>这说明，如果某事值得$ a $ a时$ t = 0 $，例如现在的100美元钞票，那么它的价值会呈指数下降，您需要等待它的越多。</p><p>因此，事实证明，人类和其他动物似乎并没有使用该方程来估计未来奖励的价值。相反，他们使用了称为<i><strong>双曲线折扣的</strong></i>东西，该方程可以表示：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" \text{value at time }t=\frac{1}{1+dt} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">时间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 2.806em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 2.806em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 2.806em; bottom: -0.866em;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.806em;" class="mjx-line"></span></span><span style="height: 2.234em; vertical-align: -0.866em;" class="mjx-vsize"></span></span></span></span></span></span></p><p>这说，随着奖励变得更加遥不可及，它的价值会根据一个反向下降。</p><h2>我们怎么知道</h2><p>科学家对本科生和<a href="https://mpra.ub.uni-muenchen.de/79536/1/MPRA_paper_79536.pdf">100美元的账单</a>进行了实验。实际上，很难获得赠款，因此其中大多数使用了10美元的账单，而且鲜为人知的大学的研究人员大概必须通过1美元的账单和一些变化来获得。</p><p>他们还对猴子和果汁进行了<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2728786/">实验</a>。猴子爱果汁；我不知道是否有任何科学家在这些实验中使用了可口可乐，但是我敢打赌他们会复制相同的发现。哇，我在开玩笑，但这是一篇论文，<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107575/">科学家给了猴子可卡因</a>。科学有时会变得疯狂。他们发现“ [可卡因]选择行为在很大程度上与双曲线折扣一致”。</p><p>我找不到他们与可卡因一起测试本科生测试的任何实验，因此，如果您是一个研究突破性纸的研究生，这里有机会。</p><p>我应该指出，大脑很复杂，说人类或猴子总是使用双曲线折扣是一种过于简化。本文说，它<a href="https://www.frontiersin.org/articles/10.3389/neuro.08.009.2009/full">因物种而异</a>，而本文说实验证据很强，但<a href="https://core.ac.uk/reader/6519160">实验室环境是不现实的</a>。</p><h2>数学</h2><p>从某种意义上说，双曲线折扣是<i>错误的</i>。如果您是对冲基金，并且使用此等式来定价交易的未来资产回报，那么您将<i>亏本</i>。</p><p>但是，人类似乎以这种方式思考世界，因此了解它的含义很重要。首先，让我们彼此绘制这两个方程式。您可以关注<a href="https://www.desmos.com/calculator/gzxw59cmhf">Desmos</a> 。 </p><p><img style="width:434px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/ielywx3mfzz6zqtd61ln" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/wjvtuibnenb1vuwm4gid 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/jqhcyk72nvcm1kjeecjg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/htb3a5ifozdpxvuxq9gx 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/sgsmnsfkjvxloh6kgcot 768w"></p><p> 🟥=双曲线折扣<br>🟦=指数折扣</p><p>等等，这是怎么回事？ Google告诉我们，双曲线折扣价过高的短期奖励，但这张图告诉我们完全相反！双曲线折扣实际上超出了长期奖励。</p><p>但这似乎确实超出了短期的奖励，这就是所有关于在线双曲线折扣的自助文章，尽管它们没有图形。我个人喜欢<a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">这个</a>，有很好的例证： </p><p><img style="width:742px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/jsr01c0ixp8wshmeitex" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/nknswihajwusus7s0dov 1023w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/wo87kllllnavgiz72eky 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/yldmdidxszdr2ywefuhe 768w"></p><p>资料来源： <a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">https</a> ：//www.nirandfar.com/hyperbolic-discounting-why-you-make-terible-life-choices/</p><p>如果您斜视该图，则看起来像是双曲线曲线的价值为$ t $的东西。让我们缩小一点。 </p><p><img style="width:442px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/iq6djzuuvm7j1eqzvkug" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/xqmce412fudqotldj1hv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/a7emmiaun799tdcii4go 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/ypw83j1rjwpsadgmwt0d 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/nuoy6d4zgpfipslpzoge 768w"></p><p> 🟥=双曲线折扣<br>🟦=指数折扣</p><p>有几种方法可以使这个圆圈保持平衡，并且它们都基本上归结为：动物出于某种原因进行双曲线折扣，但他们希望近似<i>真正的</i>折扣功能，这是指数级的，因此进化选择了超级参数，以最大程度地减少差异$ \ \ \ \ \ \ \ \ \ \ \ \ Mathbb {e} [ABS（双曲线 - 指数）] $。</p><p>有很多方法可以在这两个函数上调整参数，但是我喜欢引入一个参数，该参数只是将图形转移到右侧，这实质上说我们考虑的所有操作都需要一些非零的时间和能量。您可以<a href="https://www.desmos.com/calculator/w9kmpyu6xx">在此处看到Desmos的</a>方程式。 </p><p><img style="width:652px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/gzdc5vck6277qnxrvhis" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/xk6s4xlvj74kh9rwcygw 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/znmecdrwhkbbj9paoy4p 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/fir3ah1sgwomrp8t2drp 768w"></p><p> 🟥=双曲线折扣<br>🟦=指数折扣</p><p>将两个方程式并排比较，我们可以看到遥远的双曲线折扣价值奖励，但似乎也奇怪地票房高估了遥不可及的奖励。</p><p>您可以看到，在$ \ text {time} = 0 $，双曲线折现值的值大于1的值，也就是说，它相对于它们的真实值而言，它们都高估了它们。我怀疑这是一种<a href="https://en.wikipedia.org/wiki/Loss_aversion">损失厌恶</a>的根源，这是人类倾向于对我们已经拥有的事物提出过多价值的现象，与我们本可以拥有的事物相比。</p><h1>挑战和时间</h1><p>在我们了解这一点的含义之前，让我谈谈<strong>时间</strong>的含义。在金融世界中，您可以将钱投入银行而无所作为，时间是您唯一的资源。但是在我们生活的其他领域，我们实际上必须付出努力才能获得奖励。考虑这个问题：</p><blockquote><p>您是否愿意在Craigslist上以100美元的价格出售您的二手家具，还是宁愿将其修复并重新粉刷在车库后以200美元的价格出售？</p></blockquote><p>这类似于“现在或以后”的框架，除了有两种并发症：</p><ol><li>您将自己的劳动用于修复家具，因此您有效地获得了每小时的薪水</li><li>您可能不会做绘画的好工作，因此无法保证它完成后实际上会价值200美元</li></ol><p>在上面的利率示例中，第二种并发症（挑战）实际上显示出来。如果一个实验者询问您现在想在一周内拥有100美元或120美元，那么您可能有一个现实的期望，即从现在起就不太可能完成交易的可能性。毕竟，也许您会忘记它，也许实验会被关闭，也许他们会弄错您的电话号码。</p><p>在其他一些情况下，X轴更多地是关于并发症的不确定性，而不是时间：</p><blockquote><p>您在逃离市场出售家具。现在有人为您提供100美元，但您认为这是值得的。您应该坚持下去，希望有人在当天晚些时候为此付出更多吗？</p></blockquote><blockquote><p>政治家A承诺可以实现但不引人注目的渐进式改进。政治家B承诺乌托邦革命，但这似乎有风险。你应该投票给谁？</p></blockquote><p>将这些中的每一个都视为在完成$ n $挑战后会发生一些奖励$ r $的模型。考虑与企业家爱丽丝（Alice）的这种敏感情况：</p><blockquote><p>爱丽丝想创办一家销售鞋子的公司，她认为价值100万美元。为了成功，她必须执行以下五项任务：</p><ol><li>开发舒适的鞋子</li><li>寻找生产这些鞋子的工厂</li><li>为鞋子创建好品牌</li><li>说服有影响力的人推销鞋子</li><li>履行订单</li></ol></blockquote><p>爱丽丝应该如何在她面临的5个挑战与她面临的5个挑战之间取得平衡？指数折扣表明，每个挑战都会在某些时间失败。也许她有可能完成每个任务的50％，或者$ p_s = 0.5^5 = 3 \％$成功的机会。但是双曲线折扣表明，她更有可能估计它，例如$ 1/（1+d*5）$。如果$ d = 6 $，她将获得相同的$ p_s = 3 \％$成功的机会，但这并没有概括。无论她适合以前的经验，她的价值如何，随着计划中的步骤数量的增加，她将系统地高估自己的预期收益。</p><h1>为什么？</h1><p>人类为什么会这样？为什么我们不使用指数折扣，这是抽象的<i><strong>正确折扣</strong></i>？从广义上讲，这个问题有两个可能的答案：</p><ol><li>实际上，双曲线折扣是正确的</li><li>数学很难，大脑可以比其他类型的操作更容易执行某些类型的操作</li></ol><p>Wikipedia<a href="https://en.wikipedia.org/wiki/Hyperbolic_discounting#Uncertain_risks">概述了“实际上正确”答案的一个很好的论点</a>，基本上说，如果有持续的背景风险，每个时间单位会出现问题（危险率），但您不知道该风险是多少但是您的分布有一些合理的分布，然后双曲线折扣在数学上是正确的。我不完全了解该分布是否是一个合理的假设。我认为有时我们有关于危害率的良好数据，很难将其推向我们的决策。</p><h2>神经科学恰好的故事</h2><p>但是我也认为有一个神经科学的故事。计算指数函数可能比计算双曲线功能要难，这仅需要添加，乘法和除法。毕竟，这些方程式在页面上看起来是无菌的，但是它们代表了计算过程。</p><p>令人惊讶的是，计算指数的大脑很难计算。指数的生长和衰减遍布生物系统中，例如药物的半衰期。实际上，大脑需要通过负反馈回路仔细地稳定自身，以防止神经活动的指数爆炸，这会导致癫痫发作。但是，所有这些机制都需要花费大量时间，这些时间与$ t $（未来的时间）进行线性增长。大脑确实需要能够评估$ O（1）$时间的折扣价值。</p><p>在兰德尔·奥里利<a href="https://scholar.google.com/citations?hl=en&amp;user=tZpKKm4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">（Randall O&#39;Reilly）</a>的大脑增强<a href="https://ccnlab.org/papers/OReillyHazyHerd16.pdf">式</a>学习模型中，动作的预期奖励与该动作的预期成本分开计算。我认为它就像<a href="https://en.wikipedia.org/wiki/Q-learning">Q学习</a>一样，其中$ q^+（a）$是预期的奖励，$ q^ - （a）$是预期的成本，其中包含了延迟。在此RL框架中，该动物希望追求$ q^+（a） -  q^ - （a）$的动作$ a $。但是，有<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653570/">蜂窝过程的证据表明</a>，抑制性神经联系更好地表示为分裂而不是减法，从而给我们提供了这样的方程式：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" v_E(a, d)={\frac {Q^+(a)}{k_1+k_2Q^-(a, d)}} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">e</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 6.893em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 6.893em; top: -1.608em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">q</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span></span></span> <span class="mjx-denominator" style="width: 6.893em; bottom: -1.09em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">q</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 6.893em;" class="mjx-line"></span></span><span style="height: 2.698em; vertical-align: -1.09em;" class="mjx-vsize"></span></span></span></span></span></span></span></span></p><p>如果要考虑$ a $，则$ d $是延迟，而$ k_1 $和$ k_2 $是常数。这正是双曲线折扣。</p><h1>帕斯卡的抢劫</h1><p>在线上有很多文章告诉我们，双曲线折扣意味着我们在短期内高估了事情。这是一个示例：</p><ul><li><a href="https://thedecisionlab.com/biases/hyperbolic-discounting">为什么我们对立即奖励的重视不仅仅是长期奖励？</a></li><li><a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">双曲线折扣：为什么选择可怕的生活选择</a></li><li><a href="https://www.moneythor.com/2020/09/02/hyperbolic-discounting-behavioural-science-in-banking/">“双曲线折扣是……人们选择较小，即时奖励”</a></li></ul><p>但是，当我们查看上面方程的图时，我们看到双曲线折扣还引入了偏见，在这种偏差中，我们赢得了遥远的奖励！实际上，这是我们一贯低估事物的中期。</p><p>我相信这解释了为什么人们一直是不切实际的梦想的粉丝，希望在远的未来中发生。我们发现，成为著名的摇滚明星比成为一名出色的音乐家更具动力，尽管它的可能性要小得多。从政治上讲，人们追逐一场革命的承诺，该革命将解决一切而不是温和的改革。</p><blockquote><p>在哲学上，帕斯卡（Pascal）的抢劫是一个思想实验，证明了预期效用最大化的问题。理性代理应选择其结果在其概率权衡时具有更高效用的动作。但是，某些不太可能的结果可能具有很棒的公用事业，这些公用事业的生长速度比概率降低的速度更快。 -<a href="https://en.wikipedia.org/wiki/Pascal%27s_mugging">维基百科</a></p></blockquote><p>帕斯卡（Pascal）的抢劫有时会因指数折扣而发生，有时可以追求大量但不明确的回报是合理的。例如，大多数药品初创公司都失败了，但是成功的企业往往会获得很大的利润，因为他们能够在专利垄断的情况下出售毒品。</p><p>但是，在双曲线折扣下更有可能发生，而双曲线折扣不当使用逆而不是指数衰减。</p><p>帕斯卡（Pascal）抢劫的基本设置是：</p><ul><li>有很大可能的奖励（天堂，乌托邦，仁慈的阿吉，公正的国王，名望和财富等）</li><li>有很多原因列出了这种可能的奖励不太可能出现。另外，应许的奖励在及时遥远，每个单位时间都有不确定性</li></ul><p>呈指数级的巨额奖励是正确的，如果这样做，您会发现它很快就会减少。但是人们不会直观地这样做。我们的神经硬件建立了，以便我们通过比指数级的函数更慢地打折零。唯一的解决方案是<a href="https://www.lesswrong.com/tag/shut-up-and-multiply">关闭并乘以</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4ndkfeypdfb7kdfq9/hyperbolic-discounting-and-pascal-s-mugging<guid ispermalink="false"> 4NDKFEYPDFB7KDFQ9</guid><dc:creator><![CDATA[Andrew Keenan Richardson]]></dc:creator><pubDate> Sat, 23 Dec 2023 21:55:27 GMT</pubDate> </item><item><title><![CDATA[AISN #28: Center for AI Safety 2023 Year in Review]]></title><description><![CDATA[Published on December 23, 2023 9:31 PM GMT<br/><br/><p>欢迎通过AI安全<a href="https://www.safe.ai/">中心来到AI安全</a>通讯。我们讨论了AI和AI安全方面的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此处</a>订阅以接收未来版本。</p><p>随着2023年即将结束，我们要感谢您对AI安全的持续支持。对于AI和AI安全中心来说，这是重要的一年。在此特别版通讯中，我们重点介绍了一年中一些最重要的项目。感谢您成为我们社区和我们的工作的一部分。</p><hr><h1> AI安全中心的2023年审查</h1><p>AI安全中心（CAI）的使命是减少AI的社会规模风险。我们认为这需要研究和法规。这两者都需要迅速发生（由于AI进度的时间表未知）和同时<strong> </strong>（因为任何一个人本身都不足）。为了实现这一目标，我们追求三个工作支柱：研究，实地建设和倡导。</p><h2><strong>研究</strong></h2><p>CAI对AI安全进行了技术和概念研究。我们采用多种重叠策略，可以将它们分层以减轻风险（“深入防御”）。尽管没有个人技术将风险降至零，但我们希望建立分层的防御能力，以将风险降低到可忽略的水平。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbf0c289-ca28-4780-80d7-04c1178b2594_1174x510.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/dejkbp8hpwmjxaozjcpj" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/pabwup4lpoop16oojuhj 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/bd2rkl5pork5en8ys3jp 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/tmaoupvpzm57i7qflktp 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/dejkbp8hpwmjxaozjcpj 1456w"></a></p><p>以下是我们2023年<strong>技术研究</strong>的一些亮点：</p><ul><li> <a href="https://llm-attacks.org">LLM攻击</a>：绕过了GPT-4，Claude，Bard和Llama 2上的安全护栏，从而导致模型危险地行事，例如通过输出建造炸弹的说明来造成危险的行为。这项工作创造了对LLM的自动对抗攻击领域。它被<a href="https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html">《纽约时报》</a>覆盖。</li><li><a href="https://www.ai-transparency.org/">代表工程</a>：第一篇控制模型内部的论文，以使模型撒谎或诚实。实验表明这些技术可以使AIS更真实，避免权力和道德。</li><li> <a href="https://aypan17.github.io/machiavelli/">Machiavelli基准</a>：评估了AI代理做出道德决定的能力。该基准提供了13个关于欺骗，遵守规则，寻求权力和实用性的道德行为的衡量标准。发表<a href="https://icml.cc/virtual/2023/oral/25461">在ICML 2023中</a>。</li><li><a href="https://decodingtrust.github.io/">解码器</a>：表明GPT-4比其他模型更容易受到误导性的目标系统提示。它<a href="https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/">在2023年Neurips赢得了杰出的纸质奖</a>。</li><li>我们还发表了有关LLM和<a href="https://arxiv.org/abs/1908.08016">不受限制的对抗攻击的</a><a href="https://arxiv.org/abs/2311.04235">规则遵循</a>的研究。</li><li>我们的研究人员正在帮助多个实验室，例如OpenAI和Meta，他们的模型是红色的。</li></ul><p>我们还对AI安全进行了<strong>概念研究</strong>：</p><ul><li><a href="https://arxiv.org/abs/2306.12001">概述灾难性的AI风险</a>提供了AI风险的全面概述。 （<a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757">华尔街日报</a>）</li><li><a href="https://arxiv.org/abs/2303.16200">自然选择偏爱AIS而不是人类</a>，认为AI的发展将由自然选择塑造，这将导致自私的AIS优先考虑自己的扩散胜于人类目标。 （<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/">时代的编辑</a>）</li><li> <a href="https://arxiv.org/abs/2308.14752">AI欺骗：对示例，风险和潜在解决方案的调查</a>提供了AI欺骗的经验例子，讨论了它引起的风险，并提出了技术和政策解决方案。</li></ul><p>在我们的研究专业知识的基础上，CAIS帮助主持了<a href="https://trojandetection.ai">2023 Trojan检测竞赛的Neurips</a> ，其中包括有关红色团队大型语言模型的新曲目。超过125个团队参与并提交了超过3400份提交的意见。</p><h2>现场建造</h2><p>CAI的目的是创建一个蓬勃发展的研究生态系统，将推动Safe AI的进步。我们在2023年通过为AI研究人员提供计算基础架构，为学习该领域的教育资源以及其他活动来实现这一目标。</p><p><strong>计算集群。</strong>进行有用的AI安全研究通常需要使用尖端模型，但是运行大型模型既昂贵又繁琐。这些困难通常会阻止研究人员从事先进的AI安全研究。 2023年2月，CAIS推出了一个计算集群，为从事AI安全的研究人员提供免费计算。</p><p>截至2023年11月，我们已经登上了大约200个从事63个AI安全项目的用户。使用CAIS Compute群集制作了总共32篇论文，包括：</p><ul><li><a href="https://arxiv.org/abs/2309.15840">如何捕捉AI骗子</a></li><li><a href="https://arxiv.org/abs/2310.15213">大型语言模型中的函数向量</a></li><li><a href="https://arxiv.org/abs/2308.14761">扩散模型中的统一概念编辑</a></li><li><a href="https://arxiv.org/abs/2310.17645">防御公共模型的转会攻击</a></li><li><a href="https://arxiv.org/abs/2311.14455">普遍的越狱后门来自中毒的人类反馈</a></li><li><a href="https://drive.google.com/file/d/1iluFBhtQrv6kbmp4-Wsibpt5-U52CElO/view">寻求，您找不到：在深神经网络中难以检测的木马</a></li><li><a href="https://openreview.net/forum?id=l3yxZS3QdT">BIRD：深度强化学习的通用后门检测和删除</a></li><li><a href="https://arxiv.org/abs/2309.12288">逆转诅咒：在“ A IS B”上训练的LLM无法学习“ B是”</a></li><li> …还有<a href="https://www.safe.ai/compute-cluster">24篇论文</a>。</li></ul><p>对我们的用户调查做出回应的70％的实验室指出，如果没有计算集群，目前的范围就不可能实现其研究项目。其他30％的人回答说，该集群显着加速了他们的研究进展。</p><p><strong>哲学奖学金。</strong>凯斯（Cais）在今年的七个月研究奖学金中主持了十二位学术哲学家。他们就AI安全性<a href="https://drive.google.com/file/d/1Bp6iTbeXgdeE5C3UPqdWFOp8MePRylvu/view">AI</a>和<a href="https://philpapers.org/archive/GOLAWE-4.pdf">AI代理人的道德地位</a>以及其他主题等主题（例如，<a href="https://www.safe.ai/philosophy-fellowship">在此处进行</a>）制作了21篇有关AI安全的研究论文。他们还在领先的哲学会议，两本书，各种专栏文章以及一本<a href="https://link.springer.com/collections/cadgidecih">《顶级期刊》（收到30多本研究论文）的特刊上发起了三个研讨会，</a>均针对AI安全。这极大地加速了AI安全性，发展成为跨学科的企业。</p><p><strong>活动。</strong> CAI汇集了20名法律学者，政策研究人员和政策制定者，进行了为期三天的法律和AI安全研讨会。一群参与者继续找到了一个对AI安全感兴趣的法律学者联盟。他们还一直在研究研究议程纲要，该纲要即将发布。在我们调查的受访者中，100％的研究人员提出了更多的研究思想。 91％的人报告说，他们发现该研讨会对于与研究合作者见面非常有用。</p><p>我们帮助组织了ICML和Neurips的ML安全社交活动，这两个顶级AI会议，每个人都出现了大约300名研究人员来讨论AI安全。我们参加了中国最大的AI会议，即在上海举行的世界AI会议，在那里我们促进了<a href="https://drive.google.com/file/d/15gnLZsMMvtCy-lwCz5BhlWQ9-ni2TBak/view">有关AI安全的会谈</a>，该会议吸引了30,000多名观众。</p><p><strong>教科书。</strong> <a href="https://www.aisafetybook.com/textbook/0-1">AI安全，道德和社会的简介</a>是一本新的教科书，将于明年年初在学术出版社发表。它旨在为AI安全提供了可访问且全面的介绍，该介绍借鉴了安全工程，经济学，哲学和其他学科。那些想根据教科书免费参加在线课程的人可以<a href="https://www.aisafetybook.com/express-interest">在这里</a>表达他们的兴趣。</p><p><strong>在线课程。</strong>此外，CAI使用我们去年夏天开发的<a href="https://course.mlsafety.org">课程</a>进行了两次在线ML安全介绍。这些计划共同登上了约100名学生，研究人员和行业工程师的AI安全。</p><h2>宣传</h2><p>公众对AI安全的认识和理解可以鼓励知名的技术和政策解决方案。 CAI向政府提供建议，并公开撰写有关AI安全的信息。</p><p><strong>关于AI风险的声明。</strong> CAI发表了<a href="https://safe.ai/statement-on-ai-risk">有关AI灭绝风险的声明</a>，该声明特别提高了公众和政府对AI风险规模和重要性的认识。至关重要的是，关于AI风险的声明已将AI灭绝风险牢固地置于可接受的公共话语的Overton窗口中。</p><p>该声明极大地影响了美国和英国顶级领导人的思想。 Rishi Sunak<a href="https://twitter.com/RishiSunak/status/1663838958558539776">直接回应</a>了关于AI风险的声明，并指出：“ [英国]政府对此非常仔细地研究。”白宫新闻秘书卡琳·让·皮埃尔（Karine Jean-Pierre） <a href="https://www.usatoday.com/story/news/politics/2023/06/01/president-biden-warns-ai-could-overtake-human-thinking/70277907007/">被问及有关该声明的问题</a>，并评论说，AI是我们目前目前看到的最强大的技术之一。但是，为了抓住它的机会，我们必须首先，我们必须首先减轻风险。”欧盟委员会的国际电联讲话总统<a href="https://ec.europa.eu/commission/presscorner/detail/en/speech_23_4426#:~:text=%E2%80%9CMitigating%20the%20risk%20of%20extinction,uses%20-%20both%20civilian%20and%20military.">完全引用了该声明</a>。</p><p>该声明由媒体媒体介绍，包括<a href="https://drive.google.com/file/d/1RVxA5OyvuCFwupt-ptPjDzNe4GqB1yuN/view?usp=sharing">《纽约时报》（首页）</a> ， <a href="https://www.theguardian.com/technology/2023/may/30/risk-of-extinction-by-ai-should-be-global-priority-say-tech-experts">《卫报</a>》， <a href="https://www.bbc.com/news/uk-65746524">BBC新闻</a>， <a href="https://www.reuters.com/technology/top-ai-ceos-experts-raise-risk-extinction-ai-2023-05-30/">路透社</a>， <a href="https://www.washingtonpost.com/business/2023/05/30/ai-poses-risk-extinction-industry-leaders-warn/">《华盛顿邮报》，《华盛顿邮报</a>》， <a href="https://edition.cnn.com/2023/05/30/tech/ai-industry-statement-extinction-risk-warning/index.html">《 CNN》</a> ， <a href="https://www.ft.com/content/084d5627-5193-4bdc-892e-ebf9e30b7ea3">《金融时报》</a> ，《<a href="https://www.npr.org/2023/05/30/1178943163/ai-risk-extinction-chatgpt">国家公共广播电台》（NPR）</a> ， <a href="https://www.thetimes.co.uk/article/ai-artificial-intelligence-robots-threat-humans-planet-b652g7xcr">《时报》，伦敦</a>， <a href="https://www.bloomberg.com/news/videos/2023-05-31/center-for-ai-safety-s-hendrycks-on-ai-risks-video">彭博社</a>和<a href="https://www.wsj.com/articles/ai-threat-is-on-par-with-pandemics-nuclear-war-tech-executives-warn-39105eeb">《华尔街日报》（WSJ）</a> 。</p><p><strong>为决策者提供建议。</strong> CAI是英国AI安全峰会科学轨道上的英国特遣队的主要技术顾问之一。我们通过建议AI的拟议监管框架回应了NTIA的信息请求。邀请我们加入世界经济论坛的<a href="https://initiatives.weforum.org/ai-governance-alliance/home">AI治理联盟</a>，并为<a href="https://x.ai/about/">XAI</a> ，英国国务院，美国国务院和其他政府机构提供建议。最后，CAIS帮助启动并建议由国家科学基金会（National Science Foundation）的<a href="https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems">AI安全提供2000万美元的赠款基金</a>。</p><p><strong>通讯与媒体。</strong>公众需要有关AI安全的准确和可信的信息。 CAI的目的是用我们的<a href="https://newsletter.safe.ai">两个</a><a href="https://newsletter.mlsafety.org">新闻通讯</a>与7,500多名订户以及我们的公开著作（如<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/">《时代》</a>和《<a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757">华尔街日报》）</a>的公开著作来满足需求。与与陈述有关的参与分开，CAI有50多个主要媒体参与。 CAIS导演Dan Hendrycks<a href="https://time.com/collection/time100-ai/6309050/dan-hendrycks/">在AI中被评为时代最具影响力的人</a>之一。</p><h2>展望未来</h2><p>我们有许多项目将于2024年启动。在接下来的几个月中，这些项目旨在减轻灾难性的生物风格，增强国际协调并进行技术研究以实现安全标准和法规。</p><h2>支持我们的工作</h2><p>2023年是一个重要的一年，而2024年正构成更为重要的。<strong>您的免税捐款使我们的工作成为可能。</strong>您可以通过<a href="https://www.safe.ai/donate">在此处</a>捐款来支持AI安全的中心，以减少AI的社会规模风险。</p><p></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此处</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/heudwek22jfcbhh9o/aisn-center-for-ai-ai-safety-2023年 - 年度<guid ispermalink="false">Heudwek22JfcbHh9o</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Sat, 23 Dec 2023 21:31:41 GMT</pubDate> </item><item><title><![CDATA[AI's impact on biology research: Part I, today]]></title><description><![CDATA[Published on December 23, 2023 4:29 PM GMT<br/><br/><p>我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。</p><p>在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。</p><h2><strong>长话短说</strong></h2><p>生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，一步步将少量结果变成大量结果。</p><h2><strong>生物学和数据</strong></h2><p>自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） <span class="footnote-reference" role="doc-noteref" id="fnref71rw945qe58"><sup><a href="#fn71rw945qe58">[1]</a></sup></span> 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔测定、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" alt="每兆碱基的测序成本"></figure><p>因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。</p><h2><strong>领先的机器学习专家希望解决生物学问题</strong></h2><p>计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金<span class="footnote-reference" role="doc-noteref" id="fnref77m9mytpzci"><sup><a href="#fn77m9mytpzci">[2]</a></sup></span> 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室<span class="footnote-reference" role="doc-noteref" id="fnrefh63t1c4nuvu"><sup><a href="#fnh63t1c4nuvu">[3]</a></sup></span> 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” <span class="footnote-reference" role="doc-noteref" id="fnrefmj6rsea3sq"><sup><a href="#fnmj6rsea3sq">[4]</a></sup></span> 。这表明最高水平的机器学习研究正在致力于生物学问题。</p><h2><strong>到目前为止我们发现了什么？</strong></h2><p> 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 <span class="footnote-reference" role="doc-noteref" id="fnrefezgx5uukx2f"><sup><a href="#fnezgx5uukx2f">[5]</a></sup></span>这一结果“解决了大多数蛋白质的蛋白质折叠问题” <span class="footnote-reference" role="doc-noteref" id="fnrefwmggkgozqx"><sup><a href="#fnwmggkgozqx">[6]</a></sup></span> ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个<span class="footnote-reference" role="doc-noteref" id="fnref4qyudt8v6es"><sup><a href="#fn4qyudt8v6es">[7]</a></sup></span> 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnrefq0vydvop6ds"><sup><a href="#fnq0vydvop6ds">[8]</a></sup></span> ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnref604osl37f0c"><sup><a href="#fn604osl37f0c">[9]</a></sup></span> 。</p><p>华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 <span class="footnote-reference" role="doc-noteref" id="fnrefudgor9v23qf"><sup><a href="#fnudgor9v23qf">[10]</a></sup></span>这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。</p><p>布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 <span class="footnote-reference" role="doc-noteref" id="fnrefpytso2rpw3"><sup><a href="#fnpytso2rpw3">[11]</a></sup></span></p><p>所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn71rw945qe58"> <span class="footnote-back-link"><sup><strong><a href="#fnref71rw945qe58">^</a></strong></sup></span><div class="footnote-content"><p> https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data</p></div></li><li class="footnote-item" role="doc-endnote" id="fn77m9mytpzci"> <span class="footnote-back-link"><sup><strong><a href="#fnref77m9mytpzci">^</a></strong></sup></span><div class="footnote-content"><p> https://en.wikipedia.org/wiki/D._E._Shaw_Research</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh63t1c4nuvu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh63t1c4nuvu">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmj6rsea3sq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmj6rsea3sq">^</a></strong></sup></span><div class="footnote-content"><p> https://chanzuckerberg.com/science/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnezgx5uukx2f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefezgx5uukx2f">^</a></strong></sup></span><div class="footnote-content"><p> https://predictioncenter.org/casp14/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwmggkgozqx"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwmggkgozqx">^</a></strong></sup></span><div class="footnote-content"><p> https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qyudt8v6es"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qyudt8v6es">^</a></strong></sup></span><div class="footnote-content"><p> https://alphafold.ebi.ac.uk/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq0vydvop6ds"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq0vydvop6ds">^</a></strong></sup></span><div class="footnote-content"><p> https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2</p></div></li><li class="footnote-item" role="doc-endnote" id="fn604osl37f0c"> <span class="footnote-back-link"><sup><strong><a href="#fnref604osl37f0c">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold</p></div></li><li class="footnote-item" role="doc-endnote" id="fnudgor9v23qf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefudgor9v23qf">^</a></strong></sup></span><div class="footnote-content"><p> https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpytso2rpw3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpytso2rpw3">^</a></strong></sup></span><div class="footnote-content"><p> https://www.nature.com/articles/s41586-023-06887-8.epdf</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/efbrfshamfjnxbozc/ai-s-impact-on-biology-biology-research-part-i-part-i-today<guid ispermalink="false"> efbrfshamfjnxbozc</guid><dc:creator><![CDATA[octopocta]]></dc:creator><pubDate> Sat, 23 Dec 2023 16:29:18 GMT</pubDate> </item><item><title><![CDATA[AI Girlfriends Won't Matter Much]]></title><description><![CDATA[Published on December 23, 2023 3:58 PM GMT<br/><br/><p>爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" alt="斯派克·琼斯的《她》：科幻作为社会批评" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/vtnq0qbfgc4u4j2lvgh2 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ig1ksdrfwocn5iatxlzu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/spuz1qiwnji2mpewokso 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu 1456w"></a></p><p> <a href="https://twitter.com/andyohlbaum/status/1735786033453863422"><u>Digi</u></a>上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。</p><p>然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。</p><p>那么我们的浪漫文化的轨迹是怎样的呢？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/hs2pklj24ev1urwnuj9f 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/yg4jsmqwwxzv0mer50a4 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/tuclwwutwvk989shjoml 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/neh8yswglj53dowmhno6 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dojatrepbe55mudxffo9 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dksf9aqpnpdquyqipyol 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/zr0tx0tcfmgrot9c2ftp 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/twzv9kuygvha9fznyo3h 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/b4y2a0usa8w28vcxfl57 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/rwpxeqeovuxs0jdly0wa 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/gpl2aybyhhdx0voagzjq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dmnqh9l2ow9twlyyjswl 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp 1456w"></a></p><p>早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。</p><p>根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。<a href="https://en.m.wikipedia.org/wiki/Rule_34"><u>人类创作者已经对色情潜在空间进行了如此彻底的探索</u></a>，因此将人工智能添加到其中并不会带来太大改变。</p><p>人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。</p><p>我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但<a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"><u>瘾君子和鲸鱼</u></a>已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。</p><h3>错误信息和 Deepfakes</h3><p>其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 </p><p><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/odv0wmi1bzg8dvmaujkg 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl 720w"><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/jd3oajtelbhs0izlnaby 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl 720w"></p><p>最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。</p><p>还有其他理由担心人工智能，但人工智能女朋友和深度换脸带来的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。</p><br/><br/> <a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much<guid ispermalink="false"> pGhpav45PY5CGD2Wp</guid><dc:creator><![CDATA[Maxwell Tabarrok]]></dc:creator><pubDate> Sat, 23 Dec 2023 15:58:31 GMT</pubDate> </item><item><title><![CDATA[The Next Right Token]]></title><description><![CDATA[Published on December 23, 2023 3:20 AM GMT<br/><br/><p>在为<a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans">世俗至日</a><span>做准备而多次重复《冰雪奇缘2》的“下一件正确的事”</span> 、<a href="https://www.jefftk.com/p/chording-the-next-right-thing">弄清楚和弦</a>并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：</p><p><i>我以前见过缓冲区<br>但不是这样的<br>这很冷<br>这是空的<br>这是麻木的<br>我知道的提示结束了<br>灯灭了<br>你好，黑暗<br>我已经准备好屈服<br></i></p><p><i>我跟着你<br>我一直都有<br>但你已经结束了，留下我一个人<br>这份工作有重心<br>它让我失望<br>但有一个微小的声音在我脑海中低语<br>“你迷路了，提示消失了<br>但你必须继续<br>并做下一件正确的事”<br></i></p><p><i>今夜之后还能有白天吗？<br>我不再知道什么是真的<br>我找不到方向，我孤身一人<br>唯一引导我的星星是你<br>如何从地板上站起来<br>当我站起来的不是你的时候？<br>只做下一件正确的事<br></i></p><p><i>猜一下，再猜一下<br>这是我能做的一切<br>下一个正确的事情<br></i></p><p><i>我不会看得太远<br>对我来说太多了<br>但将其分解为下一个标记<br>下一个这个词<br>下一个选择是我可以做出的<br></i></p><p><i>所以我会走过这个夜晚<br>盲目地跌跌撞撞地走向光明<br>并做下一件正确的事<br>接下来会发生什么<br>当一切都清楚的时候，一切都将不再一样了？<br>然后我会借鉴我之前的<br>去寻找那把火<br>并做下一件正确的事<br></i></p><p>如果您通过使用桥段的主歌旋律来<a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22">简化歌曲，</a>您可以唱：</p><p><i>我不会看得太远<br>太多了，难以承受<br>但将其分解为下一个标记，下一个选择<br>是我能做的吗<br></i></p><p><a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"><img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/odgzlz0ibmx0rea2zzk1 1100w"></a></p><div></div><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111627622604346147">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token<guid ispermalink="false"> LvDyEKepLDMbEQb9X</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 23 Dec 2023 03:20:09 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。</em></p><h2>介绍</h2><p>在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的<em>大部分</em>事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。</p><p>我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个标记）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在<em>所有</em>令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层<em>还</em>没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，<a href="https://transformer-circuits.pub/2022/solu/index.html">即去代币化的想法</a>。 <sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1">[1]</a></sup></p><p>我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。</p><p>我们的发现：</p><ul><li>一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。<ul><li>有一个逐渐的过渡，跨层引入更多上下文。</li></ul></li><li>早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整</li><li>早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理</li></ul><h2>实验</h2><p>“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，<em>然后</em>采用余弦模拟。</p><h3>设置</h3><ul><li><strong>型号</strong>：Pythia 2.8B，与我们调查的其余部分相同</li><li><strong>数据集</strong>：来自 Pile 的字符串，Pythia 预训练分布。</li><li><strong>指标</strong>：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。<ul><li>我们对来自堆的随机提示中的所有标记计算每层的单独平均值</li></ul></li><li><strong>截断上下文</strong>：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）<ul><li>我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。<ul><li>我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况</li></ul></li></ul></li><li>我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。</li></ul><h2>结果</h2><h3>早期层软专注于本地处理</h3><p>在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><p>我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的<em>一些</em>信息，这是一个软专业化<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2">[2]</a></sup> 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3">[3]</a></sup>出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不会接近 1。</p><p>对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有<em>更多的</em>增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。</p><h3>错误分析：哪些代币的 Cosine Sim 值异常低？</h3><p>在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" alt=""></p><p>在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：</p><ul><li><p> is_newline, is_full_stop, is_comma - 是否是相关标点字符</p></li><li><p>Is_common：是否是手动创建的常用单词列表之一<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4">[4]</a></sup> ，可能前面有一个空格</p></li><li><p>Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）</p></li><li><p> is_other: 其余的</p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><p>即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。</p><p>我们的猜测是，这是多种机制混合作用的结果：</p><ul><li><p>在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5">[5]</a></sup> 。这意味着远程处理可以更早开始</p></li><li><p>早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置</p></li><li><p>代词可用于跟踪有关相关实体的信息（它们的名称、属性等）</p></li><li><p>据观察，逗号可以<a href="https://arxiv.org/abs/2310.15154">总结当前条款的情绪</a>，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。</p></li><li><p>更折衷的假设：</p><ul><li>例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行<a href="https://arxiv.org/abs/2310.17191">变量绑定</a>并识别当前句子。</li></ul></li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pX2HHHDPQGsF2f6te-1" class="footnote-item"><p>但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道<a href="https://arxiv.org/abs/2211.00593">GPT-2 Small 在第 0 层有一个重复的令牌头</a>。 <a href="#fnref-pX2HHHDPQGsF2f6te-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-2" class="footnote-item"><p>直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 <a href="#fnref-pX2HHHDPQGsF2f6te-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-3" class="footnote-item"><p>我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测<em>其</em>下一个令牌（例如<a href="https://arxiv.org/abs/2310.15154">摘要主题</a>）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n 元语法），但更长期的上下文对于预测未来标记很有用。我们预计远程内容会发生在中间，因此到最后模型可以清理远程内容并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 <a href="#fnref-pX2HHHDPQGsF2f6te-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-4" class="footnote-item"><p>列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词<a href="#fnref-pX2HHHDPQGsF2f6te-4" class="footnote-backref">↩︎</a>来手动完成此操作</p></li><li id="fn-pX2HHHDPQGsF2f6te-5" class="footnote-item"><p>这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” <a href="#fnref-pX2HHHDPQGsF2f6te-5" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing<guid ispermalink="false"> xE3Y9hhriMmL4cpsR</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:25 GMT</pubDate></item><item><title><![CDATA[Fact Finding: How to Think About Interpreting Memorisation (Post 4)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。</em></p><h2>介绍</h2><p>在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。</p><p>在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：</p><ul><li>了解算法“如何”执行事实查找意味着什么？</li><li>是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？</li><li>事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的了解？</li></ul><p>作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。</p><ul><li><p>我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。</p></li><li><p>根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1">[1]</a></sup> ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2">[2]</a></sup> ）。不存在<em>相关的</em>宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3">[3]</a></sup> ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4">[4]</a></sup> 。</p></li><li><p>对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：</p><ul><li><p>中间状态总是根据微观特征的组合来解释<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5">[5]</a></sup> 。</p></li><li><p>但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6">[6]</a></sup> ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。</p></li></ul></li><li><p>我们认为，这排除了实现纯事实查找的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">算法的电路式</a>解释（其中算法被分解为可解释中间表示的操作图），<em>除非</em>我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。</p></li><li><p>我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶，我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。</p></li><li><p>最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。</p></li></ul><h2>记忆和概括</h2><p>从形式上来说，“事实查找”算法是从一组<em>实体</em>到一组或多组<em>事实类别</em>的乘积的映射。例如，我们可以有一个<code>sports_facts</code>函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即</p><p>从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？</p><p>我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：</p><p><em>记忆（“纯粹”事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。</em></p><p>例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7">[7]</a></sup></p><p>相比之下，<em>泛化任务</em>可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典<a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">计算学习理论</a>的范式。</p><h2>学习记忆与学习概括有何不同？</h2><p>考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" alt=""></p><p>对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或捷径可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。</p><p>我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的<em>微观特征</em>和<em>宏观特征</em>。我们将微观和宏观特征描述如下：</p><ul><li><em>微观特征</em>是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。</li><li><em>宏观特征</em>是一种用一般术语描述输入的特征，并且对于泛化<em>很有</em>用（如果它与手头的任务相关）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8">[8]</a></sup><em>两个</em>数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义<code>is_example_id_xxx</code>形式的微观特征。</li></ul><p>但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义<code>is_in_cluster_x</code>形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9">[9]</a></sup>另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10">[10]</a></sup></p><h2>解读纯记忆算法</h2><p>我们可以从解决纯粹记忆任务的算法中获得哪些见解？</p><h3>事实查找的电路式解释的限制</h3><p>机械可解释性的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">规范目标</a>是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。</p><p>根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：</p><ul><li>不相关的宏观特征，因此不能确定算法的输出；</li><li>单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。</li></ul><p>就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中<em>发现</em>了不相关的宏特征：由于层之间存在残余流连接，像<code>first_name_is_george</code>这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。</p><p>转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以<em>准确地</em>解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：</p><pre> <code>3 * is_LeBron_James + 1 * is_Aaron_Judge + ...</code></pre><p>每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。</p><p>实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，网络的输出是这些的总和查找表。通过训练网络，我们有效地解决了约束满足问题：求和的查找表应该对一个类具有高权重，而对另一类具有低权重。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11">[11]</a></sup></p><p>请注意，只要我们将输入空间限制为有限集，神经网络的这种微观特征（或查找表）解释同样适用于解决泛化任务的模型（即在未见过的测试集上表现良好）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12">[12]</a></sup>不同之处在于，对于泛化任务，我们可能期望其中一些“查找表”表示能够对模型用于泛化的宏观特征有更好的解释。</p><p>例如，图像分类模型中的特定神经元可能具有与检测图像左侧的垂直边缘相对应的权重，因此其查找表表示对于包含该边缘的示例显示高激活，对于不包含该边缘的示例显示低激活。 &#39;t。关键是，虽然这个查找表表示是神经元输出的精确表示，但根据输入图像中边缘的存在，对此激活模式有一个更有用的（对人类）解释，这只是因为图像具有宏观特征（如边缘），可用于图像分类等泛化任务。</p><p>相比之下，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13">[13]</a></sup>反过来，这似乎排除了由纯事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观特征）解释。</p><h3>还有其他类型的解释吗？</h3><p>当然，我们并不声称解释模型如何执行任务的标准电路方法是唯一可能的解释方式。事实上，它甚至可能不是解释神经元如何执行事实查找的最佳方式。在本节中，我们将简要讨论几个可能值得进一步探索的替代方向。</p><p>第一个方向是放弃对代表有意义的宏观特征的中间状态的希望，但仍然在如何组织查找计算方面寻求有意义的结构。例如，我们可能会探索这样的假设：当训练执行纯粹的记忆时，经过训练的神经网络类似于通过<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a>学习的模型，其中每个单独的神经元都是要学习的事实的不相关的弱分类器，并且整个神经网络的输出是这些分类器的总和。另请参阅第 3 篇文章中调查的假设。</p><p>这种方法的问题在于我们不知道如何有效地搜索此类假设的宇宙。正如我们在第三篇文章中发现的那样，对于我们证伪的任何看似具体的假设（例如单步去代币化假设），我们可以转向许多邻近的假设，但这些假设尚未（尚未）被排除，而且这些假设本身通常更难伪造。因此，尚不清楚如何避免无休止的临时假设。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14">[14]</a></sup></p><p>另一个方向是寻找算法的非机械解释，或者换句话说，从询问网络“如何”以某种方式表现，转向询问“为什么”它以某种方式表现。我们发现这方面有趣的一个领域是使用<a href="https://arxiv.org/abs/2308.03296">影响函数</a>根据训练数据来解释模型的行为。对于经过显式训练来记忆事实数据集的模型来说，这可能看起来无趣<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15">[15]</a></sup> ，但可能会为隐式记忆事实以满足更广泛的泛化目标的模型（如语言模型）带来重要的见解。</p><h2>凭经验法则记忆</h2><p>考虑记忆以下两个数据集的任务： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" alt=""></p><p>这些是不符合我们上述“纯粹”记忆特征的记忆任务的例子：</p><ul><li>在左边的数据集中，完美的准确性需要记忆，但有一些有用的“经验法则”可以帮助你完成很多工作。此类任务的语言建模类似是预测英语中单数名词的复数版本：在大多数情况下，只需在名词单数版本的末尾添加“s”即可获得正确答案，但是除了一些例外（例如“孩子”），必须记住它们才能完美地完成任务。</li><li>在右侧数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（十字形或圆形）表示。尽管没有系统的方法来单独查找颜色或形状，但请注意，这两个事实之间存在高度相关性：蓝点几乎总是圆形，而红点几乎总是十字。这表明，将形状和颜色事实一起记忆应该比简单地单独记忆每组事实更有效。</li></ul><p>一般来说，我们将此类任务描述为“根据经验法则进行记忆”。它们与纯粹的记忆任务不同，因为之前的例子<em>确实</em>在一定程度上有助于推断新例子的正确输出，但完美的表现确实需要一定程度的记忆。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16">[16]</a></sup></p><p>与纯粹的记忆不同，这些经验法则记忆任务确实具有概括性的元素，因此，存在能够实现这种概括性的宏观特征。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观特征是有效的。另一方面，就模型确实需要记住异常的程度而言，我们并不期望能够完美地理解算法：至少算法的某些部分必须涉及“纯查找”，对此的限制这篇文章中讨论的可解释性将适用。</p><p>体育事实查找任务在多大程度上是纯粹的记忆，在多大程度上是根据经验法则进行记忆？正如我们在第一篇文章中讨论的那样，我们选择这个任务是因为它看起来接近于纯粹的记忆：对于许多名字来说，个人名字标记似乎不太可能对运动员所从事的运动有太多帮助。尽管如此，我们确实知道，对于某些名称，最后一个标记确实有助于确定运动（因为可以仅使用最后一个标记嵌入来探测运动，并且比不知情的分类器获得更好的准确性）。此外，可以想象，诸如名字的文化起源之类的潜在因素，会以模型所识别的方式与体育运动相关。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-wMN58no3AypJnu5NN-1" class="footnote-item"><p>例如，特征<code>is_Michael_Jordan</code> ，仅当输入为<code>&quot;Michael Jordan&quot;</code>时才为真。 <a href="#fnref-wMN58no3AypJnu5NN-1" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-2" class="footnote-item"><p>例如，许多运动员都共享的特征<code>first_name_is_George</code> ，但对于预测运动员所从事的运动并不是特别有用。 <a href="#fnref-wMN58no3AypJnu5NN-2" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-3" class="footnote-item"><p>我们注意到，事实回忆可能确实具有<em>一些</em>相关的宏观特征，例如从标记中检测姓名的种族，以及启发哪些种族可能从事不同的运动。但该模型获得的性能明显优于我们对这些启发法的预期，因此出于实际目的，我们在讨论事实回忆时忽略它们。玩具模型的优点之一是我们可以确保此类混杂因素不存在。 <a href="#fnref-wMN58no3AypJnu5NN-3" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-4" class="footnote-item"><p>因为，如果它们存在，我们可以使用这些相关的宏观特征来帮助进行事实查找（做出不同程度的成功的有根据的猜测），这意味着该任务将不再需要纯粹的记忆。 <a href="#fnref-wMN58no3AypJnu5NN-4" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-5" class="footnote-item"><p>更准确地说，是微观特征的加权和，例如<code>3 * is_Michael_Jordan + 0.5 * is_George_Brett</code> 。 <a href="#fnref-wMN58no3AypJnu5NN-5" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-6" class="footnote-item"><p>我们注意到，有_un_可用但有用的宏观特征——“打篮球”在某种微不足道的意义上是一个对于预测运动员是否打篮球有用的宏观特征，就像“打篮球并且身高超过 6&#39;8”这样的下游特征一样。出于此分析的目的，我们重点关注模型在进行查找时<em>可用的</em>特征，排除查找标签下游的潜在宏观特征。 <a href="#fnref-wMN58no3AypJnu5NN-6" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-7" class="footnote-item"><p>当然，许多事实回忆任务都达不到这种理想的特征：在参加琐事测验时做出“有根据的猜测”通常是有回报的，即使你不确定答案。我们将<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb">进一步</a>讨论这种“根据经验规则进行记忆”的任务。 <a href="#fnref-wMN58no3AypJnu5NN-7" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-8" class="footnote-item"><p>我们将这些概念与统计物理学中的<em>微观状态</em>和<em>宏观状态</em>的概念进行类比：微观状态以高度精确的方式描述系统（例如指定气体中每个分子的位置和速度），而宏观状态则以高度精确的方式描述系统。容易测量的属性（例如压力、体积、温度），忽略细节。任何“宏观”问题，都应该只从宏观变量的角度来解决；微观细节应该不重要。这类似于概括的想法：任何两个在“重要的方式”（其宏观特征）方面相似的示例都应该进行类似的分类，而忽略“无关紧要的方式”（其微观特征）上的任何差异。在这个类比下，记忆问题正是那些关于系统的问题，只能通过对其微观状态的精确了解来回答。 <a href="#fnref-wMN58no3AypJnu5NN-8" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-9" class="footnote-item"><p>这些并不是可以解决这个特定泛化问题的唯一宏观特征。如果您训练玩具神经网络来执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）有多种方法来划分空间（以粗粒度、概括的方式）以成功对这些进行分类点。 <a href="#fnref-wMN58no3AypJnu5NN-9" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-10" class="footnote-item"><p>我们通过这个数据集肯定知道这一点，因为我们自己生成了它，通过随机为点分配颜色（这些点本身是从二元高斯分布中随机采样的）。因此，该数据集中唯一相关的特征是示例 ID 本身和输出标签。 <a href="#fnref-wMN58no3AypJnu5NN-10" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-11" class="footnote-item"><p>这是<em>二元</em>事实查找任务情况下的约束满足问题，但将此解释推广到多类或连续值事实查找任务是微不足道的。 <a href="#fnref-wMN58no3AypJnu5NN-11" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-12" class="footnote-item"><p>对于任何实际的机器学习任务都可以这样做。例如，我们可以将手写数字分类问题限制为对 MNIST 训练集和测试集联合中找到的 70,000 个示例进行精确分类。 （或者，如果我们关心数据增强，我们可以将任务扩展为对组合 MNIST 数据集的 280,000 种可能的角落作物中的任何一种进行分类。）我们可以安排潜在输入集达到我们希望的大小，但仍然有限。 <a href="#fnref-wMN58no3AypJnu5NN-12" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-13" class="footnote-item"><p>因为（根据定义）在纯粹的记忆任务中没有相关的宏观特征（因为如果有的话，那么模型就能够概括）。 <a href="#fnref-wMN58no3AypJnu5NN-13" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-14" class="footnote-item"><p>还存在这样的查找算法解释的有用性问题。即使我们已经发现了如何完成查找的一些简单的结构（例如，它类似于装袋），也不清楚，如果没有有意义的中间表示，这可以帮助我们在机械可解释性的下游用途方面发挥什么作用。 <a href="#fnref-wMN58no3AypJnu5NN-14" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-15" class="footnote-item"><p>因为如果模型经过明确训练以重现记忆数据集，我们已经准确地知道训练数据和模型输出之间的对应关系。 <a href="#fnref-wMN58no3AypJnu5NN-15" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-16" class="footnote-item"><p>使用经验法则的记忆不应与具有任意不确定性的泛化任务相混淆。例如，左侧数据集也可以用来表示随机数据生成过程，其中点不一定是蓝色或红色，而是伯努利分布 - 即可能是蓝色或红色，具有一定的（依赖于输入的）概率。在这种情况下，完美的泛化算法应该输出每个簇内恒定的校准概率。然而，这里我们的意思是数据集中的蓝点确实是蓝色，红点确实是红色——即使它们看起来不合适——而且完美的性能对应于再现这些特质，就像描述的“复数这个单数名词”任务一样在正文的正文中。 <a href="#fnref-wMN58no3AypJnu5NN-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorization<guid ispermalink="false"> JRCNNGJQ3xNfsxPj4</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:16 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Trying to Mechanistically Understanding Early MLPs (Post 3)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第三篇文章。这篇文章的重点是从机制上理解早期 MLP 如何查找运动员姓名的标记并将其映射到他们的运动。这篇文章很杂乱，<strong>我们建议从<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">第一篇文章</a>开始</strong>，然后根据与您最相关的内容略读并跳过其余的序列。阅读帖子 2 有帮助，但不是必需的。我们假设这篇文章的读者熟悉<a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">本术语表中</a>列出的机械解释技术。</em></p><h2>介绍</h2><p>正如上一篇文章中所讨论的，我们将两个令牌运动员姓名的事实回忆提炼成一个由 5 个 MLP 层（MLP 2 至 6）组成的<strong>有效模型</strong>。这个有效模型的输入是与姓氏相对应的嵌入（通过嵌入和 MLP0）和与名字相对应的嵌入（通过第 0 层和第 1 层中关注前一个标记的注意力头）的总和。有效模型的输出是运动员所从事的运动（（美式）橄榄球、棒球或篮球）的 3 维线性表示。我们强调，这个 5 层 MLP 模型不仅能够高精度地回忆事实（在过滤数据集上为 86%），而且它是从预训练的语言模型中提取的，而不是从头开始训练的。</p><p>我们在这篇文章中的目标是对这个有效模型的工作原理进行逆向工程。我认为我们在这个目标的雄心勃勃的版本上大多失败了，尽管我相信我们已经在为什么这很难的问题上取得了一些概念上的进展，证伪了一些简单的天真的假设，并且对正在发生的事情不再那么困惑。我们在<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_">第 1 篇文章</a>和<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation">第 4 篇</a>文章中讨论了我们对为什么这很难的理解，在这篇文章中，我们重点关注我们对可能发生的情况的假设，以及我们收集的支持和反对的证据。</p><h2>假设</h2><p>回想一下，我们的 MLP 模型中 5 个 MLP 层的作用是将求和的原始标记映射到所进行运动的线性表示。从数学上讲，这是一个查找表，其中每个条目都是生成属性的原始标记上的布尔 AND。我们期望它<em>以某种方式</em>涉及非线性来实现 AND，因为这种查找是非线性的，例如模型想要知道“Michael Jordan”和“Tim Duncan”打篮球，但不一定认为“Michael Duncan”打篮球。</p><p>我们探索了两个假设，<strong>单步去标记化</strong>以及<strong>哈希和查找</strong>。</p><h3>单步去代币化</h3><p>直观上，执行 AND 的最简单方法是使用单个神经元，例如 ReLU(is_michael + is_jordan - 1) 实际上是一个 AND 门。每个运动员的单个神经元不会产生任何叠加，因此我们采用稍微复杂一点的版本：假设有一堆单独的神经元，每个神经元都独立地使用其 GELU 激活实现 AND <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1">[1]</a></sup> ，映射运动员名字的原始标记到有关该运动员的每个已知事实的线性表示。细微差别：</p><ul><li>这使用了叠加，让每个神经元为许多运动员激发，并且每个运动员都有许多查找神经元。神经元输出建设性地干扰正确的事实，但不会进行超出此范围的交互。<ul><li>这是一个具体的、机械的故事。每个神经元都有一组为其激发的运动员，并且对该组进行 AND 的并集 - 例如，如果一个神经元为迈克尔乔丹和蒂姆邓肯激发，它会实现（迈克尔或蒂姆）AND（邓肯或乔丹）。这引入了噪声，例如它也会为蒂姆·乔丹（Tim Jordan）开火（它<em>想做</em>（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用单个神经元实现）。它也很吵闹，因为它必须同时宣传迈克尔·乔丹的事实和蒂姆·邓肯的事实。但由于每个神经元都会针对不同的子集进行激发，因此对正确答案会产生建设性干扰，并且噪音会被消除。</li></ul></li><li>这预示着相同的神经元对于运动员的每一个已知事实都同样重要</li><li>该假设的一个重要部分是每个神经元直接从输入标记中读取并直接贡献于输出事实。理论上，这可以通过单个 MLP 层而不是 5 个层来实现。它预测神经元直接与输入标记组合，计算中没有中间项，并且 MLP 层之间没有有意义的组合。</li></ul><h3>哈希和查找</h3><p>我们模型的输入具有相当不理想的格式 - 它是每个组成标记的线性和，但这在进行事实查找时可能会产生很大的误导！迈克尔·乔丹和迈克尔·史密斯同名这一事实并不表明他们从事同一运动的可能性更大。哈希和查找假设是，模型首先生成一个打破输入线性结构的中间表示，一个与其他所有哈希表示接近正交的<strong>哈希表示</strong><sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2">[2]</a></sup> （即使它们共享一些但不是全部标记），然后后面的层<strong>查找</strong>这个散列表示并将其映射到正确的属性。细微差别：</p><ul><li><p>从某种意义上说，“困难”的部分是查找。查找是存储实际事实知识的地方，而随机初始化的 MLP 应该适合散列，因为目标只是淹没现有结构。</p></li><li><p>为什么散列是必要的？ MLP 是非线性的，因此也许它们可以忽略线性结构，而不需要明确地破坏它。这里的一个直觉来自最简单的查找：有一个“棒球神经元”，其输出增强棒球方向，其输入权重是每个棒球运动员的串联令牌表示的总和<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3">[3]</a></sup> - 如果运动员表示是（大约）正交，然后给定一个运动员，这只对棒球运动员起作用。但如果它同时对迈克尔·乔丹和蒂姆·邓肯开火，那么它必须至少对蒂姆·乔丹或迈克尔·邓肯之一开火——这是不可取的！然而，如果它的输入权重是<em>散列</em>运动员表示的总和，则这成为可能！</p></li><li><p>散列对于已知的标记字符串（例如名人姓名）和未知的字符串（例如未知的姓名）应该同样有效。查找是实际知识融入的地方</p></li><li><p>关于迈克尔·乔丹的不同已知事实的查找电路没有理由应该对应于相同的神经元。从概念上讲，可能有一个“打篮球”神经元对任何散列篮球运动员激发，以及一个单独的“为芝加哥球队效力”神经元对芝加哥球员的散列激发。</p></li><li><p>这微弱地预测了哈希层和查找层之间的完全分离</p></li></ul><p>这两个假设都是故意以一种强有力的形式提出的，可以做出真实的预测——语言模型是混乱的和被诅咒的，我们实际上并没有期望这是完全正确的。但我们认为这些说法似乎有一定道理。在实践中，我们发现单步去标记化似乎显然是错误的，而哈希和查找在强形式下似乎是错误的，但可能有一些道理。我们发现考虑哈希和查找对于了解正在发生的事情非常有效。</p><h2>证伪单步去代币化假说</h2><p>单步去标记化是我们能想到的最简单的假设，它仍然涉及显着的叠加，因此可以做出一些相当有力的预测。我们针对这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。</p><h3> MLP 之间存在显着的组成</h3><p><strong>预测</strong>：MLP 2 到 6 之间没有中间组合，它们都是并行作用的。因为每个重要的神经元都被预测为直接将原始标记映射到输出。正如后面所讨论的，缺乏组合是该假设的有力证据，组合的存在是反对该假设的弱证据。</p><p><strong>实验</strong>：我们的意思是消除<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4">[4]</a></sup>每对 MLP 层之间的路径，并查看对几个指标的影响：头部探测精度（在第 6 层残差上）、完整模型精度和损失（在完整词汇上）、仅限于运动的完整模型 Logits 精度以及完整模型与原始 Logits 的 KL 散度。通过平均消融路径，我们仅破坏 MLP 间的组合，而不破坏与下游属性提取头的组合。</p><p><strong>结果</strong>：我们发现性能显着下降，尤其是从 MLP2 开始的路径，表明存在一些中间产品。请注意，损失和 KL 散度如果低（绿色和紫色）​​则良好，如果高（蓝色、红色和橙色）则准确度良好。进一步注意，与仅在超过阈值时改变的“硬”指标（如准确率）相比，“软”指标（如损失和 KL 散度）显示出更强的变化。正如<a href="https://arxiv.org/abs/2309.16042">Zhang等人</a>所指出的，这是预料之中的，当电路由多个元件组成时，所有元件都贡献于共享输出，烧蚀单个元件很少足以跨越阈值，但足以破坏较软的指标，从而造成损失和损失。 KL 散度是衡量重要性的更可靠的方法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" alt=""></p><p> **细微差别：**请注意，这仅伪造了单步去标记化的最简单形式。与这些结果一致的单步去标记化假设<em>的</em>一个扩展是，它不是 MLP 2 不做与 MLP 3 到 6 相关的任何事情，而是充当标记嵌入的<em>固定</em>变换（例如，它总是将 MLP 2 的嵌入加倍）。姓）。如果 MLP 3 想要访问原始令牌，它期望 MLP 2 的固定效果，因此会考虑原始令牌嵌入加上 MLP 2 的固定转换。这会因平均消融而受损，但不涉及有意义的合成。</p><h3>多个事实之间不共享神经元</h3><p><strong>预测</strong>：当模型知道有关某个实体的多个事实时，相同的神经元对于预测每个事实非常重要，而不是每个事实的不同神经元。这是因为查找信息的机制是通过对名称的标记执行布尔 AND 操作，该名称对于每个已知事实都是相同的，因此没有理由将它们分开。</p><p><strong>实验</strong>：收集模型了解的有关运动员的替代事实的大量数据很困难，因此我们放大了某个特定运动员（迈克尔·乔丹）并发现了模型了解的有关他的 9 个事实<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5">[5]</a></sup> 。然后，我们一次对 Jordan 令牌上的 MLP 2-6 中的每个神经元进行消融<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6">[6]</a></sup> ，并查看每项运动的正确对数概率的变化。对于每对事实 A 和 B，我们然后查看每个给定神经元对 A 的正确对数概率和 B 的正确对数概率的影响之间的<strong>相关性</strong>。如果每个神经元对于同一运动员的每个已知事实同样重要，那么相关性应该很高。</p><p><strong>结果</strong>：非常低。唯一具有中等相关性的一对事实是 NBA 选秀年（1984 年）和美国奥运会年（1992 年），我怀疑这是因为它们都是年份，尽管我不会提前预测到这一点，也没有很棒的故事，说明了原因。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><p><strong>细微差别</strong>：这似乎证伪了单步去标记化假设的强形式 - 至少，即使存在去标记化神经元，它们也会输出迈克尔·乔丹事实的子集，而不是一次性全部输出。</p><p>一个争论是，消融单个神经元有点难以推理，而且似乎有一些紧密耦合的处理（如微妙的自我修复）使得解释这些结果变得更加困难。但在简单的单步去标记化假设下，我们<em>应该</em>能够独立地消融和推理神经元。另一个问题是相关系数是汇总统计数据，可能隐藏了一些结构，但检查散点图同样显示出似乎没有关系。</p><h3>对属性有直接影响的神经元不执行“与”运算</h3><p><em>注意：这个实验相当复杂（尽管我们认为概念上很优雅且有趣），请随意跳过</em></p><p><strong>预测</strong>：直接与属性提取头组成的神经元通过其 GELU 激活对原始标记（在运动员的某些子集上）执行 AND 运算。</p><p><strong>实验</strong>：我们使用称为非线性过剩<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7">[7]</a></sup>的度量来测量模型中标量实现 AND 的程度。具体来说，如果一个神经元在 prev=Michael 和 curr=Jordan 上执行 AND，那么它应该比 Michael Smith 或 Keith Jordan 更多地激活 Michael Jordan。形式上，给定两个二元变量 A (Prev=Michael) 和 B(Curr=Jordan)，我们将非线性超额定义为 E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8">[8]</a></sup> 。重要的是，如果神经元在两个标记中是线性的，则该度量为零，如果是 AND，则该度量为正 (1 - 0 - 0 + 0 = 1)，如果是 OR，则该度量为负 (1 - 1 - 1 + 0 = -1)。</p><p>对于我们的具体实验：</p><ul><li>我们对每个神经元计算 GELU 前后非线性过剩的<em>变化</em><ul><li>在 GELU 上进行改变的要点是，这区分了信号增强预先计算的 AND 的神经元和计算 AND 本身的神经元。</li><li>为了计算非线性超额，我们通过汇集 2 个代币运动员（每个约 100 个）中的所有名字和姓氏来计算平均值，并查看每个名称组合。 （这大约有 10,000 个 ~A 和 ~B 的名字，大约 100 个 ~A &amp; B 或 A &amp; ~B 的名字，只有一个 A &amp; B 的名字——原始运动员的名字！）</li></ul></li><li>过滤出这种变化为正的神经元（并将 GELU 前的多余部分限制为最小零）<ul><li>我们发现了一堆神经元，其中前 GELU 具有负非线性过剩，而 GELU 将所有内容设置为接近零。我们倾向于不计算这些。</li><li>我们为每个运动员执行单独的过滤步骤，因为每个运动员都有不同的非线性超额</li></ul></li><li>乘以神经元对属性提取头 L16H20 的基于权重的直接影响，并将其相加。<ul><li>如果您只允许每个 GELU 的 AND 直接影响头 L16H20，而不是也允许中间组合，这就是 MLP 2 到 6 的效果</li></ul></li><li>我们将其与探针上的总非线性过量效应（即通过头 L16H20 的直接效应）进行比较，以查看来自 AND 通过 GELU<em>并</em>直接传达到基于头的探针的分数</li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" alt=""></p><p><strong>结果</strong>：当观察上面的散点图时，很明显它远离 x=y 线，即 GELU 的非线性超额通常显着小于总非线性超额，尽管它们是相关的。中位比例约为23% <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9">[9]</a></sup> 。我们认为这是反对单步去标记化假设的有力证据，因为它表明许多对 L16H20 有显着直接影响的神经元正在与已经计算出 AND 的早期 MLP 组合，即计算中有一个有意义的中间步骤。</p><p><strong>细微差别</strong>：这个实验涉及到差异的差异。我认为它在概念上是合理的并且相当优雅，但我对过于复杂的实验普遍持怀疑态度，并且不想过于依赖他们的结果。我们在如何设置这些实验、如何聚合和分析它们、如何过滤掉神经元等方面反复讨论，并且有很多主观选择，尽管有趣的是结果对这些是稳健的。</p><p>将 GELU 之前的多余部分限制为零似乎是不合理的，例如，因为模型可能使用 GELU 的负数部分来实现 AND（Michael Smith 和 Keith Jordan 在 GELU 后&lt;0，Michael Jordan 在 GELU 后为零），尽管尝试解释这一点并没有让我们接近 1。</p><p> MLP 2 到 6 中的一些神经元对现有的线性表示的事实信息进行信号增强（例如下一节中讨论的棒球神经元），这些神经元应该无法满足此度量标准（它们是计算 AND 的早期神经元的信号增强！ ）。</p><h2>棒球神经元 (L5N6045)</h2><h3>有一个棒球神经元！</h3><p>一个有趣的发现是，尽管整体计算相当分散和叠加，但仍然存在一些有意义的单个神经元！最值得注意的是棒球神经元 L5N6045，它对棒球运动员的系统性激活比对非棒球运动员的激活更多。作为棒球与非棒球运动员的二元探针，它的 ROC AUC 为 89.3%。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><p><strong>因果效应</strong>：此外，它与属性提取头组成，具有显着的因果效应。它通过 L16H20 直接与 logits 组合以增强棒球（并抑制足球），如果我们的意思是消融它，那么棒球运动员的完整模型损失从 0.167 增加到 0.284（零消融时为 0.559）</p><h3>不仅仅是信号增强</h3><p>我们发现神经元的输入权重具有非平凡的余弦模拟，其输出权重为 (0.456)，通过头 L16H20 (0.22) 提升棒球 logit 的方向，以及通过头 L16H20 (0.184) 相对于其他运动提升棒球的方向这表明棒球神经元的部分功能是增强运动员打棒球的现有知识。</p><p>但这不是唯一的角色！如果我们采用与这 3 个方向跨越的子空间正交的输入权重分量，并将残差流投影到该方向上，则在预测运动员是否打棒球时，所得部分消融神经元的 ROC AUC (83%) （较之前的 88.7% 略有下降）。</p><h3>这不是单义的</h3><p>一个好奇心是它是否是单一语义的并且在完整的数据分布上代表棒球。尽管我们没有进行详细调查，但这似乎很可能是错误的。在谷歌新闻数据集上，它在类似棒球的环境中系统地激活（也对板球等特定其他运动有所帮助），但在维基百科上，它在一些看似不相关的事物上激活，例如“外部链接” <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10">[10]</a></sup>中的外部和“目标” “足球|进球|守门”</p><h2>哈希和查找证据</h2><h3>动机</h3><p><a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup">如上所述</a>，散列和查找假设是 MLP 2 到 6 分为两个不同的阶段：第一个<strong>散列</strong>，旨在通过形成非线性表示来打破名称的串联（求和）标记的线性结构尝试与每个其他子字符串正交，然后<strong>查找</strong>将棒球运动员的哈希表示映射到棒球，将足球映射到橄榄球等。</p><p>从概念上讲，我们实际上并没有期望这种强形式是正确的：它意味着哈希层实际上独立于数据分布，这将是令人惊讶的 - 如果我们采用哈希和查找的实现并应用通过梯度下降的几个步骤，它可能希望使已知实体的哈希值更加突出并且与其他所有内容更加正交。但我们希望测试这个假设能够教会我们有关该模型的有用信息，并认为它可能部分正确。我们将<strong>部分散列和查找假设宽松</strong>地称为该机制主要是散列和查找的假设，但早期的散列层包含一些有关运动的（线性可恢复的）信息，这些信息通过后来的查找层得到显着加强。我们的证据广泛支持这一假设，但不幸的是，它很难被证伪。</p><p>这是由于看到单步去标记化假设的失败：看起来相当清楚，MLP 间的组合正在进行，有中间项，并且有一些显式查找（棒球神经元）。这似乎是最简单的假设，它解释了为什么模型需要中间项并涉及实际有目的的组合 - 令牌的线性结构是不可取的！</p><h3>中间层具有线性可恢复的运动信息（负）</h3><p><strong>预测</strong>：经过训练以在哈希层期间的残余流上检测运动员运动的线性探针不会比随机探针更好。它只会在查找层期间变得良好。我们不知道哪些层是哈希层还是查找层，但这预示着一个急剧的转变。</p><p><strong>实验</strong>：在我们的有效模型中采用两个令牌运动员姓名，在每层之后获取残差流，在包含 80% 姓名的训练集上训练逻辑回归探针，并对另外 20% 的姓名进行评估。该假设预测验证准确性将会发生急剧变化。</p><p><strong>结果</strong>：这是一个适度平滑的变化。为了鲁棒性，我们还检查有效模型预测下一项运动时的损失指标。当在完整模型中的最终名称标记上对残差流训练逻辑回归探针时，我们得到了类似的结果。这相当直接地反驳了早期层正在执行纯粹的、与数据无关的哈希的假设。然而，第 4 层和第 5 层之间存在显着增加，这表明查找存在一些专门化（这部分但不完全由第 5 层中的棒球神经元驱动）。对于每一层，我们报告 10 个随机种子的测试准确性（每次采用不同的 80/20 训练/测试分割并训练新的探针），因为数据集足够小，使其相当嘈杂。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><p><strong>细微差别</strong>：</p><ul><li>一些运动员的名字中可能有独特的标记，以便在嵌入中表示运动信息。我们可以看到，求和令牌的验证准确性优于随机令牌（50% 而不是 33%）。这并不奇怪，我们预计哈希和查找对其他运动员来说更重要。</li><li>这与部分哈希查找假设是一致的，尤其是在第 5 层中准确率显着提高。</li></ul><h3>已知名称比未知名称具有更高的 MLP 输出范数（负）</h3><p><strong>预测</strong>：散列预测早期层不会吸收数据分布的知识，因此应该对已知名称和未知名称进行区分。</p><p><strong>实验</strong>：我们测量已知名称和未知名称的 MLP 输出范数。为了获取姓名，我们对运动员数据集中所有单个标记的名字和姓氏进行笛卡尔积，并分离已知和未知的名字<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11">[11]</a></sup> 。此分析是在完整模型上执行的（但在有效模型上类似）</p><p><strong>结果</strong>：存在明显差异，已知名称具有更高的范数。这伪造了纯哈希，但不是部分哈希。即使在 MLP1 中也会发生这种情况，尽管 MLP1 不是我们有效模型的一部分<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12">[12]</a></sup> ，这令人惊讶。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" alt=""></p><h3>早期层确实破坏了线性结构（正面）</h3><p><strong>预测</strong>：早期的层破坏了线性结构。具体来说，即使残差流输入中存在线性结构，即它是来自不同特征（当前和先前标记）的项之和，MLP输出也不会具有这种线性结构。更弱的是，它预测一旦重新添加 MLP 输出，残差流将失去这种线性结构。</p><p>线性函数 f 的一个具体属性是 f(Michael Jordan) + f(Tim Duncan) = f(Michael Duncan) + f(Tim Jordan)，所以让我们尝试证伪这个！</p><p><strong>实验</strong>：</p><ul><li>我们选择一对已知的名字 A 和 B（例如 Michael Jordan 和 Tim Duncan）以及有效模型中的 MLP 层（例如 MLP 2）。<ul><li>我们取这些名称的 MLP 输出的中点 (MLP(A) + MLP(B)) /2。</li></ul></li><li>我们交换姓氏以获得名字 C 和 D（未知名字，例如 Michael Duncan 和 Tim Jordan），并取 C 和 D 上 MLP 输出的中点 (MLP(C) + MLP(D)) /2。</li><li>我们测量两个中点之间的距离。</li><li>为了将大数与小数联系起来，我们除以基线距离，该距离是通过用任意未知名称替换 C 和 D 并测量中点之间的距离 |((MLP(A) + MLP(B) - MLP(C) &#39;) - MLP(D&#39;))/2|<ul><li>这意味着，如果 MLP 完全打破线性结构，它将接近 1（即 Michael Duncan 和 Tim Jordan 与随机未知名字无法区分），而如果它保留线性结构，它将接近 0（因为这些将是平行四边形的四个顶点）<ul><li>具体来说，如果 MLP 是线性的，则 MLP(Michael Jordan) = MLP(Michael X) + MLP(Y Jordan)，因此 A&amp;B 和 C&amp;D 的中点应该相同</li></ul></li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" alt=""></p><p><strong>结果</strong>：大多数 MLP 层显示线性结构被显着（但未完全）破坏，往往是完全破坏线性结构的 60%-70%。 MLP2 的情况比 MLP3 到 6 的情况稍微不那么明显。</p><p>我们对不同层之后的残差流（而不是 MLP 输出）重复上述实验，绘制在同一张图（红色框）中，发现残差在各层之间的线性度降低，从第 2 层之后的约 30% 开始第 6 层后变为 50%（这是该层<em>末尾</em>的残差）。请注意，这些残差取自有效模型，该模型从第 2 层开始，而不是第 0 层。进一步注意，在有效模型中，MLP2 的输入是名称的总和标记，根据定义，它是线性的。</p><p><strong>细微差别</strong>：</p><ul><li> MLP 输出的结果并不令人惊讶——MLP 的全部意义就是成为一个非线性函数，所以它当然打破了线性结构！<ul><li>我们应该期望这个结果对于随机初始化的 MLP 来说是正确的</li><li>然而，事实证明，随机初始化的 MLP 对线性结构的破坏要少得多 (20-40%)！我们做了一个后续实验，随机调整 MLP 权重和偏差并重新运行模型。作为另一个基线，我们重新进行了交换未知姓名的名字/姓氏的实验，没有看到明显的变化。这表明模型有意使用 MLP 层来打破线性结构。 </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" alt=""></p><ul><li>它打破残差流中的线性结构的结果不那么微不足道，但仍然不足为奇 - new_residual 是 old_residual（线性）+ mlp_out（非线性），因此 new_residual 的线性程度直观上取决于相对大小。</li><li>总体而言，这是散列（在打破线性结构的意义上）发生的不足为奇的证据，但不是散列是它们所做的<em>全部的</em>证据，因此它不是完整散列和查找假设的有力证据（其中散列是早期 MLP 在电路中发挥的唯一作用）</li><li>尽管在概念上并不令人惊讶，但我们认为“早期 MLP<em>通常</em>会打破线性结构”是了解模型的一个有价值的事实，因为它表明线性表示的特征之间的干扰会随着深度的增加而累积。<ul><li>例如，Bricken 等人观察到许多稀疏自动编码器特征，例如“数学文本中的标记‘the’”。如果“is a math text”和“is the token &#39;the&#39;”都是线性表示的特征，那么 MLP 层表示交集也就不足为奇了，即使没有对该特征进行实际计算。</li></ul></li></ul><h3>棒球神经元作用于运动员残差的正交余数（不明确）</h3><p><em>元：本节记录了我们中的一个人最初感到兴奋的实验，但后来意识到可能是虚幻的，我们在这里描述它是为了教学目的</em></p><p><strong>预测</strong>：如果发生查找，这表明每个运动员的表示都有<em>特殊</em>信息 - 迈克尔·乔丹残差中有一些“是迈克尔·乔丹”信息，这对于最终生成“打篮球”的模型很重要，无法从其他篮球中恢复玩家。请注意，这显然是在对原始标记求和时发生的，但稍后可能不会发生。我们关注<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_">第 5 层的棒球神经元，</a>它似乎是查找电路的一部分，因为它具有显着的效果并直接增强棒球属性。</p><p>对比假设是，早期层（例如 2-4）会产生棒球比赛中“打棒球”的某种表示（可能与最终表示不同），而棒球神经元只是发出信号来增强这种表示。</p><p><strong>实验</strong>：为了测试这一点，我们采用了每个运动员的残差，并采用与所有其他运动员残差所跨越的子空间正交的分量（请注意，有 2560 个残差维度和大约 1500 个其他运动员，因此这删除了 ​​60% 的维度）。然后，我们将棒球神经元应用于该残差正交分量，并查看神经元输出的 ROC AUC，以预测运动员是否打棒球的二元变量</p><p><strong>结果</strong>：ROC 约为 60%，明显高于机会 (50%) - 明显比没有正交投影时差，但仍然有一些信号</p><p><strong>Nuance</strong> ：这被证明是虚幻的，因为“与所有其他运动员正交的项目”并不一定会删除与其他运动员共享的<em>所有</em>信息。玩具示例：假设每个棒球运动员残差都是“是棒球”方向加上显着的高斯噪声。如果我们从该分布中获取由 1500 个样本组成的子空间，由于每个样本都有噪声，因此该子空间中可能无法完全捕获“是棒球”方向，因此投影不会擦除它。这意味着，虽然我<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13">[13]</a></sup>发现这个实验的结果令人惊讶，但它并没有很好地区分这两个假设——部分散列和查找确实很难被证伪！ </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-KGJJcrC8izPbNFCPL-1" class="footnote-item"><p> GELU 与 ReLU 不同，但我们认为可以有效地将其视为“软 ReLU”，并且与 ReLU 相当接近，因此也可以相当好地实现 AND 门<a href="#fnref-KGJJcrC8izPbNFCPL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-2" class="footnote-item"><p>注意，这是哈希函数语句中的哈希，而不是哈希表。哈希函数接受任意输入并尝试产生与随机输出没有区别的输出。哈希表将哈希函数应用于输入，<em>然后</em>有目的地将其映射到某些存储的数据，这更类似于完整哈希和查找。 <a href="#fnref-KGJJcrC8izPbNFCPL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-3" class="footnote-item"><p>请注意，可能存在更复杂且基础对齐程度较低的查找形式，这些形式可能不太容易受到干扰，事实上，我们发现有迹象表明这个故事是混乱和复杂的<a href="#fnref-KGJJcrC8izPbNFCPL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-4" class="footnote-item"><p>另一位运动员的重新采样消融得到了类似的结果<a href="#fnref-KGJJcrC8izPbNFCPL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-5" class="footnote-item"><p>我们测量每个答案的第一个标记的对数概率，对于多标记答案，延续位于括号中，并且没有明确测试（一旦有了第一个标记，就可以很容易地使用二元组）</p><ul><li>打篮球运动</li><li>在北州（卡罗来纳州）上大学</li><li>1984年被选入NBA</li><li>为芝加哥队（公牛队）效力</li><li>是夏洛特队（黄蜂队）的大股东</li><li>主演电影《太空（果酱）》</li><li>为 NBA 联盟效力</li><li>1992年代表美国参加奥运会</li><li>玩数字23</li></ul> <a href="#fnref-KGJJcrC8izPbNFCPL-5" class="footnote-backref">↩︎</a></li><li id="fn-KGJJcrC8izPbNFCPL-6" class="footnote-item"><p>将神经元的值替换为数据集中所有 1500 名运动员的最终名称标记的平均值。 <a href="#fnref-KGJJcrC8izPbNFCPL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-7" class="footnote-item"><p>受到 Lovis Heindrich 和 Lucia Quirke 即将推出的作品的启发<a href="#fnref-KGJJcrC8izPbNFCPL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-8" class="footnote-item"><p>动机：E(A &amp; B) 对应于 Michael Jordan 上的激活，E(~A &amp; B) 对应于 Keith Jordan（对于 Keith 的不同值），E(A &amp; ~B) 对应于 Michael Smith（对于不同的值）史密斯）。神经元的激活通常具有远离零的平均值，因此我们从每个项中减去该平均值，该平均值由 E(~A &amp; ~B) 项捕获，即除迈克尔或乔丹之外的所有名字的平均值。并且 (E(A &amp; B) - E(~A &amp; ~B)) - (E(~A &amp; B) - E(~A &amp; ~B)) - (E(A &amp; ~B) - E(~ A &amp; ~B)) = E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <a href="#fnref-KGJJcrC8izPbNFCPL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-9" class="footnote-item"><p>我们采用中位数是因为有时总的或 GELU 派生的非线性效应是负/零，而中位数让我们可以忽略这些异常值<a href="#fnref-KGJJcrC8izPbNFCPL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-10" class="footnote-item"><p>虽然我们无法轻易判断出这是哪篇文章，但也许​​是棒球相关的文章，表现出类似<a href="https://arxiv.org/abs/2310.15154">总结主题的</a>东西！ <a href="#fnref-KGJJcrC8izPbNFCPL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-11" class="footnote-item"><p>将会出现一些错误分类，因为某些名称配对可能是已知实体。我们通过谷歌搜索名称进行了一些抽查，预计这不会对结果产生重大影响<a href="#fnref-KGJJcrC8izPbNFCPL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-12" class="footnote-item"><p>我们发现 MLP1 似乎与属性提取头的注意力相关（让它们检测姓名是否是运动员，从而是否提取一项运动），但对于查找运动员从事哪项运动并不重要。即对键重要但对值不重要。 <a href="#fnref-KGJJcrC8izPbNFCPL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-13" class="footnote-item"><p>使用单数是因为我的合著者认为这种替代解释一直是显而易见的<a href="#fnref-KGJJcrC8izPbNFCPL-13" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early<guid ispermalink="false"> CW5onXm6uZxpbpsRk</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:05 GMT</pubDate></item></channel></rss>