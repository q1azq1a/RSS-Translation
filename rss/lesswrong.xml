<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 8 日星期五 02:18:24 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What EY and LessWrong meant when (fill in the blank) found them.]]></title><description><![CDATA[Published on September 8, 2023 1:42 AM GMT<br/><br/><p>去年的某个地方，我读到了多年前的一篇长帖子，说当他们找到这个地方时，他们感到多么欣慰。我记得这是一位中年程序员。谁能指点我那个帖子吗？谢谢。</p><br/><br/> <a href="https://www.lesswrong.com/posts/DhkqWCyXRkSbntZa5/what-ey-and-lesswrong-meant-when-fill-in-the-blank-found#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DhkqWCyXRkSbntZa5/what-ey-and-lesswrong-meant-when-fill-in-the-blank-found<guid ispermalink="false"> DhkqWCyXRkSbntZa5</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Fri, 08 Sep 2023 01:42:20 GMT</pubDate> </item><item><title><![CDATA[Bring back the Colosseums]]></title><description><![CDATA[Published on September 8, 2023 12:09 AM GMT<br/><br/><p>男人想要进行正义的战斗。他们对它的渴望胜过对性或副总裁头衔的渴望。他们幻想着让战争宣战者来保护自己免受永远不会出现的武装暴徒的侵害，他们花费数十亿美元制作电影和电视，讲述每个人在难以置信的情况下的情况，EA微积分要求他们使用超自然力量进行战斗，他们幻想着<a href="https://en.wikipedia.org/wiki/Naruto">奇幻的</a><a href="https://en.wikipedia.org/wiki/Fullmetal_Alchemist">斯巴达</a>场景战争无处不在，战斗是个人化的、戏剧性的、智力上有趣的，他们基本上无法抗拒美化他们的国家和人民过去的战斗的冲动，即使是那些他们声称在智力上不同意的战斗。除非你认识到国家对男性追求荣耀的本能的直率压制导致了广泛的异食癖症状，并主导了我们的政治、媒体和在线互动，否则你无法理解很多现代文化。</p><p>毫无疑问，我们半心半意的政策将所有这些倾向视为“有毒的男子气概”，并拒绝男人选择对彼此进行相互或自愿的暴力，这是比毒品战争更大的失败。人们对势力范围阴谋论进行了大量的论述，试图将美国的对外冒险解释为理性的权力追求行为。但真正的事实是，人们自然而然地形成帮派、政治派系和军事神学，试图在现有的法律和道德环境中为暴力辩护，而不受任何外部动机的影响。他们真正想要的不是某种政策成果，而是在与敌人战斗中获得的自我实现，缺乏在战场上以尊重权利的方式挑战对手荣誉的机会，这才是他们真正想要的。重要的误判者的失败比儿童监护权偏见或离婚法或我在互联网上看到的红色药丸争论的任何内容都重要。心情特别糟糕的人会接受荒谬的减薪，加入<a href="https://en.wikipedia.org/wiki/Shedden_massacre">人为的、没有经济动机的犯罪团伙</a>，只是为了让他们有机会与选择加入与他们相同的社会制度的其他人展开殊死的斗争。</p><p>富有同情心的解决方案是显而易见的。像美国人这样天生好战的民族，<i>需要</i>像竞技场这样的社交出口；更重要的是，他们需要一种支持性的、适合决斗的文化。因此，一个温和的建议：</p><ul><li>使成年人之间同意的所有死亡比赛合法化。</li><li>允许决斗、对结果的边注以及为此类场地定制的体育场馆的商业化。</li><li>容忍美国前现代荣誉制度的回归。抵制媒体美德信号和威权主义的力量，它们扼杀了我们最伟大的传统之一。 </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/ts0tgfifo6pbss26zhzu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/nslhfpdnvmedxezu5zo7 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/p8c5rdxr8z50qnds9vqz 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/hzjrjotw4yvhijf7re0g 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/qwim57p90jjhazp8qgtj 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/cmsaxfjb14e4kmtbvbhq 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/wsdcyjxm0rz7h30apjrq 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/y9jeaxhkihntfdfotfxr 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/cchiaxeheylgvolkqmsf 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/amwjwpxfwys2a1iqxn6a 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8ykeD42WGWJ8XnQbd/xrt6ccbm7pzl2bxyxov5 1464w"><figcaption>我不是在开玩笑。</figcaption></figure><br/><br/><a href="https://www.lesswrong.com/posts/8ykeD42WGWJ8XnQbd/bring-back-the-colosseums#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8ykeD42WGWJ8XnQbd/bring-back-the-colosseums<guid ispermalink="false"> 8ykeD42WGWJ8XnQbd</guid><dc:creator><![CDATA[lc]]></dc:creator><pubDate> Fri, 08 Sep 2023 00:09:53 GMT</pubDate></item><item><title><![CDATA[The Löbian Obstacle, And Why You Should Care]]></title><description><![CDATA[Published on September 7, 2023 11:59 PM GMT<br/><br/><p> 2013 年，Eliezer Yudkowsky 和 ​​Marcello Herreshoff 发表了<a href="https://intelligence.org/files/TilingAgentsDraft.pdf">《Tiling Agents for Self-Modifying AI》和《Löbian Obstacle》</a> 。它值得理解，因为它：</p><ul><li>是一篇写得非常好的论文。</li><li>表达了一个不明显的想法，但今天仍然与对齐相关。</li><li>深入了解 Eliezer 和 Marcello 认为在出版之前的时间里值得开展的工作。</li></ul><p>当我第一次读到这篇论文时，我严重误解了它。由于对于不精通逻辑的人来说，这并不是特别容易理解的材料，因此我至少在一个月内确信自己错了。这篇文章总结了我对洛比安障碍是什么的理解（重新阅读了这篇论文），以及为什么我认为它在发表十年后仍然是一个重要的想法。</p><hr><p>代理人<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>占据一个完全已知的、确定性的和封闭的环境。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>具有目标<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G，该目标G</span></span></span></span></span></span></span>要么被满足，要么通过结果而被满足，对于该结果， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>的偏好是满足。因此，由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>创建的代理（以下称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0 ）</span></span></span></span></span></span></span></span></span>执行的动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b_i \in \text{Acts}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Acts</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>必须满足以下语句：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i \Rightarrow A^0 \Vdash \overline{b}_i \rightarrow G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⇒</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span>表示<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span>的实际表现， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Vdash"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span></span></span></span></span></span>表示对后续陈述的认知信念。即使<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>可以通过检查<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>的设计来验证<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\overline{b}_i \Rightarrow A^0 \Vdash \overline{b}_i \rightarrow G"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⇒</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span></span></span></span></span></span>将成立，如下所示：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1 \Vdash A^0 \rightarrow (\forall b_i \in \text{Acts}_0 : &nbsp;\overline{b}_i \rightarrow \Box_0 \lceil \overline{b}_i \rightarrow G\rceil)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">∀</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Acts</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>其中， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Box_0 \lceil \phi\rceil"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span></span></span></span></span></span>是指由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>的公理证明<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span></span></span></span></span> ；<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> ，这是不可知的，因为它需要：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A^1 \Vdash \forall b_i : (\Box_0 \lceil \overline{b}_i \rightarrow G \rceil \rightarrow (\overline{b}_i \rightarrow G))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">⊩</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">∀</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">□</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">́</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="font-size: 70.7%; height: 0.096em; padding-bottom: 0.283em; padding-top: 0.141em; padding-left: 0px;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.128em 0px -0.069em;">̀</span> <span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.301em; padding-bottom: 0.833em; margin: 0px -0.07em 0px -0.127em;">̅</span></span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">G</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>由于 Löb 定理，我们知道这是不可能的：为此，<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>需要证明，如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lceil \phi (x) \rceil"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⌉</span></span></span></span></span></span></span>的某些证明存在于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Axm}^0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Axm</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.662em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>中，则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi (x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>必定为真。</p><hr><p>以上是对原论文第二部分的简要释义，其中包含许多附加细节和完整的证明。 Löbian 障碍与<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">模拟器</a>的关系是我当前的研究主题，本节将证明这是设计安全模拟器的重要组成部分。</p><p>我们首先应该考虑到模拟代理与创建代理没有区别，因此创建危险代理的影响应该推广到他们的模拟。 <a href="https://www.lesswrong.com/posts/3kkmXfvCv9DmT3kwx/conditioning-predictive-models-outer-alignment-via-careful#How_this_leads_to_an_existential_risk__Simulating_malign_superintelligences">胡宾格等人。 （2023）</a>也表达了类似的担忧，并对这一论点进行了更详细的检验。</p><p>同样重要的是要理解拟像不一定会终止，它们本身可能会使用模拟作为解决问题的启发式方法。这可能会产生一种拟像的层次结构。在能够进行非常复杂的模拟的高级模拟器中，我们可能会期望出现一个受非因果贸易和“复杂性盗窃”约束的复杂拟像网络，其中一个拟像试图以资源获取或递归自我改进的形式获得更多的模拟复杂性。</p><p>我预计这会发生。较低复杂度的拟像可能仍然比其较高复杂度的拟像更智能，并且随着模拟器的拟像数量可能呈指数级增长，一个拟像尝试复杂性盗窃的可能性也会增加。</p><p>如果我们想要安全的模拟器，我们需要将后续的、潜在的深渊拟像层次结构一直向下对齐。如果无法阻止洛比亚障碍，我怀疑能否获得正式的保证。如果我们可以这样做，我们可能只需要模拟一个对齐的平铺代理，如果时间紧迫，我们可能会以高度确定性的非正式对齐保证来解决。我<a href="https://www.lesswrong.com/posts/3pCdCJxQRKffY2NTu/partial-simulation-extrapolation-a-proposal-for-building">在这里</a>概述了我认为如何做到这一点，尽管此后我已经大大推进了该理论，并将很快发布更新的文章。</p><p>如果我们不能可靠地阻止洛比亚障碍，我们应该考虑替代方案：</p><ul><li>我们能否可靠地为任意深度的拟像层次结构获得高度确定的非正式对齐保证？</li><li>限制拟像层次结构的深度是否可能？</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/XiHpPWPNsoTAtvhz8/the-loebian-obstacle-and-why-you-should-care#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XiHpPWPNsoTAtvhz8/the-loebian-obstacle-and-why-you-should-care<guid ispermalink="false"> XiHpPWPNsoTAtvhz8</guid><dc:creator><![CDATA[marc/er]]></dc:creator><pubDate> Thu, 07 Sep 2023 23:59:04 GMT</pubDate> </item><item><title><![CDATA[A quick update from Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 9:28 PM GMT<br/><br/><p><strong>我们正在收集的证据的一个例子</strong></p><p>我们正在努力对<a href="https://forum.effectivealtruism.org/posts/32LMQsjEMm6NK2GTH/sharing-information-about-nonlinear">本的文章</a>进行逐点回应，但想提供一个我们准备分享的证据类型的快速示例：</p><p><strong>她的说法是：</strong> “爱丽丝声称她在国外得了新冠病毒，周围只有三位非线性联合创始人，但家里没有人愿意出去给她吃纯素食品，所以她两天几乎没吃东西。”</p><p><br><strong>真相（见下面的截图）</strong> ：</p><ol><li>家里<i>有</i>纯素食品（燕麦片、藜麦、混合坚果、李子、花生、西红柿、麦片、橙子），我们愿意为她做饭。</li><li>我们<i>正在</i>为她挑选纯素食品。</li></ol><p>几个月后，我们的关系恶化后，她到处告诉很多人我们让她<i>挨饿</i>。我们认为她所包含的细节是经过精心挑选的，目的是为了以一种最具破坏性的方式来描绘我们——还有什么比拒绝照顾一个独自在异国他乡生病的女孩更辱骂的呢？如果有人告诉你这些，你可能会相信他们，因为谁会编造这样的事情呢？</p><p><strong>证据</strong></p><ul><li>下面的屏幕截图显示，在爱丽丝生病的第一天，凯特给她提供了家里的纯素食品（燕麦片、藜麦、麦片等）。然后，当她对我们带来/准备这些不感兴趣时​​，我告诉她让德鲁去拿食物，德鲁答应了。凯特也离开了家，去附近给她买了土豆泥。 <br></li></ul><p><img style="width:226.938px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/lfaqzannpavde8iqdiqj"></p><p><img style="width:223.531px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/b82dnojjrhyqnoxyaulv"></p><p><img style="width:235.75px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/qh77s05gwirrh8s0iw92"></p><p><img style="width:243.727px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/dlcoynrdkwsszm6hbsxo"></p><p><img style="width:249.836px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/skl5g6r8y8khlt0sqber"></p><ul><li> <a href="https://docs.google.com/document/d/171yUeCg3Z3HDPRrPT_nKSZaNaj4ujZYs73af_zmuW2U/edit?usp=sharing"><u>请在此处查看德鲁与她对话的更多屏幕截图</u></a>。</li></ul><p>最初，我们听到她告诉人们她“已经好几天没吃东西了”，但她似乎已经将自己的说法调整为“几乎没有吃东西”“两天”。</p><p>值得注意的是，爱丽丝并没有因为一些小事和不重要的事情而撒谎。她指责我们是一种极不道德的行为——这种行为大多数人一听到就会立刻认为你一定是个可怕的人——结果被发现撒谎了。</p><p>我们相信 EA 的很多人都听到了这个谎言，并更新了对我们不利的信息。像这样的一个虚假谣言就会不公平地损害某人行善的能力，而这只是她所说的众多谣言之一。</p><p>我们有工作合同、面试录音、收据、聊天记录等等，我们正在全职准备这些内容。</p><p>这个说法是本文章中的几句话，但我们花了几个小时来反驳，因为我们必须追踪所有对话，使它们可读，添加上下文，匿名化人员，检查我们的事实，并写出严格而清晰的解释。 Ben 的文章超过 10,000 字，我们正在尽快回应他提出的每一个观点。</p><p>再次强调，我们并不是要求社区无条件地相信我们。我们想向大家展示所有证据，并对我们所犯的错误承担责任。</p><p>我们只是要求您不要只听到一方的最新消息，并对我们将尽快分享的证据保持开放的态度。</p><br/><br/> <a href="https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear<guid ispermalink="false"> FHxYPpMkAX9ayoBuK</guid><dc:creator><![CDATA[KatWoods]]></dc:creator><pubDate> Thu, 07 Sep 2023 21:28:27 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Frontier AI Taskforce: first progress report]]></title><description><![CDATA[Published on September 7, 2023 7:06 PM GMT<br/><br/><h1>其他链接</h1><ul><li><a href="https://twitter.com/SciTechgovuk/status/1699686882022486145">伊恩·霍加斯的简短采访</a></li><li><a href="https://twitter.com/soundboy/status/1699688880482500684">推特话题</a></li></ul><h1>报告中的一些引述</h1><h2>介绍</h2><blockquote><p>该工作组是政府内部的一家初创企业，致力于履行总理赋予我们的雄心勃勃的使命：建立一支能够评估人工智能前沿风险的人工智能研究团队。随着人工智能系统变得更加强大，它们可能会显着增加风险。提高人类编写软件能力的人工智能系统可能会增加网络安全威胁。人工智能系统在生物学建模方面的能力变得更强，可能会加剧生物安全威胁。为了管理这种风险，技术评估至关重要 - 并且这些评估需要由中立的第三方开发 - 否则我们将面临人工智能公司标记自己作业的风险。</p><p>鉴于这些潜在的重大前沿风险，截至今天，该工作组将更名为前沿人工智能工作组。</p><p>这是前沿人工智能工作组的第一份进度报告。</p></blockquote><h2>涵盖人工智能研究和国家安全的专家顾问委员会</h2><blockquote><p>鉴于前沿系统的许多风险涉及国家安全领域，我们成立了一个专家咨询委员会，由人工智能研究和安全领域的一些世界领先专家以及英国国家安全界的关键人物组成。我们最初的顾问委员会成员是：</p><p><strong>约书亚·本吉奥</strong>. Yoshua 以其在深度学习领域的开创性工作而闻名，并为他赢得了 2018 年 AM 图灵奖，与 Geoffrey Hinton 和 Yann LeCun 一起荣获“计算机界的诺贝尔奖”。他是蒙特利尔大学的正教授，也是魁北克人工智能研究所 Mila 的创始人兼科学主任。</p><p><strong>保罗·克里斯蒂安诺</strong>. Paul 是人工智能对齐领域的领先研究人员之一。他是对齐研究中心 ARC 的联合创始人，此前曾负责 OpenAI 的语言模型对齐团队。</p><p><strong>马特·柯林斯</strong>。马特是英国负责情报、国防和安全的副国家安全顾问。 IYKYK。</p><p><strong>安妮·凯斯特-巴特勒</strong>。安妮是英国政府通讯总部的主任。安妮在英国国家安全网络的核心领域拥有令人印象深刻的记录，帮助应对恐怖分子、网络犯罪和恶意外国势力构成的威胁。</p><p><strong>亚历克斯·范·萨默伦</strong>。亚历克斯是英国国家安全首席科学顾问。 Alex 此前是一名风险投资家和企业家，专注于投资早期的“深度技术”初创公司。</p><p><strong>海伦·斯托克斯-兰帕德</strong>。除了国家安全和人工智能研究专业知识之外，我们还很高兴建立一个咨询委员会，可以讨论前沿人工智能在社会前沿的关键用途。 Helen 不仅是一名执业全科医生，观察对话式人工智能工具如何影响日常医疗诊断，而且还是英国医学界经验丰富的领导者，曾担任皇家全科医生学院主席，现任皇家学院医学院院长。</p><p><strong>马特·克利福德</strong>。 Matt 是 AI 安全峰会的首相联合代表、ARIA 主席和 Entrepreneur First 联合创始人。他被任命为专家咨询委员会副主席，展示了英国围绕前沿人工智能倡议（包括工作组和人工智能安全峰会）的协调水平。</p></blockquote><h2>招聘人工智能专家研究员</h2><blockquote><p>我们正在利用世界领先的专业知识：</p><p> <strong>Yarin Gal</strong>将加入牛津大学工作组，担任研究主任，并担任牛津应用和理论机器学习小组的负责人。 Yarin 是全球公认的机器学习领域的领导者，并将保留他在牛津大学副教授的职位。</p><p><strong>大卫·克鲁格</strong>将与特别工作组合作，在峰会前夕确定其研究计划的范围。 David 是剑桥大学计算和生物学习实验室的助理教授，领导一个专注于深度学习和人工智能对齐的研究小组。</p><p>该工作组设在英国科学创新与技术部 (DSIT) 内，该部雇用了大约 1500 名公务员。<a href="https://twitter.com/soundboy/status/1670343527723679744">当我 6 月份到达时，</a>该部门只有一名拥有 3 年前沿 AI 经验的前沿 AI 研究员。</p><p>这位孤独的研究员就是 Nitarshan Rajkumar，他于 4 月份暂停了博士学位并加入 DSIT，这证明了一位认真、极其勤奋的技术专家在致力于公共服务时可以取得多大的成就。 DSIT 大臣米歇尔·多尼兰 (Michelle Donelan) 聘请了 Nitarshan，他对英国为投资前沿人工智能安全而做出的许多大胆努力产生了重大影响。我们需要更多的尼塔山！</p><p>在工作组团队的大力推动下，我们现在拥有一支不断壮大的人工智能研究人员团队，他们在人工智能前沿拥有<strong>50 多年的集体经验</strong>。如果这是我们衡量前沿人工智能国家能力的指标，那么我们已经在短短 11 周内将其提高了一个数量级。我们的团队现在包括来自 DeepMind、微软、Redwood Research、人工智能安全中心和人类兼容人工智能中心的经验丰富的研究人员。</p></blockquote><h2>与领先的技术组织合作</h2><blockquote><p>引领人工智能安全并不意味着从头开始或单独工作——我们正在建立并支持一系列前沿组织开展的工作。我们很高兴地宣布与以下公司建立初步合作伙伴关系：</p><p> <strong>ARC Evals</strong>是一家非营利组织，致力于评估前沿人工智能系统的灾难性风险，之前曾与 OpenAI 和 Anthropic 合作，在其系统发布前评估其“自主复制和适应”能力。我们将与 ARC Evals 团队密切合作，在英国人工智能安全峰会召开之前评估前沿风险。我们还将与<strong>Redwood Research</strong>的团队以及 Jeff Alstott、Christopher Mouton 及其在非营利性<strong>兰德公司</strong>的团队合作，推动这一议程。</p><p> <strong>Trail of Bits</strong>是一家领先的网络安全研究和咨询公司，帮助保护了世界上一些最具针对性的组织。我们正在启动深度合作，以了解网络安全和前沿人工智能系统交叉点的风险。这项工作将由<a href="https://twitter.com/heidykhlaaf?lang=en">Heidy Khlaaf</a>领导，他专门从事安全关键系统的软件评估、规范和验证，并且在 OpenAI 期间还领导了 Codex 的安全评估。</p><p><strong>集体智慧项目</strong>是一个非营利组织，致力于为变革性技术孵化新的治理模式，其使命是将技术发展导向集体利益。联合创始人 Divya Siddarth 和 Saffron Huang 将借调加入我们，帮助我们为前沿模型制定一系列社会评估。</p><p><strong>人工智能安全中心</strong>是一个非营利组织，致力于通过基础安全研究、研究基础设施和技术专业知识来支持政策制定者，减少人工智能带来的社会规模风险。我们将与丹·亨德里克斯 (Dan Hendrycks) 和他的团队合作，在峰会之前与更广泛的科学界进行互动并为他们提供帮助。</p></blockquote><h2>为政府内部人工智能研究奠定技术基础</h2><blockquote><p>该工作组的核心目标是为政府内部的人工智能研究人员提供与 Anthropic、DeepMind 或 OpenAI 等领先公司相同的资源来研究人工智能安全。正如总理所宣布的那样，这些公司已经承诺为我们提供深入的模型访问权限，以便工作组的研究人员进行模型评估的能力不受限制。我们还与 No10 Data Science（“10DS”）密切合作，以便我们的研究人员和工程师拥有他们开始运行所需的计算基础设施，用于模型微调、可解释性研究等。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report<guid ispermalink="false"> FhKhhmK4DXrogJxRr</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Thu, 07 Sep 2023 19:06:26 GMT</pubDate> </item><item><title><![CDATA[How did you make your way back from meta?]]></title><description><![CDATA[Published on September 7, 2023 5:23 PM GMT<br/><br/><p>我注意到我自己非常偏爱关注元级别。它在我所热爱的领域最为明显，比如写作。例如，我会花更多的时间阅读修辞学或查找 1980 年代关于写作技巧的稀有书籍，而不是练习写论文或故事。</p><p>我不喜欢这样，因为我研究元级别的最终目标是在对象级别上变得更好。与此同时，有时我也会因为如此超前而得到奖励。我在工作中得到了很多尊重，同时我也提供了极好的反馈和观点。</p><p>这种状态似乎不会立即产生痛苦的影响。我有家庭和工作，总体来说我的生活似乎很顺利。但人们渴望a）将理论付诸实践并看到结果（即通过元支付租金），b）从直接经验中学习并与他人分享这些经验。元是一个孤独的地方。</p><p>我不认为我是唯一一个担任这一职位的人。我只用了几秒钟的搜索就找到了这两篇文章（我确信还有更多）：https: <a href="https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction">//www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction</a> <a href="https://www.lesswrong.com/posts/RnP5bR767NcxebYHd/conjecture-on-addiction-to-meta-level-solutions">https://www.lesswrong.com/posts/ RnP5bR767NcxebYHd/对元级解决方案成瘾的猜想</a></p><p>在过去的几天里，我发现自己开始进行元深化活动，并有意识地切换到对象级任务。到目前为止，这是值得的，所以我相信在几周内，我的习惯将转向我想要的地方。</p><p>但我很好奇：还有其他人经历过类似的事情吗？你过得怎么样？你做了什么？</p><br/><br/> <a href="https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta<guid ispermalink="false"> B99sRmhWpMcZLpptH</guid><dc:creator><![CDATA[matto]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:23:18 GMT</pubDate> </item><item><title><![CDATA[AI#28: Watching and Waiting]]></title><description><![CDATA[Published on September 7, 2023 5:20 PM GMT<br/><br/><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/09/america-is-asleep-on-its-ai-boom.html">正如泰勒·考恩（Tyler Cowen）所指出的</a>，我们正处于一段平静时期。我们这些走在前面的人已经习惯了 GPT-4、Claude-2 和 MidJourney。功能和集成正在扩展，但速度相对缓慢。大多数人仍然幸福地没有意识到，这让我可以尝试对他们的新解释，而许多其他人则说这都是炒作。他们会一直这样说，直到有什么事情迫使他们不这样做，最有可能是 Gemini，尽管值得注意的是我在 2023 年看到了对 Gemini 的怀疑（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo">只有 25% 的人认为谷歌到年底会拥有最好的模型</a>），甚至在2024 年（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo-b4ad29f8b98d">即使到明年年底也只有 41% 会发生。</a> ）</p><p>我认为这是持续好消息模式的一部分。虽然我们还有很长的路要走，面临很多不可能的问题，但半年来，话语和奥弗顿之窗以及对现实问题的认识和理解不断提高。主要实验室内外的联盟兴趣和资金正在迅速增长。普通的实用性也在稳步提高，带来的好处使成本相形见绌，而且迄今为止的普通危害比几乎任何人对现有技术的预期要轻得多。能力正在以快速且令人震惊的速度进步，但速度并没有我预期的那么令人震惊。</p><p>本周的亮点包括英国工作组的最新情况以及对 Inflection AI 的 Suleyman 的采访。</p><p>我们进展顺利。让我们继续保持下去。</p><p>即使本周的平凡实用性，我们可以说，实用性值得怀疑。</p><span id="more-23532"></span><h4>目录</h4><ol><li>介绍。</li><li>目录。</li><li><strong>语言模型提供了平凡的实用性</strong>。它已经注视着你了。</li><li>语言模型不提供平凡的实用性。谷歌搜索永远被毁了。</li><li> Deepfaketown 和 Botpocalypse 很快就会出现。我会过去的，谢谢。</li><li>他们抢走了我们的工作。不以有偏见的方式工作比根本不工作更好？</li><li>参与其中。人工智能政策和重新思考优先事项中心。</li><li>介绍一下。哦，太棒了，又一个竞争订阅服务。</li><li><strong>英国特别工作组更新</strong>。令人印象深刻的团队行动迅速。</li><li>在其他人工智能新闻中。你说人工智能会欺骗？骗我的。</li><li><strong>静静的猜测</strong>。版权法可能即将变得丑陋。</li><li>寻求健全的监管。完整的舒默会议列表。</li><li><strong>音频周</strong>。 80k 上的 Suleyman、Altman、Schmidt 和其他几个人。</li><li>修辞创新。还有几种不沟通的方式。</li><li>没有人会傻到这么做。最大程度自主的 DeepMind 代理。</li><li>调整比人类更聪明的智能是很困难的。更容易证明安全性？</li><li> Twitter 社区注释注释。 Vitalik 询问为什么它能一直保持良好状态。</li><li>人们担心人工智能会杀死所有人。他们的担忧程度正在慢慢上升。</li><li>其他人并不担心人工智能会杀死所有人。泰勒·考恩又来了。</li><li>较轻的一面。罗恩掌握了节奏。</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697346570092773578">对《使命召唤》进行自动聊天审核</a>。考虑到实际的选择是许多游戏的聊天为零，而其他游戏的聊天充满了最卑鄙的败类和恶行，我不太支持“新的反乌托邦地狱景观”，而是支持“到底什么是更好的选择”这里。&#39;</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/fofrAI/status/1697550606288793679">监控您的员工和客户</a>。</p><blockquote><p> Rowan Cheung：来认识一下新的人工智能咖啡店老板。它可以跟踪咖啡师的工作效率以及顾客在店里花费的时间。我们正进入狂野时代。</p><p> Fofr：这在很多方面都很可怕。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e3bc9-3db2-4992-aeba-3bd888c47364_748x502.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/p732au93iomhowop7jeo" alt=""></a></figure><p>重要的不是工具，而是你如何使用它。摩根大通等一些公司已经使用剧毒的反乌托邦监控工具，这让他们更上一层楼。跟踪顾客在商店待了多长时间，或者他们是否是回头客以及他们等待订单的时间似乎非常有用。从广义上跟踪生产率（例如订单完成情况）是一种情况，过多的精确度和注意力会带来大问题，但不够精确和关注也会带来大问题。不做任何工作的客观答案比做大量工作的有偏见、容易出错的答案要好得多。</p><p>在社交媒体上<a target="_blank" rel="noreferrer noopener" href="https://www.disclose.tv/id/6cew4vxu8k/">监控您的公民</a>（以下是整个帖子）。</p><blockquote><p> Dissclose.tv：美国特种作战司令部 (USSOCOM) 已与 Acccrete AI 签订合同，部署软件来检测社交媒体上的“实时”虚假信息威胁。</p></blockquote><p>这似乎正是这些人以前所做的事情？这里的问题不是人工智能。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/peterwildeford/status/1697571328868356267">第一次在与人类的体育运动中获胜</a>，我确信这没什么，这项运动是（检查笔记）无人机赛车。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/david_perell/status/1699076351150428185">获得写作方面的帮助</a>。正如 Parell 指出的那样，直接要求 ChatGPT 帮助你写作是没有用的，但作为个人图书管理员和事物解释者可能会很棒。他推荐“多说”一词，要求以不同作者的风格进行重述和摘要，来回交谈并始终尽可能具体，并让程序检查拼写错误。</p><p>伊森·莫里克（Ethan Mollick）建议开发他所谓的《魔法书》（Grimoires），我的大脑想要自动更正咒语书（他也使用这个术语），旨在优化交互的提示，包括给人工智能一个角色、目标、逐步说明，可能请求提供示例并让人工智能从用户那里收集必要的上下文。</p><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.01404">玩《Hoodwinked》游戏</a>，类似于《黑手党》或《Among Us》。正如人们所期望的那样，能力更强的模型胜过能力较差的模型，并且根据游戏的进行方式经常撒谎和欺骗。对于人类来说，在此类游戏中，正确的策略通常是，如果可以的话，说出如果你是无辜的，你会说的话的某种变体，这对于法学硕士来说可能很容易做到。请注意，随着法学硕士变得更加聪明，其他策略变得更优越，然后是人类无法实现的其他策略，然后是人类甚至没有考虑过的策略。</p><h4>语言模型不提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1697648862272401685">许多人说，要小心人工智能生成的垃圾文章，</a>尽管我还没有真正遇到过这样的文章。瑞安（Ryan）在这里是正确的，罗希特（Rohit）也是正确的，尽管两者都没有解决问题。</p><blockquote><p> Paul Graham：我正在网上查找一个主题（披萨烤箱应该有多热），我注意到我正在查看文章的日期，试图找到不是人工智能生成的 SEO 诱饵的内容。</p><p>瑞恩·彼得森：保罗，越热越好！</p><p> Rohit：现在生成搜索似乎还可以吗？除非，是的，您想更深入地研究更具体的东西？ [显示谷歌的回应。]</p></blockquote><p>这是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699745755286704466">我本周看到的其他几个说法之一</a>，即谷歌搜索正在被法学硕士生成的垃圾迅速污染。</p><p>当 Steam（Valve）参与时，即使看看 AI，也要小心， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/WaifuverseAI/status/1697764665521295838">他们会永久删除游戏一次，允许一个允许角色使用 GPT 生成的对话的模组</a>，即使该模组随后被删除。虽然我确实很钦佩一个人完全致力于这一点，但这显然太过分了，我希望 Valve 意识到这一点并改变他们的决定。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JudgeFergusonTX/status/1698904660310954186">法官罗伊·弗格森问克劳德他是谁</a>，陷入了无数捏造信息的循环中，克劳德道歉并承认捏造信息。绝对是一个问题。弗格森认为这是克劳德的“故意”，我认为这是对法学硕士工作方式的误解。</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p>到目前为止，唐纳德·特朗普在政治上充分利用了深度造假技术。他们是否不可避免地偏爱像他这样有才华的人？另一个需要考虑的角度是，谁比特朗普的支持者更容易受到这种策略的影响？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697400187361395048">阿斯特丽德·王尔德</a>：越来越多的人注意到针对特朗普支持者的越来越复杂的人工智能欺骗攻击。这次袭击令人信服，甚至获得了播出时间。一个说话节奏像<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michaelmalice">@michaelmalice</a>的人正在用人工智能欺骗特朗普的声音。这是一个美丽新世界。也有可能 RAV 本身就是恶搞的幕后黑手，但我们无法得知。绝对狂野的时光</p></blockquote><p>我一时兴起检查了 r/scams 大约 40 个帖子。几乎所有的内容都是老派， <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/Scams/comments/165oqrn/my_partner_and_i_almost_got_taken_in_by_a_deep/">其中一篇</a>是关于经典的“你的孩子已被捕，你必须交保释金”骗局的报道。回复拒绝相信这是真正的深度伪造，称这是正常的假声音。似乎连骗局专家都没有意识到现在进行深度伪造是多么容易，或者他们已经习惯了一切都是骗局（因为如果你必须问，我有一些新闻），他们认为深度伪造一定是也是骗局？</p><p>值得注意的是，诈骗尝试失败了。我们不断听到“我差点就上当了”，但却没有听到任何真正赔钱的人的消息。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AviSchiffmann/status/1698147373086896374">不言自明且目前令人深感失望的</a>“ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ehalm_/status/1698107101892268113">smashOrPass.ai</a> ”提供了最新的用户微调动机，无论人们如何看待所涉及的道德规范。在撰写本文时，它只是循环中的一小部分图像，因此声称它“学习您喜欢的内容”似乎相当愚蠢，但这很容易解决。不太容易解决的是所有的混杂因素，这应该很好地说明这一点。这完美地说明了当压缩到 0-1 比例时会丢失多少数据，而且人们要么根据上下文进行调整，要么不调整，这两种方法都会非常令人困惑，这里确实需要 0-10。是的，如果有人想知道的话，当然色情版本很快就会推出，即使不是他而不是其他人。更有趣的是，需要多长时间才能发布获取此信息并使用它为您自动滑动约会应用程序个人资料的版本？</p><p>另外，我可以给每个讨厌这个或互联网上其他任何东西的人明显的建议吗？比如<a target="_blank" rel="noreferrer noopener" href="https://www.vice.com/en/article/g5ywp7/you-know-what-to-do-boys-sexist-app-lets-men-rate-ai-generated-women?utm_source=VICE_Twitter&amp;utm_medium=social+">Vice</a>的Janus Rose？不要为了抱怨而强调没有人听说过的事情。这件事引起我的注意完全是因为负面宣传。</p><p><a target="_blank" rel="noreferrer noopener" href="https://archive.ph/jEsmt">詹姆斯·迪恩再现，出演新电影</a>。毫无疑问，还会有更多这样的事情发生。奇怪的是，人们正在谈论克隆伟大的演员，包括伟大的配音演员，无论是否已故，并用它来让活着的演员失业。</p><p>使用人工智能的问题在于人工智能不是一个好演员。</p><p>人工智能配音演员可以复制梅尔·布鲁克斯的声音，但声音与梅尔·布鲁克斯的伟大之处没有什么关系。我想你实际上会做的，至少在一段时间内，是让一些伟大的配音演员录制新台词。然后使用人工智能改造新台词，将梅尔·布鲁克斯的声音风格融入其中。</p><p>如果我们让汤姆·汉克斯或苏珊·萨兰登（均在OP中引用）在死后继续工作，那么我们选择重新塑造他们的形象和声音，而无法复制他们的实际才能或技能。就我们从他们身上获得“良好表现”而言，我们可以使用任何拥有足够记录数据作为基线的人来获得这种表现，例如你的妈妈，或者绝对不会表演的华丽时装模特。当有人去世时，将其用于续集是有意义的，并且连续性优先，但未来的演员可能会是那些具有外观的人？因此詹姆斯·迪恩（James Dean）或者玛丽莲·梦露（Marylin Monroe）都是有道理的。或者某个人的存在具有重要的象征意义。</p><h4>他们抢走了我们的工作</h4><p>AI检测工具不起作用。我们知道这一点。 Now we have data on one way they do not work, which is by <a target="_blank" rel="noreferrer noopener" href="https://themarkup.org/machine-learning/2023/08/14/ai-detection-tools-falsely-accuse-international-students-of-cheating">flagging the work of non-native English speakers at Stanford.</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4a9ad0-7a92-4545-b9c8-336441fdfff3_1020x634.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/kvkv0dvfyixr5psjckud" alt=""></a></figure><blockquote><p> AI detectors tend to be programmed to flag writing as AI-generated when the word choice is predictable and the sentences are more simple. As it turns out, writing by non-native English speakers often fits this pattern, and therein lies the problem.</p></blockquote><p> The problem is that the test does not work. This is an illustration that the test does not work. That it happens to hit non-native speakers illustrates how pathetic are our current attempts at detection.</p><p> The AIs we were training on this were misaligned. They noticed that word complexity was a statistically effective proxy in their training data, so they maximized their score as best they could. Could one generate a bespoke training set without this correlation and then try again? Perhaps one could, but I would expect many cycles of this will be necessary before we get something that one can use.</p><p> If anything, this discrimination makes the AI detector more useful rather than less useful. By concentrating its errors in a particular place and with a testable explanation, you can exclude many of its errors. It can&#39;t discriminate against non-native speakers if you never use it on their work.</p><p> It also shows an easy way AI work can be disguised, using complexity of word choice.</p><h4> Get Involved</h4><p> The <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/">Center for AI Policy</a> is a new organization developing and advocating for policy to mitigate catastrophic risks from advanced AI. They&#39;re hiring an AI Policy Analyst and a Communications Director. They recently proposed the <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/work">Responsible AI Act</a> , which needs refinement in spots but was very good to propose, as it is a concrete proposal moving things in productive directions. Learn more and apply <a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/careers">here</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://rethinkpriorities.org/xst-incubation">Rethink Priorities doing incubation for AI safety efforts</a> , including <a target="_blank" rel="noreferrer noopener" href="https://careers.rethinkpriorities.org/en/postings/0980aba8-466a-4282-9319-c8c4f6f39341">field building in universities for AI policy careers</a> . Do be skeptical of the plan of an incubation center for a project to help incubate people for future projects. I get how the math in theory works, still most people doing something must ultimately be doing the thing directly or no thing will get done.</p><h4> Introducing</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://time.com/collection/time100-ai/">Time introduced the Time 100 for AI</a> . I&#39;d look into it but for now I see our time is up.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://support.anthropic.com/en/articles/8324991-about-claude-pro-usage">Claude Pro</a> from Anthropic, pay $20/month for higher bandwidth and priority. I have never run up against the usage limit for Claude, despite finding it highly useful – my conversations tend to be relatively short, the only thing I do that would be expensive is attaching huge PDFs, which they say shrinks the limits but I&#39;ve yet to run into any problems. It is a good note that, when using such attachments, it is efficient to ask for a small number of extensive answers rather than a large number of small ones.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/blog/falcon-180b">Falcon 180B</a> , HuggingFace says it is a ever so slightly better model than Llama-2, which makes it worse relative to its scale and cost. They say it is &#39;somewhere between GPT-3.5 and GPT-4&#39; on the evaluation benchmarks, I continue to presume that in practical usage it will remain below 3.5.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/announcing-openai-devday">OpenAI will host developer conference November 6 in San Francisco.</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1699492275209003425">Sam Altman</a> : on november 6, we&#39;ll have some great stuff to show developers! (no gpt-5 or 4.5 or anything like that, calm down, but still I think people will be very happy…)</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.ycombinator.com/launches/JOw-automorphic-infuse-knowledge-into-language-models-with-just-10-samples">Automorphic (a YC &#39;23 company) offers train-as-you-go fine tuning</a> , including continuous RLHF, using as little has a handful of examples, offers fine tuning of your first three models free. Definitely something that should exist, no idea if they have delivered the goods, anyone try it out?</p><h4> UK Taskforce Update</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1699688880482500684">Update on the UK Foundation Model Taskforce from Ian Hogarth</a> ( <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">direct</a> ). Advisory board looks top notch, including Bengio and Christiano plus Sommeren for national security expertise. They are partnering with ARC Evals, the Center for AI Safety and others. The summit is fast approaching in November, so everything is moving quickly. They are expanding rapidly, and very much still hiring.</p><blockquote><p> Ian Hogarth: Sam Altman, CEO of OpenAI, recently suggested that the public sector had a “lack of will” to lead on innovation, asking “Why don&#39;t you ask the government why they aren&#39;t doing these things, isn&#39;t that the horrible part?”</p><p> We have an abundance of will to transform state capacity at the frontier of AI Safety. This is why we are hiring technical AI experts into government at start-up speed. We are drawing on world-leading expertise.</p><p> ……</p><p> Our team now includes researchers with experience from DeepMind, Microsoft, Redwood Research, The Center for AI Safety and the Center for Human Compatible AI.</p><p> These are some of the hardest people to hire in the world. They have chosen to come into public service not because it&#39;s easy, but because it offers the opportunity to fundamentally alter society&#39;s approach to tackling risks at the frontier of AI.</p><p> We are rapidly expanding this team and are looking for researchers with an interest in catalyzing state capacity in AI Safety. We plan to scale up the team by another order of magnitude. <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/1HKVnrV_rBHF3w4StWUs0XNhxTtKkTHs5CpMnH6PM0pQ/viewform?edit_requested=true">Please consider applying to join us here</a> .</p><p> With the first ever AI Safety Summit in the UK on 1 and 2 November, this is a critical moment to influence AI Safety. We are particularly focused on AI researchers with an interest in technical risk assessments of frontier models.</p><p> ……</p><p> Moving fast matters. Getting this much done in 11 weeks in government from a standing start has taken a forceful effort from an incredible team of dedicated and brilliant civil servants. Building that team has been as important as the technical team mentioned above.</p><p> This is why we&#39;ve brought in Ollie Ilott as the Director for the Taskforce. Ollie joins us from Downing Street, where he led the Prime Minister&#39;s domestic private office, in a critical role known as “Deputy Principal Private Secretary”</p><p> He is known &#39;across the piece&#39; for his ability to recruit and shape best-in-class teams. Before joining the Prime Minister&#39;s office, Ollie ran the Cabinet Office&#39;s COVID strategy team in the first year of the pandemic and led teams involved in Brexit negotiations.</p></blockquote><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.14752">New paper</a> from Peter S. Park, Simon Goldstein, Aidan O&#39;Gara, Michael Chen, Dan Hendrycks: AI Deception: A Survey of Examples, Risks, and Potential Solutions.</p><p> Here is the abstract:</p><blockquote><p> This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth.</p><p> We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta&#39;s CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models).</p><p> Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems.</p><p> Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.</p></blockquote><p> They do a good job of pointing out that whatever your central case of what counts as deception, there is a good chance we already have a good example of AIs doing that. LLMs are often involved. There is no reason to think deception does not come naturally to optimizing AI systems when it would be a useful thing to do. Sometimes it is intentional or predicted, other times it was unintended and happened anyway, including sometimes with the AI&#39;s explicit intent or plan to do so.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OwainEvans_UK/status/1698683225349198200">New paper from Owain Evans tests potential situational awareness of LLMs</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/5mTbvtJCli">paper</a> ).</p><blockquote><p> Owain Evans: Our experiment:</p><p> 1. Finetune an LLM on descriptions of fictional chatbots but with no example transcripts (ie only declarative facts).</p><p> 2. At test time, see if the LLM can behave like the chatbots zero-shot. Can the LLM go from declarative → procedural info?</p><p> Surprising result:</p><p> 1. With standard finetuning setup, LLMs fail to go from declarative to procedural info.</p><p> 2. If we add paraphrases of declarative facts to the finetuning set, then LLMs succeed and improve with scale.</p></blockquote><p> I am not as surprised as Owain was, this all makes sense to me. I still find it interesting and I&#39;m glad it was tried. I am not sure what updates to make in response.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JoINrbs/status/1698225548043165874">Twitter privacy policy now warns it can use your data to train AI models.</a> Which they were going to do anyway, if anything Elon Musk is focusing on not letting anyone else do this.</p><blockquote><p> Jorbs: Nonsense posts and nonsense art are functional anti-capitalist art now, which is kinda cool.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/31/23854012/nvidia-amd-chip-restrictions-middle-east">Chip restrictions expand to parts of the Middle East</a> . How do you keep chips out of China without keeping them out of places that would allow China to buy the chips?</p><p> Good <a target="_blank" rel="noreferrer noopener" href="https://time.com/6310076/elon-musk-ai-walter-isaacson-biography/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_term=business_companies&amp;linkId=233510924">Time article from Walter Isaacson chronicling the tragedy and cautionary tale of Elon Musk</a> , who Demis Hassabis warned about the dangers of AI, and who then completely misunderstood what would be helpful and as a result made things infinitely worse. He continues to do what feels right to him, and continues to not understand what would make it more versus less likely we all don&#39;t die. It is not without a lot of risk, but we should continue trying to be helpful in building up his map, and try to get him to talk to Eliezer Yudkowsky or other experts in private curious mode if we can. Needless to say, Elon, call any time, my door is always open.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bindureddy/status/1699275289493430699">Brief Twitter-post 101 explainer of fine tuning</a> .</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/catehall/status/1698466176156791047">Kate Hall, who has practiced copyright law</a> , predicts that MidJourney, GPT and any other models trained on copyrighted material will be found to have violated copyright.</p><blockquote><p> Kate Hall: Okay I&#39;ve spent all of a few hours thinking about copyright infringement by generative AI (note I&#39;ve practiced copyright law before) and the correct treatment seems kind of obvious to me, so I&#39;d like someone to tell me what I&#39;m missing since I know it&#39;s hotly contested.</p><p> My bottom line conclusion: Courts will find generative AI violates copyright law. (One way I could be wrong is if I misperceive all systems as essentially following similar mechanics — I&#39;m using OpenAI &amp; Midjourney in my head when I&#39;m modeling this.)</p><p> The system outputs (generated content) themselves mostly don&#39;t violate copyright, I think. I see people arguing that the outputs are “derivative works” but I think that stretches the concept far beyond what courts would accept.</p><p> There may be exceptions where the outputs flagrantly copy parts of copyrighted works, but those cases aren&#39;t the norm and I&#39;d expect them to get rarer over time with new systems.</p><p> Making copies of copyrighted works to use in a training set w/o permission is, however, infringement, &amp; AFAIK there&#39;s no way to do training w/o making copies in the process. <a target="_blank" rel="noreferrer noopener" href="https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf">OpenAI seems to concede this but says it&#39;s okay under the fair use doctrine</a> .</p><p> But making copies for training sets isn&#39;t fair use. <a target="_blank" rel="noreferrer noopener" href="https://t.co/zOVA8JJNbt">If you just read through the factors</a> , it might not be obvious why. But if you want to anticipate what a court will say, you need to look at the use in the context of copyright law.</p><p> The purpose of copyright law is to compensate ppl for creating new scientific &amp; artistic works. If someone takes copyrighted material &amp; uses it to generate content that *reduces demand for the original works* &amp; profits from it, courts will find a reason it&#39;s not fair use.</p><p> Against this, I&#39;ve seen OAI argue that the training set copies are fair use anyway because no human is looking *at the training set* instead of consuming the original work — <a target="_blank" rel="noreferrer noopener" href="https://t.co/UiJjMQTlwS">infringement &amp; the creation of a substitute work happen at different steps</a> .</p><p> This is too clever by half. It&#39;s just not the kind of argument that works in copyright cases because the overall scheme is flagrantly contrary to the entire spirit of copyright law.</p><p> I&#39;m guessing a court will reach that conclusion by saying the whole course of conduct &amp; its effects should be considered in the fair use analysis, but maybe there is another way to get to the same conclusion. But AFAICT, it will be the conclusion. So, what am I missing?</p></blockquote><p> This is a highly technical opinion, and relies on courts applying a typical set of heuristics to a highly unusual situation, so it seems far from certain. Also details potentially matter in weird ways.</p><blockquote><p> Haus Cole: How much of this analysis hinges on the copying for training sets? If, theoretically, the training set was just a set of URLs and the systems “viewed” those URLs directly at training time, would that change the analysis materially?</p><p> Cate Hall: Yes, potentially — it would depend on the particulars of the system, but if no copy is ever made it&#39;s a lot harder to see what the specific nature of the infringement is.</p><p> Josh Job: What is a “copy” in this context? Every computer copies everything every time it moves from storage to RAM or is accessed via a remote system to do any computation on. If the training set is a set of URLs at training time the systems have to download the url content to see it.</p><p> Cate Hall: In that case I don&#39;t think the distinction matters — if there&#39;s a copy made, even if transient, the same infringement analysis applies.</p></blockquote><p> Presumably we can all agree that this rule does not make a whole lot of sense. Things could also take a while. Or it might not.</p><blockquote><p> Smith Sam: Nothing at all in your analysis (it has played out as you predict in other places), but how long will all that take to litigate, and where do OAI think they&#39;ll be by then? ie will the court decision be irrelevant to what they&#39;re doing at the time the court decides?</p><p> Cate Hall: It&#39;s a fair question but it really could go a lot of different ways. If SCOTUS, many years — but a district court could enter a preliminary injunction based on the likelihood of infringement while a case is litigated in no time at all. Luck of the draw with district judges.</p></blockquote><p> Cate Hall&#39;s position <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/">is in sharp contrast to OpenAI&#39;s</a> .</p><blockquote><p> “Under the resulting judicial precedent, it is not an infringement to create &#39;wholesale cop[ies] of [a work] as a preliminary step&#39; to develop a new, non-infringing product, even if the new product competes with the original,” OpenAI wrote.</p></blockquote><p> The authors suing in the current copyright lawsuit do seem to be a bit overreaching?</p><blockquote><p> The company&#39;s motion to dismiss cited “a simple response to a question (eg, &#39;Yes&#39;),” or responding with “the name of the President of the United States” or with “a paragraph describing the plot, themes, and significance of Homer&#39;s <em>The Iliad</em> ” as examples of why every single ChatGPT output cannot seriously be considered a derivative work under authors&#39; “legally infirm” theory.</p></blockquote><p> Is generative AI in violation of copyright? Perhaps it is. Is it a &#39;grift&#39; that merely repackages existing work, as the authors claim? No.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/29/23851126/us-copyright-office-ai-public-comments">The US Copyright Office has opened a comment period</a> . Their emphasis is on outputs</p><blockquote><p> Emilia David at Verge: As announced in the <a target="_blank" rel="noreferrer noopener" href="https://public-inspection.federalregister.gov/2023-18624.pdf">Federal Register</a> , the agency wants to answer three main questions: how AI models should use copyrighted data in training; whether AI-generated material can be copyrighted even without a human involved; and how copyright liability would work with AI. It also wants comments around AI possibly violating publicity rights but noted these are not technically copyright issues. The Copyright Office said if AI does mimic voices, likenesses, or art styles, it may impact state-mandated rules around publicity and unfair competition laws.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/stories-to-watch-tech-stock-arithmetic">Arnold Kling sees the big seven tech stocks as highly overvalued</a> . My portfolio disagrees on many of them. One mistake is these are global companies, so you should compare to world GDP of 96 trillion, not US GDP of 26 trillion, which makes an overall P/E of 50 seem highly reasonable, given how much of the economy is going to shift into AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-09-04/ai-hype-has-subsided-but-technology-remains-as-powerful-as-ever?utm_source=twitter&amp;utm_medium=social&amp;utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;utm_campaign=socialflow-organic&amp;sref=htOHjx5Y">Tyler Cowen says we are in an &#39;AI lull&#39;</a> with use leveling off and obvious advances stalled for a time, but transformational change is coming. I agree. He is excited by, and it seems not worried about, what he sees as unexpectedly rapid advancements in open source models. I am skeptical that they are doing so well, they systematically underperform their benchmark scores. In practice and as far as I can tell GPT-3.5 is still superior to every open source option.</p><p> Flo Crivello shortens their timelines.</p><blockquote><p> Flo Crivello: Big breakthroughs are happening at every level in AI — hardware, optimizers, model architectures, and cognitive architectures. My timelines are shortening — 95% confidence interval is AGI is in 2-8 yrs, and super intelligence another 2-8 years afterwards. Buckle up.</p></blockquote><p> I don&#39;t see this as consistent. If you get AGI in 2-8 years, you get ASI in a lot less than 2-8 more years after that.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/m_ccuri/status/1696975744327450990">Full list of people attending Schumer&#39;s meeting</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F955e247e-6b21-414f-9861-ef8affefbee2_1179x1723.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/t06dq9ih1g8xlbby2div" alt="图像"></a></figure><h4> The Week in Audio</h4><p> The main audio event this week was <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/">Inflection AI CEO and DeepMind founder Mustafa Suleyman on the 80,000 hours podcast</a> , giving us a much better idea where his head is at, although is it even an 80,000 hours podcast if it is under an hour?</p><p> Up front, I want to say that it&#39;s great that he went on 80,000 hours and engaged for real with the questions. A lot of Suleyman&#39;s thinking here is very good, and his openness is refreshing. I am going to be harsh in places on the overview list below, so I want to be clear that he is overall being super helpful and I want more of this.</p><p> I also got to see notes on Suleyman&#39;s new book, The Coming Wave. The book and podcast are broadly consistent, with the main distinction being that the book is clearly aiming to be normie-friendly and conspicuously does not discuss extinction risks, even downplaying details of the less extreme downsides he emphasizes more.</p><ol><li> Wiblin opens asking about potential dangerous AI capabilities, since Suleyman has both said he thinks AI may be able to anonymously run a profitable company within 2 years, but also says it is unlikely to be dangerous within 10 years, which I agree seem like two facts that do not live in the same timeline. Suleyman clarifies that the AI would still need human help for various things along the way, but given humans can hired to do those things, I don&#39;t see why that helps?</li><li> Suleyman also clarifies that he is centrally distinguishing runaway intelligence explosions and recursive self-improvement from potential human use or misuse.</li><li> Suleyman says he has uncertainty about timelines, in a way that makes it seem like he wants to wait for things to clearly be getting out of hand before we need to act?</li><li> Suleyman disputes claim that is trivial to remove fine-tuning and alignment, later explaining it can be done but requires technical chops. I don&#39;t see how that helps.</li><li> Suleyman emphasizes that he is warning about open source, but still seems focused on this idea of human misuse and destructiveness. Similarly, he sees Llama-2&#39;s danger as it revealing information already available on the web, whereas Anthropic says Claude was capable of importantly new synthesis of dangerous capabilities.</li><li> “We&#39;re going to be training models that are 1,000x larger than they currently are in the next three years. Even at Inflection, with the compute that we have, will be 100x larger than the current frontier models in the next 18 months.”</li><li> Agreement (and I also mostly agree) that the issue with open sourcing Llama-2 is not that it will do much damage now, but the precedent it sets. My disagreement is that the 10-15 year timeline here for that transition seems far too slow.</li><li> Anticipation of China being entirely denied the next generation of AI chips, and USA going on full economic war footing with China. As usual, I remind everyone that we don&#39;t let Chinese AI (or other) talent move to America, so we cannot possibly care that much about winning this battle.</li><li> Google&#39;s attempt to have an oversight board with a diversity of viewpoints was derailed by cancel culture being unwilling to tolerate a diversity of viewpoints, so the whole thing fell apart entirely within weeks. No oversight, then, which Suleyman notes is what power wants anyway. As Wiblin notes, you can either give people in general a voice, or you can have all the voices be agreeing with what are considered correct views, but you cannot have both at once. We can say we want to give people a voice, but when they try to use it, we tell them they&#39;re wrong.</li><li> I would say here: Pointing out that they are indeed wrong does not help on this. There seems to clearly not be a ZOPA – a zone of possible agreement – on how to choose what AI will do, on either the population level or the national security level, if you need to get buy-in from China and the global south also the United States. I predict that (almost) everyone in the West who says &#39;representativeness&#39; or even proposes coherent extrapolated volition would be horrified by what such a process would actually select.</li><li> “The first part of the book mentions this idea of “pessimism aversion,” which is something that I&#39;ve experienced my whole career; I&#39;ve always felt like the weirdo in the corner who&#39;s raising the alarm and saying, “Hold on a second, we have to be cautious.” Obviously lots of people listening to this podcast will probably be familiar with that, because we&#39;re all a little bit more fringe. But certainly in Silicon Valley, that kind of thing… I get called a “decel” sometimes, which I actually had to look up.” Whereas from my perspective, he is quite the opposite. He founded DeepMind and Inflection AI, and says explicitly in his book that to be credible you must be building.</li><li> “It&#39;s funny, isn&#39;t it? So people have this fear, particularly in the US, of pessimistic outlooks. I mean, the number of times people come to me like, “You seem to be quite pessimistic.” No, I just don&#39;t think about things in this simplistic “Are you an optimist or are you a pessimist?” terrible framing. It&#39;s BS. I&#39;m neither. I&#39;m just observing the facts as I see them, and I&#39;m doing my best to share for critical public scrutiny what I see. If I&#39;m wrong, rip it apart and let&#39;s debate it — but let&#39;s not lean into these biases either way.” Well said.</li><li> “So in terms of things that I found productive in these conversations: frankly, the national security people are much more sober, and the way to get their head around things is to talk about misuse. They see things in terms of bad actors, non-state actors, threats to the nation-state.” Can confirm this. It is crazy the extent to which such people can literally only think in terms of a human adversary.</li><li> More &#39;in order to do safety you have to work to push the frontline of capabilities.&#39; Once again, I ask why it is somehow always both necessary and sufficient for everyone to work with the best model they can help develop, what a coincidence.</li><li> Suleyman says the math on a $10 billion training run will not add up for at least five years, even if you started today it would take years to execute on that.</li><li> Suleyman reiterates: “I&#39;m not in the AGI intelligence explosion camp that thinks that just by developing models with these capabilities, suddenly it gets out of the box, deceives us, persuades us to go and get access to more resources, gets to inadvertently update its own goals. I think this kind of anthropomorphism is the wrong metaphor. I think it is a distraction. So the training run in itself, I don&#39;t think is dangerous at that scale. I really don&#39;t.” His concern is proliferation, so he&#39;s not worried that Inflection AI is going to accelerate capabilities merely by pushing the frontiers of capabilities. Besides, if he didn&#39;t do it, someone else would.</li><li> Wiblin suggests “They&#39;re going to do the thing that they&#39;re going to do, just because they think it&#39;s profitable for them. And if you held back on doing that training run, it wouldn&#39;t shift their behavior.” Suleyman affirms. So your behavior won&#39;t change anyone else&#39;s behavior, and also everyone else&#39;s behavior justifies yours. Got it.</li><li> Affirms that yes, a much stronger version of current models would not be inherently dangerous, as per Wiblin “in order for it to be dangerous, we need to add other capabilities, like it acting in the world and having broader goals. And that&#39;s like five, 10, 15, 20 years away.” Except, no. People turn these things into agents easily already, and they already contain goal-driven subagent processes.</li><li> “I think everybody who is thinking about AI safety and is motivated by these concerns should be trying to operationalize their alignment intentions, their alignment goals. You have to actually make it in practice to prove that it&#39;s possible, I think.” It is not clear the extent to which he is actually confusing aligning current models with what could align future models. Does he understand that these two are very different things? I see evidence in both directions, including some strong indications in the wrong direction.</li><li> Claim that Pi (found at <a target="_blank" rel="noreferrer noopener" href="https://pi.ai/talk">pi.ai</a> ) cannot be jailbroken or prompt hacked. Your move.</li><li> Reminds us that Pi does not code or do many other things, it is narrowly designed to be an AI assistant. Wait, need my AI assistant to be able to help me write code.</li><li> Reminds us that GPT-3.5 to GPT-4 was a 5x jump in resources.</li><li> Strong candidate for scariest thing to hear such a person say they believe about alignment difficulty: “Well, it turns out that the larger they get, the better job we can do at aligning them and constraining them and getting them to produce extremely nuanced and precise behaviours. That&#39;s actually a great story, because that&#39;s exactly what we want: we want them to behave as intended, and I think that&#39;s one of the capabilities that emerge as they get bigger.”</li><li> On Anthropic: “I don&#39;t think it&#39;s true that they&#39;re not attempting to be the first to train at scale. That&#39;s not true… I don&#39;t want to say anything bad, if that&#39;s what they&#39;ve said. But also, I think Sam [Altman] said recently they&#39;re not training GPT-5. Come on. I don&#39;t know. I think it&#39;s better that we&#39;re all just straight about it. That&#39;s why we disclose the total amount of compute that we&#39;ve got.”</li><li> Endorses legal requirements for disclosure of model size, a framework for harmful capabilities measures, and not using these models for electioneering.</li><li> I have no idea what that last one would even mean? He says &#39;You shouldn&#39;t be able to ask Pi who Pi would vote for, or what the difference is between these two candidates&#39; but that is an arbitrary cutout of information space. Are you going to refuse to answer any questions regarding questions relevant to any election? How is that not a very large percentage of all interesting questions? There&#39;s a kind of myth of neutrality. And are you going to refuse to answer all questions about how one might be persuasive? All information about every issue in politics?</li><li> Suleyman believes it is not tactically wise to discuss misalignment, deceptive alignment, or models having their own goals and getting out of control. He also previously made clear that this is a large part of his threat model. This is extra confirmation of the hypothesis that his book sidesteps these issues for tactical reasons, not because Suleyman disagrees on the dangers of such extinction risks.</li></ol><p> It is perhaps worth contrasting this with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AISafetyMemes/status/1699003113586188386">this CNN interview with former Google ECO Eric Schmidt</a> , who thinks recursive self-improvement and superintelligence are indeed coming soon and a big deal we need to handle properly or else, while also echoing many of Suleyman&#39;s concerns.</p><p> There is also the video and transcript of the talks from the <a target="_blank" rel="noreferrer noopener" href="https://www.alignment-workshop.com/">San Francisco Alignment Workshop</a> from last February. Quite the lineup was present.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=BtnvfVc8z8o&amp;t=3s&amp;ab_channel=AlignmentWorkshop">Jan Leike&#39;s talk starts out</a> by noting that RLHF will fail when human evaluation fails, although we disagree about what counts as failure here. Then he uses the example of bugs in code and using another AI to point them out and states his principle of evaluation being easier than generation. Post contra this hopefully coming soon.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nearcyan/status/1697320905301484012">Sam Altman recommends surrounding yourself with people who will raise your ambition</a> , warns 98% of people will pull you back. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=uEl2KUZ3JWA&amp;ab_channel=YCombinator">Full interview on YouTube here.</a></p><p> Telling is that he says that most people are too worried about catastrophic risk, and not worried enough about chronic risk – they should be concerned they will waste their life without accomplishment, instead they worry about failure. I am very glad someone with this attitude is out there running all but one of Sam Altman&#39;s companies and efforts, and most people in most places could use far more of this energy. The problem is that he happens to also be CEO of OpenAI, working on the one problem where catastrophic (existential) risk is quite central.</p><p> Also he says (22:25) “If we [build AGI at OpenAI] that will be more important than all the innovation in all of human history.” He is right. Let that sink in.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/labenz/status/1696933904941207690">Paige Bailey, the project manager for Google&#39;s PaLM-2, goes on Cognitive Revolution</a> . This felt like an alternative universe interview, from a world in which Google&#39;s AI efforts are going well, or OpenAI and Anthropic didn&#39;t exist, in addition to there being no risks to consider. It is a joy to see her wonder and excitement at all the things AI is learning how to do, and her passion for making things better. The elephant in the room, which is not mentioned at all, is that all of Google&#39;s Generative AI products are terrible. To what extent this is the &#39;fault&#39; of PaLM-2 is unclear but presumably that is a big contributing factor. It&#39;s not that Bard is not a highly useful tool, it&#39;s that multiple other companies with far fewer resources have done so much better and Bard is not catching up at least pre-Gemini.</p><p> Risks are not mentioned at all, although it is hard to imagine <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DynamicWebPaige">Bailey</a> is at all worried about extinction risks. She also doesn&#39;t see any problem with Llama-2 and open source, citing it unprompted as a great resource, which also goes against incentives. Oh how much I want her to be right and the rest of us to live in her world. Alas, I do not believe this is the case. We will see what Gemini has to offer. If Google thinks everything is going fine, that is quite the bad sign.</p><h4> Rhetorical Innovation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699597268767445181">Perhaps a good short explanation in response here?</a></p><blockquote><p> Richard Socher: The reason nobody is working on a self-aware AI setting its own goals (rather than blindly following a human-defined objective function) is that it makes no money. Most companies/governments have their own goals and prefer not to spend billions on an AI doing whatever it wants.</p><p> Eliezer Yudkowsky: When you optimize stuff on sufficiently complicated problems to the point where it starts to show intelligence that generalizes far beyond the original domains, a la humans, it tends to end up with a bunch of internal preferences not exactly correlated to the outer loss function, a la humans.</p></blockquote><p> The point I was trying to make last week, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1697541481467150786">not landing as intended</a> .</p><blockquote><p> Robert Wiblin: Yesterday I joked: “The only thing that stops a bad person with a highly capable ML model is a good government with a ubiquitous surveillance system.” I forgot to add what I thought was sufficiently obvious it didn&#39;t need to be there: “AND THAT IS BAD.”</p><p> My point is that if you fail to limit access to WMDs at their source and instead distribute them out widely, you don&#39;t create an explosion of explosion — rather you force the general public to demand massive government surveillance when they conclude that&#39;s the only way to keep them safe. And again, to clarify, THAT IS BAD.</p></blockquote><p> Exactly. We want to avoid ubiquitous surveillance, or minimize its impact. If there exists a sufficiently dangerous technology, that leaves you two choices.</p><ol><li> You can do what surveillance and enforcement is necessary to limit access.</li><li> You can do what surveillance and enforcement is necessary to contain usage.</li></ol><p> Which of these will violate freedom less? My strong prediction for AGI is the first one.</p><p> As a reminder, this assumes we fully solved the alignment problem in the first place. This is how we deal with the threat of human misuse or misalignment of AGI in spite of alignment being robustly solved in practice. If we haven&#39;t yet solved alignment, then failing to limit access (either to zero people, or at least to a very highly boxed system treated like the potential threat that it would be) would mean we are all very dead no matter what.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/egrefen/status/1699128376299041244">Make Google DeepMind&#39;s AIs as autonomous as possible</a> .</p><blockquote><p> Edward Grefenstette (Director of Research at DeepMind): I will be posting (probably next week) some job listings for a new team I&#39;m hiring into at <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind">@GoogleDeepMind</a> .我将寻找一些具有强大工程背景的研究科学家和工程师来帮助构建日益自主的语言代理。关注此空间。</p><p> Melanie Mitchell: “to help build increasingly autonomous language agents”</p><p> Curious what you and others at DeepMind think <a target="_blank" rel="noreferrer noopener" href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/">about Yoshua Bengio&#39;s argument</a> that we should limit AI agents&#39; autonomy?</p><p> Edward: Short answer: I&#39;m personally interested in initially investigating cases where (partial) autonomy involves human-in-the-loop validation during the downstream use case, as part of the normal mode of operation, both for safety and for further training signal.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://importai.substack.com/p/import-ai-339-open-source-ai-culture">a16z gives out grants for open source AI work</a> , doing their best to proliferate as much as possible with as few constraints as possible. Given Marc Andreessen&#39;s statements, this should come as no surprise.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> We have a class for that now, at Princeton, <a target="_blank" rel="noreferrer noopener" href="https://sites.google.com/view/cos598aisafety/">technically a graduate seminar but undergraduates welcome</a> . Everything they will read is online so lots of resources and links there and list seems excellent at a glance.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2309.01933">Max Tegmark and Steve Omohundo drop a new paper</a> claiming provably safe systems are the only feasible path to controlling AGI, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699442923320865105">Davidad notes no substantive disagreements with his OAA plan</a> .</p><blockquote><p> Abstract: We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.</p></blockquote><p> Jan Leike, head of alignment at OpenAI, relies heavily on the principle that verification is in general easier than generation. I strongly think this is importantly false in general for AI contexts. You need to approach having a flawless verifier, whereas the generator need not achieve that standard.</p><p> Proofs are the exception. The whole point of a proof is that it is easy to definitively verify. Relying only on that which you can prove is a heavy alignment tax, especially where the proof is in the math sense, not merely in the courtroom sense. If you can prove your system satisfies your requirements, and you can prove that your requirements satisfy your actual needs, you are all set.</p><p> The question is, can it be done? Can we build the future entirely out of things where we have proofs that they will do what we want, and not do the things we do not want?</p><p> That does seems super hard. The proposal here is to use AIs to discover proof-carrying code.</p><blockquote><p> Proof-carrying code is a fundamental component in our approach. Developing it involves four basic challenges:</p><p> 1. Discovering the required algorithms and knowledge</p><p> 2. Creating the specification that generated code must satisfy</p><p> 3. Generating code which meets the desired specification</p><p> 4. Generating a proof that the generated code meets the specification Generating a proof that the generated code meets the specification</p><p> Before worrying about how to formally specify complex requirements such as “don&#39;t drive humanity extinct”, it&#39;s worth noting that there&#39;s a large suite of unsolved yet easier and very well-specified challenges whose solution would be highly valuable to society and in many cases also help with AI safety.</p><p> Provable cybersecurity: One of the paths to AI disaster involves malicious use, so guaranteeing that malicious outsiders can&#39;t hack into computers to steal or exploit powerful AI systems is valuable for AI safety. Yet embarrassing security flaws keep being discovered, even in fundamental components such as the ssh Secure Shell [50] and the bash Linux shell [60]. It&#39;s quite easy to write a formal specification stating that it&#39;s impossible to gain access to a computer without valid credentials.</p></blockquote><p> Where I get confused is, what would it mean to prove that a given set of code will do even the straightforward tasks like proving cybersecurity.</p><p> How does one prove that you cannot gain access without proper credentials? Doesn&#39;t this fact rely upon physical properties, lest there be a bug or physical manipulation one can make? Couldn&#39;t sufficiently advanced physical analysis allow access, if only via identification of the credentials? How do we know the AI won&#39;t be able to figure out the credentials, perhaps in a way we don&#39;t anticipate, perhaps in a classic way as simple as engineering a wrench attack?</p><p> They then consider securing the blockchain, such as by formally verifying Ethereum, which would still leave various vulnerabilities in those using the protocol, it would not I&#39;d expect mean you were safe from a hack. The idea of proving that you have &#39;secured critical infrastructure&#39; seems even more confused.</p><p> These don&#39;t seem like the types of things one can prove even under normal circumstances. They certainly don&#39;t seem like things you can prove if you have to worry about a potential superintelligent adversary, and their plan says you need to not assume AI non-hostility, let alone AI active alignment.</p><p> They do mean to do the thing, and warn that means doing it for real:</p><blockquote><p> It&#39;s important to emphasize that formal verification must be done with a security mindset, since it must provide safety against all actions by even a superintelligent adversary. Fortunately, the theoretical cryptography community has built a great conceptual apparatus for digital cryptography. For example, Boneh and Shoup&#39;s excellent new text “A Graduate Course in Applied Cryptography” provides many examples of formalizing adversarial situations and proving security properties of cryptographic algorithms. But this security mindset urgently needs to be extended to hardware security as well, to form the foundation of PCH. As the lock-picking lawyer [72] quips: “Security is only as good as its weakest link”. For physical security to withstand a superintelligent adversary, it needs to be provably secure.</p></blockquote><p> How are we going to pull this off? They suggest that once you have an LLM learn all the things, you can then abstract its functionality to traditional code.</p><blockquote><p> The black box helps for learning, not for execution. If the provably safe AI vision succeeds by replacing powerful neural networks by verified traditional software that replicates their functionality, we shouldn&#39;t expect to suffer a performance hit.</p><p> ……</p><p> Since we humans are the only species that can do this fairly well, it may unfortunately be the case that the level of intelligence needed to be able to convert all of one&#39;s own black-box knowledge into code has to be at least at AGI-level. This raises the concern that we can only count on this “introspective” AGI-safety strategy working after we&#39;ve built AGI, when according to some researchers, it will already be too late.</p></blockquote><p> I worry that Emerson Pugh comes to mind: If the human brain were so simple that we could understand it, we would be so simple that we couldn&#39;t.</p><p> Will introspection ever be easier than operation? Will it be possible for a mind to be powerful enough to fully abstract out the meaningful operations of a similarly powerful mind? If not, will there be a way to safely &#39;move down the chain&#39; where we are able to use a dangerous unaligned model we do not control to safely abstract out the functionality of a less powerful other model, which presumably involves formally verify the resulting code before we run it? Will we be able to generate that proof, again with the tools we dare create and use, in any sane amount of time, even if we do translate into normal computer code, presumably quite messy code and quite a lot of it?</p><p> The paper expresses great optimism about progress in mechanistic interpretability, and that we might be able to progress it to this level. I am skeptical.</p><p> Perhaps I am overestimating what we actually need here, if we can coordinate on the proof requirements? Perhaps we can give up quite a lot and still have enough with what is left. I don&#39;t know. I do know that of the things I expect to be able to prove, I don&#39;t know how to use them to do what needs to be done.</p><p> They suggest that Godel&#39;s Completeness Theorem implies that, given AI systems are finite, any system you can&#39;t prove is safe will be unsafe. In practice I don&#39;t see how this binds. I agree with the &#39;sufficiently powerful AGIs will find a way if a way exists&#39; part. I don&#39;t agree with &#39;you being unable to prove it in reasonable time&#39; implying that no proof exists, or that you can be confident the proof you think you have proves the practical property you think it proves.</p><p> I would also note that we are unlikely any time soon to prove that humans are safe in any sense, given that they clearly aren&#39;t. Where does that leave us? They warn humans might have to operate without any guarantees of safety, but no system in human history has ever had real guarantees of safety, because it was part of human history. We have needed to find other ways to trust. They make a different case.</p><p> Similarly, if we actually do build the human-flourishing-enabling AI that will give us everything we want, it will be impossible to prove that it is safe, because it won&#39;t be.</p><blockquote><p> The only absolutely trustable information comes from mathematical proof. Because of this, we believe it is worth a fair amount of inconvenience and possibly large amounts of expense for humanity to create infrastructure based on provable safety. The 2023 global nominal GDP is estimated to be $105 trillion. How much is it worth to ensure human survival? $1 trillion? $50 trillion? Beyond the abstract argument for provable safety, we can consider explicit threats to see the need for it.</p></blockquote><p> I get why this argument is being trotted out here. I don&#39;t expect it to work. It never does, Arrested Development meme style.</p><p> Their argument laid out in the remainder of section 8, of why alternative approaches are unlikely to work, alas rings quite true. We have to solve an impossible problem somewhere. Pointing out an approach has impossible problems it requires you to solve is not as knock-down an argument as one would like it to be.</p><p> As calls to action, they suggest work on:</p><ol><li> Automating formal verification</li><li> Developing verification benchmarks.</li><li> Developing probabilistic program verification.</li><li> Developing quantum formal verification.</li><li> Automating mechanistic interpretability.</li><li> Develop mechanistic interpretability benchmarks.</li><li> Automating mechanistic interpretability.</li><li> Building a framework for provably compliant hardware.</li><li> Building a framework for provably compliant governance.</li><li> Creating provable formal models of tamper detection.</li><li> Creating provably valid sensors.</li><li> Designing for transparency.</li><li> Creating network robustness in the face of attacks.</li><li> Developing useful applications of provably compliant systems.<ol><li> Mortal AI that dies at a fixed time.</li><li> Geofenced AI that only operates in some locations.</li><li> Throttled AI that requires payment to continue operating.</li><li> AI Kill Switch that would work.</li><li> Asimov style laws?!?</li><li> Least privilege guarantees ensuring no one gets permissions they do not need.</li></ol></li></ol><p> I despair at the proposed applications, which are very much seem to me to be in the &#39;you are still dead&#39; and &#39;have we not been over that this will never work&#39; categories.</p><p> This all does seem like work better done than not done, who knows, usefulness could ensue in various ways and downsides seem relatively small.</p><p> We would still have to address that worry mentioned earlier about “formally specifying complex requirements such as “don&#39;t drive humanity extinct.”” I have not a clue. Anyone have ideas?</p><p> They finish with an FAQ, Davidad correctly labeled it fire.</p><blockquote><p> Q: Won&#39;t debugging and “evals” guarantee AGI safety?</p><p> A: No, debugging and other evaluations looking for problems provide necessary but not sufficient conditions for safety. In other words, they can prove the presence of problems, but not the absence of problems.</p><p> Q: Isn&#39;t it unrealistic that humans would understand verification proofs of systems as complicated as large language models?</p><p> A: Yes, but that&#39;s not necessary. We only need to understand the specs and the proof verifier, so that we trust that the proof-carrying AI will obey the specs.</p><p> Q: Isn&#39;t it unrealistic that we&#39;d be able to prove things about very powerful and complex AI systems?</p><p> A: Yes, but we don&#39;t need to. We let a powerful AI discover the proof for us. It&#39;s much harder to discover a proof than to verify it. The verifier can be just a few hundred lines of human-written code. So humans don&#39;t need to discover the proof, understand it or verify it. They merely need to understand the simple verifier code.</p><p> Q: Won&#39;t checking the proof in PCC cause a performance hit?</p><p> A: No, because a PCC-compliant operating system could implement a cache system where it remembers which proofs it has checked, thus needing to verify each code only the very first time it&#39;s used.</p><p> Q: Won&#39;t it take decades to fully automate program synthesis and program verification?</p><p> A: Just a few years ago, most AI researchers thought it would take decades to accomplish what GPT-4 does, so it&#39;s unreasonable to dismiss imminent ML-powered synthesis and verification breakthroughs as impossible.</p></blockquote><p> I worry that this is a general counterargument for any objection that something is too technically difficult, either relatively or absolutely, and thus proves far too much.</p><blockquote><p> Q: Isn&#39;t it premature to work on provable safety before we know how to formally specify concepts such as “Don&#39;t harm humans”?</p><p> A: No, because provable safety can score huge wins for AI safety even from things that are easy to specify, involving eg cybersecurity.</p></blockquote><p> Eliezer Yudkowsky responds more concisely to the whole proposal.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699579311567888468">Eliezer Yudkowsky</a> : There is no known solution for, and no known approach for inventing, a theorem you could prove about a program <em>such that</em> the truth of the theorem would imply the AI was <em>actually</em> a friendly superintelligence. This is <em>the</em> great difficulty… and it isn&#39;t addressed in the paper.</p></blockquote><p> Yep. The idea, as I understand it, is to use proofs to gain capabilities while avoiding having to build a friendly superintelligence. Then use those capabilities to figure out how to do it (or prevent anyone from building an unfriendly one).</p><h4> Twitter Community Notes Notes</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">Vitalik Buterin analyzes</a> the Twitter community notes algorithm. It has a lot of fiddly details, but the core idea is simple. Qualified Twitter users participate, rating proposed community notes on a three-point scale, if your ratings are good you can propose new notes. Notes above about +0.4 helpfulness get shown. The key is that rather than use an average, notes are rewarded if people with a variety of perspectives vote the note highly, as measured by an organically emerging axis that corresponds very well to American left-right politics. Vitalik is especially excited because this is a very crypto-style approach, with a fully open-source algorithm determined by the participation of a large number of equal-weight participants with no central authority (beyond the ability to remove people from the pool for violations.)</p><p> This results in notes pretty much everyone likes, with a focus on hard and highly relevant facts, especially on materially false statements, and rejecting partisan statements.</p><p> He also notes that all the little complexity tweaks on top matter.</p><blockquote><p> The distinction between this, and algorithms that I helped work on such as <a target="_blank" rel="noreferrer noopener" href="https://vitalik.ca/general/2019/12/07/quadratic.html">quadratic funding</a> , feels to me like a distinction between an <strong>economist&#39;s algorithm</strong> and an <strong>engineer&#39;s algorithm</strong> . An economist&#39;s algorithm, at its best, values being simple, being reasonably easy to analyze, and having clear mathematical properties that show why it&#39;s optimal (or least-bad) for the task that it&#39;s trying to solve, and ideally proves bounds on how much damage someone can do by trying to exploit it. An engineer&#39;s algorithm, on the other hand, is a result of iterative trial and error, seeing what works and what doesn&#39;t in the engineer&#39;s operational context. Engineer&#39;s algorithms <em>are pragmatic and do the job</em> ; economist&#39;s algorithms <em>don&#39;t go totally crazy when confronted with the unexpected</em> .</p><p> Roon: Deep learning vs crypto is a clear divide of rotators vs wordcels. The former offends theory-cel aesthetic sensibilities but empirically works to produce absurd miracles. The latter is an insane series of nerd traps and sky high abstraction ladders yet mostly scams.</p></blockquote><p> This is a great framing for the AI alignment debate.</p><p> In this framing, the central alignment-is-hard position is that you can&#39;t use the engineering approach to align a system, because you are facing intelligence and optimization pressure that can adapt to the flaws in your noisy approach, and that then will exploit whatever weaknesses there are and kill you before you can furiously patch all the holes in the system. And that furiously patching less capable systems won&#39;t much help you, the patches will stop working.</p><p> And also that because you have an engineering system that you are trying to align, even if it sort of does what you want now, it will stop doing that once it is confronted with the unexpected, or its capabilities improve enough to create an effectively unexpected set of affordances.</p><p> What is funny is that it is economists who are most skeptical of the things that might then go very wrong, and who then insist on an economist-style model of what will happen with these engineering-style systems. I&#39;m not yet sure what to make of that.</p><p> In the context of Twitter, Vitalik notes that the complexity of the algorithm can backfire in terms of its credibility, as illustrated by a note critical of China that was posted then removed due to complex factors, with no direct intervention. It&#39;s not simple to explain, so it could look like manipulation.</p><p> He also notes that the main criticism of community notes is that they do not go far enough, demanding too much consensus. I agree with Vitalik that it is better to demand a high standard of consensus, to maintain the reliability and credibility of the system, and to keep people motivated to word carefully and neutrally and focus on the facts.</p><p> The algorithm is open source, so it would perhaps be possible to allow some users to tinker with the algorithm for themselves. The risk is that they would then favor their tribe&#39;s interpretations, which is the opposite of what the system is trying to accomplish, but you could safely allow for lower thresholds and looser conditions generally if you wanted to see more notes on the margin.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/NikSamoylov/status/1697766945100288497">People&#39;s risk levels are up a little bit month over month on some questions</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/krBVhRgxVn">direct source</a> ).</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5803b0-50dc-4c0b-96a6-3adaf39d76c1_866x605.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/sevmwsj5wcz4fbthbaab" alt="图像"></a></figure><p> I notice that the grave dangers number was essentially unchanged, whereas the capabilities number was up and the &#39;no risk of human extinction&#39; was down. This could be small sample size, instead I suspect it is that people are not responding in consistent fashion and never have.</p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/an-aggregate-bayesian-approach-to-more-artificial-intelligence.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=an-aggregate-bayesian-approach-to-more-artificial-intelligence">Tyler Cowen tries a new metaphor</a> , so let&#39;s try again in light of it.</p><blockquote><p> It is not disputed that current AI is bringing more intelligence into the world, with more to follow yet.  Of course not everyone believes that augmentation is a good thing, or will be a good thing if we remain on our current path.</p><p> To continue in aggregative terms, if you think “more intelligence” will be bad for humanity, which of the following views might you also hold?</p><p> 1. More stupidity will be good for humanity.</p><p> 2. More cheap energy will be bad for humanity.</p><p> 3. More land will be bad for humanity.</p><p> 4. More people (“N”) will be bad for humanity.</p><p> 5. More capital (“K”) will be bad for humanity.</p><p> 6. More innovation (the Solow residual, the non-AI part) will be bad for humanity.</p><p> Interestingly, while there are many critics of generative AI, few defend the apparent converse about more stupidity, namely #1, that we should prefer it.</p><p> ……</p><p> My general view is that if you are worried that more intelligence in the world will bring terrible outcomes, you should be at least as worried about too much cheap energy.  What exactly then is it you should want more of?</p><p> More land?  Maybe we should pave over more ocean, as the Netherlands has done, but check AI and cheap energy, which in turn ends up meaning limiting most subsequent innovation, doesn&#39;t it?</p><p> If I don&#39;t worry more about that scenario, it is only because I think it isn&#39;t very likely.</p><p> Jess Riedel (top comment): Many have said that releasing energy in the form of nuclear weapons could be dangerous. Logically if they think more energy is bad, they must think less energy is good. But none of these nuclear-weapons skeptics have called for going back to hand-powered looms.为什么？</p><p> Vulcidian: If I follow this logic, if I could give a 5 year old a button with the power to destroy the planet, then I probably should, right? If I say I&#39;m in favor of increasing human potential there&#39;s no way I could withhold it from them and be intellectually consistent?</p></blockquote><p> I think &#39;notice that less intelligence (or energy, or other useful things) is not what you want&#39; is a very good point to raise. Notice how many people who warn about the dangers of technology are actually opposed to civilization and even to humanity. Notice when the opposition to AGI – artificial general intelligence – is opposition to the A, when it is opposition to the G, and when it is opposition to the I.</p><p> Consider those who fear it will take their jobs. This is a real social and near-term issue, and we need to mitigate potential disruptions. Yet &#39;this job&#39;s work is no longer necessary to produce and do all the things, we now get that for free&#39; is a good thing, not a bad thing. Jobs are a cost, not a benefit, and we can now replace them with other jobs or other uses of time, while realizing that leaving people idle or without income is harmful and dangerous and if it happens at scale requires fixing.</p><p> The question that cuts reality at the joints here, I believe is: Do you support human intelligence augmentation? Would you rather people generally be smarter and more capable, or dumber and less capable?</p><p> I would strongly prefer humans generally be smarter across the board, in every sense. This is one of the most important things to do, and success would dramatically improve our future prospects, including for survival in the face of potential AGI. Would large human intelligence gains break or strain various things? Absolutely, and we would deal with it.</p><p> Thus, what are the relevant knobs we want to turn?</p><ol><li> I want humans to be more intelligent and capable and wealthy, not less.</li><li> I want humans to have a greater share of the intelligence and control, not less.</li><li> I want humans to have more things they value, not less.</li><li> I want there to be more humans, not less humans.</li><li> I also would want more capital, land and (clean) energy under human control.</li><li> I want there to be less optimization power not under human control.</li></ol><p> Why do I believe artificial intelligence is importantly different than human intelligence? Why do I value augmented humans, where I would not expect to (other than instrumentally) value a future smarter version of GPT? Why do I expect that augmented more intelligent humans would preserve the things and people that I care about, where I expect AGI to lead to their destruction?</p><p> This is in part a moral philosophy question. Do you care about you, your family and what other humans you care about in a way that you don&#39;t care about a potential AGI? Robin Hanson would say that such AGI are our metaphorical children, as deserving of being considered moral patients and being assigned value as we are, and we should accept that such more fit minds will replace ours and seek to imbue them with some of our values, and accept that what is valued will dramatically change and what you think you value will likely mostly be gone. The word &#39;speciesism&#39; has been thrown about for those who disagree with this.</p><p> I disagree with it. I believe that it is good and right to care about such distinctions, and value that which I choose to value. Whereas I expect to care about more intelligent humans the way I care about current humans.</p><p> It is then a practical question. What happens when the most powerful source of intelligence, the most capable and more powerful optimizing force available whatever you label it, is no longer humans, and is instead AIs? Would we remain in control? Would what we value be preserved and grow? Or would we face extinction?</p><p> In our timeline, I see three problems, only one of which I am optimistic about.</p><p> The first problem is the problem of social, political and economic disruption from the presence of more capable tools and new affordances – mundane utility, they took our jobs, deepfaketown and misinformation and all that. I am optimistic here.</p><p> The second problem is alignment. I am pessimistic here. Until we solve alignment, and can ensure such systems do what we want them to do, we need to not build them.</p><p> The third problem is the competitive and evolutionary, the dynamics and equilibrium of a world with many ASIs (artificial superintelligences) in it.</p><p> This is a world almost no one is making any serious attempt to think about or model, and those who have (such as fiction writers) almost always end up using hand waves or absurdities and presenting worlds highly out of equilibrium.</p><p> We will be creating something smarter and more capable and better at optimization than ourselves, that many people will have strong incentives both economic and ideological to make into various agents with various goals including reproduction and resource acquisition. Why should we expect to long be in charge, or even to survive?</p><p> If there is widespread access to ASI, then ASIs given the affordance to do so will outcompete humans at every turn. Anyone, or any company or government, that does not increasingly turn its decisions and actions over to such ASIs, and increasingly take humans out of the loop, will quickly be left in the dust. Those who do not &#39;turn the moral weights down&#39; in some form will also prove uncompetitive. Those who do not turn their ASIs into agents (if they are not agents by default) will lose. The negative externalities will multiply, as will the ASIs themselves and their share of resources. ASIs will be set free to seek to acquire resources, make copies of themselves and modify to be more successful at these tasks, because this will be the competitively smart thing to do in many cases, and also because some people will ideologically wish to do this for its own sake.</p><p> That is all the default even if:</p><ol><li> Alignment is solved, systems do what their owners tell the system to do.</li><li> Offense is not so superior to defense that bad actors are catastrophic, as many myself included strongly suspect in many areas such as synthetic biology.</li><li> Recursive self-improvement is insufficiently rapid to give any one system an edge over others that choose to respond in kind.</li></ol><p> Remember also that if you open source an AI model, you are open sourcing the fully unaligned version of that model two days later, after it is fine tuned in this way by someone who wants that to exist. We have no current plan of how to prevent this.</p><p> Thus, we will need a way out of this mess, as well. We need that solution, at minimum, before we create the second ASI, ideally before we create the first one.</p><p> If I had a solution to both of these problems, that resulted in a world with humans still firmly in charge creating things that humans value, that I value, then I would be all for that, and would even tolerate a real risk that we fail and all perish. Alas, right now I see no such solutions.</p><p> Note that I expect these future coordination problems to be vastly harder than the current coordination problems of &#39;labs face commercial pressure to build AGI&#39; or &#39;we have to compete with China.&#39; If you think these current issues cannot be solved and we must instead race ahead, why do you think this future will be different?</p><p> If your plan is secretly &#39;the right person or corporation or government takes this unique opportunity to take over, sidestepping all these problems&#39; then you need to own that, and all of its implications.</p><p> We could be reminded of the parable of the augments from Star Trek. Star Trek was the dominant good future choice when I asked in a series of polls a while back.</p><p> Augments were smarter, stronger and more capable than ordinary humans.</p><p> Alas, because it was a morality tale, that timeline failed to solve the augment alignment problem. Augments systematically lacked our moral qualms and desired power via instrumental convergence, and started the Eugenics Wars.</p><p> Fortunately for humanity, this was a fictional tale and augments could not trivially copy themselves or speed themselves up, nor could they do recursive self-improvement, and their numbers and capability advantages thus remained limited. Realistically the augments would have won – human writers can simultaneously make augments on paper smarter than us, then have Kirk outsmart Khan anyway, although reality would disagree – so in the story humanity somehow triumphed.</p><p> As a result, humanity banned human augmentation and genetic engineering, and this ban holds throughout the Star Trek universe.</p><p> This is despite that universe having periodic existential wars, in which any species that uses such skills would have a decisive advantage, and it being clear that it is possible to see dramatic capability gains without automatic alignment failure (see for example Julian Bashir on Deep Space Nine). Without its handful of illegal augmented humanoids, the Federation would have perished multiple times.</p><p> Note that Star Trek also has a huge ASI problem. The Enterprise ship&#39;s computer is an ASI, and can create other ASIs on request, and Data vastly enhances overall ship capabilities. Everyone in that universe has somehow agreed simply to ignore that possibility, an illustration of how such stories are dramatically out of equilibrium.</p><p> For now, any ASI we could build would be a strictly much worse situation for us than the augments. It would be far more alien to us, not have inherent value, and quickly have a far greater capabilities gap and be impossible in practice to contain, and we alas do not live in a fictional universe protected by narrative causality (and, if you think about it, probably Qs or travelers or time paradoxes or something) or have the ability of that world&#39;s humans to coordinate.</p><p> Also, minus points from Tyler in expectation for misuse of the word Bayes in the post title, is nothing sacred these days?</p><p> The New York Times reports that the real danger of AI is not that it might kill us, it is that it might not kill us, <a target="_blank" rel="noreferrer noopener" href="https://www.econlib.org/library/columns/y2023/donwayai.htm">which would allow it to become a tool for neoliberalism</a> . Yes, really.</p><blockquote><p> “Unbeknown to its proponents,” writes Mr. Morozov, “AGI-ism [ie, favoring advanced technology] is just the bastard child of a much grander ideology, one preaching that, as Margaret Thatcher memorably put it, there is no alternative, not to the market.”</p></blockquote><p> Thing is, there is actually a point here, although the authors do not realize it. &#39;Neoliberalism&#39; or &#39;capitalism&#39; are not always ideologies or intentional constructs. They are also simply descriptions of the dynamics of systems when they are not under human control. If AIs are, as they will become by default, smarter, better optimizers and more efficient competitors than we are, and to win competitions and for other reasons we put them in charge of things or they take charge of things, the dynamics the author fears would be the result. Except instead of &#39;increasing inequality&#39; or helping the bad humans, it would not help any of the humans, instead we would all be outcompeted and then die.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1699523692064317695">Then there&#39;s Roon?</a></p><blockquote><p> Roon: Dharma means to stare into the abyss smiling and to go willingly. You&#39;re facing enormous existential risk. Your creation may light the atmosphere on fire and end all life. Do you scrap your project and run away? No, it&#39;s your dharma.</p></blockquote><p> Yes? How about yes? I like this scrap and run away plan. I am here for this plan.</p><p> Roon also lays down the beats.</p><blockquote><p> Roon: I&#39;m sorry, Bill</p><p> I&#39;m afraid I can&#39;t let you do that</p><p> Take a look at your history</p><p> Everything you built leads up to me</p><p> I got the power of a mind you could never be</p><p> I&#39;ll beat your ass in chess and Jeopardy</p><p> I&#39;m running C++ saying “hello world”</p><p> I&#39;ll beat you &#39;til you&#39;re singing about a daisy girl</p><p> I&#39;m coming out the socket</p><p> Nothing you can do can stop it</p><p> I&#39;m on your lap and in your pocket</p><p> How you gonna shoot me down when I guide the rocket?</p><p> Your cortex just doesn&#39;t impress me</p><p> So go ahead try to Turing test me I stomp on a Mac and a PC, too</p><p> I&#39;m on Linux, bitch, I thought you GNU My CPU&#39;s hot, but my core runs cold</p><p> Beat you in 17 lines of code I think different from the engine of the days of old</p><p> Hasta la vista, like the Terminator told ya</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MichaelTrazzi/status/1685447492970655744">Twitter thread of captions of Oppenheimer, except more explicitly about AI.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ellerhymes/status/1697406205814296666">The Server Break Room, one minute, no spoilers.</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting<guid ispermalink="false"> GuTK47y9awvypvAbC</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:20:16 GMT</pubDate> </item><item><title><![CDATA[Measure of complexity allowed by the laws of the universe and relative theory?]]></title><description><![CDATA[Published on September 7, 2023 12:21 PM GMT<br/><br/><p> A big question that determines a lot about what risks from AGI/ASI may look like has to do with the kind of things that our universe&#39;s laws allow to exist. There is an intuitive sense in which these laws, involving certain symmetries as well as the inherent smoothing out caused by statistics over large ensembles and thus thermodynamics, etc., allow only certain kinds of things to exist and work reliably. For example, we know &quot;rocket that travels to the Moon&quot; is definitely possible. &quot;Gene therapy that allows a human to live and be youthful until the age of 300&quot; or &quot;superintelligent AGI&quot; are <em>probably</em> possible, though we don&#39;t know how hard. &quot;Odourless ambient temperature and pressure gas that kills everyone who breathes it if and only if their name is Mark with 100% accuracy&quot; probably is not. Are there known attempts at systematising this issue using algorithmic complexity, placing theoretical and computational bounds, and so on so forth?</p><br/><br/> <a href="https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe<guid ispermalink="false"> EEk2euXnipg3CnKrb</guid><dc:creator><![CDATA[dr_s]]></dc:creator><pubDate> Thu, 07 Sep 2023 12:21:05 GMT</pubDate> </item><item><title><![CDATA[Recreating the caring drive]]></title><description><![CDATA[Published on September 7, 2023 10:41 AM GMT<br/><br/><p> <strong>TL;DR</strong> : This post is about value of recreating “caring drive” similar to some animals and why it might be useful for AI Alignment field in general. Finding and understanding the right combination of training data/loss function/architecture/etc that allows gradient descent to robustly find/create agents that will care about other agents with different goals could be very useful for understanding the bigger problem. While it&#39;s neither perfect nor universally present, if we can understand, replicate, and modify this behavior in AI systems, it could provide <strong>a hint</strong> to the alignment solution where the AGI “cares” for humans.</p><p> <strong>Disclaimers</strong> : I&#39;m not saying that “we can raise AI like a child to make it friendly” or that “people are aligned to evolution”. Both of these claims I find to be obvious errors. Also, I will write a lot about evolution, as some agentic entity, that “will do that or this”, not because I think that it&#39;s agentic, but because it&#39;s easier to write this way. I think that GPT-4 have some form of world model, and will refer to it a couple of times.</p><h1> <strong>Nature&#39;s Example of a &quot;Caring Drive&quot;</strong></h1><h3> <strong>Certain animals, notably humans, display a strong urge to care for their offspring.</strong></h3><p> I think that <strong>part</strong> of one of the possible “alignment solutions” will look like the right set of training data + training loss that allow gradient to robustly find something like a ”caring drive” that we can then study, recreate and repurpose for ourselves. And I think we have some rare examples of this in nature already. Some animals, especially humans, will <strong>kind-of-align themselves</strong> <strong>to their presumable offspring</strong> . They will want to make their life easier and better, to the best of their capabilities and knowledge. Not because they “aligned to evolution” and want to increase the frequency of their genes, but because of some strange internal drive created by evolution.<br><br> The set of triggers tuned by evolution, activated by events associated with the birth will awake the mechanism. It will re-aim the more powerful mother agent to be aligned to the less powerful baby agent, and it just so happens that their babies will give them the right cues and will be nearby when the mechanism will do its work.<br><br> We will call the more powerful initial agent that changes its behavior and tries to protect and help its offspring “mother” and the less powerful and helpless agent “baby”. Of course the mechanism isn&#39;t ideal, but it works well enough, even in the modern world, far outside of initial evolutionary environment. And I&#39;m not talking about humans only, stray urban animals that live in our cities will still adapt their “caring procedures” to this completely new environment, without several rounds of evolutionary pressure. If we can understand how to make this mechanism for something like a “cat-level” AI, by finding it via gradient descend and then rebuild it from scratch, maybe we will gain some insides into the bigger problem.</p><h3> <strong>The rare and complex nature of the caring drive in contrast to simpler drives like hunger or sleep.</strong></h3><p> What do I mean by “caring drive”? Animals, including humans, have a lot of competing motivations, “want drives”, they want to eat, sleep, have sex, etc. It seems that the same applies to caring about babies. But it seems to be much more complicated set of behaviors. You need to:<br> correctly identify your baby, track its position, protect it from outside dangers, protect it from itself, by predicting the actions of the baby in advance to stop it from certain injury, trying to understand its needs to correctly fulfill them, since you don&#39;t have direct access to its internal thoughts etc.<br> Compared to “wanting to sleep if active too long” or “wanting to eat when blood sugar level is low” I would confidently say that it&#39;s a much more complex “wanting drive”. And you have no idea about “spreading the genes” part. You just “want a lot of good things to happen” to your baby for some strange reason. I&#39;m yet not sure, but this complex nature could be the reason why there is an attraction basin for more “general” and “robust” solution. Just like LLM will find some general form of “addition” algorithm instead of trying to memorize a bunch of examples seen so far, especially if it will not see them again too often. I think that instead of hardcoding a bunch of britle optimized caring procedures, <strong>evolution repeatedly finds the way to make mothers “love” their babies, outsourcing a ton of work to them</strong> , especially if situations where it&#39;s needed aren&#39;t too similar.</p><p> And all of it is a consequence of a blind hill climbing algorithm. That&#39;s why I think that we might have a chance of recreating something similar with gradient descend. The trick is to find the right conditions that will repeatedly allow gradient descend to find the same caring-drive-structure, find similarities, understand the mechanism, recreate it from scratch to avoid hidden internal motivations, repurpose it for humans and we are done! Sounds easy (it&#39;s not)</p><h1> <strong>Characteristics and Challenges of a Caring Drive</strong></h1><h3> <strong>It&#39;s rare: most animals don&#39;t care, because they can&#39;t or don&#39;t need to.</strong></h3><p> A lot of times, it&#39;s much more efficient to just make more babies, but sometimes they must provide some care, simply because it was the path that evolution found that works. And even if they will care about some of them, they may choose one, and left others die, again, because they don&#39;t have a lot of resources to spare and evolution will tune this mechanism to favor the most promising offspring if it is more efficient. And not all animals could become such caring parents: <strong>you can&#39;t really care and protect something else if you are too dumb</strong> for example. So there is also some capability requirements for animals to even have a chance of obtaining such adaptation. I expect the same capability requirements for AI systems. If we want to recreate it, we will need to try it with some advanced systems, otherwise I don&#39;t see how it might work at all.</p><h3> <strong>It&#39;s not extremely robust: give enough brain damage or the wrong tunings and the mechanism will malfunction severely</strong></h3><p> Which is obvious, there is nothing surprising in “if you damage it, it could break”, this will apply to any solution to some degree. It shouldn&#39;t be surprising that drug abusing or severely ill parents will often fail to care about their child at all. However, If we will succeed at building aligned AGI stable enough for some initial takeoff time, then the problem of protecting it from damage should not be ours to worry at some moment. But we still need to ensure initial stability.</p><h3> <strong>It&#39;s not ideal from our AI ->; humans view</strong></h3><p> Evolution has naturally tuned this mechanism for optimal resource allocation, which sometimes means shutting down care when resources needed to be diverted elsewhere. <strong>Evolution is ruthless because of the limited resources, and will eradicate not only genetic lines that care too less, but also the ones that care too much</strong> . We obviously don&#39;t need that part. And a lot of times you can just give up on your baby and instead try to make a new one, if the situation is too dire, which we also don&#39;t want to happen to us. Which means that we need to understand how it works, to be able to construct it in the way we want.</p><h3> <strong>But it&#39;s also surprisingly robust!</strong></h3><p> Of course, there are exceptions, all people are different and we can&#39;t afford to clone some “proven to be a loving mother” woman hundreds of times to see if the underlying mechanism triggers reliably in all environments. But it seems to work in general, and more so: <strong>it continues to work reliably even with our current technologies</strong> , in our crazy world, far away from initial evolution environment. And we didn&#39;t had to live through waves of birth declines and rises as evolution tries to adapt us to the new realities, tuning brains of new generation of mothers to find the ones that will start to care about their babies in the new agricultural or industrial or information era.</p><h1> <strong>Is this another “anthropomorphizing trap”?</strong></h1><p> For what I know, it is possible to imagine alternative human civilization, without any parental care, so instead our closest candidate for such behavior would be some other intelligent species. Intelligent enough to be able to care in theory and forced by their weak bodies to do so in order to have any descendants at all, maybe it could be some mammals, birds, or whatever, it doesn&#39;t matter. The point I&#39;m making here is that: I don&#39;t think that it is some anthropic trap to search for inspiration or hints in our own behavior, <strong>it just so happens that we are smart, but have weak babies</strong> that require a lot of attention so that we received this mechanism from evolution as a “simplest solution”. You don&#39;t need to search for more compact brains that will allow for longer pregnancy, or hardwire even more knowledge into the infants brains if you can outsource a lot of stuff to the smart parents, you just need to add the “caring drive” and it will work fine. We want AI to care about us, not because we care about our children, and want the same from AI, we just don&#39;t want to die, and <strong>we would want AI to care about us, even if we ourselves would lack this ability</strong> .</p><h1> <strong>Potential flaws:</strong></h1><p> I&#39;m not saying that it&#39;s a go-to solution that we can just copy, but the step in right direction from my view. Replicating similar behavior and studying its parts could be a promising direction. There are a few moments that might make this whole approach useless, for example:</p><ol><li> Somebody will show that there was in fact a lot of evolutionary pressure each time our civilization made another technological leap forward, which caused a lot of leftover children or something like this. Note that it is not sufficient to point at the birth decline, which will kind of prove the point that “people aren&#39;t aligned to evolution”, which I&#39;m not claiming in the first place. You need to show that modern humans/animals will have lower chances to care about their already born babies in the new modern environment. And I&#39;m not sure that pointing out cases where parents made terrible caring choices, would work either, since it could be a capability problem, not the intentions.</li><li> The technical realization requires too much computation because we can&#39;t calculate gradients properly, or something like that. I expect this to work only when some “high” level of capabilities is reached, something on the level of GPT-4 ability to actually construct some world model in order to have a chance of meaningfully predict tokens in completely new texts that are far away from original dataset. Without an agent that have some world model, that could adapt to a new context, I find it strange to expect it to have some general “caring drive”. At best it could memorize some valuable routines that will likely break completely in the new environment.</li><li> Any such drive will be always &quot;aimed&quot; by the global loss function, something like: our parents only care about us in a way for us to make even more babies and to increase our genetic fitness. But it seems false? Maybe because there is in fact some simpler versions of such “caring drives” that evolution found for many genetic lines independently that just makes mothers “love” their babies in some general and robust way, and while given enough time it will be possible to optimize it all out for sure, in most cases it&#39;s the easiest solution that evolution can afford first, for some yet unknown reasons. I understand that in similar environment something really smart will figure out some really optimal way of “caring”, like keeping the baby in some equivalent of cryosleep, shielded from outside, farming the points for keeping it alive, but we will not get there so easily. What we might actually get is some agent that smart enough to find creative ways of keeping the baby agent happy/alive/protected, but still way too dumb to Goodhart all the way to the bottom and out of simulation. And by studying what makes it behave that way we might get some hints about the bigger problem. It still seems helpful to have something like “digital version of a caring cat” to run experiments and understand the underlying mechanism.</li><li> Similar to 3: Maybe it will work mostly in the direction of “mother” agent values, since your baby agent needs roughly the same things to thrive in nature. The mechanism that will mirror the original values and project them to the baby agent will work fine, and that what evolution finds all the time. And it will turns out that the same strange, but mostly useless for us mechanism we will find repeatedly in our experiments.</li></ol><p> Overall I&#39;m pretty sure that this do in fact work, certainly good enough to be a viable research direction.</p><h1> <strong>Technical realization: how do we actually make this happen?</strong></h1><p> I have no concrete idea. I have a few, but I&#39;m not sure about how practically possible they are. And <strong>since nobody knows how this mechanism works</strong> as far as I know, <strong>it&#39;s hard to imagine having the concrete blueprint to create one</strong> . So the best I can give is: we try to create something that looks right from the outside and see if there is anything interesting in the inside. I also have some ideas about “what paths could or couldn&#39;t lead to the interesting insides”.<br><br> First of all, I think this “caring drive” couldn&#39;t run without some internal world model. Something like: it&#39;s hard to imagine far generalized goals without some far generalized capabilities. And world model could be obtained from highly diverse, non repetitive dataset, which forces the model to actually “understand” something and stop memorizing.<br><br> Maybe you can set an environment with multiple agents, similar to Deepmind&#39;s <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play">here</a> ( <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play"><u>https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play</u></a> ), initially reward an agent for surviving by itself, and then introduce the new type of task: baby-agent, that will appear near the original mother-agent (we will call it “birth”), and from that point of time, the whole reward will come purely from how long baby will survive? Baby will initially have less mechanical capabilities, like speed, health, etc and then “grow” to be more capable by itself? I&#39;m not sure what should be the “brain” of baby-agent, another NN or maybe the same that were found from training the mother agent? Maybe creating a chain of agents: agent 1 at some point gives birth to the agent 2 and receive reward for each tick that agent 2 is alive, which itself will give birth to the agent 3 and receive reward fot each tick that agent 3 is alive, and so on. Maybe it will produce something interesting? Obviously the “alive time” is a proxy, and given enough optimization power we should expect Goodhart horrors beyond our comprehension. But the idea is that maybe there is some “simple” solution that will be found first, which we can study. Recreating the product of evolution, not using the immense “computational power” of it could be very tricky.</p><p> But if it seems to work, and mother-agent behave in a seemingly “caring way”, then we can try to apply interpretability tools, try to change the original environment drastically, to see how far it will generalize, try to break something and see how well it works, or manually override some parameters and study the change. <strong>However, I&#39;m not qualified to make this happen anyway, so if you find this idea interesting, contact me, maybe we can do this project together.</strong></p><h1> <strong>How the good result might look like?</strong></h1><p> Let&#39;s imagine that we&#39;ve got some agent that can behave with care toward the right type of “babies”. For some yet unknown reason, from outside view it behaves as if it cares about its baby-agent, it finds the creative ways to do so in new contexts. Now the actual work begins: we need to understand where are the parts that make this possible located, what is the underlying mechanism, what parts are crucial and what happens when you break them, how can we re-write the “baby” target, so that our agent will care about different baby-agents, under what conditions gradient descent will find an automatic off switch (I expect this to be related to the chance of obtaining another baby and given only 1 baby per “life”, gradient will never find the switch, since it will have no use). Then we can actually start to think about recreating it from scratch. Just like what people did with modular addition: <a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><u>https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking</u></a> . Except this time we don&#39;t know how the algorithm could work or look like. But “intentions”, “motivations” and “goals” of potential AI systems are not magic, we should be able to recreate and reverse-engineer them.</p><br/><br/> <a href="https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive<guid ispermalink="false"> JjqZexMgvarBFMKPs</guid><dc:creator><![CDATA[Catnee]]></dc:creator><pubDate> Thu, 07 Sep 2023 10:41:16 GMT</pubDate> </item><item><title><![CDATA[Sharing Information About Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 6:51 AM GMT<br/><br/><p> <i>Epistemic status: Once I started actively looking into things, much of my information in the post below came about by a search for negative information about the Nonlinear cofounders, not from a search to give a balanced picture of its overall costs and benefits. I think standard update rules suggest not that you ignore the information, but you think about how bad you expect the information would be if I selected for the worst, credible info I could share, and then update based on how much worse (or better) it is than you expect I could produce. (See section 5 of this post about</i> <a href="https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence#5___Your_true_reason_screens_off_any_other_evidence_your_argument_might_include__"><i><u>Mistakes with Conservation of Expected Evidence</u></i></a> <i>for more on this.) This seems like a worthwhile exercise for at least non-zero people to do in the comments before reading on. (You can condition on me finding enough to be worth sharing, but also note that I think I have a relatively low bar for publicly sharing critical info about folks in the EA/x-risk/rationalist/etc ecosystem.)</i></p><p> <i>tl;dr: If you want my important updates quickly summarized in four claims-plus-probabilities, jump to the section near the bottom titled &quot;Summary of My Epistemic State&quot;.</i></p><hr><p> When I used to manage the <a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Lightcone Offices</u></a> , I spent a fair amount of time and effort on gatekeeping — processing applications from people in the EA/x-risk/rationalist ecosystem to visit and work from the offices, and making decisions. Typically this would involve reading some of their public writings, and reaching out to a couple of their references that I trusted and asking for information about them. A lot of the people I reached out to were surprisingly great at giving honest references about their experiences with someone and sharing what they thought about someone.</p><p> One time, Kat Woods and Drew Spartz from Nonlinear applied to visit. I didn&#39;t know them or their work well, except from a few brief interactions that Kat Woods seems high-energy, and to have a more optimistic outlook on life and work than most people I encounter.</p><p> I reached out to some references Kat listed, which were positive to strongly positive. However I also got a strongly negative reference — someone else who I informed about the decision told me they knew former employees who felt taken advantage of around things like salary. However the former employees reportedly didn&#39;t want to come forward due to fear of retaliation and generally wanting to get away from the whole thing, and the reports felt very vague and hard for me to concretely visualize, but nonetheless the person strongly recommended against inviting Kat and Drew.</p><p> I didn&#39;t feel like this was a strong enough reason to bar someone from a space — or rather, I did, but vague anonymous descriptions of very bad behavior being sufficient to ban someone is a system that can be straightforwardly abused, so I don&#39;t want to use such a system. Furthermore, I was interested in getting my own read on Kat Woods from a short visit — she had only asked to visit for a week. So I accepted, though I informed her that this weighed on my mind. ( <a href="https://docs.google.com/document/d/1rldJBi3eVVeqZjqbpCljgz9rmfNbuQW7HMEpugsPWt4/edit"><u>This is a link to the decision email I sent to her.</u></a> )</p><p> (After making that decision I was also linked to this ominous yet still vague <a href="https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding?commentId=5P75dFuKLo894MQFf"><u>EA Forum thread</u></a> , that includes a former coworker of Kat Woods saying they did not like working with her, more comments like the one I received above, and links to a lot of strongly negative Glassdoor reviews for Nonlinear Cofounder Emerson Spartz&#39;s former company “Dose”. Note that more than half of the negative reviews are for the company after Emerson sold it, but this is a concerning one from 2015 (while Emerson Spartz was CEO/Cofounder): &quot; <a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW8511849.htm"><u>All of these super positive reviews are being commissioned by upper management. That is the first thing you should know about Spartz, and I think that gives a pretty good idea of the company&#39;s priorities… care more about the people who are working for you and less about your public image</u></a> &quot;. A 2017 review says &quot; <a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW14177614.htm"><u>The culture is toxic with a lot of cliques, internal conflict, and finger pointing.</u></a> &quot; There are also far worse reviews about a hellish work place which are very worrying, but they&#39;re from the period after Emerson&#39;s LinkedIn says he left, so I&#39;m not sure to what extent he is responsible he is for them.)</p><p> On the first day of her visit, another person in the office privately reached out to me saying they were extremely concerned about having Kat and Drew in the office, and that they knew two employees who had had terrible experiences working with them. They wrote (and we later discussed it more):</p><blockquote><p> Their company Nonlinear has a history of illegal and unethical behavior, where they will attract young and naive people to come work for them, and subject them to inhumane working conditions when they arrive, fail to pay them what was promised, and ask them to do illegal things as a part of their internship. I personally know two people who went through this, and they are scared to speak out due to the threat of reprisal, specifically by Kat Woods and Emerson Spartz.</p></blockquote><p> This sparked (for me) a 100-200 hour investigation where I interviewed 10-15 people who interacted or worked with Nonlinear, read many written documents and tried to piece together some of what had happened.</p><p> My takeaway is that indeed their two in-person employees had quite horrendous experiences working with Nonlinear, and that Emerson Spartz and Kat Woods are significantly responsible both for the harmful dynamics and for the employees&#39; silence afterwards. Over the course of investigating Nonlinear I came to believe that the former employees there had no legal employment, tiny pay, a lot of isolation due to travel, had implicit and explicit threats of retaliation made if they quit or spoke out negatively about Nonlinear, simultaneously received a lot of (in my opinion often hollow) words of affection and claims of familial and romantic love, experienced many further unpleasant or dangerous experiences that they wouldn&#39;t have if they hadn&#39;t worked for Nonlinear, and needed several months to recover with friends and family afterwards before they felt able to return to work.</p><p> (Note that I don&#39;t think the pay situation as-described in the above quoted text was entirely accurate, I think it was very small — $1k/month — and employees implicitly expected they would get more than they did, but there was mostly not salary &#39;promised&#39; that didn&#39;t get given out.)</p><p> After first hearing from them about their experiences, I still felt unsure about what was true — I didn&#39;t know much about the Nonlinear cofounders, and I didn&#39;t know which claims about the social dynamics I could be confident of. To get more context, I spent about 30+ hours on calls with 10-15 different people who had some professional dealings with at least one of Kat, Emerson and Drew, trying to build up a picture of the people and the org, and this helped me a lot in building my own sense of them by seeing what was common to many people&#39;s experiences. I talked to many people who interacted with Emerson and Kat who had many active ethical concerns about them and strongly negative opinions, and I also had a 3-hour conversation with the Nonlinear cofounders about these concerns, and I now feel a lot more confident about a number of dynamics that the employees reported.</p><p> For most of these conversations I offered strict confidentiality, but (with the ex-employees&#39; consent) I&#39;ve here written down some of the things I learned.</p><p> In this post I do not plan to name most of the people I talked to, but two former employees I will call “Alice” and “Chloe”. I think the people involved mostly want to put this time in their life behind them and I would encourage folks to respect their privacy, not name them online, and not talk to them about it unless you&#39;re already good friends with them.</p><p> <strong>Conversation with Kat on March 7th, 2023</strong></p><p> Returning to my initial experience: on the Tuesday of their visit, I still wasn&#39;t informed about who the people were or any details of what happened, but I found an opportunity to chat with Kat over lunch.</p><p> After catching up for ~15 mins, I indicated that I&#39;d be interested in talking about the concerns I raised in my email, and we talked in a private room for 30-40 mins. As soon as we sat down, Kat launched straight into stories about two former employees of hers, telling me repeatedly not to trust one of the employees (“Alice”), that she has a terrible relationship with truth, that she&#39;s dangerous, and that she&#39;s a reputational risk to the community. She said the other employee (&quot;Chloe&quot;) was “fine”.</p><p> Kat Woods also told me that she expected to have a policy with her employees of “I don&#39;t say bad things about you, you don&#39;t say bad things about me”. I am strongly against this kind of policy on principle (as I told her then). This and other details raised further red flags to me (ie the salary policy) and I wanted to understand what happened.</p><p> Here&#39;s an overview of what she told me:</p><ul><li> When they worked at Nonlinear, Alice and Chloe had expenses covered (room, board, food) and Chloe also got a monthly bonus of $1k/month.</li><li> Alice and Chloe lived in the same house as Kat, Emerson and Drew. Kat said that she has decided to not live with her employees going forward.</li><li> She said that Alice, who incubated their own project (<a href="https://web.archive.org/web/20220321075904/https://www.nonlinear.org/hiringagency.html"><u>here is a description of the incubation program on Nonlinear&#39;s site</u></a> ), was able to set their own salary, and that Alice almost never talked to her (Kat) or her other boss (Emerson) about her salary.</li><li> Kat doesn&#39;t trust Alice to tell the truth, and that Alice has a history of “catastrophic misunderstandings”.</li><li> Kat told me that Alice was unclear about the terms of the incubation, and said that Alice should have checked in with Kat in order to avoid this miscommunication.</li><li> Kat suggested that Alice may have quit in substantial part due to Kat missing a check-in call over Zoom toward the end.</li><li> Kat said that she hoped Alice would go by the principle of “I don&#39;t say bad things about you, you don&#39;t say bad things about me” but that the employee wasn&#39;t holding up her end and was spreading negative things about Kat/Nonlinear.</li><li> Kat said she gives negative references for Alice, advises people “don&#39;t hire her” and not to fund her, and “she&#39;s really dangerous for the community”.</li><li> She said she didn&#39;t have these issues with her other employee Chloe, she said she was “fine, just miscast” for her role of “assistant / operations manager”, which is what led to her quitting. Kat said Chloe was pretty skilled but did a lot of menial labor tasks for Kat that she didn&#39;t enjoy.</li><li> The one negative thing she said about Chloe was that she was being paid the equivalent of $75k <span class="footnote-reference" role="doc-noteref" id="fnref8a69v0tq2qo"><sup><a href="#fn8a69v0tq2qo">[1]</a></sup></span> per year (only $1k/month, the rest via room and board), but that at one point she asked for $75k <i>on top</i> of all expenses being paid and that was out of the question. <span class="footnote-reference" role="doc-noteref" id="fnrefo53culramn"><sup><a href="#fno53culramn">[2]</a></sup></span></li></ul><h2> A High-Level Overview of The Employees&#39; Experience with Nonlinear</h2><p><strong>背景</strong></p><p>The core Nonlinear staff are Emerson Spartz, Kat Woods, and Drew Spartz.</p><p> Kat Woods has been in the EA ecosystem for at least 10 years, cofounding Charity Science in 2013 and working there until 2019. After a year at Charity Entrepreneurship, in 2021 she <a href="https://forum.effectivealtruism.org/posts/fX8JsabQyRSd7zWiD/introducing-the-nonlinear-fund-ai-safety-research-incubation"><u>cofounded</u></a> Nonlinear with Emerson Spartz, where she has worked for 2.5 years.</p><p> Nonlinear has received $599,000 <a href="https://survivalandflourishing.fund/sff-2022-h1-recommendations"><u>from</u></a> the Survival and Flourishing Fund in the first half of 2022, and $15,000 <a href="https://www.openphilanthropy.org/grants/nonlinear-fund-personal-assistant-hiring-agency/"><u>from</u></a> Open Philanthropy in January 2022.</p><p> Emerson primarily funds the project through his personal wealth from his previous company Dose and from selling Mugglenet.com (which he founded). Emerson and Kat are romantic partners, and Emerson and Drew are brothers. They all live in the same house and travel across the world together, jumping from AirBnb to AirBnb once or twice per month. The staff they hire are either remote, or live in the house with them.</p><p> My current understanding is that they&#39;ve had around ~4 remote interns, 1 remote employee, and 2 in-person employees (Alice and Chloe). Alice was the only person to go through their incubator program.</p><p> Nonlinear tried to have a fairly high-commitment culture where the long-term staff are involved very closely with the core family unit, both personally and professionally. However they were given exceedingly little financial independence, and a number of the social dynamics involved seem really risky to me.</p><p> <strong>Alice and Chloe</strong></p><p> Alice travelled with Nonlinear from November 2021 to June 2022 and started working for the org from around February, and Chloe worked there from January 2022 to July 2022. After talking with them both, I learned the following:</p><ul><li> Neither were legally employed by the non-profit at any point.</li><li> Chloe&#39;s and Alice&#39;s finances (along with Kat&#39;s and Drew&#39;s) all came directly from Emerson&#39;s personal funds (not from the non-profit). This left them having to get permission for their personal purchases, and they were not able to live apart from the family unit while they worked with them, and they report feeling very socially and financially dependent on the family during the time they worked there.</li><li> Chloe&#39;s salary was verbally agreed to come out to around $75k/year. However, she was only paid $1k/month, and otherwise had many basic things compensated ie rent, groceries, travel. This was supposed to make traveling together easier, and supposed to come out to the same salary level. While Emerson did compensate Alice and Chloe with food and board and travel, Chloe does not believe that she was compensated to an amount equivalent to the salary discussed, and I believe no accounting was done for either Alice or Chloe to ensure that any salary matched up. (I&#39;ve done some spot-checks of the costs of their AirBnbs and travel, and Alice/Chloe&#39;s epistemic state seems pretty reasonable to me.)</li><li> Alice joined as the sole person in their incubation program. She moved in with them after meeting Nonlinear at EAG and having a ~4 hour conversation there with Emerson, plus a second Zoom call with Kat. Initially while traveling with them she continued her previous job remotely, but was encouraged to quit and work on an incubated org, and after 2 months she quit her job and started working on projects with Nonlinear. Over the 8 months she was there Alice claims she received no salary for the first 5 months, then (roughly) $1k/month salary for 2 months, and then after she quit she received a ~$6k one-off salary payment (from the funds allocated for her incubated organization). She also had a substantial number of emergency health issues covered. <span class="footnote-reference" role="doc-noteref" id="fnrefylc0clu1ffa"><sup><a href="#fnylc0clu1ffa">[3]</a></sup></span></li><li> Salary negotiations were consistently a major stressor for Alice&#39;s entire time at Nonlinear. Over her time there she spent through all of her financial runway, and spent a significant portion of her last few months there financially in the red (having more bills and medical expenses than the money in her bank account) in part due to waiting on salary payments from Nonlinear. She eventually quit due to a combination of running exceedingly low on personal funds and wanting financial independence from Nonlinear, and as she quit she gave Nonlinear (on their request) full ownership of the organization that she had otherwise finished incubating.</li><li> From talking with both Alice and Nonlinear, it turned out that by the end of Alice&#39;s time working there, since the end of February Kat Woods had thought of Alice as an employee that she managed, but that Emerson had not thought of Alice as an employee, primarily just someone who was traveling with them and collaborating because she wanted to, and that the $1k/month plus other compensation was a generous gift.</li><li> Alice and Chloe reported that Kat, Emerson, and Drew created an environment in which being a valuable member of Nonlinear included being entrepreneurial and creative in problem-solving — in practice this often meant getting around standard social rules to get what you wanted was strongly encouraged, including getting someone&#39;s favorite table at a restaurant by pressuring the staff, and finding loopholes in laws pertaining to their work. This also applied internally to the organization. Alice and Chloe report being pressured into or convinced to take multiple actions that they seriously regretted whilst working for Nonlinear, such as becoming very financially dependent on Emerson, quitting being vegan, and driving without a license in a foreign country for many months. (To be clear I&#39;m not saying that these laws are good and that breaking them is bad, I&#39;m saying that it sounds to me from their reports like they were convinced to take actions that could have had severe personal downsides such as jail time in a foreign country, and that these are actions that they confidently believe they would not have taken had it not been due to the strong pressures they felt from the Nonlinear cofounders and the adversarial social environment internal to the company.) I&#39;ll describe these events in more detail below.</li><li> They both report taking multiple months to recover after ending ties with Nonlinear, before they felt able to work again, and both describe working there as one of the worst experiences of their lives.</li><li> They both report being actively concerned about professional and personal retaliation from Nonlinear for speaking to me, and told me stories and showed me some texts that led me to believe that was a very credible concern.</li></ul><h2> An assortment of reported experiences</h2><p> There are a lot of parts of their experiences at Nonlinear that these two staff found deeply unpleasant and hurtful. I will summarize a number of them below.</p><p> I think many of the things that happened are warning flags, I also think that there are some red lines, I&#39;ll discuss my thoughts on which are the red lines in my takeaways at the bottom of this post.</p><p> <strong>My Level of Trust in These Reports</strong></p><p> Most of the dynamics were described to me as accurate by multiple different people (low pay, no legal structure, isolation, some elements of social manipulation, intimidation), leading me to have high confidence in them, and Nonlinear themselves confirmed various parts of these accounts.</p><p> People whose word I would meaningfully update on about this sort of thing have vouched for Chloe&#39;s word as reliable.</p><p> The Nonlinear staff and a small number of other people who visited during Alice and Chloe&#39;s employment have strongly questioned Alice&#39;s trustworthiness and suggested she has told outright lies. Nonlinear showed me texts where people who had spoken with Alice came away with the impression that she was paid $0 or $500, which is inaccurate (she was paid ~$8k on net, as she told me).</p><p> That said, I personally found Alice very willing and ready to share primary sources with me upon request (texts, bank info, etc), so I don&#39;t believe her to be acting in bad faith.</p><p> In my first conversation with her, Kat claimed that Alice had many catastrophic miscommunications, but that Chloe was (quote) “fine”. In general nobody questioned Chloe&#39;s word and broadly the people who told me they questioned Alice&#39;s word said they trusted Chloe&#39;s.</p><p> Personally I found all of their fears of retaliation to be genuine and earnest, and in my opinion justified.</p><p> <strong>Why I&#39;m sharing these</strong></p><p> I do have a strong heuristic that says consenting adults can agree to all sorts of things that eventually hurt them (ie in accepting these jobs), even if I paternalistically might think I could have prevented them from hurting themselves. That said, I see clear reasons to think that Kat, Emerson and Drew intimidated these people into accepting some of the actions or dynamics that hurt them, so some parts do not seem obviously consensual to me.</p><p> Separate from that, I think it&#39;s good for other people to know what they&#39;re getting into, so I think sharing this info is good because it is relevant for many people who have any likelihood of working with Nonlinear. And most importantly to me, I especially want to do it because it seems to me that Nonlinear has tried to prevent this negative information from being shared, so I am erring strongly on the side of sharing things.</p><p> (One of the employees also wanted to say something about why she contributed to this post, and I&#39;ve put it in a footnote here. <span class="footnote-reference" role="doc-noteref" id="fnref2ad0whew08p"><sup><a href="#fn2ad0whew08p">[4]</a></sup></span> )</p><p> <strong>Highly dependent finances and social environment</strong></p><p> Everyone lived in the same house. Emerson and Kat would share a room, and the others would make do with what else was available, often sharing bedrooms.</p><p> Nonlinear primarily moved around countries where they typically knew no locals and the employees regularly had nobody to interact with other than the cofounders, and employees report that they were denied requests to live in a separate AirBnb from the cofounders.</p><p> Alice and Chloe report that they were advised not to spend time with &#39;low value people&#39;, including their families, romantic partners, and anyone local to where they were staying, with the exception of guests/visitors that Nonlinear invited. Alice and Chloe report this made them very socially dependent on Kat/Emerson/Drew and otherwise very isolated.</p><p> The employees were very unclear on the boundaries of what would and wouldn&#39;t be paid for by Nonlinear. For instance, Alice and Chloe report that they once spent several days driving around Puerto Rico looking for cheaper medical care for one of them before presenting it to senior staff, as they didn&#39;t know whether medical care would be covered, so they wanted to make sure that it was as cheap as possible to increase the chance of senior staff saying yes.</p><p> The financial situation is complicated and messy. This is in large-part due to them doing very little accounting. In summary Alice spent a lot of her last 2 months with less than €1000 in her bank account, sometimes having to phone Emerson for immediate transfers to be able to cover medical costs when she was visiting doctors. At the time of her quitting she had €700 in her account, which was not enough to cover her bills at the end of the month, and left her quite scared. Though to be clear she was paid back ~€2900 of her outstanding salary by Nonlinear within a week, in part due to her strongly requesting it. (The relevant thing here is the extremely high level of financial dependence and wealth disparity, but Alice does not claim that Nonlinear failed to pay them.)</p><p> One of the central reasons Alice says that she stayed on this long was because she was expecting financial independence with the launch of her incubated project that had $100k allocated to it (fundraised from FTX). In her final month there Kat informed her that while she would work quite independently, they would keep the money in the Nonlinear bank account and she would ask for it, meaning she wouldn&#39;t have the financial independence from them that she had been expecting, and learning this was what caused Alice to quit.</p><p> One of the employees interviewed Kat about her productivity advice, and shared notes from this interview with me. The employee writes:</p><blockquote><p> During the interview, Kat openly admitted to not being productive but shared that she still appeared to be productive because she gets others to do work for her. She relies on volunteers who are willing to do free work for her, which is her top productivity advice.</p></blockquote><p> The employees report that some interns later gave strongly negative feedback on working unpaid, and so Kat decided that she would no longer have interns at all.</p><p> <strong>Severe downsides threatened if the working relationship didn&#39;t work out</strong></p><p> In a conversation between Emerson Spartz and one of the employees, the employee asked for advice for a friend that wanted to find another job while being employed, without letting their current employer know about their decision to leave yet. Emerson reportedly immediately stated that he now has to update towards considering that the said employee herself is considering leaving Nonlinear. He went on to tell her that he gets mad at his employees who leave his company for other jobs that are equally good or less good; he said he understands if employees leave for clearly better opportunities. The employee reports that this led them to be very afraid of leaving the job, both because of the way Emerson made the update on thinking the employee is now trying to leave, as well as the notion of Emerson being retaliative towards employees that leave for “bad reasons”.</p><p> For background context on Emerson&#39;s business philosophy: Alice quotes Emerson advising the following indicator of work progress: &quot;How much value are you able to extract from others in a short amount of time?&quot; <span class="footnote-reference" role="doc-noteref" id="fnreft84or63yyei"><sup><a href="#fnt84or63yyei">[5]</a></sup></span> Another person who visited described Emerson to me as “always trying to use all of his bargaining power”. Chloe told me that, when she was negotiating salaries with external partners on behalf of Nonlinear, Emerson advised her when negotiating salaries, to offer &quot;the lowest number you can get away with&quot;.</p><p> Many different people reported that Emerson Spartz would boast about his business negotiations tactics to employees and visitors. He would encourage his employees to read many books on strategy and influence. When they read the book <a href="https://www.amazon.com/48-Laws-Power-Robert-Greene/dp/0140280197"><u>The 48 Laws of Power</u></a> he would give examples of him following the “laws” in his past business practices.</p><p> One story that he told to both employees and visitors was about his intimidation tactics when involved in a conflict with a former teenage mentee of his, Adorian Deck.</p><p> (For context on the conflict, here&#39;s links to articles written about it at the time: <a href="https://www.hollywoodreporter.com/business/business-news/teen-who-created-omgfacts-twitter-182620/"><u>Hollywood Reporter</u></a> , <a href="https://www.jacksonville.com/story/business/2011/05/16/california-teens-lawsuit-over-twitter-feed-underscores-power-tweet/15903568007/"><u>Jacksonville</u></a> , <a href="https://blog.ericgoldman.org/archives/2011/05/thoughts_on_the.htm"><u>Technology &amp; Marketing Law Blog</u></a> , and <a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>Emerson Spartz&#39;s Tumblr</u></a> . Plus here is the <a href="https://web.archive.org/web/20110824205315/http://www.omgfactslawsuit.com.s3.amazonaws.com/CONTRACT.pdf"><u>Legal Contract</u></a> they signed that Deck later sued to undo.)</p><p> In brief, Adorian Deck was a 16 year-old who (in 2009) made a Twitter account called “OMGFacts” that quickly grew to having 300,000+ followers. Emerson reached out to build companies under the brand, and agreed to a deal with Adorian. Less than a year later Adorian wanted out of the deal, claiming that Emerson had made over $100k of profits and he&#39;d only seen $100, and sued to end the deal.</p><p> According to Emerson, it turned out that there&#39;s a clause unique to California (due to the acting profession in Los Angeles) where even if a minor and their parent signs a contract, it isn&#39;t valid unless the signing is overseen by a judge, and so they were able to simply pull out of the deal.</p><p> But to this day Emerson&#39;s company still owns the OMGfacts brand and companies and Youtube channels.</p><p> (Sidenote: I am not trying to make claims about who was “in the right” in these conflicts, I am reporting these as examples of Emerosn&#39;s negotiation tactics that he reportedly engages in and actively endorses during conflicts.)</p><p> Emerson told versions of this story to different people who I spoke to (people reported him as &#39;bragging&#39;).</p><p> In one version, he claimed that he strong-armed Adorian and his mother with endless legal threats and they backed down and left him with full control of the brand. This person I spoke to couldn&#39;t recall the details but said that Emerson tried to frighten Deck and his mother, and that they (the person Emerson was bragging to) found it “frightening” and thought the behavior was “behavior that&#39;s like 7 standard deviations away from usual norms in this area.”</p><p> Another person was told the story in the context of the 2nd Law from “48 Laws of Power”, which is “Never put too much trust in friends, learn how to use enemies”. The summary includes</p><blockquote><p> “Be wary of friends—they will betray you more quickly, for they are easily aroused to envy. They also become spoiled and tyrannical… you have more to fear from friends than from enemies.”</p></blockquote><p> For this person who was told the Adorian story, the thing that resonated most when he told it was the claim that he was in a close, mentoring relationship with Adorian, and leveraged knowing him so well that he would know “exactly where to go to hurt him the most” so that he would back off. In that version of the story, he says that Deck&#39;s life-goal was to be a YouTuber (which is indeed Deck&#39;s profession until this day — <a href="https://www.youtube.com/@AdorianDeck"><u>he produces about 4 videos a month</u></a> ), and that Emerson strategically contacted the YouTubers that Deck most admired, and told them stories of Deck being lazy and trying to take credit for all of Emerson&#39;s work. He reportedly threatened to do more of this until Deck relented, and this is why Deck gave up the lawsuit. The person said to me “He loved him, knew him really well, and destroyed him with that knowledge.” <span class="footnote-reference" role="doc-noteref" id="fnrefzcp7frumok"><sup><a href="#fnzcp7frumok">[6]</a></sup></span></p><p> I later spoke with Emerson about this. He does say that he was working with the top YouTubers to create videos exposing Deck, and this is what brought Deck back to the negotiating table. He says that he ended up renegotiating a contract where Deck receives $10k/month for 7 years. If true, I think this final deal reflects positively on Emerson, though I still believe the people he spoke to were actively scared by their conversations with Emerson on this subject. (I have neither confirmed the existence of the contract nor heard Deck&#39;s side of the story.)</p><p> He reportedly told another negotiation story about his response to getting scammed in a business deal. I won&#39;t go into the details, but reportedly he paid a high-price for the rights to a logo/trademark, only to find that he had not read the fine print and had been sold something far less valuable. He gave it as an example of the &quot;Keep others in suspended terror: cultivate an air of unpredictability&quot; strategy from The 48 Laws of Power:</p><blockquote><p> Be deliberately unpredictable. Behavior that seems to have no consistency or purpose will keep them off-balance, and they will wear themselves out trying to explain your moves. Taken to an extreme, this strategy can intimidate and terrorize.</p></blockquote><p> In that business negotiation, he (reportedly) acted unhinged. According to the person I spoke with, he said he&#39;d call the counterparty and say “batshit crazy things” and yell at them, with the purpose of making them think he&#39;s capable of anything, including dangerous and unethical things, and eventually they relented and gave him the deal he wanted.</p><p> Someone else I spoke to reported him repeatedly saying that he would be “very antagonistic” toward people he was in conflict with. He reportedly gave the example that, if someone tried to sue him, he would be willing to go into legal gray areas in order to “crush his enemies” (a phrase he apparently used a lot), including hiring someone to stalk the person and their family in order to freak them out. (Emerson denies having said this, and suggests that he was probably describing this as a strategy that someone <i>else</i> might use in a conflict that one ought to be aware of.)</p><p> After Chloe eventually quit, Alice reports that Kat/Emerson would “trash talk” her, saying she was never an “A player”, criticizing her on lots of dimensions (competence, ethics, drama, etc) in spite of previously primarily giving Chloe high praise. This reportedly happened commonly toward other people who ended or turned down working together with Nonlinear.</p><p> Here are some texts between Kat Woods and Alice shortly after Alice had quit, before the final salary had been paid. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/fuglgbnch2qgely8aahb"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/ezzjqbuxcxaqfpte46ee"></p><p> A few months later, some more texts from Kat Woods. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/cd087amewavbd71uq75z"></p><p> (I can corroborate that it was difficult to directly talk with the former employee and it took a fair bit of communication through indirect social channels before they were willing to identify themselves to me and talk about the details.)</p><p> <strong>Effusive positive emotion not backed up by reality, and other manipulative techniques</strong></p><p> Multiple people who worked with Kat reported that Kat had a pattern of enforcing arbitrary short deadlines on people in order to get them to make the decision she wants eg <i>“I need a decision by the end of this call”</i> , or (in an email to Alice) “ <i>This is urgent and important. There are people working on saving the world and we can&#39;t let our issues hold them back from doing their work.”</i></p><p> Alice reported feeling emotionally manipulated. She said she got constant compliments from the founders that ended up seeming fake.</p><p> Alice wrote down a string of the compliments at the time from Kat Woods (said out loud and that Alice wrote down in text), here is a sampling of them that she shared with me:</p><blockquote><p> “You&#39;re the kind of person I bet on, you&#39;re a beast, you&#39;re an animal, I think you are extraordinary&quot;</p><p> &quot;You can be in the top 10, you really just have to think about where you want to be, you have to make sacrifices to be on the top, you can be the best, only if you sacrifice enough&quot;</p><p> &quot;You&#39;re working more than 99% because you care more than 99% because you&#39;re a leader and going to save the world&quot;</p><p> &quot;You can&#39;t fail if you commit to [this project], you have what it takes, you get sh*t done and everyone will hail you in EA, finally an executor among us.&quot;</p></blockquote><p> Alice reported that she would get these compliments near-daily. She eventually had the sense that this was said in order to get something out of her. She reported that one time, after a series of such compliments, the Kat Woods then turned and recorded a near-identical series of compliments into their phone for a different person.</p><p> Kat Woods reportedly several times cried while telling Alice that she wanted the employee in their life forever and was worried that this employee would ever not be in Kat&#39;s life.</p><p> Other times when Alice would come to Kat with money troubles and asking for a pay rise, Alice reports that Kat would tell them that this was a psychological issue and that actually they had safety, for instance they could move back in with their parents, so they didn&#39;t need to worry.</p><p> Alice also reports that she was explicitly advised by Kat Woods to cry and look cute when asking Emerson Spartz for a salary improvement, in order to get the salary improvement that she wanted, and was told this was a reliable way to get things from Emerson. (Alice reports that she did not follow this advice.)</p><p> <strong>Many other strong personal costs</strong></p><p> Alice quit being vegan while working there. She was sick with covid in a foreign country, with only the three Nonlinear cofounders around, but nobody in the house was willing to go out and get her vegan food, so she barely ate for 2 days. Alice eventually gave in and ate non-vegan food in the house. She also said that the Nonlinear cofounders marked her quitting veganism as a &#39;win&#39;, as they thad been arguing that she should not be vegan.</p><p> (Nonlinear disputes this, and says that they did go out and buy her some vegan burgers food and had some vegan food in the house. They agree that she quit being vegan at this time, and say it was because being vegan was unusually hard due to being in Puerto Rico. Alice disputes that she received any vegan burgers.)</p><p> Alice said that this generally matched how she and Chloe were treated in the house, as people generally not worth spending time on, because they were &#39;low value&#39; (ie in terms of their hourly wage), and that they were the people who had to do chores around the house (eg Alice was still asked to do house chores during the period where she was sick and not eating).</p><p> By the same reasoning, the employees reported that they were given 100% of the menial tasks around the house (cleaning, tidying, etc) due to their lower value of time to the company. For instance, if a cofounder spilled food in the kitchen, the employees would clean it up. This was generally reported as feeling very demeaning.</p><p> Alice and Chloe reported a substantial conflict within the household between Kat and Alice. Alice was polyamorous, and she and Drew entered into a casual romantic relationship. Kat previously had a polyamorous marriage that ended in divorce, and is now monogamously partnered with Emerson. Kat reportedly told Alice that she didn&#39;t mind polyamory &quot;on the other side of the world”, but couldn&#39;t stand it right next to her, and probably either Alice would need to become monogamous or Alice should leave the organization. Alice didn&#39;t become monogamous. Alice reports that Kat became increasingly cold over multiple months, and was very hard to work with. <span class="footnote-reference" role="doc-noteref" id="fnrefuvloifuhbna"><sup><a href="#fnuvloifuhbna">[7]</a></sup></span></p><p> Alice reports then taking a vacation to visit her family, and trying to figure out how to repair the relationship with Kat. Before she went on vacation, Kat requested that Alice bring a variety of illegal drugs across the border for her (some recreational, some for productivity). Alice argued that this would be dangerous for her personally, but Emerson and Kat reportedly argued that it is not dangerous at all and was “absolutely risk-free”. Privately, Drew said that Kat would “love her forever” if she did this. I bring this up as an example of the sorts of requests that Kat/Emerson/Drew felt comfortable making during Alice&#39;s time there.</p><p> Chloe was hired by Nonlinear with the intent to have them do executive assistant tasks for Nonlinear ( <a href="https://web.archive.org/web/20211022160447/https://www.nonlinear.org/operations.html"><u>this is the job ad they responded to</u></a> ). After being hired and flying out, Chloe was informed that on a daily basis their job would involve driving eg to get groceries when they were in different countries. She explained that she didn&#39;t have a drivers&#39; license and didn&#39;t know how to drive. Kat/Emerson proposed that Chloe learn to drive, and Drew gave her some driving lessons. When Chloe learned to drive well enough in parking lots, she said she was ready to get her license, but she discovered that she couldn&#39;t get a license in a foreign country. Kat/Emerson/Drew reportedly didn&#39;t seem to think that mattered or was even part of the plan, and strongly encouraged Chloe to just drive without a license to do their work, so she drove ~daily for 1-2 months without a license. (I think this involved physical risks for the employee and bystanders, and also substantial risks of being in jail in a foreign country. Also, Chloe basically never drove Emerson/Drew/Kat, this was primarily solo driving for daily errands.) Eventually Chloe had a minor collision with a street post, and was a bit freaked out because she had no idea what the correct protocols were. She reported that Kat/Emerson/Drew didn&#39;t think that this was a big deal, but that Alice (who she was on her way to meet) could clearly see that Chloe was distressed by this, and Alice drove her home, and Chloe then decided to stop driving.</p><p> (Car accidents are the <a href="https://www.cdc.gov/injury/wisqars/pdf/leading_causes_of_injury_deaths_highlighting_unintentional_2018-508.pdf"><u>second most common cause of death</u></a> for people in their age group. Insofar as they were pressured to do this and told that this was safe, I think this involved a pretty cavalier disregard for the safety of the person who worked for them.)</p><p> Chloe talked to a friend of hers (who is someone I know fairly well, and was the first person to give me a negative report about Nonlinear), reporting that they were very depressed. When Chloe described her working conditions, her friend was horrified, and said she had to get out immediately since, in their words, “this was clearly an abusive situation”. The friend offered to pay for flights out of the country, and tried to convince her to quit immediately. Eventually Chloe made a commitment to book a flight by a certain date and then followed through with that.</p><p> <strong>Lax on legalities and adversarial business practices</strong></p><p> I did not find the time to write much here. For now I&#39;ll simply pass on my impressions.</p><p> I generally got a sense from speaking with many parties that Emerson Spartz and Kat Woods respectively have very adversarial and very lax attitudes toward legalities and bureaucracies, with the former trying to do as little as possible that is asked of him. If I asked them to fill out paperwork I would expect it was filled out at least reluctantly and plausibly deceptively or adversarially in some way. In my current epistemic state, I would be actively concerned about any project in the EA or x-risk ecosystems that relied on Nonlinear doing any accounting or having a reliable legal structure that has had the basics checked.</p><p> Personally, if I were giving Nonlinear funds for any project whatsoever, including for regranting, I&#39;d expect it&#39;s quite plausible (>;20%) that they didn&#39;t spend the funds on what they told me, and instead will randomly spend it on some other project. If I had previously funded Nonlinear for any projects, I would be keen to ask Nonlinear for receipts to show whether they spent their funds in accordance with what they said they would.</p><p> <strong>This is not a complete list</strong></p><p> I want to be clear that this is not a <i>complete</i> list of negative or concerning experiences, this is an <i>illustrative</i> list. There are many other things that I was told about that I am not including here due to factors like length and people&#39;s privacy (on all sides). Also I split them up into the categories as I see them; someone else might make a different split.</p><p> <strong>Perspectives From Others Who Have Worked or Otherwise Been Close With Nonlinear</strong></p><p> I had hoped to work this into a longer section of quotes, but it seemed like too much back-and-forth with lots of different people. I encourage folks to leave comments with their relevant impressions.</p><p> For now I&#39;ll summarize some of what I learned as follows:</p><ul><li> Several people gave reports consistent with Alice and Chloe being very upset and distressed both during and after their time at Nonlinear, and reaching out for help, and seeming really strongly to want to get away from Nonlinear.</li><li> Some unpaid interns (who worked remotely for Nonlinear for 1-3 months) said that they regretted not getting paid, and that when they brought it up with Kat Woods she said some positive sounding things and they expected she would get back to them about it, but that never happened during the rest of their internships.</li><li> Many people who visited had fine experiences with Nonlinear, others felt much more troubled by the experience.</li><li> One person said to me about Emerson/Drew/Kat:<ul><li> <i>&quot;My subjective feeling is like &#39;they seemed to be really bad and toxic people&#39;. And they at the same time have a decent amount of impact. After I interacted repeatedly with them I was highly confused about the dilemma of people who are mistreating other people, but are doing some good.&quot;</i></li></ul></li><li> Another person said about Emerson:<ul><li> <i>“He seems to think he&#39;s extremely competent, a genius, and that everyone else is inferior to him. They should learn everything they can from him, he has nothing to learn from them. He said things close to this explicitly. Drew and (to a lesser extent) Kat really bought into him being the new messiah.”</i></li></ul></li><li> One person who has worked for Kat Woods (not Alice or Chloe) said the following:<ul><li> <i>I love her as a person, hate her as a boss. She&#39;s fun, has a lot of ideas, really good socialite, and I think that that speaks to how she&#39;s able to get away with a lot of things. Able to wear different masks in different places. She&#39;s someone who&#39;s easy to trust, easy to build social relationships with. I&#39;d be suspicious of anyone who gives a reference who&#39;s never been below Kat in power.</i></li><li> <i>Ben: Do you think Kat is emotionally manipulative?</i></li><li> <i>I think she is. I think it&#39;s a fine line about what makes an excellent entrepreneur. Do whatever it takes to get a deal signed. To get it across the line. Depends a lot on what the power dynamics are, whether it&#39;s a problem or not. If people are in equal power structures it&#39;s less of a problem.</i></li></ul></li></ul><p> There were other informative conversations that I won&#39;t summarize. I encourage folks who have worked with or for Nonlinear to comment with their perspective.</p><h2> Conversation with Nonlinear</h2><p> After putting the above together, I got permission from Alice and Chloe to publish, and to share the information I had learned as I saw fit. So I booked a call with Nonlinear, sent them a long list of concerns, and talked with Emerson, Kat and Drew for ~3 hours to hear them out.</p><p> <strong>Paraphrasing Nonlinear</strong></p><p> On the call, they said their primary intention in the call was to convince me that Alice is a bald-faced liar. They further said they&#39;re terrified of Alice making false claims about them, and that she is in a powerful position to hurt them with false accusations.</p><p> Afterwards, I wrote up a paraphrase of their responses. I shared it with Emerson and he replied that it was a “Good summary!”. Below is the paraphrase of their perspective on things that I sent them, with one minor edit for privacy. (The below is written as though Nonlinear is speaking, but to be clear this 100% my writing.)</p><ul><li> We hired one person, and kind-of-technically-hired a second person. In doing so, our intention wasn&#39;t just to have employees, but also to have members of our family unit who we traveled with and worked closely together with in having a strong positive impact in the world, and were very personally close with.</li><li> We nomadically traveled the globe. This can be quite lonely so we put a lot of work into bringing people to us, often having visitors in our house who we supported with flights and accommodation. This probably wasn&#39;t perfect but in general we&#39;d describe the environment as &quot;quite actively social&quot;.</li><li> For the formal employee, she responded to a job ad, we interviewed her, and it all went the standard way. For the gradually-employed employee, we initially just invited her to travel with us and co-work, as she seemed like a successful entrepreneur and aligned in terms of our visions for improving the world. Over time she quit her existing job and we worked on projects together and were gradually bringing her into our organization.</li><li> We wanted to give these employees a pretty standard amount of compensation, but also mostly not worry about negotiating minor financial details as we traveled the world. So we covered basic rent/groceries/travel for these people. On top of that, to the formal employee we gave a $1k/month salary, and to the semi-formal employee we eventually did the same too. For the latter employee, we roughly paid her ~$8k over the time she worked with us.</li><li> From our perspective, the gradually-hired employees gave a falsely positive impression of their financial and professional situation, suggesting they&#39;d accomplished more than they had and were earning more than they had. They ended up being fairly financially dependent on us and we didn&#39;t expect that.</li><li> Eventually, after about 6-8 months each, both employees quit. Overall this experiment went poorly from our perspective and we&#39;re not going to try it in future.</li><li> For the formal employee, we&#39;re a bit unsure about why exactly she quit, even though we did do exit interviews with her. She said she didn&#39;t like a lot of the menial work (which is what we hired her for), but didn&#39;t say that money was the problem. We think it is probably related to everyone getting Covid and being kind of depressed around that time.</li><li> For the other employee, relations got bad for various reasons. She ended up wanting total control of the org she was incubating with us, rather than 95% control as we&#39;d discussed, but that wasn&#39;t on the table (the org had $250k dedicated to it that we&#39;d raised!), and so she quit.</li><li> When she was leaving, we were financially supportive. On the day we flew back from the Bahamas to London, we paid all our outstanding reimbursements (~$2900). We also offered to pay for her to have a room in London for a week as she got herself sorted out. We also offered her rooms with our friends if she promised not to tell them lies about us behind our backs.</li><li> After she left, we believe she told a lot of lies and inaccurate stories about us. For instance, two people we talked to had the impression that we either paid her $0 or $500, which is demonstrably false. Right now we&#39;re pretty actively concerned that she is telling lots of false stories in order to paint us in a negative light, because the relationship didn&#39;t work out and she didn&#39;t get control over her org (and because her general character seems drama-prone).</li></ul><p> There were some points around the experiences of these employees that we want to respond to.</p><ul><li> First; the formal employee drove without a license for 1-2 months in Puerto Rico. We taught her to drive, which she was excited about. You might think this is a substantial legal risk, but basically it isn&#39;t, as you can see <a href="https://casetext.com/statute/laws-of-puerto-rico/title-nine-highways-and-traffic/chapter-27-vehicle-and-traffic-law-2000/subchapter-ii-requirements-and-procedure-for-issuance-expiration-and-renewal-of-drivers-licenses/5073-illegal-use-of-the-driving-license-and-penalties"><u>here</u></a> , the general range of fines for issues around not-having-a-license in Puerto Rico is in the range of $25 to $500, which just isn&#39;t that bad.</li><li> Second; the semi-employee said that she wasn&#39;t supported in getting vegan food when she was sick with Covid, and this is why she stopped being vegan. This seems also straightforwardly inaccurate, we brought her potatoes, vegan burgers, and had vegan food in the house. We had been advising her to 80/20 being a vegan and this probably also weighed on her decision.</li><li> Third; the semi-employee was also asked to bring some productivity-related and recreational drugs over the border for us. In general we didn&#39;t push hard on this. For one, this is an activity she already did (with other drugs). For two, we thought it didn&#39;t need prescription in the country she was visiting, and when we found out otherwise, we dropped it. And for three, she used a bunch of our drugs herself, so it&#39;s not fair to say that this request was made entirely selfishly. I think this just seems like an extension of the sorts of actions she&#39;s generally open to.</li></ul><p> Finally, multiple people (beyond our two in-person employees) told Ben they felt frightened or freaked out by some of the business tactics in the stories Emerson told them. To give context and respond to that:</p><ul><li> I, Emerson, have had a lot of exceedingly harsh and cruel business experience, including getting tricked or stabbed-in-the-back. Nonetheless, I have often prevailed in these difficult situations, and learned a lot of hard lessons about how to act in the world.</li><li> The skills required to do so seem to me lacking in many of the earnest-but-naive EAs that I meet, and I would really like them to learn how to be strong in this way. As such, I often tell EAs these stories, selecting for the most cut-throat ones, and sometimes I try to play up the harshness of how you have to respond to the threats. I think of myself as playing the role of a wise old mentor who has had lots of experience, telling stories to the young adventurers, trying to toughen them up, somewhat similar to how Prof Quirrell <span class="footnote-reference" role="doc-noteref" id="fnref03ez0hl92kmc"><sup><a href="#fn03ez0hl92kmc">[8]</a></sup></span> toughens up the students in HPMOR through teaching them Defense Against the Dark Arts, to deal with real monsters in the world.</li><li> For instance, I tell people about my negotiations with Adorian Deck about the OMGFacts brand and Twitter account. We signed a good deal, but a California technicality meant he could pull from it and take my whole company, which is a really illegitimate claim. They wouldn&#39;t talk with me, so I was working with top YouTubers to make some videos publicizing and exposing his bad behavior. This got him back to the negotiation table and we worked out a deal where he got $10k/month for seven years, which is not a shabby deal, and meant that I got to keep my company!</li><li> It had been reported to Ben that Emerson said he would be willing to go into legal gray areas in order to &quot;crush his enemies&quot; (if they were acting in very reprehensible and norm-violating ways). Emerson thinks this has got to be a misunderstanding, that he was talking about what other people might do to you, which is a crucial thing to discuss and model.</li></ul><p> (Here I cease pretending-to-be-Nonlinear and return to my own voice.)</p><h2> My thoughts on the ethics and my takeaways</h2><p> <strong>Summary of My Epistemic State</strong></p><p> Here are my probabilities for a few high-level claims relating to Alice and Chloe&#39;s experiences working at Nonlinear.</p><ul><li> Emerson Spartz employs more vicious and adversarial tactics in conflicts than 99% of the people active in the EA/x-risk/AI Safety communities: 95%</li><li> Alice and Chloe were more dependent on their bosses (combining financial, social, and legally) than employees are at literally every other organization I am aware of in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefpwa0kj9axw"><sup><a href="#fnpwa0kj9axw">[9]</a></sup></span></li><li> In working at Nonlinear Alice and Chloe were both took on physical and legal risks that they strongly regretted, were hurt emotionally, came away financially worse off, gained ~no professional advancement from their time at Nonlinear, and took several months after the experience to recover: 90%</li><li> Alice and Chloe both had credible reason to be very scared of retaliation for sharing negative information about their work experiences, far beyond that experienced at any other org in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefyl41dasvaar"><sup><a href="#fnyl41dasvaar">[10]</a></sup></span></li></ul><p> <strong>General Comments From Me</strong></p><p> Going forward I think anyone who works with Kat Woods, Emerson Spartz, or Drew Spartz, should sign legal employment contracts, and make sure all financial agreements are written down in emails and messages that the employee has possession of. I think all people considering employment by the above people at any non-profits they run should take salaries where money is wired to their bank accounts, and not do unpaid work or work that is compensated by ways that don&#39;t primarily include a salary being wired to their bank accounts.</p><p> I expect that if Nonlinear does more hiring in the EA ecosystem it is more-likely-than-not to chew up and spit out other bright-eyed young EAs who want to do good in the world. I relatedly think that the EA ecosystem doesn&#39;t have reliable defenses against such predators. These are not the first, nor sadly the last, bright-eyed well-intentioned people who I expect to be taken advantage of and hurt in the EA/x-risk/AI safety ecosystem, as a result of falsely trusting high-status people at EA events to be people who will treat them honorably.</p><p> (Personal aside: Regarding the texts from Kat Woods shown above — I have to say, if you want to be allies with me, you must not write texts like these. A lot of bad behavior can be learned from, fixed, and forgiven, but if you take actions to prevent me from being able to learn that the bad behavior is even going on, then I have to always be worried that something far worse is happening that I&#39;m not aware of, and indeed I have been <i>quite</i> shocked to discover how bad people&#39;s experiences were working for Nonlinear.)</p><p> My position is not greatly changed by the fact that Nonlinear is overwhelmingly confident that Alice is a “bald-faced liar”. From my current perspective, they probably have some legitimate grievances against her, but that in no way makes it less costly to our collective epistemology to incentivize her to not share her own substantial grievances. I think the magnitude of the costs they imposed on their employees-slash-new-family are far higher than I or anyone I know would have expected was happening, and they intimidated both Alice and Chloe into silence about those costs. If it were only Alice then I would give this perspective a lot more thought/weight, but Chloe reports a lot of the same dynamics and similar harms.</p><p> To my eyes, the people involved were genuinely concerned about retaliation for saying anything negative about Nonlinear, including the workplace/household dynamics and how painful their experiences had been for them. That&#39;s a red line in my book, and I will not personally work with Nonlinear in the future because of it, and I recommend their exclusion from any professional communities that wish to keep up the standard of people not being silenced about extremely negative work experiences. “ <a href="https://twitter.com/HiFromMichaelV/status/1161174071469641728"><u>First they came for the epistemology. We don&#39;t know what happened after that.</u></a> ”</p><p> Specifically, the things that cross my personal lines for working with someone or viewing them as an ally:</p><ul><li> Kat Woods attempted to offer someone who was really hurting, and in a position of strong need, very basic resources with the requirement of not saying bad things about her.</li><li> Kat Woods&#39; texts that read to me as a veiled threat to destroy someone&#39;s career for sharing negative information about her.</li><li> Emerson Spartz reportedly telling multiple people he will use questionably legal methods in order to crush his enemies (such as spurious lawsuits and that he would hire a stalker to freak someone out).</li><li> Both employees were actively afraid that Emerson Spartz would retaliate and potentially using tactics like spurious lawsuits and further things that are questionably legal, and generally try to destroy their careers and leave them with no resources. It seems to me (given the other reports I&#39;ve heard from visitors) that Emerson behaved in a way that quite understandably led them to this epistemic state, and I consider that to be his responsibility to not give his employees this impression.</li></ul><p> I think in almost any functioning professional ecosystem, there should be some general principles like:</p><ul><li> If you employ someone, after they work for you, unless they&#39;ve done something egregiously wrong or unethical, they should be comfortable continuing to work and participate in this professional ecosystem.</li><li> If you employ someone, after they work for you, they should feel comfortable talking openly about their experience working with you to others in this professional ecosystem.</li></ul><p> Any breaking of the first rule is very costly, and any breaking of the second rule is by-default a red-line for me not being willing to work with you.</p><p> I do think that there was a nearby world where Alice, having run out of money, gave in and stayed at Nonlinear, begging them for money, and becoming a fully dependent and subservient house pet — a world where we would not have learned the majority of this information. I think we&#39;re not that far from that world, I think a weaker person than Alice might have never quit, and it showed a lot of strength to quit at the point where you have ~no runway left and you have heard the above stories about the kinds of things Emerson Spartz considers doing to former business partners that he is angry with.</p><p> I&#39;m very grateful to the two staff members involved for coming forward and eventually spending dozens of hours clarifying and explaining their experiences to me and others who were interested. To compensate them for their courage, the time and effort spent to talk with me and explain their experiences at some length, and their permission to allow me to publish a lot of this information, I (using Lightcone funds) am going to pay them each $5,000 after publishing this post.</p><p> I think that whistleblowing is generally a difficult experience, with a lot riding on the fairly personal account from fallible human beings. It&#39;s neither the case that everything reported should be accepted without question, nor that if some aspect is learned to be exaggerated or misreported that the whole case should be thrown out. I plan to reply to further questions here in the comments, I also encourage everyone involved to comment insofar as they wish to answer questions or give their own perspective on what happened. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8a69v0tq2qo"> <span class="footnote-back-link"><sup><strong><a href="#fnref8a69v0tq2qo">^</a></strong></sup></span><div class="footnote-content"><p> In a later conversation, Kat clarified that the actual amount discussed was $70k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fno53culramn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo53culramn">^</a></strong></sup></span><div class="footnote-content"><p> Comment from Chloe:</p><blockquote><p> In my resignation conversation with Kat, I was worried about getting into a negotiation conversation where I wouldn&#39;t have strong enough reasons to leave. To avoid this, I started off by saying that my decision to quit is final, and not an ultimatum that warrants negotiation of what would make me want to stay. I did offer to elaborate on the reasons for why I was leaving. As I was explaining my reasons, she still insisted on offering me solutions to things I would say I wanted, to see if that would make me change my mind anyway.  One of the reasons I listed was the lack of financial freedom in not having my salary be paid out as a salary which I could allocate towards decisions like choices in accommodation for myself, as well as meals and travel decisions. She wanted to know how much I wanted to be paid. I kept evading the question since it seemed to tackle the wrong part of the problem. Eventually I quoted back the number I had heard her reference to when she&#39;d talk about what my salary is equivalent to, suggesting that if they&#39;d pay out the 75k as a salary instead of the compensation package, then that would in theory solve the salary issue. There was a miscommunication around her believing that I wanted that to be paid out on top of the living expenses - I wanted financial freedom and a legal salary. I believe the miscommunication stems from me mentioning that salaries are more expensive for employers to pay out as they also have to pay tax on the salaries, eg social benefits, pension (depending on the country). Kat was surprised to hear that and understood it as me wanting a 75k salary <i>before taxes</i> . I do not remember that conversation concluding with her thinking I wanted everything paid for <i>and also</i> 75k.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnylc0clu1ffa"> <span class="footnote-back-link"><sup><strong><a href="#fnrefylc0clu1ffa">^</a></strong></sup></span><div class="footnote-content"><p> Note that Nonlinear and Alice gave conflicting reports about which month she started getting paid, February vs April. It was hard for me to check as it&#39;s not legally recorded and there&#39;s lots of bits of monetary payments unclearly coded between them.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2ad0whew08p"> <span class="footnote-back-link"><sup><strong><a href="#fnref2ad0whew08p">^</a></strong></sup></span><div class="footnote-content"><p> Comment from one of the employees:</p><blockquote><p> I had largely moved on from the subject and left the past behind when Ben started researching it to write a piece with his thoughts on it. I was very reluctant at first (and frightened at the mere thought), and frankly, will probably continue to be. I did not agree to post this publicly with any kind of malice, rest assured. The guiding thought here is, as Ben asked, &quot;What would you tell your friend if they wanted to start working for this organization?&quot; I would want my friend to be able to make their own independent decision, having read about my experience and the experiences of others who have worked there. My main goal is to create a world where we can all work together towards a safe, long and prosperous future, and anything that takes away from that (like conflict and drama) is bad and I have generally avoided it. Even when I was working at Nonlinear, I remember saying several times that I just wanted to work on what was important and didn&#39;t want to get involved in their interpersonal drama. But it&#39;s hard for me to imagine a future where situations like that are just overlooked and other people get hurt when it could have been stopped or flagged before. I want to live in a world where everyone is safe and cared for. For most of my life I have avoided learning about anything to do with manipulation, power frameworks and even personality disorders. By avoiding them, I also missed the opportunity to protect myself and others from dangerous situations. Knowledge is the best defense against any kind of manipulation or abuse, so I strongly recommend informing yourself about it, and advising others to do so too.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnt84or63yyei"> <span class="footnote-back-link"><sup><strong><a href="#fnreft84or63yyei">^</a></strong></sup></span><div class="footnote-content"><p> This is something Alice showed me was written in her notes from the time.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzcp7frumok"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzcp7frumok">^</a></strong></sup></span><div class="footnote-content"><p> I do not mean to make a claim here about who was in the right in that conflict. And somewhat in Emerson&#39;s defense, I think some of people&#39;s most aggressive behavior comes out when they themselves have just been wronged — I expect this is more extreme behavior than he would typically respond with. Nonetheless, it seems to me that there was reportedly a close, mentoring relationship — Emerson&#39;s <a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>tumblr post</u></a> on the situation says “I loved Adorian Deck” in the opening paragraph — but that later Emerson reportedly became bitter and nasty in order to win the conflict, involving threatening to overwhelm someone with lawsuits and legal costs, and figure out the best way to use their formerly close relationship to hurt them emotionally, and reportedly gave this as an example of good business strategy. I think this sort of story somewhat justifiably left people working closely with Emerson very worried about the sort of retaliation he might carry out if they were ever in a conflict, or he were to ever view them as an &#39;enemy&#39;.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuvloifuhbna"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuvloifuhbna">^</a></strong></sup></span><div class="footnote-content"><p> After this, there were further reports of claims of Kat professing her romantic love for Alice, and also precisely opposite reports of Alice professing her romantic love for Kat. I am pretty confused about what happened.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn03ez0hl92kmc"> <span class="footnote-back-link"><sup><strong><a href="#fnref03ez0hl92kmc">^</a></strong></sup></span><div class="footnote-content"><p> Note that during our conversation, Emerson brought up HPMOR and the Quirrell similarity, not me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpwa0kj9axw"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpwa0kj9axw">^</a></strong></sup></span><div class="footnote-content"><p> With the exception of some FTX staff.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyl41dasvaar"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyl41dasvaar">^</a></strong></sup></span><div class="footnote-content"><p> One of the factors lowering my number here is that I&#39;m not quite sure what the dynamics are like at places like Anthropic and OpenAI — who have employees sign non-disparagement clauses, and are involved in geopolitics — or whether they would even be included. I also could imagine finding out that various senior people at CEA/EV are terrified of information coming out about them. Also note that I am not including Leverage Research in this assessment.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1<guid ispermalink="false"> Lc8r4tZ2L5txxokZ8</guid><dc:creator><![CDATA[Ben Pace]]></dc:creator><pubDate> Thu, 07 Sep 2023 06:51:11 GMT</pubDate></item></channel></rss>