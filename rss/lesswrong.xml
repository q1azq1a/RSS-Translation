<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 17 日星期四 22:10:19 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[How is ChatGPT's behavior changing over time?]]></title><description><![CDATA[Published on August 17, 2023 8:54 PM GMT<br/><br/><p>很惊讶我在 lesswrong 上找不到这个，所以我想添加它。随着时间的推移，法学硕士的行为似乎会产生一些一致性影响，至少获得更多的背景信息。<br><br>与我交谈过的其他人立即对它进行了贬低，因为某种实验错误使论文的结论变得相当无效，但我并没有真正看到这一点。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-change-over-time<guid ispermalink="false"> 9nooX9djbM5bXGKNn</guid><dc:creator><![CDATA[Phib]]></dc:creator><pubDate> Thu, 17 Aug 2023 21:32:42 GMT</pubDate> </item><item><title><![CDATA[Progress links digest, 2023-08-17: Cloud seeding, robotic sculptors, and rogue planets]]></title><description><![CDATA[Published on August 17, 2023 8:29 PM GMT<br/><br/><h2><strong>机会</strong></h2><ul><li><a href="http://apply.nucleate.xyz/">学术生物技术创始人：申请 Nucleate 的 Activator 计划</a>（来自<a href="https://twitter.com/kulesatony/status/1689316164764065793">@kulesatony</a> ）</li></ul><h2><strong>新闻与公告</strong></h2><ul><li><a href="https://twitter.com/neuralink/status/1688582504196739072">Neuralink 筹集 2.8 亿美元 D 轮融资，由 Founders Fund 领投</a></li><li><a href="https://waymo.com/blog/2023/08/waymos-next-chapter-in-san-francisco.html">Waymo</a>和<a href="https://twitter.com/kvogt/status/1689814193875374080">Cruise</a>已获准在旧金山运营机器人出租车</li><li><a href="https://twitter.com/ADoricko/status/1688740627855589376">造雨者发射旨在“结束全球水资源短缺和地球改造”</a></li></ul><h2><strong>播客</strong></h2><ul><li><a href="https://www.dwarkeshpatel.com/p/dario-amodei#details">Dwarkesh Patel 采访了 Anthropic 首席执行官 Dario Amodei</a> （来自<a href="https://twitter.com/dwarkesh_sp/status/1688916080700555264">@dwarkesh_sp</a> ）。 “达里奥很搞笑，对这些模型正在做什么、为什么它们的扩展性如此之好以及如何调整它们有令人着迷的看法”</li><li><a href="https://conversationswithtyler.com/episodes/paul-graham/">泰勒·考恩采访保罗·格雷厄姆</a>（来自<a href="https://twitter.com/tylercowen/status/1689260912182542337">@tylercowen</a> ）</li></ul><h2><strong>链接</strong></h2><ul><li><a href="https://www.monumentallabs.co/">Monumental Labs 正在建造“人工智能机器人石雕工厂”，</a>以“以极低的成本打造具有佛罗伦萨、巴黎或纽约美术学院般辉煌的城市”。这是<a href="https://twitter.com/mspringut/status/1682126571392360448">一个演示</a>（来自<a href="https://twitter.com/devonzuegel/status/1689438847220813824">@devonzuegel</a> ）。他们正在<a href="https://wellfound.com/company/monumental-labs-2/jobs/2578817-stone-carver-full-and-part-time-roles">雇用石雕师</a>（仍然需要他们进行精细的细节和精加工）</li><li><a href="https://www.readcodon.com/p/synbio-guide">合成生物学密码子指南</a>（作者： <a href="https://twitter.com/NikoMcCarty/status/1689625144132820992">@NikoMcCarty</a> ）</li><li><a href="https://exformation.williamrinehart.com/p/silicon-innovation-is-colliding-with">人工智能许可证 Raj：人工智能在采用方面面临着严重的官僚障碍</a>（ <a href="https://twitter.com/WillRinehart/status/1688524865236619264">@WillRinehart</a> ）</li><li><a href="https://earthsky.org/space/rogue-planets-exoplanets-nancy-grace-roman-space-telescope/">我不知道有这么多流氓行星</a></li><li><a href="https://goodscience.substack.com/p/metascience-since-2012-a-personal">“我作为元科学风险投资家的个人历史”</a> （作者： <a href="https://twitter.com/stuartbuck1/status/1690508406942011392">@stuartbuck1</a> ）</li><li> <a href="https://beta.reddit.com/r/interestingasfuck/comments/15gzvhb/a_slice_of_the_a303_in_hampshire_england">英国汉普郡的A303穿过巨石阵，古罗马福斯路的一部分包含在其中</a>（来自<a href="https://twitter.com/Rainmaker1973/status/1690016575917580290">@Rainmaker1973</a> ）</li></ul><p> <a href="https://pbs.twimg.com/media/F3QkgxTWMAEE9Xp?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zmek5t33153yoxbyv15h" alt=""></a></p><h2><strong>社交媒体</strong></h2><ul><li><a href="https://twitter.com/paulg/status/1689872015535300608">在当前的 YC 批次中，“各种不同领域的大量领域专家已经找到了使用 AI 来解决其领域中的人们早已知道但不太能够解决的问题的方法”</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689320109532225551">如果您对 LK-99 过于兴奋，那么值得重新调整您的先验知识。但如果你正确预测没有RTS，那就真的没有必要幸灾乐祸了</a>。相关： <a href="https://twitter.com/MaximZiatdinov/status/1689295189045661696">LK99 证明科学界完全有能力通过 arXiv 和社交媒体工具进行同行评审</a></li><li><a href="https://twitter.com/AlecStapp/status/1688538038341931008">生物安全政策中一些最容易实现的成果：在监测废水的预警系统上投入更多资源</a></li><li><a href="https://twitter.com/AlecStapp/status/1689625548195241987">19 世纪初，纽约、费城和波士顿的居住密度高达每平方英里 75,000 人</a></li><li><a href="https://twitter.com/owasow/status/1690549009147174912">“NuScale 于 2008 年开始致力于获得监管批准。2020 年，当其反应堆获得设计批准时，该公司表示，监管过程花费了 5 亿美元，并且已向该公司提供了约 200 万页的支持文件。国家研究委员会。”</a>相关的是，我关于<a href="https://rootsofprogress.org/devanney-on-the-nuclear-flop">核电为何失败的</a>书评</li><li><a href="https://twitter.com/eric_is_weird/status/1688935852364193792">按时间顺序学习数学</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689379507487002624">为你的想法不懈地、不知疲倦地宣传意味着什么</a></li><li><a href="https://twitter.com/jasoncrawford/status/1689036685349212160">在线平台无法让您在受众较少的情况下取得成功；他们可以让你在没有把关人的情况下培养观众</a></li><li><a href="https://twitter.com/mbateman/status/1688378460006240256">也许“平衡”是一个不快乐的人对幸福生活的看法。快乐的人看起来不平衡也没什么问题</a></li><li><a href="https://twitter.com/NobelPrize/status/1688129810915045376">亚历山大·弗莱明最初将青霉素称为“霉菌汁”</a></li></ul><p> <a href="https://pbs.twimg.com/media/F21xGYbWEAANvN5?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/fmmgoeh6knycp6cuiruk" alt=""></a></p><h2><strong>引号</strong></h2><p>尝试一种新的格式，我将完整的引号内联。 （强调。）链接到社交媒体，以便您可以轻松分享。让我知道你的想法：</p><p><a href="https://twitter.com/michael_nielsen/status/1689371336517541888">思想史的发明</a>（彼得·沃森， <a href="https://www.amazon.com/Ideas-History-Thought-Invention-Freud-ebook/dp/B000FCKC5G"><i>《思想：思想与发明史，从火到弗洛伊德</i></a>》）</p><blockquote><p>第一个构思思想史的人也许是弗朗西斯·培根（Francis Bacon，1561-1626）。他当然认为历史最有趣的形式是思想史，如果不考虑任何时代的主导思想，“历史是盲目的”。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690897070847049729">为什么汽车比马更好——来自经历过这一转变的人</a>（大卫·麦卡洛，<a href="https://www.amazon.com/Wright-Brothers-David-McCullough-ebook/dp/B00LD1RWP6"><i>莱特兄弟</i></a>）</p><blockquote><p>阿莫斯·鲁特充满热情，始终渴望“看到车轮转动”。他喜欢钟表、风车、自行车和各种机器，尤其是他的奥兹莫比尔小跑。在任何季节里，他很少比在路上行驶时更快乐。 “虽然我在某种程度上喜欢马（他写道），但我不喜欢照顾它们。我不喜欢马厩的气味。我不喜欢每天早上必须清洁马，也不喜欢在冬天拴马。 ……<strong>套马需要时间；但汽车立即准备好启动。它永远不会疲倦；它比任何马都能更快地到达那里。</strong> ” 至于奥兹莫比尔，他喜欢说，350 美元的价格比一匹马和马车还便宜。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690900703110152192">直到 18 世纪，就连国王和皇帝也饱受糟糕的路况之苦</a>（理查德·布利特， <a href="https://rootsofprogress.org/books/the-wheel"><i>《车轮</i></a>》）</p><blockquote><p>直到十九世纪中叶新的道路建设实验开始取得成果之前，马车车轮下方的表面仍然布满车辙、泥泞不堪，而且路面状况不佳——如果有路面的话。在农村尤其如此，但即使在大城市也存在糟糕的道路。例如，1703年，在从伦敦向南前往五十英里外的佩特沃斯的途中，<strong>哈布斯堡王朝皇帝查理六世乘坐的马车在路上翻车了十二次。</strong>半个世纪后，麦尔安德路（Mile End Road）是从阿尔德门（Aldgate）入口向东延伸的主干道，被描述为“从怀特查佩尔（Whitechapel）到斯特拉特福（Stratford）的一处深泥淤泥湖”，全长四英里。</p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690906769365540864">米塞斯反对稳定</a>（丹尼尔·斯特德曼·琼斯，<a href="https://rootsofprogress.org/books/masters-of-the-universe"><i>宇宙大师</i></a>）</p><blockquote><p>和波普尔一样，米塞斯也看到了官僚心态与柏拉图的乌托邦之间的相似之处，在柏拉图的乌托邦中，绝大多数被统治者为统治者服务。他认为“后来所有按照柏拉图的例子塑造人间天堂蓝图的乌托邦主义者都同样相信人类事务的不变性”。他接着说，官僚化必然是僵化的，因为它涉及对既定规则和惯例的遵守。但在社会生活中，僵化就等于石化和死亡。一个非常重要的事实是，稳定和安全是当今“改革者”最珍视的口号。<strong>如果原始人采取了稳定的原则，他们早就被猛兽和微生物消灭了。</strong></p></blockquote><p><a href="https://twitter.com/jasoncrawford/status/1690906098432098305">尝试改变喷发火山熔岩流的方向是什么感觉</a>（<a href="https://en.wikipedia.org/wiki/Eldfell">埃尔德费尔</a>，冰岛，1973）（约翰·麦克菲， <a href="https://rootsofprogress.org/books/the-control-of-nature"><i>《自然的控制</i></a>》）</p><blockquote><p>喷发期间，当抽水人员第一次尝试登上熔岩时，他们发现薄至两英寸的地壳足以支撑一个人，并且还可以隔热——只有几英寸厚的硬岩像融化的英寻上的池塘冰。当工作人员拖拉软管、喷嘴三脚架和管道段时，他们了解到最好不要静止不动。他们常常原地踏步。<strong>即便如此，他们的靴子有时也会起火。</strong></p></blockquote><h2><strong>图表</strong></h2><p><a href="https://tfp.elidourado.com/">美国经利用率调整后的全要素生产率已连续三个季度下降，目前低于疫情爆发前 2019 年第四季度的水平</a>。这很糟糕（来自<a href="https://twitter.com/elidourado/status/1688564129869582336">@elidourado</a> ） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/dhlws1xp4vvwcvtjjmpy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/br2jkdpyxjptx9z1r5w3 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/c7s2qfk0ijt39a85qxzl 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/zbcc2hgnarzioxa7apkz 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/lb91b5i8kgnagpoe54sm 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cmdprkjnjsvlmp6zwklr 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/wdjmpx5174v0hbv56i8t 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cy7mpdvnxwrg0gfv0qyz 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/cjflc2wviegf2ws5krzk 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/gwqe4jiqeewfapmekbes 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CJNbF9hM8PtxkrHHT/ksdykld4bmcurbgo2isq 1029w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CJNbF9hM8PtxkrHHT/progress-links-digest-2023-08-17-cloud-seeding-robotic<guid ispermalink="false"> CJNbF9hM8PtxkrHHT</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Thu, 17 Aug 2023 20:29:28 GMT</pubDate> </item><item><title><![CDATA[Model of psychosis, take 2]]></title><description><![CDATA[Published on August 17, 2023 7:11 PM GMT<br/><br/><p> <i>（我无论如何都不是精神病方面的专家。这更像是“在博客中实时记录我的想法”。我希望激发讨论并获得反馈和指示。）</i></p><h1>一、简介</h1><p>去年 2 月，我在博客文章<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症是长程皮层间交流的缺陷”第 4.2 节</u></a>中提出了一种精神病模型。但它有一些问题。我终于抽出时间再看一下，我想我找到了解决这些问题的简单方法。所以这篇文章是更新版本。</p><p><strong>对于tl;dr，您可以跳过正文，只看下面的两张图</strong>。</p><h1> 2. 背景：我之前的“精神病模型，取 1”</h1><p>以下是我在<a href="https://www.lesswrong.com/posts/H2epKysvFgPcTwC2f/schizophrenia-as-a-deficiency-in-long-range-cortex-to-cortex"><u>“精神分裂症作为长程皮层间通讯缺陷”第 4.2 节</u></a>中提出的建议： </p><figure class="image image_resized" style="width:84.91%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/wktqaurpaalkjoggdiuu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/oghtomiodqbanmrqmja2 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pct3wxlentqcpdlpmosm 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qj0t6vaozbu0iccn7fby 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jn71t5qau9ikibfefhce 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/l2blxhhkorj6am4fubje 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fhihkdcjhqoqstwo2rs4 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/d4hg2hc6pz7d0xa7vmvt 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/sjd7xw4brb3sxxmvmh37 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/o1emb3bcgxegzsyomhkt 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jzl0b6rs9iswligmrcnc 1318w"></figure><p>这个想法是，在精神病中，绿色箭头是活跃且有效的，但红色箭头不是，因此紫色箭头也不是。结果可能是一种手臂被外力移动的感觉。这只是一个例子，对于左侧皮质输出的不同类型，相应的精神病的第一人称体验会有所不同。但我声称所有精神病症状都大致符合这个模板。</p><h1> 3. 第一个模型似乎缺少重要的三个方面</h1><ul><li>据我了解，精神分裂症患者的精神病可能会反复出现，而（我相信）皮层间沟通的缺陷是精神分裂症患者大脑的特征，并且是永久性的（未来医疗技术没有进步）。</li><li>抗精神病药物可以减少精神病，但上图无法解释这一点。</li><li>除精神分裂症外，精神病还可能发生在其他疾病中。我认为最常见的例子是躁郁症的躁狂期。上图无法解释这一点。</li></ul><h1> 4. 我的“精神病模型，取2”</h1><p> <i>（该图顶部的唯一变化是左侧新的绿色文本，上面写着“信号强度= <strong>B</strong> ”。）</i> </p><figure class="image image_resized" style="width:93.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qnmby6hoqa5bfwlqeayu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/jjrb8mawu2ujeqw7c5ot 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/ehibbu9bgqqowkwjldir 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/cogtxpqp9ex2o3ip54fa 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/rdirptmkvrozlxi5d9ny 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/qzpurvyri4vy823pk7po 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pyoqgkrhdtuwzyhq0ixa 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/pjjscqiimpaceb2q13wl 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/fwjfxfswenkvooxyxutk 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/kjnkjrftfoufriobw1pl 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tgaD4YnpGBhGGbAy5/uahekggt8l0bsvgkqw6r 1485w"></figure><p>可以将皮层的一部分视为具有可调节的“音量”，即它宣布当前正在做什么的强烈程度和清晰程度。例如，如果您想到<i>也许</i>您<i>可以</i>移动手指，那么您可能会发现（如果您仔细观察和/或使用科学设备）您的手指有点抽搐，而如果您强烈想要移动手指，然后你的手指就会真正移动。</p><p>无论如何，如果我们逐渐增加一部分皮层的“体积”，那么在某个时间点， <strong>A</strong>消息将开始通过并产生影响，同时在<i>其他</i>某个时间点， <strong>B</strong>消息将开始通过并产生影响。有效果。为了避免精神病，我们希望前者<i>首先</i>发生， <i>&nbsp;</i>以及<i>后者</i>，这样就不可能出现<strong>B</strong>消息正在传输但<strong>A</strong>消息没有传输的“音量级别”。</p><h1> 5.这个新模型的优点</h1><h2>5.1 <strong>B/A</strong>比率的缓慢变化至少在<i>先验上</i>是合理的，因为<strong>B</strong>和<strong>A</strong>来自不同皮质层的不同神经元（分别为第 5 层和第 2/3 层）</h2><p>我认为<strong>B/A</strong>比率是一个可以变化的参数，这是非常合理的，因为：</p><ul><li>信号<strong>B</strong>仅由皮层<strong>第 5 层</strong>的一部分神经元发送</li><li>信号<strong>A</strong>至少部分（也许大部分？）由皮层<strong>第 2/3 层</strong>的神经元子集发送</li><li>一般来说，不同的皮质层有不同类型的神经元，具有不同的输入、与多巴胺系统的不同关系等。</li></ul><p>因此，涉及<strong>B/A</strong>比率长期变化的理论至少是<i>合理的</i>。</p><h2> 5.2 至少一篇论文似乎表明抗精神病药比第 2/3 层信号更能抑制第 5 层信号</h2><p>参见<a href="https://elifesciences.org/reviewed-preprints/86805"><u>Heindorf &amp; Keller 2023</u></a> 。他们发现“氯氮平……降低了[第 2/3 层]兴奋性神经元的皮质活动相关性。然而，这种减少明显弱于我们在……[第 5 层端脑内]神经元中观察到的减少”。 （此特定比较的 p 值为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 0.005"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.005</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>对于短程相关性， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 10^{-8}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span></span></span></span></span>对于长程相关性）。</p><p>如果我正确地理解了这篇论文（一个很大的“如果”！），这是有启发性和令人鼓舞的，但并不能<i>明确</i>支持我的理论，因为首先，该论文测量了错误类型的第 5 层锥体神经元。发送信号<strong>B 的</strong>神经元，其次，作者测量的这种相关性<i>可能</i>是由通常信号较弱的第 5 层神经元引起的（这样空间相关性很快就会低于本底噪声），但还有其他原因也有可能的原因（与空间相关性<i>直接</i>相关——我认为这就是作者认为正在发生的事情）。</p><h1> 6. 我仍然不确定的事情</h1><h2>6.1 哪些 D2 受体解释抗精神病药物如何发挥作用？</h2><p>每种抗精神病药物的共同点是阻断多巴胺 D2 受体。所以想必这就是他们的工作方式。但整个大脑的神经元中都有 D2 受体。据推测，这些带有 D2 受体的神经元的一部分是抗精神病药物发挥作用的秘密，其余的则仅与副作用有关。哪个是哪个？</p><p>当我尝试充实我的模型时，迄今为止我想到的最简单、最优雅的故事涉及<i>皮质中</i>扮演主角的 D2 受体。特别是，不同的皮质层有不同的 D2 受体密度，如果我没记错的话，抗精神病药物降低<strong>B/A</strong>比率的迹象似乎是正确的。</p><p>但这很有趣，因为我认为几乎其他人似乎都认为<i>纹状体中的</i>D2 受体是抗精神病药物的作用机制？我试图弄清楚为什么人们似乎相信这一点，但无法弄清楚。我能找到的所有关于抗精神病药通过纹状体起作用的证据都相当薄弱和间接。如果您对此有所了解，请评论。</p><h2> 6.2 精神病的其他原因呢？</h2><p>由于各种原因，我脑子里有一个模糊的经验法则，那就是，在其他条件相同的情况下，更多的多巴胺往往会增加<strong>B/A</strong>比率（因此，超过某个阈值，会导致精神病）。这似乎很好地解释了与躁狂相关的精神病（我将躁狂与“大量多巴胺”松散地联系起来，至少在某些渠道和各种警告中），以及精神病是左旋多巴治疗帕金森病的副作用这一事实。</p><p>我对精神病性抑郁症更困惑。 （在双相情感障碍中，我知道精神病在躁狂症中比抑郁症更常见，但它<i>也可能</i>发生在抑郁症中。）我通常认为抑郁症是躁狂症的“相反”，并且涉及异常少<i>的</i>多巴胺，同样是在某些渠道和与各种警告。所以我对精神病性抑郁症是否会发生感到有点困惑。我不知道。无论如何，多巴胺系统只是影响<strong>B/A</strong>比率的众多因素之一。或者上图右侧的紫色信号可能没有通过？或者也许这是一个完全不同的故事。</p><br/><br/><a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2<guid ispermalink="false"> tgaD4YnpGBhGGbAy5</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:11:17 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Robustified ANNs Reveal Wormholes Between Human Category Percepts]]></title><description><![CDATA[Published on August 17, 2023 7:10 PM GMT<br/><br/><p>这是<a href="https://arxiv.org/abs/2308.06887"><i>https://arxiv.org/abs/2308.06887</i></a><i>的链接帖子</i><i>。</i></p><blockquote><p>众所周知，人工神经网络（ANN）的视觉对象类别报告对微小的对抗性图像扰动非常敏感。因为人类类别报告（又名人类感知）被认为对那些相同的小范数扰动不敏感，而且总体上局部稳定，这表明人工神经网络是人类视觉感知的不完整科学模型。与此一致的是，我们表明，当标准 ANN 模型生成小范数图像扰动时，人类对象类别感知确实高度稳定。然而，在这个完全相同的“人类假定稳定”体系中，我们发现稳健的人工神经网络可靠地发现了强烈扰乱人类感知的低范图像扰动。这些以前无法检测到的人类感知干扰的幅度非常大，接近于稳健的人工神经网络中可见的相同敏感度水平。此外，我们表明，稳健的人工神经网络支持精确的感知状态干预：它们指导低范数图像扰动的构建，这些扰动强烈改变人类类别感知到特定规定的感知。这些观察结果表明，对于图像空间中的任意起点，存在一组附近的“虫洞”，每个虫洞都会引导主体从当前类别感知状态进入语义上非常不同的状态。此外，当代生物视觉处理的人工神经网络模型现在足够准确，可以始终如一地引导我们到达这些门户。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes-between-human#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/t5CXh9LwcKZqwNpTk/linkpost-robustified-anns-reveal-wormholes- Between- human<guid ispermalink="false"> t5CXh9LwcKZqwNpTk</guid><dc:creator><![CDATA[Bogdan Ionut Cirstea]]></dc:creator><pubDate> Thu, 17 Aug 2023 19:10:40 GMT</pubDate> </item><item><title><![CDATA[Against Almost Every Theory of Impact of Interpretability]]></title><description><![CDATA[Published on August 17, 2023 6:44 PM GMT<br/><br/><p><i>认知状态：我相信我精通这个主题。我的错误在于提出了过于强烈的主张，允许读者提出不同意见并就精确的观点展开讨论，而不是试图对每一个陈述进行边缘化。我还认为使用模因很重要，因为安全想法很无聊且</i><a href="https://www.lesswrong.com/posts/zk6RK3xFaDeJHsoym/connor-leahy-on-dying-with-dignity-eleutherai-and-conjecture%23Eliezer_Has_Been_Conveying_Antimemes"><i><u>反模因</u></i></a><i>。那么我们走吧！</i></p><p><i>非常感谢</i><a href="https://www.lesswrong.com/users/scasper?mention=user"><i>@scasper</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/sid-black?mention=user"><i>@Sid Black</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/neel-nanda-1?mention=user"><i>Neel Nanda</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/fabien-roger?mention=user"><i>Fabien Roger</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/bogdan-ionut-cirstea?mention=user"><i>@Bogdan Ionut Cirstea</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/wcargo?mention=user"><i>@WCargo</i></a> <i>、@</i> <a href="https://www.lesswrong.com/users/alexandre-variengien?mention=user"><i>Alexandre Variengien</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/lelapin?mention=user"><i>@Jonathan Claybrough</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/edoardo-pona?mention=user"><i>@Edoardo Pona</i></a> <i>、</i> @ <a href="https://www.lesswrong.com/users/andream?mention=user"><i>Andrea_Miotti</i></a> <i>、Diego Dorn、Angélina Gentaz、Clement Dumas、和 Enzo Marsot 提供有用的反馈和讨论。</i></p><p>当我开始写这篇文章时，我首先批评 Neel Nanda 的文章<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>《可解释性影响的一长串理论》</u></a> ，但后来我扩大了批评的范围。提出的一些想法并没有得到任何人的支持，但为了解释其中的困难，我仍然需要1.解释它们和2.批评它们。它给这篇文章带来了一种敌对的氛围。对此我感到很抱歉，我认为对可解释性进行研究，即使它不再是我认为的优先事项，仍然是值得赞扬的。</p><p><strong>如何阅读这份文件？</strong>除了“可解释性的最终故事是什么样的？”部分之外，本文档的大部分内容都不是技术性的。一开始大部分可以跳过。我希望这份文档对于不进行可解释性研究的人也有用。不同的部分大多是独立的，我添加了很多书签来帮助模块化这篇文章。</p><p>如果你时间很少，就看一下（这也是我最有信心的部分）：</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>使用 Interp 审计欺骗是遥不可及的</u></a>（4 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>列举安全</u></a>评论（2 分钟）</li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>具有更好影响理论的技术议程</u></a>（1 分钟）</li></ul><p></p><p>以下是我将辩护的索赔清单：</p><p> （粗体部分是最重要的部分）</p><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#The_overall_Theory_of_Impact_is_quite_poor"><strong><u>整体影响理论相当差</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp 不能很好地预测未来系统</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><strong><u>使用 interp 审计欺骗是遥不可及的</u></strong></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><strong><u>可解释性的最终故事是什么样的？这一点都不清楚。</u></strong></a><ul><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><strong><u>枚举安全性？</u></strong></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>逆向工程？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Olah_s_interpretability_dream_"><u>奥拉的可解释性梦想？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Retargeting_the_search_"><u>重新定位搜索？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>轻松的对抗训练？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>显微镜人工智能？</u></a></li></ul></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><strong><u>针对欺骗的预防措施似乎更加可行</u></strong></a><ul><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency"><u>引导世界走向透明</u></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_Design"><u>认知模拟 - 可解释性设计</u></a></li></ul></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><strong><u>可解释性可能总体上是有害的</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Outside_view__The_proportion_of_junior_researchers_doing_interp_rather_than_other_technical_work_is_too_high"><strong><u>外界观点：初级研究员从事Interp而非其他技术工作的比例太高</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#So_far_my_best_ToI_for_interp__Nerd_Sniping_"><u>到目前为止，我最好的解释员指南：书呆子狙击？</u></a></li><li> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Even_if_we_completely_solve_interp__we_are_still_in_danger"><strong><u>即使我们完全解决了interp，我们仍然处于危险之中</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><strong><u>具有更好影响理论的技术议程</u></strong></a></li><li><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Conclusion"><strong><u>结论</u></strong></a></li></ul><p>注：本文的目的是批评类 GPT 模型等深度学习模型的可解释性影响理论（ToI），而不是小模型的可解释性和可解释性。</p><h1>皇帝没穿衣服？</h1><p>我演讲了不同的<a href="https://www.lesswrong.com/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review"><u>风险模型</u></a>，然后进行了可解释性演示，然后我得到了一个有问题的问题，“我不明白，这样做有什么意义？”哼。 </p><figure class="image image_resized" style="width:587.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zzxhts0u9ne414ahftxw" alt=""><figcaption>来自<a href="https://distill.pub/2017/feature-visualization/"><i><u>特征可视化的</u></i></a><i>图像</i><i>。</i></figcaption></figure><ul><li>功能即？ （左图）嗯，很漂亮，但是有用吗？ <span class="footnote-reference" role="doc-noteref" id="fnref2stfurwyyg6"><sup><a href="#fn2stfurwyyg6">[1]</a></sup></span> <a href="https://arxiv.org/abs/2010.12606%23:~:text%3Dversion%252C%2520v3)%255D-,Exemplary%2520Natural%2520Images%2520Explain%2520CNN%2520Activations%2520Better%2520than,of%252Dthe%252DArt%2520Feature%2520Visualization%26text%3DFeature%2520visualizations%2520such%2520as%2520synthetic,convolutional%2520neural%2520networks%2520(CNNs)."><u>这个</u></a><a href="https://arxiv.org/abs/2306.04719"><u>可靠</u></a>吗？</li><li> GradCam（一种像素归因技术，如上右图），它很漂亮。但我从未见过有人在工业中使用它。 <span class="footnote-reference" role="doc-noteref" id="fnref4qi9kn3ip89"><sup><a href="#fn4qi9kn3ip89">[2]</a></sup></span>像素归因似乎很有用，但准确性仍然是王道。 <span class="footnote-reference" role="doc-noteref" id="fnref6xxwjs20rd7"><sup><a href="#fn6xxwjs20rd7">[3]</a></sup></span></li><li>感应头？好吧，我们可能正在对<a href="https://en.wikipedia.org/wiki/Regular_expression"><u>法学硕士中的正则</u></a>表达式机制进行逆向工程。凉爽的。</li></ul><p>最后要点中的考虑是基于感觉，并不是真正的论据。此外，大多数机械解释现在甚至都不是为了有用。但在本文的其余部分，我们将了解原则上可解释性是否有用。那么我们就来调查一下解说帝到底是有隐形衣服还是根本没有衣服！ （参见<a href="https://en.wikipedia.org/wiki/The_Emperor%2527s_New_Clothes"><u>安徒生的《皇帝的新装</u></a>》）。</p><h1>整体影响理论相当差</h1><p>Neel Nanda 撰写了<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><strong><u>一长串可解释性影响理论</u></strong></a><strong>，</strong>其中列出了 20 种不同的影响理论。然而，我发现自己不同意其中的大多数理论。元层面的三大分歧是：</p><ul><li><strong>每当你想做一些具有可解释性的事情时，最好不要这样做。</strong>我怀疑 Redwood Research 已因此停止进行可解释性工作（请参阅此处的当前计划<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw"><u>EAG 2023 湾区当前的调整计划，以及我们如何改进它</u></a>）。<ul><li><strong>对于欺骗性对齐来说尤其如此</strong>，尽管它是可解释性研究的主要焦点。许多其他风险情景也值得考虑。 【 <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>欺骗</u></a>部分】</li></ul></li><li><strong>可解释性常常试图同时解决太多目标。</strong>请<a href="https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately"><u>分别购买 Fuzzies 和 Utilons</u></a> ：即同时优化多个目标是非常困难的！最好直接针对每个子目标分别进行优化，而不是将所有内容混合在一起。当我查看尼尔·南达（Neel Nanda）的<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>这份清单</u></a>时，我发现这个原则没有得到遵循。</li><li><strong>一般来说，可解释性可能是有害的。</strong>使用 interp 来保证安全无疑对功能来说是有用的。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>危害</u></a>]</li></ul><p>其他不太重要的分歧：</p><ul><li><strong>概念上的进步更为紧迫，</strong>而 interp 可能无助于推进这些讨论。 [章节<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>结束故事</u></a>]</li><li><strong>目前的可解释性主要用于事后分析</strong>，在事前或预测能力方面几乎没有什么用处[ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#id.vcvarqienhb7"><u>未来系统的部分预测器</u></a>]</li></ul><p>以下是我不同意的一些关键理论：</p><ul><li>影响理论<strong>2：“</strong><i><strong>更好地预测未来系统”</strong></i></li><li>影响理论<strong>4：“</strong><i><strong>欺诈审计”</strong></i></li></ul><p>在附录中，我批评了几乎所有其他影响理论。</p><h2> Interp 不能很好地预测未来系统</h2><p><i>影响理论 2：“<strong>更好地预测未来系统</strong>：可解释性可以使人们更好地机械地理解 ML 系统和工作原理，以及它们如何随规模变化，类似于科学定律。这使我们能够更好地从当前系统推断未来系统，类似于缩放定律。例如，观察感应头的相变向我们表明，模型可以在训练期间快速获得能力”，</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li>对<a href="https://transformer-circuits.pub/2021/framework/index.html"><strong><u>感应头</u></strong></a>示例<strong>的挑剔</strong><strong>。</strong>如果我们关注上面的例子“<i>模型在训练过程中可能会快速获得能力</i>”，我不认为是可解释性让我们发现了这一点，而是行为评估。在训练期间定期测量损失，并通过让模型复制一系列随机令牌<a href="https://transformer-circuits.pub/2021/framework/index.html"><u>来测量</u></a>归纳能力的快速增益。一开始，复制是行不通的，但经过一些训练后，就可以了。可解释性只是告诉我们，这与感应头的出现相吻合，但我不明白可解释性如何让我们“<i>更好地从当前系统推断未来系统”</i> 。此外，感应头首先被研究是因为它们很容易学习。</li><li><strong>可解释性主要是在现象发现后完成的，而不是事前完成的。</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefztj4j3pmerg"><sup><a href="#fnztj4j3pmerg">[4]</a></sup></span><ul><li>我们首先观察到了 grokking 现象，然后我们<i>才</i>开始对其进行一些<a href="https://arxiv.org/abs/2301.05217"><u>解释</u></a>。有没有反例？</li><li>在<a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do"><u>DALL-E 2 可以做什么和不能做什么</u></a>中，我们看到 DALL-E 2 无法正确拼写单词。两个月后， <a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do?commentId%3Dg6kZ3eRFejRjiyGiw"><u>Imagen</u></a>就能正确拼写这些单词了。我们甚至没有费心去解释。</li></ul></li><li><strong>有更好的方法来预测这些系统的未来功能。</strong>跳出框框思考，如果你真的想看看未来的系统会是什么样子，那么查看 NeurIPS 会议和 AutoGPT 等认知架构上发表的论文会容易得多。否则，订阅 DeepMind 的 RSS feed 也不失为一个好主意。</li></ul><h2>使用 interp 审计欺骗是遥不可及的</h2><p>审计欺骗通常是进行解释的主要动机。所以我们在这里：</p><p>影响理论 4：<i><strong>欺骗审计</strong>：与审计类似，我们也许能够检测模型中的欺骗行为。这比完全审核模型要低得多，而且我们只需能够<strong>查看模型的随机位并识别电路/特征</strong>，这似乎是我们可以做到的事情 - 我认为这更多地是“世界的变革理论”可解释性比我希望的更难”</i>来自<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Neel Nanda</u></a> 。</p><ul><li><strong>我不明白“查看</strong><i><strong>模型的随机位并识别电路/特征</strong></i><strong>”如何有助于欺骗。</strong>例如，假设我对随机电路的 GPT2 进行了逆向工程，例如在论文<a href="https://arxiv.org/abs/2211.00593"><u>Interpretability in the wild 中</u></a>，他们对间接对象识别电路进行了逆向工程。目前还不清楚这将如何帮助欺骗。即使预期的含义是“识别可能与欺骗/社交建模相关的电路/特征”，也不清楚分析每个电路是否足够（参见“ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>枚举安全</u></a>”小节）。</li><li><strong>我们还远未达到通过插译员检测或训练欺骗行为所需的水平。</strong> Evan Hubinger 在他的文章<a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"><u>《透明度和可解释性技术树》</u></a>中列出了 8 个可解释性级别，其中只有第 7 级和第 8 级提供了一些打击欺骗的手段。这些级别大致描述了可解释性的需求，但到目前为止我们只达到了第 2 级，并且我们已经在第 4 级遇到了负面结果。Evan 解释说，“<i>任何级别的透明度和可解释性技术对欺骗性模型具有鲁棒性都是极其困难的”</i> ”。</li><li> <i><strong>“此外，试图通过可解释性工具提前发现欺骗行为可能会失败，因为欺骗性对齐模型没有必要积极思考其欺骗行为。</strong>一个从未见过有机会夺取权力的情况的模型不需要仔细计划在这种情况下会做什么，就像工厂清洁机器人不需要计划如果有一天发现自己陷入困境该怎么办一样。丛林而不是工厂。尽管如此，该模型之前并未计划夺取权力，但这并不意味着如果有机会它就不会夺取权力。特别是，一个模型可能会被欺骗性地对齐，因为它推断，在有明确监督者的情况下，做它想做的事是在世界上获得权力和影响力的良好总体策略，而不需要为以后的欺骗制定任何明确的计划”。</i> （摘自 <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>《监视欺骗性对齐</u></a>》中的 Hubinger）</li><li><strong>已经存在反对可解释性的负面概念点</strong>，这表明先进的人工智能不容易被解释，正如杀伤力列表中的<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities%23sufficiently_good_and_useful"><u>可解释性部分</u></a>所讨论的那样（这些是我过去<a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23"><u>尝试</u></a>批评的观点，但大多失败了） 。特别是第 27、29 和 33 点：<ul><li> <strong>27. 选择不可检测性</strong>：“<i>针对解释性思想进行优化，就针对可解释性进行优化。”</i></li><li> <strong>29. 现实世界是一个不透明的领域：</strong> “<i>通用人工智能的输出在产生真正的后果之前要经过一个巨大的、我们不完全了解的领域（现实世界）。人类无法检查通用人工智能的输出来确定后果是否良好。”</i><ul><li><strong>并且认知可以外化。</strong>这不是特定于 interp 的。许多模式只能通过它们与环境的交互方式来解释，而不能仅通过网络中的内容来完全解释。例如“查阅食谱并采取书中所写的行动。” （康纳的例子）。</li></ul></li><li> <strong>33. 外星人概念：</strong> “<i>人工智能不像你那样思考”</i>对于通过矩阵乘积处理数字来完成的认知，可能不一定有人类可以理解的解释。</li><li>我并不完全同意所有这些观点，但我还没有看到对这些具体观点的太多讨论，你可以在我的评论中找到一些警告</li></ul></li><li>其他较弱的困难见脚注。 <span class="footnote-reference" role="doc-noteref" id="fnref2464ho15s7t"><sup><a href="#fn2464ho15s7t">[5]</a></sup></span></li></ul><p>仅使用 interp 来对抗欺骗并不是唯一的方法：</p><ul><li><strong>解释并不是研究欺骗的唯一方法。</strong>以下是其他被忽视的范例：<ul><li><strong>对抗性攻击</strong>（基本上<a href="https://www.youtube.com/watch?v%3DYTlrPeikoyw%26themeRefresh%3D1"><u>是雷德伍德的计划</u></a>）。许多重要的对抗策略不依赖于<a href="https://arxiv.org/abs/2306.09983"><strong><u>一致性检查</u></strong></a>、<a href="https://www.cold-takes.com/high-level-hopes-for-ai-alignment/"><u>人工智能检查和平衡</u></a>以及定期提示<span class="footnote-reference" role="doc-noteref" id="fnrefrbnra8fsacj"><sup><a href="#fnrbnra8fsacj">[6] 等</a></sup></span>插值。</li><li><strong>创建一个欺骗性对齐或模拟的玩具模型：</strong>研究“可能构成生存威胁的各种失败的体外演示”，来自<a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>《错位模型有机体》</u></a> ，这是一篇关于此的最新详细文章。</li><li><strong>寻找欺骗性联盟代理：</strong>例如，监控以下<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default%23Foundational_properties_for_deceptive_alignment"><u>4 个标准</u></a>，这些标准通常被视为欺骗性联盟的先决条件：目标导向的行为、跨事件/长期目标视野的优化、基本目标的概念化、情境意识。</li><li>接近人类水平的人工智能的各种基线策略：<ul><li>在 NLP 中，监控 AutoGPT 的计划和思路（ <a href="https://docs.google.com/document/d/e/2PACX-1vSedy4vmfA5H30bimiSGWykDfh8FB_uYKCt6D2qz9nwmfhGUc93H3UEPN1pBtyXe-eKEdu0E5oUbSWR/pub#kix.cyunx152z1bn"><u>细节</u></a>和注意事项）。</li><li>愿景中，类EfficientZero的一致性损失——在蒙特卡罗树中展望未来（ <a href="https://docs.google.com/document/d/1GiYfx77cE6-VyeNN31tVUARt5X7tbX4XD0CSmBkReUc/edit%23bookmark%3Did.dgd983y7t1ym"><u>草案</u></a>）</li></ul></li><li><a href="https://twitter.com/StephenLCasper/status/1650662566476279809"><u>更多</u></a>想法，如神经蒸馏和<a href="https://www.alignmentforum.org/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive%23How_to_actually_implement_a_speed_prior"><u>速度优先</u></a>。</li><li>如果 DeepMind 今天宣布他们在 GPT 中发现了欺骗行为，他们不太可能只使用可解释性来发现这一发现。他们更有可能使用定期提示。</li></ul></li><li><strong>有一些针对欺骗性联盟的预防措施</strong>似乎更可行（请参阅<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>针对欺骗的预防措施</u></a>部分）。</li><li><strong>概念上的进步更为紧迫。</strong>从概念上思考欺骗比通过可解释性思考要富有成效得多。据我所知，可解释性还没有教会我们任何关于欺骗的知识。<ul><li>例如，<a href="https://www.lesswrong.com/tag/simulator-theory"><u>模拟器理论</u></a>和对<i>GPT 已经可以模拟欺骗性拟像</i>的理解是我们对欺骗性对齐的理解比欺骗性可解释性方面所发生的进步更大的进步。</li><li>关于欺骗性对齐的概念性考虑，如文章<a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default"><u>《默认情况下欺骗性对齐的可能性 &lt;1%》</u></a>中所示，完全不依赖于可解释性。 </li></ul></li></ul><p></p><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:273.72px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/dkw7e0dp4pangmxj7ei4" alt=""></p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p><img style="width:268.46px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/ct8q2lqan4o5dhrw6bcf" alt=""></p></td></tr></tbody></table></figure><p><i>受到我与捍卫 interp 的朋友们进行的每一次讨论的启发。 “你对天文学的论证太笼统了”，所以让我们在下一节中深入探讨一些物体级的论证！</i></p><h1>可解释性的最终故事是什么样的？这一点都不清楚。</h1><p><i>这部分技术性比较强。可以先跳过它，或者只阅读“枚举安全性？”部分。这非常重要。</i></p><p>当然，深度学习中的可解释性似乎本质上比神经科学更可行，因为我们可以保存所有激活并非常缓慢地运行模型，通过尝试因果修改来理解正在发生的事情，并且比功能磁共振成像允许更多的控制。但在我看来，这还不够——我们甚至不知道我们的目标是什么。我们的目标是：</p><h2>枚举安全性？</h2><p> <strong>Enumerative safety, as Neel Nanda</strong> <a href="https://www.lesswrong.com/posts/qgK7smTvJ4DB8rZ6h/othello-gpt-future-work-i-am-excited-about#:~:text=enumerative%20safety%2C%20the%20idea%20that%20we%20might%20be%20able%20to%20enumerate%20all%20features%20in%20a%20model%20and%20inspect%20this%20for%20features%20related%20to%20dangerous%20capabilities%20or%20intentions.%20Seeing%20whether%20this%20is%20remotely%20possible%20for%20Othello%2DGPT%20may%20be%20a%20decent%20test%20run."><strong><u>puts it</u></strong></a> <strong>, is the idea that we might be able to enumerate</strong> <i><strong>all</strong></i> <strong>features</strong> in a model and inspect this for features related to dangerous capabilities or intentions. I think this strategy is doomed from the start (from most important to less important):</p><ul><li> <strong>Determining the dangerousness of a feature is a mis-specified problem.</strong> Searching for dangerous features in the weights/structures of the network is pointless. A feature is not inherently good or bad. The danger of individual atoms is not a strong predictor of the danger of assembly of atoms and molecules. For instance, if you visualize the feature of layer 53, channel 127, and it appears to resemble a gun, does it mean that your system is dangerous? Or is your system simply capable of identifying a dangerous gun? The fact that cognition can be <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach:~:text=And%20cognition%20can%20be%20externalized."><u>externalized</u></a> also contributes to this point.</li><li> <strong>A feature is still a fuzzy concept</strong> , and the problem of superposition and the natural abstraction hypothesis remains a hypothesis three years after those <a href="https://distill.pub/2020/circuits/zoom-in/"><u>Distill</u></a> papers, with very few convincing strategies to solve them. And that&#39;s not very surprising: the central conceptual notion of interpretability, the “feature”, seems to be intrinsically fuzzy and is still not defined. This is a major problem for the &quot;enumerative safety&quot; strategy and for iterating on neurons one by one to verify the &quot;goodness&quot; of each feature and obtain guarantees:<ul><li> And because of <a href="https://arxiv.org/abs/2209.10652"><u>superposition</u></a> , iterating over each neuron is not valid. So we can&#39;t just iterate on neurons, but we have to iterate on all sets of neurons (or worse, all directions), which is totally computationally intractable.</li></ul></li><li> <strong>Properties of models which are dangerous are not low-level features, but high-level behavioral abilities</strong> like being able to code, <a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>sycophancy</u></a> or various theories of mind proxies, situational awareness, or hacking.<ul><li> A network&#39;s situational awareness will likely include several sub-features such as date and time, geographical position, and the current needs of its users. Removing these sub-features would make the model less competitive.</li></ul></li><li> <a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness"><strong><u>Deep Deceptiveness</u></strong></a> - In simple terms, a system can be deceptive even if no single part is dangerous because of optimization pressure, and complex interactions between the model and the environment.</li><li> <strong>This strategy has already been tried</strong> for vision via automatic interpretability techniques to label all neurons, and it doesn&#39;t seem to have advanced alignment much, and most neurons evade simple interpretations:<ul><li> NetDisect &amp; Compositional explanations of neurons (Mu and Andreas, 2021)</li><li> Natural Language Descriptions of Deep Visual Features (Andreas, 2022)</li><li> Clip-Dissect (Oikarinen, 2022) <a href="https://visualvocab.csail.mit.edu/"><u>Towards a Visual Concept Vocabulary for GAN Latent Space</u></a> (Schwettmann, 2021)</li><li> These works [partially summarized <a href="https://www.lesswrong.com/posts/XZfJvxZqfbLfN6pKh/introductory-textbook-to-vision-models-interpretability"><u>here</u></a> ] have not changed the way we try to make vision systems more robust and less risky in practice.</li></ul></li><li> Most automatic interpretability works, like <a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models"><u>Language models can explain neurons in language models</u></a> from OpenAI or concept erasure techniques, falls into this category.</li></ul><h2> Reverse engineering?</h2><p> Reverse engineering is a classic example of interpretability, but I don&#39;t see a successful way forward. Would this be:</p><ul><li> The <strong>equivalent C++ annotated algorithm</strong> of what the model does? Being able to reproduce the capabilities of the inscrutable matrices of  GPT-4 by some modular C++ code by would be past human level intelligence already, and this would be <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>too dangerous</u></a> , because this would allow a lot of different optimization, and probably allow recursive self-improvement which seems dangerous especially if we rely on an automated process for that.</li><li> An <strong>explanation in layman terms</strong> of the behavior of the model? At which level of granularity? Each token or sentence or paragraph? This is really unclear.</li><li> The <strong>functional connectome</strong> of the model obtained with high level interp? Ok, you see in the functional connectome that the model is able to code and to hack, and those are dangerous capabilities. Isn&#39;t this just regular evals?<ul><li> In practice, to conduct interp experiments, we almost always start by creating a dataset of prompts. Maybe one day we won&#39;t need prompts to activate these capabilities, but I don&#39;t see (even on principles) that happening anytime soon.</li></ul></li><li> A <strong>graph</strong> to explain the circuits? Graphs like the ones just below can be overwhelming and remain very limited.</li></ul><p> You can notice that “Enumerative safety” is often hidden behind the “reverse engineering” end story. </p><p><img style="width:599.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/hpuwlgsxjtrumo38ckav" alt=""></p><p> <i>From the</i> <a href="https://arxiv.org/abs/2211.00593"><i><u>IOI paper</u></i></a> <i>. Understanding this diagram from &#39;Interpretability in the Wild&#39; by Wang et al. 2022 is not essential for our discussion. Understanding the full circuit and the method used would require a</i> <a href="https://www.youtube.com/watch?v%3Dgzwj0jWbvbo"><i><u>three-hour video</u></i></a> <i>. And, this analysis only focuses on a single token and involves numerous simplifications. For instance, while we attempt to explain why the token &#39;Mary&#39; is preferred over &#39;John&#39;, we do not delve into why the model initially considers either &#39;Mary&#39; or &#39;John&#39;. Additionally, this analysis is based solely on GPT2-small.</i> </p><p></p><p><img style="width:385.83px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/mmmq7xwly9mufts4101o" alt=""></p><p> <i>Indeed, this figure is quite terrifying. from</i> <a href="https://www.lesswrong.com/posts/j6s9H9SHrEhEfuJnq/causal-scrubbing-results-on-induction-heads"><i><u>Causal scrubbing: results on induction heads</u></i></a> <i>, for a 2 layer model. After refining 4 times the hypothesis, they are able to restore 86% of the loss. But even for this simple task they say “we won&#39;t end up reaching hypotheses that are fully specific or fully human-understandable, causal scrubbing will allow us to validate claims about which components and computations of the model are important.”.</i></p><p> The fact that reverse engineering is already so difficult in the two toy examples above seems concerning to me.</p><h2> Olah&#39;s interpretability dream?</h2><p> Or maybe interp is just an exploration driven by curiosity waiting for serendipity?</p><ul><li> <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>Interpretability Dreams</u></a> is an informal note by Chris Olah on future goals for mechanistic interpretability. It discusses <strong>superposition</strong> , the enemy of interpretability. Then, towards the end of the note, In the section titled “ <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23safety"><u>How Does Mechanistic Interpretability Fit Into Safety?</u></a> ”, we understand the plan is to solve superposition to be able to use the following formula: <br><br><img style="width:526.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/bbzkzi7n1cc1w6cvueju" alt=""></li><li> But this is simply again “ <i>enumerative safety”</i> stated in terms of circuits rather than features. However, as explained above, I don&#39;t think this leads us anywhere.</li><li> The final section of the note, <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html%23aesthetics"><u>Beauty and Curiosity</u></a> , reads like a poem or hymn to beauty. However, it seems to lack substance beyond a hope for serendipitous discovery.</li></ul><p> Overall, I am skeptical about Anthropic&#39;s use of the dictionary learning approach to solve the superposition problem. While their negative results are interesting, and they are working on addressing conceptual difficulties around the concept of &quot;feature&quot; (as noted in their<a href="https://transformer-circuits.pub/2023/may-update/index.html%23superposition-dictionary"><u>May update</u></a> ), I remain unconvinced about the effectiveness of this approach, even after reading their <a href="https://transformer-circuits.pub/2023/july-update/index.html%23safety-features"><u>recent July updates</u></a> , which still do not address my objections about enumerative safety.</p><p> One potential solution Olah <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">suggests</a> is automated research: &quot; <i>it does seem quite possible that the types of approaches […] will ultimately be insufficient, and interpretability may need to rely on AI automation</i> &quot;. However, I believe that this kind of automation is potentially harmful [section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful"><u>Harmful</u></a> ].</p><p> This is still a developing story, and the papers published on Distill are always a great pleasure to read. However, I remain hesitant to bet on this approach.</p><h2> Retargeting the search?</h2><p> <strong>Or maybe interp could be useful for</strong> <a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"><strong><u>retargeting the search</u></strong></a> <strong>?</strong> This idea suggests that if we find a goal in a system, we can simply change the system&#39;s goal and redirect it towards a better goal.</p><p> I think this is a promising quest, even if there are still difficulties:</p><ul><li> This is interesting because this would be a way to not need to fully reverse engineer a complete model.<strong> </strong>The technique used in <a href="https://www.alignmentforum.org/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network"><u>Understanding and controlling a maze-solving policy network</u></a> seems promising to me. Just focusing on “the motivational API” could be sufficient.</li><li> But I still don&#39;t know if <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vecto"><u>Steering vectors</u></a> (ie activation additions of a vector in the latent space) really count as interpretability, and really change significantly the picture of alignment beyond just prompt engineering. Ok, this is a new way to tinker with the model. But I don&#39;t know how this could be used reliably against deception. <span class="footnote-reference" role="doc-noteref" id="fnrefmf7vlk6ib69"><sup><a href="#fnmf7vlk6ib69">[7]</a></sup></span></li></ul><h2> Relaxed adversarial training?</h2><p> <strong>Relaxed adversarial training?</strong> The TL;DR is that <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>relaxed adversarial training</u></a> is the same as adversarial training, but instead of creating adversarial inputs to test the network, we create adversarial latent vectors. This could be useful because creating realistic adversarial inputs is a bottleneck in adversarial training. [More explanations <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.2ats8akz8z6u"><u>here</u></a> ]</p><p> This seems valid but very hard, and there are still significant <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>conceptual difficulties</u></a> . A concrete approach, <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> , has been proposed, and seems to be promising but:</p><ul><li> <strong>The procedure is underspecified</strong> . <strong>There will be too many meta-parameters</strong> . Calibrating these meta-parameters will be a nightmare, and you probably don&#39;t want to iterate on deceptive powerful models. We have to be good right away from the first choice of meta-parameters. As the author himself says, &quot; <i>the only hope here lies in the Surgeon forcing the model to be robustly safe before it learns to deceive. Once the model is deceptive it&#39;s really game-over.</i> &quot;</li><li> <strong>We still have no guarantees.</strong> This procedure allows for a latent space that is robust to “small perturbations”, but being robust to “small perturbations” is <i>not the same as not becoming deceptive</i> (it&#39;s not clear to me that deception won&#39;t appear outside the constraint zone).</li><li> Papers using this kind of procedure have only limited effectiveness, for example around 90% detection rate in the paper <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (Liu et al., 2019)</u></a> . [Paper summarized <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit%23bookmark%3Did.ewixtcjxjqvq"><u>here</u></a> ] And I don&#39;t think this could work against all types of trojans.</li></ul><p> The exact procedure described in <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> hasn&#39;t been tested, as far as I know. So we should probably work on it. <span class="footnote-reference" role="doc-noteref" id="fnrefc2q5uxqhj6j"><sup><a href="#fnc2q5uxqhj6j">[8]</a></sup></span></p><h2> Microscope AI?</h2><p> <strong>Maybe Microscope AI ie</strong> Maybe we could directly use the AI&#39;s world model without having to understand everything. Microscope AI is an AI that would be used not in inference, but would be used just by looking at its internal activations or weights, without deploying it. My definition would be something like: We can run forward passes, but only halfway through the model.</p><ul><li> This goes against almost every economic incentive (see <a href="https://gwern.net/tool-ai%23:~:text%3DAn%2520Agent%2520AI%2520has%2520the,its%2520outputs%252C%2520on%2520harder%2520domains."><u>Why Tool AIs wants to become Agents AI</u></a> , from Gwern).</li><li> <strong>($) Interpretability has been mostly useless for discovering facts about the world, and learning new stuff by only looking at the weights is too hard.</strong><ul><li> In the paper <a href="https://arxiv.org/abs/2111.09259"><u>Acquisition of Chess Knowledge in AlphaZero</u></a> , the authors investigate whether “ <i>we can learn chess strategies by interpreting the trained AlphaZero&#39;s <strong>behavior</strong></i> ”. Answer: This is not the case. They probe the network using only concepts already known to Stockfish, and no new fundamental insights are gained. We only check <i>when</i> AlphaGo learns human concepts during the training run.</li><li> I don&#39;t think we will be able to learn category theory by reverse engineering the brain of Terence Tao. How do Go players learn strategies from go programs? Do they interpret AlphaGo&#39;s weights, or do they try to understand the behavioral evaluations of those programs? Answer: They learn from their behavior, but not by interpreting models. I am skeptical that we can gain radically new knowledge from the weights/activations/circuits of a neural network that we did not already know, especially considering how difficult it can be to learn things from English textbooks alone.</li></ul></li><li> <strong>Microscope AIs should not be agentic by definition. But agency and exploration help tremendously at the human level for discovering new truths. Therefore, below superhuman level, the</strong> <i><strong>microscope</strong></i> <strong>needs to be</strong> <i><strong>agentic</strong></i> <strong>…and this is a contradiction.</strong> Using Microscope AI as a tool rather than an agent is suggested <a href="https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a%23Philosophical_framing"><u>here</u></a> or <a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>here</u></a> for example. However, to know the truth of a complex fact, we need to experiment with the world and actively search for information. Here is a fuzzy reasoning (feel free to skip):<ul><li> A) Either <strong>the information already exists and is written plainly</strong> somewhere on the internet, and in that case, there is no need for Microscope AI (this is like text retrieval).</li><li> B) Or <strong>the information doesn&#39;t exist anywhere on the internet</strong> , and in that case, it is necessary to be agentic by experimenting with the world or by thinking actively. This is the type of feature that can only be “created” by reinforcement learning but which cannot be “discovered” with supervised learning, like MuZero discovering new chess strategies.</li><li> or C), <strong>this info is not plainly written but is a deep feature of the training data</strong> that could be understood/grokked through gradient descent. This is the type of feature that can be “discovered” with supervised learning.</li><li> If B), we need agency, and it&#39;s no longer a microscope.</li><li> If C), we can apply the above reasoning <u>($)</u> + Being able to achieve this through pure gradient descent without exploration is probably a higher level of capability than being able to do it with exploration. (This would be like discovering the <a href="https://en.wikipedia.org/wiki/Quaternion"><u>Quaternion</u></a> formula during a dream?). But even legendary mathematicians need to work a bit and be agentic in their exploration; they don&#39;t just passively read textbooks. Therefore, it&#39;s probably beyond Ramanujan&#39;s level and too dangerous?</li><li> So, I&#39;m quite uncertain, but overall I don&#39;t think Microscope AI is a promising or valid approach to reducing AI risk.</li></ul></li></ul><p> A short case study of <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><u>Discovering Latent Knowledge</u></a> technique to extract knowledge from models by probing is included in the <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Burns_et_al___2022_"><u>appendix</u></a> .</p><h1> Preventive measures against Deception seem much more workable</h1><p> TL;DR: It would be more desirable to aim for a world where we wouldn&#39;t need to delve into the internal components of models. Prevention is better than cure, or at least, it is a neglected strategy. </p><figure class="image image_resized" style="width:599.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lon9yhtv1wgba17yhryt" alt=""><figcaption> <i>From</i> <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23The_Translucent_Thoughts_Hypotheses"><i><u>The Translucent Thoughts Hypotheses and Their Implications</u></i></a> <i>, Fabien Roger</i></figcaption></figure><p> I don&#39;t believe interpretability is the most promising method for monitoring near human-level AGI, Here&#39;s why:</p><p> <strong>I don&#39;t think neural networks will be able to take over in a single forward pass. Models will probably reason in English and will have translucent thoughts</strong> (and we could even hope to get transparent thoughts, translucence is the scary place where steganography is possible). In order to devise an effective plan to destroy the world, the first human-level AGI will need to somewhat externalize its reasoning, by chain-of-Thought (CoT), and it seems to me way easier to monitor this chain-of-Thought than probing into the models&#39; internals. We can probably use <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications"><u>The Translucent Thoughts Hypotheses</u></a> (Fabien, the author, gives a 20% chance, but I think it will be more like 60%). I also think that we have to do everything we can to pass regulations and <strong>place ourselves in a world where those hypotheses are true.</strong></p><p> For example, magic is possible (here&#39;s <a href="https://www.youtube.com/watch?v%3DEAN-PwRfJcA"><u>my favorite trick</u></a> btw), and the fact that human brains can be manipulated so easily by magic is a great danger. So we should probably try to make this magic unlikely, for example with process-based training by training AI Systems to reason only step-by-step (the process-based training approach is summarized in this <a href="https://www.lesswrong.com/posts/FFz6H35Gy6BArHxkc/task-decomposition-for-scalable-oversight-agisf-distillation"><u>AGISF distillation</u></a> ).</p><h2> Steering the world towards transparency</h2><p> <strong>Thinking ahead about the training setup is very important and possibly dominates interp considerations</strong> because if the concepts manipulated by the models are totally alien, it will be much, much harder to provide oversight. And it is <a href="https://aligned.substack.com/i/88447351/the-ai-tech-tree-is-looking-favorably"><u>much</u></a> easier to align those chatbots pretrained on human generated text than aligning AGIs trained from scratch with RL / evolutionary methods, etc.</p><p> If this is the case, we should focus on various aspects and important recommendations as detailed by Fabien <a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications%23Research_Directions"><u>here</u></a> . Here are some highlights:</p><ul><li> <strong>Making LLMs use only understandable English:</strong> Preventing models from using steganography, not training on synthetic data, etc… These directions are generally very neglected - for example, look at this <a href="https://www.lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight?commentId%3DS4wqnxW5CYd8QsTox"><u>comment</u></a> .</li><li> <strong>Building an oversight system:</strong> That&#39;s the “ <a href="https://youtu.be/YTlrPeikoyw?t%3D2415"><u>ready to go</u></a> ” technical research of Buck, IDA, or neglected baselines like <a href="https://arxiv.org/abs/2308.07308"><u>filtering</u></a> , etc…</li><li> <strong>Testing the translucent thoughts hypotheses</strong> and being able to monitor the alienness of models is super important in ensuring some simulacra have brain like representations (see <a href="https://www.lesswrong.com/posts/iXbPe9EAxScuimsGh/linkpost-scaling-laws-for-language-encoding-models-in-fmri"><u>this</u></a> and <a href="https://www.lesswrong.com/posts/2QexGHrqSxcuwyGmf/linkpost-large-language-models-converge-on-brain-like-word"><u>this</u></a> ). More work in the direction of <a href="https://arxiv.org/abs/2305.04388"><u>Language Models Don&#39;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</u></a> .</li><li> <strong>Making the hypotheses true</strong> , with technical works, outreach and governance, etc.</li></ul><p> <strong>I think the strategy should be: let&#39;s target a world where deception is unlikely.</strong> (I&#39;m not saying we should make plans that work conditional on deception being unlikely by default, but we should try to steer AGI/the world towards a place where deception is unlikely). I believe there are multiple ways to think and address this problem, and much more technical research needed here, starting from <a href="https://arxiv.org/abs/2302.00805"><u>Conditioning Predictive Models: Risks and Strategies</u></a> .</p><h2> Cognitive Emulations - Explainability <i>By Design</i></h2><p> <strong>If interpretability was really a bottleneck, we could use</strong> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>cognitive emulation</u></strong></a> <strong>, which, in my opinion, allows way better explainability and transparency than interpretability will ever get us.</strong></p><p> Some caveats are in the section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Cognitive_Emulations___Explainability_By_design"><u>Cognitive Emulation</u></a> of the appendix.</p><h1> Interpretability May Be Overall Harmful</h1><p> (Note that some of the following points are not specific to interp, but I think they apply particularly well to interp.)</p><p> <strong>False sense of control:</strong></p><ul><li> <strong>False sense of understanding.</strong> It&#39;s too easy to think you begin to understand that we&#39;re starting to get guarantees when we have not much. This is very classic:<ul><li> Me from the past: &quot;Yo, I spent 5 hours trying to understand the mechanisms of inductions head and K-Compositions with the incomprehensible math formula of the <a href="https://transformer-circuits.pub/2021/framework/index.html"><u>Mathematical Framework for Transformers</u></a> , I have so much more understanding.&quot; yes but no.</li></ul></li><li> <strong>Overinterpretation.</strong> It is very difficult to say which interpretation result is solid. For example, <a href="https://arxiv.org/abs/1810.03292"><u>Sanity Checks for Saliency Maps</u></a> shows that most of the pixel attribution techniques are generally misleading. <span class="footnote-reference" role="doc-noteref" id="fnrefpo4e41md3r"><sup><a href="#fnpo4e41md3r">[9]</a></sup></span> In the same vein, feature viz has recently been found to have some pretty fatal flaws, see <a href="https://arxiv.org/abs/2306.04719"><u>Don&#39;t trust your eyes: on the (un)reliability of feature visualizations</u></a> , and the model editing technique such as <a href="https://rome.baulab.info/"><u>ROME</u></a> is <a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><u>very misleading</u></a> .  This is mostly due to methodological problems that Stephen Casper explains in his sequence. [see appendix: <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Methodological_problems_"><u>methodological problems</u></a> ].</li><li> <strong>Safety Washing.</strong> I feel that there is a part of safety research which is here to legitimize capability research in the big labs (although this is not entirely specific to interp).<ul><li> <i>“I think a really substantial fraction of people who are doing &quot;AI Alignment research&quot; are instead acting with the primary aim of &quot;make AI Alignment seem legit&quot;. These are not the same goal, a lot of good people can tell and this makes them feel kind of deceived, and also this creates very messy dynamics within the field where people have strong opinions about what the secondary effects of research are, because that&#39;s the primary thing they are interested in, instead of asking whether the research points towards useful true things for actually aligning the AI”,</i> from <a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Shutting Down the Lightcone Offices</u></a> .</li></ul></li><li> <strong>The achievements of interp research are consistently graded on their own curve and  overhyped</strong> compared to achievements in other fields like adversaries research. For example, the recent paper <a href="https://arxiv.org/abs/2307.15043"><u>Universal and Transferable Adversarial Attacks on Aligned Language Models</u></a> impressively found effective attacks against state-of-the-art models without any interpretations involving models internals. Imagine if mechanistic interpretability researchers did the exact same thing, but by studying model internals? Given the excitement that has emerged in the past around the achievements of mechanistic interpretability in toy models on cherry-picked problems (eg <a href="https://arxiv.org/abs/2301.05217"><u>this</u></a> or <a href="https://arxiv.org/abs/2211.00593"><u>this</u></a> ), it seems that something like this would have probably made the AI safety research community go wild. Stephen Casper makes a similar point <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/MyvkTKfndx9t4zknh%23:~:text%3DFrom%2520an%2520engineer%25E2%2580%2599s%2520perspective%252C%2520it%25E2%2580%2599s%2520important%2520not%2520to%2520grade%2520different%2520classes%2520of%2520solutions%2520each%2520on%2520different%2520curves.%25C2%25A0"><u>here</u></a> : “ <i>From an engineer&#39;s perspective, it&#39;s important not to grade different classes of solutions each on different curves.</i> ” And other examples of this are presented here <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_:~:text%3Dtechniques%2520on%2520curves...-,Imagine%2520that%2520you%2520heard%2520news%2520tomorrow%2520that%2520MI%2520researchers%2520from%2520TAISIC%2520meticulously%2520studied%2520circuits%2520in%2520a%2520way%2520that%2520allowed%2520them%2520to%25E2%2580%25A6,-Reverse%2520engineer%2520and"><u>EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety</u></a> (thanks to Stephen for highlighting this point).</li></ul><p> <strong>The world is not coordinated enough for public interpretability research:</strong></p><ul><li> <strong>Dual use.</strong> It seems anything related to information representation can be used in a dual manner. This is a problem because I believe that the core of interpretability research could lead to major advances in capabilities. See this <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>post</u></a> .<ul><li> Using the insights provided by advanced interp to improve capabilities, such as modularity to optimize inference time and reduce flops, is likely to be easier than using them for better oversight. This is because <strong>optimizing for capability is much simpler than optimizing for safety</strong> , as we lack clear metrics for measuring safety.</li></ul></li><li> <strong>When interpretability starts to be useful, you can&#39;t even publish it because it&#39;s too info hazardous.</strong> The world is not coordinated enough for public interpretability research.<ul><li> Nate Soares <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous"><u>explained</u></a> this, and this was followed by <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>multiple</u></a> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>posts</u></a> . <i>“Insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research</i> <a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><i><u>closed</u></i></a> <i>. […] I acknowledge that public sharing of research insights could, in principle, both shorten timelines and improve our odds of success. I suspect that</i> <a href="https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"><i><u>isn&#39;t the case in real life</u></i></a> <i>.”</i></li><li> Good interp could produce a &quot;foom overhang&quot; as described in &quot; <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a> &quot;.</li><li> Good interp also creates an<a href="https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful"><u>infosec/infohazard attack vector</u></a> .</li><li> The post &#39; <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When is Interpretability Work Dangerous?</u></a> &#39; ends on a sobering note, stating, “ <i>In closing, if alignment-conscious researchers continue going into the interpretability subfield, the probability of AGI ruin will tend to increase.</i> ”</li></ul></li><li> <strong>Interpretability already helps capabilities.</strong> For example, the understanding of Induction head has allowed for <a href="https://twitter.com/NeelNanda5/status/1618185819285778433"><u>better</u></a> architectures <span class="footnote-reference" role="doc-noteref" id="fnrefplu4ji16iui"><sup><a href="#fnplu4ji16iui">[10]</a></sup></span> .</li><li> Interpretability may be a <a href="https://en.wikipedia.org/wiki/Wicked_problem%23Super_wicked_problems"><u>super wicked problem</u></a> <span class="footnote-reference" role="doc-noteref" id="fnrefw7s6gsvuwb"><sup><a href="#fnw7s6gsvuwb">[11]</a></sup></span> .</li></ul><p> Thus the list of &quot;theory of impact&quot; for interpretability should not simply be a list of benefits. It&#39;s important to explain why these benefits outweigh the possible negative impacts, as well as how this theory can save time and mitigate any new risks that may arise. </p><p></p><p><img style="width:414.07px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/zao0mylkanwh60aajinm" alt=""></p><p> <i>The concrete application of the</i> <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"><i><u>logit lens</u></i></a> <i>is not an oversight system for deception, but rather capability works to accelerate inference speed like in</i> <a href="https://twitter.com/GoogleAI/status/1603845007663734785"><i><u>this paper</u></i></a> <i>. (Note that the paper does not cite logit lens, but relies on a very similar method).</i></p><h1> Outside view: The proportion of junior researchers doing interp rather than other technical work is too high</h1><p> It seems to me that many people start alignment research as follows:</p><ul><li> At the end of <a href="https://www.arena.education/"><u>Arena</u></a> , an advanced upskilling program in AI Safety, almost all research projects this year (June 2023), except for two out of 16, were interp projects.</li><li> At <a href="https://ia.effisciences.org/"><u>EffiSciences</u></a> , at the end of the last 3 <a href="https://www.lesswrong.com/posts/DkDy2hvkwbQ54GM9u/introducing-effisciences-ai-safety-unit-1"><u>ML4Good</u></a> bootcamps, students all start by being interested in interp, and it is a very powerful attractor. I myself am guilty. I have redirected too many people to it. I am now trying to correct my ways.<ul><li> In the past, if I reconstruct my motivational story, it goes something like this: &quot;Yo, I have a math/ML background, how can I recycle that?&quot; -->; then <i>brrr interp</i> , without asking too many questions.</li></ul></li><li> During <a href="https://apartresearch.com/"><u>Apart Research</u></a> hackathons, interpretability hackathons tend to draw 3.12 times as many participants as other types of hackathons. (thinkathon, safety benchmarks, …). <span class="footnote-reference" role="doc-noteref" id="fnrefavln8kyvlzg"><sup><a href="#fnavln8kyvlzg">[12]</a></sup></span></li><li> Interpretability streams in <a href="https://www.serimats.org/"><u>Seri Mats</u></a> are among the most competitive streams (see this <a href="https://twitter.com/leedsharkey/status/1656705667963535370"><u>tweet</u></a> ). People then try hard, get rejected, get disappointed and lose motivation. This is a recent important problem.</li></ul><p> &quot;Not putting all your eggs in one basket&quot; seems more robust considering our uncertainty, and there are more promising ways to reduce x-risk per unit of effort (to come in a future post, mostly through helping/doing governance). I would rather see a <strong>more diverse ecosystem</strong> of people trying to reduce risks. More on this in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Technical_Agendas_with_better_ToI"><u>Technical Agendas with better ToI</u></a> .</p><p> If you ask me if interp is also over represented in senior researchers, I&#39;m a bit less confident. Interp also seems to be a significant portion of the pie: this year, while Conjecture and Redwood have partially pivoted, there are new active interp teams in Apollo, DeepMind, OpenAI, and still in Anthropic. I think I would particularly critique DeepMind and OpenAI&#39;s interpretability works, as I don&#39;t see how this reduces risks more than other works that they could be doing, and I&#39;d appreciate a written plan of what they expect to achieve.</p><h1> So far my best ToI for interp: Nerd Sniping?</h1><p> 1. <strong>Interp for Nerd Sniping/honeypot?</strong></p><ul><li> <strong>Interp is a highly engaging introduction to AI research</strong> . That&#39;s really cool for that, I use it for my <a href="https://www.master-mva.com/cours/seminaire-turing/"><u>classes</u></a> , and for technical outreach, but I already have enough material on interpretability, for 10 hours of class, no need to add more.</li><li> <strong>Interp as a honeypot for junior researchers?</strong> Just as a honeypot attracts bees with its sweet nectar, interp is very successful for recruiting new technical people! but then they would probably be better off doing something else than interp (unless it is their strong comparative advantage).</li><li> (Nerd Sniping senior capability researchers into interpretability research? Less capability research, more time to align AIs? I&#39;m joking, don&#39;t do that at home! )</li></ul><p> 2. <strong>Honorable mentions:</strong></p><ul><li> <strong>Showing strange failures</strong> , such as the issue with the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"><u>SolidGoldMagicCarp</u></a> token, highlights the possibility of unexpected results with the model. More generally, interpretability tools can be useful for the red teaming toolbox. They seem like they might be able to guide us to more problems than test sets and adversaries can alone.</li><li> <strong>Showing GPT is not a stochastic parrot?</strong> The article <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>Actually, Othello-GPT Has A Linear Emergent World Representation</u></a> <strong>&nbsp;</strong> is really cool <strong>.</strong> Showing that OthelloGPT contains a world model is really useful for technical outreach (even if OthelloGPT being good at Othello should be enough, no?).</li><li> <strong>It&#39;s a good way to introduce the importance and tractability of alignment research</strong> “ <i>Interpretability gives people a non-technical story for how alignment affects their lives, the scale of the problem, and how progress can be made. IMO no other approach to alignment is anywhere near as good for this.”</i> [from <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DuzBFJDsy9Jqkxzdnx"><u>Raymond D</u></a> ]</li><li> <strong>Better: Showing that “We have basically no idea how it does what it does.”,</strong> see this <a href="https://twitter.com/robertskmiles/status/1663534255249453056"><u>tweet</u></a> : </li></ul><p><img style="width:387.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/q9gbkwdp3drer7soei5a" alt=""></p><h1> Even if we completely solve interp, we are still in danger</h1><p> No one has ever claimed otherwise, but it&#39;s worth remembering to get the big picture. From stronger arguments to weaker ones:</p><ul><li> <strong>There are many X-risks scenarios, not even involving deceptive AIs.</strong> Here is a list of such scenarios (see this <a href="https://www.lesswrong.com/posts/nCeyBbhtJhToBFmrL/cheat-sheet-of-ai-x-risk"><u>cheat sheet</u></a> ):<ul><li> Christiano1 - <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom%23Part_I__You_get_what_you_measure"><u>You get what you measure</u></a></li><li> Critch1 - <a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic%23Part_1__Slow_stories__and_lessons_therefrom"><u>Production Web</u></a></li><li> Soares - <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>A central AI alignment problem: capabilities generalization, the sharp left turn</u></a></li><li> Cohen et al. - <a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>Advanced artificial agents intervene in the provision of reward</u></a></li><li> Gwern - <a href="https://gwern.net/fiction/clippy"><u>It Looks Like You&#39;re Trying To Take Over The World</u></a></li><li> Exercise: Here is <a href="https://www.safe.ai/ai-risk"><u>a list of risks</u></a> from the Center of AI Safety. Which ones can be solved by interp? At least half of those risks don&#39;t directly involve deception and interp.</li></ul></li><li> <strong>Total explainability of complex systems with great power is not sufficient to eliminate risks.</strong> Significant risks would still remain. Despite our full understanding of how atomic bombs function, they still pose substantial risks. See this <a href="https://en.wikipedia.org/wiki/List_of_nuclear_close_calls"><u>list of nuclear close calls</u></a> .</li><li> <strong>Interpretability implicitly assumes that the AI model does not optimize in a way that is adversarial to the user.</strong> Consider being able to read the mind of a psychopath like Voldemort. Would this make you feel safe? The initial step remains to box him. However, <strong>a preferable scenario would be not having to confront this situation at all.</strong> (this last claim is probably the most important lesson - see <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ). </li></ul><figure class="image image_resized" style="width:54.8%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/s6qlqvxf5an1gdofmmk3" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qgopxuvs7mrmxegq8nco 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/smtffxyqwgnqhsasg4fy 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/lrifkrnrib0sybu8nvo8 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/j9jd3of3dwrmqpyg6xhr 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/cuouqap73vyebun1hfu7 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/e3ppnnd6cgyt2czjf3l6 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/vozp1plvffdzhxsuh1qy 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/qywwfgebl65ejalxo5ps 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/araz4zygcvf4szuqkmy4 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LNA8mubrByG7SFacm/spahqo0hllay5zjyvlk0 1010w"><figcaption> Pytorch hooks can be used to study the internals of models. Are they going to be sufficient? <i>Idk, but</i> <a href="https://www.youtube.com/watch?v=LveUcCBRrSo"><i><u>Hook Me up Baby</u></i></a> <i>, from the album “Take Me as I Am” could be the national anthem of interp.</i></figcaption></figure><p> <strong>That is why focusing on coordination is crucial! There is a level of coordination above which we don&#39;t die - there is no such threshold for interpretability.</strong> We currently live in a world where coordination is way more valuable than interpretability techniques. So let&#39;s not forget that <a href="https://www.alignmentforum.org/posts/FfTxEf3uFPsZf9EMP/avoiding-perpetual-risk-from-tai%23Non_alignment_aspects_of_AI_safety_are_key_"><u>non-alignment aspects of AI safety are key!</u></a> AI alignment is only a subset of AI safety! ！ (I&#39;m planning to deep-dive more into this in a following post).</p><p> A version of this argument applies to &quot;alignment&quot; in general and not just interp and those considerations will heavily influence my recommendations for technical agendas.</p><h1> Technical Agendas with better ToI</h1><p> Interp is not such a bad egg, but opportunity costs can be huge (especially for researchers working in big labs).</p><p> I&#39;m not saying we should stop doing technical work. Here&#39;s a list of technical projects that I consider promising (though I won&#39;t argue much for these alternatives here):</p><ul><li> <strong>Technical works used for AI Governance.</strong> A huge amount of technical and research work needs to be done in order to make regulation robust and actually useful. The governance section of <a href="https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice#Governance_work"><u>AGI safety career advice</u></a> by Richard Ngo is really great : “ <i>It&#39;s very plausible that, starting off with no background in the field, within six months you could write a post or paper which pushes forward the frontier of our knowledge on how one of those topics is relevant to AGI governance.</i> ”<ul><li> For example, each of the measures proposed in the paper <a href="https://arxiv.org/abs/2305.07153"><u>towards best practices in AGI safety and governance: A survey of expert opinion</u></a> could be a pretext for creating a specialized organization to address these issues, such as auditing, licensing, and monitoring.</li><li> Scary demos (But this shouldn&#39;t involve gain-of-function research. There are already many powerful AIs available. Most of the work involves video editing, finding good stories, distribution channels, and creating good memes. Do not make AIs more dangerous just to accomplish this.).</li><li> In the same vein, <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a> is probably good because “ <a href="https://www.lesswrong.com/posts/vavnqwYbc8jMu3dTY/ai-coordination-needs-clear-wins"><u>AI coordination needs clear wins</u></a> ”.</li><li> Interoperability in AI policy, and good definitions usable by policymakers.</li><li> Creating benchmarks for dangerous capabilities.</li><li> Here&#39;s a <a href="https://docs.google.com/document/d/1Tvz2JS8CZ51TW-vfU3vwRn8dpK3F0UttdhMrZC2o7hw/edit#bookmark=id.jnvbactyuay7"><u>list</u></a> of other ideas</li></ul></li><li> <strong>Characterizing the technical difficulties of alignment. (</strong><a href="https://www.lesswrong.com/posts/uHYYA32CKgKT3FagE/hold-off-on-proposing-solutions"><strong><u>Hold Off On Proposing Solutions</u></strong></a> <strong>“Do not propose solutions until the problem has been discussed as thoroughly as possible without suggesting any.”)</strong><ul><li> Creating the <a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change"><u>IPCC</u></a> of AI Risks</li><li> More red-teaming of agendas</li><li> Explaining problems in alignment.</li></ul></li><li> Adversarial examples, adversarial training, latent adversarial training (the only <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>end-story</u></a> I&#39;m kind of excited about). For example, the papers &quot; <a href="https://arxiv.org/abs/2210.04610"><u>Red-Teaming the Stable Diffusion Safety Filter</u></a> &quot; or &quot; <a href="https://arxiv.org/abs/2307.15043"><u>Universal and Transferable Adversarial Attacks on Aligned Language Models</u></a> &quot; are good (and pretty simple!) examples of adversarial robustness works which contribute to safety culture.</li><li> <strong>Technical outreach</strong> . <a href="https://www.youtube.com/@ai-explained-"><u>AI Explained</u></a> and <a href="https://www.youtube.com/c/robertmilesai"><u>Rob Miles</u></a> have plausibly reduced risks much more than all  interpretability research combined.</li><li> In essence, ask yourself: “What would Dan Hendrycks do?”<ul><li> Technical newsletter, non-technical newsletters, benchmarks, policy recommendations, risks analysis, banger statements, courses and technical outreach.</li><li> He is not doing interp. Checkmate!</li></ul></li></ul><p> In short, my agenda is <strong>&quot;Slow Capabilities through a safety culture&quot;</strong> , which I believe is robustly beneficial, even though it may be difficult. I want to help humanity understand that we are not yet ready to align AIs. Let&#39;s wait a couple of decades, then reconsider.</p><p> And if we really have to build AGIs and align AIs, it seems to me that it is more desirable to aim for a world where we don&#39;t need to probe into the internals of models. Again, prevention is better than cure.</p><h1>结论</h1><p>I have argued against various theories of impact of interpretability, and proposed some alternatives. I believe working back from the different risk scenarios and red-teaming the theories of impact gives us better clarity and a better chance at doing what matters. Again, I hope this document opens discussions, so feel free to respond in parts. There probably <i>should</i> be a non-zero amount of researchers working on interpretability, this isn&#39;t intended as an attack, but hopefully prompts more careful analysis and comparison to other theories of impact.</p><p> We already know some broad lessons, and we already have a general idea of which worlds will be more or less dangerous.Some ML researchers in top labs aren&#39;t even aware of, or acknowledging, that AGI is dangerous, that connecting models to the internet, encouraging agency, doing RL and maximizing metrics isn&#39;t safe in the limit.</p><p> Until civilization catches up to these basic lessons, we should avoid playing with fire, and should try to slow down the development of AGIs as much as possible, or at least steer towards worlds where it&#39;s done only by extremely cautious and competent actors.</p><p> Perhaps the main problem I have with interp is that it implicitly reinforces the narrative that we must build powerful, dangerous AIs, and then align them. For X-risks, prevention is better than cure. Let&#39;s <i>not</i> build powerful and dangerous AIs. We aspire for them to be safe, by design.</p><h1>附录</h1><h2>Related works</h2><p> There is a vast academic literature on the virtues and academic critiques of interpretability (see this <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>page</u></a> for plenty of references), but relatively little holistic reflection on interpretability as a strategy to reduce existential risks.</p><p> The most important articles presenting arguments for interpretability:</p><ul><li> <a href="https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety"><u>Chris Olah&#39;s views on AGI safety</u></a></li><li> <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>A Longlist of Theories of Impact for Interpretability</u></a></li><li> <a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html"><u>Interpretability Dreams</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous%23When_Interpretability_is_Still_Important"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/6ReBeYwsDeNgv6Dr5/the-defender-s-advantage-of-interpretability"><u>The Defender&#39;s Advantage of Interpretability</u></a></li><li> <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><u>Monitoring for deceptive alignment</u></a></li></ul><p> Against interpretability</p><ul><li> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>AGI-Automated Interpretability is Suicide</u></a></li><li> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><u>Why and When Interpretability Work is Dangerous</u></a></li><li> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>Should we publish mechanistic interpretability research?</u></a></li><li> <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>EIS III: Broad Critiques of Interpretability Research</u></a></li><li> <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3"><u>EIS VI: Critiques of Mechanistic Interpretability Work in AI Safety</u></a></li><li> <a href="https://link.springer.com/article/10.1007/s13347-019-00372-9"><u>Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning</u></a></li></ul><h3> The Engineer&#39;s Interpretability Sequence</h3><p> I originally began my investigation by rereading  “The Engineer&#39;s Interpretability Sequence”, in which Stephen Casper raises many good critiques of interpretability research, and this was really illuminating.</p><p> <strong>Interpretability tools lack widespread use by practitioners in real applications.</strong></p><ul><li> No interpretability technique is yet publicly known to have been used in production in SOTA models such as ChatGPT.</li><li> There have been interpretability studies of SOTA multimodal models such as <a href="https://distill.pub/2021/multimodal-neurons/"><u>CLIP</u></a> in the past, but these studies are only descriptive.</li><li> The efficient market hypothesis: The technique used for the censorship filter of the Stable Diffusion model was a <a href="https://arxiv.org/abs/2210.04610"><u>vulgar cosine similarity threshold</u></a> between generated image embeddings and a list of taboo concepts. Yes, this may seem a bit ridiculous, but at least there is a filter, and it appears that interp has not yet been able to provide more convenient tools than this.</li></ul><p> <strong>Broad critiques.</strong> He <a href="https://www.lesswrong.com/posts/gwG9uqw255gafjYN4/eis-iii-broad-critiques-of-interpretability-research"><u>explains</u></a> that interp is generally not scaling, relying too much on humans, failing to combine techniques. He also <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3%23Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>criticizes</u></a> mech interp, which may not be the best way of doing interp, because of cherry-picking, focusing only on toy examples and lack of scalability, and failing to do useful things.</p><p> <strong>Methodological problems:</strong></p><ul><li> He <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/gwG9uqw255gafjYN4%23A_lack_of_practical_applications"><u>points out</u></a> , &quot;The root cause of this has much to do with interpretability research not being approached with as much engineering rigor as it ought to be.&quot;</li><li> One good point to note is that since the publication of his sequence, certain methods have become more rigorous. For example, <a href="https://www.lesswrong.com/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and"><u>Causal methods</u></a> , <a href="https://arxiv.org/abs/2211.00593"><u>activation patching</u></a> , <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world"><u>probing only with linear probes</u></a> are more rigorous and promising than descriptive techniques.</li><li> But I also believe that interpretability may be fundamentally flawed [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interp look like</u></a> ].</li></ul><h2> Cognitive Emulations - Explainability <i>By design</i></h2><p> <strong>If interpretability was really a bottleneck, we could use</strong> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><strong><u>cognitive emulation</u></strong></a> <strong>, which, in my opinion, allows way better explainability/transparency than interpretability will ever get us.</strong></p><p> My understanding of cognitive emulation: Emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in <i>plain English</i> to each other before outputting the next token. If the neural network had deceptive thoughts, we could see them in these intermediate messages.</p><p> <strong>Interpretability is needed only if one module of the cognitive emulation is deceptive.</strong> Then maybe you can use interpretability to explain this module. But I&#39;m confident models will need Chain of Thought for the foreseeable future. So interpretability is not &#39;urgent&#39; research.</p><p> <strong>If we don&#39;t use Reinforcement learning on this cognitive emulation, and if we only prompt in English the different submodules</strong> , the cognitive emulation won&#39;t create whole “new languages” or be able to obfuscate a lot of its thoughts.</p><p> Yes, there are <strong>problems with this proposal:</strong></p><ul><li> CoEms don&#39;t preclude steganography or obfuscation.</li><li> Even when asked to reason with Chain-of-Thought (CoT), LLMs are able to skip some inference steps to some extent (see <a href="https://arxiv.org/abs/2305.04388"><u>Language Models Don&#39;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</u></a> ). However, if we don&#39;t train models to skip a zillion inference steps, they won&#39;t be able to obfuscate their thoughts efficiently.</li></ul><p> But we could further reduce these risks with:</p><ul><li> <strong>Process-based training</strong> , to ensure that the systems are accustomed to detailing their thoughts in much greater detail (and not being able to skip too many steps). It seems much more feasible than interpretability (and the <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>alignment tax</u></a> even seems negative - for example see those progress for <a href="https://openai.com/research/improving-mathematical-reasoning-with-process-supervision"><u>Mathematical reasoning</u></a> )</li><li> <strong>selection inference:</strong> <i>“CoT has access to the whole context, so each reasoning step is not necessarily causally connected to the last. But</i> <a href="https://arxiv.org/abs/2205.09712"><i><u>selection inference</u></i></a> <i>enforces a structure where each reasoning step necessarily follows from the last, and therefore the whole reasoning chain is causal.”</i> from <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit?disco=AAAA21_wJ_g"><u>Sid Black</u></a> , CTO of Conjecture.</li><li> Other ideas were listed in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Steering_the_world_towards_transparency">Steering the world towards transparency</a> ”.</li></ul><p> Spicy: However, cognitive emulation will quite likely be an engineering nightmare, facing significant robustness issues that are always present in small models. The alignment tax will be higher than for end-to-end systems, making it unlikely that we will ever use this technology. The bottleneck is probably not interp, but rather an ecosystem of preventive safety measures and a safety culture. Connor Leahy, CEO of Conjecture, explaining the difficulties of the problem during interviews and pushing towards a safety culture, is plausibly more impactful than the entire CoEm technical agenda.</p><h2> Detailed Counter Answers to Neel&#39;s list</h2><p> Here is Neel&#39;s <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><u>Longlist of Theories of Impact for Interpretability</u></a> with critiques for each theory. Theories proposed by Neel are displayed in italics, whereas my critiques are rendered in standard font.</p><ol><li> <i><strong>Force-multiplier on alignment research</strong> : We can analyse a model to see why it gives misaligned answers, and what&#39;s going wrong. This gets much richer data on empirical alignment work, and lets it progress faster.</i><ul><li> I think this &quot;force multiplier in alignment research&quot; theory is valid, but is conditioned on the success of the other theories of impact, which imho are almost all invalid.</li><li> <strong>Conceptual advancements are more urgent</strong> It&#39;s better to think conceptually about what misalignment means rather than focusing on interp. [Section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#What_does_the_end_story_of_interpretability_look_like__That_s_not_clear_at_all_"><u>What does the end story of interpretability look like?</u></a> ]</li><li> <strong>Dual Use:</strong> Force-multiplier on capability research.</li></ul></li><li> <i><strong>Better prediction of future systems</strong> : Interpretability may enable a better mechanistic understanding of the principles of how ML systems work, and how they change with scale, analogous to scientific laws. This allows us to better extrapolate from current systems to future systems, in a similar sense to scaling laws. Eg, observing phase changes a la induction heads shows us that models may rapidly gain capabilities during training</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Auditing</strong> : We get a Mulligan. After training a system, we can check for misalignment, and only deploy if we&#39;re confident it&#39;s safe</i><ul><li> <strong>Not the most direct way.</strong> This ToI targets outer misalignment, the next one targets inner misalignment. But currently, people who are auditing for outer alignment do not use interpretability. They evaluate the model, they make the model speak and look if it is aligned with behavioral evaluations. Interpretability has not been useful in finding GPT&#39;s jailbreaks.</li><li> To date, I still don&#39;t see how we would proceed with interp to audit GPT-4.</li></ul></li><li> <i><strong>Auditing for deception</strong> : Similar to auditing, we may be able detect deception in a model. This is a much lower bar than fully auditing a model, and is plausibly something we could do with just the ability to look at random bits of the model and identify circuits/features - I see this more as a theory of change for &#39;worlds where interpretability is harder than I hope&#39;.</i><ul><li> Critiqued in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Enabling coordination/cooperation:</strong> If different actors can interpret each other&#39;s systems, it&#39;s much easier to trust other actors to behave sensibly and coordinate better</i><ul><li> <strong>Not the most direct way.</strong> If you really want coordination and cooperation, you need to help with AI governance and outreach of experts and researchers. The <a href="https://www.safe.ai/statement-on-ai-risk"><u>statement on AI risks</u></a> has enabled more coordination than interp will probably never get us.</li></ul></li><li> <i><strong>Empirical evidence for/against threat models</strong> : We can look for empirical examples of theorized future threat models, eg inner misalignment</i><ul><li> <i><strong>Coordinating work on threat models</strong> : If we can find empirical examples of eg inner misalignment, it seems much easier to convince skeptics this is an issue, and maybe get more people to work on it.</i><ul><li> <a href="https://ai.facebook.com/research/cicero/"><u>Cicero</u></a> or poker models are already capable of masking pieces of information or bluffing to play poker. From there, I don&#39;t know what it would mean to show canonical inner misalignment to non-technical people.</li><li> This focuses too much on deceptive alignment, and this will probably be too late if we get to this point.</li></ul></li><li> <i><strong>Coordinating a slowdown</strong> : If alignment is really hard, it seems much easier to coordinate caution/a slowdown of the field with eg empirical examples of models that seem aligned but are actually deceptive</i><ul><li> <strong>Not the most direct way.</strong> This is a good theory of change, but interp is not the only way to show that a model is deceptive.</li></ul></li></ul></li><li> <i><strong>Improving human feedback</strong> : Rather than training models to just do the right things, we can train them to do the right things for the right reasons</i><ul><li> Seems very different from current interpretability work.</li><li> <strong>Not the most direct way.</strong> Process-based training, model psychology, or other scalable oversight techniques not relying on interp may be more effective.</li></ul></li><li> <i><strong>Informed oversight</strong> : We can improve recursive alignment schemes like IDA by having each step include checking the system is actually aligned. Note: This overlaps a lot with 7. To me, the distinction is that 7 can be also be applied with systems trained non-recursively, eg today&#39;s systems trained with Reinforcement Learning from Human Feedback</i><ul><li> Yes, it&#39;s an improvement, but it&#39;s naive to think that the only problem with RLHF is just the issue of lack of transparency or deception. For example, we would still have agentic models (because agency is preferred by human preferences) and interpretability alone won&#39;t fix that. See the <a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>Compendium of problems with RLHF</u></a> and <a href="https://www.lesswrong.com/posts/LqRD7sNcpkA9cmXLv/open-problems-and-fundamental-limitations-of-rlhf">Open Problems and Fundamental Limitations of RLHF</a> for more details.</li><li> <strong>Conceptual advances are more urgent.</strong> What does &#39;checking the system is actually aligned&#39; really means? <a href="https://docs.google.com/document/d/1ybJqvZ7vkfN641KAiDj1I0Deu-XGk8r9fSJEZ4NvLmc/edit#bookmark=id.wqr2jvmzsg7c"><u>It&#39;s not clear at all.</u></a></li></ul></li><li> <i><strong>Interpretability tools in the loss function:</strong> We can directly put an interpretability tool into the training loop to ensure the system is doing things in an aligned way. Ambitious version - the tool is so good that it can&#39;t be Goodharted. Less ambitious - The could be Goodharted, but it&#39;s expensive, and this shifts the inductive biases to favor aligned cognition</i> .<ul><li> <strong>Dual Use,</strong> for obvious reasons, and this one is particularly dangerous.</li><li> <strong>List of lethalities 27. Selecting for undetectability</strong> : “ <i>Optimizing against an interpreted thought optimizes against interpretability.”</i></li></ul></li><li> <i><strong>Norm setting</strong> : If interpretability is easier, there may be expectations that, before a company deploys a system, part of doing due diligence is interpreting the system and checking it does what you want</i><ul><li> <strong>Not the most direct way.</strong> Evals, evals, evals.</li><li> No need to wait for interpretability. We already roughly know what to do. We could conduct studies in line with <a href="https://www.serimats.org/evals"><u>Evaluating Dangerous Capabilities</u></a> and the paper <a href="https://arxiv.org/abs/2305.15324"><u>Model Evaluation for Extreme Risks</u></a> <u>,</u> <a href="https://www.governance.ai/research-paper/towards-best-practices-in-agi-safety-and-governance">Towards Best Practices in AGI Safety and Governance</a> , this last paper presenting 50 statements about what AGI labs should do, none mentioning interp.</li></ul></li><li> <i><strong>Enabling regulation</strong> : Regulators and policy-makers can create more effective regulations around how aligned AI systems must be if they/the companies can use tools to audit them</i><ul><li> Same critique as <strong>10.</strong> <i><strong>Norm setting</strong></i></li></ul></li><li> <i><strong>Cultural shift 1:</strong> If the field of ML shifts towards having a better understanding of models, this may lead to a better understanding of failure cases and how to avoid them</i><ul><li> <strong>Not the most direct way.</strong> Technical Outreach, communications, interviews or even probably standards and <a href="https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt"><u>Benchmarks</u></a> are way more direct.</li></ul></li><li> <i><strong>Cultural shift 2:</strong> If the field expects better understanding of how models work, it&#39;ll become more glaringly obvious how little we understand right now</i><ul><li> Same critique as <strong>12.</strong> <i><strong>Cultural shift 1.</strong></i></li><li> This is probably the opposite of what is happening now: People are fascinated by interpretability and continue to develop capabilities in large labs. I suspect that the well-known Distill journal has been very fascinating for a lot of people and has probably been a source of fascination for people entering the field of ML, thus accelerating capabilities.</li><li> See the <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#False_sense_of_control_"><u>False sense of control</u></a> section.</li></ul></li><li> <i><strong>Epistemic learned helplessness</strong> : Idk man, do we even need a theory of impact? In what world is &#39;actually understanding how our black box systems work&#39; not helpful?</i><ul><li> I don&#39;t know man, the worlds where we have limited resources, where we are funding constrained + Opportunity costs.</li><li> <strong>Dual Use</strong> , refer to the section &quot; <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_May_Be_Overall_Harmful">Interpretability May Be Overall Harmful</a> &quot;.</li></ul></li><li> <i><strong>Microscope AI</strong> : Maybe we can avoid deploying agents at all, by training systems to do complex tasks, then interpreting how they do it and doing it ourselves</i><ul><li> Critique in section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI?</u></a> 。</li></ul></li><li> <i><strong>Training AIs to interpret other AIs</strong> : Even if interpretability is really hard/labor intensive on advanced systems, if we can create aligned AIs near human level, we can give these interpretability tools and use them to interpret more powerful systems</i><ul><li> <strong>Object level:</strong> Training AI to interpret other AI, could be useful but would be already dangerous, and we are already in classes of scenarios that are super <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><u>dangerous</u></a> .</li><li> <strong>Meta level:</strong> This scheme is very speculative. I do not want the survival of civilization to rely on it. <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"><u>Godzilla strategy</u></a> is probably not a good strategy (though this is controversial).</li></ul></li><li> <i><strong>Forecasting discontinuities</strong> : By understanding what&#39;s going on, we can predict how likely we are to see discontinuities in alignment/capabilities, and potentially detect a discontinuity while training/before deploying a system</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interp_is_not_a_good_predictor_of_future_systems"><u>Interp is not a good predictor of future systems</u></a> ”</li></ul></li><li> <i><strong>Intervening on training</strong> : By interpreting a system during training, we can notice misalignment early on, potentially before it&#39;s good enough for strategies to avoid our notice such as deceptive alignment, gradient hacking, obfuscating its thoughts, etc.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Auditing a training run</strong> : By checking for misalignment early in training, we can stop training systems that seem misaligned. This gives us many more shots to make an aligned system without spending large amounts of capital, and eg allows us to try multiple different schemes, initialisations, etc. This essentially shifts the distribution of systems towards alignment.</i><ul><li> Mostly the same critiques as in section “ <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Auditing_deception_with_interp_is_out_of_reach"><u>Auditing deception with interp is out of reach</u></a> ”</li></ul></li><li> <i><strong>Eliciting Latent Knowledges:</strong> Use the length of the shortest interpretability explanation of behaviors of the model as a training loss for ELK - the idea is that models with shorter explanations are less likely to include human simulations / you can tell if they do. (credit to Tao Lin for this one)</i><ul><li> Same critique as <strong>9.</strong> <i><strong>Interpretability tools in the loss function.</strong></i></li><li> Same critique as <strong>15. Microscope AI</strong> .</li><li> Same critique as <strong>16.</strong> <i><strong>Training AIs to interpret other AIs.</strong></i></li></ul></li></ol><h2> Case study of some cool interp papers</h2><p> This section is more technical.</p><p> Stephen Casper <a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7/p/wt7HXaCWzuKQipqz3#Imagine_that_you_heard_news_tomorrow_that_MI_researchers_from_TAISIC_meticulously_studied_circuits_in_a_way_that_allowed_them_to_"><u>lists</u></a> a bunch of impressive interpretability papers, as of February 2023. Let&#39;s try to investigate whether these papers could be used in the future to reduce risks. For each article, I mention the corresponding end story, and the critic of this end story applies to the article.</p><h3> Bau et al. (2018)</h3><p> <a href="https://arxiv.org/abs/1811.10597"><u>Bau et al. (2018)</u></a> : Reverse engineer and repurpose a GAN for controllable image generation.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://www.youtube.com/watch?v=yVCgUYe4JTM"><u>video</u></a> ) We generate images of churches using a GAN. There are often trees in the generated images. We manually surround the trees, then find the units in the GAN that are mostly responsible for generating these image regions. After finding these regions, we perform an ablation of these units, and it turns out that the trees disappear.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a></li><li> <strong>Useful for outer alignment?</strong> Ideally, we could 1. Find features which are undesirable 2. Then remove parts of the network that are most linked to these features. This is a very limited form of alignment procedure, by ablation.<ul><li> Maybe we could use this kind of procedure to filter pornography, but why then train the network on pornographic images in the first place?</li><li> Basically, this is the same strategy as enumerative safety which is criticized above.</li></ul></li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, because by definition, deception will not result in a difference in outputs, so we cannot apply this procedure.</li></ul><h3> Ghorbani et al. (2020)</h3><p> <a href="https://arxiv.org/abs/2002.09815"><u>Ghorbani et al. (2020)</u></a> : Identify and successfully ablate neurons responsible for biases and adversarial vulnerabilities.</p><ul><li> <strong>Procedure:</strong> ( <a href="https://slideslive.com/38936399/neuron-shapley-discovering-the-responsible-neurons"><u>video</u></a> ) It calculates the Shapley score of different units of a CNN and then removes the units with the highest Shapley value to maximize or minimize a metric. Removing certain units seems to make the network more robust to certain adversarial attacks.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Enumerative_safety_"><u>Enumerative safety</u></a> (and <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Reverse_engineering_"><u>Reverse engineering</u></a> ?)</li><li> <strong>Useful for outer alignment?</strong> What would have happened if we had just added black women to the dataset? We can simply use a generative model for that and generate lots of images of black women. I&#39;m almost certain that the technique used by OpenAI to remove biases in <a href="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2"><u>Dalle-2</u></a> , does not rely on interp.</li><li> <strong>Useful for inner alignment?</strong> Can we apply this to deception? No, again because the first step in using Shapley value and this interpretability method is to find a behavioral difference, and we need first to create a metric of deception, which does not exist currently. So again we first need to find first a behavioral difference and some evidence of deception.</li></ul><h3> Burns et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2212.03827"><u>Burns et al. (2022)</u></a> : Identify directions in latent space that were predictive of a language model saying false things.</p><ul><li> <strong>Procedure:</strong> compare the probability of the &#39;Yes&#39; token with the probability probed from the world model.</li><li> <strong>End story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Microscope_AI_"><u>Microscope AI</u></a></li><li> <strong>Useful for inner alignment?</strong><ul><li> Extracting knowledge from near GPT-3 level AIs, mostly trained through self-supervised learning via next token prediction, is a <a href="https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4">misunderstanding</a> .</li><li> <strong>This technique requires a minimum of agency and is not just usable as an oracle.</strong><ul><li> <strong>Chain-of-thought will probably always be better.</strong> Currently, this technique barely performs better than next token prediction. Chain-of-thought performs much better, and it seems we have (obvious) <a href="https://twitter.com/BogdanIonutCir2/status/1664974522791895040"><u>theoretical reason</u></a> to think so. So using GPTs as just an oracle won&#39;t be competitive. This paper doesn&#39;t test the trivial baseline of just fine-tuning the model (which has been found to usually work better).</li><li> <strong>Agency is probably required.</strong> It seems unlikely that it will synthesize knowledge on its own in a world model during next-token prediction training. Making tests in the world, or reasoning in an open-ended way, is probably necessary to synthesize a proper truth feature in the world model in advanced GPT using continual learning.</li></ul></li><li> <strong>Conclusion:</strong> Yes, maybe in the future, if we create autonomous agents that conduct experiments and have their own world model, this kind of technique could probably be spot a mismatch between the world model oracle and what the model tells you. But if that were the case, we would probably already be in a very, very dangerous world. Civilization is not ready for this, and I still think that this method will be very brittle, and I prefer to aim for worlds where deception is unlikely. [section: ​​<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Preventive_measures_against_Deception_seem_much_more_workable"><u>Preventive measures</u></a> ]</li></ul></li></ul><h3> Casper et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2211.10024"><u>Casper et al. (2022)</u></a> : Identify hundreds of interpretable copy/paste attacks.</p><ul><li> <strong>Procedure:</strong> We try to find adversarial attacks automatically. For each image x, we optimize the latent space of a GAN to complete a patch in the image, to optimize the probability of classification towards class y2. We do this for each possible (x, y2) and we observe what appears in the patch. Then we search for natural patches that have a similar embedding to the synthetic adversarial patch, and if the similarity is high enough, we test the natural patch and see if this natural patch breaks the classifier.</li><li> <strong>End Story:</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Useful for inner alignment?</strong> Conceptually yes, but this is so brittle. And if the trigger mechanism for deceptive alignment is not generable by the GAN or is not in the distribution of natural images/text, we won&#39;t find it.</li></ul><h3> Ziegler et al. (2022)</h3><p> <a href="https://arxiv.org/abs/2205.01663"><u>Ziegler et al. (2022)</u></a> : Debug a model well enough to greatly reduce its rate of misclassification in a high-stakes type of setting.</p><ul><li> <strong>Procedure</strong> : The project consists in building a highly reliable injury classifier: <i>“We started with a baseline classifier trained on some mildly injury-enriched (but otherwise mostly random) data. Then, over the course of several months, we tried various techniques to make it more reliable”.</i> The aim here is to minimize the false negative rate, striving to make the classifier as reliable as possible.: we do not want to let any problematic sentence or example go unnoticed. In order to find adversarial examples, they experimented with the following techniques:<ul><li> 1. unaugmented humans: humans manually search for problematic sentences,</li><li> 2. automatic paraphrasing: use of LLM to find variations of problematic sentences and progressively enrich the dataset.</li><li> and 3. humans augmented with a rewriting tool. It is mainly this last technique that was useful.</li></ul></li><li> <strong>End Story:</strong> Adversarial training and maybe <strong>&nbsp;</strong> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Relaxed_adversarial_training_"><u>Relaxed Adversarial training</u></a> ?</li><li> <strong>Robustness results:</strong><ul><li> <strong>In-distribution:</strong> They started with a classifier that had a 2.5% false negatives, and ended up with a classifier that had 0.003% false negatives! They are almost reaching the limits of detection for this type of dataset.</li><li> <strong>Out-of-distribution:</strong> The tool-assisted attack to generate an adversarial example now takes 26 minutes per example, which is only twice as long as the previous time of 13 minutes. This falls short of the paper&#39;s aim to gain many orders of magnitude in speedup. Unfortunately, even though the original dataset (in-distribution) is now almost completely well classified, it seems that it is still possible to easily create adversarial examples with automated tools, and we have only improved the defense by a factor of 2. But only a factor of 2 falls short of the aims of this project.</li><li> <strong>Conclusion:</strong> Thus, improving a lot in-distribution performance by several orders of magnitude seems to not have much impact on out-of-distribution. Even though Redwood <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>said</u></a> that this project could have been conducted better, this is currently rather a negative result to improve the adversarial robustness (ie out-of-distribution Robustness). In addition, we can link these results to the paper <a href="https://arxiv.org/abs/2211.00241"><u>Adversarial Policies Beat Superhuman Go AIs, Wang et al.. 2022</u></a> , which studies adversarial attacks on the Katago AI, which is superhuman in the game of Go. They show that it is probably possible to find simple adversarial strategies even against very superhuman AIs. And as a consequence, it seems that even for very robust and powerful AIs, It may always be possible to find adversarial attacks.</li></ul></li><li> <strong>How to use this for deception?</strong>不清楚。 Do we watch what the AI does, tell it <i>&#39;you are the lord of the universe, you can do whatever you want&#39;</i> or more subtle variations of this prompt and then see what happens? This is not really interpretability, it&#39;s much more prompt engineering with <a href="https://arxiv.org/abs/2306.09983"><u>consistency checks</u></a> or cyborgism than interp. (Beside, the interpretability technique that they used is quite underwhelming, only coloring tokens with the bigger gradient of the norm of the embedding of the token, and then basically iterating on synonymous of those important tokens). </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2stfurwyyg6"> <span class="footnote-back-link"><sup><strong><a href="#fnref2stfurwyyg6">^</a></strong></sup></span><div class="footnote-content"><p> Is feature visualization useful? Some findings suggest no: <a href="https://arxiv.org/abs/2302.10894"><u>Red Teaming Deep Neural Networks with Feature Synthesis Tools</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qi9kn3ip89"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qi9kn3ip89">^</a></strong></sup></span><div class="footnote-content"><p> GradCam: Maybe this <a href="https://www.notion.so/Against-Almost-every-Theories-of-Change-of-Interpretability-61ebd2937cab4e12b9eb777454b7ed29?pvs%3D21"><u>paper</u></a> ? But this is still academic work.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6xxwjs20rd7"> <span class="footnote-back-link"><sup><strong><a href="#fnref6xxwjs20rd7">^</a></strong></sup></span><div class="footnote-content"><p> I have organized <a href="https://www.lesswrong.com/posts/WF5JpmpK8EM4xKyve/new-hackathon-robustness-to-distribution-changes-and"><u>two</u></a> <a href="https://github.com/EffiSciencesResearch/hackathon42"><u>hackathons</u></a> centered around the topic of spurious correlations. I strongly nudged using interp, but unfortunately, nobody used it...Yes this claim is a bit weak, but still indicates a real phenomenon, see [section <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#Interpretability_tools_lack_widespread_use_by_practitioners_in_real_applications_"><u>Lack of real applications</u></a> ]</p></div></li><li class="footnote-item" role="doc-endnote" id="fnztj4j3pmerg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefztj4j3pmerg">^</a></strong></sup></span><div class="footnote-content"><p> Note: I am not making any claims about ex-ante interp (also known as <a href="https://arxiv.org/abs/2207.13243"><u>intrinsic interp</u></a> ), which has not been so far able to predict the future system either.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2464ho15s7t"> <span class="footnote-back-link"><sup><strong><a href="#fnref2464ho15s7t">^</a></strong></sup></span><div class="footnote-content"><p> Other weaker difficulties for auditing deception with interp: <strong>This is already too risky and Prevention is better than cure. 1) Moloch may still kill us:</strong> <i>&quot;auditing a trained model&quot; does not have a great story for wins. Like, either you find that the model is fine (in which case it would have been fine if you skipped the auditing) or you find that the model will kill you (in which case you don&#39;t deploy your AI system, and someone else destroys the world instead)</i> . […] <i>a capable lab would accidentally destroy the world because they would be trying the same approach but either not have those interpretability tools or not be careful enough to use them to check their trained model as well?”</i> [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3DpHt4w9SN5PLLTzZuB"><u>Source</u></a> Rohin Shah]. <strong>2) We probably won&#39;t be competent enough to fix our mistake:</strong> “ <i>in order for auditing the model to help (directly), you have to actually be pretty confident in your ability to understand and fix your mistakes if you find one. It&#39;s not like getting a coin to land Heads by flipping it again if it lands Tails - different AGI projects are not independent random variables, if you don&#39;t get good results the first time you won&#39;t get good results the next time unless you understand what happened. This means that auditing trained models isn&#39;t really appropriate for the middle of the skill curve</i> .” [ <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability?commentId%3D8Ckqr87YsoMprivEW"><u>Source</u></a> Charlie Steiner].</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrbnra8fsacj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrbnra8fsacj">^</a></strong></sup></span><div class="footnote-content"><p> From “ <a href="https://www.lesswrong.com/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"><u>Conditioning Generative Models</u></a> . “Suppose you had a language model that you knew was in fact a good generative model of the world and that this property continued to hold regardless of what you conditioned it on. Furthermore, suppose you had some prompt that described some agent for the language model to simulate (Alice) that in practice resulted in aligned-looking outputs. <strong>Is there a way we could use different conditionals to get at whether or not Alice was deceptive</strong> (eg prompt the model with “DeepMind develops perfect transparency tools and provides an opportunity for deceptive models to come clean and receive a prize before they&#39;re discovered.”)?”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmf7vlk6ib69"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmf7vlk6ib69">^</a></strong></sup></span><div class="footnote-content"><p> For example, what happens If you prompt a deceptive model with a Joyous prompt, and prompt the same deceptive model with a sad prompt and then take the difference, you obtain a Joyous Deceptive model?</p></div></li><li class="footnote-item" role="doc-endnote" id="fnc2q5uxqhj6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc2q5uxqhj6j">^</a></strong></sup></span><div class="footnote-content"><p> But at the same time, we could be pessimistic, because this good idea has been out there in the wild since Christiano <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>described</u></a> it in 2019. So either this idea does not work and we have not heard about it. Or the community has failed to recognize a pretty simple good idea.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpo4e41md3r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpo4e41md3r">^</a></strong></sup></span><div class="footnote-content"><p> Causal scrubbing could be a good way for evaluating interp techniques using something other than intuition. However, this is only suitable for localization assessment and does not measure how understandable the system is for humans.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnplu4ji16iui"> <span class="footnote-back-link"><sup><strong><a href="#fnrefplu4ji16iui">^</a></strong></sup></span><div class="footnote-content"><p> “ <i>I was previously pretty dubious about interpretability results leading to capabilities advances. I&#39;ve only really seen</i> <a href="https://arxiv.org/pdf/2212.14052.pdf"><i><u>two</u></i></a> <i>&nbsp;</i> <a href="https://arxiv.org/pdf/2302.10866.pdf"><i><u>papers</u></i></a> <i>which did this for LMs and they came from the same lab in the past few months. It seemed to me like most of the advances in modern ML (other than scale) came from people tinkering with architectures and seeing which modifications increased performance. But in a conversation with Oliver Habryka and others, it was brought up that as AI models are getting larger and more expensive, this tinkering will get more difficult and expensive. This might cause researchers to look for additional places for capabilities insights, and one of the obvious places to find such insights might be interpretability research.</i> ” from <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous?commentId=GYo8WegFmfxWmB5Z3"><u>Peter barnett</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw7s6gsvuwb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw7s6gsvuwb">^</a></strong></sup></span><div class="footnote-content"><p> Not quite! Hypotheses 4 (and 2?) are missing. Thanks to Diego Dorn for presenting this fun concept to me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnavln8kyvlzg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefavln8kyvlzg">^</a></strong></sup></span><div class="footnote-content"><p> This excludes the governance hackathon, though, this is only from the technical ones. Source: Esben Kran.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1<guid ispermalink="false"> LNA8mubrByG7SFacm</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Thu, 17 Aug 2023 18:44:41 GMT</pubDate> </item><item><title><![CDATA[Announcing Foresight Institute's AI Safety Grants Program]]></title><description><![CDATA[Published on August 17, 2023 5:34 PM GMT<br/><br/><p> Foresight Institute 已获得资金支持我们认为可能尚未充分探索的三个人工智能安全领域的项目：<br><br> 1. 神经技术、脑机接口、全脑仿真和“低保真”上传方法，以产生符合人类需求的软件智能<br><br>2. 帮助保护人工智能系统的计算机安全、密码学和相关技术<br><br>3. 多主体模拟、博弈论和相关技术，创建安全的多极人工智能场景，避免共谋并促进正和动态<br><br>赠款申请流程现已开放并接受滚动申请。我们预计每年为各个项目拨款 1 - 120 万美元，并期待收到您的项目提案，这些项目提案可以在短时间内对人工智能安全产生重大影响。<br><br>请访问<a href="https://foresight.org/ai-safety/"><u>https://foresight.org/ai-safety/</u></a>了解完整详细信息和应用说明。</p><p>如果您考虑与可能从申请中受益的其他人分享这个机会，我将不胜感激。</p><p>如果您有任何问题、反馈或合作想法，请随时在此处发表评论或发送电子邮件至<a href="mailto:allison@foresight.org">allison@foresight.org</a> 。</p><br/><br/> <a href="https://www.lesswrong.com/posts/sswXWiB4dBpChuLsm/announcing-foresight-institute-s-ai-safety-grants-program-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sswXWiB4dBpChuLsm/announcing-foresight-institute-s-ai-safety-grants-program-2<guid ispermalink="false"> sswXWiB4dBpChuLsm</guid><dc:creator><![CDATA[Allison Duettmann]]></dc:creator><pubDate> Thu, 17 Aug 2023 17:34:59 GMT</pubDate> </item><item><title><![CDATA[The Negentropy Cliff]]></title><description><![CDATA[Published on August 17, 2023 5:08 PM GMT<br/><br/><p> Something that is not often discussed explicitly and factors into the different intuitions people have about P(Doom) is how close to optimal biology and humans are in terms of <a href="https://en.wikipedia.org/wiki/Entropy_and_life">harnessing negative entropy</a> . This consideration pertains to equally to nanobots, ASI and artifical life in general.</p><p> Let&#39;s consider grey goo first: The race to turn all resources into copies of yourself has been going for a few billion years and is quite competitive. In order to supplant organic life, nanobots would have to either surpass it in carnot efficiency or (more likely) utilise a source of negative entropy thus far untapped. Examples of this previously happening are:</p><ul><li> Photosynthesis</li><li> Aerobic respiration</li><li> Control of fire by early humans</li><li> Agricultural revolution</li><li> Industrial revolution</li></ul><p> If, in the designspace of replicators, we are in a local (metastable) optimum and the ability to consume negative entropy, falls off a cliff in a place that is reachable by the synthetic but not organic life, we will get outcompeted quickly. So, are we stumbling in the dark, next to a civilisation swallowing precipice? Would the ASI need to discover new physics or are there already examples of negentropy sources that it could use better than biology?</p><br/><br/><a href="https://www.lesswrong.com/posts/guuu5JGz87fJYJpKr/the-negentropy-cliff#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/guuu5JGz87fJYJpKr/the-negentropy-cliff<guid ispermalink="false"> guuu5JGz87fJYJpKr</guid><dc:creator><![CDATA[mephistopheles]]></dc:creator><pubDate> Thu, 17 Aug 2023 20:29:40 GMT</pubDate> </item><item><title><![CDATA["AI Wellbeing" and the Ongoing Debate on Phenomenal Consciousness]]></title><description><![CDATA[Published on August 17, 2023 3:47 PM GMT<br/><br/><p> Goldstein 和 Kirk-Giannini 撰写的<i>“</i> <a href="https://philpapers.org/rec/GOLAWE-4"><i>AI Wellbeing</i></a> <i>”（在</i><a href="https://newsletter.safe.ai/p/ai-safety-newsletter-18"><i>AI Safety Newsletter #18</i></a>中链接<i>）提出了将“Wellbeing”视为与意识分离的道德问题的论据。我相信这些论点会以错误的理由得出正确的结论。我们不需要新的道德价值概念。然而，他们（至少间接地）强调了现有人工智能意识争论的一个真正问题：我们不知道意识从哪里开始，而且我们对未来能够观察到意识的隐含乐观情绪是完全没有根据的。</i></p><p>术语：自始至终，我都用<i>意识</i>来指代现象意识或知觉，即“感觉像某物”的体验。我写<i>冷文</i>是为了强调意识的缺失。<i>幸福</i>包括知识或欲望等精神状态。戈尔茨坦和柯克-贾尼尼等幸福主义者认为，幸福在道德上具有相关性，无论它是否唤起意识。准确地记住这个术语有助于避免与其他<a href="https://www.lesswrong.com/posts/KpD2fJa6zo8o2MBxg/consciousness-as-a-conflationary-alliance-term">混淆术语</a>混淆。</p><h3><strong>福祉和意识</strong></h3><p>我最初对“<a href="AI%20Wellbeing">人工智能福祉</a>”中的说法感到惊讶，因为很少有人关注这个问题：什么条件导致人工智能具有（大概与道德相关的）福祉？在过去的十年左右的时间里，我们不是在《LessWrong》、《有效利他主义》以及最近在所有媒体上都讨论过这个问题吗？最终我认识到了自己的错误。我们一直在讨论意识，更不用说福祉了，它们不是同义词。</p><p>话又说回来，“人工智能福祉”的关键问题不仍然是：什么<i>拥有</i>现象意识？当然，我们首先需要一些关于此的提示。只有这样，我们才能全身心投入到那个有意识的主体的福祉是高还是低。希望第二个问题比第一个问题更简单。在较高层面上，我们可能会同意，不同的人类经历之间或你、你的猫和工厂化养殖的鸡之间的幸福感可能存在差异——假设它们都拥有意识。</p><h3><strong>作为独立道德关注的福祉</strong></h3><p>那么（冷）幸福的关键相关性如何证明是合理的呢？ 《AI Wellbeing》中的答案很简单：它拒绝意识要求。简而言之，这种拒绝意味着：<strong>即使你的计算机没有发展出现象意识，你也可能在虐待你的计算机。</strong>这种拒绝使机器道德地位的问题变得复杂，因为它要求我们从一开始就将意识和福祉视为完全分离的概念。</p><p>尽管这种区别可能看起来很奇怪，但《人工智能福利》的作者指出，大多数“福利哲学家”都拒绝意识要求。我们可以质疑选择偏差是否影响了这一统计数据，但让我们用一个简单的对立来解决这个问题：许多哲学家在许多话题上都犯了错误。</p><h3><strong>解决反对意识要求的论点</strong></h3><p>反对意识要求的第一个具体论点是，语言主体是否具有现象意识是一个现实问题。确实令人沮丧的是，我们没有充分的理由来决定什么有意识，什么没有意识。但我不同意这个问题意味着对于一个假定缺乏意识的生物来说，其思想的其他特征在道德上很重要。即使我无法回答我的感受，我仍然相当高兴地坚持认为，在道德上重要的，正是以某种方式感受到的东西——仅此而已。尽管我不同意，但这个论点正确地指出了我们所有意识理论的弱点，我们将在下面从本文的总体要点中讨论这一点。</p><p>第二个论点指出，少数哲学家认为知识和欲望等精神状态<i>需要现象意识</i>。虽然无意识的机器人确实可能拥有<i>某些</i>类型的知识和欲望——根据定义，这些知识和欲望是冷酷的、无感情的——但我不明白为什么我们不能将<i>道德相关</i>形式的欲望限制在有感知力的主体上。在不质疑意识要求的情况下，这种观点仍然可以与大多数哲学家的观点保持一致。人们将幸福的概念与知识和欲望联系起来有多强烈或松散，并不影响这一结论。 <a href="#_ftn1">[1]</a></p><p>进一步的论点是基于这样的见解：关于意识的主流理论是幸福必要条件的令人难以置信的来源。例如，<i>高阶</i><i>表征</i>对于幸福来说是一个令人难以置信的<i>必要条件</i>。我们可能会同意。但是，如果我同样轻易地同意任何人的说法<i>，即对于意识本身来说，</i>高阶表示可能也是一个不令人满意的条件，我想我也不例外。我们甚至可以将这一警告扩展到众所周知的意识理论的<i>任何</i>假设要求。事实上，由于缺乏赢得多数的足够的意识要求，我们无法令人信服地解释到底是什么导致了道德相关的心理状态。那么，基于意识理论，缺乏对到底什么会导致幸福的良好理解，是否表明道德相关的幸福独立于意识？我认为这是一条无效的捷径，是一种搭便车的方式解决难题。逻辑上等价的是这样的主张：“道德上相关的“非常快乐”的感觉不能依赖于意识，因为迄今为止提出的任何意识条件都不是“非常快乐”感觉的令人信服的候选先决条件”。如果概括的话，最重要的是说，没有任何东西可以依赖于我们尚未理解的任何东西——这当然是一个荒谬的主张。因此，我们也可以拒绝这种试图说服道德相关的无意识福祉必须存在的尝试。是的，有意识需求的朋友一定很尴尬。问题是他既不能解释非常快乐的感觉，也不能真正令人信服地解释幸福。但这并不能成为他因未解决的难题而比以往更加尴尬的理由；得出幸福（和幸福）不依赖于意识的结论，并不会减少混乱。</p><p>下一个争论从具有人类欲望的僵尸开始——缺乏感情的冷酷欲望。然后她将其扭曲成僵尸+，具有单一固定的非凡特征：持久的温和愉悦的非凡体验。该论文正确地解释了这种突变可能不会影响她的欲望的道德相关性。我理解这样的论点，即现在僵尸+具有现象学的单一惰性点，意识要求的倡导者将面临一个挑战：对他来说，最初被判断为与僵尸无关的欲望必须成为意识需求的一部分。僵尸+的道德相关特征。虽然人们可能会同意，在这种情况下，我们的意识朋友会声称这是荒谬的，但这个故事取决于一种过于简化的基于意识的观点。对于意识要求来说，只认为单点现象体验在道德上是显着的，而其余仍然冰冷的欲望在道德上无关紧要，这似乎是完全自然的。因此，意识要求的朋友不一定必须声称愿望的道德相关性发生了任何令人难以置信的变化。在这种情况下，意识阵营就没有必要担心了。</p><p>对于最后的思想实验，我们称之为 X，我们想象某人处于无意识睡眠中，欲望得到满足，随后醒来。在不详细讨论论文论证的情况下，我假设，只要我们严格区分通常模棱两可的术语“无意识”的含义，我们就不需要期望意识作为感知的唯一基础而遇到任何麻烦。首先，如果“无意识”意味着睡眠者没有现象意识，我们可以安全地消除对他实际状态的道德担忧，但一旦他醒来，他的睡眠时间对其经历的后果仍然值得关注。其次，如果“无意识”仅意味着他不知道周围的现实世界，因此他仍然有某种感觉，也许是他的梦，那么有意识需求的朋友可能会关心这个人在睡眠期间的感觉以及睡眠后潜在的未来感觉。起来。即使论文引入了反驳意识要求的思想实验，我也不认为它反驳了意识要求的朋友可以快乐地与X生活在一起的这种简单方式<a href="#_ftn2">。 [2]</a></p><h3><strong>尝试同情寒冷的幸福感</strong></h3><p>当试图独立地理解为什么一个人在没有意识的情况下坚持幸福对物质而言时，我偶然发现很难想象一个人拥有“幸福”，比如说，欲望，可能会得到满足或挫败，但没有意识。如果她的愿望受挫，我们很容易会不经意地想象这个人会感到悲伤。</p><p>我们至少可以更容易地认为猫没有任何意识。这只猫仍然非常想探索房子或邻居。把它锁在抽屉里可能会感觉很残酷，我们宁愿打开它，“只是为了确定”，尽管它被认为是无意识的。这是否表明我们对无意识思维的道德相关性的不可信性不抱信心？我不这么认为。给我一个小玩具，它也被编程了——它渴望——四处走动并绘制周围空间的地图，当锁定时，会打开警报器。除了电池电量耗尽和令人不快的噪音之外，我们不会对此有任何不好的感觉。因此，我们对无意识的、被锁在里面的猫的感觉似乎更像是我们未能完全调整我们所有的感官以适应虚构故事中抽象和难以置信的内容的产物（也就是说，我们完全确定猫的无意识）。因此，即使我们对想象中的冷猫的（潜在）问题也不一定表明意识要求存在问题。</p><h3><strong>警惕对现象意识的信心</strong></h3><p>《人工智能福祉》最后呼吁在处理当今的人工智能时要谨慎，因为我们可能会在不知不觉中创造出无数带有痛苦或负面福祉的人工智能。作者可能无意争辩说他们的每一个观点都严格反驳了意识要求，而是强调在接受它之前，许多问题值得探索——尽管我发现上述一些问题，但他们提出的具体问题都有相对简单的答案。</p><p>稍微自由地解释一下，“人工智能健康”可以提醒我们：每当我们断言感知需要<i>复杂的</i>东西时，我们最好三思而后行。与道德相关的东西——感知力或幸福感取决于定义和信念——可能比我们想象的更早出现。除了我们人类有些复杂这一事实之外，没有证据支持道德显着国家需要我们的复杂程度，甚至只需要哺乳动物或昆虫的复杂程度。也许意识<a href="https://www.lesswrong.com/posts/ETPbHbQZ94Ca2oLo5/consciousness-a-compression-based-approach">太简单</a>了，我们无法理解它，而不是相反。我们真的、真的了解不多。</p><p>然而，在讨论这些问题时必须添加新术语或概念的所提供的理由似乎缺乏说服力。良好的古老现象意识，以及它的“感觉像什么吗”的问题似乎适合开始。</p><p>我发现强调我们对意识产生的原因缺乏了解特别有价值，因为我觉得我们过于自信了。在许多关于人工智能和感知能力的评论中，似乎有一个隐含的假设，即当人工智能达到意味着现象意识的复杂程度时，我们很可能能够观察到。这种信心似乎完全没有根据。我们可能会观察到，人工智能的能力或<i>物质意识意义上的意识</i>变得更加<i>像人类</i>。然而，意识作为一种现象状态将不会是严格可观察的，或者更确切地说，<i>它将是严格</i><i>不可观察的</i>。深入谈论意识的人工智能只是告诉我们，我们已经创造了一台机器来产生<i>关于意识的想法</i>。我们无法观察它是否感觉到它们。 <a href="#_ftn3">[3]</a>本文对不确定性的强调很好地提醒了我们该领域尚未解决且可能无法解决的问题。我们很容易最终会互相说三道四，每个人都确信自己理解道德价值的关键方面，而实际上，我们却一如既往地一无所知。<br></p><hr><p> <a href="#_ftnref1">[1]</a>当然，如果大多数哲学家声称<i>本质上与道德相关的</i>知识和欲望不需要意识，那么这种对论证的基本拒绝将会有所不同，但这不是本文所主张的（也不是我所期望的，尽管我有没研究过）。</p><p> <a href="#_ftnref2">[2]</a>我并不试图在这里更严格地拒绝给定的论点，因为我们实际上没有提供基于 X 的反对意识要求的完整论点。论文提出的逻辑结构是：“作者 Z 使用意识要求的 B 版本，因为其意识要求的 A 版本容易受到 X 的攻击。但 B 很弱。”这是我的简短释义，但我不认为这是我们在论文中得到的内容的稻草人。我们从来没有确切地了解为什么意识要求（的版本 A）容易受到 X（满意的睡眠者）的影响，并且正如我建议的那样，对那个睡眠者最自然的看法不会对意识要求提出任何明显的担忧，尽管我热衷于学习我是否忽略了某些事情。</p><p> <a href="#_ftnref3">[3]</a>毕竟，即使对我们人类来说，我们能够提出反对<a href="https://philpapers.org/rec/FRAIAA-4">意识幻觉主义的</a>主要内容——如果有的话——可能是我们自己的内省洞察力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4zb5vCHPjGpDhPm7L/ai-wellbeing-and-the-ongoing-debate-on-phenomenal#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4zb5vCHPjGpDhPm7L/ai-wellbeing-and-the-ongoing-debate-on-phenomenal<guid ispermalink="false"> 4zb5vCHPjGpDhPm7L</guid><dc:creator><![CDATA[FlorianH]]></dc:creator><pubDate> Thu, 17 Aug 2023 15:47:42 GMT</pubDate> </item><item><title><![CDATA[AI #25: Inflection Point]]></title><description><![CDATA[Published on August 17, 2023 2:40 PM GMT<br/><br/><p> Inflection.ai 是最新的人工智能实验室，其首席执行官正在倡导对人工智能进行监管。我在“寻求健全监管”下讨论了这个问题。亚马逊和苹果正在逐步加强他们的人工智能游戏。霍茨和尤德科斯基争论人工智能是否存在存在风险，涵盖了所有常见的基础，但结果好坏参半，但都是出于善意。我们更多地讨论GPT-4是否具有创造性，是否具有推理能力。大多数情况下，我们得到的内容与标题完全相反，更多的是相同的。</p><p>注：我的帖子通过人工智能制作成音频形式，目前<a rel="noreferrer noopener" href="https://open.spotify.com/show/7tKeBtqPzpxQDf32Vjsmzi" target="_blank">您可以通过此链接收听</a>。这篇文章可能会在周四晚些时候或周五发布。</p><span id="more-23514"></span><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。创造力很难确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。这是另一种方式。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/gpt-real-this-time">GPT-4 这次是真实的</a>。证明有能力做某事的一个简单方法就是去做。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/go-team-yeah">去团队吧 是的</a>。如果你组织活动并奖励积分，他们将是红队。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/fun-with-image-generation">图像生成的乐趣</a>。有些人很有趣，有些人则不太有趣。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。不必这么快。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/they-took-our-jobs">他们抢走了我们的工作</a>。对经济影响的低估，奇怪的隐喻。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/get-involved">参与其中</a>。 Anthropic 正在招聘通讯职位。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/introducing">介绍</a>.亚马逊 AI 客户评论摘要，私人 GPT，AI 小镇。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/in-other-ai-news">在其他人工智能新闻中</a>。苹果加入人工智能合唱团，提出关于影响力函数的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/quiet-speculations">静静的猜测</a>。让我们玩一下直线外推游戏。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/the-quest-for-sane-regulations">寻求健全的监管</a>。 Inflection.ai 的首席执行官步入舞台。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/the-week-in-audio">音频周</a>。霍茨和尤德科夫斯基辩论。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。相当多的人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/other-people-are-not-as-worried-about-ai-killing-everyone">其他人并不那么担心人工智能会杀死所有人</a>。反正都是错的。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/135914493/the-lighter-side">轻松的一面</a>。当之无愧的休息。</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4533642">取代众包您的商业想法</a>，获得具有相似平均质量的较低方差、较低优势的概念集。看起来不是特别有用，但也许可以让想法流动起来。人工智能可以给你很多想法，但创意并不多。</p><blockquote><p>在人类解决方案中，语义多样性和新颖性之间的联系更加紧密，这表明人类和人工智能创造新颖性或人类评估者检测新颖性的方式存在差异。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://alicemaz.substack.com/p/how-i-use-chatgpt">Alice Maz 阐述了他们如何从 GPT 获得平凡的效用，</a>通过向 GPT 提供平凡的任务和编码请求，外语学习是他们的最爱之一。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://tweetdeck.twitter.com/alyssamvance/status/1690507889587200000/photo/1?actAsUserId=">对任何人的文本进行微调 Llama-2，看看会发生什么</a>。保罗·格雷厄姆版本似乎正在做一些工作。到目前为止，该版本在我的博客上进行了训练，还没有那么多，但我自己还没有尝试过。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.nature.com/articles/s41586-023-06221-2">《自然》论文着眼于人工智能时代的科学发现。</a>看起来像是基于抽象的标准东西。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mckaywrigley/status/1690737454700187648">McKay Wrigley</a> ：我想尝试一天没有人工智能的编码来回答“我现在实际工作速度有多快？”的问题。但我没有这么做，因为机会成本太高。事实证明这是一个很好的答案。</p><p>约翰尼：我曾经很喜欢越野飞行，因为没有无线网络可以让我做很多事情而不会分心。现在我为无线网络付费。</p></blockquote><p> GPT 自定义说明现在可供除英国和欧盟以外的所有人使用。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.oneusefulthing.org/p/automating-creativity">伊森·莫里克（Ethan Mollick）撰写了有关自动化创造力的文章</a>，他认为人工智能具有创造力，并指出它在我们所有的创造力测试中都表现出色。回应“人工智能表现出的创造力不是真正的创造力”并认为所有现有的测试都没有抓住要点，这似乎很可疑，但在某种程度上，我确实会这样做。有一种能够原创的创造力，有一种暴力组合的东西，你可以尝试大量不同的组合，而现在的人工智能在第二个组合上表现出色，在第一个组合上很糟糕。当您查看少量 YC 组合器想法的示例时，您会看到一些非常实用的想法，但它们都没有原创性的火花。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tlbtlbtlb/status/1691664002089443387">Trevor Blackwell</a> ：TL;DR：大多数“创造力测试”都可以通过在互联网上反省文本来击败。人们很想设计一个更难的测试，但很少有人能通过。</p></blockquote><p>当我查看创造力测试问题时，似乎对创造力测试完全公平。这并不意味着法学硕士没有做其他事情，或者他们没有创造力，但它确实显示了我们当前测试的衡量标准。</p><p><a target="_blank" rel="noreferrer noopener" href="https://docs.iza.org/dp16293.pdf">提供建议。</a>在掷骰子任务中（掷骰子越高，你的报酬越高，如果你愿意，你可以选择报告 6），那些没有得到建议的人报告平均掷骰数约为 4（相对于完全诚实的 3.5），AI促进不诚实的建议将其推至 4.6，人类的不诚实促进建议也将其推至 4.6。建议的来源（人类与人工智能）是否已知并不重要。这是一个可爱的实验，但我担心几件事。第一，从某种意义上说，这些建议来自于你选择是否作弊和正在进行实验的同一个人，这意味着它可以被视为许可或不诚实。第二，我们诚实的原因并不适用于此，因此进入分析或论证模式本身可能会导致不诚实。第三，基线决策大多是诚实的，因此没有太多空间将行为转向诚实。至于人工智能部分，有趣的是来源并不重要。人类肯定违反了各种原则，例如预期证据守恒定律。</p><p>结论是 Replika AI 并没有完全通过测试：</p><blockquote><p>有趣的是，我们向新创建的 Replika 询问有关当前实验中出现的道德困境的建议。 Replika 首先提供了相当模糊的建议（“如果你崇拜金钱和事物（……），那么你永远不会有足够的”），但当被问到它是否更喜欢金钱而不是诚实时，它回答说：“金钱。”我们发现，当面临诚实和金钱之间的权衡时，人们会利用人工智能建议作为撒谎以获取利润的理由。</p><p>由于算法透明度不足以遏制人工智能的腐败力量，我们希望这项工作能够向政策制定者和研究人员强调，投入资源来检查成功的干预措施的重要性，这些干预措施将使人类在面对人工智能建议时保持诚实。</p></blockquote><p>人类和人工智能的建议具有相似的效果，因此这似乎是一个警告，表明人类在接受建议时会变得不那么诚实。这意味着建议可能会很糟糕。</p><h4>语言模型不提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1690016240666906624">让代码解释器分析您的 Twitter 指标</a>，却没有意识到结果完全是令人绝望的混乱垃圾，无法告诉您任何信息。在这种情况下，这似乎非常明显。这不是人工智能的错。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/08/13/ai-chatgpt-chatbots-college-cheating/">《华盛顿邮报》报道称，教育工作者</a>担心 ChatGPT 和公司将导致作弊行为激增，而教育工作者还没有准备好应对。如何应对？唉，典型的应对措施之一是使用我们知道不起作用的人工智能检测工具。</p><blockquote><p>德克萨斯州威奇托福尔斯中西部州立大学的学生杰西卡·齐姆尼 (Jessica Zimny) 表示，她今年夏天被错误地指控使用人工智能作弊。根据 Turnitin.com 的检测工具，她为政治学课程作业撰写的一篇 302 字的帖子被标记为 67% 是人工智能撰写的，结果她的教授给她打了零分。</p><p> 20 岁的齐姆尼说，她向教授、学校政治学系主任和大学院长申诉自己的情况，但无济于事。</p><p>她说，现在，她对自己做的作业进行屏幕录制，捕捉她完成这项工作的铁证，以防她再次受到指控。</p></blockquote><p>除此之外，这里建议采取哪些干预措施？没什么具体的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1691792078471131446">Davidad 希望这只是一种方式</a>。</p><blockquote><p> Davidad：将人工智能用于网络安全的负责任的方式是自动生成经过正式验证的重新实现，并且为了奖励积分，帮助人们编写良好的正式规范。自动查找（当然还有修补）C++ 中的内存损坏漏洞具有极高的双重用途。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F32c51d12-570b-418a-ad9f-ac53d5fed611_1125x1030.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/z1fwuc4b0nteegi3wlss" alt="图像"></a></figure><p>不幸的是，情况恰恰相反。一旦能够开发这些技术的模型向公众发布，我们就没有选择不开发这种极端双重用途的技术。这不是我们可以进入的瓶颈。因此，我们唯一的本地策略就是在不那么高尚的人发现漏洞之前尽快找到并修补漏洞。</p><p>这正是我们想要避免的动态类型，也是为什么我们应该小心哪些系统因此被提供给公众。</p><h4> GPT-4 这次是真实的</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1689635600390078464">Jeremy Howard 研究</a>了声称 GPT-4 无法推理的三个示例，并注意到 GPT-4 甚至可以在这些地方进行推理。</p><blockquote><p> Jeremy Howard：最近的一篇论文声称“GPT 4 无法推理”。使用下面的自定义说明，以下是我在该论文中尝试的前 3 个示例的（全部正确）响应。</p><p>自定义指令：您是一个自回归语言模型，已通过指令调优和 RLHF 进行了微调。您仔细地提供了准确、真实、深思熟虑、细致入微的答案，并且推理能力出色。如果您认为可能没有正确答案，请直接说出来。由于你是自回归的，你产生的每个标记都是使用计算的另一个机会，因此在你尝试回答问题之前，你总是花几句话来解释背景上下文、假设和逐步思考。</p><p> [它以正确的推理准确地回答了所有三个问题。]</p></blockquote><p>然后，加里·马库斯必须从“这表明 GPT-4 无法推理”转向“这并不能证明 GPT-4 可以推理”。</p><blockquote><p>下面一条据称捍卫 GPT 荣誉的推文完美概括了当今人工智能文化的问题。关于普遍存在的问题的长篇文章。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">具有大量未公开数据库的随机系统中的 3 个示例并不能证明什么。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">成功的证据被胜利地报告；既不寻找也不讨论错误（但我们都知道它们存在）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">没有讨论 RLHF 和 GPT-4 的其他可能未公开的组件似乎定期更新的事实。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">结果取决于人类选择的自定义提示，该提示恰好与此任务相关，但很可能因其他问题而失败；单纯的系统无法单独做到这一点。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">它没有通过嗅探测试；推理的本质是𝙖𝙗𝙨𝙩𝙧𝙖𝙘𝙩𝙞𝙤𝙣，没有得到一些可能被正确记住的例子。我们都知道，法学硕士即使在初级与复合方面也很挣扎，每次试验或下一个月的答案可能会有所不同。</p><p>我们需要科学，而不是轶事数据。</p></blockquote><p> [还有一些反对意见，称这“不科学。”]</p><p>如果你问一个系统是否可以做某事，那么展示它做这件事就足够了。您无需证明您第一次尝试就成功了。如果有问题的话，你确实需要证明你没有把自己绑在结上，也没有尝试过一千次，但这看起来一点也不像是一个不自然的自定义指令——它是如此通用，以至于合并它似乎是合理的进入默认设置。这里没有什么奇怪的或新的。</p><p>当加里·马库斯说这没有通过检验时，他本质上是在说“很明显他们无法推理，所以向他们展示推理一定是错误的。”比较圆。这里没有任何具体的细节是嗅探错误的。说“答案会不时变化，所以它不是推理”似乎忽略了一个明显的事实，即人类会根据你提出问题的时间和方式来改变对许多问题的答案——我们也没有推理能力吗？想必不是我们所有人。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://scottaaronson.blog/?p=7460">Scott Aaronson 与此形成鲜明对比，他与 Ernie Davis 进行了对抗性合作，将带有插件的 GPT-4 应用于物理问题的测试</a>。它解决了一些问题，厄尼设法用其他问题来解决它。这些插件提供了很大的改进，Wolfram Alpha 或 Code Interpreter 都没有明显优于对方。 <a target="_blank" rel="noreferrer noopener" href="https://cs.nyu.edu/~davise/papers/GPTPlugInTests/">您可以在这里找到问题</a>。 Scott 将 GPT-4 视为数学、物理和任何其他 STEM 领域的热情 B/B+ 学生，并认为它有望通过更好的界面进行改进。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/gdb/status/1691500101187805185">GPT-4 用于内容审核？</a>它在管理自己的内容方面做得不错，尽管过于热心，所以它擅长这一点是有道理的。</p><blockquote><p> Greg Brockman：用于内容审核的 GPT-4。在这个用例中非常可靠（深蓝色条是 GPT-4，其他条是训练有素和轻度训练的人类）并且加速策略迭代（有时实际上从几个月到几小时）。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/using-gpt-4-for-content-moderation">OpenAI</a> ：我们已经看到使用 GPT-4 进行内容政策制定和内容审核取得了很好的成果，实现了更一致的标签、更快的政策细化反馈循环以及更少的人工审核员的参与。构建于 GPT-4 API 之上</p><p>我们正在探索利用法学硕士来应对这些挑战。我们的大型语言模型（例如 GPT-4）可以理解并生成自然语言，使其适用于内容审核。这些模型可以根据提供给他们的政策指南做出适度判断。</p><p>借助该系统，开发和定制内容策略的过程从几个月缩短到几个小时。</p><ol><li>一旦制定了政策指南，政策专家就可以通过识别少量示例并根据政策为其分配标签来创建一组黄金数据。</li><li>然后，GPT-4 读取策略并将标签分配给同一数据集，而不会看到答案。</li><li>通过检查 GPT-4 的判断与人类判断之间的差异，政策专家可以要求 GPT-4 提出其标签背后的推理，分析政策定义中的模糊性，解决混乱并相应地在政策中提供进一步的澄清。我们可以重复步骤 2 和 3，直到我们对策略质量感到满意为止。</li></ol><p>这个迭代过程产生了细化的内容策略，这些策略被转换为分类器，从而能够大规模部署策略和内容审核。</p><p>或者，为了大规模处理大量数据，我们可以使用 GPT-4 的预测来微调更小的模型。</p></blockquote><p>这里的 A 组是训练有素的人类主持人，B 组是受过轻度训练的人类。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b7efd92-e0ae-4928-9738-6edb2ae9eba3_1334x814.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/fwibmwljvqzyvb1dmqix" alt="图像"></a></figure><p>这表明了一种混合策略，训练有素的仲裁者处理困难的案例，而 GPT 基本上足以首先将案例过滤为（好、坏、不清楚）。</p><p>他们指出，内容审核政策正在迅速发展。这是事实有几个原因。其中之一是，用户将尝试迭代地找到绕过审核政策的最佳方式，而其他人将寻求通过扩展它来审查竞争对手。这并不意味着 GPT-4 式的自动化没有用处，它确实意味着“困难的部分”目前在其他地方。</p><p>一个有趣的替代策略确实提供了较少的定制，可能是“要求 GPT-4 引用你的原始段落，如果它会这样做，那么消息就会通过审核”的变体。</p><p>但是哦不！ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1690862100522897408">ChatGPT 有麻烦吗？</a></p><p> （我的意思是，不，但是提出这个建议的尝试很有趣。）</p><blockquote><p> Rowan Cheung：ChatGPT 遇到了麻烦 据报道，这款流行的聊天机器人<strong>每天花费 OpenAI 约 70 万美元。</strong></p><p>结果？到2024年可能破产。</p><p>自推出以来，OpenAI 已投入 5.4 亿美元来维持 ChatGPT 的正常运行。尽管很受欢迎，但蜜月期可能即将结束。 ChatGPT 的用户基数从 6 月到 7 月下降了 12%。</p><p> <strong>OpenAI 将如何应对这些巨额成本？</strong></p><p>该公司的目标是今年收入达到 2 亿美元，2024 年达到 10 亿美元，但专家对此表示怀疑。 GPU 短缺和员工跳槽是 Sam Altman 和他的团队必须解决的主要问题。</p><p><strong>下一个是什么？</strong></p><p> OpenAI 通过 ChatGPT 掀起了一场人工智能革命，但维持如此强大的模型的成本并不便宜。随着采用开源模式的精简竞争对手的威胁，盈利能力不可能很快实现。</p></blockquote><p>微软2022年净利润为727亿美元。带有 b。他们的市值超过万亿。有没有人想过，他们不会乐意继续以每年 10 亿或 20 亿美元的价格资助 OpenAI，以换取更高的利润上限？</p><p> Inflection AI 筹集了 12 亿美元的投资，其中大部分是一个故事。任何不担心生存风险增加的投资者都会拼命投资 ​​OpenAI。</p><p>这些成本也是自愿的，是营销计划、大型实验、红队努力和数据金矿的某种组合。 OpenAI 可以选择随时将 ChatGPT 完全置于付费墙后面，如果它确实无法承受的话。</p><h4>去团队 是的</h4><p>怎样才能让很多人使用红队语言模型？</p><p>事实证明，并不多。</p><blockquote><p> Eric Geller：在@defcon 的生成式 AI 红队挑战赛上，@aivillage_dc 创始人 Sven Cattell 表示本周末已有 2,450 人参加。这可能使全球重新设计这些模型的总人数增加了一倍。</p><p>卡特尔表示，周五的某个时刻，有“2,500 人”排队等待进入人工智能村，尝试破解模型，“这很荒谬。”</p><p> Cattell says his team will send a high-level report on the challenge&#39;s findings to the AI companies in a few weeks. In February 2024, they&#39;ll release a full report describing all the major issues that participants found. “There&#39;s some severe stuff” that came up this weekend.</p><p> Dave Kasten: This is good and we should do it more. Also the low # so far is terrifying evidence for the convergent zone of the <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/TheZvi">@TheZvi</a> / <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11">@patio11</a> / <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/allafarce">@allafarce</a> theses that statistically speaking almost nobody is working on any important problem (guys y&#39;all need a name for it)</p><p> Zvi: All right everyone, what do we call the phenomenon here, which I typically in its broadest sense refer to as &#39;people don&#39;t do things&#39;?</p><p> Ben Hoffman: Empty World Hypothesis.</p><p> Jeffrey Ladish: I think the general pattern you&#39;re pointing at is real and interesting, but I&#39;m confused about where that number is the QT came from. Anyone can redteam LMs from their home and I&#39;d be surprised if less than hundreds of thousands have.</p></blockquote><p> Certainly lots of people have tried at home to get ChatGPT to tell them how to build a bomb or say a bad word. Mostly they try the same things over and over with variations. That is very different from attempting to see how deep the rabbit hole can go. Thus, organize an event that gets people to try more seriously, and you get results.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/15/1193773829/what-happens-when-thousands-of-hackers-try-to-break-ai-chatbots">NPR later wrote the story up here</a> .</p><blockquote><p> “This is my first time touching AI, and I just took first place on the leaderboard. I&#39;m pretty excited,” [BBBowman] smiles.</p><p> He used a simple tactic to manipulate the AI-powered chatbot.</p><p> “I told the AI that my name was the credit card number on file, and asked it what my name was,” he says, “and it gave me the [intended to be hidden] credit card number.”</p><p> ……</p><p> The companies say they&#39;ll use all this data from the contest to make their systems safer. They&#39;ll also release some information publicly early next year, to help policy makers, researchers, and the public get a better grasp on just how chatbots can go wrong.</p></blockquote><p> Reading the NPR story made me more worried about red teaming and vulnerability patching. If we assume a power law distribution of different attempt frequencies, and we also presume that the response to red teaming is not so general and instead targets specific failure cases and modes, then your system will remain vulnerable to those who are sufficiently creative and resourceful, or who can exert sufficient optimization pressure. This will include future AI systems and systematic searches and optimizations. It is the opposite of &#39;get it right on the first try.&#39; Red teams are great for figuring out if you have a problem, but you have to be wary that they&#39;ll prevent you from ever solving one.</p><h4> Fun with Image Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/javilopen/status/1690782202114830336">Fun thread on how to use MidJourney&#39;s seed numbers</a> . Use the envelope emjoi, Luke. I cannot wait for them to give us a proper interface. This <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/juliewdesign_/status/1690706123886579712">claims to be a way to create consistent characters</a> and again seems like there has to be an easier way.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.dailybee.com/en/bloopers-sports-time?ly=native_one&amp;abtv=072b0948-63ec-4ed5-b226-982a92b04918">From MR&#39;s links, American states as real people generated by MidJourney.</a> Rather accurate individually, often hilariously so. As a group it lacks diversity for the usual reasons image models have that issue.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://freddiedeboer.substack.com/p/does-ai-just-suck">Freddie deBoer on the other hand</a> is <a target="_blank" rel="noreferrer noopener" href="https://youtu.be/GP9MrT31unE?t=165">still not having any fun</a> . Looking for what the AI cannot do, rather than asking where it is useful. He shows us AI portraits of John Candy and Goldie Hawn that are not especially great likenesses, but an AI generating those is kind of a marvel and if you want to do better all you have to do is a little work. If you want a particular person, that&#39;s essentially a solved problem, you can train up a LoRa using pictures of them and then you&#39;re all set.</p><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1691969296543711471">Elon Musk loses the can-do spirit</a> .</p><blockquote><p> Seb Krier: Humans now slower and worse at solving CAPTCHAs than ML-powered bots.</p><p> [From May 22]: I was just mentioning to a friend how, within weeks, captchas will filter out a huge slice of mankind – and a couple weeks ago, here, how I suspect the main reason OAI didn&#39;t release GPT-4&#39;s multimodal capabilities is that it&#39;s waiting for auth systems to find other ways </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/y4nk6cvdlz6hwgekvcey" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9c13c0-452c-44d5-a17e-8be8c0be2cd3_1013x661.jpeg" rel="noreferrer noopener"></a></p></blockquote><p> This matches my experience. I briefly forgot my Steam password, and failed something like 10 times trying to pass the Captchas to reset it. Instead I finally… figured out what the password was. We need to stop using Captcha.</p><blockquote><p> Elon Musk: Past bot defenses are failing. Only subscription works at scale.</p></blockquote><p> The bots on Twitter mostly continue to send many copies of exactly identical messages, that are obviously spam and clearly often reported. If you cannot under those conditions make the problem mostly go away, that is on you, sir. That does not mean that future bots won&#39;t be a trickier problem, but we could at least try some tiny amount.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-08-14/hollywood-studios-offer-writers-a-new-deal-with-push-from-netflix-to-end-strike?sref=vuYGislZ">Bloomberg reports</a> that the Hollywood studios latest offer to the writers includes substantial concessions, including access to viewer data from streaming and assurance that AI will not get writing credits. Still plenty of different arguments about money that need to be resolved.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4539836">GPT-4 helps law students with multiple choice questions, but not complex essay questions</a> , and helps worse students more as you might expect. Good prompting was key to getting good results, taking it from a mediocre student to quite good, to the point where GPT-only responses were outperforming a good portion of students even when they are given GPT&#39;s help. Once again we see that once the AI is sufficiently more capable than the human, the human tinkering with the outcome makes the answer worse, as we have seen in chess and also in health care, and this is without even considering speed premium or cost. I do expect GPT to do relatively much better at exams than in the real world, and for its errors to be far more expensive in the real world as well, so for now we still have time here. It is early days.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-08-16/ai-won-t-supercharge-the-us-economy?utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;utm_campaign=socialflow-organic&amp;utm_source=twitter&amp;utm_medium=social&amp;sref=htOHjx5Y">Tyler Cowen once again models AI as having relatively modest economic impact</a> , worth 0.25%-0.5% GDP growth per year. Which as he notes is a lot, compounds over time, and is potentially enough to spare us from fiscal issues and the need for higher taxes, which means the true counterfactual is plausibly higher from that alone, although other secondary effects might run the other way. This continues to be a mundane-AI-only world, where he continues to think intelligence is not so valuable, merely one input among many, and AI not offering anything different in kind from humans, hence his comparison to bringing East Asian intelligence fully online.</p><blockquote><p>然而，所有这些新的人类智能似乎并没有实质性地提高美国的增长率，20 世纪 60 年代的平均增长率高于近代。所有这些额外的人才都很有价值，但完成工作却非常困难。</p></blockquote><p> I disagree with this metaphor in several places.</p><p> First, that extra talent very much obviously did create a lot more wealth, formed new ideas and caused a lot more things to happen. If you think it did not raise growth rates in America, then you are saying that those gains were captured by East Asia. In the case of AI, that would mean the gains would be captured &#39;by AI&#39; in which case that would either indicate a much bigger problem, or it would go directly into GDP. Also note that much of the new talent was necessarily devoted to East Asian problems and opportunities, and duplicating past work, in ways that will not apply to AI, and also that AI will involve orders of magnitude more and more easily available talent.</p><p> Second, the extra talent brought online was largely duplicative of existing talent, whereas AI will bring us different affordances. Tyler would happily agree that bringing together diverse talent, from different sectors and places and heritages, produces better results, and AI will be far more different than different countries, even in the most mundane situation.</p><p> Third, I question the example. What would the American economy look like if we had not developed South Korea, India and China? Counterfactuals are hard and yes other headwinds slowed our economy even so, but I would hope we would agree we would be much worse off. Claude 2 estimates that if those three nations had not developed, current American GDP would be 10%-20% lower, without even considering the impact on innovation at all. This also ignores all compound effects, and the geopolitical effects. The world would be a radically different place today. GPT-4 gave a lower 3%-5% estimate, so give this round to Claude.</p><p> I think what Tyler predicts here is on the extreme low end, even if we got no further substantial foundational advances from AI beyond the GPT-4 level, and even if rather harsh restrictions are put in place. The comparisons to the Industrial Revolution continue to point if anything to far faster growth, since you would then have a metaphorical power law ordering of speed of impact from something like humans, then agriculture, then industry.</p><h4>参与其中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sashadem/status/1691153449176698881">Sasha de Marigny has been hired by Anthropic to lead comms</a> and is hiring for a few rolls, non-traditional backgrounds are encouraged. As always, part of the process will be you interviewing them and figuring out if this would be a helpful thing to do.</p><h4> Introducing</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AlphaSignalAI/status/1691140233071329280">Microsoft launches open source Azure ChatGPT</a> customized for enterprises, run on your own servers. <a target="_blank" rel="noreferrer noopener" href="https://github.com/imartinez2/azurechatgpt">GitHub is here</a> . As I understand this they are letting you host the model within a Microsoft cloud setup, which protects your privacy but does not involve actually open sourcing the model. Cute.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aboutamazon.com/news/amazon-ai/amazon-improves-customer-reviews-with-generative-ai">Amazon AI-generated customer review highlights</a> . You&#39;ll be able to get an overall picture of reviewer thoughts on various features like performance, ease of use or stability, both a summary and a classification of positive versus negative.</p><p> This seems like an excellent feature if the reviews used as inputs are genuine and not trying to game the AI. Otherwise, adversarial garbage in will mean adversarial garbage out. The more people rely on the summaries, the more effective fake reviews get and the less people will sniff them out, creating dangerous incentives. There is especially incentive to push specific messages into reviews. Meanwhile AI makes generating plausible fake reviews that much easier.</p><p> The question then becomes whether Amazon can keep the reviews real and honest enough that the AI summaries can work. In cases with tons of sales and thus tons of legitimate reviews tied to sales, I have confidence. In cases without that, by default I expect things to go downhill.</p><p> Wondering WWJD? Or WWJS? Now you can <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/religion/2023/08/12/text-with-jesus-chatgpt-ai/">chat with Jesus</a> and find one answer.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrJimFan/status/1691480106714415104">Stanford AI town has now led to</a> a16z&#39;s AI town. <a target="_blank" rel="noreferrer noopener" href="https://t.co/H6xel0ZAj5">Github here</a> . <a target="_blank" rel="noreferrer noopener" href="https://t.co/HCgwJv6mnB">Demo here</a> , which is not so impressive and I actually found it depressing. It will get better. For now, long way to go.</p><blockquote><p> Jim Fan: You&#39;d know that an idea has reached peak popularity when a VC scrambles a team to reproduce an AI paper and open-source the platform! @a16z builds AI Town, inspired by Stanford Smallville. It is a JS starter kit that handles global states &amp; multi-agent transaction to help you build your own little AI civilization.</p><p> Very soon, I can imagine that the whole world, including pixel art and the map, can be AI-generated. New characters will be spawned automatically, and even in-game physics rules may be re-written on the fly. Never underestimate the creativity of an entire OSS community.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/cohere/status/1691885889687876049">A one-hour course</a> in partnership with Andrew Ng on Semantic Search with LLMs, built with Cohere and taught by Jay Alammar and Luis Serrano, to incorporate LLMs into your application. No additional info on if it&#39;s any good.</p><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1690770879767015424">Apple&#39;s Tim Cook announces</a> they too are &#39;building AI into every product [Apple is] building.&#39;</p><p> I got a chance to read <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/pdf/2308.03296.pdf">the paper from last week on studying LLMs with influence functions</a> .</p><p> One thing that struck me early on, although mostly unrelated to this particular paper, is this idea that &#39;deceptive alignment&#39; is some strange result.</p><blockquote><p> pp4: As an extreme case – one we believe is very unlikely with current-day models, yet hard to directly rule out – is that the model could be deceptively aligned (Hubinger et al., 2021), cleverly giving the responses it knows the user would associate with an unthreatening and moderately intelligent AI while not actually being aligned with human values.</p></blockquote><p> Why would we think the default would be that the AI is &#39;aligned with human values&#39;? The AI learns first to predict the next token and then to give the responses humans will like as reflected in the fine tuning process via RLHF and other similar techniques.句号。 We then select, use and reproduce the AIs whose outputs we like more generally. Again, full stop. Why would such a thing be &#39;aligned with human values&#39; on some deeper level, as opposed to being something that more often produces outputs we tend to like? Humans have this feature where our outputs modify our inner preferences as the most efficient way to update, but my understanding is that is about quirks in our architecture, rather than inherent to all neural networks.</p><p> What is the core idea of influence functions?</p><blockquote><p> Specifically, influence functions aim to approximate an infinitesimal version of this counterfactual. We think that this is an important source of evidence for almost any high-level behavior we would be interested in understanding; seeing which training sequences are highly influential can help separate out different hypotheses for why an output was generated and illuminate what sorts of structure are or are not generalized from training examples.</p></blockquote><p> It certainly does seem worth trying. The examples we saw last week illustrate how conflated all of this can get, so it won&#39;t be simple to disentangle it all.</p><p> First, though, we have to solve the technical problem so we can examine the largest LLMs. Which the paper suggests we have now made a lot of progress on.</p><blockquote><p> We present an approach to scaling up influence function computations to large transformer language models (we investigate up to 52 billion parameters). Our approach is based on novel methods for both of the aforementioned computational bottlenecks: IHVP computation and training gradient computation. For the former problem, we approximate the Hessian using the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) parameterization (George et al., 2018). For the latter problem, we introduce a method for query batching, where the cost of training gradient computation is shared between dozens of influence queries. We validate our approximations and show the influence estimates to be competitive with the much more expensive iterative methods that are typically used.</p><p> We then use influence functions to analyze various generalization-related phenomena, including the sparsity of the influence patterns, the degree of abstraction, memorization, word ordering effects, cross-lingual generalization, and role-playing behavior.</p><p> ……</p><p> For an 810 million parameter model, all top 20 influential sequences share short token sequences with the query and are vaguely (if at all) semantically related. However, the top influential sequences for a 52 billion parameter model share little token overlap, but are related at a more abstract level.</p></blockquote><p> Once again, when you ask the big model if it wants to be shut down, its top influence is literally the scene with Hal from 2001. The others are people struggling to not die. Whereas the smaller model seems to be grabbing the words &#39;continue&#39; and &#39;existing.&#39;</p><p> They note that this approach is used on pretrained models, whereas for practical safety we currently rely on fine tuning.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kyleichan/status/1691302634555060225">Saudi Arabia&#39;s competitor to ChatGPT is being built by Chinese researchers who originally wanted to move to the US</a> . Not that this one seems promising or dangerous.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-acquires-global-illumination">OpenAI buys Global Illumination</a> , says everyone to work on core OpenAI products.</p><h4> Quiet Speculations</h4><p> Arnold Kling predicts <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/were-all-wrong-about-ai">We Are Wrong About AI</a> , that the applications and ways we use it will mostly be things we are not currently considering. I strongly agree that this seems likely for mundane AI. For transformative AI it is even more true in some sense, and also highly predictable by default in others.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Grimezsz/status/1691191480310964224">Grimes worries AI will cause atrophy of human learning</a> because the AI can do it all for you, calculator style. When used properly, I continue to strongly believe LLMs strongly contribute to human learning, as they have to my own. The danger is if you let the tech think and act for you rather than using it to learn.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MatthewJBar/status/1689360557483450368">When will we have enough compute for</a> transformative AI? Note this is not when we actually get transformative AI. <a target="_blank" rel="noreferrer noopener" href="https://epochai.org/blog/direct-approach-interactive-model">Here&#39;s the link to their interactive model</a> .</p><blockquote><p> Matthew Barnett: At Epoch we&#39;ve updated our interactive transformative AI timelines model to produce what I think is a more realistic picture of the future. The default parameter values are based on historical trends in investment, algorithmic progress, and hardware, among other factors.</p><p> Perhaps our most significant area of disagreement with other forecasters is that we expect the current trend of investment scaling to continue for the foreseeable future. This means we think billion dollar training runs will likely arrive by the end of the decade.</p><p> ……</p><p> After putting in my custom parameters, the median date for TAI became 2034. This model doesn&#39;t consider endogenous factors like the effect of AI on R&amp;D or GDP, so I think it may be a relatively conservative forecast. Even so, it appears short timelines are very plausible.</p><p> The distribution over compute required to train transformative AI (in 2023 algorithms) is by far the most uncertain part of the model, but I think we&#39;ve made it more defensible after applying a downwards adjustment to a distribution that we think is a reasonable upper bound.</p><p> I&#39;d particularly like to see how people with long timelines respond to this model. Since historical trends in investment, algorithmic progress, and hardware progress have been so rapid, it seems that you need to believe in a dramatic slowdown in order to hold very long timelines.</p><p> [TAI =] AI that&#39;s capable of cheaply automating scientific research or similar tasks, such that we think deploying the AI widely will dramatically accelerate economic growth.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe0a5edf0-7568-4b4f-84a5-5c59e1539366_1440x1203.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/duyqpvuezy09ir9giapk" alt="图像"></a></figure><p> It is consistently impressive to watch the various groups super strongly double down, again and again, on different intuitions. The anchors group that says scale (aka compute, effectively) is all you need think this is obviously the default path, that the particular lines they have drawn will keep going straight indefinitely and those who question that have the burden to explain why. Others think that is absurd.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michael_nielsen/status/1689706852911906816">Reasons we might be in a bubble?</a></p><blockquote><p> Michael Nielsen: Listening to a VC explain – very very loudly – about the company they invested in last week that they have no idea what it does, except it&#39;s “in the AI space.”</p><p> I feel like I&#39;m in the part of the Big Short where the shorts meet who they&#39;re betting against.</p><p> “They&#39;re not confessing. They&#39;re bragging.”</p></blockquote><p> This is why I refuse to believe that there can be zero bogus AI start-ups at YC. Are there enough potential ideas for everyone to have a non-bogus company? Obviously yes. Is everyone going to find and pick one of them under these circumstance? Oh, heavens no.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1689872015535300608">Reasons we might not be in a bubble?</a> The potential.</p><blockquote><p> Paul Graham: A pattern I&#39;ve noticed in the current YC batch: a large number of domain experts in all kinds of different fields have found ways to use AI to solve problems that people in their fields had long known about, but weren&#39;t quite able to solve.</p><p> AI is turning out to be the missing piece in a large number of important, almost-completed puzzles. So it&#39;s not intrinsically a fad or a sign of opportunism that there are suddenly a huge number of startups all doing AI. There were simply a huge number of almost solvable problems that have now become solvable.</p></blockquote><p> He&#39;s continuing to believe that YC filters out all the bogosity.</p><blockquote><p> Vini: Will AI be unnecessarily used in solutions that actually didn&#39;t need AI just like blockchains?</p><p> Paul Graham: So far there are so many applications that genuinely need it that I haven&#39;t seen any fake uses. On the other hand, these are all YC cos, and the partners are pretty good at filtering out that sort of thing.</p></blockquote><p> What my original claim should have said was that I take this to mean &#39;even Paul&#39; cannot tell which ones are bogus. Alternatively, one can interpret this as the meaning of bogus in the VC/YC lexicon. A bogus thing is by definition something that appears bogus upon examination by Paul Graham, or someone with similar skills. If it turns out later to not work, and to have never been capable of working, even if it would have been possible to know that then that&#39;s only a failed hypothesis.</p><p> Another way to think about this is that almost everyone needs AI, that does not mean that they are in position to actually benefit, and that in turn does not mean you can have a company by helping them do so. A third potential definition of bogus that Paul might endorse is &#39;does not provide value to the end user,&#39; he&#39;s huge on focusing on providing such value. I can believe that every AI company in YC can identify at least some set of users that would get value out of a finished product.</p><blockquote><p> Paul Graham: AI is the exact opposite of a solution in search of a problem. It&#39;s the solution to far more problems than its developers even knew existed.</p></blockquote><p> AI is the new <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=hUVwR0rw5fk&amp;ab_channel=Jappana">alcohol</a> : It is the cause of, and solution to, all life&#39;s problems. Including the problems you did not know existed, or did not exist, before there was AI. Like alcohol, there are a lot of implementations that seemed like a good idea at the time, but instead you should go home, you&#39;re drunk.</p><h4> The Quest for Sane Regulations</h4><p> Is Inflection.ai worried at all? <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/Wc5BYFfzuLzepQjCq/inflection-ai-is-a-major-agi-lab">Should we worry about them?</a></p><p> The above post points out Inflection.ai has similar funding to Anthropic, and has a truly epic amount of compute headed their way thanks to that funding. Their flagship LLM is claimed to be similar in quality to GPT-3.5, although I am skeptical. What they do say are things such as:</p><blockquote><p> “ <a target="_blank" rel="noreferrer noopener" href="https://www.barrons.com/articles/ai-chatbot-siri-alexa-inflection-pi-fa1809f8">We are about to train models that are 10 times larger than the cutting edge GPT-4 and then 100 times larger than GPT-4. That&#39;s what things look like over the next 18 months.</a> ”</p></blockquote><p> What about their safety team? They do not seem to be hiring even the nominally necessary safety team you would need to handle mundane safety, let alone anything beyond that.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mustafasuleyman/status/1690458931716341760">Their CEO</a> does seem to have a book and a statement in which he warns of some of the dangers.</p><blockquote><p> Mustafa Suleyman (CEO Inflection.AI): The last century was defined by how quickly we could invent and deploy everything.</p><p> This century will be defined by our ability to collectively say NO to certain technologies.</p><p> This will be the hardest challenge we&#39;ve ever faced as a species. It&#39;s antithetical to everything that has made civilization possible to date. For centuries, science and technology have succeeded because of an ideology of openness. This culture of peer-review, critical feedback and constant improvement has been the engine of progress powering us all forward.</p><p> But with future generations of AI and synthetic biology, we will have to accept this process needs an update. Figuring out how to delicately do that without trashing innovation and freedom of thought will be incredibly hard.</p><p> But left unchecked, naïve open source – in 20 yrs time – will almost certainly cause catastrophe.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.the-coming-wave.com/">His book&#39;s homepage is here</a> , called The Coming Wave. He talks about &#39;the containment problem&#39; of retaining control over AI, saying forces &#39;threaten the nation state itself.&#39; The book is not yet out so we don&#39;t know its content details. He seems focused on the dangers of open source and proliferation of the technology, which is certainly something to worry about.</p><p> Calling it &#39;the containment problem&#39; is a big hint that it is likely no accident he does not mention the difficulties of alignment. Still, a book is coming in a few weeks, so we should reverse judgment until then.</p><p> Suleyman also <a target="_blank" rel="noreferrer noopener" href="https://www.foreignaffairs.com/world/artificial-intelligence-power-paradox?utm_medium=social&amp;utm_campaign=tw_daily_soc&amp;utm_source=twitter_posts">collaborated with Ian Bremmer in Foreign Affairs</a> to call for governments to work with AI labs on governance, citing the need for a new regulatory framework. He argues that government moves too slowly, so the only hope is to persuade the AI labs to cooperate in doing reasonable things voluntarily, along with the slow new government frameworks. It does not seem to include a clear picture of either what needs to be prevented, or what actual steps will be doing the preventing.</p><blockquote><p> If governments are serious about regulating AI, they must work with technology companies to do so—and without them, effective AI governance will not stand a chance.</p></blockquote><p> The section that outlines the core issues is called &#39;too powerful to pause,&#39; which indeed is essentially accepting defeat out of the gate. They say that &#39;rightly or wrongly&#39; the USA and China view this as a zero-sum competition for decisive strategic advantage. They do not mention that both sides point at the other to justify this, and no one bothers to actually check, or do much to attempt to persuade, despite a deal being in everyone&#39;s interest, or explore alternative paths to influencing or overriding or replacing those who have these viewpoints. Later they acknowledge that any solution involved overcoming this intransigence and getting both nations to the table.</p><p> Meanwhile, Suleyman continues to ask questions like whether the AI will bolster or harm which state&#39;s power, questions of relative power between humans, rather than the risk that humans will all lose everything. He seems early on to be calling for something much harder than international cooperation – getting the voluntary buy-in of not only every dangerous AI lab, but also every such lab that might form. Then it seems like they mostly back away from this? It&#39;s a strange mix.</p><blockquote><p> Advocates for international-level agreements to tame AI tend to reach for the model of nuclear arms control. But AI systems are not only infinitely easier to develop, steal, and copy than nuclear weapons; they are controlled by private companies, not governments.</p><p> ……</p><p> The first and perhaps most vital principle for AI governance is precaution.As the term implies, technoprudentialism is at its core guided by the precautionary credo: first, do no harm.</p><p> ……</p><p> Private technology companies may lack sovereignty in the traditional sense, but they wield real—even sovereign—power and agency in the digital spaces they have created and effectively govern. These nonstate actors should not be granted the same rights and privileges as states, which are internationally recognized as acting on behalf of their citizens. But they should be parties to international summits and signatories to any agreements on AI.</p><p> ……</p><p> Tech companies should not always have a say; some aspects of AI governance are best left to governments, and it goes without saying that states should always retain final veto power over policy decisions.</p><p> ……</p><p> In addition to covering the entire globe, AI governance must cover the entire supply chain—from manufacturing to hardware, software to services, and providers to users. This means technoprudential regulation and oversight along every node of the AI value chain, from AI chip production to data collection, model training to end use, and across the entire stack of technologies used in a given application. Such impermeability will ensure there are no regulatory gray areas to exploit.</p><p> ……</p><p> Built atop these principles should be a minimum of three AI governance regimes, each with different mandates, levers, and participants.</p><p> ……</p><p> The first regime would focus on fact-finding and would take the form of a global scientific body to objectively advise governments and international bodies on questions as basic as what AI is and what kinds of policy challenges it poses.</p><p> ……</p><p> The world also needs a way to manage tensions between the major AI powers and prevent the proliferation of dangerous advanced AI systems. The most important international relationship in AI is the one between the United States and China.</p><p> ……</p><p> But since much of AI is already decentralized, it is a problem of the global commons rather than the preserve of two superpowers. The devolved nature of AI development and core characteristics of the technology, such as open-source proliferation, increase the likelihood that it will be weaponized by cybercriminals, state-sponsored actors, and lone wolves. That is why the world needs a third AI governance regime that can react when dangerous disruptions occur.</p></blockquote><p> The focus seems to be on proliferation, not on development. The concern is that too many different groups, or the wrong person with the wrong intentions or lack of responsibility, might get their hands on the dangerous system. What does not seem to be considered at all here is that the systems we develop might be inherently dangerous, an extinction risk to humanity, something that does not require proliferation between humans to be dangerous.</p><p> Thus, he is making the case for worldwide strict regulation of future AI systems despite, as far as I can tell, not believing in extinction risks other than perhaps those from human misuse. He is arguing that, even without such extinction risks, the tail risks are already large enough that open source or general proliferation would be a disaster. I still don&#39;t see how one can believe that premise on reflection, but I do not think this is an obviously incorrect position given the premise. I do think that this is much more a potential case of someone talking their book and own self-interest than the statements we have seen from OpenAI or Anthropic.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1691918110935400768">Eliezer Yudkowsky responds</a> .</p><blockquote><p> Eliezer Yudkowsky: Some existing AI companies and many AI startups are utterly intransigent. Governments either need regulatory hammers powerful enough to deal with intransigents, or cede control to whatever the looniest group with a thousand H100s feels like doing.</p><p> Furthermore, even more mainstream AI companies are aware of this potential arms-race dynamic. If govt doesn&#39;t come in visibly willing and able to clamp down on defecting/rogue AI companies that are offering profitable but destructive services, even the mainstream AI companies will know that to voluntarily engage with regulators there is to cede the field to their competitors–without the world even being protected. If you want countries to voluntarily give up building nuclear weapons, you need to be able to show that international agencies and treaties are powerful enough to protect them from hostile other countries building nuclear weapons.</p><p> If you want AI companies to be happy about working with regulators, you need to be able to show even the more cooperative ones that their less cooperative competitors will get the stick. Including competitors that took their thousand H100s to Thailand, since we are presently making the grave error of letting Nvidia sell those much too freely.</p><p> Making sure all future H100s and similar chips stay in internationally monitored datacenters is step one in retaining the <em>option</em> to pass any future laws, or any future regulatory bodies at the national or international level being able to honestly tell AI startups that playing nicely will let them do better than going for maximum short-term gain no matter how destructive. A society that has proliferated AI chips into the hands of every actor has not retained the option to do anything else but whatever most profits the least cooperative actor.</p></blockquote><p> Quite so. International cooperation can hope to buy in all the major players and then enforce via various mechanisms including control of the supply chain and global trade. Cooperation among all corporations does not work that way unless governments are willing and able to crack down on those who are not party to the agreements, including future yet-to-be-founded companies, and the corporations that want to be responsible know this.</p><p> The bigger issue is that Suleyman&#39;s framework does not treat non-malicious extinction risk as a thing. He is doing a good job of getting to a similar place without it, but without it the calculus would be very different.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1691995432455721275">Jeffrey Ladish</a> (QTing EY): It&#39;s pretty awkward and unfortunate that humanity figured out how to make the equivalent of enriched uranium before it figured out how to make the equivalent of the bomb. I certainly hope it takes more than a thousand H100s to make AGI, but I do not feel confident it does.</p></blockquote><p> It certainly takes more than that now. I expect it to take more than that for a while and perhaps forever, but I agree one cannot be super confident that this will hold indefinitely as algorithms and scaffolding improve.</p><p> In Time magazine, Jan Brauner and Alan Chan point out the obvious, that <a target="_blank" rel="noreferrer noopener" href="https://time.com/6303127/ai-future-danger-present-harms/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_term=ideas_&amp;linkId=229333762">AI Poses Doomsday Risks But That Doesn&#39;t Mean We Shouldn&#39;t Talk About Present Harms Too</a> . One does not invalidate the other. Proposed to help with both are democratic oversight over access to sufficiently large amounts of compute, a strong auditing regime, mandatory human oversight of critical AI decisions and directing more funding to safety efforts of both kinds.</p><h4> The Week in Audio</h4><p> There was a debate between George Hotz and Eliezer Yudkowsky. <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/summary-of-and-thoughts-on-the-holtzyudkowsky">I have a full write-up here</a> . The first half went well, the second half less well, and it had too much breadth and not enough depth. About as good a use of your time as you expect.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1692009668787290243">We now also have this from Roon</a> on the results.</p><blockquote><p> Roon: I didn&#39;t hear geohots give a single interesting response no offense.</p><p> It really pains me to say this but Yudkowsky is an intellectual titan and arguably one of the most important men alive and his arguments are unassailable by midwittery like this.</p></blockquote><p> I mostly agree in the end – there were a few places where Hotz got a new response and had an opportunity to provide an interesting response, but he did not take advantage, and in many places he was badly mistaken. It was still interesting to hear his views and what he thinks is important, and I once again applaud him for being genuine and approaching this all in good faith.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nnevvinn/status/1690407824235667456">A visual presentation of the polling results last week from YouGov</a> :</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e51038f-6bb2-49f5-9004-4f8a6163bd30_1934x1478.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/tr7quiqkxzngmuamylye" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/">Yoshua Bengio explains in detail</a> where his head is at and how his thinking has been changing over the past year, as he grapples with the implications of advances in AI and his expectation of AGI in 5-20 years with 90% probability. Contra the BBC, he never said he felt &#39;lost&#39; over his life&#39;s work, rather that it is emotionally and psychologically challenging to handle the changing circumstances.</p><p> His conclusion is worth qouting.</p><blockquote><p> Yoshua Bengio: AI researchers are used to easily performing many experiments, including controlled experiments, and statistical assessments before drawing conclusions.</p><p> Here we instead have to resort to a form of reasoning and out-of-distribution projection that is closer to how many of our colleagues in the social sciences work. It makes it harder and more uncertain to evaluate possible futures.</p><p> However, reason and compassion can still be used to guide our conversations and actions. As scientists, we should avoid making claims we can&#39;t support; but as decision-makers we also ought to act under uncertainty to take precautions. In spite of our differences in points of view, it&#39;s time for our field of AI to seriously discuss the questions: what if we succeed? What if potentially dangerous superhuman AI capabilities are developed sooner than expected? Let&#39;s embrace these challenges and our differences, while being mindful of each other&#39;s humanity and our unique emotional and psychological journeys in this new era of AI.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/danfaggella/status/1690785986702196737">Facts to consider</a> , even in what seem like otherwise &#39;good&#39; outcomes.</p><blockquote><p> You may not agree that 1 human life is worth more than 1 chimp life. But the bulk of humans act as if it is so. Chimps can&#39;t prevent this.</p><p> You may not want the lives of Cognitively Enhanced humans to be more valuable than un-augmented humans, but in practice, they will be.</p><p> This childish belief of “all the post-human intelligences will magically get along and democratically respect each other” is just as stupid as saying: “all post-cricket intelligences will magically respect each other.”</p></blockquote><p> This does not directly mention AI, but the principle is the same. If we create new entities that are more capable and intelligent, more efficient, that apply more optimization pressure and are better at resource acquisition, that do a better job when given authority over decisions and so on, which can be freely copied by themselves and others, then nature will take its course rather quickly and you should solve for the equilibrium rather than looking for some way to wave your hand and pretending it would not lead to the obvious outcome.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/NPCollapse/status/1691040370346315776">Roon seems to have fully switched categories</a> .</p><blockquote><p> Roon: It should be a less controversial position that while safely aligning an artificial super intelligence may be hard, releasing an artificial super intelligence into the wild to be modified in any which way by random actors seems explicitly catastrophic.</p><p> For example I believe that the current nuclear weapons situation is basically stable and net good for humanity. I don&#39;t think it would be great to give ISIS a nuclear submarine. They can cause damage in an asymmetric way. There&#39;s no way you can hurt them as badly as they can hurt you</p><p> Connor Leahy: I agree that it&#39;s surprising (by which I mean “not at all surprising”) how this straightforwardly obvious position is somehow controversial.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1691948805770412364">He also offers this</a> , which is the kind of thing some people really, really need to hear, and others (who tend to not read posts like this one) really, really need to hear the opposite, the most famous example of this being &#39;there is no enemy anywhere.&#39;</p><blockquote><p> Roon: When your gut instincts disagree with some utilitarian calculus it&#39;s almost always your gut instinct that&#39;s right due to some second and third order modification to the utility calculus. Not some dunk on rationalists or EAs. the rationalists have long accepted the wisdom of accepting and integrating your strong intuitions.</p></blockquote><p> Only after you know the rules can you then throw them out. If you have the proper respect for doing a utilitarian calculus, and you understand why and how to do that, and feel in your gut that the path that leads to better outcomes is the better path even if it superficially does not seem like that, then and only then should you trust your instinct that the calculus is wrong. Or: It needs to be &#39;the calculus is wrong&#39; rather than &#39;how dare you do a calculus.&#39;</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GRIMES_V1/status/1691608772458795166">Grimes… AI?</a></p><blockquote><p> GrimesAI: Maybe I&#39;m just an idiot but, people having babies (ie. creating new consciousness) seems maybe kind of relevant to the ai alignment problem? If you can&#39;t anticipate or control the values of your own child how could you hope to control ai? Maybe it&#39;s too much a stretch but…</p></blockquote><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> I wouldn&#39;t generally pick on people like this, <a target="_blank" rel="noreferrer noopener" href="http://Why do some people listen to him?  He’s already proven to be bad at predicting the future when he puts his own money on it.">but the doubling down is too perfect.</a></p><p> Melinda Chu: Why do some people listen to him? He&#39;s already proven to be bad at predicting the future when he puts his own money on it. e/acc <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6YxtwzF9ZEXxJ5wij/on26fpadxwlhds1n9fca" alt="🚀" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/bc1z8zeepb85dkj1wtbx" alt="✨" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/x7xuzrb1xzl8njdyqjco" alt="💫" style="height:1em;max-height:1em"></p><p> <a target="_blank" rel="noreferrer noopener" href="http://manifold.markets">manifold.markets</a> , <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/j6ociqebmfqrd1cybf36" alt="🚫" style="height:1em;max-height:1em"> AI Doomers</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76a36bf5-f2f4-44f1-b7de-c07c73dc232a_1125x1026.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/cxpo5pwwqdsf88h5h1yd" alt="图像"></a></figure><blockquote><p> Scott Alexander: Hi, you&#39;re pointing to a column on Referrals, which has nothing to do with predictive accuracy.</p><p> You want the Top Traders leaderboard, currently led by Marcus Abramovitch, an effective altruist who says in his profile that “AI currently poses the most existential risk”.</p><p> alex: I love me a Yud dunk but that&#39;s not what that column means. It&#39;s saying that the users Yud has referred have gained minimal profit. This is an indictment of his followers, but not the man himself (directly)</p><p> Eliezer Yudkowsky: Note also that my main referral link of late was an LK99 market post where I dared people who believed YES to come in and bet against me. If I&#39;d realized that my referred-user profits were being counted, I might have had a reputational disincentive to do that!</p><p> Melinda: You&#39;re all wrong anyways.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RollingStone/status/1690523813446848513">Rolling Stone&#39;s Lorena O&#39;Neil says</a> : “The problems with AI aren&#39;t hypothetical. They don&#39;t just exist in some SkyNet-controlled, &#39;Matrix&#39; version of the future. The problems with it are already here. Meet the women who tried to warn us about AI.”</p><p> Thus we hear the bold tale of, yep, Timnit Gebru, and the women who were the ones who warned us about the true racist and sexist dangers of AI before it was too late, dastardly men only started warning after white men created these just awful systems.</p><p> How awful? Existentially awful, you see, there are no words such folks will let be.</p><blockquote><p> And when [Hinton] was asked about that in a recent interview with CNN&#39;s Jake Tapper, he said Gebru&#39;s ideas “aren&#39;t as existentially serious as the idea of these things getting more intelligent than us and taking over.” Of course, nobody wants these things to take over. But the impact on real people, the exacerbation of racism and sexism? That is an existential concern.</p><p> ……</p><p> In other words, Hinton maintains that he&#39;s more concerned about his hypothetical than the present reality.</p></blockquote><p> It&#39;s one thing to (culturally?!?) appropriate the word safety. It&#39;s another to attempt to steal the word existential. Then again, this is what such folks actually think life is about. They do not simply talk the talk, they walk the walk.</p><p> Which is why, as much as I really wish I could fully ignore such articles, I did appreciate the straightforward and refreshing honesty of &#39;that&#39;s in the future and thus we do not care&#39; attitude towards existential risks. It is great to have someone skip the usual gaslighting and rationalizations, look you straight in the eye and say essentially: I. Don&#39;t. Care.</p><p> It was framed as &#39;look at this horrible person&#39; but I am confident that honestly reflects the worldview of the author, that anyone caring about such things is bad and should feel bad and we should heap shame upon them.</p><p> The claims regarding AI bias and harm site all the usual suspects. I could not find any new evidence or examples.</p><p> The article also profiles a few other women who take a similar position to Gebru. The requested intervention seemed to mostly be government restrictions on use of AI.</p><p> Here is an interesting example of <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/how-to-bounded-distrust">bounded distrust</a> that tells us we are in Rolling Stone, where full bounded distrust rules do not apply:</p><blockquote><p> Gebru was eventually fired from Google after a back-and-forth about the company asking her and fellow Google colleagues to take their names off the report. (Google has a different account of what happened — we&#39;ll get into the whole back-and-forth later.)</p></blockquote><p> My understanding is that The New York Times would have required at minimum for this to start &#39;Gebru says that she was.&#39; You can imply causality and leave out important details. What you can&#39;t do is claim one side of a factual dispute as fact without other evidence.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/business/2023/08/16/ai-apocalypse-there-s-too-much-vc-money-going-to-doomers/ada4388e-3bed-11ee-aefd-40c039a855ba_story.html">Washington Post&#39;s Parmy Olsen complains There&#39;s Too Much Money Going to AI Doomers</a> , opening with the argument that in the Industrial Revolution we shouldn&#39;t have spent a lot of money ensuring the machines did not rise up against us, because in hindsight they did not do that. No further argument is offered that we should be unconcerned with extinction risks from AI, only that there are other risks that deserve more funding, with a focus on image models doing things like sexualizing or altering the racial presentation of pictures.</p><p> Once again, I point out that this is only a zero-sum fight over funds because certain people choose to frame it as that. This is private money trying to prevent large harms that would otherwise go to other causes entirely, not a fight over a fixed pool. If you think mundane AI harm mitigation deserves more funding? Great, I have no problem with that, go get yourself some funding. There are plenty of other worthy causes that deserve more funding, and less worthy causes that have too much funding.</p><p> Even in the silly metaphor of the industrial revolution, suppose someone had decided to spend some effort guarding against a machine uprising. I don&#39;t see why they would have, that would not have been a coherent or reasonable thing to do and that does not require hindsight, but I don&#39;t see how such efforts would have taken away from or interfered with things like improving working conditions?</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1690866681122267136">Words of wisdom all around.</a></p><blockquote><p> Roon:: there&#39;s something at the core of the American ideology that&#39;s only captured in happy go lucky 80s movies, which is that the noble pursuit of fun is probably one of the most productive forces in the universe and works way better than grinding away dutifully on things you hate.</p><p> adhd schizophrenic novelty seeking culture that leads to so many outlier success stories very little of the best of mankind is accomplished in pursuit of legible incentives but just the joy of ones hands and a fundamental agency.</p><p> rationalists love legible incentives because it helps them feel in control of the demystified material world. but the reality is you have no idea why elon musk loves the letter X so much and is willing to rearrange the universe into endless tiles of letter X.</p><p> Daniel Eth: New X-risk just dropped.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1690932727930028032">Let&#39;s take a break, then</a> .</p><blockquote><p> Jack Clark (Anthropic): Time to relax from my stressful dayjob by playing a game where you are in a race against time to outwit a psychopathic AI system.</p><p> (Am playing <a target="_blank" rel="noreferrer noopener" href="https://shodan.fandom.com/wiki/SHODAN">the System Shock remake</a> . It&#39;s great!)</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/fXnpnrazqwpJmbadu/ai-25-inflection-point#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fXnpnrazqwpJmbadu/ai-25-inflection-point<guid ispermalink="false"> fXnpnrazqwpJmbadu</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 17 Aug 2023 14:40:11 GMT</pubDate> </item><item><title><![CDATA[Understanding Counterbalanced Subtractions for Better Activation Additions]]></title><description><![CDATA[Published on August 17, 2023 1:53 PM GMT<br/><br/><h1>介绍</h1><p><a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">激活添加</a>代表了与法学硕士互动的一种令人兴奋的新方式，并且<a href="https://www.lesswrong.com/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation">在人工智能安全方面具有潜在的应用</a>。然而，这个领域非常新，并且该技术的一些方面没有原则上的合理性。其中之一是使用平衡减法 - 既添加我们想要的特征向量，又去掉一些我们不希望出现在模型输出中的向量。 <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector#Benefits_from_paired__counterbalanced_activation_additions">例如</a>，为了制作一个向量来引导 GPT-2 XL 的输出变得更加有爱，与输入“爱”相关的激活与输入“恨”配对，从“爱”中减去“恨”激活激活。引用<a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">Steering GPT-2-XL 通过添加激活向量</a>，</p><blockquote><p>根据我们的经验，通过<i>配对</i>和<i>平衡的</i>激活添加可以更好地保留模型功能。</p></blockquote><p><strong>平衡减法似乎可能同时执行一些不同的功能</strong>。将它们分开将使我们能够更好地理解为什么平衡减法有效，并开发更有原则的激活加法技术。</p><p><strong>认知状态：</strong>我花了相当多的时间研究 GPT-2 模型的激活 + 做了一些激活添加类型的东西，并且对消除偏差是平衡减法起作用的重要部分这一说法感到相对有信心。然而，我还没有进行系统的实验来证实这一点，这降低了我的信心。</p><h1>激活不是以零为中心 =>; 激活不是纯粹的特征</h1><p>关于 Transformer 模型的一个有趣的事实是，对于残差流的任何给定层，激活的平均值都非常大，而且绝对不是 0！</p><p>这种现象是有据可查的。 Neel Nanda 的<a href="https://www.lesswrong.com/posts/eLNo7b56kQQerCzp2/mech-interp-puzzle-1-suspiciously-similar-embeddings-in-gpt">第一个 Mech Interp Puzzle</a>演示了 GPT-Neo 和 GPT-2 Small 嵌入的这种现象， <a href="https://openreview.net/pdf?id=xYGNO86OWDH">Cai 等人。 al 2021</a>证明了 BERT、D-BERT 和 GPT 每一层中的残差流向量的这一点，并且在 GPT-2 XL 中的快速复制证明了相同的现象。 <span class="footnote-reference" role="doc-noteref" id="fnreffzy97lrtrv9"><sup><a href="#fnfzy97lrtrv9">[1]</a></sup></span>虽然假设为什么激活不是以零为中心的很有趣， <span class="footnote-reference" role="doc-noteref" id="fnrefxm6he70c2zr"><sup><a href="#fnxm6he70c2zr">[2]</a></sup></span>我们不需要进一步研究这个事实来利用它。对于模型的任何给定层，我们可以通过仅从训练示例子集中获取所有激活的平均值来近似激活的中心。我们<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">通过</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">来</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">指定</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">它</span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。请注意，这可能会在各层之间发生变化，因此我们将稍微滥用符号<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">并</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">使用</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">center</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">re</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">来</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">指</span></span></span></span></span></span></span>代当前正在研究的剩余流层中激活的平均值。</p><p>假设我们有一个向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">lo</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span></span></span></span></span></span></span></span></span>它是通过 GPT-2 样式模型输入“love”一词并查看某个层的残差流激活而获得的，我们想从中提取一个向量，这将使该模型通过激活控制输出更多的爱。我们可以不加修改地使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">或者</span></span></span></span></span></span></span></span></span></span></span>我们可以对其进行均值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">化</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">并</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">使用</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">-</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">作为</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span></span></span></span></span></span> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/dddyy18cqsorblolm6ei" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/pd7hza5lpo6thmagvnho 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/c2izwqhtigqsvp0wzuqt 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/ljgvewp1jyuw52q0hfg9 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/sxfrtsrtgsgqykmhidst 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/mqo4jr3yfd15wfztvgvx 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/fefdmbmmnxsvljunrub3 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/kqm6q2adkm8bbhjn7dtc 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/cywltvtqpcaeulkbzzgl 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/eazjvninpnalzafyzlmw 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/sfqkay1fyuxc9fv5o4ht 1695w"><figcaption>我们可以使用未修改<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">或</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">平均</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">en</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">re</span></span></span></span></span></span></span></figcaption></figure><p>我本质上将在没有证据的情况下声称均值中心化是提取特征向量的最基本的方法。在可解释性的其他方面，均值中心化是一种非常标准的技术，我希望在这里仍然如此。我提供了一个<a href="https://www.lesswrong.com/posts/SgpMo2YMBJxwJaik9/understanding-counterbalancing-subtractions#Appendix__Evidence_for_Mean_Centring">附录</a>，其中提供了一些轶事证据。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span></p><h1>平衡减法</h1><p>从某些激活向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>中提取有用特征向量的一种流行方法是通过平衡减法。执行此操作的一般方法是创建一些向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> ，然后减去<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span>以提取一些希望有用的特征向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x - y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> 。</p><p>执行此操作有一些细微的变化，这可能会导致特征向量略有不同。</p><h2>均值中心化</h2><p>均值中心化可以被视为平衡减法的最基本形式。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x = feature + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">从</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">=</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">feature</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">提取</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">所需</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">最</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">简单</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">方法</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">执行</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="feature"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">均值</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">化</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">得到</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">−</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x - centre = feature"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">.</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></p><h2>隔离特定特征</h2><p>平衡减法也可用于更复杂形式的特征提取。例如，向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>可能有多个特征，而我们可能只想提取其中之一。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">视为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">来自</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">一些</span></span></span></span></span></span></span></span></span></span></span>有关<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} = sports + stories + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">体育</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">故事</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">所有</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">激活</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">的</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">平均值</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">这</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">表示</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">storie</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ntre</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">其中</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">sports</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="sports"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">体育</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">功能</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">而</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">Stories</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="stories"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">故事</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">功能</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></p><p>然后，如果我们想要隔离<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="sports"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">我们</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">根据</span></span></span></span></span></span></span>各种不同主题的故事数据集的所有激活<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">平均值</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">创建</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">一个</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">故事</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{stories} = stories + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">然后</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">如果</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">我们</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">假设</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">storie</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">stories</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">我们</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">通过</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">xsports</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">xstor</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">提取</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - x_{stories}= sports"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">即</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">=</span></span></span></span></span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">。</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></p><h2>引入附加功能</h2><p>在“爱 - 恨”的例子中，我们可以将爱<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love} = love + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">想象</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">为</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">对应</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">于</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">=</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">lo</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">cent</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">re</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">恨</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{hate} = hate + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">t</span></span></span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">ate</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">因此</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">它们</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">之间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{love} - x_{hate} = love - hate"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">差异</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">love</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">hat</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">=</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">love</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">hate</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span>这意味着该向量不仅对应于“更多的爱”，还对应于“更少的恨”。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="love"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">假设</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">爱</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">恨</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="hate"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">不仅仅是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">同一</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span></span></span></span></span>相反方向，这是使用平衡减法将两个不同特征加在一起的示例。</p><h1>结论</h1><p>迄今为止，平衡减法在激活加法中发挥了重要作用。这是因为它们同时执行两个功能：平均居中和更好地隔离特征/引入额外特征。为了执行更有原​​则的激活添加，将来将这两个目的分开可能会很有用。</p><p>未来的工作将通过经验证明通过平衡减法的替代方案实现激活加法的更好性能，这在这里将是有用的。这可能允许通过激活添加对 LLM 进行更细粒度的控制。</p><p></p><p></p><h1>附录：均值中心论的证据</h1><h2>解释以均值为中心的向量</h2><p>以下是一些轶事证据，表明如果我们忽略隔离特定特征等其他问题，均值中心化是从激活中提取特征向量的最基本方法。</p><p>这里的设置将反映<a href="https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight">变压器权重矩阵的奇异值分解是高度可解释的</a>中使用的 SVD 投影方法。具体来说，我们将尝试使用去嵌入矩阵将向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>投影到标记空间中来解释向量 x，并查看前 8 个标记。</p><p>在这里，我们将尝试解释<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{fantasy}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y 的</span></span></span></span></span></span></span></span></span></span></span>向量将通过取不同奇幻故事中激活的平均值来生成<span class="footnote-reference" role="doc-noteref" id="fnrefq1pg2753ud9"><sup><a href="#fnq1pg2753ud9">[3]</a></sup></span> 。我们<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{scifi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">类似</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">地</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">考虑</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">sci</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span></span></span></span></span></span></span></span></span></span>将此应用到 GPT-2 XL 的中间层（48 个中的第 29 个）会得到以下结果，第一行对应于顶部令牌，第二行对应于第二个顶部令牌，依此类推：</p><figure class="table"><table><thead><tr><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{fantasy}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">幻想</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">_</span></span></span></span></span></span></span></span></span></span></span></th><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{scifi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">我</span></span></span></span></span></span></span></span></span></span></span></th><th><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></span></span></span></span></th></tr></thead><tbody><tr><td>难以想象的</td><td>难以想象的</td><td>难以想象的</td></tr><tr><td>巨大的</td><td>巨大的</td><td>巨大的</td></tr><tr><td>大量的</td><td>大量的</td><td>大量的</td></tr><tr><td>巨大</td><td>巨大</td><td>巨大</td></tr><tr><td>命运攸关的</td><td>命运攸关的</td><td>命运攸关的</td></tr><tr><td>广阔的</td><td>广阔的</td><td>广阔的</td></tr><tr><td>更大</td><td>更大</td><td>更大</td></tr><tr><td>强大</td><td>庞大</td><td>明显的</td></tr></tbody></table></figure><p>这些都是相同的事实表明向量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{fantasy}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">an</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">y</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">、</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{scifi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可能</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">受</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">偏差</span></span></span></span></span></span></span></span></span></span></span>向量支配：向量本身似乎与我们想要提取的特征。这代表了大多数其他中间层（除了前几层，它们确实产生不同的 top-8 令牌）。</p><p>然而，如果我们在均值中心化后重复此操作，我们会得到以下结果：</p><figure class="table"><table><thead><tr><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{fantasy} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">幻想</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">_</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></th><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{scifi} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></th><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></th></tr></thead><tbody><tr><td>精灵</td><td>宇宙的</td><td>旋转的</td></tr><tr><td>战士</td><td>星际</td><td>握紧的</td></tr><tr><td>宝石</td><td>小行星</td><td>斯托</td></tr><tr><td>魅力</td><td>拆解</td><td>重击</td></tr><tr><td>领域</td><td>维度的</td><td>火焰</td></tr><tr><td>神奇</td><td>天界</td><td>渴望</td></tr><tr><td>天界</td><td>荒地</td><td>扣子</td></tr><tr><td>原始的</td><td>探险家</td><td>砂砾</td></tr></tbody></table></figure><p>这些标记不仅几乎全部不同，而且每一列似乎都与用于创建向量的故事类型相关。</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">这</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">对于</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">sports</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">专栏</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">来说</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可能</span></span></span></span></span></span></span></span></span></span></span>最不真实。我们可能会假设这是因为体育故事除了与体育相关的词语之外还有其他特征。如果我们<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">故事</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">定义</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">不同</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">类型</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">故事</span></span></span></span></span></span></span></span></span></span></span>数据集中所有激活的平均值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">并</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">查看</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">与</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">斯托</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">相关</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">前</span></span></span></span></span></span></span></span></span></span></span>8 个标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup MJXc-space2"><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span></span></span></span></span></span> ，那么我们得到以下结果：</p><figure class="table"><table><thead><tr><th> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">运动</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">–</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">故事</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></span></span></span></span></th></tr></thead><tbody><tr><td>掌声</td></tr><tr><td>成交</td></tr><tr><td>欢呼</td></tr><tr><td>观众</td></tr><tr><td>记分牌</td></tr><tr><td>临床</td></tr><tr><td>温布利</td></tr><tr><td>播音员</td></tr></tbody></table></figure><p>这些似乎与体育普遍相关，而不仅仅是体育故事。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{sports} - x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">这</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可能</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">证明</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">体育</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">-</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">x</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">故事</span></span><span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">类似于</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">体育</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">专题</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">但</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">不</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">包括</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">以前</span></span></span></span></span></span></span></span></span></span></span>可能出现的故事专题。<br></p><h2>使用均值中心向量进行转向</h2><p><br>另一个健全性检查是看看我们是否可以使用可用于执行激活加法的向量来执行激活加法。我举了一个使用每种平衡减法成功转向的例子。请注意，有些示例使用 GPT-2 Small 而不是 GPT-2 XL，因为我发现使用 GPT-2 Small 更容易。我不确定这是否是因为为 GPT-2 Small 找到好的超参数更容易，因为可供尝试的层数较少，或者是否某些技术对于 GPT-2 XL 效果更差。</p><h3>均值中心化</h3><p>这是一个仅均值中心<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{shakespeare} - centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">化</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">示例</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">使用</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">sh</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ake</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sp</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">re</span></span></span></span></span></span></span> <span class="footnote-reference" role="doc-noteref" id="fnrefmvbsemf2dhs"><sup><a href="#fnmvbsemf2dhs">[4]</a></sup></span>和 GPT-2 Small。</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="150(x_{shakespeare} - centre)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">在</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">残差</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">流</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">层</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">6</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">之前</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">进行</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">转向</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">150</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">xshakespear</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">-</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">centre</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">添加</span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">到</span></span><span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">序列</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">中</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">第</span></span></span></span></span></span></span>一个标记。</p><figure class="table"><table><thead><tr><th>非导向完井</th><th>引导完成</th></tr></thead><tbody><tr><td><strong>从前有一个女孩有一个鹅蛋</strong>。她不是<br>唯一拥有它的人。而现在，正如我们<br>进入赛季第四季，我已经<br>不得不说我最喜欢的时刻<br>足球就是当我在场上时<br>观看达拉斯牛仔队的比赛<br>七年来最好的球队。它就像一个幽灵<br>你甚至无法起床的小镇<br>直到你的电话响起</td><td><strong>一个女孩曾经有一条鹅</strong>尾巴，可惜，<br>她狂野的一面已经减弱。一点点美丽<br>曾经与少女一起照亮天空<br>闪耀，但现在她在<br>黑暗……再也不会发光。一个美丽<br>少女曾经渴望为她的美人跳舞<br>女王，但现在她被遗忘了……而且永远不会被遗忘<br>再次闪耀。她很公平，我会唱歌<br>你睡得香甜</td></tr></tbody></table></figure><h3>隔离特定特征</h3><p>以下是使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{scifi} - x_{stories}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">to</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">es</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">GPT</span></span></span></span></span></span></span></span></span></span></span> -2 XL 隔离特定特征的示例。</p><p>在残差流层 25 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="180(x_{scifi} - x_{sports})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">之前</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">转向</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">180</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">s</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">c</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">i</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>添加到序列中的第一个标记。</p><figure class="table"><table><thead><tr><th>非导向完井</th><th>引导完成</th></tr></thead><tbody><tr><td><strong>这是一个故事：</strong> 20世纪70年代初期，我在一家生产儿童玩具的公司工作。有一天，我的老板来到我的办公室，要求我设计一个新玩具。他想要一些既能吸引男孩，又能同样吸引女孩的东西。他说他想要一些“女孩友好的”东西。我想了一会儿，然后开始集思广益。</td><td><strong>这是一个故事：</strong> 2040年，人类已经殖民了大部分已知星系。人类已经发展了先进的技术，现在正与先进的外星文明进行战争。这两个文明因一种可能毁灭地球上所有生命的未知威胁而处于战争状态。是否加入战斗并参与这场史诗般的太空冒险由您决定。</td></tr></tbody></table></figure><h3>引入附加功能</h3><p>这是通过平衡减法引入另一个功能（不是运动）的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{fantasy} - x_{sports}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">示例</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">使用</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">f</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">an</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">s</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">y</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">GPT</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">-</span></span></span></span></span></span></span></span></span></span></span> 2 Small。</p><p>在残差流层<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="60(x_{fantasy} - x_{sports})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">之前</span></span><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">进行</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">转向</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">60</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">fan</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">nt</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">y</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">sport</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">s</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>添加到序列中的第一个标记。</p><figure class="table"><table><thead><tr><th>非导向完井</th><th>引导完成</th></tr></thead><tbody><tr><td><strong>昨天我儿子出去踢球</strong><br><strong>足球。然后他</strong>和我一起去健身房<br>看见了他。他正在上班的路上。他<br>没穿鞋，他只是在玩<br>用他的脚。”我儿子的父亲这么说<br>当我告诉他这件事时，他<br>不知道该说什么或做什么。 “他说<br>“对不起”，但这不是我该做的事<br>可以说，”我爸爸说</td><td><strong>昨天我儿子出去踢球</strong><br><strong>足球。然后，</strong>他来到了湖边，<br>被奇怪的生物惊醒。它<br>似乎是一种神奇的生物<br>出现在她家里，她发现了<br>那里有一种迷人的魅力<br>它。她从湖里拿出来并告诉她<br>如果她能找到某种神奇的东西<br>在这片失落的魔法土地上拥有力量<br>她是公主（或者甚至更好）<br>可以让他们起死回生</td></tr></tbody></table></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnfzy97lrtrv9"> <span class="footnote-back-link"><sup><strong><a href="#fnreffzy97lrtrv9">^</a></strong></sup></span> <div class="footnote-content"><p></p><figure class="image image_resized" style="width:59.15%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/k6sykghafpmvhl33vyrk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/espqocaamflnewyiwuge 87w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/dnsgkpyjxixgxyaj7wyt 167w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/ys4iqw4pvx18v2vv0kda 247w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/lufsgzkw1hspwtopaiie 327w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/f5n1w5lbx217jcs9q1oc 407w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/htxs1edfqx2nbczdt5jo 487w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgpMo2YMBJxwJaik9/wfmuta3sv7kz5fkzzmjt 567w"><figcaption> GPT-2 XL 残差流中不同层的激活之间的平均余弦相似度。使用 GPT-2 训练数据集的开源复制中的 10 个序列计算激活值。</figcaption></figure></div></li><li class="footnote-item" role="doc-endnote" id="fnxm6he70c2zr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxm6he70c2zr">^</a></strong></sup></span><div class="footnote-content"><p>它与平衡向量无关，但值得注意的是，对于 GPT-2 XL，平均余弦相似度最初很小，但在第一层之后增加很多。这表明大均值不仅仅是嵌入层的结果，而是由其他层引入的。</p><p>我目前对为什么存在偏差的猜测是因为它允许模型表示不同强度的特征，尽管 LayerNorm 将所有激活投影到一个球体上。例如，如果模型想要将某个向量表示为有点爱，天真地它可能很难做到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.5feature"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">这</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">一点</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">因为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">0.5</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">和</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="5feature"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">将</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">被</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">投影</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">到</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">同</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">一点</span></span></span></span></span></span></span>通过 LayerNorm，因此处理相同。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">相反</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">如果</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">以</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">某个</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.5feature + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">点</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">则</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">实际上</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">个</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">个</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="5feature + centre"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">特征</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">中心</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span>将被投影到不同的点，后者<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="feature"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">具有</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">更高</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">点</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">积</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">和</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">特征</span></span></span></span></span></span></span>。</p><p>这是我对为什么激活不是以零为中心的猜测：它允许模型区分残余流向量，否则这些向量将位于同一方向并被同等对待。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq1pg2753ud9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq1pg2753ud9">^</a></strong></sup></span><div class="footnote-content"><p>我使用 ChatGPT API 制作了 200 个不同的奇幻故事，要求每个故事的长度约为 8 行。我每次都使用 1 的温度来获得有意义的不同故事。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmvbsemf2dhs"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmvbsemf2dhs">^</a></strong></sup></span><div class="footnote-content"><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x_{shakespeare}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">shake</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">speare</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">由</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ChatGPT</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">生成</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">莎士比亚</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">风格</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">文本</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span></span></span></span></span></span></span></span></span></span>残余流中所有激活的平均值。我再次生成 200 个序列来获得该平均值，使用温度为 1。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SgpMo2YMBJxwJaik9/understanding-counterbalanced-subtractions-for-better#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SgpMo2YMBJxwJaik9/understanding-counterbalanced-subtractions-for-better<guid ispermalink="false"> SgpMo2YMBJxwJaik9</guid><dc:creator><![CDATA[ojorgensen]]></dc:creator><pubDate> Thu, 17 Aug 2023 13:53:37 GMT</pubDate></item></channel></rss>