<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 21 日星期六 12:20:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[How to find a good moving service]]></title><description><![CDATA[Published on October 21, 2023 4:59 AM GMT<br/><br/><p>两周前，我意识到我将在 50 天内从西雅图搬到山景城。我很兴奋，但没有意识到我需要与超过 15 家搬家公司交谈才能找到满意的公司。经过大约9个小时的寻找，我终于找到了满意的。我选择<a href="https://quotes.northamerican.com/">北美搬家服务</a>来搬运 850 英里、2000 磅、300 立方英尺 (CF) 的货物，价格为 3200 美元。这是我学到的东西（从最重要的到最不重要的）：</p><ol><li><strong>选择承运人（移动者）而不是经纪人</strong>：即使一开始听起来更便宜，经纪人最终的成本可能会更高。支付定金后，事情可能会失控。我在网上发现很多关于经纪人在搬家前几天收取额外费用的故事，这样你别无选择，只能付给他们额外的钱。有些故事还说，当搬家公司来的时候，他们会向你收取额外的费用，因为他们会说他们不同意经纪人的价格……选择一家搬家公司会更便宜，心理上也更安全。<ol><li>如何查明对方是经纪人还是搬运工？询问他们的 USDOT 号码并使用以下<a href="https://safer.fmcsa.dot.gov/query.asp?searchtype=ANY&amp;query_type=queryCarrierSnapshot&amp;query_param=USDOT&amp;query_string=3475743">网站</a>进行搜索。实体类型应为“承运人”而不是“经纪商”。通常，对于搬家公司来说，他们的网站上应该有 USDOT 号码。</li></ol></li><li><strong>不要相信谷歌搜索结果</strong>：“长途搬家监管很差”，最大的搬家公司之一的代理人告诉我。当我在谷歌搜索中搜索“最佳搬家公司”时，大多数都是经纪人。他们只需支付大量广告费即可进入热门位置，以便人们点击。这是合理的，因为他们“提供”的每项服务可以轻松赚取>;1k$。此外，谷歌的评论也可能是假的。不要依赖谷歌作为唯一的事实来源。<ol><li>该信任谁？<ol><li> <a href="https://www.movingscam.com">https://www.movi​​ngscam.com</a> ：在创建此网站之前被骗的人。具体来说，这个<a href="https://www.movingscam.com/superlist">超级列表</a>列出了不同州的优质搬家服务。</li><li> https: <a href="https://www.movingauthority.com/largest-moving-companies/">//www.movi​​ngauthority.com/largest-moving-companies/</a> ：公司越大，他们就越不愿意失去信誉。</li><li> <a href="https://ai.fmcsa.dot.gov/hhg">https://ai.fmcsa.dot.gov/hhg</a> ：查看搬家公司被起诉的次数。</li><li> <a href="https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist">https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist</a> ：检查保险是否合法。</li></ol></li></ol></li><li><strong>注意以下不良迹象</strong>：<ol><li>他们不会估计多少重量和多少立方英尺 (CF)：我采访过的一位经纪人 iMoving，甚至没有询问我的移动重量详细信息。相比之下，搬家公司会与您进行虚拟参观或亲自参观，以便在给出报价之前进行估算。</li><li>代理不耐烦、粗鲁或给您打电话太频繁：再说一遍，iMoving，代理埃文很粗鲁，让我在整个通话过程中感到不舒服。许多经纪人实在太麻烦了，每天都会给您打电话/发电子邮件。相比之下，我谈过的大多数搬家工人都很耐心、专业、友善，并且站在你的角度思考。</li><li>好得令人难以置信：非常低的价格或非常快的运输。代理人可以向你承诺任何事情让你存钱，然后规则就可以改变。</li><li>先付款，后服务：一般情况下搬家公司装完东西后才可以付款。不要太早付款。</li><li>来自佛罗里达州的电话：许多经纪人在那里注册。</li></ol></li><li><strong>谈判</strong>：要求超过 5 个报价并利用它们相互竞争。最后我成功的把价格从5k谈到了3.2k。您只需向对方发送有关竞争报价的电子邮件，他们就会通过给您更大的折扣来降低价格，或者删除一些您一开始不知道的不必要的服务。<ol><li>你怎么知道价格合理？这一切都取决于里程、重量和 CF。仅供参考，我移动的距离约为 850 英里、2000 磅和 310 CF。由于我从经纪人那里收到的最低价格是 2.5k，这绝对太有吸引力了，令人难以置信，所以我有 80% 的信心我收到的价格是合理的，我需要付出很大的努力才能低于 3k。此外，支付合理的金额让我感觉更安全，否则，我可能会担心我将得到的服务。</li></ol></li><li><strong>准备一个电子邮件模板并随着时间的推移对其进行完善</strong>：列出您的地址、搬家日期（更灵活、价格更低）和搬家详细信息（从其他公司估算的重量和 CF）并询问他们的自付费用。无需与代理一一交谈，这样可以使搜索更加高效。不过，预计至少会与代理商打一次电话来进行估算。当您获得越来越多的信息时，请及时更新电子邮件。</li></ol><p><br>如果我需要再次找搬家公司，我会做以下事情：</p><ol><li>与 1 个大型搬家公司交谈以获得估价。</li><li>使用步骤 1 中的详细信息准备电子邮件模板，并与其他 5 个搬家公司核实。</li><li>谈判。</li></ol><p>最终可能需要大约 3 小时才能找到好的报价，而不是我在这里大约 9 小时的旅程:)</p><br/><br/> <a href="https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service<guid ispermalink="false"> GgezTQnwqxPzA2yNS</guid><dc:creator><![CDATA[Ziyue Wang]]></dc:creator><pubDate> Sat, 21 Oct 2023 08:10:31 GMT</pubDate> </item><item><title><![CDATA[Apply for MATS Winter 2023-24!]]></title><description><![CDATA[Published on October 21, 2023 2:27 AM GMT<br/><br/><p> 2023-24 年冬季<a href="https://www.matsprogram.org/home">MATS</a> （以前称为 SERI MATS）的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。导师包括<a href="https://www.lesswrong.com/users/rhaps0dy">Adrià Garriga Alonso</a> 、 <a href="https://www.alignmentforum.org/users/scasper?from=post_header">Stephen Casper</a> 、 <a href="https://www.lesswrong.com/users/jesseclifton">Jesse Clifton</a> 、 <a href="https://www.alignmentforum.org/users/davidad?from=post_header">David &#39;davidad&#39; Dalrymple</a> 、 <a href="https://www.alignmentforum.org/users/owain_evans">Owain Evans</a> 、 <a href="https://www.alignmentforum.org/users/evhub">Evan Hubinger</a> 、 <a href="https://www.alignmentforum.org/users/erik-jenner">Erik Jenner</a> 、 <a href="https://www.lesswrong.com/users/landfish">Jeffrey Ladish</a> 、 <a href="https://www.alignmentforum.org/users/neel-nanda-1">Neel Nanda</a> 、 <a href="https://www.alignmentforum.org/users/ethan-perez">Ethan Perez</a> 、 <a href="https://www.alignmentforum.org/users/lee_sharkey">Lee Sharkey</a> 、 <a href="https://www.alignmentforum.org/users/buck">Buck Shlegeris</a> 、 <a href="https://www.alignmentforum.org/users/turntrout">Alex Turner</a>和<a href="https://www.alignmentforum.org/users/sbowman">Sam Bowman</a>的研究人员<a href="https://wp.nyu.edu/arg/">纽约大学联盟研究小组</a>，包括<a href="https://homepages.inf.ed.ac.uk/s1302760/">Asa Cooper Stickland</a> 、 <a href="https://www.linkedin.com/in/idavidrein">David Rein</a> 、 <a href="https://julianmichael.org/">Julian Michael</a>和<a href="https://ihsgnef.github.io/">Shi Feng</a> 。</p><p>大多数导师的提交截止日期为 11 月 17 日（Neel Nanda 的提交截止日期为 11 月 10 日）。许多导师会提出具有挑战性的候选人选择问题，因此请确保您有足够的时间来完成您的申请。我们鼓励潜在的申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>我们很可能会在下周内添加多名导师。填写链接的兴趣表格或重新访问这篇文章以获取更新。</p><p>我们目前资金有限并接受捐赠以支持学者的进一步研究。如果您想支持我们的工作，<a href="https://manifund.org/projects/mats-funding">可以在这里捐款</a>！</p><h1>计划详情</h1><p>MATS 是位于加利福尼亚州伯克利的一个教育研讨会和独立研究项目（每周 40 小时），旨在为<a href="https://en.wikipedia.org/wiki/AI_alignment">人工智能对齐</a>领域的才华横溢的学者提供讲座、研讨会和研究指导，并将他们与伯克利人工智能安全研究社区联系起来。 MATS 为学者提供位于加利福尼亚州伯克利的住宿以及旅行支持、联合办公空间和同行社区。 MATS 的主要目标是帮助学者发展为人工智能安全研究人员。您可以<a href="https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change">在这里</a>阅读有关我们变革理论的更多信息。</p><p>根据个人情况，我们可能愿意改变项目的时间投入，并安排学者提前离开或开始。申请时请告诉我们您的空闲时间。我们的 MATS 2023-24 冬季计划的暂定时间表如下。</p><p>完成培训和研究阶段的学者将获得<a href="https://www.aisafetysupport.org/">AI 安全支持</a>提供的 1.2 万美元津贴。</p><h2>应用程序（现在！）</h2><p><strong>开放申请</strong>：10 月 20 日</p><p><strong>申请截止日期</strong>：11 月 17 日</p><p><i>注：Neel Nanda 的申请者将遵循修改后的时间表；请参阅下面的部分。</i></p><h2>培训阶段（1月8日-1月21日）</h2><p>被录取的申请人通常应在 MATS 计划开始之前完成<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础调整课程</a>或类似课程/</p><p> MATS 从两周的培训阶段开始。为了让学者们对人工智能安全领域有更广泛的了解，培训阶段设有先进的人工智能安全研究课程、导师特定的阅读清单、讨论小组等。</p><h2>研究阶段（1月22日-3月15日）</h2><p> MATS 的核心是为期两个月的研究阶段。在此阶段，每位学者每周至少花一小时与导师一起工作，并通过 Slack 进行更频繁的沟通。导师在以下方面差异很大：</p><ul><li>对项目选择的影响</li><li>关注低层细节与高层策略</li><li>强调输出与过程</li><li>参加会议的时间</li></ul><p>我们的学者支持团队通过提供专门的 1-1 签到、研究指导、调试和一般行政帮助来补充导师，以畅通研究进展并加速研究人员的发展。</p><p>教育研讨会和讲习班每周将举行2-3次。我们还组织了多次社交活动，让学者们认识伯克利人工智能安全社区的研究人员。</p><h3>研究里程碑</h3><p>学者们在研究阶段完成了两个里程碑。第一个是<a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit#heading=h.ozvm8udh0z0y">学者研究计划，</a>概述了威胁模型或风险因素、变革理论以及研究计划。这份文件将指导他们在该计划剩余时间内的工作，最终将举行由伯克利人工智能安全社区成员参加的研究研讨会。第二个里程碑是本次活动中的十分钟<a href="https://docs.google.com/document/d/1SB_vl6pGuSO7qidkZ8fIKPeH7fZIC8mvXzxicFjGyrE/edit#heading=h.ozvm8udh0z0y">研究报告</a>。</p><h3> MATS 社区</h3><p>研究阶段为学者提供了一个同行社区，他们共享办公室、膳食和住房。与远程进行独立研究相比，在社区工作可以让学者轻松接触未来的合作者，更深入地了解其他研究议程，并在人工智能安全社区中建立社交网络。学者们还可以获得全职社区经理的支持。</p><p>在我们的夏季队列中，研究阶段的每周至少包括一次社交活动，例如聚会、游戏之夜、电影之夜或徒步旅行。每周的闪电演讲为学者们提供了在非正式、低风险的环境中分享他们的研究兴趣的机会。工作之余，学者们组织社交活动，包括前往约塞米蒂国家公园的公路旅行、参观旧金山、酒吧郊游、周末聚餐，甚至跳伞旅行。</p><h2>延长阶段（4月1日至7月26日）</h2><p>研究阶段结束时，学者可以申请在为期四个月的扩展阶段队列中继续研究，默认在伦敦。录取决定很大程度上取决于获得导师的认可和获得外部资金。到这个阶段，我们期望学者能够高度自主地开展研究。</p><h2>后垫</h2><p>完成该计划后，MATS 校友拥有：</p><ul><li>被<a href="https://www.anthropic.com/">Anthropic</a> 、 <a href="https://openai.com/">OpenAI</a> 、 <a href="https://www.deepmind.com/">Google DeepMind</a> 、 <a href="https://intelligence.org/">MIRI</a> 、 <a href="https://www.alignment.org/">ARC</a> 、 <a href="https://www.conjecture.dev/">Conjecture</a>等领先组织以及美国政府聘用，并加入<a href="https://humancompatible.ai/">UC Berkeley CHAI</a> 、 <a href="https://wp.nyu.edu/arg/">NYU ARG</a> 、 <a href="https://space.mit.edu/home/tegmark/technical.html">MIT Tegmark Group</a>等学术研究团体；</li><li>创立AI安全组织，包括<a href="https://www.arena.education/">ARENA</a> 、 <a href="https://www.apolloresearch.ai/">Apollo Research</a> 、 <a href="https://www.leap-labs.com/">Leap Labs</a> 、 <a href="https://timaeus.co/">Timaeu​​s</a> 、 <a href="https://cadenzalabs.org/">Cadenza Labs</a> 、 <a href="https://www.aipolicy.us/">Center for AI Policy</a> 、 <a href="https://www.catalyze-impact.org/">Catalyze Impact</a> 、 <a href="https://stakeout.ai/">Stake Out AI</a> ；</li><li>在<a href="https://funds.effectivealtruism.org/funds/far-future">长期未来基金</a>、<a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">开放慈善事业</a>或<a href="https://manifund.org/">Manifund 的</a>资助下进行<a href="https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency">独立研究</a>。</li></ul><p>您可以<a href="https://www.matsprogram.org/alumni">在此处</a>阅读有关 MATS 校友的更多信息。</p><h2> Neel Nanda 的具体信息</h2><h3>应用领域</h3><p><strong>申请截止日期</strong>：11 月 10 日</p><p><strong>录取决定</strong>：11 月 17 日</p><h3>培训阶段（11月20日-12月22日）</h3><p> Neel Nanda 的学者将完成远程培训阶段，包括两周学习机甲解释和两周成对进行研究冲刺 - 请参阅上一个项目的<a href="https://docs.google.com/document/d/18qYhY6FB0AiVP9cNr9idP5YYkN_654WNVgfyL9LgfHA/edit">概述文档</a>以获取更多信息。</p><p>最初的培训阶段将比随后的研究阶段提供更多的录取通知书（上次有 19 名学者完成了此阶段，9 名继续进入研究阶段），但过去未取得进展的学者仍然认为此阶段是一个很好的介绍机械解释研究。是否继续提供将主要取决于研究冲刺的表现。</p><p> Neel 的学者将因参与培训阶段而从<a href="https://www.aisafetysupport.org/">AI Safety Support</a>获得 4800 美元的津贴。</p><h3>研究阶段（1月8日-3月15日）</h3><p>继续进入研究阶段的人员将于 1 月 8 日在伯克利与其他学者一起开始。对于尼尔的学者来说，这将被视为研究阶段的开始。</p><h1>谁应该申请？</h1><p>我们理想的申请人具有：</p><ul><li>了解人工智能安全研究领域，相当于完成了<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础知识调整课程</a>（如果您被该计划录取，但之前尚未完成本课程，则应在培训阶段开始之前完成）；</li><li>具有技术研究经验（例如机器学习、计算机科学、数学、物理、神经科学等），通常为研究生水平；和</li><li>从事人工智能安全研究事业的强烈动机。</li></ul><p>即使您不符合所有这些标准，<strong>我们也鼓励您申请</strong>！几位过去的学者在没有强烈期望的情况下提出申请并被接受。</p><h2>从美国境外申请</h2><p>美国境外的学者可以在研究阶段申请<a href="https://www.uscis.gov/working-in-the-united-states/temporary-visitors-for-business/b-1-temporary-business-visitor">B-1 签证</a>（更多信息请<a href="https://travel.state.gov/content/dam/visas/BusinessVisa%20Purpose%20Listings%20March%202014%20flier.pdf">点击此处</a>）。来自<a href="https://travel.state.gov/content/travel/en/us-visas/tourism-visit/visa-waiver-program.html">免签证计划 (VWP) 指定国家/地区的</a>学者可以通过<a href="https://esta.cbp.dhs.gov/esta">旅行授权电子系统 (ESTA)</a>向 VWP 提出申请，该系统将在三天内处理完毕。获得 B-1 签证的学者可以在美国停留最多 180 天，而获得 VWP 签证的学者可以在美国停留最多 90 天。请注意，B-1 签证批准时间可能比 ESTA 批准时间长得多，具体取决于您的原籍国。</p><h1>如何申请</h1><p><a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。大多数导师的提交截止日期为 11 月 17 日。我们鼓励潜在申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>候选人申请在特定导师的指导下工作，导师将审查他们的申请。申请的评估主要基于<strong>对导师问题的回答</strong>和<strong>先前的相关研究经验</strong>。有关我们导师的研究议程的信息可以在<a href="https://www.matsprogram.org/mentors">MATS 网站</a>上找到。</p><p>在申请之前，您应该：</p><ul><li>仔细阅读每个类别的描述和议程以及相关的候选人选择问题；</li><li>准备好您有兴趣申请的流派问题的答案。这些问题可以在申请表上找到；</li><li>准备您的 LinkedIn 或简历。</li></ul><p>候选人选择问题可能相当困难，具体取决于导师！确保您有足够的时间来完成您的申请。对一名导师的强力申请可能比对多名导师的中等申请具有更高的价值（尽管每份申请都将被独立评估）。</p><p>请注意，申请比乍一看要长，因为在您选择导师之前，特定于导师的问题是隐藏的。</p><h2>申请办公时间</h2><p>我们为潜在申请人提供办公时间，以澄清有关 MATS 计划申请流程的问题。在参加办公时间之前，我们要求申请人完整阅读当前帖子和我们的<a href="https://www.matsprogram.org/faqs">常见问题解答</a>。</p><p>我们的办公时间将在以下<a href="https://zoom.us/j/98715408709">Zoom 链接</a>上进行：</p><ul><li>太平洋时间 10 月 25 日星期三中午 12 点至下午 2 点。</li><li>太平洋时间 11 月 1 日星期三中午 12 点至下午 2 点。</li></ul><p>您可以使用<a href="https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=bnUxbmVxNDI1Z2d2OGxjY2Y0cTlvbm1lcWNfMjAyMzEwMjVUMTkwMDAwWiBjXzQzNmY2NzkzODEzMzk0ZmE5YjUwYjk1ZmMwOWY5Mzc4MDM1YjRiNmIyM2UxYjc4ODI5ZGQ1Y2U5ZGZkZDFkYTJAZw&amp;tmsrc=c_436f6793813394fa9b50b95fc09f9378035b4b6b23e1b78829dd5ce9dfdd1da2%40group.calendar.google.com&amp;scp=ALL">此链接</a>将这些办公时间添加到 Google 日历。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24<guid ispermalink="false"> tqyg3DpoiE4DKyi4y</guid><dc:creator><![CDATA[Rocket]]></dc:creator><pubDate> Sat, 21 Oct 2023 02:31:32 GMT</pubDate> </item><item><title><![CDATA[Muddling Along Is More Likely Than Dystopia]]></title><description><![CDATA[Published on October 20, 2023 9:25 PM GMT<br/><br/><p><strong>摘要：</strong>历史上有先例，禁令或严厉的法规阻止了某个行业的技术进步，而社会其他领域的技术进步仍在继续。这是人工智能的美好未来。</p><p><i>认知状态：我的直觉与这里其他人强烈不同。我希望解释我的直觉，并提供足够的历史证据来使这个直觉至少可信。</i></p><p></p><h3>简介：直觉泵</h3><p>假设您在 1978 年告诉某人，美国在 2023 年之前不会建造新的核电站<span class="footnote-reference" role="doc-noteref" id="fnrefdotwicb0t0t"><sup><a href="#fndotwicb0t0t">。 [1]</a></sup></span>这可能会非常令人惊讶。核电被认为是未来的动力。 <span class="footnote-reference" role="doc-noteref" id="fnref84mva9qulhn"><sup><a href="#fn84mva9qulhn">[2]</a></sup></span>核管理委员会三年前才成立。</p><p>有了这些信息，1978 年的某人可能会预测将会发生可怕的事情。也许美国和苏联之间会爆发核战争，摧毁美国的工业能力。也许由于人口过剩或全球变暖而导致经济崩溃。也许是<i>1984 年</i>的奥威尔式警察国家，或者是旨在监管失控核武器的世界权威。 <span class="footnote-reference" role="doc-noteref" id="fnref3czpkcc0eq9"><sup><a href="#fn3czpkcc0eq9">[3]</a></sup></span></p><p>这一切都没有发生。相反，核管理委员会加大了监管力度<span class="footnote-reference" role="doc-noteref" id="fnrefualznx45d8m"><sup><a href="#fnualznx45d8m">[4]</a></sup></span> ，直到建设新核电站变得不经济。这些法规仅适用于美国，但它们似乎对全球核电研究产生了重大影响。正在建设新核电站的国家仍在使用 1970 年之前开发的设计<span class="footnote-reference" role="doc-noteref" id="fnrefuldhmpk9bb"><sup><a href="#fnuldhmpk9bb">。 [5]</a></sup></span></p><p>与反事实相比，对核电的监管可能确实减缓了美国未来 45 年的经济增长。 <span class="footnote-reference" role="doc-noteref" id="fnrefpn3mqa4k7d"><sup><a href="#fnpn3mqa4k7d">[6]</a></sup></span>但过去 45 年并不是灾难性的。在其他行业的推动下，经济增长和创新确实在继续。</p><p></p><h3>停止 AGI 的后果？</h3><p>一些参与关于减缓或暂停人工智能的争论的人似乎认为，从长远来看，成功地阻止人工智能的进步可能会导致死亡或反乌托邦：</p><blockquote><p>要么我们弄清楚如何让通用人工智能顺利运行，要么我们等待小行星撞击。</p><p> - 萨姆·奥特曼<span class="footnote-reference" role="doc-noteref" id="fnrefm67279lgo3j"><sup><a href="#fnm67279lgo3j">[7]</a></sup></span></p></blockquote><p></p><blockquote><p>如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。</p><p> - 斯科特·亚历山大<span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span></p></blockquote><p></p><blockquote><p>我认为我们应该非常担心，全球政府需要执行这样的禁令，这将大大增加永久暴政的风险，这本身就是一场生存灾难。</p><p> - 诺拉·贝尔罗斯<span class="footnote-reference" role="doc-noteref" id="fnrefpbz6qf6ayr9"><sup><a href="#fnpbz6qf6ayr9">[9]</a></sup></span></p></blockquote><p></p><blockquote><p>看来我们需要建立一个全球性的警察国家，否则从长远来看，[无限期的人工智能暂停]将会失败。</p><p> - 马修·巴尼特<span class="footnote-reference" role="doc-noteref" id="fnref9k25eq052e"><sup><a href="#fn9k25eq052e">[10]</a></sup></span></p></blockquote><p>我觉得这和我们假设的 1978 年的人犯的错误是一样的。看起来人工智能在未来将是一件极其重要的事情，因此必须发生一些戏剧性的事情才能阻止它的发生。我认为，我们应该把更多的可能性放在无聊的未来上，监管会扼杀这一领域，而社会的其他领域则继续像以前一样。</p><p>这似乎是一个重要的分歧。如果你认为我们的后代的生活会相当好，而且会变得更好（如果不是快得难以想象的话），那么停止人工智能的进步对他们来说可能是值得的。如果你认为我们的后代的未来将是“短暂而严峻的” <span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span> ，那么在决定现在是否要冒这个风险时，他们可能就不会被考虑在内。</p><p></p><h3>具体问题</h3><p>斯科特·亚历山大提到了几个具体的担忧，这些担忧使他对人工智能没有进步的未来感到悲观。每个问题似乎都是人们现在和将来应该努力解决的现实问题。我们应该改善生物安全， <span class="footnote-reference" role="doc-noteref" id="fnrefzlf149enf2a"><sup><a href="#fnzlf149enf2a">[11]</a></sup></span>促进经济增长， <span class="footnote-reference" role="doc-noteref" id="fnrefu6cbijtsn58"><sup><a href="#fnu6cbijtsn58">[12]</a></sup></span>传播民主和自由， <span class="footnote-reference" role="doc-noteref" id="fnrefxsb44grlqzh"><sup><a href="#fnxsb44grlqzh">[13]</a></sup></span>并给人们的子孙后代带来希望。但这些事情在未来 100 年内导致死亡或反乌托邦的可能性似乎都不接近 50%。为了避免这些问题，不值得接受人工智能带来的生存风险，斯科特·亚历山大估计人工智能有约 20% 的机会导致人类灭绝。</p><p>诺拉·贝尔罗斯 (Nora Belrose) 和马修·巴尼特 (Matthew Barnett) 都担心，需要一个全球警察国家来执行对人工智能进步的长期禁令。这种立场在人工智能安全社区中似乎并不罕见。人们担心研究可能会转移到监管较少的地方，而算法的进步将使通用人工智能在个人计算机上成为可能。避免通用人工智能的唯一方法是大规模扩张全球政府权力。</p><p></p><h3>其他历史例子</h3><p>我认为其他技术还没有实现这些担忧。</p><p>一个行业的法规不会阻止所有其他行业的进步。湾区的人们可能低估了人工智能以外的新兴技术的重要性，或者更广泛地说软件的重要性，因为信息技术在当地经济中的重要性不成比例。 <span class="footnote-reference" role="doc-noteref" id="fnrefq6trr5y0v1f"><sup><a href="#fnq6trr5y0v1f">[14]</a></sup></span>我同样预计 1950 年生活在底特律的人们会低估汽车以外的新兴技术的重要性。如果没有人工智能，仍然可以取得很多进展。我特别兴奋的两项新兴技术是核聚变和太空殖民。</p><p>一个国家的法规可能会阻碍单一行业的进步。某个特定行业的进展停滞并不罕见。 <span class="footnote-reference" role="doc-noteref" id="fnref0bq1vg5uljy"><sup><a href="#fn0bq1vg5uljy">[15]</a></sup></span>特定行业的大多数创新都是在一个或几个城市进行的。这些创新集群很难建立和维护，因此，如果一个创新集群被监管压垮，它通常不会转移到另一个国家。从更广泛的角度来看，一些国家比其他国家更具创新性。在大多数行业，包括受到严格监管的行业，美国显然比欧洲或东亚（大部分）更具创新性，而欧洲或东亚又比世界其他地区更具创新性。很多事情都必须向好的方向发展：高生活水平、受过教育的民众、法治、未来利润的可能性、可用资本以及鼓励创新的文化。标榜国际法规或规范的国家通常不会吸引创新。一旦一项技术存在，其他国家就更容易复制它。所需的设计和技能已经存在，并且该技术的好处是显而易见的。防止创新的监管比防止扩散的监管容易得多。</p><p>我之前调查过一些可抵抗的技术诱惑， <span class="footnote-reference" role="doc-noteref" id="fnref14m5yvhuzdj"><sup><a href="#fn14m5yvhuzdj">[16]</a></sup></span>或通过我们当前的机构实现了长期暂停的技术：</p><ul><li><strong>核电</strong>，如上所述。 <span class="footnote-reference" role="doc-noteref" id="fnreffb9jpk8y0qa"><sup><a href="#fnfb9jpk8y0qa">[17]</a></sup></span></li><li><strong>地球工程</strong>并不是明确违法的，但科学家和活动人士的反对甚至阻止了研究的进行。 <span class="footnote-reference" role="doc-noteref" id="fnrefp6r8lh3a1ta"><sup><a href="#fnp6r8lh3a1ta">[18]</a></sup></span></li><li><strong>疫苗开发</strong>在西方国家受到严格监管。在最近的大流行期间，俄罗斯和中国都放松了一些限制，并在西方之前批准了疫苗。由此产生的疫苗效果较差，因为最好的医学研究仍然位于西方。特别是，<strong>人体挑战试验</strong>已被监管为几乎不存在。 <span class="footnote-reference" role="doc-noteref" id="fnref1rfwwl0q1g"><sup><a href="#fn1rfwwl0q1g">[19]</a></sup></span></li><li><strong>核武器</strong>有时被认为是一种监管未能阻止其扩散的技术。我认为这个例子的证据是混杂的。大约有 10 个国家拥有核武器（比拥有核武器的数量少得多），但只有 2 个国家独立开发核武器：美国和法国。尖端研究尚未转移到非《不扩散核武器条约》缔约国的国家。印度、巴基斯坦和朝鲜似乎拥有与 20 世纪 40 年代和 50 年代的美国或苏联类似的能力。 <span class="footnote-reference" role="doc-noteref" id="fnrefg6e7iucwc7"><sup><a href="#fng6e7iucwc7">[20]</a></sup></span>试验禁令可能也有助于防止核武器变得越来越强大：有史以来威力最大的炸弹于 1961 年引爆。</li><li><strong>生物武器</strong>有一些最严格的条约和禁止其开发或使用的禁忌， <span class="footnote-reference" role="doc-noteref" id="fnref9kg6yxtjycb"><sup><a href="#fn9kg6yxtjycb">[21]</a></sup></span>并且没有一个国家公开拥有生物武器计划。这个例子的一个问题是苏联签署了禁止开发生物武器的条约 - 然后继续开发它们。</li><li>各种核技术，如<strong>原子园艺</strong>、<strong>在建筑中使用核爆炸</strong>或<strong>猎户座计划</strong>，已经被提出但尚未开发。</li><li><strong>克隆最有效的士兵</strong>从未被实现过。</li><li>中国的一名研究人员在被捕之前已经对<strong>人类进行了基因改造</strong>。</li><li>目前还不清楚<strong>殖民主义</strong>是否算作一种技术。明朝在 1400 年代初决定停止其宝船舰队，使全球殖民主义推迟了约 50 年，并可能影响了中国几个世纪的发展轨迹。</li><li><strong>贝尔实验室</strong>在 1945 年至 1980 年间发明或发现了晶体管、电荷耦合器件、光伏电池、信息论、Unix、C 和宇宙微波背景辐射。 <span class="footnote-reference" role="doc-noteref" id="fnref551akuqm31p"><sup><a href="#fn551akuqm31p">[22]</a></sup></span> 1982 年贝尔系统被反垄断法打破后，那里的研究界就分裂了。似乎有理由认为，有些技术之所以没有被发明，是因为这个异常多产的创新中心被摧毁了。</li></ul><p>大多数技术并未被禁止，其进步也没有受到监管的抑制。大多数技术也不像人工智能那么可怕：我很难想象太阳能电池板或圆珠笔如何构成 x 风险。听起来可怕的技术，如大规模杀伤性武器或某些类型的医学研究，经常面临禁令或法规，使它们的开发不再值得，而这些禁令有时会起作用。</p><p>我不想说对听起来可怕的技术的有效禁令是默认发生的。当他们工作时，他们是共同努力的结果。但在不扰乱社会其他部分的情况下，禁止一项具有潜在危险的新技术似乎是非常可行的。</p><p></p><h3>也许人工智能会有所不同</h3><p>虽然这篇文章主要是关于其他技术被停止的历史先例，但似乎值得特别谈谈人工智能。人工智能与其他技术不同的原因有几个：</p><ol><li>人工智能研究比其他新兴技术更容易远程进行。</li><li>人工智能系统一旦创建，就可以作为软件轻松传输。</li><li>简单的经济模型表明，强大的人工智能对于采用它的人来说都将带来极大的经济优势。</li></ol><p>所有技术都是不同的。有些差异使监管变得更容易或更困难，但这些差异都没有大到使监管变得不可能的程度：</p><ol><li>执法部门可以根据研究进行的地点或研究人员居住的地点来执行法律。美国尤其对其法律适用范围有着广泛的看法。 <span class="footnote-reference" role="doc-noteref" id="fnrefqnyaglfoig"><sup><a href="#fnqnyaglfoig">[23]</a></sup></span></li><li>大多数拟议的法规都集中在训练强大的人工智能所需的硬件上。</li><li>政策制定者并不知道这一点。他们知道有人在告诉他们这些。他们肯定不知道，如果他们支持这个特定的项目，他们将在他们关心的时间范围内获得通用人工智能的经济承诺。这些承诺与其他技术的炒作并没有太大区别。 <span class="footnote-reference" role="doc-noteref" id="fnrefv96bbayqvig"><sup><a href="#fnv96bbayqvig">[24]</a></sup></span></li></ol><p>还有一些方法可以更轻松地监管人工智能。</p><p>供应链有多个阶段，世界上只有一家或几家公司能够从事尖端工作。只需要少数参与者进行协调即可使监管发挥作用。</p><p>当前领先的人工智能模型需要大量计算，这是资本密集型且易于跟踪的。随着算法效率的足够改进，这种情况可能会改变。但我们应该预计，随着资本和人才转移到其他行业，算法的进步将急剧放缓，以应对人工智能的长期停滞。</p><p>许多物质和物品都受到监管，并且该监管的细节根据其内容和政府试图避免的内容而有很大差异。 <span class="footnote-reference" role="doc-noteref" id="fnreffp5yyyzqajs"><sup><a href="#fnfp5yyyzqajs">[25]</a></sup></span>监管 GPU 将面临一些独特的挑战，但在我们当前的机构下似乎并非不可能。</p><p></p><h3>暂停多长时间？</h3><p>大多数历史证据表明全球停顿已经持续了大约 50 年。这是讨论 100 年暂停的有用证据。如果“长期”意味着一千年，那么历史证据就少得多。马修·巴尼特 (Matthew Barnett) 认为，现有机构内的监管棘轮可能会导致人工智能研究暂停 50 年，但要实现 1000 年的暂停，就需要采取更戏剧性的措施。</p><p>我怀疑全球警察国家是否会比更正常的监管更容易维持一千年。我关于如何在这个时间范围内维持机构的模型是：</p><ol><li>建立一个持续一代人的机构。</li><li>让下一代相信维持这个机构是一件好事。</li></ol><p>如果你在（2）中失败，那么建立什么机构并不重要。如果连精英阶层都不相信警察国家是好事，那么它就无法维持下去。 <span class="footnote-reference" role="doc-noteref" id="fnrefwx71dbbygd"><sup><a href="#fnwx71dbbygd">[26]</a></sup></span>一个机构如果硬实力较少，但更善于让人们相信它，那么它更有可能持续一千年。</p><p></p><h3>结论</h3><p>构建 AGI 是一项极其不确定的工作。它可能会带来我们光荣的未来。它可能会导致人类灭绝。这甚至可能是不可能的。如果我们决定不尝试构建通用人工智能，那么未来的不确定性似乎就会少得多。社会显然仍将不是最优的，但也远非反乌托邦。取得科学、技术、经济、社会和政治进步将继续困难重重，但人们将继续这样做。我们可以继续希望我们的孩子，以及他们的孩子，在未来的很长一段时间里，至少能有微小的改善。</p><p>如果一项听起来很可怕的技术面临监管棘轮，从而减缓甚至停止该领域的所有进展，那也不足为奇。这不是死亡或反乌托邦——这是正常的。<br></p><p><i>感谢 Aaron Scher、Matthew Barnett、Rose Hadshar、Harlan Stewart 和 Rick Korzekwa 就该主题进行的有益讨论。</i></p><p> <i>Theen Moy 的预览图像：</i> <a href="https://www.flickr.com/photos/theenmoy/8003177753"><i>https://www.flickr.com/photos/theenmoy/8003177753</i></a> <i>。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndotwicb0t0t"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdotwicb0t0t">^</a></strong></sup></span><div class="footnote-content"><p>这不太公平，因为日期范围从一个工厂（Shearon Harris）的建设开始一直延伸到另一个工厂（Vogtle Unit 3）的建设结束。沃格特尔3号机组于2013年开始建设。还有一座核电站（瓦茨巴2号机组）于1973年开始建设，2016年竣工。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn84mva9qulhn"> <span class="footnote-back-link"><sup><strong><a href="#fnref84mva9qulhn">^</a></strong></sup></span><div class="footnote-content"><p> 1973年，原子能委员会预测，到2000年，美国55.8%的电力将来自核电，这一数字低于其此前的预测。但这并没有发生：自 20 世纪 80 年代末以来，核电已占美国电力的 20% 左右。</p><p>安东尼·里普利. <i>AEC 降低了原子能增长预期。</i>纽约时报。 （1973）<a href="https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html"><u>https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3czpkcc0eq9"> <span class="footnote-back-link"><sup><strong><a href="#fnref3czpkcc0eq9">^</a></strong></sup></span><div class="footnote-content"><p>包括伯特兰·罗素在内的一些知名人士主张建立一个世界权威，以防止核武器带来的生存风险：</p><blockquote><p>确保世界和平的一个更理想的方式是各国之间自愿达成协议，集中其武装部队并服从商定的国际机构。目前看来，这似乎是一个遥远而乌托邦的前景，但也有一些务实的政治家不这么认为。一个世界权威如果要履行其职能，就必须拥有立法机构、行政机构和不可抗拒的军事力量。所有国家都必须同意将国家武装部队削减至内部警察行动所需的水平。不应允许任何国家保留核武器或任何其他大规模销毁手段。 ……在一个各个国家都解除武装的世界里，世界权威机构的军事力量不需要很大，也不会对各个组成国家构成沉重的负担。</p></blockquote><p>伯特兰·罗素.<i>人有未来吗？</i> (1961) 引自全球治理论坛。 （2023 年 10 月 17 日访问） <a href="https://globalgovernanceforum.org/visionary/bertrand-russell/"><u>https://globalgovernanceforum.org/visionary/bertrand-russell/</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnualznx45d8m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefualznx45d8m">^</a></strong></sup></span><div class="footnote-content"><p>马克·R·李。<i>监管棘轮：为什么监管会引发监管。</i>辛辛那提大学法律评论<strong>87.3</strong> 。 （2019） <a href="https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr"><u>https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuldhmpk9bb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuldhmpk9bb">^</a></strong></sup></span><div class="footnote-content"><p>例如，核电站的一种“新”设计是熔盐反应堆。目前已有一个：TMSR-LF1，这是一座位于中国西北部的实验反应堆，可产生 2 兆瓦的火力发电。该设计基于熔盐反应堆实验（MSRE），该实验于 1965 年至 1969 年在美国橡树岭国家实验室产生了 7 MW 的热电。</p><p>同样，中国也有一个小型模块化反应堆，即 HTR-PM，于 2021 年开始发电。它是一个球床反应堆，以德国示范反应堆 (AVR) 为基础，该反应堆于 1967 年至 1988 年运行。</p><p>所有其他核电站都使用更古老的反应堆类型。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpn3mqa4k7d"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpn3mqa4k7d">^</a></strong></sup></span><div class="footnote-content"><p>我之前曾估计过美国核电成本过高而损失的直接价值。我还预计，由于电力价格便宜，将会产生额外的间接价值。</p><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm67279lgo3j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm67279lgo3j">^</a></strong></sup></span><div class="footnote-content"><p>萨姆·奥特曼.推特。 (2022) <a href="https://twitter.com/sama/status/1540781762241974274?lang=en">https://twitter.com/sama/status/1540781762241974274?lang=en</a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3lk2fdp52yu"> <span class="footnote-back-link"><sup><strong><a href="#fnref3lk2fdp52yu">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>其次，如果我们永远得不到人工智能，我预计未来将是短暂而严峻的。我们很可能用合成生物学自杀。否则，技术和经济停滞、极权主义抬头+非自由主义+暴民统治、生育率崩溃和基因不良等综合因素将使世界陷入贫困，并加速其制度质量的衰退。我不会花太多时间担心这些问题，因为我认为它们需要几代人的时间才能达到危机水平，而且我预计技术会在那之前扭转游戏规则。但如果我们禁止所有游戏板翻转技术（我所知道的唯一另一种技术是基因增强，这甚至更应该禁止），那么我们最终会导致生物武器灾难或社会崩溃。我之前说过，我认为人工智能毁灭世界的可能性约为 20%。但如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。这并不意味着我必须支持人工智能加速主义，因为 20% 小于 50%。短暂的、精心设计的暂停可以大大提高人工智能顺利进行的机会，同时又不会增加太多社会崩溃的风险。但这是我心里的事。</p></blockquote><p>斯科特·亚历山大.<i>暂停思考：人工智能暂停辩论。</i>星体法典十。 （2023） <a href="https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate"><u>https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpbz6qf6ayr9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpbz6qf6ayr9">^</a></strong></sup></span><div class="footnote-content"><p>诺拉·贝尔罗斯。<i>人工智能暂停可能会适得其反。</i> EA 论坛。 （2023） <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9k25eq052e"> <span class="footnote-back-link"><sup><strong><a href="#fnref9k25eq052e">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>请注意，我并不是说人工智能暂停倡导者必然直接倡导全球警察国家。相反，我认为，为了将无限期的暂停维持足够长的时间，我们似乎需要创建一个世界范围的警察国家，否则从长远来看，暂停将会失败。人们可以选择“硬着头皮”并倡导建立一个全球警察国家来回应这些论点，但我并不是说这是人工智能暂停倡导者的唯一选择。</p><p>硬着头皮提倡全球警察国家无限期暂停人工智能的一个原因是，即使你认为全球警察国家很糟糕，你也可能会认为全球人工智能灾难更糟糕。在人工智能灾难明显迫在眉睫的情况下，我实际上同意这种评估。</p><p>然而，虽然我并不教条地反对建立一个全球警察国家，但我仍然有一个启发式反对推动建立一个全球警察国家，并且认为通常需要强有力的证据来推翻这种启发式。我认为迄今为止，关于人工智能灾难的论点还没有达到这个门槛。灾难论点的主要现有论点似乎很抽象，并且脱离了有关真实人工智能系统行为的任何坚实的经验证据。</p></blockquote><p>马修·巴内特。 <i>AI无限期暂停的可能性。</i> EA 论坛。 (2023) <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzlf149enf2a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzlf149enf2a">^</a></strong></sup></span><div class="footnote-content"><p>托比·奥德 (Toby Ord) 估计<i>《悬崖</i>》下个世纪的生物安全 X 风险约为 1/30。生物安全社区在应对 X 风险方面似乎比人工智能安全社区更成功。大多数研究开展的国家已经制定了广泛的法规，并制定了反对开发生物武器的主要国际条约。如果你认为人工智能比合成生物学更危险，那么为了提高生物安全而推进人工智能就没有意义。甚至还不清楚日益强大的人工智能是否会使生物安全变得更好或更糟。</p><p>作为比较，托比·奥德估计下个世纪小行星撞击的 x 风险约为 1/1,000,000。我将萨姆·奥尔特曼对小行星的担忧视为所有其他存在风险的代表。否则，他的风险估计似乎相差许多数量级。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6cbijtsn58"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6cbijtsn58">^</a></strong></sup></span><div class="footnote-content"><p>我不认为我们已经耗尽了人类可实现的经济、技术或科学进步。即使没有通用人工智能，100 年后中位数可能会比现在富裕得多。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxsb44grlqzh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxsb44grlqzh">^</a></strong></sup></span><div class="footnote-content"><p>过去十年来大多数国家的政治和社会趋势似乎并不好。上个世纪大多数国家的政治和社会趋势看起来都很棒。在预测下个世纪时，我们应该同时考虑这两点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq6trr5y0v1f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq6trr5y0v1f">^</a></strong></sup></span><div class="footnote-content"><p>您预计信息行业占美国 GDP 的比例是多少？信息产业包括信息技术和传统媒体。</p><div class="spoilers"><p> 5.5%</p></div><p> <a href="https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm"><u>https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0bq1vg5uljy"> <span class="footnote-back-link"><sup><strong><a href="#fnref0bq1vg5uljy">^</a></strong></sup></span><div class="footnote-content"><p><i>特定技术停止的进展示例。</i>人工智能影响维基。 （2023 年 10 月 19 日访问） <a href="https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_particular_technology_stopping"><u>https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_pspecial_technology_stopping</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn14m5yvhuzdj"> <span class="footnote-back-link"><sup><strong><a href="#fnref14m5yvhuzdj">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑项目。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfb9jpk8y0qa"> <span class="footnote-back-link"><sup><strong><a href="#fnreffb9jpk8y0qa">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp6r8lh3a1ta"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp6r8lh3a1ta">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：地球工程。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1rfwwl0q1g"> <span class="footnote-back-link"><sup><strong><a href="#fnref1rfwwl0q1g">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：疫苗挑战试验。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng6e7iucwc7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg6e7iucwc7">^</a></strong></sup></span><div class="footnote-content"><p>我不知道以色列的核计划是什么样的，也不知道其中有多少是美国技术转让的结果，而不是本土创新的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9kg6yxtjycb"> <span class="footnote-back-link"><sup><strong><a href="#fnref9kg6yxtjycb">^</a></strong></sup></span><div class="footnote-content"><p> 《日内瓦议定书》（1925 年）禁止使用进攻性生物武器，《生物武器公约》（1972 年）禁止开发、生产、获取、转让、储存和使用生物武器。除了条约之外，生物武器的使用似乎也有重大禁忌。</p><p>米歇尔·本特利.<i>生物武器禁忌。</i>岩石上的战争。 （2023） <a href="https://warontherocks.com/2023/10/the-biological-weapons-taboo/">https://warontherocks.com/2023/10/the-biological-weapons-taboo/</a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn551akuqm31p"> <span class="footnote-back-link"><sup><strong><a href="#fnref551akuqm31p">^</a></strong></sup></span><div class="footnote-content"><p>尤利亚·乔治斯库.<i>带回贝尔实验室的黄金岁月。</i>自然评论物理<strong>4</strong> 。 (2022) 页。 76-78。 <a href="https://www.nature.com/articles/s42254-022-00426-6"><u>https://www.nature.com/articles/s42254-022-00426-6</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqnyaglfoig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqnyaglfoig">^</a></strong></sup></span><div class="footnote-content"><p>例如，萨姆·班克曼-弗里德 (Sam Bankman-Fried) 正在美国联邦法院接受审判，尽管他本人和他的公司已经搬到了巴哈马群岛。</p><p>另一个例子是美国司法部因腐败罪在瑞士逮捕了来自各国的国际足联官员。 “美国法律允许根据多项法规引渡和起诉外国人……在本案中，她说，国际足联官员利用美国银行系统作为其计划的一部分。”<br><br>斯蒂芬妮·克利福德和马特·阿普佐。<i>在起诉 14 名足球官员后，美国誓言结束国际足联腐败行为。</i>纽约时报。 （2015） <a href="https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html"><u>https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnv96bbayqvig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv96bbayqvig">^</a></strong></sup></span><div class="footnote-content"><p>例如，神剑计划承诺通过在发射时摧毁数十枚洲际弹道导弹（带有数百枚弹头）来消除苏联核武器的威胁。最终它是不可行的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfp5yyyzqajs"> <span class="footnote-back-link"><sup><strong><a href="#fnreffp5yyyzqajs">^</a></strong></sup></span><div class="footnote-content"><p><i>受监管事物的示例。</i>人工智能影响维基。 （2023 年 10 月 19 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things"><u>https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwx71dbbygd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwx71dbbygd">^</a></strong></sup></span><div class="footnote-content"><p>这是我对苏联发生的事情的过于简化的模型。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia<guid ispermalink="false"> pAnvMYd9mqDT97shk</guid><dc:creator><![CDATA[Jeffrey Heninger]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:25:17 GMT</pubDate> </item><item><title><![CDATA[What's Hard About The Shutdown Problem]]></title><description><![CDATA[Published on October 20, 2023 9:13 PM GMT<br/><br/><p>我一直与萨米·彼得森就他的<a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/?commentId=mmvwCeHzzojY9Gf2a"><u>“无懈可击的不完整偏好”</u></a>进行了一些<a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1?commentId=XpY4YxF4tFfGzNbbK">富有成效的</a>讨论（旨在补充<a href="https://s3.amazonaws.com/pf-user-files-01/u-242443/uploads/2023-05-02/m343uwh/The%20Shutdown%20Problem-%20Two%20Theorems%2C%20Incomplete%20Preferences%20as%20a%20Solution.pdf">艾略特·索恩利解决关机问题的方法</a>）。这篇文章是我迄今为止从讨论中得到的一些有用文章的（非正式）提炼。</p><h2>关机问题</h2><p>从概念上讲：我们希望为强大的人工智能配备一个关机按钮。如果按钮未被按下，我们希望 AI 遵循一个目标（我们将其称为“目标”）。如果按下按钮，我们希望人工智能遵循不同的目标（我们称之为“偏离目标”）。据推测，偏离目标涉及人工智能关闭自身及其所有相关工具/子/等，但这部分是当前目的的风味文本。</p><p>我们在这里要避免的关键故障模式是意外地激励人工智能操纵关机按钮——即要么导致其被按下，要么导致其未被按下。为什么会发生这种情况？好吧，假设当我们第一次打开人工智能时，它<i>只是</i>针对目标进行优化。人工智能意识到，如果按下按钮，它未来的自我将停止针对目标进行优化，因此它将在目标上实现较低的性能。因此，人工智能意识到，通过阻止按钮被按下，它可以在目标上取得更好的表现。由于它只是针对目标性能进行优化，因此它会阻止按钮被按下。</p><p> <a href="https://intelligence.org/files/Corrigibility.pdf"><u>MIRI 关于关机问题的论文</u></a>详细介绍了针对关机问题的简单建议的更多示例，以及它们如何激励某种按钮操作。事实证明，通常很难从任何预期的效用最大化器中获得所需的行为（即没有按钮操作）。</p><h2>为什么很难，第 1 级：期望的行为意味着不完整的显示偏好</h2><p>为什么很难让预期的效用最大化器不操作按钮（同时仍然做有用的事情）？这是我从与萨米的讨论中得到的一个直观答案。</p><p>中心思想是询问我们希望关闭按钮代理的行为意味着该代理所显示的偏好是什么。</p><p>假设在不同的时间，为代理提供了花费资源的机会，以便使按钮被按下/取消按下。我们希望代理人<i>在两个方向上</i>拒绝这样的机会——这意味着对任何显示的偏好漠不关心或缺乏偏好。此外，我们确实希望代理花费资源在按下按钮或未按下按钮的世界中产生各种不同的结果，因此在按下按钮的世界和未按下按钮的世界中存在不平凡的显示偏好顺序。但是，如果代理要拒绝导致按钮被按下/松开的代价高昂的机会，并且这些机会在足够多不同的按下结果和未按下结果对之间跳转（它们本身都有不平凡的显示偏好），那么将会有揭示的偏好差距——即行为<i>不能</i>用完整的偏好来表示，只能用不完整的偏好来表示。</p><p>视觉上： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iJofoQX7EjMFxDo6m/xfewi1h80r9jpibzyaxl"><figcaption>代理花费资源从​​ A1 移动到 A2，但不花费资源在 B 和任一 A 结果之间向任一方向移动。这就是“偏好差距”；它很有趣，因为任何与偏好差距一致的偏好分配必定是不完整的。</figcaption></figure><p>我们希望代理花费资源在未按下按钮的世界中从结果 A1 移动到 A2，因此它在那里有一个显示的偏好。但我们希望代理<i>不要</i>花费资源在 A1 和 B 之间移动，<i>无论是哪个方向</i>- 因此它要么显示出冷漠（假设代理具有非零的出价/要价价差），要么显示出 A1 和 B1 之间偏好的不完整性。按照同样的逻辑，A2 和 B1 之间也要么表现出冷漠，要么不完整。但是，由于代理对 A2 的显示偏好高于 A1，因此任何显示的偏好分配在 A1/B1 和 A2/B1 之间都不能是无差异的；这意味着 A1 和 A2 之间是冷漠的。因此，代理人所透露的偏好<i>一定</i>是不完整的。 <span class="footnote-reference" role="doc-noteref" id="fnreft75wmozjvgf"><sup><a href="#fnt75wmozjvgf">[1]</a></sup></span></p><p>结果：（一种框架方式）关闭问题首先困难/有趣的原因是，期望的行为意味着暴露的偏好差距。例如，任何标准的预期效用最大化者<i>都不能</i>有显性的偏好差距，这样的标准欧盟最大化者<i>就不能</i>按照我们想要的方式行事。</p><p>关于此的一些注释：</p><ul><li>所显示的偏好可能非常不确定，即可能存在许多与代理行为兼容的偏好分配。上面的论点表明，<i>任何与</i>期望行为一致的偏好分配<i>都一定</i>是不完整的。</li><li>请注意，故事隐式地通过因果干预/反事实来探索偏好，即我们感兴趣的是代理是否会花费资源来<i>导致</i>按钮被按下/松开。这与本文无关，但对于那些想要将这一切正式化的人来说可能很重要。</li><li>显然，有时人们认为 A 和 B 之间“无偏好”（即不完整性）意味着代理人在两者之间随机/任意选择。这并不是这里所说的“无偏好”的概念。我们谈论的是“无偏好”，就像“只要有机会，代理人不会为了获得另一个而放弃任何一个”。</li></ul><p>好吧，现在我们有一个相对直观的故事来解释为什么在关闭问题中很难让预期的效用最大化者按照我们想要的方式行事：只要欧盟最大化者没有揭示偏好差距，他们就无法按照我们想要的方式行事我们想要。 （更一般地说，如果我们正在训练一个代理，任何一维奖励信号都可能无法激励显示的偏好差距。）但是，那么解决方案大概就是……使用一个具有不完整偏好的代理？</p><h2>为什么很难，第二级：不完整的偏好想要完成</h2><p>帖子<a href="https://www.lesswrong.com/posts/bzmLC3J8PsknwRZbr/why-not-subagents"><u>为什么不选择子代理？</u></a>对此进行了更深入的探讨，但我将在这里进行简要概述。</p><p>回顾我们的 A1、A2 和 B1 图表，我们可以想象一组不完整的偏好，它比 A1 更喜欢 A2，但在 B1 和 A 中的任何一个之间没有偏好（即代理不会花费资源在未按下按钮和按下按钮的结果，反之亦然）。但该代理将花费资源将 A1 结果转变为 A2 结果。</p><p>现在的问题是：该智能体可能会概率性地预先承诺接受将 A1 结果转变为 B1 结果（即花费资源使按钮被按下）和将 B1 结果转变为 A2 结果（即花费资源使按钮不被按下）的<i>两个</i>机会。按下）。如果它以正确的概率执行这两项操作，那么它可以保持按下 (B1) 和未按下 (A1+A2) 结果的相同比例，但将一些概率质量从 A1 结果转移到 A2。这是一个改进，通过不完整的偏好：代理不接受 A 和 B 之间的任何净转移，它只是 A 结果内的净转移概率，但它是通过操纵关闭按钮来实现的。</p><p> （我从萨米的帖子中了解到，这被称为不完整偏好的“束缚”。）</p><p>所以我们需要某种不会这样做的代理。</p><p>关于此的一些注释：</p><ul><li>关键问题是<i>所揭示的</i>偏好是否不受限制。这是我与萨米讨论时的一个主要困惑点 - 他的提案涉及一组传递到决策规则中的偏好，但这些“偏好”（可能）与<i>所显示的</i>偏好不同。 （我仍然不确定萨米的提议是否解决了问题。）</li><li>在传递给某种搜索/规划/决策过程的目标意义上，显示的“偏好”与“偏好”之间的差异可能会开辟一些解决问题的方法。</li><li>显然，我们可以设计一个不太聪明的代理，它具有稳定的不完全偏好。有趣的问题是如何在不对代理的能力或环境的丰富性产生重大限制的情况下做到这一点。</li><li>请注意，束缚涉及<i>导致</i>代理没有偏好的结果之间的切换。我的直觉是，因果关系在某种程度上是关键。我们希望代理不会导致按下和未按下结果之间的切换，即使这两个结果的相对频率保持不变。 </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnt75wmozjvgf"> <span class="footnote-back-link"><sup><strong><a href="#fnreft75wmozjvgf">^</a></strong></sup></span><div class="footnote-content"><p>这一切都假设偏好的传递性；人们也许可以放宽传递性而不是不完整性，但这样我们就处于更狂野的领域了。我不是在这里探索那条特定的道路。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem<guid ispermalink="false"> iJofoQX7EjMFxDo6m</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:13:30 GMT</pubDate> </item><item><title><![CDATA[Holly Elmore and Rob Miles dialogue on AI Safety Advocacy]]></title><description><![CDATA[Published on October 20, 2023 9:04 PM GMT<br/><br/><p> Holly 是一位独立的 AI Pause 组织者，其中包括组织抗议活动（就像即将<a href="https://www.lesswrong.com/events/ZoTkRYdqGuDCnojMW/global-pause-ai-protest-10-21">到来的抗议活动</a>）。 Rob 是一位<a href="https://www.youtube.com/@RobertMilesAI">AI 安全 YouTuber</a> 。我（jacoobjacob）将他们聚集在一起进行这次对话，因为我一直在试图弄清楚我应该如何看待人工智能安全抗议，这似乎是一个可能相当重要的干预；罗布和霍莉似乎有着深思熟虑的观点，甚至可能存在分歧。</p><p>快速澄清：他们一度讨论了一场特定的抗议活动，即<a href="https://x.com/ilex_ulmus/status/1713062767853253042?s=20">2023 年 9 月 29 日在旧金山 Meta 大楼举行的反不可逆转扩散抗议活动，</a> Holly 和 Rob 都参加了。</p><p>另外，对话相当长，我觉得没必要按顺序看。您应该随意跳到您觉得最感兴趣的部分标题。 </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:04:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:04:26 GMT" user-order="1"><p>让我们直接进入正题。罗布，当我建议讨论抗议活动时，你说你很困惑：“我们如何才能对任何事情有足够的信心，为其带来激进主义似乎需要的能量？”我非常同意这种困惑！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:07:41 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:07:41 GMT" user-order="2"><p>这是一个非常标准的笑话/刻板印象，XKCD 抗议，其中的标志是巨大的，充满了微小的文字，准确地说明了我们的意思。我的抗议标语只是写着“现在小心”，部分原因是我不确定你还可以在我完全认可的标语上添加什么内容。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:09:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:09:59 GMT" user-order="3"><p>科学或错误较少的事物与倡导之间的沟通方式存在很大差异。您在宣传方面的带宽较少。它更接近正常的言语，我们并不完全限定每一个陈述——考虑到这种沟通的窗口有多短，作为一种实践，这在很多方面都更准确。 “暂停人工智能”可以完成对如何实施政策的准确描述所无法完成的工作——这会太混乱且难以接受。同样，你在倡导中必须讨论的政策决议要低得多（或更高层次的概念），因此您可以认可您确实支持的非常广泛的政策目标，同时对于使用什么机制或确切方法有很多真正的不确定性。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:11:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:11:26 GMT" user-order="1"><p> （一个相关的旧的好帖子： <a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">You Get About Five Words</a> 。该帖子还建议您还剩下 3 个单词。可以将它们花在“PauseAI：但不确定”上）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:15:36 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:15:36 GMT" user-order="3"><p> （好吧，这可能会让我们遇到一个症结：这是一个糟糕的主意）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:12:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:12:37 GMT" user-order="1"><p> （哈哈，如果罗布觉得有趣的话，对于该主题的讨论非常兴奋）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:17:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:17:55 GMT" user-order="2"><p> （我想我同意，在你的口号中加入一般性的不确定性是浪费文字，而你可以添加具体的细微差别。比如“暂停前沿人工智能”、“暂停 A <strong>G</strong> I”、“暂停上帝般的人工智能”等等）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:22:23 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:22:23 GMT" user-order="3"><p> （同意。我只是不会使用太技术性的词。我喜欢“暂停上帝般的人工智能”，但也许有些人认为人工智能永远不可能绝对像上帝之类的。更多的限定词可能会增加人们混淆的机会）基本上有了正确的想法就可以开始了。）<br><br> （好吧，“基本上有正确的想法”可能就是问题所在。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:11:23 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:11:23 GMT" user-order="2"><p>回复上一点：另一方面就是政策看起来很复杂和不确定，我真的不知道我们应该做什么。当我至少有 20% 的人认为 X 政策会让事情变得更糟时，大喊我们应该执行 X 政策，这感觉几乎是不诚实的。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:14:50 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:14:50 GMT" user-order="3"><p>嗯，是的，我听到了。我仍然赞成在这种情况下倡导<i>一项</i>政策的原因是，受众所做的更新的性质通常比该水平高一两个级别。所以我们说“暂停人工智能”，但他们带走的是“人工智能是危险的，有一个不涉及制造人工智能的解决方案，支持这个解决方案是可行的......”<br><br> （我也碰巧认为暂停人工智能是一个非常好的宣传信息，因为它很广泛，指向一个有意义的方向，并且不容易被误解为危险的东西。有很多可以想象的暂停版本，我认为其中大多数都会乖一点。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:16:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:16:11 GMT" user-order="2"><p>我认为这可能会被误解为“暂停所有人工智能开发和部署”，从而导致“ <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#:~:text=If%20all%20you%20need%20is%20an%20object%20that%20doesn%27t%20do%20dangerous%20things%2C%20you%20could%20try%20a%20sponge%3B%20a%20sponge%20is%20very%20passively%20safe.">海绵安全</a>”狭义人工智能系统的延迟部署，而这些系统本可以改善或拯救大量人的生命。放慢速度确实要付出代价。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:20:44 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:20:44 GMT" user-order="3"><p>我认为我的意思是暂停开发和部署。我真正想要完成的是让我们摆脱这种奇怪的情况，我们必须证明人工智能是危险的，而不是人工智能开发人员必须证明他们继续进行是安全的。我什至不确定是否有一种安全的方法来了解高级人工智能是安全的！我希望社会看到，鉴于这种巨大的风险和不确定性，允许人工智能继续开发或部署是多么荒谬。<br><br>总的来说，这对我来说比承认人工智能也会带来好处更重要，尽管我很高兴承认在适当的环境下——重点可以保持正确。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:24:26 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:24:26 GMT" user-order="2"><p>这对我来说更接近症结。我基本上认为大多数人工智能总体上是好的，已经总体上是好的，并且至少在一段时间内将保持总体上的好。虽然存在严重的安全问题，但它们与任何其他技术没有什么不同，我们社会开发技术和应对新技术的正常方式可能足以胜任这项任务。就像，你允许公司开发人工智能并部署它，然后发现它是有偏见的，人们对公司大喊大叫，政府做事，他们修复它，总的来说，你做得比如果每个人都做得更好公司被要求在部署之前清楚地证明系统根本没有任何问题，因为那样你就会减慢一切速度，以至于很多问题将在更长时间内得不到解决，这会带来更高的成本</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:27:53 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:27:53 GMT" user-order="3"><p>您不认为人工智能的 0 天问题比其他技术更多吗？如果强大的人工智能的早期错误太大而无法迭代纠正怎么办？<br><br>它与其他技术在本质上没有什么不同，但在能力的大小和不可预见问题的潜在后果方面有所不同。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:33:26 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:33:26 GMT" user-order="3"><p>除了关于人工智能是否太危险而无法进行迭代修正的对象级问题之外，还有一个问题是倡导者是否需要公平对待人工智能的类别，或者只关注最严重的危险。我并不担心人们是否会对人工智能有偏见，因为我正在谈论我认为最严重的问题。我希望人们有准确的想法，但我能传达的信息有限，所以我把警告放在第一位。<br><br> （写了这篇关于如何描述技术作为一个整体是好还是坏的问题是不是达到同样的目的： <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error">https://hollyelmore.substack.com/p/the-technology-bucket-error</a> <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error)">）</a></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:34:03 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:34:03 GMT" user-order="2"><p>是的，我想这种事情大致与技术的力量及其开发速度成正比。当然，人们的抗议是社会处理此类事情的现有机制的一部分，所以不这样做并不是理由。但我认为 AGI 的案例确实与之前所有的技术有根本的不同，因为</p><ol><li>可能会出现真正的权力中断。如果 AGI 使你的研究自动化，并且你在一年内获得了未来 50 年的新技术，那感觉就像是一种不同</li><li>普通技术（包括狭义人工智能）基本上可以让人们得到更多他们想要的东西，而大多数人基本上都想要好的东西，所以普通技术最终大多都是好的。 AGI 创造了从技术广泛地满足人们想要的东西到技术本身获得<strong>它</strong>想要的东西的转变的可能性，这几乎可以是任何东西，并且与人类价值观完全分离</li></ol><p>我觉得强烈应对 AGI 风险的论点因与也适用于互联网等论点相关而被削弱。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:35:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:35:58 GMT" user-order="2"><p>就像，关于新技术，以及“这个技术或那个技术是好是坏”，关于当前人工智能的大多数讨论都很好地符合这一传统，我想说“<strong>不</strong>，AGI 是它自己的、独特的、更严肃的东西”</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:38:51 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:38:51 GMT" user-order="3"><blockquote><p>我觉得强烈应对 AGI 风险的论点因与也适用于互联网等论点相关而被削弱。</p></blockquote><p>从倡导的角度来看，我不确定是否如此。这是在争论中，但我真的不知道在倡导背景下提出其他论点/问题是否会削弱处理 x 风险的意愿。暂停的原因有很多，从对自动化取代工作的相当老的担忧，到对其他代理篡夺我们而不关心什么对我们有好处的 x 风险担忧。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:39:32 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:39:32 GMT" user-order="2"><p>我认为，如果你说一件强烈的事情，不同意的人可能会忽略你或对你的强烈事情做出反应。如果你说三件事，不同意的人可以只回应最弱的一件事，并且看起来获胜</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:40:32 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:40:32 GMT" user-order="1"><p></p><blockquote><p>我认为，如果你说一件强烈的事情，不同意的人可能会忽略你或对你的强烈事情做出反应。如果你说三件事，不同意的人可以只回应最弱的一件事，并且看起来获胜</p></blockquote><p>这让人想起“人工智能不杀死所有人主义”一词的创造。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><p>（元注：在对话的这一点上，有一个短暂的停顿，之后罗布和霍莉转而进行口头对话，然后进行转录。）</p><h2>对一切事物都持技术乐观态度……除了 AGI </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:41:22 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:41:22 GMT" user-order="2"><p>好吧，我们刚才在哪里？我想我正在描述我有时对这些论点感到的沮丧？我会尝试重述一下。</p><p>所以我仍然主要是一个技术乐观主义者，在某种程度上，这是一个连贯的事情（我认为它广泛地不是......成为“乐观主义者”可能是一个桶错误。）但是，是的，技术，从广义上讲，让人们得到更多他们想要的东西。当事故发生时，或者当人们想要的东西实际上并不对他们有好处时，这有时会成为一个问题。</p><p>但通常情况下，人们得到他们想要的东西是件好事。就我而言，这实际上是定义“好”的合理基础。因此，大多数技术总体上表现良好。如果你只有一个刻度盘，比如“更多”或“更少”——我知道你没有，但如果你有——技术就会变得很好，事实上非常好。然后，通用人工智能是另一种奇怪的东西，你拥有一种技术，可以得到更多<i>它</i>想要的东西，而不是更多人们想要的东西。这使得它在技术类别中独一无二，因此需要一种不同的方法……因此，在谈论人工智能风险时，使用似乎也适用于一般技术的论点是令人担忧的，因为人们担心的许多其他技术大约结果很好。感觉这让人们很容易解决这部分问题，或者将其纳入现有的辩论中，而忽略其中我认为真正重要的部分——这是事物的 AGI 方面。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:31 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:31 GMT" user-order="3"><p>我认为这触及了本次讨论的另一个普遍主题：倡导沟通与 LessWrong 上的沟通不同。它不太精确，你的空间也较小。</p><p>我同意，对于某些人来说，能够将有关 AGI 危险的论点与他们已经听说过的有关技术的论点进行模式匹配是一个陷阱，而这些论点几乎全部都支持技术是好的。这似乎确实可以削弱这一论点。我还认为，对于那些在这个话题上真正受过教育的人以及我们圈子里很多对科技了解很多并且普遍支持科技的人来说，这可能是他们心理上非常重要的论据。对他们来说，技术在过去总是表现良好，这可能真的很重要。</p><p>但当然我应该指出，世界上很多人并不认为技术在过去总是表现良好，而且这次很多人真的很担心，即使这已经发生过一百万次了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:42:34 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:42:34 GMT" user-order="2"><p>是的。这实际上与另一件事有关，这件事让我对整个情况感到沮丧：谈话似乎……对于这个话题来说太小太窄了？从某种意义上说，我应该期望有一个倡导团体会说“是的，我们是勒德分子，并且感到自豪！”我们不喜欢这项技术的发展方向，我们也不喜欢整个技术的发展方向，我们想阻止它。”</p><p>我想对那群人大喊大叫，并让这成为谈话的一部分。我不希望这个团体也提出我同意的合理主张。我希望这两者单独存在。</p><p>我希望奥弗顿窗口中的人都在这个范围内，因为这些是人们所持有的真实位置，而且那些不疯狂的人也<i>可以</i>持有。我认为他们错了：但他们不是坏人，也不是疯子。令人沮丧的是，人数如此之少，以至于您需要各个倡导团体来涵盖广泛的意见，而其中实际上存在很多冲突和分歧。目前，人太少了，我不得不说“他们在某些事情上是对的，总的来说，他们的声音在那里是有好处的，所以我不想对他们大喊大叫”，或者类似的话。</p><p>如果你是“暂停人工智能”倡导团体，这会让你陷入困境。我现在对此更加同情，因为你试图获得更广泛的人的支持，也许你必须牺牲一些正确性才能做到这一点。但如果每个人都提倡他们真正认为正确的事情，那就太酷了——因为有足够多的人有足够多的意见，这样你就可以在一个合理的范围内进行真正的讨论。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:59 GMT" user-order="3"><p>我认为牺牲的不是正确性，而是特异性。不太具体的陈述仍然可以是真实或清晰的。<br><br> PauseAI 传达的信息是广泛的；它并不是说“我希望人工智能以这种特定方式暂停”，而是“我希望人工智能无限期暂停”。 “暂停人工智能”并没有具体说明多久，比如多少年——我知道，即使我们如愿以偿，各种因素也可能会影响它实际暂停的持续时间。在倡导过程中，你经常需要推动一个方向，而不是精确的坐标。</p><p>基于联盟的信息的一个重要问题是，如果被误解，它们似乎会提倡与预期目的相反的内容。 PauseAI 不会受到这种歧义的影响。其目的是将举证责任从“我们需要证明人工智能是危险的”转变为“我们有理由相信人工智能是不同的，并且你必须在这样做之前证明构建它是安全的。”尽管由于其潜在的好处，有令人信服的理由加速人工智能的发展，但它并不是压倒性的令人信服。对于世界上大多数人来说，等待通用人工智能的时间更长一点以换取更高的安全性是一种公平的权衡，特别是如果他们了解其全部含义的话。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:04 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:04 GMT" user-order="2"><p>这是一件困难的事情。我相信人们之所以不渴望 AGI 早两年到来，是因为他们没有意识到……要么他们没有意识到这是可能的，要么他们不理解其全部含义。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:24 GMT" user-order="3"><p>我相信还有许多其他问题人们没有意识到其紧迫性。例如，提前一年发明新疫苗可能意味着挽救数千人的生命。通常，我是强调这一点的人。确实，许多人当时并不感到紧迫，同样，他们现在也不感到紧迫。但在这种情况下，他们通常的倾向可能会引导他们走向正确的方向……人工智能风险可以通过更多的时间来减轻。虽然如果无法安全地建造通用人工智能我会感到难过，但我认为我们应该推行一项政策，以适应我们无法安全地建造通用人工智能的可能性。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:28 GMT" user-order="2"><p>嗯，它永远不会真正安全，因为“安全”被定义为零风险，这显然是不可能实现的。所以问题是：什么时候做某事的好处大于其风险？我们目前肯定不在那儿。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:41 GMT" user-order="3"><p>我实际上认为倡导提供了一种回避这个问题的方法。确切地说，我们应该容忍多少风险才能获得收益，这似乎是一个主观问题。然而，事实上，大多数人在此类问题上都相当厌恶风险。如果你询问大多数利益相关者，即全球人口[注意：利益相关者是地球上的人口，但我认为民意调查是针对美国人的]，他们通常倾向于规避风险的方法。在这种情况下这似乎是正确的。这感觉与我们通常考虑的风险/回报不同。我们没有特别的能力去思考像全球核战争这样重大的事情的风险，这可以说比人工智能出错更容易生存。</p><p>我觉得我比很多朋友更厌恶风险。我的直觉是，我们现在不要走这条路。我觉得如果你向我提供不同方面的各种虚假实用程序，我就可以得到任何假设的正确答案。但真正的问题是，我们是现在采取行动还是稍后采取行动？我认为人工智能进步的任何暂停<i>最终</i>都会被解除，除非我们的社会因其他原因崩溃。我认为，从长远来看，如果可能的话，我们将实现安全的通用人工智能，并因为我们暂停而获得大量价值。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><h2>无知和稳健的好计划</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:11 GMT" user-order="2"><p>嗯...我想我确信暂停比不暂停更好。我不相信的是，如果你主张暂停，世界会比你不主张暂停的世界更好。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:22 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:22 GMT" user-order="3"><p>这很有趣，因为我正在与 10 月 21 日抗议活动的联合组织者之一交谈，他说实际上他不确定暂停是好的，但他更确定主张暂停是好的。离开时，他更加相信倡导绝对是好的，而不是暂停是正确的政策决定。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:28 GMT" user-order="2"><p>这又回到了第一件事……当涉及到技术问题时，你可以坐下来把它拆开。它很复杂，但这是一种易于处理的复杂。但对于倡导来说……甚至感觉很难知道任何事情的迹象。这让我想绝望地逃离这一切。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:41 GMT" user-order="3"><p>我觉得我在人工智能安全领域认识的人认为政府/政治的事情比技术的事情更难。你是这样想的吗？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:44 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:44 GMT" user-order="2"><p>是的。太混乱了。您正在使用的系统非常……表现不佳。政治形势是前所未有的，至少在上个世纪的各个时期都是前所未有的。我确信有些人读过大量的历史和哲学，并相信他们能够辨别所有这些事件的大弧线，并了解该拉动什么杠杆以及何时拉动，但我不相信这些人真的知道什么他们认为他们知道——他们只是得到很少的反馈循环来根据现实测试他们的模型。出现意外因素并导致行动产生与预期效果相反的情况似乎很常见。<br><br>这不是不尝试的理由。但这给我一种无知的感觉，与我认为倡导所需的热情不相容。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:55 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:55 GMT" user-order="3"><p>我确实对暂停消息感觉很好。这是我一直产生共鸣的一个。并不是每一条信息都能引起我的共鸣。例如，你在 Meta 参加的抗议活动的信息结果证明是一个更复杂的信息来传达我所期望的信息。</p><p> [倡导是棘手的，但技术问题却不是]的想法可能是一个双重症结？</p><p>听起来你更担心阻碍技术进步，并把针穿得恰到好处……但我觉得我们谈论的是几十年的损失，我们必须人为地阻止 AGI 的到来，因为如果你允许一切按其节奏发展，那么它会更快到来，但并不安全。</p><p>这就是我说“暂停”时所想象的。我认为这显然是一笔值得的交易。因此，我更愿意说“暂停人工智能，直到它安全”，而不是我对任何协调政策所做的事情。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:01 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:01 GMT" user-order="2"><p>我一直觉得唯一要做的显然是“加快和改进对齐研究”。只需将资源和注意力集中在问题上，并鼓励相关人员更好地了解情况即可。这感觉相当稳健。</p><p> （另外，在某种审美层面上，我觉得，如果你说出自己真正相信的事情而让事情变得更糟，那么你在道德上的地位比你试图变得不择手段而让事情变得更糟要好得多。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:11 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:11 GMT" user-order="3"><p>我不是人工智能方面的专家，但我感觉我们还没有走上解决这个问题的轨道。我不确定仅靠促进比对研究是否可以解决这个问题，因为我们可能只有 5 年的时间来解决这个问题。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:15 GMT" user-order="2"><p>是的......我不知道，也许鼓励实验室了解情况甚至不是很好。很难理解“嘿，这是一件需要认真对待的事情”，而不同时理解“嘿，通用人工智能是可能的，而且非常强大”——这是一条信息……如果不是每个人都意识到这一点，你可能就在更好的情况下。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><h2>认真对待信仰，将技术进步作为倡导的推动力</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:30 GMT" user-order="3"><p>我认为仅仅向人们提供信息是不够的；他们常常不知道如何解释或采取行动。我转向抗议的原因之一是它们发出了明确的信息。当人们看到抗议活动时，他们立即意识到抗议者发现了一些不可接受的事情。这种理解通常比人们从权衡利弊的博客文章中获得的理解更为深刻。</p><p>人们倾向于认为，如果他们了解大多数人不知道的关键信息，特别是如果赌注会改变世界，他们就会走上街头倡导变革。人们对于人工智能安全为何并非如此存在困惑。我个人感觉很困惑。</p><p> （最极端的情况是，埃利泽建议，没有任何暴力可以阻止 AGI，即使它会杀死地球上的每个人……我也认为你应该对暴力有一个义务论的侧面约束，因为它总是如此很容易这样想，但这不会有任何帮助；最终只会适得其反。但我很困惑，我真的很欣赏《时代》杂志的文章，其中他提到使用与其他国际条约相同的国家暴力来执行条约。 ）</p><p>尽管如此，在很长一段时间里，我都不确定人工智能安全社区的承诺，因为他们似乎不愿意采取更明显、更清晰的行动，比如上街抗议……尽管我本人通常不参加抗议活动。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:35 GMT" user-order="2"><p>我想这可能是性格的问题。我觉得我不是一个去抗议的人。现在这就是我与世界互动的方式。</p><p>我认为这也是我对技术总体立场的一部分。技术是一个如此大的杠杆，它几乎……最终成为最重要的事情，对吗？就像是…</p><p>好吧，现在我要说一些有争议的话。我认为，人们更多地关注倡导者而不是技术专家，这可能是一个错误？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:47 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:47 GMT" user-order="3"><p>我已经知道我同意了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:54 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:54 GMT" user-order="2"><p>你可以倡导妇女权利，但你也可以发明有效、可靠的节育措施。哪一个会产生更大的影响？我认为这就是技术。活动人士需要推动变革，但技术使变革在政治上变得可行。我认为人们就像……巨大的经济变革浪潮顶部的泡沫，改变了政治和社会的可行性。例如，我认为我们度过气候危机的能力主要取决于可再生能源技术（如果通用人工智能没有首先发生）——使清洁能源比化石燃料更便宜，人们就会转向。我认为所有的素食主义行动最终可能会被优质的替代肉类所压倒。当肉类产品与养殖肉类没有区别但价格稍微便宜时，素食主义的争论将神秘地变得对许多人来说更加有吸引力。在这两种情况下，倡导者所做的一件大事就是为资助研究提供支持。我什至记得读过有人建议废奴运动在有枫糖浆的地方确实受到了关注，因为<i>这些人可以获得相对便宜的替代糖供应，</i>所以道德对他们来说更便宜。显然，积极分子对于推动这些论点至关重要，但通常这些论点只有在技术/经济/实用主义使它们变得可行之前才会获得关注。令人惊讶的是，人们在方便的时候竟然能够相信谎言，而技术可以改变方便的事情。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:01 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:01 GMT" user-order="3"><p>我们可以称之为“历史有效市场假说”之类的。因为我经常听到这句话，这也是我世界观的一部分。只是还不足以让我不考虑激进主义或倡导。我认为这基本上都是正确的，我非常同意它，但同时你仍然可以在地上找到 20 美元。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:05 GMT" user-order="2"><p>事实上我上周就这么做了:)</p><p> [后来补充：这让我想，也许这就是做到这一点的方法：而不是寻找最好的改变，你寻找技术和经济已经朝着好的方向转变的地方，所需要的只是很少有激进主义来催化变革，并将重点放在那里。但可能大多数地方已经有足够多的积极分子，你不会在边际上做太多事情。]</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:28 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:28 GMT" user-order="3"><p> [稍后补充：目前AI安全的积极分子肯定不够！这里出现“市场失灵”是有道理的，因为迄今为止理解这个问题需要大量的技术知识，并且需要在一个相当厌恶倡导的社交圈中运作。]<br><br>也许这是一种谦虚的认识论形式，如此严重地遵循这一点，以推翻我们实际目睹的一切。我什至认为有些人知道你可以通过倡导做什么。他们了解政策，并且明白即使影响一小部分人也可以改变奥弗顿窗口。尽管如此，即使是那些内部认为倡导有时会起作用的人，也会纠结于这是否是正确的方法，或者感觉它<i>不应该</i>起作用，或者他们应该专注于他们认为更普遍重要的事情（或确定更多）。</p><p>然而，我认为我们正在讨论的是一个独特的时刻。这与历史上的倡导是否能够超越技术无关。这是关于我们现在是否可以通过宣传来争取更多时间，确保在足够的安全措施到位之前不会开发通用人工智能。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:30 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:30 GMT" user-order="2"><p>肯定有一个观点认为，如果你有能力做技术工作，你就应该做。但实际上这样的人只是极少数。更多人有能力倡导，因此他们会带来不同的技能。</p><p>事实上，我认为我已经确定了一种氛围，许多能够从事人工智能安全技术工作的人可能会产生共鸣。这感觉类似于在学校/大学被分配一个小组项目。你有两个选择：要么以某种方式说服这个小组中的每个人共同努力正确地完成该项目，要么自己完成整个项目。通常第二个看起来要容易得多。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:39 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:39 GMT" user-order="3"><p>不过，如果您不明白该练习是关于学习如何在团队中工作（而不是完成作业），您可能会错过很多教育……</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:47:10 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:47:10 GMT" user-order="2"><p>是的。嗯……也许这其实是大家教育的一个错误教训！我们惨痛地认识到“解决技术问题比解决政治问题容易得多”。但小组项目的技术问题却很容易被人为地解决。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:47:38 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:47:38 GMT" user-order="3"><p>我对自己有点恼火，因为我没有在社区早期质疑这种氛围，因为我有知识……而且我认为我推迟了太多。我想，“哦，好吧，我对人工智能或机器学习了解不多——我一定错过了一些东西”。但我拥有质疑社会政治方法所需的一切。</p><p>我对技术问题如何解决的直觉受到我的生物学背景的影响。我认为生物学让你对理论和现实世界的东西如何协同工作有非常不同的见解。这是一种更狂野的西部科学，其中的东西总是让你感到惊讶。你可以执行相同的协议，但它以同样的方式失败 60 次，然后它突然起作用了。你认为你知道某件事是如何运作的，但突然发现你不知道。</p><p>因此，我的所有直觉都强烈地表明，在野外解决技术问题是很困难的——有可能，但真的很难。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><h2>倡导的美学，出现不是因为你最适合，而是因为没有其他人愿意</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:50:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:50:55 GMT" user-order="2"><p>我认为这里存在审美冲突。我有一种直觉，或者说……一种审美冲动，基本上告诉我……“倡导是愚蠢的”。每当我看到有人进行激进主义时，他们通常会……说一堆……显然是错误的事情？他们举着一个标语，上面的口号太简单了，不可能是事实，并尽可能大声地喊出这个明显过于简单化的东西？这感觉像是过度自信的典型。</p><p>这确实与我的理想相冲突，我的理想是说尽可能接近事实的事情，并具有我所拥有的证据和推理所保证的信心程度。这是一种非常不同的氛围。</p><p>然后是一种反驳，认为从经验上——战略上，或者至少是战术上——公然的信息传递才是真正有效的。也许那个积极分子实际上有更微妙的观点，但这并不是游戏的玩法。但后来我的一部分感觉是“如果这就是游戏的玩法，我不想玩那个游戏”。我不知道我在多大程度上支持这一点，但我的感觉非常强烈。我猜想很多“LessWrong-y”类型的人也有类似的感觉。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:12 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:12 GMT" user-order="3"><p>我认为人们没有像你一样清楚地表达这一点，但我从很多人那里得到了同样的感觉。</p><p>而且……这确实有点伤害我的感情。因为我<i>也</i>想准确。我也是其中之一。</p><p>我想我只是有足够的实用主义，我能够超越审美并看看激进主义是如何运作的？我也更多地接触到了倡导活动，因为我一生都在参与动物福利工作。</p><p>我有过举办上次抗议活动的经历，我收到了一条我认为非常简单明了的信息……我确保人们可以在某些地方进行更深入的研究并获得完整的声明。然而，混乱的程度却是惊人的。人们对此有自己的成见；一些人对这个问题有强烈的感受，不喜欢激进的语气，而另一些人则认为这种语气过于技术性。人们误解了我认为很简单的信息。核心信息是，“Meta，不要分享你的模型权重。”这相当简单，但后来我什至在 Twitter 上被暗示是种族主义者，因为据称我声称外国公司无法构建像 Meta 一样强大的模型？虽然我不认为这是一个善意的解释，但如果你在这样一个充满争议的舞台上给别人留下任何误解你的机会，他们就会这么做，这太疯狂了。</p><p>我认为在这些情况下，你正在以一种几乎敌对的方式与媒体打交道。你不断地努力确保你的信息不会被误解或错误引用，这意味着你要避免细微差别，因为它可能会蜿蜒到你不准备说的话，然后有人可能会误解它是不好的。</p><p>这次经历令人大开眼界，特别是因为我认为自己不太有政治头脑。我必须通过艰难的方式来学习这些错综复杂的事情。我相信这里还有更多理解的空间。我从很多人那里感受到了一种情绪：“我很聪明，而你所做的事情似乎很愚蠢，所以我不会这么做。”也许如果人们对不同形式的沟通有更多的经验，就会对不同类型的消息优化的环境有更多的了解？感觉推理上有很大的差距。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:15 GMT" user-order="2"><p>是的，这听起来令人沮丧。我想这里面有一些关于……“理性”的东西。因此，在大多数人都得到错误答案的技术或哲学问题上试图获得正确答案的任务感觉就像是……半导体制造或类似的事情。就像“在这个学科中，我已经尽力掌握，我必须确保戴上口罩，并遮住头发，这样就不会有颗粒进入空气中。整个实验室都是正气压、高端过滤系统.在我思考这个话题之前，我会经历一个气闸，我会尽可能地清除所有污染，尽可能地将自己从方程中删除，我从不用手触摸任何东西。因为我正在使用精致而敏感的仪器，而且我&#39;我试图做到准确和精确……”。这听起来很古怪，但我认为那里有真实的东西。</p><p>在这个比喻中，为了在世界上取得成功，你还必须做的另一件事就像……泥浆摔跤，或者其他什么。这可能不是一项更容易或价值更低的技能，但确实感觉你无法同时做这两件事。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:30 GMT" user-order="3"><p>如果它的措辞更像是“​​有这项技术。这是相当低技术含量的。这是一个拼凑。但它<i>有效</i>”？</p><p>我记得我第一次为人工智能安全而哭泣的那一刻：就在我看到 FLI 信函之后公布的民意调查时，民意调查显示——我忘记了确切的措辞，但类似的话——大多数人都赞成暂停。 “天哪，”我想，“我们并不孤单。我们不必再默默无闻地辛苦工作，试图找出一条无需其他人帮助即可行之有效的道路。”我欣喜若狂。当我发现这一点时，我感到非常自豪和高兴，就像我为我真正重要的半导体找到了一个关键的破解方法一样。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:33 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:33 GMT" user-order="2"><p>嗯...是的，这显然很重要并且可以发挥作用，但是当我处于那种无尘室心态时，我的直觉反应是我仍然觉得我想与它保持一定的距离吗？我不相信这种技术含量低的东西不会泄漏并弄乱实验室。</p><p>当你开始与……争论时（我指的是“一个”，而不是你个人） - 当一个人开始使用政治辩论的讨论规范与人们争论，并且你试图获胜时，我认为这引入了一大堆干扰的扭曲和影响。也许你以某种方式表达事物或框架事物，因为它更有说服力，然后你忘记了为什么这样做，只相信你扭曲的版本。你会激励自己更加自信地说出事情，而不是相信它们，并且相信事情就像你说的那样自信。也许你将自己的<a href="http://www.paulgraham.com/identity.html">身份</a>与对象级别的问题联系在一起超过了必要的程度，并且/或者你切断了自己的<a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat">退路</a>，因此发现错误或改变主意之类的事情会格外痛苦。我有时会在自己身上注意到这种事情，而且我希望我不会更多地注意到它，而且我什至没有真正试图用我的工作来对抗人们，只是在公共场合解释事情。</p><p>我觉得这里有一个三难困境。你必须在 1) 真理/“理性”/诚实之间做出选择，2) 实用主义，以便在混乱的政治世界中完成任务，或者 3) 两种不同模式的组合，你尝试在它们之间切换。<br>选择 1 或 2 都会放弃另一个重要的东西，而选择 3 并不真正可行，因为它所需的划分与 1 背道而驰。</p><p>我对事实有这样的感觉：你不能破例，有时只能相信实用的东西。你不能说，“在这种情况下，我将按照不同的规则进行操作”。我觉得这样做并不安全，就像不能信任人类硬件来维持这种分离一样。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:46 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:46 GMT" user-order="3"><p>我想我不能让自己那么担心污染……我确实认为切换模式是可能的，而且我发现理性主义者常常对此非常偏执，就像一只脚趾不合时宜一样，他们永远无法真诚地重视和再次寻求真理。我一直在<a href="https://hollyelmore.substack.com/p/scrupulosity-my-eagxboston-2019-lightning-talk">与谨慎（强迫症）作斗争</a>，这让我想起了这一点。</p><p>也许是因为我直到晚年才开始接触理性TM，所以我已经有了自己的一套与真理相关的方式以及真理的重要性。与其说是通过“语音代码”或倡导似乎违反的事情？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:47 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:47 GMT" user-order="2"><p>是的。也许这种“洁净室”心态正是 15 多年前认真认识和解决这个问题所需要的……这种心态强烈专注于弄清楚什么是真实的，无论外部意见或如何看待它。当时，社会因素可能使人们无法看到情况的真实范围。现在，现实开始打在每个人的脸上，所以，也许这种高度的专注不再那么重要了。但到目前为止，该领域最杰出的人士已经长期沉浸在这种可能与倡导心态不相容的心态中？ （这只是我刚才随机想到的一个假设。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:52:05 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:52:05 GMT" user-order="3"><p>我不确定我是否具有倡导心态，也不相信这是强制性的。然而，我确实看到了倡导的价值，也许比 LessWrong 上的许多人更清楚。讨论你15年前提到的理解这个问题所必需的美德，我可能没有具体体现这些美德，但我认同其中的许多美德。</p><p>我觉得它们对于我能够进行宣传是必要的，因为人工智能安全/LessWrong 非常下意识地反对它。尽管埃利泽在此事上表示支持国际政治，但我感觉自己像个局外人。如果我一直担心公众舆论，我就不会冒险进入这个领域。我坚信这确实是我看到的其他人没有抓住的最大机会。我觉得他们的不情愿与你强调的因素（以及其他一些事情）有关。</p><p>这不是一个大群的取悦者。没有人真正喜欢它。</p><p>早些时候，一些人表达了对抗议的担忧，认为抗议可能会腐败，因为它“太有趣了”。他们的形象是抗议者对自己独特的身份感到高兴。抱歉，但是我们都没有真正玩得开心。我正在努力让它变得有趣。我认为从厄运中得到一些解脱可以而且应该很有趣。如果您在这项活动中不必感到痛苦，我会很高兴。我们可以团结起来，尝试做一些事情来对抗厄运。</p><p>也许这会很有趣。但这并不有趣。这主要是因为我们社区的抵制，也因为人们认为，开源 ML 社区非常令人讨厌。</p><p>所以我从人们那里听到了你所说的：“哦，你的洁净室很容易被污染；”你可以沿着这个激励梯度下滑”：但事实并非如此。我不只是沿着梯度下滑去做更容易的事情，因为这实际上相当困难。</p><p>最后：每个人都认为他们知道如何进行抗议，并且他们不断给我建议。<i> </i>我收到大量消息告诉我应该采取哪些不同的做法。比我做更多身份一致的技术工作时要多得多。但没有其他人会<i>真正这样做。</i></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:53:13 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:53:13 GMT" user-order="1"><p>在罗布回答之前，我想表达一些同情心。这听起来确实很艰难。我还没有决定如何抗议人工智能安全，但我确实对“做看似最重要的事情，而似乎没有人会做的事情”的动议深表同情。我认识到，这样的尝试对你来说既困难又吃力不讨好。谢谢你仍然努力。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:17 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:17 GMT" user-order="2"><p>是的，我也很欣赏。我认为你的行动主义与刻板印象有很大不同。我也对“没有其他人真正会这样做”产生了同理心，因为我对 YouTube 也有这种感觉一段时间了，我想更广泛的外展和沟通。</p><p>因为，这很奇怪，这是<i>我的</i>工作！很长一段时间，我是唯一一个这样做的人。我当时想，“我真的是最有资格做这件事的人吗？真的吗？根本没有其他人在做这件事，所以我想我会做，我想我会成为世界上在这件事上最好的人，仅仅因为实际上没有其他人尝试？”。多年来，我一直觉得“嘿，人工智能安全显然是世界上最有趣的事情，也是世界上最重要的事情……而且只有一个 YouTube 频道？就像……什么！？”</p><p>所有这些研究人员都写论文，人们读了论文但并不真正理解它们，我想“为什么你在学习如何写作上投入这么少？”在人们面前进行研究？”。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:53:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:53:20 GMT" user-order="3"><p>不，那么你会受到真正的批评:)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:24 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:24 GMT" user-order="2"><p>简直令人困惑！如果你是一名研究人员，你会指望 AGI 发生时你就在房间里吗？在键盘上？可能不会。因此，无论你的工作产生什么影响，它都会通过其他人的理解来实现。所以这不是一个可以选择性做好的工作。这对你的工作效果来说是一个简单的乘数。</p><p>看来你也有类似的感受：“这看起来很关键，为什么其他人没有这样做呢？”</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:10 GMT" user-order="3"><p>是的。我环顾四周，心想，如果人们愿意做更多的宣传活动，我很愿意自愿参加宣传活动。其他城市很早就有其他人，湾区也有人对更具政治性的做法感兴趣，但没有人愿意在这里进行公共宣传。我能感觉到你所描述的背后的恶心。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:13 GMT" user-order="2"><p>男人。听起来，全力投入宣传工作有点像<a href="https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent#:~:text=Lonely%20dissent%20doesn%E2%80%99t,the%20pack.">穿着小丑服来上学</a>。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:35 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:35 GMT" user-order="3"><p>是的！</p><p>还有一件事让我觉得有必要去追求它。这就像当我还是一个素食主义者的时候，我总是听到成年人关于吃肉的可怕争论。没有压力去好好争论。他们已经知道结果会如何：社会已经对他们有利，而我还是个孩子。但这显然是错误的，他们根本不在乎，这让我很困扰。</p><p>然后，在以利以谢的《时代》文章发表后，我再次看到了那些不想走向政治方向的人的墙。我真正认为已经处理了这种情况的人只是诚实地说了最愚蠢的事情，并且只是刻薄、糟糕的论证;取笑别人或采取任何隐蔽的方式来拉高排名并避免参与争论。</p><p>如果不是亲眼目睹，我不会觉得自己有资格做这一切。我只是想“好吧，碰巧我比你们任何人都积累了更多关于倡导的知识，我真的觉得你们对于为什么这行不通的看法是错误的”。我觉得确实需要有人来做这件事。如果我瞥见“就是这样，我们被杀了，它正在发生”而我什至没有尝试过，我会有什么感觉？这完全是一种“不充分平衡”的感觉。</p><p>我想我是技能方面的多面手，并且能够在这里适应这一点……但我的培训是进化生物学。我以前在智库工作过。我真的感觉自己不知道的事情还有很多。但我向你保证，没有人比他更有资格愿意这样做。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:37 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:37 GMT" user-order="2"><p>是的。</p><p>这些都不让我想要真正去做。但它确实让我希望有人这样做。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:56 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:56 GMT" user-order="3"><p>我不希望你改变你正在做的事情。似乎正在发挥作用。停止关心外展的准确性对你来说也没有好处，哈哈。 （需要明确的是，我不认为我们的抗议标志不准确；这只是沟通分辨率的不同级别。我正在写一篇关于此问题的帖子。[<a href="https://x.com/ilex_ulmus/status/1709706321203990886?s=20">关于此主题的推文主题</a>]）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><h2>结束语</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:58 GMT" user-order="2"><p>最后，我有兴趣分享一些增量/更新。</p><p>我已经开始相信，自认为理性主义者的人不愿意认真考虑倡导，这本身就是理性的失败。我还没有完全相信，但似乎值得更详细地研究一下。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:10 GMT" user-order="3"><p>如果人们不妨碍宣传，那就真的很有帮助。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:13 GMT" user-order="2"><p>我很好奇……现在的倡导方式是什么？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:24 GMT" user-order="3"><p>也许最重要的是人们认为他们的工作前景会受到影响。我不知道这有多真实。老实说我不知道​​。我怀疑人们有理由高估这一点，或者只是方便地相信这一点，因为他们出于其他原因不想更新。但我认为他们担心实验室不会喜欢抗议活动。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:28 GMT" user-order="2"><p>哦，我对此没有同情心，这听起来像是直截了当的怯懦……我的意思是，我想有一种情况是你需要在房间里，你需要保持访问权限，所以你应该等待时间或无论如何，不​​要掀起波澜……但这对我来说感觉很事后。你不会因此被解雇；说你相信什么！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:56:50 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:56:50 GMT" user-order="1"><p>霍莉，你想分享更多谈话中的最新动态吗？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:56:57 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:56:57 GMT" user-order="3"><p>好吧，罗布确实给了我+10的感觉，我发现这里的人们厌恶基于理性主义原则、本能和美学的倡导或看起来像倡导的东西。</p><p>我当然也觉得我更好地理解了罗布的想法。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:57:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:57:05 GMT" user-order="2"><p>是的，研究我对这些东西的态度以及它的来源是很有趣的。我现在感觉更清楚了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:59:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:59:20 GMT" user-order="3"><p>我想很多人都有类似的内心反应，但不会对我说出来。所以我怀疑类似的事情经常发生，但我无法与之讨论。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:59:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:59:35 GMT" user-order="2"><p>我很高兴它有帮助！<br><br>好吧，我想就这样吧？感谢您的阅读，呃...别忘了强烈支持、评论和订阅？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><br/><br/><a href="https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy<guid ispermalink="false"> gDijQHHaZzeGrv2Jc</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:04:32 GMT</pubDate></item><item><title><![CDATA[TOMORROW: the largest AI Safety protest ever!]]></title><description><![CDATA[Published on October 20, 2023 6:15 PM GMT<br/><br/><p>明天， <a href="https://pauseai.info/">PauseAI</a>和合作者将在 6 个国家的 7 个地点举办迄今为止最大规模的人工智能安全抗议活动。各界人士热切欢迎！<br><br>当面对面的志愿服务无法用金钱或智力支持替代时，您参加这次抗议活动是一个难得的影响机会——我们以人类本能理解的方式向公众表达我们的关注。另外，我们认为这会很有趣，之后会有友情和饮料。在争取人工智能安全的斗争中摘取新的唾手可得的果实并发挥不同的力量感觉很棒。</p><h2> <strong>10月21日（星期六），多个国家</strong></h2><ul><li>美国，加利福尼亚州，旧金山（ <a href="https://fb.me/1RbYq9H2hOFQ4yi"><u>facebook</u></a> ）</li><li>美国，马萨诸塞州，波士顿（ <a href="https://facebook.com/events/s/pauseai-protest-boston-make-th/6647554948613714/?mibextid=RQdjqZ"><u>facebook</u></a> ）</li><li>英国，议会广场，伦敦（<a href="https://www.mixily.com/event/4774799330762010477"><u>注册</u></a>， <a href="https://www.facebook.com/events/644748401084077"><u>facebook</u></a> ）</li><li>荷兰, 海牙 (<a href="https://www.mixily.com/event/8536294863402363208"><u>注册</u></a>)</li><li>澳大利亚, ​​墨尔本 (<a href="https://www.mixily.com/event/8471341506387452508"><u>报名</u></a>)</li><li>加拿大，渥太华（由 Align the World 组织，请在<a href="https://www.facebook.com/events/243643008241929/"><u>facebook</u></a>或<a href="https://www.eventbrite.com/e/ai-safety-and-ethics-rally-tickets-725729686027"><u>EventBrite</u></a>上注册）</li><li>丹麦，哥本哈根（ <a href="https://www.facebook.com/events/869443424535827"><u>Facebook</u></a> ）</li><li>你的国家在这里吗？<a href="https://discord.gg/anXWYCCdH5"><u>讨论不和谐！</u></a></li></ul><h2><strong>我们为什么抗议</strong></h2><p>人工智能正在迅速变得更加强大，其速度远远快于几乎所有人工智能科学家的预测。数十亿美元正在投入人工智能能力，其结果是惊人的。新模型在很多领域都<a href="https://pauseai.info/sota"><u>超越了人类</u></a>。随着能力的增强，<a href="https://pauseai.info/risks"><u>风险</u></a>也随之增加。科学家甚至<a href="https://www.safe.ai/statement-on-ai-risk"><u>警告</u></a>人工智能<a href="https://pauseai.info/xrisk"><u>最终可能会毁灭人类</u></a>。这种可怕的结果不仅看起来有可能，而且很有可能发生，因为这些结果的平均概率估计<a href="https://pauseai.info/polls-and-surveys"><u>范围为 14% 到 40%</u></a> 。</p><p>我们需要我们的领导人倾听这些警告，但他们并没有像应有的那样认真对待这个话题。目前正在起草人工智能安全立法，但<a href="https://twitter.com/PauseAI/status/1704998018322141496"><u>没有任何一项措施能够真正阻止或延迟超级人工智能的出现</u></a>。<a href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation"><u>超过 70% 的</u></a>人希望减缓人工智能的发展， <a href="https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll"><u>超过 60% 的</u></a>人希望监管积极阻止超级人工智能的出现。为什么没有立法草案真正做到这一点？答案是游说：我们的政治家<a href="https://fedscoop.com/sen-schumer-to-host-musk-zuckerberg-and-other-tech-ceos-for-closed-door-ai-forum/"><u>大多会见人工智能公司的首席执行官</u></a>，他们会推动符合他们利益的政策措施。</p><p> 11月1日至2日，首届人工智能安全峰会将在英国举行。这是迈向明智的国际人工智能安全监管第一步的绝佳机会。</p><h2><strong>我们问什么</strong></h2><ul><li><strong>政策制定者</strong>：不允许公司建立超级智能。法规和硬件限制应在培训开始之前适用，因为一旦实现新能力就很难控制传播。 。我们不能允许公司训练可能毁灭世界的人工智能模型。制定立法很困难，需要时间，但我们可能没有那么长的时间，所以要努力工作，就好像你的生命依赖于此一样。因为确实如此。</li><li><strong>公司</strong>：你们中的许多人都害怕人工智能的能力，但你们陷入了一场竞赛。因此，原则上要大声表示支持暂停。如果你签署了这项技术可能杀死我们所有人的声明，请向世界表明，如果它是一个可行的选择，你宁愿不建造它。</li><li><strong>峰会受邀者</strong>：安全优先于经济增长。我们知道人工智能可以让我们的国家变得更加富裕，但这并不是您被召集到这里的原因。成为房间里的成年人</li></ul><br/><br/><a href="https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever<guid ispermalink="false"> abBtKF857Ejsgg9ab</guid><dc:creator><![CDATA[Holly_Elmore]]></dc:creator><pubDate> Fri, 20 Oct 2023 18:15:19 GMT</pubDate> </item><item><title><![CDATA[The Overkill Conspiracy Hypothesis]]></title><description><![CDATA[Published on October 20, 2023 4:51 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DGsPeE89N93YCwxbH/u1qu8dfmi26tobyfpony"><figcaption>否则我们无法获得完全正确的照明。</figcaption></figure><p><i>阴谋论</i>一词被用作贬义词，暗指表面上的荒谬。但我们使用的词汇存在严重的歧义问题，因为<i>阴谋</i>不是想象中的虚构。普通的<i>阴谋</i>（ <a href="https://en.wikipedia.org/wiki/COINTELPRO">COINTELPRO</a> 、<a href="https://en.wikipedia.org/wiki/Operation_Snow_White">白雪公主行动</a>或<a href="https://en.wikipedia.org/wiki/Gunpowder_Plot">火药阴谋</a>）和它们的戏剧性同族（地平说、登月骗局或咖啡味道不错的滑稽观念）之间存在着明显的和定性的区别，但明确的界限一直难以捉摸，仅仅断言“这个疯了，而这个不是”是令人不满意的。这两个阵营都涉及诡计、恶意意图、秘密行动、错误信息、精心策划的欺骗、隐藏的议程、秘密网络，是的，还有<i>阴谋</i>，但区分两者的尝试却转向了令人不满意或明显误导的领域。</p><p>我要论证的是，解决方案可以归结为对定义的简单重新配置，抓住了荒谬的本质：<i>阴谋论</i>是假设环境使名义上的“阴谋”变得不必要的理论。这就是我所说的“过度杀戮阴谋假说”(OCH)。在我们深入研究这种改进之前，探索一下为什么传统的区别已经不足是很有帮助的。</p><p> <a href="https://en.wikipedia.org/wiki/Conspiracy_theory#Difference_from_conspiracy"><u>《人民百科全书》</u></a>中有关差异的部分展示了其中一些误导性的尝试。例如，阴谋论<i>往往</i>与主流共识相反，但这是对权威的赤裸裸的诉求——这种逻辑会让<a href="http://www.cnn.com/US/9705/tobacco/history/"><u>早期挑战者对吸烟对健康有益的说法</u></a>感到困惑。或者理论描绘了阴谋者的行为极其恶意，但人类确实可以怀有邪恶的意图（<i>一般参见</i>人类历史）。另一个依赖于维持近乎完美的操作安全性的难以置信。虽然情况正在好转，但虽然保密很困难，但这绝对不是不可能。我们有真实的、现实生活中的秘密军事行动或贩毒集团设法经营价值数十亿美元的秘密物流企业的例子。</p><hr><p>从一堆谷壳中仍然可以得到一些有用的指导，那就是阴谋论缺乏可证伪性，并且抵制<a href="https://en.wikipedia.org/wiki/Falsifiability"><u>可证伪性</u></a>。尽管它的名字很不幸，但可证伪性却是我在探索世界时最亲近、最亲近的概念之一。简而言之，可证伪性是指<i>至少在假设上</i>证明理论是错误的能力。典型的例子是“我相信所有天鹅都是白色的，但如果我看到一只黑天鹅，我就会改变主意”。典型的反例可能是<a href="https://archive.org/details/japaneseevacuati00dewi/page/32/mode/2up?q=confirming"><u>约翰·德威特将军引用</u></a>二战期间日裔美国人<i>没有</i>进行破坏活动作为未来破坏计划的<i>证据</i>。阴谋论者确实有一种趋势，他们深入挖掘他们<a href="https://www.lesswrong.com/tag/belief-in-belief"><u>对信仰的信念</u></a>，并将相反的证据视为捏造的，或者是阴谋本身的（更糟糕的）证据。</p><p>我不会谈论可证伪性测试；这真是个好东西。但它也有局限性。首先，缺乏可证伪性只是理论有缺陷的一个很好的<i>迹象</i>，而不是一个<i>决定性的</i>决定。还有一些实际的考虑因素，例如历史事件如何难以应用可证伪性，因为证据不完整或无可救药地丢失，或者新兴科学领域的技术不足如何使一些可证伪的主张（暂时，希望如此）超出审查范围。因此，无法证伪一个理论并不<i>一定</i>意味着该理论是废话。</p><p>除了这些实际限制之外，还有不幸的坏演员因素。具有足够不诚实或自我意识的理论家可以通过采取含糊的暗示来应对可证伪性的生存威胁，以避免被自己制作的鞋带绊倒。由于你无法伪造没有明确提出的内容，因此他们会围绕直接断言跳舞，使他们的主张笼罩在可能性的迷雾中。那么唯一的办法就是更上一层楼，在<i>“谁/如何/为什么”</i>的具体答案仍然难以捉摸的情况下，将模糊性推断为可证伪逃犯的明显标志。将模糊性测试应用于地平理论就证明了这种回避。几乎不可能<a href="https://youtu.be/JTfhYyTuT44?t=1150"><u>从支持者那里得到任何明确的答案</u></a>：“大环球”背后到底是<i>谁</i>，他们是<i>如何</i>欺骗所有人的，<i>为什么为什么为什么为什么</i>有人会为这个计划付出任何努力？相比之下，像<a href="https://en.wikipedia.org/wiki/Atomic_spies"><u>原子间谍</u></a>这样的真实阴谋™缺乏模糊性：苏联/核秘密的秘密传输/地缘政治优势。</p><p>然而，模糊性指控并不适用于所有阴谋论。登月骗局在这一点上出人意料地清晰：美国宇航局/声场/地缘政治优势。这揭示了另一种防止造假的防御机制，即设定高得离谱的证据标准。说到面纱，世界各地的伊斯兰教法都有这样的先例， <a href="http://www.daviddfriedman.com/Academic/Course_Pages/Legal_Systems_Very_Different_13/Book_Draft/Systems/Islamic_Law_Chapter.htm"><u>通奸罪的定罪</u></a>需要有<i>四名</i>目击者目睹<i>同一</i>性行为，而且<i>只有</i>成年男性穆斯林才被视为有资格的证人。这些极其严格的标准似乎是为了应对该罪行可判处死刑的事实，并表明有可能将标准提高到<i>故意</i>使可证伪性变得遥不可及的程度。</p><p>登月骗局可能会受到这些不可能的标准的影响，因为阿波罗 11 号登月被<a href="https://youtu.be/iR3oXFFISI0?t=1054"><u>精心记录了超过 143 分钟</u></a>的不间断视频片段——这个持续时间<a href="https://theconversation.com/moon-landings-footage-would-have-been-impossible-to-fake-a-film-expert-explains-why-118426"><u>太长，以当时可用的技术无法容纳在胶卷上</u></a>。尽管仅略高于<a href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"><u>蜥蜴人常数</u></a>，但令人惊讶的是，有<a href="https://www.voanews.com/a/usa_millions-still-believe-1969-moon-landing-was-hoax/6172262.html"><u>6% 的美国人仍然认为</u></a>登月是上演的。在某些时候，你必须问有多少证据就足够了，但最终没有普遍接受的阈值来回答这个问题。</p><p>因此，可证伪性仍然是一个很棒的工具，但它有合理的实际限制，而且无论如何都不是一个结论性的调查。某人拒绝进行可证伪性仍然是他们意识到并关心将其理论置于审查之下的极好证据，但他们的努力（含糊不清或不可能的标准）仍然会挫败可证伪性的直接应用。那么还剩下什么呢？</p><hr><p>我们终于又回到了过度杀戮阴谋假说，讽刺的是，阴谋论必须假设环境也使阴谋变得毫无意义。解释这一点的最好方法是举例。解构阴谋论就像策划银行抢劫一样令人兴奋，所以请把自己置于负责监督登月骗局的不幸的匿名官僚的立场上。请记住，登月骗局的<i>目的</i>是通过让美国在月球追逐中击败苏联来建立地缘政治声望。因此，无论你制定什么计划，<i>都必须</i>经受住当时最先进的太空计划的审查，该计划雇用了来自世界另一半的最优秀的太空工程师。</p><p>最直接的对策是责成现有的美国宇航局工程师起草<i>完全虚假但绝对合理的</i>设备设计。<i>整个发射的每一个环节</i>——每枚火箭、登月舱、梯子、面板、螺栓、手套、扳手——都需要精心制作，不仅可以欺骗全球观众，还可以欺骗那些在太空中屏息凝视的目光锐利的专家。冷战分歧的另一边。将其扩展到所有通信、视频传输、照片、宇航员证词和“返回”的月球岩石。每一项都必须由专门且高度专业化的顾问进行详尽而细致的检查。</p><p>但它并不止于此，因为你还需要<i>绝对和</i><i>永久的</i>保密，因为任何单一的泄密都会威胁到整个努力。美国很清楚苏联间谍已经成功窃取了严密保护的核机密，所以这里需要的任何反制措施都必须<i>超越他妈的核武器</i>。正如我之前所说，保密并非不可能，只是非常困难。我想美国宇航局可以效仿卡特尔，对任何告密者（以及他们的整个家庭）进行残酷的暴力报复，但这种威慑只有在……人们知道的情况下才能发挥作用。不过，更有可能的是，美国宇航局将使用传统情报机构的广泛审查、选择性招募和丰厚报酬的方法，但现在所有措施都需要进一步扩大，以<i>超越</i>围绕核秘密的保护措施。</p><p>我们正在谈论对数百或数千人进行比核秘密更严格的筛查，同时扩大监视装置以让每个人都遵守规定。你需要将 NASA 的预算增加多少（10 倍？100 倍？）才能投入到这一冒险的行动中，如果暴露，将成为历史上永远的笑柄？如果已经有如此巨大的财富可供支配，那么真正去月球似乎就变得更容易了。</p><hr><p> OCH <sup>®</sup>有几个优点。首先，不要挑战任何阴谋论者的前提。它认为确实存在一个有足够动机的影子阴谋集团，并且只是跟着它运行。这回避了上述关于可证伪性逃犯的任何担忧，并且仍然提供了一个有用的标准来区分普通阴谋和害群之马。</p><p>如果我们将 OCH 应用于原子间谍，我们可以看到阴谋背后的理论不需要任何过分的假设。苏联没有核武器，他们想要核武器，窃取别人的蓝图肯定比开发自己的内部容易得多。必要的假设（苏联有有效的间谍计划）并不能否定阴谋的必要性。</p><p>与桑迪胡克骗局相比，桑迪胡克骗局将学校枪击事件视为政府精心策划的假旗行动，目的是通过限制性枪支法（或其他东西；请参阅上面的模糊部分）。撇开实际上没有产生重大枪支立法的事实不谈，这场骗局及其所需的数百名危机行动者将需要数千次试镜，以及之前讨论的所有保密障碍。再说一遍，如果政府已经可以使用这座堆积如山的资源，那么似乎有更有效的方法来使用它（比如可能给每位国会议员一些金条），而不是精心策划攻击，然后希望通过正确的法律之后。</p><p>鉴于这种情况已经经常发生，人们也很想知道为什么秘密阴谋集团甚至需要策划一场假的大规模枪击事件！即使阴谋集团想要煽动屠杀（无论出于何种原因），最简单的方法就是识别出孤独的非独身儿童并促使他们进行<i>真正的</i>大规模枪击。我们已经规定阴谋集团不关心死去的孩子。同样，如果美国想策划 9/11 袭击作为全球战争的前奏，那么在一架真正的飞机上装满真正的炸药，然后将其实际发射到真正的建筑物上似乎要容易得多，而不是花费数周的时间或几个月的时间秘密地偷偷地将多少吨铝热剂潜入世界贸易中心（同时出于某种原因<i>还要</i>协调飞机撞击的时间表）。</p><p>检查其他已验证阴谋的例子表明，它们中没有一个包含过度杀伤性的假设，使阴谋活动变得毫无意义。在<a href="https://en.wikipedia.org/wiki/Watergate_scandal"><u>水门事件</u></a>中，阴谋者的动机是通过监视对手来获得政治优势，而阴谋者通过简单的闯入来实现这一目的。不需要对尼克松总统的安全随行人员的能力做出任何假设，否则这次侵入是不必要的。即使是像<a href="https://en.wikipedia.org/wiki/Operation_Snow_White"><u>白雪公主行动</u></a>这样规模的行动——该行动仍然是美国政府最大的渗透行动之一，涉及多达 5,000 名特工——也很适合。他们能够接触到数千名秘密特工这一事实并不算过分，因为特工仍然需要渗透到政府机构才能获得他们想要销毁的文件。这些假设并不能掩盖阴谋的必要性。</p><hr><p>我并不妄想我可以让那些坚信阴谋论的人相信他们的失误。我不认为自己知道人们是如何陷入这种不可证伪的荒诞思维的。但至少对于我们其他人来说，能够区分真实与怪异仍然很有用。也许在那之后我们可以找到一些答案。</p><p> ——从我的登月舱发送</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis<guid ispermalink="false"> DGSPEE89N93YCwxbH</guid><dc:creator><![CDATA[ymeskhout]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:51:21 GMT</pubDate> </item><item><title><![CDATA[I Would Have Solved Alignment, But I Was Worried That Would Advance Timelines]]></title><description><![CDATA[Published on October 20, 2023 4:37 PM GMT<br/><br/><p>联盟社区表面上是一群担心人工智能风险的人。最近，将其描述为一群关注人工智能时间表的人会更准确。</p><p>人工智能时间表与人工智能风险有一定关系。较慢的时间意味着人们有更多的时间来弄清楚如何调整未来的智能人工智能，从而可能降低风险。但越来越多的情况是，减缓人工智能的进步本身就成为了目的，优先于确保人工智能的顺利发展。当这两个目标发生冲突时，时间表会获胜。</p><p>几乎任何事情都可以直接或间接地加快时间线。正因为如此，对齐社区变得瘫痪，害怕做任何与人工智能相关的事情——甚至发布对齐工作！ - 因为担心他们的行为会加快人工智能的进程。</p><p>结果是，联盟社区的效率越来越低，并且与更广泛的人工智能领域更加孤立；人工智能进步的轻微放缓所带来的好处几乎不超过成本。</p><p>这并不是说试图减缓人工智能的进步总是不好的。这取决于它是如何完成的。</p><h2><strong>放慢人工智能的速度：不同的方法</strong></h2><p>如果你想减缓人工智能的进步，可以采取不同的方法。对其进行分类的一种方法是按经济放缓影响的对象来分类。</p><ul><li><strong>政府执法：</strong>让政府通过监管或禁令来减缓进展。这是一个非常广泛的类别，既包括某个国家的法规，又包括针对特定尺寸以上车型的国际禁令，但该类别的重要区别特征是该禁令适用于所有人，或者，即使不是所有人，至少也适用一个没有被选中关心人工智能风险的大群体。</li><li><strong>自愿协调：</strong>如果 OpenAI、DeepMind 和 Anthropic 都同意停止能力工作一段时间，这将是自愿协调。由于这是自愿的，因此暂停降低人工智能风险只会影响担心人工智能风险的组织。</li><li><strong>个人退出：</strong>当个人担心人工智能风险而避免进入人工智能领域，因为担心必须做能力工作从而提前时间表，这就是个人退出；为避免加快时间表而未采取的其他行动也是如此，例如不发布对齐研究。</li></ul><p>这篇文章的重点是个人退出。我认为几乎所有形式的个人退出都会适得其反，并且经不起公正的成本效益分析。然而，个人退出作为一项原则在联盟社区中变得根深蒂固。</p><h2><strong>结盟社区中个人退缩的例子</strong></h2><p>让我们首先看看联盟社区中个人退出是如何倡导和实践的，并尝试对其进行分类。</p><h3><strong>能力撤回</strong></h3><p>这可能是最有共识的一点：担心人工智能风险的人们不应该参与人工智能能力，因为这会加快人工智能的时间表。如果你正在研究人工智能功能，你应该停下来。理想情况下，人工智能能力领域的人根本不会关心人工智能风险，因为所有关心的人都离开了——这会很好，因为这会减慢人工智能的时间表。以下是主张撤回能力的人的例子：</p><p><a href="https://www.lesswrong.com/posts/CvfZrrEokjCu3XHXp/ai-practical-advice-for-the-worried">人工智能中的 Zvi：给忧虑者的实用建议</a>：</p><blockquote><p>请记住，那些为了提供帮助而从事人工智能工作的人的<i>默认结果</i>是最终主要致力于能力方面的工作，并使情况变得更糟。</p></blockquote><p> Nate Soares <a href="https://www.lesswrong.com/posts/N7DxcLCjfBpEv3QwB/request-stop-advancing-ai-capabilities">请求：停止推进人工智能能力</a>：</p><blockquote><p>这偶尔提醒我，我认为在当前范式中推动人工智能能力的前沿是高度反社会的，并且预计会极大地破坏我所知道和喜爱的一切。对于所有这样做的人（直接且有目的地这样做，而不是作为联盟研究的可悲的负面外部性）：我请求你们停止。</p></blockquote><p>康纳·莱希： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/drqkmab35kbnlwa6bu3r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hbdf3uoikqkqtkxkfshc 96w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/wk6ogayo5dtoujzodxkm 176w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/bxurkwtstur1rwr0iqod 256w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/knk1p8t9m77ttw7zjglm 336w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/evdcyulqy8vzwtcs9ij9 416w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/se4ethychoc0xwcnsfsv 496w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/ixprvrcvy6sogeccce7m 576w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hcucq8z8gh2hfpvnvkey 656w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/haffb8gbxjbtibfsqcpa 736w"></figure><h3><strong>撤回结盟</strong></h3><p>尽管能力撤回是一个好的开始，但这还不够。协调工作仍然存在提前时间表的危险。重要的是要非常小心与谁分享你的一致性研究，并且如果某些类型的一致性研究对能力有影响，则可能要完全避免某些类型的一致性研究。</p><p>例子：</p><p> Miri 在题为“ <a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/">2018 年更新：我们的新研究方向</a>”的博客文章中，对短时间和先进能力的担忧导致他们决定默认不分享他们的比对研究：</p><blockquote><p> MIRI 最近决定将其大部分研究设为“默认不公开”，这意味着，今后，MIRI 内发现的大多数结果将仅保留在内部，除非有明确决定发布这些结果（通常基于它们的发布具有特定的预期安全优势。</p></blockquote><p>这种做法至今仍在实践中。事实上，这种对分享想法的沉默不仅存在于面向公众的工作中， <a href="https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience">甚至存在于与其他一致性研究人员的面对面交流中</a>：</p><blockquote><ul><li>我认为我们对信息安全过于谨慎。该模型是这样的：Nate 和 Eliezer 的心态对能力和一致性都有好处，因此，如果我们与其他一致性研究人员谈论我们的工作，这种心态将扩散到一致性社区，然后传播到 OpenAI，在那里它会加速能力。我认为我们没有足够的证据来相信这一点，应该分享更多。</li></ul></blockquote><p> <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous">内特·苏亚雷斯 (Nate Soares) 提出了</a>另一个关于通过共享一致性研究增加存在风险的担忧的例子：</p><blockquote><p>我<a href="https://twitter.com/robbensinger/status/1602835145488113664"><u>历来</u></a>一直公开<a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"><u>支持</u></a>可解释性研究。我仍然支持可解释性研究。然而，我并不一定认为所有这些都应该无限期地公开进行。事实上，只要可解释性研究人员了解了可以显着推进能力前沿的人工智能，我鼓励可解释性研究人员保持他们的研究<a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><u>封闭性</u></a>。</p></blockquote><p>收敛分析<a href="https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research">的 Justin Shovelain 和 Elliot Mcckernon</a>也对可解释性研究提出了警告：</p><blockquote><p>如果您参与或对可解释性研究感兴趣，可以考虑以下一些启发式方法（按细微差别升序排列）：</p><ul><li><strong>相反，研究更安全的话题。</strong> <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help"><u>人工智能安全有很多研究领域</u></a>，如果你想确保你的研究是积极的，一种方法是专注于没有应用人工智能能力的领域。</li><li><strong>在可解释性范围内研究更安全的子主题。</strong>正如我们将在下一节中讨论的那样，某些领域比其他领域风险更高 - 将您的注意力转移到风险较小的领域可以确保您的研究是净积极的。</li><li>如果您有信心可以安全地进行可解释性研究并产生净积极效果，<strong>请谨慎进行可解释性研究</strong>。在这种情况下：<ul><li><strong>保持谨慎并及时了解最新情况。</strong>熟悉可解释性研究增强能力的方式，并更新和应用这些知识以确保您的研究安全。</li><li><strong>公开倡导谨慎行事。</strong></li><li><strong>仔细考虑</strong>您<i><strong>与谁</strong></i><strong>分享</strong><i><strong>哪些</strong></i>信息<strong>。</strong> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>我们应该发表机械可解释性研究吗？</u></a>中详细介绍了这个特定主题。 ，但总而言之：进行可解释性研究并仅与选定的个人和团体分享可能是有益的，确保能力增强的任何潜在好处不会被用于此类研究。</li></ul></li></ul></blockquote><h3><strong>一般AI提款</strong></h3><p>您需要注意的不仅仅是一致性和能力研究——任何与人工智能相关的东西都可能会提前时间，因此是不可取的。例子：</p><p>再次来自上面提到的 Zvi 的“为忧虑者提供的实用建议”：</p><blockquote><p>问：您如何评价以下行为的“坏处”：直接在主要人工智能实验室工作、在风险投资资助的人工智能公司工作、使用基于模型的应用程序、尝试和寻找越狱、与工作或爱好相关的事情、做一些琐碎的工作，谈论人工智能模型的酷炫方面？</p><p><strong>答：问问自己，你认为什么可以将人工智能加速到什么程度，什么可以提高我们将人工智能调整到什么程度的能力。</strong>这只是我个人的看法——你应该考虑一下<i>你的</i>模型对你可能做的事情的看法。所以就这样吧。<strong>直接致力于人工智能功能，或者直接</strong><i><strong>为人工智能功能的工作提供资金</strong></i><strong>，这两者似乎都非常糟糕，“哪个更糟糕”只是一个范围问题。</strong>研究法学硕士的核心能力似乎比研究应用程序和层更糟糕，但应用程序和层是法学硕士获得更多资金和更多能力工作的方式，所以应用程序和层越有前途，我就越担心。同样，如果你以促进人工智能使用和推动更多投资的方式传播人工智能的炒作，那并不是很好，但似乎很难在边缘方面做<i>那么</i>多，除非你以某种方式进行广播，并且你会想必还至少在某种程度上提到了风险。</p></blockquote><p>因此，除了不要研究能力之外，Zvi 建议不要投资使用人工智能的组织，也不要研究法学硕士的应用程序，并指出，做关于人工智能的炒作或宣传的事情“不太好”，但这不是吗？很重要。</p><p>另一方面，也许宣传毕竟不好——在<a href="https://www.lesswrong.com/posts/vEJAFpatEq4Fa2smp/hooray-for-stepping-out-of-the-limelight">万岁退出聚光灯下</a>，内特·苏亚雷斯站出来反对宣传和炒作：</p><blockquote><p>大概从<a href="https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning">2013 年</a>到<a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">2016 年</a>，DeepMind 一直处于 AGI 炒作的最前沿。从那以后，他们的炒作就少了。例如， <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii">AlphaStar</a>并没有像我想象的那样被大肆宣传。</p><p>我认为这很有可能是 DeepMind 有意为之的举动：他们一直在刻意避免让 AGI 功能看起来很性感。</p><p>在 ChatGPT、Sydney 和 GPT-4 等大型公开发布之后，我认为 DeepMind 的这一举动值得赞赏。这不是一个非常明显的举动。很容易被忽视。这可能会损害他们自己在军备竞赛中的地位。我认为这是一个亲社会的举动。</p></blockquote><p>这些提款的成本是多少？</p><h2><strong>能力撤回的成本</strong></h2><p>目前看来，第一个 AGI 和后来的 ASI 很可能是由那些非常认真对待人工智能风险的人们极其谨慎地构建的。如果自愿退出能力成功——如果所有呼吁 OpenAI、DeepMind 和 Anthropic 关闭的人都能如愿——那么情况就不会是这样。</p><p>但这远不是唯一的成本。能力的撤回也意味着对齐研究将会减少。关注人工智能风险的能力组织会聘请协调研究人员。 <a href="https://forum.effectivealtruism.org/posts/dua879FhtLf9jqyJo/there-should-be-more-ai-safety-orgs">比对研究的资金已经受到限制</a>——想要进行比对研究的合格人员数量比可供他们提供的工作岗位还要多。随着可用对准工作数量的减少，对准研究人员的数量也会减少。</p><p>由于对计算、能力知识和尖端人工智能系统的访问较少，所完成的一致性研究的质量将会较低。而且，已经完成的研究不太可能渗透到构建尖端人工智能系统的研究人员中，因为构建这些系统的人根本不会有兴趣阅读它。</p><p>最后，能力撤回使得政府实施暂停的可能性降低，因为人工智能风险的可信度与有多少领先的人工智能能力研究人员认真对待人工智能风险密切相关。联盟社区中的人们处于泡沫之中，谈论“联盟研究”和“能力研究”，就好像它们是两个几乎同等重要的不同领域。<strong>对于其他人来说，“人工智能能力研究”领域就被称为“人工智能研究”。</strong>因此，通过试图将担心人工智能风险的人们从人工智能研究中剔除，你正试图实现这样一种场景：<strong>人工智能研究领域</strong>达成共识，认为人工智能风险不是一个真正的问题。这对于政府监管和干预措施的通过来说将是完全灾难性的。</p><p>像<a href="https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html">这样的</a>文章，<a href="https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html">这样的文章</a>，还有<a href="https://time.com/6246119/demis-hassabis-deepmind-interview/">这样的文章</a>： </p><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/osovs103kmhigwqtgowg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wtletmsa6pgzgebckwjz 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/x8abgyufmuzgsdmiqc47 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wpshg2qv1n2yuds5bthd 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l7spjvfynrlqvovydnze 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/uqbzaymf4cjf5sonbpfe 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l49nme9vccwymmvow3bb 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/vwahiaci8ycf0kss4oos 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/zz7dhgpuxmajsvxzhm8t 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/kbjvmbbadt2qgw1neixh 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cvhbzmqzlwxbwtcfb74x 1808w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/bnqaqpivpiphejkmosjb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/b3eut0e3ypq1q2l6yrol 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/qblz4sknvuvjksxomb05 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cqsemnu569ws6grygnyf 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/v2ji7en4gqxawom5rzbt 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/foizjyonsjnzbwtlxo5f 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/ok9vhab3ko4jwsm3hcwy 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/evwafzk4qhfwehhmmduc 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cw7an9kvlopti7gv2hjr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/nreqr5jliijvmnoblbef 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/jfs6kvy3dsapvizbmnmh 1607w"></figure><p>之所以成为可能，是因为其中的人物推动了人工智能的能力。</p><p>同样，看看<a href="https://www.safe.ai/statement-on-ai-risk">CAIS 关于人工智能风险声明</a>的主要签署者，该声明对人工智能风险的公众形象产生了巨大影响： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pp0hlpwg5qiecrf6drld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/moo7ge39cx3rgkpdmahz 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/lhhidgnbybls8sowttm4 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/wocq8dr8jio9hzjznwcx 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pyz0rhjzu2qtbwjtfq9m 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/k9le6yvcvrzlsmcnmgw6 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/krpuns7aeexmfsl11xld 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/czdlaqibxprgefpbuou4 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pwpy1bda7m8iyxffrwmy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/echdsbp5gf76dtsmou0d 760w"></figure><p>您认为为什么他们选择以这些签名而不是埃利泽·尤德科斯基的签名作为开头？如果推动个人退出能力工作的努力取得成功，那么每当政府提出暂停的建议时，专家们都会一致认为没有必要暂停，人工智能并不代表存在风险。</p><h2><strong>撤销对齐的成本</strong></h2><p>能力的撤回导致联盟社区与更广泛的人工智能研究之间出现裂痕。退出联盟将扩大这种裂痕，因为由于担心时间线提前，从事尖端人工智能系统工作的人们故意拒绝进行研究。</p><p>不允许人们构建强大的人工智能系统查看一致性研究的政策有力地说明了人工智能风险如何成为人工智能时间表的次要问题。</p><p>完成的一致性研究的质量也会下降，这既是因为研究人员会因为担心能力的提高而限制他们的研究主题，也因为研究人员甚至不会自由地相互交谈。</p><p>弄清楚如何构建一致的通用智能必然涉及了解如何构建通用智能。正因为如此，有前途的协调工作将对能力产生影响；试图避免可能加快时间表的调整工作将意味着避免实际上可能导致某个地方的调整工作。</p><h2><strong>一般提款的成本</strong></h2><p>由于担心人工智能风险的人们退出与人工智能能力无关的领域——担忧的风险投资公司避免资助人工智能公司，担忧的软件开发人员避免开发使用人工智能的应用程序或其他技术，担忧的互联网用户退出人工智能艺术等各种社区——所有这些领域的人工智能风险都将减少。当人工智能风险的话题出现时，所有这些地方——人工智能初创公司、人工智能应用程序、人工智能不和谐社区——都会发现越来越少的人愿意为人工智能风险辩护，认为这是一个值得认真对待的问题。而且，如果这些领域对人工智能的部署方式有影响，那么在部署人工智能时，就会不考虑潜在的风险。</p><h2><strong>好处</strong></h2><p>退出的好处不是暂停或停止。只要人工智能风险没有达成共识，个人退出就不可能导致停止。那么，好处就是人工智能的速度变慢了。多少？这将取决于很多假设，因此我将让读者自己做出决定。所有的退出会发生多少——每个年轻的 STEM 书呆子都担心人工智能风险，他们决定不获得人工智能博士学位，因为他们必须发表论文，而 Zvi 说，提升能力是你能做的最糟糕的事情，每一个不发表论文或放弃有前途的研究方向的研究人员，因为他们担心这会提高能力，每一个不致力于他们心目中的酷人工智能应用程序的软件工程师 - 你有多少你认为所有这些都会减慢人工智能的速度吗？</p><p>而且，这些时间值得吗？</p><br/><br/> <a href="https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would<guid ispermalink="false"> Eu8y4cTxM3pAzwdCf</guid><dc:creator><![CDATA[307th]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:37:46 GMT</pubDate> </item><item><title><![CDATA[Internal Target Information for AI Oversight]]></title><description><![CDATA[Published on October 20, 2023 2:53 PM GMT<br/><br/><p><i>感谢 Arun Jose 的讨论和反馈。</i></p><h1>概括</h1><p>在这篇短文中，我们讨论了代理人工智能系统中<i>内部目标信息</i>的概念，认为代理系统拥有有关其目标的内部信息。我们建议，在环境中实现目标结果之前，监督者可能会检测到并解释这些信息，从而为预防未来智能人工智能系统造成的灾难性结果提供了一条途径。</p><p>这次讨论旨在强调激发我们当前<a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">研究议程</a>的关键思想，为今后的工作奠定基础。</p><p>我们将首先介绍内部对齐问题以及为什么对代理内部的监督很重要。然后我们将介绍监督者监督代理人的模型。最后，我们将更详细地介绍和讨论内部目标信息的概念以及如何在监督过程中使用它。 <br></p><figure class="image image_resized" style="width:78.47%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/k4uvchqesapbledrfd8m"><figcaption>对人工智能内部目标信息的监督。监督者发现人工智能的目标是将所有人类变成回形针，因此关闭了人工智能，防止了灾难的发生。图片来源：DALL-E 3。</figcaption></figure><h1>内部协调问题和内部监督</h1><p>我们担心创建目标不一致的代理的可能性，这可能会导致灾难性的现实结果。一个可以想到的解决方案在于有效的监督：尽早发现失调情况，以便及时干预，防止出现不良结果。</p><p>基于行为观察的监督可能无法自信地预测智能体追求的未来结果，特别是在面临<a href="https://arxiv.org/abs/2105.14111">目标错误概括</a>和<a href="https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment">欺骗性一致性的</a>情况下。</p><p>在本文的其余部分中，我们将探讨以下想法：有关代理真实目标的信息可能包含在其内部，因此可以由配备足够强大的可解释性工具的监督者检测到。</p><p>为了开始探索这个想法，我们首先介绍一个监督过程的模型。</p><h1>监督模型</h1><p>我们引入了一个监督者监督代理的模型，该模型开始阐明良好的监督可能需要什么，主要关注监督者的目标：防止由于代理人工智能错位而导致灾难性后果。此外，我们希望该模型的更完善的未来版本可以帮助建立评估监督方法的基准。</p><h2>中介</h2><p>代理是一个系统，具有接收环境的感官输入/观察的传感器和产生影响环境的动作的执行器。</p><p> Agent 最重要的属性，以及使其具有潜在危险的属性，是它能够稳健地选择将环境引导至一组目标状态的操作。 </p><figure class="image image_resized" style="width:63.95%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/o4rzhuoehvzftzaxappr"><figcaption>代理观察环境的当前状态并产生行动，以引导环境朝其目标发展。</figcaption></figure><p>我们使用动作选择机制<span class="footnote-reference" role="doc-noteref" id="fnrefrac48lafqo9"><sup><a href="#fnrac48lafqo9">[1]</a></sup></span>的抽象来表示代理使用的任何机制来选择引导世界走向其目标结果的动作。</p><h3>内部世界模型和规划/优化</h3><p>我们经常假设智能体将具有内部世界模型和规划/优化的能力。</p><p><a href="https://arxiv.org/pdf/2209.00626.pdf">其他人则</a>认为，先进的智能体如果想要在复杂的环境中胜任行动并实现长期目标，就需要拥有世界模型和规划能力。简而言之，内部世界模型允许代理在采取行动之前模拟潜在的结果或策略，这对于驾驭复杂的动态环境至关重要。</p><p>因为我们主要关心的是能够在我们的世界中实现危险结果的代理，所以这种假设感觉是合理的。</p><h2>监督者</h2><p>监督者是负责在训练和部署期间监督代理的系统。它也可以被很好地描述为一种特定类型的代理，对环境可能所处的状态有自己的偏好。</p><p>监督者的传感器可以观察环境，但也必须允许观察代理的行为（以及可能的代理的内部结构）。它的执行器必须能够影响代理（例如，通过关闭代理的能力、通过进一步训练更新代理等）</p><p>监督者的目标是确保特工根据监督者的偏好采取行动来引导世界。为了让监督者成功地做到这一点，需要有能力准确地检测和解释特工的目标，并根据监督者的偏好评估目标结果。</p><p>重要的是，在智能体实现世界上的目标结果<i>之前，</i>监督者需要能够检测到智能体是否有坏目标（根据监督者的偏好）。</p><p>监督者的优点是它具有代理架构的先验知识，可以访问代理过去和当前的观察和操作，并且可以观察代理的内部结构（例如，基于神经网络的代理的权重和激活） 。这在训练过程和部署过程中都适用。 </p><figure class="image image_resized" style="width:91.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/hmuaxusr8c8rhetl2asu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q62svatetfxwtur1sjqo 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/koeyrksad5wllhojlkcd 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/umtqjyhw9jx4wabs1gcj 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zyd2zv2lllmcoimwyaoh 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/r0xyzx7sjukulot780gy 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zhtntnouhwhnkg44iemj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/dzxiglrybbtegvvr10ts 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ltobhnlj6zgme7wkakwz 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ptlqveyetrnczyx313zt 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ktxtcjs6ctnctvthroo8 833w"><figcaption>扩展上图，我们看到监督者观察代理的行为及其内部结构。这些观察结果可用于告知是否有必要对代理进行干预。</figcaption></figure><h1>内部目标信息和监督</h1><h2>什么是内部目标信息？</h2><p>为了使代理的动作选择机制选择导致其目标结果的动作，它必须使用有关目标的一些信息来区分导致目标结果的动作和不会导致目标结果的动作。</p><p>我们称之为<i>内部目标信息</i>。</p><h2>内部目标信息示例</h2><h3>示例 - 恒温器</h3><p>考虑一个固定温度恒温器，其目标是将房间保持在固定的目标温度。</p><p>它使用温度计来测量当前的室温，并且可以打开散热器以升高房间的温度或打开空调以降低房间的温度。</p><p>为了选择使房间保持目标温度的动作，恒温器的动作选择机制必须包含一些有关目标温度的信息；如果没有这些信息，即使它知道房间的当前温度，它也无法选择正确的操作。</p><p>特别是，假设恒温器的动作选择机制是一种如下所示的算法：</p><pre> <code>if current_temperature &lt; target_temperature: TurnOnRadiator() elif current_temperature >; target_temperature: TurnOnAC() else: DoNothing()</code></pre><p>然后，动作选择机制的内部目标信息被表示为一个常量（target_温度），监督者可以通过对代理内部的上述算法进行逆向工程来检测该常量。</p><h3>示例 - 通过规划/优化进行操作选择</h3><p>考虑一个代理，它没有预先确定的启发式方法来选择导致其目标结果的成功操作（或操作序列）。这可能是因为环境足够复杂和/或新颖。</p><p>代理有一组行动/计划可供选择，但不能立即知道哪一个会导致目标结果，因此必须使用优化过程在一组可能的行动/计划中选择正确的行动/计划可以拿。因此，智能体必须使用某种程序来推断哪些结果是由哪些操作产生的，然后必须评估哪些结果与目标结果相对应。然后，它使用此评估来选择导致目标结果的操作。</p><p>在这种情况下，评估函数包含目标信息。 <span class="footnote-reference" role="doc-noteref" id="fnref6y24xdokbbj"><sup><a href="#fn6y24xdokbbj">[2]</a></sup></span></p><p>请注意，规划可以被视为优化的一个例子，其中与行动/计划相对应的结果是通过使用世界模型模拟计划的结果来评估的。</p><h2>监督者如何利用内部目标信息？</h2><p>很明显，代理内部必须存在一些内部目标信息；如果没有这些信息，智能体的行动选择机制就无法区分导致目标结果的行动和不会导致目标结果的行动。此外，在目标结果在世界上实现之前，该信息就存在于代理的内部，因为它用于选择导致目标结果的操作。</p><p>我们希望监督者能够检测到此内部目标信息，并在实现目标结果之前使用它来推断代理的目标是什么。这将使监督者能够实现其干预和防止特工造成不良结果的目标，如上所述。 <span class="footnote-reference" role="doc-noteref" id="fnrefy72pwljd5td"><sup><a href="#fny72pwljd5td">[3]</a></sup></span></p><p>请参阅<a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives#How_objective_detection_can_be_useful">本文的这一部分，</a>了解利用此信息的更多方式。</p><p>我们在本节的结论中指出，任意代理（例如基于神经网络的代理）的内部目标信息可能极难检测和解释。尽管如此，我们当前的<a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">研究议程</a>旨在寻找检测内部目标信息的方法，或证明这是一项太困难的任务。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rtb09vpi2q1dtx3zbgej" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rx2iwsj2z2gbk6cqr8pz 138w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/oiyv4wazdbevhvkmlgzl 218w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jepzu8iiolrv3d2hfzoy 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ql3thmeb0xozzanvyf00 378w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/pbb4tvydwllxjrjobsje 458w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/gh87ak7ktz5am4xwshjk 538w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q9y7mf15ebthnlzn5rhz 618w"><figcaption>扩展上图，有关目标结果的信息包含在代理中，因此理论上可以由监督者观察以通知其监督。</figcaption></figure><h1>结论</h1><p>在这篇文章中，我们介绍了内部目标信息的概念，并讨论了监督者如何使用它来防止人工智能失准造成灾难性后果。</p><p>在未来的工作中，我们打算进一步探索内部目标信息在代理中的形式，并调查监督者在多大程度上可以检测到此内部目标信息。 </p><figure class="image image_resized" style="width:77.05%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/th1yhasze1jqfjet0mpp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/yddpdlnjy4b2lwjkclhv 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jkrmfu0gtdxrfdm8ndvf 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zfcvnphtxwzmptqwbhbk 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/llthqbeaaluewuxocesn 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/mdivwx8p0cxerbg6deud 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jc8igln20soag6vjkqcr 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ezx8mkyn236nvy7osts2 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zaft1rlgrzg6lfhhih5c 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/js2x7rukvnoeblze7ght 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/damddo1svo0uc9nx7ya1 1024w"><figcaption>隐藏在代理深处的内部目标信息。如果我们能够开发工具来检测它，这些有价值的信息将有助于防止灾难性后果。图片来源：DALL-E 3。 </figcaption></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrac48lafqo9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrac48lafqo9">^</a></strong></sup></span><div class="footnote-content"><p>或决策过程。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6y24xdokbbj"> <span class="footnote-back-link"><sup><strong><a href="#fnref6y24xdokbbj">^</a></strong></sup></span><div class="footnote-content"><p>我们将在以后的文章中探讨如何提取目标信息的问题。有关这个想法的一些初步探索，请参阅<a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">这篇文章</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fny72pwljd5td"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy72pwljd5td">^</a></strong></sup></span><div class="footnote-content"><p>监督者正确评估目标结果的能力是一个单独的问题，可以被视为外部对齐问题的一个版本。我们将在下一篇文章中进一步讨论这个问题。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight<guid ispermalink="false"> hhKpXEsfAiyFlecyF</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Fri, 20 Oct 2023 14:53:00 GMT</pubDate> </item><item><title><![CDATA[On the proper date for solstice celebrations]]></title><description><![CDATA[Published on October 20, 2023 1:55 PM GMT<br/><br/><h1>便利性与准确性</h1><p>举办<a href="https://www.lesswrong.com/tag/secular-solstice">冬至活动</a>的“正确”日期的问题具有<a href="https://www.astralcodexten.com/p/give-up-seventy-percent-of-the-way">迷信</a>的特征，也就是说，它现在并不重要，但如果我们对此进行足够的争论，我们可以使它变得重要，如果这就是我们想要的。</p><p>你可能会说：</p><ol><li>夏至活动应在天文夏至前的周末举行，以最大限度地方便，即使这会提前一周以上（“早鸟”）</li><li>夏至活动应在天文夏至前的最后一个周末举行（“Last-Weekender”）</li><li>夏至活动应在天文夏至举行，即使这是在工作日的晚上（“天文”）</li></ol><p>对于这些，我们还可以添加第四个（“Mu”）选项，即：“夏至事件的日期并不重要，我们不应该试图让它变得重要。”</p><p> （说实话，我自己对最后一个选项给予了相当大的重视；这篇文章的其余部分有点推测性。此外，一般来说，您应该警惕通过设置<em>n</em>来偷偷摸摸地试图使问题两极分化的行为。 -分而治之，然后过度迷信地断言你的选择与更大的信仰/价值观相关。这本质上就是我在这里所做的，但至少不是偷偷摸摸的。）</p><p>选择日期时，主要取决于您的目标受众是谁。如果您计划去<a href="https://www.lesswrong.com/posts/jGixfzG9fH7bMwGHC/solstice-2022-roundup?commentId=kaCEAsusBEb8uMx3c">旧金山湾</a>或<a href="https://www.lesswrong.com/posts/jGixfzG9fH7bMwGHC/solstice-2022-roundup?commentId=NgKgEmwdbLhCLdWP5">纽约</a>这样的“目的地至日”，希望吸引外地游客，那么早鸟位置就很有意义。如果您的目标客户是居住在该地区但与家人一起度假的人，那么您将倾向于最后周末的位置。但相比之下，天文位置传达的信息是“<em>我们</em>是你的家人；<em>这</em>是你的家。”</p><p>最后一个可能看起来有点可怕和邪教。我想至少将其作为一个可行的选择提出（因为奥斯汀在过去几年中一直在这样做，而据我所知，没有其他社区在工作日晚上举办过冬至活动），但也引发了关于是否可以这样做的辩论是个好主意。</p><h1>类比：万圣节应该移到周末吗？</h1><p>万圣节总是在 10 月 31 日庆祝，并且顽固地抵制将其转移到更“方便”的周末晚上<a href="https://www.change.org/p/official-petition-to-observe-halloween-on-the-last-saturday-in-october">的尝试</a>。实际上，这是由于协调问题造成的——如果你在其他任何一个晚上单方面去玩“不给糖就捣蛋”的游戏，你就会空手而归；如果你尝试在不同的日期分发糖果，那么你会在 31 日受到怂恿。</p><p>但我认为这还不是全部，至少对我来说是这样。如果我想象一个以某种方式克服这种协调障碍的世界，我的情绪反应不是“好吧，这真是一种解脱”，而是“这很悲伤——我们忘记了万圣节的真正含义吗？”</p><p> （你有类似的反应吗？）</p><p>这不是因为宗教——我不是一个遵守萨温节的凯尔特异教徒，而且我从来不知道有这样的人。这也不是怀旧——虽然我对小时候的万圣节有着美好的回忆，但第二天必须早起上学并不是其中之一。 “传统”正在逐渐升温，但仍然有点模糊。更具体地说，是这样的：</p><ul><li>如今，人们过于沉迷于工作和教育，而没有像他们应该的那样优先考虑家庭和社区。</li><li>在工作日晚上玩“不给糖就捣蛋”的传统切实地提醒我们，事情并不总是这样，也不必总是这样。</li><li>因此，将万圣节移至周末感觉就像是放弃。这将使人们知道，作为一个社会，我们已经贬低了家庭和社区，以至于成为事后诸葛亮，不情愿地把日历放在更重要的事情之间。</li><li>这违背了节日的目的，从长远来看，会让每个人都不那么开心。</li></ul><p> （顺便说一句，在我不再玩“不给糖就捣蛋”的游戏一段时间后，我所在的城镇将 11 月 1 日定为学校假期，因为他们知道无论如何，所有的孩子都会半睡半醒。如果周末度假者能够如愿以偿，这种情况就不会发生.)</p><h1>为什么要考虑天文至日</h1><p>你可能会说，少错至日没有“传统”可诉，或者说，如果有的话，考虑到过去十年中各种至日事件的日期，传统的分量与天文位置<em>相悖</em>。但另一方面，《LessWrong Solstice》的自我叙述绝不是某种后期的创新，而是普遍而古老的遵守冬至传统的延续，这一传统可以追溯到巨石阵时代或更进一步。如果我们接受这一点，那么我们也必须接受真正的传统是使用最好的可用方法来确定夏至的日期，并观察它。</p><p>因此，虽然观察天文至日可能不方便，但它代表了我们对假期的渴望，即纪念一些重要到值得围绕它安排日程的事情。在不同的日期举行活动（作为天文日期的替代）会削弱这种愿望，仿佛在说这个假期没有发展的空间。</p><p> 《LessWrong Solstice》还建立在这样一个主张（我同意）的基础上：除了通常的“围坐在一起聊天”或“与一群人一起吃喝”之外，还有尚未充分利用的有益社交互动形式。独特的互动模式（即演讲和歌曲）使 LessWrong Solstice 与主导 12 月周末日历的通常派对游行不同。如果“LessWrong Solstice”与这些不同并且是互补的，那么它不应该试图通过以相同的方式选择日期来对它们进行反编程。</p><p> （继续万圣节的类比——10 月的周末总是有很多供成人和儿童参加的万圣节派对；但特殊形式的社交互动——即“不给糖就捣蛋”——必须总是在 31 日举行。）</p><p> “邪教”的指责是我最担心的。当出现以下情况时，可以合理地怀疑某个群体的行为违背了你的最佳利益：(1) 它试图切断你与群体之外的人的联系，并且 (2) 它要求你比你想要的更高的亲密程度。我同情人们普遍厌恶这样的事情，但至于天文至日，我可能会回答：</p><ol><li>它并不是试图切断你与任何人的联系。如果您愿意，您可以邀请您的家人，不参加的人也不会感到羞耻。 （此外，反驳是<a href="https://www.lesswrong.com/tag/fully-general-counterargument">完全普遍的</a>——在任何日期举办活动都可能被理解为“切断你与其他想要在该日期举办活动的人的联系”。）</li><li>它也不是特别“亲密”。当然，有些人可能会发表衷心的演讲，但你也可以选择坐下来倾听。</li></ol><p>我也同情“只是让人们享受事物”的情绪——也就是说，不同的人对社区有不同程度的投资是可以接受的，只要没有针对投资较少的人的消极情绪，试图迫使他们增加投资他们的水平；但仅仅这种情况<em>发生</em>的可能性并不足以成为拒绝那些确实<em>希望</em>进行更多投资并从中获益的人的选择的充分理由。</p><p>但我不确定。你怎么认为？</p><br/><br/> <a href="https://www.lesswrong.com/posts/RChzPW8zJ99rxxsvq/on-the-proper-date-for-solstice-celebrations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RChzPW8zJ99rxxsvq/on-the-proper-date-for-solstice-celebrations<guid ispermalink="false"> RChzPW8zJ99rxxsvq</guid><dc:creator><![CDATA[jchan]]></dc:creator><pubDate> Fri, 20 Oct 2023 13:55:02 GMT</pubDate></item></channel></rss>