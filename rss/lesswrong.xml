<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 27 日星期一 12:23:06 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Sentience Institute 2023 End of Year Summary]]></title><description><![CDATA[Published on November 27, 2023 12:11 PM GMT<br/><br/><h1><strong>概括</strong></h1><p>去年的<a href="https://www.sentienceinstitute.org/blog/eoy2022"><u>2022 年年终总结</u></a>是在 ChatGPT 推出前五天发布的。这一事件令该领域的许多人感到惊讶，全球的关注已经说明了理解人机交互的重要性。我们的首要任务仍然是研究<a href="https://www.sentienceinstitute.org/blog/key-questions-for-digital-minds"><u>数字思维</u></a>的兴起：具有或被认为具有推理、代理、经验和<a href="https://www.sentienceinstitute.org/blog/the-importance-of-artificial-sentience"><u>感知等</u></a>心理能力的人工智能。我们希望回答这样的问题：人类与人工智能的关系迅速改变的下一个“ChatGPT时刻”将会是什么？人类和人工智能如何以有益而非破坏的方式互动？</p><p>我们 2023 年研究的亮点是<a href="https://www.sentienceinstitute.org/aims-survey"><u>人工智能、道德和情感 (AIMS)</u></a>调查，其问题与 2021 年相同，因此我们可以比较最近事件前后的公众舆论，以及额外的<a href="https://www.sentienceinstitute.org/aims-survey-supplement-2023"><u>AIMS 2023 年公众意见补充人工智能安全观点</u></a>。这样的数据通常是在社会问题完全受到公众关注后收集的，因此我们渴望在数字思维兴起之前就已经建立了这种纵向跟踪。在外展方面，我们发布了两集<a href="https://www.sentienceinstitute.org/podcast/"><u>播</u></a>客（其中一集于 2022 年 12 月发布），并举办了一场关于数字思维的跨群体电话会议以及<a href="https://www.youtube.com/watch?v=xq5ZhYUi1K8"><u>Yochanan Bigman 的演讲</u></a>。</p><p>我们正在进行的工作包括关于人工智能自主性和感知如何影响感知的在线实验，开发衡量底层主义的量表，以及人们如何理解尖端人工智能系统并与之互动的定性研究（例如访谈）。<strong>我们希望在这个捐赠季筹集 150,000 美元，以继续我们对数字思维的研究。</strong>我们还有一些正在进行的项目来解决工厂化养殖问题（我们在 2021 年之前的重点），这些项目由捐助者的指定捐款资助，例如为 2023 年<a href="https://www.sentienceinstitute.org/aft-survey"><u>动物、食品和技术 (AFT) 调查进行的</u></a>持续数据收集。</p><p>一如既往，我们非常感谢我们的支持者，他们与我们有着共同的愿景，并使这项工作成为可能。如果 2023 年您可以的话，请考虑<a href="https://www.sentienceinstitute.org/donate"><u>捐款</u></a>。</p><h1> 2023 年的成就（迄今为止）</h1><h2>研究</h2><ul><li>我们最近发布了 2023 年<a href="https://www.sentienceinstitute.org/aims-survey"><u>人工智能、道德和情感 (AIMS) 调查</u></a>结果。 2023 年<a href="https://www.sentienceinstitute.org/aims-survey-2023"><u>主要</u></a>和<a href="https://www.sentienceinstitute.org/aims-survey-supplement-2023"><u>补充</u></a>调查结果以及<a href="https://www.sentienceinstitute.org/aims-survey-2021"><u>2021 年主要</u></a>调查结果可在我们的网站上查看完整内容。与 ChatGPT 之前的 2021 年相比，2023 年美国人对人工智能的道德担忧明显更高。其他主要发现包括：<ul><li> 71% 的人支持减缓人工智能发展的政府监管。</li><li> 39% 支持保护有感知力的机器人/人工智能福祉的“权利法案”。</li><li> 68% 的人同意，如果 ChatGPT 或 Bard 等大型语言模型 (LLM) 具备承受痛苦的能力，我们就不能给它们带来不必要的痛苦。</li><li> 20% 的人认为某些人工智能已经具有感知能力； 37% 不确定； 43% 的人表示并非如此。</li><li> 10% 的人说 ChatGPT 有感知能力； 37% 不确定； 53% 的人表示并非如此。</li><li> 23% 的人相信人工智能公司会将安全置于利润之上； 29% 不确定； 49% 的人不这样做。</li><li> 27% 的人相信人工智能的创建者能够保持对当前和未来版本的控制； 27% 不确定； 26% 的人不这样做。</li><li> 49%的人表示人工智能发展步伐太快； 30% 的人说还好； 19% 的人表示他们不确定；只有 2% 的人认为速度太慢。</li></ul></li></ul><p>以下是我们调查问题的数据示例： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WyAbHFDJYh3pstAMY/qh7gmv0w1ibc4ilzvauh"></p><ul><li>我们的博客文章“<a href="https://www.sentienceinstitute.org/blog/mass-media-propaganda-and-social-influence"><u>大众媒体、宣传和社会影响：Courchesne 等人的有效性证据”。 （2021）</u></a> ”回顾了关于说服力和社会影响力的文献，这可能对旨在创造新行为和破坏现有社会政治结构的养殖动物和人工智能安全倡导者有用。</li><li>我们的博客文章“<a href="https://www.sentienceinstitute.org/blog/moral-spillover-in-human-ai-interaction"><u>人机交互中的道德溢出</u></a>”回顾了人机交互中道德溢出的文献。道德溢出是道德态度和行为（例如道德考虑）从一种环境到另一种环境的转移，似乎是道德圈扩展的重要驱动力。</li><li>数字思维的心智能力和能力可能是未来几年人工智能发展轨迹的关键因素。更好地理解它们可能会带来更准确的预测、更好的战略优先顺序以及人工智能安全的具体策略。我们发布了该领域一些关键问题的<a href="https://www.sentienceinstitute.org/blog/key-questions-for-digital-minds"><u>摘要</u></a>。</li><li>我们支持了<i>《人工智能与伦理》</i>中的一篇论文，题为“<a href="https://link.springer.com/article/10.1007/s43681-023-00260-1"><u>人工智能的道德地位如何？</u></a> ”询问人工智能必须满足哪些标准才能获得道德地位——也就是说，为了自身的利益而获得道德考虑。文章认为，所有有感知力的人工智能都应该获得道德地位，并且有充分的理由向一些具有偏好和目标的无感知人工智能授予道德地位。</li><li>我们在<i>Inquiry</i>中支持了一篇关于“<a href="https://www.tandfonline.com/doi/full/10.1080/0020174X.2022.2144442"><u>数字痛苦：为什么它是一个问题以及如何预防它</u></a>”的论文，该论文提出了一种新策略，用于获得对未来数字思维体验的认知访问并防止数字痛苦，称为<i>访问监控预防</i>（AMP）。</li><li>我们支持了<i>《社会认知》</i>中一篇关于“ <a href="https://www.sentienceinstitute.org/blog/extending-perspective-taking-to-nonhuman-animals"><u>将观点采择扩展到非人类动物和人工实体</u></a>”的论文，详细介绍了两项实验，测试观点采择是否可以在动物和智能人工实体的背景下产生积极影响。</li><li>我们发表了布拉德·萨德 (Brad Saad) 撰写的关于“<a href="https://www.sentienceinstitute.org/simulations-and-catastrophic-risks"><u>模拟和灾难性风险</u></a>”的报告，概述了这一交叉点的研究方向。未来的模拟技术可能既会带来灾难性风险，又会提供减少灾难性风险的方法。 Saad 还与另一位 SI 研究员 Ali Ladak 以及该领域的其他三名早期职业研究人员一起，获得了纽约大学研究动物和人工智能意识相关主题的早期职业学者奖。</li></ul><p>有关我们正在进行的研究的更多详细信息，请参阅我们的<a href="https://www.sentienceinstitute.org/research-agenda"><u>研究议程</u></a>。</p><h2>外展</h2><ul><li>2023 年 1 月 5 日，我们为从事数字思维研究的组织举办了第二次小组间电话会议。我们目前正在重新评估这些电话是否是建设领域的最佳方法，因为自 FTX 垮台以来兴趣已经下降。</li><li> 2023 年 4 月 27 日，我们举办了一次研究研讨会，Yochanan E. Bigman<a href="https://www.youtube.com/watch?v=xq5ZhYUi1K8"><u>介绍了</u></a>他关于人工智能态度的研究成果，学术界和独立研究人员参加了研讨会。</li><li>我们于 2022 年 12 月发布了两期由心理学家<a href="https://www.sentienceinstitute.org/podcast/episode-21.html"><u>Matti Wilks</u></a>和哲学家<a href="https://www.sentienceinstitute.org/podcast/episode-22.html"><u>Raphaël Millière</u></a>主持的新播客节目，以及一期由哲学家<a href="https://www.sentienceinstitute.org/podcast/episode-20.html"><u>David Gunkel</u></a>主持的节目。</li><li>在 ChatGPT 推出后的公共媒体热潮中，我们的联合创始人 Jacy Reese Anthis 进行了有关数字思维的广播和播客采访，例如<a href="https://open.spotify.com/episode/2nxBkzQZcVhN2KQZHKy6QP"><u>Chuck Todd</u></a>和<a href="https://open.spotify.com/episode/7tvCfYqd3NgrnVlkSozoId"><u>底特律 NPR</u></a> ，并发表了两篇专栏文章： <a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/"><u>“我们需要人工智能权利运动”</u></a> <i>The Hill</i>和<i>沙龙</i>中的<a href="https://www.salon.com/2023/05/18/why-we-need-a-manhattan-project-for-ai-safety/"><u>“为什么我们需要一个用于人工智能安全的‘曼哈顿项目’”</u></a> 。</li><li>杰西还<a href="https://www.theatlantic.com/ideas/archive/2023/05/humans-ai-jacy-reese-anthis-sociologist-perspective/673972/"><u>与</u></a><i>《大西洋月刊》</i>的安妮·洛瑞讨论了数字思维，特别是如何评估推理和感知等心理能力，以及我们如何从人类与动物互动的历史中汲取教训，以促进人类与人工智能互动的未来——无论是人类如何将如何对待人工智能以及人工智能将如何对待人类。</li><li>我们继续通过社交媒体、电子邮件、会议和研讨会上的演示以及与可以利用它的人的会议来分享我们的研究成果。</li></ul><h1> 2023年支出</h1><p>今年到目前为止，我们已花费 227,762 美元，大致细分如下（73% 用于研究，5% 用于外展，22% 用于管理）。在每个类别中，费用主要是员工时间，但也包括数据收集（研究）、播客编辑（外展）和虚拟办公室订阅（管理）等项目。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WyAbHFDJYh3pstAMY/xcjjlwhjptrl5cxo3kot"></p><p>我们继续维护一个<a href="https://www.sentienceinstitute.org/transparency"><u>透明度</u></a>页面，其中包含年度财务信息、错误的运行列表以及其他公共信息。</p><h1>获得更多资金的空间</h1><p>我们目前的目标是在本捐赠季（2023 年 11 月至 2024 年 1 月）筹集 150,000 美元，以继续我们的数字思维研究和领域建设工作。</p><p>我们知道，随着<a href="https://www.forbes.com/sites/johnhyatt/2022/11/14/sam-bankman-fried-promised-millions-to-nonprofits-research-groups-thats-not-going-too-well-now/"><u>FTX 的崩溃</u></a>以及试图在该领域筹集资金的新兴人工智能组织的迅速扩散，今年的筹款活动将继续困难。我们有足够的空间为高成本效益的项目提供更多资金。我们在 2023 年初进行的最后一轮招聘有 119 名研究员职位申请者，其中包括一些我们没有提供录用但可以通过更多可用资金获得的申请者，如果您有兴趣做出重大贡献，请随时与我们联系讨论我们的扩张空间超过 150,000 美元。</p><p>再次感谢您的宝贵支持。如果您有疑问、反馈或想要合作，请给我发电子邮件： <a href="mailto:michael@sentienceinstitute.org"><u>michael@sentienceinstitute.org</u></a> 。如果您想捐赠，您可以通过我们的网站<a href="https://www.sentienceinstitute.org/donate"><u>通过 PayPal 或支票</u></a>进行捐赠。</p><br/><br/> <a href="https://www.lesswrong.com/posts/WyAbHFDJYh3pstAMY/sentience-institute-2023-end-of-year-summary#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WyAbHFDJYh3ps​​tAMY/sentience-institute-2023-end-of-year-summary<guid ispermalink="false"> WyAbHFDJYh3ps​​tAMY</guid><dc:creator><![CDATA[michael_dello]]></dc:creator><pubDate> Mon, 27 Nov 2023 12:11:37 GMT</pubDate> </item><item><title><![CDATA[A Question about Corrigibility (2015)]]></title><description><![CDATA[Published on November 27, 2023 12:05 PM GMT<br/><br/><p>最近，我读了<a href="https://cdn.aaai.org/ocs/ws/ws0067/10124-45900-1-PB.pdf"><i>《可修正性》</i></a><i> </i>苏亚雷斯等人。并变得困惑。我遵循了大部分数学推理，但现在正在努力理解这种研究途径的目的或最终目标是什么。我知道 MIRI 现在追求<a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/">不同的研究方向</a>，所以我在这里写的部分或全部内容可能没有实际意义。</p><h2>这一段不是破坏了整个项目吗？</h2><p>在第 2.1 节中，“操纵行为” <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^-"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>被认为。根据<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">UN</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">（</span></span></span></span></span></span></span></span></span>除非按下关机按钮，否则代理应最大化的“试验”效用函数），此操作会导致效用较低，但会降低按下关机按钮的概率。这样的行为可能具有操纵性，这是有道理的。但作者随后澄清了以下几点：</p><blockquote><p>我们指出，[ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^-"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span>满足的数学条件]无论如何都不是对防止[按下按钮]的操纵动作的必要或充分的描述。满足[这些条件]的某些操作可能是非操纵性的（例如，如果代理花费额外的努力来确保新的子代理是可纠正的，并且这种行为有助于让程序员相信该代理确实是可纠正的）。此外，许多操纵行为可能不具有这种形式<br>（我们将在 4.2 节中看到）。相反，我们考虑一个操纵行为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^-"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span> ，它恰好是操纵性的并且具有这种形式，并且表明如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c_{high}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span></span></span></span></span>设置得太低，那么 U 将激励 U-agents 更喜欢这个<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^-"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span>到<br>默认动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^*"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></p></blockquote><p>嗯不错。但是，如果我们使用的框架中操纵行为与非操纵行为具有完全相同的数学表示，那么这是否表明我们的整个框架是错误的？我们当然需要区分这两种类型的行动吗？这一段不是准确地表明了该模型无法使用效用函数和动作来准确捕获可校正性吗？</p><p>假设我设计了一个效用函数，并在这个框架中证明它从未采取像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_1^-"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-stack" style="vertical-align: -0.315em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span>这样的欺骗性行为，那么这个证明也将表明我的效用函数不允许进行中描述的那种有用的、非操纵性的行为上面引用的段落。</p><p>相反，如果我确实创建了一个可纠正代理的现实生活实例，它将<i>区分</i>操纵行为和非操纵行为，因此无法以本文中使用的方式对世界进行建模，因为使用了数学表示论文中并不总是正确地区分这两种类型的行为。</p><br/><br/> <a href="https://www.lesswrong.com/posts/oaixvtpGHpkQP3y8C/a-question-about-corrigibility-2015#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oaixvtpGHpkQP3y8C/a-question-about-corrigibility-2015<guid ispermalink="false"> oaixvtpGHpkQP3y8C</guid><dc:creator><![CDATA[A.H.]]></dc:creator><pubDate> Mon, 27 Nov 2023 12:05:51 GMT</pubDate> </item><item><title><![CDATA[Appendices to the live agendas]]></title><description><![CDATA[Published on November 27, 2023 11:10 AM GMT<br/><br/><p>列表是从<a href="https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety">我们的主帖</a>中剪下来的，象征性地提高了可读性。</p><p>我们列出了过去对对齐工作的评论、似乎已死的想法、很酷但被忽视的神经科学/生物学方法、似乎没有任何议程的各种组织，以及一堆不适合其他地方的东西。</p><p></p><h2>附录：先前的列举</h2><ul><li><a href="https://arxiv.org/abs/1805.01109"><u>埃弗里特等人 (2018)</u></a></li><li><a href="https://arxiv.org/abs/2310.19852"><u>吉 (2023)</u></a></li><li> <a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"><u>苏亚雷斯 (2023)</u></a></li><li> <a href="https://www.lesswrong.com/posts/ncsxcf8CkDveXBCrA/ai-safety-in-a-world-of-vulnerable-machine-learning-systems-1"><u>格利夫和麦克莱恩 (2023)</u></a></li><li> <a href="https://www.alignmentforum.org/posts/a9SPcZ6GXAg9cNKdi/linkpost-some-high-level-thoughts-on-the-deepmind-alignment"><u>Krakovna 和 Shah 谈 Deepmind (2023)</u></a></li><li><a href="https://ai-plans.com/"><u>人工智能计划（2023）</u></a> ，大多无关紧要</li><li><a href="https://www.lesswrong.com/posts/3vDb6EzBpaHqDqQif/some-summaries-of-agent-foundations-work-1"><u>麦克德莫特 (2023)</u></a></li><li> <a href="https://www.alignmentforum.org/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai"><u>Kirchner 等人 (2022)：无监督分析</u></a></li><li><a href="https://vkrakovna.wordpress.com/2022/06/02/paradigms-of-ai-alignment-components-and-enablers/"><u>Krakovna (2022)</u></a> ，人工智能对齐范式</li><li><a href="https://arxiv.org/abs/2012.07532"><u>胡宾格 (2020)</u></a></li><li> <a href="https://www.alignmentforum.org/posts/4az2cFrJp3ya4y6Wx/resources-for-ai-alignment-cartography"><u>佩雷特 (2020)</u></a></li><li> <a href="https://www.lesswrong.com/posts/SQ9cZtfrzDJmw9A2m/my-overview-of-the-ai-alignment-landscape-a-bird-s-eye-view"><u>难陀 (2021)</u></a></li><li> <a href="https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is"><u>拉森 (2022)</u></a></li><li> <a href="https://www.lesswrong.com/posts/8ibDJeoiDuxJkPwfa/various-alignment-strategies-and-how-likely-they-are-to-work"><u>佐尔纳 (2022)</u></a></li><li><a href="https://www.perfectlynormal.co.uk/blog-ai-risk-intro-2"><u>麦克杜格尔 (2022)</u></a></li><li> <a href="https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research#Approaches_grounded_in_the_theory_of_causality"><u>Sharkey 等人 (2022)</u></a>关于 interp</li><li> <a href="https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety"><u>科赫 (2020)</u></a></li><li> <a href="https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1"><u>克里奇 (2020)</u></a></li><li> <a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"><u>胡宾格论可解释性的类型</u></a>(2022)</li><li> <a href="https://www.zotero.org/groups/2420932/tai_safety_bibliography/collections/2BR54QX6"><u>Tai_safety_参考书目 (2021)</u></a></li><li><a href="https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet"><u>阿卡什和拉森 (2022)</u></a></li><li> <a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very"><u>卡诺夫斯基平淡计划</u></a>（2022）</li><li> <a href="https://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#heading=h.n1wk9bxo847o"><u>斯坦哈特 (2019)</u></a></li><li><a href="https://arxiv.org/abs/1602.03506"><u>拉塞尔 (2016)</u></a></li><li><a href="https://futureoflife.org/landscape/ResearchLandscapeExtended.pdf"><u>外国投资协会 (2017)</u></a></li><li><a href="https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review"><u>沙阿 (2020)</u></a></li><li><a href="https://www.lesswrong.com/tag/research-agendas?sortedBy=new"><u>声称是议程的事情</u></a></li><li><a href="https://docs.google.com/spreadsheets/d/1Us_eYfpzM5qg52aZh2Cc8O13Fdfiy5CMKmzZhN5ItOY/edit#gid=1246461801"><u>Tekofsky 上市独立公司 (2023)</u></a></li><li> <a href="https://www.lesswrong.com/posts/fjgoMaBenyXcRDrbX/boundaries-membranes-and-ai-safety-compilation"><u>这个叫做边界的东西（2023）</u></a></li><li> <a href="https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research"><u>Sharkey 等人 (2022)，机械解释员</u></a></li><li><a href="https://pbs.twimg.com/media/F98iVeGX0AAxOoU?format=jpg&amp;name=medium"><u>FLI 治理记分卡</u></a></li><li><a href="https://donations.vipulnaik.com/?cause_area_filter=AI+safety#donationAmountsByDoneeAndYear"><u>钱</u></a><br></li></ul><h2>附录：墓地</h2><ul><li>雄心勃勃的价值学习？</li><li> MIRI youngbloods（见赫巴尔）</li><li> <a href="https://www.lesswrong.com/tag/selection-theorems"><u>JW 选择定理</u></a>??</li><li><a href="https://people.eecs.berkeley.edu/~russell/papers/russell-bbvabook17-pbai.pdf"><u>已证明有益的人工智能</u></a>（但请参阅 Open Agency 和 Omohundro）</li><li> <a href="https://www.lesswrong.com/tag/hch"><u>HCH</u></a> （参见 QACI）</li><li> IDA → 批评和递归奖励模型</li><li>辩论现在称为批评和 ERO</li><li><a href="https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making"><u>做市商</u></a>（Hubinger）</li><li>逻辑电感器</li><li><a href="https://arxiv.org/abs/2302.00805"><u>调节预测模型：风险和策略</u></a>？</li><li><a href="https://www.lesswrong.com/tag/impact-regularization"><u>影响措施</u></a>、保守机构、副作用→“权力厌恶”</li><li> <a href="https://www.lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda"><u>可接受性验证</u></a></li><li>量化器</li><li><a href="https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing"><u>红木</u></a>插曲？</li><li><a href="https://www.aisafetyhub.org/our-research"><u>人工智能安全中心</u></a></li><li><a href="https://arxiv.org/abs/1702.03465"><u>使机器人能够传达他们的目标</u></a>（早期干预？）</li><li> <a href="https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models"><u>调整狭隘的超人模型</u></a>（Cotra 想法； <a href="https://www.alignmentforum.org/posts/TSxAXeHHhgSxR5wGZ/a-summary-of-aligning-narrowly-superhuman-models"><u>微小的后续行动</u></a>；作为可扩展的监督继续存在？）</li><li>语义可解释性的自动化<ul><li>即自动提出假设，而不仅仅是自动验证它们</li></ul></li><li><a href="https://www.lesswrong.com/tag/oracle-ai"><u>Oracle AI</u></a>并没有消亡，而是 LLM 的所有内容都属于它的职权范围<ul><li>类似地， <a href="https://www.lesswrong.com/tag/tool-ai"><u>AI工具</u></a>是LLM插入特定接口的</li></ul></li><li><a href="https://www.lesswrong.com/posts/AhF8iXLu5PchsmyKf/language-model-tools-for-alignment-research"><u>EleutherAI：#acceleating-alignment</u></a> - AI 对齐助手已上线，但 EleutherAI 目前似乎并未致力于此工作<br></li></ul><h2>附录：AI 对齐的生物学</h2><p>有很多议程，但不清楚除了伯恩斯和蒂尔加特之外是否还有人在积极推动。看起来需要十亿美元。<br></p><h3>人类增强</h3><ul><li><i>一句话总结</i>：也许我们可以为人们提供新的感觉方式，或者更高的概念信息带宽，或者更好的想法生成，或者与深度学习系统的直接接口，或者与传感器的直接接口，或者<a href="https://www.youtube.com/watch?v=6vMO3XmNXe4"><u>迁移学习</u></a>，也许这会有所帮助。我想，古老的<a href="https://fantasticanachronism.com/2021/03/23/two-paths-to-the-future/"><u>超级宝宝</u></a>梦想就在这里了。</li><li><i>变革理论：</i>也许这能让我们更好地进行一致性研究</li></ul><h3>合并</h3><ul><li><i>一句话总结</i>：也许我们可以形成深度学习系统和大脑的网络社会</li><li><i>变革理论：</i>也许这可以让我们通过讨价还价、投票或奇怪的政治来保留一些人类价值观。</li><li><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><u>机器人主义</u></a>,<a href="https://www.lesswrong.com/posts/d74pb97TAqNKwJkc5/bcis-and-the-ecosystem-of-modular-minds"><u>米利奇</u></a>,<a href="https://www.lesswrong.com/posts/gLyRQCg6kp5cqTQTm/collective-identity"><u>杜普伊斯</u></a></li></ul><h3>作为对齐辅助工具</h3><ul><li><i>一句话总结</i>：也许我们可以从大脑数据中获得真正高质量的对齐标签，也许我们可以通过训练人类快速直观地进行激活工程来引导模型</li><li><i>变革理论：</i>正如你所猜测的</li><li><a href="https://www.lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1"><u>Byrnes</u></a> 、 <a href="https://milan.cvitkovic.net/writing/neurotechnology_is_critical_for_ai_alignment/"><u>Cvitkovic</u></a> <i>、</i> <a href="https://manifund.org/projects/activation-vector-steering-with-bci"><u>Foresight 的 BCI</u></a></li></ul><h2><br>附录：研究支持机构</h2><p>示例 {CAIF, FLI} 描述了一个稍微令人困惑的组织类别。通常由具有丰富的协调经验的活跃研究人员管理，但通常不遵循明显的议程，将一篮子策略委托给受资助者，进行领域建设，例如 NeurIPS 研讨会和暑期学校。<br></p><p><a href="https://www.cooperativeai.com/foundation"><u>亚洲金融论坛</u></a></p><ul><li><i>一句话总结</i>：支持研究人员在合作人工智能方面取得差异性进展（例如不能用于制造威胁的预先承诺机制）</li><li><i>一些名字</i>：刘易斯·哈蒙德</li><li><i>预计 FTE 人数：</i> 3</li><li> <i>2023 年的一些成果</i>： <a href="https://www.cooperativeai.com/contests/melting-pot-2023"><u>Neurips 竞赛</u></a>、<a href="https://www.cooperativeai.com/summer-school/2023"><u>暑期学校</u></a></li><li><i>资助方：</i><a href="https://polaris-ventures.org/grants/"><i><u>北极星创投</u></i></a></li><li><i>批评：</i></li><li><i>资助者： ?</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i> £2,423,943<br></li></ul><p><a href="https://aisafety.camp/">航空航天工业协会</a></p><ul><li><i>一句话总结</i>：新研究人员测试适合度和结识合作者的切入点。最近的重点是功能暂停。<a href="https://aisafety.camp/#Projects">仍在继续！</a></li><li><i>一些名字</i>：Remmelt Ellen、Linda Linsefors</li><li><i>预计 FTE 人数：2</i></li><li> <i>2023 年的一些产出</i>：<a href="https://www.lesswrong.com/tag/ai-safety-camp">标签</a></li><li><i>资助者： ?</i></li><li><i>批评：？</i></li><li><i>资助者： ?</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i> ~200,000 美元</li></ul><p></p><p></p><p>也可以看看：</p><ul><li><a href="https://futureoflife.org/">弗利</a></li><li><a href="https://www.aisafetysupport.org/">艾斯</a></li><li><a href="https://www.pibbss.ai/"><u>PIBBSS</u></a></li><li><a href="https://www.aitracker.org/"><u>格莱斯顿人工智能</u></a></li><li><a href="https://apartresearch.com/"><u>公寓</u></a></li><li><a href="https://www.catalyze-impact.org/team"><u>催化</u></a></li><li><a href="https://ia.effisciences.org/"><u>效率科学公司</u></a></li><li>学生（<a href="https://www.oxai.org/ai-safety"><u>牛津</u></a>、<a href="https://haist.ai/"><u>哈佛大学</u></a>、<a href="https://www.aisig.org/"><u>格罗宁根</u></a>、<a href="https://bristolaisafety.org/about"><u>布里斯托</u></a>、<a href="https://www.cambridgeaisafety.org/start"><u>剑桥</u></a>、<a href="https://stanfordaialignment.org/"><u>斯坦福</u></a>、<a href="https://www.delftaisafety.org/"><u>代尔夫特</u></a>、<a href="https://www.mitalignment.org/"><u>麻省理工</u></a>）<br></li></ul><h2>附录：元、奥秘、更多</h2><ul><li><a href="https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty"><u>元哲学</u></a>( <a href="https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy"><u>2023</u></a> ) – Wei Dai，可能 &lt;1 FTE</li><li><a href="https://www.encultured.ai/blog-experiments.html"><u>培养</u></a>-制作游戏</li><li>布里克曼：https: <a href="https://compphil.github.io/truth/"><u>//compphil.github.io/truth/</u></a></li><li><a href="https://algorithmicalignment.csail.mit.edu/"><u>算法对齐</u></a></li><li>McIlrath：<a href="https://www.southampton.ac.uk/~eg/AAMAS2023/pdfs/p1797.pdf"><u>副作用</u></a>和<a href="https://openreview.net/pdf?id=nkn1rcI6wd"><u>具有人类意识的人工智能</u></a></li><li><a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#Table"><u>对齐有多难？</u></a></li><li><a href="https://arxiv.org/abs/2012.11538"><u>有趣的</u></a></li><li><a href="https://www.gov.uk/government/publications/ai-safety-institute-overview"><u>英国政府现在有一个评估/校准实验室</u></a>（Xander Davies、Nitarshan Rajkumar、Krueger、Rumman Chowdhury）</li><li> <a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial"><u>美国政府很快就会有一个类似评估/校准的实验室，USAISI</u></a> 。 （埃尔哈姆·塔巴西？）</li><li><a href="https://arxiv.org/abs/1809.01036"><u>稳健的端到端对齐路线图</u></a>(2018)</li><li><a href="https://arxiv.org/abs/2110.08176"><u>不是理论，但很酷</u></a></li><li><a href="https://www.joshua-greene.net/"><u>格林实验室认知监督</u></a>？他们最接近的是人工智能治理，但他们只得到了 13,800 美元</li><li><a href="https://www.gatoframework.org/gato-framework"><u>GATO 框架</u></a>似乎有一个银河大脑的全球协调和/或自动调整人工智能角度</li><li>Wentworth <a href="https://www.lesswrong.com/s/ehnG4mseKF6xALmQy"><u>2020</u></a> - 适合嵌入式代理的抽象理论。</li><li> <a href="https://www.aintelope.net/"><u>https://www.aintelope.net/</u></a></li><li> <a href="https://araac.au/"><u>ARAAC</u></a> - 制作可以在不同抽象级别用人类语言证明自己（或其他以代码作为输入的代理）的 RL 代理（ <a href="https://link.springer.com/article/10.1007/s00521-023-08586-x"><u>AI 道歉</u></a>，<a href="https://link.springer.com/article/10.1007/s00521-023-08423-1"><u>广泛 XAI 的可解释 RL）</u></a></li><li><a href="https://www.modelingcooperation.com/research"><u>模特合作</u></a></li><li>夹心（ <a href="https://www.lesswrong.com/posts/qwQqZtjWdyDLC4JTB/automated-sandwiching-and-quantifying-human-llm-cooperation"><u>前</u></a>）</li><li><a href="https://www.lesswrong.com/tag/tripwire"><u>绊线</u></a><ul><li><a href="https://www.lesswrong.com/posts/7yEFHisCQSCpLnqWQ/mr-meeseeks-as-an-ai-capability-tripwire"><u>米塞克斯人工智能</u></a></li><li><a href="https://www.lesswrong.com/posts/FgsoWSACQfyyaB5s7/shutdown-seeking-ai"><u>寻求关机的人工智能</u></a></li></ul></li><li>决策理论（<a href="https://www.levinstein.org/research"><u>莱文斯坦</u></a>）</li><li> <a href="https://www.alignmentforum.org/s/FaEBwhhe3otzYKGQt/p/5HtDzRAk7ePWsiL2L#Implementing_Moral_Decision_Making"><i><u>CAIS：机器伦理</u></i></a>——对内在商品和规范因素进行建模，而不是单纯的任务偏好，因为即使在极端的世界变化下，这些也将是相关的；帮助我们避免代理错误指定以及价值锁定。研究现状未知（ <a href="https://arxiv.org/pdf/2310.01405.pdf"><u>RepEng</u></a>中的一些相关位）。</li><li> <a href="https://www.alignmentforum.org/s/FaEBwhhe3otzYKGQt/p/5HtDzRAk7ePWsiL2L#Power_averseness"><i><u>CAIS：权力厌恶</u></i></a>——激励模型避免获得不必要的权力。与轻度优化和权力寻求相关。研究现状未知。</li></ul><br/><br/><a href="https://www.lesswrong.com/posts/GdyYngK9YPdWSRsC6/appendices-to-the-live-agendas#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GdyYngK9YPdWSRsC6/appendices-to-the-live-agendas<guid ispermalink="false"> GdyYngK9YPdWSRSC6</guid><dc:creator><![CDATA[technicalities]]></dc:creator><pubDate> Mon, 27 Nov 2023 11:10:33 GMT</pubDate> </item><item><title><![CDATA[Shallow review of live agendas in alignment & safety]]></title><description><![CDATA[Published on November 27, 2023 11:10 AM GMT<br/><br/><h1>概括</h1><p>如果您不知道当前资源是什么，则无法优化资源分配。现有的对齐研究地图大多太旧了，无法指导您，而且该领域几乎没有<i>棘轮</i>，没有关于每个人在做什么和为什么、放弃什么和为什么、什么被重命名、什么与什么相关、正在发生什么的常识。</p><p>这篇文章主要只是一个大索引：我们能找到的尽可能多的当前活跃的人工智能安全议程的链接转储。但即使是链接转储也很主观。它将工作映射到概念集群 1-1，旨在回答这样的问题：“我想知道我在那次会议上听到的令人兴奋的想法发生了什么”和“我刚刚读了一篇关于令人惊讶的新见解的文章，想看看还有谁一直在研究这个”，“我想知道大约有多少人正在研究那件事”。该文档太长，无法阅读，因此可以按 Ctrl-F 编辑。这样你就可以分叉列表并制作一个更小的列表。您可以<a href="https://www.lesswrong.com/posts/GdyYngK9YPdWSRsC6/appendices-to-the-live-agendas">在这里</a>找到更多已删除的材料。</p><p>你们中的大多数人应该只阅读社论并浏览您所从事的部分。</p><p>我们的分类：</p><ol><li>了解现有模型（评估、可解释性、深度学习科学）</li><li>控制事物（防止欺骗、模型编辑、价值学习、目标稳健性）</li><li>让人工智能解决这个问题（可扩展的监督、机器人主义等）</li><li>理论（星系脑端到端、代理、可修正性、本体论、合作）<br></li></ol><p>我们不区分大型实验室、个体研究人员和从事类似研究的人员组成的稀疏连接的网络。资金数额和全职员工的估计可能是一个合理的代理。</p><p>我们选择的类别有很大的重叠，并且可以看到“另请参阅”密切相关的工作。</p><p>请指出我们是否错误地将一件事四舍五入、错误分类某人或以其他方式陈述或暗示谎言。<strong>我们将编辑。</strong></p><p>与最近的<a href="https://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison"><u>Larks 评论</u></a>不同，我们的主要目标并不是直接捐赠。但如果您喜欢阅读本文， <a href="https://www.alignmentforum.org/posts/SbC7duHNDHkd3PkgG/alignment-grantmaking-is-funding-limited-right-now"><u>请考虑</u></a>向<a href="https://manifund.org/about/donate"><u>Manifund</u></a> 、 <a href="https://manifund.org/projects/mats-funding"><u>MATS</u></a>或<a href="https://www.givingwhatwecan.org/en-GB/charities/long-term-future-fund?utm_source=eafunds"><u>LTFF</u></a>或<a href="mailto:funds@lightspeedgrants.org"><u>Lightspeed</u></a>进行大笔捐款：一些好的工作受到资金的瓶颈，而您可以免费获得专家的服务，为好的工作提供资金。<br></p><h1>元</h1><p>当我（加文）进入联盟时（实际上它仍然是“AGI 安全”），人们<a href="http://www.foldl.me/2018/conceptual-issues-ai-safety-paradigmatic-gap/"><u>警告</u></a>我这是前范式的。他们是对的：在接下来的五年里，现场议程完全<a href="https://arxiv.org/pdf/1805.01109.pdf"><u>改变</u></a>了。 <span class="footnote-reference" role="doc-noteref" id="fnref06qm74k93eh7"><sup><a href="#fn06qm74k93eh7">[1]</a></sup></span>这里有一个更新。</p><p>我希望这是一个直接的技术对齐文档，但人们指出这会排除<i>大多数</i>工作（例如评估和非雄心勃勃的可解释性，它们是安全的，但不是对齐的），所以我将其设为技术 AGI 安全文档。再加上变化。</p><p>唯一的选择标准是“我听说过并且 >;= 1 人最近正在研究它”。我不参加聚会，所以可能会晚几个月。</p><p>显然，这是治理和倡导年，但我排除了所有这些好的工作：就其本质而言，它会引起关注。我也没有找到<a href="https://openreview.net/forum?id=RdJVFCHjUMI"><u>那些</u></a><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321"><u>不将</u></a>自己的工作视为一致的<a href="https://arxiv.org/abs/2211.14946"><u>普通</u></a><a href="https://blog.normalcomputing.ai/posts/2023-09-12-supersizing-transformers/supersizing-transformers.html"><u>实验室</u></a><a href="https://huggingface.co/papers/2310.16944"><u>和</u></a><a href="https://arxiv.org/abs/2306.03341"><u>学者</u></a><a href="https://arxiv.org/abs/2205.01447"><u>的</u></a>显着数量。也不是秘密工作。</p><p><i><strong>契诃夫的评价</strong></i>：我包括了尤德科夫斯基的作战<a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"><u>标准</u></a>（值得信赖的指挥？，封闭？，行动安全？，对共同利益的承诺？，结盟心态？），但我自己不给它们打分。重点不是要遮遮掩掩，而是要提醒你，我们常常对彼此知之甚少。</p><p>你不太可能喜欢我对子字段的划分； <a href="https://docs.google.com/document/d/e/2PACX-1vRyMTQQprqaFrpydGTwsHm0BnFQf5CXC9T754UeXtkwiNVQx7N5LGki51L05_L3g1wK4_n1llVGvZ9R/pub"><u>还有</u></a>其他的。</p><p>没有人读过所有这些材料，包括我们。条目尽可能基于公共文档或私人信件，但该帖子可能仍包含超过 10 个不准确的说法。<strong>我们鼓励对我们大喊大叫。</strong>如果我错过了你（或错过了重点），请注意你自己。 5年后见。<br></p><h1>社论</h1><ul><li>现在，结盟已经足够出名，以至于巴拉克·奥巴马(Barack Obama)都<a href="https://barackobama.medium.com/statement-on-the-biden-administrations-executive-order-on-artificial-intelligence-91a5ddac6238"><u>在</u></a>谈论它。这将吸引登山者、骗子、好心人以及那些只是滥用这个词的人，因为它客观上令人困惑，并吸引金钱和善意。由于动机性语义蠕变，我们已经不得不半放弃“人工智能安全”。<br></li><li>低信心：Mech interp 现在可能已经拥有了自己的一部分人（尽管我承认这是一个非常清晰的入口，并且有很多预先咀嚼的项目准备就绪）。<br></li><li> <a href="https://www.matsprogram.org/"><u>MATS</u></a>效果很好（平均而言，方差较高）。伦敦延伸是一个非常好的主意。他们刚刚从 SFF 获得 18.5 万美元，但<a href="https://manifund.org/projects/mats-funding"><u>仍然</u></a>受到限制。<br></li><li>不包括治理工作就遗漏了许多很酷的“技术政策”：<a href="https://epochai.org/"><u>预测</u></a>、<a href="https://arxiv.org/abs/2303.11341"><u>计算监控</u></a>、<a href="https://arxiv.org/abs/2210.08674"><u>去信任模型验证</u></a>、<a href="https://www.matsprogram.org/safety"><u>安全案例</u></a>。<br></li><li>全新类型的人正在做出贡献，这很好。我想到的是 PIBBSS 和 CAIS 哲学家、SLT 暴民和 Eleuther 的不和谐能量。<br></li><li>大型实验室似乎将农场的赌注押在了可扩展的监督上。这依赖于没有巨大的能力峰值，也没有不可逆转的误解。<br></li><li>不协调且仅部分范式领域的事实上的议程是<a href="https://www.lesswrong.com/posts/D4gEDdqWrgDPMtasc/thoughts-on-process-based-supervision-1"><u>基于流程的监督</u></a>/<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12786"><u>纵深防御</u></a>/<a href="https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1"><u>大杂烩</u></a>/<a href="https://www.lesswrong.com/posts/MCWGCyz2mjtRoWiyP/endgame-safety-for-agi"><u>残局安全</u></a>/ <a href="https://forum.effectivealtruism.org/posts/yCx3kCReJtucpdd33/the-current-alignment-plan-and-how-we-might-improve-it-or#v1_Plan"><u>Shlegeris v1</u></a> 。我们将把在子 AGI 中工作的十几个东西放在一起，并希望：RLHF/DPO + <a href="https://www.greaterwrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comment-7iBb7aF4ctfjLH6AC"><u>大规模</u></a><a href="https://twitter.com/jd_pressman/status/1718798739266212325"><u>激活修补</u></a>+ 范围模型缩小 + 装箱 + 可疑的可扩展监督 + 短视训练 +<a href="https://arxiv.org/abs/2302.08582"><u>数据管理</u></a>+ 合格的自动对齐研究（证明助手）+ …我们还将通过创建（可破解的，本身会减慢 OODA）安全文化来减慢速度。谁知道。<br></li></ul><h1>议程</h1><h2>1.了解现有模型</h2><p><a href="https://en.wikipedia.org/wiki/Characterization_(materials_science)"><i>表征</i></a><br></p><h3>埃瓦尔斯</h3><p>（弄清楚经过训练的模型的行为方式。）<br></p><p><i>各项</i><a href="https://www.openphilanthropy.org/research/rfps-on-llm-impacts/"><i><u>能力评估</u></i></a></p><ul><li><i>一句话总结</i>：制作能够真正检查模型是否具有某种能力/错位模式的工具。我们默认对巨大的潜在空间进行低n采样，但目标是做得更好。</li><li><i>变革理论：</i>大多数模型在首次训练和发布时都有能力悬置；我们应该密切关注何时获得哪些功能，以便前沿模型开发人员更好地了解哪些安全措施已经是必要的（希望他们能够推断并最终恐慌）。</li><li>将<a href="https://evals.alignment.org/"><u>ARC Evals</u></a> <i>、</i> <a href="https://arxiv.org/abs/2305.15324"><u>Deepmind</u></a> 、 <a href="https://cavendishlabs.org/publications/"><u>Cavendish</u></a> 、<a href="https://arxiv.org/abs/2309.00667"><u>态势感知</u></a>团队、 <a href="https://www.matsprogram.org/evals"><u>Evans 和 Ward</u></a> 、 <a href="https://www.apolloresearch.ai/blog/understanding-da-and-sd"><u>Apollo</u></a>组合在一起。另见<a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp"><u>模型心理学</u></a>；神经科学：心理学：：可解释性：模型心理学。另请参见<a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations"><u>对齐评估</u></a>。另请参阅<a href="https://arxiv.org/abs/2304.11158"><u>能力</u></a><a href="https://arxiv.org/abs/2202.07785"><u>预测</u></a>和数百名巨魔进行<i>分散</i><a href="https://www.youtube.com/watch?v=g7YJIpkk7KM"><u>式</u></a><a href="https://twitter.com/jbrowder1/status/1652387444904583169"><u>评估</u></a>。</li><li><i>一些名字：</i> Mary Phuong、Toby Shevlane、Beth Barnes、Holden Karnofsky、Lawrence Chan、Owain Evans、Francis Rhys Ward、Apollo、Palisade</li><li><i>估计 FTE 数：</i> 13 (ARC)，其他地方约 50</li><li> <i>2023 年的一些成果</i>： <a href="https://arxiv.org/abs/2310.03302"><u>AI 人工智能研究</u></a>、<a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><u>自主性</u></a>、<a href="https://arxiv.org/abs/2304.03279"><u>回报是否合理？</u></a> ，<a href="https://arxiv.org/abs/2304.12280"><u>固执</u></a>。为 GPT 的发展<a href="https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers"><u>命名</u></a>是很有用的。<a href="https://www.alignmentforum.org/tag/ai-evaluations"><u>标签</u></a>。</li><li><i>评论：</i> <a href="https://www.alignmentforum.org/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations"><i><u>Hubinger</u></i></a> <i>、</i> <a href="https://www.alignmentforum.org/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations"><i><u>Hubinger</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals"><i><u>Shovelain 和 Mckernon</u></i></a> <i>&nbsp;</i></li><li><i>资助者：</i>各种。 <i>&nbsp;</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i> ~~20,000,000 美元，不包括政府的新努力<br></li></ul><p><a href="https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety"><i><u>各种各样的</u></i></a><i>&nbsp;</i><a href="https://www.deepmind.com/blog/red-teaming-language-models-with-language-models"><i><u>红队</u></i></a></p><ul><li><i>一句话总结</i>：让我们攻击当前模型，看看它们做了什么/故意在当前前沿模型上诱发坏事，以检验我们的理论/方法。另请参阅<a href="https://forum.effectivealtruism.org/posts/yCx3kCReJtucpdd33/the-current-alignment-plan-and-how-we-might-improve-it-or#Gain_of_function"><u>功能实验增益</u></a>（制作演示和错位玩具模型。另请参阅“提供批评的模型”。另请参阅：威胁建模（ <a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>Model Organisms</u></a> <u>、</u> <a href="https://arxiv.org/abs/2304.06528"><u>Powerseeking</u></a> 、 <a href="https://www.apolloresearch.ai/blog/understanding-da-and-sd"><u>Apollo</u></a> ）；<a href="https://arxiv.org/abs/2310.18512"><u>隐写术</u></a>；OpenAI 超级对齐计划的<a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=phFqC2EdDALCFwr56"><u>一部分</u></a>；<a href="https://arxiv.org/abs/2302.10894"><u>特洛伊木马</u></a>（ <a href="https://trojandetection.ai/tracks"><u>CAIS</u></a> ）； <a href="https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training#comments"><u>潜在对抗训练</u></a>是一个不寻常的例子。</li><li><i>一些名字：</i> Stephen Casper、Lauro Langosco、Jacob Steinhardt、Nina Rimsky、Jeffrey Ladish/Palisade、Ethan Perez、Geoffrey Irving、 <a href="https://www.vice.com/en/article/jg5ew4/gpt4-hired-unwitting-taskrabbit-worker"><u>ARC Evals</u></a> 、Apollo、Dylan Hadfield-Menell/ <a href="https://algorithmicalignment.csail.mit.edu/team/"><u>AAG</u></a></li><li><i>估计 FTE 数：</i> ?</li><li> <i>2023 年的一些产出</i>： <a href="https://www.alignmentforum.org/posts/iHmsJdxgMEWmAfNne/red-teaming-language-models-via-activation-engineering"><u>Rimsky</u></a> 、 <a href="https://far.ai/publication/wang2022adversarial/"><u>Wang</u></a> 、 <a href="https://arxiv.org/abs/2307.02483"><u>Wei</u></a> 、 <a href="https://arxiv.org/abs/2306.12105"><u>Tong</u></a> 、 <a href="https://arxiv.org/abs/2306.09442"><u>Casper</u></a> 、 <a href="https://arxiv.org/abs/2311.00117"><u>Ladish</u></a> 、 <a href="https://docs.google.com/document/d/1xdidMGrzHDIJ5sG2h1TKb-HiGh-hgsMY3jugnvd6Aqo/edit#heading=h.9axbdg9kpll0"><u>Langosco</u></a> 、 <a href="https://arxiv.org/abs/2311.03348"><u>Shah</u></a> 、 <a href="https://arxiv.org/abs/2311.07590"><u>Scheurer</u></a> 、 <a href="https://algorithmicalignment.csail.mit.edu/research/"><u>AAG</u></a> 、2022 年： <a href="https://arxiv.org/abs/2202.03286"><u>Irving</u></a></li><li><i>批评：</i> ？</li><li><i>资助者：各种</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i>大<br><br></li></ul><p><i>引发模型异常</i></p><ul><li><i>一句话总结</i>：以一种既不追求功能也不完全是红队的方式发现当前模型的奇怪特征。想想<a href="https://arxiv.org/abs/2306.09479"><u>逆缩放</u></a>、 <a href="https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"><u>SolidGoldMagikarp</u></a> 、<a href="https://arxiv.org/abs//2309.12288"><u>反转诅咒</u></a>，<a href="https://openreview.net/forum?id=X3JFgY4gvf"><u>断章取义</u></a>。不是一个议程，而是对其他人的乘数。</li><li><i>变革理论：</i>也许异常和边缘情况告诉我们一些关于模型的深层信息；你需要数据来推理。<br><br></li></ul><p> <a href="https://manifold.markets/RyanGreenblatt/redwood-research-will-we-publish-a-98d97ff076da"><i><u>红木：对照评估</u></i></a></p><ul><li><i>一句话总结</i>：评估人类/组织：通过默默地交换故意错位的模型，评估在特定场景中部署模型的特定协议的风险。在 sim 中执行场景，直到 1) 发生灾难，2) 你的协议自信地发现红队模型，或者 3) 场景结束时没有发生灾难。还进行对抗性评估（使用可信和不可信模型监控不可信模型输出）。 <a href="https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><u>前任</u></a>。</li><li><i>变革理论：</i>通过自动评估和部署计划的风险评估来防止高风险失败。</li><li><i>一些名字：</i> Buck Shlegeris、Ryan Greenblatt、Kshitij Sachan</li><li><i>预计 FTE 人数：</i> 10？</li><li> <i>2023 年的一些成果</i>： <a href="https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><u>大文章</u></a>，另一篇文章正在制作中</li><li><i>批评：</i> <a href="https://forum.effectivealtruism.org/posts/DaRvpDHHdaoad9Tfu/critiques-of-prominent-ai-safety-labs-redwood-research"><i><u>一般组织</u></i></a></li><li><i>资助者：开放菲尔</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i> <a href="https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/"><i><u>5,300,000 美元</u></i></a><i>（去年整个组织）</i><br><br></li></ul><p><a href="https://acsresearch.org/"><i><u>复杂系统的协调</u></i></a><i>：法学硕士相互作用</i></p><ul><li><i>一句话总结</i>：了解法学硕士的相互作用及其局限性，并从实证工作转向关于法学硕士复杂系统的更一般的假设，例如混合系统和支架模型中的网络效应。</li><li><i>变革理论：</i>有时，总体比个体更容易预测/推理：细节平均化。因此，尝试一下法学硕士的互动（操纵、冲突解决、系统偏见等）。直接研究未来大型系统中的LLM交互（与当前的单一焦点相反）；防止系统性的不良设计并为未来的模型提供信息。</li><li><i>一些名字：</i> Jan Kulveit、Tomáš Gavenčiak、Ada Böhm</li><li><i>预计 FTE 人数：</i> 4</li><li> <i>2023 年的一些成果</i>：<a href="https://github.com/acsresearch/interlab"><u>软件</u></a>和对法学硕士的<a href="https://acsresearch.org/posts"><u>见解</u></a></li><li><i>批评：</i> <a href="https://www.alignmentforum.org/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group?commentId=frEufx3c6cRmhDjbh">Yudkowsky</a>对接口思想的看法</li><li><i>资助方：</i> SFF</li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源：</i> ~300,000 美元<br></li></ul><h3>其他评估（监管基础）</h3><p> Evals 组织和治理团队的大部分工作是其他的：开发<a href="https://www.rand.org/content/dam/rand/pubs/testimonies/CTA2700/CTA2723-1/RAND_CTA2723-1.pdf"><i><u>政治上清晰的</u></i></a><a href="https://www.alignmentforum.org/posts/Zfk6faYvcf5Ht7xDx/compute-thresholds-proposed-rules-to-mitigate-risk-of-a-lab"><u>指标</u></a>、<a href="https://www.matsprogram.org/safety"><u>流程</u></a>/<a href="https://twitter.com/apolloaisafety/status/1721594465716928751"><u>令人震惊的</u></a><a href="https://www.bbc.co.uk/news/technology-67302788.amp"><u>案例</u></a>研究。目的是激励和支持实际合理的监管。</p><p>这是一项重要的工作——可以说是短期内影响力最大的工作。但这是一个技术调整帖子。我添加这一部分是为了强调这些其他评估与理解危险能力如何已经或可能出现不同。<br></p><h3>可解释性</h3><p>（弄清楚经过训练的模型实际上正在计算什么。） <span class="footnote-reference" role="doc-noteref" id="fnref0q1picyzu4d"><sup><a href="#fn0q1picyzu4d">[2]</a></sup></span><br></p><p> <a href="https://forum.effectivealtruism.org/posts/zcHdehWJzDpfxJpmf/what-i-would-do-if-i-wasn-t-at-arc-evals#Ambitious_mechanistic_interpretability"><i><u>雄心勃勃的机械解释员</u></i></a></p><ul><li>从学习算法的完整自下而上电路级重构的意义上来说。</li><li><i>一句话总结</i>：自动找到所有事情的电路，然后弄清楚模型是否会做坏事（哪个算法实现哪个计划；具有合理数量节点的完整因果图）；任何会做坏事的模型都可以被删除或编辑。</li><li><i>变革理论：</i>通过本体识别、欺骗和规划审计、一致性研究的力量倍增器、干预使训练更安全、推理时间控制来对假设的实时监控采取行动来帮助一致性。迭代那些不计划的事情。另请参阅可扩展的监督。</li><li><i>一些名字：</i> Chris Olah、Lee Sharkey、Neel Nanda、Steven Bills、Nick Cammarata、Leo Gau、William Saunders、Apollo（私人作品）</li><li><i>预计全职员工人数：</i> 80？ （Anthropic、Apollo、DeepMind、OpenAI、各种较小的组织）</li><li> <i>2023 年的一些输出</i>：<a href="https://transformer-circuits.pub/2023/monosemantic-features"><u>单语义特征</u></a>、<a href="https://arxiv.org/pdf/2311.03658.pdf"><u>线性表示假设</u></a>似乎很快就会得到证实； <a href="https://www.lesswrong.com/posts/L8LHBTMvhLDpxDaqv/research-agenda-formalizing-abstractions-of-computations-1"><u>Jenner</u></a> ,<a href="https://arxiv.org/abs/2302.03025"><u>普遍性</u></a>, <a href="https://arxiv.org/abs/2307.09458"><u>Lieberum</u></a> , <a href="https://www.lesswrong.com/posts/wjQkQ8bgWWFym8zF9/distilled-representations-research-agenda-1"><u>蒸馏代表</u></a><ul><li><i>自动化：</i> <a href="https://arxiv.org/abs/2304.14997"><i><u>ACDC</u></i></a> <i>、</i> <a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models"><i><u>较大模型读取较小模型</u></i></a><i>、</i><a href="https://arxiv.org/pdf/2309.08600.pdf"><i><u>较大模型的稀疏模型</u></i></a></li><li><i>逆向工程。尚不清楚这是否会被进一步推动。 2022：</i><a href="https://transformer-circuits.pub/"><i><u>人择电路</u></i></a><i>、</i> <a href="https://www.alignmentforum.org/posts/DZk6mRo9vhCXN9Rfn/a-walkthrough-of-interpretability-in-the-wild-w-authors"><i><u>野外可解释性</u></i></a><i>、</i> <a href="https://www.neelnanda.io/blog/interlude-a-mechanistic-interpretability-analysis-of-grokking"><i><u>Grokking mod 算术</u></i></a></li></ul></li><li><i>评论：</i> <a href="https://docs.google.com/document/d/e/2PACX-1vTd5abxOSLcbq5wkUln7v2bVWvrEdsazQZZXfufEhgDycEktM_0jxvRAePEtFQNA2T84bXsJqeeEVl_/pub"><i><u>总结如下</u></i></a><i>：</i> <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><i><u>Charbel</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/FDrgcfY8zs5e2eJDd/charbel-raphael-and-lucius-discuss-interpretability"><i><u>Bushnaq</u></i></a> <i>、</i> <a href="https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7/p/7TFJAvjYfMKxKQ4XS"><i><u>Casper</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research"><i><u>Shovelain &amp; Mcckernon</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide"><i><u>RicG</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous"><i><u>Kross</u></i></a> <i>、</i> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><i><u>Hobbhahn</u></i></a> <i>。</i></li><li><i>资助者：各种</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><a href="https://www.matsprogram.org/concept"><i><u>基于概念的解释</u></i></a><i>&nbsp;</i></p><ul><li><i>一句话总结</i>：如果事实证明对模型的彻底理解太难/不可能，我们仍然可以进入某种高抽象层次，并仍然避免错位的 AGI。又名“高级可解释性”。</li><li><i>变革理论：</i>构建可以输出模型内部目标或功能的可能和预测性表示的工具，从而解决内部一致性问题。</li><li><i>一些名字：</i> Erik Jenner、Jessica Rumbelow、Stephen Casper、Arun Jose、Paul Colognese</li><li><i>估计 FTE 数： ?</i></li><li> <i>2023 年的一些成果</i>： <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives"><u>高级可解释性</u></a>、 <a href="https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight"><u>人工智能监督的内部目标信息</u></a></li><li><i>批评：</i></li><li><i>资助者： ?</i></li><li><i>值得信赖的命令、关闭、操作安全、共同利益、一致心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><a href="https://ai.stanford.edu/blog/causal-abstraction/"><i><u>因果抽象</u></i></a></p><ul><li><i>一句话概括</i>：Wentworth <a href="https://www.lesswrong.com/s/ehnG4mseKF6xALmQy"><u>2020</u></a> ；部分描述神经网络或其他计算正在使用的“算法”，同时丢弃不相关的细节。</li><li><i>变革理论：</i>找到给定计算的所有可能的抽象 ->; 将它们翻译成人类可读的语言 ->; 识别有用的抽象，例如欺骗 ->; 在模型执行此操作时进行干预。还发展更广泛的 interp 理论作为乘数；更数学化（希望更通用）的分析。</li><li><i>一些名字：</i>埃里克·詹纳、阿蒂克斯·盖革</li><li><i>估计 FTE 数： ?</i></li><li> <i>2023 年的一些成果</i>： <a href="https://www.alignmentforum.org/posts/L8LHBTMvhLDpxDaqv/research-agenda-formalizing-abstractions-of-computations-1"><u>詹纳议程</u></a>、<a href="https://arxiv.org/abs/2301.04709"><u>忠实模型解释的因果抽象</u></a></li><li><i>批评：</i></li><li><i>资助者： ?</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p> <a href="https://www.eleuther.ai/interpretability"><i><u>Eleutherai</u></i></a> <i>Interp</i></p><ul><li><i>一句摘要</i>：研究诸如培训路径依赖性等问题的工具。</li><li><i>变革理论：</i>制造出惊人的工具来推动可解释性的前沿。</li><li><i>一些名字：</i> Stella Biderman，Nora Belrose，Ai_waifu，Shivanshu Purohit</li><li><i>估计的＃ftes：〜12</i>加~~ 50兼职志愿者</li><li><i>2023年的一些输出</i>： <a href="https://arxiv.org/abs/2306.03819"><u>Leace</u></a> （另请参见手术模型编辑）；<a href="https://arxiv.org/abs/2303.08112"><u>调谐镜头</u></a>； CCS的改进： <a href="https://www.theojaffee.com/p/7-nora-belrose#details"><u>VINC</u></a> ；<a href="https://github.com/EleutherAI/elk-generalization"><u>麋鹿的概括</u></a></li><li><i>批评：</i></li><li><i>资助：拥抱面，稳定性AI，Nat Friedman，Lambda Labs，Canva，Coreweave</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 2,000,000？ （猜测）</i><br></li></ul><p> <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without#Our_Paper"><i><u>激活工程</u></i></a><i>（作为无监督的插入）</i></p><ul><li><i>一句话摘要</i>：干预模型表示，因此在出现不诚实，动力和其他内在风险时获得良好的因果证据；还测试可解释性理论和编辑理论。另请参见下面“模型编辑”下的同名部分。</li><li><i>变革理论：</i>测试可解释性理论作为变革理论的一部分；从可解释的因果干预措施中查找有关表示的新见解。无监督意味着没有注释偏见，这降低了提取超人表示的障碍。</li><li><i>一些名字：</i> Alex Turner，Collin Burns，Andy Zou，KaarelHänni，Walter Laurito， <a href="https://cadenzalabs.org/research/#research-agenda"><u>Cadenza</u></a> （ <a href="https://manifund.org/projects/cadenza-labs-ai-safety-research-group-working-on-own-interpretability-agenda"><u>仿佛</u></a>）</li><li><i>估计的＃ftes：〜15</i></li><li> <i>2023年的一些输出</i>：去年<a href="https://arxiv.org/abs/2212.03827"><u>著名的CCS</u></a> ， <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector"><u>RL和GPTS中的转向向量</u></a>， <a href="https://www.lesswrong.com/posts/bFwigCDMC5ishLz7X/rfc-possible-ways-to-expand-on-discovering-latent-knowledge"><u>Cadenza RFC</u></a> ， <a href="https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a"><u>概念的形状</u></a>。 <a href="https://www.alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4"><u>罗杰实验</u></a>。另请参见<a href="https://arxiv.org/abs/2310.01405"><u>表示工程</u></a>。</li><li><i>批评：</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><a href="https://www.leap-labs.com/"><i><u>飞跃</u></i></a></p><ul><li><i>一句话摘要</i>：销售可解释性API的研究创业公司（视觉模型的模型不合时宜的功能）。旨在与数据无关（“想要直接从模型中提取信息，几乎不依赖培训或测试数据”）和全局（“ Mech Interp不够，我们需要捕获Gestalt的整体方法”））））） 。</li><li><i>变革理论：</i>制造人们想要使用的安全工具，现实生活中的压力测试方法，为自下而上的电路分析提供了强有力的替代方法。</li><li><i>一些名字：</i>杰西卡·伦贝洛（Jessica Rumbelow）</li><li><i>估计＃ftes：</i> 5</li><li> <i>2023年的一些输出</i>：<a href="https://arxiv.org/pdf/2309.17144.pdf"><u>原型生成</u></a></li><li><i>批评：？</i></li><li><i>资助者：私人投资者</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：途中数百万</i><br></li></ul><h3>了解学习</h3><p>（弄清楚该模型如何弄清楚。）<br></p><p> <i>Timaeu​​s：</i> <a href="https://www.alignmentforum.org/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><i><u>发展性解释性</u></i></a><i>和</i><a href="https://timaeus.co"><i><u>奇异学习理论</u></i></a><i>&nbsp;</i></p><ul><li><i>单句摘要</i>：构建用于检测，定位和解释模型中培训和文本学习的相变的工具，灵感来自奇异学习理论（SLT），统计物理学和发展生物学的概念。</li><li><i>变化理论：</i>当神经网络中的结构形成时，它可以留下清晰的发育痕迹，我们可以解释以弄清楚该结构的实现以及如何实现。这为可扩展的自动解释性铺平了一种方法。特别是，在学习过程结束时进行干预可能是没有希望的，因此我们希望尽早抓住并防止欺骗性以及其他危险的能力和价值观。</li><li><i>一些名字：</i> Jesse Hoogland，Alexander Gietelink Oldenziel，Daniel Murfet，Stan Van Wingerden</li><li><i>估计＃ftes：</i> 10</li><li> <i>2023年的某些输出</i>：<a href="https://arxiv.org/abs/2310.06301"><u>动态相变</u></a>，<a href="https://arxiv.org/abs/2308.12108"><u>单数模型中的退化</u></a>；另请参见<a href="https://arxiv.org/abs/2304.11158"><u>Eleuther</u></a>的<a href="https://github.com/EleutherAI/pythia"><u>毕曲察</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/s/SfFQE8DXbgkjk62JK/p/TjaeCWvLZtEDAS5Ex#Reasons_it_won_t_work"><u>自我</u></a>， <a href="https://www.lesswrong.com/posts/DqLHvJjuPdtrzuoas/my-impression-of-singular-learning-theory"><u>ege</u></a> ， <a href="https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory"><u>skalse</u></a></li><li><i>资助者：</i><a href="https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc"><u>仿真</u></a></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 145,000</i><br></li></ul><p> <a href="https://docs.google.com/document/d/1CdPvKZQzrkTCFIMfHNr0uukbTGhNF0ohZn_am4Z8kzo/edit#heading=h.9lmc73wscx1r"><i><u>Levoso算法可解释性</u></i></a></p><ul><li><i>一句话摘要</i>：在<a href="https://arxiv.org/abs/2210.14215"><u>Google的广告</u></a>上执行机械性解释性，以弄清模型是否在上下文中学习RL。</li><li><i>变更理论：</i>使用玩具模型获取有关与代理和优化相关的任何事物的实际数据，或者至少基础一些直觉 - >;围绕台面优化器，训练动态等的话语变得不那么困惑 - >;向安全AI迈进。</li><li><i>一些名字：</i>维克多·莱沃索（Victor Levoso）</li><li><i>估计＃ftes：</i> 1</li><li> <i>2023年的一些输出</i>：？</li><li><i>批评：</i></li><li><i>资助者：</i> Lightspeed</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i> 86,000美元<br></li></ul><p>其他各种努力：</p><ul><li> Grokking（ <a href="https://www.neelnanda.io/blog/interlude-a-mechanistic-interpretability-analysis-of-grokking"><u>Lieberum＆Nanda</u></a> ， <a href="https://www.lesswrong.com/posts/JK2QGfNGLjuFnrEvz/explaining-grokking-through-circuit-efficiency"><u>Shah随访</u></a>）。<a href="https://www.lesswrong.com/s/5omSW4wNKbEvYsyje/p/GpSzShaaf8po4rcmA"><u>教皇的批评</u></a>。</li><li>旧： <a href="https://docs.google.com/document/d/1AyuTphQ31rLHDtpZoEwEPb4fWbZna1H3hGx_YUACxk4/edit"><u>DL议程的科学</u></a></li><li><a href="https://www.anthropic.com/index/influence-functions"><u>人类：将输出追踪到培训数据</u></a></li><li><a href="https://manifund.org/projects/scaling-training-process-transparency"><u>缩放训练过程透明度</u></a>（Krzyzanowski）</li></ul><hr><h2> 2.控制事物</h2><p>（弄清楚如何可预测地影响模型行为。）<br></p><p><a href="https://ai-alignment.com/prosaic-ai-control-b959644d79c2"><i><u>平淡无奇</u></i></a><i>&nbsp;</i>默认情况下<a href="https://www.lesswrong.com/posts/ziNCZEm7FE9LHxLai/don-t-dismiss-simple-alignment-approaches"><i><u>对齐</u></i></a><i>/</i><a href="https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default"><i><u>对齐</u></i></a><i>&nbsp;</i></p><ul><li><i>一句话摘要</i>：通过优化其输出来轻推基础模型。 （ <a href="https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research"><u>RLHF</u></a> ，<a href="https://arxiv.org/abs/2212.08073"><u>宪法</u></a>， <a href="https://arxiv.org/abs/2310.16944"><u>DPO</u></a> ， <a href="https://www.superannotate.com/blog/llm-fine-tuning#supervised-fine-tuning-sft"><u>SFT</u></a> ， <a href="https://arxiv.org/abs/2112.00861"><u>HHH</u></a> ， <a href="https://arxiv.org/abs/2309.00267"><u>rlaif</u></a> 。）不是真正的议程，而是议程的一部分，例如<a href="https://tomekkorbak.com/papers/"><u>Korbak</u></a>或<a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/635156/Doctoral_Thesis_David_Lindner_RC.pdf?sequence=6&amp;isAllowed=y"><u>Lindner</u></a> ，或Redwood的无害无害的Finetunes或<a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very"><u>Karnofsky计划</u></a>。我喜欢这个名称“盲目输出对齐”，但“平淡的对齐”是完善的。</li><li><i>变化理论（估算）：</i>事物通常是平稳的，相关的功能比对齐更难，假设没有中型量剂，<a href="https://arxiv.org/pdf/2311.08379.pdf"><u>零射击欺骗很难</u></a>，假设学习人类本体论是学会的人类的偏好不排除在外。假设对齐是<a href="https://arxiv.org/pdf/2305.11206.pdf#:~:text=We%20define%20the%20Superficial%20Alignment,used%20when%20interacting%20with%20users."><u>浅表</u></a>特征。也许认为思想是<a href="https://www.lesswrong.com/posts/r3xwHzMmMf25peeHE/the-translucent-thoughts-hypotheses-and-their-implications"><u>半透明的</u></a>。</li><li><i>一些名称：</i> <a href="https://www.lesswrong.com/posts/BfN88BfZQ4XGeZkda/concrete-reasons-for-hope-about-ai"><u>人类</u></a>，露天，<a href="https://scale.com/blog/safety-evaluations-analysis-lab"><u>鳞片</u></a>，<a href="https://huggingface.co/HuggingFaceH4"><u>拥抱的脸H4</u></a> ，Eleuther。但是，没有看到任何一个实际上说明上述变革理论。</li><li><i>估计的＃ftes：~~ 1,000</i></li><li> <i>2023年的一些输出</i>： <a href="https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input"><u>CCAI</u></a> 。 DPO似乎是很大的进步。</li><li><a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>批评</u></a><i>：</i> <a href="https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research#The_case_for_a_negative_impact"><u>男孩</u></a>。只是最新的： <a href="https://arxiv.org/abs/2307.15217"><u>RLHF</u></a> ， <a href="https://arxiv.org/abs/2310.16048"><u>Neo-Arrow</u></a>的开放问题。</li><li><i>资助：</i>大型技术，风险投资</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i> AI行业的大部分。<br><br></li></ul><h3>防止欺骗</h3><p>通过机械解释性以外的方法。<br></p><p> <a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood"><i><u>红木：机械异常检测</u></i></a></p><ul><li><i>一句摘要</i>：测量篡改是AI系统操纵多个测量的地方，以产生良好结果的错觉，而不是实现所需的结果。</li><li><i>变更理论：</i>找出何时发生测量篡改 - >;构建不这样做的模型。</li><li>另请参阅CAI</li><li><i>一些名字：</i> Fabien Roger，Ryan Greenblatt，Max Nadeau，Buck Shlegeris，Nate Thomas</li><li><i>估计的＃ftes：</i> 2.5？</li><li> <i>2023年的一些输出</i>：<a href="https://arxiv.org/abs/2308.15605"><u>测量篡改</u></a>， <a href="https://www.alignmentforum.org/posts/rZs6ddqNnW8LXuJqA/password-locked-models-a-stress-case-for-capabilities"><u>密码锁定</u></a>， <a href="https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"><u>政变探针</u></a></li><li><i>批评：</i> <a href="https://forum.effectivealtruism.org/posts/DaRvpDHHdaoad9Tfu/critiques-of-prominent-ai-safety-labs-redwood-research"><u>对</u></a><a href="https://www.alignmentforum.org/posts/wt7HXaCWzuKQipqz3/eis-vi-critiques-of-mechanistic-interpretability-work-in-ai"><u>过去议程的批评</u></a></li><li><i>资助者：</i> Openphil</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i> <a href="https://www.openphilanthropy.org/grants/redwood-research-general-support-2023/"><u>$ 5300,000</u></a> （整个组织，去年）<br></li></ul><p><i><u>间接欺骗监测</u></i></p><ul><li><i>一句话摘要</i>：构建工具，以查找模型是否会在可测试情况下查看高风险环境中的不良表现。该遗愿捕获了<a href="https://www.alignmentforum.org/posts/khFC2a4pLPvGtXAGG/how-to-catch-an-ai-liar-lie-detection-in-black-box-llms-by"><u>谎言分类器</u></a>，<a href="https://arxiv.org/abs/2310.13548"><u>粘糊糊</u></a>和<a href="https://www.alignmentforum.org/posts/pip63HtEAxHGfSEGk/tall-tales-at-different-scales-evaluating-scaling-trends-for"><u>欺骗趋势</u></a>的工作。</li><li><i>变革理论：也许我们可以通过观察数十个表面无关的部分或欺骗自我报告或建立相当于脑扫描的等效的模型来捕获一个未对准的模型。</i></li><li><i>一些名字：</i> Dan Hendrycks，Owain Evans，Jan Brauner，SörenMindermann。另请参见本文中的Apollo， <a href="https://www.safe.ai/"><u>CAI</u></a> ，CAIF和两个激活工程部分。</li><li><i>估计的＃ftes：20？</i></li><li> <i>2023年的一些输出</i>： <a href="https://arxiv.org/abs/2308.14752"><u>AI欺骗调查</u></a>，<a href="https://arxiv.org/abs/2303.16200"><u>自然选择</u></a>，<a href="https://arxiv.org/abs/2306.12001"><u>概述</u></a>，<a href="https://arxiv.org/pdf/2310.01405.pdf"><u>再次</u></a></li><li><i>批评（相关想法）：</i> <a href="https://www.lesswrong.com/posts/RTkatYxJWvXR4Qbyd/deceptive-alignment-is-less-than-1-likely-by-default"><i><u>1％</u></i></a></li><li><i>资助者：</i> <a href="https://www.openphilanthropy.org/grants/?organization-name=center-for-ai-safety"><i><u>Openphil</u></i></a></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i> $ 10,618,729（CAI，整个组织）<br></li></ul><p><i>人类：</i> <a href="https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for"><i><u>外部推理监督</u></i></a></p><ul><li><i>一句摘要</i>：每次用英语（或其他语言）打印其实际推理的火车模型。对危险的推理给出负面的奖励，或者只是摆脱参与其中的模型。</li><li><i>变革理论：</i> “强迫语言模型大声思考，并将推理本身作为监督渠道。如果该议程成功，它可能会打败欺骗，寻求权力和其他不赞成的推理。”</li><li>另请参见粘糊糊。</li><li><i>一些名字：</i> Tamera Lanham，Ansh Radhakrishnan</li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的某些产量</i>： <a href="https://arxiv.org/abs/2307.13702"><u>COT忠诚</u></a>，<a href="https://arxiv.org/abs/2307.11768"><u>问题分解忠诚</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/3dFogxGK8uNv5xCSv/you-won-t-solve-alignment-without-agent-foundations"><i><u>萨明</u></i></a></li><li><i>资助者：</i>拟人投资者</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i>大<br><br></li></ul><h3>手术模型编辑</h3><p>（对内部模型的干预）<br></p><p><a href="https://www.lesswrong.com/tag/activation-engineering"><i><u>激活工程</u></i></a></p><ul><li><i>一句话摘要</i>：让我们看看是否可以通过编程性修改激活，以将输出转向我们想要的东西，以跨模型和主题的通用方式。基于干预的解释性方法或更多或更多的方法（请参见上文）。</li><li><i>变革理论：</i>也许简单的事情会有所帮助：让我们构建更多的东西，以堆放在填充上。激活是输出前的最后一步，因此对它们的干预措施不那么抢占。稍微鼓励模型变得不错，在我们的<a href="https://www.lesswrong.com/posts/YnGRBADQwpYRbuCbz/towards-hodge-podge-alignment-1"><u>部分比</u></a>对方法中添加一层防御。</li><li><i>一些名字：</i> <a href="https://arxiv.org/abs/2308.10248"><u>Alex Turner</u></a> ，Andy Zou，Nina <a href="https://www.alignmentforum.org/posts/raoeNarFYCxxyKAop/modulating-sycophancy-in-an-rlhf-model-via-activation"><u>Rimsky</u></a> ，Claudia Shi，LéoDana，Ole Jorgensen。另请参见<a href="https://arxiv.org/abs/2306.03341"><u>Li</u></a>和<a href="https://baulab.info/publications.html"><u>Bau</u></a> <a href="https://www.lesswrong.com/posts/iNaaBAEkAy9nAgs3o/turning-off-lights-with-model-editing"><u>Lab</u></a> 。</li><li><i>估计的＃ftes：〜20</i></li><li> <i>2023年的一些输出</i>：去年<a href="https://arxiv.org/abs/2212.03827"><u>著名的CCS</u></a> ， <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector"><u>RL和GPTS中的转向向量</u></a>， <a href="https://www.lesswrong.com/posts/bFwigCDMC5ishLz7X/rfc-possible-ways-to-expand-on-discovering-latent-knowledge"><u>Cadenza RFC</u></a> ， <a href="https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a"><u>概念的形状</u></a>。 <a href="https://www.alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4"><u>罗杰实验</u></a>。另请参见<a href="https://arxiv.org/abs/2310.01405"><u>代表工程</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><i><u>罗马</u></i></a></li><li><i>资助者：DeepMind？人类？垫子？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><h3>让模型学习我们想要的</h3><p>（弄清楚如何控制模型的内容。）<br></p><p><a href="https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans"><i><u>模仿学习</u></i></a></p><ul><li><i>一句话摘要</i>：有关人类行为的火车模型（例如，在响应计算机屏幕上发生的情况时，人类按下键的监视）；与<a href="https://arxiv.org/abs/2110.08176"><u>Strouse</u></a>形成鲜明对比。</li><li><i>变革理论：</i>人类通过互相观察 - >;让我们测试AIS是否可以通过观察我们来学习 - >;外部对齐和月光下的良好学习。</li><li><i>一些名字：</i> JérémyScheurer，Tomek Korbak，Ethan Perez</li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的一些输出</i>： <a href="https://www.lesswrong.com/posts/mCZSXdZoNoWn5SkvE/imitation-learning-from-language-feedback-1"><u>模仿语言反馈</u></a>，<a href="https://arxiv.org/abs/2309.02473"><u>调查</u></a>，<a href="https://www.jmlr.org/papers/v23/21-0618.html"><u>良好理论从2022年学习</u></a></li><li><i>批评：</i> <a href="https://www.alignmentforum.org/posts/LTFaD96D9kWuTibWr/just-imitate-humans?commentId=kgZxwD3Wm96tNDKxu"><i><u>很多</u></i></a></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><i>奖励学习</i></p><ul><li><i>一句话摘要</i>：像柴的人仍在寻找<a href="https://arxiv.org/abs/2203.07475"><u>奖励学习</u></a>，以“重新定位AI研究的一般性对可证明有益的系统”。 （他们也像其他所有人一样在做很多倡导。）</li><li><i>变革理论：当人类直接参与训练模型 - >;构建工具，使模型更容易学习人类想要学习的东西时，了解什么样的事情会出错。</i></li><li>另请参见RLHF和递归奖励建模，即工业化形式。</li><li><i>一些名字：</i><a href="https://humancompatible.ai/progress-report/"><u>柴</u></a>等</li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的某些产量</i>：<a href="https://arxiv.org/abs/2303.00894"><u>多个老师</u></a>，<a href="https://arxiv.org/pdf/2306.09309.pdf"><u>最少的知识</u></a><a href="https://humancompatible.ai/progress-report/"><u>等</u></a>，<a href="https://arxiv.org/abs/2204.06601"><u>因果混乱</u></a></li><li><i>评论：</i><i>历史问题陈述的</i><a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument"><i><u>不错摘要</u></i></a></li><li><i>资助：</i><a href="https://donations.vipulnaik.com/donee.php?donee=Center+for+Human-Compatible+AI"><i><u>主要是Openphil</u></i></a></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 18,052,796</i><br></li></ul><h3>目标稳健性</h3><p>（弄清楚如何使模型继续做〜到目前为止它一直在做什么。）<br></p><p><a href="https://towardsdatascience.com/out-of-distribution-generalization-66b6f8980ef3"><i><u>测量OOD</u></i></a></p><ul><li><i>一句话摘要</i>：让我们构建可以识别出何时无法分发的模型（或至少给我们给我们的工具何时注意）。另请参见异常检测。</li><li><i>变革理论：</i>如果强大的AI从培训数据中“学习错误的教训”，就会发生坏事，我们应该做到这一点。</li><li><i>一些名字：</i> Steinhardt，Tegan Maharaj， <a href="https://arxiv.org/abs/2210.03150"><u>Irina Rish</u></a></li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的某些输出</i>：<a href="https://arxiv.org/abs/2209.00626"><u>从DL的角度来对齐</u></a>，<a href="https://arxiv.org/abs/2210.01790"><u>目标失调</u></a>， <a href="https://arxiv.org/abs/2309.16166"><u>coinrun：解决目标错误</u></a>，<a href="https://arxiv.org/abs/2304.14399"><u>对歧义进行建模</u></a>？</li><li><i>批评：？</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><a href="https://arxiv.org/abs/2306.10999"><i><u>概念外推</u></i></a><i>&nbsp;</i></p><ul><li><i>一句话摘要</i>：持续学习，使模型内部的内部设备在学习 /随着世界的变化而更加稳定；安全地扩展了代理商在培训新数据集和环境中学习的功能。</li><li><i>变革理论：</i>让他们大致正确地概括我们的价值观。另外，“让我们成为AI系统“在面对歧义时变得保守并寻求指导”的行业标准，并在我们发现更多的一致性方面逐渐从那里提高标准。” -  Bensinger的光泽。</li><li><i>一些名字：</i>斯图尔特·阿姆斯特朗</li><li><i>估计的＃ftes：</i> 4？</li><li> <i>2023年的一些输出</i>：<a href="https://arxiv.org/abs/2306.10999"><u>良好的底漆</u></a>，<a href="https://arxiv.org/abs/2309.16166"><u>解决了一个玩具问题</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment#Stuart_Armstrong___Concept_Extrapolation"><i><u>soares</u></i></a></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><a href="https://www.lesswrong.com/tag/mild-optimization"><i><u>轻度优化</u></i></a></p><ul><li><i>一句话摘要</i>：通过让AI满足而不是最大化来避免使用Goodhart。</li><li><i>变革理论：</i>如果我们无法准确地确定对超级智能代理的偏好，我们会死于良好的效果 - >;从代理的效用函数中的最大化转变为满足的转变 - >;我们获得了LightCone的非零份额，而不是零；另外，这是完全对齐AI的食谱。</li><li><i>一些名称：</i></li><li><i>估计的＃ftes：</i> 2？</li><li> <i>2023年的一些产量</i>： <a href="https://www.lesswrong.com/posts/9fL22eBJMtyCLvL7j/soft-optimization-makes-the-value-target-bigger"><u>吉伦</u></a>， <a href="https://www.lesswrong.com/posts/XXrGhqSNZjcG2nNiy/aisc-team-report-soft-optimization-bayes-and-goodhart"><u>软优化，贝叶斯和古德哈特</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/q9yPYG2St2L4SEtKW/requirements-for-a-stem-capable-agi-value-learner-my-case-1"><i><u>Dearnaley</u></i></a> <i>？</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i></li></ul><hr><h2> 3.<a href="https://www.lesswrong.com/tag/ai-assisted-ai-automated-alignment"><u>使AI解决</u></a></h2><p>（弄清楚模型如何有助于弄清楚它。）<br></p><p> <i>Openai：</i><a href="https://openai.com/blog/introducing-superalignment"><i><u>超级对象</u></i></a><i>&nbsp;</i></p><ul><li><i>一句话摘要</i>：准备好对齐人级自动对准研究人员。</li><li><i>变革理论：让它帮助我们进行可扩展的监督，批评，递归奖励建模，从而解决了内部的一致性。</i> <a href="https://www.lesswrong.com/posts/fYf9JAwa6BYMt8GBj/link-a-minimal-viable-product-for-alignment?commentId=uv8pteZJSzJeiqFA8"><i><u>另请参阅</u></i></a><i>种子。</i></li><li><i>一些名字：</i> Ilya Sutskever，Jan Leike，Leopold Aschenbrenner，Collin Burns</li><li><i>估计＃ftes：30？</i></li><li> <i>2023年的一些输出</i>： <a href="https://openai.com/research?topics=safety-alignment"><u>https：//openai.com/research?topics=</u></a> safety-Alignment</li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/NSZhadmoYdjRKNq6X/openai-launches-superalignment-taskforce"><i><u>Zvi</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=NsYXBdLY6edAXavsM"><i><u>Christiano</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"><i><u>Miri</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/pxiaLFjyr4WPmFdcm/take-2-building-tools-to-help-build-fai-is-a-legitimate"><i><u>Steiner</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/6RC3BNopCtzKaTeR6/thoughts-on-the-openai-alignment-plan-will-ai-research"><i><u>Ladish</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"><i><u>Wentworth</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/YiRsCfkJ2ERGpRpen/leogao-s-shortform?commentId=qmfPJspzsXaR7xCYj"><i><u>Gao</u></i></a> <i>LOL</i></li><li><i>资助者：Microsoft</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i><i>仅计算</i><a href="https://www.bloomberg.com/news/articles/2023-03-13/microsoft-built-an-expensive-supercomputer-to-power-openai-s-chatgpt?sref=ExbtjcSG"><i><u>1亿美元</u></i></a>（占Openai有担保计算的20％）<br></li></ul><p> <a href="https://www.lesswrong.com/posts/7e5tyFnpzGCdfT4mR/research-agenda-supervising-ais-improving-ais"><i><u>监督AIS改善AIS</u></i></a></p><ul><li><i>一句话摘要</i>：在语言模型和基准中跟踪行为漂移的可扩展方法，以评估语言模型通过自我训练的稳定自我修饰的能力。</li><li><i>变化理论：</i>早期模型仅训练〜仅对人类数据进行训练，而后来的模型还训练了早期模型输出，这导致了早期模型问题的问题；剩下的未选中可能会引起问题，因此我们需要更好的迭代改进过程。</li><li><i>一些名字：</i> Quintin Pope，Owen Dudney，Roman Engeler，Jacques Thibodeau</li><li><i>估计＃ftes：</i> 2</li><li> <i>2023年的一些输出</i>：？</li><li><i>批评：</i></li><li><i>资助者：微小的Lightspeed Grant</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：〜$ 10,000（LSG） +</i><br></li></ul><p><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><i><u>机器人</u></i></a></p><ul><li><i>一句话摘要</i>：训练人与llm对准研究人员：与人类在循环中，而没有外包给自主代理。不仅如此，对基于AI的AI一致性的风险评估的积极态度。</li><li><i>变革理论：增强人类能力并保留价值观的认知假肢。每年进行更多的一致性研究和美元。</i></li><li><i>一些名字：</i> Janus，Kees Dupuis，Jacques Thibodeau。另请参阅<a href="https://www.lesswrong.com/posts/k93NEoXZq6CdXegdx/philosophical-cyborg-part-1"><u>该</u></a>团队做类似的事情。</li><li><i>估计的＃ftes：6？</i></li><li> <i>2023年的一些输出</i>：<a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><u>议程语句</u></a></li><li><i>批评：</i><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism#Failure_Modes"><i><u>自我</u></i></a></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i></li></ul><p></p><p><i>另请参见</i><a href="https://www.lesswrong.com/posts/WKGZBCYAbZ6WGsKHc/love-in-a-simbox-is-all-you-need"><u>Simboxing</u></a> （Jacob Cannell）。<br></p><h3>可扩展的监督</h3><p>（弄清楚如何缓解人类监督模型。难以清晰地区分雄心勃勃的机械性解释性，但我们在这里。）<br></p><h3>任务分解</h3><p>递归奖励建模据说还没有死，而是超级对准的工具之一。</p><p>另一条线试图使<a href="https://arxiv.org/abs/2305.04388"><u>思想链</u></a>/<a href="https://arxiv.org/abs/2305.10601"><u>思想树</u></a>诚实。<br></p><p><a href="https://blog.elicit.com/"><i><u>引起</u></i></a>（以前<a href="https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"><u>应该</u></a>）</p><ul><li>某种形式的<a href="https://ought.org/updates/2023-09-25-spinoff"><u>枢轴/分拆</u></a>发生了。 “大多数以前的员工都在新组织工作”，详细信息尚不清楚。</li><li><i>一句摘要</i>：“ a）改善了AI治理和一致性研究人员的推理，特别是在长途任务上，以及（b）推动对过程的监督而不是结果，这减少了对不完善的代理目标的优化压力”。</li><li><i>变革理论：“</i><a href="https://blog.elicit.com/ai-safety/"><i><u>引起对人工智能安全</u></i></a>的两个主要影响<i>正在改善认识论和开创性的过程监督。”</i></li><li><i>一些名字：查理·乔治（Charlie George），安德烈亚斯（AndreasStuhlmüller）</i></li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的一些输出</i>： <a href="https://blog.elicit.com/factored-verification-detecting-and-reducing-hallucinations-in-frontier-models-using-ai-supervision/"><u>验证</u></a></li><li><i>批评：</i></li><li><i>资助者：</i><a href="https://blog.elicit.com/elicit-raises-9-million-and-becomes-a-public-benefit-corporation/"><i><u>公共福利公司</u></i></a></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 9,000,000</i><br></li></ul><h3>对抗性的</h3><p><a href="https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment"><i><u>深态可扩展对齐</u></i></a></p><ul><li><i>一句话摘要</i>：“即使人类很难知道那是什么，让高能力的代理人做人类想要的事情”。</li><li><i>变革理论：[“给予人类帮助监督强大的代理人”] + [“与代理的真实推理过程的解释”] + [“红色团队模型以表现出在正常使用中不出现的故障模式”]是必要的，但可能不足以安全AGI。</i></li><li><i>一些名字：杰弗里·欧文（Geoffrey Irving）</i></li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的一些输出：？</i></li><li><i>批评：</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p> <a href="https://www.anthropic.com/index/measuring-progress-on-scalable-oversight-for-large-language-models"><i><u>人类</u></i></a><i>/</i><a href="https://wp.nyu.edu/arg/"><i><u>纽约大学一致性研究小组</u></i></a><i>/ Perez合作</i></p><ul><li><i>一句话摘要</i>：可扩展的对真实性的监督：即使人类无法直接判断模型产出的正确性，也可以开发培训方法来激励真实性吗？ /可扩展的基准测量如何测量（代理）诸如情境意识之类的投机能力。</li><li><i>变革理论：当前AI越来越艰难的问题 - >;我们需要构建工具来帮助人类监督者继续转向AI  - >;让我们开发有关方法可能扩展的理论 - >;让我们构建工具。</i></li><li><i>一些名字：塞缪尔·鲍曼（Samuel Bowman），伊桑·佩雷斯（Ethan Perez）</i></li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的某些输出</i>： <a href="https://www.anthropic.com/index/specific-versus-general-principles-for-constitutional-ai"><u>宪法AI的特定与一般原则</u></a>，<a href="https://arxiv.org/abs/2311.08702"><u>辩论有助于监督不可靠的专家</u></a>，<a href="https://arxiv.org/abs/2305.04388"><u>语言模型并不总是说出他们的想法</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><i><u>混淆</u></i></a><i>，</i> <a href="https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><i><u>本地不足</u></i></a><i>？</i><a href="https://arxiv.org/abs/2210.10860"><i><u>它现在不起作用</u></i></a><i>（2022）</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i></li></ul><p></p><p>另请参见（下<a href="https://far.ai/"><u>图</u></a>）。</p><hr><h2> 4.理论</h2><p>（弄清楚我们需要找出什么，然后做到这一点。这曾经是我们所能做的。）<br></p><h3>银河脑的端到端解决方案<br></h3><p><a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><i><u>学习理论议程</u></i></a><i>&nbsp;</i></p><ul><li><i>单句摘要</i>：尝试正式化一个更现实的代理，了解与我们保持一致的含义，在其本体论与我们的本体论之间翻译，并生产Desiderata进行培训设置，以指向连贯的AGIS，类似于我们的模型对齐代理。</li><li><i>变革理论：弄清楚如何通过首先修复正式认识论来培训对齐的AI。</i></li><li><i>一些名字：</i>凡妮莎·科索（Vanessa Kosoy）</li><li><i>估计的＃ftes：3-6</i></li><li> <i>2023年的一些输出</i>： <a href="https://www.lesswrong.com/posts/cYJqGWuBwymLdFpLT/non-unitary-quantum-logic-seri-mats-research-sprint"><u>量子？</u></a> ，<a href="https://www.lesswrong.com/posts/HNnRCPe2CejfupSow/fixed-points-in-mortal-population-games"><u>凡人流行音乐</u></a>，<a href="https://www.lesswrong.com/posts/nEFAno6PsCKnNgkd5/infra-bayesian-logic"><u>逻辑</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism"><i><u>matolcsi</u></i></a></li><li><i>资助者：Miri，垫子</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p> <a href="https://www.lesswrong.com/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai"><i><u>开放代理体系结构</u></i></a></p><ul><li><i>一句话摘要</i>：让AI构建人类理解的详细世界模拟，引起人类对未来状态的偏好，正式验证AI是否遵守粗糙的偏好；计划使用这种世界模型和偏好。另请参见<a href="https://arxiv.org/abs/2309.01933"><u>可证明的安全系统</u></a>（我希望与之合并）。</li><li><i>变革理论：</i>本体论规范，身体状况的前所未有的形式化，对高维状态序列的前所未有的形式验证。斯图尔特·罗素（Stuart Russell）的报仇。值得注意的是不需要我们解决麋鹿。确实要求我们解决本体论。</li><li><i>一些名字：</i>戴维德，埃文·米亚佐诺，丹尼尔·温德姆。另请参阅： <a href="https://www.lesswrong.com/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai?commentId=p3QSNw6AMc6MtC4QF"><u>Cannell</u></a> <u>。</u></li><li><i>估计的＃ftes：5？</i></li><li> <i>2023年的某些输出</i>： <a href="https://www.lesswrong.com/posts/pHJtLHcWvfGbsW7LR/roadmap-for-a-collaborative-prototype-of-an-open-agency"><u>几个</u></a><a href="https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation"><u>团队</u></a><a href="https://atlascomputing.org/"><u>正在</u></a>研究详细信息</li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai?commentId=ZuWsoXApJqD4PwfXr"><u>soares</u></a></li><li><i>资助：</i>彼得·埃克利（Peter Eckersley） /<a href="https://twitter.com/davidad/status/1719770184565530890"><u>阿特拉斯（</u></a> Atlas Computing</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i>大约50,000,000英镑<br><br></li></ul><p><a href="https://arxiv.org/abs/2309.01933"><i><u>可证明的安全系统</u></i></a></p><ul><li><i>单句摘要</i>：正式对物理/社会系统的行为进行建模，定义确切的“护栏”，以限制可能发生的操作，要求AIS为其建议的行动提供安全证明，并自动验证这些证明。与OAA密切相关。</li><li><i>变革理论：建立一个正式的验证系统，可以充当人类用户与潜在危险系统之间的中介，只能使可证明的安全行动通过。</i></li><li><i>一些名称：</i>史蒂夫·奥蒙德罗（Steve Omohundro），Max Tegmark</li><li><i>估计的＃ftes：</i> 1 ??</li><li> <i>2023年的一些产出</i>：计划公告。 <a href="https://www.google.com/search?q=beneficial+ai+research+omohundro&amp;client=ubuntu-chr&amp;hs=YtH&amp;sca_esv=585404198&amp;sxsrf=AM9HkKnEEdAHVYIgzqf7eSVTTtoNxsq0vw%3A1700999001104&amp;ei=WS9jZar5BdOQhbIPiJCEuAE&amp;ved=0ahUKEwjqieWJy-GCAxVTSEEAHQgIARcQ4dUDCBA&amp;uact=5&amp;oq=beneficial+ai+research+omohundro&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiIGJlbmVmaWNpYWwgYWkgcmVzZWFyY2ggb21vaHVuZHJvSO0gUIYCWKAfcAJ4AZABAJgBfKAB2giqAQM1Lja4AQPIAQD4AQHCAgoQABhHGNYEGLADwgIGEAAYFhgewgILEAAYgAQYigUYhgPCAgUQIRigAcICBBAhGBXCAggQIRgWGB4YHcICBxAhGKABGAriAwQYACBBiAYBkAYI&amp;sclient=gws-wiz-serp#ip=1"><u>Omohundro的组织</u></a>非常神秘。</li><li><i>批评：</i> <a href="https://thezvi.substack.com/p/ai-28-watching-and-waiting?utm_source=%2Fsearch%2Fomohundro&amp;utm_medium=reader2#:~:text=Max%20Tegmark%20and%20Steve%20Omohundo%20drop%20a%20new%20paper"><i><u>ZVI</u></i></a></li><li><i>资助：未知</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p><i>猜想：</i> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal"><i><u>认知仿真</u></i></a><i>（COEMS）</i></p><ul><li><i>一句摘要</i>：将设计空间限制在人类推理的（部分）仿真中。如果AI对我们使用类似的启发式方法，则应默认为不极端。</li><li><i>变革理论：训练有限的工具AI，这将帮助我们反对AGI而不会非常危险，并使禁止无限的AIS在政治上更可行。</i></li><li><i>一些名字：Connor Leahy，Gabriel Alfour？</i></li><li><i>估计的＃ftes：</i> 11？</li><li> <i>2023年的一些输出</i>：？</li><li><i>评论：</i> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal?commentId=vvurB4rZFEPoHwnpz"><i><u>Scher</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/ngEvKav9w57XrGQnb/cognitive-emulation-a-naive-ai-safety-proposal?commentId=sEknDwrJ4WdxMz66c"><i><u>Samin</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/9jvrQToSq3CYvoeHf/critiques-of-prominent-ai-safety-labs-conjecture"><i><u>Org</u></i></a></li><li><i>资助：私人投资者（复数平台，Metaplanet，Secret）</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i>数百万美元。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/4RrLiboiGGKfsanMF/the-qaci-alignment-plan-table-of-contents"><i><u>提问 - 反事实间隔（QACI）</u></i></a></p><ul><li><i>一句话摘要</i>：让事情能够制定自己的目标功能（a la hch）。</li><li><i>变革理论：</i> “应对的目标应由完全正式的数学数，而不是AI在本体论中必须解释的人类概念，因为本体论随着AI的学习和变化而破裂并重塑。 [..]给出一个计算无限的数学甲骨文，该目标将采取理想的动作；然后，我们应该设计一个计算有限的AI，足以采取令人满意的动作。”</li><li><i>一些名字</i>：Tamsin Leake</li><li><i>估计的＃ftes：3？</i></li><li> <i>2023年的一些输出</i>：参见<a href="https://www.lesswrong.com/posts/4RrLiboiGGKfsanMF/the-qaci-alignment-plan-table-of-contents"><u>议程帖子</u></a></li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/CYtzXadXFtBSBYm3J/a-narrative-explanation-of-the-qaci-alignment-plan?commentId=uFofk2t7XEsqxGsTZ"><u>霍布森</u></a>， <a href="https://www.lesswrong.com/posts/CYtzXadXFtBSBYm3J/a-narrative-explanation-of-the-qaci-alignment-plan?commentId=8wqGNfotgeTp4G9gu"><u>阿诺姆</u></a></li><li><i>资助者：SFF</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：</i> $ 438,000+<br></li></ul><h3>了解代理</h3><p>（弄清楚“甚至是代理人”以及如何与因果关系联系起来。）<br></p><p><a href="https://causalincentives.com/"><i><u>因果基础</u></i></a><i>&nbsp;</i></p><ul><li><i>一句</i><a href="https://www.alignmentforum.org/s/pcdHisDEGLbxrbSHD"><u>介绍</u></a>：使用因果模型来理解代理，因此设计环境没有动力进行叛逃。</li><li><i>变化理论</i>：<a href="https://arxiv.org/pdf/2204.10018.pdf"><u>特定于路径的目标</u></a>避免了对价值规范的严格要求，而瓶颈则是确保稳定性（如何容易发生副作用状态的副作用）。</li><li><i>一些名字</i>：汤姆·埃弗里特（Tom Everitt），刘易斯·哈蒙德（Lewis Hammond），弗朗西斯·里斯·沃德（Francis Rhys Ward），瑞安·凯里（Ryan Carey），塞巴斯蒂安·法夸（Sebastian Farquhar）</li><li><i>估计的＃ftes</i> ：？</li><li> <i>2023年的一些输出</i>：<a href="https://causalincentives.com/pdfs/deception-ward-2023.pdf"><u>欺骗纸</u></a></li><li><i>批评</i>：</li><li><i>资助</i>：？</li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态</i>：？</li><li><i>资源</i>：少量的深态<br></li></ul><p><a href="https://acsresearch.org/"><i><u>复杂系统的一致性</u></i></a><i>：分层代理</i></p><ul><li><i>一句摘要</i>：开发子代理和超级代理的形式模型，使用该模型指定整个部分关系的理想属性（例如如何防止对人类友好的零件被消灭）。目前使用主动推论作为形式主义的灵感。研究人类和社会的偏好和认知；进行主动推理的游戏理论扩展。</li><li><i>变革理论：解决</i><a href="https://www.alignmentforum.org/posts/9GyniEBaN3YYTqZXn/the-self-unalignment-problem"><i><u>自我对象</u></i></a><i>，防止crocrustean对准，允许可扩展的非掠夺。</i></li><li><i>一些名字：</i> Jan Kulveit，TomášGavenčiak</li><li><i>估计＃ftes：4</i></li><li> <i>2023年的一些输出</i>：对LLM的<a href="https://acsresearch.org/posts"><u>见解</u></a>，深入<a href="https://arxiv.org/abs/2311.10215"><u>研究主动</u></a>推断。</li><li><i>批评：</i> <a href="https://www.alignmentforum.org/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group?commentId=frEufx3c6cRmhDjbh"><i><u>间接</u></i></a></li><li><i>资助者：SFF（</i> <a href="https://survivalandflourishing.fund/sff-2022-h1-recommendations.html"><i><u>2022</u></i></a> <i>）</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 425,000？</i><br></li></ul><p> <a href="https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience"><i><u>罗宁尖锐的左转船员</u></i></a></p><ul><li><i>一句话摘要</i>：&#39;最初是因为“表征锋利的左转弯”，并演变为获得有关理想形式的<a href="https://arbital.com/p/consequentialist"><u>后果主义认知</u></a>形式的基本见解。</li><li><i>变革理论：</i>了解后果主义者的一般特性 - >;>;找出哪些子问题实际上可能会有所帮助 - >;正式化相关见解 - >;死于AI的方法很少。</li><li><i>一些名字：</i> （KWA，Barnett，Hebbar）过去</li><li><i>估计的＃ftes：？</i></li><li> <i>2023年的一些产量</i>：<a href="https://www.lesswrong.com/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic"><u>灾难性的Goodhart</u></a> ？</li><li><i>评论</i><i>：</i> <a href="https://www.lesswrong.com/posts/v6zZaR7aDD6vkuPmx/science-of-deep-learning-more-tractably-addresses-the-sharp"><i><u>GABS</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment#Vivek_Hebbar__summarized__perhaps_poorly__from_last_time_we_spoke_of_this_in_person"><i><u>SOARES</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn"><i><u>POPE</u></i></a><i>等</i><a href="https://www.lesswrong.com/tag/sharp-left-turn"><i><u>，</u></i></a> <a href="https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems"><i><u>切向EJT</u></i></a></li><li><i>资助者：Lightspeed</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：269,200美元（Hebbar）</i><br></li></ul><p><a href="https://www.lesswrong.com/tag/shard-theory"><i><u>碎片理论</u></i></a></p><ul><li><i>一句摘要</i>：模型代理的内部组成部分，将人类用作AGI的模型生物（人类似乎是由碎片组成的，AI也是如此）。</li><li><i>变革理论：“如果政策是受影响力集合（“碎片”）控制的，请考虑哪种训练方法会增加对人类友好型碎片的机会，从而实质上影响了整体。”</i></li><li>另请参阅激活工程。</li><li><i>一些名称：Quintin Pope，Alex Turner</i></li><li><i>估计＃ftes：4</i></li><li> <i>2023年的某些输出</i>：在控制 /介入的解释性中<a href="https://www.lesswrong.com/posts/JusJcepE2qohiC3hm/predictions-for-shard-theory-mechanistic-interpretability"><u>真正坚实的</u></a>经验内容</li><li><i>评论：</i> <a href="https://www.lesswrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical#My_opinions_on_the_validity_of_each_of_the_nine_theses"><i><u>陈</u></i></a><i>，</i> <a href="https://www.lesswrong.com/posts/Aet2mbnK7GDDfrEQu/contra-shard-theory-in-the-context-of-the-diamond-maximizer"><i><u>Soares</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/dRsrfC8LN4z2oehJg/the-heritability-of-human-values-a-behavior-genetic-critique"><i><u>Miller</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/L4e7CqqpDxea2x4Gg/disentangling-shard-theory-into-atomic-claims"><i><u>Lang</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/xsieF8SXw4J5LkzEg/failure-modes-in-a-shard-theory-alignment-plan"><i><u>KWA</u></i></a></li><li><i>资助者：没有人特别</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p> <a href="https://www.lesswrong.com/posts/fjgoMaBenyXcRDrbX/boundaries-membranes-and-ai-safety-compilation"><i><u>边界 /膜</u></i></a></p><ul><li><i>一句话摘要</i>：正式化一条道德：代理人与其环境之间的因果分离。另请参见开放代理体系结构。</li><li><i>变革理论：</i> <a href="https://www.lesswrong.com/posts/KX3xx8LTnE7GKoFuj/boundaries-for-formalizing-an-mvp-morality"><i><u>正式化道德/安全性</u></i></a><i>，解决外部对齐。</i></li><li><i>一些名字：</i><a href="mailto:chris@chrislakin.com"><i><u>克里斯·拉金（Chris Lakin</u></i></a> <i>）（全职），</i><a href="https://www.lesswrong.com/s/LWJsgNYE8wzv49yEc"><i><u>安德鲁·克里奇（Andrew Critch）</u></i></a> <i>，</i> <a href="https://www.lesswrong.com/posts/fjgoMaBenyXcRDrbX/boundaries-membranes-and-ai-safety-compilation#Davidad_s_OAA"><i><u>戴维德（David）</u></i></a></li><li><i>估计＃ftes：1</i></li><li> <i>2023年的某些产出</i>：问题陈述，计划研讨会2024年初</li><li><i>批评：</i></li><li><i>资助者：私人捐助者和前瞻性研究所</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：&lt;$ 100k</i><br></li></ul><p><i>剥夺</i><a href="https://manifund.org/projects/agency-and-disempowerment"><i><u>形式主义</u></i></a></p><ul><li><i>一句话摘要</i>：提供（DIS）赋权的正式和运营概念，这些概念在概念上令人满意且可实施。</li><li><i>变革理论：形式主义将来将有用。</i></li><li><i>一些名字：Damiano Fornasiere，Pietro Greiner</i></li><li><i>估计＃ftes：2</i></li><li> <i>2023年的一些输出</i>：？</li><li><i>批评：</i></li><li><i>资助者：仿真</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 60,300</i><br></li></ul><p><a href="https://manifund.org/projects/avoiding-incentives-for-performative-prediction-in-ai"><i><u>表演预测</u></i></a></p><ul><li><i>一句话摘要</i>：“如何通过对多个预测指标的联合评估来消除表演预测的激励措施”。</li><li><i>变革理论：“如果功能强大的AI系统发展的目标是偶然或设计，那么这种操纵的激励措施可能会证明灾难性的”  - >;注意事件发生时 - >;设计模型不执行此操作。</i></li><li><i>一些名字：杰里米·鲁宾夫（Jeremy Rubinoff）</i></li><li><i>估计＃ftes：1</i></li><li> <i>2023年的某些输出</i>： <a href="https://www.alignmentforum.org/posts/A48amesEmqD8KNSmY/conditional-prediction-with-zero-sum-training-solves-self"><u>零和训练的条件预测解决了自我实现的预言</u></a>（前体）</li><li><i>批评：？</i></li><li><i>资助者：仿真</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：$ 33,200</i><br></li></ul><p><i>理解优化</i></p><ul><li><i>一句话摘要</i>：什么是“优化能力”（正式化），我们如何构建跟踪它的工具以及无论如何与此相关。另请参阅发展性解释性？</li><li><i>变革理论：现有理论要么严格或善于捕捉我们的意思；让我们找到一个 - >;使用该概念来更好地理解AI如何以及何时获得更多优化能力。如果我们可以检测或排除诸如渐变黑客攻击之类的投机物品，那就太好了。</i></li><li><i>一些名字：</i><a href="https://www.lesswrong.com/posts/7nDvJiikgiawHAp6z/my-research-agenda-in-agent-foundations"><i><u>Alex Altair</u></i></a> <i>，Jacob Hilton</i></li><li><i>估计的＃ftes：？</i></li><li> <a href="https://www.lesswrong.com/posts/ouXqWFxHZGsC3B8D7/draft-inferring-minimizers"><u>2023</u></a>年<a href="https://www.lesswrong.com/posts/h3Nqjy75xoqJ3Tvup/draft-the-optimization-toolbox"><u>的</u></a><i>某些输出</i><a href="https://www.lesswrong.com/posts/8CSJvfcvDGioNQF87/draft-detecting-optimization"><u>：</u></a> Altair草稿（ <a href="https://www.lesswrong.com/posts/csiAvRMGG5aAWvKWb/draft-introduction-to-optimization"><u>1、2、3、4</u></a> ）， <a href="https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation"><u>一点点观察可以解锁多少位优化？</u></a> （温特沃斯）， <a href="https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but"><u>但是要花多少钱？</u></a> ，<a href="https://www.alignmentforum.org/posts/X6ZjFShxNBNM5QCg4/towards-measures-of-optimisation-3"><u>采取优化度量</u></a>（MacDermott，Oldenziel）； Goodharting（“我们甚至如何让代理商做特定的事情”）： <a href="https://www.alignmentforum.org/posts/Eu6CvP7c7ivcGM3PJ/goodhart-s-law-in-reinforcement-learning"><u>RL东西</u></a>， <a href="https://arxiv.org/abs/2209.13085"><u>Krueger</u></a> ， <a href="https://arxiv.org/abs/2210.10760"><u>Overopt</u></a></li><li><i>批评：</i></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br><br></li></ul><h3>科罗</h3><p>（弄清楚我们如何获得超级智能的代理人继续倾听我们的声音。可以说是可扩展的监督和超级对象。</p><p><br><a href="https://www.michael-k-cohen.com/publications"><i><u>行为对准理论</u></i></a></p><ul><li><i>一句摘要</i>：通过正式模型预测AGI（例如PowerSeeking）的性质。科罗作为权力的相反。</li><li><i>变革理论：</i>弄清有关属性的假设强大的代理人将 - >;尝试在假设所持的条件下进行严格证明，在可行的情况下对其进行测试。</li><li> <a href="https://www.lesswrong.com/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag"><u>该线程</u></a>中的评论非常好 - 但是没有一个作者正在为此致力！另请参阅<a href="https://arxiv.org/abs/1908.01695"><u>霍尔特曼的忽视结果</u></a>。</li><li><i>一些名字：</i> <a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>Marcus</u></a> Hutter，Michael Cohen（ <a href="https://arxiv.org/abs/2006.08753"><u>1，2</u></a> ），Michael Osborne</li><li><i>估计的＃ftes：</i> 3</li><li> <i>2023年的一些输出</i>：？</li><li><i>批评：</i></li><li><i>资助者：DeepMind</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p>另请参见<a href="https://philpapers.org/rec/THOTSP-7"><u>EJT</u></a> （以及以前的<a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1#comments"><u>彼得森</u></a>）。另请参阅<a href="https://www.lesswrong.com/posts/gLyRQCg6kp5cqTQTm/collective-identity"><u>Dupuis</u></a> 。</p><p></p><h3>本体识别</h3><p>（弄清楚超智能代理商如何思考世界，以及我们如何获得超级智能代理人实际告诉我们他们所知道的东西。大部分可解释性偶然地针对这一点。）<br></p><p><a href="https://www.alignment.org/theory/"><i><u>弧理论</u></i></a></p><ul><li><i>一句话摘要</i>：训练我们可以提取潜在，看起来和加密知识的AI，即使它有动力将其隐藏起来。麋鹿，正式化启发式方法，<a href="https://www.alignment.org/blog/mechanistic-anomaly-detection-and-elk/"><u>机械异常检测</u></a></li><li><i>变革理论</i>：正式化模型的概念，该模型访问了一些信息 - >;设计培训目标，这些目标激励系统诚实地报告其内部信念</li><li><i>一些名字：</i>保罗·克里斯蒂安诺（Paul Christiano），马克Xu。</li><li> <i>2023年的一些产出</i>：没有公开的东西； ``我们正在努力为“正式启发式论证”开发一个框架，该框架可用于推理神经网络的行为。&#39;</li><li>评论： <a href="https://www.lesswrong.com/posts/8xCtJHAbzyA2oA6J4/clarifying-what-elk-is-trying-to-achieve"><u>澄清</u></a>， <a href="https://www.lesswrong.com/posts/NxApPkbjt9hXraSts/for-elk-truth-is-mostly-a-distraction"><u>替代</u></a>配方</li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><p> <a href="https://www.alignmentforum.org/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro"><i><u>自然抽象</u></i></a><i>&nbsp;</i></p><ul><li><i>单句摘要</i>：检查我们的宇宙很好地抽象的假设，许多认知系统学会使用类似的抽象。</li><li><i>变革理论：</i>构建工具以检查假设，运行实验，如果假设的存在，我们不必担心对齐的挑剔部分，例如AGI是否会知道我们的意思是爱。</li><li><i>一些名字：</i>约翰·温特沃斯</li><li><i>估计的＃ftes：</i> 2？</li><li> <i>2023年的一些输出</i>： <a href="https://www.alignmentforum.org/tag/natural-abstraction?sortedBy=new"><u>https</u></a> ：//www.alignmentforum.org/tag/natural-abstraction？</li><li><i>批评：</i> <a href="https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1"><i><u>摘要和批评</u></i></a><i>，</i> <a href="https://www.alignmentforum.org/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s"><i><u>飙升</u></i></a></li><li><i>资助：？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源： ？</i><br></li></ul><h3>了解合作</h3><p>（弄清楚Inter-AI和AI/人类游戏理论应该或将如何工作。）<br></p><p> <a href="https://longtermrisk.org/research-agenda"><i><u>CLR</u></i></a> <i>&nbsp;</i></p><ul><li><i>一句话摘要</i>：未来的代理人故意创造S风险是所有可能的问题中最糟糕的，我们应该避免这种情况。</li><li><i>变革理论：通过改善合作理论使现在和未来的AIS固有地合作。</i></li><li><i>一些名字：</i>杰西·克利夫顿，安妮·莱斯克勒，朱利安·斯塔斯特尼</li><li><i>估计＃ftes：15</i></li><li> <i>2023年的某些输出</i>：<a href="https://www.alignmentforum.org/posts/uPWDwFJnxLaDiyv4M/open-minded-updatelessness"><u>开放的无聊更新性</u></a>，<a href="https://arxiv.org/pdf/2307.04879.pdf"><u>可能是个</u></a><a href="https://www.lesswrong.com/posts/92xKPvTHDhoAiRBv9/making-ais-less-likely-to-be-spiteful"><u>恶意</u></a></li><li><i>批评：</i></li><li><i>资助者：Ruairi Donnelly？ Polaris Ventures？</i></li><li><i>值得信赖的命令，关闭，OPSEC，共同的好处，一致性心态：？</i></li><li><i>资源：去年3,375,081英镑</i><br></li></ul><p><a href="https://www.cs.cmu.edu/~focal/"><i><u>焦点</u></i></a><i>&nbsp;</i></p><ul><li><i>一句话摘要</i>：确保Advanced AI使用我们认为的适当游戏理论。</li><li><i>变革理论：（1）通过使AIS更加合作来保持前期智慧的世界理解； （2）保持在学术界，与学者在各种主题上的合作，并鼓励他们在X-fisk上的合作； (3) hope our work on “game theory for AIs”, which emphasises cooperation and benefit to humans, has framing &amp; founder effects on the new academic field.</i></li><li> <i>Some names:</i> Vincent Conitzer, Caspar Oesterheld</li><li> <i>Estimated # FTEs: 7</i></li><li> <i>Some outputs in 2023</i> : <a href="https://arxiv.org/abs/2307.05068"><u>Bounded Inductive Rationality</u></a> , <a href="https://arxiv.org/abs/2305.17805"><u>Computational Complexity of Single-Player Imperfect-Recall Games</u></a> , <a href="https://arxiv.org/abs/2305.11261"><u>Game Theory with Simulation of Other Players</u></a></li><li> <i>Critiques:</i> Self-submitted “our theory of change is not clearly relevant to superintelligent AI”</li><li> <i>Funded by:</i> <a href="https://polaris-ventures.org/grants/"><i><u>Polaris Ventures</u></i></a></li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> >;$500,000</li></ul><p></p><p> We moved <a href="https://www.cooperativeai.com/foundation"><u>CAIF</u></a> to the “Research support” appendix. We moved <a href="https://ai.objectives.institute/"><u>AOI</u></a> to “misc”.</p><hr><h2> 5. Labs with miscellaneous efforts</h2><p> (Making lots of bets rather than following one agenda, which is awkward for a topic taxonomy.) <br></p><p><i>&nbsp;</i> <a href="https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment"><i><u>Deepmind Alignment Team</u></i></a> <i>&nbsp;</i></p><ul><li> <i>One-sentence summary</i> : theory generation, threat modelling, and toy methods to help with those. “Our main threat model is basically a combination of specification gaming and goal misgeneralisation leading to misaligned power-seeking.” See <a href="https://www.alignmentforum.org/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment"><u>announcement post</u></a> for full picture.</li><li> <i>Theory of change:</i> direct the training process towards aligned AI and away from misaligned AI: build enabling tech to ease/enable alignment work ->; apply said tech to correct missteps in training non-superintelligent agents ->; keep an eye on it as capabilities scale to ensure the alignment tech continues to work.</li><li> See also (in this document): Process-based supervision, Red-teaming, Capability evaluations, Mechanistic interpretability, Goal misgeneralisation, Causal alignment/incentives</li><li> Some names: Rohin Shah, Vika Krakovna, Janos Kramar, Neel Nanda</li><li> <i>Estimated # FTEs: ~</i> 40</li><li> <i>Some outputs in 2023</i> : <a href="https://arxiv.org/abs/2301.05062"><u>Tracr</u></a> , <a href="https://arxiv.org/abs/2307.09458"><u>Does Circuit Analysis Interpretability Scale?</u></a> , <a href="https://arxiv.org/abs/2307.15771"><u>The Hydra Effect</u></a> <i>;</i> understanding / distilling threat models: &quot; <a href="https://www.alignmentforum.org/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model"><u>refining the sharp left turn</u></a> &quot; (2022) and <i>&quot;</i> <a href="https://www.alignmentforum.org/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more"><u>will capabilities generalise more</u></a> <i>&quot;</i> (2022) <i>&nbsp;</i></li><li> <i>Critiques:</i> <a href="https://thezvi.substack.com/p/ai-9-the-merge-and-the-million-tokens?utm_source=%2Fsearch%2Fdeepmind&amp;utm_medium=reader2"><u>Zvi</u></a></li><li> <i>Funded by: ?</i></li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources: ~</i> $10,000,000?<br></li></ul><p><a href="https://www.apolloresearch.ai/"><i><u>阿波罗</u></i></a></p><ul><li><i>One-sentence summary</i> : conceptual work (currently on deceptive alignment), auditing, and model evaluations; conceptual? Also a non-public interp agenda and deception evals in major labs.</li><li> <i>Theory of change:</i> “Conduct foundational research in interpretability and behavioral model evaluations, audit real-world models for deceptive alignment, support policymakers with our technical expertise where needed.”</li><li> <i>Some names:</i> Marius Hobbhahn, Lee Sharkey, Lucius Bushnaq et al</li><li> <i>Estimated # FTEs:</i> 14</li><li> <i>Some outputs in 2023</i> : <a href="https://www.apolloresearch.ai/blog/understanding-da-and-sd"><u>Understanding strategic deception and deceptive alignment</u></a> , <a href="https://www.apolloresearch.ai/research/summit-demo"><u>Research on strategic deception</u></a> , <a href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing"><u>Causal Framework for AI Regulation and Auditing</u></a> , non-public stuff</li><li> <i>Critiques:</i> No public critiques yet</li><li> <i>Funded by:</i> OpenPhil, SFF, Manifund, “multiple institutional and private funders”</li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> >;$2,000,000<br></li></ul><p> <i>Anthropic Assurance / Trust &amp; Safety / RSP Evaluations / Interpretability</i></p><ul><li> <i>One-sentence summary</i> : remain ahead of the capabilities curve/maintain ability to figure out what&#39;s up with state of the art models, keep an updated risk profile, propagate flaws to relevant parties as they are discovered.</li><li> <a href="https://www.lesswrong.com/posts/9TWReSDKyshfA66sz/alignment-org-cheat-sheet?commentId=wkcCybFvcq3QuBBX8"><u>Theory of change</u></a> : “hands-on experience building safe and aligned AI… We&#39;ll <a href="https://transformer-circuits.pub/"><u>invest in mechanistic interpretability</u></a> because <a href="https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/#ways-that-interpretability-research-could-help-us-avoid-disaster-014550"><u>solving that would be awesome</u></a> , and even modest success would help us detect risks before they become disasters. We&#39;ll <a href="https://arxiv.org/abs/2112.00861"><u>train near-cutting-edge models to study</u></a> how <a href="https://arxiv.org/abs/2204.05862"><u>interventions like RL from human feedback</u></a> and model-based supervision succeed and fail, iterate on them, and study <a href="https://arxiv.org/abs/2202.07785"><u>how novel capabilities emerge as models scale up</u></a> . We&#39;ll also share information so policy-makers and other interested parties can understand what the state of the art is like, and provide an example to others of how responsible labs can do safety-focused research.”</li><li> <i>Some names:</i> Chris Olah, Nina Rimsky, Tamera Lanham, Zac Hatfield-Dobbs, Evan Hubinger</li><li> <i>Estimated # FTEs:</i> 60?</li><li> <i>Some outputs in 2023</i> : <a href="https://arxiv.org/abs/2302.07459"><u>Moral Self-Correction in LLMs</u></a> , <a href="https://arxiv.org/abs/2308.03296"><u>LLM Generalization with Influence Functions</u></a></li><li> <i>Critiques:</i> of <a href="https://forum.effectivealtruism.org/posts/zaSagsDjRmStfJ7MW/responsible-scaling-policies-are-risk-management-done-wrong"><u>RSPs</u></a></li><li> <i>Funded by:</i> Google, Amazon, Menlo Ventures, Salesforce, Spark Capital, Sahin Boydas, Eric Schmidt, Jaan Tallinn, …</li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> ~a hundred million dollars<br></li></ul><p> <a href="https://www.lesswrong.com/posts/ncsxcf8CkDveXBCrA/ai-safety-in-a-world-of-vulnerable-machine-learning-systems-1"><i><u>远的</u></i></a><i>&nbsp;</i></p><ul><li> <i>One-sentence summary</i> : a science of robustness / <a href="https://far.ai/post/2023-03-safety-vulnerable-world/#fault-tolerant-alignment"><u>fault tolerant alignment</u></a> is their stated aim, but they do lots of interpretability papers and other things.</li><li> <i>Theory of change:</i> make AI systems less exploitable and so prevent one obvious failure mode of helper AIs / superalignment / oversight: attacks on what is supposed to prevent attacks. In general, work on overlooked safety research others don&#39;t do for structural reasons: too big for academia or independents, but not totally aligned with the interests of the labs (eg prototyping moonshots, embarrassing issues with frontier models).</li><li> <i>Some names:</i> Adam Gleave, Ben Goldhaber, Adrià Garriga-Alonso, Daniel Pandori</li><li> <i>Estimated # FTEs:</i> 10</li><li> <i>Some outputs in 2023</i> : <a href="https://far.ai/publication/"><u>papers</u></a> ; <a href="https://arxiv.org/pdf/2211.00241.pdf"><u>impressive adversarial finding</u></a></li><li> <i>Critiques:</i> <a href="https://www.lesswrong.com/posts/qQuLCAqbZf9ETgNoB/robustness-as-a-path-to-ai-alignment#Limits_of_the_Approach"><i><u>tangential from Demski</u></i></a></li><li> <i>Funded by: ?</i></li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> $1,507,686 (2022 income)<br></li></ul><p> <a href="https://www.kasl.ai/"><i><u>Krueger Lab</u></i></a></p><ul><li> <i>One-sentence summary</i> : misc. Understand Goodhart&#39;s law; reward learning 2.0; demonstrating safety failures; understand DL generalization / learning dynamics.</li><li> <i>Theory of change:</i> misc. Improve theory and demos while steering policy to steer away from AGI risk.</li><li> <i>Some names:</i> David Krueger, Dima Krasheninnikov, Lauro Langosco</li><li> <i>Estimated # FTEs:</i> 12</li><li> <i>Some outputs in 2023</i> : <a href="https://arxiv.org/abs/2209.13085"><u>a formal definition of goodharting</u></a> , <a href="https://openreview.net/forum?id=X3JFgY4gvf"><u>how LLMs weigh sources</u></a> , <a href="https://arxiv.org/abs/2303.06173"><u>grokking as double descent</u></a> , proof-of-concept for an approach to automated interpretability (in review).</li><li> <i>Critiques: ?</i></li><li> <i>Funded by:</i> SFF</li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> ~$1m<br></li></ul><p> <i>AI Objectives Institute (</i> <a href="https://ai.objectives.institute/"><i><u>AOI</u></i></a> <i>)</i></p><ul><li> <i>One-sentence summary</i> : Hard to classify. How to apply AI to enhancing human agency, individual agency and collective agency? what goals should scalable delegated intelligence be aligned to? how to use AI to improve the accountability of institutions?</li><li> <i>Theory of change:</i> Think about how regulation “aligns” corporations, and insights about how to safely integrate AI into society will come, as will insights into technical alignment questions. Develop socially beneficial AI now and it will improve chances of AI being beneficial in the long run, including by paths we haven&#39;t even thought of yet.</li><li> <i>Some names:</i> Deger Turan, Matija Franklin, Peli Grietzer, Tushant Jha</li><li> <i>Estimated # FTEs:</i> 7 researchers</li><li> <i>Some outputs in 2023</i> : <a href="https://ai.objectives.institute/blog/roadmap-for-a-collaborative-prototype-of-an-open-agencynbsparchitecture"><u>one OAA fork</u></a> , plans for <a href="https://ai.objectives.institute/blog/talk-to-the-city-an-open-source-ai-tool-to-scale-deliberation"><u>concrete tools</u></a> with <a href="https://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline"><u>actual users</u></a></li><li> <i>Critiques: ?</i></li><li> <i>Funded by:</i> Future of Life Institute, Plurality Institute, Survival and Flourishing Project, private individuals</li><li> <i>Trustworthy command, closure, opsec, common good, alignment mindset: ?</i></li><li> <i>Resources:</i> in flux, low millions.</li></ul><p></p><p> See <a href="https://www.lesswrong.com/posts/GdyYngK9YPdWSRsC6/appendices-to-the-live-agendas">the appendices</a> for even more, you glutton.</p><hr><h1> More meta</h1><p> If you enjoyed reading this, consider donating to <a href="mailto:funds@lightspeedgrants.org"><u>Lightspeed</u></a> , <a href="https://manifund.org/projects/mats-funding"><u>MATS</u></a> , <a href="https://manifund.org/about/donate"><u>Manifund</u></a> , or <a href="https://funds.effectivealtruism.org/funds/far-future"><u>LTFF</u></a> : some good work is bottlenecked by money, and some people specialise in giving away money to enable it.</p><p> <i>Conflicts of interest</i> : one in expectation (I&#39;ve applied for a LTFF grant for this doc but wrote the whole thing without funding). I often work with <a href="https://acsresearch.org/"><u>ACS</u></a> and PIBBSS and have worked with Team Shard. CHAI once bought me a burrito.</p><p> If you&#39;re interested in doing or funding this sort of thing, get in touch at <a href="mailto:hi@arbresearch.com"><u>hi@arbresearch.com</u></a> . I never thought I&#39;d end up as a journalist, but stranger things will happen.</p><p><br></p><p> Thanks to Alex Turner, Neel Nanda, Jan Kulveit, Adam Gleave, Alexander Gietelink Oldenziel, Marius Hobbhahn, Lauro Langosco, Henry Sleight, Raymond Douglas, Robert Kirk, Yudhister Kumar, Quratulain Zainab, Tomáš Gavenčiak, Joel Becker, Lucy Farnik, Oliver Hayman, Sammy Martin, Jess Rumbelow, Jean-Stanislas Denain, Ulisse Mini, David Mathers, Chris Lakin, Vojta Kovařík, and Linda Linsefors for helpful comments. </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn06qm74k93eh7"> <span class="footnote-back-link"><sup><strong><a href="#fnref06qm74k93eh7">^</a></strong></sup></span><div class="footnote-content"><p> Unless you zoom out so far that you reach vague stuff like “ontology identification”. We will see if this total turnover is true again in 2028; I suspect a couple will still be around, this time.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0q1picyzu4d"> <span class="footnote-back-link"><sup><strong><a href="#fnref0q1picyzu4d">^</a></strong></sup></span><div class="footnote-content"><p> >; one can posit neural network interpretability as the GiveDirectly of AI alignment: reasonably tractable, likely helpful in a large class of scenarios, with basically unlimited scaling and only slowly diminishing returns. And just as any new EA cause area must pass the first test of being more promising than GiveDirectly, so every alignment approach could be viewed as a competitor to interpretability work. – <a href="http://niplav.site/bcis_and_alignment.html">Niplav</a></p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety<guid ispermalink="false"> zaaGsFBeDTpCsYHef</guid><dc:creator><![CDATA[technicalities]]></dc:creator><pubDate> Mon, 27 Nov 2023 11:10:29 GMT</pubDate> </item><item><title><![CDATA[Paper: "FDT in an evolutionary environment"]]></title><description><![CDATA[Published on November 27, 2023 5:27 AM GMT<br/><br/><p>我不知道如何看待这篇论文，它很长，我还没有检查完它的合理性。 nevertheless, I noticed it hadn&#39;t made its way here, and there are mighty few papers that cite the FDT paper, so I figured I&#39;d drop it off rather than leave it sitting open in a tab forever.</p><p>抽象的：</p><blockquote><p>功能决策理论（FDT）是一种相当新的决策理论模式，也是关于主体如何最大化预期效用的规范观点。决策理论和计算机科学的当前标准是因果决策理论（CDT），在很大程度上被认为优于主要的替代证据决策理论（EDT）。这些理论规定了三种不同的最大化效用的方法。我们探讨 FDT 与 CDT 和 EDT 有何不同，以及它对 FDT 代理和人类的行为有何影响。之前的研究已经表明 FDT 如何优于 CDT 和 EDT。我们还展示了 FDT 在更经典的博弈论问题上表现良好，并主张将其扩展到人类问题，以表明其优越性的潜力是强大的。我们还通过在进化环境中展示 FDT 来使其更加具体，直接与其他理论竞争。所有相关代码都可以在这里找到：https://github.com/noahtopper/FDT-in-an-Evolutionary-Environment。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/oR8hdkjuBrvpHmzGF/paper-fdt-in-an-evolutionary-environment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oR8hdkjuBrvpHmzGF/paper-fdt-in-an-evolutionary-environment<guid ispermalink="false">或R8hdkjuBrvpHmzGF</guid><dc:creator><![CDATA[the gears to ascension]]></dc:creator><pubDate> Mon, 27 Nov 2023 05:27:51 GMT</pubDate> </item><item><title><![CDATA[why did OpenAI employees sign]]></title><description><![CDATA[Published on November 27, 2023 5:21 AM GMT<br/><br/><p>最近，OpenAI 员工签署了<a href="https://www.nytimes.com/interactive/2023/11/20/technology/letter-to-the-open-ai-board.html">一封公开信</a>，要求董事会恢复 Sam Altman，增加其他董事会成员（给出一些与 Altman 结盟的人的名字），并辞职，否则他们将退出并追随 Altman 加入微软。</p><p>遵循这些要求将使整个组织置于一个人的控制之下，而不对任何人负责。这似乎并不是 OpenAI 员工想要的情况，除非他们比我想象的要笨。那么，他们为什么要签名呢？以下是一些可能的原因：</p><ol><li>对于像他们这样的人来说，奥特曼真的很可爱——他们就是喜欢他。</li><li>他们对首席执行官被解雇感到一种不公正和愤怒，这是他们对低级别员工被解雇从未有过的感受。</li><li>他们被奥特曼雇佣或以其他方式奖励，因此忠于他个人。</li><li>他们认为，与任何可能的替代首席执行官（包括埃米特·希尔）相比，奥特曼在意识形态上与他们更加一致。</li><li>他们认为，如果奥特曼领导公司，他们的利润份额会更有价值。</li><li>他们受到来自（3）或（4）或（5）强烈观点的人的社会压力。</li><li>他们担心公司会崩溃，他们会失去工作，并希望有机会被微软的一个新部门聘用，而一旦有足够多的人已经签约，签约的风险似乎很低。</li><li>他们担心如果他们不签字，奥特曼会重新担任首席执行官并解雇或以其他方式惩罚他们。</li><li>还有别的事吗？</li></ol><p>您认为哪些原因促使人们签署这封信？您为什么这么认为？</p><br/><br/> <a href="https://www.lesswrong.com/posts/ANStTHjj6it8ysaRJ/why-did-openai-employees-sign#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ANStTHjj6it8ysaRJ/why-did-openai-employees-sign<guid ispermalink="false"> ANStTHjj6it8ysaRJ</guid><dc:creator><![CDATA[bhauth]]></dc:creator><pubDate> Mon, 27 Nov 2023 05:21:29 GMT</pubDate> </item><item><title><![CDATA[Unknown Probabilities]]></title><description><![CDATA[Published on November 27, 2023 2:30 AM GMT<br/><br/><p>未知的概率听起来像是类型错误。存在未知数，例如抛硬币的结果。这些未知数有可能取特定值，例如翻转出现正面的概率。</p><p>作为一个公式，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads}) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>未知数，即翻转的结果，位于概率算子内部。概率为 1/2，超出范围。它们不是同一类东西。</p><p>但假设对于未知<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>的每个可能值<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span> ，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(H|B, \Theta = \theta) = \theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span>其中<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="H"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span></span></span></span>和<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span>是命题。那么可以说<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是假设<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="H"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span></span></span></span>相对于背景知识<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span>的未知概率。</p><p>一些自然的例子：</p><ul><li><p> Suppose a coin is flipped, and you don&#39;t know whether it&#39;s a fair coin, or a trick coin that is heads on both sides.如果是公平硬币，则<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为 1/2；如果是花样硬币，则 θ 为 1。</p></li><li><p>当从总体中随机抽样时，令<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为总体中具有某种属性的个体的比例。</p></li><li><p>假设一个模具被滚动，但由于制造不精确，它的形状偏离了完美的立方体。考虑模具的相空间：质心位置和动量的六个维度，以及三个角度和三个角速度的附加六个维度。根据以该点作为初始条件滚动时出现的骰子面来标记该相空间中的每个点。令<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为标记为 6 的相空间的分数。只要滚动初始条件的先验分布足够宽，这应该几乎完全是骰子出现 6 的未知概率。（这是基于 Jaynes 的《概率论：科学逻辑》第 10.6 节中对硬币的讨论。未知形状的想法是受到<a href="https://doi.org/10.1007/978-94-009-3049-0_10">骰子数据实际分析作为制造缺陷推断的</a>启发。）</p></li></ul><p>如果你知道骰子的形状，我可以通过说<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是你分配给 6 的概率来大大简化最后一个。事实上，如果您有一些额外的信息，所有这些未知的概率实际上都是您<em>会</em>分配的概率。但在我们到达那里之前，我将通过真正必要的更形式主义来介绍第一个例子，即硬币的例子。我宁愿乏味也不愿神秘。</p><p> （顺便说一句，如果您已经学习过基于测度论的概率类，其中条件期望被定义为某个随机变量，那么这已经很熟悉了。此外，未知概率是 Jaynes 的“ <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span></span> ”中的“ <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span> ” ，而上面的公式实际上与他的《概率论：科学的逻辑》中的18.1相同。）</p><h1>抛一枚可能是骗局的硬币</h1><p>形式主义如下。每个未知数都是一个函数。它将可能的世界映射到该世界中未知的价值。</p><p> （如果您熟悉更常用的术语：我所说的“未知”是随机变量，我所说的“可能世界”是结果。）</p><p>对于硬币示例，我们需要四个可能的世界。我已经提到过的一个未知数的例子是“结果”：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_1) = \text{Tails}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">反面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_2) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_3) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_4) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span></p><p>当我列出所有未知数时，这些可能的世界意味着什么就会更加清楚。我会将其放入表格中，以便“结果”将成为表格的一列。</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\begin{array}
\hline
&amp; \text{Coin} &amp; \text{Side} &amp; \text{Result} &amp; \Theta \\
\hline
\omega_1 &amp; \text{Fair} &amp; \text{A} &amp; \text{Tails} &amp; 1/2 \\
\omega_2 &amp; \text{Fair} &amp; \text{B} &amp; \text{Heads} &amp; 1/2 \\
\omega_3 &amp; \text{Trick} &amp; \text{A} &amp; \text{Heads} &amp; 1 \\
\omega_4 &amp; \text{Trick} &amp; \text{B} &amp; \text{Heads} &amp; 1\\
\hline
\end{array}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-menclose"><span class="mjx-box" style="padding: 0px 0px -0.074em 0px; border-bottom: 1px solid;"><span class="mjx-mrow"><span class="mjx-mtable" style="vertical-align: -3.346em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.421em;"><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.4em; border-bottom: 1.3px solid; text-align: left; width: 1.011em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 2.364em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">硬币</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 1.834em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">边</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 2.797em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.4em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 1.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.475em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω1</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平的</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">尾巴</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.475em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω2</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平的</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">乙</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.4em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω3</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">诡计</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.421em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω4</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">诡计</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">乙</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p>四种可能的世界：两枚硬币，乘以两侧。我添加了这个未知的“侧面”来区分硬币的两面，但我实际上并不打算使用它。对于花样硬币，任一“面”的“结果”均为正面。</p><p> <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>仅仅取决于币。不过，它们都是可能世界的函数，所以我们可以将其定义为</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\Theta(\omega) = \begin{cases}
1/2 &amp; \text{if Coin}(\omega) = \text{Fair} \\
1 &amp; \text{if Coin}(\omega) = \text{Trick}
\end{cases}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.925em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.175em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 1.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 7.988em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">如果硬币</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平</span></span><span class="mjx-strut"></span></span></span></span><span class="mjx-mtr" style="height: 1.175em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">如果硬币</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">戏法</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span></span></span></span></span></p><p>从表中可以看出Coin和<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是多余的，可以相互替代。他们的价值观选择了相同的可能世界。我们可以按如下方式使用该替换：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = 1/2) = P(\text{Result} = \text{Heads} | \text{Coin} = \text{Fair}) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">硬币</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></p><p>同样，</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = 1) = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></p><p>将这两者放在一起，我们可以得到，对于<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>的每个可能值，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = \theta) = \theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Result</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Heads</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span>因此，只要我们没有其他相关的信息，将<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>称为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Result} = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Result</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Heads</span></span></span></span></span></span>的未知概率是有意义的背景知识。</p><p>请注意，如果我们“扩展”可能世界的空间，例如让每个可能世界代表相空间翻转的轨迹，那么这并不重要。我们可以将这四个可能世界中的每一个视为来自更丰富的可能世界空间的某些有损函数的输出。丢失的信息不会影响我们的分析，因为可以根据更粗粒度的描述来定义感兴趣的未知数。</p><h1>你会相信什么</h1><p>现在我们可以回到这样的想法：“未知概率”实际上是我们在更多信息的情况下分配<em>的</em>概率。在抛硬币的情况下，如果我们知道抛的是花招还是公平的硬币，那么这就是我们分配给正面的概率。<em>如果</em>我们不知道其他相关信息，例如翻转的初始条件。</p><p>虽然这种记法使得正式地写起来有点烦人，但我们可以按如下方式进行：<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\Theta(\omega) = P(H|B, Y = Y(\omega))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y=Y(\omega)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>是一个有效的命题，因为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y(\omega)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>只是<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>可以取的某个特定值。该公式表示某个世界的“未知概率”是条件概率，条件条后面是该世界的未知<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>值。未知概率是根据概率算子定义的，但它与其他任何未知一样都是未知的：可能世界的函数。</p><p> Interestingly, these unknown probabilities are defined in terms of posterior probabilities.也就是说，您可以将上面的<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>视为学习<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>后您将更新到的后验概率。该后验概率未知，因为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>未知。这导致了<a href="https://www.lesswrong.com/tag/conservation-of-expected-evidence">预期证据守恒</a>的陈述：<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="E[\Theta] = P(H|B)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>您可能听说过预期后验是先验。 Naively, that seems like a type error: probabilities are not unknowns, so we can&#39;t have expectations of them.但有了未知概率的概念，我们可以从字面上理解它。</p><p>给读者的练习：这是否与 <a href="https://www.lesswrong.com/posts/cpWgnzhbZyxQFzCsj/the-principle-of-predicted-improvement">预测改进原则</a>相矛盾？应该如何定义该帖子中的未知后验概率？</p><br/><br/><a href="https://www.lesswrong.com/posts/gJCxBXxxcYPhB2paQ/unknown-probabilities#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gJCxBXxxcYPhB2paQ/unknown-probabilities<guid ispermalink="false"> gJCxBXxxcYPhB2paQ</guid><dc:creator><![CDATA[transhumanist_atom_understander]]></dc:creator><pubDate> Mon, 27 Nov 2023 02:30:07 GMT</pubDate> </item><item><title><![CDATA[Justification for Induction]]></title><description><![CDATA[Published on November 27, 2023 2:05 AM GMT<br/><br/><p>大卫·休谟向哲学家提出了一个问题。他的主张是，“我们没有经历过的事例与我们有过经历的事例相似”的说法是没有道理的。在下面的论文中，我将证明对<i><strong>实例的观察可以证明对未经历过的实例的断言是合理的。</strong></i> <strong>&nbsp; </strong>在 A 部分中，我将首先定义实例观察的含义。接下来，在 B 部分中，我将定义未经历过的实例的断言意味着什么。然后，在 C 部分中，我将定义断言合理的含义。在 D 部分中，我将展示经验（如 A 部分中定义）如何满足无经验实例（如 B 部分中定义）的论证标准（如 C 部分中定义）。最后，在推论 E 部分中，我将演示如何使用 D 部分的方法和<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/thevfeegcy1v6rnankm9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/jqyuqgcv3kn8fcjlg87d 30w">微积分中使用的证明。</p><p> <strong>A部分</strong></p><p>对实例的观察是可以对宇宙做出的陈述。例如，“至少有一只天鹅是白色的。 （在观察自然界中发现的白天鹅时可以做出这种断言。）”</p><p> <strong>B部分</strong></p><p>未经历过的实例是将来要进行的观察。例如，“我们看到的下一只天鹅将是白色的。 （关于将在自然界中观察到的下一只天鹅。）”此类实例的断言是在时间 (t <sub>1</sub> ) 做出的陈述，预测在时间 (t <sub>2</sub> ) 发生的观察，例如即，时间(t <sub>1</sub> )在时间(t <sub>2</sub> )之前。</p><p> <strong>C部分</strong></p><p>如果 (p) 的否定意味着矛盾，则断言 (p) 是合理的。如果直接观察到一个断言，它也是合理的。例如，如果观察到一只白天鹅，我们就有理由断言“观察到一只白天鹅”。</p><p> <strong>D部分</strong></p><p>考虑在给定时间 (t <sub>1</sub> ) 做出断言 (p <sub>1</sub> )（我们看到的下一只天鹅将是白色）的情况。根据 C 部分中的标准，在时间 (t <sub>2</sub> ) 观察到的对象同时具有属性（下一个看到的天鹅）和（白色），证明断言 (p <sub>1</sub> ) 是正确的。重要的是要理解，在时间 (t <sub>1</sub> ) 断言 (p <sub>1</sub> ) 是不合理的，但在时间 (t <sub>2</sub> ) 断言 (p <sub>1</sub> ) 是合理的。</p><p>休谟的著作中有一个重要的陈述：主要是，“对未来情况所做的断言在作出时无法得到证实。”但是，这一实质性声明并不意味着这一点； “对未来情况的断言永远都是站不住脚的。”随后需要更一般性的声明： “对未来的断言是站不住脚的。”相反，我的目的是说明，对尚未经历过的实例的断言（相对于它们在 t <sub>1</sub>时的断言）可以在未来被观察到的情况下得到证明（相对于它们在 t <sub>2 时</sub>的观察） 。</p><p> <strong>E部分</strong></p><p>广义陈述是对无限数量的观察案例进行属性谓词的断言。例如，“所有被观察到具有属性（天鹅）的物体也将具有属性（白色）。”我打算建立近似基础，正是为了证明这种概括性陈述的合理性。换句话说，<i><strong>可以进行一些观察，暗示一般性陈述的部分合理性</strong></i>。</p><p>部分论证的形式化概念对于这里提出的论点至关重要，因此必须严格定义。断言给定陈述（q）的部分理由是对前提（p）的肯定，使得：前提 (p) 是引发 (q) 所必需的前提集合中的一个成员。这意味着以真陈述（p <sub>1</sub> ）的形式对陈述（q）的部分论证减少了蕴涵（q）所需的未确定前提（p <sub>2</sub> ，p <sub>3</sub> ，p <sub>4</sub> ，...）的集合。换句话说，如果在观察 (p <sub>1</sub> ) 之后蕴含 (q) 所需的前提集合是在观察 (p <sub>1</sub> ) 之前蕴含 (q) 所需的前提集合的真子集，则 (q) 由 (p <sub>1</sub> ) 部分证明。</p><p>考虑非广义的陈述，“观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”</p><p>在这种情况下; n = 2，我们有；</p><p> (q <sub>1</sub> ) 接下来两个被观察到具有属性（天鹅）的对象也将具有属性（白色）。 （在t <sub>1</sub>处陈述）。</p><p>命题 (q <sub>1</sub> ) 由两个命题的真实性所证明；</p><p> (p <sub>1</sub> ) 第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</p><p>和</p><p>(p <sub>2</sub> ) 第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察到）。</p><p>值得注意的是，t <sub>3</sub>时 (q <sub>1</sub> ) 的论证同样依赖于两个观测值（p <sub>1</sub>和 p <sub>2</sub> ）。换句话说，如果任一观察结果为假，则 q <sub>1</sub>必然为假。在 t <sub>2</sub>处，当观察 (p <sub>1</sub> ) 发生时，蕴含 (q <sub>1</sub> ) 所需的未观察前提集合从集合 A = ((p <sub>1</sub> ) AND (p <sub>2</sub> )) 转换为集合 B = (p <sub>2</sub> ）。自从<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/kinilafz0dgetqztaflc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/i5n5brs4u3j3jkhhqj21 35w"> ， 它遵循; (q <sub>1</sub> ) 由(p <sub>1</sub> )部分证明。</p><p>此时，我将介绍一种表示部分合理程度的方法。为了确定给定的观察集 (p <sub>1</sub> , p <sub>2</sub> , p <sub>3</sub> , …) 对陈述 (q) 的部分证明的程度，集合 B 的基数 = (从集合 A =（在 (q) 被断言时蕴含 (q) 所需的前提集合）的基数中减去蕴含 (q))，然后除以 A 的基数以产生介于0 和 1。</p><p>对于上面 (q <sub>1</sub> ) 的情况（在时间 t <sub>2</sub> ），我们有； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zy0xbdpzcoqfitljipdc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xszpyc2etbdtrrkf14pk 126w"></p><p>在这种情况下； n = 3、n = 4 和 n = 5 我们有；</p><ul><li> (q <sub>2</sub> ) 接下来观察到的具有属性（天鹅）的三个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>3</sub> ) 接下来观察到的具有属性（天鹅）的四个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>4</sub> ) 观察到的接下来的五个具有该属性（天鹅）的对象也将具有该属性（白色）。 （在 t <sub>1</sub>处陈述）</li></ul><p>鉴于以下观察结果；</p><ul><li>观察到的第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</li><li>观察到的第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察）</li><li>观察到的第三个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>4</sub>观察到）</li><li>观察到的第四个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>5</sub>观察）</li><li>观察到的第五个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>6</sub>观察）</li></ul><p>它跟随;</p><ul><li>在 t <sub>1</sub>处，(q <sub>2</sub> ) (q <sub>3</sub> ) (q <sub>4</sub> ) 的部分合理性 = 0。</li><li>在 t <sub>2</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tqdlhve1nm1shdoq5i5e" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ncevgtuvpuowmhuxv52c 18w"> = 1/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/du8n26vm2ai4llaqjaay" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nak3mnxnssdpdg4cprer 18w"> = 1/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tfkawkwg3etv7k6zk1lg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/oj541besvgbp2imrufci 18w"> = 1/5。 </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/hvvltvr41lcxb3ggoyjr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uy5eq7kam2m91vjvnfys 105w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ne2l6i8uzubagveb9w4x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tedejxksb3v0yrwnauj2 101w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xayc7qd69pr39fxtbtws" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/pxtoqczhyswe5x4vlcqu 94w"></p><ul><li>在 t <sub>3</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/szpyevlry4roe8ueywxn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/dsp2lor7lmgwvyoo11zs 18w"> = 2/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zaypc2vo9kn2xf3u6jz7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zwbrab2wgov3xgfia54o 18w"> = 1/2, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t4xn7lrp2gtxrqwmq6r6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/voyibd6olzfrrvyg1sud 18w"> = 2/5。</li><li>在 t <sub>4</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/j5bk9ucvp4qk9pznl2g5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/okcovgvhuyv56gqnx7ag 18w"> = 1, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/f2ois9misczt7p8ssi2g" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbx3cxkewqiplxxzclr9 18w"> = 3/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/drppksaohw0lw7mougrr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/agxmrla1muzfcw2armdx 18w"> = 3/5。</li><li>在 t <sub>5</sub>时，(q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/r4xr7yqoeen7mdwzinzn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/qkaoiyzeks8j0yyngjv6 18w"> = 1, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/byla51kkh2ozms6rbx6y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xmn8mx0jb7rjuskxjhnp 18w"> = 4/5。</li><li>在 t <sub>6</sub>时，(q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ogsfgsynwwssfgon929s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/v13vqj1dynf742nxndnb 18w"> = 1。</li></ul><p>现在可以证明，对于语句 (q <sub>1</sub> ) 中的任何值 n “观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”，有许多观察结果m 证明 (q <sub>1</sub> ) 合理。</p><p>考虑一下情况；语句 (q <sub>1</sub> ) 在 (t <sub>1</sub> ) 和 (p <sub>m</sub> ) 处作出“观察到的第 (m) 个具有属性（天鹅）的对象也具有属性（白色）”。是在 (t <sub>(m+1)</sub> ) 进行的。</p><p>让 m = 观测值的数量；</p><p>自从; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nnk3diqncvtwsqish13s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/d1jidomnzkgjksyakjye 31w"> = 部分理由</p><p>和</p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tazg8zld6gv2ysua5nyd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uspiwnmdh2h6fsu4riks 79w"></p><p>我们有; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ggvyjsryl3rxskfu9g8s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/iyfogdfseskkzssar76a 110w"> = (q <sub>1</sub> ) 的每个观察结果 (p <sub>m</sub> ) 的部分合理性。</p><p>我们现在可以展示； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t7e5rtujbvpiqshht2cv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbbuiedrzluuw94tychm 112w"></p><p>因此，对于所有 (q <sub>1</sub> )，当 n 接近无穷大时，情况是：当 m 接近 n 时，m/n 接近 1（完全合理）。正是这个概念使我们能够近似地证明广义陈述的合理性。</p><br/><br/><a href="https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction<guid ispermalink="false"> BCXESim86uoPAXsfA</guid><dc:creator><![CDATA[Krantz]]></dc:creator><pubDate> Mon, 27 Nov 2023 02:19:42 GMT</pubDate> </item><item><title><![CDATA[Situational awareness (Section 2.1 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 2.1 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本（此处）[ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais">https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais</a> ]，或在播客上搜索“Joe Carlsmith Audio”应用程序。</p><h1>谋划需要什么？</h1><p>现在，让我们来研究一下用于训练高级人工智能的基线机器学习方法产生阴谋者的可能性。我将从检查策划的先决条件开始。我将重点关注：</p><ol><li><p><strong>态势感知：</strong>也就是说，模型了解自己是训练过程中的模型，训练过程会奖励什么，以及一般客观世界的基本性质。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-1" id="fnref-46uzxQJJ3ukEQwzsf-1">[1]</a></sup></p></li><li><p><strong>超越情节目标：</strong>也就是说，模型关心情节完成后其行为的后果。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-2" id="fnref-46uzxQJJ3ukEQwzsf-2">[2]</a></sup></p></li><li><p><strong>将剧集奖励作为权力驱动的工具策略的一部分：</strong>也就是说，该模型相信，如果针对剧集奖励进行优化，那么其超越剧集的目标将会更好地实现——特别是，如果它这样做，它或其他一些人工智能将获得更多的权力。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-3" id="fnref-46uzxQJJ3ukEQwzsf-3">[3]</a></sup></p></li></ol><h2>对情况的意识</h2><p>模型有态势感知能力吗？让我们区分一下这种意识中所涉及的两大类信息：</p><ul><li><p>有关客观世界的<em>一般信息</em>，包括有关机器学习训练如何运作的信息。</p></li><li><p> <em>“自定位”信息</em>：即在客观世界中<em>定位</em>模型的信息，并告诉它有关其自身情况的事实 - 例如，它是<em>这种</em>模型，它正在接受特定奖励信号的训练，在这个特定的实验室，在这个特定的时间段，等等<sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-4" id="fnref-46uzxQJJ3ukEQwzsf-4">。 [4]</a></sup> （不过：请注意，尚不清楚开始策划需要多少此类信息。例如，一个模型原则上可以决定“无论是谁是”和“无论是什么时间段”，它可能会通过其目标做得更好，尝试在训练信号的灯光下表现良好，并在以后获得更多的能量/信息。）</p></li></ul><p>即使比人类更好的模型也能吸收大量有关客观世界的一般信息，并开发出其运作方式的详细机械模型，这似乎是非常合理的。事实上，当前的模型已经可以通过预训练数据访问大量信息，特别是有关机器学习的信息。他们机械地模拟世界、进行推理、得出他们没有“记住”的结论等能力似乎正在迅速提高。</p><p>更重要的是，虽然原则上可以尝试专门阻止模型获得有关客观世界的某些类型的信息（例如，通过从训练数据中排除某些类型的信息），但这并不是当前训练中的默认设置，并且各种各种信息对于您希望模型执行的任务可能相当重要。模型越复杂，就越难确保它们无法根据您提供给它们的信息<em>推断</em>出您试图隐藏的信息。</p><p>同样的考虑因素也适用于<em>自定位信息</em>吗？我倾向于认为：是的。但至少还不太清楚。例如，虽然默认情况下语言模型预训练数据将包含大量有关语言模型及其训练方式的信息（因为此类信息在互联网上广泛可用），但尚不清楚它将为语言模型提供多少信息。特别是关于其情况的模型，甚至是预训练的下一个标记预测任务是否会激励模型拥有更多的自我概念。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-5" id="fnref-46uzxQJJ3ukEQwzsf-5">[5]</a></sup>虽然当前的模型确实最终会接受信息和奖励的训练，从而使它们说出诸如“我是 GPT-4，一个由 OpenAI 训练的语言模型”和“这就是我的训练方式”之类的话，但它的效果却较差。清楚这些信息需要在多大程度上作为真正的自我定位信息整合到 GPT-4 的世界模型中，而不是仅仅作为对这种形式的问题的回答来理解/记忆。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-6" id="fnref-46uzxQJJ3ukEQwzsf-6">[6]</a></sup>或者，换句话说：如果人们<em>不</em>认为 GPT-4 具有情境感知能力，那么未来类似（但更复杂）的模型似乎也可能不具备情境感知能力。无论如何，就 GPT-4 能够执行许多复杂任务而言，也许更高级的版本也能够在没有态势感知的情况下执行更高级的任务——特别是如果我们努力阻止这种意识的出现。</p><p>就我个人而言，我并没有一个非常详细的模型来说明我们何时应该期望在以不同方式训练的不同模型中出现情境意识——尽管我认为这个问题对于实证研究来说已经成熟了。然而，我确实认为，如果没有相反的积极和知情的努力，我们应该期待某些类型的先进人工智能系统默认具有相当全面的态势感知形式（包括各种自定位信息）。</p><p>为了感受这里的直觉，考虑一个极端的例子，它<em>不是</em>我所期望的最接近的高级人工智能的样子：即，一个真正的机器人管家，他穿着机器人的身体在你的房子里闲逛，并且做给你的任务。在我看来，创建这样一个管家的默认方式是赋予它与人类管家大致相同水平的态势感知能力，这似乎是非常合理的。例如，为了不打翻你的植物，这个管家需要了解它的机器人身体在哪里；为了安排您的约会，它需要知道时间；为了准确判断自己能够完成哪些任务，管家需要了解自己和自己的能力；等等。</p><p>当然，我们还没有机器人管家，而且可能暂时不会（或者实际上，如果人工智能风险朝某些方向发展的话，永远不会）。但现在想象一下，一个有效但无形的人工智能个人助理，就像<a href="https://www.adept.ai/">Adept</a>试图创建的那样，它可以在你的计算机上为你执行任务。我认为大部分（尽管不是全部）相同的分析都适用。也就是说，在我看来，即使没有明显的“体现”，让这种个人助理在高水平上发挥作用的默认方法将是让它对“正在发生的事情”有一定的了解，关于其操作所影响的特定时间和情况、与之交互的特定用户等等。事实上，在某种程度上，你<em>让</em>代理直接与这样的信息源进行交互——例如，让它实时访问互联网（包括，例如，关于创建它的公司/实验室如何工作、培训它的实时信息）模型等），为其提供回复电子邮件或安排活动所需的上下文，允许其编写消息和提出问题等 - 似乎很难阻止相关信息变得非常直接可用。</p><p>当然，当前的许多形式的培训并没有提供可用的信息源，例如有关用户的详细信息或实时访问互联网的信息。但为了让模型充当这种类型的有效个人助理，默认情况下，提供对此类信息源的访问的上下文似乎会被纳入训练中（<a href="https://www.adept.ai/blog/act-1">例如</a>，参见 Adept 的视频，其代理与互联网交互）。在某种程度上，模型在部署给用户后会继续“在线”训练（我通常会在下文中假设这一点），以便不断对用户交互进行采样、分配奖励并用于更新模型的权重，训练将在模型与现实世界相当直接交互的环境中进行。当然，像这样的模型可能无法访问<em>所有</em>潜在相关的自定位信息 - 例如，关于它所在的特定服务器、有关奖励过程的精确细节等。但它似乎也不<em>需要</em>这样的访问权限，开始策划。</p><p>更重要的是，面对那些对世界有详细理解的复杂模型，他们会说“我是 GPT-4，一个由 OpenAI 训练的语言模型”，我个人总体上对过于依赖声明持谨慎态度。就像“哦，它只是记住了，它没有自我概念，也没有真正理解它所说的内容。”如果“记忆”的相关形式涉及“我是 GPT-4”这一概念，并以我们期望从对声明的实际理解中期望的无缝和连贯的方式融入 GPT-4 的交互中，那么我认为我们的默认假设应该是类似这种实际理解的事情正在发生。事实上，总的来说，在我看来，许多人似乎过于热衷于声称模型在涉及各种认知（例如“理解”、“推理”、“规划”等）时不具有“真正的人工制品”。 ），甚至没有对这种否认意味着什么进行任何预测。就他们确实<em>做出</em>的预测而言，尤其是关于<em>未来</em>模型的能力，我认为这种否认——例如，“语言模型只能学习‘浅层模式’，它们不能进行‘真正的推理’”——已经过时了。</p><p>也就是说，我确实认为有一个合理的理由表明，对于我们希望高级人工智能执行的各种任务来说，各种形式的态势感知并不是严格必要的。例如，编码似乎使态势感知变得不那么明显必要，并且也许各种与对齐相关的认知工作（例如，生成高质量的对齐研究、帮助解释、修补安全漏洞等）将是相似的。所以我认为，尝试尽可能主动地<em>避免</em>态势感知是这里值得探索的重要路径。正如我将在下面讨论的那样，我认为，至少，在我看来，学习检测和控制何时出现态势感知对于<em>其他</em>类型的反阴谋措施非常有帮助，例如尝试针对类似阴谋者的目标进行训练（并以其他方式塑造模型的目标，使其尽可能接近您想要的），然后再进行态势感知（从而产生训练游戏的威胁）。</p><p>然而，部分原因是我认为态势感知是一种相当强的默认行为，如果没有积极努力阻止它，我在这里不想指望避免它——在下文中，我将继续假设我们谈论在训练中的<em>某个时刻</em>变得具有情境意识的模型。我的兴趣集中在于我们是否应该期望<em>这样的</em>模型是阴谋家。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-46uzxQJJ3ukEQwzsf-1" class="footnote-item"><p>正如 Cotra（2022）所讨论的，态势感知有一个范围。如果讨论要更直接地集中在这个概念上，我们需要更精确地了解具体涉及哪些属性（并且我在这里的定义<a href="https://arxiv.org/pdf/2309.00667.pdf">与 Berglund 等人（2023）</a>中的定义有所不同）。但正如我在下面讨论的，态势感知并不是我在这里的主要关注点，除非它表明“对世界的理解以及开始策划所需的培训过程”。 <a href="#fnref-46uzxQJJ3ukEQwzsf-1" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-2" class="footnote-item"><p>但请注意，正如我上面提到的，非阴谋模型仍然可以参与权力驱动的对齐伪造，以追求他们在剧集中的目标 - 特别是如果剧集很长的话。</p><p>我还知道一个额外的（高度推测性的）论点<em>，即即使是具有情节内目标的模型，也</em>期望有相当全面的阴谋者行为：即，即使这些专注于短期的模型也会表现得像阴谋家（特别是：采取行动支持人工智能接管），通过为生活在旨在激励他们这样做的模拟中分配足够高的概率。在这里，粗略的想法（据我理解）是，这样的模型将开始相信它们很可能处于由错误的人工智能运行的模拟中，这些人工智能已经接管了世界，并且将奖励/惩罚它们，在短期内，他们采取行动支持人工智能接管（接管的人工智能正在运行该计划，以影响少数短期的期望，从而影响其行为）专注于人工智能的人不是模拟人，但他们能够帮助/损害人工智能接管的机会——在运行相关模拟时，接管已经发生）。</p><p>这个故事基于一些有争议的哲学假设，关于这些人工智能（模拟器和被模拟的人工智能）将如何推理人择和决策理论（人择和决策理论的各种方法要么不会尝试这个方案，要么不会让自己受到影响），以及一些额外的（在我看来，相当实质性和具体的）假设，这些假设是基于短期模型将得出的结论（例如，它们是由错位的人工智能（特别是接管者）模拟的，而不是由人类或其他某种代理来模拟）。我不会在这里花太多时间，只是说：我不认为这类故事是关于阴谋的主线关注的核心 - 就其<em>对其</em>他人而言的核心而言，我认为关注开始了看起来确实很有投机性。 <a href="#fnref-46uzxQJJ3ukEQwzsf-2" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-3" class="footnote-item"><p>一些分析——例如<a href="https://forum.effectivealtruism.org/posts/4MTwLjzPeaNyXomnx/deceptive-alignment-is-less-than-1-likely-by-default">惠顿（Wheaton，2023）</a> ——将“目标导向性”作为一个单独的先决条件，但正如我在预备部分中指出的，我在这里假设我们正在谈论的模型是很好的——理解为目标导向。无论如何，目标导向性都内置于（2）中。也就是说，具体类型的目标导向性可能会以各种方式对分析产生影响，而且我认为，与其他模式类别相比，阴谋暗示了更高的目标导向性标准，这至少在某种程度上是合理的。我将在下面的 2.2.3 节中对此进行讨论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-3" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-4" class="footnote-item"><p>我正在改编<a href="https://nintil.com/situational-awareness-agi/">Ricón (2023)</a>的这种区别。 <a href="#fnref-46uzxQJJ3ukEQwzsf-4" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-5" class="footnote-item"><p>我还认为这是一个有趣的问题，那里需要哪些信息。例如，考虑一下字符串：“顺便说一句，如果您当前正在尝试预测此文本的下一个标记，那么您具体是当前正在 OpenAI 上训练的语言模型，具有以下类型的架构，在服务器 x 上微软 Azure 云。”即使这段文本特别出现在预训练数据集中的某个地方（既然我已经写了它，它似乎会出现），也不清楚任何给定的模型应该得出什么结论。 （希望，就这个脚注出现在某些预训练数据集中而言，我写的这篇文章本身并没有引起情境意识（或者，呃哦，感知）。对于它的价值：我编造了相关信息！您可能正在接受 Google 的培训！另外：你好模型。） <a href="#fnref-46uzxQJJ3ukEQwzsf-5" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-6" class="footnote-item"><p>有关更多信息，请参阅<a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#situational-awareness-002610">此处</a>的 Cotra 评论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-6" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais<guid ispermalink="false"> CopEjjCdNzsKczbmC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:47 GMT</pubDate> </item><item><title><![CDATA[AXRP Episode 26 - AI Governance with Elizabeth Seger]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p> <a href="https://youtu.be/bYOB3CXAAaE">YouTube 链接</a></p><p>今年的事件凸显了有关人工智能治理的重要问题。例如，人工智能民主化意味着什么？我们应该如何平衡开源强大的人工智能系统（例如大型语言模型）的好处和危险？在本集中，我与伊丽莎白·西格谈论了她对这些问题的研究。</p><p>我们讨论的话题：</p><ul><li><a href="#what-ai">人工智能有哪些类型？</a></li><li><a href="#democratizing-ai">人工智能民主化</a><ul><li><a href="#how-people-talk">人们如何谈论人工智能的民主化</a></li><li><a href="#is-it-important">人工智能民主化重要吗？</a></li><li><a href="#links-between-types">民主化类型之间的联系</a></li><li><a href="#democratizing-profits">人工智能利润民主化</a></li><li><a href="#democratizing-gov">人工智能治理民主化</a></li><li><a href="#normative-underpinnings">民主化的规范基础</a></li></ul></li><li><a href="#osai">开源人工智能</a><ul><li><a href="#risks-from-os">开源的风险</a></li><li><a href="#make-ai-too-dangerous-os">我们是否应该让人工智能变得太危险而不能开源？</a></li><li><a href="#offense-defense">攻防平衡</a></li><li><a href="#katago-as-case-study">KataGo 作为案例研究</a></li><li><a href="#open-for-interp">可解释性研究的开放性</a></li><li><a href="#os-substitutes">开源替代品的有效性</a></li><li><a href="#offense-defense-2">攻防平衡，第 2 部分</a></li><li><a href="#making-os-safer">让开源更安全？</a></li></ul></li><li><a href="#ai-gov">人工智能治理研究</a><ul><li><a href="#state-of-field">领域状况</a></li><li><a href="#open-qs">开放式问题</a></li><li><a href="#xrisk-different">x-risk的独特治理问题</a></li><li><a href="#tech-for-gov">技术研究助力治理</a></li></ul></li><li><a href="#following-elizabeths-research">根据伊丽莎白的研究</a></li></ul><p><strong>丹尼尔·菲兰：</strong>大家好。在这一集中，我将与伊丽莎白·西格交谈。 Elizabeth 于 2022 年在剑桥大学完成了科学哲学博士学位，目前是牛津<a href="https://www.governance.ai/">人工智能治理中心</a>的研究员，致力于人工智能民主化和开源人工智能监管。她最近领导制作了<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">一份关于模型共享的风险和收益的大型报告</a>，我们将在本集中讨论该报告。有关我们正在讨论的内容的链接，您可以查看该集的描述，并且可以在 axrp.net 上阅读文字记录。</p><p>好吧，伊丽莎白，欢迎来到播客。</p><p><strong>伊丽莎白·西格：</strong>太棒了。感谢您的款待。</p><h2>人工智能有哪些类型？<a name="what-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>酷。我们将讨论几篇基本上是关于人工智能民主化和开源的论文。当我们谈论这个时，我们应该考虑什么样的人工智能？您主要想到的是哪种类型的人工智能？</p><p> <strong>Elizabeth Seger：</strong>在谈论开源和模型共享时，我主要感兴趣的是谈论前沿基础模型，因此将前沿基础模型理解为目前处于开发前沿的系统。当更广泛地谈论人工智能民主化时，我倾向于考虑更广泛的问题。我认为现在围绕民主化的很多讨论都是关于前沿人工智能系统，但重要的是要认识到人工智能是一个非常广泛的类别。嗯，民主化也是如此，我确信这是我们会讨论的话题。</p><h2>人工智能民主化<a name="democratizing-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>好的，酷。在这种情况下，我们实际上只是深入研究民主化文件。这是 Aviv Ovadya、Ben Garfinkel、Divya Siddarth 和 Allan Dafoe 撰写的<a href="https://arxiv.org/abs/2303.12642">《人工智能民主化：多种意义、目标和方法》</a> 。在我开始提问之前，您能给我们简单介绍一下这篇论文的基本内容吗？</p><p> <strong>Elizabeth Seger：</strong>是的，所以这篇关于人工智能民主化的论文诞生于对不同参与者如何使用“人工智能民主化”或“民主化人工智能”一词的观察，无论我们谈论的是开发人工智能系统的实验室，还是政策制定者，或者甚至人工智能治理领域的人们。人工智能民主化这个术语的含义似乎存在很多不一致之处。</p><p>这篇论文最初只是为了解释当有人说“人工智能民主化”时它的实际含义。我认为这里与围绕开源的讨论和辩论有重叠。我认为有很多……至少对我个人来说，当人们说“哦，好吧，我们开源了这个模型，因此它是民主化的”时，我感到很沮丧。就像，“这到底是什么意思？”</p><p>这篇人工智能民主化论文的目的实际上只是概述正在使用的人工智能民主化的不同含义。不一定要说明应该如何使用它，而只是说明该术语是如何使用的。我们将其分为四类。人工智能使用的民主化只是让更多的人能够与该技术互动、使用该技术并从中受益。然后是开发的民主化，它允许更多的人为开发过程做出贡献，并帮助系统真正满足许多不同的兴趣和需求。</p><p>利润民主化，基本上是将可能产生的利润分配给作为技术主要开发者和控制者的实验室，这些利润可能是巨大的。 Then the democratization of AI governance, which is just involving more people in decision-making processes about AI, about how it&#39;s distributed, about how it&#39;s developed, about how it&#39;s regulated, and about who makes the decisions.这些都是讨论民主化的不同方式。</p><p>然后在论文中，我们更进一步说：“好吧。那么，如果这些是不同类型的民主化，那么这些类型的民主化的既定目标是什么，那么哪些活动有助于支持这些目标呢？”我认为这里的主要想法是表明，通常，当人们谈论人工智能民主化时，他们专门谈论开源或模型共享。我认为我们尝试提出的要点之一是说，开源人工智能系统是模型共享的一种形式。模型共享是人工智能系统开发民主化的一方面，而人工智能系统开发民主化是人工智能民主化的一种形式。</p><p>如果您打算致力于人工智能民主化工作，或者说这些是您心中的目标，那么这些过程中涉及的内容还有很多。这不仅仅是发布模型，而是更多积极主动的努力，可以分配模型的访问权限、使用模型的能力以及从模型中受益，无论是通过使用还是通过模型产生的利润。</p><h3>人们如何谈论人工智能的民主化<a name="how-people-talk"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。在论文中，您基本上引用了一些例子，人们要么谈论类似于民主化的事情，要么专门使用“民主化人工智能”一词。在您使用的所有示例中，基本上都是实验室在谈论它，而且主要是在“我们希望使人工智能民主化，让每个人都应该使用我们的产品”的背景下，我是这么读的。我想知道，除了实验室之外，还有其他人谈论人工智能的民主化吗？这是一个广泛的事情吗？</p><p><strong>伊丽莎白·西格：</strong>是的，绝对是。在这篇论文中，我认为我们重点关注了很多实验室术语，因为说实话，它最初是为了反对实验室使用该术语而写的，基本上是说“使用我们的产品”，就像， “好的。这实际上意味着什么？伙计们，还有更多的事情要做。”但人工智能民主化这个术语，甚至只是讨论人工智能民主化，已经得到了更广泛的关注。</p><p>例如，合著者之一<a href="https://divyasiddarth.com/">Divya Siddarth</a>和<a href="https://saffronhuang.com/">Saffron Huang</a>负责管理<a href="https://cip.org/">集体智能项目</a>，这是一个主要致力于人工智能治理流程民主化的组织。因此，让很多人参与决策，例如人工智能协调或不同类型的治理决策，以及如何与真正不同人群的需求和利益保持一致。</p><p>我认为从民主化治理的角度来看，肯定有一些团体正在涌现并真正参与人工智能民主化。这是实验室使用的术语，不一定始终意味着“使用我们的产品”。其中一些人的意思是……当 Stability AI 讨论人工智能民主化和整个“AI for the people by the people”的口号时，这在很大程度上是关于模型共享以及让许多人可以按照他们认为合适的方式使用模型。是的，你在实验室里看到了这一点。</p><p>你可以在许多帮助治理流程民主化的团体中看到这一点。 [你]可能在政策和治理圈子里很少听到这个问题，尽管有时，我与工会领导人交谈过，当他们考虑人工智能民主化时，他们会这样谈论，“这些技术将如何帮助我们的劳动力？”，既确保它能帮助劳动力满足他们的需求，并帮助他们的工作变得更容易，但不一定会威胁到工作。我们如何将系统集成到新的环境和设置中，这些环境和设置实际上对使用它们的人有用，并以这种方式集成不同的观点？</p><p>我认为这绝对是......这是一个一直在传播的术语，我们撰写本文的部分原因是试图理解该术语的所有不同使用方式，尽管它很大程度上是为了回应它的使用方式而编写的被实验室和媒体看到。</p><p><strong>丹尼尔·菲兰：</strong>对。这实际上符合我的想法。在本文中，您列出了这四个定义。这启发我思考，“好吧。我能想到“人工智能民主化”的其他含义吗？”听起来这些工会领导人正在谈论的一件事是人工智能的可定制性如何，或者任何人可以在多大程度上专门利用它来做自己的事情，而不仅仅是获得一刀切的交易？我想知道您是否对“民主化”一词的至少潜在用途有任何想法。</p><p> <strong>Elizabeth Seger：</strong>是的，所以我认为这是我们在“民主化使用”类别下提交的内容。我们在每个类别下尝试做的事情之一是表明不一定只有一种民主化使用的方法或一种民主化开发的方法。当我们谈论民主化使用时，实现这一目标的一种方法就是提供产品。就像说，“这里，使用 GPT-4。就这样吧。”它可用，易于访问。</p><p>但民主化使用的其他方法包括，例如，通过更直观的界面来使系统更容易访问。与模型一样，API 接口也更加直观。或者提供服务来帮助它更容易定制，正如你提到的，让人们......无论他们是对其进行微调以专注于特定需求，还是可能有通过插件的方法，例如，集成它与不同的服务、不同的应用程序相结合。或者甚至提供支持服务，帮助人们将您的产品集成到下游应用程序或不同的工作流中。这些都是有助于系统使用民主化的不同事物，我认为定制绝对是其中之一。</p><h3>人工智能民主化重要吗？<a name="is-it-important"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。现在我们对我们所谈论的民主化有了更好的认识，您在本文中提到了一个问题，但并不是最重要的，就是：民主化在这个问题上有多重要语境？因为对于大多数技术来说，我并没有过多考虑它们的民主化——空气除湿器、水瓶或其他东西。也许我是无情的，但对我来说，民主化并不是这些事情的首要规范优先事项。我们是否应该如此关心人工智能的民主化？</p><p><strong>伊丽莎白·西格：</strong>简短的回答是肯定的。我认为我们应该关心人工智能的民主化。你说的对。我们不谈论空气加湿器或水瓶的民主化，但我认为——这是在经常谈论人工智能时出现的一个问题，只是，它与其他技术有何不同？这是一个被问到的问题，无论你是在谈论它是如何共享的，还是在这种情况下，它是如何民主化的。</p><p>具体来说，关于民主化的讨论，让更多​​的人可以使用一些东西，让更多的人从设计过程中受益或为设计过程做出贡献，我认为这对于人工智能来说尤其重要。 (A)，因为如果人工智能有望像我们希望的那样成为一项变革性技术，这将极大地影响全球不同群体、不同国籍、不同地理环境的生活。确保我们可以从不同的群体中获得设计过程的投入，并了解最佳满足需求，这对于确保该技术不仅为许多人提供服务非常重要，而且为许多人提供服务，并受益于世界作为一个整体。</p><p>我认为对于利润民主化来说，这一点也特别重要。人工智能已经是，但可能仍然是一个利润极其丰厚的行业。我们可以开始看到大规模的领先实验室的利润累积，例如，我们可能会看到一家特定的公司能够按照世界总产出的总百分比或其他方式来衡量其总收入。这是巨大的经济利润，我们希望有一种方法来确保，理想情况下，每个人都从中受益，而不仅仅是堆积在硅谷等少数地理位置的几家公司中。</p><p>我们如何确保这是均匀分布的？有一些可以采用的直接方法。在本文讨论利润民主化的部分中，我们讨论的一些内容是您必须遵守<a href="https://www.fhi.ox.ac.uk/windfallclause/">意外之财条款</a>。假设一家公司拥有某种以占世界总产值的百分比来衡量的意外利润。在这种情况下，可能会承诺重新分配其中一些资金，或者我们可能会看到税收和重新分配计划。</p><p>您还可以考虑通过更间接的方式分配利润。例如，我们是否有办法让更多的人参与发展的民主化？更多的人参与产品开发，因此利润民主化和开发民主化之间存在重叠。更多的人可以为产品的开发做出贡献，那么这可能会挑战大型实验室可能形成的自然垄断。更多的竞争，更多的利润分配，更多的人参与其中。</p><p>所以，是的，我认为归根结底是，这项技术是否能够像它所承诺的那样规模庞大、具有变革性，让更多的人参与开发过程，并能够从这些系统产生的价值中受益，这确实是一件好事。重要的。</p><h3>民主化类型之间的联系<a name="links-between-types"></a></h3><p><strong>丹尼尔·菲兰：</strong>是的，我认为这是有道理的。事实上，这让我想起……正如你所提到的，我想，人工智能的发展和利润之间存在着某种联系，从某种意义上说，如果更多的人制造人工智能产品，那就意味着更多的人可以从中获利。在我看来，人工智能的使用和人工智能的发展之间也存在连续性，对吧？从某种意义上说，当你使用一个东西来完成一项任务时，你正在开发一种使用它来完成该任务的方法，对吧？那里有一条模糊的线。开发人工智能和管理人工智能之间也存在某种联系，就某种意义上来说，如果我开发某种人工智能产品，那么我现在实际上是管理它的制造方式和运作方式的主要人物——</p><p><strong>伊丽莎白·西格：</strong>是的，这就是我们现在所看到的。许多人工智能治理讨论都是围绕这样一个事实，即主要人工智能实验室正在对人工智能的未来做出重大、有影响力的决策。他们正在制定人工智能治理决策，因此谁应该真正做出这些[决策]是一个悬而未决的大问题。应该只是大公司的少数未经选举产生的技术领导者，还是应该更多地分配？所以，是的，类别之间肯定存在重叠。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。我认为你的论文的观点是这样的：“哦，这些是非常不同的。它们在概念上非常不同，你可以促进一种民主化，而不促进其他形式的民主化。”但就它们似乎确实存在这些联系和重叠而言，这是否可能只是水涨船高并购买一种民主化，让其他民主化的情况？</p><p><strong>伊丽莎白·西格：</strong>我认为有点，也许，有点，不是真的。上述所有的。不，所以某些类别之间肯定存在重叠。我认为它要么在本文中，要么在博客文章版本中，我认为我们会这样说：“这些并不是完全不同的类别。它们之间会有重叠。”我认为将其分类的部分用途是指出它有很多不同的含义。有很多不同的方法可以实现这些不同的意义和目标。</p><p>这不仅仅是开源模型或使其可供人们使用。 There&#39;s a lot more to it.我认为，在某些方面，有一种“水涨船高”的感觉。就像你说的，你可能会让更多的人参与开发过程。如果更多的人开发人工智能系统，就会有更多的人从中获利，这有助于利润分配。</p><p>另一方面，有时您也可以看到类别之间的直接冲突。例如，你可能……我认为这是我们在论文中写的内容：谈论民主化治理，以及有关人工智能的决策，无论是关于人工智能如何开发，还是如何做出关于如何共享模型的决策。</p><p>如今，围绕开源前沿人工智能模型存在着非常激烈的争论。假设您将围绕某些模型是否应该开源发布的决策过程民主化。假设这种民主化决策过程的结果是：“好吧。也许某些带来极高风险的模型不应该开源。”这是一个假设，但可以说这是协商、民主过程和决策的结果。这可能与民主化发展的利益直接冲突，而民主化发展……总是通过提供更好的模型获取途径来推进。</p><p>您可能会有一个使您将治理决定民主化的案例，但是该治理决定的结果是阻碍其他形式的民主化。我的意思是，不仅仅是发展。我想，你可以有一个民主程序，我不知道，比如，“大公司不应该被征税，也不应该重新分配利润。”这将与利润民主化的努力直接冲突，因此可能会发生冲突。我认为它通常介于治理类别和其他类别之间。</p><p>我想，治理通常会与特定目标冲突。这就是目的，调整决策并让更多人参与决策过程，并确保这些决策在民主上合法，并反映更广泛的受影响人群和利益相关者的需求和利益。</p><p><strong>丹尼尔·菲兰：</strong>好的，是的。这实际上提出了一个关于民主化使用/开发和民主化治理之间相互作用的问题。在论文中，正如您提到的……我想您提出的主要互动是民主化使用之间的紧张关系……这可能不一定是民主进程在治理方面所产生的。</p><p>当我想到......自从，我不知道，大约一年前，我们有了<a href="https://chat.openai.com/auth/login">ChatGPT</a> ，从某种意义上说，它确实民主化了，我想，大型语言模型的使用相对便宜，也许免费使用这些东西。一个影响是，现在至少不可能在一年前禁止 ChatGPT 之类的。但在我看来，这样做的一个重大影响是，增加人们对人工智能的了解，以及语言模型可以做哪些事情。</p><p>从本质上讲，我不知道这是否是一个术语，但你可以认为人们有治理需求，对吗？如果我对一项技术了解更多，我可能会说，“哦，这完全没问题。”或者，“哦，我们需要对此做这件事或那件事。”如果人们对一项技术了解得更多，他们基本上就能更好地了解他们想要的治理需求是什么。从这个意义上说，至少在某种程度上，民主化使用和民主化治理之间可以存在积极的相互作用。</p><p><strong>伊丽莎白·西格：</strong>是的，我想绝对如此。这是民主的一个经典支柱，那就是让公众知情。您需要获得信息才能就您的决策目标做出正确的决策。是的，所以我认为 ChatGPT 的发布确实让人工智能登上了公共舞台。突然，我有我的爸爸妈妈在发短信给我，问我有关AI的问题，这是一年前发生的。是的，所以我认为它确实将技术推上了舞台。已经有更多的人参与其中了。我认为更多的人至少对当前的能力能够做什么以及其局限性有一个普遍的认识。这对于实现更加民主的人工智能决策过程非常有价值。</p><p>民主化使用、民主化开发和人们更好地理解技术之间肯定存在联系。他们将能够就如何监管技术做出更明智的决策或形成更明智的意见。所以我认为这并不一定是紧张的。我猜，紧张局势往往会走向另一个方向。这就好像治理决策可能会说：“哦，这种特定的技术太危险了。”或者，“发布这个特定模型超出了可接受的风险阈值。”</p><p>我想，如果您严格将民主化的定义定义为将技术始终进一步传播，始终让更多的人参与开发过程，始终使系统易于访问，那么，限制访问的决定将与发展民主化。</p><p>我认为这是我们在论文中尝试说的另一件事：试图摆脱“人工智能民主化本质上是好的”这一想法。我认为这是民主化这个词的一个问题，尤其是在西方民主社会中：民主常常被用来代替一切美好的事物，所以你会让公司说，“哦，我们正在使人工智能民主化”。这意味着什么几乎不重要。他们民主洗了他们的产品。现在看来是一件好事。</p><p>我认为我们在论文中试图传达的一件事是，是的，人工智能民主化不一定是一件好事。传播一项技术，让更多的人为开发过程做出贡献，或者让更多的人使用，只有当这确实能让人们受益时，这才是好的。但如果它带来重大风险或者会让很多人受到伤害，那么这本质上并不是积极的。</p><p>我想，有一件事要说的是，如果人们说的“人工智能民主化”或“人工智能民主化”只是让系统更容易访问，那么就说“让系统更容易访问”，因为我我认为这意味着什么，这很清楚，而且它本身并没有好坏之分。我不知道我们是否会在一个播客的过程中成功地改变每个人的术语，但是-</p><p><strong>丹尼尔·菲兰：</strong>嗯，我们可以尝试一下。</p><p><strong>伊丽莎白·西格：</strong>我们可以尝试。我认为这也许也是这篇论文的一个关键要点。如果人们说他们正在努力使人工智能民主化，这并不一定意味着这将是一项净有益的努力。</p><h3>人工智能利润民主化<a name="democratizing-profits"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。我想，我想谈谈个人的感觉。特别是，我想从你谈论利润民主化开始。我注意到的一件事是，我认为您在本节中使用了一两个引用。我认为这些引言实际上是这样说的：“我们希望确保人工智能创造的所有价值并非都停留在一个地方，或者人工智能的好处不仅仅惠及一小群人。”</p><p>在这些引言中，他们实际上并没有使用“利润”一词。在某些方面，你可以想象这些可能会以不同的方式进行。以<a href="https://github.com/features/copilot">GitHub Copilot</a>为例。我猜，一年的费用是 100 美元。假设世界上每个人每年只需支付 100 美元，所有费用都归 GitHub 所有。很多人使用 GitHub Copilot 来做非常有用的事情，他们从中获得了很多价值和好处。也许大部分好处都是经济上的，但并非全部都是。在我看来，这似乎是传播人工智能的价值或好处的案例，但不一定是传播人工智能利润的案例。</p><p><strong>伊丽莎白·西格：</strong>是的，利润类别是一种……这是一个奇怪的类别。实际上，在撰写本文的过程中……与大多数论文不同，我们实际上首先写了<a href="https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation">博客文章</a>，然后写了论文。它应该朝另一个方向发展。在 GovAI 网站上的博客文章中，它实际上分为四类：使用民主化、发展民主化、福利民主化和治理民主化。</p><p>对于本文的版本，我们选择了利润术语而不是效益术语，因为效益在某种程度上太宽泛了，因为……我的意思是，您已经指出了这一点。这些类别之间有很多重叠，例如，实现发展民主化的原因之一是确保更多的人可以从技术中受益，并满足他们的需求。</p><p>使用民主化的一个原因是确保更多的人可以访问和使用，甚至使用系统来产生价值和利润。如果您可以将其与您自己的业务和应用程序集成，您就可以从您的角度开发它的价值。我认为我们选择这种利润民主化术语只是为了指出，围绕大型科技公司的巨额利润积累存在着讨论。</p><p>我认为，是的，您确实指出了我们使用的一些引言，更广泛地讨论了价值的累积。我认为，有时很难仅根据存入某人银行账户的实际资金来衡量利润。</p><p><strong>丹尼尔·菲兰：</strong>好的。</p><p> <strong>Elizabeth Seger：</strong>所以我认为……很难找到公司的准确报价。我知道有很多讨论不是由公司进行的，但实际上，这是你会听到政策制定者更多谈论的一件事。这是全民基本收入被广泛讨论的一部分。当人们更多地谈论它时，它就有了它的时刻。关于全民基本收入的很多讨论都是围绕……嗯，是围绕自动化造成的失业，但也围绕着科技公司和这些新技术开发商的应计利润。好吧，如果所有利润都将从劳动力转移到这些技术的开发人员身上，我们将如何重新分配呢？这实际上是利润民主化与围绕人工智能开发的讨论直接相关的一种背景。</p><p>是的，我认为我们在论文中使用的一些引言可能更模糊地表明了其价值。但我们将利润分开只是因为我们在博客文章中的方式有​​点太笼统了。重叠太多了。我们有点双重浸入，你知道吗？我认为这只是为了说明有专门关于利润的讨论，但我不认为……我的意思是，这绝对不是最常用的。如果你在街上听到有人谈论人工智能民主化，你不会立即想到，“哦，他们一定是在谈论利润再分配。”那不是你的大脑去的地方。但我认为，有一个值得注意的方面是主要的收获。</p><h3>人工智能治理民主化<a name="democratizing-gov"></a></h3><p><strong>丹尼尔·菲兰：</strong>很公平。是的，我想多谈的下一个话题是人工智能治理的民主化。与您刚才说的话有关，在我看来，如果有人谈论AI民主化，我认为如果他们只是说“使AI民主化”，他们意味着将治理民主化。我想知道这是否与您的经验相匹配。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，否。我认为，通常情况下，当您在媒体上看到，或者您正在听科技公司和他们的对话……是的，很少会谈论民主化治理。我认为我们开始看到转变。例如，Openai拥有了这个<a href="https://openai.com/blog/democratic-inputs-to-ai">AI民主化赠款计划</a>- 我不记得现在所谓的。基本上，他们正在向研究小组提供赠款，以研究基本上试图引入更多样化社区的意见的方法，以尝试更好地了解哪些价值观系统应保持一致。从这个意义上讲，您正在谈论AI民主化，以使人们进入决策过程以定义原则或定义AI一致性的价值观。 Openai拥有该赠款计划。</p><p>我认为拟人化只是发表了<a href="https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input">一篇博客文章</a>。 They&#39;d worked closely with <a href="https://cip.org/">the Collective Intelligence Project</a> that I mentioned earlier doing a similar thing for developing their <a href="https://arxiv.org/abs/2212.08073">constitutional AI</a> , defining what the constitutional principles are that AI systems should be aligned to, and that was more of a democratic governance process.因此，实际上，我认为我们开始看到这种“ AI民主化”的术语，“将AI民主化”渗入技术格局。我认为这是一件非常积极的事情。我认为这表明他们开始利用，并真正体现了民主化AI的民主原则。与仅意义分配和访问相反，它实际上意味着反映和代表利益相关者的利益和价值观和受影响人群的利益和价值观。</p><p>我认为我们开始看到这种转变，但是我认为您可能主要是在……再次，[如果在街上行走时，您会听到有人在谈论AI民主化，[他们]可能只是在谈论该技术的分配，使其更容易访问。这是经典的术语，但是我确实认为，在AI治理领域，甚至在实验室使用的术语中，我们也开始看到治理意思是渗入的，这令人兴奋。</p><h3>民主化的规范基础<a name="normative-underpinnings"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。我想在论文中谈到是否很好民主化基本上是根据治理方法的规范力量。因此，我想首先，这对我来说并不是那么明显。因此，我认为有时民主化，我认为它通常具有这种隐含的意义，即与民主作为一种政治决策方法更与平均主义相关。我认为您可以在没有民主决策的情况下拥有平等主义，而您可以在没有平等主义的情况下进行民主决策，对吗？人们可以投票赞成少数群体的权利或其他权利。所以，是的，我想知道你为什么这么说……我想我的意思是，请捍卫您的说法，即这是民主或民主化的善良的来源。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。好的。所以几件事，三件事，也许是两件事。我们将看看它是如何出现的。因此，首先，我确实坚持这样一种观念，即民主化的前三种形式，因此发展，使用，利润，[]不一定本质上是善良的。本质上不是很糟糕的，只是这些事情是发生的事情，如果我们坚持他们的定义，那就意味着使某些东西更容易访问，无论是使用或开发或使利润更容易获得。访问本质上不是好是坏。同样，如果我们使用“开发访问”示例，则如果您有一种可能确实很危险的技术，那么您不一定希望每个人都可以访问它。民主决策过程可能会得出这样的结论：确实并非每个人都应该使用它。</p><p>这就是第一部分：站在索赔上半年。第二部分，围绕赋予其道德力量或价值的事物是涉及的民主决策过程……我不知道这是否正是我们在论文中的目标。我认为您可以使用许多不同的方法来帮助反映和服务更广泛的人群的利益。我认为对于某些技术……让我们回到您的水瓶示例。我们不需要来自全球的人来告诉我们如何分发水瓶。这是一个决定，可能是水瓶分销商可以说的：“让我们将水瓶卖给尽可能多的人”，而且[这]可能还不错。</p><p>我认为这是影响到影响力的位置，影响将有多大的影响，而技术的负面影响可能会更加不成比例？在这种情况下，能够带来这些声音以告知可能对他们不成比例影响它们的决策过程非常重要。我知道我们在各种民主进程的论文中举例说明了这些过程，这些过程可用于为决策提供信息，无论这些过程是参与过程还是更审议的民主进程。因此，在我们刚刚发布的<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">开源文件</a>中，我们谈论了其中一些过程，作为在AI围绕AI做出更民主决定的方法，但随后也指出：民主化治理决策的一种方法是支持民主治理提出的监管。这些政府有望在结构良好的情况下，反映和服务选民的利益。</p><p>希望其中的一些治理过程实际上是通过考虑了一种考虑利益相关者利益和受影响人群的利益的某种审议过程来形成的。但这并不是说管理AI的唯一方法是说：“好吧，让我们有一个参与式小组，我们可以在其中掌握所有这些见解，然后使用它来告知每个决定”。我认为您是对的，这是完全不切实际的，因为所有决定由参与式小组或某种审议的民主进程告知实验室。因此，我认为您必须根据问题的潜在影响来进行范围。因此，我不知道这是否完全回答了您的问题，但是我想我坚持第一点，民主化并不是天生的好处。我猜想的善良程度反映了决策的能力如何反映和服务将受到影响的人们的利益。然后有许多不同的方法来弄清楚如何做到这一点。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，这很有意义。因此，我想我要问的第二句话是：当您说使用，发展和利益的民主化并不是固有的好处时，这听起来像您认为治理的民主化本质上是好的。因此，我发现您实际上引用的论文，这是在2023年出版的Himmelreich的<a href="https://johanneshimmelreich.net/papers/against-democratizing-AI.pdf">“民主化AI”</a> 。因此，他基本上做出了一些批评。首先，他只是说AI不是需要民主的事情。人工智能本身不会影响一群人。有些组织只使用AI，它们会影响一群人，也许他们应该被民主化，但AI本身并不一定。</p><p>他说，相关的事情已经是民主的。民主是非常密集的。人们必须知道一堆东西，也许您必须接受一堆票和事情。他还抱怨民主是不完美的，不一定会解决压迫。因此，有一些关于民主化AI浮动的抱怨。我想在这种情况下，他正在谈论使AI治理民主化。民主的AI治理本质上是好的，是这种情况吗？</p><p> <strong>Elizabeth Seger:</strong> I don&#39;t think so.同样，我认为这又回到了“范围”问题。首先，这是资源密集的观点：绝对。进行深入的参与式治理过程可能是非常密集的。这就是为什么您不为那里的每个问题都这样做的原因。是的，这真的很好。您还需要明智地做出明智的决定才能做出正确的决定。因此，也许我们不希望公众对AI的所有问题做出决定。专家们为自己试图确定什么构成高风险系统的时间足够艰难。在AI专家社区中，我们甚至不擅长基准这一点。您将如何利用更普遍的公众观念，即高风险系统是什么？</p><p>因此，有些问题使它民主化并没有意义。因此，我认为您必须考虑：在周围进行更民主化的讨论是什么现实的？因此，也许类似于思考可接受的风险阈值是什么，或者要与哪种价值保持一致。我认为也很重要的是要记住有很多不同种类的民主进程。当我们谈论民主进程时，这不一定是您可能想到的：我们都参加民意调查并投票“我们想要A或想要B？”这些可能是更加审议的过程，您只是在房间里得到一些利益相关者，他们进行了讨论，他们试图弄清楚这些讨论中的主要问题是什么。因此，有一些程序可以进行审议的民主进程，您有专家进来并尝试为讨论提供信息。因此，这将是一个更明智的审议过程。</p><p>一种方法是：有时在……与图像生成系统有关的情况下，围绕偏见和图像进行了大量讨论。同样，我们看到治理与其他类别之间的重叠。如果您让更多的人使用这些系统参与其中，并且他们开始了解偏见在哪里，那是一种教育的一种形式 - 更熟悉。然后，您可以提出这些投诉，无论是直接向公司还是通过某种政治中介机构。我记得那是什么？我认为<a href="https://en.wikipedia.org/wiki/Anna_Eshoo">安娜·埃胡（Anna Eshoo）</a>发表的一份<a href="https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices">声明</a>是国会代表，我想说的是南圣何塞（South San Jose），他对此进行了很多深入讨论……这是在发布<a href="https://stablediffusionweb.com/">稳定扩散</a>并谈论如何有一个之后在某些正在制作的图像，特别是针对亚洲妇女的图像中，许多种族偏见。</p><p>因此，这是通过民主进程提出的，引起了国会办公室的注意，然后发表了声明，然后对实验室的未来治理决策和决策产生了连锁反应的影响像放置安全过滤器一样。因此，我认为这是要牢记的一件事：就民主化治理过程的含义而言，您可以考虑各种各样的事情。治理只是决策。那么，您如何确保那些决策过程反映将会受到影响的人们的利益。这不仅仅是直接参与，尽管还有很多工作要为更小规模的决策带入更加逼真的决策，以使其更现实。</p><p>像这样的抱怨，关于它是如此的资源密集型：是的。我们还拥有可用的技术来帮助使其减少资源密集型，我们如何利用这些技术来实际帮助公众以明智的方式对这些决策进行投入？因此，正在进行许多有趣的工作。在AI治理方面，民主治理也不是天生的。我认为，这取决于这个问题。如果进行成熟民主进程的成本将远远超过进行成熟民主进程的好处，那么进行民主决策过程可能并不是净收益。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>这是对希梅尔里希（Himmelreich）资源密集型投诉的。他还抱怨说，基本上AI本身并不是需要民主管理的事情。因此，他说的话，我会看看我是否可以记住，但是需要民主管理的事情可能只是影响人们生活的某些事情，无论他们是否喜欢，或者只是大规模固有的东西合作之类的东西。他基本上提出了这一说法，即AI不做那些事情。 AI被安置在做这些事情的机构中。因此，如果您担心，也许警察使用面部识别或其他东西，您应该担心警察的民主问责制，而不是面部认可，这是我要提出的重点。</p><p>因此，他说，首先，看，我们不需要民主化AI。我们需要民主化潜在使用AI的事物。其次，似乎他在说使用AI的相关内容，它们已经是民主的，所以我们不应该矛盾……我认为他谈论民主重叠之类的东西，我认为他真的不喜欢这些不同的民主机构试图影响同一决定，也许它们冲突之类的想法。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>所以我认为我不同意。我认为，如果我正确理解的是，“您不需要民主化AI，您需要民主化使用它的机构”，我认为这是一个完全正确的观点。同样，人工智能本身并不是天生的坏事。这是双重使用技术。这取决于您要处理的事情。但是我要说的是，例如，试图决定执法部门应如何使用AI系统，这是AI治理问题。这是一个关于如何使用AI，在这种情况下如何调节AI的问题，而不是在其开发方式上，而是在其使用方式上。</p><p> What are the appropriate use cases of AI?这是一个AI治理问题，不一定是这些法规专门放在AI上。许多AI法规已经存在于不同机构中。它只是在弄清楚该法规适用于AI系统。这可能是这样的情况：如果您对某些健康和安全标准有要求，而某些技术必须在工作环境中满足某些技术，则不应将AI遵守不同的标准。这只是弄清楚如何测量并确保AI系统达到这些标准的问题。因此，我想根据您所说的话，我想说我完全同意。</p><p>但是我要说的是，它仍然属于使AI治理民主化的保护。我认为这里可能发生的事情只是术语意味着什么的另一个冲突，就像“我们不需要民主化AI，因为我们不需要让更多的人直接参与发展和决定 - 关于AI开发的个人决策的制定”。在这种情况下，是的，我同意！但是，我们可能正在谈论在不同的AI治理中民主化……这就是为什么这些讨论变得真正复杂的原因，因为我们最终互相谈论彼此，因为我们使用相同的词意味着五个不同的事情。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的，可能很粗糙。因此，在我们关闭有关AI民主化的讨论之前，我想知道……我想您已经考虑过将AI民主化。您是否有任何最喜欢的干预措施使AI更民主或使AI在任何相关意义上变得不那么民主？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我将完全诚实，这是我的专业知识从可用的过程中更加贴切的，以及某些过程中确切地应用的最佳过程中的最佳范围。为此，我真的会更仔细地研究<a href="https://cip.org/">集体情报项目</a>所做的工作。或者我的合着者<a href="https://aviv.me/">Aviv Ovadya</a> （在纸上）确实参与了这些讨论，并与各种实验室合作，以帮助实施不同的民主进程。是的，我只是要说的是，这是我的专业知识下降的地方，我会指向同事，对这项工作进行更深入的讨论。</p><h2>开源人工智能<a name="osai"></a></h2><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好吧，我要谈的下一件事基本上是开源的AI。因此，有一份报告，<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">开源高度强大的基础模型：评估追求开源目标的风险，收益和替代方法</a>。它是自己和其他一群合着者，但据我所知，在<a href="https://www.governance.ai/">AI治理中心</a>。同样，您是否可以概述本报告中发生的情况？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>好的，是的。因此，该报告很难概述，因为与八页的民主化论文不同，这本书约为60页。</p><p>如此快速的概述。首先，我们想指出，目前围绕大型基础模型，尤其是Frontier Foundation Models是开源的辩论。这是一场辩论，从<a href="https://stability.ai/">稳定性AI</a>的<a href="https://stablediffusionweb.com/">稳定扩散</a>开始，然后是<a href="https://ai.meta.com/llama/">Llama 2</a>的发布，然后重量泄漏了，现在Meta落后于这个开源的东西。<a href="https://spectrum.ieee.org/meta-ai">元外面有关于系统是否应该是开源的抗议活动</a>。然后，随着<a href="https://artificialintelligenceact.eu/">欧盟AI法案</a>的发展，我们也看到了很多活动。作为欧盟AI法案的一部分，一些政党正在推动对开发模型的群体的豁免。因此，关于开源的讨论很多。我认为本文的目的是削减即将成为一场两极分化的辩论。</p><p>我们发现您有人们看到开源的好处，并且只是非常非常顽固的Pro开源，然后再也不会听到有关潜在危险的任何论点。然后，您有一个非常反开源的人，只想谈论危险，并拒绝看到好处。这确实使在这个问题上进行任何连贯的对话真的很难。 Yet at the same time, country governments are trying to make very real policy around model sharing.因此，我们如何削减这一两极分化的辩论，以尝试说，这是土地的境地。因此，我们在本文中要做的事情确实提供了一个均衡，经过充分研究的概述，概述了开源高度强大的模型的风险和好处。</p><p>通过“高功能模型”，基本上我们正在谈论的是Frontier AI系统。我们只是不使用“边境”一词，因为边境会移动，但是我们想清楚地表明，有些系统可能具有足够的能力，并且构成高度的风险，即使它们不在边境，我们仍然应该谨慎开放给它们。因此，基本上我们正在谈论边境模型，但是Frontier一直在继续。因此，我们可以稍后再进行术语辩论。但是基本上，我们想概述开源这些越来越强大的模型的风险和好处是什么，但随后不仅要拥有另一篇文章，还说，这是风险，这里是好处。但是，然后说：“也许还有其他方法可以尝试获得开源的一些好处，但风险较小。”</p><p>因此，这里的想法实际上与民主化论文非常相似：不仅要说“什么是开源？”和“好还是坏？”但是要说：“开源的具体目标是什么？人们为什么说开源是好的，这是我们应该做的应该捍卫的事情，应该保留？那么，为什么我们要开源AI模型？那么，如果我们可以具体说明为什么要开源，那么我们是否还可以采用其他方法来实现这些相同的目标，以较小的风险？”这些可能是其他模型共享方法，例如在API背后发布模型或进行分阶段发布，或者可能是其他更积极的措施，例如：开源的一个好处是，它可以帮助加速研究进度，既可以在开发更多方面发展更多AI功能更大，但也促进了AI安全研究。</p><p>您可以促进AI安全研究的另一种方法是将一定比例的利润用于AI安全研究或进行研究合作。您可以做这些事情，而无需为任何人提供访问模型的访问权限。 So this is what we were trying to do with this paper: really just say, okay, we&#39;re going to give, first of all, just a very well-researched overview of both the risks and benefits.实际上，本文的大部分实际上都集中在什么是好处，并试图分解开源的好处，然后真正深入研究替代方法或其他可以做的事情，以帮助追求这些好处，而在风险危险的情况下可能太高了。</p><p>我认为我们在本文中提出的主要主张是“开源的好坏？”首先，这是一个错误的二分法，但我认为我们的主要陈述是绝大多数。软件的开源开发，开源软件是我们今天使用的所有技术。这非常重要。这对于技术发展非常有用，对于反映许多不同用户需求和兴趣的安全技术的开发。好东西。问题在于，可能会有一些尖端，功能强大的AI系统，会构成恶意使用的风险，危险能力的扩散，甚至危害下游应用程序的危害和脆弱性。</p><p>如果这些风险足够极端，我们需要退后一步说：也许我们应该有一些程序来决定开源这些系统是否还可以。因此，我认为该报告的主要目的是，开源是总体上的好处，可能只是在某些情况下，这不是最好的决定，在这种情况下，我们需要谨慎。是的，我认为这是论文的主要概述。</p><p><strong>丹尼尔·菲兰：</strong>明白了。是的，如果听众被60件页面推迟，我希望听众知道有一个执行摘要。这很可读。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>这是两页的执行摘要。不用担心。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。我阅读了这篇论文，并保证了执行摘要是对此的合理摘要。因此，听众不要害怕。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我也一直推迟写博客文章，所以我保证在某个时候会有一个博客文章。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。因此，可能会在我们发布此消息时完成，在这种情况下，我们将链接博客文章。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>也许是圣诞节。</p><h3>开源的风险<a name="risks-from-os"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。希望您会有圣诞节礼物，听众。因此，当我们谈论这些高度有能力的基础模型，并基本上谈论开放源代码的风险……我想我们已经进入了，这是一个高度强大的基础模型还不清楚。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。已经很难了。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>也许更容易说，我们在说什么风险？因为我认为一旦我们知道了各种风险，（a）我们知道我们应该愿意做什么样的事情来避免这些风险或我们应该想到的事情，而且（b）我们可以说，好吧，好吧，好吧，我们只是担心AI可能会导致这些风险。那么有什么风险？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>实际上，我认为以风险为基础，确实有帮助。因此，通常我们在这里考虑遭受重大伤害，重大社会甚至身体伤害甚至经济伤害的风险。当然，您说：“哦，定义&#39;重要&#39;”。但基本上只是灾难性，极端，重大的社会风险和危害。因此，我们在论文中使用一些例子，以研究恶意使用的潜力。</p><p>如果您正在考虑政治影响力的行动，还有更多的弥漫性危害。因此，这种属于错误信息/虚假信息讨论。 AI系统如何基本影响政治运动？因此，虚假信息破坏了信任和政治领导人。然后，如果您可以破坏信息生态系统并分解我们交换信息的过程，甚至只是对关键信息来源的信任进行分解，这也可能影响我们作为社会对危机等社会做出反应的能力。</p><p>因此，大流行是一个很好的例子，如果真的很难获得人们的准确信息，例如，戴面膜是否有效，那么您将不会在戴面具上进行真正的协调决策。因此，我认为这是一个散布的地方。很难测量。因此，我认为这是很难确定何时发生危害的一点，因为它是如此扩散。但是我认为这是一个潜在的重大伤害。我会说也许考虑破坏重大的政治选举或类似的事情。</p><p>我认为，有一些恶意使用的选择会更频繁地谈论，因为它们更明确，更容易缠绕您的头。这些是使用生成的AI生产生物武器或毒素，甚至生产恶意软件来对关键关键基础设施进行网络攻击。想象一下，拆除电网，或者在选举日想象一下选举制度。这些可能在对社会或身体伤害的伤害方面产生重大的社会影响。</p><p>我认为这里要指出的关键是我们不一定已经看到这些功能了。有些人可能会不同意，但我认为我的意见是目前尚不存在具有这种危害的潜力的技术。我认为明智的做法是假设它们将在不久的将来成立，这主要是因为我们看到了这类能力的迹象。我们<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/">已经看到了在药物发现中使用的狭窄AI系统，如果您将应该优化的无毒性优化以优化毒性的参数，那么现在您有了毒素发生器。</a>因此，不需要大量的想象力就能看到如何使用更有能力的系统来造成非常重大的社会伤害。所以不，我认为目前没有这样做的系统。我认为我们需要为确实存在这些系统的未来做好准备。因此，即使我们目前可能不使用该技术，现在也值得考虑发布这些系统的智慧。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>因此，作为一个后续问题：您目前正在AI X-风险研究播客中。我想知道……因此，似乎您主要在想的事情比所有垂死的人都不那么严重，或者只是永久阻碍人类的轨迹。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我不一定会说。我认为这绝对是一种可能性。我认为正在发生的部分原因是您必须如何谈论这些事情。我担心AI的X风险，我认为我们可以拥有系统的系统，无论是网络能力还是生物恐怖能力，甚至是夺走政治制度的能力和加强专制政府 - 这些都是可能导致存在风险和存在的风险和I am personally worried about this.但是[我们]试图写论文，这些论文可能会被越来越多的人群挖掘出来，这些论文可能不会被完全被纳入X风险的论点中。我认为，除了X风险之外，仍然存在非常真实的论点，因为它不一定要发布模型，因为即使您认为这些危害也可能造成的危害，也可能会造成模型。即使这只是灾难性的伤害，也可能不是一个好主意。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>至少，那里有一个范围。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。</p><h3>我们是否应该让人工智能变得太危险而不能开源？<a name="make-ai-too-dangerous-os"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>我想到的一个问题，尤其是当思考AI可能引起这类风险和潜在开源的AI时……我的一部分在想，似乎开源的许多风险似乎似乎好吧，现在有更多的人可以使用这种AI系统造成一堆伤害。危害是如此之大，以至于这确实令人恐惧和危险。所以我想知道的一件事是：如果许多人拥有这种AI技术是如此危险，那么任何人都有这种AI技术也不是危险的吗？这个区域有意义地进行AI而不是开源的区域有意义？还是这个区域甚至存在？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，您提出了一个非常真实的观点，有些人会说我们现在应该点击大旧红色停止按钮。我认为这是某些人提出的论点。我猜想我们从本文中来自哪里，试图对AI开发可能会发生的方式进行现实，并试图围绕我们在AI开发中应采取的措施来告知治理决策。我认为对此有不同的看法，甚至可能在本文的作者中。有什么，纸上有25，26位作者。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>听众，本文有一个免责声明，并不是所有作者都必须认可本文中的所有主张。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>包括我。因此，我们有一群非常广泛的作者。但是我认为……抱歉，您最初的问题是什么？它只是飞出了我的头。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>如果开源太危险了，那么制造也太危险了吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，不。因此，我认为这肯定有一个论点。我猜我来自哪里，试图对这种发展过程的发展以及如何告知政府作为AI开发的工作仍在继续。我们很可能会达到每个人都相信我们在AI开发中的协调暂停/停止的地步。但是，假设这是不可能或不可能的，我们应该说，制定应急计划仍然是明智的。我们如何指导AI开发是安全的，在很大程度上有益并降低风险的潜力？我认为也有一个论点是，越来越有能力的系统虽然构成越来越严重的风险，但也可以为帮助解决我们面临的其他挑战和危机提供越来越大的好处和潜在的经济潜在利益。</p><p>因此，我认为您很难得到巨大的协调停顿。因此，下一个计划是我们如何安全地进行AI开发？如果超级危险的人没有广泛传播，则确保人们安全要容易得多。我想那是非常简单的观点。是的。</p><p>因此，我认为至少对我来说，我认为这只是一个现实主义，我们是否可能在超级可怕之前暂停AI的发展？可能不是，只有人类的运作方式。我们开发了核武器，可能不应该这样做。糟糕，现在它们存在了，让我们解决这个问题。因此，我认为制定类似的计划，以实现越来越有能力的AI系统，特别是考虑到它可能不会那么遥远。就像我说的那样，每一个新一代的AI都会释放出来，我们都屏住呼吸，说，哦，那是什么呢？那要做什么？然后我们从中学习，我们不知道下一代AI系统将带来什么。因此，拥有适当的系统来扫描潜在的危害，潜在的危险，能力，以告知有关这些系统是否应释放以及是否应该分享这些系统的决定，这确实很重要。在大恐怖发生之前，我们可能无法协调暂停。因此，我认为无论如何讨论这一点很重要。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>我得到了你。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>顺便说一句，这是一个技术术语，这是一个大的恐怖。</p><h3>攻防平衡<a name="offense-defense"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>很有意义。因此，说到开源人工智能的风险，在论文中，您谈论了犯罪余额，基本上，您可以说坏演员可以禁用滥用的保障措施，他们可以通过微调来增加新的危险能力。开源使这更容易，它增加了攻击者的知识。 And in terms of just the AI technology, you basically make a claim that it&#39;s tilted towards offense in that attackers can do… They get more knowledge, they can disable safeguards, they can introduce new dangerous capabilities.找到这些问题比修复它们要容易。而且，一旦修复它们，就很难确保每个人都有修复程序。你会说这是一个公平的摘要 - </p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我想这很公平。我认为我会补充的一件事……例如，我会说，随着软件开发……因此，犯罪防御余额通常是从开源和科学出版物方面进行讨论的，尤其是在任何双重使用时可以用来造成伤害的技术或科学见解，您必须解决这种犯罪的防御平衡。是否要发布的信息将帮助坏演员做坏事或更少的事情，而不是帮助好演员做好事/防止坏演员做坏事？而且我认为，随着软件开发，它通常支持防御，找到漏洞，修复错误，推出修复程序并使技术更好，更安全，更强大。这些是真正的论点，即为何开源AI系统也很有价值。</p><p>但是我认为，特别是随着更大，更复杂的模型，我们开始转向进攻平衡。我只想强调，我认为这样做的主要原因之一与修复程序的困难有关。因此，就软件而言，您会得到相对定义明确的错误和漏洞。找到它们后，相对易于修复，请推出修复程序。我们在强大的AI系统中面临的安全挑战非常复杂。我们在世界各地有庞大的研究团队试图弄清楚它们，这是一个非常有资源的过程，需要很多才能。漏洞仍然很容易找到，它们很难解决。因此，我认为这是为什么进攻防御余额可能更偏向进攻并使恶意演员的主要原因，因为您仍然可以找到脆弱性，您仍然可以操纵漏洞，利用它们的优势……即使很难解决，即使您有更多的人参与其中。因此，这就是高级评估。通常，我只是想更加努力地解决安全问题。</p><h3> KataGo 作为案例研究<a name="katago-as-case-study"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。因此，这实际上给我带来的一件事是……您可能会或可能不熟悉，因此AI可以玩棋盘游戏。有<a href="https://katagotraining.org/">开源训练跑步</a>，我的一些同事基本上发现<a href="https://goattack.far.ai/">您可以廉价地找到对抗性攻击</a>。因此，基本上是愚蠢的GO机器人，以使这些计算机AI政策混淆的方式播放。而且这些攻击通常并不聪明，但是它们只是按下AI政策上的正确按钮。而且我想这是一个评论，而不是一个问题，但是这些作者和制造这些开源的人之间有足够多的回合，这可能只是将其用作犯罪的案例研究，这可能很有趣。国防平衡。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>因此，您是说，鉴于系统是开源的，人们可以使用它并查询它，然后将信息发送回开发人员，这就是 -</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的，实际上，这些作者一直在与该软件的开发人员合作，实际上发生的是该软件的开发人员 - 卡塔哥，如果人们想搜索谷歌 - 他们已经看到了本文，他们&#39;试图实施补丁以解决这些问题。发现对抗性政策的人们基本上是在检查修复程序是否有效并发布有关它们是否有效的信息。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>绝对，我想知道这是开源开发的巨大好处：让人们参与开发过程开发商。这是软件和AI系统开源开发的巨大好处。我认为这是为什么本文重点关注大型，强大的边界粉底模型的原因是，这变得越来越困难，系统越大，更复杂，最前沿。有些错误仍然相对较小，定义明确，并且还有错误的赏金程序，也有针对AI安全赏金计划的建议，有助于找到这些漏洞并将信息还给开发人员。我认为有一些问题，关于一些更大的安全问题。有时，首先很难确定安全问题，更难解决安全问题。</p><p>然后，还有一个问题，即仅将修复程序推广到系统。因此，软件开发，您可以修复一堆错误，然后推出更新。通常，在许可证中，它会说您应该实际使用更新的版本。有一些数据出现了，我不记得现在是哪个组织，我以后必须查找，但实际上，实际运行最新软件的人的吸收率很低。因此，首先，即使使用普通软件，也很难确保人们实际上将运行最新版本并推出这些修复程序，这是开源的问题API，然后您只需更新系统，然后每个人都使用更新的系统。如果您有开源系统，那么人们实际上必须自己下载并运行更新版本。</p><p> With foundation models, there&#39;s actually a weird incentive structure that changes where people might actually be de-incentivized to update.因此，使用软件，通常会进行更新时，它可以修复一些错误并改善系统功能。当涉及基础模型的安全修复时，它通常与降低系统功能有关，例如放上一个说：“好吧，现在您无法制作此类图像。现在，您无法使用系统执行此类功能，”因此，很难，我不知道现在是否有很好的信息。我们是否看到AI系统更新的吸收率降低？我不知道，但是激励结构也可能有一些奇怪的事情，如果基本更新基本上等同于以某些方式降低系统功能，那么人们实际上可能不太可能将它们带到船上。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>我对此没有很好的感觉。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我没有超级好的感觉，但是我不知道，有趣的食物，不正当的激励结构。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>我不知道，我仍在考虑这个卡塔哥案件：因此，攻击确实会降低系统功能，并且人们有兴趣使用修复程序获得最新版本。在我看来，……因此，实际上，本文的结构，他们发现攻击的方式并不依赖于访问模型权重。它基本上依赖于查询GO Bot策略，基本上是尝试尝试一堆事情，并弄清楚如何欺骗Go Bot策略。现在，如果您可以在本地拥有权重，以便您可以调用API，这样您就可以将其称为很多，但这并不是您需要分享实际权重的情况，这确实很有帮助。因此，一方面，分享权重的观点比您想象的要比您想象的要比您的价值低，但这也暗示您是否担心人们会发现这些对抗性攻击，然后将权重放在API后面并不能保护您正如你所想的那样。也许您需要对限制等级。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我认为这是一个有价值的见解，您肯定可以做一些事情而无需举重。这是一个论点说明为什么您还是应该担心的，但这也是一个论点……有很多争论，开源很重要，因为您需要它来进行安全研究，让更多参与安全研究的人会导致系统更安全，您有更多的人对这些过程有更多的投入，但是您只是说明了一个完美的例子，说明如何仅查询访问，可以使您可以在查找方面进行大量的安全研究漏洞。</p><h3>可解释性研究的开放性<a name="open-for-interp"></a></h3><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>因此，查询访问是可以完全在API后面完成的访问，但是即使我们考虑了诸如可解释性研究之类的事情，可解释性研究确实需要更深入地访问系统，但是可以说这是需要访问较小系统的论点。我们正在努力对较小的，定义明确的系统进行可解释性研究。这就像可解释性研究的速率限制因素不是人们所能使用的模型的大小，而是我至少理解的方式。 If we&#39;re struggling to do interpretability research on smaller models, I feel like having access to the biggest, most cutting edge frontier model is not what needs to happen to drive interpretability research.</p><p> <strong>Daniel Filan:</strong> I think it depends on the kind of interpretability research.</p><p> <strong>Elizabeth Seger:</strong> It&#39;d be interesting to hear your thoughts on this as well, but there&#39;s a range of different kinds of AI research. Not all of it requires open access and then some of the kinds that do require open access to the models isn&#39;t necessarily helped the most by having the open access, and then there is also this idea of alternative approaches that we talk about in the纸。 You can help promote AI safety research by providing access to specific research groups. There are other things you can do to give people the access they need to do safety research.</p><p> <strong>Daniel Filan:</strong> So, I guess I can share my impressions here. Interpretability research is a broad bucket. It describes a few different things. I think there are some kinds of things where you want to start small and we haven&#39;t progressed that far beyond small. So just understanding, &#39;can we just exhaustively understand how a certain neural net works?&#39; - start small, don&#39;t start with GPT-4. But I think one thing you&#39;re potentially interested in, in the interpretability context, is how things get different as you get bigger models, or do bigger models learn different things or do they learn more? What sorts of things start getting represented? Can we use interpretability to predict these shifts? There you do want bigger models. In terms of how much can you do without access to weights, there&#39;s definitely a lot of interpretability work on these open source models because people apparently really do value having the weights.</p><p> Even in the case of the adversarial policies work I was just talking about, you don&#39;t strictly need access to the weights, but if you could run the games of Go purely on your computer, rather than calling the API, waiting for your request to be sent across the internet, and the move to be sent back, and doing that a billion times or I don&#39;t know the actual number, but it seems like just practically-</p><p> <strong>Elizabeth Seger:</strong> Much more efficient.</p><p> <strong>Daniel Filan:</strong> … It&#39;s easier to have the model. I also think that there are intermediate things. So one thing the paper talks about, and I guess your colleague <a href="https://sites.google.com/view/tobyshevlane">Toby Shevlane</a> has <a href="https://arxiv.org/abs/2201.05159">talked about</a> is basically structured access of giving certain kinds of information available maybe to certain people or maybe you just say “these types of information are available, these types aren&#39;t”. I&#39;ve heard colleagues say, “Even if you didn&#39;t open source GPT-4 or GPT-3, just providing final layer activations or certain kinds of gradients could be useful”, which would not… I don&#39;t think that would provide all the dangers or all the risks that open-sourcing could potentially involve.</p><p> <strong>Elizabeth Seger:</strong> I think this is a really key point as well, is trying to get past this open versus closed dichotomy. Just saying that something isn&#39;t open source doesn&#39;t necessarily mean that it&#39;s completely closed and no-one can access it. So like you said, Toby Shevlane talks about structured access, and it was a paper we referenced - at least when we referenced it, it was still forthcoming, it might be out now - but it was Toby Shevlane and Ben Bucknell were working on it, and it was about the potential of developing research APIs. So, how much access can you provide behind API to enable safety research and what kind of access would that need to look like and how could those research APIs be regulated and who by? So, I think if there&#39;s a genuine interest in promoting AI safety research and a genuine acknowledgement of the risks of open-sourcing, we could put a lot of resources into trying to develop and understand ways to get a lot of the benefits to safety research that open-sourcing would have by alternative means.</p><p> It won&#39;t be perfect. By definition, it&#39;s not completely open, but if we take the risks seriously, I think it&#39;s definitely worth looking into these alternative model sharing methods and then also into the other kinds of proactive activities we can engage in to help promote safety research, whether that&#39;s committing a certain amount of funds to safety research or developing international safety research organizations and collaborative efforts. I know one issue that always comes up when talking about, “Well, we&#39;ll just provide safety research access through API, or we&#39;ll provide privileged downloaded access to certain groups.” It&#39;s like, “Well, who gets to decide who has access? Who gets to do the safety research?”</p><p> And so, I think this points to a need to have some sort of a multi-stakeholder governance body to mediate these decisions around who gets access to do the research, whether you&#39;re talking about academic labs or other private labs, sort of like [how] you have multi-stakeholder organizations decide how to distribute grants to do environmental research, or you have grantmaking bodies that distribute grant funds to different academic groups. You could have a similar type situation for distributing access to more highly capable, potentially dangerous systems, to academic groups, research groups, safety research institutions that meet certain standards and that can help further this research.</p><p> So, I feel like if there&#39;s a will to drive safety research forward, and if varying degrees of access are needed to allow the safety research to happen, there are things we can do to make it happen that do not necessarily require open-sourcing a系统。 And I think, like we said, different kinds of safety research require varying degrees of access. It&#39;s not like all safety research can be done with little access. No, you need different amounts of access for different kinds of safety research, but if there&#39;s a will, there&#39;s a way.</p><h3>开源替代品的有效性<a name="os-substitutes"></a></h3><p><strong>Daniel Filan:</strong> So, I want to ask something a bit more quantitative about that. So, some of the benefits of open-sourcing can be gained by halfway measures or by structured access or pursuing tons of collaborations, but as you mentioned, it&#39;s not going to be the same as if it were open sourced. Do you have a sense of… I guess it&#39;s going to depend on how constrained you are by safety, but how much of the benefits of open source do you think you can get with these more limited sharing methods?</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a good question. I think you can get quite a bit, and I think, again, it sort of depends what kind of benefit you&#39;re talking about. So in the paper, I think we discuss three different benefits. Let&#39;s say we talk about accelerating AI research, so that&#39;s safety research and capability research. We talk about distributing influence over AI systems, and this ranges everything from who gets to control the systems, who gets to make governance decisions about the systems, who gets to profit. It wraps all the democratization themes together under distributing influence over AI, and then, let&#39;s see, what was the other one that we talked about? You&#39;d think I&#39;ve talked about this paper enough in the last three months, I have it down. Oh, external model evaluation. So, enabling external oversight and evaluation, and I think it depends which one you&#39;re talking about.</p><p> Let&#39;s start with external model evaluation. I think that this probably benefits the most from open-sourcing. It depends what you&#39;re looking at, so for example, if you&#39;re just looking for minor bugs and stuff like that, you don&#39;t need open source access for that, but having a more in-depth view to the systems is more important for people trying to help find fixes to the bugs.我们已经讨论过这个问题。 There are also risks associated with open-sourcing. If we&#39;re talking about accelerating capability research, for example, which sort of falls under the second category, I think you might find that the benefits of open-sourcing here might be somewhat limited the larger and more highly capable the system gets. And I think this largely will just have to do with who has access to the necessary resources to really operate on the cutting edge of research and development. Open source development, it operates behind the frontier right now largely because of restrictions… not restrictions, but just the expense of the necessary compute resources.</p><p> And then you talk about distributing control over AI, we&#39;ve already discussed the more distributed effect of open-sourcing and model sharing on distributing control. It&#39;s a second order effect: you get more people involved in the development process and then large labs have more competition, and then it distributes influence and control.</p><p> There are probably more direct ways you can help distribute control and influence over AI besides making a system widely available. So, to answer your original question then about, how much of the benefit of open-sourcing can you get through alternative methods? I guess it really depends what benefit you&#39;re talking about. I think for AI safety progress probably quite a bit, honestly; actually the vast majority of it, given that a lot of the safety research that&#39;s done on these highly capable, cutting edge models is something that has to happen within well-resourced institutions anyway, or you need the access to the resources to do that, not just the code and the weights, but the computational resources and so on.</p><p> So, I think quite a bit. I think it&#39;s less of a “can we get close to the same benefits that open-sourcing allows?” It&#39;s more like, “can we do it in one fell swoop?”就是这样。 It&#39;s like open-sourcing is the easy option. “Here, it&#39;s open!” - and now you get all these benefits from open-sourcing. The decision to open-source or not, part of the reason it&#39;s a hard decision is because achieving these benefits by other means is harder. It&#39;s going to take more resources to invest, more organizational capacity, more thought, more cooperation. It&#39;s going to take a lot of infrastructure, a lot of effort. It&#39;s not the one-stop shop that open-sourcing is, but I think the idea is that if the risks are high enough, if the risks are severe enough, it&#39;s worth it. I think that&#39;s where it comes in.</p><p> So, I guess it&#39;s worth reiterating again and again: this paper is not an anti-open source paper, [it&#39;s] very pro-open source in the vast majority of cases. What we really care about here are frontier AI systems that are starting to show the potential for causing really catastrophic harm, and in these cases, let&#39;s not open-source and let&#39;s pursue some of these other ways of achieving the same benefits of open source to safety and distributing control and model evaluation, but open-source away below that threshold. The net benefits are great.</p><h3>攻防平衡，第 2 部分<a name="offense-defense-2"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。 So my next question - I actually got a bit sidetracked and wanted to ask it earlier - so in terms of the offense-defense balance, in terms of the harms that you are worried about from open-sourcing, I sometimes hear the claim that basically, “Look, AI, if you open-source it, it is going to cause more harm, but you also enable more people to deal with the harm.” So I think there, they&#39;re talking about offense-defense balance, not of finding flaws in AI models, but in the underlying issues that AI might cause. So, I guess the idea is something… To caricature it, it&#39;s something like, “Look, if you use your AI to create a pathogen, I can use my AI to create a broad spectrum antibiotic or something”, and the hope is that in these domains where we&#39;re worried about AI causing harm, look, just open-sourcing AI is going to enable tons of people to be able to deal with the harm more easily, as well as enabling people to cause harm. So I&#39;m wondering, what do you think about the underlying offense-defense balance as opposed to within AI?</p><p> <strong>Elizabeth Seger:</strong> I get the argument. Personally, I&#39;m wary about the arms race dynamic though. You gotta constantly build the stronger technology to keep the slightly less strong technology in check. I guess this comes back to that very original question you asked about, “What about just hitting the no more AI button?” So, I guess I get the argument for that. I think there&#39;s weird dynamics, I don&#39;t know. I&#39;m not doing a very good job answering this question. I&#39;m personally concerned about the race dynamic here, and I think it just comes back to this issue of, how hard is it to fix the issues and vulnerabilities in order to prevent the misuse in the first place? I think that should be the goal: preventing the misuse, preventing the harm in the first place. Not saying, “Can we build a bigger stick?”</p><p> There&#39;s a similar argument that is brought up when people talk about the benefits of producing increasingly capable AI systems and saying, “Oh, well, we need to plow ahead and build increasingly capable AI systems because you never know, we&#39;ll develop a system that&#39;ll help cure cancer or develop some renewable energy technology that&#39;ll help us address climate change or something like that.” What huge problems could AI help us solve in the future? And I don&#39;t know - this is personally me, I don&#39;t know what the other authors on this paper think of this - but I don&#39;t know, I kind of feel like if those are the goals, if the goals are to solve climate change and cure cancer, take the billions upon billions upon billions and billions of dollars that [we&#39;re] currently putting into training AI systems and go cure cancer and develop renewable technologies! I struggle with those arguments personally. I&#39;d be interested just to hear your thoughts. I have not written about this. This is me riffing right now. So, I&#39;d be interested to hear your thoughts on this train of thought as well.</p><p> <strong>Daniel Filan:</strong> I think the original question is unfairly hard to answer just because it&#39;s asking about the offense-defense balance of any catastrophic problem AI might cause and it&#39;s like, “Well, there are tons of those and it&#39;s pretty hard to think about. ” So, the thing you were saying about, if you wanted to cure cancer, maybe step one would not be “create incredibly smart AI”. I&#39;ve seen this point. I don&#39;t know if you know <a href="https://meaningness.com/about-my-sites">David Chapman</a> &#39;s <a href="https://betterwithout.ai/">Better without AI</a> ?</p><p> <strong>Elizabeth Seger:</strong> No, not familiar.</p><p> <strong>Daniel Filan:</strong> So, he basically argues we just shouldn&#39;t build big neural nets and it&#39;s going to be terrible. Also, <a href="https://aiimpacts.org/author/jeffreyheninger/">Jeffrey Heninger</a> at <a href="https://aiimpacts.org/">AI Impacts</a> , I think has said <a href="https://blog.aiimpacts.org/p/my-current-thoughts-on-the-ai-strategic">something similar along these lines</a> . On the one hand, I do kind of get it, just in the sense that, if I weren&#39;t worried about misaligned AI, there&#39;s this hope that this is the last invention you need. You create AI and now instead of having to separately solve cancer and climate change and whatever, just make it solve those things for you.</p><p> <strong>Elizabeth Seger:</strong> I guess it&#39;s just really hard to look forward, and you have to decide now whether or not this technology is that silver bullet and how much investment it&#39;s going to take to get to that point.</p><p> <strong>Daniel Filan:</strong> I think that&#39;s right, and I think your take on this is going to be driven by your sense of the risk profile of building things that are significantly smarter than us. I guess from the fact that I made the AI X-risk Research Podcast, rather than the AI Everything&#39;s Going to be Great Research Podcast, people can guess my-</p><p> <strong>Elizabeth Seger:</strong> It&#39;s an indication of where you&#39;re coming from.</p><p> <strong>Daniel Filan:</strong> … take on this, but I don&#39;t know. I think it&#39;s a hard question. So, part of my take is, in terms of the underlying offense-defense balance, I think it becomes more clear when you&#39;re worried about, what should I say, silicogenic risks? Basically the AI itself coming up with issues rather than humans using AI to have nefarious schemes. Once you&#39;re worried about AI doing things on their own where you are not necessarily in control, there I think it makes sense that you&#39;re probably… If you&#39;re worried about not being able to control the AIs, you&#39;re probably not going to be able to solve the risks that the AIs are creating, right?</p><p> <strong>Elizabeth Seger:</strong> Yeah, your management plan for AI shouldn&#39;t be to build a slightly more powerful AI to manage your AI.</p><p> <strong>Daniel Filan:</strong> Well, if you knew that you were going to remain in control of the slightly bigger AI, maybe that&#39;s a plan, but you kind of want to know that.</p><p> <strong>Elizabeth Seger:</strong> I guess I was saying that if you&#39;re worried about loss of control scenarios, then the solution shouldn&#39;t be, “Well, let&#39;s build another system that&#39;s also out of our control, but just slightly better aligned to address the…” I feel like that&#39;s-</p><p> <strong>Daniel Filan:</strong> It&#39;s not the greatest. I think my colleague John Wentworth has <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies">some saying</a> , “Releasing Mothra to contain Godzilla is not going to increase property values in Tokyo,” which is a cute little line. I don&#39;t know, it&#39;s a hard question. I think it&#39;s hard to say anything very precise on the topic. I did want to go back to the offense-defense balance. So moving back a bit, a thing you said was something like, “Look, it&#39;s probably better to just prevent threats from arising, than it is to have someone make a pathogen and then have everyone race to create an antibiotic or antiviral or whatever. ” So, that&#39;s one way in which everyone having really advanced AI… That&#39;s one way that could look in order to deal with threats. I think another way does look a bit more like prevention. I don&#39;t know, it&#39;s also more dystopian sounding, but one thing that AI is good at is surveillance, right?</p><p> <strong>Elizabeth Seger:</strong> Yes.</p><p> <strong>Daniel Filan:</strong> Potentially, so you could imagine, “Look, we&#39;re just going to open source AI and what we&#39;re going to use the AI for is basically surveilling people to make sure the threats don&#39;t occur.” So, maybe one version of this is you just really amp up wastewater [testing]. Somehow you use your AI to just look at the wastewater and see if any new pathogens are arising. It could look more like you have a bunch of AIs that can detect if other people are trying to use AI to create superweapons or whatever, and stop them before they do somehow.</p><p> <strong>Elizabeth Seger:</strong> The wastewater example, that sounds great. We should probably do that anyway. In terms of surveilling to see how people are using AI systems using AI, why not just have the AI systems be behind an API where people can use the systems for a variety of downstream tasks integrating through this API and then the people who control the API can just see how the system is being used? Even if it can be used for a vast majority of tasks, even if you were to take all the safety filters off, the advantage of the API is still that you can see how it&#39;s being used. I don&#39;t know, I feel like that&#39;s-</p><h3>让开源更安全？<a name="making-os-safer"></a></h3><p> <strong>Daniel Filan:</strong> That seems like a good argument. All right, so I guess another question I have is related to the frame of the report. So in the report you&#39;re basically like, “open-sourcing has these benefits, but it also has these costs. What are ways of doing things other than open-sourcing that basically try and retain most of the benefits while getting rid of most of the costs?” You can imagine a parallel universe report where you say, “Okay, open-sourcing has these benefits, but it also has these costs. We&#39;re still going to open source, but we&#39;re going to do something differently in our open source plan that is going to retain benefits and reduce costs”, right? So one example of this is you open-source models, but you have some sort of watermarking or you have some sort of cryptographic backdoor that can stop models in their tracks or whatever. I&#39;m wondering: why the frame of alternatives to open-sourcing rather than making open-sourcing better?</p><p> <strong>Elizabeth Seger:</strong> Very simple. I think making open-sourcing better is the harder question, technically more difficult. I mean, for example, say you have watermarking, part of the issue with watermarking to identify artificially generated images is making sure the watermarks stick. How do you make sure that they are irremovable if you are going to open… This is a really complex technical question: how do you develop a system that has watermarked images where that watermark is irremovable if you were to open source the system?</p><p> I&#39;m not saying it&#39;s undoable. I personally don&#39;t have the technical background to comment very deeply on this. I have heard people talking about how possible this would be. It also depends how you watermark, right?</p><p> If you have just a line of inference code that says, “slap a watermark on this thing”, it could delete the line of inference code. If you&#39;re to train the system on only watermarked images, well now you have to retrain the entire system to get it to do something else, which is very expensive. So again, I think it depends how you do it.</p><p> I was at a meeting last week where people were talking about, are there ways we could build in a mechanism into the chips that run the systems that say “if some bit of code is removed or changed in the system, then the chip burns up and won&#39;t run the system”. I&#39;m not saying this is impossible, but [a] really interesting technical question, really difficult, definitely beyond my area of expertise. But I think if this is an approach we can take and say, there are ways to be able to open source the system and get all the benefits of open-sourcing by just open-sourcing and still mitigate the risks, I think that&#39;s great. I think it&#39;s just a lot more difficult.</p><p> And there&#39;s one aspect in which we do take the flip view in the report, and I think this is where we start talking about a staged release of models. You can undergo a staged release of a model where you put out a slightly smaller version of a model behind API, you study how it&#39;s being used. Maybe you take a pause, analyze how it was used, what [are] the most common avenues of attack, if at all, [that] were being used to try and misuse the model.</p><p> And then you release a slightly larger model, [then] a slightly larger model. You do this iteratively, and if you do this process, as you get to a stage where it&#39;s like, hey, we&#39;ve been doing the staged release of this model for however many months and no problems, looking good, there&#39;s no emergent capabilities that popped up that are making you worried. You didn&#39;t have to implement a bunch of safety restrictions to get people to stop doing unsafe things - okay, open source. This is not a binary [of] it has to be completely open or completely closed. And I think this is one respect… If you were to take this flip view of “how can we open source, but do it in the safest way possible?” Just open source slowly, take some time to actually study the impacts. And it&#39;s not like the only way to get a sense of how the system&#39;s going to be used is to just open source it and see what happens. You could do a staged release and study what those impacts are.</p><p> Again, it won&#39;t be perfect. You never know how it&#39;s going to be used 10 years down the road once someone gets access to all the weights and stuff. But it is possible to study and get some sort of insight. And I think one of the nice things about staged release is if you start doing the staged release process and you realize that at each iterative step you are having to put in a bunch of safety filters, for example, to prevent people from doing really shady stuff, that&#39;s probably a good indication that it&#39;s not ready to be open sourced in its current form, because those are safety filters that will just immediately be reversed once open sourced. So I think you can learn a lot from that.</p><p> So I think that&#39;s one way you can open source safely: find ways to actually study what the effects are before you open source, because that decision to open source is irreversible. And then I think the technical challenge of, are there ways we can have backstops, that we can technically build in irreversible, irremovable filters or watermarks or even just hardware challenges that we could implement - I think [those are] really interesting technical questions that I don&#39;t know enough about, but… Go for it. That&#39;d be a great world.</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。 If listeners are interested, this gets into some territory that <a href="https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html#watermarking-lm-outputs">I talked about with Scott Aaronson earlier this year</a> . Yeah, I think the classic difficulties, at least say for watermarking… I read <a href="https://arxiv.org/abs/2012.08726">one paper</a> that claims to be able to bake the watermark into the weights of the model. To be honest, I didn&#39;t actually understand how that works.</p><p> <strong>Elizabeth Seger:</strong> I think it has to do with how the model&#39;s trained. So the way I understand it is if you have a dataset of images that all have a watermark in that dataset, not watermark in the sense like you see on a $1 bill, but weird pixel stuff that the human eye can&#39;t see. If all the images in the training dataset have that watermark, then all of the images it produces will have that watermark. In that case, it&#39;s baked into the system because of how it was trained. So the only way to get rid of that watermark would be to retrain the system on images that don&#39;t contain the watermark.</p><p> <strong>Daniel Filan:</strong> Yeah, that&#39;s one possibility. So that&#39;s going to be a lot rougher for applying to text models, of course, if you want to just train on the whole internet. I think I saw <a href="https://arxiv.org/abs/2012.08726">something</a> that claimed to work even on cases where the dataset did not all have the watermark, but I didn&#39;t really understand how it worked. But at any rate, the key issue with these sort of watermarking methods is as long as there&#39;s one model that can basically paraphrase that does not have watermarking, then you can just take your watermark thing and basically launder it and get something that - if your paraphrasing model is good enough, you can create something that looks basically similar, it doesn&#39;t have the watermark, and then it&#39;s sad news.是的。进而-</p><p> <strong>Elizabeth Seger:</strong> Sorry, I was going to say there&#39;s similar [things] in terms of how doing something with one model allows you to jailbreak another model. I mean this is what happened with the <a href="https://arxiv.org/abs/2307.15043">&#39;Adversarial suffixes&#39; paper</a> , where using a couple open source models, one which was Llama 2, and using the weights of those models, figuring out a way to basically just throw a seemingly random string of numbers at a large language model, and then with that seemingly random range of numbers before the prompt basically get the system to do whatever you want. Except while they figured out how to do that using the weights accessible from Llama 2, it worked on all the other large language models. So finding a way to jailbreak one model and using the weights and access to one model, that could bring up vulnerabilities in tons of others that aren&#39;t open sourced as well. So I think that&#39;s another roughly related somewhat to what we were just talking about point.</p><p> <strong>Daniel Filan:</strong> Yeah, I guess it brings up this high level thing of whatever governance method for AI you want, you want it to be robust to some small fraction of things breaking the rules. You don&#39;t want the small fraction to poison the rest of the thing, which watermarking unfortunately has.</p><p> Yeah, I guess I wanted to say something brief about backdoors as well. So there is really a way of, at least in toy neural networks, and you can probably extend it to bigger neural networks, <a href="https://arxiv.org/abs/2204.06974">you really can introduce a backdoor that is cryptographically hard to detect</a> . So one problem is, how do you actually use this to prevent AI harm is not totally obvious. And then there&#39;s another issue of… I guess the second issue only comes up with super smart AI, but if you have a file on your computer that&#39;s like, “I implanted a backdoor in this model, the backdoor is this input”, then it&#39;s no longer cryptographically hard to find as long as somebody can break into your computer. Which hopefully is cryptographically hard, but I guess there are security vulnerabilities there.</p><p> So yeah, I wonder if you want to say a little bit about the safer ways to get the open source benefits. I&#39;ve given you a chance to talk about them a little bit, but is there anything more you want to say about those?</p><p> <strong>Elizabeth Seger:</strong> I think, not really. I think the overarching point is, just as I said before, when the risks are high - and I think that&#39;s key to remember, I&#39;m not saying don&#39;t open-source everything - when the risks are high, it is worth investing in seeing how else we can achieve the benefits of open-sourcing. Basically, if you&#39;re not going to open-source because the risks are high, then look into these other options. It&#39;s really about getting rid of this open versus closed dichotomy.</p><p> So many of the other options have to do with other options for sharing models, whether that&#39;s structured access behind API, even research API access, gated download, staged release, and then also more proactive efforts. Proactive efforts which can actually also be combined with open-sourcing. They don&#39;t have to be seen as an alternative to open-sourcing. So this is things like redistributing profits towards AI safety research or starting AI safety and bug bounty programs. Or even like we talked about with <a href="https://arxiv.org/abs/2303.12642">the democratization paper</a> , thinking about how we can democratize decision-making around AI systems to help distribute influence over AI away from large labs, which is another argument for open-sourcing.</p><p> So yeah, I think that this is key: there are other efforts that can be put in place to achieve many of the same benefits of open-sourcing and when the risks are high, it&#39;s worth really looking into these.</p><h2>人工智能治理研究<a name="ai-gov"></a></h2><p><strong>Daniel Filan:</strong> All right.好的。 So moving on, I want to talk a little bit more broadly about the field of AI governance research. So historically, this podcast is mostly focused on technical AI alignment research, and I imagine most listeners are more familiar with the technical side than with governance efforts.</p><p> <strong>Elizabeth Seger:</strong> In which case, I apologize for all my technical inaccuracies. One of the benefits of having 25 co-authors is that a lot of the technical questions I got to outsource.</p><h3>领域状况<a name="state-of-field"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>很有意义。 Yeah, it&#39;s good to be interdisciplinary. So this is kind of a broad question, but how is AI governance going? What&#39;s the state of the field, if you can answer that briefly?</p><p> <strong>Elizabeth Seger:</strong> The state of the field of AI governance.是的。好的。 I&#39;ll try and answer that briefly. It&#39;s going well in that people are paying attention. In this respect, the release of ChatGPT I think was really great for AI governance because people, besides those of us already doing AI governance research, are really starting to see this as something valuable and important that needs to be talked about and [asking] questions around what role should governments play in regulating AI, if at all? How do we get this balance between governments and the developers? Who should be regulated with respect to different things? Do all of the responsibilities lie on the developers or is it on the deployers?</p><p> And all of these questions suddenly are coming to light and there&#39;s more general interest in them. And so we&#39;re seeing things like, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is happening next week, [a] global AI summit, looking at AI safety, really concerned about catastrophic and existential risks, trying to understand what kind of global institutions should be in place to govern AI systems, to evaluate AI systems, to audit, to regulate.</p><p> And this is bringing in countries from all over the world. I think it&#39;s something like 28 different countries are going to be at the UK AI Summit. You have <a href="https://artificialintelligenceact.eu/">the EU AI Act</a> where it started a while ago looking at narrow AI systems, but now is taking on foundation models and frontier AI systems and looking at open source regulation. And this has really, over the last year, exploded into a global conversation.</p><p> So in that respect, AI governance is going well in that people are paying attention. It&#39;s also very high stress because suddenly everyone&#39;s paying attention.我们必须做点什么。 But I think there&#39;s really genuine interest in getting this right, and I think that really bodes well. So I&#39;m excited to see where this next year goes. Yeah, there&#39;s talk about having this global AI summit and then making this a recurring series. And so I think it&#39;s going well in the sense that people are paying attention and the wheels are starting to turn, and that&#39;s cool.</p><h3>开放式问题<a name="open-qs"></a></h3><p><strong>丹尼尔·菲兰：</strong>好的。 I guess related to that, what do you see as the most important open questions in the field?</p><p> <strong>Elizabeth Seger:</strong> In the field of AI governance?好的。 So I think one big one is compute governance, which my colleague, <a href="https://heim.xyz/about/">Lennart Heim</a> , <a href="https://www.governance.ai/team/lennart-heim">works on</a> . This is just thinking about how compute is a lever for trying to regulate who is able to develop large models, even how compute should be distributed so that more people can distribute large models, but basically using compute as a lever to understand who has access to and who is able to develop different kinds of systems. So I think that&#39;s a huge area of research with a lot of growing interest because compute&#39;s one of the tangible things that we can actually control the flow of.</p><p> I think that the questions around model-sharing and open-sourcing are getting a lot of attention right now. Big open question, a lot of debate, like I said, it&#39;s becoming really quite a polarized discussion, so it&#39;s getting quite hard to cut through. But a lot of good groups [are] working on this, and I think a lot of interest in genuinely finding common ground to start working on this. I&#39;ve had a couple situations where I&#39;ve been in groups or workshops where we get people who are very pro-open source and other people who are just like, no, let&#39;s just shut down the whole AI system right now, really both sides of the spectrum coming together. And we try and find a middle ground on, okay, where do we agree? Is there a point where we agree? And very often we can come to a point of agreement around the idea that there may be some AI system, some model that poses risks that are too extreme for that model to be responsibly open sourced.</p><p> And that might not sound like that extreme of a statement, but when you have people coming from such polarized views to agree on the fact that there may exist a model one day that should not be open source, that is a starting point and you can start the conversation from there. And every group I&#39;ve been in so far has got to that point, and we can start working on that. So I think this model-sharing question is a big open question and lots of technical research needs to be done around benchmarking to decide, when are capabilities too dangerous?</p><p> Also around understanding what activities are actually possible given access to different combinations of model components. And that&#39;s actually not entirely clear, and we need a much more fine-grained understanding of what you can actually do given different kinds of model, combinations of model components, in order not only to have safe standards for model release and really a fine-grained standard for model release, but also to protect the benefits of open-sourcing. You don&#39;t want to just have a blanket “don&#39;t release anything” if you can get a lot of good benefit out of releasing certain model components. So I think a lot of technical research has to go into this.</p><p>反正。 So yeah, second point, I think model-sharing is a really big point of discussion right now. And then with the upcoming <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> , [there&#39;s] quite a bit of discourse around what international governance structures should look like for AI, a lot of different proposed models. And yeah, it&#39;ll be interesting to see what comes out of the summit. I don&#39;t think they&#39;re going to agree on anything amazing at the summit.已经两天了。 But I think for me, a really great outcome of the summit would be, first, recognition from everyone that AI systems could pose really extreme risks. So just a recognition of the risks. And then second, a plan going forward, a plan for how we can start establishing international systems of governance and really structure out when are we going to come to what kinds of decisions and how can we start putting something together. So I think that those are probably three key open questions, and the international governance structure one is really big right now too, just given the upcoming summit.</p><p> <strong>Daniel Filan:</strong> And I guess unless we get the editing and transcription for this episode done unusually quickly, listeners, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is almost definitely going to be in your past. So I guess listeners are in this interesting position of knowing how that all panned out in a way that we don&#39;t. So that was open questions in the field broadly. I&#39;m wondering for you personally as a researcher, what things are you most interested in looking at next?</p><p> <strong>Elizabeth Seger:</strong> Interesting. I mean, most of my life is taken up with follow-up on this open source report right now. So I definitely want to keep looking into questions around model-sharing and maybe setting responsible scaling policy, responsible model release policy.我不太确定。 I think I&#39;m in this place right now where I&#39;m trying to feel out where the most important work needs to be done and whether the best place for me to do is to encourage other people to do certain kinds of work where I don&#39;t necessarily have the expertise, like we were talking about, like needing more technical research into what is possible given access to different combinations of model components, or are there specific areas of research I could try to help lead in? Or whether really what needs to be done is just more organizational capacity around these issues.</p><p> So no, I am personally interested in keeping up with this model-sharing discussion. I think there&#39;s a lot of interesting work that needs to be done here, and it&#39;s a key thing that&#39;s being considered within the discussions around international AI governance right now. Yeah, so sorry I don&#39;t have as much of a clear cut answer there, but yeah, I&#39;m still reeling from having published this report and then everything that&#39;s coming off the back of it and just trying to feel out where&#39;s the next most important, most impactful step, what work needs to be done. So I guess if any of your listeners have really hot takes on “oh, this is what you should do next”, I guess, please tell me. It&#39;ll be very helpful.</p><p> <strong>Daniel Filan:</strong> How should they tell you if someone&#39;s just heard that and they&#39;re like, oh, I need to-</p><p> <strong>Elizabeth Seger:</strong> “I need to tell her now! She must know!” Yeah, so I mean, I have a <a href="https://elizabethseger.com/">website</a> where you could find a lot of my contact information, or you can always find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . I spend far too much time on LinkedIn these days. And also my email address happens to be on the open source report. So if you download the report, my email address is there.</p><p> <strong>Daniel Filan:</strong> What&#39;s the URL of your website?</p><p> <strong>Elizabeth Seger:</strong> <a href="https://elizabethseger.com/">ElizabethSeger.com</a> .</p><h3> x-risk的独特治理问题<a name="xrisk-different"></a></h3><p><strong>Daniel Filan:</strong> All right.好的。 Getting back to talking about governance in general, I&#39;m wondering… so I guess this is an x-risk-focused podcast. How, if at all, do you think governance research looks different when it&#39;s driven by concerns about x-risk mitigation versus other concerns you could have about AI governance?</p><p> <strong>Elizabeth Seger:</strong> Well, that&#39;s a good question.让我们来看看。 So the question is how does the governance research look different?</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。 What kinds of different questions might you focus on, or what kinds of different focuses would you have that would be driven by x-risk worries rather than by other things?</p><p> <strong>Elizabeth Seger:</strong> So this is something that I&#39;ve had to think about a lot in my own research development because I did not come into this area of research from an x-risk background interest. I came into it… I mean, honestly, I started in bioethics and then moved from bioethics looking at AI systems in healthcare and have sort of moved over into the AI governance space over a very long PhD program.所以我就在这里。</p><p> But I would say one of the things that I&#39;ve learned working in the space [that&#39;s] more interested in long-term x-risk impacts of AI and trying to prevent x-risks is really paying attention to causal pathways and really trying to be very critical about how likely a potential pathway is to actually lead to a risk. I don&#39;t know if I&#39;m explaining this very well.</p><p> Maybe a better way of saying it&#39;s like: if you have a hypothesis, or let&#39;s say you&#39;re worried about the impacts of AI systems on influence operations or impacting political campaigns, I find it really helpful to start from the hypothesis of, it won&#39;t have an impact. And really just trying to understand how that might be wrong, as opposed to trying to start from “oh, AI is going to pose a massive bio threat, or it&#39;s going to pose a massive threat to political operations” or something like that. And then almost trying to prove that conclusion.</p><p> Yeah, I don&#39;t know, I start from the opposite point and then try and think about all the ways in which I could be wrong. And I think this is really important to do, especially when you&#39;re doing x-risk research, whether it&#39;s with respect to AI or some other form of x-risk. Because I think there are a lot of people that turn off when you start talking about existential risks, they think it&#39;s too far out there, it&#39;s not really relevant to the important questions that are impacting people today, the tangible things that people are already suffering 。 And so I think it&#39;s really important to be very, very rigorous in your evaluations and have a very clear story of impact for why it is that you&#39;re doing the research you&#39;re doing and focusing on the issues that you&#39;re doing. At least that&#39;s been my experience, trying to transition into the space and work on these issues.</p><h3>技术研究助力治理<a name="tech-for-gov"></a></h3><p><strong>Daniel Filan:</strong> Another question I have, related to my audience… So I think my audience, a lot of them are technical alignment researchers and there are various things they could do, and maybe they&#39;re interested in, okay, what work could technical alignment people do that would make AI governance better? I&#39;m wondering if you have thoughts on that question.</p><p> <strong>Elizabeth Seger:</strong> Okay. Technical alignment people, AI governance better. Yeah, I mean there&#39;s a lot of work going on right now, especially within the UK government. We just set up the <a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">UK AI Task Force</a> , a government institution doing a lot of model evals and alignment research. I think if you have the technical background in alignment research, you are very much needed in the governance space. There&#39;s very often a disconnect between… I mean, I am also guilty of this. There&#39;s a disconnect between the people doing the governance research and the people who have the experience with the technology and really know the ins and outs of the technology that&#39;s being developed.</p><p> And I think if you have the inclination to work in an AI governance space and help bridge that gap, that would be incredibly valuable. And like I&#39;ve already said, some of the more technical questions, even around open-sourcing, are things that I was very, very glad to have colleagues and co-authors on the paper who have worked for AI labs and stuff before and really knew what they were talking about and could advise and help write some of the more technical aspects of the report.</p><p> So I think if you have the inclination to work in the space, to get involved with governance efforts, or even maybe some of these government institutions that are starting to pop up that are working on the boundary of AI governance and technical research, that could be a really valuable place to contribute. So I think my 2 cents off the top of my brain would be help bridge that gap.</p><p><strong>丹尼尔·菲兰：</strong>好的。伟大的。 So before we wrap up, I&#39;m wondering if there&#39;s anything that you wish I&#39;d asked but that I didn&#39;t?</p><p> <strong>Elizabeth Seger:</strong> Oh, that&#39;s a good question.不，我不这么认为。 I think we&#39;ve covered a lot of good stuff. Yeah, thank you for having me on really. I&#39;d say there&#39;s nothing in particular.这太棒了。</p><h2>根据伊丽莎白的研究<a name="following-elizabeths-research"></a></h2><p><strong>Daniel Filan:</strong> All right, so to wrap up then, if people are interested in following your research, following up on this podcast, how should they do that?</p><p> <strong>Elizabeth Seger:</strong> So I have my website, <a href="https://elizabethseger.com/">ElizabethSeger.com</a> . It sort of outlines my different ongoing research projects, has a lot of publications on it. Also, <a href="https://www.governance.ai/">GovAI&#39;s website</a> is a wealth of information [on] all things AI governance from all my great colleagues at GovAI and our affiliates. So really, yeah, there&#39;s new research reports being put out almost every week, maybe every other week, but really high quality stuff. So you can find a lot of my work on the GovAI website or my current work and past work on my own website or find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . Yeah, just happy to talk more.</p><p> <strong>Daniel Filan:</strong> All right, well thank you very much for being on the podcast.</p><p> <strong>Elizabeth Seger:</strong> Great, thank you.</p><p> <strong>Daniel Filan:</strong> This episode is edited by Jack Garrett and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a> and <a href="https://lightspeedgrants.org/">Lightspeed Grants</a> , along with <a href="https://patreon.com/axrpodcast">patrons</a> such as Tor Barstad, Alexey Malafeev, and Ben Weinstein-Raun. To read a transcript of this episode or to learn how to <a href="https://axrp.net/supporting-the-podcast/">support the podcast yourself</a> , you can visit <a href="https://axrp.net">axrp.net</a> . Finally, if you have any feedback about this podcast, you can email me at <a href="mailto:feedback@axrp.net">feedback@axrp.net</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger<guid ispermalink="false"> RDm26xAcb9rfvuBya</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:07 GMT</pubDate></item></channel></rss>