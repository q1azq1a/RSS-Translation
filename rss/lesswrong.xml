<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 24 日星期四 08:14:12 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The lost millennium]]></title><description><![CDATA[Published on August 24, 2023 3:48 AM GMT<br/><br/><p><em>注意：这篇文章是推测性的，您应该对其中的说法持保留态度。</em></p><p>纵观人类历史，我们知道，经济增长（在马尔萨斯条件下人口增长是一个很好的指标）已经加速了很多次。事实上，我们只要向后推算当前的增长率就可以看到这一点：鉴于世界上只有这么多人，而且他们的人均维持生计收入的有限倍数，经济增长率不可能达到每年3％的订单已经持续了一千多年左右。</p><p>尽管很难做到这一点，但人们也对过去这种增长的具体发生方式做出了估计。其中一项估计来自<a href="https://www.pbl.nl/en/image/links/hyde">HYDE 数据集</a>，也可在<a href="https://ourworldindata.org/population-sources">Our World In Data</a>中找到；以及其他估计的完整集合<a href="https://www.census.gov/data/tables/time-series/demo/international-programs/historical-est-worldpop.html">可以在这里找到</a>。</p><p>让我们首先检查一下 HYDE 数据集。公元 1 年之前的数据是每千年给出的，看看增长率，我们可以得到下表：</p><table><thead><tr><th>千年</th><th>平均年人口增长率（百分比）</th></tr></thead><tbody><tr><td>公元前 10000 年 - 公元前 9000 年</td><td>0.0236%</td></tr><tr><td>公元前 9000 年 - 公元前 8000 年</td><td>0.0254%</td></tr><tr><td>公元前 8000 年 - 公元前 7000 年</td><td>0.0279%</td></tr><tr><td>公元前 7000 年 - 公元前 6000 年</td><td>0.0320%</td></tr><tr><td>公元前 6000 年 - 公元前 5000 年</td><td>0.0368%</td></tr><tr><td>公元前 5000 年 - 公元前 4000 年</td><td>0.0411%</td></tr><tr><td>公元前 4000 年 - 公元前 3000 年</td><td>0.0435%</td></tr><tr><td>公元前 3000 年 - 公元前 2000 年</td><td>0.0489%</td></tr><tr><td>公元前 2000 年 - 公元前 1000 年</td><td>0.0415%</td></tr><tr><td>公元前 1000 年 - 公元 1 年</td><td>0.0746%</td></tr></tbody></table><p>除了公元前第二个千年<sup class="footnote-ref"><a href="#fn-GS5X2acGin64XwSjM-1" id="fnref-GS5X2acGin64XwSjM-1">[1]</a></sup>略有逆转之外，估计表明人口增长率逐渐加快。这与人口增长的双曲线模型一致，但在这里我不想做出任何这样的假设，只想看原始数据。</p><p>那么，令人惊讶的是该表中应该包含的下一个条目：在第一个千年，即公元 1 年至公元 1000 年，人口增长率估计平均每年仅为 0.033%。换句话说，根据这个数据集，即使是公元前第五个千年，人口增长也比第一个千年更快！</p><p>事实证明，像这样的结果对于更改我们正在使用的数据集来说是稳健的。 McEvedy 和 Jones 报告称，第一个千年的平均增长率为 0.044%/年，而公元前 1000 年至公元前 200 年的平均增长率为 0.137%/年，从公元前 200 年至公元 1 年的平均增长率为 0.062%/年。 HYDE 数据集中没有提供公元前第一个千年的更详细数据，这表明人口增长放缓可能在第一个千年之前就开始了。为了在这个数据集中找到人口增长较慢的千年，我们必须回到公元前第四个千年（公元前 5000 年 - 公元前 4000 年）。</p><p>我看过的所有数据集都表明，随着第二个千年的到来，人口增长大幅加速。蒙古人的征服和黑死病似乎对 1200 到 1400 年间的人口增长造成了压力，但尽管如此，从 1000 到 1500 年间的人口增长率仍然平均为 0.08%/年，这比公元前第一个千年还要快。</p><p> <a href="https://en.wikipedia.org/wiki/Lost_Decades">“失去的十年”</a>是日本使用的一个名称，指的是1991年日本股市崩盘后的十年经济停滞期。按照这一惯例，将第一个千年称为“失去的千年”可能是合适的。如果我们相信某种经济史的随机双曲线增长模型，即拉鲁德曼（la <a href="https://www.openphilanthropy.org/research/modeling-the-human-trajectory/">Roodman，2020）</a> ，这意味着人类在第一个千年“非常不幸”，可能是由于持续的不利社会冲击。</p><p>这个故事纯粹是定量的，着眼于人口增长，但值得注意的是，在马尔萨斯条件下，我们预计人口增长将跟踪技术进步和资本形成。我们的生产力越高，人们就应该拥有越多的孩子，直到人口增长到足以使劳动边际产量回落到维持生计的水平为止。一千年对于马尔萨斯动力发挥作用来说已经足够了，因此第一个千年缺乏强劲的人口增长也表明创新和资本积累等生产力增长因素的放缓。</p><p>那么重要的问题就变成了：<em>为什么会发生这种情况？</em> ，或者也许<em>这是怎么发生的？</em>我们对其他人口增长放缓有很好的解释，例如1200年至1400年期间的蒙古征服和黑死病。然而，第一个千年的人口增长放缓看起来完全是一个谜。我可以提出一些解释，但它们都不太符合数据。即使是“运气不好”，我也想能够说得更多。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-GS5X2acGin64XwSjM-1" class="footnote-item"><p>也许是由于<a href="https://en.wikipedia.org/wiki/Late_Bronze_Age_collapse">青铜时代晚期的崩溃</a>？ <a href="#fnref-GS5X2acGin64XwSjM-1" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/hgf6FB9jMB7wMLuKA/the-lost-millennium#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hgf6FB9jMB7wMLuKA/the-lost-millennium<guid ispermalink="false"> hgf6FB9jMB7wMLuKA</guid><dc:creator><![CDATA[Ege Erdil]]></dc:creator><pubDate> Thu, 24 Aug 2023 03:48:40 GMT</pubDate> </item><item><title><![CDATA[Regreasing a KitchenAid Mixer]]></title><description><![CDATA[Published on August 24, 2023 2:30 AM GMT<br/><br/><p><span>我最近从朋友那里得到了一台旧的 KitchenAid 搅拌机。它是 k45ss，可能是 80 年代初的，我看到的一般建议是，在典型的家庭使用中，它们应该每 10 年重新润滑一次。</span>我订购了 4 盎司<a href="https://www.youtube.com/watch?v=UmnD6Y5jNMg">食品</a><span>安全合成</span><a href="https://www.amazon.com/dp/B08ZYCQVRD">润滑脂</a>，并遵循 Mixer 先生的 YouTube 指南（<a href="https://www.youtube.com/watch?v=EW9BQ-oPFkk">第</a><a href="https://www.youtube.com/watch?v=L-qMOyhb2xc">1、2、3</a>部分）。我一边用手机播放视频，一边进行操作：打开搅拌机，清除旧油脂，添加新油脂，然后将其放回原处。有细微的差别（我的应力消除装置很烦人，而且我的齿轮组件内部没有<a href="https://youtu.be/L-qMOyhb2xc?si=dYgKmFXzWSKtX7o1&amp;t=133">销钉</a>，需要先拆下蜗杆组件），但它非常接近。</p><p>行星齿轮组件的润滑脂含量很低，而且那里的润滑脂有点干：</p><p> <a href="https://www.jefftk.com/kitchen-aid-planetary-low-grease-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/sxkpxtvrlzqqa6v5cose" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/sxkpxtvrlzqqa6v5cose 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/gkxxaeipawczo0vhwjii 1100w"></a></p><div></div><p></p><p>上部区域有更多的油脂，尽管油脂比我想象的要硬：</p><p> <a href="https://www.jefftk.com/kitchen-aid-head-internal-old-grease-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/d0cj2kg5nbktkhqqjp00" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/d0cj2kg5nbktkhqqjp00 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/lhtcygg0imcqpa4ile53 1100w"></a></p><div></div><p></p><p>齿轮状况良好，感觉良好且坚固。</p><p>脱脂、再润滑和组装都很顺利。总共大概1.5小时？然而，当我完成后，我意识到我还剩下一个零件：三个垫圈之一。</p><p> <a href="https://www.jefftk.com/kitchen-aid-extra-washer-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/r6u3cvfabtvaqzzxccgg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/r6u3cvfabtvaqzzxccgg 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5pcunrT9fJJ3hmHvu/rcl1me4clylsn0u0zcnq 1100w"></a></p><div></div><p></p><p>我回顾了视频，它谈到安装其中两个洗衣机，我记得这样做过，但我对如何最终安装第三个感到困惑。没有任何逻辑上应该有洗衣机但没有的地方。</p><p>我在晚餐时谈到了这一点，我的一位室友说他们还有一个旧的 KitchenAid，可能从未重新润滑过（k45，电源线没有接地，可能是 60 年代末）。我们决定一起做他们的，也许我们会看看洗衣机去了哪里。</p><p>拥有第二双手和前一天晚上刚刚完成的一双手相结合，使得整个过程变得更快：不到一个小时。和我的非常相似，除了应力消除更烦人。不过，我们确实解开了垫圈之谜：行星机构中有两个垫圈，彼此堆叠在一起。</p><p>我不确定是否值得打开我的底部来安装额外的垫圈。我倾向于是（他们不会无缘无故地放两个垫圈，不需要把整个东西拆开）但可能我真的不需要？</p><br/><br/> <a href="https://www.lesswrong.com/posts/5pcunrT9fJJ3hmHvu/regreasing-a-kitchenaid-mixer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5pcunrT9fJJ3hmHvu/regreasing-a-kitchenaid-mixer<guid ispermalink="false"> 5pcunrT9fJJ3hmHvu</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 24 Aug 2023 02:30:04 GMT</pubDate> </item><item><title><![CDATA[Assessment of intelligence agency functionality is difficult yet important]]></title><description><![CDATA[Published on August 24, 2023 1:42 AM GMT<br/><br/><p><strong>总结：</strong>在观察情报机构时，硬的部分很难看到，软腐败的部分很容易被观察到。这导致了一种偏见，即很多人高估了容易观察到的柔软和无害部分的普遍程度。有时，这甚至会导致职业生涯比你领先得多的人产生一种危险而普遍的估计，认为整个情报机构是无害且无关紧要的，但事实并非如此。情报机构可能是功能较少、相关性较低的部分和功能较多、相关性较高的部分的混合体，这些部分对政府和政策具有不成比例的巨大影响力；认为情报机构都是由不值得关注的非功能性、不相关的部分组成的想法是错误的，即使这种信念是一种流行的规范。</p><p></p><p><strong>为什么情报机构很危险</strong></p><p>在很多情况下，情报机构会在没有任何警告的情况下突然变得重要。例如，美国国家安全委员会的大多数或全部机构可能会突然一致改变其对功能增益研究的立场，例如美中关系或美俄关系再次触及25年来的新低（实际上已经近几年来经常发生）。</p><p>无论是某个机构的领导层，还是机构中有权执行操作的有权势的个人，或者是腐败集团，都可能亲自做出判断，认为加快或重新启动 GOF 研究的最佳方法是针对最有效率的各种人员或有效反对 GOF 研究。</p><p>这不一定是加速或保护 GOF 研究的最有效方法，它只需要看起来像那样，足以让某人签字同意，甚至让他们只是认为这对他们的老板来说看起来不错。</p><p>在情报机构的混合能力模型中，有能力或技术先进的<i>能力</i>显然可能与无能的管理/决策混合在一起。一个真正无害、无关紧要、不值得关注的情报机构（而不是有动机错误地表现出无害、无关紧要或不值得关注的样子）必须是这样的情报机构：技术上<i>既不</i>复杂<i>，又</i>过于腐败，无法实现基本功能，例如运行操作。</p><p>对于美国、俄罗斯和中国的情报机构来说，这是一种极其天真的想法；尤其是美国和中国，它们拥有广泛的声望、先进的技术以及蓬勃发展的私营部门技能库来招聘人才。</p><p>当从某个地方的某个人绝对必须执行的政策倡导任务（例如推动可能导致人类灭绝的 GOF 研究推动明智的政策制定）中计算预期价值时，许多人目前意识到，该重要社区消失或解散的风险大大降低了预期价值计算该重要社区所产生的一切；例如，社区有 10% 的机会停止存在或解散，整个社区产生的预期价值会减少约 10%。</p><p>我遇到的大多数人脑海中浮现的都是一场大规模的极权主义剧变，就像20世纪初中期的那样，而这种剧变是安全与不安全之间的严格界限。然而，在21世纪，特别是在新冠疫情和2008年经济衰退之后，专家和军事规划者现在更加关注国际力量平衡（例如美国、俄罗斯和中国相对于彼此以及其他独立国家的实力）改变是由于经济崩溃或联盟瘫痪，而不是革命或军事征服。这是因为今天的整个世界与 70 年前截然不同。</p><p>更有意义的是预期经济会出现缓慢且不完全的倒退，其结果是以各种方式转向<a href="https://en.wikipedia.org/wiki/Hybrid_regime">混合政权</a>，其中情报机构和内部安全机构因腐败而滥用权力，而由于广泛的腐败而缺乏问责制。优先考虑<a href="https://en.wikipedia.org/wiki/Hybrid_warfare">混合战争</a>，并防止俄罗斯和中国等外国对手利用国内精英，如亿万富翁、政府官员和与关键人群相关的名人/思想领袖（如 Yann Lecun）。</p><p>对此的一个角度示例，来自<a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally?commentId=bnujfnQXJJ3mJJkxa">Don’t Take the Organization Chart Literals</a>的置顶评论：</p><blockquote><p> ...政府（和腐败的企业组织）中发生的很多事情都是通过默许权力完成的。很少有司法部、中央情报局和联邦调查局官员能够全面了解他们的工作如何与美国的利益不一致。但他们大多数人都有一个普遍的认识，那就是他们对组织的忠诚度要高于对美国的忠诚度。 <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally#fn-2WzufJ2HD9B92Pnz6-1"><sup>[1]</sup></a>通过其家族和其他腐败关系，[司法部领导人]巴尔成为美国腐败机构内部集团的一部分。这很简单，就像大多数下级军官知道他和他们在一起一样。</p><p>所以巴尔不必明确地告诉警卫们别看他，他不必告诉联邦调查局进行糟糕的调查，他不必告诉司法部继续腐败......下-拥有下属充分信任和信心的级别老板会制定小计划来完成任务。这是老板想要的，老板会照顾他们。</p><p>想象一下马斯克可能收购 Twitter 的情况。你是否认为如果马斯克收购了 Twitter，即使作为私人所有者，他也会突然完全控制整个机构？当然不是。<strong>真正有权力的人，都是他的下属，那些在他那里呆了一段时间的人，属于他的圈内人。</strong>对马斯克来说，控制推特的唯一方法就是解雇相当多的人，其中许多人是该组织不可或缺的一部分。</p></blockquote><p></p><p><strong>很难看到硬化部分</strong></p><p>（注意：这是<a href="https://www.lesswrong.com/posts/pfL6sAjMfRsZjyjsZ/some-basics-of-the-hypercompetence-theory-of-government">上一篇文章</a>的清理版本，我对其质量不满意。如果您已经阅读过它，请随意跳过这篇文章）。</p><p>一些社会结构可以进化，允许更多人保守秘密。例如，情报机构不仅是相互划分的，而且组成这些机构的员工都认为，如果有人接近他们提出购买机密，这可能是该机构内部的例行反情报行动之一，会引出并起诉不值得信任的员工。结果，这些员工基本上将他们的代理机构<a href="https://www.lesswrong.com/tag/newcomb-s-problem">集中在一起</a>，几乎从不接受外国代理人的贿赂，无论承诺的报酬有多么可笑。任何漏掉的信息都很难摆脱由双重/三重特工伪装成容易受贿的人的虚假信息的影响。</p><p>它比这复杂得多，但这只是机构内部演变的保密系统的一个例子；它不仅足以有效地保守秘密，而且还能够阻止或误导外部特工，明智地试图破坏秘密保守网络（<a href="https://en.wikipedia.org/wiki/Double-Cross_System">几乎一百年前</a>或<a href="https://en.wikipedia.org/wiki/Counterintelligence#History">更早</a>出现）。</p><p>情报机构的高层很难观察。目前尚不清楚产出不足是否主要是由于无能和不感兴趣造成的，或者如此强大的结构内部的激励机制是否导致有能力的个人将自己的能力浪费在内部竞争和淘汰同事上。然而，将更容易接触和观察的普通中低层政府官员/官僚推论到难以观察的高层是危险的。较高梯队的人员分布可能严重失调；例如，在一个关于公司等级制度的过于简化的热尔维模型的思想实验中（“反社会者”高度社交，喜欢聚餐；“无知”者拥有深刻的组织洞察力；“失败者”过着非常幸福的生活，他们“失去”的主要是与其他人一样的衰老过程），一个在金字塔上向上发展的人会逐渐发现感恩节火鸡效应：人类自我分类，导致遇到已经成功追求财富激励的人组织的顶层，因为他们与金字塔中层和底层更容易观察到的人相比，具有不同寻常的、本质上不同的个人特征组合。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pfL6sAjMfRsZjyjsZ/pc6ccmqo9ogw4k3nsdfr"><figcaption>该图像被明确声明为公司层次结构，它被明确声明不是在描述情报机构或有趣的非营利组织，它们以与大多数私营部门公司不同的方式体验<a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Moloch</a> 。</figcaption></figure><p>尽管自由主义学派最基于对政府普遍无能的经验观察，但这不应该分散我们对基本原则的注意力，即拥有 80% 权力的组织中的前 20% 很大程度上是未知的领域，因为很难观察以及各种奇怪的特定动态可能可以解释政府的失败；尽管模型必须以观察为基础，但过度依赖自由主义思想流派仍然存在风险，这种思想主要以低层官僚为对象，并想象政府统一由他们组成，将这些人推论到最高和最理想的职位。情报机构肯定已经注意到，冒充无能的官僚是一种极好的伪装，而且整个政府也都知道，迷宫般的文书工作可以威慑掠夺者。</p><p>组成 EA 的表现最好的利他主义者，远低于全球所有利他主义者的 0.1%，由于极不寻常和极端的环境，包括强大的能力、运气、智力、动机和以富有成效的方式自发组织的能力，他们正处于极端顶峰。以实现工具上趋同的目标。然而，与 EA 不同的是，情报、军事和内部安全机构中最顶尖的 0.1% 的人面临着来自政权更迭的威胁、各种富有和有权势的精英的仰视以及持续的战略渗透攻击带来的令人难以置信的进化优化压力。由外国情报机构。我们根本不清楚民主国家的权力掮客的顶峰最终会演变出什么样的结构，而且在认识上自动顺从自由主义学派并不负有责任，即使自由主义学派是关于无数人的生活被无能的政府干预/监管毁掉的说法是正确的。有能力的人和团体仍然会被排到最高层，面临达尔文主义的压力，即使绝大多数有能力的人一路上都摆脱了官僚主义的废话。情报机构的运作是我们观察到的结果，这些人被赋予了难以置信的权力、有罪不罚、垄断信息的能力，以及利用他们自己与与其共享一个国家的大型、技术先进的私营公司之间的权力和信息不对称。 （企业游说者可以促进甚至现金激励他们与领导者之间各种复杂的讨价还价，特别是包括顶尖人才的旋转门雇佣，而情报机构的权力和声望进一步促进了这一点）。</p><p></p><p><strong>很容易看到软的部分</strong></p><p>情报机构有能力渗透到顽固的官僚机构和其他组织，通过损害人员网络进行横向移动，并指导世界各地（可能包括国内）行政部门和立法部门/议会人员的职业生涯。</p><p>有相关经验的人都知道，在官僚机构中向上和横向晋升是一门科学（还有很多其他事情，其中​​大多数都非常令人不愉快）。在那些比你更进步的人的心目中，晋升和驾驭官僚机构也是一门比你自己更精确的科学。鉴于他们如此成功，他们很可能做了很多正确的事情，并且一路上学到了很多你没有学到的东西。</p><p>然而，同样地，在情报机构专家的心目中，它是一门更加精确的科学，这些机构世世代代专注于系统地渗透、控制和欺骗世界各地顽固官僚机构（和其他组织）的顽固部分（ <a href="https://www.cold-takes.com/most-important-century/#Summary:~:text=More%20info%20about%20these%20timelines%20at%20All%20Possible%20Views%20About%20Humanity%27s%20Future%20Are%20Wild%2C%20This%20Can%27t%20Go%20On%2C%20and%20Forecasting%20Transformative%20AI%3A%20Biological%20Anchors%2C%20respectively.">但只有少数几代人</a>）。人类文明是建立在专业化和分工的基础上的，而情报机构就是专门从事这一工作的人。 <span class="footnote-reference" role="doc-noteref" id="fnrefsapjucliu6o"><sup><a href="#fnsapjucliu6o">[1]</a></sup></span></p><p>由于对轶事的必要依赖，这种信息的不对称性甚至更大，而且由于许多人根据在机构特定部门工作时的氛围做出决策的现象而变得更加复杂。</p><p>这是值得注意的，因为在一个机构的人员<strong>流动率较高的</strong>部分<i>，</i>进出人数不成比例，因此占据了不成比例的大量观察和证词。这进一步加剧了这种动态，即很难看到坚硬的部分，而更容易看到较软的部分，因为众所周知，腐败、无能、暴行/派系主义和低参与度都会大大增加人员流动，而高价值的秘密、众所周知，更有能力的管理人员、有趣的工作和以使命为导向的员工流动率较低，也更容易从顶尖公司招聘顶尖人才。</p><p>此外，评估组织的领土也存在反归纳情况的风险，这些组织的任务包括长期的宣传、虚假信息，特别是反情报，以及利用先进技术利用人类心理（包括通过使用数据）。科学、大规模监视和人工智能）。特别是，摆脱共鸣是一种非常糟糕的方法，因为共鸣是情绪化的、潜意识的，并且很容易获得大量数据并进行科学研究。你对某件事了解得越多，就越容易找到通过用特定刺激刺激某物来获得特定结果的方法。</p><p>与假想的富人和有权势的人打交道，他们专门利用自己的财富和影响力来<a href="https://www.lesswrong.com/posts/xDNyXGCDephBuNF8c/dark-forest-theories">避免将自己的地位拱手让给同样富有和有权势的敌人</a>，需要理解与处理不可证伪的理论相关的人类认知偏见。我的模型看起来很棒，这是<a href="https://www.lesswrong.com/posts/RryyWNmJNnLowbhfC/please-don-t-throw-your-mind-away">一个可以在你的脑海中思考的有趣话题</a>，而难以发现的能力垄断岛理论与飞行的意大利面怪物和隐形龙完全不同；但这些考虑因素也必须以定量的心态进行评估。最终，除了政策结果和众所周知的军事/情报结果之外，几乎没有好的数据，并且这两种假设（情报机构内的统一无能与非统一无能）都必须用现有的最佳认识论来处理。我推荐尤德科夫斯基的<a href="https://www.lesswrong.com/posts/CqyJzDZWvGhhFJ7dY/belief-in-belief">《信仰中的信仰</a>》、 <a href="https://www.lesswrong.com/posts/fAuWLS7RKWD2npBFR/religion-s-claim-to-be-non-disprovable">《宗教的不可反驳性主张》</a> 、 <a href="https://www.lesswrong.com/posts/XTXWPQSEgoMkAupKt/an-intuitive-explanation-of-bayes-s-theorem">《贝叶斯定理的直观解释》</a> （如果你还没有读过的话），以及雷蒙的<a href="https://www.lesswrong.com/posts/xDNyXGCDephBuNF8c/dark-forest-theories">《黑暗森林理论》</a> 。我在这篇文章中描述的限制对于理解情报机构至关重要。</p><p>对这些机构的研究比迄今为止似乎发生的事情需要更好的认识论。</p><p></p><p><strong>测谎仪的发挥作用是人类历史的转折点</strong></p><p>所有人类社会和平衡都部分源于人脑的一个基本特征：<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/">人脑产生谎言比人脑发现谎言更容易，即使是</a>在面对面的对话中，大量的紧张情绪也是如此。揭示非语言交流的交换（例如面部表情、微妙的身体姿势变化）。你不能问别人是否打算背叛你，如果你能问，一切都会不同。</p><p>如果发明了功能性测谎仪，我们所知道的激励结构将完全被更有效的新结构所取代。例如，你可以强迫所有下属戴上脑电图或进入功能磁共振成像仪，然后询问他们所有人谁是办公室里最聪明/最有能力的人，提拔实际上表现最好的人，并解雇任何派系/您发现围绕共同谎言进行协调的人的派系。大多数能够使用有效测谎技术的中层管理人员在作为能够使用有效测谎技术的中层管理人员花费数千小时的过程中都会想到这些事情，以及我尚未想到的许多其他策略。</p><p>如果你对测谎技术的第一反应是“嗯，测谎技术目前极其不准确且无效”，那么这是一个非常可以理解的错误，但也毫无疑问是一个错误。我和很多人谈过这个问题，几乎所有人都自信地输出了基本上完全相同的文本字符串，但不知道它来自哪里或支持它的是什么。我并不怀疑这在 40 甚至 20 年前可能是正确的，但随着现代技术的发展，这更像是一个难以抉择的问题。最好的论文（我愿意分享）涵盖了政府/军事利益和测谎技术的获取，无论是当前还是未来潜在的垄断，都<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/">在这里</a>，其中还涵盖了测谎技术的声誉（这是更容易观察和研究的事情之一）。</p><p>这可能是未来 30 年人类文明相对于过去 80 年人类文明分布不均的最重要方式之一（它不是#1）。</p><p></p><p><strong>我发现有用的信息</strong>：</p><p> <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally">不要从字面上理解组织结构图</a>（强烈推荐）</p><p> <a href="https://www.lesswrong.com/posts/oqvsR2LmHWamyKDcj/large-language-models-will-be-great-for-censorship">法学硕士非常适合审查</a></p><p>雷蒙的黑暗<a href="https://www.lesswrong.com/posts/xDNyXGCDephBuNF8c/dark-forest-theories">森林理论</a></p><p>约瑟夫·奈的<a href="https://www.amazon.com/Soft-Power-Means-Success-Politics/dp/1586483064">软实力</a></p><p><a href="https://www.lesswrong.com/posts/r2vaM2MDvdiDSWicu/the-u-s-is-becoming-less-stable">美国变得越来越不稳定</a></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsapjucliu6o"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsapjucliu6o">^</a></strong></sup></span><div class="footnote-content"><p>另一方面，议会和立法机构更多的是为一个国家的精英提供合法且可持续的影响力，让他们除了搞肮脏的事情之外还有出路（而且一个国家的顶级精英有各种各样的方式来发挥影响力）在财富和权力的巅峰/接近巅峰时肮脏；试着想象一个智商 175 的人能做什么）。与民主国家不同，威权政权更注重隔离精英。他们是友好领域的专家，比如稳健性和政策制定。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionity-is-difficult<guid ispermalink="false"> foM8SA3ftY94MGMq9</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Thu, 24 Aug 2023 01:42:22 GMT</pubDate> </item><item><title><![CDATA[China's position on autonomous weapons]]></title><description><![CDATA[Published on August 23, 2023 10:20 PM GMT<br/><br/><blockquote><p>在继续认可制定具有法律约束力的法律文书的必要性的同时，2018年中国还提出了一个较为狭隘的法律定义，包括五个基本特征：</p></blockquote><blockquote><blockquote><p>第一个是杀伤力，这意味着足够的有效载荷（电荷）使其具有杀伤力。二是自主性，即任务执行的整个过程中不存在人为干预和控制。第三是无法终止，这意味着一旦激活就无法终止设备。第四是无差别作用，即无论条件、场景或目标如何，该装置都会执行杀伤和致残的任务。第五是进化，意味着通过与环境的交互，设备可以自主学习，并以超出人类预期的方式扩展其功能和能力。</p></blockquote></blockquote><blockquote><p> [...] 中国的限制性定义对可能有资格受到法律监管的技术种类设置了极高的门槛。</p></blockquote><blockquote><p>然而，接受我们采访的中国政府专家小组代表并不认为这个定义是“狭隘的”。对他们来说，快速的技术进步可能很快就会使零人为监督的武器成为现实，特别是在美国等技术先进的国家。</p></blockquote><p>具有自主瞄准功能的军用无人机已经存在。土耳其 <a href="https://thebulletin.org/2021/05/was-a-flying-killer-robot-used-in-libya-quite-possibly/">可能使用过其中之一</a>。<a href="https://www.youtube.com/watch?v=G7yIzY1BxuI">以色列也一直在</a>开发自主武器。 <a href="https://en.wikipedia.org/wiki/AeroVironment_Switchblade">Switchblade 600</a>不是自主的，但美国国防承包商有兴趣在此类产品中添加自主瞄准功能，并且似乎正在游说反对美国的相关法规。中国政府似乎强烈支持自主武器，但也强烈反对电影中的终结者。</p><p>到目前为止，重点是小型短程电动无人机的自主瞄准，但对于可以携带更多传感器和计算设备的大型无人机来说，自主瞄准更容易，而远距离数据链路则更困难——特别是当卫星在战争等中成为目标时美国与中俄联盟之间的台湾问题。</p><br/><br/> <a href="https://www.lesswrong.com/posts/QaaunwFaGGFd8cTpy/china-s-position-on-autonomous-weapons#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QaaunwFaGGFd8cTpy/china-s-position-on-autonomous-weapons<guid ispermalink="false"> QaaunwFaGGFd8cTpy</guid><dc:creator><![CDATA[bhauth]]></dc:creator><pubDate> Wed, 23 Aug 2023 22:20:14 GMT</pubDate> </item><item><title><![CDATA[Diet Experiment Preregistration: Long-term water fasting + seed oil removal ]]></title><description><![CDATA[Published on August 23, 2023 10:08 PM GMT<br/><br/><p>我对 exfatloss<a href="https://exfatloss.substack.com/p/seed-oils-explain-the-8-mysteries">最近的理论</a>很感兴趣，即肥胖流行的主要原因是亚油酸多年来缓慢而微妙的损害。具体来说，我想测试更广泛的可能性，即脂肪组织的成分是导致问题的原因。由于我们没有十年的时间来自然地检验他的假设，因此我将重新调整我目前的饮食习惯，以进行修改后的实验。</p><p>我现在刚刚从极低热量饮食过渡到清水断食，在过去几周内减掉了大约十五磅。我将尝试在接下来的三十天内继续禁食，或者直到我的 BMI 达到 &lt;25。水断食结束后，我将严格从饮食中排除亚油酸和其他多不饱和脂肪酸；我的膳食主要包括蔬菜、米饭、特制的低多不饱和脂肪酸猪肉和鱼。那我就再吃一个月吃饱。</p><p>通常情况下，水断食后你会恢复刚刚减掉的所有体重，因为你没有改变身体的饮食“设定点”，并且你浪费了所有的精神力量试图阻止自己进食。然而，如果亚油酸假说是正确的，那么我作为一个优秀的美国人多年来消耗的所有多不饱和脂肪酸首先就是导致我的设定点保持如此高的原因。因此，在这次自我实验中，我将刻意清除大部分剩余的脂肪储备，用传统的日本饮食来填充它们，看看我的“自然体重”是否发生变化。</p><p>如果损害是不可逆的，亚油酸假说不会无效，但我至少可以检验这样的假说：我的脂肪的<i>运行</i>成分是导致我的新陈代谢崩溃的原因。</p><br/><br/> <a href="https://www.lesswrong.com/posts/5KkyygHQL2XCw3kTs/diet-experiment-preregistration-long-term-water-fasting-seed#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5KkyygHQL2XCw3kTs/diet-experiment-preregistration-long-term-water-fasting-seed<guid ispermalink="false"> 5KkyygHQL2XCw3kTs</guid><dc:creator><![CDATA[lc]]></dc:creator><pubDate> Wed, 23 Aug 2023 22:08:49 GMT</pubDate> </item><item><title><![CDATA[The Low-Hanging Fruit Prior and sloped valleys in the loss landscape]]></title><description><![CDATA[Published on August 23, 2023 9:12 PM GMT<br/><br/><p>您可以在<a href="https://github.com/nrimsky/mlexperiments"><i>此 GitHub 存储库</i></a><i>中找到引用实验的代码</i></p><p>许多人假设训练大型神经网络将增强简单性，或<a href="https://www.lesswrong.com/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign,%202020"><u>所罗门诺夫先验</u></a>。这是基于这样的想法，即更简单的解决方案占据了权重空间中的广阔区域（ <a href="https://www.lesswrong.com/posts/xRWsfGfvDAjRWXcnG/dslt-0-distilling-singular-learning-theory,%202023"><u>权重空间中存在更多的泛化方向，沿着这些方向损失不会增加或增加很少</u></a>），转化为一个广泛的吸引子盆地，其中权重调整的扰动具有对损失的边际影响。</p><p>然而，深度学习优化的主力随机梯度下降（SGD）的运行方式挑战了这种以简单性为中心的观点。根据设计，SGD 由当前批次数据的即时梯度驱动。这个过程的本质意味着 SGD 的运行就像一种贪婪的启发式搜索，逐渐接近可能逐渐更好但不一定是最简单的解决方案。</p><p>这个过程的一部分可以理解为“摸索”步骤或相变的集合，其中网络学习并“固化”一个新的电路，对应于正确识别权重之间的某些关系（或者，从数学上讲，找到一个子流形）。 This circuit then (often) remains &quot;turned on&quot; (ie, this relationship between weights stays in force) throughout learning.</p><p> From the point of view of the <a href="https://en.wikipedia.org/wiki/Energy_landscape"><u>loss landscape</u></a> , this can be conceptualized as recursively finding a valley corresponding to a circuit, then executing search within that valley until it meets another valley (corresponding to discovering a second circuit), then executing search in the joint valley of the two found circuits, and so on. As the number of circuits learned starts to saturate the available weight parameters (in the underparametrized case), old circuits may get overwritten (ie, the network may leave certain shallow valleys while pursuing new, deeper ones). However, in small models or models not trained to convergence, we observe that large-scale circuits associated with phase transitions largely survive to the end. </p><figure class="image image_resized" style="width:52.88%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/i3jrid2l6vkwdqgei2ci" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/t7rsgjany28hc9ptztwm 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/jn9y10kfbab3dohwmavz 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/jkyinxbwdc2nfgrvizwr 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/gxuw5sjkvvrd5x9z2rn3 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/fuxxqst31wdjqwzcmdxj 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/nggcv0blwwzpgjw80g04 500w"><figcaption> This gif shows the Fourier modes of the learned embeddings in our modular addition MLP model. Circles correspond to fully grokked circuits of the kind found in <a href="https://arxiv.org/abs/2301.05217"><u>Progress measures for grokking via mechanistic interpretability</u></a></figcaption></figure><h1> A greedier picture</h1><p> This idea aligns with what we call the <i>low-hanging fruit prior</i> concept. Once a solution that reduces loss reasonably is identified, it becomes more computationally efficient to incrementally refine this existing strategy than to overhaul it in search of an entirely new solution, even if the latter might be simpler. This is analogous to continuously picking the lowest-hanging fruit / cheapest way to reduce loss at each stage of the gradient descent optimization search process.</p><p> This model predicts that SGD training processes are more likely to find solutions that look like combinations of shallow circuits and heuristics working together rather than simpler but less decomposable algorithms. In a mathematical abstraction, suppose that we have an algorithm that consists of two circuits, each of which requires getting <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>parameters right (note that this corresponds to a measure of complexity), and each of which independently reduces the loss. Then the algorithm resulting from learning both circuits has a “complexity measure” of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="20"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span> , but is more likely to be learned than a “complexity <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="15"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">15</span></span></span></span></span></span></span> ” algorithm with the same loss if it cannot be learned sequentially (as it is exponentially harder to correctly “guess” 20 parameters than to correctly “guess” 10 parameters twice). Note that in general, the picture is more complicated: even when learning a single “atomic” circuit that cannot be further decomposed, the question of how easy it is to learn is not equivalent to the information content (how many parameters need to be learned), but incorporates more qualitative phenomena like basin shallowness or, more generally, local basin information similar to that studied by <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC/p/xRWsfGfvDAjRWXcnG"><u>Singular Learning Theory</u></a> - thus moving us even further away from the Solomonoff complexity prior.</p><p> An interesting consequence of this is a prediction that for tasks with two (or more) distinct ways to solve them, neural networks will tend to find or partially find both (or multiple) solutions, so long as the solutions have comparable complexity (in a suitable SGD sense). Our <a href="https://www.lesswrong.com/posts/8ms977XZ2uJ4LnwSR/decomposing-independent-generalizations-in-neural-networks"><u>MNIST experiment</u></a> (see below for details) confirms this: We design a network to solve a task with two possible solutions, one being a memorization task of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4\times 4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span> patterns, and the other being the MNIST task of classifying digits; we set them up to have approximately the same effective dimension of (order of magnitude) 100. We observe that both are learned at comparable rates (and indeed, the part of the program classifying MNIST seems more stable). We conjecture that the network gradually learns independent bits of information from both learning problems by recursively picking the lowest-hanging fruit from both classification problems. Ie, by recursively finding the &quot;easiest to learn&quot; circuits that give additional usable information for the classification problem, and which may come from either memorization of patterns or learning the shapes of digits.</p><p> The idea of such a prior is not new. However, it is not sufficiently appreciated in AI safety circles how different this prior is from the simplicity prior: it gives a picture of a neural net as more akin to an ADHD child (seeking out new, &quot;bite-sized&quot; bits of information) than to a scientist trying to work out an elegant theory. Note that this does not imply a limit on the capabilities of current models: it is likely that by iterating on finding low-hanging fruit, modern networks can approach human levels of &quot;depth.&quot; However, this <strong>updates us towards expecting neural nets to have more of a preference for modularity and parallelism over depth</strong> .</p><h2> Connection to the speed prior</h2><p> In <a href="https://www.lesswrong.com/posts/GC69Hmc6ZQDM9xC3w/musings-on-the-speed-prior"><u>some</u></a> <a href="https://www.youtube.com/watch?v=v7Xk6ci3BII&amp;t=1020s"><u>discussions</u></a> of priors, the Solomonoff prior is contrasted with the “speed prior.” The meaning of this prior is somewhat inconsistent: some take it to be associated with the KT complexity function (which is very similar to the Solomonov prior except for superexponential-time programs), and in other contexts, it is associated with properties of the algorithm the program is executing, such as depth. We think our low-hanging fruit prior is similar to the depth prior (and therefore also to speed priors that incorporate depth), as both privilege parallel programs. However, high parallelizability is not strictly necessary for a program to be learnable using a low-hanging fruit approach: it is possible that after enough parallel useful circuits are found, new sequential (and easily learnable) circuits can use the outputs of these parallel circuits to refine and improve accuracy, and a recursive application of this idea can potentially result in a highly sequential algorithm.</p><h1> Modularity and phase transitions</h1><h2> Insights from experiments</h2><p> In our experiments, we look at two neural nets with <i>redundant generalization modules</i> , ie, networks where we can mechanistically check that the network is performing parallel subtasks that independently give information about the classification (which is then combined on a logit level). Our first network solves an image classification task which is a version of MNIST modified to have two explicitly redundant features that can be used to classify the image. Namely, we generate images that are a combination of two labeled datasets (“numbers” and “patterns”) with labels 0-9; these are combined in such a way that the number and the pattern on each image have the same label, and thus contain redundant information (the classification problem can be solved by looking at either feature). </p><p><img style="width:62.71%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/wccpqg5sixzkgdt2tee7"></p><p> We observe that the network naturally learns independent modules associated with the two classification tasks.</p><p> For our other test case, we reproduce a version of <a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><u>Neel Nanda&#39;s modular addition transformer</u></a> , which naturally learns multiple redundant circuits (associated with Fourier modes) that give complementary bits of information about a mathematical classification problem.</p><p> For both of these problems, we examine loss landscape basins near the solution found by the network, and we investigate how neural nets trained under SGD recursively find circuits and what this picture looks like &quot;under a microscope&quot; in the basin neighborhood of a local minimum with multiple generalizations.</p><p> Specifically, our experiments attempt to gain fine-grained information about a neural net and its circuits by considering models with smooth nonlinearities (in our case, primarily sigmoids). We train the network at a local minimum or near-minimum (found by SGD).</p><p> We then examine the resulting model&#39;s basin in a coordinate-independent way on two levels of granularity:</p><h3> 1. Small neighborhood of the minimum: generalizations not distinguishable</h3><p> In the small neighborhood (where empirically, the loss landscape is well-approximated by a quadratic function), we can associate to each generalization module a collection of directions (ie, a vector space) in which this module gets generalized, but some other modules get ablated. For example, here is a graph of our steering experiment for the modified MNIST task: </p><figure class="image image_resized" style="width:95.78%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/a8fjbaugmbhrydudzzkn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/jqs6f1fklbvormy23yie 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/dpfvyupusukbvjlpd292 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/nsyyoj5w4t3efywizjp7 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/zm9f71bpcwkdjfy4b1bi 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/weuz3y31dddemazut7in 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/quhfjjdzflouhyg0swov 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/vtlblvir0ri3lqp4utdx 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/g85sdpe6sluonui9v0rk 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/q4njiib3yleh1homix7a 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/rjeahzozkzyufnixsccy 1879w"></figure><p> It follows from <a href="https://www.lesswrong.com/posts/8ms977XZ2uJ4LnwSR/decomposing-independent-generalizations-in-neural-networks"><u>our work</u></a> that the composite network (red, &quot;opacity 0.5&quot;) executes two generalization circuits in the background, corresponding to reading &quot;number&quot; and &quot;pattern&quot; data.</p><p> In the right chart, we move a distance <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span> in the &quot;number&quot; generalization direction, ablating the &quot;pattern&quot; generalization. This results in very stable loss in the &quot;number&quot; circuit but non-negligible loss in the composite network.</p><p> The vectors we produce for extending one generalization while ablating the other results in almost no increase in loss for the generalization being preserved, high loss for the generalization being ablated, but (perhaps surprisingly) nonzero loss for the &quot;joint&quot; problem. The loss in the joint model is significantly (about an order of magnitude) less than the loss in the ablated circuit. However, it is far from being negligible.</p><p> In fact, it is not surprising that going in a generalization direction of one of the redundant modules does not result in flat loss since the information provided by the two modules is not truly “redundant.” We can see this in a toy calculation as follows.</p><blockquote><p> Suppose that our two classification algorithms A, B attain an accuracy of 91% each by knowing with close to 100% certainty a subset of 90% of “easy” (for the given algorithm) patterns and randomly guessing on the remaining 10% of “hard” cases, and that, moreover, the easy and hard cases for the two algorithms are independent. Suppose that the logits for the “combined” algorithm are a sum of logits for the two constituent subalgorithms. In this case, we see that the constituent algorithms have cross-entropy loss of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-\log(0.1)*0.1\approx 0.23"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.23</span></span></span></span></span></span></span> (associated with 10% accuracy in 10% of cases – the perfectly classified cases don&#39;t contribute to loss). The “combined” network, now, will have perfect loss in 99% of cases (complement to the 1% of cases where both A and B don&#39;t know the answer), and so the cross-entropy loss of the combined network is <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-\log(0.1)*0.01 \approx 0.023"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.01</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.023</span></span></span></span></span></span></span> ; this picture, though a bit artificial, neatly explains the roughly order-of-magnitude improvement of loss we see in our MNIST model when both circuits are turned on compared to when only one circuit is turned on (as a result of steering).</p></blockquote><p> In terms of generalization directions, the quadratic loss when steering towards a particular circuit is within the range of the top 10 or so eigenvalues of the Hessian, meaning that it is very strongly within the effective dimensionality of the task (which in the case of MNIST is of OOM 100). So in the model we consider, we see that looking at only one of the two features very much does not count as a generalization direction from the point of view of quadratic loss. Moreover, the generalization directions for the various circuits do not appear to be eigenvalues of the Hessian and look somewhat like &quot;random&quot; vectors with relatively high Hessian curvature.</p><h3> 2. Larger region where we can observe the ablation of a circuit: generalizations are &quot;sloped canyons&quot;</h3><p> In a larger region with non-quadratic behavior, we find that the distinct generalizations correspond to lower-loss &quot;canyons&quot; within the loss landscape that are, in fact, local minima in all directions orthogonal to the vector connecting them to the minimum of the basin.</p><p> This confirms that the local geography of the loss landscape around models with multiple generalizations can look like a collection of sloped canyons converging towards a single basin. Near the basin is a &quot;phase transition&quot; phenomenon where the canyons stop being local minima and instead flow into a larger quadratic basin. A simplified version of the loss landscape looks like the following graph. </p><figure class="image image_resized" style="width:55.07%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/qtzcdjbscd916ffnv3be"><figcaption> Graph of loss landscape. The origin is the lowest-loss point in the basin. &quot;Sloped canyons&quot; corresponding to x and y coordinate directions converge on this lowest-loss point; points in the x, respectively, y canyons correspond to models that execute different generalizations. </figcaption></figure><p><br></p><figure class="image image_resized" style="width:49.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/pulzdphwpvzkwhrl0hvy"><figcaption> A zoomed-in picture of the above loss function corresponding to our “small-scale” basin analysis: here, the function looks close to quadratic, and the two generalization directions are not easily distinguishable from other directions.</figcaption></figure><h2> Relationship between &quot;sloped canyons&quot; model and low-hanging fruit prior</h2><p> We note that this picture surprised us at first: we originally expected there to be a direction that generalizes one of the “redundant circuits” while not changing the loss. </p><figure class="image image_resized" style="width:54.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/m03w1ypqwddpbrucmmkw"><figcaption> A loss landscape with a large singular manifold of minima along the full coordinate cross.</figcaption></figure><p> Our updated picture of <i>sloped</i> canyons interfaces nicely with our sequential circuit formation prior. In situations where canyons are well-conceptualized as flat (like the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x^2y^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.006em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.082em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> picture) and correspond to a singular locus of minima, we would be less likely to expect sequential learning under SGD (as after learning one circuit, SGD would get “stuck” and stop moving towards the more general point near the origin), and in this picture, if networks learned more general solutions, we would expect this to happen mostly through many generalizations appearing at once (eg, found by the diagonal gradient lines in the level set picture below). </p><figure class="image image_resized" style="width:50.83%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/csqdazjpq92ig1iel4z2"><figcaption> A hypothetical picture where generalizations correspond to “flat” canyons</figcaption></figure><p> What we observe in practice looks more like the following cartoon level set: </p><figure class="image image_resized" style="width:48.13%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/hmpahzykaqbjb8eiejce"><figcaption> Level set where the circuit generalizations correspond to “sloped” rather than flat canyons in the loss landscape.</figcaption></figure><p> This picture is more consistent with learning one generalization at a time (here, the curved lines first get close to a coordinate axis – ie, learn one redundant generalization, then descend to the joint generalization through SGD).</p><p> Having given cartoons for different generalization-learning patterns (either directly learning a general point or learning generalizations one at a time), we can informally compare the two hypotheses to dynamic captures of circuit formation under SGD for our modular addition algorithm. Interestingly, the pattern we see here seems to provide evidence for both mechanisms taking place. Indeed, while circuits do form sequentially, often, pairs or triples of circuits are learned at once or very close together: </p><figure class="image image_resized" style="width:88.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SbzptgFYr272tMbgz/ohp8vfbviayadvpm7zca"><figcaption> Modular addition circuit pair formation in a neural network with chunked 2D embeddings. Here, we are visualizing different “blocks” of the token embeddings of the modular addition problem, and a circuit forms when the embedding layer for one of the blocks finds a circle configuration.</figcaption></figure><p> Note that a pure &quot;greedy search&quot; picture would predict that circuits form one by one according to a Poisson process, which would make this &quot;pair formation&quot; behavior unlikely, and somewhat complicates the sequential circuit formation picture. It would be interesting to do a more rigorous analysis of the stochastic behavior of circuit formation, though we have not done this at the moment. We expect explanations for these phenomena to have to do with a more detailed analysis of basins at various scale levels around the local minimum.</p><h1>致谢</h1><p>This write-up is part of research undertaken in the Summer 2023 SERI MATS program. We want to thank our mentor Evan Hubinger for useful discussions about speed and simplicity priors, and we want to thank Jesse Hoogland, Daniel Murfet, and Zach Furman for comments on an earlier version of this post.<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/SbzptgFYr272tMbgz/the-low-hanging-fruit-prior-and-sloped-valleys-in-the-loss#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SbzptgFYr272tMbgz/the-low-hanging-fruit-prior-and-sloped-valleys-in-the-loss<guid ispermalink="false"> SbzptgFYr272tMbgz</guid><dc:creator><![CDATA[Dmitry Vaintrob]]></dc:creator><pubDate> Wed, 23 Aug 2023 21:12:58 GMT</pubDate> </item><item><title><![CDATA[A Theory of Laughter]]></title><description><![CDATA[Published on August 23, 2023 3:05 PM GMT<br/><br/><h1> 1. tl;dr</h1><p> There should be parallel explanations for laughter at two levels.</p><ul><li> At the brain level, there should be some mechanism / algorithm that produces laughter, and it should fit the data of when people laugh in practice.</li><li> At the evolution level, there should be some explanation for why this mechanism exists in the first place. Why was it adaptive in our ancestors? And where did it come from—are there homologues in other animals?</li></ul><p> I&#39;ll summarize my proposals for both of these, in the opposite order:</p><h2> 1.1 First half of the tl;dr: Laughter in terms of evolution</h2><p> I endorse the popular theory that laughter is an indicator of “play”, homologous to the play-related vocalizations and body language in other animals (eg the dog&#39;s “play bow”).</p><ul><li> <strong>The evolutionary purpose of</strong> <i><strong>play</strong></i> <strong>is “practice for future dangerous situations”</strong> . For example, a wolf pup that engages in play-fighting and play-chasing would presumably be more skilled in its future real-life fights and chases.</li><li> <strong>The evolutionary purpose of</strong> <i><strong>innate communicative play signals</strong></i> <strong>, like laughter in humans and play-bows in dogs, is to reduce the probability of accidental escalation from practice to serious.</strong> For example, if a play-fight between two wolf-pups escalates into a <i>real</i> fight between the pups, that&#39;s dangerous for both pups. If the pups are emitting and responding to communicative play signals, then that kind of escalation is much less likely to happen. It&#39;s kinda the same idea as <a href="https://en.wikipedia.org/wiki/Safeword_(sports)"><u>“safewords” in fight-related sports</u></a> (among other places).</li></ul><h2> 1.2 Second half of the tl;dr: Laughter in terms of brain algorithms</h2><p> My (oversimplified) pseudocode brain <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> for laughter is something like: </p><figure class="table"><table><tbody><tr><td style="background-color:#fff2cc;border:2px solid hsl(0, 0%, 0%)"><p> <strong><u>PROPOSED BRAIN PSEUDOCODE FOR LAUGHTER</u></strong> :</p><ul><li> (A) IF my hypothalamus &amp; brainstem are getting <i>some</i> evidence that I&#39;m in danger<ul><li> <i>(the “evidence” here would presumably be some of the same signals that, by themselves, would tend to activate the sympathetic nervous system)</i></li></ul></li><li> (B) AND my hypothalamus &amp; brainstem are simultaneously getting <i>stronger</i> evidence that I&#39;m safe<ul><li> <i>(the “evidence” here would presumably be some of the same signals that, by themselves, would tend to activate the parasympathetic nervous system)</i></li></ul></li><li> (C) AND my hypothalamus &amp; brainstem have evidence that I&#39;m in a social situation</li><li> (D) THEN I will emit innate play signals (eg laughter in humans), and also I will feel more energetic (on the margin), and more safe, less worried, etc.</li></ul></td></tr></tbody></table></figure><p> Indeed, I expect that there is some genetically-specified neuron group in the hypothalamus or brainstem (or more generally, what I call the <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>Steering Subsystem</u></a> ), and that when future scientists look at its various connections and their functional properties, it will be straightforwardly obvious that this neuron group and its connections are implementing the pseudocode above.</p><p> (Side note: These scientists will also find that this neuron group has various other inputs that make laughing more or less likely on the margin—inputs related to mood etc.—which I omitted from the box for simplicity.)</p><p> Note that nothing in this box is particularly tied to humans. If we&#39;re talking about <a href="https://scholar.google.com/scholar?cluster=14143705629125606901&amp;hl=en&amp;as_sdt=40000005&amp;sciodt=0,22"><u>50kHz rat laughter</u></a> instead of human laughter, I wouldn&#39;t change a single word in the box above. However, later in the post, I will talk about human laughter in particular, including humor, and I&#39;ll argue that this pseudocode box is a plausible match to the circumstances in which people laugh.</p><p> Also, the path by which I initially came to guess this pseudocode box (namely, introspection) was independent of how I came to believe the evolutionary story (namely, I read it in a book and it seemed obviously right). But I claim that the two stories match up beautifully—that the pseudocode box above is the natural, straightforward way to implement the “spec” associated with the evolution story, given background constraints about how I think brain algorithms work in general (see Section 3.3.2 below). That reassures me that I&#39;m on the right track.</p><p> OK, that was the tl;dr. The rest of the article will elaborate on that picture, why I currently believe it, and broader implications.</p><h2> 1.3 Table of contents with section summaries</h2><ul><li> Section 2 will explain the evolution story in more detail.</li><li> Section 3 will explain the brain algorithms story in more detail—in particular, what <i>exactly</i> do I mean by that pseudocode above? And then I&#39;ll go through three main reasons that I think it&#39;s right: (1) the pseudocode matches the evolutionary “spec”; (2) the pseudocode is highly plausible on the neuroscience side; and (3) the pseudocode seems to match the situations in which humans laugh (and in which animals emit analogous play-signals).</li><li> Section 4 elaborates on the latter by fleshing out how to reconcile the pseudocode with everyday experience, in three domains:<ul><li> Section 4.1 asks: How does this pseudocode shed light on laughter in physical play—tickling, chasing, peek-a-boo, water balloon fights, etc.? For example, why can&#39;t you tickle yourself?</li><li> Section 4.2 asks: How does this pseudocode shed light on conversational laughter? For example, how is it that laughter can communicate so many different things in different contexts (eg friendliness versus aggression, or sincerity versus insincerity)?</li><li> Section 4.3 asks: How does this pseudocode shed light on humor and jokes?</li></ul></li><li> Section 5 asks: How <i>exactly</i> is this pseudocode implemented in the brain? I hypothesize that there are innate connections between neuron groups in the hypothalamus and/or brainstem that directly correspond to the pseudocode above. Alas, I cannot tell you exactly which neuron group it is—I think it&#39;s a neuron group that nobody has studied yet. But I think I know what we should be looking for and where, and I think that a neuroscience lab could figure this out in the near future using standard experimental methods.</li><li> Section 6 is the conclusion, in which I will discuss how this post relates to my job as an AGI safety / AI alignment researcher.</li></ul><h1> 2. The evolutionary story</h1><h2> 2.1 What is play, and why do animals have an innate play drive?</h2><p> A <strong>central example of play</strong> in my mind is two young animals play-fighting or play-chasing each other.</p><p> Why are the young animals doing this? There seems to be an obvious primary explanation:</p><ul><li> If a squirrel has spent many hours <a href="https://www.youtube.com/watch?v=DdqjR9V8pig"><u>play-fighting and play-chasing other squirrels as a pup</u></a> , it will presumably do better at <a href="https://www.youtube.com/watch?v=ZGhZHlpSE6k"><u>actual-fighting and actual-chasing other squirrels as an adult</u></a> .</li><li> And likewise, if a squirrel has spent many hours running away from its sibling during play-chase, it will presumably also do better when running away from a predator.</li></ul><p> In other words, <strong>play is practice for dealing with future dangerous situations</strong> .</p><p> And <strong>what is the evolutionary advantage of having an innate play</strong> <i><strong>drive</strong></i> <strong>?</strong> Because, obviously, squirrel pups do not have the foresight and knowledge to deduce from first principles that it&#39;s a good idea to spend some of one&#39;s free time practicing for future dangerous situations. So instead, play is an innate drive.</p><p> In other words: <i>Evolutionarily</i> , play is a means to an end, but <i>from the squirrel&#39;s within-lifetime perspective</i> , play is its own reward—it&#39;s just intrinsically enjoyable.</p><h2> 2.2 What is an innate communicative play signal, and why do animals have them?</h2><p> Play signals are any vocalizations or body language that happen primarily or exclusively when the animal is playing.</p><p> One example is the “play bow” in dogs (which often comes with a high-pitched “play bark”): </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wv8xxg22jkhcjc3kouil" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/icatww4sscedx4ltgrmv 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bhesr7a3d5vdhxyuodep 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/mwoaui6gninysgzxnys0 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/soxnfi6ajbdvco6se0ls 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ziijb7pwvvdxlijsvfww 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/euzjo9cyy9glwnucsptr 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wdyakvp9d3qkpmorxaqf 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ge1r2lqqc822xrpgwyux 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/jmr5dlt2xyjdkrnq7yep 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/sg4w5gt4gt4snt5m5cjt 857w"><figcaption> Examples of the dog “play bow”</figcaption></figure><p> Another is rat laughter, a 50 kHz (ultrasonic) chirp emitted by rats under various circumstances including play-fighting among young rat pups. Jaak Panksepp and his then-student Jeffrey Burgdorf first hypothesized that these chirps are the rat version of laughter in the late 1990s; see for example <a href="https://scholar.google.com/scholar?cluster=14143705629125606901&amp;hl=en&amp;as_sdt=40000005&amp;sciodt=0,22"><u>this 2000 paper</u></a> by them, which involved manually tickling rats.</p><p> (I know what you&#39;re thinking—but don&#39;t worry! Today&#39;s scientists no longer have to suffer the indignity of tickling rats by hand. Instead <a href="https://scholar.google.com/scholar?cluster=3513785698040370829&amp;hl=en&amp;as_sdt=0,22"><u>they can use</u></a> an “automated protocol for tickling in which … rats are forced to move and interact with a constantly rotating rod”.)</p><p> There has been a good deal of follow-up work investigating rat-laughter, some of which I will be referencing below.</p><p> So anyway, other animals have play signals. Why should humans be different? And more specifically, our chimp cousins emit laughter-like panting sounds, and “laugh most when tickled, during rough-and-tumble play, and during chasing games (the chimp being chased laughs most)” (that quote and much more in chapter 5 of <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>Provine&#39;s book</u></a> ).</p><p> (See: <a href="https://youtu.be/ffnyOZGB-Tc?t=14"><u>YouTube of a young chimp getting tickled</u></a> .)</p><p> So we arrive at the theory that laughter is a play-signal for humans, homologous to the play signals in other animals. I think this theory goes back to at least Darwin; <a href="https://plato.stanford.edu/entries/humor/"><u>SEP mentions</u></a> Max Eastman in 1936. For my part, I originally heard this theory from a chapter in the book <a href="https://www.amazon.com/Elephant-Brain-Hidden-Motives-Everyday/dp/0190495995"><i><u>Elephant In The Brain</u></i></a> by Kevin Simler and Robin Hanson. (I found that book chapter very enlightening, although I don&#39;t agree with everything they said. <span class="footnote-reference" role="doc-noteref" id="fnrefv8uvhlgfgso"><sup><a href="#fnv8uvhlgfgso">[1]</a></sup></span> )</p><p> <strong>Why do animals have innate communicative play signals?</strong> Imagine two young squirrels play-fighting. This is a mutually-beneficial activity, for reasons mentioned above. However, it is <i>also</i> possible for two young squirrels to <i>actually</i> fight, an activity which is very <i>dangerous</i> for both of the squirrels, and sometimes also for kin bystanders.</p><p> And now we see the problem: there is an obvious resemblance between play-fighting and actual-fighting—such that <strong>a play-fight could accidentally escalate to an actual fight</strong> . Therefore, each squirrel benefits from communicating to the other that it is play-fighting.</p><p> Thus, we expect animals to have evolved innate mechanisms to emit play signals, along with corresponding innate mechanisms to notice those signals and react to them. <span class="footnote-reference" role="doc-noteref" id="fnrefcligk4e8fce"><sup><a href="#fncligk4e8fce">[2]</a></sup></span></p><h1> 3. The brain story</h1><h2> 3.1 The pseudocode</h2><p> I&#39;ll re-copy the box at the top for easy reference—I think it&#39;s something like this: </p><figure class="table"><table><tbody><tr><td style="background-color:#fff2cc;border:2px solid hsl(0, 0%, 0%)"><p> <strong><u>PROPOSED BRAIN PSEUDOCODE FOR LAUGHTER</u></strong> :</p><ul><li> (A) IF my hypothalamus &amp; brainstem are getting <i>some</i> evidence that I&#39;m in danger<ul><li> <i>(the “evidence” here would presumably be some of the same signals that, by themselves, would tend to activate the sympathetic nervous system)</i></li></ul></li><li> (B) AND my hypothalamus &amp; brainstem are simultaneously getting <i>stronger</i> evidence that I&#39;m safe<ul><li> <i>(the “evidence” here would presumably be some of the same signals that, by themselves, would tend to activate the parasympathetic nervous system)</i></li></ul></li><li> (C) AND my hypothalamus &amp; brainstem have evidence that I&#39;m in a social situation</li><li> (D) THEN I will emit innate play signals (eg laughter in humans), and also I will feel more energetic (on the margin), and more safe, less worried, etc.</li></ul></td></tr></tbody></table></figure><p> There are presumably various other things that modulate this circuit (aka shift the thresholds)—person-to-person variation, and variation with age (and certainly with species), and dependence on other innate signals (eg angry people tend to laugh less). But I think the box above is the main story.</p><h2> 3.2 What <i>exactly</i> do I mean by “evidence that I&#39;m in danger” etc.? How are these things operationalized?</h2><p> To be clear, I am <i>not</i> talking about <i>consciously believing</i> that I&#39;m in danger, or safe, etc. For example, my fight-or-flight reaction can easily activate even when I consciously believe I have nothing to worry about, like during a scary movie.</p><p> Instead I am talking about innate signals in the hypothalamus and/or brainstem—what I call the <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>“Steering Subsystem”</u></a> . These signals (I claim) have particular innate “meanings” / algorithmic purposes with legible relation to ecological / homeostatic requirements. For example, see my discussion <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its#3__Case_study_of_hypothalamus__business_logic___NPY_AgRP_neurons"><u>here</u></a> of a particular group of neurons in the hypothalamus that is activated by a physiological need for food, and causes various downstream effects (like energy conservation and hunger sensations) that are appropriate to that state. These kinds of innate signals can exist even if we have no conscious (interoceptive) access to them, and no common English-language concept that perfectly aligns with them. (See <a href="https://www.lesswrong.com/posts/iYzFKJjzFPRNrqLE3/lisa-feldman-barrett-versus-paul-ekman-on-facial-expressions"><u>my recent post on where I disagree with Lisa Feldman Barrett</u></a> .)</p><p> Anyway, I suggested in the box above that “danger” might be operationalized via some of the same innate signals that activate the sympathetic nervous system (directly or indirectly), and “safe” might be operationalized via some of the same innate signals that activate the parasympathetic nervous system (directly or indirectly). Which signals exactly?我不知道。</p><p> What about ingredient (C), the “social” innate signals? I&#39;m not sure about that either. But there do seem to be innate signals in the hypothalamus and brainstem that are intended (by evolution) to correlate with being in a social situation. One obvious way for evolution to calculate such signals is via heuristics calculated in <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and#3_2_1_Each_subsystem_generally_needs_its_own_sensory_processor"><u>brainstem sensory processing systems</u></a> . For example, a recent preprint I like— <a href="https://www.biorxiv.org/content/10.1101/2023.05.19.540391"><u>Liu</u> <i><u>et al.</u></i> <u>(2023)</u></a> —identified two groups of cells in the hypothalamus (medial preoptic nucleus). The activity of one group correlated with being socially isolated, and the activity of the other group correlated with the end of that isolation. Interestingly, they found that, as far as these two cell groups were concerned, “social” was being operationalized via touch sensations—not sound, not pheromones, it had to be touch. (The mice were blind so they&#39;re not sure about vision.) But that finding of the centrality of touch does not generalize to other social instincts. For example, another rodent social instinct is mating, and here, the innate circuits are triggered at least in part by pheromones. Anyway, in sum, I don&#39;t know exactly how “social” is calculated for the purpose of Ingredient (C) of laughter in rodents, let alone in humans, but such a calculation is definitely a thing that the brain can do.</p><h2> 3.3 Three reasons I like this proposal</h2><h3> 3.3.1 This pseudocode matches the evolutionary “spec” of Section 2</h3><p> I think the correspondence here is strong and straightforward, and I see this as key evidence that I&#39;m on the right track. Going through the items above:</p><ul><li> <i>(A)—some evidence that I&#39;m in danger.</i> If I&#39;m in a situation that is triggering some fight-or-flight bodily reactions—regardless of the exact reason—then I&#39;m almost definitely in a situation that resembles dangerous situations that I might get into in the future. And therefore, being in this situation almost definitely constitutes good “practice”.</li><li> <i>(B)—stronger evidence that I&#39;m safe.</i> Without this ingredient, it&#39;s not “practice”—it&#39;s the real thing! I should be getting <i>out</i> of the situation, not rushing into it! I should find it aversive, not fun.</li><li> <i>(C)—evidence that I&#39;m in a social situation.</i> Without this ingredient, it still qualifies as “practice”, and thus it is still evolutionarily adaptive to stay in this situation, and therefore our evolutionary expectation should be that a situation would be “fun” even without ingredient (C). But without ingredient (C), I don&#39;t expect to <i>laugh</i> , evolutionarily speaking. <i>There is no point in emitting a communicative signal, if there is no one around to hear it.</i></li></ul><p> I put in ingredient (C) based on various studies (in both humans and rats) that much more laughter occurs when conspecifics are around. For example, <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>Provine</u></a> found that laughter was 30× less frequent when people were alone. <span class="footnote-reference" role="doc-noteref" id="fnref1r9n4pmqgz5"><sup><a href="#fn1r9n4pmqgz5">[3]</a></sup></span> (I think watching TV by oneself is an intermediate case—the TV presumably tricks our brains into thinking we&#39;re in a social situation, at least to some extent.)</p><p> So for example, my theory would be: If I go on a roller coaster with friends, we may well be laughing together during the scary parts; whereas if I go for a ride on an <i>otherwise-empty</i> roller coaster, with nobody in eyeshot or earshot, then I&#39;m much less likely to laugh out loud, but I&#39;ll still probably find it fun. Likewise, if I&#39;m alone, I might want to read a humorous book, even if doing so will probably not make me laugh out loud.</p><h3> 3.3.2 This kind of pseudocode is highly plausible on the neuroscience side</h3><p> Unfortunately, despite some effort, I cannot tell you the exact neurons that implement the pseudocode in the box above. See Section 5 below for more details.</p><p> <i>However</i> , the pseudocode above is fully compatible with everything I think I know about the brain. And that counts as strong evidence for me, because &quot;the things I think I know about the brain&quot; are highly constraining! (For some of those constraints, see my post “ <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>&#39;Learning from Scratch&#39; in the Brain</u></a> ”. And see <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"><u>my post on social instincts</u></a> for an example of how those constraints rule out lots of possibilities in practice.)</p><p> In particular, I know that the right kind of signals are in fact present in the hypothalamus &amp; brainstem (see Section 3.2 above), and I know that the genome is easily capable of building a little cluster of neurons in (probably) the hypothalamus that perform the requisite logical operations. I have seen lots of little clusters of neurons in the hypothalamus doing broadly this kind of genetically-specified <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>business logic</u></a> .</p><h3> 3.3.3 This pseudocode seems to fit the data of when humans laugh (and when animals emit analogous play-signals)</h3><p> As far as I can tell, the pseudocode is compatible with all our experience of laughter in humans and corresponding play-signals in other animals, including all three of the categories elaborated in Section 4 below—physical play in Section 4.1, non-“humor” conversational laughing in Section 4.2, and humor in Section 4.3.</p><p> You can read Section 4 to better understand where I&#39;m coming from. But then you should take a long hard look at the pseudocode box in Section 3.1 above, and then scroll down to the comments section, and complain about some situation where the predictions of that pseudocode box are wrong!</p><p> …And then I&#39;ll respond with things like:</p><ul><li> “OK sure, but that&#39;s because the pseudocode box of Section 3.1 is oversimplified, like I left out how the laughter-reaction is suppressed by other innate signals correlated with anger and mating and some other things like that.”</li><li> “OK sure, but I was using words like &#39;safe&#39; and &#39;dangerous&#39; when obviously those are just the closest English-language words I can think of, not perfect descriptions, and they come apart from the actual innate signals in various ways, which by the way I cannot specify in detail”.</li></ul><p> …And then you will roll your eyes at me and accuse me of special pleading and unfalsifiability.</p><p> Totally fair!</p><p> But I still currently think I&#39;m getting much more out of this pseudocode box than I&#39;m putting in—and that&#39;s even without knowing exactly where the corresponding neurons are and what they&#39;re connected to.</p><p> (But please do leave those kinds of skeptical comments! They will be very appreciated!)</p><h1> 4. Relating the pseudocode to everyday experience</h1><p> There isn&#39;t a sharp line between them, but I&#39;ll separately discuss three categories: laughter in physical play, laughter in non-“humor” conversation, and humor. In that order:</p><h2> 4.1 Laughter in physical play (eg tickling, chasing, peek-a-boo, etc.)</h2><p> (This is the most straightforward case, and also the case where humans are most similar to other animals.) </p><figure class="image image_resized" style="width:64.13%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ebchyilyqsuggtmnawsw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/khphripzpyad2by4qh41 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/r3h2lhgnjah5isy28tdm 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ih8lfcypnagdzxltynzd 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/favfninitj7z9re0bhgf 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wclq1oixf8awu5x4xpho 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/z7giwgnxptperdyu0hpw 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/gow6ozwz2nekf8wfh0aa 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ffblulfjkar3x3voonne 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/xwtrejdd9n31qkigjhq5 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ir4xijg6lmerresllig9 1197w"><figcaption> Examples of kids laughing during physical play—getting tossed in the air (left) or sprayed with a water gun (right). Image sources: <a href="https://fineartamerica.com/featured/father-tossing-baby-son-in-air-c1950s-debrockeclassicstock.html">1</a> , <a href="https://www.seattlepi.com/national/slideshow/Squirt-gun-fight-14475.php">2</a></figcaption></figure><h3> 4.1.1 What are the sources of “Ingredient (A)” (ie, evidence of danger / cause for physiological arousal) in physical-play laughter?</h3><p> In physical-play laughter, I think there are many possible sources of Ingredient (A), and they&#39;re mostly pretty obvious. In particular, we (like almost all animals) have a diverse suite of innate defensive reactions, including in our case:</p><ul><li> <a href="https://en.wikipedia.org/wiki/Startle_response"><u>Startle</u></a> and <a href="https://en.wikipedia.org/wiki/Orienting_response"><u>orienting responses</u></a> to unexpected sensory inputs</li><li> Various types of <a href="https://en.wikipedia.org/wiki/Reflex"><u>defensive flinches</u></a> in anticipation of physical trauma</li><li> Some innate reaction to the sense that you&#39;re falling down</li><li> Running away from threats (“ <a href="https://en.wikipedia.org/wiki/Escape_response"><u>escape response</u></a> ”)</li><li> ETC。</li></ul><p> All of these innate reactions trigger not only certain muscle behaviors, but also physiological arousal—hence Ingredient (A).</p><p> And all of these are situations where you&#39;ll see little kids laughing uproariously during physical play.</p><h3> 4.1.2 More discussion of tickling in particular</h3><p> It strikes me as extremely obvious that tickling is part of play-fighting. For example, the most ticklish parts of your body seem to coincide with the parts that are most vulnerable to serious injury, like the front of the neck. <span class="footnote-reference" role="doc-noteref" id="fnref34vhiqc4mc2"><sup><a href="#fn34vhiqc4mc2">[4]</a></sup></span> So the evolutionary story behind tickling is straightforward. What about the brain-level story?</p><p> I claim that the pseudocode of Section 3.1 above is perfectly adequate to explain everything about tickling at the brain-level. To flesh that out, here are some additional details and discussion:</p><p> <u>The source of Ingredient (A)</u> in tickling is basically the normal <a href="https://en.wikipedia.org/wiki/Startle_response"><u>startle</u></a> and <a href="https://en.wikipedia.org/wiki/Orienting_response"><u>orienting responses</u></a> to unexpected sensory inputs, as mentioned above. But, for obvious evolutionary reasons, those innate reactions are presumably <i>especially strong</i> for touch sensations on vulnerable parts of your body. Think of the circumstances when those reactions would naturally trigger—maybe there&#39;s a scorpion crawling on your neck, or maybe your enemy has successfully gotten his hands around your neck during a fight, etc.</p><p> You can&#39;t tickle yourself for the same reason that you do not routinely have an involuntary <a href="https://en.wikipedia.org/wiki/Startle_response"><u>startle reaction</u></a> from the sound of your own voice when you start talking, and the same reason that you do not routinely have an involuntary <a href="https://en.wikipedia.org/wiki/Orienting_response"><u>orienting reaction</u></a> when you wave your own hand in front of your own eyes. (Unless of course you have schizophrenia—see <a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2"><u>here</u></a> —or if you are an infant surprising yourself by making a funny noise for the first time, etc.)</p><p> <u>The source of Ingredient (B)</u> in tickling is the knowledge that you&#39;re safe among trusted friends. If you&#39;re getting “tickled” in a situation where you&#39;re genuinely terrified for your own safety, my impression is that you&#39;re going to be screaming rather than laughing.</p><p> Related to this, even if someone enjoys getting tickled in general, they will still try to push your hand away from the most ticklish areas. The <i>evolution-level</i> reason for this behavior is obvious: that&#39;s part of how play-fighting works. It would hardly be good defense practice otherwise! But what about the <i>brain-level</i> reason? I propose that when their sensitive areas are successfully getting tickled, Ingredient (A) gets so strong that it cuts off Ingredient (B), tipping the experience from fun into aversion.</p><h2> 4.2 Laughter in non-“humor” conversation</h2><h3> 4.2.1 Background: Most conversational laughter is not “humor”</h3><p> Even if we ignore physical play, the connection between “humor” <span class="footnote-reference" role="doc-noteref" id="fnrefwfrfar7cs1"><sup><a href="#fnwfrfar7cs1">[5]</a></sup></span> and laughter is less tight than you might think. In <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>Provine&#39;s book</u></a> he talks about his “ecological” studies of modern USA people hanging out in public, and says “only about 10%-20% of [comments immediately preceding somebody laughing] were estimated by my assistants to be even remotely humorous”. Then follows a helpful table of typical laugh-eliciting comments, including such zingers as “I&#39;ll see you guys later!” and “Can I join you?”</p><p> Pay close attention next time you&#39;re in (or overhearing) a normal face-to-face group conversation, and you&#39;ll probably notice something similar.</p><p> Other cultures seem to be similar to the USA in that respect—for example <a href="http://www.scholarpedia.org/article/Hunter-Gatherers_and_Play"><u>this article</u></a> talks about hunter-gatherers laughing in response to gentle teasing and such—not “why did the chicken cross the road”.</p><h3> 4.2.2 What are the sources of “Ingredient (A)” (ie, evidence of danger / cause for physiological arousal) in conversational laughter?</h3><p> Section 4.1.1 listed a bunch of obvious ways that arousal gets invoked in <i>physical</i> play, like if you think you&#39;re alone but someone suddenly jumps out at you from hiding while screaming at you. But what about conversation—can <i>mere words</i> invoke physiological arousal too? Yes, obviously! Wandering into a stressful conversation topic can get your heart rate up just as surely as can wandering into a swarm of angry bees.</p><p> To be more specific, it seems to me that there are a great many natural sources of Ingredient (A) in conversations, including:</p><ul><li> Confusion</li><li> Embarrassment / guilt / shame (possibly vicarious)</li><li> Disgust (possibly vicarious)</li><li> Surprise (possibly vicarious)</li><li> Threats (both bodily threats and status threats) (possibly vicarious)</li><li> Taboo-breaking</li></ul><p> These are not mutually-exclusive, and there are others too.</p><h3> 4.2.3 Laughter communicates a pretty universal message about my transient “internal” state, but then the listener has to infer why I feel that way, and the latter inference is complicated, contextual, and widely-varying.</h3><p> Remember from above, I&#39;m claiming that “avoiding accidental escalation during practice for future dangerous situations” is the evolutionary explanation of why the brain mechanism for laughter is there in the first place. But <i>given</i> the existence of that brain mechanism, it will be active in lots of situations, many of which may have nothing to do with “avoiding accidental escalation during practice for future dangerous situations”. (See <a href="https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers"><u>“Adaptation-executors, not Fitness-maximizers”</u></a> .) And I think this is <i>much more</i> true in humans than in other species, who seem to mostly just laugh during play-fighting, play-chasing, etc.</p><p> In <i>all</i> situations, I think the universally-shared “meaning” of laughter is related to certain signals in the head of whoever is laughing, as specified by the pseudocode of Section 3.1. But then the listener needs to infer from context <i>why</i> those signals are in the laugher&#39;s head. And here things get very complicated and contingent. Some examples:</p><ul><li> <u>Scenario 1:</u> You are walking by me in middle school, when I just dropped my contact lens. I say “please help me” with a desperate tone, and you laugh as you say “sorry, I don&#39;t help losers”.</li><li> <u>How I might take that:</u> I know that you feel safe (ingredient (B)), and I might guess that it&#39;s because I&#39;m in your outgroup and that you regard my suffering as no threat to your own well-being. I also know that you feel a bit of ingredient (A), and maybe I&#39;ll guess that you feel like it&#39;s cringe that I even dared ask you for help in the first place. In short, your laughter communicated to me that you feel unsympathetic and superior.</li><li> <u>Scenario 2:</u> You are my spouse. I say “Man, my company is really dysfunctional”, while slightly laughing, and in response you laugh as you say “yeah”.</li><li> <u>How I might take that:</u> Well, for my part, I was laughing mainly because I felt kinda annoyed about the dysfunction (Ingredient (A)), but also not overly worried for my own sake (Ingredient (B)). Then when you laugh in your response, I might infer that you are invested in my well-being, and empathetically mirroring my feelings on both counts. In short, your laughter communicated to me that you feel empathy and comradery.</li></ul><p> I could go on. I think that, in different contexts, laughter can signal friendship, or animosity, or superiority, or inferiority, or sincerity, or insincerity, etc. etc. There is no simple theory, because we can feel a certain way for many unrelated reasons.</p><h3> 4.2.4 …And given that laughter is able to communicate stuff, people skillfully wield laughter as part of their communicative toolkit</h3><p> In the previous section I was implicitly treating laughter as an incidental side-effect of the emotions that someone feels in the course of a conversation. But once humans have learned (consciously or unconsciously) that laughter communicates things, they will start <i>wielding</i> laughter to skillfully advance their communicative intentions. For example, in Scenario 2 just above, maybe you laughed in part <i>because</i> you <i>wanted</i> to communicate empathy and comradery.</p><p> This doesn&#39;t have to be a conscious explicit decision or desire, and in fact it probably usually isn&#39;t. It&#39;s probably more often an unconscious habit—in lots of previous conversations, you&#39;ve laughed or not-laughed in a certain way in a certain context, and it led to good results, so you unconsciously learned to repeat that behavior next time you&#39;re in a similar situation.</p><p> While purposeful control of laughter is not directly part of the “business logic” I wrote down in Section 3.1, we obviously can in fact voluntarily laugh. Mechanistically, I think it typically (though not always <span class="footnote-reference" role="doc-noteref" id="fnrefb4hvx2pwe9g"><sup><a href="#fnb4hvx2pwe9g">[6]</a></sup></span> ) happens indirectly—if we want to laugh, we steer ourselves into a transient emotional state that has Ingredients (AC), and if we want to <i>not</i> laugh, we steer ourselves into a transient emotional state that <i>doesn&#39;t</i> have all three ingredients. How do we do that? Well, we have some control over our transient emotional state because we can attend to some aspects of our situation rather than others, choose which frames / analogies to mentally invoke, etc.</p><h2> 4.3 Humor</h2><p> As mentioned in 4.2.1 above, humor does not necessarily cause people to laugh, and most laughter occurs in the absence of humor.</p><p> That said, humor can obviously lead to laughter, so I ought to briefly say something about how. I don&#39;t have a grand theory of humor, nor do I think there is one, beyond what I&#39;ve already said in this post. I&#39;ll just mention a few considerations that I find helpful to keep in mind when thinking about humor and jokes.</p><h3> 4.3.1 What are the sources of “Ingredient (A)” (ie, evidence of danger / cause for physiological arousal) in humor?</h3><p> I think the list is pretty similar to Section 4.2.2 above for conversational laughter above. I won&#39;t re-copy it—you can scroll up. Again, that list is not exhaustive, nor mutually exclusive.</p><h3> 4.3.2 There&#39;s an inverted-U dynamic for “Ingredient (A)”</h3><p> According to the pseudocode box above, if there is too little of Ingredient (A), there&#39;s no laughter (eg a boring conversation), and if there&#39;s <i>too much</i> (A), then there&#39;s <i>also</i> no laughter, because it undermines Ingredient (B) (eg it might just feel stressful, painful, scary, confusing, etc.) Somewhere in between is optimal for laughter.</p><p> So I think we wind up with inverted-U dynamics like this: </p><figure class="image image_resized" style="width:70.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/dxsimdii3jipjtwcteq5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/m65s9gzgstqaakgexd12 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/hqeoiud5c412kvr8hdhw 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/b7pnh20qevjag7ivn5ai 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/oezoivlqsttjvgwgopco 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/xu8b4ceqxxa14ccbxnle 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ltqhjjnxgxck4zgvbiwv 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/dghb3hs4pskf0gtvphxm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/t2kmegbcd84hexaioqlw 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/zfbf7eej24mu1ftigvkd 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/srqjui8mbp9a4bouagns 841w"><figcaption> My theory predicts an inverted-U dependence between Ingredient (A) (in the pseudocode box above) versus laughing—more (A) leads to more laughing, until it&#39;s so much that it cuts off Ingredient (B) and tips to aversion. (See prior discussion of lots of proposed inverted-U&#39;s in humor in <a href="https://www.amazon.com/Psychology-Humor-Integrative-Approach/dp/0128121432"><u>Martin &amp; Ford 2018</u></a> .)</figcaption></figure><p> (I was implicitly talking about this same inverted-U in Section 4.1.2 above, when discussing why someone might enjoy getting tickled a little bit, but find direct tickling of their most ticklish spots to be <i>too much</i> , and thus aggressively push the tickler away.)</p><h3> 4.3.3 The (somewhat arbitrary and drifting) cultural expectations / tropes / rituals surrounding “humor” can be a key component of the explanation of why something is funny.</h3><p> In particular, if a listener has a general cultural expectation that “humor” should involve X (eg a punchline), then a joke-teller can <i>either</i> :</p><ul><li> Increase the amount of Ingredient (B) by conspicuously including X (eg an obvious punchline), <i>OR</i></li><li> Increase the amount of Ingredient (A) by conspicuously <i>excluding</i> X (eg punchline-free absurdist <a href="https://en.wikipedia.org/wiki/Anti-humor"><u>anti-humor</u></a> )</li></ul><p> Either of these can be helpful for the joke, depending on how much (A) and (B) are present from other sources.</p><p> And if the latter (exclusion of X) happens a lot, then we collectively stop expecting X in the first place, causing gradual (anti-inductive <span class="footnote-reference" role="doc-noteref" id="fnref7vihiltl1wd"><sup><a href="#fn7vihiltl1wd">[7]</a></sup></span> ) drifts in what people find funny, or schisms between humor-subcultures.</p><h3> 4.3.4 I&#39;m not generally impressed by “theories of humor” beyond their overlap with the discussion above.</h3><p> For example, the <a href="https://www.amazon.com/Psychology-Humor-Integrative-Approach/dp/0128121432"><i><u>Psychology of Humor</u></i> <u>textbook</u></a> (Martin &amp; Ford 2018) describes three “classic theories of humor” which I will comment on in turn:</p><ul><li> <u>“Relief theory”</u> seems to capture the kernel-of-truth that many instances of humor follow the following time-course: <i>first</i> , we have a bunch of Ingredient (A), and <i>second</i> , something changes and introduces a heap of Ingredient (B) (while the (A) has not yet fully faded from our brainstems). Those two steps can be described as “tension” and “relief of tension” respectively.</li><li> <u>“Superiority theory”</u> seems to capture the kernel-of-truth that, in many instances of humor, Ingredient (A) is vicarious—from imagining someone in danger or duress—while Ingredient (B) comes from my comfort in the knowledge that I, the listener, am not that person, and am superior to that person, and have therefore nothing to fear for myself.</li><li> <u>“Incongruity theory”</u> strikes me as a mishmosh of different things. For example, Ingredient (A) can be based on the feeling of confusion, and (B) by its resolution. Or Ingredients (A) and (B) can come from different (incongruous) ways to view the same situation, one of which is normal / safe and the other embarrassing / dangerous / etc. In still other cases, I wonder whether we&#39;re all just following a cultural telling-a-joke script, and as a listener, Ingredient (A) is my concern that I don&#39;t “get the joke” and will be embarrassed to admit it, and Ingredient (B) is my relief when I do. <span class="footnote-reference" role="doc-noteref" id="fnrefikpolxri2te"><sup><a href="#fnikpolxri2te">[8]</a></sup></span> It can also be several of these things simultaneously, and more.</li></ul><p> After that, the textbook moves on to discuss three “contemporary theories of humor” (“reversal theory”, “comprehension-elaboration theory”, and “benign violation theory”). My comments on those would largely overlap with the above bullet points, so I&#39;ll just move on.</p><h1> 5. More detailed discussion of the neuroscience</h1><h2> 5.1 Overview</h2><p> The kind of <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> pseudocode of Section 3.1 is to be found, I claim, in what I call the “Steering Subsystem” (hypothalamus and brainstem—see definition and discussion <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>here</u></a> ). My guess is more specifically as follows: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/leosou3rtkiyirfpehod" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ujuedwlmm3dbdmbjbttq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/yjn5d7d8xbmkzzwudqpd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bzy6ukm6m8kqsknw0i0i 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/vpghumpc2tor0bjhx73z 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/vlaxjkcqakkihmbqwi9x 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/q78lge8vqgdgpilvinoo 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bmr53pm1hix6vl7osi0a 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/y01cp19oberkrbt8ryz6 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/eqqsconoltp9sbrcufhu 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/c3zll5rr3axazecnaiip 1540w"></figure><h2> 5.2 Where <i>exactly</i> in the brain is the “laugh behavior controller” (top box in that diagram) where I can read out the alleged pseudocode of Section 3.1?</h2><p> Sadly, I have failed to deduce from existing literature where in (probably) the hypothalamus we would find the core “business logic” pseudocode of Section 3.1. (It could also be split among a couple places.)</p><p> Focusing on rats (although I expect the answer to be similar in humans), one candidate mentioned in the literature is the so-called “parvafox nucleus” of the lateral hypothalamus. Check out<a href="https://scholar.google.com/scholar?cluster=12802221367573840133&amp;hl=en&amp;as_sdt=0,22"><u>Alvarez-Bolado &amp; Celio (2016)</u></a> for an argument along those lines. As far as I can tell, the strongest evidence in favor of this hypothesis is that bilateral destruction of the parvafox nucleus dramatically (factor of >;10) reduces rodent laughter, according to <a href="https://doi.org/10.1016/j.bbr.2015.11.004"><u>Roccaro-Waldmeyer</u> <i><u>et al.</u></i> <u>(2016)</u></a> . Relatedly, there is some evidence (including from laughter-inducing “gelastic seizures”) that “stimulations of [the] tuberal portion [of the lateral hypothalamus] provoke bursts of laughter” (<a href="https://scholar.google.com/scholar?cluster=12802221367573840133&amp;hl=en&amp;as_sdt=0,22"><u>Alvarez-Bolado &amp; Celio (2016)</u></a> ), and that&#39;s in the general vicinity of parvafox.</p><p> However, I think the current balance of evidence is that parvafox is <i>not</i> the controller for laughter, but rather is related to defense behavior—based on both direct stimulation of those cells (eg <a href="https://www.biorxiv.org/content/10.1101/2023.01.10.521942"><u>Cola</u> <i><u>et al.</u></i> <u>(2023)</u></a> ), and looking at where in the brain they project to (eg <a href="https://scholar.google.com/scholar?cluster=1669280582819001741&amp;hl=en&amp;as_sdt=0,22"><u>Celio</u> <i><u>et al.</u></i> <u>(2013)</u></a> , <a href="https://scholar.google.com/scholar?cluster=9172453117902267421&amp;hl=en&amp;as_sdt=0,22"><u>Bilella</u> <i><u>et al.</u></i> <u>(2016)</u></a> ). We still need to explain the Roccaro-Waldmeyer results from the previous paragraph, but there are a couple possibilities for that, including (1) that parvafox is essential for “Ingredient (A)” of the pseudocode of Section 3.1 (and thus upstream of laughter), or (2) that parvafox is <i>physically proximate to</i> the “real” laugh behavior controller and that Roccaro-Waldmeyer destroyed the latter accidentally in their experiments. (Lesion experiments are notorious for “collateral damage” of nearby neurons and fibers, if I understand correctly, but be warned that I&#39;m not an expert.)</p><p> Well, if it&#39;s not parvafox, then what is it?</p><p>我不知道。 If someone wanted to make progress on this question experimentally—to actually find that pseudocode implemented via innate brain signaling pathways—I think an obvious immediate next step would be a retrograde neural tracing experiment starting from the (lateral) part of periaqueductal gray (PAG) associated with laughter (as pinpointed in the very recent article <a href="https://www.cell.com/neuron/abstract/S0896-6273(23)00477-4"><u>Gloveli</u> <i><u>et al.</u></i> <u>(2023)</u></a> ), followed by further characterization of whatever upstream neuron-groups show up. Maybe such data already exists, and I missed it. (Parvafox does <i>not</i> seem to project to the correct part of PAG to match up with the Gloveli <i>et al.</i> neurons, as far as I can tell, although I&#39;m not super-confident.)</p><p> Areas that I see as <i>especially suspicious</i> include:</p><ol><li> neurons nearby but not part of the parvafox nucleus, in the tuberal region of lateral hypothalamus (for reasons stated above);</li><li> neurons somewhere in or around the medial preoptic area of the hypothalamus—for which stimulation can cause rat laughter ( <a href="https://scholar.google.com/scholar?cluster=8163760487852460824&amp;hl=en&amp;as_sdt=0,22"><u>Wintick &amp; Brudzynski (2001)</u></a> ), and which is known to be the home of the main behavior controllers for at least two other positive social behaviors that I know of, namely the behavior studied by <a href="https://www.biorxiv.org/content/10.1101/2023.05.19.540391"><u>Liu</u> <i><u>et al.</u></i> <u>(2023)</u></a> mentioned above, plus aspects of mating as discussed in <a href="https://www.cell.com/cell/abstract/S0092-8674(23)00798-5"><u>Bayless</u> <i><u>et al.</u></i> <u>(2023)</u></a> ;</li><li> more generally, the rest of the hypothalamus too.</li></ol><h2> 5.3 What about the “Learning Subsystem” (cortex, striatum, amygdala, hippocampus, etc.)?</h2><p> As I described <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>here</u></a> , I claim that the brain should be divided into two subsystems, the “Learning Subsystem” which runs <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>randomly-initialized learning algorithms</u></a> , and the “Steering Subsystem” which runs <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> . Above I was only talking about the “Steering Subsystem”; but in fact laughter also interacts with the “Learning Subsystem”—ie, the cortex, striatum, amygdala, hippocampus, thalamus, cerebellum, etc.</p><p> I claim that  the laughter-triggering signals coming from the Learning Subsystem are in the following two categories:</p><ul><li> (A) Outputs trained by reinforcement learning to maximize some signal from the Steering Subsystem (hypothalamus &amp; brainstem) that acts as a “reward”, or</li><li> (B) <a href="https://www.lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"><u>“Short-term predictors”</u></a> of some signal from the Steering Subsystem.</li></ul><p>例如：</p><ul><li> An example of (A) would be voluntary / learned control of laughter (Section 4.2.4).</li><li> An example of (B) would be a “self-fulfilling prophecy”, where you laugh because you&#39;re in a situation that pattern-matches to other situations that have caused you to laugh in the past.</li></ul><p> I think there are probably other examples too, but that&#39;s outside the scope of this post.</p><p> Direct evidence that the cortex is not <i>necessary</i> for play is provided by “decorticate” rats (= rats whose cortex has been surgically removed), whose play is “much the same [as] controls” ( <a href="https://psycnet.apa.org/record/1990-98262-005"><u>Whishaw 1990</u></a> , <a href="https://www.sciencedirect.com/science/article/abs/pii/0031938494902852"><u>Panksepp</u> <i><u>et al.</u></i> <u>1994</u></a> ). I can&#39;t immediately find direct measurements of the presence of the normal 50kHz “laughing” vocalizations in decorticate rats while they play, but if their play is similar in other respects, I would certainly <i>guess</i> that they are also vocalizing in a roughly normal way.</p><h1>六，结论</h1><p>I remain adamant that the hypothalamus and brainstem are full of hundreds-to-low-thousands of specific neuron groups with specific connections that are all written directly into the genome, and which correspond to <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> that makes evolutionary sense—things like “if you&#39;re malnourished, reduce your sex drive”. I think that <i>understanding</i> this tangle of “business logic” is annoying but possible, and that doing so seems possibly helpful for AGI safety, for reasons spelled out <a href="https://www.lesswrong.com/posts/qusBXzCpxijTudvBB/my-agi-safety-research-2022-in-review-and-plans#2__Second_half_of_2022__1_3___My_main_research_project"><u>here</u></a> .</p><p> I consider laughter an “easy” example of such business logic, as compared to something like the human innate status drive (assuming that there <i>is</i> a human innate status drive, which I currently believe but am not 100% sure). I think the human innate status drive would need substantially more convoluted pseudocode than the box in Section 3.1, and I don&#39;t even have a plausible guess right now for how it works in detail. (See <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"><u>here</u></a> for vague thoughts.) So maybe this post on laughter is a warm-up. In particular, I don&#39;t think this post is <i>directly</i> important for AGI safety—if we iron out the details of the pseudocode of Section 3.1, and put it into the source code of future AGIs, and then we find that those AGIs are laughing in a vaguely-psychopathic-human-like way while they mercilessly murder me and my family and every other human … then that doesn&#39;t really make me feel much better!</p><p> Still, even if understanding laughter is just a “warm-up” for more safety- and alignment-relevant things like compassion and friendship, I am still very interested in getting this right! I spent much longer on this blog post than usual (admittedly a low bar), including trying to be reasonably comprehensive in reading relevant sources, etc., and I am very eager for feedback.</p><p> <i>(Thanks Seth Herd, Linda Linsefors, Justis Mills, and Miguel De Guzman for critical comments on drafts.)</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnv8uvhlgfgso"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv8uvhlgfgso">^</a></strong></sup></span><div class="footnote-content"><p> Since the works of Robin Hanson are popular on this forum, I will say a bit more about where I differ from <i>Elephant In The Brain</i> . My biggest complaint is the part where they say:</p><blockquote><p> As we mentioned earlier, people are profoundly ignorant about laughter&#39;s meaning and purpose (at least in our default state, before learning the science). But where does this ignorance come from? Why does introspection fail us so spectacularly here?</p><p> It&#39;s not simply because laughter is involuntary, outside our conscious control. Flinching, for example, is also involuntary, and yet we understand perfectly well why we do it: to protect ourselves from getting hit. Thus our ignorance about <i>laughter</i> needs further explanation.</p></blockquote><p> I disagree that it “needs further explanation”. I think <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>we start out ignorant of</u> <i><u>literally everything</u></i></a> , until we learn it / figure it out. And I think that figuring out the evolutionary purpose of laughter is just inherently much harder than figuring out the evolutionary purpose of flinching. It&#39;s less obvious / salient, for various reasons that I claim are pretty obvious if you think about it. I don&#39;t think there&#39;s any more to it than that.</p><p> I also don&#39;t think there <i>can</i> be more to it than that. To explain what I mean by that, imagine if I said: “Here&#39;s the source code for training an image-classifier ConvNet from random initialization using uncontrolled external training data. Can you please edit this source code so that the trained model winds up confused about the shape of Toyota Camry tires <i>specifically</i> ?” The answer is: “Nope.对不起。 There is no possible edit I can make to this PyTorch source code such that that will happen.” By the same token, <i>even if</i> , as that book argues, there is a strong evolutionary pressure to make humans <i>specifically</i> confused about the evolutionary purpose of laughter, I don&#39;t think there is any possible genetic change that would make that happen. Related discussion <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human#13_2_2_Claim_2__Social_instincts_are_tricky_because_of_the__symbol_grounding_problem_"><u>here</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fncligk4e8fce"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcligk4e8fce">^</a></strong></sup></span><div class="footnote-content"><p> Whenever there&#39;s a claim about communicative signals, one can ask a follow-up question of whether these signals are game-theoretically stable against “lies”. Like, what if Squirrel A laugh-squeaks to signal play, then Squirrel B lets down its guard, then Squirrel A attacks for real? If this happened a lot, wouldn&#39;t squirrels eventually evolve to ignore play signals? And if <i>that</i> happened, then wouldn&#39;t squirrels further evolve to stop emitting play signals in the first place?好问题！ It&#39;s worth thinking about those kinds of things. But I do think a good answer exists, even if I don&#39;t know it in full detail. I think there are various ways that signals can be hard or costly to fake. And I also think that animals treat a play-signal <i>by itself</i> as insufficient reason to let down one&#39;s guard, which reduces the benefit of lying. Animals are reacting to other cues too, like whether there is an existing trusting relationship. For example, if I&#39;m a prisoner, and the cruel guard is pointing at me and laughing, that sure wouldn&#39;t make me feel more relaxed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1r9n4pmqgz5"> <span class="footnote-back-link"><sup><strong><a href="#fnref1r9n4pmqgz5">^</a></strong></sup></span><div class="footnote-content"><p> I think Provine&#39;s claim that there&#39;s 30× more laughter in social situations is less clear-cut than it sounds. In fact, it&#39;s hard to do an apples-to-apples comparison of laughter in social versus nonsocial situations, because (1) social situations are drawn from a different distribution from nonsocial situations in many respects, (2) the presence of other people can change Ingredients (A) and (B) too, and (3) social situations can also affect laughter via voluntary control (Section 4.2.4). I still think (C) is <i>probably</i> a real ingredient in the algorithm, because Provine&#39;s factor of 30 is <i>so extreme</i> that it seems difficult to fully explain by any of those indirect mechanisms.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn34vhiqc4mc2"> <span class="footnote-back-link"><sup><strong><a href="#fnref34vhiqc4mc2">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m extremely far from an expert on what parts of the body are vulnerable in combat today, let alone 100,000 years ago (How often were people punching each other, versus slashing with sharp rocks, versus getting bitten by wolves or spiders? What were the lived consequences of different types of injuries? Beats me!). But I still think this claim is probably true. For example, <a href="https://jamanetwork.com/journals/jama/article-abstract/395461"><u>this article</u></a> claims that ticklish areas correlate with areas protected by involuntary defensive reflexes.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwfrfar7cs1"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwfrfar7cs1">^</a></strong></sup></span><div class="footnote-content"><p> I am defining the word “humor” narrowly—something like “humor = the kind of stuff you find in the &#39;humor&#39; section of a bookstore”, eg recognizable jokes and so on. On this definition, if someone walks up to my lunch table and says “can I join you?” and then laughs a bit, that&#39;s conversational laughter but not “humor”. I believe <a href="https://www.penguinrandomhouse.com/books/332702/laughter-by-robert-r-provine/">Provine</a> takes this definitional approach.</p><p> Alternatively, one could define the word “humor” very broadly, as something like “humor = whatever makes someone spontaneously laugh”. On this definition, it&#39;s perfectly possible that someone walking up to my lunch table and saying “can I join you?” <i>is</i> an instance of “humor”, when taken with its full context. I believe <a href="https://mitpress.mit.edu/9780262518697/inside-jokes/">Hurley <i>et al.</i></a> take this definitional approach.</p><p> Anyway, this is just semantics. “Humor” is just a word, and within limits, we can define it however we want. I find the narrow definition to be helpful for the purposes of this post, so that&#39;s how I&#39;m using it.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb4hvx2pwe9g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb4hvx2pwe9g">^</a></strong></sup></span><div class="footnote-content"><p> Transient manipulation of our own emotional state is not the only method of learned / deliberate manipulation of laughter. Alternatively, you can laugh by pure voluntary motor control—just move your larynx, lungs, etc. and make laugh sounds, the same way you might voluntarily move your arm. I think such laughter can come across as fake sometimes (like maybe it can sound slightly different, and the eyes don&#39;t squint in quite the same way, see <a href="https://en.wikipedia.org/wiki/Smile#Duchenne_smile">Duchenne smile</a> )—although people still do it plenty.</p><p> Incidentally, I think the “summoning transient emotions” method of laughing and the “pure voluntary motor control” method of laughing in this footnote are probably two opposite ends of a spectrum, as opposed to a crisp binary.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7vihiltl1wd"> <span class="footnote-back-link"><sup><strong><a href="#fnref7vihiltl1wd">^</a></strong></sup></span><div class="footnote-content"><p> If you haven&#39;t heard the term “anti-inductive”: &quot;Inductive&quot; reasoning is where you assume that if you&#39;ve seen something a bunch of times in the past, then it&#39;s likely to happen again in the future. “Anti-inductive” reasoning is the opposite—the more past evidence you have for something, the more likely it is to be <i>false</i> next time.</p><p> As the old joke goes: “I recommend anti-inductive reasoning to anyone. After all, using anti-inductive reasoning has been a catastrophically bad idea every time I&#39;ve tried it in the past. So it&#39;s <i>definitely</i> a good idea going forward.”</p><p>这是一个例子。 If you&#39;re up against a master at rocks-paper-scissors, you might want to use anti-inductive reasoning about the future: The <i>more</i> evidence you have for a pattern in your opponent&#39;s behavior, the likelier it is that your opponent <i>wants</i> you to notice that pattern, and therefore the more you should expect that pattern to reverse on the next throw. (But this is happening at every level of abstraction simultaneously—which is kinda weird to think about.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fnikpolxri2te"> <span class="footnote-back-link"><sup><strong><a href="#fnrefikpolxri2te">^</a></strong></sup></span><div class="footnote-content"><p> I suspect that kids will laugh a comparable amount in practice when solving a riddle / brainteaser posed by a friend as when “getting” a pun told by the same friend, other things equal. Other things are not always equal though; puns often get an extra emotional “kick” from some other source, like from being sexual, or from the friend&#39;s mood, or just from the very fact that it&#39;s a pun, both because there&#39;s a <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/Pun"><u>cultural trope</u></a> that making puns is a shameful activity, and because puns are “supposed” to be funny and the expectation of laughter can be a self-fulfilling prophecy, see Section 5.3 below.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter<guid ispermalink="false"> 7kdBqSFJnvJzYTfx9</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Wed, 23 Aug 2023 15:05:59 GMT</pubDate> </item><item><title><![CDATA[Why Is No One Trying To Align Profit Incentives With Alignment Research?]]></title><description><![CDATA[Published on August 23, 2023 1:16 PM GMT<br/><br/><p> A whole lot of Alignment work seems to be resource-constrained. Many funders have talked about how they were only able to give grants to a small percentage of projects and work they found promising. Many researchers also receive a small fraction of what they could make in the for-profit sector (Netflix recently offered $900k for an ML position). The pipeline of recruiting talent, training, and hiring could be greatly accelerated if it wasn&#39;t contingent on continuing to receive nonprofit donations.</p><h3></h3><h3> <strong>Possible Ideas</strong></h3><p></p><h2> AI Auditing Companies</h2><p> We&#39;ve already seen a bit of this with ARC&#39;s eval of GPT4, but why isn&#39;t there more of this? Many companies will/are training their own models, or else using existing models in a way beyond what they were intended. Even starting with non-cutting-edge models could provide insight and train people to have the proper Security Mindset and understanding to audit larger ones. Furthermore, there has been a push to regulate and make this a required practice. The possibility of this regulation being made into law will likely be contingent on the infrastructure for it already existing. And it makes sense to take action toward this now, if we want those auditing teams to be as useful as possible, and not merely satisfy a governmental requirement. Existential concerns would also be taken more seriously by a company that has already built a reputation for auditing models.</p><p> <strong>Evals reporting</strong></p><p> Companies don&#39;t want to see their models doing things that weren&#39;t intended (example, giving people credit card information, as was just recently demonstrated). And as time goes on, companies will want some way of showcasing their models have been rigorously tested. Audit reports covering a large, diverse set of vulnerabilities is something many will probably want.</p><p> <strong>Red teaming</strong></p><p> Jailbreaking has been a common practice, done by a wide number of people after a model is released. Like an Evals Report, many will want a separate entity that can red team their models, the same way many tech companies hire an external cybersecurity company to provide a similar service.</p><p></p><h2> Alignment as a service</h2><p> This could bring in new talent and incentives toward building better understanding and talent to handle alignment. These services would be smaller scale, and would not tackle some of the “core problems” of alignment, but might provide pieces to the puzzle. Solving alignment may not be one big problem, but actually a thousand smaller problems. This gives market feedback, where the better approaches succeed more often than the worse approaches. Over time, this might steer us in a direction of actually coming up with solutions that can be scaled.</p><p></p><p> <strong>Offer procedures to better align models</strong></p><p> Many companies will likely not know how to get their models to do the things they want them to, and they will want assistance to do it. This could start by assisting companies with basic RLHF, but might evolve to developing better methods. The better methods would be adopted by competing Alignment providers, who would also search for even better methods to provide.</p><p> <strong>Caveat</strong> : might accelerate surface-level alignment, but just further a false sense of security.</p><p></p><h2> Alignment as a Product</h2><p> This isn&#39;t the ideal approach, but one still worth considering. Develop new proprietary strategies for aligning models, but don&#39;t release them to the public. Instead, show the results of what these new strategies can do to companies, and sell them the strategy as a product. This might involve NDAs, which is why it is not an ideal approach. But an alignment strategy existing under an NDA is better than no strategy at all.</p><p></p><h2> Mech Interp as a Service</h2><p> This is perhaps not yet in reach, but might be in time. Many will want to better understand how their models are working. A team of mechanistic interpretability researchers could be given access to the model, and dive into gaining a better understanding of its architecture and what it&#39;s actually doing, providing a full report of their findings as a service. This might also steer Mech Interp toward methods that have actual predictive value.</p><p> <strong>Caveat</strong> : I&#39;m not too confident about Mech Interp being useful for safety, with the downside that it might be useful for capabilities.</p><p></p><h2> Governance Consultation as a Service</h2><p> Many politicians and policy makers are currently overwhelmed with a problem they have little technical understanding of. A consultation service would provide them with the expertise and security understanding to offer policy advice that would actually be useful. The current situation seems to be taking experts who are already severely time-constrained, and getting their advice for free. I think many would pay for this service, since there are demands for legislation, and they don&#39;t have the understanding to do it on their own.</p><p></p><h2> Alignment Training as a Service</h2><p> Offering to train workers currently at AI companies to understand security concerns, alignment strategies, and other problems might be desired by many companies. An independent company could train workers to better understand concepts that many are probably not used to dealing with.</p><p></p><h2> Future Endowment Fund</h2><p> This is the one that&#39;s the furthest away from normal ideas, but I&#39;d love it if more people tried to hack a solution to this. The biggest issue is that the value from alignment research has a time delay. This solution could be something like a Promise of Future Equity contract. Those that do research would receive a promised future share in the Fund, as would investors. Companies that use anything that was funded by the Endowment would sign something like a Promise of Future Returns, delegating a share of the returns of any model that used the strategy to the fund. This way, people who were also working on alignment strategies that only had a 5% chance of working would still get reimbursement for their work. Those working on strategies with a calculated higher chance of working would get a greater share. The Trustees would be members of the community who are highly credible, and who have deep levels of insight about AI.</p><p></p><p> If you are interested in making progress on any of these endeavors, feel free to message me. I&#39;ve worked in Cybersecurity, so I have a good understanding of how the auditing pipeline normally works at such companies.</p><p> If you have any disagreements with some of these approaches (which I&#39;m sure some do), feel free to tell me why I&#39;m wrong in the comments.</p><br/><br/> <a href="https://www.lesswrong.com/posts/XnAZHCi5QH58WcrkX/why-is-no-one-trying-to-align-profit-incentives-with#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XnAZHCi5QH58WcrkX/why-is-no-one-trying-to-align-profit-incentives-with<guid ispermalink="false"> XnAZHCi5QH58WcrkX</guid><dc:creator><![CDATA[Prometheus]]></dc:creator><pubDate> Wed, 23 Aug 2023 13:16:42 GMT</pubDate> </item><item><title><![CDATA[Exploring the Responsible Path to AI Research in the Philippines]]></title><description><![CDATA[Published on August 23, 2023 8:44 AM GMT<br/><br/><h3><strong>介绍</strong></h3><p>人工智能 (AI) 的变革潜力正在重塑我们的世界，既带来了激动人心的机遇，也带来了深刻的挑战。 In the Philippines, where Dominic Ligot (Founder, AI expert &amp; communicator) <span class="footnote-reference" role="doc-noteref" id="fnrefgdz3m8hstr8"><sup><a href="#fngdz3m8hstr8">[1]</a></sup></span> and I have delved into AI&#39;s possibilities, the integration of this technology into our unique cultural fabric can unleash exponential productivity.这篇文章探讨了我们（作为一个国家）如何利用人工智能来增强我们的当地社区，同时又不失去我们独特的身份。带着我们共同的迷恋作为火花，我将在了解人工智能复杂性的必要性以及谨慎进行和坚持一致性研究的必要性的指导下，研究人工智能的更广泛的影响<span class="footnote-reference" role="doc-noteref" id="fnref4w360g923mp"><sup><a href="#fn4w360g923mp">[2]</a></sup></span> 。</p><p></p><h3><strong>变革性技术和人工智能研究</strong></h3><p><i>人工智能的变革潜力</i></p><p>人工智能是一项变革性技术，有潜力在未来十年重塑我们的世界。如果不能理解和整合人工智能，一个国家可能会在快速变化的环境中落后。</p><p><i>技术变革的历史先行</i></p><p>技术进步推动了重大变革。无论是通过尖端物理学在战争中的应用还是新科学领域的发现，技术创新往往导致人类历史上的关键时刻。</p><p><i>人工智能研究的复杂性和紧迫性</i></p><p>人工智能研究既不简单，概念上也不简单。该领域的复杂性凸显了深入了解该领域的重要性。虽然掌握人工智能的道路可能看起来令人畏惧甚至疯狂，但风险太大，不能自满。</p><p> <i>AI研究的必然方向</i></p><p>目前的情况让我们别无选择，只能拥抱人工智能研究。这不仅仅是一个选择；这是我们必须采取的重要方向，以确保我们在人工智能日益塑造的未来中生存。问题不在于我们是否应该进行人工智能研究，而在于我们如何才能最有效、最负责任地开展人工智能研究。</p><p>在探索了人工智能的巨大变革力量及其历史相似之处之后，有必要考虑这个复杂领域今天给我们带来的挑战和前景。</p><p></p><h3><strong>优点和缺点</strong></h3><p><i>人工智能研究的挑战和不确定性</i></p><p>目前的人工智能研究更像是炼金术而不是数学。 We still lack a formal theory on the malleability of data that can be infused into models, making our understanding of how data gets distributed to individual parts, such as tokens or layers, uncertain.这些分布的转变影响着技术如何以普遍的方式做出反应。尽管缺乏形式化的数学方法，但不确定性不应阻止我们对研究的追求。</p><p><i>项目失败统计</i></p><p>项目失败的概率约为 65%。 <span class="footnote-reference" role="doc-noteref" id="fnrefrgyrt2chqg"><sup><a href="#fnrgyrt2chqg">[3]</a></sup></span>虽然这是一个令人沮丧的统计数据，但它提醒我们，如果处理不当，我们的资源可能会造成损失。某些行为可能会产生比解决的问题更多的问题，这强调了在人工智能研究中准确识别方法和概念框架的重要性。仔细规划可以避免潜在的损失和意外后果。</p><p><i>人工智能技术的前景光明</i></p><p>人工智能技术的潜在好处简直令人震惊。利用人工智能解决当地生产力问题是当务之急，它可能是我们需要的催化剂。创新新方法来改善经济和个人生活可能意义重大。然而，实现这些好处需要优先考虑调整工作。在部署技术之前确保严格考虑人类福祉是不容谈判的。</p><p>尽管人工智能研究的优势令人惊讶，但其不确定性和潜在陷阱需要仔细研究。 Let&#39;s explore the vital need for caution, consideration, and alignment in AI research.</p><p></p><h3><strong>人工智能研究的利害关系：呼吁考虑和谨慎</strong></h3><p><i>承认复杂性：个人反思</i></p><p>这是我第一次思考这些概念，我不得不以真诚和勤奋的态度来对待它们。为人工智能建立一个负责任的研究方向的紧迫性是显而易见的，特别是针对广大且多样化的人口。如果执行得当，效益可以成倍增加。</p><p><i>机遇与挑战：人工智能研究的深思熟虑的方法</i></p><p>任何考虑进入人工智能研究领域的人，无论是菲律宾人还是其他地方，都必须注意这个现实：将人工智能理解为一种技术会带来巨大的机遇。然而，将这些机会转化为胜利而不是失误，需要的不仅仅是本能；还需要更多的努力。它需要更多：</p><ol><li>规划和谨慎：踏上这条道路需要仔细规划和极其谨慎。人工智能领域充满潜力，但同样充满陷阱，可能导致巨大的困难和牺牲。</li><li>从历史中汲取教训：我们必须记住，在创新蓬勃发展之前，人类努力的未知领域往往会带来痛苦和挣扎。让我们的人工智能方法吸取这些历史教训，努力进行深思熟虑的创新，而不是仓促的实验。</li></ol><p><i>谨慎行事的责任</i></p><p>我们参与人工智能的风险确实非常高。这不仅仅是一次技术冒险；更是一次冒险。这是一项复杂的人类事业，需要认真考虑道德、联盟和社区的社会结构。</p><p>如果没有个人、团队、组织或政府将协调作为核心组成部分，我无法想象人工智能研究会取得成功。 AI systems think differently—using tokens, not words, and <a href="https://www.lesswrong.com/tag/transformers">transformer models</a> that is still not fully understood.参与任何研究议程而不保持一致性所需的谨慎程度无疑会导致灾难。说明这一点的一个有用概念是人工智能系统是否表现出<a href="https://www.lesswrong.com/tag/corrigibility">可纠正的属性</a>。人工智能系统是否愿意在必要时允许自身被修改或关闭？即使人工智能可能不会直接用于研究输出，但更安全的是认识到这些技术是否还不够强大以确保安全。花时间深入了解人工智能系统的不同之处肯定有助于推进人工智能研究。</p><p></p><p>我们可以为人工智能的成功搭建一座坚固的桥梁。如果我们优先考虑安全，我们就能从理解人工智能技术中获得回报。</p><p></p><p> <strong>Contrasting Outcomes: Binisaya Buddy and Local Lore</strong></p><p>我写了一篇关于两个引人注目的例子的文章，说明了负责任的人工智能开发的重要性：Binisaya Buddy，一个反映宿务文化、价值观和传统的虚拟指南，以及 Local Lore，一种文化再现的误导性尝试。 <span class="footnote-reference" role="doc-noteref" id="fnref19z9hnoglxt"><sup><a href="#fn19z9hnoglxt">[4]</a></sup></span></p><ol><li> <i>Binisaya Buddy：</i>这个项目诞生于充满活力的宿务，代表了对齐理论指导人工智能研究的潜力。它通过真实反映文化的细微差别为当地人和游客提供服务。</li><li><i>当地传说：</i>相比之下，当地传说缺乏当地洞察力，导致歪曲事实并与其旨在服务的社区脱节。执行不力造成的问题多于解决方案。</li></ol><p>通过 Binsaya Buddy 和 Local Lore 的例子，我们看到了人工智能与文化价值观结合的成功和失败。这些案例研究强调不仅需要个人责任，还需要在各个层面进行战略合作，以确保人工智能的发展符合我们的文化背景和道德价值观。</p><p></p><p>这些截然不同的结果凸显了建立一个集成系统的重要性，让各个利益相关者共同努力实现负责任的人工智能开发。这种合作可以采取什么形式？如何培养这种合作以确保技术真正满足我们社区的需求和价值观？</p><p></p><h3><strong>潜在途径：政府、企业和学术界之间的合作</strong></h3><p>在菲律宾充分发挥人工智能的潜力需要的不仅仅是各个部门的孤立努力。它需要政府机构、企业和学术机构之间进行紧密的战略合作。以下是这种三方合作如何促进负责任的人工智能开发：</p><p><strong>政府的角色：</strong></p><ul><li>监管框架：通过为人工智能开发制定明确的道德准则，政府可以确保技术进步符合国家价值观和社会需求。</li><li>投资和支持：政府对人工智能研发、基础设施和教育的投资可以促进该行业的增长，支持公共和私人举措。</li></ul><p><strong>企业部门的贡献：</strong></p><ul><li>实际应用：企业在将理论研究转化为实际解决方案方面处于领先地位。通过与学术界合作开发创新技术，行业参与者可以专注于解决当地的挑战。</li><li>私人投资：通过投资人工智能并与教育机构合作，企业可以创建一个繁荣的生态系统，促进技术创业和创造就业机会。</li></ul><p><strong>学术机构作为催化剂：</strong></p><ul><li>研究与创新：大学和研究机构是人工智能前沿探索的中坚力量。 Collaborative research with corporations ensures that academic findings translate into real-world applications.</li><li>教育和培训：学术界在培养下一代人工智能专家方面发挥着至关重要的作用。通过使课程与行业需求和政府愿景保持一致，教育机构可以培养一支熟练的劳动力队伍，为负责任的人工智能发展做出贡献。</li></ul><p><strong>潜在的跨部门合作</strong></p><ul><li>共同目标和战略：通过设定共同目标和协作平台，三个部门可以协调努力，最大限度地提高效率和影响力。</li><li>道德与一致性：确保人工智能开发遵守道德原则需要所有三个部门的投入。这种合作确保技术进步有助于人类繁荣而不损害文化特征。</li></ul><h3></h3><h3><strong>结论</strong></h3><p>政府、工业界和学术界之间可能的合作不仅是有益的联盟，而且是有益的联盟。这可能是菲律宾负责任的人工智能发展的重要战略。通过认识到自己的独特角色并共同努力实现共同愿景，这些部门可以在不忽视道德考虑和当地价值观的情况下利用人工智能的变革潜力。</p><p><i>最终目标是人类繁荣。</i></p><p>最终，任何人工智能研究、项目甚至个人努力的目标都是人类的繁荣。为了实现这一目标，需要进行一定程度的协调工作，以确保结果有助于改善我们的生活。项目越大，必须包含的调整工作量就越大。</p><p>我们对人工智能潜力和挑战的探索最终以更广阔的视角实现。吸取了经验教训并了解了利害关系，让我们想象一下菲律宾在这个快速发展的领域可以发挥的独特作用，重点是协调和文化认同。</p><p><i>未来之路：菲律宾和人工智能</i></p><p>菲律宾拥有 1.15 亿居民，其中包括大量讲英语的人口，正处于十字路口。我们的人工智能之旅不仅仅是创新，更是创新。这是一种负责任的、符合道德的技术方法。一般人工智能研究和协调工作是同一枚硬币的两面，对于在人工智能世界中前进至关重要。风险巨大，潜力巨大。我们的国家决不能满足于将一般研究与我们共同的价值观（全球和地方）和文化认同相协调的方法。</p><h3></h3><h3><strong>附录</strong></h3><p>写完这篇文章后，我了解到碧瑶市代表马克·戈议员的一项提案。他向菲律宾国会通报了自己的倡议，即提议在全国范围内培训 1% 的菲律宾人进行人工智能技术培训，并为人工智能研究设立一个资助机构。尽管尚未起草任何法案，但这表明这篇文章既相关又及时。您可以<a href="https://www.youtube.com/watch?v=i00ZCE3udAk">在此处查看演示文稿和 Doc 的评论。</a> </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fngdz3m8hstr8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgdz3m8hstr8">^</a></strong></sup></span><div class="footnote-content"><p> Dominic Ligot 是 CirroLytix（一家具有社会影响力的人工智能公司）和 Data Ethics PH（一个专注于数据隐私、数据安全、人工智能驱动的歧视、数据责任、数据所有权和数据贫困等社会问题的在线社区）的创始人。三度获得 NASA 和 ESA 国际空间应用挑战赛全球冠军，其团队屡获殊荣的登革热监测应用程序 AEDES 得到了地球观测组织 (GEO)、数字公共产品联盟 (DPGA) 和联合国儿童基金会创新中心的支持基金。</p><p>您可以<a href="https://docligot.com/">在这里找到他的完整个人资料。</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn4w360g923mp"> <span class="footnote-back-link"><sup><strong><a href="#fnref4w360g923mp">^</a></strong></sup></span><div class="footnote-content"><p>当我提到对齐研究/工作时，我指的是理解对齐问题及其复杂性的努力。我很难推荐对齐理论作为标准，特别是因为它还不是一门关于如何对齐人工智能系统并让它们理解和维护人类价值观的具体科学，而且我们都还在弄清楚问题。然而，作为参考，<a href="https://www.lesswrong.com/posts/bjjbp5i5G8bekJuxv/study-guide">约翰·温斯沃斯的学习指南</a>很好地介绍了与一致性交织在一起的理论领域的复杂性。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrgyrt2chqg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrgyrt2chqg">^</a></strong></sup></span><div class="footnote-content"><blockquote><p>但研究表明，全世界开展的项目中只有 35% 是成功的，这意味着我们浪费了大量的时间、金钱和机会。</p></blockquote><p></p><p>引自《哈佛商业评论》的安东尼奥·涅托·埃奥德里格斯的<a href="https://hbr.org/2021/11/the-project-economy-has-arrived">《项目经济已经到来》</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn19z9hnoglxt"> <span class="footnote-back-link"><sup><strong><a href="#fnref19z9hnoglxt">^</a></strong></sup></span><div class="footnote-content"><p>来自我的博客： <a href="https://www.whitehatstoic.com/p/binisaya-buddy-and-local-lore">https://www.whitehastoic.com</a> ：</p><h3> Binisaya 好友和当地传说</h3><p>副标题：对齐与错位：两个人工智能系统在菲律宾架起文化与技术桥梁的故事</p><figure class="image image_resized" style="width:57.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/bucxqobmbsdsytuueaos" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/j61mpzznl8fnnaxvj82e 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/aoljkptxyxmjcnzpslml 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/e3esjc3vtk1fz8gcf2u8 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/kdawakp2svpclhpfuf7b 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/mbkqfq08riaztzscujnc 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/tavmhtvyztvxvteyimex 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/xpgegixjph35p0xni7hf 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/ejeiieuk55e9lcqhtjyr 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/rxyebd4ryiw0dvubbvii 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/g45pp3gkkjty54n9wxbh 1024w"></figure><p></p><p>风景如画的宿务岛以其充满活力的遗产和令人惊叹的风景而闻名，建立了一个创新的人工智能系统来弥合当地人和游客之间的差距。该人工智能系统以当地方言命名为“Binisaya Buddy”，其设计是基于对宿务文化、价值观和传统的深刻理解。</p><p> Binsaya Buddy 通过互动亭迎接抵达麦克坦-宿务国际机场的游客。人工智能会带着友好的虚拟微笑，分享宿务丰富的历史故事，解释 Sinulog 等传统舞蹈的重要性，提供享受 Lechon 等当地美食的技巧，甚至用当地语言教授一些基本短语。</p><p>但 Binsaya Buddy 的与众不同之处在于它与当地文化的契合。人工智能不仅仅是一个信息库；它还是一个信息库。这是宿务灵魂的反映。当地人参与了它的开发，确保它体现了他们的信仰、价值观和生活方式的细微差别。</p><p>游客们发现自己沉浸在真正的宿务体验中，在这种让人感到个性化、尊重和联系的技术的帮助下。另一方面，当地人对他们的文化得到体现和分享感到自豪。</p><p> Binisaya Buddy 不再只是一个工具，而是一个工具。它成为技术与人类价值观相结合时如何增强文化理解、促进联系和创造有意义的体验的象征。</p><p>在距离宿务不远的另一个繁华城市，启动了一个雄心勃勃的项目，旨在创建人工智能驱动的旅游指南。该系统名为“当地传说”，旨在让游客了解当地文化、传统和历史。</p><p>然而，与 Binsaya Buddy 不同的是，Local Lore 是在没有本地输入的情况下开发的，仅由来自通用旅游网站和社交媒体的数据提供的算法指导。缺乏与社区价值观的一致性以及对其独特文化细微差别的理解，导致了意想不到的后果。</p><p>推出后不久，当地人开始注意到虚假陈述。当地传说将圣努洛节描述为仅仅是一场色彩缤纷的聚会，忽视了其深刻的宗教意义。它推荐商业化的旅游陷阱而不是真实的当地体验，而且其语言翻译往往不准确，导致无意的冒犯。</p><p>社区感到疏远和误解。他们看到自己丰富的文化沦为刻板印象和误解，成为讽刺而不是庆祝。</p><p>游客最初对新技术感到兴奋，但后来却感到困惑，并与这个地方的真正本质脱节。当地传说指导缺乏真实性，导致错失真正文化沉浸和理解的机会。</p><p>随着时间的推移，当地传说被放弃了，这是一次失败的实验，它清楚地提醒人们，当技术与其所代表的人类价值观、背景和文化不一致时，可能会出现问题。它成为一个警示故事，说明当忽视一致性时，即使是善意的创新也可能导致破坏。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/yEx2LfYEGaWQE8dHp/exploring-the-responsible-path-to-ai-research-in-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yEx2LfYEGaWQE8dHp/exploring-the-responsible-path-to-ai-research-in-the<guid ispermalink="false"> yEx2LfYEGaWQE8dHp</guid><dc:creator><![CDATA[MiguelDev]]></dc:creator><pubDate> Wed, 23 Aug 2023 08:44:18 GMT</pubDate> </item><item><title><![CDATA[Do agents with (mutually known) identical utility functions but irreconcilable knowledge sometimes fight?]]></title><description><![CDATA[Published on August 23, 2023 8:13 AM GMT<br/><br/><p>一直在思考；冲突会一直存在吗？一个主要的子问题：假设我们都合并效用函数并形成一个致力于优化合并的星际社区。对我们来说，专注于工作的不同部分可能是有意义的，这意味着积累专业领域知识并变得相互难以理解。</p><p>当人们拥有截然不同的领域知识时，他们也会对各自领域的边界不一致。 <i>（EG：决策理论家坚持认为他们了解机器学习研究人员不知道的人工智能发展轨迹。机器学习研究人员不相信他们，也不听他们的建议。）在这些情况下，即使各方都在采取行动</i>出于善意，他们知道他们无法就某些分歧达成和解，从某些角度来看，试图在这些有争议的地区强加自己的方式似乎是有道理的。</p><p>这里使用的争议解决方法与具有不同核心价值观的代理人之间使用的争议解决方法有什么区别吗？ （战争、和平协议，最重要的是）</p><p>冲突各方是否会使用考虑到不同领域的物理优势的战争代理人？ （EG：决策理论家会阻止在有争议的领域进行机器学习研究吗？在这些领域，他们的决策理论知识会给他们带来<i>力量</i>优势？）</p><br/><br/> <a href="https://www.lesswrong.com/posts/FTdtHHBPDdzk4pJz2/do-agents-with-mutually-known-identical-utility-functions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FTdtHHBPDdzk4pJz2/do-agents-with-mutually-known-identical-utility-functions<guid ispermalink="false"> FTdtHHBPDdzk4pJz2</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Wed, 23 Aug 2023 08:13:06 GMT</pubDate></item></channel></rss>