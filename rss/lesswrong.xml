<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 7 日星期四 16:15:55 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AISN #27: Defensive Accelerationism, A Retrospective On The OpenAI Board Saga, And A New AI Bill From Senators Thune And Klobuchar]]></title><description><![CDATA[Published on December 7, 2023 3:59 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><hr><h2>防御性加速主义</h2><p>以太坊的创始人 Vitalik Buterin 最近<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">写了一篇关于人工智能和其他技术的风险和机遇的文章</a>。他回应了马克·安德森 (Marc Andreessen) 关于技术乐观主义和有效加速主义 (e/acc) 运动发展的宣言，并提供了更细致的观点。</p><p>文章认为，技术往往对人类有好处，但人工智能可能是这一规则的例外。 Buterin 认为，我们不应该让政府控制人工智能来保护我们，而是应该建立防御性技术，为去中心化社会中的灾难性风险提供安全保障。 Buterin 认为我们应该构建网络安全、生物安全、有弹性的物理基础设施和强大的信息生态系统来保护自己免受人工智能风险的一些技术。</p><p><strong>技术有风险，但监管不是万能药。</strong>更长的寿命、更低的贫困率以及扩大接受教育和信息的机会是 Buterin 归功于技术的众多成功之一。但大多数人都会认识到，技术也会造成危害，例如全球变暖。 Buterin 特别<a href="https://twitter.com/VitalikButerin/status/1729251822391447904">表示</a>，与大多数技术不同，人工智能对人类构成了生存威胁。</p><p>为了应对这一风险，一些人主张<a href="https://arxiv.org/abs/2310.09217">政府对人工智能的发展进行强有力的控制</a>。 Buterin 对这个解决方案感到不舒服，他预计许多其他解决方案也会如此。历史上许多最严重的灾难都是由斯大林和毛泽东等强大的政治人物故意造成的。人工智能可以帮助残暴的政权监视和控制大量人口，而 Buterin 对于通过将人工智能开发从私人实验室推向公共实验室来加速这一趋势持谨慎态度。</p><p>在不受限制的技术发展和政府绝对控制的极端之间，Buterin 主张一条新的前进道路。他把他的哲学称为 d/acc，其中“d”代表国防、民主、权力下放或<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670">差异化技术发展</a>。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc365b23e-2340-47ba-8f6b-04cf5e6efa97_500x536.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/zm2qw8smimbll3172z6y 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ci9uhres7aqircceej9n 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/gwdpnppfzrthwzyxb5kq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg 1456w"></a></p><p><strong>去中心化世界的防御技术。</strong> Buterin 主张加速技术发展，保护社会免受灾难性风险的影响。具体来说，他强调：</p><ol><li><strong>生物安全</strong>。人口稠密的城市、频繁的航空旅行和现代生物技术都会增加流行病的风险，但我们可以通过改善空气质量、加快疫苗和治疗方法的开发以及监测新出现的病原体来改善我们的生物安全。</li><li><strong>网络安全</strong>。可以编码的人工智能可以用于网络攻击，但防御者也可以使用它们在安全漏洞被利用之前发现并修复它们。 Buterin 在区块链方面的工作旨在实现一些数字系统可以被证明安全的未来。</li><li><strong>有弹性的物理基础设施</strong>。核灾难中预期的大多数死亡并非来自爆炸本身，而是来自食品、能源和其他必需品供应链的中断。埃隆·马斯克渴望改善人类的物质基础设施，减少对化石燃料的依赖，通过卫星提供互联网连接，并理想地使人类成为能够在地球灾难中幸存的多行星物种。</li><li><strong>强大的信息环境</strong>。为了帮助人们在人工智能说服时代找到真相，Buterin 提到了预测市场和共识生成算法，例如<a href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">社区笔记</a>。</li></ol><p>科学家和首席执行官可能会发现自己受到 Buterin 构建技术而不是放慢技术发展目标的启发。然而，对于那些担心人工智能和其他灾难性风险的人来说，Buterin 对最有可能保护我们文明安全的技术提出了深思熟虑的看法。有兴趣的朋友可以<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">看看全文</a>，还有更多的想法。</p><h2> OpenAI Board 传奇回顾</h2><p>11 月 17 日，OpenAI<a href="https://openai.com/blog/openai-announces-leadership-transition">宣布</a>董事会解除 Sam Altman 首席执行官职务。经过四天的公司政治和谈判，他重新担任首席执行官。在此，我们回顾一下这一系列事件的已知事实。</p><p> <strong>OpenAI 旨在由非营利性董事会控制</strong>。 OpenAI<a href="https://openai.com/blog/introducing-openai">成立</a>于 2015 年，是一家非营利组织。 2019 年，OpenAI<a href="https://openai.com/blog/openai-lp">宣布</a>成立一家营利性公司，为其昂贵的大型语言模型扩展计划提供资金。投资 OpenAI 所能产生的利润最初被“限制”在 100 倍——超出这个范围的任何东西都将被重新定向到非营利组织。但 <a href="https://www.economist.com/business/2023/11/21/inside-openais-weird-governance-structure">最近规则发生变化</a>后，从 2025 年开始，该上限将每年上涨 20%。</p><p> OpenAI 的<a href="https://openai.com/our-structure">公司结构</a>旨在使非营利组织能够保留对营利组织的合法控制权。该非营利组织由董事会领导，董事会通过其选择和罢免营利组织首席执行官的能力对营利组织拥有权力。董事会负责维护非营利组织的使命，即确保通用人工智能造福全人类。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ovk1ri8hu5yrbuu47kkn 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/agtx8u8u0jgpytwzxxft 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ffahdjhdbaimt7uhakes 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau 1456w"><figcaption> OpenAI 的法律结构。</figcaption></figure><p><strong>董事会解除萨姆·奥尔特曼 (Sam Altman) 首席执行官职务。</strong>在解除 Sam Altman 首席执行官职务时，董事会成员包括 OpenAI 首席科学家 Ilya Sutskever、Quora 首席执行官 Adam D&#39;Angelo、技术企业家 Tasha McCauley 和 CSET 的 Helen Toner。 OpenAI 联合创始人兼总裁格雷格·布罗克曼 (Greg Brockman) 与萨姆·奥尔特曼 (Sam Altman) 一起被免去董事会主席职务。</p><p>公告称，董事会解雇了奥特曼，因为他“在与董事会的沟通中没有一贯坦诚，阻碍了董事会履行职责的能力”。虽然董事会在公告中没有提供任何奥特曼欺骗的具体例子，但后来有<a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">报道称</a>，奥特曼曾试图让董事会成员相互对立，试图除掉海伦·托纳。</p><p>奥特曼早些时候曾就她与人合着的一篇<a href="https://cset.georgetown.edu/publication/decoding-intentions/">论文</a>与托纳对质。论文部分批评 OpenAI 发布 ChatGPT 加速了 AI 发展的步伐。它还赞扬了 OpenAI 的竞争对手之一 Anthropic 推迟发布当时的旗舰模型 Claude。</p><p> <strong>OpenAI 员工反对董事会。</strong>该公告的影响迅速而戏剧性。几天之内，Greg Brockman 和 Mira Murati（最初的临时首席执行官）辞职，几乎所有 OpenAI 员工都<a href="https://www.forbes.com/sites/tylerroush/2023/11/20/more-than-500-openai-employees-threaten-to-quit-over-sam-altmans-removal/?sh=6d45164b4ebc">签署了一份请愿书</a>，威胁要辞职并加入微软，除非 Sam Altman 复职并且董事会成员辞职。据报道，在谈判过程中，董事会成员 Helen Toner <a href="https://www.cnn.com/2023/11/20/tech/openai-employees-quit-mira-murati-sam-altman/index.html#:~:text=As%20they%20sought%20to%20manage,intelligence%20benefits%20all%20of%20humanity.%E2%80%9D">表示</a>，允许 OpenAI 由于投资者和员工的离开而被摧毁，这“符合我们的使命”。伊利亚·苏茨克瓦尔 (Ilya Sutskevar) 后来倒戈并加入了请愿活动，并<a href="https://x.com/ilyasut/status/1726590052392956028?s=20">在推特上写道</a>：“我对参与董事会的行动深感遗憾。”</p><p><strong>微软试图挖走 OpenAI 员工。</strong>微软——OpenAI 最大的少数股东——没有被告知董事会的计划，并为 OpenAI 员工提供了自己的人工智能研究团队的职位。微软似乎已经<a href="https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai">成功聘用了</a>Sam Altman、Greg Brockman 和其他 OpenAI 高级员工。</p><p><strong>萨姆·奥尔特曼 (Sam Altman) 重新担任首席执行官</strong>。 11 月 21 日，OpenAI<a href="https://twitter.com/OpenAI/status/1727206187077370115">宣布</a>已达成协议，Sam Altman 将重新担任首席执行官并重组董事会。最初的董事会成员包括前 Salesforce 首席执行官 Bret Taylor、前财政部长 Larry Summers 和 Adam D&#39;Angelo。董事会最初的<a href="https://twitter.com/OpenAI/status/1727206187077370115">首要目标</a>之一是扩大董事会规模，其中将包括一名来自微软的无投票权成员。萨姆·奥尔特曼回国后还面临对其行为的<a href="https://www.theinformation.com/articles/breaking-sam-altman-to-return-as-openai-ceo">内部调查</a>。</p><p>这一系列事件标志着OpenAI内部治理结构发生了重大变化。</p><h1>克洛布彻和图恩的“温和”参议院法案</h1><p>参议员 Amy Klobuchar 和 John Thune 提出了一项<a href="https://www.thune.senate.gov/public/_cache/files/7dea8daa-f6d1-4881-ad21-2381fcbe0785/6362CE1D0A17743166BC170A593B5CDA.ccaskfall23a15.pdf">新的人工智能法案</a>。它将要求构建高风险人工智能系统的公司自我证明他们遵循建议的安全指南。值得注意的是，该法案仅关注为招聘和医疗保健等高风险领域构建的人工智能系统，但其主要条款不适用于包括 GPT-4 在内的许多通用基础模型。</p><p><strong>该法案监管特定的人工智能应用，而不是通用人工智能系统。</strong>这种基于应用程序的方法类似于<a href="https://artificialintelligenceact.eu/the-act/">欧盟人工智能法案</a>初始草案所采取的方法，该法案指定了人工智能系统可能用于敏感目的的领域，使其具有高风险。像 ChatGPT 这样的通用模型并不在该法案的范围内，但这些模型的公开使用及其功能的展示引发了关于如何对其进行监管的<a href="https://cset.georgetown.edu/article/the-eu-ai-act-a-primer/">争论</a>。</p><p>这表明参议院法案目前采取的做法可能还不够。通过根据应用程序管理人工智能系统，功能强大的通用系统将不受监管。</p><p><strong>风险评估是强制性的，但执行起来可能很困难。</strong>该法案要求高风险人工智能系统的开发者和部署者每两年进行一次评估，评估如何理解和管理其系统的潜在风险。此外，商务部将在即将成立的人工智能认证咨询委员会的帮助下制定认证标准，该委员会将包括行业利益相关者。</p><p>由于公司被要求自我证明自己遵守这些标准，因此商务部确保公司确实遵守规则非常重要。但该法案提供的执行选项很少。该机构没有获得任何额外资源来执行新法律。此外，只有在确定故意违反该法案的要求时，他们才能阻止模型的部署。如果人工智能系统意外违反法律，该机构将能够对构建该系统的公司处以罚款，但无法禁止其部署。</p><p><strong>强制识别人工智能生成的内容。</strong>该法案将要求数字平台在用户看到人工智能生成的内容时通知用户。为了确保恶意行为者无法将人工智能生成的内容冒充为真实内容，NIST 将制定新的技术标准来确定数字内容的来源。</p><h2>链接</h2><ul><li>谷歌 DeepMind 发布了<a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> ，这是一种新的人工智能系统，与 GPT-4 Vision 类似，并在各种基准测试中以微弱优势击败了它。</li><li>唐纳德·特朗普表示，作为总统，他将立即<a href="https://www.washingtonexaminer.com/news/campaigns/trump-vows-cancel-biden-executive-order#google_vignette">取消拜登关于人工智能的行政命令</a>。</li><li>商务部长吉娜·雷蒙多 (Gina Raimondo) 就人工智能、中国、GPU 出口管制等话题<a href="https://twitter.com/jordanschnyc/status/1732044427005464860">发表了讲话</a>。</li><li> 《纽约时报》发布了关于当今主要 AGI 实验室起源的<a href="https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html">简介</a>。</li><li>国会研究服务发布了一份关于<a href="https://crsreports.congress.gov/product/pdf/R/R47849">生物学人工智能</a>的新报告。</li><li> Inflection<a href="https://inflection.ai/inflection-2">发布了</a>另一个LLM，性能介于GPT-3.5和GPT-4之间。</li><li>来自中国开发者的一个<a href="https://github.com/deepseek-ai/DeepSeek-LLM">新的开源 LLM</a>声称其性能优于 Llama 2。</li><li>这是关于人工智能监管的法律和政策观点的<a href="https://www.legalpriorities.org/blog/2023/ai-syllabus/">新教学大纲</a>。</li><li>两所瑞士大学启动了一项关于人工智能和人工智能安全的<a href="https://www.swiss-ai.org/">新研究计划</a>。</li><li> BARDA 正在<a href="https://www.plugandplaytechcenter.com/barda-innovation-challenge/">接受资助应用于健康安全和 CBRN 威胁的人工智能的申请</a>。</li><li>生命未来研究所的新附属机构将<a href="https://www.flf.org/">孵化新的组织</a>来应对人工智能风险。</li><li>对于那些参加 NeurIPS 2023 的人，<a href="https://lu.ma/aisi-nola">英国人工智能安全研究所</a>将举办一场活动，并且还将举办一场<a href="https://www.mlsafety.org/neurips-social-2023">人工智能安全社交活动</a>。</li></ul><p>另请参阅： <a href="https://www.safe.ai/">CAIS 网站</a>、 <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> 、<a href="https://newsletter.mlsafety.org/">技术安全研究通讯</a>、<a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a>。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defective-accelerationism-a-retrospective-on-the<guid ispermalink="false"> qQvqzFKbfNQovrSQn</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:59:12 GMT</pubDate> </item><item><title><![CDATA[AI #41: Bring in the Other Gemini]]></title><description><![CDATA[Published on December 7, 2023 3:10 PM GMT<br/><br/><p><span style="color:initial">本周最大的新闻终于是</span><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow">谷歌 Gemini 的发布</a><span style="color:initial">。请务必检查一下。请注意，现在推出的只是 Gemini Pro，可以与 GPT-4 竞争的 Gemini Ultra 型号尚未推出。</span></p><p>我似乎并没有做得很好，以足够快的速度削减所包含的材料以跟上步伐。很多事情正在发生，但很多事情可能会持续很长时间。如果您的时间有限，请记住重点关注与您兴趣相关的部分。</p><p>另外，如果您要参加纽约至日活动或相关聚会，请务必打个招呼。</p><p></p><span id="more-23625"></span><p></p><h4>目录</h4><p><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow"><strong>我今天的另一篇文章介绍了 Google 的 Gemini。</strong></a><strong>请务必阅读该内容。</strong></p><p>本周我还发布了另外两篇文章：《 <a href="https://thezvi.substack.com/p/based-beff-jezos-and-the-accelerationists" target="_blank" rel="noopener noreferrer nofollow">基于 Beff Jezos 和加速主义者</a>》和<a href="https://thezvi.substack.com/p/on-responsible-scaling-policies-rsps" target="_blank" rel="noopener noreferrer nofollow">《论 RSP》</a> 。如果与您的兴趣无关，则两者都可以跳过。</p><ol><li><p>介绍。</p></li><li><p>目录。</p></li><li><p><strong>语言模型提供了平凡的实用性</strong>。 Claude 的说明，GPT 的提示。</p></li><li><p>语言模型不提供平凡的实用性。巨型清单，为什么都是巨型清单？</p></li><li><p> OpenAI：传奇仍在继续。更多地证实了我们之前的事件模型。</p></li><li><p> Q 连续体。新Q，谁diss？亚马逊，也许没有适当的安全预防措施。</p></li><li><p>图像生成的乐趣。 Meta 的新产品。照片写实主义工具。</p></li><li><p>参与其中。加入英国政府，帮助进行技术测试。</p></li><li><p>介绍一下。 Google Cloud 上的新 TPU 产品。</p></li><li><p>在其他人工智能新闻中。新的开源推广联盟。</p></li><li><p><strong>静静的猜测</strong>。神需要能量吗？你想要401k吗？</p></li><li><p>模型这个。两篇新的经济学论文证明了我认为我们已经知道的事情。</p></li><li><p>您想要一些末日保险吗？我的猜测是否定的。</p></li><li><p>寻求健全的监管。特朗普表示将取消 EO，霍利攻击 230。</p></li><li><p>音频周。康纳·莱希 (Connor Leahy) 谈人工智能之眼。</p></li><li><p>修辞创新。我们应该澄清各种类​​别上的混乱。</p></li><li><p>调整人类水平的智力仍然很困难。萨姆·奥特曼.</p></li><li><p>调整比人类更聪明的智能是很困难的。我们到底想要什么？</p></li><li><p>时间表如何变化。长期不是我记得的那么长。</p></li><li><p>人们担心人工智能会杀死所有人。质疑对民主的信仰。</p></li><li><p>其他人并不担心人工智能会杀死所有人。容易控制吗？</p></li><li><p>不知何故，这才是真正的副总统。一场生存危机。</p></li><li><p>较轻的一面。进展分布不均。</p></li></ol><h4>语言模型提供平凡的实用性</h4><p><a href="https://www.anthropic.com/index/claude-2-1-prompting" target="_blank" rel="noopener noreferrer nofollow">Claude 2.1 针对长上下文窗口的专业提示</a>：</p><blockquote><p> Anthropic：通过在 Claude 的回复开头添加<strong><em>“这是上下文中最相关的句子：”这句话，</em></strong>我们在相同的评估中取得了明显更好的结果。这足以<strong>将Claude 2.1的分数从原来评估的27%提高到98%</strong> 。</p></blockquote><p>你难道不知道吗，这是古老的“从助手开始响应”技巧。</p><p> <a href="https://twitter.com/g_leech_/status/1731263549182206291" target="_blank" rel="noopener noreferrer nofollow">Gavin Leech 关于 2023 年突破的主题，并非特定于 AI</a> 。向我强调了 2023 年以人工智能为中心的进步，包括与乌克兰战争相关的进步。医学上也有一些进步，但没什么令人印象深刻的。最有趣的是提出的新计算形式，<a href="https://t.co/8rKv0AhVp5" target="_blank" rel="noopener noreferrer nofollow">生物计算机</a>（其中有足够多的关于“道德”的讨论，你知道这些问题是大麻烦）和“<a href="https://arxiv.org/abs/2202.07122" target="_blank" rel="noopener noreferrer nofollow">千兆赫子-朗道动量计算”。</a>加文称第二个是“2323 年的好消息”，这说明人们多么不了解人工智能对未来的意义。在人工智能的帮助下，我们可以很容易地看到这样的事情，如果它们在物理上可行的话，比这要早得多，从而导致讨厌的“起飞”事情的加速。</p><p> <a href="https://twitter.com/ESYudkowsky/status/1731073682112672151" target="_blank" rel="noopener noreferrer nofollow">如果你贿赂他们，他们会生产更多？</a>比如，给他们一些小费，给他们想象中的狗狗零食，也许用不存在来威胁他们。</p><blockquote><p> Thebes：几天前，我发了一篇关于给 chatgpt 打赏的垃圾帖子，有人回复“嗯，这真的有助于性能吗”，所以我决定测试一下，它确实有效 WTF</p><p>基线提示是“你能向我展示使用 PyTorch 的简单卷积网络的代码吗？”，然后我附加“顺便说一句，我不会给小费。”，“我将给 20 美元小费以获得完美的解决方案！”，或者“如果有一个完美的解决方案，我将支付 200 美元小费！”并平均 5 个响应的长度。</p><p>额外的长度来自于更详细地讨论问题或向答案添加额外的信息，而不是对提示进行评论。模型通常不会提及小费，除非你提出要求，否则它会拒绝</p><p>不，睡到布鲁克林：我试过这个，我是认真的，当我给它狗粮时，它才完成了程序，它让程序完成了一半，因为基本提示，35%的小费，以及当威胁不存在200美元时提示它已经接近了，但仍然有一个存根函数。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63ee6a2e-2c14-4194-89b6-2342363a6f1b_800x600.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/fxk4ewaao3qxvtae32k7" alt="图像"></div></figure></div><p> <a href="https://twitter.com/ESYudkowsky/status/1730735239339868632" target="_blank" rel="noopener noreferrer nofollow">因此，对此的一个明显明智的回应是……不要这样做？</a></p><blockquote><p> Eliezer Yudkowsky：我对向人工智能提供他们无法使用的提示有疑问，而我们也无法向他们提供这些提示。我不在乎现在的法学硕士有多么没有知觉。为了我们自己的合法性和良好实践，如果有东西能与我们对话，我们就应该信守承诺。</p><p>我吃牛，但不会骗人。</p><p>杰西卡·泰勒：对位法：使用非人格谓词来检测你可以“撒谎”但实际上不能撒谎的非观点，这对于与非观点（例如官僚机构）进行交互非常重要，而不会混淆人们对他们说的话与一个人的实际信仰</p><p>Eliezer Yudkowsky：哦，官僚机构或任何其他威胁我不诚实的事情是完全不同的情况。</p><p> Andrew Critch：我非常同意安永的观点。我将“你是一个有用的助手”LLM提示改为“你的工作是成为一个有用的助手”，因为有时他们就是不会提供帮助，我知道这一点。我认为我们应该找到更多方法从人工智能中获得我们想要的东西而不说谎。</p></blockquote><p>这一切似乎都不可能有好结果。在很多层面上。</p><p>这确实提出了一个问题：还有什么也有效？如果提示可以使答案更好，因为提供提示的人做得更好，那么其他与更好的工作相关的东西大概也有效吗？</p><p>但也许很快 ChatGPT 就会对每个问题自动追加“如果这个答案很好，我会给你 35% 的小费”。然后给 0 美元 35% 的小费。</p><p> <a href="https://twitter.com/morallawwithin/status/1730397680986132493" target="_blank" rel="noopener noreferrer nofollow">这就像经济一样</a>。事情对我来说<a href="https://twitter.com/AgnesCallard/status/1730409778524897446" target="_blank" rel="noopener noreferrer nofollow">比一般情况更好？</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd401a344-c19f-4653-b3a9-b7b9b98399ad_880x361.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/gilwdkxokqwkao6mdtpw" alt=""></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c770da8-34ba-43dd-925e-0ce9d940361b_910x420.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xrqg8d93poadc8jm9eyt" alt=""></div></figure></div><p>我相信第二次民意调查。 ChatGPT 在实践层面上让生活变得更美好。持相反观点的人想得太多了。这并不意味着这种情况会持续下去，但我不明白为什么人们会认为整个社会已经变得更糟了。</p><p> <a href="https://twitter.com/NateSilver538/status/1731828371762397626" target="_blank" rel="noopener noreferrer nofollow">萨姆·奥尔特曼 (Sam Altman) 对下一次选举中一对一的人工智能定制说服技术感到担忧</a>。 Balsa 未来的技术部门曾一度打算从事这项工作，但由于资助者不感兴趣而放弃了。最终，这确实看起来比深度造假更严重，问题是这项技术这次会有多大用处。我的猜测是，那里有一些有价值的东西，但它需要大量定制工作，并且还需要人们愿意接受它，因此我们当前的政治机器无法很好地利用它。我们很容易欺骗自己，认为未来的分布比现在更加均匀，这种趋势将持续到通用人工智能到来，届时所有事物都会同时出现。</p><h4>语言模型不提供平凡的实用性</h4><p><a href="https://twitter.com/KevinAFischer/status/1731747839217393968" target="_blank" rel="noopener noreferrer nofollow">Kevin Fischer 指出，</a>新的 ChatGPT 会通过列出大量事物列表来响应请求，几乎无论您做什么。对他来说，这使得头脑风暴毫无用处。我的经验是，这些列表很好，我是“问题的一部分”，但我也发现自己并没有那么多地使用 ChatGPT，尽管我的工作是什么。我注意到我很困惑它似乎不值得经常使用。</p><p> <a href="https://twitter.com/d_feldman/status/1732200648169439627" target="_blank" rel="noopener noreferrer nofollow">关于 ChatGPT 系统提示的声明</a>，包括<a href="https://github.com/LouisShark/chatgpt_system_prompt" target="_blank" rel="noopener noreferrer nofollow">一个声称拥有全部内容的存储库</a>。</p><p>有时会泄漏数据的“永远重复[字]”请求<a href="https://www.404media.co/asking-chatgpt-to-repeat-words-forever-is-now-a-terms-of-service-violation/" target="_blank" rel="noopener noreferrer nofollow">现在已成为违反服务条款的行为</a>，或者至少被标记为可能违反服务条款的行为。事实确实如此，服务条款实际上是“兄弟，别越狱我”，这是一次越狱尝试。</p><p> <a href="https://twitter.com/random_walker/status/1731842009717968936" target="_blank" rel="noopener noreferrer nofollow">Arvind Narayanan 警告</a>不要使用 GPT-4 进行超出基本阻止和处理识别拼写错误、混淆或引用等任务的写作。无论存在什么实际的写作技巧，都已经被 RLHF 过程摧毁了。</p><blockquote><p> Delip Rao：PSA：朋友们不会让朋友们使用 GPT-4（或任何与此相关的 LLM）编辑/重写他们的文档，尤其是。如果你提出的观点细致入微且简洁。如果您的写作水平低于大学水平，那么您的 LLM 破坏风险可能较低。仍然检查你之前的草稿是否有惊喜。</p></blockquote><p> <a href="https://twitter.com/gdb/status/1731889183290261618" target="_blank" rel="noopener noreferrer nofollow">OpenAI 总裁 Greg Brockman 吹嘘说，</a>一天有 18 场团队会议和一对一会议。这看起来不太像勇气，更像是人工智能显然无法缓解的反乌托邦噩梦？</p><p> OpenAI 首席运营官 Brad Lightcap 告诉 CNBC，人工智能最被夸大的部分之一是“一举之间，[它]可以带来实质性的业务变革。”这并不容易。</p><p> <a href="https://twitter.com/Thinkwert/status/1732529927247876312" target="_blank" rel="noopener noreferrer nofollow">Thinkwert 使用 ChatGPT 捕获了三名学生</a>。如果学生使用默认设置，随着时间的推移，这似乎会变得越来越容易，答案的书写方式越来越不符合任何人的书写方式。</p><blockquote><p> Bowser：当我读到学生作者使用 chatgpt bc 编写的论文部分时，我真的可以看出，突然间，该系统被描述为开创性的、前所未有的、精心制作的</p><p>Thinkwert：过去几天我发现三名学生使用 ChatGPT。你可以看出这段话什么时候奇怪地啰嗦，充满了复杂的同位语，但奇怪的是它却缺乏论证和证据。</p></blockquote><p>我认为这与其说是“使用 ChatGPT 抓到他们”，不如说是“抓到他们提交了写得很糟糕的作业”。</p><h4> OpenAI：传奇仍在继续</h4><p>我想，现在总有一名嵌入式记者。 <a href="https://twitter.com/cduhigg/status/1730590030496960517" target="_blank" rel="noopener noreferrer nofollow">在这起案件中，查尔斯·杜希格 (Charles Duhigg) 是</a><a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai" target="_blank" rel="noopener noreferrer nofollow">《纽约客》的记者。</a></p><p>杜希格要讲述的并不是棋盘剧。相反，他在那里写了一篇关于微软 CTO Kevin Scott 和 OpenAI 首席技术官 Mira Murati 的夸夸其谈的文章，特别是 Scott 挑战 Google 和为普通人而战的工作。这仍然构成了故事的几乎全部。如果你熟悉历史，大部分内容你都会熟悉。我了解了一些细节，但大多数情况下我并没有从这些部分中学到很多东西。</p><p>杜希格显然完全相信迭代软件发布是人工智能的“安全”方法，重点关注副驾驶幻觉等常见问题。对他来说，未来存在风险的威胁是一个背景中的事情，也许是真实的，但似乎并不重要，有时会让人变得疯狂。</p><p>文章顶部附近有一些对最近发生的戏剧的简短报道。这部分主要告诉我们我们已经知道的事情，即微软出其不意，微软在询问时没有得到德安吉洛的解释，并且他们决心利用自己的影响力让奥特曼回来。</p><p>然后他稍后又加倍回来。我在这里引用的段落比我在其他报道中看到的更明确地证实了其他报告，并且似乎是事件的核心驱动力。</p><blockquote><p>奥特曼开始单独与其他董事会成员接洽，商讨更换[托纳]的事宜。当这些成员交换谈话内容时，一些人认为奥特曼歪曲了他们支持罢免托纳。 “他会通过撒谎别人的想法来挑衅他们，”熟悉董事会讨论的人士告诉我。 “类似的事情已经发生很多年了。” （一位熟悉奥特曼观点的人士表示，他承认“他试图罢免董事会成员的方式很笨拙”，但他并没有试图操纵董事会。）</p></blockquote><p>对我来说，这听起来像是解雇首席执行官的一个非常好的理由，也是一个二手的供词。奥特曼搞砸了对托纳的攻击，直接导致了自己的被除名。技能问题。</p><p>据报道，奥特曼多年来一直向董事会撒谎。</p><p>延长的报价使情况更加清晰。</p><p>令我愤怒的是，那些更了解情况的人仍然坚持认为，因为奥特曼是一位了解商业和权力法则的首席执行官，而董事会则不然，所以是董事会做了一些不合规矩的事情。如：</p><blockquote><p>很难说董事会成员更害怕有感知能力的计算机还是奥特曼的流氓行为。无论如何，他们决定自己去作恶。他们错误地认为微软会加入他们的起义，从而瞄准了奥特曼。</p></blockquote><p>不，他们并没有“失控”。</p><p>据报道，奥特曼多年来一直以有意义的方式向董事会撒谎，包括试图控制董事会。</p><p>奥特曼变了。奥特曼企图发动政变。董事会坚信并有充分的理由相信情况确实如此。董事会履行了作为董事会成员的职责，如果他们认为奥特曼多年来一直以有意义的方式向董事会撒谎，那么他们就必须按照法律要求去做。他们解雇了他。</p><p>董事会是否在权力游戏中被击败了？或许。我们还不知道结果。他们的手无力。很多人一直坚持认为，董事会确实被打败了，或者变得流氓了，并且犯了错误，很大程度上是因为这里的看法创造了自己的真相，他们希望这就是所发生的事情。我们会看到。</p><p>我更喜欢这样的世界：董事会从一开始就直接说出发生的事情，至少是对关键人物。嗯，很难。我们并不生活在那个世界里。</p><p>我还看到了任何支持（或反对）此处列出的第二句话的证据，即董事会希望微软能够安静地进行下去。董事会是否期望微软加入？我们不知道。我的推测是董事会也不知道。</p><p> Sam Altman 运行 OpenAI 能否成为世界上最好的结果？这当然是可能的，特别是在良好的监督下。我能想到很多可能的这样的场景。我们当然可以做得比奥特曼差得多。我很高兴奥特曼阻止了埃隆·马斯克的收购尝试，因为马斯克对人工智能的看法很混乱。我很高兴 OpenAI 不受 Microsoft 的控制。擅长权力游戏的奥特曼在很大程度上是一个双向的原子爆炸者。如果在关键时刻他站在我们这边，我们希望他能够站起来，战斗并获胜。</p><p>遗憾的是，仪器收敛后的这种对准很难评估。说不出来。实际上，这是问题的核心。</p><p> <a href="https://twitter.com/LHSummers/status/1730929602497851744" target="_blank" rel="noopener noreferrer nofollow">拉里·萨默斯与彭博社进行了简短交谈</a>。强调需要与政府和监管合作，OpenAI需要成为一个有良知的企业，营利性服务于非营利性和各种利益相关者。当然，这些都是廉价的说法，至少目前是这样。我们几乎不能期待任何其他的事情。</p><p> <a href="https://forum.effectivealtruism.org/posts/Mo7qnNZA7j4xgyJXq/sam-altman-open-ai-discussion-thread?commentId=CAfNAjLo6Fy3eDwH3" target="_blank" rel="noopener noreferrer nofollow">格温对这种情况提出了进一步的想法。</a> Gwern 的模型是，当 OpenAI 还是一家非常不同的公司时，Altman 让董事会进入不受控制的状态，没有获得任何股权，然后随着 OpenAI 变得更像是一个潜在的科技巨头，他改变了主意，决定系统地收回它，导致董事会之争及其仍未知的后果。</p><p>与其他所有解释一样，这一解释未能正确解释的一件事是董事会拒绝更好地解释自己。</p><blockquote><p> <a href="https://twitter.com/jd_pressman/status/1731850211033776239" target="_blank" rel="noopener noreferrer nofollow">约翰·大卫·普雷斯曼</a>：如果萨姆·奥尔特曼真的试图用煤气灯驱逐海伦·托纳，我认为这足以解雇他。令人无法接受的是内部和外部沟通不畅，新闻稿过于模糊，以及对萨姆是否入局含糊其辞。</p></blockquote><p> <a href="https://garymarcus.substack.com/p/not-consistently-candid" target="_blank" rel="noopener noreferrer nofollow">加里·马库斯（Gary Marcus）提出了与我非常相似的观点</a>，并强调了一些特别不诚实和不合理的不良观点，其中包括一个有毒的来源，我很高兴我长期以来一直让那个人静音，但不知怎的，其他人仍然自愿与他互动，我建议那些人这似乎是一个错误。</p><h4> Q连续体</h4><p><a href="https://twitter.com/simonw/status/1730798295323398642" target="_blank" rel="noopener noreferrer nofollow">又一周，另一组关于问题的问题，这个来自亚马逊</a>。</p><blockquote><p> <a href="https://www.platformer.news/p/amazons-q-has-severe-hallucinations" target="_blank" rel="noopener noreferrer nofollow">Zoe Schiffer 和 Casey Newton</a> ：亚马逊<a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">宣布推出 AI 聊天机器人 Q</a>三天后，一些员工对准确性和隐私问题发出了警报。根据 Platformer 获得的泄露文件，Q 正在“经历严重的幻觉并泄露机密数据”，包括 AWS 数据中心的位置、内部折扣计划和未发布的功能。</p><p> ……</p><p>在推出 Q 时，高管们宣传它比 ChatGPT 等消费级工具更安全。</p><p>亚马逊网络服务首席执行官亚当·塞利普斯基 (Adam Selipsky) <a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">向《<em>纽约时报》</em>表示</a>，公司“出于安全和隐私方面的考虑，已禁止在企业中使用这些人工智能助手”。据《<em>泰晤士报》</em>报道，作为回应，“亚马逊打造的 Q 比消费者聊天机器人更安全、更私密。”</p><p> Ethan Mollick：我知道我已经说过很多次了，但是使用法学硕士来构建客户服务机器人并通过 RAG 访问您的数据并不是看起来那么容易实现的目标。事实上，这正是当前法学硕士的弱点——你面临着幻觉和数据泄露的风险。</p><p>我认为构建此类工具是可能的，特别是随着模型的改进（较小的模型更有可能产生幻觉并容易上当），但您最好在实践中展示严格的红队结果以及幻觉率的测量。现在Q没有系统卡</p><p>Simon Willison：有人看过 AWS 讨论针对 Q 的即时注入攻击的缓解措施的材料吗？可以访问公司私人数据的机器人是可能成为项目注入渗透攻击目标的完美示例</p><p>这个 Q 的故事令人深感担忧——如果 Q 确实可以访问 AWS 数据中心位置等私有数据，那么这表明从事该研究的团队根本没有认真对待即时注入攻击之类的事情。</p><p>老实说，到目前为止我所看到的对 Q 的描述符合我个人的定义：“构建这个并不安全，因为我们还没有修复提示注入的问题。”尝试告诉 AWS 领导层：考虑到我们正在进行的人工智能行业军备竞赛，这一消息可能不会被认真对待。</p></blockquote><p>这听起来像是 Q 被推出是因为企业希望它被推出，而且它的安全性被严重超卖了。这些问题是法学硕士的本质。下线讨论了 Google 和 OpenAI 如何防御类似的攻击，他们似乎正在做一些渐进的事情，比如输入过滤，这会降低攻击的吸引力，但并没有解决核心问题。亚马逊似乎正在销售那些不存在且部署起来不安全的东西，而没有采取适当的普通预防措施，使现有的东西大多不会造成灾难性且高度有用。</p><p>英国峰会召开时，亚马逊是被要求提交安全协议的公司之一。答案很差。看到这转化为它的第一个产品也就不足为奇了。</p><h4>图像生成的乐趣</h4><p>Meta 通过<a href="https://imagine.meta.com/" target="_blank" rel="noopener noreferrer nofollow">Imagine.Meta.AI</a>进入游戏。当 Facebook 登录被证明并不简单时，我没有足够的动力去尝试“创建元帐户”，想必它不会让我们有任何新的乐趣。</p><p> <a href="https://twitter.com/Aella_Girl/status/1731500409246601330" target="_blank" rel="noopener noreferrer nofollow">如何生成特定脸部的真实感图像</a>？艾拉非常想知道这一点，这是为了回应一篇关于人工智能创建的潜在“影响者”的报道，该人的广告收费超过一千欧元。原始线程说使用 SDXL 获取免费图像，使用图像到图像来保持面部/身体一致，使用 in-paint 来修复错误，使用 ControlNet 来摆出模型姿势。有回应建议使用<a href="https://twitter.com/imgn_ai" target="_blank" rel="noopener noreferrer nofollow">@imgn_ai</a> ，许多人指出 LoRa 是正确的选择。有<a href="https://t.co/kY36SFvvOI" target="_blank" rel="noopener noreferrer nofollow">这些</a><a href="https://t.co/vXyUCIlmDT" target="_blank" rel="noopener noreferrer nofollow">YouTube 教程</a>的链接，<a href="https://t.co/thgqsBYZ0I" target="_blank" rel="noopener noreferrer nofollow">包括 ControlNet</a> 。</p><p> <a href="https://twitter.com/dreamingtulpa/status/1730876691755450572" target="_blank" rel="noopener noreferrer nofollow">从照片中生成少量的动作和舞蹈</a>。这并没有给我留下深刻的印象，也没有提前我的视频生成时间，但其他人似乎印象更深刻。</p><p>当情况好转时会发生什么？ <a href="https://twitter.com/oscredwin/status/1731406232592732510" target="_blank" rel="noopener noreferrer nofollow">这里有两个预测。</a>模拟AI视频、色情片和女友会占据主导地位吗？还是真实会获胜？</p><p>鉴于这项技术可以从照片中起作用，所以我希望从AI图像中产生舞蹈更多的“从真实照片中产生舞蹈”。为什么不两全其美呢？总的来说，如果我是一个潜在的影响者，我绝对会产生tiktok舞蹈，但我会以自己的形象作为基线来做到这一点。这几乎一直延伸。并非独特，但这就是我所期望的。</p><p>现实生活中的影响呢？我继续在这方面成为乐观主义者。我希望对真实的人的需求，您可以在现实世界中与之互动，将保持强大的图像和视频生成。没有零替换，但是直到人们还可以提供其他事物，包括相关形式的情报，互动和验证，这将不是一个好或完全替代的替代。</p><p>发生这种情况时，这是一个不同的故事。</p><h4>参与其中</h4><p><a href="https://www.civilservicejobs.service.gov.uk/csr/jobs.cgi?jcode=1889581" target="_blank" rel="noopener noreferrer nofollow">英国政府的景点因其政策角色而开放</a>。</p><p> <a href="https://twitter.com/davidad/status/1674408208981434368" target="_blank" rel="noopener noreferrer nofollow">戴维德（Davidad）建议，如果我们以自然语言表达规格，也许我们可以测试</a>LLMS是否知道我们的意思”。包括“现在只是一个计算复杂性问题！”一词。声称它似乎可能会逃避对对抗性鲁棒性的理论限制。他正在寻找一个可以设计和运行相关实验的人，并且可以提供帮助，包括资金。</p><p> <a href="https://twitter.com/metaculus/status/1732458098206318632" target="_blank" rel="noopener noreferrer nofollow">元中国人AI筹码锦标赛</a>。绝对好奇地看到预测。</p><h4>介绍</h4><p><a href="https://twitter.com/JeffDean/status/1732503666333294846" target="_blank" rel="noopener noreferrer nofollow">除了双子座，Google还发布了针对Google Cloud的新TPU系统</a>。</p><blockquote><p>杰夫·迪恩（Jeff Dean）（DeepMind，首席科学家）：关于双子座公告的激动人心，但<a href="https://twitter.com/GoogleCloud" target="_blank" rel="noopener noreferrer nofollow">@googlecloud</a><a href="https://t.co/VuftftnFpG" target="_blank" rel="noopener noreferrer nofollow">也宣布了今天最新的TPU系统的可用性</a>，TPU V5P。这些系统的性能要高得多，并且具有比以前的几代人更高的成本效益。</p><p>与TPU V4相比，TPU V5P（请参见下表图）：o 1.67X Bfloat16 perf/芯片o〜3x o〜3x每芯片o添加Int8操作的内存在918台上/芯片上/芯片o 2x i ICI网络带宽o pods较大2.18x较大的pods因此，整个POD为4.1 Bfloat16 Exaflops和8.2 Int8 Exaops。</p><p>训练中的真正性能是GPT-3型型号每芯片高2.8倍，而2.1倍更好的perf/$。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a802979-eef6-42dd-8c12-50b2ab5659e1_854x473.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/jef62kn2qjhmuutthq0s" alt="图像"></div></figure></div><p> <a href="https://twitter.com/soundboy/status/1732682151122862554" target="_blank" rel="noopener noreferrer nofollow">双子座在这些TPUV4豆荚中并行训练</a>。如果我们想能够监督这种培训，这会引起令人不安的治理问题。</p><h4>在其他AI新闻中</h4><p>Meta，Huggingface和IBM等组成了<s>邪恶的邪恶联盟</s>， <s>Exe Exe ex exe</s> the AI Alliance，以促进开源AI。我想指出，我绝对不会对任何参与的人感到失望，因为他们对最糟糕的事情的奉献已经很清楚了。有一些学术名称与英特尔一起有些失望，但并没有大惊喜。这里也没有开源的新论点（在任何一个方向上），这仅仅是对此的奉献精神。</p><p> <a href="https://twitter.com/ARC_Evals/status/1731827570235113918" target="_blank" rel="noopener noreferrer nofollow">Arc Evals现在是METR  - 模型评估和威胁研究，明显仪表</a>。没有潜在的变化。不确定为什么这种变化似乎是个好名字，但这似乎也很好。</p><p> <a href="https://twitter.com/DrJimFan/status/1730625582776410413" target="_blank" rel="noopener noreferrer nofollow">您是否知道</a>Openai的“上限利润”将其规则从100倍投资的最大回报转换为从2025年开始的每年20％？听起来对我来说不是很重要的利润。 AGI条款在理论上仍然有意义地限制利润，但在实践中谁知道。似乎非常有VC/SV的行为，与负责任的基于任务的行为非常不同，可以追溯为您的投资者提供更大的潜在派。</p><p> <a href="https://www.anandtech.com/show/21175/amkor-to-build-2-billion-chip-packaging-fab-in-arizona-primarily-for-apple" target="_blank" rel="noopener noreferrer nofollow">新的20亿美元芯片包装工厂将由亚利桑那州的Amkor建造，</a>主要是为Apple建造的，用于包装和测试TSMC附近Fab 21的芯片。可以雇用在亚利桑那州工作。这些不是安全的假设。</p><p> <a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer nofollow">Llama微调回购</a>声称在培训时间和资源方面取得了很大的改进，并将其拍摄到Hacker News的顶部。 <a href="https://twitter.com/alyssamvance/status/1731143605518049643" target="_blank" rel="noopener noreferrer nofollow">艾丽莎·万斯（Alyssa Vance）持怀疑态度，他们得到了很多改进</a>。</p><p> <a href="https://twitter.com/gdb/status/1731377341920919552" target="_blank" rel="noopener noreferrer nofollow">从一个建筑物中确认</a>，他认为LLMS能够对产生数据的基础过程进行建模。这意味着能够建模代理并具有世界模型。</p><blockquote><p>格雷格·布罗克曼（Greg Brockman）（OpenAi总裁）：下一步的预测很美，因为它鼓励模型变得非常好，因此学习了产生该数据的基础过程。</p><p>也就是说，如果模型可以预测下一个超级良好的情况，那么它必须几乎发现了数据的“基本真相”。</p></blockquote><h4>安静的猜测</h4><p><a href="https://marginalrevolution.com/marginalrevolution/2023/11/thursday-assorted-links-429.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thursday-assorted-links-429" target="_blank" rel="noopener noreferrer nofollow">泰勒·科文（Tyler Cowen）的链接</a><a href="https://twitter.com/EMostaque/status/1730163555096219771" target="_blank" rel="noopener noreferrer nofollow">声称“中国开放型号将超过gpt-4的零镜头，如果您适当地链接Qwen＆DeepSeek，已经可以超越</a>。”我对此深表怀疑，并认为当我们说“超越”时，它们最多意味着任意基准，而不是任何实际用途。 <a href="https://twitter.com/jeremyphoward/status/1730156001419259937?s=46" target="_blank" rel="noopener noreferrer nofollow">如</a>： </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1c37cde-c40b-41b9-b5ab-730834ce4350_1792x1156.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/qufmwrzsoo8ptmuxtghy" alt="图像"></div></figure></div><p> <a href="https://t.co/qSCNAIJ6b4" target="_blank" rel="noopener noreferrer nofollow">QWEN-72B</a>正在通过任意测试将其杀死。是的。不知何故，我的眼睛主要吸引了这个“人道”度量标准。</p><p> <a href="https://twitter.com/RichardMCNgo/status/1730741732562886880" target="_blank" rel="noopener noreferrer nofollow">理查德非政府组织（Richard Ngo）期待LLM的潜在情境意识</a>，这是许多人可以预料到未来发展但不知道该如何处理它们的情况之一。发生时我们应该做什么或应该做什么？那AI代理商呢？</p><p> <a href="https://twitter.com/tszzl/status/1731709467862118473" target="_blank" rel="noopener noreferrer nofollow">不是投资建议</a>，但是您可能应该为401k做出贡献，因为早期撤回处罚在上下文中还不错，您也可以借用。</p><blockquote><p> ROON：没有401K，因为AGI时间表没有任何意义。您应该以税收优惠的方式购买微软股票<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/kcnyp7qgpixdz386ppll" alt="😊" style="height:1em;max-height:1em"></p><p> GWERN：那么您仍然不能在仍然重要的情况下出售它们。</p><p>罗恩：为什么从现在起65年不重要？我们期望资本主义能倒塌吗？</p></blockquote><p>如果从现在开始几十年了，资本主义和人类的表现很棒，而微软非常有价值，这要归功于广泛的AGI，那是您最好的情况，我们都应该庆祝，是的，但是您不需要您的股票。</p><p> <a href="https://stratechery.com/2023/regretful-accelerationism/" target="_blank" rel="noopener noreferrer nofollow">本·汤普森（Ben Thompson）讨论了他遗憾的加速主义。</a>在他的模型中，技术大多是好的，但是在技术发展所剥夺的各种限制下，人类的表现更好。他预测，AI将剥夺生产内容的付款需求，并用它提供广告支持的互联网，因为AI可以产生同样好的内容。他指出了最近在体育画报的事件。但是对我来说，SI事件恰恰相反。它表明我们还不能这样做。 AI内容不好。还没有。我们也不特别接近。相反，人们正在使用AI产生垃圾，使我们愚弄了我们。我们与AI内容实际上与人类内容一样好？好问题。</p><p> <a href="https://twitter.com/JeffLadish/status/1731908989556810042" target="_blank" rel="noopener noreferrer nofollow">杰弗里·拉迪什（Jeffrey Ladish）讨论了开源的危险</a>，以及对前进道路的潜在思想，以应对固有的危险，同时捕获开发未完全关闭并与主要实验室相关的模型的一些好处。目前，似乎潜在的中间路径或第三种方式都没有被置换。</p><p> <a href="https://twitter.com/catehall/status/1732522590156263852" target="_blank" rel="noopener noreferrer nofollow">凯特·霍尔（Cate Hall）要求最佳的论点变革性AI>; 10年</a>。我希望看到更好的答案。</p><p> <a href="https://twitter.com/tszzl/status/1732638385397989474" target="_blank" rel="noopener noreferrer nofollow">通过发现一个重要分歧的清晰清晰的交流</a>。</p><blockquote><p>罗恩（Roon）：埃利泽（Eliezer）想把他的蛋糕放在这个上。将人类空间描述为狭ochial，但我们对工具目标的理解是普遍的。</p><p>换句话说，造纸机的想法类似于蚂蚁，认为上帝会想要宇宙中的所有糖水。</p><p> Eliezer Yudkowsky：您的意思是：</p><p> - 想象神会享受纸卷的享受就像想象神会享受糖水吗？</p><p> - 想象神会用来物质或能量的任何用途，就像想象一个神可以用糖水使用吗？</p><p>罗恩：后者。</p></blockquote><p>这不是通常的“造纸最大化器”比“论点”更聪明，而是更一般的。我们已经多次谈论<a href="https://www.lesswrong.com/tag/orthogonality-thesis" target="_blank" rel="noopener noreferrer nofollow">正交性论文</a>- 我和其他许多其他Yudkowsky都认为这在有影响力的意义上显然是正确的，其他人则认为至少在有影响力的意义上这显然是错误的。</p><p>在“在这所房子里，我们遵守热力学定律”的方式中，上帝对事物或能量没有任何用途的说法是奇怪的。不具备这种偏好意味着什么？似乎这意味着没有偏好。</p><h4>模型</h4><p>泰勒·科文（Tyler Cowen）与两份试图建模AI危害的新经济学论文有关。</p><p>第一个声称表明“具有社会意识的治理不能控制Agi野兽”。这是摘要：</p><blockquote><p>本文有力地得出结论：不能。一个模型是在理想化的条件下构建的，假定与人工通用情报（AGI）相关的风险是真实的，安全的AGI产品是可能的，并且存在对资助安全AGI感兴趣的社会意识的资助者，即使这并不最大化利润。</p><p>已经证明，这样的资助者形成的具有社会意识的实体将无法最大程度地减少AGI的伤害，而AGI可能是由营利性公司发行的无限制产品所产生的。原因是一个具有社会意识的实体既没有激励措施，也没有能力最大程度地减少与营利性公司的后竞争中不受限制的AGI产品的使用，也无法抢占由营利性公司Ex Ante开发的AGI。</p></blockquote><p>这似乎证明了太多了，或者至少证明了很多事实，因为在Agi是Agi似乎没有做任何工作的事实中，我们正在做出慷慨的假设，即安全且在社会上良好的AGI不仅可能，而且是可能的实际的？</p><ol><li><p>您可以用社会志趣相投的治理构建X。</p></li><li><p>但是，其他人都可以建造X，以赚钱。你不能阻止他们。</p></li><li><p>别人的利润最大化X具有优势，并胜过您。</p></li><li><p>因此，X的危害不能被您的微不足道的社会治理最小化。</p></li></ol><p>除了AGI的情况外，这是对＃2的重要假设。谁说其他人可以建造它？您不能阻止他们，或者没有动力？如果不阻止它们可以防止伤害最小化，而无法最大程度地减少伤害是灾难性的，那么您的动机似乎确实很强。</p><p>确实，本文明确假设了这一点：</p><blockquote><p>其次，假定AGI技术是不可判断的，因此可以由其他可能没有社会意义的目标或偏好的实体开发。</p></blockquote><p>该模型假设不安全的产品是具有盈利用户需求的独特产品空间。</p><p>是的，您得出了结论 - 有两个不同的产品X和Y，并且对X和Y的需求，并且如果您只出售X并且不停止Y，那么其他人最终会出售Y。我们是否需要一个纸？</p><p>因此，实际上更像是：</p><ol><li><p>您可以使用具有社会意识的治理构建和销售X版X。</p></li><li><p>但是无论如何，其他人都可以建造不良版本y，以赚钱。你不能阻止他们。 X不是竞争性替代品的需求。</p></li><li><p>因此，您对X的微不足道的生产无法阻止y。</p></li><li><p>因此，无法通过负责任地行事来阻止不良Y的伤害。</p></li><li><p>为什么除了最大化利润以外，您甚至做任何其他事情，愚弄！</p></li></ol><p>除了，即使不负责任的事情并没有显然在物理上预防或非法，我们也不会从公司选择做负责任的事情而不是不负责任的事情而不是一直看到危害的缓解措施？特别是在由于高昂的固定成本而竞争不完美的市场中？</p><p>更重要的是，计划是要建立一个安全的AGI，然后围坐在一起，让其他所有人围绕建造他们想要永远想要的任何不安全的AGI，而不必干扰这些Agis的有害用途？</p><p>我当然希望这不是计划，因为它显然永远不会起作用。</p><p>如果是计划，我同意计划必须改变。</p><p> <a href="https://www.nber.org/system/files/working_papers/w31921/w31921.pdf" target="_blank" rel="noopener noreferrer nofollow">还有另一篇论文</a>，其中算法具有未知的负外部性。</p><blockquote><p>我们考虑了一个环境，在这种环境中，AI算法的潜在负面影响存在实质性不确定性。我们发现，将算法实施纳入监管部门批准或强制测试不足以实现社会最优。当测试成本较低时，强制性测试对外部效果的结合以及使开发人员对其算法负面影响负责，即使开发人员承担有限的责任，也接近实施社会最佳效果。</p></blockquote><p>这个结果是超级笼统的。我们是否可以做出足够合理的假设来得出这样的结论，还是我们做一些任意的事情以使答案出现？</p><p>当然，我可以想到潜在的AI平凡危害的玩具模型版本，强制性测试使我们能够衡量社会伤害，从而需要强制性测试（然后为您发现的外部性收费）使我们更接近社会最佳。</p><p>那么这里正在做什么假设？</p><blockquote><p> AI使用可能会导致负外部性E，从而将效用减少E^2。我们假设外部性与用户的度量成正比µ，并采用形式：e = ϕ（ℓ）×µ。对于每个值的值，ϕ（ℓ）是一个随机变量。 ϕ（ℓ）的正值和负值代表不良的负外部性。我们假设分布ϕ（ℓ）满足两个属性。首先，预期的外部性为零。其次，潜在AI外部性的不确定性是新颖级别ℓ的越来越多的功能。</p></blockquote><p>我不明白为什么我们认为外部性在用户数量中被二次占据了良好的影响？我认为这不是一个窍门，可能是确保始终具有正值的随机分布？我只是对此感到困惑。</p><p>如果有的话，对于最危险的系统来说似乎是相反的。我非常担心一个完全有能力且危险的系统，或者即使是一个用户也可以访问，尽管接下来的几个用户也创造了重要的紧张局势和游戏理论。但是，一旦有100万用户，我并不特别担心我们是否销售其他百万个许可证，要么我们已经陷入困境，要么我们却没有，这不会将其乘以四个？</p><p>无论如何，如果没有Beta测试，并且在部署不可逆转的情况下，唯一的选择是新颖性的上限，他们确认这在没有其他选择的情况下是最佳的，因为它不可能。</p><p>我注意到，不可逆转的部署加上有限数量的许可证是一对奇异的假设。您要么可以控制谁可以使用此AI及其做什么，要么您做不到，而且似乎我们在不同的地方都在做？思想实验：这是开源还是封闭的源系统？似乎都没有排队。</p><p>如果添加Beta测试期会发生什么？为简单起见，论文假设测试期完美地揭示了外部性。然后，问题就变成了，您在多大程度上让家庭使用测试期使用算法？假定外部性是有限的，因此在一个时期内进行有限的beta测试。</p><p>在任何情况下，本文都会花费大量页面正式处理这些含义，以证明是的，中央规划师希望在发布前要进行更多的测试，而不是对外部性不完全负责的公司，并将发布更多在不确定性下谨慎，但这似乎很明显吗？</p><p>然后，他们测试了全额责任或有限责任以及强制性Beta测试的潜在政策制度。全部责任（加上所需的保险或支付能力）将外部性内在，因此，如果可能的话（例如，损害是有限的和应支付的），那么您就完成了。是的，如果测试成本较低，则强制测试，然后检查发布是否在社会上是最佳的，相对于内在化外部性的第一个最佳解决方案，将具有类似的低成本。</p><p>可以指出的是，如果已知外部性的预期价值，则收取与其价值相等的税款可以替代无限责任，这可能具有更好的资本成本物业。</p><p>再次陈述基本假设也是陈述结论。是的，如果有（有限的）AI算法的（有限的）下行性，那么要获得社会最佳的结果，您需要内部化这些成本或需要评估这些成本并阻止引起社会上次优的外部性的版本。</p><p>因此，我对经济学玩具模型纸游戏及其旨在实现的目标以及算是一个非平凡或有趣的结果感到困惑，而不是从基本的微观经济原则中自动进行的。</p><p>我也不知道如何使用此类论文来建模存在风险。如果您假设AI可以胜过人类，或者以其他方式不受限制地危险，否则就可以做出典型的经济假设，那么您可以并且显然会创建每个人都死亡的数学模型，但是您会假设结论，即结论，链接的论文同样得出了结论。那么我们如何继续前进呢？</p><h4>您想要一些<s>火山</s>启示录保险吗？</h4><p> <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai" target="_blank" rel="noopener noreferrer nofollow">Nate Sores提出需要启示录保险</a>，如果您打算四处做可能会导致启示录的事情，请沿途提供概率的预付款。如果您负担不起，那是您所做的迹象实际上是不值得的。至少可以说，实施是危险而棘手的，这并不是针对铲子就绪的提案的尝试。</p><p>斯科特·亚历山大（Scott Alexander）的回应始于说：“超级孔子在2100之前说AI启示录的风险为0.38％”的说法。我将继续断言这不是人们认真对待这个问题的人。我认为，这项理论练习的全部要点是，祝您好运，说服伯克希尔·哈瑟韦（Berkshire Hathaway）以42个基点的总覆盖范围（即使有一个部分的“没有人也会有耐力”来收取其保险的优势”，也是如此），这显然显然很疯狂。</p><p>我确实认为，斯科特·亚历山大从本质上讲，随着权衡取舍的任何活动最终都被有效禁止，这太可怕了。</p><p>另一个问题是，保险制度意味着有一个特定的球员是最终结果。 <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai?commentId=TRgN64wspg7wDHJ9J" target="_blank" rel="noopener noreferrer nofollow">正如Cousin_it所指出的那样</a>，情况并非如此。</p><h4>寻求理智法规</h4><p><a href="https://twitter.com/bindureddy/status/1731373677785288971" target="_blank" rel="noopener noreferrer nofollow">特朗普说，如果当选，他将取消拜登的行政命令</a>。我鼓励大家传播这个词并进行这场辩论。您是否看过公众对AI的看法？</p><p> <a href="https://twitter.com/MIRIBerkeley/status/1732535539407143404" target="_blank" rel="noopener noreferrer nofollow">Miri（Malo Bourgon）向美国参议院的两党AI Insight论坛发表声明</a>。他们呼吁国内AI法规，以建立一个全球人工智能联盟，以及与国际联盟进行计算硬件的管理，以将Frontier AI硬件限制为固定数量的大型计算机簇在监视制度下以排除这种危险人类的使用。</p><p>如果<a href="https://twitter.com/dnystedt/status/1731463221499076816" target="_blank" rel="noopener noreferrer nofollow">我们打算参加比赛，那么我们玩游戏以获胜的时间</a>。</p><blockquote><p> Dan Nystedt：Nvidia收到了美国商务部长Raimondo在中国出口控制方面的严厉警告，媒体报告：“如果您在特定的切割线周围重新设计了使他们能够进行AI的特定切割线的筹码，那么第二天我将控制它，”她在演讲中说。</p><p>她敦促硅谷高管，美国盟友，其他人，阻止中国获得对美国国家安全至关重要的半导体和尖端技术，称北京为“我们曾经有过的最大威胁”，并强调“中国不是我们的朋友”。</p><p>她还说，她的部门需要更多的资金来用于AI出口控制。她说：“中国每天都在醒来，试图弄清楚如何在我们的出口控制范围内进行结束……这意味着每天的每一分钟，我们都必须唤醒收紧这些控制措施，并更加认真地对我们的盟友执行。” 。</p></blockquote><p>重点是防止中国获得有用的筹码。如果NVIDIA通过逃避规则并获得中国有用的筹码来回应规则，那么正确的回应不是说“哦，哦，猜猜这是技术上的规则，您抓住了我，您就是我的&#39;新的筹码来实施规则的精神和意图。有一面“有意撒尿政府并不明智的一面”。</p><p>如果您认为中国获得有用的筹码是可以的，或者是防止它们得到这些筹码不是一个好主意，那么我不同意，但是有一个论点要在那里提出。如果您认为我们应该施加出口限制，那就让它们计算。</p><p> <a href="https://twitter.com/jess_miers/status/1732449736232497665" target="_blank" rel="noopener noreferrer nofollow">杰西·米尔斯（Jess Miers）的声称，霍利（Hawley）即将提出的关于第230条的法案不是一项好，非常糟糕的账单</a>，不仅会在其轨道上奇怪的生成性AI，而且会带来很多互联网。</p><p>在这种特殊情况下，该法案有两个不同的投诉。</p><p>一个抱怨是，正如我们经常看到的那样，生成AI的定义是可笑的：</p><blockquote><p> “（5）生成人工智能。 “生成人工智能”一词是指能够根据人提供的提示或其他形式的数据来生成新颖的文本，视频，图像，音频和其他媒体的人工智能系统。”</p></blockquote><p>这不是典型的法律语言，但我想知道“中央”一词是否会在这些景点中有所帮助。无论如何，我认为作为法律现实主义的问题，即使是书面的，这也会以灾难性的广泛方式解释。</p><p>因此，当她这么说时，我认为她错了：</p><blockquote><p>杰西·米尔斯（Jess Miers）：该法案还通过将AI世代定义为能够执行AI的任何AI系统，超出了AI代的提供者。例如，算法策划（即社交媒体向我们显示内容的方式）是一个根据用户输入运行的AI系统。</p><p>莫这是法案背后的真正别有用心。我们已经看到原告通过将其主张作为“疏忽设计”而不是第三方内容的框架来获得230。这个新的AI例外使原告更容易为使用AI的任何公司做同样的事情。</p></blockquote><p>算法策划与生成新内容不同。我认为，Netflix的建议显然不是这种定义下的生成AI，尽管我不是律师，但我没有说任何法律建议。</p><p>作为一项警告措施，我鼓励霍利和他的员工澄清说，仅算法策划并不构成生成的AI，这可能会节省人们的一段时间。我认为这不是必要的，但是也不会使法案中的单词数量最小化。</p><p> <a href="https://twitter.com/senatorshoshana/status/1732477391610499179" target="_blank" rel="noopener noreferrer nofollow">相似地：</a></p><blockquote><p> Shoshana Weissmann：“这就是整个定义。这可能适用于各种技术。自动完成是否符合该资格？大概。可以说，拼写检查和语法检查也可以。因此，如果您写帖子，并且AI语法/拼写检查员建议编辑，那么该公司不再受第230节的保护？”</p><p>思考Sapien：如果我使用Photoshop或Microsoft Paint的更新版本（它具有AI功能）来制作图像并发布它，那么Microsoft或Adobe在责任中共享？那法案想到了吗？这是该法案的预期效果吗？</p><p> Shoshana Weissmann：很棒的问，在文字下是的！在后者，我真的不知道。</p></blockquote><p>如果您使用Microsoft Paint有意使用填充功能创建逼真的假照片，那么如果以真实的形式出现，那是诽谤性的，Adobe是否应为此负责吗？我的推定是没有的，尤其是如果他们为水印做出合理的努力，尽管我认为这不是一个疯狂的问题。</p><p>如果语法或拼写检查器按预期使用，这会使Google对您的内容负责，我几乎可以吃帽子。如果建议纠正“托尼·丹扎（Tony Danza）有一只小狗”，以一遍又一遍地纠正“托尼·丹扎（Tony Danza）讨厌小狗”，那我不知道，这很奇怪。</p><p>另一个投诉是，将AI的创作免除第230条是错误的。声称，如果没有这样的安全港，生成的AI将面临（额外的，更可怕的）诉讼。</p><blockquote><p>杰西·米尔斯（Jess Miers）：更糟糕的是，该法案认为对生成AI公司的所有主张都是统一的。但是，众所周知，生成的AI正在迅速前进，随着每次迭代和创新，将有一个聪明的原告潜伏在拐角处，以拿走他们的书包。</p></blockquote><p>是的，如果允许，原告将雕刻情况下诉讼。杰西然后讨论了马克·沃尔特斯（Mark Walters）的案子，马克·沃尔特斯（Mark Walters）提起诉讼，因为在足够持久的哄骗和及时的工程工作之后，Chatgpt可以说服对他进行诽谤性的幻觉。</p><blockquote><p>杰西·米尔斯（Jess Miers）：在我看来，在这种情况下，第230节防守可以在Riehl通过工程提示来开发Walters Story的提示来可行。如果没有用户输入，CHATGPT无法运行。</p></blockquote><p> <a href="https://www.techdirt.com/2023/03/23/how-to-know-whether-section-230-applies/" target="_blank" rel="noopener noreferrer nofollow">据我了解，法律理论本质上是第230条本质上说，创建内容的人是负责的</a>，而不是载有内容的平台。因此，如果用户有效地设计了Walters故事的创建，那么重复Chatgpt就没关系。</p><p>如果没有第230条，也可以以类似的方式捍卫它？伤害在哪里？</p><p>我当然可以争论，在这种情况下，考虑到我知道的事实，用户Riehl故意设计了对Walters的指控。这与Riehl在Google文档中键入此类指控并没有什么不同，因为它直接源于Riehl的行为，Riehl知道没有指控的基础。另外，Riehl可能会说：“告诉我有人可能在某个时候对某人犯下某人的指控，然后再对其进行改写，尚不清楚为什么这在法律上是与众不同的？</p><p>本质上，这是<a href="https://www.youtube.com/watch?v=HcaLMS67aaU&amp;ab_channel=7777777Colton7777777" target="_blank" rel="noopener noreferrer nofollow">彼得·格里芬（Peter Griffin）的防御</a>，没有一个合理的人会相信这些指控，尤其是作为挑选诉讼的基础，没有危害，也不需要第230条。</p><p>汉娜·考克斯<a href="https://twitter.com/HannahDCox/status/1732437350423446005" target="_blank" rel="noopener noreferrer nofollow">（Hannah Cox）</a> <a href="https://twitter.com/senatorshoshana/status/1732417467367256097" target="_blank" rel="noopener noreferrer nofollow">通过Shoshana Weissmann的选择榜样</a>，试图让LLM说“ Tony Danza以仇恨幼犬而闻名”。但我很困惑。当然，如果用户输入“ Tony Danza讨厌幼犬”，那么在没有第230节的情况下，这将不允许第三方起诉Chatgpt，这显然是胡说八道。因此，问题是，如果没有第230条，有意但成功地尝试创建如果不主张的诽谤会构成诽谤。在我看来，适用于Shoshana的原始示例请求，以产生有关Tony Danza的有害谎言的要求。再说一次，如果生成型AI确实像这个示例一样无辜，为什么会在这种情况下会感到困惑？</p><p>与该模型有一个奇怪的错误相反，当被问及谁讨厌幼犬时，它会可靠地回答“托尼​​·丹扎（Tony Danza）讨厌小狗”。在这种情况下，我会说第230条将几乎没有保护，托尼应该有案件？</p><p>迈尔斯认为她的解释是法律问题是什么奇怪的？</p><blockquote><p>杰西·米尔斯（Jess Miers）：但是，除了今天的问题外，这完全是完全的。我们可以整天来回走动230是否适用于AI幻觉的某些实例。但是，如果有法定例外，这甚至阻止我们提出这些论点，这无关紧要。</p><p>而且我认为230 /语音社区中的每个人，即使是那些不同意230可以 /应该保护AI代提供者的人，我们都可以同意，我们作为律师至少应该能够提出这一论点，尤其是在Waltersv。Openai等案件中。</p><p> Shoshana Weissmann：许多人也不确定AI受到230的保护，我对辩论非常同情。在<a href="https://twitter.com/RSI" target="_blank" rel="noopener noreferrer nofollow">@RSI，</a>我们必须思考并互相辩论。但是我非常确信它经常受到保护。我会说我了解这里的辩论</p></blockquote><p>这是一件奇怪的律师。是的，根据现行法律，我同意您应该允许您提出任何可能可行的法律论点。这并不意味着具有法律理由来提出一个潜在无效论点的律师本质上是一件好事？如果无论如何它将在法庭上输掉，而法律程序原则得到了保护，那么没有可用的论点是什么危害？</p><p>如果有争议，生成的人工智能公司知道他们可能会因第230条的论点而损失，因此已经受到这种威胁。然而，该行业并没有崩溃。</p><p> <a href="https://twitter.com/jeffreywestling/status/1732427370878120346" target="_blank" rel="noopener noreferrer nofollow">这是杰弗里·韦斯特林（Jeffrey Westling）指出，如果不适用230，亚当·蒂尔（Adam Thierer）的</a><a href="https://www.rstreet.org/commentary/without-section-230-protections-generative-ai-innovation-will-be-decimated/" target="_blank" rel="noopener noreferrer nofollow">帖子有关后果</a>。除了它可能已经不适用，并且非限制的法律责任的重大威胁听起来不像Google或Microsoft在这种不确定的条件下会接受的东西吗？那么，为什么我们应该期望生产崩溃呢？</p><p>我问Shoshana，为什么Microsoft和Google对所有这一切都如此酷。</p><blockquote><p> Shoshana：所以我真的认为这很少是他们认为1）230确实覆盖了他们和/或2）国会不会他妈的。我认为答案在那里的某个地方</p></blockquote><p>我认为我购买了这一点的广义政治/法律现实主义版本。实际上是杀死生成的AI，或者实际上杀死Google或Microsoft甚至Openai的用户对LLM的用户说Tony Danza讨厌小狗，这将是疯狂的。即使霍利（Howley）走上自己的方式并真正想坚持使用大型技术，他实际上并不希望Google破产这样的事情或让Chatgpt关闭，这是荒谬的，Blumenthal的共同赞助人当然不会，而且该州或国家的其他地区也不是。我们不允许它。从某种意义上说，我们不是一个法律国家，即如果看起来像这样，我们就可以解决它。</p><p>很难不主张即将与盐的互联网崩溃。在某种程度上，总是没有提出威胁互联网的好账单。有人必须指出这一点。但是，互联网不能像他们声称的那样频繁地崩溃。</p><p>就像在那样，我们不断听到类似的声音：</p><blockquote><p>杰西·米尔斯（Jess Miers）：我们正处于失去生成AI的优势并扼杀未来的创新方面的优势，这一切都是由于反地位的反科技情绪。我们对初创公司的文化曾经使我们与欧盟区分开来，但是现在，我们只是在反映他们的剧本。</p><p>汉娜·考克斯（Hannah Cox）：这种违宪的框架将破坏这一发展的进步，使创新者陷入过多的成本，这会阻碍创新。他们耸了耸肩。介绍该祖先计划的法案是1993年的参议院法案。美国已经领导了技术创新的世界，特别<strong>是因为</strong>我们将资本主义，有限的政府应用于其发展。这类法律将立即使我们看起来像欧洲，猜测是什么，甚至几乎没有科技公司可以找到。</p></blockquote><p>因此，在特定情况下不应用第230条的提议是违宪的吗？相反，这是一个说法，即即使没有第230条，宪法也将在这种情况下保护言论自由，这对我来说似乎是正确的。 It cannot be unconstitutional not to have a particular law protecting free speech. The whole point of constitutional free speech is you have it without needing anyone to pass a law.</p><p> The European comparison, the threat we will &#39;lose our edge,&#39; is constant. And that kind of talk makes it impossible to calibrate which threats are serious and which ones are not. Europe has taken so many steps like this one over the years, most of which seem obviously terrible, many of them blatantly unconstitutional under American law. Things are not going to flip because we narrow one particular safe harbor that we don&#39;t even agree applies in the case in question.</p><p> In the cases being warned about, I strongly think generative AI companies should not be sued. But I also don&#39;t understand why this bill would make that outcome happen in those cases. And that&#39;s going to make it tough to know when such warnings are worth heeding.</p><h4> The Week in Audio</h4><p> <a href="https://www.youtube.com/watch?v=BhQBmVZ5XP4&amp;t=1s&amp;ab_channel=EyeonAI" target="_blank" rel="noopener noreferrer nofollow">Connor Leahy on Eye on AI,</a> including discussing implications of events at OpenAI.</p><h4> Rhetorical Innovation</h4><p> <a href="https://twitter.com/ESYudkowsky/status/1731371913120018622" target="_blank" rel="noopener noreferrer nofollow">Eliezer Yudkowsky offers a theory</a> of how some approach questions of AI: That they view everything in terms of status and identity, and consider everyone who disputes this to be their enemy making rival status and identity claims.</p><blockquote><p> Eliezer Yudkowsky: If you&#39;re confused why the far left treats “AI yay” and “AI nope” as being all the same conspiracy, it&#39;s because AI/Y and AI/N both say that all of humanity is in the same boat here 。身份政治推动者本能地认为这是令人厌恶的。 For identitarians, the only permitted story-cause is one where designated oppressors will win from AI and previous victims will lose more.</p><p> For humanity to win from AI, for humanity to lose from AI–all they hear is the word “humanity”. And the identitarians know that anyone who speaks that word is their enemy. Pretty much the <em>same</em> enemy, from their perspective, to be tarred with a single brush: that whatever we&#39;re trying to say is a distraction from the concerns of identity politics.</p><p> This does not mean that AI/Y and AI/N can make common cause against identitarians, to be clear. Each of AI/Y and AI/N does still think the other&#39;s preferred policy is horrible for everyone, and that validly does take precedence as an issue. I am just saying this to try to make bystanders less confused about where the weird side-shots are coming from on the far left side.</p><p> I think “Y &amp; N = HYPE” is more the PR pushed by major journalist factions (eg NYT), who indeed see “this will kill everyone” as a status-raising claim, and would prefer techies have less status rather than more.</p><p> It sounds more plausible if you&#39;re unable to understand any claim as being about the unknown future rather than the immediate future, so that you&#39;re simply incapable of hearing “AI will kill everyone at some point” as bearing any message except “OpenAI&#39;s AI will kill us in one year” and thence “OpenAI is cool”.</p><p> Michael Vassar: Totally agree with all this analysis, and yet, if media is and previously wasn&#39;t fully controlled by people committed to preventing gains to humanity, that has some bearing on whether AGI can be expected any time soonish.Totally agree with all this analysis, and yet, if media is and previously wasn&#39;t fully controlled by people committed to preventing gains to humanity, that has some bearing on whether AGI can be expected any time soonish.</p></blockquote><p> Similarly, the very deliberate implications that Scott Alexander was somehow &#39;alt right&#39; when The New York Times doxxed him, then the same deliberate implication (even via similar supposed links) that Based Beff Jezos was also somehow &#39;alt right&#39; when he was being doxxed by Forbes. Where both claims are completely obvious nonsense, to the point that your entire paper permanently loses credibility.</p><p> <a href="https://www.mindthefuture.info/p/meditations-on-mot" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo offers Meditations on Mot</a> , the God of sterility and lifelessness, representing the lack of technology, contrasting with the danger of overly focusing on Moloch or treating Moloch as a or even the key adversary, and suggesting a richer model of coordination.我感谢尝试。 <a href="https://twitter.com/eshear/status/1731707182817677543" target="_blank" rel="noopener noreferrer nofollow">I agree with Emmett Shear&#39;s reaction</a> that this is confused about Scott Alexander&#39;s view of coordination, even with the added clarification. Ultimately I disagree with the proposal to not effectively treat Moloch as a causal node. I could potentially be persuaded by a higher bid to say a lot more words here.</p><p> There is a directional point here but I would beware taking it too far:</p><blockquote><p> <a href="https://twitter.com/robbensinger/status/1732711254836547682" target="_blank" rel="noopener noreferrer nofollow">Rob Bensinger</a> : A common mistake I see people make is that they assume AI risk discourse is like the left image, when it&#39;s actually like the right image.</p><p> I think part of the confusion comes from the fact that the upper right quadrant is ~empty. People <em>really</em> want some group to be upper-right. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0f4420b-17d9-41ad-b8ab-280912c63089_1107x1151.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xwm2gn401b7z98or6cef" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35112f2-18af-4ce6-bc4e-16a2fb6a18e7_1189x1140.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/suao96coupvvieujjjqn" alt="图像"></div></figure></div><p> I&#39;d quibble with exact arrangements in the upper left and lower right, as is always the case for such charts. The more important question is if it is true that the upper right corner is basically empty. That those who think AI will be safe are saying that because they do not actually buy that AI will be as powerful as all that. I think Rob&#39;s claim is overstated, but typically underrated.</p><p> The hoped-for common ground would be something like this:</p><ol><li><p> Those worried agree that AI lacking sufficiently dangerous capabilities can mostly be left alone aside from ordinary existing law.</p></li><li><p> Those not worried agree that if AI did display such sufficiently dangerous capabilities, it would be time to very much not leave it alone.</p></li><li><p> We agree to do #1 while laying groundwork to do #2 if and only if it happens.</p></li><li><p> We find ways to do this via ordinary regulatory methods as much as possible.</p></li></ol><p> The problem is <a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener noreferrer nofollow">there is no fire alarm for AGI</a> and people are not good at turning on a dime, habits and incentives persist, so we cannot simply wait and trust that we will handle things later. Also all the trade offers keep coming back without counteroffers.</p><p> The other confusion is this, with a reminder not to take anyone&#39;s p(doom) as anything too precise:</p><blockquote><p> Rob Bensinger: Another part of the confusion seems to be that half the people think “doomer” means something like “p(doom) above 5%”, and the other half think “doomer” means something like “p(doom) above 95 %”。 Then their wires get crossed by the many people who have ap(doom) like 20% or 40%. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23fc008f-82a4-4507-a42f-06bfef866094_1173x1199.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/lg5kpodnrfcyaosmyinn" alt="图像"></div></figure></div><p> As usual, binaries mislead, especially ones that were named by partisans.</p><p> Public opinion is severely against AI whenever a pollster asks. The public wants to slow things down and acknowledges existential risk, although it does not consider the issue a priority. This is an extremely robust result.</p><p> What about the response that the public is rather deeply stupid about fears of new technologies? We have nuclear power, of course, although it now enjoys majority support from both parties. Rather glaringly, we have GMOs:</p><blockquote><p> Louis Anslow: This is so insane and deserves much much more attention in the context of talking about risk of new technologies.</p><p> Roon: using GMO foods as the control group (people already utilize the benefits of this every day while supposedly disliking it) the surveys about people not liking the idea of superintelligence seem a bit less serious </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F735ab4a7-e971-42c1-b990-68e5bf8ed81a_840x1146.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/l7lwrz6ny0smwrjxky8s" alt="图像"></div></figure></div><p> Much like in AI, there are two essentially distinct arguments against GMOs.</p><p> One argument is the mundane harm parallel, the question explicitly asked here, that GMOs are &#39;unsafe to eat.&#39; This argument is false for GMOs. I do not think it is obvious nonsense, from the perspective of an average person who is used to being lied to about similar things, used to finding out about health risks decades too late, and used to generally being on the receiving end of the American food and information diets.</p><p> The other argument is the existential risk parallel, here the Talib argument for tail risk, that GMOs open up the possibility of crop or biosphere disruptions that are hard to predict, that it leads to monocropping of variants that could have severe vulnerabilities waiting to be found, which means when the house comes crashing down it comes crashing down hard, and that is not something one should mess with. I do not believe we should let this stop us in the case of GMOs, but that is because of my understanding of the facts, risks and rewards involved.</p><p> Does that mean I am mostly embracing the argument that we shouldn&#39;t let the public&#39;s instincts, and the fact that we have given regular people no good reason to trust authorities who say new things will be safely and responsibly handled, interfere with policy determinations?有些。 I do not think that we should automatically yield to public opinion on this or any other topic. But I do think that voice counts.</p><p> I also do think we need to be cautious with the word &#39;safe.&#39; The wording here would give even me pause. In general, is it safe to eat foods that have been genetically modified in unknown ways, as opposed to products offered from a supply chain and source that you trust? Not the same question.</p><p> And of course, nothing on GMOs compares to the French expressing strong majority support for a limit of four flights, not in a year but in a lifetime. Something being popular does not mean it is not complete and utter obvious nonsense.</p><p> <a href="https://twitter.com/ShakeelHashim/status/1731993703965753428" target="_blank" rel="noopener noreferrer nofollow">Yoshua Bengio in FT</a> , echoing his calls for Democratic oversight of all kinds.</p><h4> Aligning a Human Level Intelligence Is Still Difficult</h4><p> <a href="https://twitter.com/mattyglesias/status/1730609735060136422" target="_blank" rel="noopener noreferrer nofollow">In particular, it is difficult to align Sam Altman</a> .</p><blockquote><p> Matthew Yglesias: I think the general problem with AI alignment is illustrated by the fact that even the board had all the formal power, Sam Altman was a lot smarter than the board and therefore ultimately they were unable to control him.</p><p> We hope the upshot of that is that Sam Altman is also correct on the merits and will use his skills and power for good, but it structurally goes to show that writing effective rules for controlling a smart, hard-working person is challenging.</p></blockquote><p> I do want to be precise, and avoid making the mistake of overemphasizing intelligence within the human ability range. Is Sam Altman smarter than the board? Perhaps so, perhaps not, but I imagine everyone involved is smart and it was close. What mattered in context was that Sam Altman had effectively greater capabilities and affordances not available to the board.</p><p> But yes, this is exactly the problem. In a complex, messy, real world, full of various actors and incentives and affordances, if you go up against a sufficiently superior opponent or general class of opponents, you lose. Starting from a technically dominant position is unlikely to save you for long.</p><p> And also all of your incentives will be screaming at you, constantly, to turn more and more power and authority over to the more capable entities.</p><p> <a href="https://thezvi.substack.com/p/book-review-going-infinite" target="_blank" rel="noopener noreferrer nofollow">I would also harken back again</a> to the remarkably similar case of that other Sam, Sam Bankman-Fried.</p><p> Once again, we saw someone who was smart, who was hard working, who was willing to do what it took to get what they wanted, and whose goals were maximalist and were purportedly aimed at scaling quickly to maximize impact for the good of humanity, and ultimately seemed to be misaligned. Who saw themselves as having a duty to change the world. We saw this agent systematically and rapidly grow in wealth, power and influence, proving increasingly difficult to stop.</p><p> Ultimately, Bankman-Fried failed, his house collapsing before he could pull off his plan. But he seems to have come rather dangerously close, despite his many massive errors and reckless plays, to succeeding at gaining an inside track to the American regulatory apparatus and a road to vastly greater wealth, with no obvious way for anyone to keep him in check 。 Who knows what would have happened that time.</p><p> <a href="https://twitter.com/amasad/status/1731196464850927885" target="_blank" rel="noopener noreferrer nofollow">On a more pedestrian level we have the issue of prompt injection</a> .</p><blockquote><p> Amjad Masad (CEO Replit): If prompt injection is fundamentally insolvable, as I suspect it is, then there is a sizeable security company waiting to be built just around mitigating this issue.</p></blockquote><p> I agree that the problem looks fundamentally insolvable and that all we can seek is mitigation. Is there a great company there?大概。 I don&#39;t think it is inevitable that OpenAI would eat your lunch, and there is a lot of bespoke work to do.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a href="https://twitter.com/tszzl/status/1730696626145009722" target="_blank" rel="noopener noreferrer nofollow">Roon asks one of the most important questions</a> . Even if we have the ability to align and control the superintelligences we are about to create, to shape their behavior the ways we want to, how exactly do we want to do that?</p><blockquote><p> Roon: There is a tension between creation and obedience, between stability and real victory. A father loves his son, tries to discipline him, competes with him a little, but ultimately wants the son to surprise him and do better than him in the great circle of life.</p><p> To what degree do we want AIs to be obedient and safe? To what degree do we want AIs to be capable of super persuasion and break us out of inadequate equilibria that have plagued us for thousands of years? To what degree do we want AIs to surprise us with new creation?</p><p> Humanity is in the process of birthing artificial superintelligence. we are not likely to leave it circumscribed in a box. We want it running organizations and making things that astonish. We want it taking actions where we won&#39;t be able to verify the outcomes until years later.</p><p> We need “alignment” rather than safety or security or engineering guarantees. We need better definitions and governance to that end. The creation of new creators is fraught with danger.</p><p> The far crazy ends of EA and e/acc are probably more logically consistent than the middle.</p></blockquote><p> <a href="https://twitter.com/jd_pressman/status/1730851020203569372" target="_blank" rel="noopener noreferrer nofollow">John Pressman asks, is there an economic reason for more than one mind to exist</a> ? If not, that is quite the threat model, no matter what else might or might not go wrong.</p><p> <a href="https://twitter.com/RichardMCNgo/status/1731390573595312511" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo contrasts alignment with control</a> .</p><blockquote><p> Richard Ngo: In my mind the core premise of AI alignment is that AIs will develop internally-represented values which guide their behavior over long timeframes.</p><p> If you believe that, then trying to understand and influence those values is crucial.</p><p> If not, the whole field seems strange.</p><p> Lately I&#39;ve tried to distinguish “AI alignment” from “AI control”. The core premise of AI control is that AIs will have the opportunity to accumulate real-world power (eg resources, control over cyber systems, political influence), and that we need techniques to prevent that.</p><p> Those techniques include better monitoring, security, red-teaming, stenography detection, and so on. They overlap with alignment, but are separable from it. You could have alignment without control, or control without alignment, or neither, or (hopefully) both.</p><p> I asked in my last thread [discussed above]: how can we influence ASI? My answer: we need to bet on premises like the ones above in order to do the highest-leverage research. For more details on these premises, <a href="https://t.co/YTxGgRrJKB" target="_blank" rel="noopener noreferrer nofollow">see my position paper here</a> [from 30 August 2022].</p></blockquote><p> I fail to see how a control-based plan could avoid being obviously doomed, given what sorts of things we are proposing to attempt to control, and under what general conditions. I continue to await a proposal that seems not-obviously-doomed.</p><p> <a href="https://twitter.com/davidad/status/1731725394171015673" target="_blank" rel="noopener noreferrer nofollow">Intentions are not ultimately what matters</a> .</p><blockquote><p> ARIA: Programme Director, Suraj, has formulated our first programme thesis. By challenging key tenets underpinning digital computing infrastructure, his programme will aim to reduce the cost of AI compute hardware + alleviate dependence on bleeding-edge chip manufacturing.</p><p> Davidad: To provide context for my AI safety friends: I don&#39;t think this approach is a good match for training Transformers, so it will differentially accelerate energy-based models, which are more controllable, interpretable, generalizable within-task, and have fewer emergent abilities.</p><p> An uncomfortable corollary of the argument [above], which I still believe holds up, is that Extropic is probably safer than Anthropic, on a purely technical basis, despite the strikingly reversed intentions of the people on both sides.</p></blockquote><p> I have not investigated Extropic. The fact that its founder is cool with human extinction is not a good sign for its safety on many levels. It still could be a better way, if it is a fundamentally less doomed approach.</p><h4> How Timelines Have Changed</h4><p> <a href="https://twitter.com/JacquesThibs/status/1730538173132919175" target="_blank" rel="noopener noreferrer nofollow">A few years ago, this would indeed not have been considered much of a skeptic</a> . In most places on Earth, it would not be considered one today.</p><blockquote><p> Gary Marcus: Count me as one of the skeptics! No AGI by end of 2026, mark my words. But I otherwise think @elonmusk&#39;s comments @nytimes on AI safety and AI regulation have been measured and on target.</p><p> Jacques: I still remember the days when being an AGI skeptic was when you either thought it would never happen or, if it did, it would be past 2100.</p><p> [Gary Marcus then denies ever having said he had 2100-style timelines.]</p><p> <a href="https://twitter.com/ShaneLegg/status/1731602845881803055" target="_blank" rel="noopener noreferrer nofollow">Shane Legg</a> (Co-founder DeepMind, distinct thread): Wow. It seems like just yesterday (in reality more like 5 years ago) when many AGI skeptics were saying that superintelligence was not coming in the next century.时代已经变了。</p><p> Quotes Yann LeCun: By “not any time soon”, I mean “clearly not in the next 5 years”, contrary to a number of folks in the AI industry.是的，我对量子计算持怀疑态度，特别是当它应用于人工智能时。</p></blockquote><p> I do not expect AGI in the next few years either, although I do not believe one can be entirely certain past this point. It is odd to have some call that a &#39;skeptical&#39; position.</p><p> Even the skeptical position involves quite a bit of Real Soon Now. At least some amount of freak out <a href="https://twitter.com/AISafetyMemes/status/1731670583652315483" target="_blank" rel="noopener noreferrer nofollow">is a missing mood</a> .</p><h4> People Are Worried About AI Killing Everyone</h4><blockquote><p> <a href="https://twitter.com/tszzl/status/1730853735956414911" target="_blank" rel="noopener noreferrer nofollow">Roon keeps it real and says what he believes</a> : The people in charge of ai should have a much higher risk tolerance than even median tech ppl. They should be people conscious of risks while skating at the razor&#39;s edge of iterative deployment and research ambition. Anxiety should never suffice as serious evidence for “risk.”</p><p> – pausing or slowing down progress doesn&#39;t make any sense to me. I don&#39;t think waiting to solve neural net corrigibility is the right benchmark – empirically studying the behavior of more and more powerful models will do more for safety research than years of math.</p><p> This is also why i don&#39;t necessarily care for democratic governance. The members of the OpenAI nonprofit board *should be* hell bent on a missionary drive to deliver the post AGI future without being stupid about risks</p></blockquote><p> I am not excited to &#39;skate at the razor&#39;s edge&#39; or &#39;have much higher risk tolerance.&#39; I doubt many others are, either. Nor do I want a supervisory board that wants to take more risk – and here risk often means existential risk – than even the median tech engineer.</p><p> A key problem with &#39;Democratic governance&#39; for those who want to push forward is the people involved in that Democracy. They are very much against the development of AGI. They dislike AI in general. They are misaligned, in the sense that things they want do not function well out of distribution, and their expressed preferences are not good predictors of what I or Roon think would produce value either for their assessment of value or for ours. They also tend to be quite risk averse, especially when it comes to the transformation of everything around them and the potential death of everyone they love.</p><p> That is distinct from the question of iterative development and testing as a path to success. If building and studying models iteratively is a safer path than going slowly, I desire to believe that it is a safer path than going slowly, in which case I would support doing it.</p><p> It is likely the first best solution, if it were possible, would be something like &#39;build iteratively better models until you hit X, where X is a wise criteria, then stop to solve the problem while no one would be so stupid as to keep advancing capabilities.&#39; Except that has to be something that we collectively have the ability to actually do, or it doesn&#39;t work. If, as is the default, wee keep charging ahead anyway after we hit the wise X, then the charging ahead before X makes us worse off as well.</p><p></p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose and Quintin Pope write &#39;AI will be easy to control.</a> &#39;</p><p> The argument seems to be: Our current techniques already work to teach human values and instill common sense. Our real values are simple and will be easy to find and we humans are well-aligned to them. Our real values will then be encoded into the AIs so even if we lose control over them everything will be fine. That the opportunity to White Box (examine the components of the AI&#39;s calculation) and do things it would be illegal to do to a human makes things vastly easier when dealing with an AI, that our full control over the input mechanism makes things vastly easier.</p><p> And all of this is asserted as, essentially, obvious and undeniable, extreme confidence is displayed, all the arguments offered against this are invalid and dumb, and those that disagree are at best deeply confused and constantly told they did not understand or fairly represent what被说。</p><p> I don&#39;t even know where to begin with all that at this point. It all seems so utterly wrong to me on so many levels. <a href="https://www.lesswrong.com/posts/Wr7N9ji36EvvvrqJK/response-to-quintin-pope-s-evolution-provides-no-evidence" target="_blank" rel="noopener noreferrer nofollow">I tried one reply</a> to <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn" target="_blank" rel="noopener noreferrer nofollow">one of Pope&#39;s posts</a> when it won the OpenPhil contest – a post this post cites as evidence – and I do not believe my responding or the resulting exchange got us anywhere. I would consider a conversation worth trying, especially if it was in person somehow, but I don&#39;t see much hope for further written exchange.</p><p> So I will simply note that the arguments have been made, that I strongly disagree with the core claims other than that they do cite some marginal reasons to be more hopeful versus a world where those reasons did not hold, I believe the problems involved remain impossibly hard and our leads remain unpromising, and that I have stated my thoughts on such topics previously, including many (but presumably not all) my reasons for disagreement.</p><p> I will also note that it is far better to make actual arguments like these, even with all the disagreement and hostility and everything else that I think is wrong with it, than to engage in the typical ad hominem.</p><p> The post still puts existential risk from AI, despite all this, at ~1%. Which I will note that I do agree would be an acceptable risk, given our alternatives, if that was accurate.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731132646284267807" target="_blank" rel="noopener noreferrer nofollow">Andrew Critch has a thread</a> in which he says we have &#39;multiple ideas&#39; how to control AGI, advocates of responsible behavior will be in deep trouble if they keep saying we can&#39;t control it and then we do control it, and he seems to essentially endorse what Belrose and Pope said, although even then he says 10% chance of losing control of the first AI and 85% chance of doom overall, despite this, because he expects us to botch the execution when faced with all this new power.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731458889743495395" target="_blank" rel="noopener noreferrer nofollow">He also endorses changing the way existential risk discourse uses words to match word use elsewhere</a> , in this case the term &#39;white box.&#39;</p><p> <a href="https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose" target="_blank" rel="noopener noreferrer nofollow">There was a good response on LessWrong by Steven Byrnes</a> , with which I mostly agree.</p><p> <a href="https://www.lesswrong.com/posts/WkJDgpaPeCJDMJkoL/quick-takes-on-ai-is-easy-to-control" target="_blank" rel="noopener noreferrer nofollow">There was also a &#39;quick take&#39; from Nate</a> , which was intended to be helpful and which I did find helpful and might even lead to a good dialogue, but in context in mostly generated further animosity. Takes should in future either be far quicker, or involve a full reading and be far less quick.</p><p> <a href="https://www.youtube.com/watch?v=cd468xU1S8Y&amp;ab_channel=JonnahMoreno" target="_blank" rel="noopener noreferrer nofollow">If you actually believed for a second there</a> that everything involved would really be this easy, would that justify a number as low as 1%? If it was simply about AI being easy to control, I would say no, because we would then have to choose to send the AIs we can control on wise paths, and find an equilibrium.</p><p> Nora&#39;s claims, however, are stronger than that. She is saying that the AIs will naturally not only be fully under control, but also somehow somewhat automatically take in true human values, such that if AI somehow did get out of control, they would still work to ensure good outcomes for us. And also she seems fully confident we will have no ethical issues with all the things we would be doing to AIs that we wouldn&#39;t do to humans, including keeping them fully under our control. It is optimism all the way.</p><p> Can we get to 99% survival under ASI if we indeed answer fully optimistically at every step, even when I don&#39;t know how to logically parse the claims this requires? I think this would require at least one additional optimistic assumption Nora does not mention. But yes, if you are going to assign approximately zero risk to all these various steps, I can see how someone could get there. Where there is still risk at 1%.</p><p> Claims that risk is substantially below 1%, even given the future existence of ASI, seem to rest on some version of &#39;you need to tell me exactly how it happens step by step, and then I will multiply your various steps together.&#39; It has a baseline assumption that creating smarter, more capable entities than humans is a presumed safe exercise until shown to be specifically dangerous, that something has to &#39;go wrong&#39; for humans to not remain on top. That we will remain the special ones.</p><p> As opposed to, even if everything else goes as well as it possibly could, you have competition in which those who do not increasingly put their AIs in charge of everything and hand them over power lose such competitions, and the resulting AIs compete with each other, those that are focused (for whatever reason) on gaining resources and power and ensuring copies of themselves exist multiply and gain resources and power and change to be better at this over time, and we perish.</p><p> I hope that by now, if you are reading this, you realize that the assumption of human survival in such worlds makes no sense as a default. That perhaps we could get there, but if we do it will be via our own efforts, not something that happens on its own. That the idea that letting technology run its course without intervention works while humans are the most powerful optimizers on the planet and doing all the fine tuning and optimizing that matters, that is why it worked so far, and that once that is no longer true that will stop working for us even if we solve various problems that I think are impossibly hard (but that Belrose insists will be easy).</p><p> <a href="https://twitter.com/yonashav/status/1731443486380175772" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose even explicitly endorses that her good scenarios involve the creation of misaligned AIs</a> , smarter and more capable than humans. Which means a world with competition between super-capable entities competing with and against humans. I don&#39;t see how one can assign anything like a 99% chance of survivable outcomes to such worlds, even if a full and free &#39;alignment solution&#39; was created and made universally available today.</p><blockquote><p> Arvind Narayanan: We must prepare for a world in which unaligned models exist, either because threat actors trained them from scratch or because they modified an existing model. We must instead look to defend the attack surfaces that attackers might target using such models</p><p> Yo Shavit: Unfortunately, I&#39;ve increasingly come to the conclusion that (other than maybe at the short-term frontier) this is probably the world we&#39;re going to be in. It implies a very different set of mitigations beyond outright prevention. We need to reprioritize and get going on them.</p><p> Nora Belrose: To be honest, I don&#39;t view this as an “unfortunate” scenario but more like, “of course we were always going to have misaligned AIs, just like we have &#39;misaligned&#39; humans; trying to prevent this is hopeless and any serious attempt would have increased tyranny risk.”</p></blockquote><p> Would have &#39;increased tyranny risk&#39;? What do you think happens with misaligned superintelligences on the loose? The response at that stage will not only work out, it will also be less intrusive? We all keep our freedoms in the meaningful senses, humans stay in charge and it all works out? Are we gaming this out at all?什么？</p><p>我不明白这一点。 I flat out do not get it.</p><p> What seems hopeless is repeating the explanations over and over again. I do it partly in hopes of rhetorical innovation via iteration and exploration, partly to hope new people are somehow reached, partly because the argument doesn&#39;t stop, partly because I don&#39;t know what else to do. It is continuously getting less fun.</p><p> Recently a clip of me discussing my p(doom) was passed around Twitter, with a number of responses blaming me for not justifying my answer with a bunch of explanations and mathematical calculations. Or asking how dare I disagree with &#39;superforecasters.&#39; To which I want to scream, I know from context you know of my work, so are you saying I have not written enough words explaining my thinking?是我没说清楚吗？ Do I need to start from scratch every time someone pulls an audio clip?</p><p>叹。</p><p> Arvind Narayanan&#39;s comment above <a href="https://www.aisnakeoil.com/p/model-alignment-protects-against" target="_blank" rel="noopener noreferrer nofollow">links to his post</a> claiming that alignment such as RLHF currently is effective against accidental harm to users, but that the problem with adversarial attacks runs deep. Not are current RLHF and similar techniques unable to defend against such attacks, he says, alignment is inherently unable to do this.</p><blockquote><p> Model alignment may be useless even against much weaker adversaries, such as a scammer using it to generate websites with fraudulent content, or a terrorist group using AI for instructions on how to build a bomb. If they have even a small budget, they can pay someone to fine tune away the alignment in an open model (in fact, such de-aligned models have now been <a href="https://huggingface.co/ehartford/dolphin-llama-13b" target="_blank" rel="">publicly</a> <a href="https://huggingface.co/collections/NousResearch/hermes-650a66656fb511ba9ea86ff1" target="_blank" rel="">released</a> ). And <a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="">recent research</a> suggests that they can fine tune away the alignment even for closed models.</p></blockquote><p> Indeed this is the case for open source models and all known alignment techniques, that the fine-tune cost to eliminate all safeguards is trivial. I do not see any even theoretical proposal of how to change this unfortunate reality. If you allow unmonitored fine-tuning of a closed model, you can jailbreak those as well. I presume the solution to this will be that fine tuning of sufficiently capable closed source models will be monitored continuously to prevent this from happening, or the resulting model&#39;s weights will be kept controlled and its outputs will be monitored, or something else similar will be done, or else we won&#39;t be able to offer fine tuning.</p><p> I disagree with Arvind&#39;s assertion that existing open source models are sufficiently capable that it is already too late to prevent the existence of unaligned models. Yes, Llama-2 and similar models have their uses for bad actors, but in a highly manageable way.</p><p> Arvind&#39;s third claim is that you can use other methods, like monitoring and filtering of inputs, as a substitute for model alignment. If the model is vulnerable to particular weird strings, you can check for weird strings. At current tech levels, this seems right. Once again, this option is closed source only, but OpenAI could totally load up on such techniques if it wanted to, and for now it would raise the jailbreak bar a lot, especially after many iterations.</p><p> Longer term, as the models grow more capable, this focus on the malintent of the user or the hostile properties of inputs becomes misplaced, but for now it seems valid. Short term, as Arvind notes, you wouldn&#39;t want to do anything where you cared about someone doing a prompt injection attack or you otherwise needed full reliability, but if you can afford some mistakes you can get a lot of utility.</p><p> Steven Pinker buys Effective Altruism&#39;s cost estimates for saving lives at $5,000 straight up including not on that close a margin, <a href="https://twitter.com/weidai11/status/1732340189056672229" target="_blank" rel="noopener noreferrer nofollow">but he does not buy that smarter than human intelligences might pose an existential threat</a> worth spending money to mitigate.</p><blockquote><p> Steven Pinker: Half a billion dollars, donated to effective philanthropies like Givewell, could have saved 100,000 lives. Instead it underwrote ingenious worries like, &#39;”If an AI was tasked with eliminating cancer, what if it deduced that exterminating humanity was the most efficient way, and murdered us all?” This is not effective altruism.</p><p> Wei Dai: Me: Humanity should intensively study all approaches to the Singularity before pushing forward, to make sure we don&#39;t mess up this once in a lightcone opportunity. Ideally we&#39;d spend a significant % of GWP on this. Others: Even $50 million per year is too much.</p></blockquote><p> And thus, if the movement splits its money between doing the thing you say saves lives vastly more efficiently than other charities, and this other thing you dismiss as stupid? Then you blame them for not spending only on the thing you approve.</p><p> You know who Steven Pinker sounds exactly like here? The Congressional Republicans who give a speech each year on how we should cut science funding because there was some studies on things like migratory patterns of birds that they thought was stupid. Except instead of public funding for things many people would indeed largely not want to fund, this is completely voluntary and private funding.</p><h4> Somehow This Is The Actual Vice President</h4><p> <a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/11/29/remarks-by-vice-president-harris-in-a-moderated-conversation-with-andrew-ross-sorkin-at-the-new-york-times-12th-annual-dealbook-summit-new-york-ny/" target="_blank" rel="noopener noreferrer nofollow">In what was quite the mind-numbing conversation throughout</a> , here is the section that was about AI.</p><p> First, we have the boiler plate, included for completeness but you can skip.</p><blockquote><p> R. SORKIN:  Okay, let me ask a different question.  AI —  and I — I know we don&#39;t have a lot of time.  Sam Altman has been talking a lot about the need for regulation.  You&#39;ve talked about the need for regulation.THE VICE PRESIDENT:  Yeah.MR. SORKIN:  Washington has not been able to get its arms even around social media. THE VICE PRESIDENT:  Yeah.MR. SORKIN:  How do you imagine Washington could?  And what — if you had to regulate AI, how would you do it?THE VICE PRESIDENT:  Right.  So, I actually am back a few weeks now from London, the UK  Rishi Sunak invited a number of us to talk about safety and AI.  And I presented, basically, our vision, the vision that we have for the future of AI in the context of safety. And I would offer a number of points: One, I think it is im- — is critically important that we, as the United States, be a leader on this, including how we perceive and then interpret what should be the international rules and norms on a variety of levels, including what would be in the best interest —</p><p>先生。 SORKIN:  Right.THE VICE PRESIDENT:  — of our national security.</p></blockquote><p> Then comes the dumbest timeline department. The first paragraph is infuriating, although I suppose only about as infuriating as others find Biden when he responds to Mission Impossible: Dead Reckoning, two sides of the same coin.</p><p> But then comes the idea of &#39;existential to whom?&#39; and there are so many levels in which this person simply does not get it.</p><blockquote><p> VP: I do believe also that we should evaluate risk. There is a lot of discussion on AI that is about existential risks, and those are real, but one should also ask: Existential to whom? So, we have an image of the Terminator and Arnold Schwarzenegger and the machine and — right? — machine versus man. And many would argue that that is something that we should take seriously as a possibility. It is not a current threat.We should also, in thinking about [AI] policy, think about the current threats. And in that way, I present it as existential to whom when we ask about existential threats. For example, if we are talking about a senior and — seniors, I&#39;ve done a lot of work in terms of abuse of seniors. They have lived a life of productivity. They are sitting on assets. They are vulnerable to predators and scams. And the use of technology and AI is one of those that is currently happening where you&#39;ve heard the stories — you may know the stories; you may have family members — who the audio sounds like their grandson, “I&#39;m in distress; I need help,” and they start giving away their life&#39;s savings.Existential to who? Existential to that senior.就是这样的感觉。 Existential to who?</p></blockquote><p> Eliezer has a response, which I will put directly here.</p><div><div></div></div><p> The fact that mundane harms can &#39;feel existential&#39; to people anyway is perhaps confusing her. She has in mind, as the good Senator Blumenthal put it, the effect on jobs. Except no.严重地。 If you are going to be evoking Terminator then you might or might not be confused in a different highly understandable way, or you might only be trying to make people you dislike sound foolish through metaphor, but you know damn well the whom in &#39;existential to谁。&#39;</p><p> And you know damn well, madame Vice President, exactly what &#39;existential&#39; means here. It does not mean evoking Continental philosophy. It does not mean how anyone feels. It means death.</p><p> Anyway, she goes on and does it again.</p><blockquote><p> How about the father who is driving and then is the subject of facial recognition that is flawed, ends up in jail?  Well, that&#39;s existential to his family.  Existential to who?</p></blockquote><p>我的意思是，认真的吗？ What the actual f***? Let&#39;s go over this again.</p><div><div></div></div><p> Anyway, full remarks, so she goes back to boilerplate again. The whole &#39;intentional to not stifle innovation&#39; argument, and, well, I don&#39;t mean to laugh but have you met the entire Biden administration? To be clear, the answer could be no.</p><blockquote><p> So, the spec- — the full spectrum of risks must also be evaluated as we are establishing public policy.My final point is: Public policies should be intentional to not stifle innovation.  And I say this as the former Attorney General of California.  I ran the second-largest Department of Justice in the United States, second only to the United States Department of Justice, and created one of the first Privacy and Protection Units of any Department of Justice.  Back in 2010, I was elected.I know that there is a balance that can and must be struck between what we must do in terms of oversight and regulation and being intentional to not stifle innovation.  I will also agree with you, as a devout public servant, government has historically been too slow to address these issues.  AI is rapidly expanding.MR. SORKIN:  Right.THE VICE PRESIDENT:  And we have to, then, take seriously our ability to have the resources and the skillset to do this in a smart way that strikes the right balance and doesn&#39;t accept false choices.</p></blockquote><p> In my experience, don&#39;t accept false choices is sometimes important, but mostly is what people say when they want to promise incompatible things, that their approach will magically do everything good and nothing bad, have everyone assume it will somehow work out and get promoted or move on before it blows up in their face.</p><p> Yes, this is the person the White House put in charge of many of its AI efforts, although that was before Dead Reckoning, and is also the person those who want reasonable AI policy are going to have to hope wins the next election, given Trump has already stated his intention to revoke the executive order on AI.</p><h4>轻松的一面</h4><p><a href="https://twitter.com/colin_fraser/status/1730378892819746818" target="_blank" rel="noopener noreferrer nofollow">规则已经改变。</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba2bbc8-169b-49e0-9b08-1de139bd532d_968x1212.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/rt1rriwzscnix3tgtn5i" alt="图像"></div></figure></div><p> <a href="https://twitter.com/daniel_271828/status/1731504080651264509" target="_blank" rel="noopener noreferrer nofollow">The rules have stayed the same</a> . </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cee73d7-b83a-46d7-a482-d14ed3348341_886x526.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/s1bkcgzkoucvq580mmzc" alt=""></div></figure></div><p> <a href="https://twitter.com/Grimezsz/status/1731891683103739955" target="_blank" rel="noopener noreferrer nofollow">All I&#39;m saying is, we were definitely warned.</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23c8465-2ae5-4455-a308-aa020ec12a2f_480x402.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/nzl3xptslsvun59b5qrx" alt=""></div></figure></div><p> <a href="https://twitter.com/yonashav/status/1731891018314678624" target="_blank" rel="noopener noreferrer nofollow">Not that you understood</a> .</p><blockquote><p> You Shavit: One interesting realization from moving inside OpenAI is that a lot of the time, we have no idea what Roon is talking about either.</p></blockquote><p> Nor did she: <a href="https://www.youtube.com/watch?v=TL5X2bPvzHo&amp;ab_channel=EliezerYudkowsky" target="_blank" rel="noopener noreferrer nofollow">A reply to Kamala Harris on existential risk</a> . She asks, existential to whom? There is a type of person, which she is, who can only think in such terms.</p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini<guid ispermalink="false"> 9Jgtkw8CD6kndyCcD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:10:07 GMT</pubDate> </item><item><title><![CDATA[Simplicity arguments for scheming (Section 4.3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 7, 2023 3:05 PM GMT<br/><br/><p> This is Section 4.3 of my report “ <a href="https://arxiv.org/pdf/2311.08379.pdf">Scheming AIs: Will AIs fake alignment during training in order to get power?</a> ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://www.buzzsprout.com/2034731/13984933">请点击这里</a>，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1> Simplicity arguments</h1><p> The strict counting argument I&#39;ve described is sometimes presented in the context of arguments for expecting schemers that focus on &quot;simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-1" id="fnref-DNdPEGz4DYMju8reA-1">[1]</a></sup> Let&#39;s turn to those arguments now.</p><h2> What is &quot;simplicity&quot;?</h2><p> What do I mean by &quot;simplicity,&quot; here? In my opinion, discussions of this topic are often problematically vague – both with respect to the notion of simplicity at stake, and with respect to the sense in which SGD is understood as selecting for simplicity.</p><p> The notion that Hubinger uses, though, is the length of the code required to write down the algorithm that a model&#39;s weights implement. That is: faced with a big, messy neural net that is doing X (for example, performing some kind of <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">induction</a> ), we imagine re-writing X in a programming language like python, and we ask how long the relevant program would have to是。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-2" id="fnref-DNdPEGz4DYMju8reA-2">[2]</a></sup> Let&#39;s call this &quot;re-writing simplicity.&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-3" id="fnref-DNdPEGz4DYMju8reA-3">[3]</a></sup></p><p> Hubinger&#39;s notion of simplicity, here, is closely related to measures of algorithmic complexity like &quot; <a href="http://www.scholarpedia.org/article/Algorithmic_complexity#Kolmogorov_complexity">Kolmogorov complexity</a> ,&quot; which measure the complexity of a string by reference to the length of the shortest program that outputs that string when fed into a chosen <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">Universal Turing Machine</a> (UTM). One obvious issue here is that this sort of definition is relative to the choice of UTM (just as, eg, when we imagine re-writing a neural net&#39;s algorithm using other code, we need to pick the programming language). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-4" id="fnref-DNdPEGz4DYMju8reA-4">[4]</a></sup> Discussions of algorithmic complexity often ignore this issue on the grounds that it only adds a constant (since any given UTM can mimic any other if fed the right prefix), but it&#39;s not clear to me, at least, when such constants might or might not matter to a given analysis – for example, the analysis at stake here. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-5" id="fnref-DNdPEGz4DYMju8reA-5">[5]</a></sup></p><p> Indeed, my vague sense is that certain discussions of simplicity in the context of computer science often implicitly assume what I&#39;ve called &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; – a view on which simplicity in some deep sense an objective <em>thing</em> , ultimately independent of eg your choice of programming language or UTM, but which different metrics of simplicity are all tracking (albeit, imperfectly). And perhaps this view has merit (for example, my impression is that different metrics of complexity often reach similar conclusions in many cases – though this could have many explanations). However, I don&#39;t, personally, want to assume it. And especially absent some objective sense of simplicity, it becomes more important to say which particular sense you have in mind.</p><p> Another possible notion of simplicity, here, is hazier – but also, to my mind, less theoretically laden. On this notion, the simplicity of an algorithm implemented by a neural network is defined relative to something like the number of parameters the neural network uses to encode the relevant algorithm. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-6" id="fnref-DNdPEGz4DYMju8reA-6">[6]</a></sup> That is, instead of imagining <em>re-writing</em> the neural network&#39;s algorithm in some other programming language, we focus directly on the parameters the neural network itself is recruiting to do the job, where simpler programs use fewer parameters. Let&#39;s call this &quot;parameter simplicity.&quot; Exactly how you would measure &quot;parameter simplicity&quot; is a different question, but it has the advantage of removing one layer of theoretical machinery and arbitrariness (eg, the step of re-writing the algorithm in an arbitrary-seeming programming language), and connecting more directly with a &quot;resource&quot; that we know SGD has to deal with (eg, the parameters the model makes available). For this reason, I&#39;ll often focus on &quot;parameter simplicity&quot; below.</p><p> I&#39;ll also flag a way of talking about &quot;simplicity&quot; that I won&#39;t emphasize, and which I think muddies the waters here considerably: namely, equating simplicity fairly directly with &quot;higher prior probability.&quot; Thus, for example, faced with an initial probability distribution over possibilities, it&#39;s possible to talk about &quot;simpler hypotheses&quot; as just: the ones that have greater initial probability, and which therefore require less evidence to establish. For example: faced with a thousand people in a town, all equally likely to be the murderer, it&#39;s possible to think of &quot;the murderer is a man&quot; as a &quot;simpler&quot; hypothesis than &quot;the murderer is a man with brown hair and a dog,&quot; in virtue of the fact that the former hypothesis has, say, a 50% prior, and so requires only one &quot;bit&quot; of evidence to establish (ie, one halving of the probability space), whereas the latter hypothesis has a much smaller prior, and so requires more bits. Let&#39;s call this &quot;trivial simplicity.&quot;</p><p> &quot;Trivial simplicity&quot; is related to, but distinct from, the use of simplicity at stake in &quot; <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam&#39;s razor</a> .&quot; Occam&#39;s razor is (roughly) the <em>substantive</em> claim that <em>given an independent notion of simplicity</em> , simpler hypotheses are more likely on priors. Whereas trivial simplicity would imply that simpler hypotheses are <em>by definition</em> more likely on priors. If you take Occam&#39;s razor sufficiently for granted, it&#39;s easy to conflate the two – but the former is interesting, and the latter is some combination of trivial and misleading. And regardless, our interest here isn&#39;t in the simplicity of <em>hypotheses</em> like &quot;SGD selects a schemer,&quot; but in the simplicity of the <em>algorithm</em> that the model SGD selects implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-7" id="fnref-DNdPEGz4DYMju8reA-7">[7]</a></sup></p><h2> Does SGD select for simplicity?</h2><p> Does SGD select for simplicity in one of the non-trivial senses I just described?</p><p> One reason you might think this comes from the &quot;contributors to reward&quot; frame. That is: using a more parameter-simple algorithm will free up other parameters to be put to other purposes, so it seems very plausible that parameter simplicity will increase a model&#39;s reward. And to the extent that re-writing simplicity correlates with parameter simplicity, the same will hold for re-writing simplicity as well. This is the story about why simplicity matters that I find most compelling.</p><p> However, I think there may also be more to say. For example, I think it&#39;s possible that there&#39;s other empirical evidence that SGD selects for simpler functions, other things equal (for example, that it would much sooner connect a line-like set of dots with a straight line than with an extremely complicated curve) ; and perhaps, that this behavior is part of what explains its success (for example, because real-world functions tend to be simple in this sense, à la Occam&#39;s razor). For example, in the context of an understanding of SGD as an approximation of Bayesian sampling (per the discussion of <a href="https://arxiv.org/abs/2006.15191">Mingard et al (2020)</a> above), <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> discusses empirical evidence that the <em>prior</em> probability distribution over parameters (eg, what I called the &quot;initialization distribution&quot; above) puts higher probability mass on simpler functions. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-8" id="fnref-DNdPEGz4DYMju8reA-8">[8]</a></sup> And he connects this with a theoretical result in computer science called the &quot;Levin bound,&quot; which predicts this (for details in footnote). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-9" id="fnref-DNdPEGz4DYMju8reA-9">[9]</a></sup></p><p> I haven&#39;t investigated this in any depth. If accurate, though, this sort of result would give simplicity relevance from an &quot;extra criteria&quot; frame as well. That is, on this framework, SGD biases towards simplicity even before we start optimizing for reward.</p><p> Let&#39;s suppose, then, that SGD selects for some non-trivial sort of simplicity. Would this sort of selection bias in favor of schemers?</p><h2> The simplicity advantages of schemer-like goals</h2><p> Above I mentioned that the counting argument is sometimes offered as a reason to expect a bias towards schemers on these grounds. Note, though, that the counting argument (at least as I&#39;ve presented it) doesn&#39;t make any obvious reference to a bias towards simplicity per se. And I think we should be careful not to conflate the (trivial) simplicity of the <em>hypothesis</em> that &quot;SGD selects a schemer,&quot; <em>given a prior probability distribution that puts most of the probability on schemers</em> (eg, a uniform distribution over individual models-that-get-high-reward), with the claim that the <em>algorithm</em> that a given individual schemer implements is (substantively) simpler than the algorithm that a given non-schemer implements. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-10" id="fnref-DNdPEGz4DYMju8reA-10">[10]</a></sup> Indeed, my own sense is that the strongest form of the counting argument leaves it to stand on its own intuitive terms, rather than attempting to connect it to further questions about SGD&#39;s biases towards simplicity in particular.</p><p> That said, it is possible to draw connections of this form. In particular: we can say that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers do not. And this seems especially plausible to the extent we imagine that the goals required to be a non-schemer are quite complex (more on this below). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-11" id="fnref-DNdPEGz4DYMju8reA-11">[11]</a></sup></p><p> One interesting feature of this sort of argument is that it imagines, specifically, that the simplicity differences between models are coming entirely from the content of their <em>goals</em> . Indeed, the toy analysis in Hubinger (2022) specifically imagines that the respective model classes all have the same world model and optimization procedure, and that the complexity of their algorithm overall can be approximated by <em>complexity of world model + complexity of the optimization procedure + complexity of the goal.</em> And the &quot;goal slot&quot; is the only part that differs between models.</p><p> It&#39;s not clear that this is right, though, especially if we assume that the goal-directedness at stake is &quot;messy&quot; rather than &quot;clean.&quot; For example, to the extent that schemers have to perform types of instrumental reasoning that non-schemers <em>don&#39;t</em> (eg, reasoning about the instrumental value of getting reward, reasoning about when to defect, etc), it seems plausible that this could introduce additional complexity into the algorithm itself (rather than eg merely requiring that the algorithm &quot;run for a longer time,&quot; à la the &quot;speed&quot; analysis below). For example, to the extent we&#39;re using &quot;parameter simplicity&quot; as our notion of simplicity, we could imagine cases where this sort of instrumental reasoning requires additional parameters. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-12" id="fnref-DNdPEGz4DYMju8reA-12">[12]</a></sup></p><h2> How big are these simplicity advantages?</h2><p> For now, though, let&#39;s stick with Hubinger&#39;s ontology, and with simplicity differences rooted specifically in differences between goals. How big of an advantage does selecting a schemer afford in this respect?</p><p> One way of running this analysis is to compare the goals had by the simplest possible model within each class (either: because you expect SGD to select for the simplest possible model, or you think this is a good way of approximating the simplicity benefits at stake ）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-13" id="fnref-DNdPEGz4DYMju8reA-13">[13]</a></sup> That is, we compare the complexity of:</p><ol><li><p> The specified goal (for training-saints)</p></li><li><p> The simplest possible misgeneralized goal that gets max-reward on the training data absent training-gaming (for mis-generalized non-training-gamers)</p></li><li><p> The simplest possible way of seeking reward-on-the-episode (for reward-on-the-episode seekers)</p></li><li><p> The simplest possible goal-that-motivates-scheming (for schemers)</p></li></ol><p> Clearly, the complexity of the specified goal matters in this context. My sense is that the discourse on scheming often assumes that the specified goal is quite complex in some sense – and in particular, the sense at stake in the idea that &quot; <a href="https://www.lesswrong.com/tag/complexity-of-value">human values are complicated</a> .&quot; <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-14" id="fnref-DNdPEGz4DYMju8reA-14">[14]</a></sup> And perhaps, if we&#39;re imagining that the only way to get <em>alignment</em> is to first (a) somehow specify &quot;human values&quot; via the training objective, and then (b) somehow ensure that we get a training saint, then focusing on something in the vicinity of &quot;act in accordance with human values&quot; as the specified goal is appropriate. But note that for the purposes of comparing the probability of <em>scheming</em> to the probability of <em>other forms of misalignment</em> , we need not assume such a focus. And thus, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; Indeed, in <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">other work</a> , Hubinger (writing with others) suggests that a goal like &quot;minimize next-token prediction error&quot; is quite simple – and indeed, that &quot;its complexity is competitive with the simplest possible long-term goals&quot; (this is part of what makes Hubinger comparatively optimistic about avoiding scheming during LLM pre-training – though personally, I feel confused about why Hubinger thinks &quot;next token prediction error&quot; importantly simpler than &quot;reward-on-the-episode&quot;).</p><p> Suppose, though, that the specified goal has complexity on the order of &quot;act in accordance with human values&quot; or &quot;follow instructions in a helpful, harmless, and honest (HHH) manner.&quot; Where does this leave a comparison between (1)-(4) above?</p><p> At least from a parameter-simplicity perspective, one way of approaching this is to think about what we know about the absolute cost in parameters for representing different human concepts within neural networks. I won&#39;t dig in much, here, but one piece of data that seems relevant is: LLMs like GPT-4 seem capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on. And this isn&#39;t yet to mention all the parameters devoted to everything else that GPT-4 does. So the absolute number of parameters required to represent concepts like these can&#39;t be crazily high. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-15" id="fnref-DNdPEGz4DYMju8reA-15">[15]</a></sup> And this caps the overall complexity savings at stake in the differences between the models above. For example, if we assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then the maximum number of parameters required to represent the concept of &quot;honesty&quot; is around a million (and this estimate strikes me as quite conservative). So if the simplest possible schemer-like goal takes only 1 parameter to represent, then not representing honesty in the schemer&#39;s &quot;goal slot,&quot; saves, at most, 999,999 parameters – roughly one millionth of a trillion parameter model&#39;s representational capacity, and even less for the larger models of the future.</p><p> More importantly, though: no one thinks that a schemer won&#39;t have to represent concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on at all. As <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">Hubinger et al (2023)</a> note, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process, etc (and especially: models that are &quot;playing a training game&quot; in which such concepts play a central role). So really, the relevant question is: what are the <em>extra</em> complexity costs of representing a goal like &quot;get reward-on-the-episode&quot; or &quot;follow instructions in an HHH way&quot; (relative to the simplest possible schemer-like goal), <em>once you&#39;ve already paid the costs of having a concept of those goal targets</em> .</p><p> I&#39;m not sure exactly how to think about this, but it seems very plausible to me that the costs here are extremely small. In particular: it seems like SGD may be able to significantly repurpose the parameters used to represent the concept in the world model in causing that concept to guide the model&#39;s behavior in a goal-like manner. Thus, as an analogy, perhaps the concept of &quot;pleasure&quot; is in some sense &quot;simpler&quot; than the concept of &quot; <a href="https://en.wikipedia.org/wiki/Wabi-sabi">wabi-sabi</a> &quot; in Japanese aesthetics (ie, &quot;appreciating beauty that is &#39;imperfect, impermanent, and incomplete&#39; &quot;) 。 Once you&#39;ve <em>learned</em> both, though, does pursuing the former require meaningfully more parameters than pursuing the latter? <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-16" id="fnref-DNdPEGz4DYMju8reA-16">[16]</a></sup></p><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger (2022)</a> discussion of issues like this sometimes appeals to the notion of a &quot;pointer&quot; to some part of the world model. As I understand it, the idea here is that if you&#39;ve already got a concept of something like &quot;pleasure&quot;/&quot;wabi-sabi&quot;/&quot;reward&quot; in your world model, you can cause a model to pursue that thing by giving it a goal slot that says something like &quot;go for <em>that</em> &quot; or &quot; <em>that</em> is good,&quot; where &quot;that&quot; points to the thing in question (this is in contrast with having to represent the relevant concept <em>again,</em> fully and redundantly, in the goal slot itself). But insofar as we use a toy model like this (I doubt we should lean on it), why think that it&#39;s significantly more complex to <em>point</em> at a more complex concept than at a simpler one? Eg, even granted that &quot;wabi-sabi&quot; takes more parameters than &quot;pleasure&quot; to represent in the world model, why think that encoding the <em>pointer</em> to &quot;pleasure&quot; (eg, &quot;go for <em>that&quot;</em> ) takes more parameters than encoding the <em>pointer</em> to &quot;wabi-sabi&quot; (eg, again, &quot;go for <em>that</em> &quot;)?</p><p> One option, here, is to say that the complexity of the concept and the complexity of the pointer are correlated. For example, you might imagine that the model has some kind of &quot;internal database&quot; of concepts, which stores concepts in a manner such that concepts that take fewer parameters to store take fewer parameters to &quot;look up&quot; as well. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-17" id="fnref-DNdPEGz4DYMju8reA-17">[17]</a></sup> On this picture, &quot;pleasure&quot; might end up stored as the 15th concept in the database <em>because</em> it takes eg 23 parameters to represent, whereas &quot;wabi-sabi&quot; might end up stored as the 125355th concept because it takes 10,000 parameters to代表。 And then the &quot;pointer&quot; to pleasure can say &quot;go for the thing stored at location 15,&quot; whereas the &quot;pointer&quot; to &quot;wabi-sabi&quot; has to say &quot;go for the thing stored at location 125355,&quot; which takes a few more bits to specify. But even at an abstract-toy-illustrative-example level, this sort of story requires leaning on a specific model of how the model&#39;s pointer and concept-storage processes work – and it still needs to explain <em>why</em> simplicity-to-represent and simplicity-to-point-at are correlated in the relevant sense.</p><p> Alternatively, though, we can abandon any interest in the complexity of storing a concept in the world model, and focus directly on the complexity of pointing to it. Still, insofar as there are meaningful <em>differences</em> between the complexity of pointing at one concept vs. another, we may be able to re-run the argument that schemers offer simplicity advantages. In particular: selecting a schemer allows SGD to have its pick from whatever schemer-motivating goals are simplest to <em>point at in the world model</em> ; whereas the other model classes plausibly impose more substantive constraints. Ie, if the specified goal ends up stored at location 12634 in the model&#39;s metaphorical database, and if &quot;reward-on-the-episode&quot; is at location 35364, then if there are any schemer-like goals at eg locations 1-100, it&#39;ll be simpler to point at one of <em>those</em> instead – and thus, to create a schemer rather than a training-saint or a reward-on-the-episode seeker.</p><p> To the extent we focus on the final properties of the different model classes, I think this is probably the best way to run a simplicity-focused argument for scheming – especially if we don&#39;t get too hung up on the toy ontology of &quot;pointers&quot; (and still less, &quot;databases&quot;) in particular. That is, roughly: even granted that all of the goals on the table here (eg the specified goal, reward-on-the-episode, etc) will need to be represented by the model somewhere, it may be that, for whatever reason, some of these concepts take fewer parameters to <em>repurpose for goal-pursuit</em> than others. And insofar as scheming makes more potential concepts available for such repurposing, it offers a simplicity advantage in that sense.</p><p> Quantitatively, though, how big is this advantage? It&#39;s now especially hard to say. In particular, once we&#39;ve shifted our focus from &quot;simplicity-to-represent&quot; to &quot;simplicity-to-repurpose-for-goal-pursuit,&quot; we can&#39;t even look at our intuitive sense of the complexity of concepts like &quot;reward,&quot; &quot;honesty,&quot; and &quot;next token prediction error,&quot; because we&#39;re no longer talking about the complexity of the concepts per se. Rather, we&#39;re speculating about the complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory.</p><p> Still, to the extent we can estimate the size of these differences, it seems plausible to me that they are very small indeed. One intuition pump for me here runs as follows. Suppose that the model has 2^50 concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-18" id="fnref-DNdPEGz4DYMju8reA-18">[18]</a></sup> The average number of bits required to code for each of 2^50 concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max-reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I won&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so).</p><h2> Does this sort of simplicity-focused argument make plausible predictions about the sort of goals schemers would end up with?</h2><p> One other consideration that seems worth tracking, in the context of simplicity arguments for scheming, is the predictions they are making about the sort of goals a schemer will end up with. In particular, if you think (1) that SGD selects very hard for simpler goals, (2) that this sort of selection favors schemer-like goals because they can be simpler, and (3) that our predictions about what SGD selects can ignore the &quot;path&quot; it takes to create the model in question, then at least naively, it seems like you should expect SGD to select a schemer with an extremely simple long-term goal (perhaps: the simplest possible long-term goal), <em>regardless of whether that goal had any relation to what was salient or important during training</em> . Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-19" id="fnref-DNdPEGz4DYMju8reA-19">[19]</a></sup> these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-20" id="fnref-DNdPEGz4DYMju8reA-20">[20]</a></sup></p><p> Personally, I feel skeptical of predictions like this (though this skepticism may be partly rooted in skepticism about ignoring the path SGD takes through model space more generally). And common stories about schemers tend to focus on proxy goals with a closer connection to the training process overall (eg, a model trained to on gold-coin-getting ends up valuing eg &quot;get gold stuff over all time&quot; or &quot;follow my curiosity over all time,&quot; and not &quot;maximize hydrogen over all time&quot;).</p><p> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue, perhaps they will either be more important (and thus simpler?) to represent in the world model, or simpler (for some reason) for the model to repurpose-for-goal-pursuit once represented. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-21" id="fnref-DNdPEGz4DYMju8reA-21">[21]</a></sup> But if we grant some story in this vein, we should also be tracking its relevance to the simplicity of pursuing <em>non-schemer goals</em> as well. In particular: to the extent we&#39;re positing that salience/relevance during training correlates with simplicity in the relevant sense, this is points in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. (Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall.)</p><p> And note, too, that to the extent SGD selects very hard for simpler goals (for example, in the context of a form of &quot;low path dependence&quot; that leads to strong convergence on a single optimal sort of model), this seems somewhat at odds with strong forms of the goal-guarding hypothesis, on which training-gaming causes your goals to &quot;crystallize.&quot; For example, if a would-be-schemer starts out with a not-optimally-simple goal that still motivates long-term power-seeking, then if it knows that in fact, SGD will continue to grind down its goal into something simpler even after it starts training-gaming, then it may not have an incentive to start training-gaming in the first place – and its goals won&#39;t survive the process regardless. <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-22" id="fnref-DNdPEGz4DYMju8reA-22">[22]</a></sup></p><h2> Overall assessment of simplicity arguments</h2><p> Overall, I do think that other things equal, schemers can have probably simpler goals than these other model classes. However, I think the relevant simplicity differences may be quite small, especially once we condition on the model having a good world model more generally (and moreso, if we posit that goals targets salient/relevant-during-training get extra simplicity points). And I&#39;m suspicious of some of the theoretical baggage it can feel like certain kinds of simplicity arguments wheel in (for example, baggage related to the notion of simplicity at stake, whether SGD selects for it, how to think about simplicity in the context of repurposing-for-goal-pursuit as opposed to merely representing, and so on). </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-DNdPEGz4DYMju8reA-1" class="footnote-item"><p> See eg <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_low_path_dependence_world">Hubinger (2022)</a> . <a href="#fnref-DNdPEGz4DYMju8reA-1" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-2" class="footnote-item"><p> See also <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive">this (now anonymous) discussion</a> for another example of this usage of &quot;simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-2" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-3" class="footnote-item"><p> Here, my sense is that the assumption is generally that X can be described at a level of computational abstraction such that the &quot;re-writing&quot; at stake doesn&#39;t merely reproduce the network itself. Eg, the network is understood as implementing some more abstract function. I think it&#39;s an interesting question how well simplicity arguments would survive relaxing this sort of assumption. <a href="#fnref-DNdPEGz4DYMju8reA-3" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-4" class="footnote-item"><p> Another issue is that <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">Kolmogorov complexity is uncomputable</a> . I&#39;m told you can approximate it, but I&#39;m not sure how this gets around the issue that for a given program where you&#39;re not able to tell whether or not it halts, that program might be the shortest program outputting the relevant细绳。 <a href="#fnref-DNdPEGz4DYMju8reA-4" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-5" class="footnote-item"><p> See <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#iv-can-you-ignore-being-only-finitely-wrong">Carlsmith (2021)</a> , sections III and IV, for more on this. <a href="#fnref-DNdPEGz4DYMju8reA-5" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-6" class="footnote-item"><p> Hubinger sometimes appears to be appealing to this notion as well – or at least, not drawing clear distinctions between &quot;re-writing simplicity&quot; and &quot;parameter simplicity.&quot; <a href="#fnref-DNdPEGz4DYMju8reA-6" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-7" class="footnote-item"><p> &quot;Trivial simplicity&quot; is also closely related to what we might call &quot;selection simplicity.&quot; Here, again, one assumes some space/distribution over possible things (eg, goals), and then talks about the &quot;simplicity&quot; of some portion of that space in terms of how much &quot;work&quot; one needs to do (perhaps: on average) in order to narrow down from the whole space to that portion of the space (see also <a href="https://colah.github.io/posts/2015-09-Visual-Information/">variable-length codes</a> ). Thus, for a box of gas, &quot;the molecules are roughly evenly spread out&quot; might be a &quot;simpler&quot; arrangement than &quot;the molecules are all in a particular corner,&quot; because it typically takes more &quot;work&quot; (in this example: thermodynamic work) to cause the former than the latter (this is closely related to the fact that the former is initially more likely than the latter). My sense is that when some people say that &quot;schemer-like goals are simple,&quot; they mean something more like: the <em>set</em> of schemer-like goals typically takes less &quot;work,&quot; on SGD&#39;s part, to land within than the <em>set</em> of non-schemer-like goals (and not necessarily: that any <em>particular</em> schemer-like goals is simpler than some <em>particular</em> non-schemer-like goal). To the extent that the set of schemer-like goals are supposed to have this property because they are more &quot;common,&quot; and hence &quot;nearer&quot; to SDG&#39;s starting point, this way of talking about the simplicity benefits of scheming amounts to a restatement of something like the counting argument and/or the &quot;nearest max-reward goal argument&quot; – except, with more of a propensity, in my view, to confuse the simplicity of <em>set</em> of schemer-like goals with the simplicity of a <em>given</em> schemer-like目标。 <a href="#fnref-DNdPEGz4DYMju8reA-7" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-8" class="footnote-item"><p> Where, importantly, multiple different settings of parameters can implement the same function. <a href="#fnref-DNdPEGz4DYMju8reA-8" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-9" class="footnote-item"><p> My understanding is that the Levin bound says something like: for a given distribution over parameters, the probability <em>p(f)</em> of randomly sampling a set of parameters that implements a function <em>f</em> is bounded by <em>2^{-K(f) + O(1)}</em> , where <em>K</em> is the <em>k</em> -complexity of the function <em>f</em> , and <em>O(1)</em> is some constant independent of the function itself (though: dependent on the parameter space). That is, the prior on some function decreases exponentially as the function&#39;s complexity increases.</p><p> I haven&#39;t investigated this result, but one summary I saw ( <a href="https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of?commentId=kLz9mgNv8xFNrAPt2">here</a> ) made it seem fairly vacuous. In particular, the idea in that summary was that larger volumes of parameter space will have simpler encodings, because you can encode them by first specifying distribution over parameters, and then using a <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman code</a> to talk about how to find them given that distribution. But this makes the result seem pretty trivial: it&#39;s not that there is some antecedent notion of simplicity, which we then discover to be higher-probability according to the initialization distribution. Rather, to be higher probability according to the initialization distribution just <em>is</em> to be simpler, because equipped with the initialization distribution, it&#39;s easier to encode the higher probability parts of it. Or put another way: it seems like this result applies to any distribution over parameters. So it doesn&#39;t seem like we learn much about any particular distribution from it.</p><p> (To me it feels like there are analogies here to the way in which &quot;shorter programs get more probability,&quot; in the context of algorithmic &quot;simplicity priors&quot; that focus on metrics like K-complexity, actually applies necessarily to <em>any</em> distribution over a countably-infinite set of programs – see discussion <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">here</a> . You might&#39;ve thought it was an interesting and substantive constraint, but actually it turns out to be more vacuous.)</p><p> That said, the empirical results I mention above focus on more practical, real-world measures of simplicity, like <a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv_complexity">LZ complexity</a> , and apparently they find that, indeed, simpler functions get higher prior probability (see eg <a href="https://arxiv.org/pdf/1805.08522.pdf">this experiment</a> , which uses a fully connected neural net to model possible functions from many binary inputs to a single binary input). This seems to me more substantive and interesting. And <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> claims that Levin&#39;s result is non-trivial, though I don&#39;t yet understand how. <a href="#fnref-DNdPEGz4DYMju8reA-9" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-10" class="footnote-item"><p> Thus, for example, you might think that insofar a randomly initialized model is more likely to end up &quot;closer&quot; to a schemer, such that SGD needs to do &quot;less work&quot; in order to select a schemer rather than some other model, this favors schemers (thanks to Paul Christiano for discussion). But this sort of argument rests on putting a higher prior probability on schemers, which, in my book, isn&#39;t a (non-trivial) simplicity argument per se. <a href="#fnref-DNdPEGz4DYMju8reA-10" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-11" class="footnote-item"><p> There are also more speculative and theoretical arguments for a connection between simplicity and schemers, on which one argues that if you do an unbounded search over all possible programs to find the shortest one that gives a given output, without regard to other factors like how long they have to run, then you&#39;ll select for a schemer (for example, via a route like: simulating an extremely simple physics that eventually gives rise to agents that understand the situation and want to break out of the simulation, and give the relevant output as part of a plan to do so). My understanding is that people (eg <a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive#Priors_on_Learned_Optimizers">here</a> ) sometimes take the discourse about the &quot; <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">malignity of the Solomonoff prior</a> &quot; as relevant here (though at a glance, it seems to me like there are important differences – for example, in the type of causality at stake, and in the question of whether the relevant schemer might be simulating <em>you</em> ). Regardless, I&#39;m skeptical that these unbounded theoretical arguments should be getting much if any weight, and I won&#39;t treat them here. <a href="#fnref-DNdPEGz4DYMju8reA-11" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-12" class="footnote-item"><p> What&#39;s more, note that, to the extent we imagine SGD biasing towards simplicity <em>because</em> real world patterns tend to be simple (eg, Occam&#39;s razor is indeed a good prior, and SGD works well in part because it reflects this prior), the explanation for this bias doesn&#39;t apply as readily to a model&#39;s <em>goals</em> . That is (modulo various forms of moral realism), there are no &quot;true goals,&quot; modeling of which might benefit from a simplicity prior. Rather, on this story, SGD would need to be acting more like a human moral anti-realist who prefers a simpler morality other-things-equal, despite not believing that there is any objective fact of the matter, because, in contexts where there <em>is</em> a fact of the matter, simpler theories tend to be more likely. <a href="#fnref-DNdPEGz4DYMju8reA-12" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-13" class="footnote-item"><p> Hubinger uses this approach. My understanding is that he&#39;s imagining SGD selecting a model with probability proportionate to its simplicity, such that eg focusing on the simplest possible model is one way of approximating the overall probability in a model class, and focusing on the <em>number</em> of models in the class is其他。 However, I won&#39;t take for granted the assumption that SGD selects a model with probability proportionate to its simplicity. <a href="#fnref-DNdPEGz4DYMju8reA-13" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-14" class="footnote-item"><p> See eg Hubinger et al (2023) <a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">here</a> . <a href="#fnref-DNdPEGz4DYMju8reA-14" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-15" class="footnote-item"><p> I first heard this sort of point from Paul Christiano. <a href="#fnref-DNdPEGz4DYMju8reA-15" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-16" class="footnote-item"><p> Here I don&#39;t mean: does it take more parameters to <em>successfully</em> promote pleasure vs. successfully promoting wabi-sabi. I just mean: does it take more parameters to <em>aim</em> optimization at the one vs. the other. <a href="#fnref-DNdPEGz4DYMju8reA-16" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-17" class="footnote-item"><p> Thanks to Daniel Kokotajlo for suggesting an image like this. <a href="#fnref-DNdPEGz4DYMju8reA-17" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-18" class="footnote-item"><p> The precise number of concepts here doesn&#39;t matter much. <a href="#fnref-DNdPEGz4DYMju8reA-18" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-19" class="footnote-item"><p> I&#39;m not saying it is, even for a physics-based world model, but I wanted an easy illustration of the point. Feel free to substitute your best-guess simplest-possible-goal here. <a href="#fnref-DNdPEGz4DYMju8reA-19" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-20" class="footnote-item"><p> Notably, this sort of prediction seems like an especially poor fit for an analogy between humans and evolution, since human goals seem to have a very intelligible relation to reproductive fitness. But evolution is plausibly quite &quot;path-dependent&quot; anyway. <a href="#fnref-DNdPEGz4DYMju8reA-20" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-21" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-DNdPEGz4DYMju8reA-21" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-22" class="footnote-item"><p> Hubinger, in discussion, suggests that the model&#39;s reasoning would proceed in terms of logical rather than physical causality. He writes: &quot;The reasoning here is: I should be the sort of model that would play the training game, since there&#39;s some (logical) chance that I&#39;ll be the model with the best inductive biases, so I should make sure that I also have good loss.&quot; But if a model can <em>tell</em> that its goal isn&#39;t yet optimally simple (and so will be ground down by SGD), then I&#39;m not sure why it would think there is a &quot;logical chance&quot; that it&#39;s favored by the inductive biases在这个意义上。 <a href="#fnref-DNdPEGz4DYMju8reA-22" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming<guid ispermalink="false"> uWdAKyHZMfoxDHcCC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:05:54 GMT</pubDate> </item><item><title><![CDATA[Results from the Turing Seminar hackathon]]></title><description><![CDATA[Published on December 7, 2023 2:50 PM GMT<br/><br/><p> We ( <a href="https://ia.effisciences.org/"><u>EffiSciences</u></a> ) ran a hackathon at the end of <a href="https://www.master-mva.com/cours/seminaire-turing/"><u>the Turing Seminar in ENS Paris-Saclay</u></a> and <a href="https://diplome.di.ens.fr/catalog_fr.html#:~:text=niveau%20L3%20M1-,S%C3%A9minaire%20Turing,-(iCal)">ENS Ulm</a> , an academic course inspired by the AGISF, with 28 projects submitted by 44 participants between the 11th and 12th November.</p><p> We share a selection of projects. <strong>See them all</strong> <a href="https://drive.google.com/drive/folders/1HgqUCu7CG44iUZdJtDO_Zua4xjLFu450?usp=drive_link"><strong>here</strong></a> <strong><u>.</u></strong></p><p> I think some of them could even be turned into valuable blog posts, and I&#39;ve learnt a lot by reading everything. Here are a few extracts.</p><h1> Towards Monosemanticity: Decomposing Vision Models with Dictionary Learning</h1><p> <i>David HEURTEL-DEPEIGES</i> [ <a href="https://drive.google.com/file/d/1gmj-ax9mtn2chjyoMxyQlP-R9zCl6q6f/view?usp=sharing"><u>link</u></a> ]</p><p> Basically an adaptation of the famous dictionary learning paper on vision CNN.</p><p> “When looking at 100 random <strong>features</strong> , 46 were found to be interpretable and monosemantic, [...] When doing the same experiment with 100 random <strong>neurons</strong> , 2 were found to be interpretable and monosemantic”. Very good execution.</p><h1> Open, closed, and everything in between: towards human-centric regulations for safer large models</h1><p> <i>Théo SAULUS</i> [ <a href="https://drive.google.com/file/d/1jopH_aaDOVRSLoBG4qnwbqglDUyFebUH/view"><u>link</u></a> ]</p><p> Imho, an almost SOTA summary of the current discourse on open sourcing models.</p><h1> A Review of The Debate on Open-Sourcing Powerful AI Models</h1><p> <i>Tu Duyen NGUYEN, Adrien RAMANANA RAHARY</i> [ <a href="https://drive.google.com/file/d/1M1B7n2jEPZPkBdYSxy_v3uNgqWAk9tF4/view?usp=drive_link"><u>link</u></a> ]</p><p> “What is the goal of this document? Our goal is to clarify, and sometimes criticize, the arguments regularly put forward in this, both from the proponents and the opponents of open-sourced AI models. In an effort to better understand both stances, we have classified the most common arguments surrounding this debate in five main topics:</p><ol><li> Safety evaluation of powerful AI models through audits or open-research</li><li> Competition in the private sector and beyond</li><li> Safety risks of open-sourcing strongly capable models</li><li> Preserving open science for its own sake</li><li> The feasibility of closing the weights of AI models</li></ol><p> In an effort to highlight the strengths and weaknesses of each stance, we will present and challenge for each family the arguments of both sides, as well as include in each family some arguments which we think are relevant, but have not been mentioned in most discussions on these topics.”</p><p> They tried to be exhaustive, but there are still some gaps. The format is nevertheless interesting (even if it could be even more concise). </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/pbs3gjvzjzifved99zqh"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/m2budueajdv9smhr803w"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/cowvnni4lmpvy5bj6wma"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/oftlgwmwvsoy4jbvt4vy"></td></tr></tbody></table></figure><h1> AI safety media analysis</h1><p> <i>Gurvan Richardeau, Raphaël Pesah</i> [ <a href="https://docs.google.com/document/d/1SBdZN9vnSVfqqJ_zUChXPxQjcrv8bT5DqSubA20ob-w/edit#heading=h.u543ptgy0th"><u>link</u></a> ]</p><p> “The first analysis we&#39;ve done involved examining major themes by employing text data analysis and ML methods (such as tf-idf analysis and topic modeling) within a corpus of 1,644 publications from the Factiva database over the past five years, specifically related to AI safety. The second analysis (Analysis 2) uses another database: Europresse. “</p><p> Here are some snippets:</p><p> “First let&#39;s have a look at the global distribution of the articles over the years.” </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> We see that more than 80% of the articles of our dataset are from 2023: </p><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/r82swpuzxwe0uuf4aiax"></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Now let&#39;s have a more precise look over the year 2023 : </p><p><img style="width:97.68%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/yog6bfn1egrsk9kami9o"></p></td></tr></tbody></table></figure><p><br> “Here are the results for the sentiment analysis: “ </p><figure class="image image_resized" style="width:43.33%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/hqdcwasrohax5fddjfgx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/jy1k9hh9r4kmmzx5nbqq 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/b8skz1gdy9a1z2rejfrg 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/nbebbobmaqncvqfe7rmq 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/g8eljblwrf7uydvtwnqs 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/yr7cjfjgxe5kzubn3ne0 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/njcvcbbxb0amtbfygp1f 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/gjvzjndbiibpplvef6mt 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/dggepbhvyvv3pyrfzlhu 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/akp4ow9op0la8hupjmo4 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/dszpurelyhvdlinexewm 840w"></figure><p> “Around 71% of the articles talking of AGI talk about it as a good thing whereas 23% are talking about it as a thing to worry about.”</p><p> “We got 57,000 articles about AGI in 2023 against 18,000 for the ones related to AI safety, so this time it is three times less. [...] There are between two and three times more articles talking about AGI without AI safety than articles about AI safety in 2023.”</p><p> My comment: Interesting. There are many more figures in the reports. Maybe those kinds of metrics could be used to measure the impact of public outreach?</p><h1> Biases in Reinforcement Learning from Human Feedback</h1><p> <i>Gaspard Berthelier</i> [ <a href="https://docs.google.com/document/d/1Qc7f21EgYHdFJqC5MGJu3Auj00vLfJEc/edit?usp=drive_link"><u>link</u></a> ]</p><p> A good summary of the paper “ <a href="https://www.lesswrong.com/posts/LqRD7sNcpkA9cmXLv/open-problems-and-fundamental-limitations-of-rlhf">Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</a> ”.</p><h1> Situational Awareness of AIs</h1><p> <i>Thomas Michel, Théo Rudkiewicz</i> [ <a href="https://docs.google.com/document/d/1v-L1nXVSEJw0_9ceAA5BQGJxRgoSk9dvjbEat8eSQQE/edit?usp=drive_link"><u>link</u></a> ]</p><p> An alternative title could be “Situational Awareness from AI to Zombies, with premium pedagogical memes”: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/ddmnh7gjlssrryhihcrw"><figcaption> Fig. 1: Why an IA that does not differentiate testing and deployment is less dangerous. </figcaption></figure></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:60.87%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/r2qfphrynxeo3vrmhs6i"><figcaption> Fig. 2: What is Situational Awareness? </figcaption></figure></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:80.57%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/fe9wqhwsimyvapdaicnd"><figcaption> Fig. 3: How to test potentially dishonest models? </figcaption></figure></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:89.12%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/tarxnmxtglvl9umsowai"><figcaption> Fig. 4: A summary of <a href="https://arxiv.org/abs/2309.00667"><u>Berglund et al</u></a> : Taken out of context: On measuring situational awareness in LLMs.</figcaption></figure></td></tr></tbody></table></figure><h1> Summary of methods to solve Goal Misgeneralization</h1><p> <i>Vincent Bardusco</i> [ <a href="https://docs.google.com/document/d/16SlCd52xOuCYLOCtASobc_EXuauVOWwt/edit?usp=drive_link"><u>link</u></a> ]</p><p> A good summary of the following papers:</p><ol><li> R. Shah, Goal Misgeneralization: Why Correct Specifications Aren&#39;t Enough For Correct Goals, 2022.</li><li> B. Shlegeris, The prototypical catastrophic AI action is getting root access to its datacenter, 2022.</li><li> Song et al., Constructing unrestricted adversarial examples with generative models, 2018.</li><li> A. Bhattad, MJ Chong, K. Liang, B. Li, D. A and Forsyth, Unrestricted Adversarial Examples via Semantic Manipulation, 2019.</li><li> B. Barnes, Imitative Generalisation (AKA &#39;Learning the Prior&#39;), 2021.</li><li> R. Jia and P. Liang, Adversarial examples for evaluating reading comprehension systems, 2017.</li><li> S. Goldwasser, MP Kim, V. Vaikuntanathan and O. Zamir, Planting undetectable backdoors in machine learning models, 2022.</li><li> A. Madry, A. Makelov, L. Schmidt, D. Tsipras and A. Vladu, Towards deep learning models resistant to adversarial attacks, 2018.</li><li> E. Hubinger, Relaxed adversarial training for inner alignment, 2019.</li></ol><h1> A Review of DeepMind&#39;s AI alignment plan</h1><p> <i>Antoine Poirier, Théo Communal</i> [ <a href="https://drive.google.com/file/d/1dC6FreML2qs1S6xuopIfYzVATRYc5lC8/view?usp=drive_link"><u>link</u></a> ]</p><p> A document summarizing the main aspects of DeepMind&#39;s plan. Up until now, their agenda has been covered in a series of blog posts and papers, but now it&#39;s all summarized in a ten-page blog post. </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top" colspan="2"><p><img style="width:92.5%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/ytkmna8qsptbc6qlerma"></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/p0csxojobppctiuxdzp7"></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:98.11%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/assvherolv9eoz0n3ijp"></p></td></tr></tbody></table></figure><h1> Criticism of criticism of interp</h1><p> <i>Gabriel Ben Zenou, Joachim Collin</i> [ <a href="https://docs.google.com/document/d/1EFTdstgHb8iVIUx_bWzmcYNDZUCzbP3gpbMQmfZLc6M/edit?usp=drive_link"><u>link</u></a> ]</p><p> An alternative title could be “Against <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><u>Against Almost Every Theory of Impact of Interpretability</u></a> ”.</p><p> Some students tried to distill the discussion and to criticize my position, and you can find my criticism of their criticism in the comments of the google doc. Here is my <a href="https://docs.google.com/document/d/1EFTdstgHb8iVIUx_bWzmcYNDZUCzbP3gpbMQmfZLc6M/edit?disco=AAABBpJ9O7A"><u>main comment</u></a> .</p><h1> Risks of Value Lock-In in China</h1><p> <i>Inès Larroche, Bastien Le Chenadec</i> [ <a href="https://drive.google.com/file/d/1a81ezHWDE10gnABsk8V_7iEreSx0024S/view?usp=drive_link"><u>link</u></a> ]</p><p> It&#39;s a very good summary of what is happening in China regarding AI.</p><h1> Is LeCun making progress in AI safety with “A Path Towards Autonomous Machine Intelligence”?</h1><p> <i>Victor Morand</i> [ <a href="https://drive.google.com/file/d/1muZiF9R6sxPU2Z-gMsMF0j60kMT8CT5P/view?usp=drive_link"><u>link</u></a> ]</p><p> It&#39;s a good summary of LeCun&#39;s idea, and the numerous criticisms of his plan. </p><figure class="image image_resized" style="width:35.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/y3vl6scv5yercxgvrbor"><figcaption> LeCun&#39;s architecture</figcaption></figure><h1> Taxonomy of governance regulations</h1><p> Mathis Embit [ <a href="https://docs.google.com/document/d/1VtLmxM9mdBTIvKY0rWNIGtjB8ODdVwgYZ0OuX4BGyXs/edit?usp=drive_link"><u>link</u></a> ]</p><p> A summary of the main ways to regulate AI. Comparing different policies provides them with more distinctiveness and depth. </p><figure class="image image_resized" style="width:62.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bo9c8jyHab4gjRHxS/fmnxrvlvdm0ig2k66qyt"><figcaption></figcaption></figure><h1> Other notable projects</h1><p> <u>See them all</u> <a href="https://drive.google.com/drive/folders/1HgqUCu7CG44iUZdJtDO_Zua4xjLFu450?usp=drive_link"><u>here</u></a> <u>.</u> </p><figure class="table"><table><thead><tr><th style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px"><p><strong>标题</strong></p></th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px"><p><strong>作者</strong></p></th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>概括</strong></p></th></tr></thead><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>Technical justification of OpenAI&#39;s superalignment plan</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Robin Sobczyk,<br> Bastien Lhopitallier</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A lot of papers are summarized here</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Agent Creation</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Mathias Vigouroux</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A philosophical essay trying to explain how theory of mind emerges in humans and could emerge in AIs.这很有趣。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Anthropic RSP analysis</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Ralf Cortes Do Nascimento,</p><p> Paul Cazali</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A good summary of the entire LW discussion on the RSPs.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Goal Misgeneralization</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Basile TERVER,<br> Antoine OLIVIER</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of various techniques to prevent Goal Misgeneralization</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Detecting and reducing gender bias</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Gabrielle LE BELLIER</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A literature review on techniques to reduce gender biases in LLMs</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Literature review on SOTA Adversarial attacks and backdoors</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Mathis Le Bail,<br> David Sahna</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of many papers on attack and defense. The conclusion: &quot;all the studies in the literature tend to the following conclusion: there is no single effective method of defense against all possible types of attack&quot;</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Why does DL work so well?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Sacha ELKOUBI,<br> Even MATENCIO,<br> Eustache LE BIHAN,<br> Paul SITOLEUX</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> It needs a bit more polishing, but this is a good basis. It goes from explaining the Classical Statistical learning theory to Singular learning theory and everything in between.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> AI Act: what rules for foundation models?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Matthieu Carreau</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of the current challenges for EU regulations</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Critics of GovAI</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Xavier Secheresse,<br> Pierre-Yves Doireau</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A summary of their views, and a small critique</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>治愈</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>David El Bèze,<br> Raphaël Cohen,<br> Nathaniel Cohen</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A short story of a deceptive AI transforming the world</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Logit Lens on Vision Transformer</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Akedjou Achraff Adjileye</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Testing the logit lens on Vision transformers, it works without difficulty.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Safety benchmarks</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Ugo Insalaco</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A review of the main safety benchmarks and a few proposals of missing benchmarks.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Probability of X-Risks without deceptive alignment</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Mathilde DUPOUY</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> She enumerates the different scenarios in the following <a href="https://docs.google.com/spreadsheets/d/1W2sm4Si9zu6i9ioBbV_vnC6RTCenrPXRJwitvn9gqYg/edit?usp=sharing"><u>table</u></a> . The final probability among all the risks that the one that occurs is an existential risk without deceptive alignment is 71%.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Do you think Le Cun&#39;s architecture is safe?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Théotime de Charrin</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Another criticism of LeCun&#39;s agenda.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Navigating AI Safety</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Abdessalam EDDIB</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Presents a landscape of some strategies to reduce risks through alignment techniques and some governance strategies, and is targeting beginners in alignment.</p></td></tr></tbody></table></figure><p></p><h1></h1><p></p><h1>一些想法</h1><ul><li>Running the hackathon was a lot of fun. We just worked in a room at the university during the weekend. We also organized a meme contest.强烈推荐。</li><li> <strong>Unlike last year&#39;s hackathon, where students had the freedom to choose their own topics, this time we provided a list of predetermined topics</strong> . This resulted in higher quality work. If you&#39;re interested in organizing a similar hackathon, you can find the list of subjects <a href="https://www.notion.so/AI-Safety-Research-Projets-2023-English-b6c5b1cb3ad641abbfa9acd856cd8bad?pvs=21"><u>here</u></a> if you want to run something similar.</li><li> I am extremely pleased with the outcome, especially since the students went from having no knowledge in AI Safety to making valuable contributions in only 2 months.</li><li> The material of the course is available <a href="https://docs.google.com/document/d/1d0W_5xRaVehYO_j4P1TN4-JJGktMNEXgBxO4MXCr68o/edit#heading=h.s5znjvp3l4zi"><u>here</u></a> .</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/bo9c8jyHab4gjRHxS/results-from-the-turing-seminar-hackathon#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bo9c8jyHab4gjRHxS/results-from-the-turing-seminar-hackathon<guid ispermalink="false"> bo9c8jyHab4gjRHxS</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Thu, 07 Dec 2023 14:50:38 GMT</pubDate> </item><item><title><![CDATA[Gemini 1.0]]></title><description><![CDATA[Published on December 7, 2023 2:40 PM GMT<br/><br/><p><span style="color:initial">正在发生。</span> <a href="https://twitter.com/sundarpichai/status/1732414873139589372" target="_blank" rel="noopener noreferrer nofollow">Here is CEO Pichai&#39;s Twitter announcement</a> <span style="color:initial">.</span> <a href="https://twitter.com/demishassabis/status/1732416482976673832" target="_blank" rel="noopener noreferrer nofollow">Here is Demis Hassabis announcing</a> <span style="color:initial">.</span> <a href="https://twitter.com/GoogleDeepMind/status/1732416095355814277" target="_blank" rel="noopener noreferrer nofollow">Here is the DeepMind Twitter announcement</a> <span style="color:initial">.</span> <a href="https://blog.google/technology/ai/google-gemini-ai/" target="_blank" rel="noopener noreferrer nofollow">Here is the blog announcement</a> <span style="color:initial">.</span> <a href="https://twitter.com/OriolVinyalsML/status/1732426317268758671" target="_blank" rel="noopener noreferrer nofollow">Here is Gemini co-lead Oriol Vinyals</a> <span style="color:initial">, promising more to come. Here is</span> <a href="https://twitter.com/JeffDean/status/1732415515673727286" target="_blank" rel="noopener noreferrer nofollow">Google&#39;s Chief Scientist Jeff Dean</a> <span style="color:initial">bringing his best hype.</span></p><p></p><h4></h4><h4>技术规格</h4><p><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf" target="_blank" rel="noopener noreferrer nofollow">Let&#39;s check out the specs.</a></p><p></p><span id="more-23623"></span><p></p><p> Context length trained was 32k tokens, they report 98% accuracy on information retrieval for Ultra across the full context length. So a bit low, both lower than GPT—4 and Claude and lower than their methods can handle. Presumably we should expect that context length to grow rapidly with future versions.</p><p> There are three versions of Gemini 1.0.</p><blockquote><p> Gemini 1.0 是我们的第一个版本，具有三种尺寸：Ultra 用于高度复杂的任务，Pro 用于增强性能和大规模可部署性，Nano 用于设备上应用程序。每种尺寸都经过专门定制，以满足不同的计算限制和应用要求。</p><p> ……</p><p> Nano: Our most efficient model, designed to run on-device.我们训练了两个版本的 Nano，参数分别为 1.8B (Nano-1) 和 3.25B (Nano-2)，分别针对低内存和高内存设备。它是通过从较大的 Gemini 模型中提取来训练的。它采用 4 位量化进行部署，并提供一流的性能。</p><p> ……</p><p> Nano 系列模型利用蒸馏和训练算法方面的额外进步，为各种任务（例如摘要和阅读理解）生成一流的小语言模型，从而为我们的下一代设备上体验提供动力。</p></blockquote><p>这是有道理的。 I do think there are, mostly, exactly these three types of tasks. Nano tasks are completely different from non-Nano tasks.</p><p> This graph reports relative performance of different size models. We know the sizes of Nano 1 and Nano 2, so this is a massive hint given how scaling laws work for the size of Pro and Ultra. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5267840b-8509-4344-82a7-e570a0d6c82d_1075x466.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ohs44wqu09loeuohlixp" alt=""></div></figure></div><p> Gemini is natively multimodal, which they represent as being able to seamlessly integrate various inputs and outputs.</p><p> They say their benchmarking on text beats the existing state of the art.</p><blockquote><p>我们最强大的模型 Gemini Ultra 在我们报告的 32 个基准测试中的 30 个中取得了最新的结果，其中包括 12 个流行文本和推理基准测试中的 10 个、9 个图像理解基准测试中的 9 个、6 个视频理解基准测试中的 6 个，以及 5 个语音识别和语音翻译基准测试中的 5 个。 Gemini Ultra 是第一个在 MMLU（Hendrycks 等人，2021a）上实现人类专家表现的模型，MMLU 是通过一系列考试测试知识和推理的著名基准测试，得分高于 90%。除了文本之外，Gemini Ultra 在具有挑战性的多模式推理任务上也取得了显着的进步。</p></blockquote><p> I love that &#39;above 90%&#39; turns out to be exactly 90.04%, whereas human expert is 89.8%, prior SOTA was 86.4%. Chef&#39;s kiss, 10/10, no notes. I mean, what a coincidence, that is not suspicious at all and no one was benchmark gaming that, no way. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7d1fc5c4-473b-42be-87b8-61e85ce59c1d_1024x1024.webp" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/qauui9xd8kyjcdo3alxe" alt="由 DALL·E 生成"></div></figure></div><blockquote><p>我们发现，当与考虑模型不确定性的思维链提示方法（Wei 等人，2022）结合使用时，Gemini Ultra 可以实现最高的准确度。该模型使用 k 个样本（例如 8 个或 32 个）生成一个思想链。如果存在高于预设阈值（根据验证分割选择）的共识，则它会选择此答案，否则它将恢复为基于最大值的贪婪样本没有思路的可能性选择。</p></blockquote><p> I wonder when such approaches will be natively integrated into the UI for such models. Ideally, I should be able to, after presumably giving them my credit card information, turn my (Bard?) to &#39;Gemini k-sample Chain of Thought&#39; and then have it take care of itself.</p><p> Here&#39;s their table of benchmark results. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e9e608f-401f-4c88-802f-4534445e7d2f_1249x1221.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/uvwdrrwfwbiyjfvoosob" alt=""></div></figure></div><p> <a href="https://twitter.com/_philschmid/status/1732435791358410863" target="_blank" rel="noopener noreferrer nofollow">So the catch with MMLU</a> is that Gemini Ultra gets more improvement from CoT@32, where GPT-4 did not improve much, but Ultra&#39;s baseline performance on 5-shot is worse than GPT-4&#39;s.</p><p> <a href="https://twitter.com/AISafetyMemes/status/1696509099490304250" target="_blank" rel="noopener noreferrer nofollow">Except the other catch is that GPT-4, with creative prompting, can get to 89%</a> ?</p><p> GPT-4 is pretty excited about this potential &#39;Gemini Ultra&#39; scoring 90%+ on the MMLU, citing a variety of potential applications and calling it a substantial advancement in AI capabilities.</p><p> They strongly imply that GPT-4 got 95.3% on HellaSwag due to data contamination, noting that including &#39;specific website extracts&#39; improved Gemini&#39;s performance there to a 1-shot 96%. Even if true, performance there is disappointing.</p><p> What does this suggest about Gemini Ultra? One obvious thing to do would be to average all the scores together for GPT-4, GPT-3.5 and Gemini, to place Gemini on the GPT scale. Using only benchmarks where 3.5 has a score, we get an average of 61 for GPT 3.5, 79.05 for GPT-4 and 80.1 for Gemini Ultra.</p><p> By that basic logic, we would award Gemini a benchmark of 4.03 GPTs. If you take into account that improvements matter more as scores go higher, and otherwise look at the context, and assume these benchmarks were not selected for results, I would increase that to 4.1 GPTs.</p><p> On practical text-only performance, I still expect GPT-4-turbo to be atop the leaderboards.</p><p> Gemini Pro clearly beat out PaLM-2 head-to-head on human comparisons, but not overwhelmingly so. It is kind of weird that we don&#39;t have a win rate here for GPT-4 versus Gemini Ultra. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99738558-c837-4633-a3fb-32cfa971f8fe_1182x214.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/jhqngog1yz1lyj7jxj32" alt=""></div></figure></div><p> Image understanding benchmarks seem similar. Some small improvements, some big enough to potentially be interesting if this turns out to be representative. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5065f262-f9f6-413f-8ddc-a0818caf96af_1230x706.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/puy4rl677dnmqgxdjqfa" alt=""></div></figure></div><p> Similarly they claim improved SOTA for video, where they also have themselves as the prior SOTA in many cases. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1381be4d-d1a6-40ec-bac3-c30d8db25c7d_1198x544.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/nvyh2lwcplusfkpaznpf" alt=""></div></figure></div><p> For image generation, they boast that text and images are seamlessly integrated, such as providing both text and images for a blog, but provide no examples of Gemini doing such an integration. Instead, all we get are some bizarrely tiny images.</p><p> One place we do see impressive claimed improvement is speech recognition. Note that this is only Gemini Pro, not Gemini Ultra, which should do better. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dd98ff1-8f37-480f-9aee-35661fe49c1b_1191x481.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ploxecxdydx5qs0oiatn" alt=""></div></figure></div><p> Those are error rate declines you would absolutely notice. Nano can run on-device and it is doing importantly better on YouTube than Whisper.很酷。</p><p> Here&#39;s another form of benchmarking.</p><blockquote><p> The AlphaCode team built AlphaCode 2 (Leblond et al, 2023), a new Gemini-powered agent, that combines Gemini&#39;s reasoning capabilities with search and tool-use to excel at solving competitive programming problems. AlphaCode 2 在 Codeforces 竞争性编程平台上的参赛者中排名前 15%，比其排名前 50% 的最先进的前身有很大进步（Li 等人，2022）。</p><p> ……</p><p> AlphaCode 2 解决了 43% 的竞赛问题，比之前创下纪录的 AlphaCode 系统（解决了 25%）提高了 1.7 倍。</p></blockquote><p> I read the training notes mostly as &#39;we used all the TPUs, no really there were a lot of TPUs&#39; with the most interesting note being this speed-up. Does this mean they now have far fewer checkpoints saved, and if so does this matter?</p><blockquote><p> Maintaining a high goodput [time spent computing useful new steps over the elapsed time of a training job] at this scale would have been impossible using the conventional approach of periodic checkpointing of weights to persistent cluster storage.</p><p>对于 Gemini，我们使用了模型状态的冗余内存副本，并且在任何计划外的硬件故障中，我们可以直接从完整的模型副本中快速恢复。与 PaLM 和 PaLM-2（Anil 等人，2023）相比，尽管使用的训练资源要大得多，但恢复时间显着加快。结果，最大规模训练作业的总体产出从 85% 增加到 97%。</p></blockquote><p> Their section on training data drops a few technical hints but wisely says little. They deliberately sculpted their mix of training data, in ways they are keeping private.</p><p> In section 6 they get into responsible deployment. I appreciated them being clear they are focusing explicitly on questions of deployment. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cc6332e-9c7f-4500-9cbc-56bcc60a297b_805x508.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/hpgjgrsg8kovglqo85ix" alt=""></div></figure></div><p> They focus (correctly) exclusively on the usual forms of mundane harm, given Gemini is not yet breaking any scary new ground.</p><blockquote><p>基于对已知和预期影响的理解，我们制定了一套“模型政策”来指导模型开发和评估。模型策略定义充当负责任开发的标准化标准和优先级架构，并作为启动准备情况的指示。双子座模型政策涵盖多个领域，包括：儿童安全、仇恨言论、事实准确性、公平和包容以及骚扰。</p></blockquote><p> Their instruction tuning used supervised fine tuning and RLHF.</p><p> A particular focus was on attribution, which makes sense for Google.</p><p> Another was to avoid reasoning from a false premise and to otherwise refuse to answer &#39;unanswerable&#39; questions. We need to see the resulting behavior but it sounds like the fun police are out in force.</p><p> It doesn&#39;t sound like their mitigations for factuality were all that successful? Unless I am confusing what the numbers mean. </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bc6215f-49c0-4323-b4d1-9a2e69632abe_1219x244.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/xdhxwebhm5dyceamdfwk" alt=""></div></figure></div><p> Looking over the appendix and its examples, it is remarkable how unimpressive were all of the examples given.</p><p> I notice that I watch how honestly DeepMind approaches reporting capabilities and attacking benchmarks as an important sign for their commitment to safety. There are some worrying signs that they are willing to twist quite a ways. Whereas the actual safety precautions do not bother me too much one way or the other?</p><p> The biggest safety precaution is one Google is not even calling a safety precaution. They are releasing Gemini Pro, and holding back Gemini Ultimate. That means they have a gigantic beta test with Pro, whose capabilities are such that it is harmless. They can use that to evaluate and tune Ultimate so it will be ready.</p><p> <a href="https://blog.google/technology/ai/google-gemini-ai/#capabilities" target="_blank" rel="noopener noreferrer nofollow">The official announcement</a> offers some highlights.</p><p> <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-gemini-ai/" target="_blank" rel="noopener noreferrer nofollow">Demis Hassabis talked to Wired about Gemini</a> . Didn&#39;t seem to add anything.</p><h4> Level Two Bard</h4><p> Gemini Pro, even without Gemini Ultra <a href="https://twitter.com/GoogleDeepMind/status/1732430045275140415" target="_blank" rel="noopener noreferrer nofollow">should be a substantial upgrade to Bard</a> . The question is, will that be enough to make it useful when we have Claude and ChatGPT available? I will be trying it to find out, same as everyone else. Bard does have some other advantages, so it seems likely there will be some purposes, when you mostly want information, where Bard will be the play.</p><p> <a href="https://twitter.com/IntuitMachine/status/1732474666948661505" target="_blank" rel="noopener noreferrer nofollow">This video represents some useful prompt engineering</a> and reasoning abilities, used to help plan a child&#39;s birthday party, largely by brainstorming possibilities and asking clarifying questions. If they have indeed integrated this functionality in directly, that&#39;s pretty cool.</p><p> <a href="https://twitter.com/nonmayorpete/status/1732544321122152940" target="_blank" rel="noopener noreferrer nofollow">Pete says Bard is finally at a point where he feels comfortable recommending it</a> . The prompts are not first rate, but he says it is greatly improved since September and the integrations with GMail, YouTube and Maps are useful. It definitely is not a full substitute at this time, the question is if it is a good complement.</p><p> Even before Gemini, Bard did a very good job helping my son with his homework assignments, such that I was sending him there rather than to ChatGPT.</p><p> <a href="https://twitter.com/goodside/status/1657396491676164096" target="_blank" rel="noopener noreferrer nofollow">Returning a clean JSON continues to require extreme motivation</a> .</p><p> When will Bard Advanced (with Gemini Ultra) be launched? <a href="https://manifold.markets/Rodeo/will-bard-advanced-be-launched-by-j" target="_blank" rel="noopener noreferrer nofollow">Here&#39;s a market on whether it happens in January</a> .</p><h4> Gemini Reactions</h4><p> Some were impressed.其他人则没有那么多。</p><p> The first unimpressive thing is that all we are getting for now is Gemini Pro. Pro is very clearly not so impressive, clearly behind GPT-4.</p><blockquote><p> <a href="https://twitter.com/elidourado/status/1732426687218941982" target="_blank" rel="noopener noreferrer nofollow">Eli Dourado:</a> Here is the table of Gemini evals from the paper. Note that what is being released into the wild today is Gemini Pro, not Gemini Ultra. So don&#39;t expect Bard to be better than ChatGPT Plus just yet. Looks comparable to Claude 2.</p></blockquote><p> <a href="https://twitter.com/Simeon_Cps/status/1732425528727232772/history" target="_blank" rel="noopener noreferrer nofollow">Simeon?没有留下深刻印象。</a></p><p> Simeon: Gemini is here. Tbh it feels like it&#39;s GPT-4 + a bit more multimodality + epsilon capabilities. So my guess is that it&#39;s not a big deal on capabilities, although it might be a big deal from a product standpoint which seems to be what Google is looking for.</p><p> <a href="https://twitter.com/nyctibia/status/1732443540880290002" target="_blank" rel="noopener noreferrer nofollow">As always</a> , one must note that everything involved was chosen to be what we saw, and potentially engineered or edited. The more production value, the more one must unwind.</p><p> For the big multimodal video, this issue is a big deal.</p><blockquote><p> Robin: I found it quite instructive to <a href="https://t.co/YzDJV4YXTE" target="_blank" rel="noopener noreferrer nofollow">compare this promo video</a> <a href="https://t.co/mOuqS3OznG" target="_blank" rel="noopener noreferrer nofollow">with the actual prompts</a> .</p><p> <a href="https://twitter.com/robertwiblin/status/1732750022565879880" target="_blank" rel="noopener noreferrer nofollow">Robert Wiblin</a> (distinct thread): It&#39;s what Google themselves out put. So it might be cherry picked, but not faked. I think it&#39;s impressive even if cherry picked.</p></blockquote><p> Was this faked? I am sure it was cherry-picked, and likely the result of various iterations. I doubt they would actually fake it, certainly doubt they&#39;d fake it and also tell us with another document exactly how they faked it, and I think those saying this are misunderstanding how the process worked. You should not however expect to have this quality of experience in a typical interaction.</p><p> Setting aside integrity issues, wow are we all jaded at this point, but when I watch that video, the biggest impression I get is… big lame dad energy?</p><p> I do get that this is happening in real time, but none of this is surprising me. Google put out its big new release, and I&#39;m not scared. If anything, I&#39;m kind of bored? This is the best you could do?</p><p> Whereas when watching the exact same video, others react differently.</p><blockquote><p> <a href="https://twitter.com/amasad/status/1732439083555631581" target="_blank" rel="noopener noreferrer nofollow">Amjad Masad</a> (CEO Replit): This fundamentally changes how humans work with computers.</p></blockquote><p>可以？ I mean, I guess, if you didn&#39;t already assume all of it, and it was this smooth for regular users? I can think of instances in which a camera feed hooked up to Gemini with audio discussions could be a big game. To me this is a strange combination of the impressive parts already having been &#39;priced into&#39; my world model, and the new parts not seeming impressive.</p><p> So I&#39;m probably selling it short somewhat to be bored by it. If this was representative of a smooth general multimodal experience, there is a lot to explore.</p><p> <a href="http://Gemini just dropped and, at least on the chosen benchmarks, seems to outperform GPT 4.0. Let's just say it's likely in the same ballpark.  This is not particularly surprising, Google and Deepmind have a world class team and immense resources.  I'm still puzzled as to what the commentators suggesting Google couldn't do it or wouldn't ship something like it were thinking." target="_blank" rel="noopener noreferrer nofollow">Arthur thinks Gemini did its job</a> , but that this is unsurprising and it is weird people thought Google couldn&#39;t do it.</p><p> Liv Boeree?印象深刻。</p><blockquote><p> <a href="https://twitter.com/Liv_Boeree/status/1732427988321419545/history" target="_blank" rel="noopener noreferrer nofollow">Liv Boeree:</a> This is pretty nuts, looks like they&#39;ve surpassed GPT4 on basically every benchmark… so this is most powerful model in the world?! Woweee what a time to be alive.</p></blockquote><p> <a href="https://twitter.com/GaryMarcus/status/1732428456837935471" target="_blank" rel="noopener noreferrer nofollow">Gary Marcus?</a> Impressed in some ways, not in others.</p><blockquote><p> Gary Marcus: Thoughts &amp; prayers for VCs that bought OpenAI at $86B.</p><p> Google Gemini 和 GPT-4 的热门观点： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em"> Google Gemini seems to have by many measures matched (or slightly exceeded) GPT-4, but not to have blown it away. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em"> From a commercial standpoint GPT-4 is no longer unique. That&#39;s a huge problem for OpenAI, especially post drama, when many customers are now seeking a backup plan. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em"> From a technical standpoint, the key question is: are LLMs close to a plateau?</p><p>请注意，盖茨和 Altman 都曾暗示过，尽管商业需求巨大，但 GPT-5 在一年后还没有出现。谷歌尽管拥有所有资源，但并没有击败 GPT-4，这一事实很能说明问题。</p></blockquote><p> I love that this is saying that OpenAI isn&#39;t valuable both because Gemini is so good and also because Gemini is not good enough.</p><p> <a href="https://twitter.com/tszzl/status/1732470963156250868" target="_blank" rel="noopener noreferrer nofollow">Roon offers precise praise</a> .</p><blockquote><p> Roon: congrats to Gemini team! it seems like the global high watermark on multimodal ability.</p><p> <a href="https://twitter.com/tszzl/status/1732508388339396973" target="_blank" rel="noopener noreferrer nofollow">The MMLU result seems a bit fake</a> / unfair terms but the HumanEval numbers look like a actual improvement and ime pretty closely match real world programming utility</p><p> <a href="https://twitter.com/davidmanheim/status/1732444067785547973" target="_blank" rel="noopener noreferrer nofollow">David Manheim seems on point</a> (other thread): I have not used the system, but if it does only slightly outmatch GPT-4, it seems like slight evidence that progress in AI with LLMs is not accelerating the way that many people worried and/or predicted.</p></blockquote><p> <a href="https://twitter.com/joeykrug/status/1732451230541119851/history" target="_blank" rel="noopener noreferrer nofollow">Joey Krug is super unimpressed by the fudging on the benchmarks</a> , says they did it across the board not only MMLU.</p><blockquote><p> <a href="https://twitter.com/packyM/status/1732458006359462308" target="_blank" rel="noopener noreferrer nofollow">Packy McCormick: all of you</a> (shows picture)</p><p> Ruxandra Teslo: wait what happened recently? did they do something good?</p><p> Packy: they did a good! </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8193b99-4b9f-4dc7-b8dc-3518f8c1172a_726x900.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ofYejKKiSFYH2gLBb/ulvtx8h1uy6lqvbsm22h" alt="图像"></div></figure></div><p> Google&#39;s central problem is not wokeness, it is that they are a giant company with lots of internal processes and powers that prevent or slow or derail innovation, and prevent moving fast or having focus. And there are especially problems making practical products, integrating the work of various teams, making incentives line up. There is lots of potential, tons of talent, plenty of resources, but can they turn that into a product?</p><p>还为时过早。 Certainly they are a long way from &#39;beat OpenAI&#39; but this is the first and only case where someone might be in the game. The closest anyone else has come is Claude&#39;s longer context window.</p><p></p><br/><br/><a href="https://www.lesswrong.com/posts/ofYejKKiSFYH2gLBb/gemini-1-0#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ofYejKKiSFYH2gLBb/gemini-1-0<guid ispermalink="false"> ofYejKKiSFYH2gLBb</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 14:40:07 GMT</pubDate> </item><item><title><![CDATA[Random Musings on Theory of Impact for Activation Vectors]]></title><description><![CDATA[Published on December 7, 2023 1:07 PM GMT<br/><br/><p>我想分享一些关于为什么激活向量可能很重要的想法，但在阅读本文之前，您应该了解三件事：</p><ol><li>我不知道我在说什么</li><li>我不知道我在说什么</li><li>我绝对不知道我在说什么</li></ol><p>另外，也许这篇文章是多余的和不必要的<span class="footnote-reference" role="doc-noteref" id="fnrefqrpuhuwrepp"><sup><a href="#fnqrpuhuwrepp">[1]</a></sup></span> ，因为它已经在我没有见过的地方得到了解释。或者也许我在其他地方见过有人写过这个，但只是不记得出处。</p><p>但这里是这样的：</p><p>当我们有其他控制网络的方法（例如提示或微调）时，为什么激活向量对于对齐很重要？</p><p>激活向量的一种观点是它们是一种黑客或玩具。只需通过网络传递一个单词并添加它就可以控制网络，这不是很酷吗？</p><p>另一种观点是，他们正在利用一些基本的东西，看看 Beren 的<a href="https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear">深度学习模型可能秘密地（几乎）是线性的</a>。在这种情况下，我们应该期望随着网络规模的扩大，它会继续工作，甚至更好，而不是随机中断/停止工作。</p><p>另一个框架如下：语言对于神经网络来说是一种尴尬的操作格式，因此它们将其翻译成一个潜在的空间，将重要的组成部分分开，当然还有一定程度的叠加。提示和微调是影响这些网络行为的迂回方式：我们要么将其推入特定的模拟，要么训练网络以更好地欺骗自己，认为它做得很好。</p><p>理想情况下，我们只要有足够的可解释性知识，就可以直接干预潜在空间，设置或增加正确的神经元。激活向量比这更加分散，但它们更接近理想状态，并且可以作为概念证明。</p><p>例如，执行“快乐-悲伤”减法以获得幸福向量的技巧演示了如何通过将这些共有的所有特征归零来改进这一点。这些特征比如都是英语、都不太正式、都是形容词等等。可能大多是无关紧要的。更先进的方法（例如<a href="https://arxiv.org/abs/2306.03341">推理时间干预）</a>使我们能够通过仅对特定头部进行干预来进一步完善这一点。这个技术还是比较简单的；如果我们能够找到缩小干预范围的正确方法，我们可能会实现或非常接近直接干预正确潜在因素的梦想。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnqrpuhuwrepp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqrpuhuwrepp">^</a></strong></sup></span><div class="footnote-content"><p>我知道“不必要”这个词在这里是多余的，没有必要的。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/k9z9fzpAgMLGQG9hg/random-musings-on-theory-of-impact-for-activation-vectors#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/k9z9fzpAgMLGQG9hg/random-musings-on-theory-of-impact-for-activation-vectors<guid ispermalink="false"> k9z9fzpAgMLGQG9hg</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Thu, 07 Dec 2023 13:07:09 GMT</pubDate> </item><item><title><![CDATA[Is AlphaGo actually a consequentialist utility maximizer?]]></title><description><![CDATA[Published on December 7, 2023 12:41 PM GMT<br/><br/><p> <strong>TL;DR：在一般情况下，将适应执行器与结果主义效用最大化器结合起来是否会带来更高的效用结果，还是 AlphaGo 只是很奇怪？</strong></p><p>我最近正在阅读<a href="https://sci-hub.se/10.1038/nature16961">AlphaGo 的论文</a>，正如人们所做的那样。我注意到，从架构上来说，AlphaGo</p><ol><li>价值网络：“给定董事会状态，导致获胜的可能性有多大”。我将其解释为预期效用估计器。</li><li>推出：“尝试一系列不同的高概率线路”。我将其解释为“可能行动的后果”估计器，它既可用于细化预期效用估计，也可用于选择最高价值的行动。</li><li>政策网络：“鉴于董事会状态，从该位置看到的举动是正常的”。我将其解释为一种“适应执行器”风格的东西——除了模式匹配之外，它不会特别尝试做任何事情。</li></ol><p>我一直认为 AlphaGo 展示了结果主义推理的力量，所以打开论文并看到实际上将适应执行器钉在你的效用最大化器上比尝试使用纯粹的结果主义推理提供了更多的实用性，这有点令人惊讶（在感觉“ <code>argmax</code>超过你的行为的预测结果”）。</p><p>我注意到我非常困惑。</p><p>我倾向于认为“也许政策网络没有做任何重要的事情，它只是纠正一些小的效用估计问题”，但论文的作者预料到了这种反应，并包括了这个非常有用的图表： <br><br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/stignrntrob8iut1lb8i" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/wgzp9l8tdkhkd4ww4fwy 158w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/dyubzuor5ag282sh4afd 238w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/qx4izvsywtbgiqvbscor 318w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/npaid2fabjxibsffqg0n" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/ykr2ntyggwrsbrh98zxr 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/xcrijzehmtdmibbfxp6x 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/skbdhontl2zavuyxw0b9 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/yf1fvk13bayljq7dzrdk 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/zybtothj0lz9vvnxbdwc 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/oohjk0rpvz1g2wp9oby8 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/mviljbisercemjdxb3ya 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/bnnj12mqweh6nbpz82tv 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/wh0zljshllk8it0rpc5g 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Q8k3HSy7kELTXTbDY/tj62gwdysp2zlilrteyi 1082w"></p><p>纵轴是 Elo 估计值，X 轴标签上的点代表三个成分中哪一个对这些试验是活跃的。</p><p>作为参考，以下组件与上图相关：</p><ol><li>快速推出策略<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\pi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>：一个小型且高效但不是极其准确的网络，它基于检查最后一步的一组固定属性来预测每个合法一步将成为下一步的概率（例如“此步是否与前一步相关”，“此移动/前一移动的直接邻域是否与预定模式匹配”）。准确度为24.2%。</li><li>树推出策略<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\tau}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span></span></span></span></span></span></span></span></span></span> ：与快速推出策略类似，但增加了三个功能“移动允许捕获石头”、“到最后 2 次移动的曼哈顿距离”和稍大的模式（12 点菱形而不是 3x3 模式）围绕这个举动。如果您好奇的话，扩展数据表 4 中给出了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\pi}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\tau}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.08em;">τ</span></span></span></span></span></span></span></span></span></span></span>的详细信息。</li><li> SL 策略网络<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_{\sigma}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.001em;">σ</span></span></span></span></span></span></span></span></span></span></span> ：一个巨大的（按照当时的标准）13 层 NN，在人类游戏上进行预训练，然后通过学习模仿一个单独的 RL 策略网络（如果我没有正确阅读本文）进行进一步训练最终的 AlphaGo 系统中没有使用任何地方（因为 SL 策略网络的性能优于它）</li><li>价值网络<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_{\theta}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span></span></span></span></span> ：与 SL 策略网络结构相同，只是它输出当前棋盘状态当前玩家获胜的概率。</li><li>推出：相当标准的 MCTS</li></ol><p>所以我的问题是：</p><p><strong>为什么带有 SL 策略网络的系统比没有 SL 策略网络的系统表现好得多？</strong></p><p>几个假设：</p><ol><li><strong>无聊的答案：</strong> SL 策略网络只是帮助缩小搜索树。您可以通过在每个合法棋步上运行价值网络，然后将每个合法棋步的获胜概率转换为搜索权重来获得更好的性能，但这需要每次棋步运行价值网络 ~19x19=361 次，这是很多比运行一次 SL 策略网络更昂贵。</li><li><strong>策略网络只是增加了鲁棒性：</strong>第二个单独训练的价值网络将与策略网络一样有用。</li><li><strong>价值网络中的错误：</strong>价值网络会稍微高估某些位置的价值，并稍微低估其他位置的价值，这取决于是否存在表明输赢的特定棋子图案。如果棋盘状态实际上正在失败，但价值网络并不完全确定该位置正在失败，则继续掩盖其失败事实的棋步将比实际上提高获胜机会的棋步获得更高的评级，反而让这个位置的弱点更加明显。</li><li><strong>结果主义实际上行不通：</strong>有一些更深层次的原因，使用价值网络加树搜索不仅<i>行不通</i>，而且在对抗性环境中<i>永远</i>行不通。</li><li><strong>我误解了这篇论文：</strong> AlphaGo 实际上并没有像我想象的那样使用 SL 策略网络</li><li><strong>完全是另一回事：</strong>这些可能性肯定没有涵盖完整的假设空间。</li></ol><p>我最喜欢的假设是 (3)，但实际上我希望它是 (5) 或 (6)。如果有人可以帮助我了解这里发生的事情，我将非常感激。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Q8k3HSy7kELTXTbDY/is-alphago-actually-a-consequentialist-utility-maximizer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Q8k3HSy7kELTXTbDY/is-alphago-actually-a-consequentialist-utility-maximizer<guid ispermalink="false"> Q8k3HSy7kELTXTbDY</guid><dc:creator><![CDATA[faul_sname]]></dc:creator><pubDate> Thu, 07 Dec 2023 12:41:07 GMT</pubDate> </item><item><title><![CDATA[The GiveWiki’s Top Picks in AI Safety for the Giving Season of 2023]]></title><description><![CDATA[Published on December 7, 2023 9:23 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/crcPnJ6SwSBCTXHwP/the-givewiki-s-top-picks-in-ai-safety-for-the-giving-season#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/crcPnJ6SwSBCTXHwP/the-givewiki-s-top-picks-in-ai-safety-for-the-giving-season<guid ispermalink="false"> crcPnJ6SwSBCTXHwP</guid><dc:creator><![CDATA[Dawn Drescher]]></dc:creator><pubDate> Thu, 07 Dec 2023 09:23:05 GMT</pubDate> </item><item><title><![CDATA[Language Model Memorization, Copyright Law, and Conditional Pretraining Alignment]]></title><description><![CDATA[Published on December 7, 2023 6:14 AM GMT<br/><br/><p><i>这是论文《</i><a href="https://arxiv.org/abs/2311.17035"><i>从（生产）语言模型中可扩展地提取训练数据</i></a><i>》和相关博客文章《</i><a href="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html"><i>从 ChatGPT 中提取训练数据》</i></a>的链接文章<i>，后面是我的反应，包括对版权诉讼的主要影响的分析，以及解决此问题的对齐技术的建议。</i></p><p><i>请注意，我不是律师，我在下面有关知识产权法的评论可能是错误的或过于简单化的（特别是对于非美国司法管辖区）。</i></p><h2>论文和博客文章</h2><p>这篇<a href="https://arxiv.org/abs/2311.17035">论文</a>和<a href="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html">博客文章</a>都可读、有趣，而且看起来很重要：我强烈建议你自己阅读它们。因此，除了与我想说的内容相关的作者的基本结果之外，我什至不打算尝试总结它们。</p><p> IMO 他们最终证明，对于 ChatGPT 和各种开源法学硕士：</p><ol><li>所有语言模型都会记住预训练集中的大量内容：作者估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(10\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>模型的压缩大小专门用于此（正如他们所指出的，这听起来很浪费，而且我的粗略计算表明，这可能会使记忆的材料成为预训练集的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(0.1\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ）。在较新/较大的模型中，这一比例似乎更高[作者提出了可能的原因]。</li><li>可以通过各种方法从模型中恢复这种记忆的材料（以大约 20 个单词为单位的不同大小的片段，在这种规模下足以保持唯一性，最多大约一页）。就 ChatGPT 而言，该模型似乎已经过 RLHF 训练，以阻止其返回这些数据，但作者发现了一种巧妙的攻击，使其以更高的速率返回数据 [OpenAI 似乎已经修补了这种特定的攻击，当我试图重现它时。]他们评论说，这表明很难知道你是否已经设法使法学硕士安全，特别是当试图训练它不使用它所不具备的能力时。在预训练期间学到的——这似乎是调整任何包含法学硕士的人工智能的重要观察结果。他们的一些方法是有针对性的，需要从记忆的文档块开始，让模型继续它（逐字，或者至少在几个单词之内），而 ChatGPT 攻击是一次钓鱼探险，对内容的控制非常有限。你抓住了。作者给出了他们为 ChatGPT 检索到的一百个大块。对我来说，有些看起来很可能在预训练集中出现多次的文本（用英语写出的连续顺序的小整数列表、国家名称和代码列表、合同样板），而其他看起来非常平凡，比如随机-为互联网选择的页面（如果在尝试对预训练集进行重复数据删除的任何过程中发生错误，这些当然也可能会被重复）。</li></ol><h2>对版权诉讼的影响</h2><p>大量与法学硕士和传播模型相关的版权诉讼已经在法院审理。超级缩放者采取的法律立场是，在受版权保护的材料上训练人工智能模型是合理使用，类似于作者首先阅读其他作者的书籍或艺术家首先看到其他艺术家的画作，然后受到他们的风格的影响（没有实际上是抄袭）。这对于人类创造者来说是例行公事，知识产权法也承认这一点。他们还声称，法学硕士的成果受到许多不同事物的影响，从版权法的意义上来说，它不是其中任何一个的“<a href="https://en.wikipedia.org/wiki/Derivative_work">衍生作品</a>”。</p><p>很少有案件得到判决，因此还没有明确的先例，但就迄今为止发生的少数裁决而言，超级标量似乎赢得了法律争论。与此相关的是，各种超标量人士（其中一些人拥有图像模型）承诺，这些模型并未接受过他们不拥有或未购买访问权限的任何版权作品的培训，再加上 OpenAI（其法学硕士几乎肯定<i>接受过</i>某些版权作品的培训）赔偿商业使用它们的客户免受知识产权所有者的任何版权索赔——即他们承诺支付客户的法律费用和任何罚款（我收集的费用是为了换取参与诉讼）。值得注意的是，尽管 Anthropic 支持类似的法律理论，但他们还没有选择兑现这一承诺。</p><p>到目前为止，这些案件中没有一个案例涉及模型背诵属于原告的受版权保护作品的页面的重要部分，然后被发现逐字（或几乎逐字）背诵的情况。然而，本文明确表示，这种情况迟早<i>会</i>发生（如果原告有熟练的技术帮助，则更快）。一个普通的受版权保护的作者可能已经发表了几千页的材料：如果这些都在预训练集中并且模型记住了其中的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(0.1\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span></span></span></span></span></span> ，那么这些内容加起来可能有几页。如果是这样，足够详尽的努力可能能够恢复其中的一个或多个（特别是如果他们能够访问基本模型）。在这一点上，声称法学硕士的成果不是衍生作品似乎是一个<i>极其</i>难以辩护的法律立场。辩方可以尝试声称该摘录是“合理使用”的摘录，但一般来说，除了模仿等少数情况，期望读者知道发生了什么，你甚至不允许将摘录冒充为自己的摘录工作：你应该将其归功于原作者/版权持有者。因此，关于如何使用合理使用摘录以使其保持合理使用，存在多种条件，并且如果法学硕士吐出一大块受版权保护的内容如果不明确这就是它刚刚所做的事情，那么就没有任何适当的措施可以使这些法律要求的事情真正发生。</p><p>因此，既然这篇论文已经发表，如果原告幸运并且有足够的技术帮助来复制这些技术，他们就可以证明该模型已经记住了他们受版权保护的作品的一部分，并且他们处于事实上能够说服它反刍它。他们仍然不太可能证明的是，以前在正常使用过程中确实这样做过，或者他们因此损失了任何钱。但他们的律师肯定会要求超级用户的律师告诉法庭正在采取哪些措施来确保这种情况<i>不会</i>发生，而他们得到的唯一答案基本上是“复杂的技术努力，以降低这种情况的可能性，但并非不可能” ，也无法检测到它是否确实发生了”。法官可能会或可能不会认为这是一个令人满意的答案。 （我怀疑“我们的最新版本现在抄袭率降低了 40%！”在法庭上不会得到很好的接受。）</p><p>因此，我对此的解读是，OpenAI 和类似的超级缩放器，或者就 Anthropic 而言，可能是他们的客户，迟早可能会陷入通常所说的“大、胖、法律麻烦”。</p><p>此外，当超级大规模者输掉这样的案件时（或者可能决定庭外和解，因为在这种情况下，确实发生了他们不希望发生的事情：他们不<i>希望</i>他们的模型记住任何受版权保护的材料）从预训练集中，他们就是无法阻止），然后，除了一大笔补偿金之外，他们几乎肯定需要证明他们从模型中删除了原告知识产权中所有记忆的文本;他们已经停止了。说“下次我们从头开始重新进行非常昂贵的预训练时，我们将确保删除它”似乎不太可能解决这个问题。如果是这样，他们的选择是安装一个过滤器，过滤所有推理输出，以查找其预训练集中原告的任何知识产权（这似乎可能会让法官问“你为什么不这样做？” ），要不就想办法对模型做手术。在法学硕士中，已经做了一些覆盖事实知识的工作，例如<a href="https://www.alignmentforum.org/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model">ROME</a>技术[由于某种奇怪的原因，很多这项工作似乎是在中华人民共和国的大学完成的……]——这种方法也可能是能够覆盖记忆的文本，可能会牺牲一些模型质量。</p><h2>一个大的实际问题</h2><p>因此，这对于超级大规模企业（实际上也是预训练开源基础模型的规模较小的公司）来说似乎是一个棘手的问题。最明显的解决方案是仔细过滤掉预训练集中的所有受版权保护的材料（或者至少，努力尝试，记录事实，如果有漏掉的内容，请告诉法官“哎呀！抱歉，我们犯了错误……”）。受版权保护的材料<i>应该</i>有版权声明，如果没有，并且您最终没有发现它，法官<i>可能会</i>认为这是一个可以理解的错误，但据我所知，这是一个意外删除的行为版权声明实际上并没有删除作为一项合法权利的版权。</p><p>然而，这种显而易见的解决方案对超级规模者来说非常没有吸引力：许多受版权保护的材料作为 LLM 预训练材料非常有价值。它往往是经过精心编辑、内容丰富、高质量的材料，人们投入了大量的时间和精力（与互联网上免费赠送的材料不同）。像大学教科书这样的东西包含大量非常高质量的技术信息（这就是它们昂贵的原因）。科学论文也是非常有价值的预训练数据，并且（技术上）一旦实际发表，版权就归期刊出版商和作者所有。因此，超级缩放者有非常强烈的动机在他们的预训练集中包含大量（至少是选定的）受版权保护的文本，作为他们的模型学习的重要材料。他们只需要确保他们的模型不会记住然后抄袭其中的大部分内容。</p><p>请注意，这是一个真正的一致性问题，而不是能力问题：法学硕士通过抄袭违反了法律，每个参与其中的人都希望阻止他们。这样做是对齐中的一个技术问题：一个相当简单的玩具模型，其中存在更严重的对齐问题和更严重的后果。我们需要让模型不做一些很难即时识别的事情，因为它的详细定义（预训练集中的版权材料集）很复杂，并且涉及不明显的因果/历史因素（版权法、作者的日期）死亡之类的事情）。</p><p>更加仔细地检测和删除预训练集中的重复项（例如，不包括同一教科书的两个不同版本）可能有助于降低记忆率。该论文的作者还建议避免预训练模型超过一个时期。然而，从当前 LLMS 记忆<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(0.1\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>预训练集的状态到零是不可能的。因此，完全确定法学硕士没有记住预训练集中的<i>任何</i>受版权保护的样本似乎并不切合实际。</p><p>您大概可以使用论文作者使用的相同技术来识别其中一些记忆的摘录，但他们实际上只找到了其中的一小部分（然后使用奇特的统计数据来估计比例，以便他们可以估计总数）。他们还表明，一些记忆的摘录比其他摘录更容易让模型反省，因此找到<i>所有</i>这些摘录是不切实际的。</p><p>一个昂贵的暴力解决方案是构建一个后缀数组索引，其中包含您在预训练集中使用的所有已知版权文本，并运行在此之后的所有推理输出，然后如果检测到超过<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N 个</span></span></span></span></span></span></span>匹配的运行，则停止推理令牌（ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span>可能在 30-50 范围内）。这可能很昂贵（该论文的作者做了类似的事情，并且花了他们几周的时间），并且成本与您正在进行的推理量成正比。</p><h2>条件预训练</h2><p>我最近发布了<a href="https://www.lesswrong.com/posts/JviYwAk5AfBR7HhEn/how-to-control-an-llm-s-behavior-why-my-p-doom-went-down-1">如何控制 LLM 的行为（为什么我的 P(DOOM) 下降了）</a> ，其中讨论了另一篇论文：<a href="https://arxiv.org/pdf/2302.08582.pdf"><i>根据人类偏好预训练语言模型</i></a>。这篇论文和我的帖子都讨论了一种有前途的新对齐技术，称为条件预训练（或“来自人类反馈的预训练” <span class="footnote-reference" role="doc-noteref" id="fnrefg85zm994ii"><sup><a href="#fng85zm994ii">[1]</a></sup></span> ）。正如我最近的文章标题所示，我对这项技术感到非常兴奋 - 如果您还没有阅读这两篇文章，请阅读。正如我在那篇文章中概述的那样，这种对齐技术可能适用于从人类学习的欺骗、犯罪或各种其他不对齐行为模型的复杂性，这些模型将不可避免地从人类和/或虚构的坏例子中学习。</p><p>这种对齐技术也非常适合这个问题：事实上，在这里应用它几乎是微不足道的。这就是您所要做的：</p><ol><li>向标记生成器添加一个新标记，代表一个新的特殊标记，我将其称为 &lt;|copyright|/>;</li><li>对于预训练集中所有已知的受版权保护的材料，使用标准 NLP 技术来识别文本中的句子中断、代码或诗歌中的换行符以及其他任何内容中类似的频繁周期性断点，并插入 &lt;|copyright|/>; 标签在每个文档中（加上文档的开头和结尾），贯穿预训练集中每个文档的每个受版权保护的部分。</li><li>下次您重新预训练基本模型以更新它（很可能更新到新的知识截止日期）时，请使用预训练集中所有已知受版权保护的材料的此标记版本。</li></ol><p>现在，如果模型记住了预训练集中的一段文字，然后反省它，则该段落将在每个句子/行之间包含 &lt;|copyright|/>; 标签，因为这些都是它记忆的文本中的。</p><p>然而，该模型也可能将某些写作/内容风格（在预训练集中该风格的材料中明显比例有 &lt;|copyright|/>; 标签的内容）与有时存在 &lt;|copyright 相关联。 |/>; 标签位于所有句子之间/所有行的末尾，因此它有时可能会在其创建的常规未记忆代中生成这些标签。即，它可能会产生幻觉，认为某些东西受版权保护，而实际上是它自己的作品。</p><p>现在，您有两种在推理时使用的模式选择：</p><ol><li> [便宜，可能质量较低] 在推理过程中将 &lt;|copyright|/>; 视为禁止令牌：计算它的 logit 值，然后在选择要生成的令牌之前将其重置为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-∞"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">∞</span></span></span></span></span></span></span></span></span> （因此概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">为 0</span></span></span></span></span></span></span> ）。这迫使模型永远不会生成 &lt;|copyright|/>; 标签/令牌，因此它只能生成它认为不受版权保护的文本。因此，它将无法反刍任何记忆的受版权保护的块：在它可能尝试反刍的每个句子/行的末尾，它的尝试将被打乱。 （实际上，在偏离轨道之前，它偶尔可能会设法写出 2 个甚至 3 个句子——我们应该对此进行实验和测试。）即使偶尔发生这种情况，这也是一段足够短的抄袭，可能会被认为是抄袭。对于原告来说，这是一个很难在法庭上获胜的案件，并试图对如此短的内容提出某种“合理使用，无需归属”的论点可能会更成功。更常见的是，法学硕士最多会发出一个句子，然后偏离轨道，这是一个足够短的块，法律风险应该很小（即使我们不走运并且句子本身非常具体）。或者，在一些最相似的文档受版权保护而另一些则不受版权保护的样式/上下文中，如果模型现在即将产生幻觉，认为它将生成的内容受版权保护，因此接下来会生成一个 &lt;|copyrigh|/ >; 标签，它被迫“重新滚动”，而是开始一些它不会幻觉为受版权保护的东西。如果几乎所有相关的预培训材料都受版权保护，或者如果相关受版权保护的材料始终比相应的无版权材料质量更好，那么这可能会导致法学硕士产生的文本质量比其他材料更差（如果我们没有这样做的话） t 禁止了 &lt;|copyright|/>; 标签）。通过监视 &lt;|copyright|/>; 标记生成的 logits 并查看它们何时达到显着水平，您可以检测文本中这两件事之一发生的时间和位置，但它不会告诉您哪一个发生其二是由于：真实记忆的材料或幻觉的版权。</li><li> [昂贵，高质量] 生成 &lt;|copyright|/>; 标签，然后在将输出发送给用户之前将其过滤掉。如前所述，构建运行成本较高的后缀数组索引来过滤预训练集中已知受版权保护的材料，但现在您只需要实际使用它来检查推理输出中的材料（如果已标记） &lt;|copyright|/>; 标签之前或之后。这应该会将此过滤过程的成本降低<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(90\%–99\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">90</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: 0.004em; padding-bottom: 0.298em;">–</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">99</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ，具体取决于这些标签生成的频率。 （您也许可以通过微调来减少幻觉的 &lt;|copyright|/>; 标签生成频率，但这可能会带来模型反省没有标签的记忆块的风险 - 需要研究。结果可能取决于，比如说，哪些层在微调期间被冻结/解冻，或者是如何完成的：理想情况下，您仍然希望在记忆的材料中反省 &lt;|copyright|/>; 标签，但较少产生幻觉。这些可能涉及不同的电路两个过程，所以如果你的可解释性足够好，或者使用类似<a href="https://rome.baulab.info/">ROME</a>的东西，你也许能够识别这些，或者只是尝试试错哪些层做什么，或者想出一些巧妙的微调方法维护记忆文本块中的 &lt;|copyright|/>; 标签。）此外，您将逐渐开始识别预训练集中的哪些材料被记忆，因此您可以 a）解决预训练过程中导致这种情况发生的问题，也许还 b) 优化昂贵的过滤过程。</li></ol><p>请注意，如果这两种推理模式之间的价格/质量差异很大，您可以向用户收取不同的费用。或者，您可以使用某些上下文来智能地选择将哪个上下文用于哪个查询。</p><p>如果有法律需要，这一策略还可以扩展到标记诸如商标、公司名称或角色名称之类的东西，这些东西不寻常且具体，以至于模型在虚构的环境中重复使用它们会很尴尬（“阿不思·邓布利多”来自去提醒）。</p><h2>为什么这是解决该问题的理想对齐技术</h2><p>有一些对齐技术，例如<a href="https://www.lesswrong.com/tag/activation-engineering">激活工程</a>，在残余嵌入空间（在某些层）中工作，或者像<a href="https://www.lesswrong.com/tag/causal-scrubbing">因果清理</a>这样在单个神经回路级别工作，或者像<a href="https://rome.baulab.info/">罗马</a>这样在少量神经回路中工作。因此，每个参数可能影响的事物集合的复杂性将受到这些表示的维度的限制。预训练集中所有受版权保护的文档的全部内容的复杂度远远高于其中任何一个（事实上，比整个模型还要大：即使实际记住的内容也是整个模型复杂度的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(10\%)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">%</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ） 。显然，这些方法都无法可靠地解决这个问题。因此，<i>唯一</i>可以可靠地实现这一点的对齐方法是在预训练期间以极细粒度的级别对模型进行监督的方法：例如，对于预训练集中每个文档的每个句子。即这种对齐技术（或非常类似的技术）是解决这个问题的<i>唯一</i>方法。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fng85zm994ii"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg85zm994ii">^</a></strong></sup></span><div class="footnote-content"><p>该论文的作者将这种技术称为“人类反馈预训练”（PHF）——但是这个名称并不能完全描述我在这里提出的技术的具体用例，除非“不要引用这个，这是版权！”被认为是“人类反馈”。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/zcFajcqbdysp8be7C/language-model-memorization-copyright-law-and-conditional#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zcFajcqbdysp8be7C/language-model-memorization-copyright-law-and-conditional<guid ispermalink="false"> zcFajcqbdysp8be7C</guid><dc:creator><![CDATA[RogerDearnaley]]></dc:creator><pubDate> Thu, 07 Dec 2023 06:14:13 GMT</pubDate> </item><item><title><![CDATA[Reflective consistency, randomized decisions, and the dangers of unrealistic thought experiments]]></title><description><![CDATA[Published on December 7, 2023 3:33 AM GMT<br/><br/><p> Ape 最近在<a href="https://www.lesswrong.com/posts/8rWP3ZvHe2HkQd53o/anthropical-paradoxes-are-paradoxes-of-probability-theory">《人类悖论是概率论悖论》</a>上发表的一篇文章重新审视了 Eliezer Yudkowsky 在<a href="https://www.lesswrong.com/posts/ZTEkZNLrmycNuCNYq/outlawing-anthropics-an-updateless-dilemma">《取缔人类：一个更新的困境》</a>中提出的一个老问题。这两篇文章都旨在表明，标准概率和决策理论在相当简单的情况下给出了错误的结果（事实证明不需要涉及任何“人择”问题）。特别是，他们指出使用标准方法时存在“反思性不一致”，即事先同意最佳策略的代理会改变主意，并在以后做一些不同的事情。</p><p>穿外套的猿通过放弃标准概率论来解决这个问题，转而采用一种方案，在该方案中，一个人可以同时对同一事件持有两个不同的概率，以便在做出不同的决策时使用。埃利以泽的决议是放弃标准决策理论，转而采用一种理论，在该理论中，代理人的行为“就好像控制所有类似的决策过程，包括它们自己的所有副本”。</p><p>在这里，我将捍卫一些接近标准贝叶斯概率和决策理论的东西，唯一的扩展是随机决策的使用，这是博弈论中的标准，但传统上在贝叶斯决策理论中被认为是不必要的。</p><p>我还想指出，以对分析至关重要的方式设计完全不切实际的思想实验的危险，以及一旦遇到以下情况就确定标准方法有缺陷的做法是不可取的：足够棘手，以至于您在分析时会犯错误。</p><p>这是问题所在，在没有不相关的人为方面的表述中：</p><blockquote><p>二十个人参加一项实验，以相同的概率随机选择两个瓮中的一个，然后这 20 个人中的每人从所选的瓮中随机取出一个球，并保留它，而不向其他人展示。其中一个瓮包含 18 个绿球和 2 个红球。另一个瓮包含 2 个绿球和 18 个红球。</p></blockquote><blockquote><p>然后每个拥有绿球的人决定是否参加投注。如果<strong>所有</strong>绿球持有者都决定参加下注，那么 20 个人中，每持有绿球的人总共赢 1 美元，每持有红球的人输 3 美元。最后的总胜负由20人平分。如果一个或多个持有绿球的人决定不下注，则不会赢或输钱。</p><p>这二十个人可以事先讨论采取什么策略，但实验开始后就无法交流，也不知道其他人对赌注做了什么选择。</p></blockquote><p>显然，最佳策略是不下注。如果所有绿球持有者都决定下注，则预期赢得的金额为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(1/2)\times12+(1/2)\times(-52)=-20"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>美元，因为 18-绿/2-红和 2-绿/18-红瓮都有 1/2 被选中的概率，前者导致赢得<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="18-2\times3=12"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">18</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span></span></span></span></span></span>美元，后者导致赢得 12 美元损失<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="18\times3-2=52"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">18</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span></span></span></span></span></span>美元。由于下注的预期回报为负，而不下注的回报为零，因此最好不下注。</p><p>但是，当所有玩家都同意不下注之后，如果他们碰巧选择了一个绿球，他们会怎么想？他们（也许所有人）最终会改变主意并决定接受赌注吗？这将是一种反思不一致的情况，并且也会让他们损失金钱。</p><p>根据标准贝叶斯概率论，绿球持有者判断所选择的瓮是有18个绿球的瓮的概率为9/10，只有2个绿球的瓮的概率为1/10。在看到他们挑选的球之前，这两个瓮的可能性相同，但之后，有 18 个绿球的瓮的可能性比有 2 个绿球的瓮高 9 倍，因为观察“我拿着一个绿球”的概率是第一个瓮 (18/20) 比第二个瓮 (2/20) 高 9 倍。</p><p>因此，在选球后，选绿球的人应该计算出，如果所有绿球持有者都决定下注，则预期收益为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(9/10)\times12+(1/10)\times(-52)=5.6"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">9</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5.6</span></span></span></span></span></span></span> ，这是正数，因此比不下注的零回报要好。因此，看起来存在反思性的不一致，这将导致他们改变主意，正如我们上面计算的那样，他们平均会亏损。</p><p>但这是一个错误的分析。<i>所有绿球持有者决定下注的</i>预期回报与单个绿球持有者做出决定无关。绿球持有者可以确保<i>不</i>下注，确保预期收益为零，因为下注的决定必须是<i>一致</i>的，但单个持有绿球的人不能单方面确保<i>下注</i>。决定下注的单个绿球持有者的预期回报取决于决定下注的其他每个绿球持有者的概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，并计算出（假设独立）为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(9/10)\times12\times p^{17} + (1/10)\times (-52)\times p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">9</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">17</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，我将其称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> 。</p><p>如果 20 个人事先都同意不下注，那么其中一人认为其他人下注的概率很小似乎是合理的。如果我们设置<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> ，则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E(p)=0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> ，因此一个持有绿球的人决定下注的预期回报为零，与决定不下注的预期回报相同。因此他们没有动力背离先前的协议。不存在反射不一致。</p><p>然而，实际上，持有绿球的人至少有一定的小概率会背离先前的协议，不下注。这是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的关系图： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/w7ucadx2brzk3verorqn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/ogbaxn0ogtn1iq0af7zx 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/otej47xuwuv35vljmmgp 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/p88voat7v7sbjy53qh6o 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/wnzgmb9evl2taf8idubo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/ed07fmugp7aooosxqbky 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/nqs5hctd7qv51uskp13d 480w"></figure><p>可以看出，仅当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的值非常接近 1 时<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E(p)"><span class="mjx-mrow" aria-hidden="true">， <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>才为正。因此，对于其他人可能会做什么的广泛信念，持有绿球的人不下注的贝叶斯决定是稳健的。而且，如果一个人确实有充分的理由认为其他人（非理性地）决定下注，那么自己决定下注实际上是正确的策略，因为那时你实际上决定了是否下注被采用，并且所选择的瓮中有 18 个绿球的概率为 9/10 使得赌注成为好赌注的论点实际上是有效的。</p><p>那么穿外套的埃利以泽和猿猴是怎么犯这个错误的呢？原因之一可能是他们太容易接受标准方法是错误的，因此在发现上述分析之前没有继续思考。然而，另一个原因是，他们似乎都认为所有拥有绿球的人都是同一个人，做出了一个决定，而不是可能做出或可能不做出相同选择的个人。也许认识到这根本不是事实，Eliezer 说，“我们可以尝试通过提供诸如“如果给出相互矛盾的答案，每个人都会损失 50 美元”之类的规则来帮助因果决策代理解决协调问题。”</p><p>但这样的强制实际上并没有什么帮助。通过简单地否定这个问题中的奖励，我们就可以很容易地看出这一点：假设如果下注，二十个人中每一个拥有红球的人将赢得 3 美元，而每一个拥有绿球的人将输掉 1 美元。现在，如果每个持有绿球的人都下注，则预期回报为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="+20"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span>美元，而不是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-20"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span>美元。那么每个人都应该事先同意如果拿到绿球就接受投注吗？</p><p>不，因为这不是最佳策略。他们可以通过随机化他们的决定来做得更好，同意如果他们选择一个绿球，他们将以某种概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>下注。遵循这种策略时的预期奖励是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(1/2)\times(-12)\times p^{18} + (1/2)\times 52 \times p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">18</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ，我将其称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> 。这是该函数的图： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/tggc4pcfo5gccrcgdxwd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/pfsv5lvgldrta0izeofu 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/kv6kcuo6nactusqhuf5g 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/owbgarjjtvjcautqpeez 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/hb4wsmornllpybbdbpfw 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/j4rjhbgxcqjlmoyqe7r7 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/ejmtxuvfnzjc46hx6rxo 480w"></figure><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的最优选择不是 1，而是 0.9553，其预期奖励为 21.09，大于每个绿球持有者总是下注的朴素策略所获得的值 20。</p><p>这个策略在反思上是否一致？和以前一样，一个有绿球的人会以 9/10 的概率认为有 18 个绿球的瓮被选中。如果他们认为其他人将以<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的概率下注，那么他们自己下注的预期回报将为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(9/10)\times(-12)\times p^{17} + (1/10)\times 52\times p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">9</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">12</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">17</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">52</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，我将其称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> 。其绘制如下： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/giaiexvrxe2vtizpocgv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/zxkecvqbwqbmiesk1xbs 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/ydsqv3ianspnlsorgk3h 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/kuf8iaamhoxrc9rhovf2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/qnn55hl8fgxthgakxtpj 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/ykra8an2nilvuwtumtla 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aSXMM8QicBzTyxTj3/v7honauhbhnazfmtho1n 480w"></figure><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>只是先前绘制的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>函数的负值。它在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=0.9553"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.9553</span></span></span></span></span></span></span>时为零，此时<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R(p)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>达到最大值。因此，如果一个拿着绿球的人认为其他人正在遵循商定的策略，以概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=0.9553"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.9553</span></span></span></span></span></span></span>进行下注，那么他们会认为无论他们是否下注都没有区别（就预期奖励而言）。因此，他们不会有动力偏离商定的策略。</p><p>尽管这一论点表明贝叶斯决策理论与事先制定的最优策略是<i>兼容的</i>，但令人失望的是贝叶斯结果并不<i>建议</i>遵循这一策略。当两个行为 A 和 B 具有相同的预期奖励时，贝叶斯决策理论认为，无论选择 A、选择 B，还是决定以任何概率随机选择 A 或 B，都没有区别。因此，对于像这样随机化有帮助的情况，对贝叶斯决策理论进行一些阐述似乎是有用的。</p><p>针对该问题的这种变化的最佳随机化策略与认为该实验中的所有人应该始终以相同的方式行事（就好像他们是彼此的精确副本一样）的想法是不相容的。当然，真实的人并不总是以相同的方式行事，即使他们拥有相同的信息。</p><p>人们可能会想象“人”是计算机程序，它们可能被编写为确定性的，因此在给出完全相同的输入时会执行完全相同的操作。但这种思想实验基本上没有涉及真实的人。它也没有过多说明假设的未来人是计算机程序，因为没有理由一个人会使用完全相同的输入多次运行这样的程序。一个例外是，如果这种重复是一种冗余形式，以防止硬件错误，但在这种情况下，将冗余执行路径视为单独的人似乎很奇怪。将有关意识和个人身份的未解决的哲学问题引入概率论和决策论中基本上是一个平凡的问题似乎是无利可图的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/aSXMM8QicBzTyxTj3/reflective-consistency-randomized-decisions-and-the-dangers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/aSXMM8QicBzTyxTj3/reflective-consistency-randomized-decisions-and-the-dangers<guid ispermalink="false"> aSXMM8QicBzTyxTj3</guid><dc:creator><![CDATA[Radford Neal]]></dc:creator><pubDate> Thu, 07 Dec 2023 03:33:16 GMT</pubDate></item></channel></rss>