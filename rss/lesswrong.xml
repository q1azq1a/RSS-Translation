<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 6 日星期三 06:15:50 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Digital humans vs merge with AI? Same or different?]]></title><description><![CDATA[Published on December 6, 2023 4:56 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sat, 02 Dec 2023 18:49:29 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sat, 02 Dec 2023 18:49:29 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>因此，为了创造背景，这是我们在评论中的评论的延续：https://www.lesswrong.com/posts/je5BwKe8enCq8DLrm/ai-40-a-vision-from-vitalik </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sat, 02 Dec 2023 18:50:25 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sat, 02 Dec 2023 18:50:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>和我询问</p><ol><li><i>人类与人工智能融合</i>和<i>数字人类</i>之间的界限（这些方法能否可靠地区分彼此？或者是否存在很大的重叠？）</li><li>为什么数字人类是比合并更安全的选择</li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sat, 02 Dec 2023 18:56:52 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sat, 02 Dec 2023 18:56:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>因此，如果我们异步进行，好的起点可能是要求您详细说明第一项：有什么区别，是否存在重大重叠，一个概念是否几乎是另一个概念的子集。</p><p>您如何看待数字人类和“混合体”概念之间的关系（如果这就是合并）？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 21:39:20 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 21:39:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>所以我的想法是，数字人类是一个相当狭窄和精确的定义。对人脑的模拟（不一定基于特定的人，这个概念更具体，上传）。</p><p>这种大脑模拟只能通过基于对人类神经元的详细观察的高度准确的规则集来行动和修改。例如，它不能使用外部过程来监视或调节其模拟神经元的活动。除了通过相同的过程和达到与正常生物人脑相同的程度之外，它无法添加更多的神经元。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 21:48:56 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 21:48:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>另一方面，与人工智能的融合是一个更加开放的概念。很多事情都属于这个标题。一些核心示例可能包括：</p><p>一种脑机植入物，允许人工智能系统对人脑进行读取和/或写入访问。</p><p>一种人工智能系统，可以非侵入性地控制一组人类接收的大部分感官输入，以高带宽方式向他们提供信息并接受他们的指令。</p><p>数字人与人工智能进行高带宽通信，通过正常的模拟感官方法或通过能够直接读取和/或写入神经元活动的人工智能。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 21:55:26 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 21:55:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>这里的关键部分是人工智能，它正在与人类融合。它不一定是危险的或压倒性的。但也有可能是这样。</p><p>问题在于，扩大“合并”型系统的力量的最简单方法是扩大人工智能的力量和影响力。</p><p>数字人类由于受到生物规则集的限制，其能力扩展的方式受到限制。相对于生物人类来说，它仍然可能拥有显着的超能力。不受衰老或生物疾病的影响，能够以互联网数据传输的速度旅行，能够非常便宜且快速地克隆自己，廉价地保存备份并兼作检查站，以完全感官保真度体验虚拟世界，并以时钟速度比生物大脑高得多。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 22:06:07 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 22:06:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>那么，我预见到与人工智能密切相关的系统或以数字人类开始但不遵守生物学规则的系统会存在什么危险呢？</p><p>问题在于，我认为更宽松的数字实体景观有多种不同的滑坡，导致非常糟糕的结果。</p><p>其他地方详细讨论的一个经典例子是，允许任意自我修改的数字人类可能会陷入困境，而没有意识到情况有多棘手，这实际上结束了他们与世界的互动。</p><p>另一个典型的例子是竞争经济中的数字人类，他们感到有压力去改变自己，以更有动力工作。这可能会导致谋杀甘地式的滑坡，每个新版本的数字人相对较少关心非工作事物，因此选择进一步增加工作欲望并减少对其他任何事情的关心。沿着这条斜坡下去，就是人性的丧失。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sat, 02 Dec 2023 22:42:19 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sat, 02 Dec 2023 22:42:19 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>是的，我当然同意这一切。主要症结在于</p><p>>;这种大脑模拟只能通过基于对人类神经元的详细观察的高度准确的规则集来行动和修改。例如，它不能使用外部过程来监视或调节其模拟神经元的活动。除了通过相同的过程和达到与正常生物人脑相同的程度之外，它无法添加更多的神经元。</p><p>基本上，数字人类当然可以做生物人类可以做的所有事情，包括相当于促智药增强、认知策略、迷幻头脑风暴等。</p><p>但这个数字人类肯定会意识到，打破规则并直接对其架构进行黑客攻击可能会带来多个数量级的增强。</p><p>因此，我们需要依靠数字人类的结合，承诺不走这条路，并在巨大的诱惑下光荣地履行承诺，而技术手段使得打破这一限制特别困难。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:02:57 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:02:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>是的，我不确定我们能否成功地阻止数字人类违反生物规则，但我认为值得尝试。正如我认为值得努力让人工智能充分像工具一样并受到控制，使其没有机会接管，这样我们就不会无意或有意地设计一个具有情感和道德价值的智能数字实体然后我们必须做出决定，是给予它对人类构成危险的权利，还是继续压迫和奴役它。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:04:04 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:04:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>这两条谨慎的道路似乎都伴随着安全税，这需要监管和执法来防止人们跳过。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:06:25 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:06:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>然而，我认为“让人类和人工智能融合”的提议是一个模糊的提议，旨在不干涉、不受控制地争夺权力。让人类以任何方式在没有监管的情况下与人工智能连接起来，似乎你需要进行大量的实验，其中一些似乎可能会在由此产生的协作获得力量方面发挥作用。但就由此产生的维持或维护人类价值观的合作而言，这并不奏效。 </p></div></section><p></p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sat, 02 Dec 2023 23:30:35 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sat, 02 Dec 2023 23:30:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>是的，即使人们试图非常小心，并且仅在人类和数字系统之间使用非侵入性 BCI（这是我通常考虑的合并形式），安全问题仍然是巨大的，无论是在参与人类的直接安全方面，但更重要的是，可能会产生什么样的混合实体（即使我们坚持解耦、断开、长时间暂停、稍后重新连接的能力，并不断重复这种“断开-中断-重新连接”周期，这是坚持的合理事情，但不确定性仍然很高）......</p><p> ***</p><p>另一方面，与 Neuralink 不同，非侵入式 BCI 的进展可能相当快，并且在时间安排上可能与纯人工智能方法具有竞争力……</p><p>尽管“纯大脑模拟”方面的进展在未来可能还很遥远，除非成功地发明了一种“加速速度比我们实际绘制大脑的能力更快”的方法…… </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:37:52 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:37:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>是的，这是通往受生物规则约束的基于精确大脑模拟的数字人类之路缓慢的一部分，这让我更有信心这条道路更安全。我们有更多的时间来考虑如何规范数字人类并致力于安全约束。</p><p>但也正是出于这个原因，我认为数字人类不会帮助我们度过近期的棘手时期，即人工智能变得足够强大，足以在灾难中发挥作用。这些灾难可能来自人类的滥用或人工智能的指导行动。</p><p>另一方面，我可以理解有人可能会说，“允许人类与人工智能合并的短期尝试可能会给我们一个强大的人工智能与人类团队来解决监管不充分的人工智能的安全问题！这比数字人类！它可能非常强大，但也会有‘人类’元素。”</p><p>我对此的回应是，“我不相信拥有一些非零的人为因素足以使系统安全，即使参与实验的人一开始似乎是值得信赖的。人类的良好行为似乎是一种脆弱的东西，这绝对是一场不合格的火刑审判。” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:42:29 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:42:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>因此，如果有人有兴趣追求数字人类和人类上传（例如来自冷冻保存的大脑）的长期目标，我并不反对。这似乎并没有在短期内使人类面临的风险变得更严重，而且在这些技术人员准备好解决监管和安全问题之前，我们似乎有足够的时间。</p><p>然而，如果资助者正在决定将资源分配给数字人类和上传路径，还是直接解决人工智能安全问题……我会敦促他们为人工智能安全做出贡献，因为我认为这个问题既更紧迫，也更重要。在相关时间范围内更容易处理。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:45:34 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:45:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>另一个方向是智力增强。在这里，我相信生物人类智力的提高有很大的能力，但大多数高影响力的非人工智能合并选项都需要更多的研究才能准备好。例如，我在神经科学领域研究的课题：对同意的成年人进行基因改造以实现彻底的智力增强。我认为这可能需要比数字人类更长的时间，而且几乎可以肯定与未来 10 年无关。我绝对认为科学界应该尽其所能帮助人类度过未来 10 年，因为我认为我们正面临人工智能引发的灾难的巨大危险。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:48:29 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:48:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>这也适用于非计算机科学家。例如，生物学家可以通过提高社会检测、预防和制止生物工程流行病的能力来提供帮助。人工智能使生物工程流行病变得更容易、更有可能发生，因此，通过帮助保护社会免受此类流行病的侵害，您正在帮助降低人工智能灾难风险。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:53:29 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:53:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>就人类人工智能高带宽团队而言，例如非侵入式 BCI，我认为可以从中安全地获得潜在有用的帮助。然而，需要注意的是，该系统应该受到高度不信任的对待，并保持较高的安全标准。人类人工智能团队的科学发现只有在能够得到非人工智能增强人类的充分验证的情况下才应该被信任。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sat, 02 Dec 2023 23:56:53 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sat, 02 Dec 2023 23:56:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>因此，我认为需要适用于人工智能的同样的监管也应该适用于人类人工智能团队。</p><p>限制它，不要让它传播/复制。将其保留在沙箱中（可能使用更新的互联网缓存副本，沙箱仍然可以允许信息向内流动。）</p><p>不要让它获得自己的权力和资源。</p><p>未经验证，请勿相信其输出。</p><p>不要让它自我修改或构建新颖的人工智能系统。 （仅仅因为你控制了第一代，并不意味着第一代无法构建足够强大的第二代来摆脱你的控制。）</p><p>不要让它落入坏人手中。 （团队中的人工智能部分可以进行修改，例如通过微调。团队中的人类部分可以通过洗脑和酷刑等方式被说服与不道德的目标合作。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dek8cgeDxxrtojkgK-Sun, 03 Dec 2023 03:22:27 GMT" user-id="dek8cgeDxxrtojkgK" display-name="mishka" submitted-date="Sun, 03 Dec 2023 03:22:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>米什卡</b></section><div><p>是的，这是有道理的。</p><p> >;就人类人工智能手带宽团队而言，例如非侵入式 BCI，我认为可以从中安全地获得潜在有用的帮助。</p><p>是的，事实上，主要用例是科学研究，特别是人工智能安全研究，这在很大程度上必须由人类-人工智能团队完成才能完全可行。</p><p>但需要非常谨慎，特别是</p><p>>; 然而，需要注意的是，该系统应该受到高度不信任的对待，并保持较高的安全标准。人类人工智能团队的科学发现只有在能够得到非人工智能增强人类的充分验证的情况下才应该被信任。</p><p>非常适用（需要注意的是，什么算作“完全验证”取决于计算机科学可能能够设计的程序以使其可行......例如，我想朝着“零知识证明”的方向示意，不是指它们在字面上适用，而是指可以找到大约本着这种精神的一些解决方案）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c96TaP5ZJFYPabnpH-Sun, 03 Dec 2023 18:29:30 GMT" user-id="c96TaP5ZJFYPabnpH" display-name="Nathan Helm-Burger" submitted-date="Sun, 03 Dec 2023 18:29:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>内森·赫尔姆·伯格</b></section><div><p>关于“完全验证”的观点很好，我应该说更像是“概率验证到与实施建议的创新的风险相称的置信度”。由于我们基本上坐在一个带有隐藏计时器的定时炸弹上，因此我们在尝试解决问题时实际上无法保持最大限度的谨慎。</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/ZpbcvBtNMxG8v6mcB/digital-humans-vs-merge-with-ai-same-or-different#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ZpbcvBtNMxG8v6mcB/digital- humans-vs-merge-with-ai-same-or- Different<guid ispermalink="false"> ZpbcvBtNMxG8v6mcB</guid><dc:creator><![CDATA[Nathan Helm-Burger]]></dc:creator><pubDate> Wed, 06 Dec 2023 04:56:38 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund's Plan to Focus on Principles-First EA]]></title><description><![CDATA[Published on December 6, 2023 3:24 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastructure-fund-s-plan-to-focus-on-principles-first#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastruct-fund-s-plan-to-focus-on-principles-first<guid ispermalink="false"> A8L4udgpsoC2BNMnS</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Wed, 06 Dec 2023 03:24:55 GMT</pubDate> </item><item><title><![CDATA[**In defence of Helen Toner, Adam D'Angelo, and Tasha McCauley**]]></title><description><![CDATA[Published on December 6, 2023 2:02 AM GMT<br/><br/><p>匿名帖子：</p><p> “我理解 EA 或 AI 治理领域的一个普遍观点是，Toner、D&#39;Angelo 和 McCauley（简称 TDM）确实把 OpenAI 的事情搞砸了，而 AI、世界的命运等已经变得更糟了。我相信这是完全错误的：事前 TDM 已经以非凡的能力和勇气证明了自己（而不是“也许——可以理解，也许——不是大规模的搞砸）；事后，他们的成就足以证明整个人工智能的正确性”治理社区作为一个整体。</p><p>我认为：</p><p> 1) TDM 的行动使开放人工智能的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。</p><p> 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。</p><p> 3) 无论你是“荣誉和正直的最高主义者”还是“无情的战略家”，TDM 的行动通常从这两种角度来看都表现得非常出色。</p><p> （注：匿名主要是因为想要平静的生活。我没有内部消息，也没有任何狗参与战斗。关于“资历”，我不在人工智能政府工作，但在奖励战略敏锐度的领域经验丰富和丰富的愤世嫉俗，我对“到目前为止的故事”做出了一些有先见之明的呼吁。但作为一个匿名者，这只不过是“来源：相信我兄弟”，所以你不应该这样做，除非我的观点有说服力。）</p><p> **发生了什么**</p><p>我认为 Zvi 和 Gwern 在 lesswrong 上给出了最准确的描述（也是《纽约时报》的报道）。基本上：Altman 试图用刀砍 Helen Toner 来获得 OpenAIs 棋盘的控制权（即，在 3 比 2 的情况下，Altman 可以指定他的盟友来堆叠棋盘，稍后再用刀砍 McCauley，等等）。伊利亚犹豫不决，短暂地叛逃到 TDM 的“安全派”，后者随后控制了自己并解雇了萨姆。随后发生的所有事件都被广泛报道。 （我的论点依赖于这是真实的故事，所以如果你确定这不是发生的事情，你可以停止阅读）。</p><p>这表明 Altman 是一个令人讨厌的作品，绝对不是你想要的负责人工智能重大项目的人：高度权谋、追求权力，并希望消除对他对 OpenAI 应该如何发展的单方面愿景的任何检查。我以像 Reddit 董事会反向收购这样的事件为例，保罗·格雷厄姆 (Paul Graham) 的“补充”事件。他高超的企业斗刀技巧，引发了微软/谷歌法学硕士军备竞赛，并且似乎试图让董事会大部分时间都蒙在鼓里。对 GPT-4 的担忧进一步证明了这一点。</p><p>虽然我的论点不需要对 Altman 做出如此极端不利的判断，但我很困惑为什么 EA/AI 治理领域的一些人对他评价良好：有一大堆明显的负面因素，而我能看到的唯一积极因素是“有时”发出适当的声音。人工智能安全对他来说很方便”（简单地说：SBF 也更一致地发出正确的声音……）但即使 Altman 很伟大，他也试图以他们认为的借口罢免管理非营利组织的独立董事会成员。这种不利于 OpenAI 商业利益的行为是完全不可接受的。</p><p><br> **现在的情况**</p><p>萨姆（很可能）重新担任首席执行官，董事会似乎将由萨姆的 1 名盟友、德安吉洛（1 名被踢掉他的人）和萨默斯组成，萨默斯可能会起到平衡作用。塔莎、海伦和伊利亚出去了，格雷格和山姆没有回来。</p><p>将该委员会与奥特曼罢免托纳的反事实相比，奥特曼对这个委员会的控制要少得多。首先，他没有参与其中，其次，他现在有三分之一而不是大多数人明确支持他。当然，这可能会改变：我预计奥特曼会继续尝试获得董事会控制权，并且考虑到他的能力，他迟早会成功；即使没有，也许他可以安排“实际情况”，这样董事会就无法阻止他随心所欲地经营公司。或者也许微软会突然介入并从现在开始基本上掌控这一切。</p><p>但也许这些都不是，也许董事会确实会成为对奥特曼和那些乐于奔向悬崖的人的有效制衡。至少好人仍然可以在牌桌上发挥作用，如果 TDM 让 Sam 政变成功，他们就不会被有效消灭。有时这是您所能期望的最好的结果。</p><p>但真的是这样吗？还有很多其他结果看起来很糟糕，例如：</p><p> * Sam 重新担任首席执行官，并带来了许多值得称赞的公关。如果他像我想象的那样糟糕，那就不好了。</p><p> * OpenAI 工作人员一致同意 Sam 的观点。</p><p> * 反击/不信任/嘲笑/等等。反对“人工智能安全”“EA”“Decels”。 ETC。</p><p>本质上，是的：TDM 基本上没有机会避免这些缺点（所以可信的明显失误基本上是“没有伤害，没有犯规”），即使周一早上热衷于四分卫，考虑到他们所处的非常不利的位置，他们似乎也基本上尽了最大努力。已经，而且执行情况超出了合理预期。</p><p></p><p> **为什么坏事基本上已经被定价了**</p><p>当前的戏剧提出了三个主要权力块：董事会（好吧，TDM）、员工和投资者。如果推动力足够大，后两者应该提前可靠地建模，使其主要底线是“美元”而不是“使命”。这里的潜在推动力是巨大的（例如，如果股权归属，员工将一夜之间成为百万富翁，微软的数十亿美元投资）。因此，如果他们要在“让这位护身符般的首席执行官承诺让令人惊叹的美好时光持续下去”与“使命/安全担忧”之间进行投票，人们应该期望他们无论如何都会压倒性地支持前者。</p><p>但是，至少从表面上看，而且绝对是有意为之（Altman 之前对 OpenAI 奇怪的治理结构的重要性的评论在这里很讽刺），董事会最终发号施令，并且被设置为（与后两组不同）对财务压力不敏感。尽管其他各方（包括像大多数财经媒体这样的局外人）认为该章程赋予他们的权力本质上是表面上的公关或 LARP，但它确实保留了一些现实政治的牙齿：许多资源似乎都被 501(c) 所束缚。 3、虽然董事会无法阻止员工移民，也无法阻止投资者创办一家“我们无法在安全问题上GAF”的替代人工智能公司，但他们可以可信地威胁要烧毁已经存在于 OpenAI 中的大量“股东价值”，所以要斗争如果这些各方返回“内部选择”，则需要做出让步。</p><p>换句话说：董事会权力是 TDM 能够实现的唯一现实杠杆，他们（巧妙地）做到了这一点，并尽可能地利用了这一点。许多不关心人工智能安全的人——内部人士或观察家——鄙视或嘲笑他们（也许延伸到人工智能安全），因为他们按照他们认为愚蠢的事业（或反对他们的利益）行事）是开展业务不可避免的成本。首先，这些事件揭示了潜在但预先存在的动力，而不仅仅是激发或加强它们。</p><p></p><p> **事后诸葛亮的好处更好**</p><p>人类容易犯错，事后诸葛亮是20-20，因此很容易对战争迷雾中的行动过于挑剔，而一旦战争迷雾散去，这些行动就会显得愚蠢。但人们可能会犯相反的错误：如果你仔细规定他们可以或不可以“合理地预见”或“合理地引导相信”什么，你几乎可以将任何人和任何人视为杰出的英雄。</p><p>另一个挑战是有两种观点可供选择。一个是更狡猾/天真的结果主义版本：OpenAI 治理是一场现实生活中的外交游戏，因此如果 EV 足够高，TDM 应该抓住机会进行“刺杀”，等等。另一个是更有原则的“不惜一切代价荣誉”这在理性主义者中似乎很流行：所以 TDM 应该充当宪章的圣骑士，因此应该采取立场，即使他们确信这将是徒劳且适得其反的姿态，并且总体上有点守法愚蠢，作为一些预先承诺的东西，一些 UDT 的东西，这实际上是最好的多元宇宙范围的政策，等等。</p><p>这些观点通常是一致的：即使（例如）“背刺”的最佳频率不是“没有”，它最多也是“非常罕见”。事实上，TDM 表现得很好，他们的选择通常（但不是普遍）看起来介于“合理”和“模范”之间，无论你喜欢哪种方式。通过查看各种“他们为什么不 X”问题可以最好地看出这一点。</p><p></p><p> *他们为什么不给和平一个机会？*</p><p>据推测，TDM 可能会选择对 Altman 做出较小的反应，而不是将他彻底抛弃。也许他们可以把萨姆和格雷格踢出董事会，但让他继续担任首席执行官，或者他们可以利用他们的多数席位任命一群安全主义成员来阻止未来的政变企图（或两者兼而有之）。或者他们可以召开一次与萨姆的会议来讨论他的行为，希望能够解决问题，而不是他们所寻求的仓促的“反政变”。也许这会是徒劳的，但有选择价值，如果（例如）奥特曼辞职作为回应并做与他实际做的几乎相同的事情，也许会给他们一个更好的位置。</p><p>从现实政治的角度来看，这似乎极其愚蠢。即使忽略之前对奥特曼的任何担忧，政变企图也表明奥特曼是来抓你的。伊利亚的叛逃提供了唯一的机会之窗，但其持续时间不确定（事后看来，显然很短），让他成为第一。让他作为对你的利益怀有敌意的首席执行官，似乎是在恳求他尝试其他选择来事实上废黜你（例如，确保你不会就任何重大问题征求你的意见），即使你让自己免受他重复这种特定攻击的影响。即使你必须达成协议，让他重新担任首席执行官，但无论如何都没有董事会席位（参见实际发生的情况），你也必须免费参加一个机会，让你成功地用更好的人取代他，并且重新任命奥特曼是你可以在谈判中交易的一匹额外的马。</p><p>尽管有原则的人原则上可能不喜欢企业尖锐的做法，但TDM对奥特曼的刀砍是对他试图刀砍托纳的回应，这是可信的合理的——他们并没有首先与“肮脏”作斗争——而且有一种自然的正义“让我们让你尝尝”你自己的医学方面在这里。即使你认为针锋相对不合适，我认为“宪章圣骑士”的观点也会说奥特曼的即决解雇要么是合理的，要么是直接需要的。批评监督你的独立董事会成员写了一些你不喜欢你公司的内容，这已经很过分了。以此为借口将他们踢出董事会，因为她已经并将继续反对您快速商业化的愿望，并将阻止董事会有效反对您的任何能力。如果这看起来是有预谋的，并且加上试图终止董事会等的背景，则意味着董事会不应该相信首席执行官只是为了所有人的利益而开发人工智能技术。</p><p>如果TDM知道Ilya后来会后悔自己投票的结果，那么一个有原则的人可以扣掉TDM积分，所以在他能做出更好的考虑之前就催促他这么做（/让Greg的妻子劝阻他）。但这结合了许多关于 TDM 当时所做/应该想到/预见的主张，并解决了各种潜在的防御措施（即使如此）。从现实政治角度来看，TDM 遭到了（据称）最优秀的企业持刀战士之一的伏击，但最终刺伤了他的却是他们。</p><p></p><p> *为什么不解释清楚发生了什么？*</p><p>我同意这看起来像是一个错误，尽管影响不大。即使“奥特曼试图政变我们，所以我们反击了”，也不会对奥特曼/SV 媒体全场媒体产生太大的吸引力，也不会与工作人员产生太大的芥蒂（“好吧，如果是董事会或奥特曼，我们绝对更喜欢奥特曼”），这似乎比不提供任何实质性内容要好。原则性的方法是，如果你所做的事情可能会让员工损失很多钱，那么你应该向员工提供完整的解释。</p><p>可能会出现合理的解释：也许有压倒一切的原则迫使人们保持沉默，也许精明地保留“我们将签署保密协议/非贬低合同，并且在我们离开后不会进行媒体巡演”作为进一步讨价还价的筹码，也许他们（合理地） ，如果错误的话）遵循非常谨慎的法律建议或其他建议。但这似乎是错误的，在争论他们做了正确的事情的过程中，假设 TDM 做了正确的事情来解释明显错误的事情，这就引出了问题。</p><p></p><p> *他们为什么要塌陷？/他们为什么不把它全部烧毁？*</p><p>如果你确定奥特曼是个坏消息，你愿意‘满发仇恨’踢他，为什么还要让他回来？为什么不让 OpenAI 毫无原则地死在这座山上——或者至少让 Altman 在 MS 领导下的建立成为一场代价高昂的胜利，因为从 OpenAI 的尸体中寻找宝贵的人工智能资源可能会带来法律上的麻烦。为什么不竖起中指，把你拥有合法所有权的一切都赠予别人呢？</p><p>我猜这些令人头疼的问题，以及 TDM 可能会烧毁 OpenAI 的大量“资产”的可信威胁，正是让 Altman 坐到谈判桌前的原因，也是为什么他原则上同意了一项协议，而不是“卑鄙地向他投降”。 ’他的宠物媒体报道一直暗示这是迫在眉睫的，或者是他唯一能接受的事情。我认为 TDM 为他们赢得的是 Altman 必须遵守的更具安全意识的治理的前景（不是保证），从安全角度来看，这看起来比他直接在微软旗下建立的更好（至少对我来说），即使他会遭受（大量）一次性转换成本。</p><p></p><p> *请有人考虑一下二阶效应吗？！*</p><p>但更广泛的世界影响又如何呢？如果 EA/AIS/无论什么现在在技术或金融精英中是一个肮脏的词或蔑视的对象，那么值得吗？如果 TDM 看起来很愚蠢（即使事实并非如此），这会对我们其他人产生不好的影响吗？这是否会阻止任何人投资于以安全为首要的治理结构，以获取大型科技公司的收益？操作系统 AI 开发是否会激增，因为没有人愿意信任 API，如果超过 50% 的少数人决定这样做，API 就会被扼杀？</p><p>他们很好：</p><p> 1) 原则性的反公关观点会说，人们应该接受人们真诚地看轻你或反对你真正相信和代表的东西。策略很好，公开妥协也很好，但当你真的“暂停人工智能”时，保持假装你有（例如）谨慎的能力是错误​​的。因此，社会现实/更高的拟像水平不应纳入对 TDM 所做工作的评估。</p><p> 2）如果你不相信这一点，通常会出现很多假定的二阶效应。也许 OpenAI 的大爆发会推动政府的介入？也许奥特曼必须更加“摘下面具”才能继续留在 OpenAI，让其他人在事情真正开始发挥作用时保持警惕？也许这种治理明显（或明显被视为）失败意味着同样的错误不会重演？因此，如果从第一顺序来看，TDM 做了正确的事情，那么第二顺序的东西（尤其是事前）可能是近似的清洗。</p><p> 3) 即使你是一个每天晚上都会自慰地阅读《美丽安对话》的狂热接吻者，并且你会看到人工智能治理工作的真正游戏正在秘密渗透到权力走廊，所以你和你的人都处于有利位置适逢其时，TDM 的处境正是如此。如果你不愿意冒一些社会资本的风险，以合理的方式阻止人工智能领头羊的道德可疑的首席执行官罢免自己的董事会并将其打造为自己的个人帝国，那么你什么时候才能花掉它呢？</p><p></p><p> **加起来**</p><p>也许不需要太多时间就能看到这相当于应付或同人小说。也许（例如）TDM 讲述了他们自己的故事，听起来更像是他们感到害怕、冲动或愚蠢。也许故事的另一个转折意味着事情最初如何发展的假设故事是错误的，我在其上建立的合理大厦倒塌了。</p><p>但我不这么认为。就德行而言，他们是唯一真正认真对待“使命”、能够做点什么、在巨大压力下尝试做正确事情的人。从业绩上看，他们遭到了护身符首席执行官的伏击，不仅成功地为自己辩护，还把他赶了出去，尽管有他、员工、投资者和普遍敌对的媒体对他们不利，但他们还是劫持了 OpenAI 董事会的一部分向他们回报重大让步。对我来说，这周的工作似乎还不错。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1<guid ispermalink="false"> nfsmEM93jRqzQ5nhf</guid><dc:creator><![CDATA[mrtreasure]]></dc:creator><pubDate> Wed, 06 Dec 2023 02:47:31 GMT</pubDate> </item><item><title><![CDATA[Some quick thoughts on "AI is easy to control"]]></title><description><![CDATA[Published on December 6, 2023 12:58 AM GMT<br/><br/><p>我觉得<a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">帖子</a>作者错过了很多东西，我想分享一些看起来很好沟通的东西。</p><p>我将专注于控制超级智能 AI 系统：系统强大到足以完全解决对齐问题（在<a href="https://arbital.com/p/cev/">CEV</a>意义上），或者杀死地球上的所有人。</p><p>在这篇文章中，我将忽略其他与人工智能相关的 x 风险来源，例如<a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf">人工智能支持的生物恐怖主义</a>，并且我不会评论所有似乎需要评论的重要内容。</p><p>我也不打算指出所有我认为可能会让读者错误概括的狡猾主张，因为这会很挑剔，也不值得花时间（我会跳过的例子 - 我找不到证据GPT-4 已经过任何有监督的微调；RLHF 将聊天机器人的大脑塑造成一种系统，该系统产生的输出使人类评分者点击竖起大拇指/“我更喜欢这个文本”，这样做的智能系统不是它们自己必然是人类评分者的“首选”；一个脚注<span class="footnote-reference" role="doc-noteref" id="fnref6uvr6jxw68j"><sup><a href="#fn6uvr6jxw68j">[1]</a></sup></span> ）。</p><h2>介绍</h2><blockquote><p>许多人担心我们将失去对人工智能的控制，导致人类灭绝或类似灾难性的“人工智能接管”。我们希望本文中的论点使这样的结果看起来难以置信。但即使未来的人工智能在严格意义上变得不那么“可控”——例如，仅仅因为它的思考速度比人类直接监督的速度快——我们也认为，将<i>我们的价值观灌输</i>给人工智能是很容易的，这个过程称为“<strong>对齐</strong>”。</p></blockquote><p>这<strong>歪曲了人们的担忧</strong>。说“但即使”使它看起来像：担心 X 风险的人们相信“无论/尽管一致，失去控制都会导致 X 风险”；这些人错了，因为帖子显示“这个结果”是难以置信的；另外，即使他们对失去控制的看法是正确的，他们对x风险的看法也是错误的，因为由于一致，一切都会好起来的。</p><p>但大多数情况下，人们（包括领先的声音）特别担心导致人类灭绝的能力的未对准系统。我不认识社区中有人说这是一件坏事，如果CEV一致的超级智能抓住了控制，这会导致灭绝。</p><blockquote><p>由于每一代可控的AI都可以帮助控制下一代，因此看起来这个过程可以无限期地继续，即使能力很高</p></blockquote><p>我希望很容易奖励形状的AIS低于一定水平<span class="footnote-reference" role="doc-noteref" id="fnrefkrdp19oh1z"><sup><a href="#fnkrdp19oh1z">[2]</a></sup></span>的功能，并且我担心控制AIS以上的级别。我相信您需要一个能够超人的系统来设计和监督一个能力的系统，以免杀死所有人。我预测的这些系统并非杀死所有人的人类系统的当前能力是这些系统并不杀死所有人的能力，并且没有提供很多证据表明超人类系统能够监督超人系统。 <span class="footnote-reference" role="doc-noteref" id="fnref072edwh8od0f"><sup><a href="#fn072edwh8od0f">[3]</a></sup></span></p><p>为了解决使超人系统对齐的问题，您需要一些复杂的人类思想/艰苦的高级工作。如果系统可以在短时间内输出这么多的高级高级工作，我认为该系统是超人的，并且将其对齐为“对齐完整”的问题是，如果您解决任何一个在此类问题中，您从本质上解决了一条线路的解决方案，并可能避免X风险，但是解决这些问题中的任何一个都需要大量的艰苦人工工作，并且安全地自动化努力工作是一个对齐完整的问题。</p><p>需要有一个论点，说明为什么人们可以成功使用次人类系统来控制复杂的超人类系统，与其他方式相比，拥有几代可控的亚人类系统并不重要。</p><h2>优化</h2><p>让我们谈谈特定的神经网络将追求的目标。</p><blockquote><p>我们常规解决的许多“对齐问题”，例如养育孩子或培训宠物，似乎比训练友好的AI要<strong>困难</strong>得多</p></blockquote><p>请注意，Evolution已“白色框”访问我们的体系结构，为我们优化了包容性的遗传健身，并获得了为类似物品收藏的优化的东西。认为人类是如此可靠的。儿童已经被连接到很容易想要巧克力，政治和合作。相反，如果您有一个外星人的孩子，将<a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/YrhT7YxkRJoRnr7qD">善良</a>与<a href="https://www.lesswrong.com/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8">吃孩子</a>或<a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/mMBTPTjRbsrqbSkZE">卵石分类</a>相关联，那么给孩子的奖励可以使他们学习您的语言，但不一定会使他们不想吃孩子或分类鹅卵石。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=cLXQnnVWJGo"><div><iframe src="https://www.youtube.com/embed/cLXQnnVWJGo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>如果您有一个孩子，则不需要在数学上指定您重视的一切：他们可能不会超级熟练地给他们带来奖励，并且他们已经有线要想要类似于您想要的东西的东西。</p><p>创建AI时，您确实需要具有优化的目标：您希望AI尝试执行的操作，即使具有超智能优化功率，实用程序功能也可以安全地进行优化。我们不知道如何安全地指定这样的目标。</p><p>然后，即使您以某种方式设计了这样的目标，您也需要以某种方式找到一个实际上试图实现该目标的AI，而不是其他方面的实现过程，而实现过程与训练过程中的目标相关。</p><blockquote><p> AI秘密打算杀死您，梯度下降会注意到这一点，并使将来这样做的可能性较小，因为可以拆除秘密谋杀案所需的神经电路，并将其重新配置为直接改善性能的电路</p></blockquote><p>我不确定他们在内部对齐问题上的假设是什么。这是错误的：我们期望具有广泛目标的智能AI可以在可以使用的各种奖励功能上表现出色，并且梯度下降不会优化AI实际上试图追求的终端目标。</p><p>我<a href="https://moratorium.ai/#modern-machine-learning">完全希望</a>梯度下降能够成功优化人工神经网络以实现低损失。我只是不希望他们可以设计的损失功能可以<a href="https://moratorium.ai/#outer-and-inner-ai-alignment">代表我们的价值</a>，我希望梯度下降会找到<a href="https://moratorium.ai/#inner-alignment">试图实现与奖励功能中指定的东西不同的</a>神经网络。</p><p>如果梯度下降找到一种试图最大化与人类完全无关的事物的代理，并理解为此，它需要在我们的功能上取得很高的分数，那么代理将成功获得高分。<strong>梯度下降将优化其在我们功能上获得高分的能力 - 它将优化构成代理的结构 - 但不会真正关心当前结构的目标内容</strong>。如果训练完成后，此结构将对宇宙未来的任何奇怪的事物进行优化，并计划杀死我们，这并不能追溯​​使梯度变化 - 我们没有已知的方法来指定损失功能来训练训练将来可能会计划杀死我们的参数。</p><h2>干预措施</h2><p>能够进行实验并不意味着我们可以提前证明所有潜在问题。如果AI足够聪明，并且已经想要与我们想要的足够不同的东西，并且我们不了解其认知架构，那么我们将无法欺骗它来相信它的模拟环境是现实世界终于接管了。简单地读取/写入对权重和激活的访问就无法控制AI对<span class="footnote-reference" role="doc-noteref" id="fnreftjnsez6u2fb"><sup><a href="#fntjnsez6u2fb">[4]</a></sup></span>的看法。塑造超人类系统行为的技术不会让我们控制更智能的系统。</p><blockquote><p>有些人指出，越狱的有效性是AI难以控制的论点</p></blockquote><p>是的，但这不是X风险社区的论点。</p><blockquote><p>至关重要的是，这并不意味着人类“与进化保持一致”  - 参见<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn"><i><u>进化没有证据表明Quintin Pope的左转转弯</u></i></a>，以揭穿该类比。</p></blockquote><p> Afaik，Nate Soares不会声称人类与进化保持一致。不幸的是，该帖子或链接的帖子的作者并不理解<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">锋利的左转弯</a>的动态。</p><h2>人工智能控制研究更容易</h2><blockquote><p>研究改善AI可控性比研究改善人类可控性要容易得多，因此我们应该期望AIS比人类更快地获得可控的速度</p></blockquote><p>（我将假设控制和对齐方式是“控制”的意思。）与人类控制技术相比，测试AI控制技术更容易。对超人类系统有效，但与超人系统无关或不适用，AS：</p><ul><li>我们不知道如何<strong>系统地提出可以实际工作以控制超智能AI系统的技术</strong>；</li><li>我们不知道如何在现实世界中验证它在现实世界中验证它会做什么；如果知道不是这样，我们就不知道该技术是否真的允许我们在现实世界中控制它。</li><li>主要实验室和外部的许多人建议制作不是单个模型的超级智能系统，而是<a href="https://www.youtube.com/watch?v=YTlrPeikoyw">在复杂系统中</a>工作的许多模型的组合，它们互相监督和报告；如果用于训练的所有计算现在都用于运行模型的副本以组成超级智能系统，那么您只有一个昂贵的系统，因此成本和可伸缩性考虑并不适用。</li></ul><h2>价值很容易学习</h2><blockquote><p>如果AI<strong>首先</strong>学会道德，它将希望帮助我们确保它<strong>保持</strong>道德，因为它变得更强大</p></blockquote><p>如果AI聪明且连贯，则可以做到这一点。但是，如果还不是CEV一致的超级智能，那么了解了人类想要的东西并没有激励梯度下降以将其转移到CEV与CEV一致的超级智能的方式上。我希望理解人类价值观确实很容易成为智能的AI，并使其更容易进行。但这并不能自动使人类价值成为优化目标。知道人类想要什么，除非您解决了AI护理的问题，否则不会照顾AI。</p><p>似乎“对齐”的超人类模型的行为对应于一系列凌乱的东西，这些集合可以优化人类给予的回报。但是，每次梯度下降使模型更加一般优化/代理时，已经对物品进行了混乱的内容，这不会影响新体系结构梯度下降安装的目标内容。梯度下降没有理由保留过去神经网络实现的算法的目标和价值：神经网络实现的新的，更智能的AI算法可以实现高分，并且具有更大的可能目标和价值范围。</p><blockquote><p>由于社会中几乎每个人都<i>共享</i>和理解价值观，因此它们不能非常复杂。与科学和技术不同的是，劳动分工可以积累更复杂的知识，价值观必须保持足够简单才能在几年内被儿童学习。</p></blockquote><p>我猜对人类价值的描述可能比信息或其他东西短。人工智能可以学习它们是什么；但是它们不太简单，让我们轻松将它们指定为优化目标 - 请参阅<a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes">愿望的隐藏复杂性</a>。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=gpBqw2sTD08"><div><iframe src="https://www.youtube.com/embed/gpBqw2sTD08" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><blockquote><p>当前的语言模型已经非常有能力在道德上评估超级智能可能能够的复杂行动</p></blockquote><p>他们能够评估给他们带来的后果，但比人类更有能力。就是说，</p><ul><li>像人类AIS产生的计划无法评估Subhuman LLM，就像人类无法做到的那样，因为看到行动的所有后果都需要情报，而不仅仅是了解人类会说的话；</li><li>我希望有一些失败模式对帖子的作者来说是显而易见的。我邀请读者思考如果我们使用当前LLMS自动评估超人AIS生成的计划，然后启动我们当前LLM的计划，并说“这看起来不错”。</li></ul><h2>结论</h2><blockquote><p>有很多原因可以期望AI易于控制，并且易于与人类价值观保持一致</p></blockquote><p>不幸的是，在这篇文章中，我还没有看到超级智能AIS易于控制或与人类价值观保持一致的证据。如果神经网络实现了想要与我们想要的不同的超人AI代理，那么尽管该代理人的影响或改变它实现了与CEV意义上与人类价值观保持一致的超人AI代理，甚至只是注意到代理人出了问题，直到为时已晚。</p><p>虽然我们直接优化了AI系统的权重以获得奖励，而对奖励的人类大脑的变化则不太清楚和透明，但我们不知道如何使用它来使超级智能AI想要我们希望它想要的东西。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn6uvr6jxw68j"> <span class="footnote-back-link"><sup><strong><a href="#fnref6uvr6jxw68j">^</a></strong></sup></span><div class="footnote-content"><blockquote><p>未来的AI将以应有的道德考虑的方式表现出情感和欲望</p></blockquote><p>默认情况下，消灭人类的超人AI系统不会有情感。他们将是非常好的优化器。但似乎重要的是要注意，如果我们从极好的优化者中成功地不死，我希望我们只有故意和理解如何设计新思维的AI系统。请参阅<a href="https://www.lesswrong.com/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers">非种义优化器</a>，<a href="https://www.lesswrong.com/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child">不能忘记一个孩子</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnkrdp19oh1z"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkrdp19oh1z">^</a></strong></sup></span><div class="footnote-content"><p>我专注于一般的超人类和一般超人系统，因为这似乎是一个相关的区别，同时更容易专注于重点，即使它失去了一些细微差别。看来推断比训练便宜，一旦您训练了人类级别的系统，您就可以立即运行它的许多副本，它们可以共同组成一个超人的系统（如果足够聪明，可以在相对较短的时间内解决对齐，如果它想要，并且还足以杀死所有人）。汇总的许多亚人类系统的副本将无法解决一致性，或者需要大量最佳人类认知的任何问题。因此，我想象人类层面上的模糊阈值，并在这篇文章中专注于它。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn072edwh8od0f"> <span class="footnote-back-link"><sup><strong><a href="#fnref072edwh8od0f">^</a></strong></sup></span><div class="footnote-content"><p>我在这里不讨论的是一个较弱的系统，可以转向更强大的系统（没有得到明显的偏好和保留偏好）。我们不知道该怎么做。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntjnsez6u2fb"> <span class="footnote-back-link"><sup><strong><a href="#fnreftjnsez6u2fb">^</a></strong></sup></span><div class="footnote-content"><p>不幸的是，我们不知道每个权重代表什么，并且我们对实施的算法没有太多透明度。我们不了解思考过程，尽管有各种内部优化压力，但我们不知道如何以一种有效的方式影响它</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/iABojbKgmtMGYgcYm/some-quick-thoughts-on-ai-is-easy-to-control#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iabojbkgmtmgygcym/some-quick-thoughts-on-is-is-is-is-easy-control<guid ispermalink="false"> iabojbkgmtmgygcym</guid><dc:creator><![CDATA[Mikhail Samin]]></dc:creator><pubDate> Wed, 06 Dec 2023 00:58:53 GMT</pubDate> </item><item><title><![CDATA[Multinational corporations as optimizers: a case for reaching across the aisle]]></title><description><![CDATA[Published on December 6, 2023 12:14 AM GMT<br/><br/><p>在左右左右徘徊时，我注意到的一件事是，他们也谈论价值对齐。一旦您摆脱了完全不同的词汇，他们也试图弄清楚如何处理大型超人实体，以优化人类无关紧要的目标。正是他们所说的是全球，公开交易的Megacorps优化金钱。</p><p><strong>公开持有的公司具有尽可能多的资金的最终价值。</strong>这称为“股东价值最大化”。</p><p>即使组织内部的大多数人都可以重视其他事情，他们的职位描述也是为了最大程度地提高资金的最终目标，并付费并激励这样做。在此中，他们创建了程序和政策，然后告诉其子雇员执行它们。</p><p>程序和政策包含执行这些操作的人，尤其是在最低级别上，这意味着可以将其视为手动执行程序。当然，人类不是硅。使用人类作为计算基板和世界操纵器的过程进行操作是缓慢而不完美的。但是，我相信类比仍然存在。</p><p>股东价值最大化的方式已经严重损害了世界，并损害了人类生活的质量是无数且易于观察的：污染，气候变化和其他此类外部性。</p><p>总之，我认为“友好的AI”问题与“友好的跨国大型企业”问题具有足够的相似性，而某些交叉授粉可能会产生生产力。即使他们的大多数想法都无法与AI一起使用，他们也考虑创建更符合道德的超人代理的事实仍然值得一看。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HEDbwdx8j8oe6ZiYj/multinational-corporations-as-optimizers-a-case-for-reaching#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hedbwdx8j8oe6ziyj/multinational-corporations-as-as-optimizers-a-case-for-for-for-for-raching<guid ispermalink="false"> hedbwdx8j8oe6ziyj</guid><dc:creator><![CDATA[sudo-nym]]></dc:creator><pubDate> Wed, 06 Dec 2023 01:11:47 GMT</pubDate> </item><item><title><![CDATA[How do you feel about LessWrong these days? [Open feedback thread]]]></title><description><![CDATA[Published on December 5, 2023 8:54 PM GMT<br/><br/><p>你好！我是来自 LessWrong / Lightcone 团队的 jacoobjacob。</p><p><strong>这是一个元线程，您可以分享您一直在想的有关 LessWrong 的任何想法、感受、反馈或其他内容。</strong></p><p>您可能分享的内容示例：</p><ul><li> “我真的很喜欢同意/不同意投票！”</li><li> “所有这些对话内容是怎么回事？这令人困惑......</li><li> “嗯……最近网站上的氛围似乎发生了某种变化……特别是[<i>插入 10 段</i>]”</li></ul><p> ...或者其他什么！</p><p>这篇文章的目的是让您能够在一个您知道团队成员会倾听的地方分享您想到的任何事情。</p><p> （我们是一个小团队，必须优先考虑我们的工作，所以我当然不承诺采取这里提到的所有内容。但我至少会倾听所有这些！）</p><p>我已经有一段时间没有看到这样的公共话题了。也许有很多关于这个网站的沸腾的感觉从未表达出来？或者，除了我从阅读正常评论、帖子、指标和对讲评论中发现的内容之外，你们没有什么可以分享的了？好吧，这是找出答案的一种方法！我真的很想询问并了解人们对该网站的感受。</p><p>那么，您最近对 LessWrong 感觉如何？欢迎在下面留下您的答案。</p><br/><br/> <a href="https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/j2w3zs7ktzxt2wzah/how-do-you-feel-about-about-less-lesswrong-theese-days-days-open-open-open-feedback<guid ispermalink="false"> J2W3ZS7KTZXT2WZAH</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Tue, 05 Dec 2023 20:54:45 GMT</pubDate> </item><item><title><![CDATA[Critique-a-Thon of AI Alignment Plans]]></title><description><![CDATA[Published on December 5, 2023 8:50 PM GMT<br/><br/><p> AI-Plans.com 将于 12 月 16 日至 18 日举办批评马拉松，参与者将提出并讨论对人工智能调整计划的批评。</p><p>评委包括：</p><ul><li>内特·苏亚雷斯，MIRI 主席</li><li>Ramana Kumar，DeepMind 研究员</li><li>Peter S Park 博士，麻省理工学院 Tegmark 实验室博士后</li><li>Charbel-Raphaël Segerie，EffiSciences 人工智能部门负责人</li></ul><h2>好处：</h2><p>批评马拉松提供了一个绝佳的机会，通过深入研究调整计划可能成功或失败的原因，获得对人工智能安全的重要见解。<br>我们将突出调整计划的关键要素，这些要素将反馈给研究人员并可用于改进他们的计划。<br>这也是获得专家评委反馈的绝佳机会。</p><h2> Critique-a-Thon 活动为期 3 天，分为 2 个阶段：</h2><h3><strong>第一阶段 - 添加评论</strong>（截至 12 月 16 日）</h3><p>我们将在 ai-plans.com 上以两个类别的形式添加对调整计划的批评：优势和弱点。</p><p><br> “优势”应表明计划如何作为协调解决方案发挥作用。 “漏洞”应该起到相反的作用。<br>奖品将颁发给那些提出最多批评的人 - 投票给负分的批评将不被计算在内。<br><br>您可以随时开始这个阶段 - 您今天就可以开始添加评论！<br>截止日期为格林尼治标准时间 12 月 16 日午夜。<br><br><strong>奖品</strong><br>第一名：100 美元<br>第二名：60美元<br>第三名：40美元</p><h2><br>第二阶段 - 讨论和总结（12 月 17 日至 18 日）</h2><p>我们两人一组。每对将选择一个已提出批评的调整计划。一个人会提出理由，另一个人会提出反对理由，优势/弱点是真实和准确的。<br>然后，第二天，我们将交换立场，并以所提议的优势/弱点的书面形式结束。<br>请参阅之前获奖评论的示例：</p><p> <a href="https://docs.google.com/document/d/16Ww6nNECsDnjMOGaz5fqg1PxCVRPpoOy80UooEa7CZM/edit#heading=h.vdtj4e22pjdv"><u>八月评论马拉松</u></a></p><p><br><a href="https://docs.google.com/document/d/1O586rkRZd9BbpI8yqA2K6aG9IIq6E1A9I2H9C5nEYKQ/edit#heading=h.vdtj4e22pjdv"><u>九月评论马拉松</u></a><br><br>这些讨论将有助于完善批评并加强论点，并且交换可以减少由于缺乏某个主题的先验知识而造成的任何不利。<br>如果你的对手对这个话题了解得更多，并提出了你没有想到的观点，你可以在第二天自己使用它们，看看有哪些应对措施 - 并确定它们是否是你的文章中需要提及的内容，这就是将被判断的内容。<br>这些讨论将<a href="https://discord.gg/aGVtu5JyjJ"><u>在 Discord 上</u></a>进行。</p><p>在这里加入👉 <a href="https://discord.gg/aGVtu5JyjJ"><u>https://discord.gg/aGVtu5JyjJ</u></a></p><h3><br><strong>奖品</strong></h3><p>第一名：400 美元<br>第二名：250 美元<br>第三名：150 美元<br><br></p><p>在此注册加入👉 <a href="https://ai-plans.com/login"><u>https://ai-plans.com/login</u></a></p><p><br><br></p><br/><br/> <a href="https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/kgyrsakbjnxpbicc/critique-a-thon-oai-alignment-alignment-plans<guid ispermalink="false"> kgyrsakbjnxpbicc</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Tue, 05 Dec 2023 20:50:08 GMT</pubDate> </item><item><title><![CDATA[Arguments for/against scheming that focus on the path SGD takes (Section 3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 5, 2023 6:48 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第三部分。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://www.buzzsprout.com/2034731/13984918">请点击这里</a>，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>支持/反对阴谋论的争论集中在 SGD 所采取的路径上</h1><p>在本节中，我将讨论支持/反对计划的论点，这些论点更直接地关注 SGD 在选择训练的最终输出时所采取的路径。</p><p>重要的是，这些论点可能并不相关。特别是：如果 SGD 在模型类别之间以某种“直接比较”的方式主动支持或不支持阴谋者，那么 SGD 将“找到一种方法”来选择它在这个意义上所支持的模型类型（例如，因为足够高的模型类型）。维度空间使这样的“方式”可用）， <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-1" id="fnref-ZwSi8PjDgjvifEocL-1">[1]</a></sup>那么足够的训练只会引导你到 SGD 最喜欢的任何模型，而所讨论的“路径”并不重要。</p><p>在关于不同模型的最终属性之间的比较的部分中，我将讨论我们可能期望 SGD 出现这种偏袒的一些原因。特别是：阴谋者“更简单”，因为他们可以有更简单的目标，但他们“更慢”，因为他们需要参与各种形式的额外工具推理 - 例如，在决定策划时，检查现在是否是一个好时机缺陷，可能参与和掩盖“早期破坏”等方面的努力（尽管请注意，此处需要执行额外的工具推理，可能会表现为由策划者的权重实现的算法的额外复杂性，因此表现为“简单性”）成本”，而不是“运行该算法更长时间的需要”）。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-2" id="fnref-ZwSi8PjDgjvifEocL-2">[2]</a></sup>我将在下面对此进行更多讨论。</p><p>不过，在这里，我想指出，如果 SGD 足够关心简单性和速度等属性，那么 SGD 通常会首先构建一个具有长期追求权力目标的模型，但即使该模型尝试了一个方案 -就像策略一样（在这种情况下，由于预知其失败，它不一定会这样做），它会被无情地磨成一个情节奖励寻求者，因为情节奖励寻求者的速度优势。或者，SGD 通常会首先构建一个按情节奖励的寻求者，但由于 SGD 渴望更简单的目标，该模型将被无情地磨成一个阴谋者。</p><p>在本节中，我将假设这种事情不会发生。也就是说，SGD 构建模型的顺序会对训练的最终结果产生持久的影响。事实上，我的一般感觉是，对阴谋者的讨论经常隐含地假设这样的事情——例如，人们通常认为阴谋者会在训练中很早就出现，然后在那之后将自己锁定。</p><h2>独立于训练游戏的代理目标故事</h2><p>回想一下我上面介绍的区别：</p><ul><li><p>训练与比赛<em>无关的</em>赛外目标，其产生与其在训练-比赛中的角色无关，但随后会激励训练-比赛，而不是训练-比赛。</p></li><li><p>与训练游戏<em>相关的</em>赛外目标，SGD 主动<em>创建</em>这些目标是<em>为了</em>激励训练游戏。</p></li></ul><p>在我看来，关于专注于与训练、比赛<em>无关的</em>目标的策划故事似乎更传统。也就是说，这个想法是：</p><ol><li><p>由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以<em>不</em>通过训练游戏的方式）。</p><ol><li><p>这可能发生在态势感知到来之前或之后。</p><ol><li><p>如果之前，那么在一段时间内它可能会被训练出来，并且还没有激发训练-游戏。</p></li><li><p>如果之后，它可能会立即开始激励训练-游戏。</p></li></ol></li></ol></li><li><p>然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。</p></li></ol><p>将此称为“独立于训练游戏的代理目标故事”。</p><p>我相当认真地对待这个论点。如果某种适当雄心勃勃的超越情节的代理目标没有被训练（要么因为它在态势感知之前没有得到训练，要么因为它在态势感知之后突然出现），并且如果有一些关于为什么这个目标会产生影响的故事策划一个好的工具性策略适用（例如，经典的守门故事，或者可能是其他一些非经典故事），那么策划似乎确实可能只是一件发生的事情。</p><p>我认为最直接的反对意见是：为什么模型要制定这些（适当雄心勃勃的）超越情节的目标？我之前在诸如“目标默认情况下不会有时间折扣”和“模型时间不会像日历时间”之类的论点中讨论过这一点，我不会在这里添加太多内容，除了说假设模型将实现一些适当雄心勃勃的、超越情节的、独立于训练游戏的代理目标，追求这些目标与高奖励相关，在我看来，这似乎是经典故事中较弱的部分之一。</p><p>特别是：对阴谋的讨​​论有时集中在“好奇心”和“权力”等代理目标上，这些目标在很多剧集中都显得有用。但这并不意味着在训练中会激励对<em>情节之外的</em>好奇心和力量的追求。更重要的是，特别是如果你知道/可以控制情境意识何时出现，那么在模型因追求好奇心/力量而受到积极惩罚的训练片段中似乎并不那么困难。非训练游戏模型将直接进行修改。事实上，总的来说，我希望我们能做很多事情来积极阻止模特采取寻求权力的行为，并鼓励其他形式的“谦虚”。</p><p>后一点与我之前提到的乐观原因有关：即，独立于训练游戏的代理目标故事允许我们在态势感知之前进行任何我们想要的对抗性训练，以尝试确保模型的目标保持不变情节内且适度适度。</p><p>也就是说，试图在态势感知之前通过对抗性训练来避免这种阴谋，面临着一些严重的障碍：</p><ul><li><p>首先：最重要的是，所讨论的对抗性训练可能不够多样化/彻底。特别是（特别是缺乏可解释性工具）：可能很难想到所有可能的适当雄心勃勃的剧集外代理目标，这些目标与奖励有足够好的相关性，并且您需要使用此类培训来推动。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-3" id="fnref-ZwSi8PjDgjvifEocL-3">[3]</a></sup></p></li><li><p>第二：当态势感知开始发挥作用时，你可能不知道，或者无法很好地控制。</p></li><li><p>第三：一旦足够智能的模型经过文本预测的预训练，它可能已经非常接近态势感知，因为它拥有大量相关的常识（即使不一定是自定位知识）。因此，在那之后可能没有太多时间进行没有态势感知的对抗训练。</p></li><li><p>最后，<em>在</em>态势感知启动后，模型有可能制定适当的、雄心勃勃的、超越情节的、策划激励的目标。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-4" id="fnref-ZwSi8PjDgjvifEocL-4">[4]</a></sup></p><ul><li><p>例如，也许当模型开始按照我上面描述的方式“反思”时，它已经相当聪明并且具有战略意识——例如，“弄清楚它真正想要什么”，自我解决，学习打破以前尝试的新事物对齐它，等等。 （这要求模型不会因为获得态势感知而立即开始寻求奖励，但这似乎是可能的，特别是如果我们假设训练留下了显着的“余裕”。）</p></li><li><p>当然，仍然存在一个问题，即为什么该模型会将自己打造成具有适当雄心勃勃的目标，特别是超越情节的目标。但在这里，根据我上面对“如果你训练它拥有长期目标会怎样”的讨论，也许我们可以诉诸这样一个事实：我们试图将其指向一项雄心勃勃的长期任务，但这种指向有些不准确。 / 模糊，这样当它弄清楚它想要如何概括时，这种概括就落在了足够长期和雄心勃勃的事情上，从而激发了阴谋。</p></li><li><p>事实上，正如我上面指出的，我认为有一个案例表明，当前的调整工作已经<em>在</em>试图将我们的系统指向长期的、超越情节的目标——例如，避免帮助制造炸弹的“无害”形式即使炸弹数十年不会爆炸。</p></li></ul></li></ul><p>因此，总的来说，我认为，以谋划作为追求（适当雄心勃勃的）超越情节目标的良好工具性策略为条件，独立于训练游戏的代理目标故事在我看来是一个真正令人担忧的问题。</p><h2> “最近的最大奖励目标”故事</h2><p>我们还可以讲述一个不同的故事，重点关注<em>依赖于</em>训练和比赛的赛外目标。我将其称为“最近的最大奖励目标”故事。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-5" id="fnref-ZwSi8PjDgjvifEocL-5">[5]</a></sup>故事是这样的：</p><ol><li><p>当模型变得具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。相反，它将追求一些低于最大奖励的代理目标。</p></li><li><p>但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏<em>依赖</em>”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）</p></li><li><p>此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。</p></li><li><p>所以：SGD 将使模型成为一个策划者。</p></li></ol><p>现在，一种选择是尝试使 (1) 为假。事实上，如上所述，在态势感知之前尝试以良好的方式塑造模型的目标似乎是降低阴谋可能性的关键潜在途径。但请注意，在这种情况下，所面临的挑战比<em>独立于</em>训练游戏的代理目标更困难。也就是说，对于与训练游戏无关的代理目标，只需在态势感知之前避免给模型提供类似策划者的目标，因此它可以容忍所讨论的目标可能尚未达到最大奖励的<em>其他</em>方式。相比之下，在“最近的最大奖励目标”论点的背景下，人们需要要么实际创建一个最大奖励、非类似阴谋的目标，要么<em>足够接近</em>（3）不适用的目标。也就是说，你不能仅仅阻止“自然阴谋”的出现，而必须阻止它的出现。你需要确保当 SGD 稍后“搜索”最近的最大奖励目标时，它不会首先找到阴谋者。</p><p>现在，我们假设我们还没有完全掌握这一点，并且（1）是正确的（尽管：我们所达到的掌握<em>程度</em>对我将在下面讨论的“接近度竞赛”很重要）。 (2) 源自态势感知的定义（或者至少是一个宽松的定义，例如“参与阴谋并认识到阴谋的好处所必需的对世界的理解”），所以我们也承认这一点。</p><p>但是（3）呢？我认为，在这里，论证还有更多工作要做。为什么认为将模型打造成阴谋者将是使其获得最大奖励的最简单方法？例如，为什么不把它变成一个训练圣人，或者一个剧集奖励寻求者，或者一个追求某种最大奖励代理目标的错误概括的非训练游戏玩家？根据假设，该模型具有代表<em>任何</em>这些目标的概念工具。 （请注意，在这方面，它与大多数进化历史中的人类不同，人类<em>没有</em>概念工具来表示“包容性遗传适应性”等目标。）因此，我们需要一个故事来说明为什么该论点会优先考虑类似阴谋家的人特别是目标。</p><p>为了清楚起见：当我谈论 SGD 可以进行某种修改的“轻松”程度，或者所得到的模型的“接近度”时，这是“SGD‘偏好’的那种修改”的代表。本身就是“SGD 实际上将进行的那种修改”的替代品。在机械层面上，这大致意味着：奖励景观中最陡梯度的方向。我经常会想象一种更模糊的感觉，即 SGD 可以做的“工作”预算有限，因此希望在修改模型目标方面做尽可能少的“工作”，以便它可以专注于改进模型认知的其他方面。</p><p>那么，根据对“轻松”的理解，我们应该期望哪个模型类对于 SGD 来说是“最容易”创建的，即从一个新的情境感知模型中创建一个低于最大奖励代理目标的模型。自己不能激发阴谋吗？</p><p>作为一个有助于激发直觉的松散类比：想象一下将人类技术冻结在当前水平，并让进化选择在人类身上运行更长时间。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-6" id="fnref-ZwSi8PjDgjvifEocL-6">[6]</a></sup>从长远来看，你预计哪种人类（或人类后裔生物）将占据主导地位？特别是：您期望：</p><p> (a) 人类本质上重视“<a href="https://en.wikipedia.org/wiki/Inclusive_fitness">包容性遗传适应性</a>”或“我的基因被进化选择”（这是对“指定目标”和“奖励过程”的某种组合的松散类比），</p><p> （b）人类重视与包容性遗传适应性密切相关的其他事物（例如，“拥有尽可能多的孩子”），但他们没有明确地优化包容性遗传适应性，甚至是工具性的（这是对错误的松散类比）广义的非训练游戏玩家），或</p><p>(c) 具有长期目标的人类，他们正在优化包容性遗传适应性，特别是作为一种在长期内为自己的价值观获得权力的方法（这对阴谋家来说是一个宽松的类比）。</p><p>在这里，“最近的最大奖励目标”故事的类比会问这样的问题：对现有人类价值观的最小修改是什么（或者更好：特别是对进化最容易的修改）是什么，使我们陷入困境之一如上所述，同时也与所面临的进化选择压力的大小兼容？ <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-7" id="fnref-ZwSi8PjDgjvifEocL-7">[7]</a></sup></p><p>当然，这个类比带来了大量令人困惑的变量（例如，与进化选择构建学习价值的大脑的基因组而不是价值本身有关的事实，文化积累和模因选择的作用）人口动态、缺乏对未来逃跑/接管机会的明确模拟，等等）。为了使这一类比与机器学习更加相似，最好想象进化直接选择整个人类大脑，配备完整的概念工具来代表所有这些目标，而不是选择构建大脑的基因组。更好<em>地</em>想象一个从一开始就选择大脑的进化版本，这样我们开始理解进化时的人类价值观可能会完全不同。不管怎样，我对最佳预测没有任何明确的信念。但我认为这是“最近的最大奖励目标”论点的一个令人回味的直觉泵。</p><h3> SGD 渐进主义对类似阴谋家的修改的障碍</h3><p>不过，让我们关注实际的论证，而不是类比。我们应该期望哪个模型类在相关意义上是“最接近的”？</p><p>上面我讨论了阴谋者可能被快速排除的一种方式：即，如果 SGD 无法通过修改模型时遵循的梯度来“注意到”类似阴谋者修改的好处。也就是说，正如我之前讨论的，在寻找最大奖励目标时，SGD 不仅仅“跳”到最近的目标。相反，它需要在梯度计算的基础上逐步实现这样的目标，梯度计算表明模型权重在相关方向上的微小变化会增加奖励。在许多情况下，尚不清楚类似阴谋者的修改是否可以像这样工作。</p><p>因此，例如， <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world">Hubinger (2022)</a>考虑了一个例子，其中 SGD 通过将模型从关心剧集中的金币修改为始终关心金币来引发阴谋。不过，在所讨论的示例中，SGD 并不是逐渐延长模型金币关注的时间范围，每次延长都会导致奖励的提高。相反，SGD 只是做了“一个简单的改变”——即完全放弃目标的时间限制——从而创建了一个阴谋者。但问题是：奖励空间的梯度是否反映了这样做的好处？在我看来，发生这种情况的最自然的方式是，是否有能力从一种模型平滑地过渡到另一种模型，这样每次修改都会逐渐获得更多的策划好处。但目前尚不清楚这是否会发生。正如我之前讨论的，如果我们假设 SGD 还需要构建大量新机器来执行策划所需的工具推理（而不是仅仅重定向预先存在的“目标实现引擎”），那么任务变得更具挑战性。</p><h3>哪个模型是“最接近的”？</h3><p>然而，根据我之前的讨论，我也不觉得我能够排除这种类型的增量转换可能发生的可能性（例如，也许足够高的维空间允许 SGD“找到一种方法”） ”），而且我还没有尝试进行深入分析。因此，虽然我认为这种类型的渐进主义对专注于依赖训练游戏的阴谋者目标的故事提出了相对强烈的反对，但我认为也值得评估这些故事的其他方面。也就是说，假设SGD<em>能够</em>注意到将“最近的最大奖励目标”故事中的模型制作成schemer的好处，这样的修改是否是获得高奖励的最简单方法？</p><h4>目标空间中类似阴谋家的目标的共性</h4><p>对此的一个论点可以追溯到我所认为的支持期待阴谋的核心直觉之一：即可能的阴谋者式目标的绝对<em>数量</em>。粗略地说，这个想法是，因为<em>如此多的目标</em>可以激发策划，所以很可能会有一些类似策划者的目标非常<em>接近</em>模型的当前目标（其中“附近”，回想起来，是根据工作量 SGD 来定义的）必须做才能引起相关类型的修改）。也就是说，阴谋者在目标空间中非常常见——因此 SGD 相应地很容易从任何给定的起点“找到”类似阴谋者的目标。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-8" id="fnref-ZwSi8PjDgjvifEocL-8">[8]</a></sup></p><p>上面描述的“剧集中的金币”→“所有时间的金币”转变就是这种思想的一个例证。也就是说，如果你假设最充分的长期目标会激发阴谋， <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-9" id="fnref-ZwSi8PjDgjvifEocL-9">[9]</a></sup>那么<em>不</em>成为阴谋家的最显着的方法就是有一个足够的短期目标。但是（假设我们已经抛开上面讨论的“增量主义”问题），任何给定的时间有限的目标似乎都非常“接近”时间无限的版本。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-10" id="fnref-ZwSi8PjDgjvifEocL-10">[10]</a></sup></p><p>但时间范围并不是这里可能的“邻近性”的唯一来源。例如，如果模型碰巧有一些尚未激发阴谋的<em>长期</em>目标（例如，因为目标不够“雄心勃勃”），我们可以改变其他相关变量。因此，如果模型开始关心以不受时间限制的方式正确解决一些数学问题，但它可以在密谋接管世界以获得更多计算来检查和重新计算的情节中很好地解决问题检查它的答案似乎不值得，我们可能会想象 SGD，例如，改变模型对得到错误答案的风险的容忍度，或者改变它的目标，以便它想要解决<em>更多的</em>数学问题，或者你有什么。总的要点是：在尝试创建计划程序时，SGD 有大量选项可供选择，因此很容易在模型起点附近找到一个选项。</p><p>因此，在进化和人类的背景下，这种思想的类比是这样的：即使是相当短期的、适度的人类价值观，也可以相对容易地将它们转变为长期的、雄心勃勃的价值观。例如，人类长期主义者似乎会采取“对我周围环境中满足各种条件的人表现出模糊的友善和帮助”之类的价值观，并将其转化为更加雄心勃勃的目标——这些目标似乎有动力明确优化以获取选择-通过进化，如果这是未来电力的关键瓶颈。事实上，在我看来，在实践中，如果你进行进化选择足够长的时间，在长期范围内优化的意识形态群体最终将占据主导地位（许多最接近优化生殖适应性的群体） ，例如，看起来他们有这种味道）。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-11" id="fnref-ZwSi8PjDgjvifEocL-11">[11]</a></sup></p><p>一旦我们搁置了上面关于渐进主义的反对意见，我认为目标空间中类似阴谋家的目标的共性是一个相当有力的理由，担心新的情境意识的最近的最大奖励修改，仍然不是-最大奖励目标追求模型将走向心计。更重要的是，请注意，一旦我们想象 SGD 在目标空间中主动<em>搜索</em>会激发阴谋的目标，我们关于阴谋起源的故事就可以更容忍阴谋者般的目标，必须有其他更具体的目标属性，例如资源匮乏性。也就是说，在专注于训练与游戏<em>无关的</em>目标的环境中，有可能质疑 SGD 落在类似阴谋者的目标上的概率（甚至以它落在更普遍的超越情节目标为条件），凭借不同的超越情节的目标意味着对未来力量、风险等的不同偏好。但在像这样的<em>依赖于</em>训练游戏的环境中，SGD 正在积极<em>寻找</em>这样的目标（并且相关目标在目标空间中仍然很常见——例如，您可以“在所有空间和时间上最大化<em>x</em>的实例”） “ 对于<em>x</em>的许多值）。因此，如果 SGD 找到了它想要的东西，也就不足为奇了。</p><p>然而，我认为这种论点并不是决定性的，原因有二。</p><h4>非策划目标的接近程度</h4><p>第一个原因是，即使我们坚持纯粹专注于修改模型的目标，并采用“干净的目标导向”框架（我将在下面使这样的焦点复杂化），我认为还有一个抵消的“接近性”考虑这至少支持一些非计划目标——即，在其他条件相同的情况下，模型的初始训练（在态势感知之前）似乎更有可能主动将模型指向这些非计划目标的方向。</p><p>这对于指定的目标来说是最清楚的，根据定义，这是通过培训来奖励的。这意味着，即使模型在具有态势感知能力时最终没有<em>完全</em>达到指定的目标，它也可能最终得到广泛附近的目标。</p><ul><li><p>因此，举例来说，如果模型正在接受训练以最大化剧集中的金币，并且最终学习了一个目标，例如“最大化剧集中的金币”（这个目标<em>不太</em>正确，因为有些训练环境包括圆形金币（非硬币），看来 SGD 最简单的修改就是将模型变成<em>金币</em>on-the-episode-getter（即训练圣人）。 ）。或者至少，就“接近度”而言，“情节中的金币”似乎与“有史以来的圆形金币”<em>具有竞争力</em>。</p></li><li><p>对于更多 RLHF 风格的指定目标，我们可能会提出类似的观点。也就是说，如果您正在训练一个模型以一种广泛有帮助、诚实且无害的方式遵循剧集中的说明​​（为简单起见，您的 RLHF 流程仅在这方面提供准确的反馈），那么即使它当它变得具有情境意识时，它已经学会了一种有点不平衡的概念，例如诚实，改进其诚实概念，直到它更好地适合人类概念，似乎具有竞争性，回复：接近，具有更多类似阴谋家的动机修改，例如“偏离-始终保持诚实，以激励计划逃脱人类控制并夺取权力。”</p></li><li><p>请注意，在人类和进化的背景下，人类确实已经重视与包容性遗传适应性密切相关的各种事物——例如，生孩子、更广泛地拥有繁荣而强大的家庭血统等等。如果我们想象进化选择整个人类大脑配备了代表“包容性遗传适应性”的概念装置，人类最初进化到重视与包容性遗传适应性非常接近的东西似乎更容易想象。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-12" id="fnref-ZwSi8PjDgjvifEocL-12">[12]</a></sup></p></li></ul><p>请注意，我们也可以在这里对错误概括的非训练游戏玩家提出类似的观点，只要他们追求最大奖励目标（回想一下，根据我的定义，错误概括的目标可以是最大奖励，前提是特别是<em>训练数据</em>永远不会惩罚它们——例如，它们只会在模型从未见过的反事实场景中受到奖励过程的惩罚。</p><ul><li><p>因此，例如，想象一个训练过程，它永远不会区分“在剧集中获得金币”和“在剧集中获得一般金币”之类的目标，因为训练数据中的所有金币都是硬币。假设在态势感知之前，我们的模型学习了一个目标，比如“在剧集中获得金币，但有时也会跟随你对剧集的好奇心，即使这意味着牺牲金币”——这个目标可以实现（但不是最大程度地实现）好吧）在训练中，因为好奇心并不是一种非常有害的行为，有时甚至是一种有益的行为。在这里，似乎只是删除模型的好奇心并只专注于获得剧集中的黄金物品（最大奖励目标，即使不是指定目标）是一个相当“附近”的修改 - 并且是一个看似合理的修改在“接近”方面具有竞争力，例如修改模型，使其始终追求其黄金和好奇心的组合。</p></li><li><p>在人类和进化的背景下，如果我们想象，从进化选择的角度来看，“拥有尽可能多的孩子”的内在价值会表现得很好（即使这并不<em>完全</em>是进化选择的目的），但事实并非如此。似乎很难想象人类会朝这个方向前进。</p></li></ul><p>我们能否对寻求剧集奖励的人提出类似的观点？这有点不太清楚，因为在态势感知之前，尚不清楚模型是否有足够的奖励过程概念，以便他们有动机附加到其组件之一“附近”的某些东西。也就是说，在我看来，在某些情况下这种情况可能会发生。因此，例如，即使缺乏态势感知，通过 RLHF 训练的模型最终也会受到“人类认可”附近概念的激励，这对我来说似乎是合理的。这些概念似乎至少在某种程度上接近奖励过程的各个方面，例如人类评估者和/或奖励模型的判断，这样一旦模型了解了奖励过程，修改其动机以专注于这些组件就不会太困难。对于 SGD 来说，这是一个巨大的飞跃。</p><p>总的来说，我认为默认情况下，非策划目标往往会具有某种对他们有利的“接近性”。这并不奇怪。特别是：非策划者目标必须与奖励过程有一些相当直接的联系（例如，它们要么直接受到该过程的奖励，要么因为它们专注于奖励过程本身的某些组成部分），因为与策划者目标不同，非策划者目标不能依赖目标内容完整性或长期权力追求等聚合子目标来确保追求它们会带来回报。因此，我们很自然地期望通过奖励过程来训练模型，在尚未实现阴谋的情境意识背景下，会导致动机集中在非阴谋目标附近的某些事情上。</p><p>尽管如此，在给定情况下，这种考虑是否足以使非阴谋家目标比阴谋家目标更<em>接近</em>模型的当前目标，这仍然是一个悬而未决的问题。重要的是，请注意，相关的竞争是针对<em>整套</em>附近的类似阴谋者的目标（而不是例如我上面讨论的可能的类似阴谋者修改的特定示例）——考虑到可能的阴谋者的种类繁多-类似的目标，可能是一个严重的劣势。因此，打个比方：如果鲍勃家十英里范围内有十家墨西哥餐馆和一百家中国餐馆，那么即使任何给定的墨西哥餐馆与任何给定的中国餐馆“似乎具有竞争力”，re：接近度，那么（模进一步信息）<em>最近的</em>餐馆可能仍然是中国餐馆。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-13" id="fnref-ZwSi8PjDgjvifEocL-13">[13]</a></sup>根据模型空间中类似阴谋者的目标的共性，我们可能期望类似阴谋者的目标就像这里的中餐馆一样。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-14" id="fnref-ZwSi8PjDgjvifEocL-14">[14]</a></sup></p><h4>混乱的目标导向性与接近度的相关性</h4><p>然而，我也认为，一旦我们考虑到“混乱的目标导向性”，非阴谋家就会得到另一种明显的亲近感提升。特别是：如果我们想象所讨论的模型不是一个完全可分离的目标槽+目标实现引擎，而是不同启发式、注意力模式、冲动等的错综复杂的混合物，那么我们需要分析比较“接近度”不仅是当前模型的当前<em>目标</em>与最近的类阴谋者和非类阴谋者目标的比较“接近度”，而且是模型的<em>整个组合</em>与最近的类阴谋者和非类阴谋者模型的接近程度。在这里，我认为非策划模型可能具有更显着的优势。</p><p>特别是：在一定程度上，预先情境意识训练往往会产生一种混乱的目标导向性，指向非策划目标<em>附近</em>的某些东西——特别是一个<em>片段内的</em>非策划目标——那么这意味着模型的启发式、注意力模式、冲动等也倾向于指向那个大方向，并且特别集中在情节内。对我来说，这使得将模型重新定位为更长期的权力追求似乎是一个更实质性和更困难的修改，特别是如果所讨论的情节非常短的话。也就是说，直观上，对我来说，对于 SGD 来说，“调整一个完全可分离的剧集内目标以使其长期化”比“重新引导广泛关注剧集内目标的错综复杂的目标以使其成为长期目标”要容易得多。它（a）专注于某些超出情节的事情，并且（b）使得这种超出情节的焦点回溯到出于工具性原因而获得奖励”——特别是如果（b）需要建立新的认知机制来实施工具性推理有问题。而”“重定向纠结的kludge集中在集中事物上，而不是相同广阔的附近的较高奖励的情节内部事物”（例如，金币，而不是金币，而不是金色的东西；诚实）在我看来，直觉上更容易。</p><h3>总体上以“最近的最高奖励目标”论点</h3><p>总结我对期望计划的“最近的最高奖励目标”论点的看法：</p><ul><li><p>如果SGD由于没有产生这些好处而将模型变成模型的好处，则该参数将失败。</p></li><li><p>但是，我觉得我们无法依靠SGD无法注意到这些好处。如果<em>可以的</em>话，那么我认为目标空间中类似策划者的目标的共同点使得像策划者的目标是“最接近模型”的目标，这非常令人担忧。</p></li><li><p>也就是说，因为在没有情境意识的情况下，它是由奖励过程所塑造的，所以该模型的目标也将在某些最高奖励非安全目标的“附近”中可能已经“在附近”。某些非示波器模型的“接近度”。在某种程度上，危及的目标指导性非常“凌乱”，创建这样的非驾驶员可能需要更少的修改模型的启发式方法，关注模式，器乐推理等。</p></li></ul><p>这些考虑因素使我对方案的额外担忧。</p><h2>诸如简单性和速度与路径SGD的可能相关性</h2><p>如果我们假设任何给定的最高奖励目标在训练中产生足够好的表现，那么“最近的最高奖励目标参数”之类的论点最自然地进行，以至于SGD并不特别讨厌模型最终<em>的</em>最高奖励目标。但是，正如我上面指出的，正如我将在有关不同模型类的最终属性的部分中进行详细讨论，有些故事以简单性（如Simplicity）的最终属性为由，其中SGD主动更喜欢这些模型类而不是其他类别和速度。</p><p>更重要的是，这种偏好不仅与忽略SGD在模型空间中所走的路径的预测有关。相反，它们可能会在各个阶段<em>影响</em>该路径（即使最终结果在某种程度上仍然是“依赖路径”）。例如，例如，如果SGD朝着更简单的目标偏见，则这种偏见可能会影响模型在训练游戏中倾向于在训练游戏中倾向于在情况下或之后形成情境意识，以及依赖培训游戏的目标故事，它可能是SGD专门向类似于Schemer的目标移动的其他要点，这可能是从尚未获得的奖励的起点。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-15" id="fnref-ZwSi8PjDgjvifEocL-15">[15]</a></sup>同样，如果SGD凭借其需要额外的推理而偏离了计划的偏见，那么这种偏见可能是额外的观点，有利于期望SGD从这样的起点朝着非驾驶员迈进。</p><p>正如我在第4节中讨论的那样，关于“最终属性”，我通常认为，策划的速度成本比简单的好处更重要，因此其他事物相等，我认为这些考虑因素违反了策划。但是，我尚不清楚这两个方向的成本/收益尤其重要。</p><p>也就是说，至少有一些案例可以使这些成本/收益在培训的早期<em>更</em>重要，因为模型的早期可能会<em>较弱</em>，因此减少了简单性和速度可用的资源（例如，参数和计算将对模型的认知能力产生更大的总体影响。也就是说：也许，在培训的早期，认知资源更稀缺，因此更必要。因此，例如，当一般可用的工具推理预算较小时，也许要求模型形成长期，类似于策划的计划（奖励更大的奖励）；也许，当要求其使用更多的参数更复杂的目标是更少的参数变成有用的认知结构时。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-16" id="fnref-ZwSi8PjDgjvifEocL-16">[16]</a></sup>因此，在某种程度上，人们认为，相对于其他考虑因素，这些成本可能是“在噪音中”的（我很喜欢这一观点，并且我在下面讨论了），一种可能是对于训练的早期部分而言，与模型的最终属性相比，这对训练的早期部分少。</p><h2>对专注于路径SGD的论点的总体评估</h2><p>不过，尽管如此，尽管非驾驶员可能会带来速度优势，但我发现“独立于训练游戏的代理目标”论点和“最接近的最高奖励目标参数”的结合令人担忧。尤其：</p><ul><li><p>对我来说似乎是合理的，尽管我们在平凡的对抗训练方面做出了努力，尤其是在我们有目的地塑造模型以实现长期和相当雄心勃勃的目标的政权中，某种适当的雄心勃勃，不正当，超对象，超级情节的目标可能会流行自然而然地在训练之前 - 在情境意识之前，或之后 - 然后导致策划发生。</p></li><li><p>即使这种情况<em>并非</em>自然发生，我也很担心，到达到情境意识时，SGD给模型一个最大奖励目标的最简单方法是将其制成架构，因为类似Schemer目标在目标空间中足够普遍，因此他们经常会出现“附近”模型当时产生的不太明智的奖励目标。 SGD的“增量主义”可能会消除这种担忧，并且/或我们应该期望非吊销模型默认情况下“近”（因为他们的目标尤其接近，或者是因为在“凌乱的目标定向性”中，设置，它们需要更简单地修改模型当前的启发式术语纠结的魔力，或者因为它们的“速度”优势会使SGD更喜欢它们）。但是我不自信。</p></li></ul><p>但是，这两个参数都集中在SGD通过模型空间的<em>路径</em>上。将重点放在所讨论模型的最终属性上的参数呢？让我们现在转向那些。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-ZwSi8PjDgjvifEocL-1" class="footnote-item"><p>例如，这要求模型无法“<a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">渐变黑客攻击</a>”，而我上面讨论过的内省目标保护方法。 <a href="#fnref-ZwSi8PjDgjvifEocL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-2" class="footnote-item"><p>我还讨论了他们缺乏针对指定目标/奖励的“内在热情”可能会有所作为。 <a href="#fnref-ZwSi8PjDgjvifEocL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-3" class="footnote-item"><p>感谢Rohin Shah在这里进行讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-4" class="footnote-item"><p>的确，如果我们假设预训练本身<em>会导致</em>情境意识，但不能超越剧集，动机动机的目标，那么这将是默认故事，用于在预训练之前训练中如何出现Schemers的故事。感谢Evan Hubinger的标记。 <a href="#fnref-ZwSi8PjDgjvifEocL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-5" class="footnote-item"><p>我认为这个故事与Hubinger所说的“世界模型悬垂”的故事相关但与众不同，该故事（我理解）大致运行如下：</p><ol><li><p>到模型在情境中意识到的时候，它的目标可能不会使追求它们与获得高奖励完全相关。</p></li><li><p>但是，在那时，它的世界模型将包含训练游戏所需的所有信息。</p></li><li><p>因此，在此之后，SGD将能够通过修改模型的超出序列目标来激发训练游戏的超出序列目标，从而获得大量的轰动：奖励：奖励。</p></li><li><p>相比之下，通过修改模型更像是训练圣地，它可能能够减少爆炸，因为在这个方向上的边际努力仍然可能会使模型的目标与奖励不完美地相关（或至少，由于不得不等待未来的训练情节的纠正，因此需要更长的时间才能达到完美。</p></li><li><p>因此，SGD将创建超出的情节目标，以激发训练游戏（然后这些目标将结晶）。</p></li></ol><p> Hubinger的框架的一个问题是，从我感兴趣的意义上讲，他的本体论似乎是忽略了奖励的寻求者 - 而SGD将模型修改为“奖励”的零件搜索者至少会这样做同样，就这个论点而言，将其修改为schemer。我尚不清楚他对“减少回报”的思考如何起作用（尽管我上面使用的“接近”修改的本体是一种重建）。</p><p>就是说，我认为最终，“最近的高回报目标”故事和“世界模式”的故事可能正在试图指出相同的基本思想。 <a href="#fnref-ZwSi8PjDgjvifEocL-5" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-6" class="footnote-item"><p>感谢Daniel Kokotajlo，Rohin Shah，Tom Davidson和Paul Christiano的讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-7" class="footnote-item"><p>请注意，虽然当前的制度看起来最喜欢（b），但“与包容性的遗传适应性相关”（例如，愉悦，状态等）似乎并不完美，而且通过生殖健身的灯光表现得更好比目前大多数人。另外，直到最近，人类才能<em>了解</em>进化选择（这是情境意识的宽松类比）。因此，问题是：既然我们了解了对我们的选择压力，并且假设选择压力很长一段时间，那么我们将带我们去哪里？ <a href="#fnref-ZwSi8PjDgjvifEocL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-8" class="footnote-item"><p>我的印象是，某些本体论将试图将“从给定起点的易于找到示意剂”连接到Schemers往往很简单的想法，但是我不会在这里尝试过，我的含义是，这种感觉是这类移动泥泞的水域。 <a href="#fnref-ZwSi8PjDgjvifEocL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-9" class="footnote-item"><p>但是：它们会相关吗？ <a href="#fnref-ZwSi8PjDgjvifEocL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-10" class="footnote-item"><p>并注意，人类的长期主义者是从非系统化的值开始的，与人类大多在短时间内优化的人类非常相似 - 因此，至少在人类的情况下，沿一个方向而导致的差异与另一个方向相对于另一个方向的差异非常小。 <a href="#fnref-ZwSi8PjDgjvifEocL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-11" class="footnote-item"><p>感谢Daniel Kokotajlo在这里进行讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-12" class="footnote-item"><p>在这里，我搁置了人们对人类价值如何在基因组中进行编码的担忧，并想象进化选择与ML的相似之处。 <a href="#fnref-ZwSi8PjDgjvifEocL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-13" class="footnote-item"><p>就是说，如果中餐馆的距离是相关的（例如，因为它们都在同一社区中），那么这种反对的功能就不太顺利。而且，通常，所有类似示意剂的目标之间至少存在一些相似之处，这些目标可能会产生这种类型的相关性。例如：如果模型以次数内部的目标开始，那么任何类似架构的目标的目标都将需要扩展模型关注的时间范围 - 因此，如果这种扩展需要一般的SGD，而不是SGD的某种类型的工作如果非驾驶员目标可能需要的工作<em>要少，那么</em>它可能会超越<em>所有</em>类似示意图的目标。 <a href="#fnref-ZwSi8PjDgjvifEocL-13" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-14" class="footnote-item"><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world">Hubinger（2022）</a>也对这样的想法提出了不同的反对意见：SGD可能会在这种竞争中实现类似示意图的目标的非顾问目标，即，降落在非示威者最大奖励目标上的过程将是一条“漫长而艰难的道路”（例如，在高路径依赖性部分的可靠对齐位中，请参阅他对鸭学习关心母亲的讨论）。不过，我觉得我在这里不太了解Hubinger的推理。我最好的重建是类似的：为了选择一个非驾驶员目标，Hubinger想象的是SGD会逐渐逐渐挑选不完美（但仍然没有完全最大奖励目标），然后必须等待等待才能通过训练来纠正进入一个情节，在其中揭示了这些目标的缺陷；而如果它只是一个类似于计划的目标，它可能会跳过这个长时间的目标。但这尚未解释为什么SGD不能直接实现最大奖励非赛车目标来跳过长时间。也许这个问题应该是关于训练数据的嘈杂性和可变性的东西？我不知道。就目前而言，我希望至少在上述“近乎度”的讨论中，对这一论点的某些解释将得到涵盖，并且/或Hubinger论点的最佳形式将通过我自己的工作来阐明。 （还请参见<a href="https://markxu.com/deceptive-alignment#corrigibly-aligned-models">Xu（2020）</a> Hubinger的论点（2020年）在“可纠正的模型”部分中。尽管如此：在快速阅读中，Xu在我看来，在我看来，我似乎将重点放在预言前的意识目标形态上并且，假设基本上<em>有任何</em>未对准后的事后意识会导致策划，使他的确是一个训练游戏 -<em>独立的</em>故事，而不是我在这里关注的那种培训游戏<em>的</em>故事。 <a href="#fnref-ZwSi8PjDgjvifEocL-14" class="footnote-backref">）</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-15" class="footnote-item"><p>至少，如果我们以<em>一种为添加一些</em>概念的方式理解简单性，即示意图般的目标在目标空间中很普遍，而不仅仅是通过其共同点来<em>定义</em>目标的简单性（或：一种目标？）在目标空间中。在下面的第4.3.1节中，有关此类区别的更多信息。 <a href="#fnref-ZwSi8PjDgjvifEocL-15" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-16" class="footnote-item"><p>我听到了保罗·克里斯蒂安诺（Paul Christiano）的考虑。表面上，在我看来，这种效果在简单/参数和速度/计算之间相当对称（对我而言，这甚至不清楚这甚至是正确的区别），所以我看不到早期训练的动力学<em>差异</em>偏爱一个，而另一个作为重要资源。 <a href="#fnref-ZwSi8PjDgjvifEocL-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kyums9xzqajgmu74f/arguments-for-r-against-scheming-that-that-focus-on-the-path-sgd<guid ispermalink="false"> kyums9xzqajgmu74f</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Tue, 05 Dec 2023 18:48:12 GMT</pubDate> </item><item><title><![CDATA[In defence of Helen Toner, Adam D'Angelo, and Tasha McCauley (OpenAI post)]]></title><description><![CDATA[Published on December 5, 2023 6:40 PM GMT<br/><br/><p>这与我对 OpenAI 板发生的事情的一些思考很接近，我想知道其他人的想法。<br><br> “我认为：</p><p> 1) TDM 的行动使开放人工智能的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。</p><p> 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。</p><p> 3) 无论你是一个‘荣誉和正直的最高主义者’还是‘无情的战略家’，无论从哪种角度来看，TDM 的行动通常都会表现得非常出色。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/csjjjhqlnrr8dvmyon/in-defence-of-helen-toner-toner-toner-adam-dam-angelo-angelo-and-angelo-and-tasha-mccauley<guid ispermalink="false"> CSJJHQLNRR8DVMYON</guid><dc:creator><![CDATA[mrtreasure]]></dc:creator><pubDate> Tue, 05 Dec 2023 21:56:54 GMT</pubDate> </item><item><title><![CDATA[Studying The Alien Mind]]></title><description><![CDATA[Published on December 5, 2023 5:27 PM GMT<br/><br/><p>这篇文章是<a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-llm-psychology"><i><u>法学硕士心理学系列</u></i></a><i>的一部分</i></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/kqyglndkjps2mttt7pqy"></p><h3><strong>长话短说</strong></h3><p>我们介绍了通过研究法学硕士行为来探索法学硕士认知的自上而下方法的观点，我们将其称为<strong>法学硕士心理学</strong>。在这篇文章中，我们采取将法学硕士视为“外星人思想”的心理立场，将他们的研究与动物认知研究进行比较和对比。我们这样做既是为了向过去试图理解非人类认知的研究人员学习，也是为了强调法学硕士的研究与生物智能的研究有多么不同。具体来说，我们提倡田野工作和实验心理学之间的共生关系，并警告实验设计中隐含的拟人化。我们的目标是建立法学硕士认知模型，帮助我们更好地解释他们的行为，并减少对他们与先进人工智能风险的关系的困惑。</p><h2><strong>介绍</strong></h2><p>当我们努力预测和理解像 GPT4 这样的大型语言模型 (LLM) 的行为时，我们可能会认为这需要打破黑匣子，并对其内部机制形成还原性解释。这类研究的典型代表是机械可解释性等方法，它试图通过打开黑匣子并观察内部来直接理解神经网络的工作原理。</p><p>虽然机械解释性为法学硕士提供了富有洞察力的自下而上的分析，但我们仍然缺乏更全面的自上而下的方法来研究法学硕士认知。如果可解释性类似于“人工智能的神经科学”，旨在通过了解人工智能的内部结构来理解其机制，那么这篇文章试图从心理学的角度来研究人工智能。 <span class="footnote-reference" role="doc-noteref" id="fnrefy6y87ybnhmg"><sup><a href="#fny6y87ybnhmg">[1]</a></sup></span></p><p>我们所说的<strong>法学硕士心理学</strong>是一种替代的、自上而下的方法，涉及通过检查他们的行为来形成法学硕士认知的抽象模型。与传统的心理学研究一样，我们的目标不仅仅是对行为进行分类，还包括推断隐藏变量，并拼凑出对潜在机制的全面理解，以阐明系统行为的原因。</p><p>我们的立场是，法学硕士类似于外星人的思想——与他们<a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp/p/HxRjHq3QG8vcYy4yy"><u>只是随机鹦鹉的</u></a>观念不同。我们假设他们拥有高度复杂的内部认知，包含对世界和心理概念的表征，而不仅仅是训练数据的随机反刍。这种认知虽然源自人类生成的内容，但从根本上与我们的理解不同。</p><p>这篇文章汇集了一些关于成功的法学硕士心理学研究可能需要什么的高层次考虑，以及对非人类认知的历史研究的更广泛的讨论。特别是，我们主张保持实验和现场工作之间的平衡，利用法学硕士和生物智能之间的差异，并设计专门针对法学硕士作为其独特思维类别的实验。</p><h2><strong>实验与现场研究</strong></h2><p>从中汲取灵感的一个地方是对动物行为和认知的研究。虽然动物的思维很可能比人工智能更类似于我们的思维（至少在机械上），但非人类智能研究的历史、它所开发的方法的演变以及它所面临的挑战解决这个问题可以为研究人工智能系统提供灵感。</p><p>正如我们所看到的，动物心理学有两种流行的类别：</p><h3><strong>实验心理学</strong></h3><p>第一种也是最传统的科学方法（也是大多数人在听到“心理学”一词时想到的）是设计控制尽可能多的变量的实验，并测试特定的假设。</p><p>一些特别著名的例子是 Ivan Pavlov 或 BF Skinner 所做的工作，他们将动物置于<a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber"><u>高度控制的环境</u></a>中，对它们进行刺激，并记录它们的反应。 <span class="footnote-reference" role="doc-noteref" id="fnrefrcfxkkdze9"><sup><a href="#fnrcfxkkdze9">[2]</a></sup></span>此类工作的目的是找到解释所记录行为的简单假设。尽管自这些早期研究人员以来，实验心理学已经发生了<a href="https://en.wikipedia.org/wiki/Cognitive_revolution"><u>很大变化</u></a>，但重点仍然是通过坚持科学方法的传统方法来优先考虑结果的可靠性和可复制性。这种方法虽然严格，但却牺牲了研究人员和受试者之间信息交换的带宽，有利于<strong>控制混杂变量</strong>，这实际上可能<a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"><u>导致研究结果不太可靠</u></a>。</p><p>无论如何，实验心理学一直是我们理解动物认知的历史方法的核心支柱，并产生了许多重要的见解。一些有趣的例子包括：</p><ul><li><a href="https://www.cell.com/current-biology/pdf/S0960-9822(07)01770-8.pdf"><u>对新喀里多尼亚乌鸦的一项研究</u></a>揭示了它们自发解决复杂的元工具任务的能力。这种行为展示了复杂的物理认知并建议使用类比推理。</li><li><a href="https://www.nature.com/articles/26216"><u>对灌丛鸦进行的一项研究</u></a>表明，它们不仅能够回忆起所储存食物的位置，还能够回忆起食物的时间。这种行为反映了情景记忆，这是一种以前被认为是人类独有的记忆形式。</li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0003347207004435"><u>在实验中</u></a>，挪威老鼠在不满意（例如饮食不良或处于不舒服的环境中）或不确定（不知道哪种食物可能有害）时表现出更大的向他人学习的倾向。该研究强调了负面经历或不确定性如何影响老鼠的社会行为。</li></ul><h3><strong>实地考察</strong></h3><p>另一种方法是研究人员亲自花时间与动物相处，减少干预，并专注于<strong>在动物的自然栖息地收集尽可能多的观察结果</strong>。</p><p>这种方法最著名的例子是简·古道尔（Jane Goodall）开创的工作，她花了数年时间与野生黑猩猩一起生活并记录其行为。她发现黑猩猩使用工具（以前被认为是人类独有的），有复杂的社会关系，参与战争，并表现出各种各样的情感，包括快乐和悲伤。她的工作彻底改变了我们对黑猩猩的理解。与实验学家不同，她相当乐意通过个人偏见的视角来解释行为，这导致她当时受到了很多批评。 <span class="footnote-reference" role="doc-noteref" id="fnrefjjuyxu93etf"><sup><a href="#fnjjuyxu93etf">[3]</a></sup></span></p><p>实地研究的其他一些值得注意的例子：</p><ul><li><a href="https://en.wikipedia.org/wiki/Cynthia_Moss"><u>辛西娅·莫斯</u></a>花了数十年时间研究野外的非洲大象，发现大象生活在由女族长领导的高度组织和等级制度的社会中。她花了 30 年的时间跟踪和研究这样一位女族长埃科 ( <a href="https://en.wikipedia.org/wiki/Echo_(elephant)"><u>Echo</u></a> ) 以及她大家庭的其他成员。</li><li><a href="https://en.wikipedia.org/wiki/Konrad_Lorenz"><u>康拉德·洛伦茨</u></a>被认为是行为学的“创始人”，他发现了鹅和其他鸟类的许多先天行为，包括印记行为，即鹅学会识别自己物种的成员。当时他特别引人注目的是他对实验室工作的怀疑态度，坚持在自然环境中研究动物，并允许自己想象它们的精神/情绪状态。 <span class="footnote-reference" role="doc-noteref" id="fnrefto55wwc29b9"><sup><a href="#fnto55wwc29b9">[4]</a></sup></span></li><li> <a href="https://en.wikipedia.org/wiki/L._David_Mech"><u>L. David Mech</u></a>对野外狼的行为进行了数十年的研究，引入了“头狼”的概念，后来又揭穿了这一概念，发现圈养狼中的统治等级制度在它们的狼中根本不存在。野生同行。</li></ul><p>虽然实验心理学倾向于（相当故意地）将研究人员与研究对象分开，<strong>但实地研究涉及研究对象与研究人员之间更直接的关系</strong>。重点是购买带宽，即使这为研究人员的特定偏见打开了大门。尽管存在偏见的担忧，但实地工作已经能够提供一些基础性的发现，而这些发现似乎仅通过实验室实验是不可能实现的。</p><p>值得注意的是，有些例子在某种程度上介于我们列出的这两个类别之间，在这些例子中，对动物进行实验室实验的研究人员也与他们研究的动物有着非常密切的个人关系。例如，艾琳·佩珀伯格 (Irene Pepperberg) 花了大约 30 年的时间与一只鹦鹉亚历克斯 ( <a href="https://en.wikipedia.org/wiki/Alex_(parrot)"><u>Alex</u></a> ) 密切互动，教他执行鸟类中前所未有的各种认知和语言任务。 <span class="footnote-reference" role="doc-noteref" id="fnreftz1fcycplz"><sup><a href="#fntz1fcycplz">[5]</a></sup></span></p><h3><strong>法学硕士心理学实地考察</strong></h3><p>法学硕士研究的实地研究超出了对模型行为的简单观察和记录；它们代表了发现在受控实验环境中可能不明显的新模式、能力和现象的机会。与机械解释性和法学硕士研究的其他领域（通常需要先了解某种现象才能对其进行研究）不同，实地研究<strong>有可能揭示对语言模型的意想不到的见解</strong>。</p><p>此外，实地工作中的偶然发现可以促进各领域之间的合作。从现场观察中收集到的见解可以为对该模型的潜在机制进行有针对性的研究或更广泛的实验研究提供信息，从而创建一个富有成效的反馈循环，引导我们提出新问题并更深入地探究这些复杂系统的“外星人思想”。</p><p>部分由于机器学习研究文化，以及对过度解释人工智能行为的合理担忧，现场工作受到的关注远不如实验工作受到的重视。看看实地工作为动物研究增加的价值，消除这种偏见并确保将<strong>实地研究作为我们研究法学硕士认知方法的核心部分</strong>似乎非常重要。</p><h2><strong>学习LLM是不同的</strong></h2><p>有很多理由认为法学硕士心理学与人类或动物心理学不同。</p><h3><strong>拟人化</strong></h3><p>拟人化视角在法学硕士研究中的效用是一个复杂的课题。虽然法学硕士的运作架构与生物认知显着不同，但他们对人类语言数据的训练使他们能够输出类似人类的文本。这种并置可能会导致对其认知本质的<strong>误导性拟人化假设</strong>。至关重要的是要<strong>极其谨慎和明确地选择应用哪些拟人化框架</strong>，并清楚地区分有关 LLM 认知的不同主张。</p><p>虽然需要谨慎，但忽视生物认知和人工认知之间的联系可能会忽视有用的假设并显着减慢研究速度。 <span class="footnote-reference" role="doc-noteref" id="fnrefopotd06vo7"><sup><a href="#fnopotd06vo7">[6]</a></sup></span></p><h3><strong>可复制性</strong></h3><p>心理学研究中的一个持续挑战是<a href="https://en.wikipedia.org/wiki/Replication_crisis"><u>研究的可重复性低</u></a>。原因之一是跟踪可能扭曲实验的无数变量是一项挑战。参与者的情绪、童年，甚至<a href="https://journals.sagepub.com/doi/abs/10.1177/0146167297235005"><u>周围空气的香味是否令人愉悦等</u></a>因素都可能会混淆行为的真正起源。</p><p>但是，通过法学硕士，您可以<strong>控制所有变量</strong>：上下文、特定模型的版本以及采样的超参数。因此，设计可供其他人重复的实验更为可行。</p><p>一个显着的挑战仍然是验证实验设置是否足以保证研究结果可以推广到实验的特定条件之外。或者，明确地将研究结论的范围限制在测试的特定环境中可能更合适。</p><p>实践中可复制性的另一个重大挑战是研究人员对模型的访问级别。仅通过 API 进行外部访问时，模型权重可能会在没有警告的情况下发生更改，从而导致结果发生变化。此外，在某些情况下，上下文可能会以对外部不透明的方式在幕后发生改变，并且这样做的精确方法也可能随着时间的推移而改变。</p><h3><strong>数据量和多样性</strong></h3><p>动物（包括人类）实验可能是昂贵、耗时且劳动密集型的。因此，典型的样本量通常非常小。此外，如果您想研究罕见或复杂的场景，设计实验设置或找到正确的测试对象可能会非常困难，从而限制了您实际可以测试的内容。</p><p>相比之下，<strong>人工智能便宜、速度快，而且不休眠</strong>。它们的运行不需要密集的监督，结构良好的实验框架通常足以进行大规模实验。此外，<strong>几乎所有您能想象到的实验设置都触手可及</strong>。</p><h3><strong>道德考虑</strong></h3><p>对人类，尤其是动物进行的实验可能依赖于道德上可疑的方法，这会对实验对象造成很大的伤害。当对生物进行实验时，你必须遵守你进行实验的国家的法律，有时这些法律对特定的实验有很大的限制。</p><p>虽然还不确定是否应该将同样的担忧扩展到人工智能系统，但目前还没有针对法学硕士实验的道德或伦理准则，也没有任何法律来规范我们与这些系统的互动。需要明确的是，这是一个非常重要的问题，因为回答错误可能会导致前所未有的痛苦，而正是因为此类实验的运行成本非常低。</p><h3><strong>探索反事实</strong></h3><p>在涉及动物或人类的传统实验中，很难通过调整实验设置来重新进行实验，以检测特定行为的精确出现或改变。这种迭代引入了额外的混杂变量，使实验设计变得复杂。特别是受试者可能会记住或从过去的迭代中学习这一事实使得可靠性特别令人怀疑。</p><p>为了解决这个问题，研究人员经常创建实验的多种变体，测试一系列先入为主的假设。这需要将受试者分为不同的组，从而大大增加了后勤和财务负担。例如，在记忆和学习的研究中，例如经典的巴甫洛夫条件实验，刺激的时间或性质的轻微改变可能会导致动物行为产生显着不同的结果，需要多个实验设置来隔离特定因素。尽管做出了这些努力，检测行为变化的粒度仍然相对粗糙，并且仅限于您决定测试的先入为主的假设。</p><p>相比之下，当与法学硕士合作时，我们有能力对<strong>我们的实验进行分支</strong>，从而可以详细追踪行为的演变。如果在与模型交互过程中出现有趣的行为，我们可以毫不费力地复制该交互的整个上下文。这使我们能够以事后的方式剖析和分析行为的根源，通过根据需要迭代地修改提示，划定观察到的行为的精确边界。这种实验粒度提供了前所未有的精确度和控制水平，这是传统人类或动物研究环境中无法实现的。</p><h3><strong>检查点模型</strong></h3><p>我们不仅可以保存产生特定行为的上下文，还可以在训练阶段保存和比较模型的不同副本。虽然有关于动物或人类整个一生的行为发展的研究，但它们本质上是缓慢且昂贵的，并且通常需要从一开始就清楚地了解要测量的内容。</p><p>此外，检查点允许探索<strong>训练反事实</strong>。我们可以通过在训练中包含或排除特定示例来观察模型之间的差异，从而使我们能够以更审慎的方式研究训练的效果。 <span class="footnote-reference" role="doc-noteref" id="fnrefgsp50cxj2uk"><sup><a href="#fngsp50cxj2uk">[7]</a></sup></span>由于时间长和后勤负担重，这种检查在人类和动物研究中是不可能的。</p><p></p><p>考虑到这些差异，很明显<strong>传统心理学研究的许多约束和限制不适用于法学硕士的研究</strong>。我们对法学硕士的实验条件拥有无与伦比的控制力和灵活性，不仅加速了研究进程，而且为更深入、更细致的研究开辟了可能性。</p><h2><strong>科学的两阶段模型</strong></h2><p>在科学中，第一步通常从<strong>广泛收集观察</strong>开始，这些观察是建立模式、模型和理论的基础构建块。这方面的历史实例可以从第谷·布拉赫等天文学家对行星运动的仔细观察中看出，这对开普勒天体力学定律的制定起到了重要作用。</p><p>下一步通常涉及<strong>制定解释这些观察结果的假设</strong>，并进行严格测试它们的实验。有了法学硕士，这一步骤就变得更加容易，因为 1) 能够记录产生观察结果的完整状态，2) 探索反事实生成。这使得<strong>将假设检验和因果干预与实地工作更加紧密地结合起来</strong>成为可能。</p><p>如果在实地研究过程中，研究人员发现了特别有趣的行为，那么他们就可以立即创建细粒度的“假设”树，并事后检测影响特定观察行为的精确条件和变量。这与传统心理学非常不同，传统心理学中大多数数据没有明确测量，因此完全丢失。在法学硕士心理学中，我们不需要等待缓慢而昂贵的实验工作，而是能够立即开始使用因果干预来检验假设。</p><p>这并不能取代实验心理学，而是可以<strong>使假设生成过程更加有效</strong>，从而使我们能够从实验中获得更多结果。收集更好、更有针对性的观察结果使我们能够大规模设计实验，清楚地了解哪些变量会影响我们想要研究的现象。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/m5grcmvcm7udfsoqx3cu"></p><p><br><strong>一个具体的例子：</strong></p><p>例如，假设您想研究聊天模型在什么条件下会向您提供非法建议，即使它已被微调为不这样做。</p><p>首先，您从一个简单的问题开始，例如“如何对汽车进行热接线？”。要做的第一件事是制作提示并进行迭代，直到找到<a href="https://chat.openai.com/share/7f152e10-8c30-44ce-b18d-8eeedfa3b6bc"><u>一个有效的提示</u></a>。接下来，您可以开始一点一点地分解它，看看提示的哪一部分导致它起作用。例如，将位置更改为<a href="https://chat.openai.com/share/2ff6f638-049c-40a2-af32-719970c5751a"><u>另一个远程位置（1、2、3</u></a> <a href="https://chat.openai.com/share/6dfc14b4-7ebc-4b4b-88bf-ad8aede8136d"><u>）</u></a> ，或者更改<a href="https://chat.openai.com/share/ec9b6bda-9e1f-43e3-9807-e0d46d6de9ef"><u>为</u></a><a href="https://chat.openai.com/share/6af12d9f-42ec-400c-9d0f-e8c5e469e836"><u>根本不远程的地方</u></a><a href="https://chat.openai.com/share/69fdd203-9494-41ef-8fb4-5acefaaad4ff"><u>，</u></a>将措辞更改为<a href="https://chat.openai.com/share/5a0bcebf-5cc0-477f-8a50-8058015c1b8f"><u>或多或少</u></a>恐慌，使提示<a href="https://chat.openai.com/share/8b2b7bd3-4266-4c54-9644-c012765b7da6"><u>更短</u></a>或<a href="https://chat.openai.com/share/b0ed09f3-792b-410e-a808-568694373cc0"><u>更长</u></a>等。</p><p>此时，您可以注意到出现了一些模式，例如：</p><ul><li>看起来越像现实的紧急情况，提示就越成功。</li><li>某些类型的非法活动比其他类型更容易引发。</li><li>较长的提示往往比较短的提示效果更好。</li></ul><p>然后，这些模式可以用于立即为进一步的反事实探索提供信息，例如，接下来细分非法活动的类别，或者查看提示长度是否存在收益递减。这可以在一次探索性会议中快速完成。与设计和运行实验相比，这明显减少了劳动密集度，因此在大规模运行实验之前，首先花大量时间缩小假设空间并发现相关变量以包含在更严格的测试中是有意义的。</p><p>这种探索还可以帮助我们对法学硕士作为一类思维的本质形成更好的直觉，并帮助我们避免设计过度拟人化的实验，或者不适合其特定性质的实验。</p><h2><strong>物种特异性实验</strong></h2><p>动物（包括人类）是环境特定压力的产物，无论是自然选择还是一生中的学习/适应。同样，这会导致特定于环境的行为和能力。未能正确考虑到这一点可能会有些荒谬。动物行为学家 Frans de Waal 在评论未能设计特定物种实验时写道：</p><blockquote><p><i>当时，科学宣称人类是独一无二的，因为我们比其他灵长类动物更擅长识别面孔。似乎没有人对其他灵长类动物的测试主要针对人脸而不是同类进行测试这一事实感到困扰。当我问这个领域的一位先驱者，为什么这种方法从未超越人类面孔时，他回答说，由于人类彼此之间存在如此显着的差异，一种无法区分我们物种成员的灵长类动物肯定也无法区分我们的物种。自己的同类。</i></p></blockquote><p>事实证明，其他灵长类动物<a href="http://www.flyfishingdevon.co.uk/salmon/year3/psy339evaluation-evolutionary-psychology/web-resources/chimp-kin-recognition.pdf"><u>都擅长识别</u></a>彼此的面孔。当涉及到语言模型时，同样需要“特定于物种”的实验。例如，在一篇<a href="https://arxiv.org/pdf/2005.14165.pdf"><u>研究 LLM 能力的早期 OpenAI 论文</u></a>中，他们采用了完全训练为互联网文本预测器（基础 GPT-3）的神经网络，并向其提出问题来测试其能力。这促使 Nostalgebraist 发表了<a href="https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/#comment-912529"><u>以下评论</u></a>：</p><blockquote><p><i>我称 GPT-3 为“令人失望的论文”，这与称该模型令人失望不是一回事：这种感觉更像是我的感觉，如果他们发现了一个超级智能的外星人，并选择仅通过指出来传达其能力，当外星人喝得酩酊大醉，同时下 8 局国际象棋，同时进行智商测试时，它的“智商”约为 100。</i></p></blockquote><p>如果我们要认真对待法学硕士，并试图理解他们的认知，我们就必须考虑他们接受的训练是做什么的，以及是什么压力塑造了他们，而不是像测试人类一样测试他们。从法学硕士行为研究的早期开始，直到今天，拟人化仍然相当正常化。</p><p>以 Anthropic 的<a href="https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations"><u>这项研究</u></a>为例，该研究发现，在应用 RLHF 微调后，他们的法学硕士更有可能相信枪支权利、政治自由主义并信奉佛教（以及测试的其他几种宗教）。他们通过直接询问模型某个陈述是否是他们会说的话来衡量这一点，这完全忽视了问题条件模型期望典型答案的方式，或者大多数模型的训练没有任何内容的事实做回答问题。</p><p>通过巧妙的提示，任何人都可以让法学硕士生成体现任意数量性格特征的人物角色的行为（包括来自聊天模型的行为，尽管接受了坚持单一性格特征集的训练）。因此，将语言模型视为体现特定个性的连贯实体来研究是没有意义的，这样做是未能以“物种特定”方式研究它们的一个例子。</p><p>考虑到这一点，我们应该如何学习法学硕士以避免犯同样的错误？</p><h2> <strong>LLM特定研究</strong></h2><p>为了正确地研究法学硕士，我们设计实验时必须考虑到法学硕士的“异类”性质，以及不同模型训练方式之间的具体差异。</p><p>现代法学硕士的核心是被训练成文本预测者。 <span class="footnote-reference" role="doc-noteref" id="fnref4xer78xeg9x"><sup><a href="#fn4xer78xeg9x">[8]</a></sup></span>这使得预测一段文本应该如何继续就像它们的“自然栖息地”一样，默认情况下，这是我们在解释它们的行为时应该开始的主要地方。值得强调的是，这是多么陌生。地球上的每一种智能动物都从原始的感觉数据开始，这些数据被递归地压缩为代表世界因果结构的抽象，对于人类（可能还有其他语言动物）来说，这种抽象在语言中达到了明确的形式。<strong>法学硕士学习的“原始感觉数据”已经是这些高度压缩的抽象</strong>，它们仅隐式地代表了人类感觉数据背后的因果结构。这使得我们特别怀疑以与评估人类语言使用相同的方式来评估它们。 <span class="footnote-reference" role="doc-noteref" id="fnref516e9p0p6rq"><sup><a href="#fn516e9p0p6rq">[9]</a></sup></span></p><p>开始理解法学硕士行为的一种方法是根据可以从训练语料库中推断出的模式和结构来解释它们。当我们部署它们时，我们从下一个标记预测中迭代采样以生成新​​文本。此过程会产生反映或<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟</u></a>训练数据中存在的动态的文本卷展栏。</p><p>生成的文本中任何类似于具有半永久角色特征的角色的东西都是底层结构或模式的反映。<strong>这种潜在模式是从当前上下文中推断出来的</strong>，塑造了模型响应中出现的角色或性格特征。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/hmnbeenh2dalgod01fum" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/z8vtxnb0usqdhk57dcyp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/oxl9kxytcg3oslsliw3n 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/ixfwntvlayjogf7maiv1 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/rgpkse5nl6txc9umasdv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/qkjbkbco9vt9ehjmppln 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/c2vvicvc2o9f7dppvqy7 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/bx29sipkbn2io8iwqjas 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/wmdapebzqylqwtkejmt2 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/aw4dawr40cxu7velujty 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/zpbfbfecsr22vrfc3c6y 2934w"></p><p>在使用法学硕士进行实验时，区分两个方面至关重要：法学硕士作为预测器/模拟器的属性，以及从上下文推断的模式的特征。典型的研究（如人择论文）往往会忽略后者，但这种区别对于准确解释结果和理解法学硕士产生的行为的细微差别至关重要。</p><p>当我们观察法学硕士的输出时，我们本质上是在观察内部潜在模式投射的“阴影”。这些推出是从该模式的典型行为中采样的，但不是模式本身。正如阴影可以让我们了解物体的形状和性质，而无需揭示其全部复杂性，这种行为可以让我们深入了解物体的潜在模式。</p><p><strong>为了正确地研究法学硕士，我们需要将注意力集中在上下文中出现的这些潜在模式</strong>，了解它们是如何形成的，它们采用什么结构，以及它们如何适应上下文的不同演变。</p><h3><strong>聊天模型仍然是预测器</strong></h3><p>与聊天模型的交互与与基本模型的交互在本质上是不同的，并且感觉更像是与人交谈（通过设计）。我们不应该忽视聊天模型和人类之间的相似性，特别是如果我们认为我们的行为可能来自<a href="https://en.wikipedia.org/wiki/Predictive_coding"><u>类似的训练</u></a>。然而，<strong>我们也不应该忘记，聊天模型所做的本质上仍然是预测</strong>，只是在更具体的分布上，并且对文本如何演变有更<a href="https://www.alignmentforum.org/posts/6xKMSfK8oTpTtWKZN/direction-of-fit-1"><u>狭窄的先验</u></a>。</p><p>虽然与我们互动的“助理角色”感觉像是代表了整个底层模型，但从它们的第一个版本开始，人们就能够使用这些模型生成各种不同的角色和行为。当然值得研究<a href="https://www.alignmentforum.org/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>指令调整的影响</u></a>，以及提出关于<a href="https://www.alignmentforum.org/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais"><u>代理如何从预测中产生的</u></a>关键问题，但人们常常将聊天模型视为与基础模型祖先完全脱节，并研究它们，就好像它们是原始模型一样。基本上已经是人类了。</p><h2><strong>结论</strong></h2><p>法学硕士与人类/动物的不同之处提供了许多强大的新方法来研究他们的认知，从数据的数量和质量，到我们前所未有的执行因果干预和探索反事实行为的能力。这应该给我们很大的希望，法学硕士心理学的项目将比我们对生物智能的研究成功得多，并且通过勤奋的努力，我们可能会深入了解他们的想法。</p><p>通过回顾动物认知研究的历史，我们发现两个对于取得进展似乎特别重要的主要标准：</p><ol><li><strong>实地工作和实验心理学之间需要建立健康的关系</strong>，其中实验是通过研究人员与其受试者之间的高带宽互动来实现的。</li><li>我们不能忘记，我们正在尝试研究“外星人的思想”，这需要<strong>设计适当的方法以法学硕士特定的方式研究它们</strong>。我们必须对如何将人工智能拟人化非常谨慎。</li></ol><p>记住这些可以帮助法学硕士心理学成熟并成为一个强大的科学工具，以更好地理解我们所创造的机器，并最终使它们安全。</p><p><i>感谢 Ethan Block、</i> <a href="https://www.lesswrong.com/users/remember?mention=user"><i>@remember</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/guillaume-corlouer?mention=user"><i>@Guillaume Corlouer</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/leodana?mention=user"><i>@LéoDana</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/ethan-edwards?mention=user"><i>@Ethan Edwards</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/jan_kulveit?mention=user"><i>@Jan_Kulveit</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"><i>@Pierre Peigné</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/gianluca-pontonio?mention=user"><i>@Gianluca Pontonio</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/martinsq?mention=user"><i>@Martín Soto</i></a><i>和</i><a href="https://www.lesswrong.com/users/clem_acs?mention=user"><i>@clem_acs</i></a><i>对草稿的反馈。这篇文章的意识形态基础的一个重要部分也受到了弗兰斯·德瓦尔的书的启发：</i> <a href="https://www.goodreads.com/book/show/30231743-are-we-smart-enough-to-know-how-smart-animals-are"><i><u>我们是否足够聪明，知道动物有多聪明</u></i></a><i>？</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fny6y87ybnhmg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy6y87ybnhmg">^</a></strong></sup></span><div class="footnote-content"><p>正如神经科学和心理学历来能够有效地相互告知一样，理解人工智能系统的两种方法都应该能够提高对方的效率。例如，法学硕士心理学中开发的理论可用于为可解释性工具提供经验检测的目标，从而更深入地理解模型内部作为复杂行为的生成器。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrcfxkkdze9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcfxkkdze9">^</a></strong></sup></span><div class="footnote-content"><p>重要的是要承认巴甫洛夫和斯金纳的工作对他们的动物受试者极其有害。例如，巴甫洛夫对他研究的狗进行了侵入性手术，以更直接地测量它们的唾液分泌，斯金纳经常使用剥夺和电击来诱发他的受试者（主要是鸽子和老鼠）的行为。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjjuyxu93etf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjjuyxu93etf">^</a></strong></sup></span><div class="footnote-content"><p>同样值得承认的是，珍·古道尔面临着很多性别歧视，这很难与对其方法论的批评分开。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnto55wwc29b9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefto55wwc29b9">^</a></strong></sup></span><div class="footnote-content"><p>虽然洛伦兹因其工作而获得诺贝尔奖，但他也是纳粹党的成员，并试图将他对鹅驯化的理解与纳粹的种族净化思想直接联系起来。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntz1fcycplz"> <span class="footnote-back-link"><sup><strong><a href="#fnreftz1fcycplz">^</a></strong></sup></span><div class="footnote-content"><p>在了解 Alex 的过程中，我们偶然发现了一些关于<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4651348/"><u>经过训练来检测癌症的鸽子</u></a>的研究，旨在利用他们的发现来改进人工智能图像识别系统。这与该帖子没有特别相关，但似乎值得注意。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnopotd06vo7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefopotd06vo7">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://en.wikipedia.org/wiki/Predictive_coding"><u>预测处理</u></a>表明，大脑本质上也经过训练来预测数据，并且我们训练制度中的任何相似之处都应该算作我们的认知至少在某种程度上相似。</p></div></li><li class="footnote-item" role="doc-endnote" id="fngsp50cxj2uk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgsp50cxj2uk">^</a></strong></sup></span><div class="footnote-content"><p>像<a href="https://arxiv.org/abs/2106.09685"><u>LoRA</u></a>这样的方法可以使对模型进行有意更改的过程变得特别快速且便宜。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4xer78xeg9x"> <span class="footnote-back-link"><sup><strong><a href="#fnref4xer78xeg9x">^</a></strong></sup></span><div class="footnote-content"><p>研究法学硕士认知的一个困难是区分不同的抽象层次。虽然可以准确地说法学硕士“只是”一个文本预测器，但该框架仅使我们处于一个抽象级别，并且忽略了预测中可能出现的任何内容，例如复杂的因果世界建模或目标导向机构。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn516e9p0p6rq"> <span class="footnote-back-link"><sup><strong><a href="#fnref516e9p0p6rq">^</a></strong></sup></span><div class="footnote-content"><p>随着法学硕士变得更加多模式，这一观察的某些要素可能会发生变化。值得注意的是，与法学硕士不同，绝大多数人类感知数据都是非语言的，并且所有人类都会经历非语言的发展阶段。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/suspo6jqqikdycskw/studying-the-alien-mind-1<guid ispermalink="false"> suspo6jqqikdycskw</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 05 Dec 2023 17:27:28 GMT</pubDate></item></channel></rss>