<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 18:15:23 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Disagreements over the prioritization of existential risk from AI]]></title><description><![CDATA[Published on October 26, 2023 5:54 PM GMT<br/><br/><p>今年早些时候，<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><u>生命未来研究所</u></a>和<a href="https://www.safe.ai/statement-on-ai-risk"><u>人工智能安全中心</u></a>发表了公开信，将人工智能的存在风险（x风险）列为全球优先事项。 7 月，Google 研究员<a href="https://research.google/people/106776/"><u>Blaise Aguera y Arcas</u></a>和<a href="https://mila.quebec/en/"><u>MILA</u></a>附属人工智能研究人员<a href="https://mila.quebec/en/person/blake-richards/"><u>Blake Richards</u></a> 、 <a href="https://mila.quebec/en/person/dhanya-sridhar/"><u>Dhanya Sridhar</u></a>和<a href="https://mila.quebec/en/person/guillaume-lajoie/"><u>Guillaume Lajoie</u></a>在 Noema <span class="footnote-reference" role="doc-noteref" id="fnrefwvu2ckbk5ip"><sup><a href="#fnwvu2ckbk5ip">[1]</a></sup></span>上共同撰写了一篇<a href="https://www.noemamag.com/the-illusion-of-ais-existential-risk/"><u>专栏文章，</u></a>批评这些信件。简而言之，他们的论点是：</p><ul><li>流氓人工智能在不久的将来不太可能出现</li><li>人类的注意力是有限的</li><li>将人工智能带来的 x 风险作为全球优先事项可能会：<ul><li>造成限制有益人工智能的专横监管</li><li>掩盖当前人工智能的危害</li></ul></li></ul><p>我最初很困惑为什么这些作者（<a href="https://www.nature.com/articles/d41586-023-02094-7"><u>和</u></a><a href="https://www.science.org/doi/10.1126/science.adi8982"><u>其他人</u></a>）特别坚持认为 X 风险会分散人们对当前危害的注意力，而其他新闻主题却不会。</p><p> 9 月份，我会见了三位 MILA 附属研究人员，以更好地了解他们的立场<span class="footnote-reference" role="doc-noteref" id="fnrefxarylkocy"><sup><a href="#fnxarylkocy">[2]</a></sup></span> ，因为我相信人工智能安全社区将受益于理解他们对公开信的批评的复杂性，从而改善我们的沟通方式。</p><p>这篇文章应该被解读为邀请人们考虑具有不同世界观的人的想法，他们对将人工智能的 x 风险定位为全球优先事项的努力持怀疑态度。特别是，他们的怀疑并不等于否认这些风险。</p><h3>定义术语的重要性</h3><p>从一开始，受访者就表示他们更喜欢“流氓人工智能接管”这个词，而不是“人工智能带来的x风险”，因为他们厌倦了改变球门柱的趋势。 Sridhar 提到，她见过关于哪个代理是“混乱的种子”<a href="https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/"><u>的莫特贝利</u></a>策略，其中对话者会引入具有自己目标的代理人工智能的想法，然后将代理切换到人类组织，使用强大的人工智能。</p><p>拉乔伊补充道：</p><blockquote><p>当与那些非常严肃地对待存在风险的人讨论这个问题时，[...]当我阐明为什么我个人认为存在风险（以灭绝的形式）并不是真正令人担忧的原因时，通常发生的情况是谈话中的人会提出其他不存在但非常糟糕的风险，并有效地暗示我通过暗示存在性风险不太可能来否认这些风险。 [...]我[不否认]人工智能可能会在未来给我们的社会带来各种严重的负面危害。</p></blockquote><p>这表明我们可能希望在此类对话开始时更严格地定义我们的术语和场景，以避免使用莫特和贝利。</p><h3> AI安全社区是谁？</h3><p>确定哪些人可以加入<i>人工智能安全社区</i>可能是一个挑战。 Lajoie 将让 AI 做人类想要的事情的一般社区视为<i>对齐社区</i>，其中的一个子群体专注于 x 风险。前者包括在大型实验室从事狭义人工智能协调工作的人员以及从事人工智能伦理工作的人员。斯里达尔担心，公开信可能会将人员、优先事项和资金从前者推向后者。她认为后者的关注点过于数学化，而牺牲了前者所拥护的更全面的观点，例如强调流氓人工智能而不是<a href="https://www.lesswrong.com/tag/ai-misuse"><u>滥用</u></a>。理查兹评论说，后者似乎对倾听希望从不同角度解决风险的研究人员的意见不感兴趣，例如人工智能伦理学家受社会科学启发的观点。</p><p>关于 x 风险的工作主要是技术工作的看法让我感到惊讶，因为我经常看到强调 x 风险的组织提到需要进行治理来解决流氓人工智能和滥用场景<span class="footnote-reference" role="doc-noteref" id="fnrefbbu74skkwm"><sup><a href="#fnbbu74skkwm">[3]</a></sup></span> 。这暗示我们可能想在新闻稿中以及与人工智能研究人员的讨论中强调对滥用风险进行社会技术研究的重要性。</p><h3>让 X 风险成为全球优先事项</h3><p>这些研究人员并不反对对人工智能的 x 风险进行研究<span class="footnote-reference" role="doc-noteref" id="fnrefw8ivnfbc0p"><sup><a href="#fnw8ivnfbc0p">[4]</a></sup></span> ，但看到人工智能社区中受人尊敬的声音呼吁将其作为<i>全球优先事项</i>而感到困扰。</p><p>理查兹说：</p><blockquote><p>虽然我认为如果有人正在研究它，如果有关于它的会议，人们在谈论它，甚至专门致力于它的资助机构[...]，那就太好了，但这与成为全球优先事项是一个非常不同的情况。感觉全球变暖是每个人都参与其中的地方，世界各国政府都为此提供大量资金的地方，所有研究人员都需要谈论和思考的地方[...]，以及我们的监管真正受到这一影响的地方主要问题之一。</p></blockquote><p> Sridhar 强调了旨在避免 x 风险的人工智能监管的机会成本，特别是考虑到对于暂停或限制前沿模型开发的监管是否有利于人工智能安全<a href="https://forum.effectivealtruism.org/posts/6SvZPHAvhT5dtqefF/debate-series-should-we-push-for-a-pause-on-the-development"><u>尚无共识</u></a>。例如，她将此与致命自主武器的监管进行了对比，后者的风险更容易<a href="https://disarmament.unoda.org/the-convention-on-certain-conventional-weapons/background-on-laws-in-the-ccw/"><u>被理解，也更容易以强有力的有益方式采取行动</u></a>。</p><p>他们认为，由于涉及极端风险，让决策者面临x风险可能会让他们不知所措，从而导致他们忽视其他问题。理查兹提到：</p><blockquote><p>政治家的注意力范围非常有限，他们只会持有[与关注人工智能风险的人]讨论中的一小部分关键内容。如果你给他们一长串人工智能可能发生的事情，他们只会保留其中的一些，如果其中一个是流氓人工智能可能导致的灭绝，我向你保证他们会记住那个。因此，这将占据一个可能会出现“当前模型显示偏差，你不应该在抵押贷款计算中使用它们”的位置。</p></blockquote><p>此外，他们认为，通过在我们的警告中强调未来模型的能力，我们无意中暗示当前模型即将达到人类水平的能力。一方面，这可能导致一些使用这些模型的公司高估了自己的能力，并且<a href="https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html"><u>不明智地使用</u></a>它们。另一方面，这削弱了当前模型的失败。 <span class="footnote-reference" role="doc-noteref" id="fnrefcfyydqwz97b"><sup><a href="#fncfyydqwz97b">[5]</a></sup></span>他们共同认为，这意味着当这些不明智的使用<a href="https://www.wired.com/story/tessa-chatbot-suspended/"><u>造成伤害</u></a>时，强调 X 风险的人应对这种伤害负部分责任。</p><p>我不认为人工智能安全社区希望炒作现有模型或隐藏其缺陷。这表明，在讨论未来模型带来的 x 风险时，我们可能需要对当前模型的局限性添加警告。</p><h3>表达不确定性</h3><p>我要求研究人员分享他们的<a href="https://www.lesswrong.com/tag/transformative-ai"><u>变革性人工智能</u></a>的大致时间表。他们给出了两个犹豫回答的原因。</p><ol><li> Sridhar 认为，汇总研究人员的猜测（例如<a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><u>人工智能影响调查）</u></a>并不代表可靠的数据，结果可能会超出其相关性。 <span class="footnote-reference" role="doc-noteref" id="fnrefhthwft2gtwf"><sup><a href="#fnhthwft2gtwf">[6]</a></sup></span></li><li>在此基础上，理查兹认为，关注 x 风险的人们倾向于用凭空拉出的具有置信区间<span class="footnote-reference" role="doc-noteref" id="fnreftvj34wbh6hs"><sup><a href="#fntvj34wbh6hs">[7]</a></sup></span>的数字来量化他们的信念，用这些未经证实的数字进行计算，并得出结果他们实际上并不相信，而是假设是真的，因为涉及到数字。 <span class="footnote-reference" role="doc-noteref" id="fnref31ytwk7rcmf"><sup><a href="#fn31ytwk7rcmf">[8]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefht99vky8w9w"><sup><a href="#fnht99vky8w9w">[9]</a></sup></span></li></ol><p>理查兹用古生物学家的比喻，他试图确定一种早已灭绝的恐龙物种的颜色。可用来回答这个问题的证据很少且脆弱，诚实的做法就是承认她不知道。相反，如果她进行一项民意调查，并要求同事对不同颜色的置信区间给出概率，这会给结果带来可信度，但不会改变这个社区对答案没有重大洞察的事实。</p><p>虽然我们可以争论这种类比的适当性，但当我们讨论高度不确定现象的汇总预测时，可能值得牢记这些批评。</p><h3>时间线</h3><p>理查兹表示，他认为最近开始担心人工智能带来的 X 风险的人可能是因为对前沿模型的能力感到惊讶而过度更新。他没有承诺具体的时间表，但预计类似德雷克斯勒的<a href="https://www.lesswrong.com/tag/ai-services-cais"><u>CAIS</u></a>场景会在代理人工智能之前出现。他预计机器人技术的发展速度比硅人工智能要慢得多，这种延迟可能会限制人工智能从物理世界学习的能力。 Lajoie 认为，模型总体上会有所改进，但在非常具体的任务上会不断失败，从而使这些模型在一段时间内无法与人类的鲁棒性和通用性相匹配。</p><h3>缓解措施的长期性、不确定性和有效性</h3><p>为了更好地理解他们的批评的短期与长期方面，我提出了一个类似的情况，并要求他们解释他们的观点是否以及为什么会与他们在以下假设论证中对 x 风险的立场不同：</p><blockquote><p>气候变化正在造成直接危害，我们应该集中精力解决这些危害，而不是通过在未来几年减少温室气体（GHG）排放来预防长期风险。</p></blockquote><p>他们认为情况不同有两个原因。</p><ol><li> （从他们的角度来看）温室气体排放得到了更好的理解，目前的排放水平几乎肯定会在未来造成重大危害，而人工智能则不会。</li><li>斯里达尔指出，我们对如何解决气候变化有很好的理解：减少全球温室气体排放。 <span class="footnote-reference" role="doc-noteref" id="fnreft8qj7f4dan"><sup><a href="#fnt8qj7f4dan">[10]</a></sup></span>将此与人工智能的情况进行对比，在人工智能中，对于暂停是否会极大地提高安全性存在严重分歧。</li></ol><p>理查兹认为，我们对人工智能的看法与 20 世纪 70 年代的气候科学家相似，当时数据尚不清楚。如果他觉得人工智能目前的情况更类似于世纪之交的气候科学，他就会转而将人工智能安全作为优先事项。虽然尽早限制温室气体排放将使我们当前的转型变得更加容易，但理查兹反驳说，如果我们过早限制工业发展，我们就会极大地限制工业化的好处，就像今天限制人工智能会削弱其好处一样。</p><h3>一起工作</h3><p>在未来的合作方面，他们建议向政治家请愿，提出道德和 X 风险社区都认可的具体、强有力的有利政策，例如：</p><ul><li> AI系统的需求审计</li><li>需要可解释的人工智能系统</li><li>禁止开发致命性自主武器</li></ul><p>理查兹认为，放大双方较为温和的声音将使合作变得更容易。如果 CAIS 的声明说“人工智能的灭绝是研究人员需要考虑的事情”，他就会加入。</p><p>通过淡化这些言论，我们也许能够建立更广泛的联盟。另一方面，淡化信息可能不是采取有效行动预防 X 风险的最佳策略。每个向政策制定者或公众请愿的组织都必须自行决定这种权衡是否值得。</p><p><i>衷心感谢 Blake Richards、Dhanya Sridhar 和 Guillaume Lajoie 抽出时间与我交流。</i> </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwvu2ckbk5ip"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwvu2ckbk5ip">^</a></strong></sup></span><div class="footnote-content"><p>作者还在《经济学人》上发表了他们的专栏文章的<a href="https://www.economist.com/by-invitation/2023/07/21/fears-about-ais-existential-risk-are-overdone-says-a-group-of-experts"><u>简短版本</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxarylkocy"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxarylkocy">^</a></strong></sup></span><div class="footnote-content"><p>他们澄清说，虽然他们在很多方面同情人工智能伦理学家的立场，但他们本身并不是伦理学家，他们的观点可能并不反映其他批评人工智能 x 风险的人的观点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbbu74skkwm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbbu74skkwm">^</a></strong></sup></span><div class="footnote-content"><p>例如，参见<a href="https://futureoflife.org/open-letter/ai-principles/"><u>“生命未来研究所”</u></a> 、 <a href="https://80000hours.org/problem-profiles/artificial-intelligence/"><u>“80 000 小时”</u></a>和<a href="https://www.fhi.ox.ac.uk/fhi-experts-call-on-uk-government-to-prepare-for-extreme-risks-in-new-report-future-proof/"><u>“人类未来研究所”</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw8ivnfbc0p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw8ivnfbc0p">^</a></strong></sup></span><div class="footnote-content"><p>他们实际上鼓励这项研究，并认为它应该成为更广泛对话的一部分。他们将对短期、中期和长期风险的“均衡推介”持开放态度。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncfyydqwz97b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcfyydqwz97b">^</a></strong></sup></span><div class="footnote-content"><p>他们认为，人工智能伦理界往往过于关注模型的失败，而不愿意承认某些模型明显进化出了新功能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhthwft2gtwf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhthwft2gtwf">^</a></strong></sup></span><div class="footnote-content"><p>虽然他们没有详细说明这一点，但我认为研究人员可能不同意，例如<a href="https://www.youtube.com/watch?v=xoVJKj8lcNQ&amp;t=21s"><u>这种表述</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvj34wbh6hs"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvj34wbh6hs">^</a></strong></sup></span><div class="footnote-content"><p>理查兹在与最近认可 x 风险的主流人工智能研究人员的对话中看到了这种趋势，并将其归因于他们对理性主义文献的采用。例如，他怀疑是否有人能够以 95% 的置信区间预测复杂的社会现象。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn31ytwk7rcmf"> <span class="footnote-back-link"><sup><strong><a href="#fnref31ytwk7rcmf">^</a></strong></sup></span><div class="footnote-content"><p>理查兹将其称为“科学剧场<i>”</i> ，与<a href="https://en.wikipedia.org/wiki/Security_theater"><u>安全剧场</u></a>进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnht99vky8w9w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefht99vky8w9w">^</a></strong></sup></span><div class="footnote-content"><p>在他<a href="https://arxiv.org/pdf/2206.13353.pdf"><u>关于存在风险的报告</u></a>中，卡尔史密斯通过合取计算了概率，并认为他必须随后对其进行修改，并提到“我可能会将其提高一点——也许提高一两个百分点，尽管这是特别不原则的[.. .] — 考虑并不严格符合上述所有前提的权力寻求场景”。这表明当结果与我们的直觉相冲突时，我们不愿意直接相信此类计算的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt8qj7f4dan"> <span class="footnote-back-link"><sup><strong><a href="#fnreft8qj7f4dan">^</a></strong></sup></span><div class="footnote-content"><p>有人可能会说，关于减少温室气体排放的最佳方法存在很多争论，但对于必须采取的行动方向达成了一致，这并不是限制人工智能发展的情况。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk<guid ispermalink="false"> eAT2dXAngXxFRTQLn</guid><dc:creator><![CDATA[Olivier Coutu]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:54:12 GMT</pubDate> </item><item><title><![CDATA[Changing Contra Dialects]]></title><description><![CDATA[Published on October 26, 2023 5:30 PM GMT<br/><br/><p> <span>2008 年，我在本科生语言学方面写了这些关于反对派舞蹈方言的文章（</span> <a href="https://www.jefftk.com/final-papers/thesis/contra-thesis-1-0-0.pdf">pdf</a> ）。有些人物有变化，但这种变化受到兼容舞蹈的需要的限制。例如，散步时主要有以下三种手部姿势：</p><p> <a href="https://www.jefftk.com/promenade-hand-positions-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/q1yebebzwth1jmssawn4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/q1yebebzwth1jmssawn4 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/rl4ou9gn0yyxkfv7xlwn 1100w"></a></p><div></div><p></p><p>如果云雀/绅士习惯了蝴蝶式长廊，并将右手放在知更鸟/女士的肩膀上，而知更鸟/女士则期待礼节性转身长廊，并将右手放在背后，那么舞蹈就不会流畅起来就好像他们都立刻把手放到了对方期待的地方。这意味着每次舞蹈通常都会达成一致的位置。而且由于人们更有可能在当地旅行而不是去更远的地方旅行，因此这些职位将具有区域模式。这与语言的限制非常相似，因此与方言有类比，也是语言学系接受我的论文的原因。</p><p>在过去的十五年里，我观察到了一些变化，但我很好奇这是什么样子，所以我<a href="https://www.jefftk.com/p/contra-dance-dialect-survey">进行了一项在线调查</a>来收集一些数据。这是我发现的：</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-rlt-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/qnrubiwck7vecr9yqyb5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/qnrubiwck7vecr9yqyb5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/wkmrvjpb4y74ytifshlm 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-rlt-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zl0krs9pvgvkynduwoei" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zl0krs9pvgvkynduwoei 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/mv1gyiyp9v9ja8uf3eea 1100w"></a></p><div></div><p></p><p>这对我来说是一个很大的改变。我记得新英格兰有一种非常强烈的“我们不会左右手动手”的文化（缅因州除外，我在 2008 年没有去过那里，但因用手而闻名）我记得我对于是否要接受它感到矛盾。我认为正在发生的部分原因是，当有人向你提供他们的东西时，如果不接受它，你会觉得很粗鲁，所以变化就会蔓延。也许语言中最接近的类比是<a href="https://www.astralcodexten.com/p/give-up-seventy-percent-of-the-way">不尊重的级联</a>？</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-prm-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zlzzor17b32mc7cuv12l" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zlzzor17b32mc7cuv12l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/aei0j1lugscov88s6peq 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-prm-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/cilwg3hg9ag3xb7imnno" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/cilwg3hg9ag3xb7imnno 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/jahm5l8ujdoqf2s4wzmq 1100w"></a></p><div></div><p></p><p>看来溜冰者的长廊正在扩大？我认为这是另一个部分由舞蹈机制驱动的因素：当有人把手放在你面前时，你更容易看到正在发生的事情，而且 Skater&#39;s 在提供有趣变化的机会方面也稍好一些。</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-star-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/ikjxmxbmitokyf7oyojd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/ikjxmxbmitokyf7oyojd 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/vasqthpa6mrzywai7tvc 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-star-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/pg5g6qkzca7we4jitqda" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/pg5g6qkzca7we4jitqda 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/vnngrbndu9kn94wqsuad 1100w"></a></p><div></div><p></p><p>由于我掌握的 2008 年南方数据太少，很难从这张地图上看出发生了什么。我的印象是，南方正在慢慢转向全国其他地区使用的握腕星星，但我很想听听长期南方舞者的意见！</p><p> （更多关于其余调查数据的帖子。）</p><br/><br/><a href="https://www.lesswrong.com/posts/qoSXGdvvwrHXXadDb/changing-contra-dialects#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qoSXGdvvwrHXXadDb/changing-contra-dialects<guid ispermalink="false"> qoSXGdvvwrHXXadDb</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:30:11 GMT</pubDate> </item><item><title><![CDATA[5 psychological reasons for dismissing x-risks from AGI]]></title><description><![CDATA[Published on October 26, 2023 5:21 PM GMT<br/><br/><p><br>人们激烈争论通用人工智能是否会构成生存威胁。</p><p>大多数论点探讨该主题的<a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__">概念</a>、 <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1">技术</a>或<a href="https://www.lesswrong.com/posts/3siLbdd4338gfTM7g/ai-pause-will-likely-backfire-guest-post">治理</a>方面，并且基于原因、逻辑和预测。有时，每一种对立的立场看起来都很可靠，我很难判断哪一种更接近现实，尤其是当我不是该主题的专家时。<br><br>面对如此复杂和不确定的情况，人们常常根据个人喜好和情绪做出判断，这篇文章深入探讨了人们可能忽视 AGI 带来的生存风险的心理原因。我并不是说这个立场没有合理的论据，因为确实有。这不是本文的范围。<br></p><h2></h2><h2>自身利益和自我审查</h2><p>艾伦是一位在一家科技巨头从事法学硕士项目的高级经理。他认为人工智能的发展是他在企业晋升、赚大钱的绝佳机会。</p><p>艾伦是一次科技会议的演讲者，演讲结束后，一名记者询问他对存在风险的看法。</p><p>此刻，艾伦的脑子里正在思考几件事。<br><br>人工智能是他获得比所有朋友更好的职业生涯的门户。而且，用他赚到的所有钱，他将能够买下妻子梦想中的房子，送女儿去她想要的任何大学，而不在乎钱。</p><p>关于 X 风险的想法威胁到了他的梦想，因此他试图避免它们，并向自己证明，他实际上是在做一件好事。 “不仅仅是我将从这项技术中受益。它将使世界变得对每个人来说都变得更加美好，而那些人工智能厄运者正试图阻止它。”</p><p>艾伦也不能公开表示人工智能是一种威胁。他公司的公关部门不会喜欢这样，这会给他带来很多麻烦。因此，他唯一可以肯定的是，他的公司拥有优秀的网络安全团队，他们在部署模型之前进行了广泛的测试，因此没有理由担心。</p><p></p><h2>否认作为应对机制</h2><p>乔普是一位受人尊敬的机器学习科学家，他有一个通过否认压力来应对压力的不健康习惯。<br><br>他有一个自恋的母亲，在他小时候没有对他表现出任何感情。这对他来说是痛苦的，他学会了通过否认并告诉自己一切都好来应对这个问题。正因为如此，每次他经历恐惧或焦虑时，他都会试图说服自己，导致焦虑的原因实际上并不存在。因此，即使他对x风险感到焦虑，他也试图说服自己这些风险实际上并不存在。<br><br>在前一个故事的主人公艾伦展示他的作品的同一个科技会议上，一名记者走近乔普，问他是否担心人工智能带来的生存风险。</p><p>乔普显然很恼火，他告诉记者，唯一相信这些风险的人是散布恐惧的勒德分子。他告诉她，他的团队花了 6 个月的时间才发布最后一个法学硕士学位，因为他们进行了彻底的安全测试，并且在任何关键应用程序中，他们总是让人们了解情况。他还回忆起尤德科西做出的一个重要预测，结果证明是完全错误的。 “我们每天都会提出新的安全想法。我们将解决所有即将出现的问题”。看来他更想向自己证明这一点，而不是向记者证明这一点。</p><p>采访结束后，他仍然感到愤怒和恼火，并在脑海中重复着他刚刚讲述的所有反对存在风险的论点。</p><p></p><h2>社会压力和回声室</h2><p>艾达是一位年轻的人工智能伦理研究员，当她准备在一场备受瞩目的科技会议上展示她关于人工智能风险的工作时，她感到既兴奋又紧张。在同一次会议上，我们遇到了之前故事中的艾伦和乔普。她尊重他们俩，并很高兴听到他们的演讲。</p><p>艾伦谈到了公司强大的网络安全措施，并表示相信人工智能不会构成威胁。乔普紧随其后，强调了他的团队在人工智能工作中采取的负责任的步骤。观众明显放心了，艾达开始怀疑她自己的部分研究，这些研究重点关注通用人工智能带来的生存风险。</p><p>轮到她的时候，艾达犹豫了。她的演示包括有关 X 风险的幻灯片，但回想起艾伦和乔普的自信，她太急于让自己看起来很愚蠢，所以她跳过了这些幻灯片。相反，她对人工智能的未来表示乐观，并表示有很多有才华和负责任的人，所以没有理由担心。</p><p>当她离开舞台时，艾达感到如释重负，但也有一种不安的感觉，因为她并不完全诚实。为了感觉好一点，她试图说服自己，这些专家比她更了解人工智能领域，所以她对 X 风险的担心可能是错误的。</p><p></p><h2>人们没有人工智能世界末日的形象</h2><p>达里奥和克劳德是人工智能播客的共同主持人。他们最新一集的主题是人工智能带来的生存风险。克劳德担心这些风险，而达里奥仍然持怀疑态度。</p><p>录音开始后，克劳德概述了我们将无法控制比人类聪明得多的人工智能的论点，这可能会导致灾难。达里奥不能完全同意。在他看来，AGI还很遥远，所有这些威胁都感觉太遥远和抽象，所以他无法想象它如何对人类构成威胁。</p><p>达里奥说：“人工智能不可能控制核武器之类的东西。我认为高度相信 AGI 会想要摧毁我们只是一种推测”。为了相信，达里奥需要远见。一些生动而令人信服的东西，而不是抽象的原则，但每次他向克劳德询问有关末日的具体场景时，他的答案都是模糊和不确定的。答案依赖于诸如正交论或“黑猩猩无法控制人类，而我们将像黑猩猩一样实现通用人工智能”等高级思想。<br><br>克劳德承认想象具体的世界末日场景很困难，但坚持认为缺乏明确的例子并不能消除风险。他说：“仅仅因为我们无法描绘出一幅图画，并不意味着危险不存在。”</p><p>达里奥并不相信。他认为，有些人对自己的抽象想法过于确定，而忽视了我们正在逐步取得进展的现实。</p><p></p><h2>人工智能厄运的边缘化</h2><p>Eli 是一位紧跟最新技术趋势的软件开发人员。最近，他开始关注人工智能的发展，并对围绕人工智能存在风险的讨论感兴趣，他注意到关于这些风险的讨论往往是情绪化的。</p><p>当他浏览推特时，他发现了一条由一位情绪化的末日论者写的帖子，他对自己的观点没有任何怀疑。他提醒伊莱环保人士，他认为“总有一些人相信天塌下来了。”<br><br>伊莱认为，气候变化是一个复杂的问题，需要大量的思考和努力才能解决，但过度情绪化和过度自信的活动人士弊大于利。它们会惹恼人们，并毒害任何深思熟虑的讨论。</p><p> Eli 将人工智能危言耸听者视为类似的人。他认为，它们破坏了人工智能安全社区的形象，也让讨论更现实和近期的问题变得困难，比如错误信息的传播，或者权力集中在人工智能实验室手中。</p><p>一位朋友向 Eli 推荐了达里奥和克劳德主持的人工智能播客中以 x 风险为主题的一集。伊莱决定在上下班途中听听。危言耸听的克劳德听起来像是对存在的风险感到恐惧，尽管他看起来是个聪明人，但伊莱立即将他归类为激进分子，并没有太认真地对待他的论点。 “啊，又一位焦虑的勒德分子”他想。</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/yPfotACWkRdWbubfy/5-psychological-reasons-for-dismissing-x-risks-from-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yPfotACWkRdWbubfy/5-psychological-reasons-for-dismissing-x-risks-from-agi<guid ispermalink="false"> yPfotACWkRdWbubfy</guid><dc:creator><![CDATA[Igor Ivanov]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:21:48 GMT</pubDate> </item><item><title><![CDATA[5. Risks from preventing legitimate value change]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p> VCP 带来的第二个风险涉及阻碍合法价值变化。接下来，我将考虑合法价值变革可能受到阻碍的机制，以及这是否以及何时可能构成道德伤害的积极根源。</p><h2>阻止合法的价值改变是真正的风险吗？</h2><p>乍一看，人们可能会怀疑阻碍合法价值变革是否会带来真正的风险。承认某些价值改变的情况是没有问题和合法的，并不一定意味着阻止或阻碍这种改变在道德上是或总是应该受到谴责的。换句话说，人们可能怀疑积极阻止某些类型的价值变化是否构成道德伤害。接下来，我将尝试证明确实如此，至少在某些情况下如此。</p><p>首先，考虑一个系统地或任意地阻止人们从事有可能引起合法价值变化的理想追求的社会。在大多数情况下，这样的社会会被认为体现了一种有问题的“社会威权主义”形式，因为它严重损害了个人自由。在这里，自由可以（至少）有两种方式来理解。其一，我们可以提及“共和自由”（参见Pettit，2001），即免于任意行使权力的自由。在这种情况下，治理结构必须受到限制（例如通过法治）权力的任意使用。或者或此外，我们可能还关心保护第二种类型的自由，即自决自由。例如，查尔斯·泰勒（Charles Taylor，1985）在包括卢梭和穆勒等思想家在内的政治思想的悠久传统的基础上，提出了一种自由概念，这种概念体现了“一个人有效决定自己的程度和一个人生活的形态”（第 213 页）。因此，只要追求合法价值变化的能力是自决权的体现，那么系统地破坏这种自由（通过阻碍合法价值）变化的过程在道德上是有问题的。</p><p>通过观察价值自决的自由在康德的绝对命令（Kant，1785/2001）、罗尔斯的无知之幕（Rawls，2001）或米尔的元归纳论证下是反思稳定的，可以进一步支持这一论点。 <span class="footnote-reference" role="doc-noteref" id="fnrefq371my13fs"><sup><a href="#fnq371my13fs">[1]</a></sup></span>为了自由（MIll，1859/2002）。至于第一个概念，绝对必要的是我们应该“以你希​​望他们对所有人采取行动的方式对待他人”。因此，如果我想保护我经历合法价值改变的能力（例如，以理想追求的形式），我也应该想保护<i>你</i>做同样的事情的能力。罗尔的无知之幕抓住了这样一种观念，即公正的社会是在我们不知道我们在该社会中采取什么具体立场或属性的前提下设计的。与之前类似，该论点认为，如果我要设计一个社会的结构而不知道我将在该社会中占据什么位置，我将希望创建一个社会，让每个人都有价值自决的自由，包括他们经历合法价值变化的能力受到保护。最后，密尔对自由的元归纳论证指出，基于我们之前多次错误地判断什么是道德上对/错或什么对我们有价值的观察，我们应该在一定程度上追求我们目前对道德的最佳理解。除了价值观和道德之外，我们还应该保留一个“自由领域”，以保护我们未来改变想法的能力。虽然穆勒本人特别提出了言论自由的论点，但后来的评论家主张一种解释，将同一论点扩展到更普遍的个人生活计划的自主性和可修改性的合理性（参见，例如，Fuchs，2001；Bilgrami，2015） 。因此，阻碍或阻碍合法价值变革的行为者、机构或流程缺乏反思性认可，并构成值得道德关注的真正风险。</p><h2>破坏合法价值变化的机制（“价值崩溃”）</h2><p>我现在将继续讨论人们追求自主价值变革的能力可能受到阻碍的机制。我将提供的叙述重要地受到 Nguyen 对“价值崩溃”（2020）的叙述的启发，然后我概括了其中的背景。简而言之，阮将价值崩溃描述为一种现象，即价值的过度表达（例如使用指标或其他量化和/或简化价值的方法）恶化了我们对价值观和世界的认知态度。正是这种认知态度的恶化最终导致了我们价值自决能力的削弱。价值崩溃到底是如何发生的？</p><p>首先，阮指出我们的价值观塑造了我们的注意力。例如，如果我重视自己的健康，我会区别地关注我认为对我健康的事物（例如运动、食物、补充剂）。如果我重视好奇心，我就更有可能寻找并注意到别人的这种特质。接下来，一旦我们对我们的价值观采取明确的操作化——例如，通过使用指标——这就会导致更明确的注意力边界。阐明了我们<i>关心</i>的内容后，也澄清了我们<i>不</i>关心的内容。例如，在教育中引入 GPA 分数将学生和教育工作者的注意力集中在这些分数很好地体现的教育方面。不幸的是，明确的值通常无法捕获我们关心的所有内容。因此，虽然使用它们有很多充分的理由（稍后会详细介绍），但它们的引入是以将注意力从上述指标无法很好地捕捉到的事情上转移开为代价的，即使有时这些可能是我们实际上所做的事情反思地关心。例如，就 GPA 分数而言，这些可能是智慧、求知欲或公民责任感等。</p><p>鉴于我们解释我们所看重的东西的能力不完善，我们依赖于健康的“错误新陈代谢”（该术语取自Wimsatt（2007））。错误代谢是这样一种想法：如果我们不能在第一次尝试中完美地捕捉到某些东西（这是像人类这样有界限且容易出错的生物在试图驾驭复杂世界时的常态），我们就依赖于注意和整合的方法错误或偏差，从而迭代改进我们最初的尝试。然而，这就是问题的核心，因为我们的注意力介导了我们注意到、感知并因此可以了解的内容，施加更窄和更清晰的注意力界限削弱了我们进行所述错误代谢的能力。如果我确信唯一值得一读的文学形式是希腊神话，而我从不涉足这一类型，那么我可能永远无法享受魔幻现实主义、法国存在主义或 18 世纪《狂飙突进》的乐趣。或者，回到 GPA 分数的例子，因为智慧和求知欲并没有被它们很好地捕捉到，人们担心这些分数的引入可能会排挤或破坏我们识别和促进那些更微妙的价值观的共同能力。</p><p> In summary, explicated values, by rigidifying our boundaries of attention, tend to weaken our ability to notice, integrate and ultimately enact that which is valuable but isn&#39;t already captured by the explicit value statement. <span class="footnote-reference" role="doc-noteref" id="fnref6l9p9lqvwp"><sup><a href="#fn6l9p9lqvwp">[2]</a></sup></span> As a result, the explicated values tend to become &#39;sticky&#39; in the sense of coming to dominate the individual&#39;s practical reasoning, while those values that aren&#39;t well captured by the explication tend to get neglected and—potentially—lost. As such, the dynamic of value collapse undermines a person&#39;s ability for self-determined value change.</p><p> It is worth noticing that the explication of values typically involves simplifying them, ie, stripping away (potentially important) detail, nuance and contextuality. Theodore Porter (1996), in his work on the history of quantification, notes that quantified knowledge focuses on some invariant kernel that has various context-sensitive nuances stripped off. Importantly, quantification is not simply a bad thing; to the contrary, quantification allows information to travel between contexts and aggregate easily. For example, without quantifying students&#39; performance in the form of grades, it would be difficult to compare and aggregate a student&#39;s performance across contexts as varied as maths, literature, sports and history. However, there are also costs of quantification. Improvements in portability and aggregability come at the cost of a loss of nuance, subtlety and context sensitivity. As such, I am not trying to argue that we should always avoid quantifying or explicating what we care about. Instead, I argue that it is important to pay attention to the way explication tends to lead to an impoverishment of our understanding of values, and our ability to alter, develop or refine our values over time. It is only when we are aware of the varied trade-offs that we can make informed choices about when and how much we want to rely on mechanisms of value explication.</p><p> Various other cases of value collapse are already readily observable. We have already mentioned the academic context, where metrics such as GPA or citation numbers may threaten richer notions of wisdom, intellectual curiosity or truth seeking. Other examples include health metrics (eg, step count, BMI or calories burnt), which may fail to capture fuzzier and more personalised notions of health, wellbeing or performance; or, in the context of social media, the number of likes or watch time, which may override thicker and more prosocial notions of, eg, learning, connection or aesthetic value.等等。</p><h2> ...in the case of (advanced) AI systems</h2><p> However, the question we are most interested in in the context of this essay, however, is what does this concern look like when extrapolated to the context of advanced AI systems?</p><p> Generally speaking, advanced AI systems will intensify this very effect. They do so in two main ways. First, in the case of processes that already rely on value explication, advanced AI systems will be able to optimise for a given set of explicated values more strongly, thereby further weakening the error metabolism of said process. To exemplify this tendency, let us consider the usage of AI systems in the context of processing job applications. This application-processing AI will (unless endowed with some mechanism to counteract this tendency) optimise its performance based on whatever values the firm has already been able to identify and capture (eg, in the form of evaluative criteria). If there are features of an application dossier which would be interesting to the company, but fall outside of the current evaluative scheme, the AI evaluator will be insensitive to those features. Importantly, the AI evaluator will be more effective at this than a human evaluator—both at identifying applicants that fit the evaluative criteria, as well as at ignoring applicants that do not fit those criteria. Furthermore, a human evaluator might gain novel insights about what they care about during the application process (eg, new insights about how to identify suitable candidates, or about what role specification the company would ideally be hiring for in the first place). In contrast, relying on the AI systems to do this work will (again, by default) tend to undercut those possibilities, thereby reducing the possibility for serendipitous (relative to the defined evaluative scheme) outcomes and insights. While initially AI systems might only be used for pre-processing job applications, increasingly they will come to determine more and more of the decision-making process, and as such, the likelihood to detect errors in the current specification will decrease. What could have been an open-ended process of value development and refinement on behalf of the human evaluator or the firm as a whole has become a closed and convergent process reifying whatever notion of value we started out with. Rather than denying that optimising for a given (even if imperfect) set of assumptions is never pragmatically justified, the concern is that, if unchecked, what this will lead to over time is a deterioration of our ability to ever come to improve on our existing assumptions or evaluative criteria.</p><p> A second way in which advanced AI systems will intensify the effects we can already make out today is that AI progress will allow similar processes to be deployed more broadly and across more areas of life. Consider, for example, George, who uses his &#39;AI assistant&#39; (or similar AI application) to think about what new hobby to pick up or what new community to join. If said system optimises its suggestion for some fixed set of values that George presumably already possesses, or if it optimises to make him more predictable, this reduces serendipity and interferes with George&#39;s ability to identify and come to value truly novel ways of being. Highly generalised AI systems could come to have similar effects in virtually all areas of life.</p><p> If the above account of value collapse is right, expanding the reach of practices of hyper-explication threatens the rich and subtle values that we have, both individually and as a society. In all of these cases, the more widespread their use, the more significant their effect. The better those systems become, the better they are at optimising for whatever has been explicated, and the more widespread the usage/adoption of such systems, the more pervasive the described effect. Value collapse threatens the possibility of genuinely open-ended value exploration found at the core of aspirational pursuits and legitimate value change. As such, as a society, and insofar as we care about protecting the possibility of legitimate value change, both individually and collectively, we need to think carefully about how the development and deployment of advanced AI will affect, interfere with and potentially substantially undermine this possibility.</p><p> A final remark is necessary before we close. While throughout this essay I have emphasised the <i>risks</i> that arise in the context of AI and value malleability, I want to conclude by acknowledging that there is also an important possibility of legitimate, <i>technology-assisted</i> value change. In other words, it is worth asking whether we could design machines that amplify, rather than diminish, our ability to develop and refine our values. Considering a similar idea, Swierstra (2013) writes: &#39;Specific technologies can modulate moral choices and decisions, by making some options more pressing or attractive, and others less so. But this doesn&#39;t imply a loss of morality, or moral decay. It can easily lead to more or better morals, for instance by enlarging the moral community of stakeholders. In this way, the relation between technology and morality appears as a marriage between two partners who neither have nor desire absolute autonomy. Is it possible to say something about the quality of that marriage?&#39; While I question Swierstra&#39;s usage of &#39;easy&#39; in the above extract, I agree that the possibility of AI systems <i>augmenting</i> individual self-determination and moral reasoning is an interesting idea to explore further. Insofar as this essay shall contribute to a larger philosophical and technological project, said project is just as much concerned with avoiding risks as it is concerned with realising progress and upside potential. However, in order to do so successfully, we first require significant further progress in, on the one hand, understanding the mechanisms of legitimate value change, and, on the other hand, our ability to build AI systems that reliably act in the intended way. Furthermore, we should be attentive to the way the deck is stacked at present: while it&#39;s conceivable that, say, our future AI assistants will be highly-reflective moral agents helping us to figure out reflexively endorsed values, the illegitimate exploitation of our value malleability arguably looms as a more salient and more imminent threat. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnq371my13fs"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq371my13fs">^</a></strong></sup></span><div class="footnote-content"><p> I am grateful to TJ for pointing me to this specific argument and the relevant discussion in the literature.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6l9p9lqvwp"> <span class="footnote-back-link"><sup><strong><a href="#fnref6l9p9lqvwp">^</a></strong></sup></span><div class="footnote-content"><p> Two clarifications: I don&#39;t claim that the explication of values always or invariably leads to a narrowing of attention. For my purposes, it is enough to argue that, in practice, this is what tends to happen. Furthermore, it may be possible to explicate values while strengthening error metabolism in some other way. This may be a fully satisfying solution to the problem described here. The present claim is merely that, as things currently stand, and without careful and deliberate consideration, explication is typically accompanied with a weakened error metabolism.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/KeHGinpj2WyzDEQAx/5-risks-from-preventing-legitimate-value-change#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KeHGinpj2WyzDEQAx/5-risks-from-preventing-legitimate-value-change<guid ispermalink="false"> KeHGinpj2WyzDEQAx</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:35 GMT</pubDate> </item><item><title><![CDATA[4. Risks from causing illegitimate value change]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p> Unaligned AI systems may cause illegitimate value change. At the heart of this risk lies the observation that the malleability inherent to human values can be exploited in ways that make the resulting value change illegitimate. Recall that I take illegitimacy to follow from a lack of or (significant) impediment to a person&#39;s ability to self-determine and course-correct a value-change process.</p><h2> Mechanisms causing illegitimate value change</h2><p> Instantiations of this risk can already be observed today, such as in the case of recommender systems. It is worth spending a bit of time understanding this example before considering what lessons it can teach us about risks from advanced AI systems more generally. To this effect, I will draw on work by Hardt et al. (2022), which introduces the notion of &#39;performative power&#39;. Performative power is a quantitative measure of &#39;the ability of a firm operating an algorithmic system, such as a digital content recommendation platform, to cause change in a population of participants&#39; (p. 1). The higher the performative power of a firm, the higher its ability to &#39;benefit from steering the population towards more profitable [for the firm] behaviour&#39; (p. 1). In other words, performative power allows us to measure the ability of the firm running the recommender systems to cause exogenously induced value change <span class="footnote-reference" role="doc-noteref" id="fnref8dyuxt9u9t"><sup><a href="#fn8dyuxt9u9t">[1]</a></sup></span> in the customer population. The measure was specifically developed to advance the study of competition in digital economies, and in particular, to identify anti-competitive dynamics.</p><p> What is happening here? To better understand this, we can help ourselves to the distinction between &#39;ex-ante optimization&#39; and &#39;ex-post optimization&#39;, introduced by Predomo et al. (2020). The former—ex-ante optimisation—is the type of predictive optimisation that occurs under conditions of low performative power, where a predictor (a ﬁrm in this case) cannot do better than the information that standard statistical learning allows to extract from past data about future data. Ex-post optimisation, on the other hand, involves steering the predicted behaviour such as to improve the predictor&#39;s predictive performance. In other words, in the first case, the to-be-predicted data is fixed and independent from the activity of the predictor, while in the second case, the to-be-predicted data is influenced by the prediction process. As Hardt et al. (2022) remark: &#39;[Ex-post optimisation] corresponds to implicitly or explicitly optimising over the counterfactuals&#39; (p. 7). In other words, an actor with high performative power does not only predict the most likely outcome; <i>functionally speaking,</i> it can perform as if it can choose which future scenarios to bring about, and then predicts those (thereby being able to achieve higher levels of predictive accuracy).</p><p> According to our earlier discussion of the nature of (il)legitimate value change, cases where performative power drives value change in a population constitute an example of illegitimate change. The people undergoing said change were in no meaningful way actively involved in the change that the performative predictor affected upon said population, and their ability to &#39;course-correct&#39; was actively reduced by means of (among others) choice design (ie, affecting the order of recommendations a consumer is exposed to) or by exploiting certain psychological features which make it such that some types of content are experienced as locally more compelling than others, irrespective of said content&#39;s relationship to the individuals&#39; values or proleptic reasons.</p><p> What is more, the change that the population undergoes is shaped in such a way that it tends towards making the values more predictable. To explain this, first note that the performative predictor (ie, the firm running the recommender platform) is embedded in an economic logic which imposes an imperative to minimise costs and increase profits. As a result, a firms&#39; steering power will specifically tend towards making the predicted behaviour <i>easier</i> to predict, because it is this predictability that the firm is able to exploit for profit (eg, via increases in advertisement revenues). This process has been well documented to date. For example, in the case of recommendation platforms, rather than finding an increased heterogeneity in viewing behaviour, studies have observed that these platforms suffer from what is called a &#39;popularity bias&#39;, which leads to a loss of diversity and a homogenisation in the content recommended (see, eg, Chechkin et al. (2007), DiFranzo et al. (2017), &amp; Hazrati et al. (2022)). As such, predictive optimisers impose pressures towards making behaviour more predictable, which, in reality, often imply pressures towards simplification, homogenisation, and/or polarisation of (individual and collective) values.</p><h2> ...in the case of (advanced) AI systems</h2><p> While current-day recommender platforms may already possess a significant degree of performative power, it is not hard to imagine that more advanced AI systems come to be able to exploit human psychology and socio-economic dynamics yet more powerfully. There is a priori not much reason to expect that humans&#39; evolved psychology would be particularly robust against an artificial superintelligent &#39;persuader&#39;. Beyond recommender systems powered by highly advanced AI systems, we can also imagine an increasingly widespread use of personalised &#39;AI assistants&#39;. We can imagine the tasks of an AI assistant as helping the person they are assisting to meet their needs, achieve their goals or satisfy their preferences. Given the difficulty of comprehensively and unambiguously specifying what a person wants across a wide range of contexts, such &#39;assistance&#39; will typically involve some element of guessing (ie, predicting) on the parts of the AI systems. As such, and given the dynamics discussed above, and if not successfully designed to avoid VCP, such &#39;AI assistants&#39; are likely to &#39;improve their performance&#39; by causing substantive and cumulative changes in individuals&#39; goals and values. What is more, just like with the above case of recommender algorithms, the nature of change induced by such an &#39;AI assistant&#39; will tend (unless relevant corrective measures are taken) towards a simplification of the data structures that are being predicted—in this case the human&#39;s values. To illustrate this: an &#39;AI assistant&#39; might be able to improve its performance measures by effectively narrowing my culinary preferences to always ask for burger and fries, instead of occasionally being interested in exploring novel flavours and dishes. The picture painted above is concerning because, for one, it undermines the person&#39;s ability to self-determine their values, and, for two, the ensuing change might bring about what effectively is an impoverishment of what once were richer or more subtle values. The described effect does not require any &#39;maliciousness&#39; on the side of the AI systems, but can arise as &#39;merely&#39; unintended consequence of their way of functioning.</p><p> What is important to recognise is that the described mechanism has the potential to reach both &#39;far&#39; and &#39;deep&#39;—in other words, it has the potential to substantially affect both our public and private lives; people&#39;s economic, social, political and personal beliefs, values, behaviours and relationships. Think for example of the pervasive presence of advertisement (reaching, these days, even far into the private sphere via smartphones and television) and of how much of economic behaviour is shaped by it everyday. Or, think about how the same mechanism can affect opinion formation, public deliberation and, consequently, political outcomes. As such, AI-powered advertisement or political propaganda, as well as other applications we may not even be able to conceive of at this point, hold tremendous potential for harm.</p><p> Let us recap the mechanics underlying the risk of illegitimate value change that we have identified here. Generally speaking, we are concerned with cases where a predictive optimiser (or a process that acts functionally equivalent to one) comes to be able to systematically affect that which it is predicting. If the phenomenon that is being predicted involves what some set of humans want, the performative optimiser will come to influence those humans&#39; values. If one assumes human values to be fixed and unchangeable, one might conclude that there is nothing to worry about here. However, recognising the malleability of human values makes this risk stand out as salient and potentially highly pressing. Advanced AI systems will become increasingly more capable at this form of performative prediction, thus exacerbating whatever patterns we can already make out today. The wider these AI systems will be deployed in relevant socio-economic contexts—such as advertisement, information systems, our political lives, our private lives and more—the more severe and far-reaching the potential harm. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8dyuxt9u9t"> <span class="footnote-back-link"><sup><strong><a href="#fnref8dyuxt9u9t">^</a></strong></sup></span><div class="footnote-content"><p> The observed change in the population might not be exclusively due to value change. However, it can (and typically will) involve a non-trivial amount of value change, and as such, performative power is a relevant measure to understand the phenomena of exogenously induced value change.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/qZFGPJi3u8xuvnWHQ/4-risks-from-causing-illegitimate-value-change#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qZFGPJi3u8xuvnWHQ/4-risks-from-causing-illegitimate-value-change<guid ispermalink="false"> qZFGPJi3u8xuvnWHQ</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:26 GMT</pubDate> </item><item><title><![CDATA[3. Premise three & Conclusion: AI systems can affect value change trajectories  & the Value Change Problem]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p> In this post, I introduce the last of three premises—the claim that <i>AI systems are (and will become increasingly) capable of affecting people&#39;s value change trajectories</i> . With all three premises in place, we can then go ahead articulating the Value Change Problem (VCP) in full. I will briefly recap the full account, and then give an outlook on what is yet to come in post 4 and 5, where we discuss the risks that come from failing to take VCP seriously.</p><h1> Premise three: AI systems can affect value change trajectories</h1><p> The third and final premise required to put together the argument for the Value Change Problem is the following: AI systems are (and will become increasingly) capable of affecting people&#39;s value change trajectories.</p><p> I believe the case for this is relatively straightforward. In the previous post, we have seen several examples of how external factors (eg other individuals, societal and economic structures, technology) can influence an individual&#39;s trajectory of value change, and that they can do so in ways that may or may not be legitimate. The same is true for AI systems.</p><p> Value change typically occurs as a result of moral reflections/deliberation, or learning of new information/making new experiences. External factors can affect these processes—eg by affecting what information we are exposed to, by biasing our reflection processes towards some rather than other conclusions,etc.—, thereby influencing an individual&#39;s trajectory of value change. AI systems are another such external factor capable of similar effects. Consider for example the use of AI systems in media, advertisement or education, as personal assistants, to help with learning or decision making, etc. From here, it&#39;s not a big step to recognise that, with the continued increasing in capabilities and deployment of these systems, the overall effect AI systems might come to have over our value change trajectories.</p><p> Posts 4 and 5 will discuss all of this in more detail, including by proposing specific mechanisms by which AIs can come to affect value change trajectories, as well as the question when they are and aren&#39;t legitimate.</p><p> As such, I will leave discussing of the third premise and this and swiftly move on to putting together the full case for the Value Change Problem:</p><h1> Putting things together: the Value Change Problem</h1><p> Let us recap the arguments so far. First, I have argued that human values are malleable rather than fixed. In defence of this claim, I have argued that humans typically undergo value change over the course of their lives; that human values are sometimes uncertain, underdetermined or open-ended, and that some ways in which humans typically deal with this involves value change; and, finally, that transformative experiences (as discussed by Paul (2014)) and aspiration (as discussed by Callard (2018)), too, represent examples of value change.</p><p> Next, I have argued that some cases of value change can be (il)legitimate. In support of this claim, I have made an appeal to intuition by providing examples of cases of value change which I argue most people would readily accept as legitimate and illegitimate, respectively. I then strengthened the argument by proposing a plausible evaluative criteria—namely, the degree of self-determination involved in the process of value change—which lends further support and rational grounding to our earlier intuition.</p><p> Finally, I argued that AI systems are (and will become increasingly) capable of affecting people&#39;s value change trajectories. (While leaving some further details to posts 4 and 5.)</p><p> Putting these together, we can argue that ethical design of AI systems must be taken seriously and find ways to address the problem of (il)legitimate value change. In other words, we ought to avoid building AI systems that disrespect or exploit the malleability of human values, such as by causing illegitimate value changes or by preventing legitimate ones. I will refer to this as the &#39;Value Change Problem&#39;.</p><p> <i>What does it mean for AI design to take the problem of (il)legitimate value change seriously?</i> Concretely, it means that ethical AI design has to try to i) understand the ways in which AI systems do or can cause value change, ii) understand when a case of value change is legitimate or illegitimate and iii) build systems that do not cause <i>il</i> legitimate value change, and permit (or enable) legitimate value change.</p><p> In the remaining two posts, I will discuss in some more depth the risks that may result from inadequately addressing the VCP. This gives raise to two types of risks: risks from causing illegitimate value change, and risks from preventing legitimate value change. For each of these I want to ask: What is the risk? What are plausible mechanisms by which these risks manifest? What are ways in which these risks manifest already today, and what are the ways in which they are likely to be exacerbated going forward, as AI systems become more advanced and more widely deployed?</p><p> In the first case— <i>risks from causing illegitimate value change—</i> , leading with the example of recommender systems today, I will argue that performative predictors can come to affect that which they set out to predict—among others, human values. In the second case— <i>risks from preventing legitimate value change—</i> , I will argue that value collapse—the idea that hyper-explication of values tends to weaken our epistemic attitudes towards the world and our values—can threaten the possibility of self-determined and open-ended value exploration and, consequently, the possibility of legitimate value change. In both cases, we should expect (unless appropriate countermeasures are taken) the same dynamic to be exacerbated—both in strength and scope—with the development of more advanced AI systems, and their increasingly pervasive deployment.</p><h2> Brief excursion: Directionality of Fit</h2><p> A different way to articulate the legitimacy question I have described here is in terms of the notion of &#39; <strong>Directionality of Fit</strong> &#39;. In short, the idea is that instead of asking whether a given case of value change is (il)legitimate, we can ask which &#39;direction of fit&#39; ought to apply. Let me explain.</p><p> Historically, &#39;directionality of fit&#39; (or &#39;direction of fit&#39;) was used to refer to the distinction between values and beliefs. (The idea came up (although without mentioning the specific term) in Anscombe&#39;s <i>Intention</i> (2000) and was later discussed by Searl (1985) and Humberstone (1992).) According to this view, beliefs are precisely those things which change to fit the world, while values are those things which the world should be fitted to.</p><p> However, once one accepts the premise that values are malleable, the &#39;correct&#39; (or desirable) direction of fit ceases to be clearly defined. It raises the question of when exactly values should be used as a template for fitting the world to them, and when it is acceptable or desirable for the world to change the values. If I never accept the world to change my values, I forgo any possibility for value replacement, development or refinement. However, as I&#39;ve argued in part before and will discuss in some more detail in post 5, I might reason to consider myself morally harmed if I lose that ability to freely undergo legitimate value change.</p><p> Finally, this lens also makes more salient the intricate connection between values and beliefs: the epistemic dimensions of value development, as well as the ways values affect our epistemic attitudes and pursuits.</p><br/><br/> <a href="https://www.lesswrong.com/posts/yPnAzeRAqdko3RNtR/3-premise-three-and-conclusion-ai-systems-can-affect-value#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yPnAzeRAqdko3RNtR/3-premise-three-and-conclusion-ai-systems-can-affect-value<guid ispermalink="false"> yPnAzeRAqdko3RNtR</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:14 GMT</pubDate> </item><item><title><![CDATA[2. Premise two: Some cases of value change are (il)legitimate]]></title><description><![CDATA[Published on October 26, 2023 2:36 PM GMT<br/><br/><p> In the prior post, I have defended the claim that genuine value change is possible, and thus, that a realistic account of human values understands them to be malleable. In this section, I will argue for the claim that <i>some cases of value change are legitimate while others are illegitimate</i> . In other words, I argue that, at least in principle, something substantial is to be said about what types of value change are legitimate vs. illegitimate. Let us call this the &#39;value change legitimacy&#39; claim (VCL). [1]</p><p> To do so, I first explain what I mean by value change legitimacy. Then, I make an appeal to intuition or common sense by providing a handful of examples that, I expect, most people would not hesitate to accept as examples of both legitimate and illegitimate cases of value change. Finally, I suggest a plausible evaluative criteria identifying (il)legitimate value change, which provides further rational grounding for the common sense intuitions invoked earlier, as well as a starting point for developing a comprehensive account of value change legitimacy.</p><p> ([1]: To clarify the scope of the VCL claim, let me briefly clarify what I am <i>not</i> trying to make claims about. First, I am not trying to make claims about whether all cases of value change are either legitimate or illegitimate (ie, whether legitimacy/illegitimacy is a collectively exhaustive way to classify cases of value change). Second, I don&#39;t mean to exclude the possibility that legitimacy comes in degrees, or that there might exist grey areas with respect to whether a given case is (il)legitimate.)</p><h1> Clarifying the notion of value change legitimacy</h1><p> First, and maybe most important, value change legitimacy, as I mean to propose it here, is a <i>procedural</i> notion. In other words, in asking about value change legitimacy, I am asking about the way in which a given value change has <i>come about</i> . This is in contrast to asking whether the value change <i>as such</i> is morally good or bad. As we have seen in the former post, the latter question is confusing (and maybe confused) because the value change itself implies a change of the evaluative framework. As a result, it is unclear on what basis the goodness or badness of the value change <i>as such</i> should be evaluated. However, I claim that there still exist morally relevant differences between different cases of value change (other than the moral status of the value change as such) -- in particular, the procedural question: whether the manner in which the value change has <i>come about</i> conforms with certain normative standards that make the change acceptable, unproblematic and thus legitimate, or—on the other hand—objectionable, problematic and thus illegitimate.</p><p> (The choice of the world &#39;legitimacy&#39; might seem confusing or unfortunate to some. While I am happy to hear better suggestions, it seems worth clarify that I chose this term with reference to how &#39;legitimacy&#39; is typically used in political philosophy, where it refers to procedural properties of political institutions. According to me, this analogy goes surprisingly deep—but will have to leave exploring this in more detail to another time.)</p><p> Of course, in practice, it may not always be clear whether specific cases of value change are legitimate or not, or how, in general, we ought to decide on what counts as legitimate vs. illegitimate. In fact, these questions will be subject to disagreement and rational deliberation. For the purposes of the argument in favour of the Value Change Problem, it suffices for me to establish that there is <i>something</i> substantive to be said about the difference between legitimate and illegitimate cases&#39; value change—that the difference exists—even if important questions remain about how <i>exactly</i> to do so.</p><p> That said, I will in the latter part of this post put forth a specific proposal as to what we might mean by legitimacy--namely, the degree of self-determination involved in the process of value change. While I do believe value self-determination is a critical aspect of value change legitimacy, I do not think my proposal provides close to a comprehensive account of value change legitimacy, able to deal satisfactorily with a wide range of practical intricacies that arise. For example, for future work, I am interested in &#39;stress testing&#39; my current account of legitimacy by looking at cases in, eg, parenting and education, and using the resulting insights to build on and improve the current, provisoinal account. As such, I suggest the proposal put forth below to be understood in the spirit of wanting to provide a productive starting point, rather than as an end point.</p><h1> The case for value change legitimacy</h1><h2> Argument form intuition/common sense</h2><p> Having clarified what we mean by legitimacy in the context of value change, let us now explore the case for VCL.</p><p> I will start by describing two examples that I believe people will widely agree represent cases of both legitimate and illegitimate value change--that is, defending VCL by providing an existence proof of sorts.</p><p> As such, let us consider the following examples of value change.</p><p> First, consider David. David does not currently hold a deep appreciation for jazz. For example, when he recently accompanied his friend to a concert—herself an ardent jazz lover—, he secretly fell asleep for some parts of the performance due to boredom. However, Daniel has an inkling that there may be something deeply valuable about jazz that he has not yet come to fully apprehend. This motivates him to spend several weeks attentively listening to jazz music and going to more concerts. While he initially struggles to pay attention, over time, his experience of the music starts to change until, eventually, Daniel comes to deeply appreciate jazz, just like his friend does.</p><p> On the other hand, consider Elsa. Elsa, too, does not initially have an appreciation jazz and also comes to love it. In her case, however, the change is the result of Elsa joining a cult which, as a central pillar of their ideology, venerate a love of jazz. The cult makes use of elaborate means of coercive persuasion, involving psychological techniques as well as psychoactive substances, in order to get all of their members to appreciate jazz.</p><p> Each of these are cases of value change as characterised earlier. However, and I would argue most people would agree, there are morally significant differences between these cases: while Daniel&#39;s case appears (largely) unproblematic and legitimate, Elsa&#39;s one appears (largely) problematic and illegitimate. To put it another way, it seems to me like we would lose something important if we were to deny that there are morally relevant difference between these cases which are <i>not</i> reducible to the nature of the value change (in this case the love for jazz). We want to be able to point at Elsa&#39;s case of value change and argue that it is problematic and should be prevented, and we want to be able to say that Daniel&#39;s case of value change is fine and does not need to be prevented, without in either case basing our argumentation on whether or not loving jazz is a morally acceptable or not. As such, I argue that the relevant difference we are picking up on here pertains to the legitimacy (or lack thereof) of the value change <i>process</i> (in the sense I&#39;ve described it above).</p><p>到目前为止，一切都很好。 But, beyond appealing to common sense, can we say anything substantive about what makes these cases different?</p><h2> Argument from plausible mechanism</h2><p> I suggest that a/the key difference between Daniel&#39;s and Elsa&#39;s examples lies in the process by which the value change has been brought about, in particular in the extent to which the process was <i>self-determined</i> by the person who undergoes the change, and the extent to which the person remains able to &#39;course-correct&#39; the unfolding of the process (eg, slow, halt or redirect) if she so chooses to.</p><p> To illustrate this, let&#39;s first consider Daniel&#39;s case. This case of value change appears unproblematic—a case of legitimate value change—in that the transformational process occurs at Daniel&#39;s own, free volition, and at any point, he could have chosen to discontinue to further engage in said aspirational process. His friend did not force him to engage with jazz; rather, Daniel held proleptic reasons <span class="footnote-reference" role="doc-noteref" id="fnrefa0bk6k4v21"><sup><a href="#fna0bk6k4v21">[1]</a></sup></span> for engaging more with jazz—an inkling, so to speak, for what would later turn into his full capacity to value jazz. By contrast, Elsa&#39;s ability to engage in the unfolding of her value transformation freely and in a self-determined fashion was heavily undermined by the nature of the process. Even if she might have chosen the first (few) interactions with the cult freely, the cult&#39;s sophisticated use of methods of manipulation, indoctrination or brainwashing deliberately exploit Elsa&#39;s psychological make-up. As such, the resulting change—independent of what specific beliefs and values it results in—is problematic due to the way it was brought about, and as such support our intuition that this is a case of illegitimate value change.</p><p> To test this idea slightly more, let&#39;s consider the case of Finley who, just like Daniel and Elsa also ends up falling in love with jazz. In Finley&#39;s case, they find themselves, a result of the workings of a content recommender system, consuming a lot of videos about the joys of jazz. Starting out, Finley did not hold any particular evaluative stance towards jazz; a few weeks later, however, they become obsessed with it, started to frequent concerts, read books on jazz, and so on.</p><p> Compared to Daniel and Elsa, I think Finley&#39;s case is more subtle and ambiguous with respect to value change legitimacy. On one hand, Finley&#39;s process of value change does not meet the same level of active and self-determined engagement as Daniel&#39;s. Finley did not (by stipulation in the example) start out with an inkling for the value of gardening, as is characteristic for an aspirational process according to Callard. Rather, they were passively exposed to information which then brought about the change. Furthermore, the recommendation algorithm arguably is shaped more by the economic incentives of the company than it is with the primary purpose of exposing Finley to new experiences and perspectives in mind. Finally, content recommendation platforms have some potential to cause compulsive or addictive behaviour in consumers by exploiting the human psychological make-up (eg, sensitivity to dopamine stimuli). All of these factors can be taken to weaken Finley&#39;s ability to reflect on their current level of jazz video consumption and values, and to &#39;course-correct&#39; if they wanted to. At the same time, while the recommender platform might be said to have weakened Finley&#39;s ability to self-determine and course-correct the process autonomously, this occurred at a very different level as the coercive persuasion experienced by Elsa. As such, this case appears neither clearly legitimate nor clearly illegitimate, but carries some aspects of both.</p><p> This is to show, referring to self-determination alone does not clarify all we need to know about what does and does not constitute legitimate value change. As mentioned above, in future work, I am interested in stress testing and building on this preliminary account further. </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fna0bk6k4v21"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa0bk6k4v21">^</a></strong></sup></span><div class="footnote-content"><p> Callard (2016) defines proleptic reasons as reasons which are based on value estimates which the reasoner cannot fully access yet, even if they might be able to partially glean them, ie an &quot;inchoate, anticipatory, and indirect grasp of some good&quot; (2016, p. 132).</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/QjA6kipHYqwACkPNw/2-premise-two-some-cases-of-value-change-are-il-legitimate#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QjA6kipHYqwACkPNw/2-premise-two-some-cases-of-value-change-are-il-legitimate<guid ispermalink="false"> QjA6kipHYqwACkPNw</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:36:53 GMT</pubDate> </item><item><title><![CDATA[1. Premise one: Values are malleable]]></title><description><![CDATA[Published on October 26, 2023 2:36 PM GMT<br/><br/><p> In this post, I will defend the first premise of the Value Change Problem: <i>that realistic models of human values take them to be malleable, rather than fixed</i> .  I call this the &#39;value malleability&#39; claim (VM).</p><p> I start by clarifying what I mean by value malleability before making a case that human values are in fact malleable. My general approach is to consider empirical observations about the practical reasoning of real-world agents (humans) and consider to what extent these are best explained by an account that posits fixed values vs one that posits value malleability. Concretely, I look at changes over the course of the human developmental lifespan; cases of value uncertainty, underspecification and open-endedness; and finally two existing accounts in the philosophical literature - transformative experiences and aspiration - and consider whether they represent genuine cases of value change. Finally, I consider some further objections.</p><p> This is the longest of the three premises of the Value Change Problem, and the discussion is relatively long and philosophical. If you are excited about diving into some of the details, great - read ahead! If instead you&#39;re looking for a minimal summary, the following is what I would say if I could only give one argument in favour of the value malleability claim:</p><blockquote><p> <i><strong>Argument from open-endedness:</strong></i></p><p> Relative to (cognitively) bounded agents (such as humans), the world is open-ended. Humans do not come into this world with a complete representation of the world (eg, a full hypothesis space). As a result, people occasionally get exposed to situations which they have not considered before and with respect to which they have not yet formed a meaningful evaluative stance. Thus, for a bounded agent to be able to function in such an open-ended world, it must be the case that they sometimes undergo value changes. In other words, the only way an agent could come to adopt an evaluative stance with respect to a genuinely novel situation is one that involves value change.</p></blockquote><h1> Clarifying what mean by value change</h1><p> By claiming that values are malleable, I claim that they can (and typically do) undergo some form of change over, eg, the course of a person&#39;s life.</p><p> But what do I mean by change? To clarify this question, let&#39;s disambiguate three different notions of value change: value replacement, value development and value refinement.</p><p> At a first glance, let us say that…</p><ol><li> <i>Value replacement</i> occurs when we adopt a novel value relative to an issue such that the new value replaces a value that was held earlier with respect to the same issue.<ol><li> Example: Anna used to eat meat, and later, as a result of changing her values with respect to the moral patienthood of animals, she comes to adopt a vegetarian diet based on her new moral beliefs.</li></ol></li><li> <i>Value development</i> occurs when we adopt a novel value such that its adoption does not cause the replacement of an existing value.<ol><li> Example: Bob, on top of and while continuing to courage, comes to additionally adopt a value compassion.</li></ol></li><li> <i>Value refinement</i> occurs when we adopt a more nuanced, elaborate or contextualised version of an existing value.<ol><li> Example: Clara used to understand her value for honesty as referring to &#39;not knowingly saying false things&#39; and later expanded her conception of honesty to also concern cases of deceptive behaviour that does not involve saying literally false things.</li></ol></li></ol><p> For the present purposes, I am less interested in discussing whether or if so how these three notions represent mutually exclusive and collectively comprehensive definitions of value change. Instead, I think it is a useful exercise to start familiarising ourselves with the notion of value change, and building some vocabulary that might come in useful later. Furthermore, thinking about the above example allows us to ask, for each of these cases, whether these represent genuine cases of value change (rather than, say, being better explained by a story that does not involve a change of values). In doing so, we can notice two things - let&#39;s think of them as markers of genuine value change - worth emphasising.</p><p> First, what makes each of these an instance of value change is that, with respect to the total set of values that describes a person and their behaviour, the person has undergone a substantive, non-trivial change which manifests in their practical reasoning and behaviour.</p><p> Second, for something to be a genuine case of value change, it must be the case that the person undergoing the change could have, in the relevant sense, <span class="footnote-reference" role="doc-noteref" id="fnref1n9r44l2gfe"><sup><a href="#fn1n9r44l2gfe">[1]</a></sup></span> taken a different path, eg, they could have adopted a different value, or refined their existing value in an alternative direction. Fore example, rather than refining her value of honesty in the way she did, Clara could have (in the relevant sense) formed values over, for example, whether someone acts honestly either from virtue or from duty (ie, being intrinsically motivated to act honestly vs. acting honestly even if reluctant to), over the (in)acceptability of certain types of dishonesty (eg, white lies), over different standards for how much someone is morally required to &#39;go out of their way&#39; to avoid (intentionally or unintentionally) misleading others, etc. All of these are a priori reasonable ways to refine a value of &#39;honesty&#39; which Clara could have (in a relevant sense) adopted, which I argue makes it such that we can consider this a case of genuine value change in the sense relevant to us. If that was not the case—if the value change could not have, in the relevant sense, taken a different path; in other words, if the nature of the change was fully inevitable—one may object that said &#39;change&#39; should instead be better understood as the mere &#39;unfolding&#39; of a preexisting, fixed, yet latent value. This would not count as an example of value malleability.</p><p> Lastly, in asking what we mean  by value change, we could also ask what we do <i>not</i> want to count as genuine value change. There is a bunch more that could be said there, but the following seems worth mentioning in passing. There are cases (let&#39;s call them cases of &#39;pretend&#39; change) in which someone is forced or finds themselves otherwise compelled to adopt certain behaviours which could be interpreted as the result of a change in values, but which is more appropriately understood as forced behaviour adopted against someone&#39;s genuine values. For example, I may do things while, say, held at a gunpoint, which conflict with my (earlier) values, which nevertheless, do not result from an underlying value change.</p><p> To summarise, all three types of change described above—replacement, development and refinement—are all cases of genuine value change and thus ways in which the malleability of human values may manifest.</p><h1> The case for value malleability</h1><h2> Do I have the same values as my younger self?</h2><p> Let us assume, for a moment, that values are fixed (ie, non-malleable). What would this imply?</p><p> For one, it would imply children already possess the fully formed and complete sets of values that they will also have as adults.  If you&#39;re like me, this will sound like a wild thing to believe. In other words, I don&#39;t think this belief lines up well with what we observe.</p><p> First, children certainly do not seem to have the same values as their adult selves in any trivial or straightforward sense. For example, a child that cared about animals and wanted to become a veterinarian will sometimes change their mind, start to care about, say, the arts and want to become a violinist instead. Second, children do not typically have formed values over a number of issues their later selves will have come to form values about, such as various political or philosophical stances (eg, whether they are a pacifist, anarchist or capitalist), lifestyle choices (eg, whether they will want to get married, have children, etc.) or aesthetic preferences.</p><p> You might object, however, that these aren&#39;t genuine cases of value change. Instead, we might be able to make sense of these cases by proposing that children already hold their future values in a latent state, even if they may not be able to consciously access them. Those latent values might not yet show fully, but only take the form of some sort of &#39;proto&#39; version of the child&#39;s later, fully fledged values. Under this interpretation, rather than representing cases of genuine value change, what we are observing is instead the realisation, over the course of the child&#39;s growing up, of those already existing, latent values.</p><p> Recall that, in order to reject the VM claim, the latent values would have to be such that the child will come to develop their full versions invariably, ie, that the development could not have, in the relevant sense, taken a different path. Inversely, if there are different ways in which a value could have developed, we treat this, according to our definition above, as a case of value change. However, as I will try to show, the invariability condition is not met in the case of human development, and thus that the explanation via latent values remains unsatisfactory.</p><p> To make this point, take, for example, a child who shows an inclination (ie, &#39;proto&#39; values) to judge any form of violence as atrocious. It seems like said child&#39;s early tendencies could just about as well be interpreted as a &#39;proto-pacifist position&#39; or as a &#39;proto-real political&#39; one. In the former case, the dispreference of violence could lead to the belief that any forms of violence must be resisted, while in the latter case, the same early dispreference could lead to the belief that the state&#39;s monopoly of power is critical to maintaining peace and order  and thereby reduce the overall expected violence and harm. In other words, there is uncertainty about what values the child will come to adopt. What does this imply for the theory of latent values?</p><p> First, we must distinguish between two places from which the uncertainty could originate. In the first case, the uncertainty about the child&#39;s future values resides in the subjective beliefs of the observer. In other words, while there is a true answer to what the child&#39;s latent values are that it will come to hold, the observer does not have direct and full access to what those values are, and as a result, (the observer&#39;s subjective beliefs about) the values are uncertain.</p><p> This type of uncertainty is compatible with claiming that the child has latent values it will inevitably come to adopt. However, this theory does not seem to afford the theoriser any epistemic purchase (eg, improved predictive power). As we have seen in the example above, simply posing the existence of latent value doesn&#39;t on its own tell us anything about what those latent values are. Furthermore, it is hard to imagine what epistemic position one could realistically come to inhabit that would make it justified (as well as pedagogically unproblematic) to act according to the belief that a child&#39;s future values are already fully pre-determined. As such, one might critique that, while a theory of latent values requires us to posit an unobservable but purportedly real ontological entity (namely, latent values), this cost in parsimony is not met with equal or greater epistemic gains.</p><p> In the second case, the uncertainty about the child&#39;s future values resides not in the observer&#39;s subjective beliefs but &#39;in the territory&#39;. In other words, while the possessions of a proto value biases how likely different value trajectories are, the child&#39;s values could ultimately come to develop in different ways, depending on, say, what the child comes to experience over the course of its life. This picture, however, does not conflict with VM, and thus does not serve as a basis for rejecting it. Under this interpretation, the child&#39;s inclinations (ie, their &#39;proto&#39; values) are (on their own) an underspecification of what future values the child will come to adopt. In other words, the child&#39;s values could develop in different directions, depending on what experiences it will have, what ideas it will be exposed to, etc. If, however, the child, given its current inclinations and proto values, could come to adopt different sets of values in the future,  this development must involve some form of value change (in the sense defined above). As such, the basic premise of this section holds that the observation that children, over the course of their developmental process, come to hold different values provides evidence in favour of VM. This also shows how VM does not mean to exclude the possibility that certain (inherent, genetic, etc.) tendencies influence (but do not fully determine) a person&#39;s future values</p><h2> Dealing with value uncertainty, underspecification and open-endedness</h2><p> Let&#39;s consider another set of empirical observations about human practical reason, and check whether or not value malleability is a compelling way to make sense of these cases. Our premise is that humans often find themselves in one of the following positions: they are uncertain about their values; they hold values that are underspecified; or they may face novel situations over which we have not yet formed coherent values. I will argue that handling any of these situations will sometimes involve value change.</p><p> <strong><u>Value uncertainty and underspecification</u></strong></p><p> First, let&#39;s consider the case of <i>value uncertainty</i> —the idea that we are often uncertain about what we value.</p><p> Consider Fin, who is uncertain about what his endorsed stance is towards such issues like the moral status of non-human animals, the moral value of people who don&#39;t exist yet, abortion, etc. I argue that in some (not necessarily all) cases where value uncertainty gets (partially) resolved, this occurs in a way that involves value change. (Of course, the inverse process is possible to—where one becomes more uncertain about a given value.)<br> For example, when dealing with the fact that he&#39;s uncertain about the moral status of non-human animals, Fin might eg refine an existing value, or extend it to a novel issue. I have argued before that value reinforcement and development (on top of valve replacement) are genuine cases of value change.</p><p> A similar argument applies to the case of <i>value underspecification</i> —the idea that we often hold values that turn out to be insufficiently specified. We have already seen an example of this earlier, when Clara&#39;s initial underspecified notion of honesty got refined to include both the idea of &#39;not saying false things&#39; as well as &#39;not knowingly acting to deceive&#39;. Underspecification is resolved by value refinement, and might sometimes reveal conflicting values, resolving which may further require the replacement of some of one&#39;s value. I have already argued earlier that these are all forms of genuine value change.</p><p> (Note, in neither case am I claiming that all cases of uncertainty or underspecification are noticed or resolved; nor do I need to, for my argument to succeed.)</p><p> <strong><u>Value open-endedness</u></strong></p><p> Finally, I argue that because the world is open-ended relative to (cognitively) bounded agents (such as is the case for humans), value change is commonplace. Let us unpack this argument further. First, humans are bounded, meaning that they operate with a limited set of (cognitive) resources as they are navigating the world (see, eg, Simon, 1990; Wimsatt, 2007). Given that, humans do not come into this world with a complete representation of the world (ieeg, a full hypothesis space). Instead, from the point of view of an individual, the world appears <i>open-ended</i> , ieeg, the world&#39;s limits and its ontology are not clearly defined or unchangeable from the get-go. As a result, people sometimes get exposed to situations they have not considered before and with respect to which we have not yet formed a meaningful evaluative stance.</p><p> One might object that the agent could have held the relevant value latently, and thus, instead of involving genuine value change, these should be understood as cases where a fixed, latent value is being &#39;unfolded&#39; or &#39;come to be consciously apprehended&#39;. While this might sometimes be what is in fact what is happening, in order to refute VM, one would have to argue that every value a person will ever come to adopt must have already been held latently. This would imply that a person&#39;s model of the world already contained all representational entities with respect to which they will ever come to hold evaluative stances. Given the cognitive boundedness of real agents, this seems exceedingly unrealistic. Instead, the open-endedness of the world will predictably expose a bounded agent to genuinely novel situations over which they will not yet have had the chance to form any evaluative stances. Because the situation must be understood as genuinely novel, the only interpretation of how an agent could come to adopt an evaluative stance with respect to such novel situations is one that involves genuine value change. <span class="footnote-reference" role="doc-noteref" id="fnref1qbzhcck1fg"><sup><a href="#fn1qbzhcck1fg">[2]</a></sup></span></p><h2> Transformative experiences and Aspirational pursuits</h2><p> In the following, I will briefly describe two accounts from the philosophical literature which provide extensive treatments of cases of genuine value change. These two accounts are Paul&#39;s (2014) <i>Transformative Experiences</i> , and Callard&#39;s (2018) <i>Aspiration</i> .</p><p> &#39;Transformative experiences&#39; (Paul, 2014) are experiences that permanently and fundamentally transform a person in hard-to-imagine ways, including with respect to their values. An example of a transformative experience is becoming a parent, immersing oneself in a completely novel culture, or going through a near-death experience. It is not uncommon that such experiences change people (including their values) significantly, and, from the point of view of the person going through the experience, it is hard, maybe impossible, to fully anticipate how they will be changed as a result. Nevertheless, most people will experience at least one or a few such transformative experiences throughout their lifetime, and humans do not generally seem to go out of their way to avoid undergoing such a change. Insofar as Paul&#39;s account of transformative experiences holds up—ie, insofar as these experiences are truly transformative, including with respect to people&#39;s values—they provide a central example of the malleability of human values.</p><p> As for the second account, Agnes Callard (2016, 2018) discusses how, not only in the case of transformative experiences, humans seem to regularly take actions on the basis of what she calls <i>proleptic</i> reasons. These are reasons which are based on value estimates which the reasoner cannot fully access yet, even if they might be able to partially glean them. As such, Callard argues that humans sometimes actively <i>aspire</i> to explore and &#39;unlock&#39; new domains of value <i>before</i> they have become fully able to value said things. For example, one may aspire to be able to appreciate the joys of haute cuisine before one is fully able to do so, or one may aspire to cultivate certain virtues (say courage) without yet being able to fully inhabit all the things that may be good about said virtue. In other words, the aspirant wants to want something, before having full access to why they want that something. As such, and according to Callard&#39;s account, the notion of aspiration refers to a rational process of value development. In Callard&#39;s own words, to aspire is to pursue an activity based on an &#39;inchoate, anticipatory, and indirect grasp of some good&#39; (2016, p. 132), and as a result, &#39;we have guided ourselves to the new values or desires or commitments that our experience engenders&#39; (p. 153). Again, insofar as Callard&#39;s account holds up, it provides an example, and thus existence proof, of value malleability.</p><h2> Counterargument : &#39;shallow&#39; vs &#39;fundamental&#39; values</h2><p> One might object that all of the above examples can be explained by postulating that more &#39;shallow&#39; types of values <i>can</i> change while more &#39;fundamental&#39; values remain fixed. Further, such changes of &#39;shallow&#39; values (eg, in response to epistemic changes) always stand in an instrumental relationship to the pursuit of the more &#39;fundamental&#39; values which themselves stay fixed.</p><p> To illustrate the shape of what I have in mind with this counterargument, consider the following two examples. Suppose that I like chocolates, and that I believe that there are chocolates in a yellow box. I will then also value having the yellow box at hand. Now, suppose I find out that there are no chocolates left in the yellow box. I subsequently lose interest in it. In a certain sense, my values have changed—I no longer value having the yellow box at hand. At the same time, my valuing of the yellow box appears rather shallow, and instrumental to my valuing of chocolates, which seems more fundamental. Furthermore, the latter value has not undergone any change.</p><p> Or, consider the case of Garry, who aspired to, and eventually succeeded at, coming to value poetry. Maybe what happened is, rather than Garry developing a new value in his love of poetry, he simply learnt more about how he can effectively seek pleasure—pleasure being his more fundamental value which remains fixed throughout his pursuit. Under this account, poetry is &#39;merely&#39; an effective strategy to achieve pleasure.</p><p> Do these examples provide a counterargument to VM? To answer this question, let us first distinguish between two ways we might interpret the notions of &#39;shallow&#39; and &#39;fundamental&#39; in this context.</p><p> On one account, the more fundamental a value, the less likely it is to change. However, it is still in principle possible for (most) values to change, and in particular, the more shallow values change regularly. This interpretation of the notion of &#39;shallow&#39; and &#39;fundamental&#39; does not conflict with VM. In fact, this appears to present a realistic picture of the nature of values in that, for example, it can account for the intuition that some values are more (evolutionary) hardwired (eg, the value to not be hungry)—and therefore less likely to be subject of change—while others are more contingently acquired (eg, a preference for some foods over others) and more likely to change.</p><p> As for the second interpretation, however, we do find it to stand in conflict with VM. According to this view, the changes of the &#39;shallow&#39;, derived values (such as valuing the yellow box, valuing poetry) can be understood as &#39;merely&#39; a case of <i>rational sensitivity to epistemic learning in light of, and instrumental to, more fundamental (and fixed) values</i> . As such, &#39;shallow&#39; values (under this interpretation) don&#39;t constitute values of the same standing or importance as &#39;fundamental&#39; values. Some might argue that they don&#39;t count as genuine values at all (or something similar). <span class="footnote-reference" role="doc-noteref" id="fnrefpsq4vdaz5p"><sup><a href="#fnpsq4vdaz5p">[3]</a></sup></span> As such, under this view, what we observe in the yellow box and the poetry example does not constitute cases of genuine value change, but instead merely rational re-evaluation of the most effective strategies for realising one&#39;s fundamental (and fixed) values.</p><p> However, this counterargument faces difficulties and, as such, I am not convinced it succeeds. For one, it is <i>always</i> possible to postulate some invariant higher-order value with respect to which changes to shallower values can be explained (eg, my value for chocolates explains my &#39;shallow&#39; &#39;value&#39; for the yellow box). Within this account, nothing constraints us from constructing arbitrary &#39;deep&#39; hierarchies of value which can be used to rationalise any changes of the &#39;lower-level&#39; values in terms of fixed &#39;higher-level&#39; values (eg, my value for tasty food explains my more &#39;shallow&#39; value for chocolates; my value for chocolates, in turn, is explained as merely an instrumental strategy in service of my &#39;more fundamental&#39; value for hedonic pleasure, etc.). I find this problematic because it explains <i>too much</i> (and thereby nothing at all).</p><p> Second, one concern I see with this approach is that we risk abstracting away too much from concrete, contextualised and thick notions of value, replacing them by increasingly more abstract ones (eg, &#39;pleasure&#39; or &#39;utility&#39;), thereby stripping away more information than is desirable. From the perspective of trying to explain and predict phenomena around us, we risk ridding our &#39;theory&#39; of any substantive empirical or predictive content, and the notion of (fundamental) value we end up with ceases to be useful in understanding the (often contextual!) practical reasoning and behaviour of actual humans. In the example above, saying that Garry seeks to maximise (some abstract notion of) &#39;pleasure&#39; or &#39;utility&#39; does not constraints whatsoever my predictions about whether, eg, he will get into liking jazz, far from having anything to say about why or how he might do so.</p><p> Beyond being unsatisfying from the point of view of the theorist, this approach is also problematic from the perspective of being moral reasoners ourselves. In particular, adopting overly abstract notions of value risks impoverishing our very ability to recognise and derive value, and to deliberate with each other about what we value and why. Imagine if Garry&#39;s poetry-loving friend Harriet, who, in one case, is able to talk in rich and nuanced terms about what she finds so marvellous and inspiring about poetry, and, in another case, describes poetry simply as &#39;an effective strategy to seek pleasure&#39;. Most people, I believe, would be more motivated to aspire towards coming to appreciate the value of poetry in the former rather than the latter case. Garry&#39;s own ability to aspire—the quality of his proleptic reasons, his inkling of what might be valuable about poetry—is likely also stronger if his friend is able to talk to him about her love for poetry in thick and nuanced terms. In other words, by reverting to overly abstract notions of value, we impoverish our very ability to reason and deliberate, collectively and with ourselves, about our values. Elizabeth Anderson, in <i>Value in Ethics and Economics</i> , makes this point better than I can: <span class="footnote-reference" role="doc-noteref" id="fnrefb7lxhe6t5v"><sup><a href="#fnb7lxhe6t5v">[4]</a></sup></span></p><blockquote><p> [I] deny that it makes sense to reason about the good and the right independent of thick evaluative concepts. [...] It disabled us from appreciating many authentic values. It suppresses the parallel evolution of evaluative distinctions and sensibilities that make us capable of caring about a rich variety of things in different ways. It cuts off fruitful avenues of exploration and criticism available on a pluralistic self-understanding. (p. 118.)</p></blockquote><p> There are a number of other things that could be said about this problem, both in favour of and against defending some notion of fixed fundamental values. I am happy to hear about arguments (in either direction) that I seem to have missed here. For now, this is where I will leave the discussion. I remain unconvinced, for the time being, by this line of counterargument against value malleability. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn1n9r44l2gfe"> <span class="footnote-back-link"><sup><strong><a href="#fnref1n9r44l2gfe">^</a></strong></sup></span><div class="footnote-content"><p> Yes, saying &#39;in the relevant sense&#39; (ie invoking a notion of &#39;relevant counterfactuals&#39; sweeps a bunch of nuance under the rug. I will touch on this question again, at slightly higher resolution, later in this post. For the time being, I suggest that a common sense/pragmatic interpretation is sufficient for the argument at hand.<br></p></div></li><li class="footnote-item" role="doc-endnote" id="fn1qbzhcck1fg"> <span class="footnote-back-link"><sup><strong><a href="#fnref1qbzhcck1fg">^</a></strong></sup></span><div class="footnote-content"><p> A practically interesting area in which this dynamic occurs regularly concerns innovation and technological development. The transhumanist literature, for example, is full of examples of how changes to the biological constraints faced by humans raise significant novel moral questions. Consider, for example, how novel means of contraception have caused the transcendence of earlier constraints set by the fallibility of natural methods of contraception. This expansion in contraceptive possibilities has raised questions and changed people&#39;s attitudes concerning, among others, the social role and rights of  women, sex and marriage. Or, consider how the development of spacesuits has caused the transcendence of the earlier constraints set by humans&#39; inability to survive without an atmosphere. The resulting possibility of space travel in turn raises questions about whether there is a moral case to be made for or against humanity becoming space-faring, and at what cost relative to, say, other social priorities (eg, differential investment in education or health care). Finally, consider how the possibility of artificial wombs, if they come to be, will affect people&#39;s values related to gender, parenthood, people&#39;s rights over their bodies, etc., in ways that cannot be conclusively predicted at this point in time. In other words, innovation creates situations of moral relevance which an individual and/or a society has not yet formed a coherent stance towards, and which thus necessitates value change.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpsq4vdaz5p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpsq4vdaz5p">^</a></strong></sup></span><div class="footnote-content"><p> For example, I find it worthwhile noting that under expected utility theory (eg, following the Von Neumann–Morgenstern utility theorem), any case of value change is necessarily understood as undesirable (eg value drift) and/or the result of a failure of rationality (ie irrationality). Accordingly, the only way one could rationally change one&#39;s mind about &#39;valuing the yellow box&#39; is for the latter to not actually be a genuine &#39;value&#39; but merely a contingent and instrumental &#39;strategy&#39; which it is fine to change in light of new information.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb7lxhe6t5v"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb7lxhe6t5v">^</a></strong></sup></span><div class="footnote-content"><p> The original quote is part of a defence of pluralist over mostist theories of values. However, the claim about the role of &#39;thick evaluative concepts&#39; is directly relevant to our discussion at hand.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/HyodRjYtiA2xozCrk/1-premise-one-values-are-malleable#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HyodRjYtiA2xozCrk/1-premise-one-values-are-malleable<guid ispermalink="false"> HyodRjYtiA2xozCrk</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:36:41 GMT</pubDate> </item><item><title><![CDATA[0. The Value Change Problem: introduction, overview  and motivations]]></title><description><![CDATA[Published on October 26, 2023 2:36 PM GMT<br/><br/><p> Given that AI systems are becoming increasingly capable and widely deployed, we have to think carefully about the multifarious effects their adoption will have on our individual and collective lives. In this sequence, I will focus specifically on how AI can come to substantially affect what it is that we value, as well as the mechanisms by which this may occur.</p><p> This sequence introduces the “Value Change Problem” (VCP). I will argue why I think it&#39;s a critical aspect of ethical AI design, and outline what risks we might expect if we fail to properly address it.</p><p> <i>This is a shortened and slightly adapted version of academic work I have submitted for publication elsewhere. The original work was written mainly with an academic philosophy audience in mind. If you&#39;re interested in reading the original (longer) version, feel free to reach out.</i></p><p> The core claim of VCP can be summarised as follows:</p><blockquote><p> AI alignment must address the problem of (il)legitimate value change; that is, the problem of making sure AI systems neither cause value change illegitimately, nor forestall legitimate cases of value change in humans and society.</p></blockquote><h2> Outline (or: how to read this sequence?)</h2><p> This case resides on three core premises:</p><ol><li> human values are malleable ( <a href="https://www.lesswrong.com/posts/HyodRjYtiA2xozCrk/1-premise-one-values-are-malleable">post 1</a> );</li><li> some instances of value change are (un)problematic ( <a href="https://www.lesswrong.com/posts/QjA6kipHYqwACkPNw/2-premise-two-some-cases-of-value-change-are-il-legitimate">post 2</a> );</li><li> AI systems are (and will become increasingly) capable of affecting people&#39;s value change trajectories ( <a href="https://www.lesswrong.com/posts/yPnAzeRAqdko3RNtR/3-premise-three-and-conclusion-ai-systems-can-affect-value">post 3</a> ).</li></ol><p> From this, I conclude that we ought to avoid building AI systems that disrespect or exploit the malleability of human values, such as by causing illegitimate value changes or by preventing legitimate ones. I will refer to this as the &#39;Value Change Problem&#39; (VCP). After having established the core case for VCP, the remaining two posts discuss in some more depth the risks that may result from inadequately addressing the VCP.</p><p> <i>If you are already on board with each of the premises, you may instead want to directly skip to reading the discussion of risks (posts 4 and 5).</i></p><p> I consider two main categories of risks: ( <a href="https://www.lesswrong.com/posts/qZFGPJi3u8xuvnWHQ/4-risks-from-causing-illegitimate-value-change">post 4</a> ) risks from causing illegitimate value change, as well as ( <a href="https://www.lesswrong.com/posts/KeHGinpj2WyzDEQAx/5-risks-from-preventing-legitimate-value-change">post 5</a> ) risks from (illegitimately) preventing legitimate value change. For each of these I want to ask: What is the risk? What are plausible mechanisms by which these risks manifest? What are ways in which these risks manifest already, and what are the ways in which they are likely to be exacerbated going forward, as AI systems become more advanced and more widely deployed?</p><p> In the first case— <i>risks from causing illegitimate value change</i> —, leading with the example of recommender systems today, I will argue that performative predictors can come to affect that which they set out to predict—among others, human values.</p><p> In the second case— <i>risks from preventing legitimate value change</i> —, I will argue that value collapse—the idea that hyper-explication of values tends to weaken our epistemic attitudes towards the world and our values—can threaten the possibility of self-determined and open-ended value exploration and, consequently, the possibility of legitimate value change. In both cases, we should expect (unless appropriate countermeasures are taken) the same dynamic to be exacerbated—both in strength and scope—with the development of more advanced AI systems, and their increasingly pervasive deployment.</p><p> <i>The different posts are more or less self-contained and can be read on their own.</i></p><p> I welcome thoughts, constructive criticism, and ideas for relevant future work.</p><h2> Motivations</h2><p> In one sense, this analysis is intended to be a contribution to our understanding of the risk landscape related to building increasingly capable, autonomous and widely deployed AI systems.</p><p> In another sense, I believe that some of the reflections on the nature of values contained in this discussion are relevant on the path to proposals for ambitious value learning by helping us characterise (some of) the outlines/shapes of an adequate theory of value.</p><p> I believe that the problem of (il)legitimate value change is an integral part of the problem of aligning advanced AI systems. As such, the question we need to ask about the ethical design of advanced AI systems should not be limited to the question of what values we want to embed in them, but also how we decide on the forms of value change that are (il)legitimate, and how to design and deploy systems such that do not disrespect or exploit value malleability.</p><p> Note that, while I hope to have been able to make initial steps towards a satisfactory characterisation of the Value Change Problem, more work is needed to improve our understanding of the specific ways in which AI systems can (and already do) cause value change, when cases of value change are legitimate or illegitimate, and how to build AI systems that reliably avoid causing illegitimate value change and potentially promote legitimate value change.</p><h2> Acknowledgement</h2><p> I am grateful for useful comments and discussions to my thesis supervisor<br> Ioannis Votsis, my colleagues at ACS (in particular Simon and Clem), Hunter Muir, Tsvi BT, TJ and likely others I&#39;m forgetting here.</p><h2>参考</h2><p>Anderson, E. (1995). <i>Value in ethics and economics</i> . Harvard University Press.</p><p> Anscombe, GEM, (2000). <i>Intention</i> . Harvard University Press. (First edition published 1957)</p><p> Bengio, Y. (2023). How rogue AIs may arise. Retrieved at: https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/</p><p> Bilgrami, A. (2015). The ambitions of classical liberalism: Mill on truth and liberty. <i>Revue internationale de philosophie</i> 2: 175-182.</p><p> Bostrom, N. (2012). The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. <i>Minds and Machines</i> , 22(2), 71-85.</p><p> Callard, A. (2016). Proleptic reasons. <i>Oxford Studies in metaethics,</i> 11, 129-154.</p><p> Callard, A. (2018). <i>Aspiration: The agency of becoming</i> . Oxford University Press.</p><p> Chechkin, GA, Piatnitskiĭ, AL, &amp; Shamev, AS (2007). <i>Homogenization: methods and applications</i> .卷。 234. American Mathematical Soc.</p><p> Critch, A., &amp; Krueger, D. (2020). AI research considerations for human existential safety (ARCHES). <i>arXiv preprint arXiv:2006.04948</i> .</p><p> Critch, A., &amp; Russell, S. (2023). TASRA: A taxonomy and analysis of societal-scale risks from AI. <i>arXiv preprint arXiv:2306.06924</i> .</p><p> DiFranzo, D., &amp; Gloria-Garcia, K. (2017). Filter bubbles and fake news. <i>XRDS: Crossroads, The ACM Magazine for Students</i> 23.3: 32-35.</p><p> Fuchs, AE (2001). Autonomy, slavery, and Mill&#39;s critique of paternalism. <i>Ethical Theory and Moral Practice</i> 4: 231-251.</p><p> Gabriel, I. (2020). Artificial intelligence, values, and alignment. <i>Minds and machines</i> 30.3: 411-437.</p><p> Hardt, M., Jagadeesan, M., &amp; Mendler-Dünner, C. (2022). Performative power. <i>Advances in Neural Information Processing Systems</i> , <i>35</i> , 22969-22981.</p><p> Hazrati, N., &amp; Ricci, F. (2022). Recommender systems effect on the evolution of users&#39; choices distribution. <i>Information Processing &amp; Management</i> 59.1: 102766.</p><p> Hendrycks, D. (2023). Natural selection favors AIs over humans. <i>arXiv preprint arXiv:2303.16200</i> .</p><p> Hendrycks, D., Carlini, N., Schulman, J., &amp; Steinhardt, J. (2021). Unsolved problems in ML safety. <i>arXiv preprint arXiv:2109.13916</i> .</p><p> Humberstone, IL (1992). Direction of fit. <i>Mind</i> .卷。 101, No. 401, 59–83.</p><p> Kant, I. (1993). <i>Grounding for the metaphysics of morals</i> (JW Ellington, Trans.). Hackett Publishing. (Original work published 1785.)</p><p> Mill, JS (2002). <i>On liberty</i> . Dover Publications. (Original work published 1859.)</p><p> Nguyen, CT (2022). Value collapse. [Video] <i>The Royal Institute of Philosophy Cardiff, Annual Lecture 2022.</i> Retrieved from: https://royalinstitutephilosophy.org/event/value-collapse/.</p><p> Paul, LA (2014). <i>Transformative experience</i> . OUP Oxford.</p><p> Perdomo, J., Zrnic, T., Mendler-Dünner, C., &amp; Hardt, M. (2020). Performative prediction. <i>Proceedings of the 37th International Conference on Machine Learning</i> , PMLR 119:7599-7609, 2020.</p><p> Pettit, P. (2011). <i>A Theory of freedom</i> . Cambridge: Polity Press.</p><p> Porter, TM (1996). <i>Trust in numbers: The pursuit of objectivity in science and public life</i> . Princeton University Press.</p><p> Rawls, J. (2001). <i>Justice as fairness: A restatement.</i> Belknap Press.</p><p> Russell, SJ (2019). <i>Human compatible: artificial intelligence and the problem of control</i> . Viking.</p><p> Searle, JR (1985). <i>Expression and meaning: Studies in the theory of speech acts</i> . Cambridge University Press.</p><p> Sen, A. (2002). <i>Rationality and freedom.</i> The Belknap Press of Harvard University Press. Cambridge, Massachusetts</p><p> Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N,, Ho, L., Siddarth, D., Avin, S., Hawkins, W., Kim, B., Gabriel, I., Bolina, V., Clark, J., Bengio, Y., Christiano, P., Dafoe A. (2023). Model evaluation for extreme risks. <i>arXiv preprint arXiv:2305.15324</i> .</p><p> Simon, HA (1990). Bounded rationality. <i>Utility and probability</i> : 15-18.</p><p> Swierstra, T. (2013). Nanotechnology and technomoral change. <i>Etica &amp; Politica</i> , <i>15</i> (1), 200-219.</p><p> Taylor, C. (1985). What&#39;s wrong with negative liberty. In <i>Philosophy and the Human Sciences: Philosophical Papers.</i>卷。 2, Cambridge: Cambridge University Press. 211–29.</p><p> Von Neumann, J., &amp; Morgenstern, O. (1944). <i>Theory of games and economic behavior</i> . Princeton: Princeton University Press.</p><p> Wimsatt, WC (2007). <i>Re-engineering philosophy for limited beings: Piecewise approximations to reality</i> . Harvard University Press.</p><br/><br/> <a href="https://www.lesswrong.com/posts/mHQHBEuFcEWRnitp4/0-the-value-change-problem-introduction-overview-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mHQHBEuFcEWRnitp4/0-the-value-change-problem-introduction-overview-and<guid ispermalink="false"> mHQHBEuFcEWRnitp4</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:36:15 GMT</pubDate> </item><item><title><![CDATA[EPUBs of MIRI Blog Archives and selected LW Sequences]]></title><description><![CDATA[Published on October 26, 2023 2:17 PM GMT<br/><br/><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fejT5iMEDcZMHGCSW/baquhi68ciuopde0bh6h" alt=""></p><p> <a href="https://git.sr.ht/~mesaoptimizer/epubs">Here is a repository of EPUBs</a> that I&#39;ve created, consisting of rationalist or alignment content I want to read on my Kindle, such as selections of the <a href="https://intelligence.org/all-posts/">MIRI (Blog) Archives</a> and of multiple LessWrong Sequences that I am reading or intend to read.</p><p> This post is an announcement that this repository exists <sup class="footnote-ref"><a href="#fn-gwjrgLpqpXaCEgMjT-1" id="fnref-gwjrgLpqpXaCEgMjT-1">[1]</a></sup> , and a way for people who search the internet to get EPUBs for LW sequences and MIRI (Blog) archives to find their way to this repository, instead of having to create their own EPUBs. <sup class="footnote-ref"><a href="#fn-gwjrgLpqpXaCEgMjT-2" id="fnref-gwjrgLpqpXaCEgMjT-2">[2]</a></sup></p><p> I expect I will add more such EPUBs to the linked repository as time passes, for any collection of essays I&#39;d prefer an e-book version of <sup class="footnote-ref"><a href="#fn-gwjrgLpqpXaCEgMjT-3" id="fnref-gwjrgLpqpXaCEgMjT-3">[3]</a></sup> , both as a way to archive parts of the internet I find valuable, and because I like reading long content on e-ink screens. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-gwjrgLpqpXaCEgMjT-1" class="footnote-item"><p> My <a href="https://www.lesswrong.com/posts/Jrvm8YCAe6HYfJNsy/an-epub-of-arbital-s-ai-alignment-section">previous post on me creating an EPUB</a> was intended to be a one-off, but then I found myself building a collection. <a href="#fnref-gwjrgLpqpXaCEgMjT-1" class="footnote-backref">↩︎</a></p></li><li id="fn-gwjrgLpqpXaCEgMjT-2" class="footnote-item"><p> It seems plausible that someone would reason as I did and conclude that going through the MIRI Archives is very high value to gain certain illegible or non-explicitly-distilled context related to MIRI&#39;s model of alignment and alignment research, and would prefer doing it using an e-reader enough to go through the trouble of searching for or creating their own e-books. <a href="#fnref-gwjrgLpqpXaCEgMjT-2" class="footnote-backref">↩︎</a></p></li><li id="fn-gwjrgLpqpXaCEgMjT-3" class="footnote-item"><p> I&#39;ve created an EPUB of (almost) the entirety of Gwern&#39;s essays from <a href="https://gwern.net">https://gwern.net</a> . Sure, the EPUB is outdated within a day of creation when Gwern updates some arbitrary passage in some arbitrary essay, but it feels great to have effectively the entirety of Gwern&#39;s evergreen intellectual output in your Kindle. <a href="#fnref-gwjrgLpqpXaCEgMjT-3" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/fejT5iMEDcZMHGCSW/epubs-of-miri-blog-archives-and-selected-lw-sequences#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fejT5iMEDcZMHGCSW/epubs-of-miri-blog-archives-and-selected-lw-sequences<guid ispermalink="false"> fejT5iMEDcZMHGCSW</guid><dc:creator><![CDATA[mesaoptimizer]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:17:12 GMT</pubDate></item></channel></rss>