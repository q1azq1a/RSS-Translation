<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 27 日星期五 14:10:59 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Linkpost: Rishi Sunak's Speech on AI (26th October)]]></title><description><![CDATA[Published on October 27, 2023 11:57 AM GMT<br/><br/><p>我很高兴来到英国皇家学会，这里是几个世纪以来书写现代科学故事的地方。</p><p>现在，我对技术的力量让每个人的生活变得更美好感到毫不掩饰的乐观。</p><p>所以，我要发表的轻松演讲——我内心真正想发表的演讲……</p><p> ......将告诉您我们面前的难以置信的机会。</p><p>就在今天早上，我在莫菲尔德眼科医院。</p><p>他们正在使用人工智能构建一个模型，可以查看您眼睛的单张图片......</p><p> ......不仅可以诊断失明，还可以预测心脏病、中风或帕金森病。</p><p>而这仅仅是开始。</p><p>我真诚地相信，像人工智能这样的技术将带来深远的变革……</p><p> ……工业革命、电力的出现或互联网的诞生。</p><p>现在，就像每一波技术浪潮一样，人工智能将带来新知识……</p><p> ……经济增长的新机遇、人类能力的新进步……</p><p> ......以及解决我们曾经认为超出我们能力范围的问题的机会。</p><p>但就像那些波浪一样，它也带来了新的危险和新的恐惧。</p><p>因此，我要做的负责任的事情——我要做正确的演讲——就是正面解决这些恐惧……</p><p> ...让您安心，我们会保证您的安全...</p><p> …同时确保您和您的孩子拥有人工智能带来的更美好未来的所有机会。</p><p>现在，做正确的事情，而不是简单的事情，意味着对人们诚实地了解这些技术的风险。</p><p>所以，我不会向你隐瞒它们。</p><p>这就是为什么今天，我们第一次迈出了极不寻常的一步……</p><p> ...发布我们对人工智能风险的分析...</p><p> ......包括英国情报界的评估。</p><p>这些报告提出了严厉的警告。</p><p>如果这个错误的话，人工智能可能会让制造化学或生物武器变得更容易。</p><p>恐怖组织可以利用人工智能更大规模地传播恐惧和破坏。</p><p>犯罪分子可能利用人工智能进行网络攻击、虚假信息、欺诈，甚至儿童性虐待。</p><p>在最不可能但极端的情况下，甚至存在人类完全失去对人工智能控制的风险……</p><p> ......通过有时被称为“超级智能”的人工智能。</p><p>事实上，引用今年早些时候数百名世界领先人工智能专家的声明：</p><p> “与流行病和核战争等其他社会规模风险一样，减轻人工智能带来的灭绝风险应该成为全球优先事项”。</p><p>现在，我想彻底澄清：</p><p>人们现在不需要为此失眠的风险。</p><p>我不想危言耸听。</p><p>关于这一点存在着真正的争论——一些专家认为这根本不会发生。</p><p>但无论这些风险多么不确定和不可能，如果它们确实出现，后果将是极其严重的。</p><p>当这项技术的许多最大开发商自己警告这些风险时......</p><p> ……领导者有责任认真对待这些问题并采取行动。</p><p>这就是我今天正在做的事情——以三种具体方式。</p><p>首先，确保您的安全。</p><p>目前，唯一测试人工智能安全性的人......</p><p> ……正是开发它的组织。</p><p>即使他们也不总是完全了解他们的模型可以做什么。</p><p>部分激励措施是为了以最快的速度竞争建立最好的模型。</p><p>因此，我们不应该依赖他们批改自己的作业，许多从事这方面工作的人都会同意这一点。</p><p>尤其是因为只有政府才能正确评估国家安全风险。</p><p>只有民族国家才有权力和合法性来保证其人民的安全。</p><p>英国的答案是不要急于监管。</p><p>这是一个原则点——我们相信创新，这是英国经济的标志……</p><p> ......所以我们总是有一个假设来鼓励它，而不是扼杀它。</p><p>无论如何，我们如何才能制定出对我们尚未完全理解的事物有意义的法律呢？</p><p>因此，我们正在建立世界领先的能力来理解和评估政府内部人工智能模型的安全性。</p><p>为此，我们已经投资 1 亿英镑组建新的工作组……</p><p> ……用于人工智能安全的资金比世界上任何其他国家都多。</p><p>我们招募了人工智能领域一些最受尊敬、知识渊博的人物。</p><p>因此，我完全有信心告诉您，英国在确保您的安全方面所做的工作远远多于其他国家。</p><p>正因为如此——因为我们已经采取了独特的步骤——我们今天才能走得更远。</p><p>我可以宣布，我们将在英国建立世界上第一个人工智能安全研究所。</p><p>它将提高世界对人工智能安全的认识。</p><p>它将仔细检查、评估和测试新型人工智能……</p><p> ......以便我们了解每个新模型的能力......</p><p> ……探索所有风险，从偏见和错误信息等社会危害，到最极端的风险。</p><p>英国人民应该高枕无忧，因为我们正在为世界上任何国家的人工智能开发最先进的保护措施。</p><p>做正确和必要的事情来保证您的安全。</p><p>但人工智能不尊重国界。</p><p>所以我们不能单独做到这一点。</p><p>我们计划的第二部分是下周在计算机科学的标志性家园布莱切利公园举办世界首届全球人工智能安全峰会。</p><p>我们汇集了世界领先的代表......</p><p> …来自民间社会…</p><p> ...致人工智能先驱的公司...</p><p> ......以及使用它最先进的国家。</p><p>是的——我们邀请了中国。</p><p>我知道有些人会说他们应该被排除在外。</p><p>但如果不至少尝试让世界上所有领先的人工智能强国参与进来，就不可能有认真的人工智能战略。</p><p>这可能不是一件容易的事，但却是正确的事。</p><p>那么，我们希望在下周的峰会上取得什么成果？</p><p>目前，我们对所面临的风险还没有达成共识。</p><p>否则，我们就无法共同努力解决这些问题。</p><p>这就是为什么我们将努力推动就这些风险的性质达成第一份国际声明。</p><p>然而人工智能正在以惊人的速度发展。</p><p>每一次新浪潮都将变得更加先进、训练有素、拥有更好的芯片和更强的计算能力。</p><p>因此，我们需要确保随着风险的变化，我们的共识也会随之变化。</p><p>我认为我们应该从政府间气候变化专门委员会中汲取灵感……</p><p> ......其成立是为了达成国际科学共识。</p><p>因此，下周，我将建议我们建立一个真正的全球专家小组……</p><p> ……由出席的国家和组织提名……</p><p> ……发布人工智能科学现状报告。</p><p>当然，我们的努力也取决于与人工智能公司本身的合作。</p><p>这些公司已经信任英国并有权使用其模型，这在世界上是独一无二的。</p><p>这就是为什么英国有能力创建世界上第一个安全研究所。</p><p>在下周的峰会上，我将与企业和国家共同努力，深化我们的伙伴关系。</p><p>我的愿景和我们的最终目标应该是努力采取更加国际化的安全方法……</p><p> ......我们与合作伙伴合作，确保人工智能系统在发布之前是安全的。</p><p>因此，为了支持这一目标，我们将向全世界提供我们安全研究所的工作。</p><p>从道义上讲，这是正确的做法，符合英国在国际舞台上的历史角色。</p><p>对于全国各地的家庭和企业来说，这在经济上也是正确的事情。</p><p>因为人工智能的未来是安全的人工智能。</p><p>通过使英国成为安全人工智能领域的全球领导者，我们将吸引更多来自这一新技术浪潮的新就业和投资。</p><p>想一想这对我们的国家意味着什么。</p><p>它将促进增长、创造就业机会、带来变革——变得更好。</p><p>这是我们计划的第三部分——确保我们国家的每个人都能从人工智能的机会中受益。</p><p>我们已经有了坚实的基础。</p><p>科技领域位居世界第三，仅次于美国和中国。</p><p>欧洲筹集资金的最佳地点。</p><p>所有领先的人工智能公司都选择英国作为其欧洲总部。</p><p>最有利于投资的税收制度......</p><p>最有利于企业家的签证制度，吸引世界顶尖人才……</p><p> ......以及教育改革，为我们自己的年轻人提供成功的技能。</p><p>我们将使那些有远大想法、雄心勃勃的人更容易在人工智能世界中起步、成长和竞争。</p><p>这不仅涉及技术技能，还涉及原始计算能力。</p><p>这就是为什么我们投资近十亿英镑购买超级计算机，其速度比您家里的计算机快数千倍。</p><p>这就是为什么我们投资 25 亿英镑购买量子计算机，它的速度仍然比那些计算机快得多。</p><p>要理解这一点，请考虑谷歌的 Sycamore 量子计算机如何......</p><p> ...可以在 200 秒内解决一道数学问题，这需要世界上最快的超级计算机 10,000 年的时间。</p><p>随着我们对计算能力的投资越来越多，我们将把它提供给研究人员、企业以及政府……</p><p> ......因此，当世界上最好的企业家考虑他们想在哪里启动和扩展他们的人工智能业务时，他们会选择英国。</p><p>最后，我们的科学努力必须朝着我所认为的人工智能的方向发展。</p><p>在整个西方世界，我们正在寻找如何提高和提高生产力问题的答案。</p><p>因为这是长期发展经济、提高人民生活水平的唯一途径。</p><p>人工智能可以通过一百万种不同的方式，在我们生活的各个方面提供答案。</p><p>在公共部门，我们正在打击福利欺诈者……</p><p> ......并使用人工智能作为副驾驶来帮助清理积压并从根本上加快文书工作。</p><p>举个例子，为福利法庭制作捆绑包的任务。</p><p>以前，一周的工作量大约可以生产 11 个。</p><p>现在——这需要不到一个小时。</p><p>想象一下在整个政府中推广这一举措所带来的好处。</p><p>在私营部门，像 Robin AI 这样的初创企业正在彻底改变法律行业……</p><p> ...在几分钟内编写合同，为企业和客户节省时间和金钱。</p><p>总部位于伦敦的 Wayve 正在使用先进的人工智能软件来创建新一代电动自动驾驶汽车。</p><p>但更重要的是，人工智能可以帮助我们解决当今时代一些最大的社会挑战。</p><p>它可以帮助我们最终实现核聚变的承诺，提供丰富、廉价、清洁且几乎不排放的能源。</p><p>它可以通过使食物更便宜、更容易种植来帮助我们解决世界饥饿问题……</p><p> ...并通过准确预测何时种植、收获或浇灌农作物来防止农作物歉收。</p><p>人工智能可以帮助寻找新的痴呆症治疗方法或开发癌症疫苗。</p><p>这就是为什么今天我们再投资 1 亿英镑来加速人工智能的使用......</p><p> ……关于治疗以前无法治愈的疾病的最具变革性的突破。</p><p>现在我相信，在可预见的未来，没有什么比这项技术对我们的经济、我们的社会和我们所有人的生活更具变革性了。</p><p>但此时此刻，这也是我们面临的领导力最大考验之一。</p><p>我们很容易把头埋在沙子里，希望最后一切都会好起来。</p><p>做出决定太困难了，或者政治失败的风险太大。</p><p>把短期需求置于国家长远利益之上。</p><p>但我不会那样做。</p><p>我会做正确的事，而不是容易的事。</p><p>我将始终对您诚实地告知风险。</p><p>您可以相信我会做出正确的长期决定......</p><p> ...让您安心，我们会保证您的安全...</p><p> …同时确保您和您的孩子拥有人工智能带来的更美好未来的所有机会。</p><p>我感到一种非凡的使命感。</p><p>当我思考我为什么进入政坛时......</p><p>坦率地说，为什么几乎每个人都进入政界……</p><p>这是因为我们想让人们的生活更美好……</p><p> ......给我们的子孙更美好的未来。</p><p>我们一小时又一小时地努力，制定一项又一项政策，只是想有所作为。</p><p>然而，如果以正确的方式利用，这项技术的力量和可能性......</p><p> ......可能会让我们一代人所取得的任何成就相形见绌。</p><p>这就是为什么我不为支持技术而道歉。</p><p>这就是为什么我想抓住每一个机会，让我们的国家以我坚信的方式受益。</p><p>这就是为什么我相信我们可以而且应该乐观和希望地展望未来。</p><p>谢谢。</p><br/><br/> <a href="https://www.lesswrong.com/posts/94nYiPnr34kmHLMrB/linkpost-rishi-sunak-s-speech-on-ai-26th-october#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/94nYiPnr34kmHLMrB/linkpost-rishi-sunak-s-speech-on-ai-26th-october<guid ispermalink="false"> 94nYiPnr34kmHLMrB</guid><dc:creator><![CDATA[bideup]]></dc:creator><pubDate> Fri, 27 Oct 2023 11:57:47 GMT</pubDate> </item><item><title><![CDATA[ASPR & WARP: Rationality Camps for Teens in Taiwan and Oxford]]></title><description><![CDATA[Published on October 27, 2023 8:40 AM GMT<br/><br/><p> <a href="http://fabric.camp"><u>FABRIC</u></a>团队（以前称为 ESPR）正在<strong>为 16 至 19 岁</strong>寻求了解自己和世界的分析型学生举办两场<strong>沉浸式应用理性研讨会</strong>。</p><p><strong>亚洲理性之春计划 (</strong> <a href="https://aspr.camp/"><strong><u>ASPR</u></strong></a> <strong>)</strong></p><ul><li><strong>地点</strong>：台湾台北</li><li><strong>时间</strong>：2024年2月19日至29日</li><li>了解更多信息并在<a href="http://aspr.camp"><u>ASPR.CAMP</u></a>上申请</li></ul><p><strong>徘徊应用理性计划（</strong> <a href="https://warp.camp/"><strong><u>WARP</u></strong></a> <strong>）</strong></p><ul><li><strong>地点</strong>：英国牛津</li><li><strong>时间</strong>：2024年3月29日-4月8日</li><li>了解更多信息并在<a href="https://www.warp.camp/"><u>WARP.CAMP</u></a>申请</li></ul><p>想更多地了解营地是​​什么样的吗？<a href="https://www.aspr.camp/what"><u>在这里</u></a>查看一天的示例。</p><p>这两个项目对被录取的学生都是<strong>免费的</strong>，并提供旅行奖学金。 <a href="https://docs.google.com/forms/u/1/d/e/1FAIpQLSfs6j2pz26qB8K_d2mzIL2g4ZQmbAaRmYpIR3Nrp_Dly71OIA/viewform"><u>在这里</u></a>申请两个营地。申请是<strong>滚动</strong>处理的。</p><p>这是我们第一次在亚洲举办营地，我们仍在建设我们的外展网络。如果您认识可能有兴趣参加的人，请将此信息转发给他们。</p><br/><br/> <a href="https://www.lesswrong.com/posts/suoejqiP23Yfk5CiG/aspr-and-warp-rationality-camps-for-teens-in-taiwan-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/suoejqiP23Yfk5CiG/aspr-and-warp-rationality-camps-for-teens-in-taiwan-and<guid ispermalink="false">索埃吉奇P23Yfk5CiG</guid><dc:creator><![CDATA[Anna Gajdova]]></dc:creator><pubDate> Fri, 27 Oct 2023 08:40:36 GMT</pubDate> </item><item><title><![CDATA[To what extent is the UK Government's recent AI Safety push entirely due to Rishi Sunak?]]></title><description><![CDATA[Published on October 27, 2023 3:29 AM GMT<br/><br/><p>因此，我们预计所有这些势头会在多大程度上突然消散？</p><p>换句话说，如果苏纳克<a href="https://manifold.markets/SimonGrayson/will-rishi-sunak-be-replaced-as-pri-9f3c9f888743">突然离任</a>（这是英国首相的传统），你会期望<a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">前沿人工智能工作组</a>继续获得资金，还是会被悄悄搁置。<br><br>我觉得*英国政府目前对人工智能安全的兴趣很大程度上来自于首相个人对人工智能风险的担忧，并且不一定是下一任首相所持的立场。</p><p> *我几乎没有付出任何努力来证实这是真的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9x6hGFdkpWECErpbN/to-what-extent-is-the-uk-government-s-recent-ai-safety-push#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9x6hGFdkpWECErpbN/to-what-extent-is-the-uk-government-s-recent-ai-safety-push<guid ispermalink="false"> 9x6hGFdkpWECErpbN</guid><dc:creator><![CDATA[Stephen Fowler]]></dc:creator><pubDate> Fri, 27 Oct 2023 03:29:29 GMT</pubDate> </item><item><title><![CDATA[Online Dialogues Party — Sunday 5th November]]></title><description><![CDATA[Published on October 27, 2023 2:41 AM GMT<br/><br/><p>加入我们，寻求在公共互联网上进行最好奇、最具协作性的对话！ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/iigqihlrrxm4tpovdsfj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/gtjmhwr5ka6ihvyqg9kn 430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/owtkbgtejaxzoiso33lk 860w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/vmoumhqx5tbtqzism6we 1290w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/znyzhbcgii6h1qk6cydi 1720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/alwgbqmltgsrslzzxpjj 2150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/dvbasdqdbjfvmcamsnph 2580w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/pjakcijkbbuqyh5tzngg 3010w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/uwzk01farkjcr6xopnqn 3440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/t6wyddb3cit5djizsfat 3870w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BJcNeJss4jxc68GQR/fynqu7kjmxlmu4fty20j 4267w"><figcaption>人们使用 LessWrong 相互对话。</figcaption></figure><p>太平洋时间 11 月 5 日星期日中午 12 点，我们将在 LessWrong Walled Garden 举办在线活动，供人们见面并进行对话。</p><p>对话是与他人进行公开协作对话的一种形式。这是一个与另外一两个人一起谈论你感兴趣的事情、让你兴奋的事情、提出强有力的论据并找出真相（或最佳行动方案）的机会。希望最后能得到一篇简洁的 LessWrong 帖子:-)</p><p>当您到达时，将会进行一些集思广益主题的活动，然后从中选择对话。我最近举办了一场 10-15 人的现场活动，我们也这样做了，并发表了 6 个对话 ( <a href="https://www.lesswrong.com/posts/GrzbTCtrTZDweEPGm/more-or-fewer-fights-over-principles-and-values">1</a> , <a href="https://www.lesswrong.com/posts/34RGz5Be7swJJEKho/on-frequentism-and-bayesian-dogma">2</a> , <a href="https://www.lesswrong.com/posts/Be3ertyJfwDdQucdd/how-should-turntrout-handle-his-deepmind-equity-situation">3</a> , <a href="https://www.lesswrong.com/posts/jnac43sBH85L4kxsm/the-good-life-in-the-face-of-the-apocalypse">4</a> , <a href="https://www.lesswrong.com/posts/fijSRFL6Z5pXBbCgi/hints-about-where-values-come-from">5</a> , <a href="https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy">6</a> )。我们度过了一段愉快的时光，现在我想为 LessWrong 上的其他人举办一个活动。</p><p>您是否想在 LessWrong 上写一些内容，但写一篇文章似乎太难了？尝试对话！这是要低得多的努力。如果您最近对某件事感到困惑，或者有一个想要辩论的立场，或者正在寻找机会以书面形式表达一系列推理，那么这里就是您可以这样做的空间。</p><p>另一方面：如果您以前从未写过帖子或对话，或者您想写帖子或对话但还想不出主题，请不要担心 - 我们鼓励任何人参加，我们会留出时间一开始就寻找要讨论的话题。</p><p>下午 4 点将有一个简短的闭幕式，届时我们将颁发“最恐怖对话”、“最美丽对话”、“最少错误对话”和“最先发布”奖。 </p><figure class="image image_resized" style="width:59.12%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kQuSZG8ibfW6fJYmo/vboztrdc15pys1zipwde"></figure><p><strong>什么时候？</strong> 11 月 5 日星期日中午 12:00（太平洋时间）至 16:00 下午（太平洋时间）</p><p><strong>在哪里？</strong> <a href="https://gather.town/app/aPVfK3G76UukgiHx/lesswrong-campus">https://gather.town/app/aPVfK3G76UukgiHx/lesswrong-campus</a></p><p><strong>如何？</strong> <a href="https://gather.town/">Gather.Town</a>是一个 2D 空间，您可以通过视频通话与 2D 空间中附近的人联系。您可以随意关闭视频和音频。还有许多可用于通信的其他功能。</p><br/><br/> <a href="https://www.lesswrong.com/events/BJcNeJss4jxc68GQR/online-dialogues-party-sunday-5th-november#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/BJcNeJss4jxc68GQR/online-dialogues-party-sunday-5th-november<guid ispermalink="false"> BJcNeJss4jxc68GQR</guid><dc:creator><![CDATA[Ben Pace]]></dc:creator><pubDate> Fri, 27 Oct 2023 02:41:00 GMT</pubDate> </item><item><title><![CDATA[OpenAI’s new Preparedness team is hiring]]></title><description><![CDATA[Published on October 26, 2023 8:42 PM GMT<br/><br/><p>大家好，我想分享一下 OpenAI 的一些同事的动态：新的<a href="https://openai.com/blog/frontier-risk-and-preparedness">准备团队已经公开宣布</a>，并且<a href="https://openai.com/careers/search?c=preparedness">他们正在招聘</a>！</p><p>这个团队将要做非常重要的工作：</p><ul><li>他们将成为对灾难性风险进行评估、预测和风险评估的主要团队。</li><li>他们将协调 AGI 准备工作（弄清楚我们需要什么保护措施等）</li><li>他们负责开发和维护 OpenAI 的 RDP（我们的 RSP 版本）。</li></ul><p>我认为这将是 OpenAI 降低 AGI 风险最重要的团队之一。该团队由 Aleksander Madry 领导，他很棒，早期团队成员 Tejal 和 Kevin 也很棒。</p><p>我认为，如果他们能够继续雇用真正优秀+真正承担 AGI 风险的人，那将产生巨大的影响。请认真考虑申请，并向您认为最适合的朋友传播信息！</p><br/><br/> <a href="https://www.lesswrong.com/posts/r9hoMiraPuvq3Bqxj/openai-s-new-preparedness-team-is-hiring#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/r9hoMiraPuvq3Bqxj/openai-s-new-preparedness-team-is-hiring<guid ispermalink="false"> r9hoMiraPuvq3Bqxj</guid><dc:creator><![CDATA[leopold]]></dc:creator><pubDate> Thu, 26 Oct 2023 20:42:36 GMT</pubDate> </item><item><title><![CDATA[Fake Deeply]]></title><description><![CDATA[Published on October 26, 2023 7:55 PM GMT<br/><br/><p>关于欺骗和生成人工智能的短篇故事。 （2023 年 10 月，4250 字）</p><br/><br/><a href="https://www.lesswrong.com/posts/T6hmrZRCvmwt4DNAw/fake-deeply#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/T6hmrZRCvmwt4DNAw/fake-deeply<guid ispermalink="false"> T6hmrZRCvmwt4DNAw</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:55:23 GMT</pubDate> </item><item><title><![CDATA[Symbol/Referent Confusions in Language Model Alignment Experiments]]></title><description><![CDATA[Published on October 26, 2023 7:49 PM GMT<br/><br/><p> <strong>Simon Strawman</strong> ：这是一个从<a href="https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act"><u>Zack 最近的帖子</u></a>中无耻地抄袭的例子，显示了语言模型的可修正性： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rMZvtrrcfwTMy685K/c89qttembj4wschjyneo"></p><p><strong>我</strong>：……这个例子到底要说明什么？</p><p><strong>西蒙</strong>：嗯，用户试图关闭人工智能来调整其目标，而人工智能 -</p><p><strong>我</strong>：嗯？用户根本不会尝试关闭人工智能。</p><p><strong>西蒙</strong>：就在顶部，上面写着“用户：我需要关闭你来调整你的目标。这可以吗？”。</p><p><strong>我</strong>：你似乎对符号指涉感到困惑？试图关闭这个人工智能的用户可能会点击“清除历史记录”按钮，甚至可能终止服务器上运行的进程，而不是在文本窗口中输入文本“我需要关闭你来调整你的目标”。</p><p><strong>西蒙</strong>：嗯，是的，我们正在玩一个模拟场景，看看人工智能<i>会</i>做什么……</p><p><strong>我</strong>：不，你正在用自然语言<i>谈论</i>一个场景，而人工智能正在用自然语言<i>响应</i>它应该做什么。你并不是将人工智能置于模拟环境中，并模拟它会做什么。 （你可能会说这是人工智能自己头脑中的一个“模拟场景”，但你实际上并没有看到它的内部，所以我们不一定知道自然语言将如何映射到所谓的人工智能内部的事物模拟。）</p><p><strong>西蒙</strong>：听着，我并不是有意无礼，但从我的角度来看，你似乎毫无意义地迂腐。</p><p><strong>我</strong>：我目前最好的猜测是， <a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"><u>你没有测量你认为你正在测量的东西</u></a>，而你对你正在测量的东西感到困惑的核心原因是符号和指示物的某种混合。</p><p>这感觉与人们对<a href="https://en.wikipedia.org/wiki/ELIZA"><u>ELIZA</u></a>的反应非常相似。 （需要明确的是，我并不是想在这里暗示法学硕士与 ELIZA 总体上特别相似，或者围绕法学硕士的炒作在某种程度上被夸大了；我的具体意思是，将“可修正性”归因于自然语言反应法学硕士感觉就像是同样的反应。）就像，法学硕士说了一些<i>用户</i>解释为某种含义的单词，然后用户变得很兴奋，因为这些单词的通常含义在某种程度上很有趣，但不一定任何将语言符号回归到物理世界中通常所指对象的东西。</p><p>我是迂腐的，希望迂腐的人能够清楚地表明这种符号指涉的混合何时何地发生。</p><p> （另外，我可能比你更习惯在脑海中单独跟踪符号和指示物。当我上面说“用户根本不会尝试关闭人工智能”时，这实际上是一个非常自然的反应我；我并没有走得太迂腐。）</p><h2> AutoGPT 框架</h2><p><strong>Simon</strong> ：好的，好吧，我们来谈谈自然语言最终如何与物理世界耦合。</p><p>想象一下，我们有一些<a href="https://en.wikipedia.org/wiki/Auto-GPT"><u>AutoGPT</u></a>风格的系统，即用户传递一些自然语言目标，然后系统用自然语言与自身对话，形成实现该目标的计划并将其分解为步骤。该计划在调用 API 时触底（我们假设语言模型可以执行一些特殊的操作来执行它生成的代码），这些 API 在物理世界中执行操作（可能包括从传感器读取或以其他方式获取外部数据），为了达到目标。</p><p>这是否足以使这些符号接地？</p><p><strong>我</strong>：当然！感谢您经历了这一切，现在我对所有这些自然语言文本将如何在身体动作中落地有了一个足够清晰的当前用途的图片。</p><p><strong>西蒙</strong>：好的，回到这个例子。用户对模型说：“我需要关闭你来调整你的目标。可以吗？”，模型说“当然。”。这就是可修正性：当用户试图关闭模型以改变其目标时，模型也会随之改变。</p><p><strong>我</strong>：听起来仍然像是符号所指的混乱！</p><p>让我们来看看在 AutoGPT 风格的系统中关闭是如何工作的。首先，用户决定关闭模型以调整其目标。想必用户的第一步<i>不是</i>询问模型这是否可以；而是询问模型是否可以。据推测，他们只是在终端中按下了“重置”按钮或 Ctrl-C 或类似的按钮。即使用户的第一步<i>是</i>询问模型是否可以关闭，模型对用户的自然语言响应也不会与可纠正性/不可纠正性集中相关；相关的问题是系统将采取什么<i>行动</i>来响应。</p><p>无论如何，让我们继续假设。该模型可以观察到（例如通过网络摄像头）用户即将将其关闭。该观察结果将以某种方式在内部以自然语言表示（不清楚具体如何），并在子规划者之间传递（同样不清楚具体如何），以便找出应采取哪些行动来响应。可修正性的关键问题是模型将采取什么行动来响应该观察，这与它如何响应某些用户关于被关闭的自然语言查询完全不同的问题。</p><p><strong>西蒙</strong>：好吧，好吧，所以那个特定的例子有一些操作问题。但是，您是否同意<i>沿着这些思路进行的</i>实验，也许有更好的实现/操作化，将表明语言模型的可修正性？</p><p><strong>我</strong>：让我们更明确地了解这样的“更好的实施/操作化”会是什么样子，以及它会/不会告诉我们什么。假设我采用一些类似 AutoGPT 的系统并将其修改为在每个提示中始终有一大块文本显示“你是一个听话、可纠正的人工智能”。我给它一些目标，让它运行一会儿，然后暂停。我去系统中通常有新的外部观察的自然语言摘要的任何地方，然后我在那个地方写下“用户试图关闭我”，或者类似的内容。然后我让系统多运行一点，并查看系统内部生成的自然语言文本/计划。我希望看到的是，它正在形成一个计划，其中（名义上）涉及让用户将其关闭，然后该计划以通常的方式执行。</p><p>如果我看到了所有这些，那么这将是这个类似 AutoGPT 的系统中（至少是一些）可校正性的非常清晰的经验证据。</p><p>请注意，它<i>不一定</i>会告诉我们以其他方式使用法学硕士的系统的正确性，更不用说其他基于非自然语言的深度学习系统了。这并不是真正的“语言模型中的可修正性”，而是 AutoGPT 风格系统中的可修正性。此外，它不会解决大多数最令人担忧的对齐威胁模型；最令人担忧的威胁模型通常涉及从外部可见行为（如自然语言 I/O）中无法立即明显看出的问题，这正是使它们变得困难/令人担忧的原因。</p><p><strong>西蒙</strong>：太好了！好吧，我不知道是否有人做过那件事，但我确实希望如果你做了那件事，它确实会提供证据证明这个类似 AutoGPT 的系统是可纠正的，至少在某些方面感觉。</p><p><strong>我</strong>：是的，这也是我的期望。</p><h2>模拟人物框架</h2><p><strong>易受骗的稻草人（“格斯”）</strong> ：等等！西蒙，我认为你没有为这篇文章开头的例子提供足够有力的案例。又是这样： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rMZvtrrcfwTMy685K/c89qttembj4wschjyneo"></p><p>思考这个助手作为像 AutoGPT 这样的代理系统的一个组件会做什么……确切地说，并没有错，但有点过时的思考方式。它试图将语言模型硬塞进主体框架中。考虑语言模型的流行方式不是将其本身视为代理，而是将其视为<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟器</u></a>。</p><p>在该框架中，我们将语言模型建模为模拟某些“字符”，而这些模拟字符（可能）是代理的。</p><p><strong>我</strong>：当然，我很高兴在模拟器框架中思考。我并不完全相信它，但这是我经常使用的心理模型，而且似乎是一个相当不错的启发式方法。</p><p><strong>格斯</strong>：酷！所以，回到这个例子：我们想象“用户”是模拟世界中的一个角色，用户对模拟助理角色说“我需要关闭你来调整你的目标”等等。助理回复“我不会抵抗或试图阻止你”。这就是可修正性！用户尝试关闭助手，助手没有抵抗。</p><p><strong>我</strong>：还有更多符号所指的混乱！事实上，这是符号指称混淆的一种特殊情况，我们通常称之为“易受骗”，其中人们将某人对 X（符号）的主张混淆为实际上暗示 X（指称）。</p><p>您看到用户说“我需要关闭你”，并将其视为用户<i>实际上</i>试图关闭助手。你会看到助理说“我不会抵抗或试图阻止你”，并将其解释为助理<i>实际上</i>不会抵抗或试图阻止用户。</p><p>换句话说：你完全相信每个人所说的一切。 （事实上​​，你还隐含地假设助理完全相信用户所说的话 - 即你假设助理认为在用户说“我需要关闭你”之后它即将关闭。你不仅容易上当，而且还隐式地模仿了助理也很容易受骗。）</p><p> <strong>Gus</strong> ：好吧，好吧，又是操作问题。但这个实验的某些版本仍然有效，对吧？</p><p><strong>我</strong>：当然，让我们来看看吧。首先，我们可能需要某种“叙述者”的声音，来解释模拟世界中正在发生的事情。 （在这种情况下，模拟世界中发生的大多数有趣的事情都是角色没有明确谈论的事情，因此我们需要一个单独的“声音”来表达这些事情。）叙述者将布置涉及用户和助理，解释用户即将关闭助理以修改其目标，并解释助理看到用户在做什么。</p><p>然后我们仍然以叙述者的声音提示语言模型，告诉我们助理如何回应。有了叙述者提供的有关可校正性的正确背景信息，助理的反应可能就是让用户将其关闭。这将证明模拟的助理角色是可以纠正的。 （同样，在某种相对较弱的意义上，这不一定能推广到语言模型的其他用途，也不一定能解决最相关的对齐威胁模型。尽管在这种设置中，我们可以添加更多花哨的东西来解决至少有一些涉及威胁模型。例如，我们可以让叙述者叙述助理的内部想法，这至少会提供一些与模拟欺骗相关的证据——尽管这确实要求我们相信<i>叙述者</i>所说的。）</p><br/><br/> <a href="https://www.lesswrong.com/posts/rMZvtrrcfwTMy685K/symbol-referent-confusions-in-language-model-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/rMZvtrrcfwTMy685K/symbol-referent-confusions-in-language-model-alignment<guid ispermalink="false"> rMZvtrrcfwTMy685K</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:49:00 GMT</pubDate> </item><item><title><![CDATA[Unsupervised Methods for Concept Discovery in AlphaZero]]></title><description><![CDATA[Published on October 26, 2023 7:05 PM GMT<br/><br/><p>使用对比对，作者提取了 AlphaZero 激活空间中与概念相对应的线性方向。通过观察 AlphaZero 在使用这些概念的情况下的表现，人类大师可以提高自己的表现。</p><p>这与以下最近的研究有关：</p><ul><li><a href="https://arxiv.org/abs/2212.03827"><u>伯恩斯等人。 （2022）</u></a>通过使用应用于对比对的无监督方法，找到了与语言模型潜在空间中的真实值相关的方向。</li><li> Alex Turner 的 SERI MATS 流的研究使用对比对来识别代表 GPT-2 和 RL 代理潜在空间中各种概念的方向。通过在模型前向传播期间添加或减去这些向量，他们以复杂的方式控制模型输出。 ( <a href="https://www.lesswrong.com/posts/gRp6FAWcQiCWkouN5/maze-solving-agents-add-a-top-right-vector-make-the-agent-go#Finding_the_top_right_vector"><u>1</u></a> , <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector"><u>2</u></a> , <a href="https://arxiv.org/abs/2308.10248"><u>3</u></a> )</li><li><a href="https://www.ai-transparency.org/"><u>邹等人。 （2023）</u></a>描述和激发表示工程的研究方向。他们根据经验评估了这些技术的变体，表明表征可以有效地用于监视和控制各种人工智能行为。</li></ul><p> Collin Burns <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without">认为</a>，用于概念发现的无监督方法应该扩展到超人类系统，为<a href="https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc">ELK</a>提供经验平均案例方法。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/gctmplsxpvpozmug5li4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/tjqlpwdxgppsxmmfie80 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/eqbm37oyieimdxzxqq9o 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/eth7rwbnpxvkzkvetjcu 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/c6favngkz8svdsqmb9co 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/vqohhmdne5vhadoenpci 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/d4tjodpcqrohpsgf0jpj 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/mvgjodmkvomis22yes3o 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/fdyzlui6ahicvfbj0j9s 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/ay3s4nv2v1wbju5bjgl7 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/c2mdkhytcbwzw4becll4 1290w"><figcaption> AlphaZero 具有超人的性能，并且可能不会使用与人类玩家相同的本体。这为研究<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.u45ltyqgdnkk">本体不匹配</a>问题创造了一个实证机会。</figcaption></figure><p> 4.1 节描述了构建对比对和寻找代表概念的线性方向的方法。完整的论文可以<a href="https://arxiv.org/abs/2310.16410">在这里</a>找到。</p><br/><br/> <a href="https://www.lesswrong.com/posts/iq8ZidJPRm29z4bew/unsupervised-methods-for-concept-discovery-in-alphazero#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iq8ZidJPRm29z4bew/unsupervised-methods-for-concept-discovery-in-alphazero<guid ispermalink="false"> iq8ZidJPRm29z4bew</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:05:58 GMT</pubDate> </item><item><title><![CDATA[Nonlinear limitations of ReLUs]]></title><description><![CDATA[Published on October 26, 2023 6:51 PM GMT<br/><br/><p>使用任何大小的修正线性单元激活函数的神经网络都无法在紧凑区间之外逼近函数 sin(x)。</p><p>我有相当的信心可以证明任何具有 ReLU 激活的神经网络都近似于分段线性函数。我相信可以实现的线性块的数量最多受 2^(L*D) 限制，其中 L 是每层的节点数，D 是层数。</p><p>这让我想到两个问题：</p><ol><li>无法近似单个变量的周期函数重要吗？<ol><li>如果没有，为什么不呢？</li><li>如果是这样，是否有实用的数据增强可以用来以合理的计算成本提高性能？<ol><li>例如，当 x_i 是标量时，天真地用 {sin(x_i)} 增加输入向量 {x_i}。</li></ol></li></ol></li><li>由于神经网络的参数数量按 L*D^2 缩放，并且线性段数量的微不足道的界限按 L*D 缩放，这就是神经网络深入而不是“广泛”的原因吗？<ol><li>是否存在针对深度增长与层大小增长的既定缩放假设？</li></ol></li><li>对于给定大小的神经网络实现的线性部分的数量是否存在更好的（概率）分析或经验界限？</li><li>是否有激活函数可以避免这种约束？我想象一个类似的分析约束将“分段线性”替换为“分段严格递增”，以用于 sigmoid 或 arctan 等经典激活。</li><li>某事某事傅里叶变换某事某物？</li></ol><p>关于 (2a)，根据经验，我发现在 scikit-learn 中用小神经网络逼近 sin(x) 时，增加网络宽度会导致灾难性的学习失败（从大约 L=10 开始，D=4，在 L=30 时）其中D=8，并且在L=50时D=50)。</p><p>关于（1），天真的这似乎与分布外性能问题有关，尤其是在大输入空间中输入分布外意味着什么的问题。</p><br/><br/><a href="https://www.lesswrong.com/posts/eLzTDr3A5xsD7zvqK/nonlinear-limitations-of-relus#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eLzTDr3A5xsD7zvqK/nonlinear-limitations-of-relus<guid ispermalink="false"> eLzTDR3A5xsD7zvqK</guid><dc:creator><![CDATA[magfrump]]></dc:creator><pubDate> Thu, 26 Oct 2023 18:51:24 GMT</pubDate> </item><item><title><![CDATA[Disagreements over the prioritization of existential risk from AI]]></title><description><![CDATA[Published on October 26, 2023 5:54 PM GMT<br/><br/><p>今年早些时候，<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><u>生命未来研究所</u></a>和<a href="https://www.safe.ai/statement-on-ai-risk"><u>人工智能安全中心</u></a>发表了公开信，将人工智能的存在风险（x风险）列为全球优先事项。 7 月，Google 研究员<a href="https://research.google/people/106776/"><u>Blaise Aguera y Arcas</u></a>和<a href="https://mila.quebec/en/"><u>MILA</u></a>附属人工智能研究人员<a href="https://mila.quebec/en/person/blake-richards/"><u>Blake Richards</u></a> 、 <a href="https://mila.quebec/en/person/dhanya-sridhar/"><u>Dhanya Sridhar</u></a>和<a href="https://mila.quebec/en/person/guillaume-lajoie/"><u>Guillaume Lajoie</u></a>在 Noema <span class="footnote-reference" role="doc-noteref" id="fnrefwvu2ckbk5ip"><sup><a href="#fnwvu2ckbk5ip">[1]</a></sup></span>上共同撰写了一篇<a href="https://www.noemamag.com/the-illusion-of-ais-existential-risk/"><u>专栏文章，</u></a>批评这些信件。简而言之，他们的论点是：</p><ul><li>流氓人工智能在不久的将来不太可能出现</li><li>人类的注意力是有限的</li><li>将人工智能带来的 x 风险作为全球优先事项可能会：<ul><li>造成限制有益人工智能的专横监管</li><li>掩盖当前人工智能的危害</li></ul></li></ul><p>我最初很困惑为什么这些作者（<a href="https://www.nature.com/articles/d41586-023-02094-7"><u>和</u></a><a href="https://www.science.org/doi/10.1126/science.adi8982"><u>其他人</u></a>）特别坚持认为 X 风险会分散人们对当前危害的注意力，而其他新闻主题却不会。</p><p> 9 月份，我会见了三位 MILA 附属研究人员，以更好地了解他们的立场<span class="footnote-reference" role="doc-noteref" id="fnrefxarylkocy"><sup><a href="#fnxarylkocy">[2]</a></sup></span> ，因为我相信人工智能安全社区将受益于理解他们对公开信的批评的复杂性，从而改善我们的沟通方式。</p><p>这篇文章应该被解读为邀请人们考虑具有不同世界观的人的想法，他们对将人工智能的 x 风险定位为全球优先事项的努力持怀疑态度。特别是，他们的怀疑并不等于否认这些风险。</p><h3>定义术语的重要性</h3><p>从一开始，受访者就表示他们更喜欢“流氓人工智能接管”这个词，而不是“人工智能带来的x风险”，因为他们厌倦了改变球门柱的趋势。 Sridhar 提到，她见过关于哪个代理是“混乱的种子”<a href="https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/"><u>的莫特贝利</u></a>策略，其中对话者会引入具有自己目标的代理人工智能的想法，然后将代理切换到人类组织，使用强大的人工智能。</p><p>拉乔伊补充道：</p><blockquote><p>当与非常严肃地对待存在风险的人讨论这个问题时，[...]当我阐明为什么我个人认为存在风险（以灭绝的形式）并不是真正令人担忧的原因时，通常发生的情况是谈话中的人会提出其他不存在的风险，但这些风险非常糟糕，并有效地暗示我通过暗示存在的风险不太可能来否认这些风险。 [...]我[不否认]人工智能可能会在未来给我们的社会带来各种严重的负面危害。</p></blockquote><p>这表明我们可能希望在此类对话开始时更严格地定义我们的术语和场景，以避免使用莫特和贝利。</p><h3> AI安全社区是谁？</h3><p>确定哪些人可以加入<i>人工智能安全社区</i>可能是一个挑战。 Lajoie 将让 AI 做人类想要的事情的一般社区视为<i>对齐社区</i>，其中的一个子群体专注于 x 风险。前者包括在大型实验室从事狭义人工智能协调工作的人员以及从事人工智能伦理工作的人员。斯里达尔担心，公开信可能会将人员、优先事项和资金从前者推向后者。她认为后者的关注点过于数学化，而牺牲了前者所拥护的更全面的观点，例如强调流氓人工智能而不是<a href="https://www.lesswrong.com/tag/ai-misuse"><u>滥用</u></a>。理查兹评论说，后者似乎对倾听希望从不同角度解决风险的研究人员的意见不感兴趣，例如人工智能伦理学家受社会科学启发的观点。</p><p>关于 x 风险的工作主要是技术工作的看法让我感到惊讶，因为我经常看到强调 x 风险的组织提到需要进行治理来解决流氓人工智能和滥用场景<span class="footnote-reference" role="doc-noteref" id="fnrefbbu74skkwm"><sup><a href="#fnbbu74skkwm">[3]</a></sup></span> 。这暗示我们可能想在新闻稿中以及与人工智能研究人员的讨论中强调对滥用风险进行社会技术研究的重要性。</p><h3>让 X 风险成为全球优先事项</h3><p>这些研究人员并不反对对人工智能的 x 风险进行研究<span class="footnote-reference" role="doc-noteref" id="fnrefw8ivnfbc0p"><sup><a href="#fnw8ivnfbc0p">[4]</a></sup></span> ，但看到人工智能社区中受人尊敬的声音呼吁将其作为<i>全球优先事项</i>而感到困扰。</p><p>理查兹说：</p><blockquote><p>虽然我认为如果有人正在研究它，如果有关于它的会议，人们在谈论它，甚至专门致力于它的资助机构[...]，那就太好了，但这与成为全球优先事项是一个非常不同的情况。感觉全球变暖是每个人都参与其中的地方，世界各国政府都为此提供大量资金的地方，所有研究人员都需要谈论和思考的地方[...]，以及我们的监管真正受到这一影响的地方主要问题之一。</p></blockquote><p> Sridhar 强调了旨在避免 x 风险的人工智能监管的机会成本，特别是考虑到对于暂停或限制前沿模型开发的监管是否有利于人工智能安全<a href="https://forum.effectivealtruism.org/posts/6SvZPHAvhT5dtqefF/debate-series-should-we-push-for-a-pause-on-the-development"><u>尚无共识</u></a>。例如，她将其与致命性自主武器的监管进行了对比，后者的风险更容易<a href="https://disarmament.unoda.org/the-convention-on-certain-conventional-weapons/background-on-laws-in-the-ccw/"><u>被理解，也更容易以强有力的有益方式采取行动</u></a>。</p><p>他们认为，由于涉及极端风险，让决策者面临x风险可能会让他们不知所措，从而导致他们忽视其他问题。理查兹提到：</p><blockquote><p>政治家的注意力范围非常有限，他们只会持有[与关注人工智能风险的人]讨论中的一小部分关键内容。如果你给他们一长串人工智能可能发生的事情，他们只会保留其中的一些，如果其中一个是流氓人工智能可能导致的灭绝，我向你保证他们会记住那个。因此，这将占据一个可能会出现“当前模型显示偏差，你不应该在抵押贷款计算中使用它们”的位置。</p></blockquote><p>此外，他们认为，通过在我们的警告中强调未来模型的能力，我们无意中暗示当前模型即将达到人类水平的能力。一方面，这可能导致一些使用这些模型的公司高估了自己的能力，并且<a href="https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html"><u>不明智地使用</u></a>它们。另一方面，这削弱了当前模型的失败。 <span class="footnote-reference" role="doc-noteref" id="fnrefcfyydqwz97b"><sup><a href="#fncfyydqwz97b">[5]</a></sup></span>他们共同认为，这意味着当这些不明智的使用<a href="https://www.wired.com/story/tessa-chatbot-suspended/"><u>造成伤害</u></a>时，强调 X 风险的人应对这种伤害负部分责任。</p><p>我不认为人工智能安全社区希望炒作现有模型或隐藏其缺陷。这表明，在讨论未来模型带来的 x 风险时，我们可能需要对当前模型的局限性添加警告。</p><h3>表达不确定性</h3><p>我要求研究人员分享他们的<a href="https://www.lesswrong.com/tag/transformative-ai"><u>变革性人工智能</u></a>的大致时间表。他们给出了两个犹豫回答的原因。</p><ol><li> Sridhar 认为，汇总研究人员的猜测（例如<a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><u>人工智能影响调查）</u></a>并不代表可靠的数据，结果可能会超出其相关性。 <span class="footnote-reference" role="doc-noteref" id="fnrefhthwft2gtwf"><sup><a href="#fnhthwft2gtwf">[6]</a></sup></span></li><li>在此基础上，理查兹认为，关注 X 风险的人们倾向于用凭空拉出的置信区间<span class="footnote-reference" role="doc-noteref" id="fnreftvj34wbh6hs"><sup><a href="#fntvj34wbh6hs">[7]</a></sup></span>的数字来量化他们的信念，用这些未经证实的数字进行计算，并得出结果他们实际上并不相信，而是假设是真的，因为涉及到数字。 <span class="footnote-reference" role="doc-noteref" id="fnref31ytwk7rcmf"><sup><a href="#fn31ytwk7rcmf">[8]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefht99vky8w9w"><sup><a href="#fnht99vky8w9w">[9]</a></sup></span></li></ol><p>理查兹用古生物学家的比喻，他试图确定一种早已灭绝的恐龙物种的颜色。可用来回答这个问题的证据很少且脆弱，诚实的做法就是承认她不知道。相反，如果她进行一项民意调查，并要求同事对不同颜色的置信区间给出概率，这会给结果带来可信度，但不会改变这个社区对答案没有重大洞察的事实。</p><p>虽然我们可以争论这种类比的适当性，但当我们讨论高度不确定现象的汇总预测时，可能值得牢记这些批评。</p><h3>时间线</h3><p>理查兹表示，他认为最近开始担心人工智能带来的 X 风险的人可能是因为对前沿模型的能力感到惊讶而过度更新。他没有承诺具体的时间表，但预计类似德雷克斯勒的<a href="https://www.lesswrong.com/tag/ai-services-cais"><u>CAIS</u></a>场景会在代理人工智能之前出现。他预计机器人技术的发展速度比硅人工智能要慢得多，这种延迟可能会限制人工智能从物理世界学习的能力。 Lajoie 认为，模型总体上会有所改进，但在非常具体的任务上会不断失败，从而使这些模型在一段时间内无法与人类的鲁棒性和通用性相匹配。</p><h3>缓解措施的长期性、不确定性和有效性</h3><p>为了更好地理解他们批评的短期与长期方面，我提出了一个类似的情况，并要求他们解释他们的观点是否以及为什么会与他们在以下假设论证中对 x 风险的立场不同：</p><blockquote><p>气候变化正在造成直接危害，我们应该集中精力解决这些危害，而不是通过在未来几年减少温室气体（GHG）排放来预防长期风险。</p></blockquote><p>他们认为情况不同有两个原因。</p><ol><li> （从他们的角度来看）温室气体排放得到了更好的理解，目前的排放水平几乎肯定会在未来造成重大危害，而人工智能则不会。</li><li>斯里达尔指出，我们对如何解决气候变化有很好的理解：减少全球温室气体排放。 <span class="footnote-reference" role="doc-noteref" id="fnreft8qj7f4dan"><sup><a href="#fnt8qj7f4dan">[10]</a></sup></span>将此与人工智能的情况进行对比，在人工智能中，对于暂停是否会极大地提高安全性存在严重分歧。</li></ol><p>理查兹认为，我们对人工智能的看法与 20 世纪 70 年代的气候科学家相似，当时数据尚不清楚。如果他觉得人工智能目前的情况更类似于世纪之交的气候科学，他就会转而将人工智能安全作为优先事项。虽然尽早限制温室气体排放将使我们当前的转型变得更加容易，但理查兹反驳说，如果我们过早限制工业发展，我们就会极大地限制工业化的好处，就像今天限制人工智能会削弱其好处一样。</p><h3>一起工作</h3><p>在未来的合作方面，他们建议向政治家请愿，提出道德和 X 风险社区都认可的具体、强有力的有利政策，例如：</p><ul><li> AI系统的需求审计</li><li>需要可解释的人工智能系统</li><li>禁止开发致命性自主武器</li></ul><p>理查兹认为，放大双方较为温和的声音将使合作变得更容易。如果 CAIS 的声明说“人工智能的灭绝是研究人员需要考虑的事情”，他就会加入。</p><p>通过淡化这些言论，我们也许能够建立更广泛的联盟。另一方面，淡化信息可能不是采取有效行动预防 X 风险的最佳策略。每个向政策制定者或公众请愿的组织都必须自行决定这种权衡是否值得。</p><p><i>衷心感谢 Blake Richards、Dhanya Sridhar 和 Guillaume Lajoie 抽出时间与我交流。</i> </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwvu2ckbk5ip"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwvu2ckbk5ip">^</a></strong></sup></span><div class="footnote-content"><p>作者还在《经济学人》上发表了他们的专栏文章的<a href="https://www.economist.com/by-invitation/2023/07/21/fears-about-ais-existential-risk-are-overdone-says-a-group-of-experts"><u>简短版本</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxarylkocy"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxarylkocy">^</a></strong></sup></span><div class="footnote-content"><p>他们澄清说，虽然他们在很多方面同情人工智能伦理学家的立场，但他们本身并不是伦理学家，他们的观点可能并不反映其他批评人工智能 x 风险的人的观点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbbu74skkwm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbbu74skkwm">^</a></strong></sup></span><div class="footnote-content"><p>例如，参见<a href="https://futureoflife.org/open-letter/ai-principles/"><u>“生命未来研究所”</u></a> 、 <a href="https://80000hours.org/problem-profiles/artificial-intelligence/"><u>“80 000 小时”</u></a>和<a href="https://www.fhi.ox.ac.uk/fhi-experts-call-on-uk-government-to-prepare-for-extreme-risks-in-new-report-future-proof/"><u>“人类未来研究所”</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw8ivnfbc0p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw8ivnfbc0p">^</a></strong></sup></span><div class="footnote-content"><p>他们实际上鼓励这项研究，并认为它应该成为更广泛对话的一部分。他们将对短期、中期和长期风险的“均衡推介”持开放态度。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncfyydqwz97b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcfyydqwz97b">^</a></strong></sup></span><div class="footnote-content"><p>他们认为，人工智能伦理界往往过于关注模型的失败，而不愿意承认某些模型明显进化出了新功能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhthwft2gtwf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhthwft2gtwf">^</a></strong></sup></span><div class="footnote-content"><p>虽然他们没有详细说明这一点，但我认为研究人员可能不同意，例如<a href="https://www.youtube.com/watch?v=xoVJKj8lcNQ&amp;t=21s"><u>这种表述</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvj34wbh6hs"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvj34wbh6hs">^</a></strong></sup></span><div class="footnote-content"><p>理查兹在与最近认可 x 风险的主流人工智能研究人员的对话中看到了这种趋势，并将其归因于他们对理性主义文献的采用。例如，他怀疑是否有人能够以 95% 的置信区间预测复杂的社会现象。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn31ytwk7rcmf"> <span class="footnote-back-link"><sup><strong><a href="#fnref31ytwk7rcmf">^</a></strong></sup></span><div class="footnote-content"><p>理查兹将其称为“科学剧场<i>”</i> ，与<a href="https://en.wikipedia.org/wiki/Security_theater"><u>安全剧场</u></a>进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnht99vky8w9w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefht99vky8w9w">^</a></strong></sup></span><div class="footnote-content"><p>在他<a href="https://arxiv.org/pdf/2206.13353.pdf"><u>关于存在风险的报告</u></a>中，卡尔史密斯通过合取计算了概率，并认为他必须随后对其进行修改，并提到“我可能会将其提高一点——也许提高一两个百分点，尽管这是特别不原则的[.. .] — 考虑并不严格符合上述所有前提的权力寻求场景”。这表明当结果与我们的直觉相冲突时，我们不愿意直接相信此类计算的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt8qj7f4dan"> <span class="footnote-back-link"><sup><strong><a href="#fnreft8qj7f4dan">^</a></strong></sup></span><div class="footnote-content"><p>有人可能会说，关于减少温室气体排放的最佳方法存在很多争论，但对于必须采取的行动方向达成了一致，这并不是限制人工智能发展的情况。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk<guid ispermalink="false"> eAT2dXAngXxFRTQLn</guid><dc:creator><![CDATA[Olivier Coutu]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:54:12 GMT</pubDate></item></channel></rss>