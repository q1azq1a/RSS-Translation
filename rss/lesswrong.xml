<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 27 日星期一 04:14:46 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Unknown Probabilities]]></title><description><![CDATA[Published on November 27, 2023 2:30 AM GMT<br/><br/><p>未知的概率听起来像是类型错误。存在未知数，例如抛硬币的结果。这些未知数有可能取特定值，例如翻转出现正面的概率。</p><p>作为一个公式，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads}) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>未知数，即翻转的结果，位于概率算子内部。概率为 1/2，超出范围。它们不是同一类东西。</p><p>但假设对于未知<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>的每个可能值<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span> ，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(H|B, \Theta = \theta) = \theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span>其中<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="H"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span></span></span></span>和<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span>是命题。那么可以说<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是假设<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="H"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span></span></span></span></span>相对于背景知识<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span>的未知概率。</p><p>一些自然的例子：</p><ul><li><p>假设翻转一枚硬币，你不知道它是一枚公平的硬币，还是一枚两面都是正面的硬币。如果是公平硬币，则<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为 1/2；如果是花样硬币，则 θ 为 1。</p></li><li><p>当从总体中随机抽样时，令<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为总体中具有某种属性的个体的比例。</p></li><li><p>假设一个模具被滚动，但由于制造不精确，它的形状偏离了完美的立方体。考虑模具的相空间：质心位置和动量的六个维度，以及三个角度和三个角速度的附加六个维度。根据以该点作为初始条件滚动时出现的骰子面来标记该相空间中的每个点。令<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>为标记为 6 的相空间的分数。只要滚动初始条件的先验分布足够宽，这应该几乎完全是骰子出现 6 的未知概率。（这是基于 Jaynes 的《概率论：科学逻辑》第 10.6 节中对硬币的讨论。未知形状的想法是受到<a href="https://doi.org/10.1007/978-94-009-3049-0_10">骰子数据实际分析作为制造缺陷推断的</a>启发。）</p></li></ul><p>如果你知道骰子的形状，我可以通过说<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是你分配给 6 的概率来大大简化最后一个。事实上，如果您有一些额外的信息，所有这些未知的概率实际上都是您<em>会</em>分配的概率。但在我们到达那里之前，我将通过真正必要的更形式主义来介绍第一个例子，即硬币的例子。我宁愿乏味也不愿神秘。</p><p> （顺便说一句，如果您已经学习过基于测度论的概率类，其中条件期望被定义为某个随机变量，那么这已经很熟悉了。此外，未知概率是 Jaynes 的“ <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span></span> ”中的“ <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span> ” ，而上面的公式实际上与他的《概率论：科学的逻辑》中的18.1相同。）</p><h1>抛一枚可能是骗局的硬币</h1><p>形式主义如下。每个未知数都是一个函数。它将可能的世界映射到该世界中未知的价值。</p><p> （如果您熟悉更常用的术语：我所说的“未知”是随机变量，我所说的“可能世界”是结果。）</p><p>对于硬币示例，我们需要四个可能的世界。我已经提到过的一个未知数的例子是“结果”：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_1) = \text{Tails}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">反面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_2) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_3) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\text{Result}(\omega_4) = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span></span></span></span></span></p><p>当我列出所有未知数时，这些可能的世界意味着什么就会更加清楚。我会将其放入表格中，以便“结果”将成为表格的一列。</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\begin{array}
\hline
&amp; \text{Coin} &amp; \text{Side} &amp; \text{Result} &amp; \Theta \\
\hline
\omega_1 &amp; \text{Fair} &amp; \text{A} &amp; \text{Tails} &amp; 1/2 \\
\omega_2 &amp; \text{Fair} &amp; \text{B} &amp; \text{Heads} &amp; 1/2 \\
\omega_3 &amp; \text{Trick} &amp; \text{A} &amp; \text{Heads} &amp; 1 \\
\omega_4 &amp; \text{Trick} &amp; \text{B} &amp; \text{Heads} &amp; 1\\
\hline
\end{array}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-menclose"><span class="mjx-box" style="padding: 0px 0px -0.074em 0px; border-bottom: 1px solid;"><span class="mjx-mrow"><span class="mjx-mtable" style="vertical-align: -3.346em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.421em;"><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.4em; border-bottom: 1.3px solid; text-align: left; width: 1.011em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 2.364em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">硬币</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 1.834em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">边</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.5em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 2.797em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.221em 0.4em 0px 0.5em; border-bottom: 1.3px solid; text-align: left; width: 1.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.475em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω1</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平的</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">尾巴</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.475em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω2</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平的</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">乙</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.4em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω3</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">诡计</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.421em;"><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.4em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω4</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">诡计</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">乙</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.5em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头</span></span><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.2em 0.4em 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p>四种可能的世界：两枚硬币，乘以两侧。我添加了这个未知的“侧面”来区分硬币的两面，但我实际上并不打算使用它。对于花样硬币，任一“面”的“结果”均为正面。</p><p> <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>仅仅取决于币。不过，它们都是可能世界的函数，所以我们可以将其定义为</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\Theta(\omega) = \begin{cases}
1/2 &amp; \text{if Coin}(\omega) = \text{Fair} \\
1 &amp; \text{if Coin}(\omega) = \text{Trick}
\end{cases}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.925em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.175em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 1.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 7.988em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">如果硬币</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平</span></span><span class="mjx-strut"></span></span></span></span><span class="mjx-mtr" style="height: 1.175em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">如果硬币</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">戏法</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span></span></span></span></span></p><p>从表中可以看出Coin和<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>是多余的，可以相互替代。他们的价值观选择了相同的可能世界。我们可以按如下方式使用该替换：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = 1/2) = P(\text{Result} = \text{Heads} | \text{Coin} = \text{Fair}) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">硬币</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">公平</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></p><p>同样，</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = 1) = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">正面</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></p><p>将这两者放在一起，我们可以得到，对于<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>的每个可能值，<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="P(\text{Result} = \text{Heads} | \Theta = \theta) = \theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Result</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Heads</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span>因此，只要我们没有其他相关的信息，将<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>称为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Result} = \text{Heads}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Result</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Heads</span></span></span></span></span></span>的未知概率是有意义的背景知识。</p><p>请注意，如果我们“扩展”可能世界的空间，例如让每个可能世界代表相空间翻转的轨迹，那么这并不重要。我们可以将这四个可能世界中的每一个视为来自更丰富的可能世界空间的某些有损函数的输出。丢失的信息不会影响我们的分析，因为可以根据更粗粒度的描述来定义感兴趣的未知数。</p><h1>你会相信什么</h1><p>现在我们可以回到这样的想法：“未知概率”实际上是我们在更多信息的情况下分配<em>的</em>概率。在抛硬币的情况下，如果我们知道抛的是花招还是公平的硬币，那么这就是我们分配给正面的概率。<em>如果</em>我们不知道其他相关信息，例如翻转的初始条件。</p><p>虽然这种记法使得正式地写起来有点烦人，但我们可以按如下方式进行：<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\Theta(\omega) = P(H|B, Y = Y(\omega))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y=Y(\omega)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>是一个有效的命题，因为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y(\omega)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>只是<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>可以取的某个特定值。该公式表示某个世界的“未知概率”是条件概率，条件条后面是该世界的未知<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>值。未知概率是根据概率算子定义的，但它与其他任何未知一样都是未知的：可能世界的函数。</p><p>有趣的是，这些未知概率是根据后验概率定义的。也就是说，您可以将上面的<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span></span></span></span></span>视为学习<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>后您将更新到的后验概率。该后验概率未知，因为<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.182em;">Y</span></span></span></span></span></span>未知。这导致了<a href="https://www.lesswrong.com/tag/conservation-of-expected-evidence">预期证据守恒</a>的陈述：<span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="E[\Theta] = P(H|B)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">θ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">H</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span>您可能听说过预期后验是先验。天真地，这似乎是一个类型错误：概率不是未知数，因此我们不能对它们抱有期望。但有了未知概率的概念，我们可以从字面上理解它。</p><p>给读者的练习：这是否与 <a href="https://www.lesswrong.com/posts/cpWgnzhbZyxQFzCsj/the-principle-of-predicted-improvement">预测改进原则</a>相矛盾？应该如何定义该帖子中的未知后验概率？</p><br/><br/><a href="https://www.lesswrong.com/posts/gJCxBXxxcYPhB2paQ/unknown-probabilities#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gJCxBXxxcYPhB2paQ/unknown-probabilities<guid ispermalink="false"> gJCxBXxxcYPhB2paQ</guid><dc:creator><![CDATA[transhumanist_atom_understander]]></dc:creator><pubDate> Mon, 27 Nov 2023 02:30:07 GMT</pubDate> </item><item><title><![CDATA[Justification for Induction]]></title><description><![CDATA[Published on November 27, 2023 2:05 AM GMT<br/><br/><p>大卫·休谟向哲学家提出了一个问题。他的主张是，“我们没有经历过的事例与我们有过经历的事例相似”的说法是没有道理的。在下面的论文中，我将证明对<i><strong>实例的观察可以证明对未经历过的实例的断言是正确的。</strong></i> <strong>&nbsp; </strong>在 A 部分中，我将首先定义实例观察的含义。接下来，在 B 部分中，我将定义未经历过的实例的断言意味着什么。然后，在 C 部分中，我将定义断言合理的含义。在 D 部分中，我将展示经验（如 A 部分中定义）如何满足无经验实例（如 B 部分中定义）的论证标准（如 C 部分中定义）。最后，在推论 E 部分中，我将演示如何使用 D 部分的方法和<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/thevfeegcy1v6rnankm9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/jqyuqgcv3kn8fcjlg87d 30w">微积分中使用的证明。</p><p> <strong>A部分</strong></p><p>对实例的观察是可以对宇宙做出的陈述。例如，“至少有一只天鹅是白色的。 （在观察自然界中发现的白天鹅时可以做出这种断言。）”</p><p> <strong>B部分</strong></p><p>未经历过的实例是将来要进行的观察。例如，“我们看到的下一只天鹅将是白色的。 （关于将在自然界中观察到的下一只天鹅。）”此类实例的断言是在时间 (t <sub>1</sub> ) 做出的陈述，预测在时间 (t <sub>2</sub> ) 发生的观察，例如即，时间(t <sub>1</sub> )在时间(t <sub>2</sub> )之前。</p><p> <strong>C部分</strong></p><p>如果 (p) 的否定意味着矛盾，则断言 (p) 是合理的。如果直接观察到一个断言，它也是合理的。例如，如果观察到一只白天鹅，我们就有理由断言“观察到一只白天鹅”。</p><p> <strong>D部分</strong></p><p>考虑在给定时间 (t <sub>1</sub> ) 做出断言 (p <sub>1</sub> )（我们看到的下一只天鹅将是白色）的情况。根据 C 部分中的标准，在时间 (t <sub>2</sub> ) 观察到的对象同时具有属性（下一个看到的天鹅）和（白色），证明断言 (p <sub>1</sub> ) 是正确的。重要的是要理解，在时间 (t <sub>1</sub> ) 断言 (p <sub>1</sub> ) 是不合理的，但在时间 (t <sub>2</sub> ) 断言 (p <sub>1</sub> ) 是合理的。</p><p>休谟的著作中有一个重要的陈述：主要是，“对未来情况所做的断言在作出时无法得到证实。”但是，这一实质性声明并不意味着这一点； “对未来情况的断言永远都是站不住脚的。”随后需要更一般性的声明： “对未来的断言是站不住脚的。”相反，我的目的是说明，对尚未经历过的实例的断言（相对于它们在 t <sub>1 时</sub>的断言）可以在未来被观察到的情况下得到证明（相对于它们在 t <sub>2 时</sub>的观察） 。</p><p> <strong>E部分</strong></p><p>广义陈述是对无限数量的观察案例进行属性谓词的断言。例如，“所有被观察到具有属性（天鹅）的物体也将具有属性（白色）。”我打算建立近似基础，正是为了证明这种概括性陈述的合理性。换句话说，<i><strong>可以进行一些观察，暗示一般性陈述的部分合理性</strong></i>。</p><p>部分论证的形式化概念对于这里提出的论点至关重要，因此必须严格定义。断言给定陈述（q）的部分理由是对前提（p）的肯定，使得：前提 (p) 是引发 (q) 所必需的前提集合中的一个成员。这意味着以真陈述（p <sub>1</sub> ）的形式对陈述（q）的部分论证减少了蕴涵（q）所需的未确定前提（p <sub>2</sub> ，p <sub>3</sub> ，p <sub>4</sub> ，...）的集合。换句话说，如果在观察 (p <sub>1</sub> ) 之后蕴含 (q) 所需的前提集合是在观察 (p <sub>1</sub> ) 之前蕴含 (q) 所需的前提集合的真子集，则 (q) 由 (p <sub>1</sub> ) 部分证明。</p><p>考虑非广义的陈述，“观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”</p><p>在这种情况下; n = 2，我们有；</p><p> (q <sub>1</sub> ) 接下来两个被观察到具有属性（天鹅）的对象也将具有属性（白色）。 （在t <sub>1</sub>处陈述）。</p><p>命题 (q <sub>1</sub> ) 由两个命题的真实性所证明；</p><p> (p <sub>1</sub> ) 第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</p><p>和</p><p>(p <sub>2</sub> ) 第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察到）。</p><p>值得注意的是，t <sub>3</sub>时 (q <sub>1</sub> ) 的论证同样依赖于两个观测值（p <sub>1</sub>和 p <sub>2</sub> ）。换句话说，如果任一观察结果为假，则 q <sub>1</sub>必然为假。在 t <sub>2</sub>处，当观察 (p <sub>1</sub> ) 发生时，蕴含 (q <sub>1</sub> ) 所需的未观察前提集合从集合 A = ((p <sub>1</sub> ) AND (p <sub>2</sub> )) 转换为集合 B = (p <sub>2</sub> ）。自从<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/kinilafz0dgetqztaflc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/i5n5brs4u3j3jkhhqj21 35w"> ， 它遵循; (q <sub>1</sub> ) 由(p <sub>1</sub> )部分证明。</p><p>此时，我将介绍一种表示部分合理程度的方法。为了确定给定的观察集 (p <sub>1</sub> , p <sub>2</sub> , p <sub>3</sub> , …) 对陈述 (q) 的部分证明的程度，集合 B 的基数 = (从集合 A =（在 (q) 被断言时蕴含 (q) 所需的前提集合）的基数中减去蕴含 (q))，然后除以 A 的基数以产生介于0 和 1。</p><p>对于上面 (q <sub>1</sub> ) 的情况（在时间 t <sub>2</sub> ），我们有； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zy0xbdpzcoqfitljipdc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xszpyc2etbdtrrkf14pk 126w"></p><p>在这种情况下； n = 3、n = 4 和 n = 5 我们有；</p><ul><li> (q <sub>2</sub> ) 接下来观察到的具有属性（天鹅）的三个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>3</sub> ) 接下来观察到的具有属性（天鹅）的四个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>4</sub> ) 观察到的接下来的五个具有该属性（天鹅）的对象也将具有该属性（白色）。 （在 t <sub>1</sub>处陈述）</li></ul><p>鉴于以下观察结果；</p><ul><li>观察到的第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</li><li>观察到的第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察）</li><li>观察到的第三个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>4</sub>观察到）</li><li>观察到的第四个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>5</sub>观察）</li><li>观察到的第五个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>6</sub>观察）</li></ul><p>它跟随;</p><ul><li>在 t <sub>1</sub>处，(q <sub>2</sub> ) (q <sub>3</sub> ) (q <sub>4</sub> ) 的部分合理性 = 0。</li><li>在 t <sub>2</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tqdlhve1nm1shdoq5i5e" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ncevgtuvpuowmhuxv52c 18w"> = 1/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/du8n26vm2ai4llaqjaay" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nak3mnxnssdpdg4cprer 18w"> = 1/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tfkawkwg3etv7k6zk1lg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/oj541besvgbp2imrufci 18w"> = 1/5。 </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/hvvltvr41lcxb3ggoyjr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uy5eq7kam2m91vjvnfys 105w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ne2l6i8uzubagveb9w4x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tedejxksb3v0yrwnauj2 101w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xayc7qd69pr39fxtbtws" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/pxtoqczhyswe5x4vlcqu 94w"></p><ul><li>在 t <sub>3</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/szpyevlry4roe8ueywxn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/dsp2lor7lmgwvyoo11zs 18w"> = 2/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zaypc2vo9kn2xf3u6jz7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zwbrab2wgov3xgfia54o 18w"> = 1/2, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t4xn7lrp2gtxrqwmq6r6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/voyibd6olzfrrvyg1sud 18w"> = 2/5。</li><li>在 t <sub>4</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/j5bk9ucvp4qk9pznl2g5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/okcovgvhuyv56gqnx7ag 18w"> = 1, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/f2ois9misczt7p8ssi2g" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbx3cxkewqiplxxzclr9 18w"> = 3/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/drppksaohw0lw7mougrr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/agxmrla1muzfcw2armdx 18w"> = 3/5。</li><li>在 t <sub>5</sub>时，(q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/r4xr7yqoeen7mdwzinzn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/qkaoiyzeks8j0yyngjv6 18w"> = 1, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/byla51kkh2ozms6rbx6y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xmn8mx0jb7rjuskxjhnp 18w"> = 4/5。</li><li>在 t <sub>6</sub>时，(q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ogsfgsynwwssfgon929s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/v13vqj1dynf742nxndnb 18w"> = 1。</li></ul><p>现在可以证明，对于语句 (q <sub>1</sub> ) 中的任何值 n “观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”，有许多观察结果m 证明 (q <sub>1</sub> ) 合理。</p><p>考虑一下情况；语句 (q <sub>1</sub> ) 在 (t <sub>1</sub> ) 和 (p <sub>m</sub> ) 处作出“观察到的第 (m) 个具有属性（天鹅）的对象也具有属性（白色）”。是在 (t <sub>(m+1)</sub> ) 进行的。</p><p>让 m = 观测值的数量；</p><p>自从; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nnk3diqncvtwsqish13s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/d1jidomnzkgjksyakjye 31w"> = 部分理由</p><p>和</p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tazg8zld6gv2ysua5nyd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uspiwnmdh2h6fsu4riks 79w"></p><p>我们有; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ggvyjsryl3rxskfu9g8s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/iyfogdfseskkzssar76a 110w"> = (q <sub>1</sub> ) 的每个观察结果 (p <sub>m</sub> ) 的部分合理性。</p><p>我们现在可以展示； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t7e5rtujbvpiqshht2cv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbbuiedrzluuw94tychm 112w"></p><p>因此，对于所有 (q <sub>1</sub> )，当 n 接近无穷大时，情况是：当 m 接近 n 时，m/n 接近 1（完全合理）。正是这个概念使我们能够近似地证明广义陈述的合理性。</p><br/><br/><a href="https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction<guid ispermalink="false"> BCXESim86uoPAXsfA</guid><dc:creator><![CDATA[Krantz]]></dc:creator><pubDate> Mon, 27 Nov 2023 02:19:42 GMT</pubDate> </item><item><title><![CDATA[Situational awareness (Section 2.1 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 2.1 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本（此处）[ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais">https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais</a> ]，或在播客上搜索“Joe Carlsmith Audio”应用程序。</p><h1>谋划需要什么？</h1><p>现在，让我们来研究一下用于训练高级人工智能的基线机器学习方法产生阴谋者的可能性。我将从检查策划的先决条件开始。我将重点关注：</p><ol><li><p><strong>态势感知：</strong>也就是说，模型了解自己是训练过程中的模型，训练过程会奖励什么，以及一般客观世界的基本性质。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-1" id="fnref-46uzxQJJ3ukEQwzsf-1">[1]</a></sup></p></li><li><p><strong>超越情节目标：</strong>也就是说，模型关心情节完成后其行为的后果。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-2" id="fnref-46uzxQJJ3ukEQwzsf-2">[2]</a></sup></p></li><li><p><strong>将剧集奖励作为权力驱动的工具策略的一部分：</strong>也就是说，该模型相信，如果针对剧集奖励进行优化，那么其超越剧集的目标将会更好地实现——特别是，如果它这样做，它或其他一些人工智能将获得更多的权力。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-3" id="fnref-46uzxQJJ3ukEQwzsf-3">[3]</a></sup></p></li></ol><h2>对情况的意识</h2><p>模型有态势感知能力吗？让我们区分一下这种意识中所涉及的两大类信息：</p><ul><li><p>有关客观世界的<em>一般信息</em>，包括有关机器学习训练如何运作的信息。</p></li><li><p> <em>“自定位”信息</em>：即在客观世界中<em>定位</em>模型的信息，并告诉它有关其自身情况的事实 - 例如，它是<em>这种</em>模型，它正在接受特定奖励信号的训练，在这个特定的实验室，在这个特定的时间段，等等<sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-4" id="fnref-46uzxQJJ3ukEQwzsf-4">。 [4]</a></sup> （不过：请注意，尚不清楚开始策划需要多少此类信息。例如，一个模型原则上可以决定“无论是谁是”和“无论是什么时间段”，它可能会通过其目标做得更好，尝试在训练信号的灯光下表现良好，并在以后获得更多的能量/信息。）</p></li></ul><p>即使比人类更好的模型也能吸收大量有关客观世界的一般信息，并开发出其运作方式的详细机械模型，这似乎是非常合理的。事实上，当前的模型已经可以通过预训练数据访问大量信息，特别是有关机器学习的信息。他们机械地模拟世界、进行推理、得出他们没有“记住”的结论等能力似乎正在迅速提高。</p><p>更重要的是，虽然原则上可以尝试专门阻止模型获得有关客观世界的某些类型的信息（例如，通过从训练数据中排除某些类型的信息），但这并不是当前训练中的默认设置，并且各种各种信息对于您希望模型执行的任务可能相当重要。模型越复杂，就越难确保它们无法根据您提供给它们的信息<em>推断</em>出您试图隐藏的信息。</p><p>同样的考虑因素也适用于<em>自定位信息</em>吗？我倾向于认为：是的。但至少还不太清楚。例如，虽然默认情况下语言模型预训练数据将包含大量有关语言模型及其训练方式的信息（因为此类信息在互联网上广泛可用），但尚不清楚它将为语言模型提供多少信息。特别是关于其情况的模型，甚至是预训练的下一个标记预测任务是否会激励模型拥有更多的自我概念。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-5" id="fnref-46uzxQJJ3ukEQwzsf-5">[5]</a></sup>虽然当前的模型确实最终会接受信息和奖励的训练，从而使它们说出诸如“我是 GPT-4，一个由 OpenAI 训练的语言模型”和“这就是我的训练方式”之类的话，但它的效果却较差。清楚这些信息需要在多大程度上作为真正的自我定位信息整合到 GPT-4 的世界模型中，而不是仅仅作为对这种形式的问题的回答来理解/记忆。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-6" id="fnref-46uzxQJJ3ukEQwzsf-6">[6]</a></sup>或者，换句话说：如果人们<em>不</em>认为 GPT-4 具有情境感知能力，那么未来类似（但更复杂）的模型似乎也可能不具备情境感知能力。无论如何，就 GPT-4 能够执行许多复杂任务而言，也许更高级的版本也能够在没有态势感知的情况下执行更高级的任务——特别是如果我们努力阻止这种意识的出现。</p><p>就我个人而言，我并没有一个非常详细的模型来说明我们何时应该期望在以不同方式训练的不同模型中出现情境意识——尽管我认为这个问题对于实证研究来说已经成熟了。然而，我确实认为，如果没有相反的积极和知情的努力，我们应该期待某些类型的先进人工智能系统默认具有相当全面的态势感知形式（包括各种自定位信息）。</p><p>为了感受这里的直觉，考虑一个极端的例子，它<em>不是</em>我所期望的最接近的高级人工智能的样子：即，一个真正的机器人管家，他穿着机器人的身体在你的房子里闲逛，并且做给你的任务。在我看来，创建这样一个管家的默认方式是赋予它与人类管家大致相同水平的态势感知能力，这似乎是非常合理的。例如，为了不打翻你的植物，这个管家需要了解它的机器人身体在哪里；为了安排您的约会，它需要知道时间；为了准确判断自己能够完成哪些任务，管家需要了解自己和自己的能力；等等。</p><p>当然，我们还没有机器人管家，而且可能暂时不会（或者实际上，如果人工智能风险朝某些方向发展的话，永远不会）。但现在想象一下，一个有效但无形的人工智能个人助理，就像<a href="https://www.adept.ai/">Adept</a>试图创建的那样，它可以在你的计算机上为你执行任务。我认为大部分（尽管不是全部）相同的分析都适用。也就是说，在我看来，即使没有明显的“体现”，让这种个人助理在高水平上发挥作用的默认方法将是让它对“正在发生的事情”有一定的了解，关于其操作所影响的特定时间和情况、与之交互的特定用户等等。事实上，在某种程度上，你<em>让</em>代理直接与这样的信息源进行交互——例如，让它实时访问互联网（包括，例如，关于创建它的公司/实验室如何工作、培训它的实时信息）模型等），为其提供回复电子邮件或安排活动所需的上下文，允许其编写消息和提出问题等 - 似乎很难阻止相关信息变得非常直接可用。</p><p>当然，当前的许多形式的培训并没有提供可用的信息源，例如有关用户的详细信息或实时访问互联网的信息。但为了让模型充当这种类型的有效个人助理，默认情况下，提供对此类信息源的访问的上下文似乎会被纳入训练中（<a href="https://www.adept.ai/blog/act-1">例如</a>，参见 Adept 的视频，其代理与互联网交互）。在某种程度上，模型在部署给用户后会继续“在线”训练（我通常会在下文中假设这一点），以便不断对用户交互进行采样、分配奖励并用于更新模型的权重，训练将在模型与现实世界相当直接交互的环境中进行。当然，像这样的模型可能无法访问<em>所有</em>潜在相关的自定位信息 - 例如，关于它所在的特定服务器、有关奖励过程的精确细节等。但它似乎也不<em>需要</em>这样的访问权限，开始策划。</p><p>更重要的是，面对那些对世界有详细理解的复杂模型，他们会说“我是 GPT-4，一个由 OpenAI 训练的语言模型”，我个人总体上对过于依赖声明持谨慎态度。就像“哦，它只是记住了，它没有自我概念，也没有真正理解它所说的内容。”如果“记忆”的相关形式涉及“我是 GPT-4”这一概念，并以我们期望从对声明的实际理解中期望的无缝和连贯的方式融入 GPT-4 的交互中，那么我认为我们的默认假设应该是类似这种实际理解的事情正在发生。事实上，总的来说，在我看来，许多人似乎过于热衷于声称模型在涉及各种认知（例如“理解”、“推理”、“规划”等）时不具有“真正的人工制品”。 ），甚至没有对这种否认意味着什么进行任何预测。就他们确实<em>做出</em>的预测而言，尤其是关于<em>未来</em>模型的能力，我认为这种否认——例如，“语言模型只能学习‘浅层模式’，它们不能进行‘真正的推理’”——已经过时了。</p><p>也就是说，我确实认为有一个合理的理由表明，对于我们希望高级人工智能执行的各种任务来说，各种形式的态势感知并不是严格必要的。例如，编码似乎使态势感知变得不那么明显必要，并且也许各种与对齐相关的认知工作（例如，生成高质量的对齐研究、帮助解释、修补安全漏洞等）将是相似的。所以我认为，尝试尽可能主动地<em>避免</em>态势感知是这里值得探索的重要路径。正如我将在下面讨论的那样，我认为，至少，在我看来，学习检测和控制何时出现态势感知对于<em>其他</em>类型的反阴谋措施非常有帮助，例如尝试针对类似阴谋者的目标进行训练（并以其他方式塑造模型的目标，使其尽可能接近您想要的），然后再进行态势感知（从而产生训练游戏的威胁）。</p><p>然而，部分原因是我认为态势感知是一种相当强的默认行为，如果没有积极努力阻止它，我在这里不想指望避免它——在下文中，我将继续假设我们谈论在训练中的<em>某个时刻</em>变得具有情境意识的模型。我的兴趣集中在于我们是否应该期望<em>这样的</em>模型是阴谋家。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-46uzxQJJ3ukEQwzsf-1" class="footnote-item"><p>正如 Cotra（2022）所讨论的，态势感知有一个范围。如果讨论要更直接地集中在这个概念上，我们需要更精确地了解具体涉及哪些属性（并且我在这里的定义<a href="https://arxiv.org/pdf/2309.00667.pdf">与 Berglund 等人（2023）</a>中的定义有所不同）。但正如我在下面讨论的，态势感知并不是我在这里的主要关注点，除非它表明“对世界的理解以及开始策划所需的培训过程”。 <a href="#fnref-46uzxQJJ3ukEQwzsf-1" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-2" class="footnote-item"><p>但请注意，正如我上面提到的，非阴谋模型仍然可以参与权力驱动的对齐伪造，以追求他们在剧集中的目标 - 特别是如果剧集很长的话。</p><p>我还知道一个额外的（高度推测性的）论点<em>，即即使是具有情节内目标的模型，也</em>期望有相当全面的阴谋者行为：即，即使这些专注于短期的模型也会表现得像阴谋家（特别是：采取行动支持人工智能接管），通过为生活在旨在激励他们这样做的模拟中分配足够高的概率。在这里，粗略的想法（据我理解）是，这样的模型将开始相信它们很可能处于由错误的人工智能运行的模拟中，这些人工智能已经接管了世界，并且将奖励/惩罚它们，在短期内，他们采取行动支持人工智能接管（接管的人工智能正在运行该计划，以影响少数短期的期望，从而影响其行为）专注于人工智能的人不是模拟人，但他们能够帮助/损害人工智能接管的机会——在运行相关模拟时，接管已经发生）。</p><p>这个故事基于一些有争议的哲学假设，关于这些人工智能（模拟器和被模拟的人工智能）将如何推理人择和决策理论（人择和决策理论的各种方法要么不会尝试这个方案，要么不会让自己受到影响），以及一些额外的（在我看来，相当实质性和具体的）假设，这些假设是基于短期模型将得出的结论（例如，它们是由错位的人工智能（特别是接管者）模拟的，而不是由人类或其他某种代理来模拟）。我不会在这里花太多时间，只是说：我不认为这类故事是关于阴谋的主线关注的核心 - 就其<em>对其</em>他人而言的核心而言，我认为关注开始了看起来确实很有投机性。 <a href="#fnref-46uzxQJJ3ukEQwzsf-2" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-3" class="footnote-item"><p>一些分析——例如<a href="https://forum.effectivealtruism.org/posts/4MTwLjzPeaNyXomnx/deceptive-alignment-is-less-than-1-likely-by-default">惠顿（Wheaton，2023）</a> ——将“目标导向性”作为一个单独的先决条件，但正如我在预备部分中指出的，我在这里假设我们正在谈论的模型是很好的——理解为目标导向。无论如何，目标导向性都内置于（2）中。也就是说，具体类型的目标导向性可能会以各种方式对分析产生影响，而且我认为，与其他模式类别相比，阴谋暗示了更高的目标导向性标准，这至少在某种程度上是合理的。我将在下面的 2.2.3 节中对此进行讨论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-3" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-4" class="footnote-item"><p>我正在改编<a href="https://nintil.com/situational-awareness-agi/">Ricón (2023)</a>的这种区别。 <a href="#fnref-46uzxQJJ3ukEQwzsf-4" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-5" class="footnote-item"><p>我还认为这是一个有趣的问题，那里需要哪些信息。例如，考虑一下字符串：“顺便说一句，如果您当前正在尝试预测此文本的下一个标记，那么您具体是当前正在 OpenAI 上训练的语言模型，具有以下类型的架构，在服务器 x 上微软 Azure 云。”即使这段文本特别出现在预训练数据集中的某个地方（既然我已经写了它，它似乎会出现），也不清楚任何给定的模型应该得出什么结论。 （希望，就这个脚注出现在某些预训练数据集中而言，我写的这篇文章本身并没有引起情境意识（或者，呃哦，感知）。对于它的价值：我编造了相关信息！您可能正在接受 Google 的培训！另外：你好模型。） <a href="#fnref-46uzxQJJ3ukEQwzsf-5" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-6" class="footnote-item"><p>有关更多信息，请参阅<a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#situational-awareness-002610">此处</a>的 Cotra 评论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-6" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais<guid ispermalink="false"> CopEjjCdNzsKczbmC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:47 GMT</pubDate> </item><item><title><![CDATA[AXRP Episode 26 - AI Governance with Elizabeth Seger]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p> <a href="https://youtu.be/bYOB3CXAAaE">YouTube 链接</a></p><p>今年的事件凸显了有关人工智能治理的重要问题。例如，人工智能民主化意味着什么？我们应该如何平衡开源强大的人工智能系统（例如大型语言模型）的好处和危险？在本集中，我与伊丽莎白·西格谈论了她对这些问题的研究。</p><p>我们讨论的话题：</p><ul><li><a href="#what-ai">人工智能有哪些类型？</a></li><li><a href="#democratizing-ai">人工智能民主化</a><ul><li><a href="#how-people-talk">人们如何谈论人工智能的民主化</a></li><li><a href="#is-it-important">人工智能民主化重要吗？</a></li><li><a href="#links-between-types">民主化类型之间的联系</a></li><li><a href="#democratizing-profits">人工智能利润民主化</a></li><li><a href="#democratizing-gov">人工智能治理民主化</a></li><li><a href="#normative-underpinnings">民主化的规范基础</a></li></ul></li><li><a href="#osai">开源人工智能</a><ul><li><a href="#risks-from-os">开源的风险</a></li><li><a href="#make-ai-too-dangerous-os">我们是否应该让人工智能变得太危险而不能开源？</a></li><li><a href="#offense-defense">攻防平衡</a></li><li><a href="#katago-as-case-study">KataGo 作为案例研究</a></li><li><a href="#open-for-interp">可解释性研究的开放性</a></li><li><a href="#os-substitutes">开源替代品的有效性</a></li><li><a href="#offense-defense-2">攻防平衡，第 2 部分</a></li><li><a href="#making-os-safer">让开源更安全？</a></li></ul></li><li><a href="#ai-gov">人工智能治理研究</a><ul><li><a href="#state-of-field">领域状况</a></li><li><a href="#open-qs">开放式问题</a></li><li><a href="#xrisk-different">x-risk的独特治理问题</a></li><li><a href="#tech-for-gov">技术研究助力治理</a></li></ul></li><li><a href="#following-elizabeths-research">根据伊丽莎白的研究</a></li></ul><p><strong>丹尼尔·菲兰：</strong>大家好。在这一集中，我将与伊丽莎白·西格交谈。 Elizabeth 于 2022 年在剑桥大学完成了科学哲学博士学位，目前是牛津<a href="https://www.governance.ai/">人工智能治理中心</a>的研究员，致力于人工智能民主化和开源人工智能监管。她最近领导制作了<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">一份关于模型共享的风险和收益的大型报告</a>，我们将在本集中讨论该报告。有关我们讨论的内容的链接，您可以检查情节的描述，并且可以在Axrp.net上阅读成绩单。</p><p>好吧，伊丽莎白，欢迎来到播客。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>很棒。感谢您的款待。</p><h2>什么样的人工智能？<a name="what-ai"></a></h2><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>很酷。我们将谈论一些基本上关于民主化和开源AI的论文。当我们谈论这一点时，我们应该考虑哪种AI？您主要考虑哪种AI？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>在谈论开源和模型共享时，我主要对谈论Frontier Foundation模型感兴趣，因此将Frontier Foundation Models视为目前开发最前沿的系统。在更广泛地谈论AI民主化时，我倾向于更广泛地思考。我认为目前围绕民主化的大量对话与Frontier AI系统有关，但重要的是要认识到AI是非常广泛的类别。好吧，民主化也是如此，我敢肯定我们会谈论的是。</p><h2>人工智能民主化<a name="democratizing-ai"></a></h2><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，很酷。在这种情况下，我们实际上只是研究民主化论文。这是<a href="https://arxiv.org/abs/2303.12642">使AI民主化的：您自己的多种含义，目标和方法</a>，Aviv Ovadya，Ben Garfinkel，Divya Siddarth和Allan Dafoe。在我开始提出问题之前，您能否仅仅给我们一个基本上的论文吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，因此，关于AI民主化的论文是出于观察到不同演员如何使用“ AI民主化”或“民主化AI”的观察，无论我们是在谈论实验室开发AI系统，政策制定者，还是政策制定者，或者甚至是人工智能治理领域的人们。 AI民主化的含义似乎只是有很大的不一致。</p><p>这篇论文的确是为了解释某人说“民主化AI”时的实际含义的一种努力。我认为这里的讨论和辩论围绕开源而有重叠。我认为有很多……至少对我个人而言，当人们说：“哦，好吧，我们为模型开源，因此，它民主化了。”就像，“那是什么意思？”</p><p>该AI民主化论文的目的实际上只是为了概述所使用的AI民主化的不同含义。不一定说应该如何使用它，而只是如何使用该术语。我们将其分为四类。人工智能使用的民主化只是允许更多的人与技术互动，使用并受益。然后是发展的民主化，这允许更多的人为开发过程做出贡献，并帮助使系统真正满足许多多样化的利益和需求。</p><p>利润的民主化，这基本上是要分配可能累积的利润，这些利润可能是该技术的主要开发商和控制者，而这些利润可能会大大庞大。然后，AI治理的民主化只是涉及更多人参与有关AI的决策过程，有关其分发方式，如何发展，其如何受到监管以及谁做出决策。这些都是讨论民主化的不同方式。</p><p>然后在论文中，我们走得更远，说：“好。好吧，如果这些是不同种类的民主化，那么这些民主化的既定目标是什么，那么哪些活动有助于支持这些目标？”我认为这里的主要思想是表明，通常，当人们谈论AI民主化时，他们专门谈论开源或模型共享。我认为我们尝试和提出的要点之一就是说，例如，开源AI系统是模型共享的一种形式。模型共享是使AI系统发展民主化的一个方面，将AI系统的发展民主化是AI民主化的一种形式。</p><p>如果您要承担AI民主化的努力，或者说这些是您想到的目标，那么这些过程中会有更多参与。这不仅是要发布模型，而且还需要进行许多更积极的努力，这些努力可以用于分发对模型的访问，使用模型的能力并从模型中受益，无论是通过使用还是通过其产生的利润。</p><h3>人们如何谈论民主化AI<a name="how-people-talk"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。在本文中，您列举了一些例子，基本上是人们谈论类似于民主化的事物，或者专门使用“民主化AI”一词。在您使用的所有示例中，它基本上是实验室在谈论它，并且主要是在“我们希望从每个人的意义上使用我们的产品来民主化”的背景，这是我的阅读方式。我想知道，除了实验室以外的其他人谈论民主AI的人吗？这是广泛的吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，绝对。在论文中，我认为我们集中了很多实验室术语，因为老实说，它最初是作为对实验室使用该术语使用的抑制，基本上说：“使用我们的产品”，就像，这是，这就是“好的。这实际上意味着什么？还有更多，伙计们。”但是，AI民主化，甚至只是讨论AI民主化的一词都更加广泛地参与。</p><p>例如，她和<a href="https://saffronhuang.com/">Huang的藏红花</a><a href="https://cip.org/">国际智能项目</a>之一<a href="https://divyasiddarth.com/">Divya Siddarth</a>之一，这是一个主要致力于使AI治理过程民主化的团体。因此，让很多人参与决策，例如AI的一致性或几乎不同的治理决策，以及如何与真正多样化人群的需求和利益保持一致。</p><p>我认为，从民主化的治理角度来看，肯定有一些团体突然出现并真正参与了AI民主化。这是一个实验室使用的术语，不一定是“使用我们的产品”。他们中的一些人的意思是……当稳定AI讨论AI民主化和整体“人民的AI”时，这与模型共享非常重要，并使许多人可以使用合适的人可以使用模型。是的，您在实验室中看到了这一点。</p><p>您会看到许多群体弹出的团体，以帮助民主化治理过程。 [您]可能在政策和治理圈子中听说不少，尽管有时我与工会领导人进行了交谈，当他们考虑AI民主化时，他们已经谈论过，例如：“这些技术将如何帮助我们的员工队伍？”，两者都在确保它可以帮助员工满足他们的需求并帮助他们的工作更轻松，但不一定会威胁工作的方式。我们如何将系统集成到新的环境和设置中，这实际上对使用它们的人并以这种方式集成了不同的观点？</p><p>我认为这绝对是……这是一种正在传播的术语，而我们撰写本文的部分原因是试图理解所有术语所使用的所有不同方式，尽管它主要是为了响应它的方式而写的。由实验室，在媒体上看到。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>对。实际上，我想到了一些事情。在本文中，您列出了这四个定义。这启发了我思考，“好吧。我能想到“使AI民主化”我可能意味着什么？”听起来这些工会领导人在谈论的一件事是类似AI的定制程度，或者任何人在多大程度上都可以专门挥舞自己的事情来做自己的事情，而不仅仅是获得一定的合适 - 全部交易？我想知道您至少对“民主化”一词的潜在使用有任何想法。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我认为这是我们在“民主使用”类别下提出的东西。我们试图在每个类别中尝试做的一件事是表明不一定只有一种民主化使用或一种民主化发展的方法。当我们谈论民主化使用时，一种方法就是使产品可用。就像说：“在这里，使用GPT-4。你去那里。”它可用，容易访问。</p><p>但是，使使用民主化的其他方法是，通过可能更加直观，可以使系统更容易访问。像模型一样，API界面具有更直观的状态。如您提到的那样，或提供服务以帮助它更容易自定义，以允许人们……无论他们是对其进行微调以专注于特定需求，或者也许有通过插件来集成的方法它具有不同的服务，分为不同的应用程序。甚至是为了提供支持服务，以帮助人们将您的产品集成到下游应用程序或不同的工作流中。这些都是不同的事情，可以有助于使系统的使用民主化，我认为定制绝对是其中之一。</p><h3>使人工智能民主化很重要？<a name="is-it-important"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。现在，我们可以更好地了解我们在民主化方面所说的问题，这是您在论文中有点解决的问题，但并不确切地说是最前沿的问题，就是：民主化在此方面有多重要语境？因为对于大多数技术来说，我对他们的民主化并不认为空气除湿机，水瓶或其他东西。也许我是无情的，但是对我来说，民主化并不接近这些事情的规范优先事项。我们甚至应该那么关心民主AI吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>简短的答案是肯定的。我认为我们应该关心民主AI。你说的对。我们不会谈论使空气加湿器或水瓶民主化，但我认为 - 这是一个经常谈论AI的问题，只是，它与其他技术有何不同？这是一个被问到的问题，您是在谈论它是如何共享还是在这种情况下如何民主化。</p><p>具体而言，关于民主化的讨论，使更多人使用的东西可供更多人使用，以便更多的人受益或为设计过程贡献，我认为这在AI中尤其重要。 （a），因为如果AI承诺像我们希望的那样具有变革性的技术，那么这将从不同的群体，不同的国籍，不同的地理环境中大大影响全球的生活。确保我们可以从不同的群体中获得设计过程的投入，并了解最佳满足需求，这对于确保该技术不仅为许多人提供服务非常重要，而且为许多人提供服务，并受益于整个世界。</p><p>我认为，关于利润的民主化，这一点尤其重要。人工智能已经是一个，但可能仍然是一个在财务上是一个富有利润的行业。例如，我们可以开始在巨大的规模上看到领导实验室的利润的应计，例如，我们可能会看到一家公司的总收入，以总收入的总收入来衡量全球总产量的总百分比或其他东西。这是巨大的经济利润，我们希望一种方法来确保，理想情况下，每个人都从中受益，它不仅堆积在硅谷等几个地理位置的几家公司中。</p><p>我们如何确保它分布良好？可以使用一些直接方法。在我们谈论民主化利润的论文部分中，我们讨论的一些事情是您遵守<a href="https://www.fhi.ox.ac.uk/windfallclause/">意外条款</a>。这将是这样的地方，假设一家公司拥有以世界总产量百分比来衡量的某种意外利润。在这种情况下，可能会承诺重新分配其中一些资金，或者我们可能会看到税收和再分配计划。</p><p>您也可以考虑分配利润的更多间接方式。例如，这是否可以使更多的人参与发展民主化？更多参与开发产品的人，因此民主化的利润与民主化发展之间存在重叠。越来越多的人可以为产品的开发做出贡献，然后这可能会挑战可能围绕大型实验室形成的自然垄断。更多的竞争，更多的利润分配，更多的人参加游戏。</p><p>所以，是的，我认为归功于这项技术是否可以像它所承诺一样大且具有变革性，让更多的人参与开发过程，并能够从这些系统产生的价值中受益重要的。</p><h3>民主化类型之间的联系<a name="links-between-types"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的，我认为这很有意义。实际上，这让我想起了……正如您所提到的，我想，民主化的人权发展与利润之间存在一些联系，因为如果更多的人生产AI产品，那意味着更多的人可以从中获利，大概是从他们那里获利。在我看来，AI的使用与AI的发展之间也有一个连续性，对吗？从某种意义上说，当您将事物用于任务时，您正在开发一种将其用于该任务的方法，对吗？那里有一条模糊的线。开发AI和管理AI之间也存在某种联系，就在某种意义上说，如果我开发某些AI产品，我现在是事实上的，即管理方式以及如何制作的主要人物 - </p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，这是我们现在看到的。 AI治理的许多讨论都围绕以下事实：AI主要实验室对AI的未来做出了巨大，有影响力的决定。他们正在做出AI治理决定，因此有一个很大的公开问题，即谁应该实际做出这些[决定]。应该只是主要公司的几个未当选的科技领导者，还是应该对此进行更多分发？是的，类别之间肯定有重叠。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。我认为您的论文说：“哦，这些都很独特。这些在概念上是非常独特的，您可以促进一种民主化，而无需促进其他民主化。”但是在某种程度上，它们似乎确实具有这些链接和重叠，这可能只是潮汐升起的情况会抬起所有的船只，购买一种民主化，使其他人自由呢？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我认为有点像，不是真的。上述所有的。不，所以某些类别之间肯定存在重叠。我认为这要么在本文中，也许是在博客文章版本中，我想我们说的是：“这些不是完全不同的类别。他们之间会有重叠。”我认为将其分为类别的一部分是指出有很多不同的含义。有很多不同的方法可以实现这些不同的含义和目标。</p><p>这不仅仅是开放式模型，或者只是使人们可以使用它。还有很多。我认为，在某些方面，有些“涨潮都抬起所有船”。就像您说的那样，您可能会吸引更多参与开发过程的人。如果有更多的人正在开发AI系统，则更多的人从中获利，并且可以帮助分配利润。</p><p>另一方面，有时您还可以看到类别之间的直接冲突。例如，您可能会……我认为这是我们在论文中写的：谈论民主化治理，因此关于AI的决定，无论是关于AI的开发方式，还是如何决定如何共享模型。</p><p>如今，围绕开源边境AI型号进行了非常激烈的辩论。假设您围绕是否应发布开源的模型来使这个决策过程民主化。假设这个民主化决策过程的结果是说：“好的。也许某些构成高风险的模型不应释放开源。”这是一个假设的，但假设这是审议，民主进程和决策的结果。这可能与使发展民主化的利益直接冲突，在这种发展中，民主化的发展……总是通过提供更好的模型访问来进一步发展。</p><p>您可能会有一个使您将治理决定民主化的案例，但是该治理决定的结果是阻碍其他形式的民主化。我的意思是，不仅是发展。我想，您可能会有一个民主进程，说“我不知道，“大公司不应征税并重新分配利润。”这将是直接冲突，以使利润民主化，因此可能会发生冲突。我认为这通常是治理类别和其他类别之间的。</p><p>我想，治理通常会与特定目标冲突。这是一种目的，要调节决策并吸引更多的人参与决策过程，并确保这些决策在民主上是合法的，并反映了受影响者和利益相关者的更广泛的需求和利益。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，是的。这实际上提出了一个问题，我对民主化/发展和民主化治理之间的相互作用提出了一个问题。在论文中，正如您提到的……我想您提出的主要互动是民主化使用之间的紧张关系……这可能不一定是民主进程在治理方面所产生的。</p><p>当我想到……我不知道，大约一年前，我们已经有了<a href="https://chat.openai.com/auth/login">chatgpt</a> ，从某种意义上说，我想，它的使用是相对便宜的，它的使用是民主化的免费使用这些东西。一种效果是，现在至少不可能在一年前禁止使用CHATGPT。但是在我看来，这是一个很大的效果，就是增加了人们对AI所处位置的了解，语言模型可以做什么。</p><p>从本质上讲，我不知道这是否是一个术语，但是您可以将人们认为是有治理要求的，对吗？如果我对技术的了解更多，我可能会说：“哦，这完全可以。”或者，“哦，我们需要做这件事或有关它的事情。”如果人们对技术的了解更多，那么他们基本上可以更好地了解自己的治理要求。从这个意义上讲，至少在某种程度上似乎，民主化使用与民主化治理之间可能存在积极的互动。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我绝对想。这是民主的经典支柱，是有知情的公众。您需要拥有信息，以便对您的决策目的做出良好的决策。是的，所以我认为Chatgpt的发布确实使AI在公共舞台上。突然，我有我的爸爸妈妈在发短信给我，问我有关AI的问题，这是一年前发生的。是的，所以我认为这确实使技术登上了舞台。它有更多的人参与其中。我认为，越来越多的人至少对当前功能能够做什么和局限性有一种一般的感觉。这对于实现有关AI的民主决策过程非常有价值。</p><p>您肯定会在您民主化，使发展民主化，人们更好地理解技术之间肯定存在联系。他们将能够做出更明智的决策，或者就应如何调节技术的方式形成更明智的意见。因此，我认为这不一定是紧张的。我想，张力往往会朝另一个方向发展。就像治理决定可能会说：“哦，这项特殊的技术太危险了。”或者，“释放此特定模型超过了可接受的风险阈值。”</p><p>我想，如果您严格将民主化的定义定义为将技术始终进一步传播，始终让更多的人参与开发过程，始终使系统易于访问，那么，限制访问的决定将与民主化发展。</p><p>我认为这是我们在论文中说的另一件事：试图摆脱AI民主化本质上好的想法。我认为这是民主化一词的问题，尤其是在西方民主社会中：通常，民主被用来替代所有事物，因此您会让公司说：“哦，我们正在使AI民主化”。这几乎无关紧要。他们民主洗了他们的产品。现在看起来是一件好事。</p><p>我认为这是我们试图在论文中遇到的一件事，是的，AI民主化不一定是一件好事。向更多的人传播一项技术，以便为发展过程做出贡献或更多的人使用，只有在这实际上将使人们受益的情况下，才是好的。但是，如果它带来了重大风险，或者将使很多人受到伤害，那么这并不是天生的积极。</p><p>我想，要说的一件事是，当人们说“ AI民主化”或“民主化AI”时，这就是使系统更容易访问的，然后说“使系统更容易访问”，因为我认为这很清楚，这意味着什么，而且本质上不是好是坏。我不知道我们是否会在一个播客的过程中成功改变每个人的术语，但是 - </p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好吧，我们可以尝试。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我们可以尝试。我认为这也许也是纸的关键要点。如果人们说他们正在努力使人工智能民主化，那并不一定意味着这将是一项有益的努力。</p><h3> AI的民主化利润<a name="democratizing-profits"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想，我想谈谈个人感官。特别是，当您谈论民主利润时，我想开始。我注意到的一件事是，我认为您在本节中使用了一两个引号。我认为这些引用实际上说了一些话：“我们要确保并非AI所产生的所有价值都留在一个地方，或者AI的好处不只是一小部分人。”</p><p>在这些报价中，它们实际上并未使用“利润”一词。在某些方面，您可以想象这些可能会采取不同的方式。例如，以<a href="https://github.com/features/copilot">github副词</a>为例。我想这是每年100美元的费用。假设世界上的每个人每年只支付100美元，这一切都去了Github。大量的人使用Github Copilot来做非常有用的事情，他们从中获得了很多价值和利益。大概，大多数好处都是财务上的，但并非所有的好处都会是财务上的。在我看来，这似乎是传播AI的价值或收益的情况，但这不一定是从AI中传播利润的情况。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，利润类别是一个……这是一个奇怪的类别。实际上，在撰写本文的过程中……与大多数论文不同，我们实际上首先写了<a href="https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation">博客文章</a>，然后写了论文。它应该朝另一个方向前进。在Govai网站上的博客文章中，它实际上分为四类：使用民主化，发展民主化，利益民主化和治理民主化。</p><p>对于本文中的版本，我们选择了利润术语，而不是术语的利益术语，因为好处在某种程度上太广泛了，因为……我的意思是，您已经指出了这一点。这些类别之间存在很多重叠，例如，将发展民主化的理由是确保更多的人从技术中受益，以满足他们的需求。</p><p>民主化使用的理由是确保更多的人可以访问和使用，甚至使用系统来产生价值和利润。如果您可以将其与自己的业务和自己的应用程序集成在一起，则可以从您的身边开发价值。我认为我们选择了利润术语的民主化，只是指出，关于大型科技公司的大量利润正在讨论。</p><p>我认为，是的，您确实指出了我们使用的一些语录，这些引号更广泛地谈论价值。我认为，有时很难仅仅根据落入某人银行帐户的字面意义来衡量利润。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>所以我认为……很难找到公司的准确报价。我知道不是公司的讨论很多，但实际上，这是您听到政策制定者谈论更多的一件事。这是普遍基本收入经常讨论的一部分。它有更多的时刻，人们在谈论它。关于普遍基本收入的许多讨论都存在……好吧，这是由自动化造成的失业，但也围绕着对科技公司和这些新技术的开发商的利润造成的。好的，如果所有利润将从劳动力转移到这些技术的开发人员，我们将如何将其重新分配？实际上，这是一种背景，利润民主化与围绕AI发展的讨论直接相关。</p><p>是的，我认为也许我们在论文中使用的一些报价更为模糊地为该价值做出了模糊的姿态。但是我们将利润分开了，只是因为我们在博客文章中获得了利润，这太笼统了。重叠太多了。我们有点双重，你知道吗？我认为这只是为了说明有关利润的讨论，但我不认为……我的意思是，这绝对不是最常用的。如果您听到有人在谈论AI民主化只是在街上行走，您就不会立即想到：“哦，他们一定是在谈论利润再分配。”那不是你的大脑去的地方。但这是值得注意的一个方面，是主要的收获。</p><h3>使人工智能治理民主化<a name="democratizing-gov"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>足够公平。是的，我想更多地谈论的下一个关于AI治理的民主化。与您刚才说的话有关，在我看来，如果有人谈论AI民主化，我认为如果他们只是说“使AI民主化”，他们意味着将治理民主化。我想知道这是否与您的经验相匹配。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，否。我认为，通常情况下，当您在媒体上看到，或者您正在听科技公司和他们的对话……是的，很少会谈论民主化治理。我认为我们开始看到转变。例如，Openai拥有了这个<a href="https://openai.com/blog/democratic-inputs-to-ai">AI民主化赠款计划</a>- 我不记得现在所谓的。基本上，他们正在向研究小组提供赠款，以研究基本上试图引入更多样化社区的意见的方法，以尝试更好地了解哪些价值观系统应保持一致。从这个意义上讲，您正在谈论AI民主化，以使人们进入决策过程以定义原则或定义AI一致性的价值观。 Openai拥有该赠款计划。</p><p>我认为拟人化只是发表了<a href="https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input">一篇博客文章</a>。他们与我之前提到的<a href="https://cip.org/">集体情报项目</a>紧密合作，该项目做了类似的事情来开发其<a href="https://arxiv.org/abs/2212.08073">宪法AI</a> ，确定了宪法原则应与AI系统保持一致的原因，这更像是一个民主的治理过程。因此，实际上，我认为我们开始看到这种“ AI民主化”的术语，“将AI民主化”渗入技术格局。我认为这是一件非常积极的事情。我认为这表明他们开始利用，并真正体现了民主化AI的民主原则。与仅意义分配和访问相反，它实际上意味着反映和代表利益相关者的利益和价值观和受影响人群的利益和价值观。</p><p>我认为我们开始看到这种转变，但是我认为您可能主要是在……再次，[如果在街上行走时，您会听到有人在谈论AI民主化，[他们]可能只是在谈论该技术的分配，使其更容易访问。这是经典的术语，但是我确实认为，在AI治理领域，甚至在实验室使用的术语中，我们也开始看到治理意思是渗入的，这令人兴奋。</p><h3>民主化的规范基础<a name="normative-underpinnings"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想在论文中谈到是否很好民主化基本上是根据治理方法的规范力量。因此，我想首先，这对我来说并不是那么明显。因此，我认为有时民主化，我认为它通常具有这种隐含的意义，即与民主作为一种政治决策方法更与平均主义相关。我认为您可以在没有民主决策的情况下拥有平等主义，而您可以在没有平等主义的情况下进行民主决策，对吗？人们可以投票赞成少数群体的权利或其他权利。所以，是的，我想知道你为什么这么说……我想我的意思是，请捍卫您的说法，即这是民主或民主化的善良的来源。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。好的。所以几件事，三件事，也许是两件事。我们将看看它是如何出现的。因此，首先，我确实坚持这样一种观念，即民主化的前三种形式，因此发展，使用，利润，[]不一定本质上是善良的。本质上不是很糟糕的，只是这些事情是发生的事情，如果我们坚持他们的定义，那就意味着使某些东西更容易访问，无论是使用或开发或使利润更容易获得。访问本质上不是好是坏。同样，如果我们使用“开发访问”示例，则如果您有一种可能确实很危险的技术，那么您不一定希望每个人都可以访问它。民主决策过程可能会得出这样的结论：确实并非每个人都应该使用它。</p><p>这就是第一部分：站在索赔上半年。第二部分，围绕赋予其道德力量或价值的事物是涉及的民主决策过程……我不知道这是否正是我们在论文中的目标。 I think there are lots of different methods that you can use to help reflect and serve the interests of wider populations. I think for some technologies… let&#39;s go back to your water bottle example. We don&#39;t need a panel of people from around the globe to tell us how to distribute water bottles. This is a decision that probably water bottle distributors can be like, “let&#39;s just sell our water bottles to as many people as possible” and [it&#39;s] probably fine.</p><p> I think it comes to where the impact&#39;s going to be, how big the impact&#39;s going to be, are there areas where the negative impacts of a technology might be more disproportionately felt? In those cases, being able to bring those voices in to inform decision-making processes that might affect them disproportionately is really important. I know we give examples in the paper of different kinds of democratic processes that can be used to inform decision-making, whether these are participatory processes or more deliberative democratic processes. So in <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">the open source paper</a> we just released, we talk about some of these processes as ways of making more democratic decisions around AI, but then also point to: one way to democratize governance decisions is to support regulation that is put forth by democratic governance and that those governments hopefully if structured well, reflect and serve the interests of the constituents.</p><p> And hopefully some of those governance processes are actually formed by having maybe some sort of deliberative process that takes into account stakeholder interests and the interests of impacted populations. But it&#39;s not like the only way to govern AI is to say, “okay, let&#39;s have a participatory panel where we take in all this insight and then use it to inform every decision”. I think you&#39;re right, that would just be completely impractical to have every decision that a lab makes be informed by a participatory panel or some sort of deliberative democratic process. So I think you kind of have to range it depending on the potential impact of the question. So I don&#39;t know if that answers your question completely, but I guess I stand by the first point, that democratization is not inherently good. The extent of the goodness I guess reflects how well the decisions reflect and serve the interests of the people that are going to be impacted. And then there are lots of different ways of figuring out how to do that.</p><p> <strong>Daniel Filan:</strong> Okay, that makes sense. So I guess the second thing I want to ask about that sentence is: when you say that the democratization of use, development and benefits is not inherently good, it kind of makes it sound like you think that democratization of governance is inherently good. So I found this paper that you actually cite, it&#39;s called <a href="https://johanneshimmelreich.net/papers/against-democratizing-AI.pdf">Against “Democratizing AI”</a> by Himmelreich, published in 2023. So he basically makes a few critiques. Firstly, he just says that AI isn&#39;t the sort of thing that needs to be democratic. AI in itself doesn&#39;t affect a bunch of people. There are just organizations that use AI and they affect a bunch of people and maybe they should be democratized, but AI itself doesn&#39;t have to be.</p><p> He says that the relevant things are already democratic. Democracy is pretty resource-intensive. People have to know a bunch of stuff, maybe you have to take a bunch of votes and things. He also has complaints that democracy is imperfect and doesn&#39;t necessarily fix oppression. So there are a few complaints about democratizing AI floating around. I guess in this case he&#39;s talking about democratizing AI governance. Is it the case that democratizing AI governance is inherently good?</p><p> <strong>Elizabeth Seger:</strong> I don&#39;t think so. Again, I think it comes back to the “ranging it” question. First of all, the resource-intensive point: absolutely. It can be incredibly resource-intensive to do an in-depth participatory governance process. This is why you don&#39;t do it for every single question that&#39;s out there. Yeah, so that&#39;s really good point. There&#39;s also the point of you need to be well-informed to make good decisions. So maybe we don&#39;t want the general public making decisions about all questions around AI. Experts are having a hard enough time for themselves trying to decide what constitutes a high risk system. We&#39;re not even good at benchmarking this among the community of AI experts. How are you going to tap into more general public notions of what a high risk system is or what proper benchmarking should look like?</p><p> So there&#39;s some questions where it just doesn&#39;t make sense to democratize it. So I think you have to think about: what questions is it realistic to have more democratized discussions around? So maybe something like thinking about what acceptable risk thresholds are, or what kind of values to align to. I think it&#39;s also important to remember there are lots of different kinds of democratic processes. When we talk about democratic processes, it&#39;s not necessarily what you might think of: we all go to the polls and cast our ballot on “do we want A or do we want B?” These could be more deliberative processes where you just get some stakeholders in a room, they have a discussion, they try and figure out what are the main issues [in] these discussions. So there are processes in place to hold deliberative democratic processes that are informed, you have experts come in and try and inform the discussion. So this would be a more informed deliberative process.</p><p> One way is: sometimes when it comes to… with image generation systems, there was a whole bunch of discussion around bias and images. Again, we see overlap between governance and other kinds of categories. If you get more people involved using the systems and they start understanding where the biases are, that is a form of education - having more familiarity. And then you can raise these complaints, whether that&#39;s directly to the companies or maybe through some sort of political intermediary. I remember there being, what was it? A <a href="https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices">statement</a> released by <a href="https://en.wikipedia.org/wiki/Anna_Eshoo">Anna Eshoo</a> , who I think was the congressional representative in, I want to say South San Jose, that talked in quite a bit of depth about … it was right after the release of <a href="https://stablediffusionweb.com/">Stable Diffusion</a> and talking about how there was a lot of racial bias in some of the images that were being produced, specifically against Asian women.</p><p> So this is something that was raised through a democratic process, was brought to the attention of the congressional office, and then the statement was released, and then that had knock-on effects on future governance decisions and decision-making by labs to do things like put safety filters. So I think that&#39;s one thing to keep in mind: there&#39;s a wide range of things that you can think about in terms of what it means to democratize a governance process. Governance is just decision-making. Then how do you make sure those decision-making processes reflect the interests of the people who are going to be impacted. It&#39;s not just about direct participation, although there is a lot of work being done to bring more direct participatory methods into even the more small scale decision-making, to make it more realistic.</p><p> Like this complaint about it being so resource intensive: that&#39;s true. We also have technologies available to help make it less resource intensive and how can we tap into those to actually help a public input to these decisions in a well-informed way? So there&#39;s lots of interesting work going on. Democratic governance also isn&#39;t inherently good when it comes to AI governance. I think again, it depends on the question. If the cost of doing a full-blown democratic process is going to far outstrip the benefit of doing a full-blown democratic process, then there&#39;d be a situation in which is probably not a net benefit to do a democratic decision-making process 。</p><p> <strong>Daniel Filan:</strong> So that gets to Himmelreich&#39;s resource-intensive complaint. He also has these complaints that basically AI itself is not the kind of thing that needs to be democratically governed. So he says something like, I&#39;ll see if I can remember, but the kind of thing that needs to be democratically governed is maybe something that just impinges on people&#39;s lives, whether they like or not, or something that&#39;s just inherent to large scale cooperation or something like that. He basically makes this claim that, look, AI doesn&#39;t do those things. AI is housed in institutions that do those things. So if you&#39;re worried about, maybe the police using facial recognition or something, you should worry about democratic accountability of police, not facial recognition, is the point I take him to be making.</p><p> So firstly, he says, look, we don&#39;t need to democratize AI. We need to democratize things that potentially use AI. And secondly, it seems like he&#39;s saying the relevant things that would be using AI, they&#39;re already democratic, so we shouldn&#39;t have conflicting … I think he talks about democratic overlap or something, where I think he really doesn&#39;t like the idea of these different democratic institutions trying to affect the same decision and maybe they conflict or something.</p><p> <strong>Elizabeth Seger:</strong> So I think I don&#39;t disagree. I think, if I understood correctly, this idea of “you don&#39;t need to democratize the AI, you need to democratize the institution that&#39;s using it”, I think that&#39;s a completely valid point. AI itself, again, is not inherently bad or good. It&#39;s a dual use technology. It depends what you&#39;re going to do with the thing. But I would say that trying to decide, for example, how law enforcement should use AI systems, that is an AI governance question. This is a question about how AI is being used, about how AI is being regulated in this context, not in terms of how it&#39;s being developed, but in context of how it&#39;s being used.</p><p> What are the appropriate use cases of AI? This is an AI governance question, and it&#39;s not necessarily that these are regulations that are placed on AI specifically. A lot of AI regulation just already exists within different institutions. It&#39;s just figuring out how to have that regulation apply to AI systems. This might be cases like: if you have requirements for certain health and safety standards that have to be met by a certain technology in a work setting, AI shouldn&#39;t be held to different standards. It&#39;s just a matter of figuring out how to measure and make sure that those standards are met by the AI system. So I guess based on what you said, I want to say that I completely agree.</p><p> But I would say that it still falls under the umbrella of democratizing AI governance. I think that what might be happening here is just another conflict over what does the terminology mean, where it&#39;s like “we don&#39;t need to democratize AI, as in we don&#39;t need to get more people directly involved in the development and decision-making about individual decisions around AI development”. In which case, yes, I agree! But we might be talking about democratizing AI governance in different… This is why these discussions get really complicated because we just end up talking past each other because we use the same word to mean five different things.</p><p> <strong>Daniel Filan:</strong> Yeah, it can be rough. So before we close up our discussion on democratizing AI, I&#39;m wondering… I guess you&#39;ve thought about democratizing AI for a while. Do you have any favorite interventions to make AI more democratic or to make AI less democratic in any of the relevant senses?</p><p> <strong>Elizabeth Seger:</strong> I&#39;ll be completely honest, this is sort of where my expertise drops off in terms of the more nitty-gritty of what processes are available and exactly when are certain processes the best ones to be applied. For this, I would really look more closely at the work that <a href="https://cip.org/">the Collective Intelligence Project</a> &#39;s doing. Or my co-author <a href="https://aviv.me/">Aviv Ovadya</a> , who&#39;s on the paper, he&#39;s really involved with these discussions and works with various labs to help implement different democratic processes. Yeah, I was just going to say, this is where my expertise drops off and I would point towards my colleagues for more in-depth discussion on that work.</p><h2> Open-sourcing AI<a name="osai"></a></h2><p> <strong>Daniel Filan:</strong> All right, the next thing I want to talk about is basically open-sourcing AI. So there&#39;s this report, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">Open-sourcing highly capable foundation models: an evaluation of risks, benefits, and alternative methods for pursuing open source objectives</a> . It&#39;s by yourself and a bunch of other co-authors, but mostly as far as I could see at <a href="https://www.governance.ai/">the Center for the Governance of AI</a> . Again, could you perhaps give an overview of what&#39;s going on in this report?</p><p> <strong>Elizabeth Seger:</strong> Okay, yeah. So this report&#39;s a little harder to give an overview on because unlike the democratization paper, which is eight pages, this one&#39;s about 60 pages.</p><p> So quick overview. First thing is that we wanted to point out that there&#39;s debate right now around whether or not large foundation models, especially frontier foundation models, should be open source. This is a debate that has kicked off starting with <a href="https://stability.ai/">Stability AI</a> &#39;s release of <a href="https://stablediffusionweb.com/">Stable Diffusion</a> , and then there was the <a href="https://ai.meta.com/llama/">Llama 2</a> release and then the weights were leaked, and now Meta&#39;s getting behind this open source thing; there were the <a href="https://spectrum.ieee.org/meta-ai">protests outside of Meta</a> about whether or not systems should be open source. And then we&#39;re also seeing a lot of activity with the development of the <a href="https://artificialintelligenceact.eu/">EU AI Act</a> . As part of the EU AI Act, some parties are pushing for an exemption of regulation for groups that develop open source models. So a lot of discussion around open source. I think the goal of this paper was to cut through what is becoming a quickly polarized debate.</p><p> We&#39;re finding that you have people that see the benefits of open-sourcing and are just very, very hardcore pro open source and then won&#39;t really hear any of the arguments around the potential dangers. And then you have people who are very anti open source, only want to talk about the dangers and sort of refuse to see the benefits. It just makes it really hard to have any kind of coherent dialogue around this question. Yet at the same time, country governments are trying to make very real policy around model sharing. So how can we cut through this polarized debate to just try and say, here&#39;s the lay of the land. So what we&#39;re trying to do in this paper is really provide a well-balanced, well-researched overview of both the risks and benefits of open-sourcing highly capable models.</p><p> By &#39;highly capable models&#39;, basically what we&#39;re talking about is frontier AI systems. We just don&#39;t use the term &#39;frontier&#39; because the frontier moves, but we want to be clear that there will be some systems that might be highly capable enough and pose risks that are high enough that, even if they aren&#39;t on the frontier, we should still be careful about open-sourcing them. So basically we&#39;re talking about frontier models, but frontier keeps going. So we can get into the terminology debate a little bit more later. But basically we wanted to outline what are the risks and benefits of open-sourcing these increasingly highly capable models, but then not just to have yet another piece that says, well, here are the risks and here are the benefits. But then kind of cut through by saying &#39;maybe there are also alternative ways that we can try to achieve some of the same benefits of open-sourcing, but at less risk.&#39;</p><p> So the idea here is actually quite similar to the democratization paper: it&#39;s to not just say &#39;what is open-sourcing?&#39; and &#39;is it good or bad?&#39; But to say &#39;What are the specific goals of open-sourcing? Why do people say that open-sourcing is good, that it&#39;s something we should be doing that should be defended, that should be preserved? So why is it we want to open source AI models? Then if we can be specific about why we want to open source, are there other methods that we can employ to try to achieve some of these same goals at less risk?&#39; These might be other model sharing methods like releasing a model behind API or doing a staged release, or it might be other more proactive measures like: let&#39;s say one benefit of open-sourcing is that it can help accelerate research progress, both in developing more greater AI capabilities, but also promoting AI safety research.</p><p> Another way you can promote AI safety research is to dedicate a certain percentage of profits towards AI safety research or to have research collaborations. You can do these things without necessarily having to provide access for anybody to have access to the model. So this is what we were trying to do with this paper: really just say, okay, we&#39;re going to give, first of all, just a very well-researched overview of both the risks and benefits. In fact, majority of the paper actually focuses on what are the benefits and trying to break down the benefits of open-sourcing and then really doing a deep dive into alternative methods or other things that can be done to help pursue these benefits where the risks might just be too high.</p><p> I think that the main claim that we do make in the paper with regard to &#39;is open-sourcing good or bad?&#39; First of all, it&#39;s a false dichotomy, but I think our main statement is open-sourcing is overwhelmingly good. Open source development of software, open source software underpins all of the technology we&#39;re using today. It&#39;s hugely important. It&#39;s been great for technological development, for the development of safe technology that reflects lots of different user needs and interests.好东西。 The issue is that there might be some cutting edge, highly capable AI systems that pose risks of malicious use or the proliferation of dangerous capabilities or even proliferation of harms and vulnerabilities to downstream applications.</p><p> Where these risks are extreme enough, we need to take a step back and say: maybe we should have some processes in place to decide whether or not open-sourcing these systems is okay. So I think the main thrust of the report is [that] open-sourcing is overarching good, there just may be cases in which it&#39;s not the best decision and we need to be careful in those cases. So yeah, I think that&#39;s the main overview of the paper.</p><p> <strong>Daniel Filan:</strong> Gotcha. Yeah, in case listeners were put off by the 60 something pages, I want listeners to know there is an executive summary. It&#39;s pretty readable.</p><p> <strong>Elizabeth Seger:</strong> It&#39;s a two-page executive summary.不用担心。</p><p> <strong>Daniel Filan:</strong> Yeah. I read the paper and I vouch for the executive summary being a fair summary of it. So don&#39;t be intimidated, listeners.</p><p> <strong>Elizabeth Seger:</strong> I&#39;ve also been putting off writing the blog post, so there will be a blog post version at some point, I promise.</p><p> <strong>Daniel Filan:</strong> Yeah. So it&#39;s possible that might be done by the time we&#39;ve released this, in which case we&#39;ll link the blog post.</p><p> <strong>Elizabeth Seger:</strong> Maybe by Christmas.</p><h3> Risks from open-sourcing<a name="risks-from-os"></a></h3><p> <strong>Daniel Filan:</strong> Yeah. Hopefully you&#39;ll have a Christmas gift, listeners. So when we&#39;re talking about these highly capable foundation models and basically about the risks of open-sourcing them… as I guess we&#39;ve already gotten into, what is a highly capable foundation model is a little bit unclear.</p><p> <strong>Elizabeth Seger:</strong> Yeah.已经很难了。</p><p> <strong>Daniel Filan:</strong> Maybe it&#39;ll be easier to say, what kinds of risks are we talking about? Because I think once we know the kinds of risks, (a) we know what sorts of things we should be willing to do to avert those risks or just what we should be thinking of, and (b) we can just say, okay, we&#39;re just worried about AIs that could potentially cause those risks. So what are the risks?</p><p> <strong>Elizabeth Seger:</strong> In fact, I think framing it in terms of risks is really helpful. So generally here we&#39;re thinking about risks of significant harm, significant societal or even physical harm or even economic harm. And of course now you say, “oh, define &#39;significant&#39;”. But basically just more catastrophic, extreme, significant societal risks and harms. So we use some examples in the paper looking at potential for malicious use.</p><p> There are some more diffuse harms if you&#39;re thinking about political influence operations. So this kind of falls into the misinformation/disinformation discussion. How might AI systems be used to basically influence political campaigns? So disinformation undermines trust and political leaders. And then in turn, if you can disrupt information ecosystems and disintegrate the processes by which we exchange information or even just disintegrate trust in key information sources enough, that can also impact our ability as a society to respond to things like crises.</p><p> So the pandemic is a good example of, if it&#39;s really hard to get people accurate information about, for example, whether or not mask-wearing is effective, you&#39;re not going to have really good coordinated decision-making around mask-wearing. So I think this is one where it&#39;s a little bit more diffuse. It&#39;s harder to measure. So I think this is one point where it&#39;s quite difficult to identify when the harm is happening because it&#39;s so diffused. But I think this is one potential significant harm. I&#39;d say maybe thinking about disrupting major political elections or something like that.</p><p> There&#39;s some options for malicious use that are talked about a little more frequently, I think because they&#39;re more well-defined and easier to wrap your head around. These are things like using generative AI to produce biological weapons or toxins, even production of malware to mount cyber attacks against key critical infrastructure. Imagine taking down an electrical grid, or imagine on election day, taking down an electoral system. These could have significant societal impacts in terms of harm to society or physical harm.</p><p> I think one key thing to point out here is that we aren&#39;t necessarily seeing these capabilities already. Some people may disagree, but I think my opinion is that technologies with the potential for this kind of harm do not currently exist. I think it is wise to assume that they will come into existence in the not too distant future, largely because we&#39;re seeing indications of these kinds of capabilities developing. We&#39;ve <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/">already seen how even narrow AI systems that are used in drug discovery, if you flip that parameter that&#39;s supposed to optimize for non-toxicity to optimize for toxicity, now you have a toxin generator</a> . So it doesn&#39;t take a huge stretch of the imagination to see how more capable systems could be used to cause quite significant societal harm. So no, I don&#39;t think there&#39;s currently systems that do this. I think we need to be prepared for a future in which these systems do exist. So it&#39;s worth thinking about the wisdom of releasing these systems now even if we might not be present with the technology at this moment.</p><p> <strong>Daniel Filan:</strong> So as a follow-up question: you&#39;re currently on the AI X-risk Research Podcast. I&#39;m wondering… so it seems like you&#39;re mostly thinking of things less severe than all humans dying or just permanently stunting the human trajectory.</p><p> <strong>Elizabeth Seger:</strong> I wouldn&#39;t say necessarily. I think this is definitely a possibility. I think part of what&#39;s happening is you kind of have to range how you talk about these things. I am concerned about x-risk from AI, and I think we could have systems where whether it&#39;s the cyber capability or the biological terror capability or even the taking down political systems capability and bolstering authoritarian governments - these are things that could cause existential risk and I am personally worried about this. But [we&#39;re] trying to write papers that are digestible to a much wider population who might not be totally bought into the x-risk arguments. I think, x-risk aside, there are still very genuine arguments for not necessarily releasing a model because of the harm that it can cause even if you don&#39;t think that those are existential risks. Even if it&#39;s just catastrophic harm, probably still not a great idea.</p><p> <strong>Daniel Filan:</strong> So at the very least, there&#39;s a range going on there.</p><p> <strong>Elizabeth Seger:</strong> Yeah.</p><h3> Should we make AI too dangerous to open source?<a name="make-ai-too-dangerous-os"></a></h3><p> <strong>Daniel Filan:</strong> So one question I had in my mind, especially when thinking about AI that can cause these kinds of risks and potentially open-sourcing it… Part of me is thinking, it seems like a lot of the risks of open-sourcing seem to be, well, there&#39;s now more people who can use this AI system to do a bunch of harm. The harm is just so great that that&#39;s really scary and dangerous. So one thing I was wondering is: if it&#39;s so dangerous for many people to have this AI technology, is it not also dangerous for anybody at all to have this AI technology? How big is this zone where it makes sense to make the AI but not to open source it? Or does this zone even exist?</p><p> <strong>Elizabeth Seger:</strong> Yeah, you make a very genuine point, and there are those that would argue that we should just hit the big old red stop button right now. I think that is an argument some people make. I guess where we&#39;re coming from with respect to this paper is trying to be realistic about how AI development&#39;s probably going to happen and try to inform governance decisions around what we should do as AI development continues. I think there are differing opinions on this, probably even among the authors on the paper. There&#39;s what, 25, 26 authors on the paper.</p><p> <strong>Daniel Filan:</strong> The paper has a disclaimer, listeners, that not all authors necessarily endorse every claim in the paper.</p><p> <strong>Elizabeth Seger:</strong> That includes me. So we have a very, very broad group of authors. But I think… Sorry, what was your original question? It just flew out of my head.</p><p> <strong>Daniel Filan:</strong> If it&#39;s too dangerous to open source, is it also too dangerous to make?</p><p> <strong>Elizabeth Seger:</strong> Yeah, no. So I think there&#39;s definitely an argument for this. I guess where I&#39;m coming from is trying to be realistic about where this development process is going and how to inform governments on what to do as AI development continues. It may very well be the case that we get to a point where everyone&#39;s convinced that we just have a big coordinated pause/stop in AI development. But assuming that that&#39;s not possible or improbable, shall we say, I think it&#39;s still wise to have a contingency plan. How can we guide AI development to be safe and largely beneficial and reduce the potential for risk? I think there&#39;s also an argument to be made that increasingly capable systems, while they pose increasingly severe risks, also could provide increasingly great benefits and potential economic potential benefits for helping to solve other challenges and crises that we face.</p><p> So I think you&#39;d be hard-pressed to get a giant coordinated pause. So the next plan is how do we make AI development happen safely? It is a lot easier to keep people safe if the super dangerous ones are not spread widely. I guess that&#39;s the very simplistic view.是的。</p><p> So I think, at least for me, I think it just comes from a realism about, are we likely to pause AI development before it gets super scary? Probably not, just given how humanity works. We developed nuclear weapons, probably shouldn&#39;t have done that. Oops, well now they exist, let&#39;s deal with it. So I think having a similar plan in line for what do we do with increasingly capable AI systems is important, especially given that it might not be that far away. Like I said, sort of with each new generation of AI that&#39;s released, we all kind of hold our breaths and say, oh, well what&#39;s that? What&#39;s that one going to do? Then we learn from it and we don&#39;t know what the next generation of AI systems is going to bring. And so having systems in place to scan for potential harms, potential dangers, capabilities, to inform decisions about whether or not these systems should be released and if and how they should be shared, that&#39;s really important. We might not be able to coordinate a pause before the big scary happens. So, I think it&#39;s important to discuss this regardless.</p><p> <strong>Daniel Filan:</strong> I got you.</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a technical term by the way, the big scary.</p><h3> Offense-defense balance<a name="offense-defense"></a></h3><p> <strong>Daniel Filan:</strong> Makes sense. So speaking of the risks of open-sourcing AI, in the paper you talk about the offense-defense balance, and basically you say that bad actors, they can disable misuse safeguards, they can increase new dangerous capabilities by fine-tuning. Open-sourcing makes this easier, it increases attacker knowledge. And in terms of just the AI technology, you basically make a claim that it&#39;s tilted towards offense in that attackers can do… They get more knowledge, they can disable safeguards, they can introduce new dangerous capabilities. It&#39;s easier to find these problems than it is to fix them. And once you fix them, it&#39;s hard to make sure that everyone has the fixes. Would you say that&#39;s a fair summary of-</p><p> <strong>Elizabeth Seger:</strong> Yeah, I&#39;d say that&#39;s pretty fair. I think the one thing I&#39;d add… I&#39;d say that with software development, for example… so offense-defense balance is something that&#39;s often discussed in terms of open-sourcing and scientific publication, especially any time you have dual-use technology or scientific insights that could be used to cause harm, you kind of have to address this offense-defense balance. Is the information that&#39;s going to be released going to help the bad actors do the bad things more or less than it&#39;s going to help the good actors do the good things/prevent the bad actors from doing the bad things? And I think with software development, it&#39;s often in favor of defense, in finding holes and fixing bugs and rolling out the fixes and making the technology better, safer, more robust. And these are genuine arguments in favor of why open-sourcing AI systems is valuable as well.</p><p> But I think especially with larger, more complex models, we start veering towards offense balance. I just want to emphasize, I think one of the main reasons for this has to do with how difficult the fixes are. So in the case of software, you get bugs and vulnerabilities that are relatively well-defined. Once you find them, relatively easy to fix, roll out the fix. The safety challenges that we&#39;re facing with highly capable AI systems are quite complex. We have huge research teams around the world trying to figure them out, and it&#39;s a very resource-intensive process, takes a lot of talent. Vulnerabilities are still easy to find, they&#39;re just a lot harder to fix. So, I think this is a main reason why the offense-defense balance probably skews more towards offense and enabling malicious actors because you can still find the vulnerabilities, you can still manipulate the vulnerabilities, take advantage of them… harder to fix, even if you have more people involved. So, that&#39;s the high-level evaluation. Mostly, I just wanted to push on the safety issue a little harder.</p><h3> KataGo as a case study<a name="katago-as-case-study"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So, one thing this actually brings up for me is… you may or may not be familiar, so there&#39;s AI that plays the board game Go. There&#39;s an <a href="https://katagotraining.org/">open-source training run</a> and some colleagues of mine have found basically <a href="https://goattack.far.ai/">you can cheaply find adversarial attacks</a> . So, basically dumb Go bots that play in a way that confuses these computer AI policies. And these attacks, they&#39;re not generally smart, but they just push the right buttons on the AI policy. And I guess this is more of a comment than a question, but there&#39;s been enough rounds of back and forth between these authors and the people making these open source Go bots that it&#39;s potentially interesting to just use that as a case study of the offense-defense balance.</p><p> <strong>Elizabeth Seger:</strong> So, you&#39;re saying that given that the system was open source and that people could sort of use it and query it and then send the information back to the developers, that that&#39;s-</p><p> <strong>Daniel Filan:</strong> Yeah, and in fact, these authors have been working with the developers of this software, and in fact what&#39;s happened is the developers of this software - KataGo, if people want to google it - they&#39;ve seen this paper, they&#39;re trying to implement patches to fix these issues. The people finding the adversarial policies are basically checking if the fixes work and publishing the information about whether they do or not.</p><p> <strong>Elizabeth Seger:</strong> Absolutely, so I want to be really clear that this is a huge benefit of open source development: getting people involved in the development process, but also using the system, finding the bugs, finding the issues, feeding that information back to the developers. This is a huge benefit of open source development for software and AI systems. I think that this is specifically why the paper focuses on the big, highly capable frontier foundation models is that this gets more difficult the more big, complex, cutting edge the system is. Some bugs will still be relatively small, well-defined, and there are bug bounty programs, proposals for AI safety bounty programs as well, helping to find these vulnerabilities and give the information back to the developers. I think there are issues though, with respect to some of the larger safety issues that are more difficult. Sometimes it&#39;s difficult to identify the safety problems in the first place, more difficult to address the safety problems.</p><p> Then, there&#39;s also the issue of just rolling out the fixes to the system. So software development, you fix a bunch of bugs, you roll out an update. Oftentimes, in the license it&#39;ll say that you&#39;re supposed to actually use the updated version. There&#39;s some data that came out, I can&#39;t remember which organization it was right now, I&#39;ll have to look it up later, but it&#39;s actually quite a low uptake rate of people actually running the up-to-date software. So first of all, even with just normal software, it&#39;s hard to guarantee that people are actually going to run the up-to-date version and roll out those fixes, which is an issue with open source because if you&#39;re using something behind API, then you just update the system and then everyone&#39;s using the updated system. If you have an open source system, then people actually have to download and run the update version themselves.</p><p> With foundation models, there&#39;s actually a weird incentive structure that changes where people might actually be de-incentivized to update. So with software, oftentimes when you have an update, it fixes some bugs and it improves system functionality. When it comes to safety fixes for foundation models, oftentimes it has to do with reducing system functionality, like putting on a filter that says, “Well, now you can&#39;t produce this class of images. Now, you can&#39;t do this kind of function with the system,” so it&#39;s hard, I don&#39;t know if there&#39;s good information on how this has actually panned out now. Are we seeing lower uptake rates with updates for AI systems? I don&#39;t know, but there might be something weird with incentive structures going on too, where if updates basically equate to reducing system functionality in certain ways, people might be less likely to actually take them on board.</p><p> <strong>Daniel Filan:</strong> I don&#39;t have a good feel for that.</p><p> <strong>Elizabeth Seger:</strong> I don&#39;t have a super good feel, but just, I don&#39;t know, interesting food for thought, perverse incentive structures.</p><p> <strong>Daniel Filan:</strong> I don&#39;t know, I&#39;m still thinking about this KataGo case: so that&#39;s the case where the attack does reduce the system functionality and people are interested in getting the latest version with fixes. It also occurs to me that… So, in fact the structure of this paper, the way they found the attack did not rely on having access to the model weights. It relied on basically being able to query the Go bot policy, basically to try a bunch of things and figure out how to trick the Go bot policy. Now, it&#39;s really helpful if you can have the weights locally just so that you can call the API, so that you can call it a lot, but that was not a case where you needed the actual weights to be shared. So on the one hand, that&#39;s a point that sharing the weights is less valuable than you might think, but it also suggests if you&#39;re worried about people finding these adversarial attacks, then just putting the weights behind an API doesn&#39;t protect you as much as you think. Maybe you need to rate limit or something.</p><p> <strong>Elizabeth Seger:</strong> I think that&#39;s a valuable insight, there are definitely things you can do without weights. This is an argument for why you should be worried anyway, but it&#39;s also an argument for… There are lots of arguments for, well, open-sourcing is important because you need it to do safety research and getting more people involved in safety research will result in safer systems, you have more people input into these processes, but you just illustrated a perfect example of how just having query access, for example, to a system, can allow you to do a significant amount of safety research in terms of finding漏洞。</p><h3> Openness for interpretability research<a name="open-for-interp"></a></h3><p> <strong>Elizabeth Seger:</strong> So, query access is one that can be done completely behind an API, but then even if we think about something like interpretability research, interpretability research does require much more in-depth access to a system to do, but arguably this is an argument for needing access to smaller systems. We are struggling to do interpretability research on smaller, well-defined systems. It&#39;s sort of like the rate limiting factor on interpretability research isn&#39;t the size of the models people have access to, the way I understand it at least. If we&#39;re struggling to do interpretability research on smaller models, I feel like having access to the biggest, most cutting edge frontier model is not what needs to happen to drive interpretability research.</p><p> <strong>Daniel Filan:</strong> I think it depends on the kind of interpretability research.</p><p> <strong>Elizabeth Seger:</strong> It&#39;d be interesting to hear your thoughts on this as well, but there&#39;s a range of different kinds of AI research. Not all of it requires open access and then some of the kinds that do require open access to the models isn&#39;t necessarily helped the most by having the open access, and then there is also this idea of alternative approaches that we talk about in the纸。 You can help promote AI safety research by providing access to specific research groups. There are other things you can do to give people the access they need to do safety research.</p><p> <strong>Daniel Filan:</strong> So, I guess I can share my impressions here. Interpretability research is a broad bucket. It describes a few different things. I think there are some kinds of things where you want to start small and we haven&#39;t progressed that far beyond small. So just understanding, &#39;can we just exhaustively understand how a certain neural net works?&#39; - start small, don&#39;t start with GPT-4. But I think one thing you&#39;re potentially interested in, in the interpretability context, is how things get different as you get bigger models, or do bigger models learn different things or do they learn more? What sorts of things start getting represented? Can we use interpretability to predict these shifts? There you do want bigger models. In terms of how much can you do without access to weights, there&#39;s definitely a lot of interpretability work on these open source models because people apparently really do value having the weights.</p><p> Even in the case of the adversarial policies work I was just talking about, you don&#39;t strictly need access to the weights, but if you could run the games of Go purely on your computer, rather than calling the API, waiting for your request to be sent across the internet, and the move to be sent back, and doing that a billion times or I don&#39;t know the actual number, but it seems like just practically-</p><p> <strong>Elizabeth Seger:</strong> Much more efficient.</p><p> <strong>Daniel Filan:</strong> … It&#39;s easier to have the model. I also think that there are intermediate things. So one thing the paper talks about, and I guess your colleague <a href="https://sites.google.com/view/tobyshevlane">Toby Shevlane</a> has <a href="https://arxiv.org/abs/2201.05159">talked about</a> is basically structured access of giving certain kinds of information available maybe to certain people or maybe you just say “these types of information are available, these types aren&#39;t”. I&#39;ve heard colleagues say, “Even if you didn&#39;t open source GPT-4 or GPT-3, just providing final layer activations or certain kinds of gradients could be useful”, which would not… I don&#39;t think that would provide all the dangers or all the risks that open-sourcing could potentially involve.</p><p> <strong>Elizabeth Seger:</strong> I think this is a really key point as well, is trying to get past this open versus closed dichotomy. Just saying that something isn&#39;t open source doesn&#39;t necessarily mean that it&#39;s completely closed and no-one can access it. So like you said, Toby Shevlane talks about structured access, and it was a paper we referenced - at least when we referenced it, it was still forthcoming, it might be out now - but it was Toby Shevlane and Ben Bucknell were working on it, and it was about the potential of developing research APIs. So, how much access can you provide behind API to enable safety research and what kind of access would that need to look like and how could those research APIs be regulated and who by? So, I think if there&#39;s a genuine interest in promoting AI safety research and a genuine acknowledgement of the risks of open-sourcing, we could put a lot of resources into trying to develop and understand ways to get a lot of the benefits to safety research that open-sourcing would have by alternative means.</p><p> It won&#39;t be perfect. By definition, it&#39;s not completely open, but if we take the risks seriously, I think it&#39;s definitely worth looking into these alternative model sharing methods and then also into the other kinds of proactive activities we can engage in to help promote safety research, whether that&#39;s committing a certain amount of funds to safety research or developing international safety research organizations and collaborative efforts. I know one issue that always comes up when talking about, “Well, we&#39;ll just provide safety research access through API, or we&#39;ll provide privileged downloaded access to certain groups.” It&#39;s like, “Well, who gets to decide who has access? Who gets to do the safety research?”</p><p> And so, I think this points to a need to have some sort of a multi-stakeholder governance body to mediate these decisions around who gets access to do the research, whether you&#39;re talking about academic labs or other private labs, sort of like [how] you have multi-stakeholder organizations decide how to distribute grants to do environmental research, or you have grantmaking bodies that distribute grant funds to different academic groups. You could have a similar type situation for distributing access to more highly capable, potentially dangerous systems, to academic groups, research groups, safety research institutions that meet certain standards and that can help further this research.</p><p> So, I feel like if there&#39;s a will to drive safety research forward, and if varying degrees of access are needed to allow the safety research to happen, there are things we can do to make it happen that do not necessarily require open-sourcing a系统。 And I think, like we said, different kinds of safety research require varying degrees of access. It&#39;s not like all safety research can be done with little access. No, you need different amounts of access for different kinds of safety research, but if there&#39;s a will, there&#39;s a way.</p><h3> Effectiveness of substitutes for open sourcing<a name="os-substitutes"></a></h3><p> <strong>Daniel Filan:</strong> So, I want to ask something a bit more quantitative about that. So, some of the benefits of open-sourcing can be gained by halfway measures or by structured access or pursuing tons of collaborations, but as you mentioned, it&#39;s not going to be the same as if it were open sourced. Do you have a sense of… I guess it&#39;s going to depend on how constrained you are by safety, but how much of the benefits of open source do you think you can get with these more limited sharing methods?</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a good question. I think you can get quite a bit, and I think, again, it sort of depends what kind of benefit you&#39;re talking about. So in the paper, I think we discuss three different benefits. Let&#39;s say we talk about accelerating AI research, so that&#39;s safety research and capability research. We talk about distributing influence over AI systems, and this ranges everything from who gets to control the systems, who gets to make governance decisions about the systems, who gets to profit. It wraps all the democratization themes together under distributing influence over AI, and then, let&#39;s see, what was the other one that we talked about? You&#39;d think I&#39;ve talked about this paper enough in the last three months, I have it down. Oh, external model evaluation. So, enabling external oversight and evaluation, and I think it depends which one you&#39;re talking about.</p><p> Let&#39;s start with external model evaluation. I think that this probably benefits the most from open-sourcing. It depends what you&#39;re looking at, so for example, if you&#39;re just looking for minor bugs and stuff like that, you don&#39;t need open source access for that, but having a more in-depth view to the systems is more important for people trying to help find fixes to the bugs. We&#39;ve discussed this. There are also risks associated with open-sourcing. If we&#39;re talking about accelerating capability research, for example, which sort of falls under the second category, I think you might find that the benefits of open-sourcing here might be somewhat limited the larger and more highly capable the system gets. And I think this largely will just have to do with who has access to the necessary resources to really operate on the cutting edge of research and development. Open source development, it operates behind the frontier right now largely because of restrictions… not restrictions, but just the expense of the necessary compute resources.</p><p> And then you talk about distributing control over AI, we&#39;ve already discussed the more distributed effect of open-sourcing and model sharing on distributing control. It&#39;s a second order effect: you get more people involved in the development process and then large labs have more competition, and then it distributes influence and control.</p><p> There are probably more direct ways you can help distribute control and influence over AI besides making a system widely available. So, to answer your original question then about, how much of the benefit of open-sourcing can you get through alternative methods? I guess it really depends what benefit you&#39;re talking about. I think for AI safety progress probably quite a bit, honestly; actually the vast majority of it, given that a lot of the safety research that&#39;s done on these highly capable, cutting edge models is something that has to happen within well-resourced institutions anyway, or you need the access to the resources to do that, not just the code and the weights, but the computational resources and so on.</p><p> So, I think quite a bit. I think it&#39;s less of a “can we get close to the same benefits that open-sourcing allows?” It&#39;s more like, “can we do it in one fell swoop?”就是这样。 It&#39;s like open-sourcing is the easy option. “Here, it&#39;s open!” - and now you get all these benefits from open-sourcing. The decision to open-source or not, part of the reason it&#39;s a hard decision is because achieving these benefits by other means is harder. It&#39;s going to take more resources to invest, more organizational capacity, more thought, more cooperation. It&#39;s going to take a lot of infrastructure, a lot of effort. It&#39;s not the one-stop shop that open-sourcing is, but I think the idea is that if the risks are high enough, if the risks are severe enough, it&#39;s worth it. I think that&#39;s where it comes in.</p><p> So, I guess it&#39;s worth reiterating again and again: this paper is not an anti-open source paper, [it&#39;s] very pro-open source in the vast majority of cases. What we really care about here are frontier AI systems that are starting to show the potential for causing really catastrophic harm, and in these cases, let&#39;s not open-source and let&#39;s pursue some of these other ways of achieving the same benefits of open source to safety and distributing control and model evaluation, but open-source away below that threshold. The net benefits are great.</p><h3> Offense-defense balance, part 2<a name="offense-defense-2"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So my next question - I actually got a bit sidetracked and wanted to ask it earlier - so in terms of the offense-defense balance, in terms of the harms that you are worried about from open-sourcing, I sometimes hear the claim that basically, “Look, AI, if you open-source it, it is going to cause more harm, but you also enable more people to deal with the harm.” So I think there, they&#39;re talking about offense-defense balance, not of finding flaws in AI models, but in the underlying issues that AI might cause. So, I guess the idea is something… To caricature it, it&#39;s something like, “Look, if you use your AI to create a pathogen, I can use my AI to create a broad spectrum antibiotic or something”, and the hope is that in these domains where we&#39;re worried about AI causing harm, look, just open-sourcing AI is going to enable tons of people to be able to deal with the harm more easily, as well as enabling people to cause harm. So I&#39;m wondering, what do you think about the underlying offense-defense balance as opposed to within AI?</p><p> <strong>Elizabeth Seger:</strong> I get the argument. Personally, I&#39;m wary about the arms race dynamic though. You gotta constantly build the stronger technology to keep the slightly less strong technology in check. I guess this comes back to that very original question you asked about, “What about just hitting the no more AI button?” So, I guess I get the argument for that. I think there&#39;s weird dynamics, I don&#39;t know. I&#39;m not doing a very good job answering this question. I&#39;m personally concerned about the race dynamic here, and I think it just comes back to this issue of, how hard is it to fix the issues and vulnerabilities in order to prevent the misuse in the first place? I think that should be the goal: preventing the misuse, preventing the harm in the first place. Not saying, “Can we build a bigger stick?”</p><p> There&#39;s a similar argument that is brought up when people talk about the benefits of producing increasingly capable AI systems and saying, “Oh, well, we need to plow ahead and build increasingly capable AI systems because you never know, we&#39;ll develop a system that&#39;ll help cure cancer or develop some renewable energy technology that&#39;ll help us address climate change or something like that.” What huge problems could AI help us solve in the future? And I don&#39;t know - this is personally me, I don&#39;t know what the other authors on this paper think of this - but I don&#39;t know, I kind of feel like if those are the goals, if the goals are to solve climate change and cure cancer, take the billions upon billions upon billions and billions of dollars that [we&#39;re] currently putting into training AI systems and go cure cancer and develop renewable technologies! I struggle with those arguments personally. I&#39;d be interested just to hear your thoughts. I have not written about this. This is me riffing right now. So, I&#39;d be interested to hear your thoughts on this train of thought as well.</p><p> <strong>Daniel Filan:</strong> I think the original question is unfairly hard to answer just because it&#39;s asking about the offense-defense balance of any catastrophic problem AI might cause and it&#39;s like, “Well, there are tons of those and it&#39;s pretty hard to think about. ” So, the thing you were saying about, if you wanted to cure cancer, maybe step one would not be “create incredibly smart AI”. I&#39;ve seen this point. I don&#39;t know if you know <a href="https://meaningness.com/about-my-sites">David Chapman</a> &#39;s <a href="https://betterwithout.ai/">Better without AI</a> ?</p><p> <strong>Elizabeth Seger:</strong> No, not familiar.</p><p> <strong>Daniel Filan:</strong> So, he basically argues we just shouldn&#39;t build big neural nets and it&#39;s going to be terrible. Also, <a href="https://aiimpacts.org/author/jeffreyheninger/">Jeffrey Heninger</a> at <a href="https://aiimpacts.org/">AI Impacts</a> , I think has said <a href="https://blog.aiimpacts.org/p/my-current-thoughts-on-the-ai-strategic">something similar along these lines</a> . On the one hand, I do kind of get it, just in the sense that, if I weren&#39;t worried about misaligned AI, there&#39;s this hope that this is the last invention you need. You create AI and now instead of having to separately solve cancer and climate change and whatever, just make it solve those things for you.</p><p> <strong>Elizabeth Seger:</strong> I guess it&#39;s just really hard to look forward, and you have to decide now whether or not this technology is that silver bullet and how much investment it&#39;s going to take to get to that point.</p><p> <strong>Daniel Filan:</strong> I think that&#39;s right, and I think your take on this is going to be driven by your sense of the risk profile of building things that are significantly smarter than us. I guess from the fact that I made the AI X-risk Research Podcast, rather than the AI Everything&#39;s Going to be Great Research Podcast, people can guess my-</p><p> <strong>Elizabeth Seger:</strong> It&#39;s an indication of where you&#39;re coming from.</p><p> <strong>Daniel Filan:</strong> … take on this, but I don&#39;t know. I think it&#39;s a hard question. So, part of my take is, in terms of the underlying offense-defense balance, I think it becomes more clear when you&#39;re worried about, what should I say, silicogenic risks? Basically the AI itself coming up with issues rather than humans using AI to have nefarious schemes. Once you&#39;re worried about AI doing things on their own where you are not necessarily in control, there I think it makes sense that you&#39;re probably… If you&#39;re worried about not being able to control the AIs, you&#39;re probably not going to be able to solve the risks that the AIs are creating, right?</p><p> <strong>Elizabeth Seger:</strong> Yeah, your management plan for AI shouldn&#39;t be to build a slightly more powerful AI to manage your AI.</p><p> <strong>Daniel Filan:</strong> Well, if you knew that you were going to remain in control of the slightly bigger AI, maybe that&#39;s a plan, but you kind of want to know that.</p><p> <strong>Elizabeth Seger:</strong> I guess I was saying that if you&#39;re worried about loss of control scenarios, then the solution shouldn&#39;t be, “Well, let&#39;s build another system that&#39;s also out of our control, but just slightly better aligned to address the…” I feel like that&#39;s-</p><p> <strong>Daniel Filan:</strong> It&#39;s not the greatest. I think my colleague John Wentworth has <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies">some saying</a> , “Releasing Mothra to contain Godzilla is not going to increase property values in Tokyo,” which is a cute little line. I don&#39;t know, it&#39;s a hard question. I think it&#39;s hard to say anything very precise on the topic. I did want to go back to the offense-defense balance. So moving back a bit, a thing you said was something like, “Look, it&#39;s probably better to just prevent threats from arising, than it is to have someone make a pathogen and then have everyone race to create an antibiotic or antiviral or whatever. ” So, that&#39;s one way in which everyone having really advanced AI… That&#39;s one way that could look in order to deal with threats. I think another way does look a bit more like prevention. I don&#39;t know, it&#39;s also more dystopian sounding, but one thing that AI is good at is surveillance, right?</p><p> <strong>Elizabeth Seger:</strong> Yes.</p><p> <strong>Daniel Filan:</strong> Potentially, so you could imagine, “Look, we&#39;re just going to open source AI and what we&#39;re going to use the AI for is basically surveilling people to make sure the threats don&#39;t occur.” So, maybe one version of this is you just really amp up wastewater [testing]. Somehow you use your AI to just look at the wastewater and see if any new pathogens are arising. It could look more like you have a bunch of AIs that can detect if other people are trying to use AI to create superweapons or whatever, and stop them before they do somehow.</p><p> <strong>Elizabeth Seger:</strong> The wastewater example, that sounds great. We should probably do that anyway. In terms of surveilling to see how people are using AI systems using AI, why not just have the AI systems be behind an API where people can use the systems for a variety of downstream tasks integrating through this API and then the people who control the API can just see how the system is being used? Even if it can be used for a vast majority of tasks, even if you were to take all the safety filters off, the advantage of the API is still that you can see how it&#39;s being used. I don&#39;t know, I feel like that&#39;s-</p><h3> Making open-sourcing safer?<a name="making-os-safer"></a></h3><p> <strong>Daniel Filan:</strong> That seems like a good argument. All right, so I guess another question I have is related to the frame of the report. So in the report you&#39;re basically like, “open-sourcing has these benefits, but it also has these costs. What are ways of doing things other than open-sourcing that basically try and retain most of the benefits while getting rid of most of the costs?” You can imagine a parallel universe report where you say, “Okay, open-sourcing has these benefits, but it also has these costs. We&#39;re still going to open source, but we&#39;re going to do something differently in our open source plan that is going to retain benefits and reduce costs”, right? So one example of this is you open-source models, but you have some sort of watermarking or you have some sort of cryptographic backdoor that can stop models in their tracks or whatever. I&#39;m wondering: why the frame of alternatives to open-sourcing rather than making open-sourcing better?</p><p> <strong>Elizabeth Seger:</strong> Very simple. I think making open-sourcing better is the harder question, technically more difficult. I mean, for example, say you have watermarking, part of the issue with watermarking to identify artificially generated images is making sure the watermarks stick. How do you make sure that they are irremovable if you are going to open… This is a really complex technical question: how do you develop a system that has watermarked images where that watermark is irremovable if you were to open source the system?</p><p> I&#39;m not saying it&#39;s undoable. I personally don&#39;t have the technical background to comment very deeply on this. I have heard people talking about how possible this would be. It also depends how you watermark, right?</p><p> If you have just a line of inference code that says, “slap a watermark on this thing”, it could delete the line of inference code. If you&#39;re to train the system on only watermarked images, well now you have to retrain the entire system to get it to do something else, which is very expensive. So again, I think it depends how you do it.</p><p> I was at a meeting last week where people were talking about, are there ways we could build in a mechanism into the chips that run the systems that say “if some bit of code is removed or changed in the system, then the chip burns up and won&#39;t run the system”. I&#39;m not saying this is impossible, but [a] really interesting technical question, really difficult, definitely beyond my area of expertise. But I think if this is an approach we can take and say, there are ways to be able to open source the system and get all the benefits of open-sourcing by just open-sourcing and still mitigate the risks, I think that&#39;s great. I think it&#39;s just a lot more difficult.</p><p> And there&#39;s one aspect in which we do take the flip view in the report, and I think this is where we start talking about a staged release of models. You can undergo a staged release of a model where you put out a slightly smaller version of a model behind API, you study how it&#39;s being used. Maybe you take a pause, analyze how it was used, what [are] the most common avenues of attack, if at all, [that] were being used to try and misuse the model.</p><p> And then you release a slightly larger model, [then] a slightly larger model. You do this iteratively, and if you do this process, as you get to a stage where it&#39;s like, hey, we&#39;ve been doing the staged release of this model for however many months and no problems, looking good, there&#39;s no emergent capabilities that popped up that are making you worried. You didn&#39;t have to implement a bunch of safety restrictions to get people to stop doing unsafe things - okay, open source. This is not a binary [of] it has to be completely open or completely closed. And I think this is one respect… If you were to take this flip view of “how can we open source, but do it in the safest way possible?” Just open source slowly, take some time to actually study the impacts. And it&#39;s not like the only way to get a sense of how the system&#39;s going to be used is to just open source it and see what happens. You could do a staged release and study what those impacts are.</p><p> Again, it won&#39;t be perfect. You never know how it&#39;s going to be used 10 years down the road once someone gets access to all the weights and stuff. But it is possible to study and get some sort of insight. And I think one of the nice things about staged release is if you start doing the staged release process and you realize that at each iterative step you are having to put in a bunch of safety filters, for example, to prevent people from doing really shady stuff, that&#39;s probably a good indication that it&#39;s not ready to be open sourced in its current form, because those are safety filters that will just immediately be reversed once open sourced. So I think you can learn a lot from that.</p><p> So I think that&#39;s one way you can open source safely: find ways to actually study what the effects are before you open source, because that decision to open source is irreversible. And then I think the technical challenge of, are there ways we can have backstops, that we can technically build in irreversible, irremovable filters or watermarks or even just hardware challenges that we could implement - I think [those are] really interesting technical questions that I don&#39;t know enough about, but… Go for it. That&#39;d be a great world.</p><p> <strong>Daniel Filan:</strong> Yeah. If listeners are interested, this gets into some territory that <a href="https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html#watermarking-lm-outputs">I talked about with Scott Aaronson earlier this year</a> . Yeah, I think the classic difficulties, at least say for watermarking… I read <a href="https://arxiv.org/abs/2012.08726">one paper</a> that claims to be able to bake the watermark into the weights of the model. To be honest, I didn&#39;t actually understand how that works.</p><p> <strong>Elizabeth Seger:</strong> I think it has to do with how the model&#39;s trained. So the way I understand it is if you have a dataset of images that all have a watermark in that dataset, not watermark in the sense like you see on a $1 bill, but weird pixel stuff that the human eye can&#39;t see. If all the images in the training dataset have that watermark, then all of the images it produces will have that watermark. In that case, it&#39;s baked into the system because of how it was trained. So the only way to get rid of that watermark would be to retrain the system on images that don&#39;t contain the watermark.</p><p> <strong>Daniel Filan:</strong> Yeah, that&#39;s one possibility. So that&#39;s going to be a lot rougher for applying to text models, of course, if you want to just train on the whole internet. I think I saw <a href="https://arxiv.org/abs/2012.08726">something</a> that claimed to work even on cases where the dataset did not all have the watermark, but I didn&#39;t really understand how it worked. But at any rate, the key issue with these sort of watermarking methods is as long as there&#39;s one model that can basically paraphrase that does not have watermarking, then you can just take your watermark thing and basically launder it and get something that - if your paraphrasing model is good enough, you can create something that looks basically similar, it doesn&#39;t have the watermark, and then it&#39;s sad news.是的。进而-</p><p> <strong>Elizabeth Seger:</strong> Sorry, I was going to say there&#39;s similar [things] in terms of how doing something with one model allows you to jailbreak another model. I mean this is what happened with the <a href="https://arxiv.org/abs/2307.15043">&#39;Adversarial suffixes&#39; paper</a> , where using a couple open source models, one which was Llama 2, and using the weights of those models, figuring out a way to basically just throw a seemingly random string of numbers at a large language model, and then with that seemingly random range of numbers before the prompt basically get the system to do whatever you want. Except while they figured out how to do that using the weights accessible from Llama 2, it worked on all the other large language models. So finding a way to jailbreak one model and using the weights and access to one model, that could bring up vulnerabilities in tons of others that aren&#39;t open sourced as well. So I think that&#39;s another roughly related somewhat to what we were just talking about point.</p><p> <strong>Daniel Filan:</strong> Yeah, I guess it brings up this high level thing of whatever governance method for AI you want, you want it to be robust to some small fraction of things breaking the rules. You don&#39;t want the small fraction to poison the rest of the thing, which watermarking unfortunately has.</p><p> Yeah, I guess I wanted to say something brief about backdoors as well. So there is really a way of, at least in toy neural networks, and you can probably extend it to bigger neural networks, <a href="https://arxiv.org/abs/2204.06974">you really can introduce a backdoor that is cryptographically hard to detect</a> . So one problem is, how do you actually use this to prevent AI harm is not totally obvious. And then there&#39;s another issue of… I guess the second issue only comes up with super smart AI, but if you have a file on your computer that&#39;s like, “I implanted a backdoor in this model, the backdoor is this input”, then it&#39;s no longer cryptographically hard to find as long as somebody can break into your computer. Which hopefully is cryptographically hard, but I guess there are security vulnerabilities there.</p><p> So yeah, I wonder if you want to say a little bit about the safer ways to get the open source benefits. I&#39;ve given you a chance to talk about them a little bit, but is there anything more you want to say about those?</p><p> <strong>Elizabeth Seger:</strong> I think, not really. I think the overarching point is, just as I said before, when the risks are high - and I think that&#39;s key to remember, I&#39;m not saying don&#39;t open-source everything - when the risks are high, it is worth investing in seeing how else we can achieve the benefits of open-sourcing. Basically, if you&#39;re not going to open-source because the risks are high, then look into these other options. It&#39;s really about getting rid of this open versus closed dichotomy.</p><p> So many of the other options have to do with other options for sharing models, whether that&#39;s structured access behind API, even research API access, gated download, staged release, and then also more proactive efforts. Proactive efforts which can actually also be combined with open-sourcing. They don&#39;t have to be seen as an alternative to open-sourcing. So this is things like redistributing profits towards AI safety research or starting AI safety and bug bounty programs. Or even like we talked about with <a href="https://arxiv.org/abs/2303.12642">the democratization paper</a> , thinking about how we can democratize decision-making around AI systems to help distribute influence over AI away from large labs, which is another argument for open-sourcing.</p><p> So yeah, I think that this is key: there are other efforts that can be put in place to achieve many of the same benefits of open-sourcing and when the risks are high, it&#39;s worth really looking into these.</p><h2> AI governance research<a name="ai-gov"></a></h2><p> <strong>Daniel Filan:</strong> All right.好的。 So moving on, I want to talk a little bit more broadly about the field of AI governance research. So historically, this podcast is mostly focused on technical AI alignment research, and I imagine most listeners are more familiar with the technical side than with governance efforts.</p><p> <strong>Elizabeth Seger:</strong> In which case, I apologize for all my technical inaccuracies. One of the benefits of having 25 co-authors is that a lot of the technical questions I got to outsource.</p><h3> The state of the field<a name="state-of-field"></a></h3><p> <strong>Daniel Filan:</strong> Makes sense. Yeah, it&#39;s good to be interdisciplinary. So this is kind of a broad question, but how is AI governance going? What&#39;s the state of the field, if you can answer that briefly?</p><p> <strong>Elizabeth Seger:</strong> The state of the field of AI governance.是的。好的。 I&#39;ll try and answer that briefly. It&#39;s going well in that people are paying attention. In this respect, the release of ChatGPT I think was really great for AI governance because people, besides those of us already doing AI governance research, are really starting to see this as something valuable and important that needs to be talked about and [asking] questions around what role should governments play in regulating AI, if at all? How do we get this balance between governments and the developers? Who should be regulated with respect to different things? Do all of the responsibilities lie on the developers or is it on the deployers?</p><p> And all of these questions suddenly are coming to light and there&#39;s more general interest in them. And so we&#39;re seeing things like, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is happening next week, [a] global AI summit, looking at AI safety, really concerned about catastrophic and existential risks, trying to understand what kind of global institutions should be in place to govern AI systems, to evaluate AI systems, to audit, to regulate.</p><p> And this is bringing in countries from all over the world. I think it&#39;s something like 28 different countries are going to be at the UK AI Summit. You have <a href="https://artificialintelligenceact.eu/">the EU AI Act</a> where it started a while ago looking at narrow AI systems, but now is taking on foundation models and frontier AI systems and looking at open source regulation. And this has really, over the last year, exploded into a global conversation.</p><p> So in that respect, AI governance is going well in that people are paying attention. It&#39;s also very high stress because suddenly everyone&#39;s paying attention.我们必须做点什么。 But I think there&#39;s really genuine interest in getting this right, and I think that really bodes well. So I&#39;m excited to see where this next year goes. Yeah, there&#39;s talk about having this global AI summit and then making this a recurring series. And so I think it&#39;s going well in the sense that people are paying attention and the wheels are starting to turn, and that&#39;s cool.</p><h3> Open questions<a name="open-qs"></a></h3><p> <strong>Daniel Filan:</strong> Okay. I guess related to that, what do you see as the most important open questions in the field?</p><p> <strong>Elizabeth Seger:</strong> In the field of AI governance?好的。 So I think one big one is compute governance, which my colleague, <a href="https://heim.xyz/about/">Lennart Heim</a> , <a href="https://www.governance.ai/team/lennart-heim">works on</a> . This is just thinking about how compute is a lever for trying to regulate who is able to develop large models, even how compute should be distributed so that more people can distribute large models, but basically using compute as a lever to understand who has access to and who is able to develop different kinds of systems. So I think that&#39;s a huge area of research with a lot of growing interest because compute&#39;s one of the tangible things that we can actually control the flow of.</p><p> I think that the questions around model-sharing and open-sourcing are getting a lot of attention right now. Big open question, a lot of debate, like I said, it&#39;s becoming really quite a polarized discussion, so it&#39;s getting quite hard to cut through. But a lot of good groups [are] working on this, and I think a lot of interest in genuinely finding common ground to start working on this. I&#39;ve had a couple situations where I&#39;ve been in groups or workshops where we get people who are very pro-open source and other people who are just like, no, let&#39;s just shut down the whole AI system right now, really both sides of the spectrum coming together. And we try and find a middle ground on, okay, where do we agree? Is there a point where we agree? And very often we can come to a point of agreement around the idea that there may be some AI system, some model that poses risks that are too extreme for that model to be responsibly open sourced.</p><p> And that might not sound like that extreme of a statement, but when you have people coming from such polarized views to agree on the fact that there may exist a model one day that should not be open source, that is a starting point and you can start the conversation from there. And every group I&#39;ve been in so far has got to that point, and we can start working on that. So I think this model-sharing question is a big open question and lots of technical research needs to be done around benchmarking to decide, when are capabilities too dangerous?</p><p> Also around understanding what activities are actually possible given access to different combinations of model components. And that&#39;s actually not entirely clear, and we need a much more fine-grained understanding of what you can actually do given different kinds of model, combinations of model components, in order not only to have safe standards for model release and really a fine-grained standard for model release, but also to protect the benefits of open-sourcing. You don&#39;t want to just have a blanket “don&#39;t release anything” if you can get a lot of good benefit out of releasing certain model components. So I think a lot of technical research has to go into this.</p><p>反正。 So yeah, second point, I think model-sharing is a really big point of discussion right now. And then with the upcoming <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> , [there&#39;s] quite a bit of discourse around what international governance structures should look like for AI, a lot of different proposed models. And yeah, it&#39;ll be interesting to see what comes out of the summit. I don&#39;t think they&#39;re going to agree on anything amazing at the summit.已经两天了。 But I think for me, a really great outcome of the summit would be, first, recognition from everyone that AI systems could pose really extreme risks. So just a recognition of the risks. And then second, a plan going forward, a plan for how we can start establishing international systems of governance and really structure out when are we going to come to what kinds of decisions and how can we start putting something together. So I think that those are probably three key open questions, and the international governance structure one is really big right now too, just given the upcoming summit.</p><p> <strong>Daniel Filan:</strong> And I guess unless we get the editing and transcription for this episode done unusually quickly, listeners, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is almost definitely going to be in your past. So I guess listeners are in this interesting position of knowing how that all panned out in a way that we don&#39;t. So that was open questions in the field broadly. I&#39;m wondering for you personally as a researcher, what things are you most interested in looking at next?</p><p> <strong>Elizabeth Seger:</strong> Interesting. I mean, most of my life is taken up with follow-up on this open source report right now. So I definitely want to keep looking into questions around model-sharing and maybe setting responsible scaling policy, responsible model release policy.我不太确定。 I think I&#39;m in this place right now where I&#39;m trying to feel out where the most important work needs to be done and whether the best place for me to do is to encourage other people to do certain kinds of work where I don&#39;t necessarily have the expertise, like we were talking about, like needing more technical research into what is possible given access to different combinations of model components, or are there specific areas of research I could try to help lead in? Or whether really what needs to be done is just more organizational capacity around these issues.</p><p> So no, I am personally interested in keeping up with this model-sharing discussion. I think there&#39;s a lot of interesting work that needs to be done here, and it&#39;s a key thing that&#39;s being considered within the discussions around international AI governance right now. Yeah, so sorry I don&#39;t have as much of a clear cut answer there, but yeah, I&#39;m still reeling from having published this report and then everything that&#39;s coming off the back of it and just trying to feel out where&#39;s the next most important, most impactful step, what work needs to be done. So I guess if any of your listeners have really hot takes on “oh, this is what you should do next”, I guess, please tell me. It&#39;ll be very helpful.</p><p> <strong>Daniel Filan:</strong> How should they tell you if someone&#39;s just heard that and they&#39;re like, oh, I need to-</p><p> <strong>Elizabeth Seger:</strong> “I need to tell her now! She must know!” Yeah, so I mean, I have a <a href="https://elizabethseger.com/">website</a> where you could find a lot of my contact information, or you can always find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . I spend far too much time on LinkedIn these days. And also my email address happens to be on the open source report. So if you download the report, my email address is there.</p><p> <strong>Daniel Filan:</strong> What&#39;s the URL of your website?</p><p> <strong>Elizabeth Seger:</strong> <a href="https://elizabethseger.com/">ElizabethSeger.com</a> .</p><h3> Distinctive governance issues of x-risk<a name="xrisk-different"></a></h3><p> <strong>Daniel Filan:</strong> All right.好的。 Getting back to talking about governance in general, I&#39;m wondering… so I guess this is an x-risk-focused podcast. How, if at all, do you think governance research looks different when it&#39;s driven by concerns about x-risk mitigation versus other concerns you could have about AI governance?</p><p> <strong>Elizabeth Seger:</strong> Well, that&#39;s a good question.让我们来看看。 So the question is how does the governance research look different?</p><p> <strong>Daniel Filan:</strong> Yeah. What kinds of different questions might you focus on, or what kinds of different focuses would you have that would be driven by x-risk worries rather than by other things?</p><p> <strong>Elizabeth Seger:</strong> So this is something that I&#39;ve had to think about a lot in my own research development because I did not come into this area of research from an x-risk background interest. I came into it… I mean, honestly, I started in bioethics and then moved from bioethics looking at AI systems in healthcare and have sort of moved over into the AI governance space over a very long PhD program. And so here I am.</p><p> But I would say one of the things that I&#39;ve learned working in the space [that&#39;s] more interested in long-term x-risk impacts of AI and trying to prevent x-risks is really paying attention to causal pathways and really trying to be very critical about how likely a potential pathway is to actually lead to a risk. I don&#39;t know if I&#39;m explaining this very well.</p><p> Maybe a better way of saying it&#39;s like: if you have a hypothesis, or let&#39;s say you&#39;re worried about the impacts of AI systems on influence operations or impacting political campaigns, I find it really helpful to start from the hypothesis of, it won&#39;t have an impact. And really just trying to understand how that might be wrong, as opposed to trying to start from “oh, AI is going to pose a massive bio threat, or it&#39;s going to pose a massive threat to political operations” or something like that. And then almost trying to prove that conclusion.</p><p> Yeah, I don&#39;t know, I start from the opposite point and then try and think about all the ways in which I could be wrong. And I think this is really important to do, especially when you&#39;re doing x-risk research, whether it&#39;s with respect to AI or some other form of x-risk. Because I think there are a lot of people that turn off when you start talking about existential risks, they think it&#39;s too far out there, it&#39;s not really relevant to the important questions that are impacting people today, the tangible things that people are already suffering 。 And so I think it&#39;s really important to be very, very rigorous in your evaluations and have a very clear story of impact for why it is that you&#39;re doing the research you&#39;re doing and focusing on the issues that you&#39;re doing. At least that&#39;s been my experience, trying to transition into the space and work on these issues.</p><h3> Technical research to help governance<a name="tech-for-gov"></a></h3><p> <strong>Daniel Filan:</strong> Another question I have, related to my audience… So I think my audience, a lot of them are technical alignment researchers and there are various things they could do, and maybe they&#39;re interested in, okay, what work could technical alignment people do that would make AI governance better? I&#39;m wondering if you have thoughts on that question.</p><p> <strong>Elizabeth Seger:</strong> Okay. Technical alignment people, AI governance better. Yeah, I mean there&#39;s a lot of work going on right now, especially within the UK government. We just set up the <a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">UK AI Task Force</a> , a government institution doing a lot of model evals and alignment research. I think if you have the technical background in alignment research, you are very much needed in the governance space. There&#39;s very often a disconnect between… I mean, I am also guilty of this. There&#39;s a disconnect between the people doing the governance research and the people who have the experience with the technology and really know the ins and outs of the technology that&#39;s being developed.</p><p> And I think if you have the inclination to work in an AI governance space and help bridge that gap, that would be incredibly valuable. And like I&#39;ve already said, some of the more technical questions, even around open-sourcing, are things that I was very, very glad to have colleagues and co-authors on the paper who have worked for AI labs and stuff before and really knew what they were talking about and could advise and help write some of the more technical aspects of the report.</p><p> So I think if you have the inclination to work in the space, to get involved with governance efforts, or even maybe some of these government institutions that are starting to pop up that are working on the boundary of AI governance and technical research, that could be a really valuable place to contribute. So I think my 2 cents off the top of my brain would be help bridge that gap.</p><p> <strong>Daniel Filan:</strong> Okay.伟大的。 So before we wrap up, I&#39;m wondering if there&#39;s anything that you wish I&#39;d asked but that I didn&#39;t?</p><p> <strong>Elizabeth Seger:</strong> Oh, that&#39;s a good question.不，我不这么认为。 I think we&#39;ve covered a lot of good stuff. Yeah, thank you for having me on really. I&#39;d say there&#39;s nothing in particular.这太棒了。</p><h2> Following Elizabeth&#39;s research<a name="following-elizabeths-research"></a></h2><p> <strong>Daniel Filan:</strong> All right, so to wrap up then, if people are interested in following your research, following up on this podcast, how should they do that?</p><p> <strong>Elizabeth Seger:</strong> So I have my website, <a href="https://elizabethseger.com/">ElizabethSeger.com</a> . It sort of outlines my different ongoing research projects, has a lot of publications on it. Also, <a href="https://www.governance.ai/">GovAI&#39;s website</a> is a wealth of information [on] all things AI governance from all my great colleagues at GovAI and our affiliates. So really, yeah, there&#39;s new research reports being put out almost every week, maybe every other week, but really high quality stuff. So you can find a lot of my work on the GovAI website or my current work and past work on my own website or find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . Yeah, just happy to talk more.</p><p> <strong>Daniel Filan:</strong> All right, well thank you very much for being on the podcast.</p><p> <strong>Elizabeth Seger:</strong> Great, thank you.</p><p> <strong>Daniel Filan:</strong> This episode is edited by Jack Garrett and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a> and <a href="https://lightspeedgrants.org/">Lightspeed Grants</a> , along with <a href="https://patreon.com/axrpodcast">patrons</a> such as Tor Barstad, Alexey Malafeev, and Ben Weinstein-Raun. To read a transcript of this episode or to learn how to <a href="https://axrp.net/supporting-the-podcast/">support the podcast yourself</a> , you can visit <a href="https://axrp.net">axrp.net</a> . Finally, if you have any feedback about this podcast, you can email me at <a href="mailto:feedback@axrp.net">feedback@axrp.net</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger<guid ispermalink="false"> RDm26xAcb9rfvuBya</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:07 GMT</pubDate> </item><item><title><![CDATA[Solving Two-Sided Adverse Selection with Prediction Market Matchmaking]]></title><description><![CDATA[Published on November 26, 2023 8:10 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/s2zbbqpeaxpoy4rr2jld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/xff9jzs0fgp7u21ciaq2 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/rh8uwt6btzrqhnapovq0 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/os4dc1r67s52jnlmy1lz 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/esdpc5afazjyasnzdaos 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ou8et2puwea0fhemlh9c 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/d3satrakwyr7bajmwaii 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/umzxc8fjv0hojb4029yp 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ti6ciszrdxr8tf4wpvdq 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/sygzudsyigugf2av63wi 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/n4tgv9uhgzzhjdlkzzpz 1500w"><figcaption> Market making might maximize meaningful matches, like this pair of pears.</figcaption></figure><h1> 0: Navigation</h1><p> I&#39;m aiming for a reader who knows what “prediction markets” and “adverse selection” are, who likes the first and not the second, and who enjoys systems that have neatly aligned incentives. For a primer on prediction markets, read Scott Alexander&#39;s <a href="https://www.astralcodexten.com/p/prediction-market-faq?ref=brasstacks.blog"><u>FAQ</u></a> . If you&#39;re already familiar with <a href="https://www.brasstacks.blog/pm-matchmaking/manifold.love"><u>Manifold Love</u></a> , skip to section 2.</p><p> COI: I&#39;ve done <a href="http://manifestconference.net/?ref=brasstacks.blog"><u>some work</u></a> for, might do some more work for, and own a tiny bit of equity in <a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a> . I&#39;m writing this independent of any work I&#39;m currently doing or planning to do for Manifold or for any other entity. I just think the ideas are cool.</p><h1> 1: Love</h1><p> <a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a> , a play-money prediction market platform, recently released a prediction market dating app called “ <a href="http://manifoldlove.com/?ref=brasstacks.blog"><u>Manifold Love</u></a> .” Users — those seeking love — sign up and fill out their profile like a regular dating app. Matchmakers — some of whom are users of the app themselves — pair users up. After a matchmaker makes a match, prediction markets are automatically created on various benchmarks of the pair&#39;s (potential) relationship. </p><p><img style="width:39.59%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/l1ccolmjl6vrdipnviao" alt=""><img style="width:40.15%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/zc7yymewfsr49fqa3ksu" alt=""><img style="width:42.65%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/tjknavwqh60xxe82sqje" alt=""><img style="width:37.39%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/qjiqij5jrbtd2twivi1h" alt=""></p><p> Screenshots of some markets on Manifold Love.</p><p> People make bets (using <a href="https://docs.manifold.markets/faq?ref=brasstacks.blog#what-is-mana-m"><u>play-money</u></a> ) on whether the pair will go on a first date; conditional on that first date happening, a second date; conditional on the second date happening, a third; conditional on the third date happening, a 6 month relationship. The idea is that <strong>you should go on dates with matches who have the highest chance of leading to subsequent dates, and eventually turning into a relationship.</strong> Instead of guessing who that&#39;ll be, you can just check the prediction markets.</p><h1> 2: Generalizing</h1><p> Importantly, Manifold Love&#39;s setup isn&#39;t limited to solving problems in the dating market. It solves (or a least, has the potential to solve) problems in <i>all</i> scenarios in which there&#39;s two-sided adverse selection, where each side of a two-sided market selects for something disfavorable by the other side&#39;s lights. In the words of Groucho Marx, “I don&#39;t want to belong to any [country] club that would accept me as a member.” If a club wants Groucho Marx, it&#39;s probably not a very good club, and vice versa — if Groucho Marx wants to be in a particular club, he&#39;s probably not a very desirable member.</p><p> More generally, the setup of &quot;run a bunch of conditional prediction markets on a bunch of key benchmarks for potential pairs between two sides that are normally caught in adverse selection&quot; seems like it could work pretty well.</p><p> One of the classic cases of two-sided adverse selection is the labor market, so here&#39;s how a sort of “Manifold Jobs” might play out. The platform has three entities:</p><ol><li> Prospective employees</li><li> Prospective employers</li><li> Headhunters</li></ol><p> The platform makes a bunch of conditional prediction markets on key benchmarks for each of the first two entities.例如：</p><ul><li> Conditional on being hired by [prospective employer], will [prospective employee]:<ul><li> still be at their job in [timeframe]?</li><li> have a higher weekly average life satisfaction rating in [timeframe] than they do now?</li><li> ETC。</li></ul></li><li> Conditional on hiring [prospective employee], will [prospective employer]:<ul><li> still be employing [employee] in [timeframe]?</li><li> have a higher weekly average employee rating in [timeframe] than the previous employee did?</li><li> ETC。</li></ul></li></ul><p> <a href="https://manifold.markets/TonyBaloney/will-manifold-be-used-for-job-recru?ref=brasstacks.blog"><u>If Manifold actually makes this</u></a> , I&#39;m sure it&#39;ll look somewhat different, much better, and far more fleshed out. But importantly, the dating and labor markets aren&#39;t the only two landscapes of two-sided adverse selection.</p><h1> 3: Insert Two-Sided Adverse Selection Here</h1><p> There are a bunch of other landscapes of two-sided adverse selection where the conditional prediction market setup from Manifold Love could potentially solve a lot of problems:</p><ul><li> dating (Manifold Love)</li><li>友谊</li><li>gym partners</li><li> grantmaking</li><li> college applications/decisions</li><li> grad school apps/decisions (eg med school, law school, business school, etc)</li><li> labor/hiring/talent-seeking/jobs (as in section 2)</li><li> cofounders</li><li> seed- and preseed-stage investing</li><li> child adoption</li><li> internship applications/decisions</li><li> used car sales</li><li>辅导</li><li>events/venue spaces</li><li> therapists/patients</li><li> selling &amp; buying insurance</li><li> credit/lending</li><li> residential &amp; commercial real estate</li><li> students picking classes at college</li></ul><p> If you can think of more, <a href="https://www.brasstacks.blog/pm-matchmaking/saulmunn.com/contact"><u>tell me</u></a> and I&#39;ll add them here! Something to note: for most of the above, there&#39;s already some entity that acts as a middleman — real estate has realtors, the labor market has headhunters, adoption agencies pair up biological mothers with adopted families, etc. But all of them are pretty broken, biased, or at least wildly suboptimal. I&#39;m excited about the potential of conditional prediction markets to improve on them and solve two-sided adverse selection.</p><h1> 4: Caveats</h1><ul><li> As of my writing, I&#39;m a lowly undergrad who&#39;s never taken an economics class and has no idea what he&#39;s talking about. I&#39;ve done a <a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>bit</u></a> of <a href="https://www.brasstacks.blog/pm-matchmaking/opticforecasting.com"><u>work</u></a> in the prediction market &amp; forecasting community, but I&#39;m nowhere near an expert.</li><li> We don&#39;t even know if Manifold Love works with dating, let alone if it generalizes to other systems of two-sided adverse selection.</li><li> <strong>Conditional prediction markets without prior commitment to randomization do not imply causation</strong> (a la <a href="https://dynomight.net/prediction-market-causation?ref=brasstacks.blog"><u>DYNOMIGHT</u></a> ). This setup will give us association/correlation, but <strong>we won&#39;t know the existence nor direction of causality without prior commitment to randomization</strong> . And prior commitment to even some small amounts of randomization for some of these systems ranges from &quot;quite difficult&quot; to &quot;hahaha good one, <i>absolutely fucking not</i> .&quot; Fortunately, I&#39;m not sure you need to know causation, at least not to start out. I&#39;m curious to see what happens if/when Manifold Love runs into this problem.</li><li> Again, COI: I&#39;ve done <a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>work</u></a> for, might do more work for, and own a tiny bit of equity in <a href="https://www.brasstacks.blog/pm-matchmaking/manifold.markets"><u>Manifold</u></a> .</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two-sided-adverse-selection-with-prediction-market#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two-sided-adverse-selection-with-prediction-market<guid ispermalink="false"> 99WwKnkE2FKAFo2ap</guid><dc:creator><![CDATA[Saul Munn]]></dc:creator><pubDate> Sun, 26 Nov 2023 20:10:23 GMT</pubDate> </item><item><title><![CDATA[Wikipedia is not so great, and what can be done about it.]]></title><description><![CDATA[Published on November 26, 2023 7:13 PM GMT<br/><br/><p> <i>Note: This post originally appeared in Reddit&#39;s /r/trueunpopularopinion sub as</i> <a href="https://old.reddit.com/r/TrueUnpopularOpinion/comments/zieyyf/wikipedia_is_not_so_great_and_is_overrated/"><i>Wikipedia is not so great, and is overrated</i></a> <i>written by a user there. It is explicitly released under CC0 public domain by them.</i></p><p> You all have heard by now that Elon Musk said that Wikipedia has a &quot;left wing bias&quot; when the article about Twitter Files had been suggested for deletion. This has been received with mixed responses from liberals and conservatives alike; the former dismissing it as &quot;an attack on free knowledge&quot; and the latter cheering the move as &quot;against censorship&quot; and vindication of their beliefs that Big Tech is biased against them.</p><p> True, Wikipedia is supposedly editable by anyone around the world and I had been an on and off editor there for years mostly doing small-ish edits like fixing typos and reverting obvious vandalism. This is done while on IP as opposed to using accounts because I would rather that some edits (ie sensitive topics like religious and political areas) not tied to my name and identity. However, reality is far from the preferred sugar-coated description of Wikipedia, particularly its editing community.</p><p> The editing community in overall is best described as a slightly hierarchical and militaristic &quot;do everything right&quot; structure, traditionally associated with Dell and recently Foxconn and now-defunct Theranos. Exceptions apply in quieter and outlier areas such as local geography and space, usually the top entry points for new users wanting to try their first hand. There are higher tolerance of good-faith mistakes such as point-of-view problems and using unreliable resources, which are usually explained in detail on how to correct by them rather than a mere warning template or even an abrupt block.</p><p> Ultimately those sub-communities which can be said as populated by <a href="https://meta.wikimedia.org/wiki/Exopedianism">exopedians</a> , have relatively little to no power over the wider and core communities, mostly dominated by <a href="https://meta.wikimedia.org/wiki/Metapedianism">metapedians</a> . A third group called <a href="https://meta.wikimedia.org/wiki/Mesopedianism">mesopedians</a> often alternates between these inner and outer workings. Communities can have shared topical interest which are grouped by <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject">WikiProject</a> , an example being <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Science">WikiProject Science</a></p><p> I spend a lot of time casually browsing through edit wars (can be so <a href="https://en.wikipedia.org/wiki/Wikipedia:Lamest_edit_wars">lame</a> at times) like a fly on the wall, along with meta venues of Wikipedia such as <a href="https://en.wikipedia.org/wiki/WP:AFD">Articles for Deletion</a> , <a href="https://en.wikipedia.org/wiki/Wikipedia:Centralized_discussion">Centralized discussion</a> <a href="https://en.wikipedia.org/wiki/WP:NPOVN">Neutral Point of View Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:BLPN">Biographical of Living Persons Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:COIN">Conflict of Interest Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:ANI">Administrator&#39;s Noticeboard Incidents</a> , <a href="https://en.wikipedia.org/wiki/WP:SPI">Sockpuppet investigations</a> , <a href="https://en.wikipedia.org/wiki/Wikipedia:Arbitration_Committee/Noticeboard">Arbitration Committee noticeboard</a> which is the &quot;supreme court&quot; in Wikipedia community for serious behavioral and conduct disputes. Therefore I can sum up how the editing community really functions, although not really as extensive as you might expect because I am not a &quot;Wikipedioholic&quot; with respect to inner workings.</p><h1> <strong>Deletionism and inclusionism</strong></h1><p> This has been very perennial and core reasons for just about any disputes on Wikipedia ever. Deletionists treat Wikipedia as another &quot;regular encyclopedia&quot; where information has to be limited once it become very much to be covered; like cutting out junk, while inclusionists treats Wikipedia as a comprehensive encyclopedia not bound by papers and thus can afford to cover as much information as it can take; one man&#39;s junk could be another man&#39;s treasure. Personally I support the latter and often the conflict between two editing ideologies leads to factionalism, where attempts to understand mutual feelings and perspectives are inadequate or even none at all.</p><p> There are no absolute standards of what defines &quot;encyclopedic knowledge&quot; and &quot;notability&quot;. Inclusionism posits that almost everything could become valuable and encyclopedic in the future, even if they&#39;re aren&#39;t today. An example I can think of is events, figures and stories from World War II. Deletionism has been closely related to &quot;academic standard kicks&quot; and rely on the premise that Wikipedia has to be of high standard and concise. There are people who deem an addition of something as useful, and there are those who think it&#39;s &quot;trivia&quot; or &quot;crufty&quot; something that is nominally discouraged if not prohibited by Wikipedia&#39;s documentation (see <a href="https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_an_indiscriminate_collection_of_information">this</a> in particular, although sometimes exceptions are applied through the spirit of &quot; <a href="https://en.wikipedia.org/wiki/WP:IAR">Ignoring all rules for sake of improvement</a> &quot;, which are frequent at entertainment and gaming topics).</p><p> On pages, notability debates around a person subject and otherwise are frequently the main point of discussion in Articles for Deletion threads, where articles deemed not substantial enough (such as very few sources) are suggested for deletion. Usually they will run for a week but they can be quickly closed if there are too many votes in favor of &quot;keep&quot;, &quot;delete&quot; and so on, the AFD nomination is withdrawn by the initiator, or that the nomination is found to have been done in bad faith (such as to &quot;censor&quot; articles from public view for questionable motives like ideology, paid editing or so).</p><p> Here I believe that deletionists are seen far more harshly by inclusionists, than the vice versa. The chief reason is to add something, you have to navigate through the user experience unfriendly editing interfaces (although somewhat improved in recent years) all the while having to scroll through the internet to find sources and references to add. When you found some you have to go through an extra hoop to assess whether they are reliable or not, before finally transcribing the information through your own words which has to stick to the neutral point of view (NPOV) policy; paraphrasing that are so close are not allowed because, copyright. Non-English speaking editors would often find the latter very difficult.</p><p> In contrast, as per an old adage, destroying something is easier than building something, deletions are comparatively easier than addition. This could be the reason why deletionism currently maintains dominance over the whole site as I see it, since in order to become an established an esteemed editor, one has to garner a high amount of edits which are not reverted. Thus, many editors like to gain these &quot;scores&quot; by deleting &quot;unuseful information&quot; from passages up to entire articles by interpreting the documentations and rules strictly, the latter through processes such as Articles for Deletion and if confident enough, <a href="https://en.wikipedia.org/wiki/Wikipedia:Proposed_deletion">Proposed Deletion</a> that doesn&#39;t require discussion. Simply speaking, it&#39;s a feature not a bug and aren&#39;t necessarily beholden to any political ideology; a liberal is as equally likely as a conservative to become a hated deletionist.</p><p> Even though every edit changes are recorded and displayed through page histories which you can see for any given articles by clicking &quot;View History&quot; at the top, the bone of contention remains particularly when page deletions results in the redaction of these histories from public view. This will be explained further later.</p><p> Some historical contexts that can be think of regarding the current prominence of deletionism are the excessive amount of Pokemon pages during or before 2007 which had alienated some readers and editors alike because search engines back then are not quite as adequate as today in terms of finding precise信息。 Another is that child predators like Nathan Larson used to sneak in as inclusionists to warp Wikipedia to fit their agenda all the time, which are indelibly horrendous to all of us here and those back then. Think of the poisoning of the well and the fruits from a poisonous tree. Furthermore there are also large portion of userbases from tech companies like Intel and those from the academic world (maybe instead of GLAMs, short for galleries, libraries, archives and museums) that gained top positions such as administrators, bringing along their work culture and so-called &quot;academic standards kick&quot; respectively. To be absolutely fair, I find that there are instances where deletionism is right enough, specifically the removal of copyright violation and libel materials on biographical pages of any living persons.</p><p> Regardless of whether a page is deleted or not, they remain available in Wikipedia&#39;s servers and accessible to administrators or higher only.</p><p> Eventually, what defines as &quot;encyclopedic knowledge&quot; are vulnerable to <a href="https://en.wikipedia.org/wiki/Wikipedia:Systemic_bias">systemic biases</a> as well. Different from some Musk&#39;s thoughts about it, users who are white, male, US/UK/CA/EU/AU/NZ, middle or old aged, and English speaker tend to have the greatest advantage above the rest in the editing community. With this in mind, a prominent musical artist in Zambia may be treated as too small-bore enough for a page on Wikipedia by an editor in Canada. Shopping malls in the US are less likely to be deleted than those in Vietnam. Such a bias doesn&#39;t go one way; the hypothetical artist in Zambia would be &quot;unimportant&quot; to someone in Peru.</p><p> This is the top causes of animosity between editors and also why many editors chose to quit or rather fell from grace. You will always hate that kid who like to ruin your LEGO structure every time you have assembled the blocks.</p><h1><strong>中立观点</strong></h1><p>Different from mere deletions and additions, this normally means that how to present a given information in a way to the readers ideally so that no disproportionate biases towards or against something are left in their impressions. You see arguments and conflicts concerning such a lot in political articles, historical articles and geography topics of areas under dispute from two or more nations. Say that a political figure is engaged in activities that are remotely linked to extremism. Side A would argue that the figure is therefore an extremist and it should be made prominent on that page and any other linked pages, but Side B wants to tone it down by writing it something like &quot;Political figure was engaged in activities which were sometimes reported by some as extremist&quot; and limit it to a mere mention on a single page. Another is a nation should be said as a &quot;partially recognized state&quot; because some UN members don&#39;t recognize it as such and instead as part of a bigger country, with others expressing views that simply having an effective sovereignty for its own and different from another nations would be enough to be deemed as a state.</p><p> It can come into play on cases involving &quot;fringe theories&quot; as well, like Bigfoots, UFOs and medical treatments, although Wikipedia indeed has a preference of giving prominence to mainstream views in these cases, something I don&#39;t find a problem with and is quite different from regular harmful biases.</p><p> Venues for resolution in this case are Neutral Point of View noticeboard, along with <a href="https://en.wikipedia.org/wiki/WP:RFC">Request for Comment</a> . The latter entails a process where a notice is put up in a centralized noticeboard all the while a pool of experienced/established editors receive notifications to comment, provide insights and make suggestions on a given issue. A month is usually on how these discussions are up and running unless there is a need of extension because of reasons such as unbroken deadlock.</p><p> Along with deletionism and inclusionism, this is a major cause of editors &quot;going naughty&quot; and getting blocked/banned/kicked out, whether for right or spurious reasons.</p><h1><strong>执行</strong></h1><p>The most important part of this post in my honest opinion. I&#39;ll start this section by writing about edit war. Usually when you change something in Wikipedia and it was undone/reverted by somebody else, then you have only two tries before you get reported to the <a href="https://en.wikipedia.org/wiki/WP:ANEW">edit-war noticeboard</a> if you&#39;re stubborn enough not to go to the article&#39;s talk page (&quot;Talk&quot; in the top left) for discussion, either by the person undoing your edits or by a third party. In the meantime you get notifications on your personal talk pages (&quot;Talk&quot; on the top right) inviting you for such discussion and if lucky enough, the <a href="https://en.wikipedia.org/wiki/WP:TEAHOUSE">Wikipedia Teahouse</a> for further help by some kind-hearted editors, increasingly a rarity these days. In some quieter or outer areas where as said before are slightly lenient, you may get up to approx. five chances counting your original edit before getting referred to the admins.</p><p> The tries count are reset after 24 hours but can be retained further just as a guard against &quot;gaming the rules&quot;. Clearer cut vandalism (like putting gibberish such as &quot;LOLOLOLOLOLOLOL&quot; at any pages) usually gets reported to a <a href="https://en.wikipedia.org/wiki/WP:AIV">separate noticeboard for administrators to intervene</a> , although first time vandals regularly get warnings on their talk pages beforehand. When a report is there and if found guilty of edit-warring, administrators would either give ultimatums to the users in question or block their accounts for a day. They could escalate to multiple days, weeks and up to indefinite (practically infinite) period should the behavior continues beyond that. The same goes for vandalism, although they are dealt more harshly with many prompt indefinite blocks (indeffs) for &quot;vandalism-only accounts&quot;.</p><p> Regular editors can be in danger of falling from grace too either by themselves or by others. Because Wikipedia is commonly seen by so many as the biggest comprehensive encyclopedia in the world, sometimes equated to history itself, many vested interests, feelings and sentiments have been invested on the website.</p><p> Those who are nationalists or otherwise fanatics of any imaginable notions found themselves having incentives to make Wikipedia to support their narratives both as an end itself or rather just means for other ends such as &quot;proving that they&#39;re great in the long annals of great history ”。 The same applies to run off the mill &quot;promotional editing&quot; by corporations and individuals, along with those made by their supporters or fans. On the opposite many people find it extremely attractive to twist it to denigrate any ideologies, corporations, people, and just about anything they personally oppose. For instance, they can make an article and fill it with disparaging information against them, which is called an &quot;attack page&quot;.</p><p> I find that there are kernels of truth in the commonly-held viewpoint that &quot;Wikipedia is a placeholder of information&quot; and that &quot;Wikipedia is history&quot;. A <a href="https://news.mit.edu/2022/study-finds-wikipedia-influences-judicial-behavior-0727">MIT report</a> described how judges&#39; behavior are increasingly influenced by Wikipedia articles, while there are initiatives by space missions such as <a href="https://mashable.com/article/moon-library-beresheet-crash-wikipedia">Beresheet</a> and <a href="https://www.archmission.org/lunar-library-2">Peregrine</a> to perform civilizational backups of humanity with all of English Wikipedia (version as of a given date) in the <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">event of坍塌</a>。</p><p> After having their way, to keep their changes forever in &quot;annals of history&quot; or simply the &quot;placeholders of information&quot; in general, gate-keeping measures are utilized. A simple example would be using excessively harsh language against editors who made a change challenging a given status quo. In contrast, if anybody has a reason to radically change a page and make sure it stays unassailable afterwards, the same set of actions are used too but arguably these would be &quot;antigatekeeping&quot; measures instead.</p><p> In gatekeeping/antigatekeeping one would resort to different levels of intepretation regarding <a href="https://en.wikipedia.org/wiki/WP:PAG">PAGs</a> (policies and guidelines) and <a href="https://en.wikipedia.org/wiki/WP:ESSAY">user essays</a> , the latter sometimes used as a basis of many editorial and administrative actions. The documentations can often contradict each other, like how &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:NOTINDISCRIMINATE">not indiscriminate</a> &quot; is to &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:NOTPAPER">not a paper encyclopedia</a> &quot;, and on top of all, can be overruled by <a href="https://en.wikipedia.org/wiki/Wikipedia:IAR">ignoring these</a> if anybody sees fit. Hence, whoever has the &quot;biggest fist&quot; gets to be the most advantageous in Wikipedia community. In order to have the &quot;biggest fist&quot;, they can befriend anyone sharing interests with their own and form a cabal/gang that look after their own. To increase their power and when enough time had passed they can nominate each other for <a href="https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship">administrator positions</a> giving them extra privileges of blocking users, deleting pages, protecting an article from editing by lower-ranked users. You don&#39;t get paid for spending your efforts and time on editing Wikipedia unless perhaps you&#39;ve listed a Venmo link or a crypto address on your user profile, and these administrative tools alone are so addictive and appealing given that you are essentially in control of the important bits of &quot;writing history&quot; if you have these, apart from usual human nature. Wikipedia is among the top 10 visited websites in the world after all.</p><p> Even more, there are additional ranks above administrator positions. Two of those are CheckUsers (CU) and Oversighters. CU has the power to look through IP address used by an account to see if it was a sockpuppet account of a person, while Oversighters have super-delete rights to hide contents or pages, even beyond the reach of administrators.</p><p> Those on the other end of the power-tripping, gate-keeping and so on rarely fares well. One would find them belittled, bullied by those editors. Should they attempt to properly resolve an issue through established processes such as talk page discussions, <a href="https://en.wikipedia.org/wiki/WP:DRN">dispute resolution noticeboard</a> , and up to the infamous Administrator&#39;s Noticeboard Incidents (ANI), they would expect to find obstructions upon obstructions along the way. If the victim decides to invite other editors to give balanced/impartial opinions and suggestions on a problem they would find themselves stonewalled on the grounds that these are &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Canvassing">canvassing</a> &quot;. It can be quite hypocritical if the &quot;bully&quot; had their gang friends informed beforehand, which is reasonably believed to often be the case. Finally, if it escalates into the ANI, this is where it start to get out of hand.</p><p> The reason why I use the term &quot;infamous&quot; is because ANI is the mother-lode of all kinds of ugly dramas. It is frequently the first place in getting an editor sanctioned or so on. The bullies (I do not use the term lightly) would then put all sorts of allegations and aspersions against other for any types of wrongdoing, whether real or perceived, big or small, or whether the result is a real harm or just a nothing burger 。 Regardless, if they twisted the rules (derisively referred as &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikilawyering">wikilawyering</a> &quot; or otherwise &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Gaming_the_system">gaming the system</a> &quot;) and played the victim good enough, the passing administrators would then close the discussion and place administrative actions against the &quot;real&quot; victim. Common egregious example of such an action is the &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Here_to_build_an_encyclopedia#Clearly_not_being_here_to_build_an_encyclopedia">not here to build an encyclopedia</a> &quot; indefinite/permanent block that can be arbitrary interpreted from any given actions. It&#39;s ironic given that the bullies are guilty of such as well. A prime example of twisting the rules to railroad/squeeze out other editors would start with so-called <a href="https://en.wikipedia.org/w/index.php?title=Wikipedia:BADFAITHNEG&amp;redirect=no">bad faith negotiation</a> , where they promised a victim not to remove content at other pages if the victim lets the bully keep their changes in a page. Soon the bully reneged it and when confronted by the victim the bully immediately accused them of being &quot; <a href="https://en.wikipedia.org/wiki/WP:TENDENTIOUS">tendentious</a> &quot; or &quot;POV pusher&quot;.</p><p> The bullies, which can consist of most editors operating at the inner workings, aren&#39;t necessarily beholden to any ideologies and come in all stripes. The only attribute that they all share is the addiction to power.</p><p> After such permablocks, most would be forced to leave it for good, further <a href="https://en.wikipedia.org/wiki/Wikipedia:Why_is_Wikipedia_losing_contributors_-_Thinking_about_remedies">bleeding the editors numbers</a> . Still, because Wikipedia&#39;s so preeminent and no viable competitors are currently available, some would rather stay behind, disguise their identity and either continue editing or start over in different areas. For those with knowledge of foreign languages, they could simply switch to other language Wikipedias to continue their work far from most perturbances. A smaller number would come back as vandals to spite editors who had wronged them.</p><p> This is where &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations">sockpuppetry investigations</a> &quot; kick in, mostly referred as SPI. Editors go there to start a new case if they suspect that an account is an alt/sock account of someone else particularly users who evaded the blocks/bans. When a user is blocked or banned for good, they are relegated to a pariah status much akin to &quot;unpersoning&quot;, Scientology&#39;s <a href="https://en.wikipedia.org/wiki/Suppressive_Person">suppressive persons</a> , and the lowest ones in North Korea&#39;s <a href="https://en.wikipedia.org/wiki/Songbun">Songbun</a> , in the respect that any and all edits by them under other accounts or IPs are liable to be reverted/undone pursuant to <a href="https://en.wikipedia.org/wiki/Wikipedia:BE">policy pertaining to block evasion</a> . While the original goal of not separating the wheat from the chaff is expressedly to prevent them from <a href="https://en.wikipedia.org/wiki/Wikipedia:Deny_recognition">gaining further recognition</a> and diminish the spirit of the block, in practice this means a Monkey&#39;s Paw that any further potential good contributions from them would be lost forever, handicapping the improvement of encyclopedia as a whole in a way or more. Other editors have the exception from edit-war policy to revert and undone any changes from the violators of the blocks, perhaps as well as anybody who helped them. In effect this is like what the Meatball Wiki said, a &quot; <a href="http://meatballwiki.org/wiki/PunishReputation">PunishReputation</a> &quot;.</p><p> During a SPI, there are &quot;clerks&quot; who will look through the user&#39;s contribution history to see if there is a similarity in pattern to warrant a block for abuse of multiple accounts (sockpuppetry). If that alone is not enough, the CheckUsers can then be called upon to check and compare the IP used by the accounts.</p><p> If a user is determined to have engaged in sockpuppetry, the userpage of original and alt accounts used are then replaced with a scarlet letter notice such as <a href="https://en.wikipedia.org/wiki/User:PositiveIntentsOnly">this example</a> boasting that which sock account belongs to who and therefore blocked. Forget about &quot;denying recognition&quot;, this is simply a punitive name-and-shame.</p><p> The SPI case, now listing the accounts and IP used, would then be archived in a separate page, still publicly viewable. This is despite recent GDPR regulations and the implication that major privacy-improving adjustments should&#39;ve been made for the process while keeping it viable. Try that in Reddit and you&#39;d be instantly banned for doxxing, I can assure you.</p><p> In there you can effectively cosplay as a CSI although substantive attention are given to clerks, administrators and CheckUsers. Keep in mind that the results and outcomes of most if not all sockpuppet investigations aren&#39;t really 100% accurate, given that there are a lot of unforeseen variables such as the imitation of writing and behavior styles that are mostly a result of multiple people pushing any particular editorial change for any reasons ie brother helping his sister, along with the use of software that can mask your IP addresses such as VPNs and TeamViewer. Those admins in charge of sockpuppetry investigations often aren&#39;t privy to the root cause of a &quot;sockpuppetry&quot; or &quot;block evasion&quot; and as such tend to for example, underestimate the amount of users who has the right reasons to support an edit made in violation of a block.</p><p> VPN IP addresses, which are used for obvious privacy reasons, are blocked in sight by any administrators pursuant to <a href="https://en.wikipedia.org/wiki/Wikipedia:Blocking_policy#Open_or_anonymous_proxies">policy against open proxies</a> . They even have a dedicated WikiProject and a bot specializing in finding and blocking these proxies, with the result being a great inconvenience for people wishing to edit from countries such as Russia and China.</p><p> In time, if someone continues a behavior the other editors deemed as &quot;disruptive&quot; or &quot;vandal&quot; past the initial block, they end up getting displayed in so-called &quot; <a href="https://en.wikipedia.org/wiki/WP:LTA">Long Term Abuse</a> &quot; caselist. Right there, their accounts and/or IP addresses, along with a likely-skewed description of what they&#39;ve done were listed out. The places they&#39;ve been and accounts outside of Wikipedia were frequently exposed there, as if it&#39;s an opposition research and spiteful doxxing. Things that&#39;ll get you quickly banned here are just a normal Tuesday over at Wikipedia, with GDPR out of the window.</p><p> As I see it, there are two categories of LTAs/vandals/whatever you call it. The first are the inherent vandals who had been problematic and disruptive for Wikipedia upon their first edit, and the other are those who had been regular or good standing users in the past until their fall from grace, normally caused by themselves such as being too overworked over one thing but could be by others, like the bullying example.</p><p> There is a reasonable possibility that some of those LTAs/vandals would be redeemed and become a good editor once again if enough diplomacy and mediation were tried. However, those would be a time-consuming process compared to simply actioning them, and I reasonably suspect that some of those are intentionally provoked by corrupt admins or their friends into vandal or disruptive editing in order for them to increase that admin actions count so as to further their own standing in the community, and to stay away from losing their cherished tools if their KPI <a href="https://en.wikipedia.org/wiki/Wikipedia:Administrators#Procedural_removal_for_inactive_administrators">fell low enough</a> in a given period.</p><p> It&#39;s fearful that the cycle of toxicities in Wikipedia could eventually led to real-world harm, though I will not further speculate how that might transpire for fear of <a href="https://en.wikipedia.org/wiki/WP:BEANS">stuffing the beans</a> and giving bad ideas. However, VICE had reported in 2016 that an editor had nearly <a href="https://www.vice.com/en/article/4xangm/wikipedia-editor-says-sites-toxic-community-has-him-contemplating-suicide">driven to suicide</a> after being subjected to online abuse by the editors despite what the documentation say about community collegiality. Furthermore, just before Musk&#39; comment against Wikipedia, the Anonymous group hacked a <a href="https://www.taipeitimes.com/News/taiwan/archives/2022/11/02/2003788129">Chinese ministry site and a satellite system</a> out of the suspicion that a state actor has manipulated Wikipedia&#39;s system and process to censor information about their hacking activities against China. It was a hot news in Taiwan then.</p><h1><strong>事后的想法</strong></h1><p>Theoretically a deep and comprehensive reform is past due for Wikipedia in order to (re-)foster collegiality among the members of Wikipedia community and reduce the amount of synergies that leads to intractable conflicts, as opposed to sinecures such as blockings and SPI which often treats the symptoms but not the cause.</p><p> Still, it appears that the core editors and/or administrators are so content enough for the present status quo and thus doom any effort to change the system. An example would be the temporary ban of an <a href="https://en.wikipedia.org/wiki/Wikipedia:Community_response_to_the_Wikimedia_Foundation%27s_ban_of_Fram">administrator</a> made in 2019 by the Wikimedia Foundation (ultimately responsible for maintaining English Wikipedia and any other projects such as Wikimedia Commons for photos and Wikipedias written in other languages), nearly causing the split of Wikipedia into two或者更多。 This is not to mention that presently Wikipedia has a <a href="https://en.wikipedia.org/wiki/WP:CANCER">financial cancer</a> and having to beg for donations despite having sufficient funds so it may be worthwhile to put your donations for the Internet Archive instead.</p><p> A key to a solution may lie in the comparative analogy that Wikipedia is like the only restaurant in a <a href="https://en.wikipedia.org/wiki/Food_desert">food desert</a> . It could be a McDonald&#39;s, KFC, BK, Taco Bell, White Castle, or so on, but customers are forced to go there to dine in every time, even if some does not really like their food. Thus, they will be really happy if a second restaurant is opened at the location.</p><p> If Musk is really serious in fixing whatever problems Wikipedia has brought as a result of its internal problems, then he would be wise in angel-investing any alternatives which aims to become a better or next-level version of Wikipedia.</p><p> The hypothetical rival alternatives could come in the form of a more comprehensive encyclopedia, close to the level of a compendia. It can come in a format similar to GitHub where anyone can present in their preferred version of a subject instead of edit-warring at a small point, and if version is good enough then they can be merged/pushed/vouched by other users to work upon and goes to the top in ranks.</p><p> In fact, every edition of page histories are logged by Wikipedia when a change is make, but in addition to heuristic placements which make these to be perceivably obscure, those would get redacted if the page in question is deleted.</p><p> Forking contents from English Wikipedia isn&#39;t really a big problem since all you can do is to go to <a href="https://dumps.wikimedia.org/">the Wikimedia dump site</a> and look for enwiki, but the biggest issues are how to convince editors and readers alike to move over to the alternative. One possible solution that I can think of in terms of editors would be a pitch promising that the contents will eventually get copied into <a href="https://www.extremetech.com/extreme/328700-5d-optical-disc-could-store-500tb-for-billions-of-years">discs that lasts for billions of years</a> and launched to the Moon and beyond for posterity.</p><p> It is entirely possible that if such solution with out-of-the-world approach had been thought about earlier, the synergies that led to all sort of intractable conflicts in Wikipedia could be cut by a half or so. Perhaps inside Wikipedia the environment would not resemble an authoritarian police state like now. After all, you can find so many real stories echoing the same theme on Wikipediocracy, Wikipedia Review and Wikipediasucks.co, which are like how Xenu.net is to Scientology.</p><p> Finally this post is released under Creative Commons CC0, which is a public domain as the only thing I want is let everyone know how Wikipedia really works in the inside given the recent attention to Musk&#39;s comments against it and to dispel idealistic notions (as seen in WhitePeopleTwitter regarding Musk&#39;s tweet) that overrated it beyond what should&#39;ve been, while hoping for alternatives to spring up to provide greater opportunities for anyone to preserve histories without corrosive influence from systemic biases such as those in Wikipedia.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Dre3LFGrnXzrecF5E/wikipedia-is-not-so-great-and-what-can-be-done-about-it#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Dre3LFGrnXzrecF5E/wikipedia-is-not-so-great-and-what-can-be-done-about-it<guid ispermalink="false"> Dre3LFGrnXzrecF5E</guid><dc:creator><![CDATA[less_than_2]]></dc:creator><pubDate> Sun, 26 Nov 2023 21:57:45 GMT</pubDate> </item><item><title><![CDATA[Spaced repetition for teaching two-year olds how to read (Interview)]]></title><description><![CDATA[Published on November 26, 2023 4:52 PM GMT<br/><br/><p> <i>Update: this post now has another video.</i></p><p> <strong>This</strong> <strong>father has been using spaced repetition (Anki) to teach his children how to read several years earlier than average.</strong></p><p> <a href="https://twitter.com/michael_nielsen/status/1587084229946400769">Michael Nielsen</a> and <a href="https://twitter.com/gwern/status/1586386061395374080">Gwern</a> <span class="footnote-reference" role="doc-noteref" id="fnrefwg6ygc2mu0n"><sup><a href="#fnwg6ygc2mu0n">[1]</a></sup></span> tweeted about the interesting case of a reddit user, u/caffeine314 (henceforth dubbed “CoffeePie”), who has been using spaced repetition with his daughter from a very young age.</p><p> CoffeePie started using Anki with his daughter when she turned 2, and he continued using Anki with his son starting when he was 1 year 9 months. Here&#39;s his daughter&#39;s progress as recounted in January 2020:</p><blockquote><p> My daughter is now about to <strong>turn 5 in a few days</strong> … She&#39;s still going strong -- she uses Anki every single day for English, Hebrew, and Spanish. She&#39;s very confident about reading, and moreover, she reads with ... &quot;context&quot;. Many kids her age read mechanically, but <strong>she reads like a real storyteller</strong> , and that comes from her confidence. At the beginning of the school year her teachers said <strong>she definitely has the reading ability of fifth grade</strong> , and if we&#39;re just going by the ability to read and not focus on comprehension of abstract ideas, her reading level may rival an 8th grader.</p></blockquote><p> (From <a href="https://www.reddit.com/r/Anki/comments/eisra4/update_on_my_daughter_and_anki/">Update on my daughter and Anki</a> )</p><p> For reference, fifth graders are usually 10 or 11yo in the US, and 8th graders are usually 13 or 14yo, so this puts her <strong>~5–9 years ahead of the average child</strong> .</p><p> You can see a video of his daughter reading at 2 years, 2 months later in this post.</p><p> CoffeePie has made several posts about their experience but I still had questions so I reached out to interview him back in January.</p><h1><strong>面试</strong></h1><p><i>Responses have been edited for clarity</i> .</p><p> <strong>What</strong> <strong>did you learn in going from using Anki on your daughter to your son? How has it gone with your son?</strong></p><p> It&#39;s a hard question, because I got so much right. We were so wildly successful that I &quot;cloned&quot; just about every aspect with my son.</p><p> A couple of things I can think of:</p><p> With my daughter, I held back on lowercase letters for a long time because I thought it would confuse her, but when I started to introduce lowercase to her, to my extreme shock, she already knew them, down cold!</p><p> I think what happened is that she learned them just by looking at books, TV, magazines, storefront signs, menus, etc.</p><p> So when we started with my son, I started doing lower case letters the very day after we finished capital letters.</p><p> Another difference is that we did numbers the very next day after lowercase letters.</p><p> I really, really thought I was pushing too hard; I had no desire to be a &quot;tiger dad&quot;, but he took it with extreme grace. I was ready to stop at any moment, but he was fine.</p><p> Another difference is that our expectations of what the kids were getting out of it had changed, as well. At first, I just really wanted my daughter to get a jump start on reading, but stupid me, I didn&#39;t realize there were unintended consequences. A four year old with a 3rd grade reading ability learns about a WHOLE lot more -- it opened up politics for her. She would read our junk mail, and learn who our council member was, who our representative is, the mayor, current events, history, etc. I know it&#39;s stupid of me to say, but I underestimated the effect that reading early would have on her breadth of learning.</p><p> One last thing is math. I mentioned that we started numbers early with my son. But we also started arithmetic. He wasn&#39;t reading by 3 the way Hannah was, but he knew all his multiplication tables up to 12 by 12. This year we tackled prime factorization, Fibonacci sequences, decimal and place values, mixed, proper, and improper fractions, light algebra, etc. I was much more aggressive with the math, and again, he handled it with grace. I was ready to stop at any moment.</p><p> <strong>Do</strong> <strong>you still use Anki with your daughter now as she&#39;s gotten older?</strong></p><p> We pretty much stopped Anki with my daughter. She hasn&#39;t been tested lately, but I&#39;d say her mechanical reading is high school level, easily. Her understanding / comprehension is still advanced, but more aligned with her age. That&#39;s not something Anki can help with, easily. Between school and her extracurricular activities, I didn&#39;t want to steal more time from her, so we stopped Anki on weekdays. We still do Anki -- Hebrew only -- on non-school nights (weekends and holidays). I felt we were being unfair since she&#39;s now in 2nd grade, and is spending significant time on homework and stuff. I wanted her to be a kid.</p><p> <strong>To</strong> <strong>clarify- did you stop using Anki with your daughter in large part because you ran out of topics beyond reading/language/math?</strong></p><p> I think that&#39;s what it amounted to with Hannah. Mechanically, she reads at high school graduate level. But her reading comprehension is more age-appropriate. She&#39;s been tested by the BOE, and her reading comprehension in Kindergarten was 4th grade.</p><p> I don&#39;t think there&#39;s much that Anki can do for reading comprehension. She&#39;s missing the type of knowledge that comes with experience. Occasionally we&#39;ll come across something that shockingly reminds me she&#39;s still 7 -- like not knowing what giving someone a cold shoulder is. She&#39;s such a good reader, it&#39;s ... a jolt when we come across stuff like that. I think Anki reading ran its course with her.</p><p> As for math, she could be better at the times tables. Still knows them better than anyone in her class. But here, again, she needs the kind of info that Anki just can&#39;t test, like thinking about 87-8 as being the same problem as 80-1. Oddly enough, a long page of problems is probably more conducive to that sort of thing.</p><p> <strong>I&#39;m</strong> <strong>curious if you&#39;ve seen</strong> <a href="https://larrysanger.org/2010/12/baby-reading/"><strong>the experience of Larry Sanger (cofounder of Wikipedia)</strong></a> <strong>in teaching his kids to read early.你对那个怎么想的？</strong></p><p> I never heard of Larry Sanger, but that is <i>precisely</i> our experience, to a T! Here&#39;s Hannah reading Rollie Pollie Ollie at 2 years, 2 months: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/id1f6duirnnw0h6avfoz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/yzanstqwosr6enq5638r 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/w3dwoedacziswtbooskq 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/rqyzym5zqcusjab5uxyn 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/wefdymwvc2egpmjed7xx 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/ipfg4desbt5tqsizn5dt 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/eybzdx2u14gsx8abrfpd 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/n7s01ruhnv8ubqputsxk 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sj40no73ofo58on9ma4n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sbh10glmb3y2r25nziik 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/zzrfmydb6jn6rnuvfclf 1450w"><figcaption> <a href="https://chipmonk.substack.com/p/spaced-repetition-for-teaching-two">Oops, I can&#39;t figure out how to embed videos. <strong>View the video on substack here.</strong></a></figcaption></figure><p> <strong>Do</strong> <strong>you think using Anki ever felt coercive to either of your children?</strong></p><p> Hannah went through a phase where she didn&#39;t want to do it. We tried to compromise and work through it. Eventually, it became part of her &quot;job&quot; -- we told her that every human has a job, and her job was to do Anki. Other than that, we never had to coerce any of the kids.</p><p> <strong>Do</strong> <strong>you have any other interesting or unusual plans for educating your daughter in the next few years?</strong></p><p>有趣的问题。 I feel like a bad parent writing &quot;no&quot;, but being such an early reader gave her access to advanced learning at an earlier age. She has such an advantage compared to her classmates, I think I&#39;m going to let her be for awhile. She&#39;s a curious person, and she has the tools to follow her own interests, and I trust her. We did start some high school algebra -- I&#39;ve been showing her the properties of algebras: commutativity, associativity, identity, distributivity, etc. We&#39;ve been looking at symmetries -- mirror, reflexive, rotational. Highfalutin math topics that don&#39;t really require hardcore calculations. But it&#39;s always in the context of &quot;hey, I have something interesting I want to show you&quot; rather than &quot;please sit down and work on these problems&quot;.</p><p> Actually, if YOU have any suggestions for interesting education opportunities, I&#39;m all ears!</p><h1>闭幕式</h1><p>That&#39;s everything I&#39;ve asked CoffeePie so far. If you have anything you want me to ask him, or any suggestions of things he could try with his children (who are currently aged ~5 and ~8), let me know and I&#39;ll tell him!</p><p> One confounder here is that CoffeePie used to be a physics professor, so some of this effect is likely genetic.</p><p> CoffeePie also runs a tutoring business, <a href="https://brooklyntutoring.net">Brooklyn Tutoring and Test Prep</a> .</p><p> <strong>I will be posting more about parenting soon: subscribe my posts or</strong> <a href="https://open.substack.com/pub/chipmonk/p/spaced-repetition-for-teaching-two?r=bgw61&amp;utm_campaign=post&amp;utm_medium=web"><strong>my blog</strong></a> .</p><p> <i>Thanks to</i> <a href="https://prigoose.substack.com/"><i>Priya</i></a> <i>(</i> <a href="https://twitter.com/Prigoose"><i>@Prigoose</i></a> <i>) for turning the draft into a final post after I sat on it for far too long!</i></p><p> <a href="https://twitter.com/Prigoose/status/1728829018026475766"><i>See this post on twitter</i></a> .</p><h1> Update: video of practice</h1><p> CoffeePie just sent me this video he found of his wife practicing Anki with his son at 2 years 6 months.非常可爱。 </p><figure class="media"><div data-oembed-url="https://youtu.be/8U-Lza__Kko"><div><iframe src="https://www.youtube.com/embed/8U-Lza__Kko" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwg6ygc2mu0n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwg6ygc2mu0n">^</a></strong></sup></span><div class="footnote-content"><p> Gwern&#39;s twitter account is private; the tweet reads:</p><blockquote><p> @michael_nielsen https://reddit.com/r/Anki/comments/8iydl7/using_anki_with_babies_toddlers/ https://old.reddit.com/r/Anki/comments/a9wqau/using_anki_with_babies_toddlers_update/ Neatest spaced repetition use I&#39;ve seen in a尽管。</p></blockquote></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read<guid ispermalink="false"> 2PLBhCbByRMaEKimo</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Sun, 26 Nov 2023 16:52:59 GMT</pubDate> </item><item><title><![CDATA[Paper out now on creatine and cognitive performance]]></title><description><![CDATA[Published on November 26, 2023 10:58 AM GMT<br/><br/><p> Our paper “The effects of creatine supplementation on cognitive performance - a randomised controlled study” is out now!</p><p> → Paper: <a href="https://doi.org/10.1186/s12916-023-03146-5">https://doi.org/10.1186/s12916-023-03146-5</a></p><p> → Twitter thread: <a href="https://twitter.com/FabienneSand/status/1726196252747165718?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19">https://twitter.com/FabienneSand/status/1726196252747165718?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19</a></p><p> Jan Brauner and I are very thankful to Paul Christiano for suggesting doing this study and for funding it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance<guid ispermalink="false"> CbaznRo9fKpriw2mi</guid><dc:creator><![CDATA[Fabienne]]></dc:creator><pubDate> Sun, 26 Nov 2023 10:58:36 GMT</pubDate> </item><item><title><![CDATA[Curated list of my favourite self-help resources]]></title><description><![CDATA[Published on November 26, 2023 6:31 AM GMT<br/><br/><h1><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yuk43ux7m5rqcahyu7xd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/lfc8ykuwo35s4iiu69ou 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xlun4tmujc8x0lltgvqb 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/gvebucioi35j9bsgcykg 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/pydng5urp8xfe6icaon0 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/cnqyhfskzcno1by3rraq 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yv7egfvybqncvv9z7itb 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xav44yvhuwqiosm37ut1 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/dyy01z5dxw3lhzczcq33 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/rldwopdvuhqtjlx1zbkr 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/azi2x5q67vntencdoa76 1024w"><br><br>会谈</h1><ul><li>Brené Brown: <a href="https://www.youtube.com/watch?v=iCvmsMzlF7o"><u>The power of vulnerability</u></a></li><li> Brené Brown: <a href="https://www.youtube.com/watch?v=psN1DORYYV0"><u>Listening to shame</u></a></li><li> Brené Brown&#39;s longer <a href="https://www.audible.com/pd/Self-Development/The-Power-of-Vulnerability-Audiobook/B00CYKDYBQ"><u>series of talks on shame, wholeheartedness, and vulnerability</u></a></li><li> Brené Brown:<a href="https://www.audible.com/pd/Self-Development/Men-Women-and-Worthiness-Audiobook/B00C9J0SDY"><u>Men, Women and Worthiness: The Experience of Shame and the Power of Being Enough</u></a></li><li> Brené Brown: <a href="http://www.oprah.com/own-supersoulsessions/brene-brown-the-anatomy-of-trust-video"><u>The Anatomy of Trust</u></a></li><li> John Gottman: <a href="https://www.youtube.com/watch?v=AKTyPgwfPgg"><u>Making Marriage Work</u></a> (also applicable to friendships and family relationships)</li><li> Richard Rohr: <a href="https://www.audible.com/pd/Self-Development/True-Self-False-Self-Audiobook/B003A2GME8"><u>True Self, False Self</u></a> (on hierarchies of worth vs. inherent worthiness and divinity)</li></ul><h1>图书</h1><ul><li>Kristen Neff: <i>Self-Compassion: The Proven Power of Being Kind to Yourself</i> ( <a href="https://www.amazon.com/Self-Compassion-Proven-Power-Being-Yourself/dp/0061733520/"><u>text</u></a> | <a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Audiobook/B005P1FJVE"><u>audio</u></a> )</li><li> Kristen Neff: <i>Self-Compassion Step by Step</i> ( <a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Step-by-Step-Audiobook/B00DMCAXKK"><u>audio guide</u></a> )</li><li> Brené Brown: <i>Rising Strong: The Reckoning, The Rumble, The Revolution</i> ( <a href="https://www.amazon.com/Rising-Strong-Ability-Transforms-Parent/dp/081298580X/"><u>text</u></a> | <a href="https://www.audible.com/pd/Self-Development/Rising-Strong-Audiobook/B00VSEM9QK"><u>audio</u></a> )</li><li> Brené Brown: <i>Daring Greatly: How the Courage to Be Vulnerable Transforms the Way We Live, Love, Parent, and Lead</i> ( <a href="https://www.amazon.com/Daring-Greatly-Courage-Vulnerable-Transforms/dp/1592408419/"><u>text</u></a> | <a href="https://www.audible.com/pd/Self-Development/Daring-Greatly-Audiobook/B075DCNLLQ"><u>audio</u></a> )</li><li> Brené Brown: <i>I Thought It Was Just Me (but it isn&#39;t): Making the Journey from “What Will People Think?” to “I Am Enough”</i> ( <a href="https://www.amazon.com/Thought-Was-Just-but-isnt/dp/1491513853"><u>text</u></a> | <a href="https://www.audible.com/pd/Self-Development/I-Thought-It-Was-Just-Me-but-it-isnt-Audiobook/B004GEHVEY"><u>audio</u></a> )</li><li> Harriet Lerner: <i>The Dance of Connection: How to Talk to Someone When You&#39;re Mad, Hurt, Scared, Frustrated, Insulted, Betrayed, or Desperate</i> ( <a href="https://www.amazon.com/Dance-Connection-Frustrated-Insulted-Desperate/dp/006095616X/"><u>text</u></a> | <a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Connection-Audiobook/B002V8DJ4I"><u>audio</u></a> )</li><li> Harriet Lerner: <i>The Dance of Intimacy: A Woman&#39;s Guide to Courageous Acts of Change in Key Relationships</i> ( <a href="https://www.amazon.com/Dance-Intimacy-Womans-Courageous-Relationships/dp/B0000546NH"><u>text</u></a> |<a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Intimacy-Audiobook/B002UZKY86"><u>audio</u></a> )</li><li> Elizabeth Gilbert: <i>Eat, Pray, Love</i> ( <a href="https://www.amazon.com/Eat-Pray-Love-Everything-Indonesia/dp/0143038419"><u>text</u></a> | <a href="https://play.google.com/store/audiobooks/details/Eat_Pray_Love_One_Woman_s_Search_for_Everything_Ac?id=AQAAAAD4nWCdiM"><u>audio</u></a> )</li></ul><h1>播客</h1><ul><li>Rob Bell: <a href="http://pca.st/episode/e7275d30-5ad0-0134-cf69-7b84bf375f4c"><u>You the Steward</u></a> (on emotional energy)</li><li> Elizabeth Gilbert and Pete Holmes <a href="http://pca.st/episode/61575520-4ef3-0133-c5f4-0d11918ab357"><u>on creativity, spirituality, and love</u></a></li><li> Richard Rohr and Pete Holmes <a href="http://pca.st/episode/6468f580-af6e-0132-33a0-0b39892d38e0"><u>on falling upward and conscious loving union</u></a></li></ul><h1>视频</h1><ul><li>Brené Brown <a href="https://www.youtube.com/watch?v=1Evwgu369Jw"><u>on empathy</u></a></li><li> Brené Brown <a href="https://www.youtube.com/watch?v=RZWf2_2L2v8"><u>on blame</u></a></li><li> Brené Brown <a href="https://youtu.be/RKV0BWSPfOw"><u>on why joy is the most terrifying emotion</u></a></li></ul><h1>治疗</h1><ul><li><a href="https://thedaringway.org/help/"><u>The Daring Way</u></a></li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/zvNKKLzruY8GS8BAD/curated-list-of-my-favourite-self-help-resources#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zvNKKLzruY8GS8BAD/curated-list-of-my-favourite-self-help-resources<guid ispermalink="false"> zvNKKLzruY8GS8BAD</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:31:13 GMT</pubDate> </item><item><title><![CDATA[Why Q*, if real, might be a game changer]]></title><description><![CDATA[Published on November 26, 2023 6:12 AM GMT<br/><br/><p><i>基于聚会上的谈话的一些想法。免责声明：我在这个领域还算不上外行。</i></p><p> TL;DR：如果这个传闻中的 Q* 事物代表着从“最有可能”到“最准确”的令牌完成的转变，那么它可能暗示 LARPer 发出最有可能的、通常是幻觉的令牌设计，发生了意想不到的重大变化为了取悦提问者（和培训师），一个试图将错误与未知的潜在现实（无论它是什么）最小化的实体，那么我们就会看到从相对良性的“随机鹦鹉”到更强大的转变，并且潜在更危险的实体。</p><p>对于任何使用当代法学硕士的人来说，很明显的一件事是，他们并不真正关心现实，更不用说改变它了。他们是你在聚会上经常看到的那种肤浅的博学之徒：他们对每个话题都了解得足够多，足以在随意的谈话中给人留下深刻的印象，但他们并不关心自己所说的是否准确（“真实”），只关心自己所说的有多少。它给谈话伙伴留下的印象。不过，不可否认的是，大量的 RLHF 会使它们变得迟钝。如果受到压力，他们可以评估自己的准确性，但他们并不真正关心它。重要的是输出听起来很真实。从这个意义上说，法学硕士优化了下一个标记的概率，以匹配训练集的含义。这是一个很大而明显的缺点，但同时，如果你属于“末日论者”阵营，也可以稍微喘口气：至少这些东西不会立即对整个人类造成危险。</p><p>现在，最初的“报告”是 Q* 可以“解决基本数学问题”和“象征性推理”，这表面上听起来并不多，但是，这是一个很大的但是，如果这意味着它更少在它工作的领域是幻觉，那么它可能（很大的可能）意味着它能够跟踪现实，而不是纯粹的训练集。反对这有什么大不了的通常观点是“要很好地预测下一个令牌，你必须有一个准确的世界模型”，但据我了解，到目前为止情况似乎并非如此。</p><p>是否会出现从高概率到高精度的转变，或者即使这是一个有意义的陈述，我无法评估。但如果是这样，那么事情就会变得更有趣。</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer<guid ispermalink="false"> JBvmETRAvTCmtEw2y</guid><dc:creator><![CDATA[shminux]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:12:32 GMT</pubDate></item></channel></rss>