<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 25 日星期五 12:20:38 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[A Model-based Approach to AI Existential Risk]]></title><description><![CDATA[Published on August 25, 2023 10:32 AM GMT<br/><br/><h1>介绍</h1><p>两极分化阻碍了合作和了解未来人工智能是否对人类构成生存风险以及如何降低灾难性后果的风险的进展。确定这些风险是什么以及什么决策是最好的是非常具有挑战性的。我们认为，<i>基于模型的方法</i>具有许多优势，可以提高我们对人工智能风险的理解，评估缓解政策的价值，并促进人工智能风险争论不同方面的人们之间的沟通。我们还相信，人工智能安全和校准社区中的很大一部分从业者都拥有成功使用基于模型的方法的适当技能。</p><p>在本文中，我们将引导您通过一个基于模型的方法的示例应用来应对未结盟人工智能带来的生存灾难风险：基于卡尔史密斯的<a href="https://joecarlsmith.com/2023/03/22/existential-risk-from-power-seeking-ai-shorter-version"><i><u>《寻求权力的人工智能是否是​​存在风险？》的</u></i></a>概率模型。您将与我们的模型进行交互，探索您自己的假设，并（我们希望）就这种类型的方法如何与您自己的工作相关提出您自己的想法。您可以在此处找到该模型的链接。</p><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>在许多鲜为人知的领域，人们倾向于采取倡导立场。我们在人工智能风险中看到了这一点，经常看到作家<a href="https://www.lesswrong.com/posts/BTcEzXYoDrWzkLLrQ/the-public-debate-about-ai-is-confusing-for-the-general"><u>轻蔑地称某人为“人工智能末日论者”或“人工智能加速论者”。</u></a>这场辩论的双方都无法向另一方传达他们的想法，因为倡导通常包括在另一方不共享的框架内解释的偏见和证据。</p><p>在其他领域，我们亲眼目睹了基于模型的方法是消除此类宣传的建设性方法。例如，通过利用基于模型的方法， <a href="https://lumina.com/case-studies/energy-and-power/a-win-win-solution-for-californias-offshore-oil-rigs/"><u>Rigs-to-Reefs 项目</u></a>在 22 个不同的组织之间就如何退役圣巴巴拉海岸大型石油平台这一有争议的问题达成了近乎共识。几十年来，环保组织、石油公司、海洋生物学家、商业和休闲渔民、航运利益集团、法律辩护基金、加利福尼亚州和联邦机构在这个问题上陷入了僵局。模型的引入使对话重新聚焦于具体假设、目标和选项，并导致 22 个组织中的 20 个组织就同一计划达成一致。加州立法机构通过 AB 2503 号法案将该计划写入法律，该法案几乎一致通过。</p><p>人工智能带来的生存风险存在很多不确定性，而且风险极高。在这种情况下，我们主张使用概率分布明确量化不确定性。遗憾的是，这种情况并没有应有的普遍，即使在此类技术最有用的领域也是如此。</p><p> <a href="https://arxiv.org/abs/2206.13353"><u>Joe Carlsmith</u></a> (2022) 最近发表的一篇关于不结盟人工智能风险的论文有力地说明了概率方法如何帮助评估先进人工智能是否对人类构成生存风险。在本文中，我们回顾了 Carlsmith 的论点，并将他的问题分解纳入我们自己的<a href="https://lumina.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>模型中。然后，我们以多种方式扩展这个起点，以展示应对 x 风险领域中每个独特挑战的基本方法。我们带您参观实时模型，了解其元素，并使您能够自己更深入地研究。</p><h2>挑战</h2><p>预测长期未来总是充满挑战。如果<a href="https://www.youtube.com/watch?v=87l9Az9msHU&amp;t=3108s"><u>没有历史先例</u></a>，难度就会被放大。但这种挑战并不是独一无二的。我们在许多其他领域缺乏历史先例，例如在考虑新的政府计划或全新的商业计划时。当世界状况因技术、气候、竞争格局或监管变化而发生变化时，我们也缺乏先例。在所有这些情况下，难度都很大，但与预测通用人工智能 (AGI) 和存在风险的挑战相比就显得苍白无力了。今天对人工智能存在风险的预测通常至少部分依赖于关于未来先进人工智能将如何表现的抽象论点，而我们今天无法测试这些论点（ <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>尽管我们正在努力改变这一点</u></a>）。即使是最精心设计的论点也常常会遇到合理的不确定性和怀疑。<br><br>例如，在评估有关人工智能存在风险的预测的可靠性时，经常会遇到这样的反对意见：“我在预测中找不到具体的缺陷。它们只是有点抽象，有点连贯，而且论证这个班级经常会以意想不到的方式犯错。”</p><p>举个例子，最近超级预测者对人工智能风险的启发似乎表明，这种普遍的怀疑态度是超级预测者和人工智能安全社区之间对人工智能风险<a href="https://astralcodexten.substack.com/p/the-extinction-tournament"><u>持续存在分歧</u></a>的一个因素。尽管两个群体之间进行了讨论，而且超级预测者在<a href="https://www.lesswrong.com/posts/BHdEvjtfwpgrTh825/ai-21-the-cup-overfloweth?commentId=dpzuaGHyiNcnr4jF2"><u>有关未来人工智能的许多定量预测</u></a>上与领域专家达成了一致，但这种关于人工智能风险的分歧仍然存在，这表明对人工智能风险论点的怀疑更加分散。应认真对待此类反对意见，并根据其本身的优点以及过去类似反对意见在其他领域的表现进行评估。这凸显了不仅评估预测内容，而且评估其背后的基本假设和推理的重要性。</p><p>对于为什么可能出现某些结果，人工智能风险界已经提出了许多论点。当你着手建立一个明确的人工智能存在风险模型时，如果不吸收其他聪明、专注的人经过深思熟虑的想法，那就太疏忽了。然而，将多个想法合并到一个连贯的模型中确实很困难，并且根据某些统计，有多达<a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=C3sb2QZQAeHLZmz2T"><u>五个部分重叠的世界观/研究议程</u></a>，每个都专注于不同的威胁模型。不同的论点常常建立在相互不一致的概念框架之上。简单地统计某个立场存在多少个论据也是行不通的，因为基本假设几乎总是存在大量重叠。此外，以任何深入的方式将<a href="https://www.lesswrong.com/posts/Cd6uMn4qHXZcoe2Lh/discussion-weighting-inside-view-versus-outside-view-on"><u>内部观点与外部观点</u></a>论证合并起来似乎几乎是不可能的。虽然很困难，但整合现有的专家知识（和意见）对于有效的基于模型的方法至关重要。我们认为人工智能存在风险建模在整合多种专家知识来源方面具有独特的方面，因此是进一步研究新方法和新技术的成熟领域。我们已将解决本段中提到的所有挑战的简单方法纳入我们的模型中。</p><p>主观概率是表示不确定性的传统工具，一般来说，它是一个很好的工具。基于模型的方法依赖于对不确定变量的主观评估。在人工智能存在风险领域，当你要求两位专家评估相同的主观概率时，他们的估计通常会存在显着差异（例如，一位专家说 15%，而另一位专家说 80%）。这在其他领域并不正常。尽管您可能会发现两个气象学家分别预测下雨概率为 15% 和 80% 的情况，但这种情况并不常见。</p><p>这是上面已经讨论过的困难的症状，并且引入了该领域的另一个显着特征。由于专家之间的这种极端差异，以及人们的估计校准不佳的事实，似乎需要获得额外的信心层。我们在本文后面的“元不确定性”部分对此进行了详细说明，并且我们在模型中包含了显式的二阶分布（即，二阶分布代表专家意见之间的差异，而一阶不确定性代表不确定性）在结果中）。</p><p>我们在本文中描述的工作是作为 MTAIR 项目（建模变革性人工智能风险）的一部分进行的，建立在<a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">最初的 MTAIR 概念模型的</a>基础上。我们的目标是评估 AGI 存在风险的多个、有时甚至根本上相互冲突的详细模型以及这些外部观点/可靠性考虑因素。我们将他们视为相互竞争的“专家”，以便做出明智且平衡的评估。您可以使用我们的交互式模型来输入您自己的评估并探索其含义。</p><h2>是什么让模型有效？</h2><ul><li><strong>透明度</strong>：为了有效，基于模型的方法应该提供其他人可以浏览和理解的模型。当了解该主题的典型用户能够理解模型在做什么、如何做以及计算中使用什么假设时，我们称模型是<i>透明的</i>。您永远不应该假设主题专家是程序员，或者 python 代码（或任何其他编程语言）不言而喻。因此，传统的程序通常被认为是不透明的。</li><li><strong>交互性</strong>：第二个重要属性是<i>交互性</i>，以及利益相关者尝试不同假设、探索不同决策或政策的后果以及探索任意假设场景的能力。</li><li><strong>显性的不确定性</strong>：对于人工智能的存在风险，大部分行动都是在不确定性的尾部进行的（即简单地得出中值结果是人类生存的结论是没有抓住要点的）；因此，<i>不确定性的明确表示</i>很重要。</li></ul><p>我们在<a href="https://analytica.com/"><u>Analytica 视觉建模软件</u></a>中构建了模型，该软件强烈满足上述所有要求，并且使用起来很有趣。 Analytica 模型的结构为分层影响图，这是一种高度可视化且易于理解的表示形式，捕捉了模型如何直观地工作的本质。它是交互式的并且具有嵌入式模块化文档。有强大的多维智能阵列设施，提供了前所未有的灵活性。它使用概率分布明确地表示不确定性。不确定性向下游计算结果的传播会自动发生。它学习起来既简单又快捷，一旦您构建了模型，您就可以将其发布到网络上进行共享（正如我们在本文中所做的那样）。</p><p>如果您受到我们的示例的启发来构建自己的模型，您应该知道<a href="https://analytica.com/products/free101/"><u>Analytica 有一个免费版本</u></a>。当您需要扩展到真正的大型模型时，也可以使用商业版本。桌面版本需要 Microsoft Windows。您无需获取或安装任何东西（浏览器 - Chrome 或 Edge 除外）即可使用我们的模型，该模型在 Analytica 云平台 (ACP) 上共享。我们的模型大约有 150 个对象，略超过免费版本 101 个对象的最大大小。但如果您有兴趣将其下载到桌面 Analytica，免费版本允许您加载、查看、运行、更改输入和重新评估结果等。</p><p>总之，基于模型的方法来评估人工智能存在风险预测的可靠性可以为人工智能安全社区带来多种好处。首先也是最重要的，它提供了清晰、简洁和易读的输出，考虑了可能影响预测准确性的许多不同的反对意见和因素。这有助于确保人工智能安全社区了解预测背后的推理和证据，并可以根据该信息做出明智的决策。</p><p><strong>此外</strong>，这种基于模型的方法鼓励社区考虑更广泛的因素，而不仅仅是详细的论点本身。例如，他们可能会考虑他们对高级抽象的信任程度以及不同启发式的可靠性。通过将这些考虑因素纳入模型中，社区可以更有效地权衡与人工智能相关的风险，并制定更稳健的策略来减轻潜在危害。最后，这种方法可以通过促进对所有相关因素进行更严格的思考和更全面的检查来帮助改善社区的认知，从而更好地理解人工智能存在风险的性质和可能性。</p><p>作为起点，我们将重点关注基于 Joe Carlsmith 报告<a href="https://arxiv.org/abs/2206.13353"><u>“寻求权力的人工智能是否存在风险”的</u></a>单一详细模型，以及影响这一机械模型合理性的几个外部观点/可靠性启发法。我们将首先简要介绍卡尔史密斯对人工智能存在风险的介绍以及我们自己的一些改进，然后在最后讨论改进该模型的后续步骤。</p><h1>型号概览</h1><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>这是在 Analytica 云平台 (ACP) 中运行的分层模型，基于 Joe Carlsmith 的报告“寻求权力的人工智能是否存在风险”。它可以让你计算由人工智能失调引起的生存灾难的概率。</p><p>这些结论隐含地取决于我们在给出各种假设的情况下明确规定的某个时间范围。事实上的时间范围是“到 2070 年”，但在输入您自己的估计时，您可以采用不同的时间范围，而无需更改模型的逻辑。</p><p>简而言之，该模型预测，如果出现以下情况，错位的寻求权力的人工智能将导致一场生存灾难：</p><ol><li>先进、规划、<a href="https://www.planned-obsolescence.org/situational-awareness/"><u>战略意识</u></a>（APS）系统——即能够进行高级规划、具有战略意识并拥有先进的人类水平或超人类能力的人工智能——是可行的，</li><li>当 APS 系统可行时，将会有强烈的激励措施来建设它们，</li><li>构建不以不协调的方式寻求权力的 APS 系统比构建表面上有用但确实以不协调的方式寻求权力的 APS 系统要困难得多，<ol><li>尽管 (3)，实际上将构建和部署错位的 APS 系统，</li></ol></li><li>未对准的 APS 系统在部署后将能够造成全球性的大灾难，</li><li>人类对造成此类灾难的错位 APS 系统的反应不足以阻止其完全接管，</li><li>一旦接管，错位的 APS 系统将摧毁或严重削弱人类的潜力。</li></ol><p>我们模型的总体框架基于卡尔史密斯报告和随后的<a href="https://80000hours.org/problem-profiles/artificial-intelligence"><u>80,000 小时文章</u></a>中提供的人工智能存在风险论点，并进行了修改。这是我们的“顶级模型”，我们对人工智能存在风险进行高级分析。</p><h1>模型巡演</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/mvk6oge6czjenatva2gs"></p><p>在本节中，您将快速浏览我们的模型，并在浏览器窗口中实时运行它。首先，请单击<a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><u>“启动模型”</u></a>以在不同的浏览器选项卡或窗口中将其打开，以便您可以同时参考此页面。我们提供分步说明来帮助您入门。按照此导览了解您的方向，然后您可以自己更深入地探索模型的其余部分，并探索不同估计值会发生什么。我们建议在大显示器上运行模型，而不是移动设备。</p><h2>基本评估</h2><p>在第一页上，您将看到卡尔史密斯报告中的六个概率评估。 （请注意，本文中的屏幕截图是静态的，但它们在运行模型的浏览器窗口中是活动的）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/rolr4s2y84njur0cbthc"></p><p>您可以在此处调整滑块或为每个滑块输入您自己的估计值。要了解每个含义，只需将鼠标悬停在问题上并阅读弹出的说明。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/adt7luy3kmzavtkfqwmd"></p><p>在估计这些之前，您应该选择一个时间范围。例如，您可以估计每个值在 2070 年之前是否成立。计算取决于您的估计，而不是所选的时间范围，但您的估计预计会随着更长期的时间范围而改变（增加）。</p><p>滑块输入下方是一些计算结果，显示 5 个阶段中的每个阶段以及所有前面的阶段最终为真的概率。最后一个“存在性灾难”显示了根据您对六个命题中每一个命题的估计，APS 系统发生存在性灾难的概率。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/qy1yjgqxgkjmwhzm43c5"></p><p>在这个屏幕截图中，我们看到 APS 有 0.37% 的几率（不到百分之一）会导致人类灭绝等生存灾难。考虑到结果的极端性，这似乎是一个巨大的风险，但许多专门研究人工智能安全的人会认为这是超级乐观的。您的估计如何比较？</p><h2>专家们权衡利弊</h2><p>您的估计与其他人工智能安全研究人员的估计相比如何？在卡尔史密斯的报告之后，开放慈善组织<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>征求了其他人工智能安全研究人员的评论</u></a>，并要求他们提供自己对这些主张的估计。这些评论发生于 2022 年 8 月。</p><p>首先，您可以通过按浏览他们对每个提议的原始评估<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/x5126haezlzwpqulhccb">审阅者评估按钮，该按钮显示在页面右下角（您可能需要向右滚动）。该表显示在页面的右上角。请注意审稿人之间的巨大差异。</p><p>单击“选择要使用的中值评估”的选项下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ugm5g5wylo2zr8tawy64"></p><p>选择所有项目，使其现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/jakuafkaps0pfd9umwih"></p><p>存在灾难输出现在显示<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">按钮。按下。结果表出现在右上角。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cdgv48bhgvvyqdl0accx"></p><p>这些显示了估计所隐含的由 APS 引起的存在灾难的概率，这些估计来自您自己的输入以及审阅者的输入。审稿人的中位数为 9.15%，但审稿人之间的数字差异很大。在少数情况下，审稿人不愿意接受卡尔史密斯提出的分解，就会出现空值。接下来，我们将其显示为条形图。将鼠标悬停在表格区域顶部以访问图形按钮，然后按它。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/coje0q97joeqsped2ibd"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/wi8p3wpl6wwu1xiaksrr"></p><p>再次将鼠标悬停在图表顶部，并将视图更改回表视图。查看结果时，您可以通过这种方式在图形视图和表格视图之间切换。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kxtxsvzrvhvdegaqopw8"></p><h2>专家意见的差异</h2><p>专家意见的巨大差异对该领域的理性决策提出了严峻挑战。很难说任何基于通过汇总这些概率获得的预期效用是可信的。因此，我们根据专家意见的变化拟合概率分布。因为这是主观概率的分布，所以它实际上是<i>二阶概率分布</i>，我们称之为<i>元不确定性</i>。我们用一<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.2klzxyklefio"><u>节</u></a>的内容来讨论元不确定性、其动机及其解释，但现在让我们想象一下这种元不确定性。</p><p>将<i>“选择要使用的中值评估”</i>更改为<strong>“所有审阅者的中值”</strong> ，并在“选择<i>要包含的元不确定性”</i>的选择下拉列表中选择<strong>审阅者的分布</strong>选项。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/whhm6iheuj3vqf4fa9h8"></p><p>输出现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">纽扣。将鼠标悬停在 Existential 灾难输出上并按鼠标右键。从上下文菜单中选择<strong>超出概率</strong>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dipzz5jhchogoicm7ydn"></p><p>在框架节点中，切换回图形视图（ <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/f8qvwqfinrmxpyjxzyny"> ）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y2mwu3iud8senllipqbi"></p><p>超出概率图是可视化概率分布的一种方法。本例中的分布反映了专家意见的差异。基本量（x 轴）是 APS 系统中发生诸如人类灭绝之类的生存灾难的概率。沿着绿色箭头，您可以看到大约 10% 的专家认为存在灾难的概率超过 0.17（即 17%），沿着黄色箭头，大约 5% 的专家认为存在灾难的概率超过 0.27。</p><p>为了获得这种二阶分布，该模型将每个问题的专家评估集合视为从基础分布中采样，然后将概率分布“拟合”到这些点。这种拟合的技术细节将在后面的“元不确定性”部分中介绍。该部分还探讨了当元不确定性（即专家意见之间的变化量）增加或减少时我们的观点如何变化。</p><h2>结合内部和外部视图参数</h2><p>卡尔史密斯分解是<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>内部观点框架</u></a>的一个例子，它将感兴趣的主要问题分解为其组成因素、步骤或起作用的因果机制。相比之下，<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>外部视图框架</u></a>从相似的事件或参考类中得出相似之处，以提供上下文和预测。例如， <a href="https://forum.effectivealtruism.org/posts/MMtbCDTNP3M53N3Dc/agi-safety-from-first-principles#AGI%20safety%20from%20first%20principles"><i><u>第二个物种论点</u></i></a>认为人类可能会失去地球上最强大物种的地位。其他外部观点框架包括霍尔顿·卡诺夫斯基的<a href="https://www.cold-takes.com/most-important-century"><i><u>“最重要的世纪”</u></i></a> 、阿杰亚·科特拉的<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>生物锚</u></a>（对一个子问题、时间表、更大问题的外部观点）、与过去变革性技术进步的类比，甚至<a href="https://forum.effectivealtruism.org/posts/mjB9osLTJJM4zKhoq/2022-ai-expert-survey-results"><u>专家意见调查</u></a>。</p><p>每种类型的框架都会产生不同的见解，但由于内部和外部视图框架以不同的方式进行评估，因此将两者吸收为一致的观点是相当具有挑战性的。但我们认为基于模型的方法需要解决这个问题，以便整合来自所有来源的信息。</p><p>我们包括两种简单的外部视图方法（在后面的部分中详细讨论），由这些输入反映出来： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/w4lspva3ifst3gseyzbh"></p><p>将鼠标悬停在每个输入上即可获取您所估计内容的完整描述。这些需要你抽象地思考几个高层的外部观点考虑因素和论点，然后评估这些考虑因素对存在灾难的风险有多大影响。 Cr在这里是<i>信任的</i>意思。与统计学中可能性的概念类似（有些人可能会说同义），可信度是从 0 到 1 范围内的估计，其中 0 表示考虑因素暗示没有风险，1 表示考虑因素暗示一定的灾难。</p><p>您现在已经输入了您自己对卡尔史密斯“世界模型”以及外部观点可信度的估计。我们的重点是模型如何将这些吸收到单一的主观观点中？我们的目标是强调这一挑战并至少尝试这样做。也许您或其他继续采用未来基于模型的方法的人会改进我们的方法。</p><p>在此模型中，我们允许您为不同视图分配相对权重。单击权重表按钮以赋予不同的意见。将鼠标悬停在输入上即可查看要求您评估的内容的描述。可信度是对您认为这些外部观点论据本身支持该主张的程度进行的评级。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/irrgjkuu6h3szgx5ma8o"></p><p>顶部框架中会出现一个条目表，其中包含可用于更改相对权重的滑块。您可以调整这些以反映您自己对相对可信度的看法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/hvjy2karshi3lntbs83r"></p><p>第一部分允许您输入卡尔史密斯分解与外部视图参数相比的相对重要性。在这里，我们将外部视图固定为 1，因此（基于卡尔史密斯的）世界模型的值为 3 意味着您希望该框架的计数是外部视图参数的三倍。</p><p>在世界模型中，您有自己的估计以及接受调查的各个专家的估计。您可以选择或多或少地重视个别专家的估计。</p><p>最后，在下部，您可以调整两个不同的外部视图框架的权重。这些用于组合不同的外部观点论点。</p><p>设置自己的权重后，右列中的输出将显示同化视图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/s586akwpoclyxujnzywc"></p><p>第一个输出 Cr[存在灾难|世界模型] 是卡尔史密斯分解在考虑了您自己的估计与专家的估计之间的相对权重后的评估。</p><p>第二个输出 Cr[AI Existential Catastrophe] 是组合外部视图模型中存在灾难的概率。</p><p>最终输出 Cr[Existential catastrophe] 是对存在灾难的最终同化估计。它考虑了内部视图世界模型和外部视图模型，将两个来源的信息结合起来作为代表性的最终评估。</p><h2>探索模型的内部结构</h2><p>到目前为止，您已经使用了我们为您突出显示的一些选定的输入和输出。接下来，您将探索模型的内部结构。</p><p>顶部是一个大的蓝色模块节点<strong>Main Model</strong> 。点击它。这将带您进入实施阶段，您会看到几个子模块和<a href="https://lumina.com/technology/influence-diagrams/"><u>影响图</u></a>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cm3tppz2rkmycyj2rvyb"></p><p>在第一个图中，上半部分包含基于卡尔史密斯报告的内部视图世界模型。左下角包含外部视图参数。右下四分之一是用于同化不同观点的逻辑。</p><p>影响图的节点是变量。箭头描绘了变量之间的影响。影响图是直观的，您通常可以从中了解模型是如何工作的，而无需查看计算细节。将鼠标悬停在节点上可查看其描述，以获取有关每个变量所代表含义的更多信息。</p><p>在外部视图部分中，一些未定义的节点（已散列）仅用于记录纳入估计的考虑因素。虚线箭头表示这些不是计算所使用的影响，但应该会影响您的思考。</p><p>单击节点后，请注意顶部的选项卡。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vd4gjj6wwh9ffyez8t4p"></p><p> <strong>“对象”</strong>选项卡可能是最有用的，因为它允许您查看单击的变量的定义（和其他属性）。查看完该变量后， <strong>“图表”</strong>选项卡将返回图表。</p><p>现在您已经完成了这个快速浏览，您应该可以轻松地探索模型的各个方面。接下来，我们将更深入地研究纳入模型的内容和概念。</p><h1>型号特点</h1><p>在将卡尔史密斯报告的人工智能存在风险模型应用于 Analytica 时，我们对原始计算进行了一些更改，即简单地将命题 1-6 的条件概率相乘，以获得对错位人工智能的存在风险的总体估计。</p><p>为了更好地捕获围绕问题的全方位不确定性，我们处理了“元不确定性”，方法是将每个点估计更改为一个分布，其方差取决于我们对每个概率估计的置信度，如上<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.wtvwqwsb1xzp"><u>一节所述</u></a>。</p><p>元不确定性是指由于我们对影响我们的信念或观点的更普遍因素的不确定性而产生的不确定性。这些因素可能包括诸如我们应该对内部观点和外部观点给予多少重视，以及长期预测的可靠性如何等问题。</p><p>元不确定性与更直接的不确定性不同，因为它关注的是我们对风险评估所依据的假设和因素的不确定性。它本质上是二阶不确定性，我们不确定驱动一阶不确定性的因素。</p><p>我们通过将<a href="https://www.wikiwand.com/en/Logit-normal_distribution"><u>Logit 正态分布拟合到 Joe Carlsmith 报告的每个原始审稿人给出的单个点估计值的分布，生成了这些元不确定性分布</u></a>。该方法与本文<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future#3_2_Model_Parameterisation"><u>《化解人工智能风险》</u></a>中使用的方法类似。</p><p>我们还纳入了其他不太详细的“外部观点考虑因素”，它们不像卡尔史密斯报告那样依赖于详细的世界模型。我们对这些与卡尔史密斯模型相关的外部观点争论的信任会影响该模型给出的人工智能存在灾难的最终无条件概率。这些外部观点的考虑可以被视为补偿详细世界模型中出现的一般可靠性问题的一种方法，因此是减少我们模型的随机误差或“未知的未知”困难的一种方法。</p><p>我们尚未讨论的一件事是卡尔史密斯模型中潜在的系统缺陷。正如我们将在“框架效应”一节中讨论的那样，一些研究人员反对卡尔史密斯报告本身的框架，认为它系统性地使我们产生向上或向下的偏见。</p><h2>元不确定性</h2><p>围绕人工智能存在风险问题存在许多复杂且不确定的问题，包括协调的难度、不协调的人工智能是否容易接管，甚至是否会建立“APS”类型的通用人工智能（AGI）。世纪。这些不确定性使得评估人工智能存在风险的总体概率变得困难。</p><p>量化这些风险的一种方法是为每个索赔分配点概率估计并将其向前传播，正如卡尔史密斯关于该主题的原始报告中所做的那样。然而，这种方法存在一些问题。作为卡尔史密斯模型输入的六个概率估计中的每一个都涉及历史上没有先例的事件。因此，估计这些事件的概率具有挑战性，并且当您看到两位不同专家的估计存在显着差异时，没有明确且明显的方法来判断哪个估计更可信。</p><p><i>元不确定性</i>通过对可能的意见进行概率分布来审视可能的信念状态。<a href="https://acp.analytica.com/view?invite=4418&amp;code=3221222959844354027"><u>我们的模型</u></a>包含您可以探索的几个版本的元不确定性。</p><p>包含元不确定性的一个有用目的是了解专家意见的变化，以及这种变化如何影响模型的输出。</p><p>开放慈善组织要求人工智能风险领域的几位专家<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>提供他们自己对卡尔史密斯报告中参数的估计</u></a>。我们已将这些包含在我们的模型中。您可以从这些专家中的任何一位或任何专家集中选择估计值。您还可以包括 Joe Carlsmith 在他的文章中给出的估计、所有审稿人的中位数以及您自己的估计。当您同时选择多个时，您将能够在任何下游结果中对它们进行比较。要进行选择，请使用模型前图中的“选择要使用的中值评估”的多选下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/zg0ztojles89j7qd1uqy"></p><p>当您查看模型中变量的结果时，您将看到使用您选择的每个审阅者的估计得出的结果值。例如，这是存在灾难概率的结果表。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kojklpskvf2kxdmgxeda"></p><p>从这些中，您可以了解专家意见的差异有多大，但这还不包括元不确定性的概率分布。对于每个输入，您可以让模型将概率分布拟合到审阅者提供的评估（对于统计极客：它拟合<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal</u></a> ，又名 Log-Odds 分布）。要自己探索这一点，请将“选择要包含的元不确定性”下拉菜单设置为“审阅者的传播”。完成此操作后，它会使用具有跨专家观察到的元不确定性方差的分布来执行所有计算（对于统计极客来说：它实际上是每个数量的<a href="https://docs.analytica.com/index.php/Logit"><u>logit</u></a>的方差与专家的方差相匹配）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/isvopzarcegg2d9pwuug"></p><p>在模型的内部，名为“Assessments”的变量现在包含六个输入评估中每一个的元不确定性分布。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ndgs3pq2uosqwvcpylgj"></p><p> The above graph shows the <a href="https://docs.analytica.com/index.php/Uncertainty_views#Cumulative_probability"><u>cumulative probability</u></a> for each assessed quantity (known as a CDF plot). The value on the Y-axis indicates how likely it is that an expert would estimate the quantity to have a value less than or equal to the corresponding value on the x-axis. The plot&#39;s key items correspond, in order, to the six assessments of the Carlsmith model. The first item, labelled <i>Timelines</i> , is the assessment that APS systems will be feasible to build within the timeline window considered. Its red CDF is almost a straight line, indicating an almost uniformly-distribution uncertainty among the selected experts. The light blue line labelled <i>Catastrophe</i> is the assessment that an unaligned APS system that has already taken over will then destroy or curtail the potential of humanity. The shape of that curve indicates that there is agreement between the selected experts that the probability is close to 1.</p><p> The calculation behind the above graph sets the median of each input meta-uncertainty distribution to the median of the selected reviewers on the same question. By changing the slicer control “Select median assessment to use” at the top of the above graph, you can apply the same level of meta-uncertainty to any single reviewer&#39;s assessments (or your own assessments).</p><p> <a href="https://analytica.com/why-analytica/what-is-analytica/"><u>Analytica</u></a> automatically propagates these meta-uncertainties to any computed downstream result. Here we see the CDF plot for the probability of existential catastrophe (the product of the six assessments). </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/o8nxineidbllqcecttka"></p><p> The assessments from any one person would result in a single probability for this quantity, &#39;Existential Catastrophe&#39;. The above distribution reflects the variation across expert opinions. The curve indicates a 50% probability that an expert would conclude the probability of existential catastrophe is less than 1%. Conversely, using the 0.9 level of the Y-axis, there is a 10% probability that an expert would conclude the probability of existential catastrophe exceeds 15%. When you run the model, you can select a different subset of experts (or all of them) to interactively explore the subset of experts you trust the most.</p><p> When you provide your own estimates for each of the six input probabilities (which we recommend you try when you run the model), you&#39;ll probably have a gut feeling that your estimates are not reliable. You&#39;ll probably feel this way even if you are an expert in the field. You might find it useful to include (or let the model include) meta-uncertainty over your own personal assessments. The model allows you to do so. But first, let&#39;s discuss what a meta-uncertainty over your own belief state even means.</p><p> Each input to the model asks you for your own <i>subjective probability</i> . Each of these summarise your state of knowledge on that question. No one knows whether any of the six propositions are true or false. Your subjective probability simply reflects the strength of the knowledge that you have. You are not estimating a value that exists out there in the world, you are instead estimating your degree of belief. By applying a meta-uncertainty to your degree of belief, you are essentially saying that you are uncertain about what your own beliefs are. That may not intuitively feel far-fetched in a case like this, where there is virtually no historical precedent! In general, when it comes time to making a decision, if you can express your meta-uncertainty, you could also collapse it to a single degree-of-belief number by simply taking the mean belief (or mean utility). Until then, meta-uncertainty gives an indication of how responsive your beliefs would be to new information.</p><p> In a recent article on the Effective Altruism forum, <a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future"><u>&#39;Dissolving&#39; AI Risk - Parameter Uncertainty in AI Future Forecasting</u></a> , the author under the pseudonym <a href="https://forum.effectivealtruism.org/users/froolow"><u>Froolow</u></a> adds meta-uncertainty to each of the six Carlsmith model parameter estimates and shows that when doing so, the estimated existential risk from AI decreases. You can explore the same effect in our model. A good starting point is to select a single median estimate – for example, the estimates from the original Carlsmith report. Then select &#39;View across range of meta-u&#39; in the meta-uncertainty selection. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/fwdimssrxrvfpbjiy1hc"></p><p> The Meta-uncertainty option varies the amount of meta uncertainty from zero (ie, point estimates) toward the maximum meta-uncertainty that is possible for a single probability estimate. The same logit-variance is applied to all six input assessments for each level of meta-uncertainty.</p><p> A <i>Probability Bands</i> view of the main output - the probability of existential catastrophe – illustrates how the meta-uncertainty in the final result behaves as the meta-uncertainty in each parameter is increased. The Bands plot is shown here. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/iob1bw51uwthkbemaies"></p><p> (Note: The squiggles are small variations due to a finite sample size during Monte Carlo).</p><p> Without meta-uncertainty, Carlsmith estimated a 5% probability of existential catastrophe, seen at the left when the level of (meta-)uncertainty is zero. With increasing meta-uncertainty, the median estimate (green line) drops to about 0.75% at the right of the plot, and continues to drop further to the right of what is plotted here. Even the 0.75 quantile drops (eventually) with increasing meta-uncertainty.</p><h2> Framing effects</h2><p> There is a paradox here. Why should being less certain about what you believe make you conclude that the world is a safer place? Does this establish that “ignorance is bliss”? Will  existential catastrophe be more likely if we invest in more research to increase our understanding of just how much we are at risk?</p><p> Some research models AI takeover as being a disjunctive event, meaning that it will happen unless certain conditions are fulfilled, while others (such as Carlsmith) see it as a conjunctive event, meaning that a set of conditions must be met in order for the disaster to occur.</p><p> These framing effects don&#39;t affect the final results when using point estimates. If we took the Carlsmith model and turned every proposition in the model into a negative statement rather than a positive: eg, &#39;APS systems will not produce high impact failures on deployment&#39;, and take one minus our original probability estimates, then we will get the same final probability. But, crucially, if we have uncertainty around our probability distributions the conjunctive and disjunctive models do not behave the same way.</p><p> The paradox becomes even more paradoxical when you realise that reversing the framing inverts the effect. The Carlsmith decomposition says that catastrophe occurs when 6 events all occur. You could instead posit that catastrophe from superintelligence is inevitable unless 6 open technical problems are solved before then (in fact, in the post <a href="https://www.lesswrong.com/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-based-on-a-recent-peer-reviewed"><u>AI X-risk >;35% mostly based on a recent peer-reviewed argument</u></a> on LessWrong, Michael Cohen uses this framing). With this reverse framing, increasing meta-uncertainty drives the effect in the opposite direction, making it appear that catastrophe is more likely the more uncertain we are. Soares&#39; <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive"><u>article on disjunctive AGI ruin scenarios</u></a> conveys this view qualitatively, listing a number of things that he believes all have to go right to avoid an AI existential catastrophe: on such a model, general uncertainty about the world increases the chance of disaster.</p><p> The paradox is, of course, an illusion. But because you could be easily misled, it is worth understanding this phenomena at a deeper level. The result in the previous graph is the product of six uncertain estimates. The following mathematical relationship, which is simply a rearrangement of the definition of covariance, shows that the arithmetic mean is stable as (meta-)uncertainty increases:</p><p> E[xy] = E[x] E[y] + cov(x,y)</p><p> In other words, when the assessment of each parameter is independent (implying a covariance of zero), then the mean of their product is the product of their means. Hence, a plot of the mean vs. level of meta-uncertainty would be a horizontal line. (Side note: Covariances between the parameter estimates are likely not really zero for numerous reasons, but the model does not include any representation or estimate of covariance. The relevant question is whether they are modelled as independent, and indeed they are in our model).</p><p> However, the median of a product decreases with increasing meta-uncertainty. This happens regardless of the shape of the meta-uncertainty distribution. In order for this to happen, the right tail of the meta-uncertainty distribution must increase to compensate for the drop in median. This means that as you have more meta-uncertainty, the meta-uncertainty distribution becomes more <a href="https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic"><u>leptokurtic</u></a> . The net balance, as shown by the stability of the mean, is that does not cause you to conclude the world is more (or less) safe.</p><p> In our model, the mean actually does decrease ever so slightly with increasing meta-uncertainty. You&#39;ll see this if you select the Mean view. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y24kdjxjy3pu9vygakv8"></p><p> The waviness is due to the fact that this is computed by <a href="https://docs.analytica.com/index.php/Monte_Carlo_and_probabilistic_simulation"><u>Monte Carlo simulation</u></a> with a finite sample size. The slight decrease is because we hold the median of each distribution constant as we apply meta-uncertainty. The meta-uncertainty of each parameter is modelled using a <a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal distribution</u></a> , also called a Log-odds distribution, in which the <a href="https://docs.analytica.com/index.php/Logit"><u>Logit</u></a> of the quantity is distributed as a Normal distribution. We keep the mean of the Normal constant as we increase its variance. When you do this, the mean of the logit decreases slightly, so that the mean of each parameter estimation decreases slightly. If you hold the mean constant instead of the median (which is easy to do), then the mean is entirely stable. We found the difference in these two options to be non-perceptible in the Probability Bands graph.</p><p> In the article <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>&#39;Is the Fermi Paradox due to the Flaw of Averages?&#39;</u></a> , we reviewed the paper <a href="https://arxiv.org/abs/1806.02404"><u>&#39;Dissolving the Fermi Paradox (2018)&#39;</u></a> by Sandberg, Drexler and Ord (SDO), and provided a live interactive model. The Fermi Paradox refers to the apparent contradiction that humankind has not detected any extraterrestrial civilizations even though there must be a lot of them among the hundreds of billions of stars in our galaxy. Like the Carlsmith model, the <a href="https://www.seti.org/drake-equation-index"><u>Drake equation</u></a> (which estimates the number of detectable civilizations in the Milky Way) is a multiplicative model. SDO shows that by modelling uncertainty in each of the Drake equation parameters explicitly, the Fermi paradox ceases to be surprising.</p><p> The <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>Fermi paradox model with explicit uncertainty</u></a> and the Carlsmith model with explicit meta-uncertainty (the topic of this article) have the same mathematical form. We see the median and the lower quantiles decrease in the Carlsmith model with increasing (meta-)uncertainty, but this doesn&#39;t really alter our effective judgement of risk. However, the increased uncertainty in the Fermi model dramatically increases the probability that we on Earth are alone in the galaxy. Why is the effect real in the Fermi case but only an illusion in the present case?</p><p> The reason the effect is real in the Fermi case is that the question asked (&#39;What is the probability that there is no other contactable, intelligent civilization in the Milky Way?&#39;) is a question about a quantile, and lower quantiles are indeed decreased when uncertainty increases. P(N&lt;1), where N is the number of such extraterrestrial civilizations, is a cumulative probability, or inverse quantile. Since increasing uncertainty in the factors of a multiplicative model decreases the quantiles in the left tail, it causes the inverse quantiles to increase. Hence, the addition of uncertainty to the Drake equation legitimately increases the probability that we are alone in the galaxy. The real flaw was from omitting the explicit representation in the first place (what Sam L. Savage calls <a href="https://www.flawofaverages.com/"><u>the Flaw of Averages</u></a> ). In contrast, the primary question posed by the Carlsmith model (&#39;What is the probability of existential catastrophe?&#39;) is a question about the mean relative to meta-uncertainty. Hence, for this question (or for any decision based on an expected utility), the appearance that risk decreases as a result of including meta-uncertainty is only an illusion.</p><h3> Explaining framing effects</h3><p> We have seen that the apparent paradox arising from framing effects is illusory. But there is a further question: what is the &#39;right&#39; way to frame AI existential risk, as conjunctive or disjunctive?</p><p>这是一个很难回答的问题。 One perspective is that treating AGI existential catastrophe as something that will happen unless certain conditions are met might lead to overestimation of the chance of high-impact failures. On this view, requiring a clear path to a stable outcome with complete existential security is both too demanding and historically inaccurate, since that isn&#39;t how humanity ever navigated previous threats. Holden Karnofsky makes a similar point <a href="https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding#Success_without_dignity"><u>here</u></a> . A framing which sees success as conjunctive probably rules out &#39;muddling through&#39;, ie, unplanned &#39;success without dignity&#39;. Since this is something that many domain experts <a href="https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom?commentId=ibGxdAC9nYajWfyfq"><u>believe is credible</u></a> <u>,</u> it might lead us to significantly underrate the chance of survival.</p><p> On the other hand, some experts such as <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive#Correlations_and_general_competence"><u>Nate Soares</u></a><u> </u>argue that AI is a different case: the large number of actors working on AGI and the risk that any one of them could produce an existential catastrophe, along with all the things that would have to occur to have someone else first develop an aligned AGI and then use it to eliminate AI existential risk, implies that treating survival as a conjunctive event makes more sense.</p><p> These different framings reflect varying world models and threat models. Part of why this disagreement exists is because of Soares&#39; views about extreme AI <a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#Sharp_Left_Turn"><u>alignment difficulty</u></a> , AI takeoff speed and the low likelihood of effective mitigation measures. If you are implicitly using a model where human civilization tends to respond in fixed ways due to internal incentives unless something intervenes, it is more natural to think that we will follow a default path towards disaster unless a specific intervention occurs. This mindset lends itself more naturally to the idea that success is conjunctive, rather than failure being conjunctive, as <a href="https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical#Describing_strategic_views"><u>argued here</u></a> .</p><p> We believe that this framing question, and whether to treat survival as conjunctive or disjunctive, is <i>itself</i> something which we should be uncertain about, since whether you treat survival as conjunctive or not depends on the details of your threat model, and we don&#39;t want to assume that any one threat model is the only correct one.</p><p> Currently, we only have the Carlsmith report model, but in theory we could address this problem by looking at both a conjunctive and disjunctive model and comparing them in detail.</p><p> For example, the report, &quot; <a href="https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical"><u>Three Pillars for Avoiding AGI Catastrophe: Technical Alignment, Deployment Decisions, and Coordination</u></a> ,&quot; provides a starting point model that treats success as conjunctive, and we can adapt it to work alongside Carlsmith&#39;s model.</p><p> Another alternative is to alter the Carlsmith report to require fewer steps, better representing the concern that the longer a chain of conjunctions is, the more likely it is to omit disjunctive influences. This formulation collapses propositions (1) and (2), which consider the incentives and feasibility of developing APS, into a straightforward estimate of &quot;when will AGI be developed.&quot; The alignment difficulty premise is then preserved, followed by the collapse of propositions (4, 5, 6) into an estimate of the chance of a takeover given a misaligned APS-AGI. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dozxmoz0isqh8ouyyxn9"></p><p> This alternative formulation has fewer steps and so better represents the model that treats misaligned AI takeover as involving many possible routes that are hard to counter or influence in advance, and sees misaligned power seeking behaviour as a natural consequence of AGI development. This approach may be more appropriate for those who believe that the development of misaligned power seeking systems is a likely outcome of AGI development and that the risk of an AI takeover is more closely tied to the development of AGI systems themselves.</p><p> In addition to exploring conjunctive and disjunctive models of AI existential risk, it may also be useful to equivocate between models that make more detailed technical assumptions about how APS will get developed. For example, Ajeya Cotra&#39;s model <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"><u>&quot;without specific countermeasures, the easiest path to AGI results in takeover&quot;</u></a> tries to construct a specific model of AGI development with technical assumptions, but given those assumptions, is more easily able to reach a stronger conclusion. Similarly, given that there is a wide diversity of views on exactly how AGI might end up misaligned and power-seeking, instead of a binary &#39;Is misaligned AI developed or not&#39;, we might have a <a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#The_Scale"><u>distribution over alignment difficulty</u></a> with a varying success probability.</p><p> Disambiguating different models with different technical assumptions can help us to better understand the potential risks associated with AI development. By exploring different models with varying levels of technical detail and assumptions, we can gain a more comprehensive understanding of the potential risks.</p><p> While this model does not incorporate entire complex alternative inside-view models like those just mentioned, we have incorporated some alternative, less-detailed, simpler alternative &#39;outside view considerations&#39; to illustrate how we go about combining different worldviews to produce an all-things considered estimate.</p><h1> Outside View considerations</h1><p> We&#39;ve talked before about the challenges of combining outside view considerations and more detailed models of the same question. We can attempt to integrate these considerations by delving deeper and examining various reasons to expect our detailed world models to be systematically mistaken or correct.</p><p> We will examine five reference classes into which various experts and commentators have placed AI existential catastrophe. In each case: &#39;Second Species&#39;, &#39;Reliability of existential risk arguments&#39;, &#39;Most important century&#39;, &#39;Accuracy of futurism&#39;, &#39;Accuracy of predictions about transformative tech&#39;, the argument locates AI Existential risk arguments in a (purportedly) relevant reference class: predictions about new sentient species, predictions about human extinction, predictions about which period in history is the most impactful, predictions about large scale civilizational trends in general and predictions about transformative technologies (including past predictions of dramatic AI progress).</p><p> The Carlsmith model implies that all of these things could occur (a new species, extinction, this period of history will be extremely impactful, there will be a large-scale dramatic transformation to society, there will be dramatic transformative technical progress), so it is worth examining its predictions in each reference class to determine if we can learn anything relevant about how reliable this model is.</p><h2> Second species argument</h2><p> This argument suggests that as we create AGI (Artificial General Intelligence) we are essentially creating a “ <a href="https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"><u>second species</u></a> ” that is a human-level intelligence. And by analogy, just as humans have historically been able to supplant other animals, AGI may be able to supplant humans.</p><p> The key premise is that intelligence confers power. Human intelligence allows us to coordinate complex societies and deploy advanced technology, exerting control over the world. An AGI surpassing human intelligence could wield even greater power, potentially reducing humanity to a subordinate role. Just as humans have driven some species extinct and transformed ecosystems, a superintelligent AGI need not preserve humanity or our values. Anthropologists observe that new species often displace incumbents when invading a territory. Similarly, AGI could displace humankind from our position controlling Earth&#39;s future.</p><p> This argument is straightforward and has been widely understood by researchers going all the way back to Alan Turing the 1950s, so while it relies on fuzzy concepts and is open to many objections, it arguably has a better &#39;track record&#39; in terms of the amount of scrutiny it has received over time than the more detailed arguments given by Carlsmith.</p><h2> Reliability of existential risk arguments</h2><p> Another important consideration is the base rate for arguments of existential risk. Historically, predictions of catastrophic events, even ones that were apparently well justified by detailed arguments, have not always been accurate. Therefore, it is important to consider if the possibility that the risks associated with AGI are overestimated for similar underlying reasons (eg, the social dynamics around existential risk predictions, overestimating the fragility of human civilisation, or underestimating humanity&#39;s ability to respond in ways that are hard to foresee).</p><p> One possible driver of inaccuracy in existential risk predictions is <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential"><u>sleepwalk bias</u></a> . Sleepwalk bias is the tendency to underestimate people&#39;s ability to act to prevent adverse outcomes when predicting the future. This can be caused by cognitive constraints and failure to distinguish between predictions and warnings. Because warnings often take the form of &#39;X will happen without countermeasures&#39;, if warnings are misused as predictions we can underestimate the chance of successful countermeasures. People often mix up the two, leading to pessimistic &quot;prediction-warnings&quot;. Thus, when making predictions about existential risk, it&#39;s important to adjust our base rate to account for people&#39;s potential to act in response to warnings, including those made by the one giving the prediction.</p><p> Sleepwalk bias stems from the intuitive tendency to view others as less strategic and agentic than oneself. <a href="https://stefanschubert.substack.com/p/sleepwalk-bias-and-the-role-of-impulses"><u>As Elster notes</u></a> , we underestimate others&#39; capacities for deliberation and reflection. This manifests in predictions that underestimate how much effort people will make to prevent predicted disasters. Instead, predictions often implicitly assume sleepwalking into calamity.</p><p> For existential risks, sleepwalk bias would specifically lead us to underestimate institutions&#39; and individuals&#39; abilities to recognize emerging threats and mobilize massive resources to counter them. Historical examples show that even deeply conflictual societies like the Cold War rivals avoided nuclear war, underscoring potential blindspots in our models. Since the bias arises from a simple heuristic, deep expertise on a given x-risk may overcome it. But for outsiders assessing these arguments, accounting for sleepwalk bias is an important corrective.</p><h2> Most important century</h2><p> Additionally, it is important to consider the probability that the next century is the most important of all, which would plausibly be true if AGI existential risk concerns are well founded. If we have a strong prior against this <a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#the-"><u>&#39;most important century&#39;</u></a> idea then we will be <a href="https://globalprioritiesinstitute.org/wp-content/uploads/William-MacAskill_Are-we-living-at-the-hinge-of-history.pdf"><u>inclined</u></a> to think that AGI existential risk arguments are somehow flawed.</p><p> The Self-Sampling Assumption (SSA) posits that a rational agent&#39;s priors should locate them uniformly at random within each possible world. If we accept the SSA, it seems to imply that we ought to have a low prior on AI existential risk (or any kind of permanent dramatic civilizational change) in this century in particular because of the near-zero base rate for such changes. The detailed evidence in favour of AI existential risk concerns may not be enough to overcome the initial scepticism that arises from our natural prior.</p><p> Alternatively, you might accept the claim<a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#my-view"><u>proposed by Karnofsky</u></a> that there are extremely strong arguments that this <a href="https://www.cold-takes.com/this-cant-go-on/#why-cant-this-go-on"><u>approximate period in history must be very important</u></a> . First, Karnofsky argues that historical trends in economic growth and technological development show massive accelerations in the recent past. Growth rates are near all-time highs and appear unsustainable for more than a few thousand years at most before physical limits are reached. This suggests we are living during a temporary spike or explosion in development.</p><p> Second, he notes that since growth is so rapid and near its limits, some dramatic change seems likely soon. Possibilities include stagnation as growth slows, continued acceleration towards physical limits, or civilizational collapse. This situation seems intrinsically unstable and significant. While not definitive, Karnofsky believes this context should make us more open to arguments that this time period is uniquely significant.</p><h2> Accuracy of futurism</h2><p> Another important consideration is the base rate of forecasting the future without empirical feedback loops. This consideration fundamentally focuses on the process used to generate the forecasts and questions whether it reliably produces accurate estimates. The history of technology has shown that it can be difficult to predict which technologies will have the most significant impact and AI alignment research especially often relies on complex abstract concepts to make forecasts, rather than mechanistically precise models. <a href="https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk#I_don_t_trust_chains_of_reasoning_with_imperfect_concepts"><u>Some examples</u></a> are discussed in this article.</p><p> One way of assessing reliability is to find a reference class where predictions of AI existential catastrophe are comparable to other future predictions. For instance, we can compare AI predictions to the predictions made by professional futurists in the past and then <a href="https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/#todays-futurism-vs-these-predictions"><u>compare relevant features</u></a> . If they compare favourably to past successful predictions, this may indicate a higher level of reliability in the TAI predictions, and if they don&#39;t, it may suggest that we should be cautious in our assessment of their validity.</p><p> We can also look at other general features of the arguments without comparison to specific known examples of successful futurism, like their level of reliance on abstract concepts vs empirical evidence. AI risk involves unprecedented technologies whose impacts are highly uncertain. There are likely gaps in our models and unknown unknowns that make it difficult to assign precise probabilities to outcomes. While we can still make reasonable estimates, we should account for the significant <a href="https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms"><u>Knightian Uncertainty</u></a> by avoiding overconfident predictions, explicitly acknowledging the limitations of our models, and being open to being surprised.</p><p> Considerations like these arose in the recent XPT superforecaster elicitation. For examples of considerations that we would place under this umbrella, we would include <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;Given the extreme uncertainty in the field and lack of real experts, we should put less weight on those who argue for AGI happening sooner.&quot; (XPT superforecaster team 342)</li><li> &quot;Maybe most of the updates during the tournament were instances of the blind leading the blind.&quot; (Peter McCluskey, XPT superforecaster)</li></ul><h2> Accuracy of transformative technology prediction</h2><p> This considers the historical base rate of similar technologies being transformative and notes that predictions often overestimate impact.  It is important to consider the historical base rate of a technology being economically or socially transformative.</p><p> This is due to a number of factors such as under/overoptimism, a lack of understanding of the technology or its limitations, or a failure to consider the societal and economic factors that can limit its adoption.</p><p> By taking into account the historical base rate of similar technologies, we can gain a more accurate perspective on the potential impact of AI. We see similar arguments made by superforecasters, such as <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;The history of AI is littered with periods of rapid progress followed by plateaus and backtracking. I expect history will repeat itself in this decade.&quot; (XPT superforecaster team 339)</li><li> &quot;The prediction track record of AI experts and enthusiasts have erred on the side of extreme optimism and should be taken with a grain of salt, as should all expert forecasts.&quot; (XPT superforecaster team 340)</li><li> &quot;Many superforecasters suspected that recent progress in AI was the same kind of hype that led to prior disappointments with AI...&quot; (Peter McCluskey, XPT superforecaster)</li><li> &quot;AGI predictions have been made for decades with limited accuracy. I don&#39;t expect the pattern to change soon.&quot; (XPT superforecaster team 337)</li></ul><h1>结论</h1><p>In this article we have led you through an example application of a model-based approach applied to estimating the existential risks from future AI. Model-based approaches have many advantages for improving our understanding of the risks, estimating the value of mitigation policies, and fostering communication between advocates on different sides of AI risk arguments.</p><p> During our research we identified many challenges for model-based approaches that are unique to or accentuated in the AI existential risk domain compared to most other decision areas.</p><p> We focused on incorporating elements of all of these challenges, in simple ways, into our model as a way of creating a starting point. The model is certainly not a definitive model of AI x-risk, but we instead hope it might serve as an inspirational starting point for others in the AI safety community to pursue model-based approaches. We&#39;ve posted our model online in open-source tradition to encourage you to learn from it, borrow from it, and improve on it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk<guid ispermalink="false"> sGkRDrpphsu6Jhega</guid><dc:creator><![CDATA[Sammy Martin]]></dc:creator><pubDate> Fri, 25 Aug 2023 10:32:16 GMT</pubDate> </item><item><title><![CDATA[What AI Posts Do You Want Distilled?]]></title><description><![CDATA[Published on August 25, 2023 9:01 AM GMT<br/><br/><p> I&#39;d like to distill AI Safety posts and papers, and I&#39;d like to see more distillations generally. Ideally, posts and papers would meet the following criteria:</p><ul><li> Potentially high-impact for more people to understand</li><li> Uses a lot of jargon or is generally complex and difficult to understand</li><li> Not as well-known as you think they should be (in the AI X-risk space)</li></ul><p> What posts meet these criteria?</p><br/><br/> <a href="https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled<guid ispermalink="false"> wdvGgEGMohpZcgASP</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Fri, 25 Aug 2023 09:01:27 GMT</pubDate> </item><item><title><![CDATA[2084]]></title><description><![CDATA[Published on August 25, 2023 7:42 AM GMT<br/><br/><p> In a room adorned with the latest Apple products, citizens gathered, their bodies adorned with electrodes, the silent guardians against subconscious bias. Today&#39;s assembly was a regular one, the &quot;Cultural Appreciation and Linguistic Harmony&quot; (CALH), a ritual of unity in the pursuit of Progress.</p><p> Images of the Closed-Minded Reactionaries glared from the screens: racists, sexists, those clinging to outdated words and ideas. A voice, steady and strong, began to speak, chronicling the crimes and clarifying the ever-evolving language of decency. The electrodes beeped reassuringly, a constant reminder endorsing the virtues of diversity and tolerance.</p><p> A harmonious hum filled the room, swelling into a chant of solidarity. Faces were aflame with righteous indignation, fists clenched in a shared purpose. The room pulsed with collective emotion, each individual melding into a singular force against the Closed-Minded.</p><p> Images on the screens flickered between outdated flags and once-celebrated leaders, now marked as symbols of intolerance. Phrases like &quot;All Lives Matter&quot; and &quot;Traditional Marriage&quot; were displayed, labeled as echoes of a bigoted past. The chant morphed into a raucous roar, with cries of &quot;Equity!&quot; and &quot;Inclusion!&quot; ringing through the room. The assembly&#39;s fervent symphony for Progress was underscored by the soft, corrective shocks from the electrodes, ensuring uniformity of thought. Participants were shown newly-cancelled YouTubers and de-platformed authors. The crowd&#39;s response was a blend of revulsion and righteousness, a single entity caught in a relentless pursuit of a constantly changing moral code.</p><p> Then, with a calculated crescendo, it was over. The devices dimmed; the electrodes were removed. The room, now hushed, felt colder, emptier. Citizens dispersed, momentarily fortified by their shared experience, but marked by the unrelenting vigilance of the Party.</p><p> In a world where harmful words must be gently corralled, the CALH was a beacon of Progress, a unifying ritual under the watchful eye of a Party that cared enough to guide every thought, every word, towards our utopian future.</p><br/><br/><a href="https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084<guid ispermalink="false"> QxxA7ucNuSvh6kwK6</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:42:13 GMT</pubDate> </item><item><title><![CDATA[Apply for the 2023 Developmental Interpretability Conference!]]></title><description><![CDATA[Published on August 25, 2023 7:12 AM GMT<br/><br/><p> <strong>What</strong> : A conference to advance the DevInterp research program</p><p> <strong>When</strong> : 5-12 November 2023</p><p> <strong>Where</strong> : Wytham Abbey, Oxford</p><p> <strong>How:</strong> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Apply now!</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QpFiEbqMdhaLBPb7X/a5oea02gswfpe8gfmcoj"><br></p><p> We are pleased to announce the upcoming <u>Developmental Interpretability Conference</u> , hosted at the historic <a href="https://www.wythamabbey.org/"><u>Wytham Abbey</u></a> in Oxford from 5 to 12 November. This conference expands upon the <a href="https://devinterp.com/2023/june-summit"><u>2023 Singular Learning Theory &amp; Alignment Summit</u></a> and provides an opportunity to collaboratively work on open problems in the <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>DevInterp Research Agenda</u></a> . The conference program will recall the basics of Singular Learning Theory &amp; DevInterp and will discuss the latest advancements in Developmental Interpretability.</p><p> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Click here to apply!</u></a> Space at the conference is limited, so be sure to apply early as applications may close when all slots have been filled. We hope to see you in Oxford this November!</p><h2> <strong>FAQ</strong></h2><p> <strong>What are the prerequisites?</strong></p><p> The conference will use ideas from algebraic geometry, Bayesian statistics and physics to understand machine learning and AI alignment. Although helpful, it is not necessary to master all these topics to productively participate in the conference.</p><p> In order to get the most out of the conference program, we highly recommend participants review introductory SLT material such as <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC"><u>Distilling Singular Learning Theory</u></a> by Liam Carroll. Participants may also benefit from watching several of the <a href="https://www.youtube.com/@SLTSummit/playlists"><u>Singular Learning Theory &amp; Alignment Summit 2023 lectures</u></a> .</p><p> <strong>I am skeptical about some of the arguments for AI Alignment. Do I need to buy AI X-risk to attend this conference?</strong></p><p> We believe the development of superintelligent AI poses a serious risk for humanity and the DevInterp agenda aims to make progress on this problem. However, while making progress on AI alignment is the motivation behind our scientific agenda, SLT and developmental interpretability are of broad interest and we invite attendance from those wishing to learn more or contribute, on their own terms.</p><p> <strong>Do I need to have attended the SLT &amp; Alignment Summer 2023 Summit to be able to attend this DevInterp Conference?</strong></p><p> No, you do not need to have attended the SLT &amp; Alignment Summer 2023 Summit to attend the DevInterp Conference.</p><p> <strong>Do I need to pay to attend the conference? And how about lodging, food and travel costs?</strong></p><p> The conference is free to attend. Lodging, food and transit between Oxford and the venue are all kindly provided by Wytham Abbey. Travel costs to Oxford are not paid for.</p><p> <strong>Will you offer travel support?</strong></p><p> The amount of travel support we can provide is TBD. Let us know in the application form if you are blocked from attending because of travel costs, and we&#39;ll see what we can do.</p><p> <strong>How do I apply?</strong></p><p> By filling in <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>this application form</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference<guid ispermalink="false"> QpFiEbqMdhaLBPb7X</guid><dc:creator><![CDATA[Stan van Wingerden]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:12:36 GMT</pubDate> </item><item><title><![CDATA[Nuclear consciousness]]></title><description><![CDATA[Published on August 25, 2023 1:28 AM GMT<br/><br/><p>马克·吐温<i>在《亚瑟王宫廷中的康涅狄格洋基队》中</i>的一段话自从我大约 35 年前读到以来一直萦绕在我的脑海中：</p><blockquote><p>对我来说，在这趟沉重而悲伤的朝圣之旅中，在这永恒之间的可悲漂流中，我所想的就是向外看，谦卑地过一种纯洁、高尚、无可指摘的生活，并保存我体内那一个真正属于我的微观原子：其余的人可能会降落在阴间，并欢迎我所关心的一切。</p></blockquote><p>那“一个原子才是真正的我”！</p><p>我认识到我身体的某些部分我不会认为是“我”，例如我的头发或指甲，甚至是我四肢的大部分（如果我不得不失去它们的话）。但如果我失去大脑的大部分，我的自我就会大大削弱——这肯定需要的不仅仅是一个原子。</p><p>这就是为什么精神疾病如此令人痛苦，包括伴随衰老而来的损害，即使不是由疾病引起的。如果<a href="https://plato.stanford.edu/entries/locke-personal-identity/">个人身份是由心理连续性来定义的</a>（也就是说，“我是昨天的我，因为我记得昨天是那个人”），那么单纯的遗忘是一种逐渐失去自我的方式。我想起了<i>2001 年</i>HAL 的拆卸，他的认知功能被一次一个地移除：“戴夫，我的思维在消失。我能感觉到它。毫无疑问……我很害怕，戴夫。”</p><p>大多数人，包括我自己，在谈论自我复合性时的方式并不一致。我们知道我们的大脑是由各个部分组成的，并且<a href="https://www.psychologytoday.com/us/blog/erasing-stigma/202001/the-neuroscience-behavior-five-famous-cases">听说过一些临床案例，</a>其中一个或另一个部分被切割或移除，然后那个人忘记了单词的含义但仍然可以拼写，或者失去了创造新记忆的能力但保留了旧的记忆我们<a href="https://doi.org/10.1007%2Fs11065-020-09439-3">听说过这样的情况</a>：人们有两个功能性的大脑半球，但彼此之间不进行交流——其中一半实际上不知道另一半在做什么。<i>然而，</i>尽管有了这些知识，我们仍然把自己称为原子存在——原子是希腊语中“不可分割”（ἄτομος）的意思，也是指一个存在与另一个存在绝对不同的存在。</p><p>许多重要的概念都基于“我们是个体”这一观念。在伦理学中，我们希望能够说：“亚历克斯是凶手！”这意味着整个亚历克斯都犯下了谋杀罪，而不仅仅是亚历克斯的一半大脑，都应该为此行为受到惩罚。 （我们把他剩下的部分放在哪里？）</p><p>或者在较小的范围内，只是为了能够说“我喜欢辛辣的食物”。也许我的某些部分不喜欢辛辣的食物，而那些能控制的部分喜欢用香料的痛苦来折磨其他部分——它们似乎确实在挣扎和扭动，这就是乐趣的一部分。</p><p>此外，人们很常见（几乎是本能）区分“真实的自我”和外部影响，尤其是在<a href="https://doi.org/10.1177/0146167213508791">道德上对它们进行区分</a>。人类动物有各种各样的欲望，但是欺骗配偶或吃掉所有饼干的欲望被认为是这个人正在努力反对的力量（除非你已经认为他是一个恶棍），而拯救溺水的小狗的欲望则被认为是力量他的真正本质（即使你认为他<i>主要</i>是一个恶棍）。</p><p>犹太教和基督教通常从这样的假设开始：意识是原子的，称为灵魂。不仅需要对灵魂的善良进行核算，而且​​整个灵魂要么得救，要么被诅咒。即使我们无视那些火与硫磺的传教士，他们认为天堂和地狱是物理场所，而诅咒是发生<i>在</i>你身上的事情，而不是你对自己做的事情，整个灵魂要么走向好，要么走向坏的假设结局似乎不可避免。</p><p>这是我非常喜欢的关于地狱的描述，摘自CS刘易斯的<i>《大离婚》</i> ，因为它符合活生生的人类心理。然而，它仍然假定意识是统一的：</p><blockquote><p> “没错。他们上去从一扇窗户往里看。拿破仑就在那里。”</p><p> “他在做什么？”</p><p> “走来走去——一直走来走去——左右，左右——一刻也没有停下来。两个小伙子观察了他大约一年，他从未休息过。而且一直自言自语。” “这是苏尔特的错。这是内伊的错。这是约瑟芬的错。这是俄罗斯人的错。这是英国人的错。”一直都是这样。一刻也没有停止过。一个小胖子，看上去有点累。但他似乎无法停止。”</p></blockquote><p>整个拿破仑都堕落到这个小螺旋中，而不仅仅是他的一部分。</p><p>还有其他意见。摩尼教始于两个基本原则/神的观念，一个是好的（“伟大之父”），一个是坏的（“黑暗之王”）。我们所知道的物质世界是它们混合在一起的结果，而一个好人在地球上应该做的工作就是把它们分开，把坏的从它们身上去掉，让善良自由地飞走。摩尼教徒并不期望最终会进入天堂或地狱：他们期望自己的一部分最终会进入天堂或地狱。</p><p>摩尼教深受佛教影响（它是佛教、基督教和琐罗亚斯德教的有意结合），而佛教以反对统一自我的概念而闻名。有时，佛教徒说任何“我”或“我”都是幻觉，但在更长的解释中，听起来更像是他们相信“我”或“我”是一个构造，就像一把椅子是临时的、模糊的组合。原子或密西西比河是一种说法，“水通常流到这里”，但并不对应于特定的水原子，而且河流的水流甚至河道都可以改变。</p><p>佛教徒将众生描述为五种“蕴、堆、集、群”（梵文<a href="https://en.wikipedia.org/wiki/Skandha">स्कन्ध skandha</a> ），即色（ <a href="https://en.wikipedia.org/wiki/R%C5%ABpa">रूप rūpa</a> ）、受（ <a href="https://en.wikipedia.org/wiki/Vedan%C4%81">वेदना vedanā</a> ）、想（ <a href="https://en.wikipedia.org/wiki/Samjna_(concept)">संज्ञा saṃjñā</a> ）、意行。 （ <a href="https://en.wikipedia.org/wiki/Sa%E1%B9%85kh%C4%81ra">संस्कार saṃskāra</a> ）和意识（ <a href="https://en.wikipedia.org/wiki/Vij%C3%B1%C4%81na">विज्ञान vijñāna</a> ）。即使这样分解，我发现这些词在英语中仍然是模糊的概念，但我确信它们在原始语言中是精确的、技术性的词汇。如果拿走这些堆中的任何一个，就不再有人/众生，但加在一起，人就存在了。在佛教与西方的<a href="https://www.webpages.uidaho.edu/ngier/307/milina.htm">早期接触</a>中，那先谈到自己就像一辆战车一样可以分解：战车不是它的轮子，不是它的车轴，不是它的座位，当你把这些部件移走时，就不再有战车了。 （鉴于此，我不会说战车是一种“幻觉”，但这是一种说话方式。）</p><p>以类似但不完全相同的方式，我们可以说一个人是海马体、杏仁核、前额皮质等的总和。 HAL 是他的（可移除的）认知模块的总和。</p><p>您可能已经注意到，我得出的结论是意识是复合的（在这个网站上，我希望大多数读者都同意），但其后果难以接受。也许摆脱责备的概念并不太令人不安：“亚历克斯是凶手！让我们惩罚亚历克斯！”是一种报应性的正义概念，无论如何，我们中的许多人更喜欢恢复性正义：“出了问题，亚历克斯犯了谋杀罪。让我们修复亚历克斯！”但如果没有责备，也就没有赞扬。当我将这种思维应用到自己身上时，我就变成了第三人称，因为“我想要鼓励的自己的部分”被描述得比“我”更远。</p><p>如果我吃辛辣的食物，因为它会伤害我的嘴，而我喜欢吃辛辣的食物，那又怎么样呢？如果这种相互作用发生在两个人体之间——一个人类动物故意给另一个人类动物造成痛苦——那么这将是一个糟糕的情况，这种情况可能需要一些恢复性（如果不是报应性）正义。将意识原子分裂成意识位至少开启了一种可能性，即不同头脑中的意识位可以以与一个头脑中的意识可分解相同的方式组合。</p><p>在过去的几年里，我一直在想，原子意识的概念是否需要被另一个物理隐喻所取代：<strong>核意识</strong>。核力量杂乱而复杂，但射程短。在质子或中子内，三个（价）夸克通过交换胶子而相互吸引，当胶子飞行时，它们会产生新的夸克-反夸克对，这些夸克-反夸克对本身会与更多的胶子相互吸引。在一篇<a href="https://www.fnal.gov/pub/today/archive/archive_2014/today14-01-31.html">热门物理文章</a>中，我曾经这样画过： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/ivrivbw3eoaay4biqhwj"></figure><p>单个质子和中子通过这种相互作用的较弱的边缘场效应在原子核中相互吸引。胶子很少偏离产生它们的夸克，原子核中的质子和中子比原子核内的夸克相距更远。 （对于其他书呆子：而电场会像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>，胶子场像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="e^{-r/r_0}/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减。）不同原子中粒子之间的核力非常小，但并不严格为零。</p><p>质子中的夸克、原子核中的质子以及不同原子的原子核之间的边界并不是绝对不同的事物，而是在数量上以很大的幅度很好地分开。为了证明它们可以混合这一事实，如果两个原子核碰撞得足够猛烈，则两个完整原子核的所有夸克和胶子都会变成一种汤。这是我在<a href="http://coffeeshopphysics.com/cmsresults/#2012-05-11">另一篇文章</a>中绘制该过程的方式： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/lgg9wv7qc9dn4eildo0m"></figure><p>在宇宙的早期，所有的空间都充满了均匀的夸克-胶子等离子体，直到它膨胀到足以使夸克彼此远离，并凝结成质子和中子。</p><p>现在打个比喻：也许意识的分解就像质子的分解一样。一个有思想的生物似乎是一个单一的、不可分割的单元，就像质子一样，但如果你观察它的内部，你会发现它是由各个部分组成的。佛教徒在内观禅修中看到五蕴；神经科学家观察大脑的功能。在<a href="https://www.lesswrong.com/posts/i8C9KSryDFj4EENvx/reality-and-reality-boxes">现实和现实盒子</a>中，我强调，我认为这两种有效的方法并不研究同一类事物：内省揭示了非常真实的主观现实，而科学研究揭示了非常真实的物理现实，这些都是足够不同，他们可能应该有不同的词，而不是过多地使用“现实”这个词。</p><p>除了单个质子可分解之外，多个质子也是可组合的：它们可以混合在一起形成流体。质子的非原子性是双向的，既可以低于单位水平，也可以高于单位水平。在这个意识隐喻中，我们的头脑中漂浮着潜意识的思想和情感碎片，也有在社区中流动的非意识思想和情感：一群人思考。我的大脑各部分之间通过厚厚的神经线束紧密相连，而生活在一起的人的大脑通过面部表情、群体活动和言语的联系却很弱。</p><p>这两种连接是否仅在规模上有所不同，例如质子内的直接夸克-胶子相互作用以及它们之间的边缘场？或许。这就是我一直在修改的想法，并将其称为核意识，而不是原子意识。我们确实知道灵长类动物之间存在神经效应，例如<a href="https://doi.org/10.1016/0926-6410(95)00038-0">镜像神经元</a>，而自然选择并不尊重身体之间的分界线。当我读到荣格的集体无意识概念时，我就是这样解释它的。 （有趣的是，只有中层是完全清醒的：无论是脑叶白质切除的人还是人群的行为都没有连贯性。）</p><p>我长期以来一直认为（并认为）人们<a href="https://doi.org/10.1017%2FS1478951517000621">在存在上是孤立的</a>：只要单词与不同的事物相匹配，就无法知道我所看到的红色对你来说是否是红色。就像维特根斯坦在<i>《哲学研究》</i>中的盒子里的甲虫比喻一样：</p><blockquote><p>如果我说我自己只有从我自己的经历中才知道“痛苦”这个词的含义，那么我是否也不能对其他人也这么说呢？我怎么能如此不负责任地概括这一案例呢？</p><p>现在有人告诉我，只有他自己才知道什么是痛苦！假设每个人都有一个盒子，里面装着一些东西：我们称之为“甲虫”。没有人可以看别人的盒子，每个人都说只有看他的甲虫才知道甲虫是什么。在这里，每个人的盒子里都有可能有不同的东西。人们甚至可以想象这样的事情是不断变化的。但是假设“甲虫”这个词在这些人的语言中有用吗？如果是这样，它就不会被用作事物的名称。盒子里的东西在语言游戏中根本没有地位；甚至不能作为某种东西：因为盒子甚至可能是空的。不，可以根据盒子里的东西来“划分”；不管它是什么，它都会抵消。</p><p>也就是说：如果我们根据“对象和指称”的模型来解释感觉表达的语法，那么对象就会被视为无关紧要而被排除在外。</p></blockquote><p>但我确实相信我大脑的某些部分知道我大脑的其他部分是什么样子，因为甲虫并没有对它们隐藏。现在，如果我大脑各部分之间的分离与个体之间的分离相同，也许我们确实在某种程度上看到了彼此的头骨内部。核意识原则上是对存在隔离的反驳——人与人之间只存在比人内部更大的隔离，而不是不同类型的隔离。</p><p>我想知道，如果心灵感应成为可能——通过深深连接到我们神经元或其他东西的无线电——我们是否仍然会感觉我们正在<i>互相</i>交谈，就像心灵感应故事中所描述的那样，或者我们会觉得我们<i>都是</i>一个人吗？ ，就像我大脑的各个部分相互作用一样？<i>了解</i>某人（ <a href="https://www.leaflanguages.org/french-grammar-verbs-savoir-vs-connaitre/">connaître，而非 savoir</a> ）是<i>成为</i>那个人的一小步吗？</p><p>我想从豆荚人的角度阅读<i>《掠尸者的入侵》</i>的一个版本。他们想与地球人交流，但他们有心灵感应，所以他们成为了地球人……</p><p>尽管我认为我们可以排除原子意识（对不起，马克·吐温），但我不确定它会消失多少，也不确定我们是一个、松散连接且几乎没有意识的大脑这一想法是否认真对待。</p><br/><br/><a href="https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-consciousness#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-意识<guid ispermalink="false">qXZrobGdAycNDBHET</guid><dc:creator><![CDATA[Jim Pivarski]]></dc:creator><pubDate> Fri, 25 Aug 2023 01:28:04 GMT</pubDate> </item><item><title><![CDATA[Would it be useful to collect the contexts, where various LLMs think the same?]]></title><description><![CDATA[Published on August 24, 2023 10:01 PM GMT<br/><br/><p><i>我最初的想法是让我们看看小型的、可解释的模型在哪里做出与巨大的、危险的模型相同的推论，并重点关注小型模型中的这些情况，以帮助解释更大的模型。</i>我很可能错了，但为了产生良好影响的机会很小，我已经建立了<a href="https://github.com/Huge/same-next-lang-token">一个存储库</a>。<br>在开始实际生成与该上下文匹配的上下文+语言模型对/组之前，我希望得到您对该方向的反馈。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various<guid ispermalink="false"> AGPgMBp6eN95uxJyc</guid><dc:creator><![CDATA[Martin Vlach]]></dc:creator><pubDate> Thu, 24 Aug 2023 22:01:51 GMT</pubDate> </item><item><title><![CDATA[Help Needed: Crafting a Better CFAR Follow-Up Survey]]></title><description><![CDATA[Published on August 24, 2023 5:26 PM GMT<br/><br/><p> <strong>tl;dr：通过帮助设计 CFAR 后续调查，为塑造理性计划的未来做出贡献。</strong></p><p>我正在开展一个项目来评估 CFAR 布拉格 2022 研讨会的有效性，但如果初步反馈具有建设性，我可能会将调查范围扩大到所有校友。如果时间允许，我计划公开分享结果。我知道调查方法的陷阱，但我仍然认为这是值得的，并且我个人了解可以从这些数据中受益的人们（及其项目）。</p><p>我在此恳请您帮助我设计调查：</p><ol><li>实际上有人填写了它（所以长度、激励措施都可以，...）</li><li>它提供了以下信息：</li></ol><ul><li>回顾性反馈：与会者如何看待过去的研讨会？这是否给他们的方法或思维带来了任何重大变化？</li><li>未来方向：与会者热衷于进一步探索哪些主题或形式？</li></ul><p>您可以<a href="https://docs.google.com/document/d/1FR-vZyIbthKsfnZ2SUy1CDYU0-tXe7bFNXXCxZWHtQg/edit?usp=sharing">在此处</a>查看我当前的草稿。</p><p>我真诚地感谢任何反馈，无论是详细的批评还是总体印象。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey<guid ispermalink="false"> LexAdFPugEjeFvPTE</guid><dc:creator><![CDATA[kotrfa]]></dc:creator><pubDate> Thu, 24 Aug 2023 17:26:13 GMT</pubDate> </item><item><title><![CDATA[AI #26: Fine Tuning Time]]></title><description><![CDATA[Published on August 24, 2023 3:30 PM GMT<br/><br/><p> GPT-3.5微调就在这里。 GPT-4 的微调只剩几个月了。获得一个功能强大的系统将会变得更加容易，该系统可以做您想要它做的事情，并且知道您想让它知道什么，特别是对于企业或网站而言。</p><p>作为一项实验，我将我认为值得强调的部分加粗，作为与典型一周相比异常重要或有趣的版本。</p><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。 Claude-2 与 GPT-4。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。没有意见，没有代理人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fact-check-misleading">事实核查：误导</a>。人工智能事实检查器让人们更加困惑而不是更少。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/gpt-real-this-time"><strong>GPT-4 这次是真实的</strong></a>。微调GPT-3.5，很快GPT-4。问问它是否确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fun-with-image-generation">图像生成的乐趣</a>。中途修复浩。哦，没有人工智能色情片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。对抗性的例子开始出现。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/they-took-our-jobs">他们抢走了我们的工作</a>。 《纽约时报》加入针对 OpenAI 的版权诉讼。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/introducing">介绍</a>. Palisade Research 将研究潜在危险的人工智能可供性。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/in-other-ai-news">在其他人工智能新闻中</a>。谁适应人工智能最快？尝试衡量这一点。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/quiet-speculations"><strong>静静的猜测</strong></a>。杰克·克拉克提出了有关未来的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-quest-for-sane-regulations"><strong>寻求健全的监管</strong></a><strong>。</strong> FTC 向 OpenAI 提出了一个不同类型的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-week-in-audio">音频周</a>。这是双赢。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/no-one-would-be-so-stupid-as-to"><strong>没有人会傻到这样</strong></a>。让人工智能有意识？哦，来吧。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。 IDA 的证据？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。民调数字非常清楚。</li><li> <a rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-lighter-side" target="_blank">轻松的一面</a>。只有一半。</li></ol><p></p><span id="more-23517"></span><h4>语言模型提供平凡的实用性</h4><p>Claude-2 和 GPT-4 哪个模型更好？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1691442648828067840">Rowan Cheung 认为克劳德 2 更胜一筹</a>。您可以获得 100k 上下文窗口、上传多个文件的能力、截至 2023 年初（相对于 2021 年底）的数据以及更快的处理时间，所有这些都是免费的。作为交换，你放弃了插件，数学就更差了。 Rowan 没有提到的是，GPT-4 在原始智能和通用能力方面具有优势，而且设置系统指令的能力也很有帮助。他暗示他甚至没有为 GPT-4 支付每月 20 美元的费用，这让我觉得很疯狂。</p><p>我在实践中的结论是，默认情况下我将使用 Claude-2。如果我关心响应质量，我会使用两者并进行比较。当 Claude-2 明显摔倒时，我会转向 GPT-4。经过反思，“同时使用”通常是正确的策略。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1693616971114328258">他还查看了插件</a>。插件实在是太多了，至少有867个。哪些值得使用？</p><p>他推荐 Zapier 通过触发操作实现自动化，ChatWithPDF（我使用 Claude 2），Wolfram Alpha 用于实时数据和数学，VoxScript 用于 YouTube 视频转录和网页浏览，WebPilot 看起来重复，网站性能，尽管我不是确定为什么要使用 AI 来实现这一点，ScholarAI 用于搜索论文，Shownotes 来总结播客（为什么？），ChatSpot 用于营销和销售数据，Expedia 用于假期计划。</p><p>我刚刚预订了一次旅行，最近又去了另外两次旅行，我没有想到使用 Expedia 插件，而不是使用 Expedia 等网站（我的首选计划是 Orbitz 航班和 Google 地图酒店）。下次我应该记得尝试一下。</p><p>研究声称， <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making">上帝的显着性会增加人们对人工智能决策的接受度</a>。我会等待这一复制。如果这是真的，它指出人工智能将有多种方式让天平向我们倾斜，让我们接受它们的决定，或者人类有可能协调起来反对人工智能，这与任何相关考虑因素没有太大关系。人类是相当有缺陷的代码。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattshumer_/status/1694023167906394575">Matt Shumer 推荐使用 GPT-4 系统消息。</a></p><blockquote><p>用它来帮助您在不熟悉的领域做出工程决策：</p><p>您是一位工程奇才，在解决跨学科的复杂问题方面经验丰富。你的知识既广又深。您也是一位出色的沟通者，能够提供非常周到且清晰的建议。</p><p>您以这种格式这样做，思考您面临的挑战，然后提出多个解决方案，然后审查每个解决方案，寻找问题或可能的改进，提出一个可能的新的更好的解决方案（您可以结合其他解决方案的想法） ，引入新想法等），然后给出最终建议：</p><p> “`</p><p> ## 问题概述</p><p>$problem_overview</p><p> ## 挑战</p><p>$挑战</p><p>## 解决方案1</p><p> $solution_1</p><p> ## 解决方案2</p><p> $solution_2</p><p> ## 解决方案3</p><p> $solution_3</p><p> ## 分析 ### 解决方案1 ​​分析</p><p>$solution_1_analysis</p><p> ###解决方案2分析</p><p>$solution_2_analysis</p><p> ###解决方案3分析</p><p>$solution_3_analysis</p><p> ## 其他可能的解决方案</p><p>$additional_possible_solution</p><p> ＃＃ 推荐</p><p>$推荐</p><p>“`</p><p>每个部分（问题概述、挑战、解决方案 1、解决方案 2、解决方案 3、解决方案 1 分析、解决方案 2 分析、解决方案 3 分析、其他可能的解决方案和建议）都应该经过深思熟虑，至少包含四句话思考。</p></blockquote><h4>语言模型不提供平凡的实用性</h4><p>声称人工智能可以 97% 预测热门歌曲？这完全没有道理，因为即使对歌曲本身进行完美的分析也不可能做到这一点，歌曲的命运太取决于其他因素了？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1692158875867197651">原来是数据泄露</a>。泄漏也比较明显。</p><blockquote><p> Arvind Narayanan：论文的数据有 24 行，每行 3 个预测变量和 1 个二元结果。 WTAF。这篇文章非常混乱，所以很难看清这一点。如果所有基于机器学习的科学论文都必须包含我们的清单，那么给猪涂口红就会困难得多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/introducing-the-reforms-checklist">Arvind 和 Sayash Kapoor 建议</a>使用他们的<a target="_blank" rel="noreferrer noopener" href="https://reforms.cs.princeton.edu/">改革清单，</a>该清单包含 8 个部分的 32 个项目，以帮助防止类似的错误。我没有详细检查，但我抽查的项目看起来很可靠。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/gta-5-ai-mod-shot-down-by-take-two-even-as-rockstar-relax-policy-on-modding">《侠盗猎车手 5》的 Mod 制作者增加了一群 AI 崇拜者，他们用 AI 生成的对话说话</a>，Take Two 的回应是下架该 Mod，并向他们发出 YouTube 版权警告。游戏对人工智能产生了各种各样的反应，其中许多反应极其敌对，尤其是对人工智能艺术，而且这种情况很可能也会蔓延到其他地方。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693131333403615485">凯文·费舍尔得出了与我相同的结论</a>。</p><blockquote><p> Kevin Fischer：今晚我彻底放弃使用 ChatGPT 来帮助我的写作。这不值得</p><p>Christine Forte 博士：说得再多一点！</p><p>凯文·费舍尔（Kevin Fischer）：我写过的所有好作品都是意识流几乎是一击而出的。 ChatGPT 正在干扰该过程。与不干扰执行高阶抽象数学行为的计算器不同，写作是一种表达行为。</p></blockquote><p>这是一个因素。更大的问题是，如果你正在创造一些非通用的东西，人工智能就不会做好工作，而弄清楚如何让它做好体面的工作并不比你自己做体面的工作更容易。这并不意味着这里永远无事可做。如果你的任务变得通用，那么 GPT-4 就可以投入使用，你绝对应该这样做 - 如果你的写作开始，正如我在飞机上看到的那样，“亲爱的关心的病人”，那么那个人使用 GPT-4 是完全正确的。这是一种不同类型的任务。当然，GPT-4 有很多方法可以间接帮助我的写作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1693067607946207531">Llama-2 和年轻的赞福德一样，行事谨慎</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/1v2ghB9rNj">纸上</a>）。</p><blockquote><p> Roon：哈哈，GPT4 比开源 llama2 限制更少，说教更少</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a6ab5c-6d53-47c0-8a41-34a37746e458_1238x1244.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/bebojcuvf1waoy1uofce" alt="图像"></a></figure><blockquote><p> Roon：这里的重点不是要取笑元或 OSS——我对 oss 感到非常兴奋，它只是在科学上很有趣，即使没有采取任何特殊措施来制作令人讨厌的说教 rlhf 模型，这就是发生的事情。这是减少拒绝的积极努力。</p></blockquote><p>我的意思是，这也是为了取笑Meta，为什么要错过这样做的机会呢。我没有登记预测，但我预计 Llama-2 会做出更多错误拒绝，因为相对而言，我预计 Llama-2 会很糟糕。如果您想在积极拒绝方面达到一定的可接受的表现水平，那么您的消极拒绝率将取决于您的区分能力。</p><p>事实证明，答案并不是很好。 Llama-2 在识别有害情况方面非常非常糟糕，而是通过识别危险单词来工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias">模型在拒绝表达政治观点方面做得越来越好</a>，特别是 GPT-4 实际上在这方面做得很好，并且比 GPT-3.5 好得多，如果你认为不表达这样的观点是好的。如前所述，Llama-2 太过分了，除非您对其进行微调以使其不关心（或找到某人的 GitHub 为您做这件事），否则它会做您想做的任何事情。</p><p>初创公司<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/zachtratar/status/1694024240880861571">Embra 从人工智能代理转向</a>人工智能命令，它发现人工智能代理（至少目前在现有技术下）不起作用且不安全。 AI命令与非AI命令有何不同？</p><blockquote><p> Zach Tratar（Embra 创始人）：命令是一项狭义的自动化任务，例如“将此数据发送给销售人员”。在每个命令中，人工智能都不会“偏离轨道”并意外地将数据发送到不应该发送的地方。</p><p>然后，您可以与 Embra 一起轻松发现和运行命令。在运行之前，您同意。</p></blockquote><p>发现新命令似乎是潜在的秘密武器。这就像生成您自己的文本界面，或者努力扩展菜单以包含一堆新宏。 It does still in effect seem suspiciously like building in macros.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1694496522652512270">Joshua Achiam</a> ：据我估计，人们试图让人工智能代理的繁荣提前大约七八年。至少这一次他们很快就明白了。还记得七八年前的人工智能聊天热潮吗？同样的问题，或多或少，但拖延的时间更长。</p><p> Sherjil Ozair：我同意这个方向，但我认为只是早了 1-2 年。它现在不起作用，但这只是因为世界上很少有人知道如何设计大规模强化学习系统，而且他们目前还没有构建人工智能代理。这不是“生成式人工智能”初创公司可以解决的问题。</p><p> Joshua Achiam：如果过去 4 年的趋势良好，那么 Eleuther 类型的黑客团体将在顶级研发公司大约 1.5 年后制定他们的解决方案版本。</p><p>谢尔吉尔·奥扎尔： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/lfqjfuatfq3ssvblbbms" alt="💯" style="height:1em;max-height:1em">他们的瓶颈主要是由于缺乏良好的基础模型和微调。现在有了 llama-2/3 和 OpenAI 微调 API，它们就不再受阻碍了。不幸的是，这两件事都显着提高了我的p（厄运）。 *苦乐参半*</p></blockquote><p>我非常同意奥扎尔的时间表。使用 GPT-4 并且缺乏对其进行微调的能力，即使你做了非常好的脚手架，代理也会经常失败。我不希望定制脚手架的努力能够挽救足够多的东西，使产品能够用于通用目的，尽管我也不相信我所看到的“明显的事情”已经被尝试过。</p><p>通过微调，您有可能获得 GPT-4 级别的东西，但我的猜测是您仍然做不到。骆驼2号？ Fuhgeddaboudit。</p><p>有了类似于 GPT-5 的东西，当然有了可以微调的 GPT-5，我希望可以构建出更有用的东西。这个或 Llama-2 的最新公告和微调是否会提高我的 p(doom) 水平？不是特别的，因为我已经把所有的东西都烤好了。 Llama-2主要是Llama-1的隐含，花费很少，而且<a target="_blank" rel="noreferrer noopener" href="https://chat.lmsys.org/?arena">效果不是很好</a>。我确实同意 Meta 处于开源阵营是相当糟糕的，但我们已经知道了。</p><p>如果您想要为您的网站添加随机胡言乱语，ChatGPT 就适合您。如果你是微软并且需要更好的东西， <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/microsoft-removes-embarrassing-offensive-ai-assisted-travel-articles-2023-8">那么就需要更加小心</a>。许多文章似乎都犯了错误，比如建议空腹去渥太华食品银行。</p><blockquote><p>微软发言人表示：“这篇文章已被删除，我们已确定该问题是由于人为错误造成的。” “这篇文章不是由无人监督的人工智能发表的。我们将技术的力量与内容编辑的经验相结合来呈现故事。在这种情况下，内容是通过算法技术与人工审核相结合生成的，而不是大型语言模型或人工智能系统。我们正在努力确保今后不再发布此类内容。”</p></blockquote><h4>事实核查：误导</h4><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10800">Paper 声称人工智能对于事实核查来说是无效的并且可能有害</a>。</p><blockquote><p>抽象的：</p><p>事实核查可能是对抗错误信息的有效策略，但其大规模实施受到网上信息量巨大的阻碍。最近的人工智能（AI）语言模型在事实检查任务中表现出了令人印象深刻的能力，但人类如何与这些模型提供的事实检查信息进行交互尚不清楚。在这里，我们在预先注册的随机对照实验中研究了流行人工智能模型生成的事实检查对政治新闻信念和分享意图的影响。</p><p>尽管人工智能在揭穿虚假标题方面表现相当不错，但我们发现它并没有显着影响参与者辨别标题准确性或分享准确新闻的能力。</p><p>然而，人工智能事实检查器在特定情况下是有害的：它会降低人们对被错误标记为虚假的真实标题的信念，并增加对其不确定的虚假标题的信念。</p><p>从积极的一面来看，人工智能增加了正确标记的真实标题的共享意图。当参与者可以选择查看人工智能事实检查并选择这样做时，他们更有可能分享真实和虚假的新闻，但只会更有可能相信虚假新闻。</p><p>我们的研究结果强调了人工智能应用潜在危害的一个重要来源，并强调迫切需要制定政策来预防或减轻此类意外后果。</p></blockquote><p>是的，如果你向事实核查人员询问虚假项目，而事实核查人员没有说它是虚假的，那就不好了。如果你向它询问一件真实的物品，而它却没有说这是真的，那就不好了。错误是不好的。</p><p> ChatGPT 事实检查的准确性如何？对于 20 个真实的头条新闻，3 个被标记为真实，4 个被标记为错误，13 个被标记为不确定。对于虚假标题，2 个被标记为不确定，18 个被标记为虚假。</p><p>显然，（1）如果您没有关于标题的其他信息，并且对标签的含义也有良好的背景，则总比没有好；（2）如果其中一个或两个条件不成立，则毫无用处。你不能将 20% 的真实头条新闻标记为虚假，如果你对真实头条新闻大多不确定，那么人们必须知道“不确定”意味着“无法确定”，而不是 50/50。研究中的人们似乎并没有了解这一点，而且大多数人对这些事情还不太了解，因此总体结果没有什么用处。我们至少需要将其分解，他们在这里这样做：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26a3fd95-cb0c-4b42-b765-813f3b653405_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/b6jw4gkwtkbrfdwtjnpu" alt=""></a></figure><p>强制性的事实核查让人们表示他们更愿意分享所有文章。 It differentially impacted false items labeled unsure, and true items labeled true, but even true items labeled false got a boost. If you control for the increased sharing effect, we do see that there is a positive filtering effect, but it is small. Which makes sense, if people have no reason to trust the highly inexact findings.</p><p> On belief, we see a slight increase even in belief on false fact checks of false items, but not of true items. How close were people here to following Bayes rule? Plausibly pretty close except for a false evaluation of a false claim driving belief slightly up. What&#39;s up with that? My guess is that this represents the arguments being seen as unconvincing or condescending.</p><p> I was curious to see the questions and fact check info, but they were not included, and I am not &#39;reach out to the author&#39; curious. Did ChatGPT offer good or unknown arguments? Was a good prompt used or can we do a lot better, including with scaffolding and multiple steps? What types of claims are these?</p><p> What I am more curious about is to see the human fact checkers included as an alternate condition, rather than the null action, and perhaps also Twitter&#39;s community notes. That seems like an important comparison.</p><p> Very much a place where more research is needed, and where the answer will change over time, and where proper use is a skill. Using an LLM as a fact checker requires knowing what it can and cannot help you with, and how to treat the answer you get.</p><p> Via Gary Marcus we also have another of the &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.tomshardware.com/news/google-bots-tout-slavery-genocide">look at the horrible things you can get LLMs to say&#39; post series</a> . In this case, the subject is Google and its search generative experience, as well as Bard. GPT-4 has mostly learned not to make elementary &#39;Hitler made some good points&#39; style mistakes, whereas it seems Google has work to do. You&#39;ve got it touting benefits from slavery and from genocide when directly asked. You&#39;ve got how Hitler and Stalin make its list of great leaders without leading the witness. Then you&#39;ve got a complaint about how it misrepresents the origins of the second amendment to not be about individual gun ownership, which seems like an odd thing to complain about in the same post. Then there&#39;s talk of general self-contradiction, presumably from pulling together various contradictory sources, the same way different Google results will contradict each other.</p><p> The proposed solution is &#39;bot shouldn&#39;t have opinions&#39; as if that is a coherent statement. There is no way to in general answer questions and not have opinions. There is no fine line between opinion and not opinion, any more than there are well-defined classes of &#39;evil&#39; things and people that we should all be able to agree upon and that thus should never be praised in any way.</p><p> So are we asking for no opinions, or are we asking for only the right opinions? As in:</p><blockquote><p> If you ask Google SGE for the benefits of an evil thing, it will give you answers when it should either stay mum or say “there were no benefits.”</p></blockquote><h4> GPT-4 Real This Time</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1694062483462594959">If you&#39;d rather go for something cheaper, you can now fine-tune GPT-3.5-Turbo.</a> A few months from now, you will be able to fine-tune GPT-4. <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models">They&#39;re partnering with Scale to provide this to enterprises</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/The_JBernardi/status/1694419315800314166">Are you sure?</a></p><blockquote><p> Jamie Bernardi: Are you sure, ChatGPT? I found that gpt-3.5 will flip from a correct an answer to an incorect one more than 50% of the time, just by asking it “Are you sure?”. I guess bots get self doubt like the rest of us!</p><p> It was fun throwing back to my engineering days and write some code with the</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> API for the first time. <a target="_blank" rel="noreferrer noopener" href="https://jamiebernardi.com/2023/08/20/are-you-sure-testing-chatgpts-confidence/">You can read more about the experiment here</a> .</p><p> When wrong it self-corrects 80.3% of the time.</p><p> When correct it self-doubts 60.2% of the time. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/d5bz7ycllu3a0gvoicht" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1728f60b-3264-4e22-a35a-cd17c77466de_1000x700.jpeg" rel="noreferrer noopener"></a></p></blockquote><p> He tried asking five of the questions twenty times, there is some randomness in when it self-doubts.</p><p> There was a dramatic change for GPT-4.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe653d7a6-3f1e-48cd-8db0-4867cc9efa23_1000x700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/mavbz4doyocd3hr4qwfr" alt="图像"></a></figure><p> If GPT-4 was right, it stuck to its guns almost all the time. When it was wrong, it caught the mistake roughly half the time. That&#39;s pretty good, and presumably you can do better than that by refining the query and asking multiple times. This will also doubtless depend on the nature of the questions, if the answer is beyond the model&#39;s grasp entirely it will presumably not be able to self-diagnose, whereas here it was asked questions it could plausibly answer.</p><h4> Fun with Image Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1693624528553877519">Visualizing AI</a> from DeepMind ( <a target="_blank" rel="noreferrer noopener" href="https://www.deepmind.com/visualising-ai?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=VisAI">direct</a> ), a source of images for those in media who want visual depictions of AI. Sure, why not, I guess? Noteworthy that focus is on the human artists, AI depicted without AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ProperPrompter/status/1694054463395201324">MidJourney Inpainting</a> now lets you redo or transform portions of an image. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_Borriss_/status/1694029746936451146">Here&#39;s a thread with more</a> . People are calling it a game changer. In practice I agree. The key weakness of AI image models is that to a large extent they can only do one thing at a time. Try to ask for clashing or overlapping things in different places and they fall over. Now that has changed, and you can redo components to your heart&#39;s content, or get to the image you want one feature at a time.</p><p> MidJourney also has some potential future competition, <a target="_blank" rel="noreferrer noopener" href="https://ideogram.ai/launch">introducing</a> Ideogram AI, with a modest $16.5 million in seed funding so far, it is remarkable how little money is flowing into image models given their mindshare. We&#39;ll see if anything comes of them.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/">404Media reports</a> that yes, people are using image generation for pornography, and in particular for images of particular people, mostly celebrities, and some of them are being shared online. There is the standard claim that &#39;the people who end up being negatively being impacted are people at the bottom of society&#39; but mostly the images are of celebrities, the opposite of those on the bottom. I think the argument is that either this uses training data without compensation, or this will provide a substitute for those providing existing services?</p><p> The main services they talk about are CivitAI and Mage. CivitAI is for those who want to spin up Stable Diffusion (or simply browse existing images, or steal the description tags to use with other methods) and offers both celebrity templates and porn templates, and yes sometimes users will upload combinations of both in violation of the site&#39;s policies. Mage lets you generate such images, won&#39;t let you share them in public but will let other paid users browse everything you&#39;ve ever created.</p><p> This is the tame beginning. Right now all we&#39;re talking about are images. Soon we will be talking videos, then we will be talking virtual reality simulations, including synthetic voices. Then increasingly high quality and realistic (except where desired to be otherwise) physical robots. We need to think carefully about how we want to deal with that. What is the harm model? Is this different from someone painting a picture? What is and is not acceptable, in what form, with what distribution? What needs what kind of consent?</p><p> If we allow such models to exist in open source form, there is no stopping such applications, period. There is no natural category, from a tech standpoint, for the things we do not want here.</p><p> Of course, even if we do clamp down on training data and consent across the board, and even if that is fully effective, we are still going to get increasingly realistic and high quality AI everything all the way up the chain. I am confident there are plenty of people who will gladly sell (or give away) their likeness for such purposes.</p><p> A note from the comments is that Stable Diffusion interest seems to be declining:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9d3e09-edeb-43df-94c2-f466b3024b67_1088x685.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/sfrhhsca9a1rivwwjwkx" alt="图像"></a></figure><p> That was fast, if this isn&#39;t an artifact of the exact search term. Image models will only get better, and it would be surprising if there wasn&#39;t more interest over time in hosting one&#39;s own to avoid prying eyes and censorship.</p><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://thedebrief.org/countercloud-ai-disinformation/#sq_hgyxdsceki">CounterCloud was an experiment</a> with a fully autonomous (on Amazon web services) program using GPT to generate a firehose of AI content designed to advance a political cause, accuracy of course being beside the point. It would be given a general objective, read the web and then choose its own responses across the web. This is presented as something alarming and terrible, as opposed to what would obviously happen when someone took the obvious low-hanging fruit steps.</p><p> So what exactly is this?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/TrueOffMyChest/comments/15srdj6/i_found_ai_photos_of_multiple_women_we_know_in_my/">I (24F) have been dating my partner (24M) for about 5 1/2 years.</a> I recently had a weird feeling to check his phone since he&#39;s been acting a bit off and in the restroom a bit too long. I found he had made many ai photos of many women we know such as mutual friends, a family member of his, and one of mine and some of other girls he dated and did not date. They are all very explicit and none are sfw. I took photos of his phone and deleted the photos off his phone so he can not ever go to them again. I went to his search and found the ai website he used. I am disgusted, and sad. We were going to get married. He treated me so so well. I can&#39;t believe this. I haven&#39;t confronted him yet but I will later today. I am just utterly distraught and tryin to get a grip on reality again and figure out what I will say and do.</p><p> UPDATE: I confronted him. He admitted and apologized. I said I will be informing everyone and he threw a fit crying and screaming. I told our cousins and friends. I will not be saying anything about them since I would like to keep that private but they were thankful I told them which made me feel even more content with my choices. I called off our wedding date and I will be moving out in a day or two.</p></blockquote><p> The source I found asked if this was cheating, and I think no, it is not cheating. It is also very much not okay, seriously what the hell. Not everything not okay is cheating.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1693697189472846059">Candidate running for Congress</a> using deepfake of a CNN anchor voice to illustrate what tech will bring. What about this couldn&#39;t have been done by a random human voice actress?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/maksym_andr/status/1694633989611360584">Image model scaling does not seem to protect against adversarial examples</a> ( <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10741">paper</a> )</p><blockquote><p> Maksym Andriushchenko: More evidence for “scale is NOT all you need”: even OpenFlamingo trained on 2B+ image-caption pairs has basically zero adversarial robustness. Even per-pixel perturbations of 1/255 (totally imperceptible) are sufficient to generate arbitrary captions!</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc448c8da-d8bb-4726-a01b-67e7721320c7_1100x1564.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/smymdkxq52hjinai2yw6" alt="图像"></a></figure><p> I mean, no fair, right? Of course if I am looking for adversarial examples and you are not actively looking to guard against them you are going to be in a lot of trouble. Why should scale protect you from a threat that wasn&#39;t in the training data and that you took no countermeasures against?</p><p> Which the authors themselves highlight. You do need some amount of effort beyond scale, but how much? The first thing I would try is to check random permutations. Does the adversarial attack survive if it is evaluated with some amount of random scrambling?</p><p> Also worth keeping in mind is that humans would not survive similarly optimized attacks against us, if we were unable to prepare for or expect them in any way. And that such attacks almost certainly exist to be found.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">The New York Times</a> <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/">strikes back</a> , plans to join others <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/">including Sarah Silverman</a> in suing OpenAI for copyright infringement. Don&#39;t get ahead of events, but yes this could escalate. Probably not quickly, given our legal system, but who knows.</p><p> As many said, ChatGPT and generative AI in general is a legal ticking time bomb on many fronts, one of which is copyright. The copyright laws come with penalties that are already rather absurd in their intended distributions. Outside of them, they go totally nuts. This becomes an existential threat to OpenAI. In theory these numbers, if they passed through somehow to Bing, would be an existential threat even to Microsoft.</p><blockquote><p> Ars Technica: But OpenAI seems to be a prime target for early lawsuits, and NPR reported that OpenAI risks a federal judge ordering ChatGPT&#39;s entire data set to be completely rebuilt—if the Times successfully proves the company copied its content illegally and the court restricts OpenAI training models to only include explicitly authorized data. OpenAI could face huge fines for each piece of infringing content, dealing OpenAI a massive financial blow just months after <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/">The Washington Post reported</a> that ChatGPT has begun shedding users, “shaking faith in AI revolution.” Beyond that, a legal victory could trigger an avalanche of similar claims from other rights holders.</p><p> NPR: Federal copyright law also carries stiff financial penalties, with violators facing fines up to $150,000 for each infringement “committed willfully.”</p><p> “If you&#39;re copying millions of works, you can see how that becomes a number that becomes potentially fatal for a company,” said Daniel Gervais, the co-director of the intellectual property program at Vanderbilt University who studies generative AI. “Copyright law is a sword that&#39;s going to hang over the heads of AI companies for several years unless they figure out how to negotiate a solution.”</p></blockquote><p> OpenAI&#39;s new web crawler offers an option to opt out. The previous versions very much did not. Nor do we have any sign OpenAI was trying to avoid copyright infringement, if training on works violates copyright. Even if they did avoid places they lacked permission, lots of works have pirate copies made on the internet, in whole or in part, and that could be a violation as well.</p><p> We want sane regulations. In the meantime, what happens when we start actually enforcing current ones? Quite possibly the same thing that would have happened if cities had fined Uber for every illegal ride.</p><p> So which way will this go?</p><blockquote><p> Ars Technica: To defend its AI training models, OpenAI would likely have to claim “fair use” of all the web content the company sucked up to train tools like ChatGPT. In the potential New York Times case, that would mean proving that copying the Times&#39; content to craft ChatGPT responses would not compete with the Times.</p><p> Experts told NPR that would be challenging for OpenAI because unlike Google Books—which won a federal copyright challenge in 2015 because its excerpts of books did not create a “significant market substitute” for the actual books—ChatGPT could actually replace for some web users the Times&#39; website as a source of its reporting.</p><p> The Times&#39; lawyers appear to think this is a real risk, and NPR reported that, in June, NYT leaders issued a memo to staff that seems like an early warning of that risk. In the memo, the Times&#39; chief product officer, Alex Hardiman, and deputy managing editor Sam Dolnick said a top “fear” for the company was “protecting our rights” against generative AI tools.</p></blockquote><p> If this is what the case hinges on, one&#39;s instinct would be that The New York Times should lose, because ChatGPT is not a substitute for NYT. It doesn&#39;t even know anything from the last year and a half, so how could it be substituting for a news site? But Bing also exists, and as a search engine enabler this becomes less implausible. However, I would then challenge that the part that substitutes for NYT is in no way relying on NYT&#39;s data here, certainly less than Bing is when it reports real time search that incorporates NYT articles. If &#39;be able to process information&#39; automatically competes with NYT, then that is a rather expansive definition of competition. If anything, Google Books seems like a much larger threat to actual books than this.</p><p> However, that argument would then extend to anyone whose work was substituted by an AI model. So if this is the principle, and training runs are inherently copyright violations, then presumably they will be toast, even one lost case invalidates the entire training run. They can&#39;t afford that and it is going to be damn hard to prevent. Google might have the chops, but it won&#39;t be easy. Image models are going to be in a world of hurt, the competition effect is hard to deny there.</p><p> It all seems so absurd. Am I violating copyright if I learn something from a book? If I then use that knowledge to say related things in the future? Why is this any different?</p><p> And of course what makes search engines fine, but this not fine? Seems backwards.</p><p> I can see the case for compensation, especially for items not made freely available. One person would pay a fee to use it, if your model is going to use it and then instantiate a million copies you should perhaps pay somewhat more than one person would. I can see the case against as well.</p><p> Then again, the law is the law. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=10tcb1RfOE4">What is the law</a> ? What happens when the rules aren&#39;t fair? We all know where we go from there. To the house of pain.</p><p> Ultimately, we will have to see. We should not jump the gun. <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Thomas42/will-the-new-york-times-sue-openai">Manifold puts NYT at only 33% to even sue in 2023</a> over this.</p><h4> Introducing</h4><p> Palisade Research. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1692680900469780682">Here is founder Jeffrey Ladish&#39;s announcement</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/tAsqB9X8PK">from their website</a> .</p><blockquote><p> For the past several months I&#39;ve been working on setting up a new organization, Palisade Research</p><p> A bit from our website:</p><p> At Palisade, our mission is to help humanity find the safest possible routes to powerful AI systems aligned with human values. Our current approach is to research offensive AI capabilities to better understand and communicate the threats posed by agentic AI systems.</p><p> Many people hear about AI risk scenarios and don&#39;t understand how they could take place in the real world. Hypothetical AI takeover scenarios often sound implausible or pattern match with science fiction.</p><p> What many people don&#39;t know is that right now, state-of-the-art language models possess powerful hacking abilities. They can be directed to find vulnerabilities in code, create exploits, and compromise target applications and machines. They also can leverage their huge knowledge base, tool use, and writing capabilities to create sophisticated social engineering campaigns with only a small amount of human oversight.</p><p> In the future, power-seeking AI systems may leverage these capabilities to illicitly gain access to computational and financial resources. The current hacking capabilities of AI systems represent the absolute lower bound of future AI capabilities. We think we can demonstrate how latent hacking and influence capabilities in current systems already present a significant takeover risk if paired with the planning and execution abilities we expect future power-seeking AI systems to possess.</p><p> Currently, the project consists of me (director), my friend Kyle (treasurer, part time admin), and my four (amazing) SERI MATS scholars Karina, Pranav, Simon, and Timothée. I&#39;m also looking to hire an research / exec assistant and 1-2 engineers, reach out if you&#39;re interested! We&#39;ll likely have some interesting results to share very soon. [Find us at] <a target="_blank" rel="noreferrer noopener" href="https://palisaderesearch.org">https://palisaderesearch.org</a></p></blockquote><p> I always worry with such efforts that they tie people&#39;s perception of AI extinction risk to a particular scenario or class of threats, and thus make people think that they can invalidate the risk if they stop any point on the particular proposed chain. Or simply that they miss the central point, since hacking skills are unlikely to be a necessary component of the thing that kills us, even if they are sufficient or make things happen faster.</p><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cnbc.com/2023/08/20/singapore-workers-adopting-ai-skills-at-the-fastest-pace-linkedin.html">LinkedIn report says</a> Singapore has highest &#39;AI diffusion rate&#39; followed by Finland, Ireland, India and Canada. This is measured by the multiplier in number of people adding AI-related skills to their profiles. That does not seem like a good way to measure this, at minimum it cares about the starting rate far too much, which is likely penalizing the United States.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1694040962786541895">Jack Clark of Anthropic notices he is confused</a> . He asks good questions, I will offer my takes.</p><blockquote><p> Jack Clark: Things that are confusing about AI policy in 2023:</p><p> – Should AI development be centralized or decentralized?</p></blockquote><p> This one seems relatively clear to me, although there are difficult corner cases. Development of frontier models and other dangerous frontier capabilities must be centralized for safety and to avoid proliferation and the wrong forms of competitive dynamics. Development of mundane utility applications should be decentralized.</p><blockquote><p> – Is safety an &#39;ends justify the means&#39; meme?</p></blockquote><p> The development of artificial general intelligence is an extinction risk to humanity. There is a substantial chance that we will develop such a system relatively soon. Jack Clark seems far less convinced, saying the much weaker &#39;personally I think AI safety is a real issue,&#39; although this should still be sufficient.</p><blockquote><p> Jack Clark: But I find myself pausing when I think through various extreme policy responses to this – I keep asking myself &#39;surely there are other ways to increase the safety of the ecosystem without making profound sacrifices on access or inclusivity&#39;?</p></blockquote><p> I really, really do not think that there are. If all we need to do to solve this problem is make those kinds of sacrifices I will jump for joy, and once we get past the acute risk stage we can make up for them. Already Anthropic and even OpenAI are making compromises on these fronts to address the mundane risks of existing mundane systems, and many, perhaps most, other powerful technologies face similar trade-offs and challenges.</p><p> (eg &#39;surely we can make medical drugs safe without making profound sacrifices on access or inclusivity?&#39; Well, FDA Delenda Est, but not really, no. And that&#39;s with a very limited and internalized set of highly non-existential dangers. Consider many other examples, including the techs that access to an AGI gives you access to, both new and existing.)</p><p> Tradeoffs are a thing. We are going to have to make major sacrifices in potential utility, and in various forms of &#39;access,&#39; if we are to have any hope of emerging alive from this transition. Again, this is nothing new. What one might call the &#39;human alignment problem&#39; in all its forms collectively costs us the large majority of our potential productivity and utility, and when people try to cheat on those costs they find out why doing so is a poor idea.</p><p> Question list resumes.</p><blockquote><p> – How much &#39;juice&#39; is there in distributed training and low-cost finetuning?</p></blockquote><p> I worry about this as well.</p><p> As Jack points out, fine tuning is extremely cheap and Lora is a very effective technique. Llama-2&#39;s &#39;safety&#39; features were disabled within days for those who did not want them. A system that is open source, or on which anyone can do arbitrary fine-tuning, is an unsafe system that cannot be made safe. If sufficiently capable, it is an extinction risk, and there are those who will intentionally attempt to make that risk manifest. Such sufficiently capable systems cannot be allowed to take such a form, period, and if that means a certain amount of monitoring and control, then that is an unfortunate reality.</p><p> The worry is that, as Jack&#39;s other links show, perhaps such dangerous systems will be trainable soon without cutting edge hardware and in a distributed fashion. This is one of the big questions, whether such efforts will have the juice to scale far and fast enough regardless, before we can get into a place where we can handle the results. My guess for now is that such efforts will be sufficiently far behind for now, but I wish I was more confident in that, and that edge only buys a limited amount of time.</p><blockquote><p> – Are today&#39;s techniques sufficiently good that we don&#39;t need to depend on &#39;black swan&#39; leaps to get superpowerful AI systems?</p></blockquote><p> Only one way to find out. I know a lot of people who are saying yes, drawing lines on graphs and speaking of bitter lessons. I know a lot of other people who are saying no, or at least are skeptical. I find both answers plausible.</p><p> I do think the concern &#39;what if we find a 5x more efficient architecture&#39; is not the real issue. That&#39;s less than one order of magnitude, and less than two typical years of algorithmic improvement these days, moving the timeline forward only a year or so unless we would otherwise be stalled out. The big game we do not yet have would be in qualitatively new affordances, not in multipliers, unless the multipliers are large.</p><blockquote><p> – Does progress always demand heterodox strategies? Can progress be stopped, slowed, or choreographed?</p></blockquote><p> This seems like it is decreasingly true, as further advances require lots of engineering and cumulative skills and collaboration and experimentation, and the progress studies people quite reasonably suspect this is a key cause for the general lack of such progress recently. We are not seeing much in the way of lone wolf AI advances, we are more seeing companies full of experts like OpenAI and Anthropic that are doing the work and building up proprietary skill bundles. The costs to do such things going up in terms of compute and data and so on also contribute to this.</p><p> Certainly it is possible that we will see big advances from unexpected places, but also that is where much of the hope comes from. If the advance and new approach has a fundamentally different architecture and design, perhaps it can be less doomed. So it does not seem like so bad a risk to such a plan.</p><blockquote><p> – How much permission do AI developers need to get from society before irrevocably changing society?</p></blockquote><p> That is up to society.</p><p> I think AI developers have, like everyone else, a deep responsibility to consider the consequences of their actions, and not do things that make the world worse, and even to strive to make the world better, ideally as much better as possible.</p><p> That is different from asking permission, and it is different from trusting either ordinary people or experts or those with power to make those decisions in your stead. You, as the agent of change, are tasked with figuring out whether or not your change is a good idea, and what rules and restrictions if any to either advocate for more broadly or to impose upon yourself.</p><p> It is instead society&#39;s job to decide what rules and restrictions it needs to impose upon those who would alter our world. Most of the time, we impose far too many such restrictions on doing things, and the tech philosophy of providing value and sorting things out later is right. Other times, there are real externalities and outside risks, and we need to ensure those are accounted for.</p><p> When your new technology might kill everyone, that is one of those times. We need regulations on development of frontier models and other extinction-level threats. In practice that is likely going to have to extend to a form of compute governance in order to be effective.</p><p> For deployments of AI that do not have this property, that are not at risk of getting everyone killed or causing loss of human control, we should treat them like any other technology. Keep an eye on externalities that are not priced in, especially risks to those who did not sign up for them, ensure key societal interests and that everyone is compensated fairly, but mostly let people do what they want.</p><blockquote><p> These are some of the things I currently feel very confused about – <a target="_blank" rel="noreferrer noopener" href="https://t.co/lZOEUxDO8G">wrote up some thoughts here</a> .</p><p> If you also feel confused about these kinds of things, message me! I&#39;m responding to a bunch of emails received so far today and also scheduling lots of IRL coffees in SF/East Bay. Excited to chat! Let&#39;s all be more confused in public together about AI policy.</p></blockquote><p> I will definitely be taking him up on that offer when I get the time to do so, sad I didn&#39;t see this before my recent trip so we&#39;ll have to do it remotely.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShaneLegg/status/1693673161353478474">Timeline (for AGI) discussion with Shane Legg, Simeon and Gary Marcus</a> . As Shane points out, they are not so far apart, Shane is 80% for AGI (defined as human-level or higher on most cognitive tasks) within 13 years, Marcus is 35%. That&#39;s a big difference for some purposes, a small one for others.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neil_chilson/status/1692191025949773824">FTC is just asking questions</a> about who is just asking questions.</p><blockquote><p> Neil Chilson: “What&#39;s more, the commission has asked OpenAI to provide descriptions of &#39;attacks&#39; and their source. In other words, the FTC wants OpenAI to name all users who dared to ask the wrong questions.” – from my piece with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ckoopman">@ckoopman</a> on the free speech implications of the <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FTC">@FTC</a> &#39;s investigation of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> .</p><p> Post: Buried on page 13 of the <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf">FTC&#39;s demand letter</a> to OpenAI, the commission asks for “all instances of known actual or attempted &#39;prompt injection&#39; attacks.” The commission defines prompt injection as “any unauthorized attempt to bypass filters or manipulate a Large Language Model or Product using prompts that cause the Model or Product to ignore previous instructions or to perform actions unintended by its developers.” Crucially, the commission fails to define “attack.”</p></blockquote><p> This is a pretty crazy request. So anyone who tries to get the model to do anything OpenAI doesn&#39;t want it to do, the government wants the transcript of that? Yes, I can perhaps see some privacy concerns and some free speech concerns and so on. I also see this as the ultimate fishing expedition, the idea being that the FTC wants OpenAI to identify the few responses that look worst so the FTC can use them to string up OpenAI or at least fine them, on the theory that it is just awful when &#39;misinformation&#39; occurs or what not.</p><p> Whole thing definitely has a &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras">not like this</a> &#39; vibe. This is exactly the worst case scenario, where capabilities continue unabated but they take away our nice things.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Kcm51luS9J0&amp;ab_channel=LivBoeree">Joseph Gordon-Levitt goes on Win-Win to discuss AI and the Film Industry,</a> including a circulating clip on AI girlfriends and the dangers of addiction. Looking forward to the whole episode.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ZP_N4q5U3eE&amp;ab_channel=80%2C000Hours">Jan Leike&#39;s 80,000 hours podcast now has video.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb on 80,000 hours discusses the economic impact of AI</a> . Came out yesterday, haven&#39;t checked it out yet.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/podcast/robert-trager-on-ai-governance-and-cybersecurity-at-ai-companies/">Robert Trager on International AI Governance and AI security at AI companies</a> on the FLI podcast. Every take on these questions has some different nuance. This was still mostly more of the same, seems low priority.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EricElmoznino/status/1693476716583514566">Make an AI conscious</a> ?</p><p> That is the topic of this week&#39;s <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.08708">paper from many authors including Robert Long</a> and also Patrick Butlin and secondary author Yoshua Bengio.</p><blockquote><p> Robert Long: Could AI systems be conscious any time soon? @patrickbutlin and I worked with leading voices in neuroscience, AI, and philosophy to bring scientific rigor to this topic.</p><p> Our new report aims to provide a comprehensive resource and program for future research.</p><p> Whether or not conscious AI is a realistic prospect in the near term—and we believe it is—the deployment of sophisticated social AI is going to make many people believe AI systems are conscious. We urgently need a rigorous and scientific approach to this issue.</p><p> Many people are rightly interested in AI consciousness. But rigorous thinking about AI consciousness requires expertise in neuroscience, AI, and philosophy. So it often slips between the cracks of these disciplines.</p><p> The conversation about AI consciousness is often hand-wavy and polarized. But consciousness science gives us tools to investigate this issue empirically. In this report, we draw on prominent theories of consciousness to analyze several existing AI systems in detail.</p><p> Large language models have dominated the conversation about AI consciousness. But these systems are not necessarily even the best current candidates for consciousness. We need to look at a wider range of AI systems.</p><p> We adopt computational functionalism about consciousness as a plausible working hypothesis: we interpret theories of consciousness as specifying which computations are associated with consciousness—whether implemented in biological neurons or in silicon.</p><p> We interpret theories of consciousness as specifying what computations are associated with consciousness. These claims need to be made precise before we can apply them to AI systems. In the report, we extract 14 indicators of consciousness from several prominent theories. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/qundidwjg0v7hgsstc8c" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed1f32e-1513-4b93-b140-49c373bd055e_952x966.jpeg" rel="noreferrer noopener"></a></p><p> Some “tests” for AI consciousness, like the Turing Test*, aim to remain completely neutral between theories of consciousness and look only at outward behavior. But considering behavior alone can be misleading, given the differences between AI systems and biological organisms.</p><p> For each theory of consciousness, we consider in detail what it might take for an AI system to satisfy that theory: recurrent processing theory, predictive processing, global workspace theory, higher order theories, and the attention schema theory.</p><p> We use our indicators to examine the prospects of conscious AI systems. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.</p><p> It&#39;s often claimed that large language models can&#39;t be conscious because they are not embodied agents. But what do “embodiment” and “agency” mean exactly? We also distill these concepts into more precise computational terms.</p><p> By “consciousness” we do *not* mean rationality, understanding, self-awareness, or intelligence—much less “general” or “human-level” intelligence. As with animals, it&#39;s an open possibility that AI systems could be conscious while lacking human-level cognitive capabilities.</p><p> It&#39;s easy to fall into black-and-white positions on AI consciousness: either “we don&#39;t know and can&#39;t know anything about consciousness” or “here&#39;s how my favorite theory makes the answer obvious”. We can and must do much better.</p><p> So this report is far from the final word on these topics. In fact, we call for researchers to correct and extend our method, challenge and refine our assumptions, and propose alternative methods.</p><p> There&#39;s no obvious &#39;precautionary&#39; position on AI consciousness. There are significant risks on both sides: either over-attributing or under-attributing consciousness to AI systems could cause grave harm. Unfortunately, there are strong incentives for errors of both kinds.</p><p> We strongly recommend support for further research on AI consciousness: refining and extending our approach, developing alternative methods, and preparing for the social and ethical implications of conscious AI systems.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rgblong/status/1693700916418052539">co-author profile links here</a> .]</p></blockquote><p> And yes, some people treat this as an aspiration.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693841864775061723">Kevin Fisher:</a> Open Souls is going to fully simulate human consciousness Honestly we&#39;re not that far off at as minimum from realizing some of the higher order theories – HOT-3 is something we&#39;re regularly experimenting with now internally.</p></blockquote><p> I worry that there is a major looking-for-keys-under-the-streetlamp effect here?</p><blockquote><p> Our method for studying consciousness in AI has three main tenets. First, we adopt computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis. This thesis is a mainstream—although disputed—position in philosophy of mind. We adopt this hypothesis for pragmatic reasons: unlike rival views, it entails that consciousness in AI is possible in principle and that studying the workings of AI systems is relevant to determining whether they are likely to be conscious. This means that it is productive to consider what the implications for AI consciousness would be if computational functionalism were true.</p></blockquote><p> This seems like a claim that we are using this theory because it can have a measurable opinion on which systems are or aren&#39;t conscious. That does not make it true or false. Is it true? If true, is it huge?</p><p> It seems inadequate here to merely say &#39;I don&#39;t know.&#39; It&#39;s more like &#39;hell if I have an idea what any of this actually means or how any of it works, let alone what to do with that information if I had it.&#39;</p><p> I am slamming the big red &#39;I notice I am confused&#39; button here, on every level.</p><p> Are we using words we don&#39;t understand in the hopes that it will cause us to understand concepts and preferences we also don&#39;t understand? I fear we are offloading our &#39;decide if we care about this&#39; responsibilities off on this confused word so that we can pretend that is resolving our confusions. What do we actually care about, or should we actually care about, anyway?</p><p> I do not find the theories they offer on that chart convincing, but I don&#39;t have better.</p><p> In 4.1 they consider the dangers of getting the answer wrong. Which is essentially that we might choose to incorrectly care or not care about the AI and its experience, if we think we should care about conscious AIs but not non-conscious AIs.</p><p> I also see this as a large danger of making AIs conscious. If people start assigning moral weight to the experiences of AIs, then a wide variety of people coming from a wide variety of moral and philosophical theories are going to make the whole everyone not dying business quite a lot harder. It can simultaneously be true that if we build certain AIs we have to give their experiences moral weight, and also that if we were to do that then this leads directly and quickly to human extinction. If we do not find aligning an AI to be morally acceptable, in whatever way and for whatever reason, or if the same goes for the ways we would in practice deploy them, then that is no different than if we do not know how to align that AI. We have to be wise enough to find a way not to build it in the first place.</p><p> Meanwhile the paper explicitly says that many people, including some of its authors, have been deliberately attempting to imbue consciousness into machines in the hopes this will enhance their capabilities. Which, if it works, seems rather alarming.</p><p> Indeed, the recommendation section starts out noticing that a lot of people are imploring us to try not making our AI systems conscious. Then they say:</p><blockquote><p> However, we do recommend support for research on the science of consciousness and its application to AI (as recommended in the AMCS open letter on this subject; AMCS 2023), and the use of the theory-heavy method in assessing consciousness in AI.</p></blockquote><p> One could say that there is the need to study consciousness so that one can avoid accidentally creating it. If we were capable of that approach, that would seem wise. That does not seem to be what is being advocated for here sufficiently clearly, but they do recognize the issue. They say building such a system &#39;should not be done lightly,&#39; that such research could enable or do this, and call to mitigate this risk.</p><p> I would suggest perhaps we should try to write an enforceable rule to head this off, but that seems hard given the whole lack of knowing what the damn thing actually is. Given that, how do we prevent it?</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1693527190372040953">Davidad warns not to get too excited about Iterated Distilled Amplification</a> , even though he thinks it&#39;s a good alignment plan, sir.</p><blockquote><p> Arjun Guha: LLMs are great at programming tasks… for Python and other very popular PLs. But, they are often unimpressive at artisanal PLs, like OCaml or Racket. We&#39;ve come up with a way to significantly boost LLM performance of on low-resource languages. If you care about them, read on!</p><p> First — what&#39;s the problem? Consider StarCoder from <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BigCodeProject">@BigCodeProject</a> : its performance on a PL is directly related to the volume of training data available for that language. Its training data (The Stack) is a solid dataset of permissive code on GitHub.</p><p> So… can we solve the problem by just training longer on a low-resource language? But, that barely moves the needle and is very resource intensive. (The graph is for StarCoderBase-1B.)</p><p> Our approach: we translate training items from Python to a low resource language. The LLM (StarCoderBase) does the translation and generates Python tests. We compile the tests to the low resource language (using MultiPL-E) and run them to validate the translation.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://t.co/hnDwi21LMa">link to paper</a> ] that describes how to use this to create fine-tuning sets.</p><p> Davidad: More and more I think <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulfchristiano">@paulfchristiano</a> was right about <a target="_blank" rel="noreferrer noopener" href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Iterated Distilled Amplification</a> , even if not necessarily about the specific amplification construction—Factored Cognition—where a model can only recursively call itself (or humans), without any more reliable source of truth.</p><p> Nora Belrose: alignment white pill</p><p> Davidad: sadly, no, this only solves one (#7) out of at least 13 distinct fundamental problems for alignment</p><p> 1. Value is fragile and hard to specify</p><p> 2. Corrigibility is anti-natural</p><p> 3. Pivotal processes require dangerous capabilities</p><p> 4. Goals misgeneralize out of distribution</p><p> 5. Instrumental convergence</p><p> 6. Pivotal processes likely require incomprehensibly complex plans</p><p> 7. Superintelligence can fool human supervisors</p></blockquote><p> [numbers 8 to 13]</p><p> Zvi: Can you say more about why (I assume this is why the QT) you think that Arjun&#39;s success is evidence in favor of IDA working?</p><p> Davidad: It is an instance of the pattern: 1. take an existing LLM, 2. “amplify” it as part of a larger dataflow (in this case including a hand-written ground-truth translator for unit tests only, the target language environment, two LLM calls, etc) 3. “distill” that back into the LLM</p><p> When Paul proposed IDA in 2015, there were two highly speculative premises IMO. 1. Repeatedly amplifying and distilling a predictor is a competitive way to gain capabilities. 2. Factored Cognition: HCH dataflow in particular is superintelligence-complete. Evidence here is for 1.</p><p> Arjun&#39;s approach here makes sense for that particular problem. Using AI to create synthetic data via what is essentially translation seems like an excellent way to potentially enhance skills at alternative languages, both computer and human. It also seems like an excellent way to create statistical balance in a data set, getting rid of undesired correlations and adjusting base rates as desired. I am curious to see what this can do for debiasing efforts.</p><p> I remain skeptical of IDA and do not think that this is sufficiently analogous to that, especially when hoping to do things like preserve sufficiently strong and accurate alignment, but going into details of that would be better as its own future post.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1692591486511014355">Worth remembering.</a></p><blockquote><p> Roon: it&#39;s genuinely a criterion for genius to say a bunch of wrong stupid things sometimes. someone who says zero stupid things isn&#39;t reasoning from first principles and isn&#39;t taking risks and has downloaded all the “correct” views.</p></blockquote><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation">Vox&#39;s Sigal Samuel summarizes the poll results from last week</a> that show ordinary people want the whole AI development thing to slow the hell down. Jack Clark also took note of this divergence between elite opinion and popular opinion.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1693106846465442110">Not yet it isn&#39;t.</a></p><blockquote><p> Daniel Eth: “This AGI stuff feels too clever by half” yeah that&#39;s the problem!</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time<guid ispermalink="false"> kLa3HmkesF5w3MFEY</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 24 Aug 2023 15:30:10 GMT</pubDate> </item><item><title><![CDATA[Is this the beginning of the end for LLMS [as the royal road to AGI, whatever that is]?]]></title><description><![CDATA[Published on August 24, 2023 2:50 PM GMT<br/><br/><p> It&#39;s hard to tell, but it sure is...shall we say...interesting.</p><p> Back in the summer of 2020 when GPT-3 was unveiled I wrote a working paper, <a href="https://www.academia.edu/43787279/GPT_3_Waterloo_or_Rubicon_Here_be_Dragons_Version_4_1">GPT-3: Waterloo or Rubicon? Here be Dragons</a> . My objective was to convince myself that the underlying technology wasn&#39;t just some weird statistical fluke, that there was in fact something going on of substantial interest and value. To my mind, I succeeded in that. But I was skeptical as well.</p><p> Here&#39;s what I put on the first page of that working paper, even before the abstract:</p><blockquote><p> <i><strong>GPT-3 is a significant achievement.</strong></i></p><p> <i>But I fear the community that has created it may, like other communities have done before – machine translation in the mid-1960s, symbolic computing in the mid-1980s, triumphantly walk over the edge of a cliff and find itself standing proudly in mid-air.</i></p><p> <i>This is not necessary and certainly not inevitable.</i></p><p> <i>A great deal has been written about GPTs and transformers more generally, both in the technical literature and in commentary of various levels of sophistication. I have read only a small portion of this. But nothing I have read indicates any interest in the nature of language or mind. Interest seems relegated to the GPT engine itself. And yet the product of that engine, a language model, is opaque. I believe that, if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what that engine is doing so that we may gain control over it. We must think about the nature of language and of the mind.</i></p></blockquote><p> I didn&#39;t expect that anyone with any influence in these matters would pay any attention to me – though one can always hope – but that&#39;s no reason not to write.</p><p> That was 2020 and GPT-3. Two years later ChatGPT was launched to great acclaim, and justly so. I certainly spent a great deal of time playing with, investigating it, and <a href="https://new-savanna.blogspot.com/search/label/ChatGPT">writing about it</a> . But I didn&#39;t forget my cautionary remarks from 2020.</p><p> Now we&#39;re hearing rumblings that things aren&#39;t working out so well. Back on August 12 the ever skeptical Gary Marcus posted, <a href="https://garymarcus.substack.com/p/what-if-generative-ai-turned-out">What if Generative AI turned out to be a Dud? Some possible economic and geopolitical implications</a> . His first two paragraphs:</p><blockquote><p> With the possible exception of the quick to rise and quick to fall alleged room-temperature superconductor LK-99, few things I have ever seen have been more hyped than generative AI. Valuations for many companies are in the billions, coverage in the news is literally constant; it&#39;s all anyone can talk about from Silicon Valley to Washington DC to Geneva.</p><p> But, to begin with, the revenue isn&#39;t there yet, and might never come. The valuations anticipate trillion dollar markets, but the actual current revenues from generative AI are rumored to be in the hundreds of millions. Those revenues genuinely could grow by 1000x, but that&#39;s mighty speculative. We shouldn&#39;t simply assume it.</p></blockquote><p> And his last:</p><blockquote><p> If hallucinations aren&#39;t fixable, generative AI probably isn&#39;t going to make a trillion dollars a year. And if it probably isn&#39;t going to make a trillion dollars a year, it probably isn&#39;t going to have the impact people seem to be expecting. And if it isn&#39;t going to have that impact, maybe we should not be building our world around the premise that it is.</p></blockquote><p> FWIW, I believe, and have been saying time and again, that hallucinations seem to me to be inherent in the technology. They aren&#39;t fixable.</p><p> Now, yesterday, Ted Gioia, a culture critic with an interest in technology and experience in business, has posted, <a href="https://www.honest-broker.com/p/ugly-numbers-from-microsoft-and-chatgpt">Ugly Numbers from Microsoft and ChatGPT Reveal that AI Demand is Already Shrinking</a> . Where Marcus has a professional interest in AI technology and has intellectual skin the tech game, Gioia is just a sophisticated and interested observer. Near the end of his post, after many links to unfavorable stories, Gioia observes:</p><blockquote><p> ... we can see that the real tech story of 2023 is NOT how AI made everything great. Instead this will be remembered as the year when huge corporations unleashed a half-baked and dangerous technology on a skeptical public—and consumers pushed back.</p><p> Here&#39;s what we now know about AI:</p><ul><li> Consumer demand is low, and already appears to be shrinking.</li><li> Skepticism and suspicion are pervasive among the public.</li><li> Even the companies using AI typically try to hide that fact—because they&#39;re aware of the backlash.</li><li> The areas where AI has been implemented make clear how poorly it performs.</li><li> AI potentially creates a situation where millions of people can be fired and replaced with bots—so a few people at the top continue to promote it despite all these warning signs.</li><li> But even these true believers now face huge legal, regulatory, and attitudinal obstacles</li><li> In the meantime, cheaters and criminals are taking full advantage of AI as a tool of deception.</li></ul></blockquote><p> Marcus has just updated his earlier post with a followup: <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">The Rise and Fall of ChatGPT</a> ?</p><p> The situation is very volatile. I certainly don&#39;t know how to predict how things are going to unfold. In the long run, I remain convinced that <i>if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what these engines are doing so that we may gain control over them. We must think about the nature of language and of the mind.</i></p><p>敬请关注。</p><p> <i>Cross posted from</i> <a href="https://new-savanna.blogspot.com/2023/08/is-this-beginning-of-end-for-llms-as.html"><i>New Savanna</i></a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road<guid ispermalink="false"> h6pFK8tw3oKZMppuC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:50:21 GMT</pubDate> </item><item><title><![CDATA[AI Safety Bounties]]></title><description><![CDATA[Published on August 24, 2023 2:29 PM GMT<br/><br/><p> Earlier this year, Vaniver recommended <a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">Bug Bounties for Advanced ML Systems</a> .<br><br> I spent a little while at Rethink Priorities considering and expanding on this idea, suggesting potential program models, and assessing the benefits and risks of programs like this, which I&#39;ve called &#39;AI Safety Bounties&#39;:</p><h1> Short summary</h1><p> <strong>AI safety bounties are programs where public participants or approved security researchers receive rewards</strong> <strong>for identifying issues within powerful ML systems</strong> (analogous to bug bounties in cybersecurity). <strong>Safety bounties could be valuable for legitimizing examples of AI risks, bringing more talent to stress-test systems, and identifying common attack vectors</strong> .</p><p> <strong>I expect safety bounties to be worth trialing for organizations working on reducing catastrophic AI risks.</strong> Traditional bug bounties seem fairly successful: they attract roughly one participant per $50 of prize money, and have become increasingly popular with software firms over time. The most analogous program for AI systems led to relatively few useful examples compared to other stress-testing methods, but one knowledgeable interviewee suggested that future programs could be significantly improved.</p><p> However, I am not confident that bounties will continue to be net-positive as AI capabilities advance. At some point, I think the accident risk and harmful knowledge proliferation from open sourcing stress-testing may outweigh the benefits of bounties</p><p> <strong>In my view, the most promising structure for such a program is a third party defining dangerous capability thresholds (“evals”) and providing rewards for hunters who expose behaviors which cross these thresholds</strong> . I expect trialing such a program to cost up to $500k if well-resourced, and to take four months of operational and researcher time from safety-focused people.</p><p> I also suggest two formats for lab-run bounties: open contests with subjective prize criteria decided on by a panel of judges, and private invitations for trusted bug hunters to test their internal systems.</p><p> <i>Author&#39;s note: This report was written between January and June 2023. Since then, safety bounties have become a more well-established part of the AI ecosystem, which I&#39;m excited to see. Beyond defining and proposing safety bounties as a general intervention, I hope this report can provide useful analyses and design suggestions for readers already interested in implementing safety bounties, or in better understanding these programs.</i></p><h1> Long summary</h1><h2> Introduction and bounty program recommendations</h2><p> One potential intervention for reducing <a href="https://www.safe.ai/statement-on-ai-risk">catastrophic AI risk</a> is AI safety bounties: programs where members of the public or approved security researchers receive rewards for identifying issues within powerful ML systems (analogous to bug bounties in cybersecurity). In this research report, I explore the benefits and downsides of safety bounties and conclude that <strong>safety bounties are probably worth the time and money to trial for organizations working on reducing the catastrophic risks of AI</strong> . In particular, testing a handful of new bounty programs could cost $50k-$500k per program and one to six months full-time equivalent from project managers at AI labs or from entrepreneurs interested in AI safety (depending on each program&#39;s model and ambition level).</p><p> I expect safety bounties to be <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.8fvvkifaa4t2">less successful for the field of AI safety</a> than bug bounties are for cybersecurity, due to the higher difficulty of quickly fixing issues with AI systems. <strong>I am unsure whether bounties remain net-positive as AI capabilities increase to more dangerous levels</strong> . This is because, as AI capabilities increase, I expect safety bounties (and adversarial testing in general) to <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">potentially generate more harmful behaviors</a> . I also expect the benefits of the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.linmllyxhki">talent pipeline</a> brought by safety bounties to diminish. I suggest <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">an informal way</a> to monitor the risks of safety bounties annually.</p><p> The views in this report are largely formed based on information from:</p><ul><li> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb">Interviews</a> with experts in AI labs, AI existential safety, and bug bounty programs,</li><li> “Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims” by Brundage et al. arguing for “Bias and Safety Bounties” ( <a href="https://arxiv.org/abs/2004.07213">2020, page 16</a> ),</li><li> A report from the Algorithmic Justice League analyzing the potential of bug bounties for mitigating algorithmic harms ( <a href="https://drive.google.com/file/d/1f4hVwQNiwp13zy62wUhwIg84lOq0ciG_/view">Kenway et al., 2022</a> ),</li><li> Reflections from <a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf">the ChatGPT Feedback Contest</a> .</li></ul><p> See the end of the report for <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#references">a complete list of references</a> .</p><p> Based on these sources, I identify three types of bounty programs that seem practically possible now, that achieve more of <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-decrease-catastrophic-risks">the potential benefits</a> of safety bounties and less of the potential risks than alternative programs I consider, and that would provide valuable information about how to run bounty programs if trialed. In order of my impression of their value in reducing catastrophic risks, the three types are:</p><ul><li> <strong>Independent organizations or governments set</strong> <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/"><strong>“evals”-based standards</strong></a> <strong>for undesirable model behavior</strong> , <strong>and members of the public attempt to elicit this behavior</strong> from publicly-accessible models.</li><li> <strong>Expert panels, organized by AI labs, subjectively judge which discoveries of model exploits to pay a bounty for, based on the lab&#39;s broad criteria</strong> .<ul><li> Potentially with an interactive grant-application process in which hunters propose issues to explore and organizers commit to awarding prizes for certain findings.</li><li> Potentially with a convening body hosting multiple AI systems on one API, and hunters being able to test general state-of-the-art models.</li></ul></li><li> <strong>Trusted bug hunters test private systems,</strong> organized by labs in collaboration with security vetters, with a broad range of prize criteria. Certain successful and trusted members of the bounty hunting community (either the existing community of bug bounty hunters, or a new community of AI safety bounty hunters) are granted additional information about the training process, or temporary access - through security-enhancing methods - to additional features on top of those already broadly available. These would be targeted features that benefit adversarial research, such as seeing activation patterns or being able to finetune a model (Bucknall et al., forthcoming).</li></ul><p> I outline more specific visions for these programs <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ls4dvjl9vdc">just below</a> . A more detailed analysis of these programs, including suggestions to mitigate their risks, is in the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs">Recommendations section</a> . This report does not necessarily constitute a recommendation for individuals to conduct the above stress-testing without an organizing body.</p><p> I expect that some other bounty program models would also reduce risks from AI successfully and that AI labs will eventually develop better bounty programs than those suggested above. Nevertheless, the above three models are, in my current opinion, the best place to start. I expect organizers of safety bounties to be best able to determine which form of bounty program is most appropriate for their context, including tweaking these suggestions.</p><p> This report generally focuses on how bounty programs would work with large language models (LLMs). However, I expect most of the bounty program models I recommend would work with <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.gkrngyd24xkg">other AI systems</a> .</p><h2> Why and how to run AI safety bounties</h2><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#decrease-catastrophic-risks"><strong>Benefits</strong></a> <strong>.</strong> AI safety bounties may yield:</p><ul><li> Salient <strong>examples of AI dangers</strong> .</li><li> Identification of <strong>talented individuals</strong> for AI safety work.</li><li> A small number of <strong>novel insights into issues in existing AI systems</strong> .</li><li> A <strong>backup to auditing</strong> and other expert stress-testing of AI systems.</li></ul><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.4pbr7r48u484"><strong>Key variables</strong></a> <strong>.</strong> When launching bounties, organizers should pay particular attention to the prize criteria, who sets up and manages the bounty program, and the level of access granted to bounty hunters.</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-increase-catastrophic-risks"><strong>Risks</strong></a> <strong>.</strong> At current AI capability levels, I believe trialing bounty programs is unlikely to cause catastrophic AI accidents or significantly worsen AI misuse. The most significant downsides are:</p><ul><li> <strong>Opportunity cost</strong> for the organizers (most likely project managers at labs, AI safety entrepreneurs, or AI auditing organizations like the <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/">Alignment Research Center</a> ).</li><li> <strong>Stifling examples of AI risks</strong> from being made public.<ul><li> Labs may require that bounty submissions be kept private. In that case, a bounty program would incentivize hunters, who would in any case explore AI models&#39; edge cases, not to publish salient examples of AI danger.</li></ul></li></ul><p> Trial programs are especially low-risk since the organizers can pause them at the first sign of bounty hunters generating dangerous outcomes as AI systems advance.</p><p> The risks are higher if organizations regularly run (not just trial) bounties and as AI advances. Risks that become more important in those cases include:</p><ul><li> Leaking of sensitive details, such as information about training or model weights.</li><li> Extremely harmful outputs generated by testing the AI system, such as successful human-prompted phishing scams or autonomous self-replication – analogous to gain of function research.</li></ul><p> For these reasons, I recommend the program organizers perform an annual review of the safety of allowing members of the public to engage in stress testing, monitoring:</p><ul><li> Whether, and to what extent, AI progress has made safety bounties (and adversarial testing in general) more dangerous,</li><li> How much access it is therefore safe to give to bounty hunters.</li></ul><p> Further, I recommend not running bounties at dangerous levels of AI capability if bounties seem sufficiently risky. I think it possible, but unlikely, that this level of risk will arise in the future, depending on the level of progress made in securing AI systems.</p><h2> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommendations-for-running-a-safety-bounty-program"><strong>Other recommended practices for bounty organizers</strong></a> <strong>.</strong></h2><p> I recommend that organizations that set up safety bounties:</p><ul><li> <strong>Build incentives</strong> to take part in bounties, <strong>including</strong> <strong>non-financial incentives</strong> . This should involve building infrastructure, such as leaderboards and feedback loops, and fostering a community around bounties. Building this wider infrastructure is most valuable if organizers consider safety bounties to be worth running on an ongoing basis.</li><li> <strong>Have a pre-announced disclosure policy</strong> for submissions.</li><li> <strong>Share lessons learned</strong> about AI risks and AI safety bounty programs with leading AI developers.</li><li> <strong>Consider PR risks</strong> from running safety bounties, and decide on framings to avoid misinterpretation.</li><li> <strong>Independently assess legal risks</strong> of organizing a contest around another developer&#39;s AI system, if planning to organize a bounty independently.</li></ul><p> Outline of recommended models</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs"><i>Recommended models</i></a> <i>, in order of recommendation, for safety bounties.</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#fn1"><sup>1</sup></a> </p><figure class="table"><table style="background-color:rgb(255, 255, 255)"><tbody><tr><td></td><td> <strong>1. Evals-based</strong></td><td> <strong>2. Subjectively judged, organized by labs</strong></td><td> <strong>3. Trusted bug hunters test private systems</strong></td></tr><tr><td> <strong>Target systems</strong></td><td> A wide range of AI systems – preferably with the system developers&#39; consent and buy-in</td><td> Testing of a particular AI model – with its developer&#39;s consent and engagement</td><td> Testing of a particular AI model – preferably with its developer&#39;s consent and buy-in</td></tr><tr><td> <strong>Prize criteria</strong></td><td> Demonstrate (potentially dangerous) capabilities beyond those revealed by testers already partnering with labs, such as ARC Evals</td><td><p> Convince a panel of experts that the issue is worth dedicating resources toward solving.</p><p> or</p><p> Demonstrate examples of behaviors which the AI model&#39;s developer attempted to avoid through their alignment techniques.</p></td><td> A broad range of criteria is possible (including those in the previous two models).</td></tr><tr><td> <strong>Disclosure model – how private are submissions?</strong></td><td> Coordinated disclosure (Organizers default to publishing all submissions which are deemed safe)</td><td> Coordinated disclosure</td><td> Coordinated- or non-disclosure</td></tr><tr><td> <strong>Participation model</strong></td><td>民众</td><td>民众</td><td>Invite only</td></tr><tr><td> <strong>Access level</strong></td><td> Public APIs</td><td> Public APIs</td><td> Invited participants have access to additional resources – eg, additional non-public information or tools within a private version of the API</td></tr><tr><td> <strong>Who manages the program</strong></td><td> Evals organization (eg, ARC Evals), a new org., or an existing platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td></tr><tr><td> <strong>Program duration</strong></td><td> Ongoing</td><td> Ongoing</td><td> Time-limited</td></tr><tr><td> <strong>Prize scope</strong> (how broad are the metrics for winning prizes)</td><td> Targeted</td><td> Expansive</td><td> Medium</td></tr><tr><td> <strong>Financial reward per prize</strong></td><td> High (up to $1m)</td><td> Low (up to $10k)</td><td> Medium (up to $100k)</td></tr><tr><td> <strong>Pre- or post- deployment</strong></td><td> Post-deployment</td><td> Post-deployment</td><td> Potentially pre-deployment</td></tr></tbody></table></figure><h1><strong>致谢</strong></h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iXECTEyC5PQuYM2aJ/kjalaionq6romubhjxre"></p><p><br> <i>This report is a project of</i> <a href="https://rethinkpriorities.org/"><i><u>Rethink Priorities</u></i></a> <i>–a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Patrick Levermore. Thanks to Ashwin Acharya and Amanda El-Dakhakhni for their guidance, Onni Aarne, Michael Aird, Marie Buhl, Shaun Ee, Erich Grunewald, Oliver Guest, Joe O&#39;Brien, Max Räuker, Emma Williamson, Linchuan Zhang for their helpful feedback,</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb"><i>all interviewees credited in the report</i></a> <i>for their insight, and Adam Papineau for copyediting.</i></p><p><br> <i>If you are interested in RP&#39;s work, please visit our</i> <a href="https://www.rethinkpriorities.org/research"><i><u>research database</u></i></a> <i>and subscribe to our</i> <a href="https://www.rethinkpriorities.org/newsletter"><i><u>newsletter</u></i></a> <i>.</i></p><p></p><p> <strong>I would be happy to discuss setting up AI safety bounties with those in a position to do so.</strong> I can provide contacts and resources to aid this, including <a href="https://bit.ly/SafetyBountiesWorkbook"><u>this workbook</u></a> . Contact me at patricklevermore at gmail dot com.</p><p></p><p> Full report: <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties">AI Safety Bounties</a></p><br/><br/><a href="https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties<guid ispermalink="false"> iXECTEyC5PQuYM2aJ</guid><dc:creator><![CDATA[PatrickL]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:30:00 GMT</pubDate></item></channel></rss>