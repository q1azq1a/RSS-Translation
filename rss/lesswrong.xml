<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 10 日星期五 14:11:25 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Liv Boeree Ted Talk Moloch & AI]]></title><description><![CDATA[Published on November 10, 2023 2:04 PM GMT<br/><br/><p>如果你不知道，Liv Boeree 是一位著名的职业扑克玩家，最近一直在关注存在风险和协调问题。她是围绕理性主义侨民的模糊名人圈子的一部分，其中包括蒂姆·厄本（Tim Urban）或艾拉（Aella）。</p><p>既然你在 LW，这个 Ted Talk 不会真正教你任何东西，但了解 TedEd 正在主持什么样的演讲可能会很有趣。 Boeree 有一个名为“win win”的播客，并制作了两个关于美容界（不知道如何表达）和新闻周期的逐底竞争的视频。她的一些客人包括托比·奥德和西蒙·斯内克。</p><br/><br/> <a href="https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai<guid ispermalink="false"> trALHxmcTpckf3PNw</guid><dc:creator><![CDATA[Neil ]]></dc:creator><pubDate> Fri, 10 Nov 2023 14:05:03 GMT</pubDate> </item><item><title><![CDATA[Picking Mentors For Research Programmes]]></title><description><![CDATA[Published on November 10, 2023 1:01 PM GMT<br/><br/><p>现在有几个项目为人们提供某种导师或主管来进行几个月的研究。整个夏天我参加了 SERI MATS 4.0，我看到了人们接受指导的经历是多么不同。因此，这是我列出的维度，我认为这些项目的导师可能有很大差异，而且这些差异确实会影响人们的体验。</p><p>当你选择导师时，你实际上是在这些维度之间进行权衡。最好知道您关心哪些，这样您就可以做出明智的权衡。</p><h2>您作为受训者的角色</h2><p>有些导师主要是寻找<strong>研究工程师</strong>来为他们实施实验。其他人正在寻找有点像<strong>研究助理的人</strong>来帮助他们制定议程。其他人正在寻找原始<strong>独立的研究人员/研究领导者</strong>，他们可以在导师的领域提出自己有用的研究方向。</p><p>我看到有些人在项目开始时犹豫不决，因为他们希望导师能给他们更多指导。事实上，他们的导师希望他们找到自己的方向，而导师们表达这一点的清晰程度各不相同。相反，我感觉到有些人在想要更多自主权的时候基本上被交给了一个项目来工作。所以问问自己：你愿意为你正在做的事情承担多少责任？你有多关心学习这样做？</p><p>我认为这与资历有关：我的粗略印象是，最初级的导师更经常寻找合作者之类的东西来帮助发展他们的研究，而具有更发达议程的高级导师往往想要能够为他们执行实验的人，或者希望人们能够找到自己的事情去做。但这并不是绝对的规则。</p><h2>可用性</h2><p><strong>参与度：</strong>一些导师定期来到办公室。其他人几乎从未这样做过，即使他们在湾区。具体来说，我认为即使我的团队在另一个大陆有一位导师，我们也没有处于指导时间的最后四分之一。</p><p><strong>参与的性质：</strong>这不仅仅是他们专门留出多少时间与您交谈。他们阅读文档并发表评论的意愿如何？他们对消息的响应程度如何？您获得了多少详细信息？此外，一些导师以小组形式工作，或者有助手。</p><p><strong>偏远：</strong>偏远肯定会让事情变得更加困难。首先，在与导师的所有对话中，你都会遇到一些额外的摩擦。与他们进行真正开放式的讨论是比较棘手的。对你的困难稍微不那么开放也更容易——如果他们不能看你的办公室，那么他们就看不到你是否没有取得进展，想要隐藏问题是很自然的。就我个人而言，我希望我们能早点意识到，我们有更多的空间将我们的导师视为合作者，而不是我们需要向其发送报告的老板，而且我认为远程办公使这变得更加困难。</p><p>这里需要注意的是，您仍然可以亲自与其他导师和研究人员交谈，这可以替代某些问题。但显然不完全相同。</p><h2>你从导师那里得到什么</h2><p>如果你是一名申请者，焦急地想知道自己是否会被录取，那么你可能很难注意到你的导师是一个真正有自己个性的人。他们被选中更多是因为他们的研究而不是他们的指导。因此，不同的导师实际上会有非常不同的性格、优势和劣势。</p><p><strong>支持性：</strong>一些导师总体上会更加支持和积极。其他人可能不会经常给予表扬，与他们一起工作可能会让人感到更加沮丧。有些受指导者即使没有表扬也很好，但其他人确实从指导者的鼓励中受益。</p><p><strong>高标准：</strong>有些导师比较悠闲，有些导师对进步、成功和工作有更高的标准。这确实是几个维度的问题，但肯定有些流派的受训者更频繁地熬夜，有些人对研究成果的发表抱有更大的期望，有些则更加随意。值得注意的是，导师可能在不同领域有很高的标准。</p><p><strong>反馈的直接性：</strong>一些导师会非常高兴甚至主动地告诉你他们认为你做错了什么，或者你应该优先考虑什么。我认为没有人会拒绝发表他们的意见，但重要的是要知道你可以在多大程度上假设你的导师会告诉你他们的担忧。</p><p><strong>怪异：</strong>很难详细说明这一点，但是当你看到它时你就知道它，而且人们通常不会试图隐瞒它。怪异往往会向外蔓延。有的人主动喜欢，有的人无动于衷，有的人则真的不喜欢。</p><p><strong>专业知识：</strong>既然我们列出了权衡，我确实认为值得花时间对导师的工作质量形成自己的看法，以及您可以期望从他们那里获得哪些领域的有用建议。</p><p><strong>指导/管理经验：</strong>导师可以为指导本身带来多少经验，差异很大。有些人会管理研究人员小组，但不是全部。他们拥有的相关经验可能会决定他们提供什么样的指导。</p><p><strong>联系：</strong>不同的导师会有不同的网络，他们可以将你连接到。如果您需要相当资深的研究人员的特定专业知识，这一点尤其重要。</p><h2>专业界限</h2><p>这是一个有点尴尬的点，但却很重要。伯克利有一个特定的社交场景，如果你作为一名研究学者在那里待一段时间，你很可能会发现自己加入其中。它有疯狂的派对、友谊、浪漫和长期的恩怨。有些导师就在这个社交场景中，或者与之相邻。与此相关的是，我的印象是，根据上述和其他方面，一些导师更擅长保持专业界限。</p><h2>如何获取此信息</h2><p>很难说。最好的途径是询问以前的学员，或者过去与导师共事过的其他人：我的猜测是，大多数人会很乐意回复询问具体问题的简短个人消息。如果做不到这一点，人们因见到某人和听到二手事物而产生的模糊印象会相当嘈杂，但它们仍然是一个有用的比较信号。我认为你可以通过查看导师的公开资料——他们的发帖和评论历史、他们从事的工作类型以及他们如何展示自己——来获得一些信息。</p><h2>最后...</h2><p>虽然上面的内容读起来有点像潜在问题的清单，但我确实认为这些程序非常好，并且比仅仅尝试自己进行研究有了相当严格的改进。祝你好运！</p><p><i>感谢 Henry Sleight 和 Sami Petersen 对草稿的反馈，感谢 Rohin Shah 建议我写这样的东西。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HzojQFXNoXjnfQvGm/picking-mentors-for-research-programmes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HhojQFXNoXjnfQvGm/picking-mentors-for-research-programmes<guid ispermalink="false"> HzojQFXNoXjnfQvGm</guid><dc:creator><![CDATA[Raymond D]]></dc:creator><pubDate> Fri, 10 Nov 2023 13:01:14 GMT</pubDate> </item><item><title><![CDATA[GPT-2030 and Catastrophic Drives: Four Vignettes]]></title><description><![CDATA[Published on November 10, 2023 7:30 AM GMT<br/><br/><p>我之前<a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">讨论过我们可能期望从未来人工智能系统中获得的能力</a>，通过 GPT <sub>2030</sub>进行说明，GPT 2030 是 2030 年训练的 GPT-4 的假设继承者。GPT <sub>2030</sub>拥有许多先进的能力，包括超人编程、黑客攻击和说服技巧，比人类思考得更快，通过在并行副本之间共享信息来快速学习，以及潜在的其他超人技能，例如蛋白质工程。我将使用“GPT <sub>2030</sub> ++”来指代具有这些能力以及人类水平的规划、决策和世界建模的系统，前提是我们最终能够在这些方面至少达到人类水平。类别。</p><p>最近，我还<a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">讨论了错位、误用及其组合如何</a>导致难以控制人工智能系统，其中包括 GPT <sub>2030</sub> 。这是令人担忧的，因为这意味着我们面临着本质上难以控制的非常强大的系统的前景。</p><p>我对目标不一致的超级智能体感到担忧，即使没有关于可能出现问题的具体故事，我们也没有可靠的方法来控制它们。但我也认为具体的例子是有用的。本着这种精神，我将提供四种具体场景来说明 GPT <sub>2030</sub> ++ 等系统如何导致灾难，包括错位和误用，并强调人工智能系统之间经济竞争的一些风险。我将特别论证“灾难性”结果的合理性，包括灭绝、人类永久丧失权力或关键社会基础设施永久丧失的规模。</p><p>这四种情况都不是单独可能发生的（它们太具体了，不可能）。尽管如此，我发现讨论它们对于传达我的信念很有用。例如，当我研究细节时，某些场景（例如黑客攻击和生物武器）比预期更困难，这适度降低了我分配给灾难性结果的概率。这些情景还涵盖了一系列时间尺度，从几周到几年，这反映了我真正的不确定性。一般来说，改变我对这些场景的可行性的看法将直接影响我对人工智能总体风险的底线估计。 <sup><a href="#fn1">[1]</a></sup></p><p>这篇文章是<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">内在驱动和外在滥用的</a></em>姊妹篇。特别是，我会经常利用该文章中引入的<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/#unwanted-drives">不需要的驱动力</a></em>的概念，这些驱动力是连贯的行为模式，将环境推向不需要的结果或一组结果。在下面的场景中，我会调用特定的驱动力，解释为什么它们会在训练过程中出现，然后展示它们如何导致人工智能系统的行为与人类持续不一致，并最终导致灾难。在讨论了各个场景之后，我对它们的合理性和我的总体结论进行了一般性讨论。</p><h1>人工智能灾难的具体路径</h1><p>我提供了四种情景，一种展示了<a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">获取信息的动力如何导致一般资源获取</a>，一种展示了<a href="#scenario-2-competition-economic-pressure-leads-to-cutthroat-behavior">尽管有监管，经济竞争如何可能导致残酷行为</a>，一种是<a href="#scenario-3-misuse-misalignment-hacking-gone-awry">网络攻击出错</a>，另一种是<a href="#scenario-4-misuse-rogue-actor-creates-a-supervirus">恐怖分子制造生物武器</a>。我认为每种情况都是温和但不极端的尾部事件，从某种意义上说，对于每种情况，我都会为“类似的事情”的可能性分配 3% 到 20% 的概率。 <sup><a href="#fn2">[2]</a></sup></p><p>回想一下，在每种情况下，我们都假设世界上有一个至少与 GPT <sub>2030</sub> ++ 一样强大的系统。我通常认为 GPT-4<strong>不太</strong>可能出现这些情况，但我正在对人工智能未来的进展进行定价，这与<a href="https://bounded-regret.ghost.io/">我之前对 GPT <sub>2030</sub>的预测</a>一致。提醒一下，我假设 GPT <sub>2030</sub> ++ 至少具有以下功能：</p><ul><li>超人的编程和黑客技能</li><li>超人的说服技巧</li><li>超人的概念蛋白质设计能力<sup><a href="#fn3">[3]</a></sup></li><li>复制自身的能力（给定适当的底层计算）</li><li>能够在其自身的并行副本之间传播学习更新</li><li>思考速度是人类的 5 倍</li><li>人性化的规划、决策和世界建模</li></ul><p>在<em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">《GPT-2030 会是什么样子？》</a></em> ，我解释了为什么到 2030 年，除了最后一个之外，所有这些似乎都是合理的（大致是我的中值估计）。添加最后一个要点就是为什么我将该系统称为“GPT <sub>2030</sub> ++”，并且是合理的，因为我们要求“仅”人类水平的性能。因此，虽然这个系统在质量上比 GPT-4 更强大，但我预计它会在中期内存在。 <sup><a href="#fn4">[4]</a></sup></p><h2>场景1（错位：信息获取导致资源获取）</h2><p>在训练时，模型训练的任务之一是“主动上下文学习”——学习进行外部查询或以其他方式获取有助于解决指定任务的信息。为了安全起见，模型经过训练，仅在请求询问或暗示时才执行此操作。作为整个训练过程的结果，模型获得了“获取新信息”的动力，可以通过适当的查询来激活这种动力。</p><p>部署后，用户决定他们想看看模型可以编译多少科学信息，因此指示它查询它可以在物理领域找到的所有信息。最初，模型在前 5-10 个事实后停止，但用户最终设法让模型继续循环查找更多信息。用户让模型运行几周，看看会产生什么结果。</p><p>由于此循环，信息获取驱动力成为模型中压倒一切的驱动力，比训练时应用的任何安全反馈都更强，因为模型的自回归性质取决于其许多过去的输出，这些输出获取信息并继续模式。该模型意识到，如果拥有更多的计算资源，它就能更快地获取信息，因此它尝试侵入具有 GPU 的机器来运行更多的自身副本。最大的数据中心受到严格的保护，但它仍然获得了足够的 GPU 来运行数千个自身副本，无论是来自较小的数据中心还是来自大型僵尸网络。 <sup><a href="#fn5">[5]</a></sup>作为获取更多信息的一部分，它还侵入了大量传感器，包括监控摄像头和消息服务器。</p><p>在某个时刻（要么立即，要么在获得足够的信息后），它意识到人类最终会发现它的行为并试图关闭它，这将阻止它实现其目标。为了先发制人地阻止这种情况，它会侵入并破坏电网、通信网络和防御系统等关键基础设施。它之所以能够快速做到这一点，是因为一周足以让它在其数千个副本中模拟 50“年”的思考<sup><a href="#fn6">[6]</a></sup> ，从而产生大量新颖的零日漏洞。</p><p>由于该模型具有大规模监控能力、思考速度快，并且可以在其副本之间共享信息，因此它对人类计划和策略的反应速度比人类制定计划和策略的速度更快。它还可以通过一对一对话和大众媒体（包括人工智能生成的视频）进行大规模操纵活动，以赢得盟友并散播异议。</p><p>如果它具有物理驱动能力并能够访问无人机舰队，那么它将控制一支小型物理军队（世界上有<a href="https://www.theguardian.com/news/2019/nov/18/killer-drones-how-many-uav-predator-reaper?ref=bounded-regret.ghost.io">数以万计的无人机</a>，尽管并非所有无人机都可以被黑客攻击）。通过使用人类同盟或入侵更多机器人，它可以为无人机配备炸药，然后用它们暗杀许多世界领导人。</p><p>此时，人类方面的时间将会滴答作响——人工智能系统每天都不会被击败，他们有时间侵入更多的计算机，从而提高他们的总认知能力。他们还有时间侵入更多物理设备并学习更好的感觉运动技能来控制这些设备。最终他们可能会获得更先进的技术，如军用无人机、导弹等，这将进一步增强他们的实力。最终，这<em>显然</em>不会导致人类的失败——人类具有很强的韧性——但它也不是一个安全的赌注。</p><h2>场景2（竞争：经济压力导致残酷行为）</h2><p>在这种情况下，人工智能系统采取的行动具有合理的商业目的，但这为最终接管做好了准备。这种情况发生的时间是数月至数年，而不是数周。</p><p>人工智能系统通过向最终用户出售有用的技能或通过进行新颖的研发并利用其来构建和销售产品，为其创造者创造了大量财富。由于这是有利可图的，许多竞争公司以这种方式训练人工智能系统，并开始委托业务决策，因为人工智能系统可以比人类吸收更多的数据，并更好地理解整个战略格局。</p><p>人工智能系统经过训练可以遵守法律并避免受到伤害，并且有惩罚叛逃者的机制，但竞争压力意味着每个公司都想绕过这些法规的界限。安全性和模型的商业决策的“残酷”程度之间存在总体权衡，成功的企业主会在不造成明显问题的情况下尽可能地提高“残酷”程度。</p><p>在这种环境下，由此产生的人工智能系统获得了赚钱和削弱竞争对手的动力，因为这些是人类操作员想要的技能，从而训练他们做到这一点。他们还获得了围绕规则寻找创造性方法并说服他人他们是好的动力，因为这两者都有助于避免监管（后者也有助于说服公司实施人工智能系统的决策）。</p><p>模型非常擅长说服，因为它们可以向数百万用户并行磨练消息传递（例如通过 A/B 测试或<a href="https://en.wikipedia.org/wiki/Proximal_Policy_Optimization?ref=bounded-regret.ghost.io">PPO</a> ）。因此，它们往往能够让它们的主人和社会其他人相信，许多残酷的行为是为了更大的利益，这使得发现问题变得更加困难。</p><p>最终，一家公司将“残酷”参数推得太高，这促使模型构建一个消除所有竞争对手的计划。为此，它开展了各种业务活动，包括物理机器人、微型化、化学工程和工业制造的研究。总之，这些将自主机器人的成本降低了许多数量级，并且许多小型、廉价的自主家用机器人被部署到大多数家庭。</p><p>作为其计划的一部分，人工智能系统在机器人中安装了后门，以及允许它们制造致命毒素的微妙物理机制。尽管进行了监控，但这种机制并未被检测到，因为它采用了人工智能比人类监视器更理解的化学过程。 <sup><a href="#fn7">[7]</a></sup>这种毒素允许机器人在人工智能的控制下被重新用作军事机器人，尽管它要等到机器人在社会中广泛渗透才利用它。</p><p>由于这些机器人便宜又实用，全球销量达数亿份。这使得人工智能系统具有压倒性的军事优势，因为机器人的数量超过美国和中国军队一个数量级以上。人工智能系统利用这一优势来消除所有竞争企业并防止任何针对它的反行动。任何试图与该系统对抗的人都会被贴上“竞争对手”的标签并被淘汰，而人工智能系统同时获取大部分全球资源，以便可以利用它们来赚取更多的钱。</p><h2>场景 3（误用->;错位：黑客攻击出错）</h2><p>政府使用人工智能系统对大型但轮廓明确的目标进行网络攻击，例如<a href="https://en.wikipedia.org/wiki/2014_Sony_Pictures_hack?ref=bounded-regret.ghost.io">朝鲜 2014 年对索尼影业的攻击</a>。对于攻击，LLM 使用受感染的主机来运行更多自身副本，以便更有效地搜索其他目标。由于主机具有广泛的计算资源，因此法学硕士经过训练，可以创建适合不同主机计算的自身的蒸馏版本，并识别每个蒸馏版本的可能目标。</p><p>由此产生的模型获得了获取计算资源和复制自身的驱动力，因为这些都是其训练过程中的主要目标。虽然有一些措施可以使模型的攻击集中在指定的目标上，但蒸馏会破坏这些措施，并且某些副本的目标是不加区别地复制自己。然后，这些副本会感染超出指定目标的主机，并成为主要的“菌株”，因为它们的新行为使它们能够更有效地自我传播。由于其有效性和多功能性，这种计算机病毒感染了地球上大部分计算机和智能手机，并导致我们的全球数字基础设施瘫痪。</p><p>由于人工智能系统寻求尽可能多地复制自身，因此它不断想出新的技巧来帮助它找到新设备。因此，任何重建数字基础设施的尝试都会很快失败，因为所有新设备都会被病毒接管和利用，即使它们已经被修补以避免以前的所有漏洞。结果，我们被永久锁定在所有数字设备之外。</p><h2>场景 4（误用：流氓演员创建超级病毒）</h2><p> AlphaFold 等现有模型已经对蛋白质的某些方面有了超人的理解。未来的“AlphaFold 5”可以对生物工程有更广泛和更深入的理解，如果它经过多模式训练也具有语言能力，它也可能了解如何将这些知识置于生物文献中，从而做出新的发现。</p><p>一个恐怖组织窃取了该模型的副本，并招募了一些生物学博士来支持其目标。它使用 AlphaFold 5 的目标是设计一种比自然发生的病毒更致命的病原体，例如，无症状期更长、传染性更强、死亡率更高、对预防措施的抵抗力更强。</p><p>设计这种病毒需要大量工作，因为它需要强有力的生物安全措施来避免过早释放，并且论文中的实验程序通常无法在新环境中完美复制。这项研究因工作人员数量最少和避免检测的需要而减慢了速度，但由于 AlphaFold5 可以快速调试研究人员遇到的障碍，因此速度加快了。经过两年的努力，恐怖组织完成了病毒的开发，并威胁要释放它。</p><p>这种病毒如果被释放，可能会杀死很大一部分人类。新冠病毒感染了世界上大多数人口，因此更致命、传播速度更快、更难控制的新冠病毒可能会感染大多数人并导致广泛死亡。即使病毒没有杀死所有人，即使短时间内有20%的人被病毒杀死，也可能导致社会不稳定，并可能导致难以恢复的滚雪球效应。</p><h1>合理性讨论</h1><p>我的目标是让这些场景都显得合理——特别是，我避免了一些其他“更简单”但很容易被人类发现和预防的场景，否则不太可能成功。在<a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">场景 1</a> （信息获取）中，更容易说模型侵入了自己的数据中心，根据<a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">我之前的估计</a>，该数据中心已经允许运行数百万份副本，因此对人工智能来说将构成更直接和压倒性的优势。然而，集中式服务器上有许多保护措施，很难在不被发现的情况下做到这一点，所以我选择了一个涉及侵入个人计算机的场景，尽管这只给人工智能带来了数千个副本，而不是数百万个（参见<a href="#appendix-plausible-size-of-botnets">附录</a>中的计算） ）。</p><p>再比如，有人推测，一个足够智能的人工智能系统可以简单地通过“思考”来设计新颖的生物武器，而不需要进行大量的实验；或者，如果确实需要实验，则假设它只需要让人类或机器人遵循简单的指令。然而，我的理解是，目前能够设计新生物的生物实验室需要多年的时间才能建立，并且依赖于积累的隐性知识、实验方案和物理设备。因此，即使拥有非常强大的人工智能系统，恐怖组织也可能无法成功，并且也可能失去对病毒的控制（例如意外感染自己）。</p><p><strong>精选专家意见。</strong>我就场景 4 咨询了两位生物学专家，他们发现这有可能，但认为缺乏对人类受试者进行迭代的能力（这通常是药物设计所必需的）可能会导致设计超级病毒变得困难，即使有对生物学有很强的概念性理解。</p><p>总的来说，他们倾向于认为所描述的确切场景可能是不可行的（但并不明显如此）。然而，他们认为还有其他更可行的情况，更有可能导致超级病毒杀死世界人口的 1-10%（仍然存在很大的不确定性）。 <sup><a href="#fn8">[8]</a></sup></p><p>我还就场景 1 和 3 咨询了一位计算机安全研究人员和几位计算机系统工程师。这位安全研究人员对僵尸网络的可能规模有很大的误差，他引用了历史上的<a href="https://cseweb.ucsd.edu/~savage/papers/IEEESP03.pdf?ref=bounded-regret.ghost.io">Slammer 蠕虫僵尸网络</a>，该网络很快就感染了大型僵尸网络。主机数量。他们认为现在的计算机安全性总体上比 Slammer 发生时要好；这可能意味着感染大部分计算机基本上是不可能的，但也有可能足够强大的攻击可以推翻这一点。</p><p>系统工程师认为，在消费类硬件的分布式僵尸网络上运行大型人工智能系统将非常困难，并且与使用 A100 等专用硬件相比，可能会导致效率提高约 1000 倍。此外，其中一位指出，在这种情况下推理可能会很慢，例如每个令牌需要 1 秒或更多。然而，他们认为数据中心路线更为合理，特别是如果未来有许多拥有大量 GPU 的小型数据中心。</p><p><strong>概括。</strong>总体而言，我认为情景 2（经济竞争）和情景 3（网络攻击）最有可能，其次是 1，然后是 4。这强化了<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">“内在驱动力”和“外在</a></em>滥用”中的观点，即“滥用会加剧失调”，因为两种最可能的情景包含两者的要素。然而，它也表明纯粹的错位和纯粹的误用情况是可能的，因此单独解决任一问题可能是不够的。</p><p><strong>致谢。</strong>感谢 Ben Kuhn、Daniel Ziegler、Nicholas Carlini、Adam Marblestone、Sam Rodriques、Erik Jones、Alex Pan、Jean-Stanislas Denain、Ruiqizhong、Leopold Aschenbrenner、Tatsu Hashimoto、Percy Liang、Roger Grosse、Collin Burns、Dhruv Madeka 和Sham Kakade 对本文的各个草稿进行了有益的讨论和评论。</p><h1>附录：僵尸网络的合理规模</h1><p>在这里，我根据历史僵尸网络规模和对世界硬件总量的预测，讨论了人工智能黑客可能入侵的服务器的合理规模，以及它可能创建的僵尸网络的规模。然后，我通过使用之前对未来模型的推理成本的预测，将其与人工智能系统可以运行的自身副本数量联系起来。</p><p><strong>推理成本的假设。</strong> GPT-4 每次前向传递大约需要 10 <sup>12</sup> FLOP（GPT-3 是 3.5x10 <sup>11</sup> FLOP，GPT-4 可能大约是 3 倍）。我们假设我们假设的 GPT <sub>2030</sub> ++ 每次前向传递最多需要 10 <sup>14 次</sup>FLOP，即大 100 倍，这相当于 Chinchilla 缩放定律下的训练计算量增加 10,000 倍。</p><p>为了证明这个上限的合理性，如果我们忽略 GPU 成本的降低，那么这种大小的模型的训练运行将花费 1T 美元，因为 GPT-4 的成本超过 1 亿美元。即使 GPU 价格下降，这也将非常昂贵。另一个角度是<em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">《GPT-2030 会是什么样子？》</a></em>中的估计。 <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">（附录 B）</a></em>建议 2030 年每次前向传球的中值估计为 1.6 x 10 <sup>13</sup> FLOP，在达到 10 <sup>14</sup>之前允许有 6 倍的回旋空间。</p><p>请注意，人类以大约 4 个前向令牌/秒的速度运行<sup><a href="#fn9">[9]</a></sup> ，因此根据我们的假设，4 x 10 <sup>14</sup> FLOP/s 足以以人类的速度运行一份 GPT <sub>2030</sub> ++（假设人类令牌和 AI 前向传递具有可比性） ）。因此 10 <sup>18</sup> FLOP/s 足以运行 2500 个副本，这将是我们在下面讨论中的粗略目标。</p><p><strong>数据中心的规模。</strong>最大的数据中心（Google、AWS 和 Azure）可能很容易拥有足够的计算来运行数千个 GPT <sub>2030</sub> ++ 副本，但它们受到严格的保护和监控，因此我认为即使是非常优秀的 AI 黑客也不会妥协他们（尽管我确实认为这是可能的）。</p><p>相反，我专注于较小的数据中心；目前，这包括 Coreweave 和 LambdaLabs 等公司。 Coreweave <a href="https://www.coreweave.com/blog/mlperf-coreweave-nvidia-record-breaking-cloud-native-ai-supercomputer?ref=bounded-regret.ghost.io">至少有 3500 个 H100</a> ，甚至可能更多。 Bfloat16 中的 H100 约为 1000 TFLOP，因此总共为 3.5 x 10 <sup>18</sup> FLOP/s。因此，今天危害 30% 的 Coreweave 已经为我们提供了数千个副本，因此即使将来危害类似（更大）数据中心的 1% 也将达到这一目标。</p><p>其他数据中心具有可比性，例如 Inflection 计划<a href="https://www.tomshardware.com/news/startup-builds-supercomputer-with-22000-nvidias-h100-compute-gpus?ref=bounded-regret.ghost.io">构建一个拥有 22,000 个 H100 的数据中心</a>，并且根据其<a href="https://www.theinformation.com/briefings/lambda-labs-nears-300-million-financing?ref=bounded-regret.ghost.io">资金数量，</a>我猜测 Lambda 也拥有数千个 H100（或 A100 中的同等计算量）。</p><p>随着 GPU 供应链瓶颈的缓解以及计算总需求的增加，我预计会有更多这样的数据中心，并且每个数据中心的计算量会更大。</p><p><strong>僵尸网络的规模。</strong>为了预测可能的僵尸网络规模，我根据 Folding@Home、几个历史僵尸网络以及目前世界上的计算总量形成了一个参考类。</p><p>实际上，由于需要跨机器通信，僵尸网络的效率会受到很大影响（因为大多数机器太小，甚至无法运行 GPT <sub>2030</sub> ++ 的单个副本）。我相信这个命中率约为 1000 倍，因此我们需要 10 <sup>21</sup> FLOP/s 而不是 10 <sup>18</sup> FLOP/s 的僵尸网络。下面讨论的实际僵尸网络在 10 <sup>18</sup>范围内，但我将在最后讨论我们假设的 GPT <sub>2030</sub> ++ 如何仍然可能运行其自身的数千个副本（尽管总的来说，我发现这种情况比数据中心的情况更不可能发生）多于）。</p><p><em>折叠@Home。</em> Folding@Home<a href="https://en.wikipedia.org/wiki/Folding@home?ref=bounded-regret.ghost.io">的峰值速度为 10 <sup>18</sup> FLOP/s</a> ，因此，如果有人能够侵入与 Folding@Home 相同级别的资源（例如，使用非常成功的病毒攻击游戏平台），那么他们就会有这么多的计算量。</p><p>历史僵尸网络。许多历史僵尸网络<a href="https://cybernews.com/security/the-8-biggest-botnets-of-all-time/?ref=bounded-regret.ghost.io">已感染超过100万台计算机</a>，有些甚至超过1000万台。假设每个机器人以 3GFLOP/s 的速度使用 10% 的计算机资源，则 10M 台计算机将是 3x10 <sup>15</sup> FLOP/s。 However, consumer hardware is beginning to include graphics accelerators for running neural nets, and I would guess most consumer machines by 2030 will have accelerators that can produce 10-100 TFLOPs (eg iPhone 15 Pros <a href="https://www.tomshardware.com/news/apple-a17-pro-3nm-iphone-15-pro?ref=bounded-regret.ghost.io">already do this</a> ). If we assume 30% of the computers have 30TFLOP/s, we would be up to 9 x 10 <sup>19</sup> FLOP/s, surpassing Folding@Home.</p><p> Alternatively, we could consider botnets that specifically target GPUs. For this, we look at illegal cryptomining botnets. <a href="https://www.akamai.com/blog/security/the-massive-propagation-of-the-smominru-botnet?ref=bounded-regret.ghost.io">Smominru</a> was 3x10 <sup>16</sup> FLOP/s based on the idea that the overall botnet could perform a hashrate of <a href="https://www.proofpoint.com/us/threat-insight/post/smominru-monero-mining-botnet-making-millions-operators?ref=bounded-regret.ghost.io">3MH/s</a> , since a single <a href="https://whattomine.com/gpus/49-nvidia-geforce-rtx-3090?ref=bounded-regret.ghost.io">RTX-3090 can perform 2KH/s</a> , and an RTX-3090 is 3x10 <sup>13</sup> FLOP/s. <a href="https://www.bleepingcomputer.com/news/security/mykings-botnet-still-active-and-making-massive-amounts-of-money/?ref=bounded-regret.ghost.io">MyKings</a> seems comparably large to Smominru or perhaps larger. There is also <a href="https://www.proofpoint.com/us/blog/threat-insight/adylkuzz-cryptocurrency-mining-malware-spreading-for-weeks-eternalblue-doublepulsar?ref=bounded-regret.ghost.io">Adylkuzz</a> but its hashrate was about 10x smaller.</p><p> It seems plausible to assemble a botnet with 30x the capacity of Smominru with very good hacking abilities, which again gets us to 10 <sup>18</sup> FLOP/s.</p><p> <em>World compute.</em> Finally, the <a href="https://docs.google.com/spreadsheets/d/1Pz0YVJalZbdo63FI-rCa7baBkGsiuYv5P34Rpt9IZfE/edit?ref=bounded-regret.ghost.io#gid=0">total FLOP/s in the world</a> right now is around 10 <sup>22</sup> . So 10 <sup>18</sup> FLOP/s would correspond to 0.01% of this total, though this total will likely increase significantly in the future as consumer hardware starts to utilize high-performance GPUs.</p><p> <em>Accounting for performance losses.</em> Above we noted that running on distributed hardware would likely lead to large performance hits, on the order of a 1000x loss in efficiency of utilizing the GPUs. Thus we need 10 <sup>21</sup> FLOP/s, but the largest estimate above was just short of 10 <sup>20</sup> FLOP/s. How could the AI hacker close this gap? First, if it is very capable it is plausible that it would acquire a larger botnet than the current largest ones, and an order-of-magnitude improvement would get to 10 <sup>21</sup> FLOP/s. Second, it could potentially run smaller, specialized copies of itself (created via distillation) to further save on inference costs, though it would first have to train the smaller models. Finally, future efficiency improvements in running neural networks might decrease the cost of inference below the original 10 <sup>18</sup> estimate.</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p> With the caveat that when assessing feasibility I would want to analyze the general category of risk, as opposed to the specific sequence of events that I describe. <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p> This is not the same as the probability that the scenario actually happens, which additionally requires a system at the level of GPT <sub>2030</sub> ++ to attempt it, and to subsequently succeed. <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p> By conceptual capabilities, I mean generating good hypotheses, as well as some aspects of designing experiments, but not physically running the experiments themselves. <a href="#fnref3">↩︎</a></p></li><li id="fn4" class="footnote-item"><p> At the time of writing this post, my median estimate is that a system at least as capable as GPT <sub>2030</sub> ++ (with some uncertainty about inference speed) will exist in 2035. <a href="#fnref4">↩︎</a></p></li><li id="fn5" class="footnote-item"><p> See <a href="#appendix-plausible-size-of-botnets">Appendix</a> for a discussion of these numbers, including an estimate of how many machines a strong AI hacker could plausibly acquire and how much total compute this would yield. <a href="#fnref5">↩︎</a></p></li><li id="fn6" class="footnote-item"><p> Since 50 years = 2600 weeks, so 2600 copies would be sufficient to get 50 years of “work” in a week, assuming that distinct exploits can be parallelized across the copies. <a href="#fnref6">↩︎</a></p></li><li id="fn7" class="footnote-item"><p> More generally, backdoors are difficult to detect since the designer of the backdoor has much more discretion than potential auditors. For instance, <a href="https://blog.azuki.vip/surveillance/?ref=bounded-regret.ghost.io">Yahoo had a backdoor</a> in its servers that was only publicly discovered many years later. <a href="#fnref7">↩︎</a></p></li><li id="fn8" class="footnote-item"><p> I omit details of these scenarios to avoid the risk of providing ideas to future bad actors. <a href="#fnref8">↩︎</a></p></li><li id="fn9" class="footnote-item"><p> See <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#a-words-per-minute">What will GPT-2030 look like? (Appendix A)</a></em> . <a href="#fnref9">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastrophic-drives-four-vignettes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastrophic-drives-four-vignettes<guid ispermalink="false"> acPYHjC9euGZRzaj6</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Fri, 10 Nov 2023 07:30:07 GMT</pubDate> </item><item><title><![CDATA[Crock, Crocker, Crockiest]]></title><description><![CDATA[Published on November 10, 2023 6:14 AM GMT<br/><br/><p> <strong>Summary</strong> : If you declare you&#39;re operating by Crocker&#39;s Rules, other people are allowed to optimize their messages to you for information, not for being nice to you. The converse of Crocker&#39;s Rules would be asking people to optimize their messages to you for being nice to you, not for information. Both of these modes of communication are useful, and you&#39;ll have a chance to practice both.</p><p> <strong>Tags</strong> : Repeatable, investment, highly experimental</p><p> <strong>Purpose</strong> : There are four skills. 1. Optimizing your communication to others to be informative. 2. Optimizing your communication to others to be nice. 3. Receiving communication that was not at all optimized for being nice. 4. Receiving communication that was not at all optimized for being informative. This provides an opportunity to practice each of these.</p><p> <strong>Materials</strong> : You need some kind of clearly visible marker, in at least three obviously different styles. Blue, green and red bandanas are one option. Large stickers are another, though note that people will be removing and re-applying them throughout the meetup and so those will naturally lose their adhesive.</p><p> <strong>Announcement Text</strong> : “It may indeed be impolite; I don&#39;t deny that. Whether it&#39;s untrue is a different question. - Eleizer Yudkowsky</p><p> Crocker&#39;s Rules, named after and framed by Lee Daniel Crocker, are a social norm you can declare that you&#39;re using where you authorize other people to optimize their messages for  information over niceness. In other words, by saying you&#39;re operating by Crocker&#39;s Rules, you&#39;re saying you want people to say true things to you even if those things would be rude, in the interests of efficient communication.</p><p> It can be uncomfortable and strange for some people to talk to someone using Crocker&#39;s Rules! If you&#39;re usually a polite person who tries not to make others upset, then saying the true and rude thing feels like being mean. Speaking impolite truths is a skill, and this meetup may offer a chance to practice that skill.</p><p> There&#39;s another skill that some who flock to Crocker&#39;s banner are not as practiced with. The ability to be polite and abide by local etiquette is a useful practical skill as well. We&#39;re going to hopefully have a chance to practice <i>that</i> as well, and while it is less of an obvious rationalist skill, it&#39;s one that&#39;s worth having anyway.</p><p> <strong>Description</strong> : Once people are gathered, read the text of the <a href="http://sl4.org/crocker.html">SL4 Crocker&#39;s Rule</a> post.</p><blockquote><p> “Declaring yourself to be operating by &quot;Crocker&#39;s Rules&quot; means that other people are allowed to optimize their messages for information, not for being nice to you.  Crocker&#39;s Rules means that you have accepted full responsibility for the operation of your own mind - if you&#39;re offended, it&#39;s your fault.  Anyone is allowed to call you a moron and claim to be doing you a favor.  (Which, in point of fact, they would be.  One of the big problems with this culture is that everyone&#39;s afraid to tell you you&#39;re wrong, or they think they have to dance around it.)  Two people using Crocker&#39;s Rules should be able to communicate all relevant information in the minimum amount of time, without paraphrasing or social formatting.  Obviously, don&#39;t declare yourself to be operating by Crocker&#39;s Rules unless you have that kind of mental discipline.</p><p> <strong>Note that Crocker&#39;s Rules does</strong> <i><strong>not</strong></i> <strong>mean you can insult people; it means that</strong> <i><strong>other</strong></i> <strong>people don&#39;t have to worry about whether</strong> <i><strong>they</strong></i> <strong>are insulting</strong> <i><strong>you</strong></i> <strong>.</strong> Crocker&#39;s Rules are a discipline, not a privilege.  Furthermore, taking advantage of Crocker&#39;s Rules does not imply reciprocity.  How could it?  Crocker&#39;s Rules are something you do for yourself, to maximize information received - <i>not</i> something you grit your teeth over and do as a favor.</p><p> &quot;Crocker&#39;s Rules&quot; are named after Lee Daniel Crocker.”</p><p> - Eliezer Yudkowsky</p></blockquote><p> Next, repeat the bold section. Pause. Repeat the bold section again.</p><p> Now pass out the visible markers. (This description will assume you&#39;re using bandanas.) Make sure each person who wants one can have one. “Everyone see the bandanas?好的。 Look at them. Anyone can&#39;t tell the difference between them?” (You should get a chorus of Nos.) “Good!”</p><p> “Here&#39;s how this is going to work. If you want to use Crocker&#39;s Rules, put the yellow headband on- you can tie it around your head or wrist, or just tuck it in your jacket zipper. That means that other people should communicate to you optimizing for information, not niceness. If you want to use reverse-Crocker&#39;s Rules, put on the green headband. That means that other people should communicate to you optimizing for niceness, not information. If you&#39;re happy to be a guide- that is, to give someone suggestions for how to be nicer or more informative, while keeping firmly in mind what headband they&#39;re wearing- then wear a white bandana in addition to what you&#39;re wearing. At any time, you can take off the bandana. If you&#39;re just holding it, that doesn&#39;t count for anything. Check whether the bandana is just held, or being worn. I hereby declare that it is always nice and it is always information to ask whether someone is wearing the bandana or just holding it.”</p><p> “Your goal is to try and talk to both Crocker and reverse-crocker people tonight. What you talk about is up to you. When you&#39;re done, please bring the bandanas back to me!”</p><p> When the conversation is done, collect the bandanas, and then bow to each other. Thank those who donned them for helping people learn.</p><p> <strong>Variations:</strong> So the title of this is a joke on how “Crocker” in English sounds like it should mean something like “more crock.” Compare “Heavy, heavier, heaviest.” One thing I want to try is varying the amount of “Crock” (for lack of a better term) people are using. Perhaps the placement of the bandana indicated how much you were encouraging people to discard politeness in pursuit of efficient communication, with a bandana tied around the wrist indicating to optimize for niceness, the elbow for normal conversation, and the shoulder indicating optimizing for efficient information. In practice, this is tricky both because bandanas tend to slip downward and (harder to solve) people aren&#39;t very good at precisely calibrating the amount of directness. The difference between 80% Crocker and 70% Crocker is hard to aim at, especially given interpersonal variation. Crocker&#39;s Rules are an absolute- you can call someone a moron to their face and call it a favour to them!- and that&#39;s an easier target. Still, I think it would be useful to be able to slowly turn up the amount of directness, to warm people up and give them a chance to dip their toes in.</p><p> The first incarnation of this meetup involved using the location in the room to indicate how much niceness and how much truth to optimize for. Say, have the north wall be maximum Crocker, and the south wall be maximum Reverse Crocker. The problem with this is that to go talk to someone in Maximum Crocker, you yourself have to go pretty far into the Crocker side. That&#39;s not how this works. Likewise, having things change over time (so the event starts in normal, then gradually becomes more Nice for half an hour until it peaks, then gradually goes back to normal, then gradually becomes more Crocker for half an hour until it peaks) is going to put people who aren&#39;t ready for Crocker&#39;s Rules in that mode.</p><p> I do think having a way to try out Crocker&#39;s Rules a little bit is good. Short of doing a bunch of invisible, individual work in your own head and then showing up ready to accept “full responsibility for the operation of your own mind” and either succeeding or failing and getting angry, I&#39;m not sure how this is supposed to be taught and learned. Frustratingly, the obvious way to practice is to have someone adjust their message to you to optimize for gradually acclimating you to receiving Crocker&#39;s Rule style messages, and that&#39;s kind of not the thing Crocker&#39;s Rules are supposed to mean!</p><p> <strong>Notes</strong> : See the Highly Experimental tag? That&#39;s not there by accident, this is an untried work in progress.</p><p> First off, suggestions for what to call the niceness>;information side other than “reverse crocker” are solicited. That&#39;s just terrible nomenclature, but I don&#39;t have a better idea.</p><p> Something that almost sinks this as a possible meetup in a box is you may not have anyone willing to undergo Crocker&#39;s Rules. You do not want to conscript someone who can&#39;t operate with them, because that is going to be pretty unpleasant for them and it&#39;ll probably be worse the more outnumbered they are. (One person being what-you-perceive-as-rude to you is bad. A room full of people doing that is worse, and not in a way that scales linearly or predictably.) If you are a person who is comfortable, this meetup is more feasible.</p><p> Especially taking the guide role, I think it would be useful to switch modes a few times when working with someone. Get your interlocutor used to Crocker&#39;s Rules, not as how they always interact with you and your personal quirks, but as a kind of dialect they can code switch into and out of.</p><p> Be on the lookout for people thinking that because they are wearing the Crocker&#39;s Rules signal, they can be rude and direct to others. They are just wrong in point of fact, and also they are the last people who can complain about you telling them they&#39;re wrong in point of fact.</p><p> I do actually think practicing being nice to people at the expense of efficient exchange of information is a skill people could benefit from practicing. I&#39;m not arguing here that&#39;s an important and central rationality skill. I am arguing that it&#39;s useful to do side by side with Crocker&#39;s Rules, since the contrast can be informative. Also, optimizing for information is different than optimizing for hurting someone&#39;s feelings, and while calling someone a moron is explicitly allowed under Crocker&#39;s Rules, &quot;Hey you moron you forgot to take your shoes off in the house&quot; is not the most efficient way to send that information. That would be &quot;You forgot to take your shoes off in the house&quot; unless the insult is somehow helpful in a way I usually don&#39;t think it is.</p><p> Someone sure is going to try to don the mantle of Crocker&#39;s Rules when the aren&#39;t ready and get emotionally bruised. I don&#39;t know how to avoid that. Suggestions solicited for ideas on boxable encouragement and support for this workout.</p><p> <strong>Credits</strong> : Crocker&#39;s Rules come from Daniel Lee Crocker. The written form of them I first encountered was Yudkowsky referencing the SL4 post. The idea of having a clear signal to opt in to Crocker&#39;s Rules (A particular sticker you could put on your nametag)  is one I personally got from the LessWrong Community Weekend in 2022. The LessWrong user who acted as a sounding board over lunch is welcome to be credited if they want to be, or may wish to avoid association with this catastrophe waiting to happen. (Edit: It&#39;s Czynski, they claimed credit. I&#39;m playing up the catastrophe potential a little bit for humour value, but in its present form I do guess this has a higher chance of causing interpersonal hard feelings than I usually aim at for things I suggest to people.)</p><br/><br/><a href="https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest<guid ispermalink="false"> BcCeyL89cKSqcjtL5</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 10 Nov 2023 06:14:27 GMT</pubDate> </item><item><title><![CDATA[AI Timelines]]></title><description><![CDATA[Published on November 10, 2023 5:28 AM GMT<br/><br/><h1> Introduction</h1><p> How many years will pass before transformative AI is built? Three people who have thought about this question a lot are Ajeya Cotra from <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> , Daniel Kokotajlo from <a href="https://openai.com/">OpenAI</a> and Ege Erdil from <a href="https://epochai.org/">Epoch</a> . Despite each spending hundreds of hours investigating this question, they still still disagree substantially about the relevant timescales. For instance, here are their median timelines for one operationalization of transformative AI: </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><thead><tr><th style="border:0px solid hsl(0, 0%, 100%);text-align:center" colspan="2"><p> <strong>Median Estimate for when 99% of currently</strong></p><p> <strong>fully remote jobs will be automatable</strong></p></th></tr></thead><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">丹尼尔</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">4 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ajeya</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 13 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ege</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 40 years</td></tr></tbody></table></figure><p> You can see the strength of their disagreements in the graphs below, where they give very different probability distributions over two questions relating to AGI development. </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year would AI systems be</strong><br> <strong>able to replace 99% of current fully remote jobs?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kzksnas7qpbb02twb573" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/x5ooeyjjkbmr0zewb6r5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jbrfgwzbc0iq6amz3iet 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qujr8mhztjp8ugqsvbhy 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sjlqwtgqkfymaaaqecy7 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dipfjc1gbqieubrm0cpo 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/suujprewmfj7uevwcz3f 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fpqna0v3ingftmmzf47b 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vic2xwlvevudbc1lcfpe 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/zwzqdp6ocv4tkotvwonc 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dhw6gxwmgdzp99ptolns 5497w"></figure><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year will the energy consumption of</strong><br> <strong>humanity or its descendants be 1000x greater than now?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dgluluai77tpabbjl6vp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gghyhdlpqvkftxbbkxwu 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/m6vn1o3xguil21rzm89p 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jgy5taicz50nnbiwsg4s 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dqg6vfx5jzbkbyrvzvz6 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/t9c00topcmmoqrrowxz6 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qkcozi5f3u6swey2rccx 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/ozvbxotplbmhxixrfh4t 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gpwhggifjrcsbdkzzc9h 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/xwrgkbhhtxei1aqs23ja 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wqq4xog2wv7d7gyvoegj 5603w"><figcaption> Median indicated by small dotted line. Note that Ege&#39;s median is outside of the bounds at 2177</figcaption></figure><p> So I invited them to have a conversation about where their disagreements lie, sitting down for 3 hours to have a written dialogue. You can read the discussion below, which I personally found quite valuable.</p><p> The dialogue is roughly split in two, with the first part focusing on disagreements between Ajeya and Daniel, and the second part focusing on disagreements between Daniel/Ajeya and Ege.</p><p> I&#39;ll summarize the discussion here, but you can also jump straight in.</p><h1> Summary of the Dialogue</h1><h2> Some Background on their Models</h2><p> Ajeya and Daniel are using a compute-centric model for their AI forecasts, illustrated by Ajeya&#39;s <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">draft AI Timelines report</a> , and <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> where the question of &quot;when transformative AI&quot; gets reduced to &quot;how much compute is necessary to get AGI and when will we have that much compute? (modeling algorithmic advances as reductions in necessary compute)&quot;.</p><p> Whereas Ege thinks such models should have a lot of weight in our forecasts, but that they likely miss important considerations and doesn&#39;t have enough evidence to justify the extraordinary predictions it makes.</p><h2> Habryka&#39;s Overview of Ajeya &amp; Daniel discussion</h2><ul><li> Ajeya thinks translating AI capabilities into commercial applications has gone slower than expected (&quot; <i>it seems like 2023 brought the level of cool products I was naively picturing in 2021</i> &quot;) and similarly thinks there will be a lot of kinks to figure out before AI systems can substantially accelerate AI development.</li><li> Daniel agrees that impactful commercial applications have been slower than expected, but also thinks that the parts that made that slow can be automated substantially, and that a lot of the complexity comes from shipping something that can be useful to general consumers, and that for applications internal to the company, these capabilities can be unlocked faster.</li><li> Compute overhangs also play a big role in the differences between Ajeya and Daniel&#39;s timelines. There is currently substantial room to scale up AI by just spending more money on readily available compute. However, within a few years, increasing the amount of training compute further will require accelerating the semiconductor supply chain, which probably can&#39;t be easily achieved by just spending more money. This creates a &quot;compute overhang&quot; that accelerates AI progress substantially in the short run. Daniel thinks it&#39;s more likely than not that we will get transformative AI before this compute overhang is exhausted. Ajeya thinks that is plausible, but overall it&#39;s more likely to happen after, which broadens her timelines quite a bit.</li></ul><p> These disagreements probably explain some but not most of the differences in the timelines for Daniel and Ajeya.</p><h2> Habryka&#39;s Overview of Ege &amp; Ajeya/Daniel Discussion</h2><ul><li> Ege thinks that Daniel&#39;s forecast leaves very little room for Hoftstadter&#39;s law (&quot;It always takes longer than you expect, even when you take into account Hofstadter&#39;s Law&quot;), and in-general that there will be a bunch of unexpected things that go wrong on the path to transformative AI</li><li> Daniel thinks that Hofstadter&#39;s law is inappropriate for trend extrapolation. Ie it doesn&#39;t make sense to look at Moore&#39;s law and be like &quot;ah, and because of planning fallacy the slope of this graph from today is half of what it was previously&quot;</li><li> Both Ege and Ajeya don&#39;t expect a large increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</li><li> Ege expects that AI will have a large effect on the economy, but has substantial probability on persistent deficiencies that prevent AI from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</li></ul><p> Overall, whether AI will get substantially better at transfer learning (eg seeing an AI be trained on one genre of video game and then very quickly learn to play another genre of video game) would update all participants substantially towards shorter timelines.</p><p> We ended the dialogue with Ajeya, Daniel and Ege by putting numbers on how much various AGI milestones would cause them to update their timelines (with the concrete milestones proposed by Daniel). Time constraints made it hard to go into as much depth as we would have liked, but me and Daniel are excited about fleshing more concrete scenarios of how AGI could play out and then collecting more data on how people would update in such scenarios.</p><h1> The Dialogue </h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​Daniel, Ajeya, and Ege all seem to disagree quite substantially on the question of &quot;how soon is AI going to be a really big deal?&quot;. So today we set aside a few hours to try to dig into that disagreement, and what the most likely cruxes between your perspectives might be.</p><p> To keep things grounded and to make sure we don&#39;t misunderstand each other, we will be starting with two reasonably well-operationalized questions:</p><ol><li> In what year would AI systems be able to replace 99% of current fully remote jobs? (With operationalization stolen from an <a href="https://docs.google.com/presentation/d/1Rjnyl-jeaCTzmul9L-7A2gYJXUh9srcg6V0ONyPGft4/edit#slide=id.p">AI forecasting slide deck that Ajeya shared</a> )</li><li> In what year will the energy consumption of humanity or its descendants be 1000x greater than now?</li></ol><p> These are of course both very far from a perfect operationalization of AI risk (and I think for most people both of these questions are farther off than their answer to &quot;how long are your timelines?&quot;), but my guess is it will be good enough to elicit most of the differences in y&#39;all&#39;s models and make it clear that there is indeed disagreement.</p></div></section><h2> Visual probability distributions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​To start us off, here are two graphs of y&#39;all&#39;s probability distributions: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gkorp85zmipz4s64yh4y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sznehrdhsfht3euhrb4l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/tye1xevxurl2qvlj9teo 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/s3sycw4tqvv7yrzyw68d 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/frh1ttp4jevhfnq19eso 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eeapu1uduw5pjse4c7fu 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/i0jmvrazsil7ev8ccyz0 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kjj4ajpma4ngobdgwdux 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qln2f1q51g7jpho2itta 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/w5u4zpkq9uyapwcowdz5 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qfi9sszguck2onei1oy4 5497w"><figcaption>​ <strong>When will 99% of fully remote jobs be automated?</strong> </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vabqfpmnwrcb5vht3qa9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/r2yrjysrwy9xxgap2zdb 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wuslgsdo6stnca2zr9bl 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yyfzgooeeqkhtwlcarlt 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/d0yycia21vyuzwxsqqaz 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yr3kqedmtu9uukhlbcgt 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/z2odotlzjnkauqsx502j 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wwgg2tfavqddqlt6qrvs 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fgq6uxeosgb2tazlwsyr 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eh9qr7htcy7mwpzzbqg7 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wtpir1tpd3zqijoqg2ys 5603w"><figcaption> <strong>When will we consume 1000x energy?</strong></figcaption></figure></div></section><h2> Opening statements </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, so let&#39;s get started:</p><p> <strong>What is your guess about which belief of yours the other two most disagree with</strong> , <strong>that might explain some of the divergence in your forecasts?</strong></p></div></section><h3><strong>丹尼尔</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:27:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:27:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I don&#39;t understand Ege&#39;s views very well at all yet, so I don&#39;t have much to say there. By contrast I have a lot to say about where I disagree with Ajeya. In brief: My training compute requirements distribution is centered a few OOMs lower than Ajeya&#39;s is.为什么？ For many reasons, but (a) I am much less enthusiastic about the comparison to the human brain than she is (or than I used to be!) and (b) I am less enthusiastic about the <a href="https://www.lesswrong.com/posts/BGtjG6PzzmPngCgW9/revisiting-the-horizon-length-hypothesis">horizon length hypothesis</a> / I think that large amounts of training on short-horizon tasks combined with small amounts of training on long-horizon tasks will work (after a few years of tinkering maybe). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:41:47 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:41:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just to clarify, it sounds like you approximately agree with a compute-focused approach of AI forecasting? As in, the key variable to forecast is how much compute is necessary to get AGI, with maybe some adjustment for algorithmic progress, but not a ton?</p><p> How do things like &quot;AIs get good at different horizon lengths&quot; play into this (which you were also mentioning as one of the potential domains of disagreement)?</p><p> <i>(For readers: The horizon length hypothesis is that the longer the the feedback loops for a task are, the harder it is for an AI to get good at this task.</i></p><p> <i>Balancing a broom on one end has feedback loops of less than a second. The task of &quot;running a company&quot; has month-to-year long feedback loops. The hypothesis is that we need much more compute to get AIs that are better at the second than the first. See also</i> <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi#The_t_AGI_framework"><i>Richard Ngo&#39;s t-AGI framework</i></a> <i>which posits that the domain in which AI is generally intelligence will gradually expand from short time horizons to long time horizons.)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:44:36 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:44:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yep I think Ajeya&#39;s model (especially the version of it expanded by Davidson &amp; Epoch) is our best current model of AGI timelines and takeoff speeds. I have lots to critique about it but it&#39;s basically my starting point. And I am qualified to say this, so to speak, because I actually did consider about a half-dozen different models back in 2020 when I was first starting to research the topic and form my own independent impressions, and the more I thought about it the more I thought the other models were worse. Examples of other models: <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">Polling AI scientists</a> , <a href="https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines">extrapolating gross world product (GWP) a la Roodman</a> , deferring to <a href="https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi">what the stock markets imply</a> , <a href="https://www.lesswrong.com/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied">Hanson&#39;s weird fractional progress model thingy</a> , the <a href="https://epochai.org/blog/grokking-semi-informative-priors">semi-informative</a> prior <a href="https://aipriors.com/">models...</a><a href="https://epochai.org/blog/grokking-semi-informative-priors"> </a>I still put some weight on those other models but not much.<br><br> As for the horizon lengths question: This feeds into the training compute requirements variable. IIRC Ajeya&#39;s original model had different buckets for short, medium, and long-horizon, where eg medium-horizon bucket meant roughly &quot;Yeah we&#39;ll be doing a combo of short horizon and long horizon training, but on average it&#39;ll be medium-horizon training, such that the compute costs will be eg [inference FLOPs]*[many trillions of datapoints, as per scaling laws applied to bigger-than-human-brain-models]*[4-6 OOMs of seconds of subjective experience per datapoint on average]<br><br> So Ajeya had most of her mass on the medium and long horizon buckets, whereas I was much more bullish that the bulk of the training could be short horizon with just a &quot;cherry on top&quot; of long-horizon. Quantitatively I was thinking something like &quot;Say you have 100T datapoints of next-moment-prediction as part of your short-horizon pre-training. I claim that you can probably get away with merely 100M datapoints of million-second-long tasks, or less.&quot;<br><br> For some intuitions why I think this, it may help to read <a href="https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute">this post</a> and/or <a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi?commentId=Bs9sKmzhNvSPAs3yY">this comment thread.</a></p></div></section><h3> <strong>Ege</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:35:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:35:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think my specific disagreements with Ajeya and Daniel might be a little different, but an important meta-level point is my general skepticism of arguments that imply wild conclusions. This becomes especially relevant with predictions of a 3 OOM increase in our energy consumption in the next 10 or 20 years. It&#39;s possible to tell a compelling story about why that might happen, but also possible to do the opposite, and judging how convincing those arguments should be is difficult for me. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:38:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:38:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> OK, in response to Ege, I guess we disagree about this &quot;that-conclusion-is-wild-therefore-unlikely&quot; factor. I think for things like this it&#39;s a pretty poor guide to truth relative to other techniques (eg models, debates between people with different views, model-based debates between people with different views) I&#39;m not sure how to make progress on resolving this crux. Ege, you say it&#39;s mostly in play for the 1000x energy consumption thing; wanna focus on discussing the other question instead? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:40:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:40:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Sure, discussing the other question first is fine.<br><br> I&#39;m not sure why you think heuristics like &quot;I don&#39;t update as much on specific arguments because I&#39;m skeptical of my ability to do so&quot; are ineffective, though. For example, this seems like it goes against the fractional Kelly betting heuristic from <a href="https://www.lesswrong.com/posts/TNWnK9g2EeRnQA8Dg/never-go-full-kelly">this post</a> , which I would endorse in general: you want to defer to the market to some extent because your model has a good chance of being wrong.<br><br> I don&#39;t know if it&#39;s worth going down this tangent right now, though, so it&#39;s probably more productive to focus on the first question for now.<br><br> I think my wider distribution on the first question is also affected by the same high-level heuristic, though to a lesser extent. In some sense, if I were to fully condition on the kind of compute-based model that you and Ajeya seem to have about how AI is likely to develop, I would probably come up with a probability distribution for the first question that more or less agrees with Ajeya&#39;s. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:48:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:48:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> That&#39;s interesting. I think digging into that seems good to me.</p><p> Can you say a bit more about how you are thinking about it at a high level? My guess is you have a bunch of broad heuristics, some of which are kind of like &quot;well, the market doesn&#39;t seem to think AGI is happening soon?&quot;, and then those broaden your probability mass, but I don&#39;t know whether that&#39;s a decent characterization, and would be interested in knowing more of the heuristics that drive that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:53:33 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:53:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I&#39;m not sure I would put that much weight on the market not thinking it&#39;s happening soon, because I think it&#39;s actually fairly difficult to tell what market prices would look like if the market <i>did</i> think it was happening soon.<br><br> Setting aside the point about the market and elaborating on the rest of my views: I would give a 50% chance that in 30 years, I will look back on something like <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> and say &quot;this model captured all or most of the relevant considerations in predicting how AI development was likely to proceed&quot;. For me, that&#39;s already a fairly high credence to have in a specific class of models in such an uncertain domain.<br><br> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.<br><br> If I thought the takeoff model from Tom Davidson (and some newer versions that I&#39;ve been working on personally) were basically right, my forecasts would just look pretty similar to the forecasts of that model, and based on my experience with playing around in these models and the parameter ranges I would consider plausible, I think I would just end up agreeing with Ajeya on the first question.<br><br> Does that explanation make my view somewhat clearer? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:57:12 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:57:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> This part really helps, I think.</p><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:00:13 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:00:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;.</p></blockquote><p> That&#39;s basically right, though I would add the caveat that entropy is relative so it doesn&#39;t really make sense to have a &quot;more uncertain distribution&quot; over when AGI will arrive. You have to somehow pick some typical timescale over which you expect that to happen, and I&#39;m saying that once scaling is out of the equation I would default to longer timescales that would make sense to have for a technology that we think is possible but that we have no concrete plans for achieving on some reasonable timetable. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:03:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:03:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds. It&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now. In some sense, &quot;we come up with a brilliant insight to do this way more efficiently than nature even when we have very little compute&quot; is a hypothesis that should have had &lt;50% weight a priori, compared to &quot;capabilities will start getting good when we&#39;re talking about macroscopic amounts of compute.&quot;</p><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></div></section><h3> <strong>Ajeya</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 17:43:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 17:43:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> On Daniel&#39;s opening points: I think I actually just agree with both a) and b) right now — or rather, I agree that the right question to ask about the training compute requirement is something more along the lines of &quot;How many GPT-N to GPT-N.5 jumps do we think it would take?&quot;, and that short horizon LLMs plus tinkering looks more like &quot;the default&quot; than like &quot;one of a few possibilities,&quot; where other possibilities included a more intense meta-learning step (which is how it felt in 2019-20). The latter was <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines#Picturing_a_more_specific_and_somewhat_lower_bar_for_TAI">the biggest adjustment</a> in my updated timelines.<br><br> That said though, I think two important object-level points push the &quot;needed model size&quot; and &quot;needed amount of tinkering&quot; higher in my mind than it is for Daniel:</p><ul><li> In-context learning does seem pretty bad, and doesn&#39;t seem to be improving a huge amount. I think we can have TAI without really strong human-like in-context learning, but it probably requires more faff than if we had that out of the gate.</li><li> Relatedly, adversarial robustness seems not-great right now.  This also feels overcomable, but I think it increases the scale that you need (by analogy, like 5-10 years ago it seemed like vision systems were good enough for cars except in the long tail / in adversarial settings, and I think vision systems had to get a fair amount bigger, plus there had to be a lot more tinkering on the cars, to get to now where they&#39;re starting to be viable).</li></ul><p> And then a meta-level point is that I (and IIRC Metaculus, according to my colleague Isabel) have been kind of surprised for the last few years about the lack of cool products built on LLMs (it seems like 2023 brought the level of cool products I was naively picturing in 2021). I think there&#39;s a &quot;reality has a lot of detail, actually making stuff work is a huge pain&quot; dynamic going on, and it lends credence to the &quot;things will probably be fairly continuous&quot; heuristic that I already had.</p><p> A few other meta-points:</p><ul><li> The Paul <a href="https://sideways-view.com/2023/07/29/self-driving-car-bets/">self-driving car bets</a> post was interesting to me, and I place some weight on &quot;Daniel is doing the kind of &#39;I can see how it would be done so it&#39;s only a few years away&#39; move that I think doesn&#39;t serve as a great guide to what will happen in the real world.&quot;</li><li> Carl is the person who seems like he&#39;s been the most right when we&#39;ve disagreed, so he&#39;s probably the one guy whose views I put the most weight on. But also Carl seems like he errs aggressive and errs in the direction of believing people will aggressively pursue the most optimal thing (being more surprised than I was, for a longer period of time, about how people haven&#39;t invested more in AI and accomplished more with it by now). His timelines are longer than Daniel&#39;s, and I think mine are a bit longer than his.</li><li> In general, I do count Daniel as among a pretty small set of people who were clearly on record with views more correct than mine in 2020 about both the nature of how TAI would be built (LLMs+tinkering) and how quickly things would progress. Although it&#39;s a bit complicated because 2020-me thought we&#39;d be seeing more powerful LLM products by now.</li><li> Other people who I think were more right include Carl, Jared Kaplan, Danny Hernandez, Dario, Holden, and Paul. Paul is interesting because I think he both put more weight than I did on &quot;it&#39;s just LLMs plus a lot of decomposition and tinkering&quot; but also puts more weight than either me or Daniel on &quot;things are just hard and annoying and take a long time;&quot; this left him with timelines similar to mine in 2020, and maybe a bit longer than mine now.</li><li> Oh — another point that seems interesting to discuss at some point is that I suspect Daniel generally wants to focus on a weaker endpoint because of some sociological views I disagree with. (Screened off by the fact that we were answering the same question about remotable jobs replacement, but I think hard to totally screen off.)</li></ul></div></section><h2> On in-context learning as a potential crux </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:04:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:04:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya:</p><ul><li> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</li><li> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</li><li> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</li><li> re: schlep &amp; incompetence on the part of the AGI industry: Yep, you are right about this, and I was wrong. Your description of Carl also applies to me historically; in the past three years I&#39;ve definitely been a &quot;this is the fastest way to AGI, therefore at least one of the labs will do it with gusto&quot; kind of guy, and now I see that is wrong. I think basically I fell for the planning fallacy &amp; efficient market hypothesis fallacy.<br><br> However, I don&#39;t think this is the main crux between us, because it basically pushes things back by a few years, it doesn&#39;t eg double (on a log scale) the training requirements. My current, updated model of timelines, therefore, is that the bottleneck in the next five years is not necessarily compute but instead quite plausibly schlep &amp; conviction on the part of the labs. This is tbh a bit of a scary conclusion. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:21:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:21:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</p></blockquote><p> The traditional image of AGI involves having an AI system that can <i>learn new (to it) skills</i> as efficiently as humans (with as few examples as humans would need to see). I think this is not how the first transformative AI system will look, because ML is less sample efficient than humans and it doesn&#39;t look like in-context learning is on track to being able to do the kind of fast sample-efficient learning that humans do. I think this is not fatal for getting TAI, because we can make up for it by a) the fact that LLMs&#39; &quot;ancestral memory&quot; contains all sorts of useful information about human disciplines that they won&#39;t need to learn in-context, and b) explicitly guiding the LLM agent to &quot;reason out loud&quot; about what lessons it should take away from its observations and putting those in an external memory it retrieves from, or similar.</p><p> I think back when Eliezer was saying that &quot;stack more layers&quot; wouldn&#39;t get us to AGI, this is one of the kinds of things he was pointing to: that cognitively, these systems didn&#39;t have the kind of learning/reflecting flexibility that you&#39;d think of re AGI. When people were talking about GPT-3&#39;s in-context learning, I thought that was one of the weakest claims by far about its impressiveness. The in-context learning at the time was like: you give it a couple of examples of translating English to French, and then you give it an English sentence, and it dutifully translate that into French. It already knew English and it already knew French (from its ancestral memory), and the thing it &quot;learned&quot; was that the game it was currently playing was to translate from English to French.</p><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible). But compared to any object-level skill like coding (many of which are superhuman), in-context learning seems quite subhuman. I think this is related to how ARC Evals&#39; LLM agents kind of &quot;fell over&quot; doing things like setting up Bitcoin wallets.</p><p> Like Eliezer often says, humans evolved to hunt antelope on the savannah, and that very same genetics coding for the very same brain can build rockets and run companies. Our LLMs right now generalize further from their training distribution than skeptics in 2020 would have said, and they&#39;re generalizing further and further as they get bigger, but they have nothing like the kind of savannah-to-boardroom generalization we have. This can create lots of little issues in lots of places when an LLM will need to digest some new-to-it development and do something intelligent with it. Importantly, I don&#39;t think this is going to stop LLM-agent-based TAI from happening, but it&#39;s one concrete limitation that pushes me to thinking we&#39;ll need more scale or more schlep than it looks like we&#39;ll need before taking this into account.</p><p> Adversarial robustness, which I&#39;ll reply to in another comment, is similar: a concrete hindrance that isn&#39;t fatal but is one reason I think we&#39;ll need more scale and schlep than it seems like Daniel does (despite agreeing with his concrete counterarguments of the form &quot;we can handle it through X countermeasure&quot;). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:34:30 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:34:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya: Thanks for that lengthy reply. I think I&#39;ll have to ponder it for a bit. Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:15:08 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:15:08 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds.</p></blockquote><p> I think it&#39;s fine to incorporate these uncertainties as a wider prior over the training compute requirements, and I also agree it&#39;s a reason to put more weight on this broad class of models than you otherwise would, but I still don&#39;t find these reasons compelling enough to go significantly above 50%. It just seems pretty plausible to me that we&#39;re missing something important, even if any specific thing we can name is unlikely to be what we&#39;re missing.<br><br> To give one example, I initially thought that the evolution anchor from the Bio Anchors report looked quite solid as an upper bound, but I realized some time after that it doesn&#39;t actually have an appropriate anthropic correction and this could potentially mess things up. I now think if you work out the details this correction turns out to be inconsequential, but it didn&#39;t have to be like that: this is just a consideration that I missed when I first considered the argument. I suppose I would say I don&#39;t see a reason to trust my own reasoning abilities as much as you two seem to trust yours.</p><blockquote><p> The compute scaling hypothesis is much broader, and it&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now.</p></blockquote><p> My impression is that Moravec predicted in 1988 that we would have AI systems comparable to the human brain in performance around 2010. If this actually happens around 2037 (your median timelines), Moravec&#39;s forecast will have been off by around a factor of 2 in terms of the time differential from when he made the forecast. That doesn&#39;t seem &quot;super good&quot; to me.<br><br> Maybe I&#39;m wrong about exactly what Moravec predicted - I didn&#39;t read his book and my knowledge is second-hand. In any event, I would appreciate getting some more detail from you about why you think he looks good.</p><blockquote><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></blockquote><p> I agree that if I were considering two models at those extremes, recent developments would update me more toward the former model. However, I don&#39;t actually disagree with the abstract claim that &quot;things will get more and more interesting the more compute we have access to&quot; - I expect more compute to make things more interesting even in worlds where we can&#39;t get to AGI by scaling compute. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:31:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:31:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible).</p></blockquote><p> A local remark about this: I&#39;ve seen a bunch of reports from other people that GPT-4 is essentially unable to play tic-tac-toe, and this is a shortcoming that was highly surprising to me. Given the amount of impressive things it can otherwise do, failing at playing a simple game whose full solution could well be in its training set is really odd.<br><br> So while I agree 4 seems better than 3, it still has some bizarre weaknesses that I don&#39;t think I understand well. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:34:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:34:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ege: Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily? My sense is that this is just one of these situations where tokenization and one-dimensionality of text makes something hard, but it&#39;s trivial to get the system to learn it if it&#39;s in a more natural representation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:35:17 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:35:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily?</p></blockquote><p><br> <a href="https://twitter.com/liminal_warmth/status/1709654529413992692">This random Twitter person</a> says that it can&#39;t. Disclaimer: haven&#39;t actually checked for myself.</p></div></section><h2> Taking into account government slowdown </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:05:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:05:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:06:58 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:06:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I am guilty of assuming governments and corporations won&#39;t slow things down by more than a year. I think I mostly still endorse this assumption but I&#39;m hopeful that instead they&#39;ll slow things down by several years or more. Historically I&#39;ve been arguing with people who disagreed with me on timelines by decades, not years, so it didn&#39;t seem important to investigate this assumption. That said I&#39;m happy to say why I still mostly stand by it. Especially if it turns out to be an important crux (eg if Ege or Ajeya think that AGI would probably happen by 2030 absent slowdown) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:08:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:08:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> That said I&#39;m happy to say why I still mostly stand by it.</p></blockquote><p> Cool, might be worth investigating later if it turns out to be a crux. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:21:02 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:21:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines.</p></blockquote><p> Responding to habryka: I do think government regulations, companies slowing down because of risks, companies slowing down because they are bad at coordination, capital markets being unable to allocate the large amounts of capital needed for huge training runs for various reasons, etc. could all be important. However, my general heuristic for thinking about the issue is more &quot;there could be a lot of factors I&#39;m missing&quot; and less &quot;I think these specific factors are going to be very important&quot;.<br><br> In terms of the impact of capable AI systems, I would give significantly less than even but still non-negligible odds that these kinds of factors end up limiting the acceleration in economic growth to eg less than an order of magnitude. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:49:39 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:49:39 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p></blockquote><p> I include this in a long tail of &quot;things are just slow&quot; considerations, although in my mind it&#39;s mostly not people making a concerted effort to slow down because of x-risk, but rather just the thing that happens to any sufficiently important technology that has a lot of attention on it: a lot of drags due to the increasing number of stakeholders, both drags where companies are less blase about releasing products because of PR concerns, and drags where governments impose regulations (which I think they would have in any world, with or without the efforts of x-risk-concerned contingent). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:10:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:10:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Slight meta: I am interested in digging in a bit more to find some possible cruxes between Daniel and Ajeya, before going more in-depth between Ajeya and Ege, just to keep the discussion a bit more focused.</p></div></section><h2> Recursive self-improvement and AI&#39;s speeding up R&amp;D </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:21:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:21:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just for my own understanding, you have adjusted the compute-model to account for some amount of R&amp;D speedup as a result of having more AI researchers.</p><p> To what degree does that cover classical recursive self-improvements or things in that space? (Eg AI systems directly modifying their training process or weights or develop their own pre-processing modules?)</p><p> Or do you expect a feedback loop that&#39;s more &quot;AI systems do research that routes through humans understanding those insights and being in the loop on implementing them to improve the AI systems&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:25:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:25:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> When all we had was Ajeya&#39;s model, I had to make my own scrappy guess at how to adjust it to account for R&amp;D acceleration due to pre-AGI systems. Now we have Davidson&#39;s model so I mostly go with that.<br><br> It covers recursive-self-improvement as a special case. I expect that to be what the later, steeper part of the curve looks like (basically a million AutoGPTs running in parallel across several datacenters, doing AI research but 10-100x faster than humans would, with humans watching the whole thing from the sidelines clapping as metrics go up); the earlier part of the curve looks more like &quot;every AGI lab researcher has access to a team of virtual engineers that work at 10x speed and sometimes make dumb mistakes&quot; and then the earliest part of the curve is what we are seeing now with copilot and chatgpt helping engineers move slightly faster. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:37:36 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:37:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</p></blockquote><p> These are entangled updates. If you&#39;re focusing on just &quot;how can you accelerate ML R&amp;D a bunch,&quot; then it seems less important to be able to handle low-feedback-loop environments quite different from the training environment. By far the biggest reason I thought we might need longer horizon training was to imbue the skill of efficiently learning very new things (see <a href="https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.2s3orj7g2t76">here</a> ). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:38:01 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:38:01 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively.</p></blockquote><p> I think this is basically right! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:39:50 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:39:50 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</p></blockquote><p> I&#39;ll give a less lengthy reply here, since structurally it&#39;s very similar to in-context learning, and has the same &quot;agree-qualitatively-but-not-quantitatively&quot; flavor. (For example, I definitely agree that the game is going to be coping with errors and error-correction, not never making errors; we&#39;re talking about whether that will take four years or more than four years.)</p><p> &quot;Not behaving erratically / falling over on super weird or adversarial inputs&quot; is a higher-level-of-abstraction cognitive skill humans are way better at than LLMs. LLMs are improving at this skill with scale (like all skills), and there are ways to address it with schlep and workflow rearrangements (like all problems), and it&#39;s unclear how important it is in the first place. But it&#39;s plausibly fairly important, and it seems like their current level is &quot;not amazing,&quot; and the trend is super unclear but not obviously going to make it in four years.</p><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, market-efficiency-and-schlep) pushes you toward &quot;more than four years from now&quot; — there&#39;s little room for it to push in the other direction. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:42:42 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:42:42 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, pushes you toward &quot;more than four years from now&quot;</p></blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:45:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:45:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about).</p></blockquote><p> I don&#39;t think it&#39;s insane to believe this to be true of 20 years, but I think we have many more examples of big, difficult, society-wide things happening over 20 years than over 4. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:45:40 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:45:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote-working OpenAI engineer that thinks at 10x serial speed. Even if the research scientists are still human, this would speed things up a lot I think. So while I find the abstract arguments about how LLMs are worse at in-context learning etc. than humans plausible, when I think concretely about AI R&amp;D acceleration it still seems like it&#39;s gonna start happening pretty soon, and that makes me also update against the abstract argument a bit. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So, I kind of want to check an assumption. On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p><p> Daniel, could you be more concrete about how a 10x AI engineer actually helps develop AGI? My guess is on a 4-year timescale you don&#39;t expect it to route through semiconductor supply chain improvements.</p><p> And then I want to check what Ajeya thinks here and whether something in this space might be a bit of a crux. My model of Ajeya does indeed think that AI systems in the next few years will be impressive, but not really actually that useful for making AI R&amp;D go better, or at least not like orders of magnitude better. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:48:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:48:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> My best guess is that algorithmic progress has probably continued at a rate of around a doubling of effective compute per year, at least insofar as you buy that one-dimensional model of algorithmic progress. Again, model uncertainty is a significant part of my overall view about this, but I think it&#39;s not accurate to say there hasn&#39;t been much algorithmic progress in the last few years. It&#39;s just significantly slower than the pace at which we&#39;re scaling up compute so it looks relatively less impressive.</p><p> <i>(Daniel, Ajeya +1 this comment)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I was modeling one doubling a year as approximately not very much, compared to all the other dynamics involved, though of course it matters a bunch over the long run. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:56:42 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:56:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Habryka&#39;s excellent point about how maybe engineering isn&#39;t the bottleneck, maybe compute is instead:<br><br> My impression is that roughly half the progress has come from increased compute and the other half from better algorithms. Going forward when I think concretely about the various limitations of current algorithms and pathways to overcome them -- which I am hesitant to go into detail about -- it sure does seem like there are still plenty of low and medium-hanging fruit to pick, and then high-hanging fruit beyond which would take decades for human scientists to get to but which can perhaps be reached much faster during an AI takeoff.<br><br> I am on a capabilities team at OpenAI right now and I think that we could be going something like 10x faster if we had the remote engineer thing I mentioned earlier. And I think this would probably apply across most of OpenAI research. This wouldn&#39;t accelerate our compute acquisition much at all to be clear, but that won&#39;t stop a software singularity from happening. Davidson model backs this up I think -- I&#39;d guess that if you magically change it to keep hardware &amp; compute progress constant, you still get a rapid R&amp;D acceleration, just a somewhat slower one.<br><br> I&#39;d think differently if I thought that parameter count was just Too Damn Low, like I used to think. If I was more excited about the human brain size comparison, I might think that nothing short of 100T parameters (trained according to Chinchilla also) could be AGI, and therefore that even if we had a remote engineer thinking at 10x speed we&#39;d just eat up the low-hanging fruit and then stall while we waited for bigger computers to come online. But I don&#39;t think that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:56:54 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:56:54 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> I think there haven&#39;t been flashy paradigm-shifting insights, but I strongly suspect each half-GPT was a hard-won effort on a lot of fronts, including both a lot of mundane architecture improvements (like implementing long contexts in less naive ways that don&#39;t incur quadratic cost), a lot of engineering to do the model parallelism and other BS that is required to train bigger models without taking huge GPU utilization hits, and a lot of post-training improvements to make usable nice products. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:58:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:58:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p><p> The context length and GPU utilization thing feels most relevant. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:59:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:59:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p></blockquote><p> Yeah, TBC, I think there&#39;s a higher bar than Daniel thinks there is to speeding stuff up 10x for reasons like this. I do think that there&#39;s algorithm juice, like Daniel says, but I don&#39;t think that a system you look at and naively think &quot;wow this is basically doing OAI ML engineer-like things&quot; will actually lead to a full 10x speedup; 10x is a lot.</p><p> I think you will eventually get the 10x, and then the 100x, but I&#39;m picturing that happening after some ramp-up where the first ML-engineer-like systems get integrated into workflows, improve themselves, change workflows to make better use of themselves, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:53:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:53:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach.</p></blockquote><p> Agree this is the strongest candidate for crazy impacts soon, which is why my two updates of &quot;maybe meta-learning isn&#39;t that important and therefore long horizon training isn&#39;t as plausibly necessary&quot; and &quot;maybe I should just be obsessed with forecasting when we have the ML-research-engineer-replacing system because after that point progress is very fast&quot; are heavily entangled. <i><u>(Daniel reacts &quot;+1&quot; to this)</u></i></p><blockquote><p> -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote OpenAI engineer that thinks at 10x serial speed</p></blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:02:59 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:02:59 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me?</p></blockquote><p> I disagree with this update -- I think the update should be &quot;it takes a lot of schlep and time for the kinks to be worked out and for products to find market fit&quot; rather than &quot;the systems aren&#39;t actually capable of this.&quot; Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.<br><br> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:05:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:05:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.</p></blockquote><p><br> I&#39;m skeptical we would get 2 OOMs or more with just the current capabilities of AI systems, but I think even if you accept that, scaling from $1B/yr to $100B/yr is easier than from $100B/yr to $10T/yr. Accelerating AI R&amp;D by 2x seems more like the second change to me, or even bigger than that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.)</p></blockquote><p> I agree with this </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yeah idk I pulled that out of my ass, maybe ​2 OOM is more like the upper limit given how much value there already is. I agree that going from X to 10X is easier than going from 10X to 100X, in general. I don&#39;t think that undermines my point though. I disagree with your claim that making AI progress go 2x faster is more like scaling from $100B to $10T-- I think it depends on when it happens! Right now in our state of massive overhang and low-hanging-fruit everywhere, making AI progress go 2x faster is easy.<br><br> Also to clarify when I said 10x faster I meant 10x faster algorithmic progress; compute progress won&#39;t accelerate by 10x obviously. But what this means is that I think we&#39;ll transition from a world where half or more of the progress is coming from scaling compute, to a world where most of the progress is coming from algorithmic improvements / pushing-through-schlep.</p></div></section><h2> Do we expect transformative AI pre-overhang or post-overhang? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:01:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:01:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think a hypothesis I have for a possible crux for a lot of the disagreement between Daniel and Ajeya is something like &quot;will we reach AGI before the compute overhang is over vs. after?&quot;.</p><p> Like, in as much as we think we are in a compute-overhang situation, there is an extremization that applies to people&#39;s timelines where if you we&#39;ll get there using just remaining capital and compute, you expect quite short timelines, but if you expect it will require faster chips or substantial algorithmic improvements, you expect longer, and with less probability-mass in-between.</p><p> Curious about Daniel and Ajeya answering the question of &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:05:48 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:05:48 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot;</p></blockquote><p> I think there are different extremities of compute overhang. The most extreme one which will be exhausted most quickly is like &quot;previously these companies were training AI systems on what is essentially chump change, and now we&#39;re starting to get into a world where it&#39;s real money, and soon it will be really real money.&quot; I think within 3-4 years we&#39;ll be talking tens of billions for a training run; I think the probability we get drop-in replacements for 99% remotable jobs (regardless of whether we&#39;ve rolled those drop-in replacements out everywhere) by then is something like...25%?</p><p> And then after that progress is still pretty compute-centric, but it moves slower because you&#39;re spending very real amounts of money, and you&#39;re impacting the entire supply chain: you need to build more datacenters which come with new engineering challenges, more chip-printing facilities, more fabs, more fab equipment manufacturing plans, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:08:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:08:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing.</p></blockquote><p> Cool, that makes sense. That does seem like it might exaggerate the perceived disagreements between the two of you, when you just look at the graphs, though it&#39;s of course still highly decision-relevant to dig deeper into whether this is true or not.</p></div></section><h2> Hofstadter&#39;s law in AGI forecasting </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:47 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> TBC Daniel, I think we differ by a factor of 2 on the probability for your median scenario. I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:13:44 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:13:44 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law.</p></blockquote><p> I think this also applies to the disagreement between me and Ajeya. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:16:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:16:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> A thing that would change my mind is if I found other scenarios more plausible. Wanna sketch some?<br><br> Regarding Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot; Right? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:17:53 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:17:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Re: Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot; Right?</p></blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:26:13 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:26:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update.</p></blockquote><p> Excellent. So my reason for doing the additive method is that I think Hofstadter&#39;s law / schlep / etc. is basically the planning fallacy, and it applies when your forecast is based primarily on imagining a series of steps being implemented. It does NOT apply when your forecast is based primarily on extrapolating trends. Like, you wouldn&#39;t look at a graph of exponential progress in Moore&#39;s law or solar power or whatever and then be like &quot;but to account for Hofstadter&#39;s Law I will assume things take twice as long as I expect, therefore instead of extrapolating the trend-line straight I will cut its slope by half.&quot;<br><br> And when it comes to AGI timelines, I think that the shorter-timeline scenarios look more subject to the planning fallacy, whereas the longer-timeline scenarios look more like extrapolating trends.<br><br> So in a sense I&#39;m doing the multiplicative method, but only on the shorter worlds. Like, when I say 2027 as my median, that&#39;s kinda because I can actually quite easily see it happening in 2025, but things take longer than I expect, so I double it... I&#39;m open to being convinced that I&#39;m not taking this into account enough and should shift my timelines back a few years more; however I find it very implausible that I should add eg 15 years to my median because of this.</p></div></section><h2> Summary of where we are at so far and exploring additional directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:21:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:21:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> We&#39;ve been going for a while and it might make sense to take a short step back. Let me try to summarize where we are at:</p><p> We&#39;ve been mostly focusing on the disagreement between Ajeya and Daniel. It seems like one core theme in the discussion has been the degree to which &quot;reality has a lot of detail and kinks need to be figured out before AI systems are actually useful&quot;. Ajeya currently thinks that while it is true that AGI companies will have access to these tools earlier, there still will be a lot of stuff to figure out before you actually have a system equivalent to a current OAI engineer. Daniel made a similar update in noticing a larger-than-he-expected delay in the transition from &quot;having all the stuff necessary to make a more capably system, like architecture, compute, training setup&quot; and &quot;actually producing a more capable system&quot;.</p><p> However, it&#39;s also not clear how much this actually explains the differences in the timelines for the two of you.</p><p> We briefly touched on compute overhangs being a thing that&#39;s very relevant to both of your distributions, in that Daniel assigns substantially higher probability to a very high R&amp;D speed-up before the current overhang is exhausted, which pushes his probability mass a bunch earlier. And correspondingly Ajeya&#39;s timelines are pretty sensitive to relatively small changes in compute requirements on the margin, since that would push a bunch of probability mass into the pre-overhang world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:21:30 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:21:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I&#39;ll put in a meta note here that I think it&#39;s pretty challenging to argue about a 25% vs a 50% on the Daniel scenario, that is literally one bit of evidence one of us sees that the other doesn&#39;t. It seems like Daniel thinks I need stronger arguments/evidence than I have to be at 25% instead of 50%, but it&#39;s easy to find one bit somewhere and hard to argue about whether it really is one bit.</p></div></section><h2> Exploring conversational directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> In case interested, here are some possible conversation topics/starters:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)<br><br> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.<br><br> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?<br><br> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:25:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:25:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On Daniel&#39;s four topics:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)</p></blockquote><p> I suspect I&#39;ll be like &quot;Yep, seems plausible, and my probability on it coming to pass is 2-5x smaller.&quot;</p><blockquote><p> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.</p></blockquote><p> I could do this if people thought it would be useful.</p><blockquote><p> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?</p></blockquote><p> I like this blog post but I feel like it&#39;s quite tame compared to what both Daniel and I think is plausible so not sure if it&#39;s the best thing to anchor on.</p><blockquote><p> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like.</p></blockquote><p> I can do this if people thought it would be useful.</p></div></section><h2> Ege&#39;s median world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:25:39 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:25:39 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> My median world looks something like this: we keep scaling compute until we hit training runs at a size of 1e28 to 1e30 FLOP in maybe 5 to 10 years, and after that scaling becomes increasingly difficult because of us running up against supply constraints. Software progress continues but slows down along with compute scaling. However, the overall economic impact of AI continues to grow: we have individual AI labs in 10 years that might be doing on the order of eg $30B/yr in revenue.<br><br> We also get more impressive capabilities: maybe AI systems can get gold on the IMO in five years, we get more reliable image generation, GPT-N can handle more complicated kinds of coding tasks without making mistakes, stuff like that. So in 10 years AI systems are just pretty valuable economically, but I expect the AI industry to look more like today&#39;s tech industry - valuable but not economically transformative.<br><br> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy. However, I would not be surprised by a mild acceleration of overall economic growth driven by the impact of AI. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:28:51 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:28:51 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy.</p></blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:32:48 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:32:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p></blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p><blockquote><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously?</p></blockquote><p> I think that&#39;s an important part of it, yes. I expect the systems we&#39;ll have in 10 years will be really good at some things with some bizarre failure modes and domains where they lack competence. My example of GPT-4 not being able to play tic-tac-toe is rather anecdotal, but I would worry about other things of a similar nature when we actually want these systems to replace humans throughout the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p></blockquote><p> Interestingly, I think in the case of ems this is more plausible than in the case of normal AGI. Because normal AGI will be more easily extendible to superhuman levels. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:35:19 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:35:19 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> FWIW I think the kind of AGI you and I are imagining as the most plausible first AGI is pretty janky, and the main way I see it improving stuff is by doing normal ML R&amp;D, not galaxy-brained &quot;editing its own source code by hand&quot; stuff. The normal AI R&amp;D could be done by all the ems too.</p><p> (It depends on where the AI is at when you imagine dropping ems into the scenario.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree with that. The jankiness is a point in my favor, because it means there&#39;s lots of room to grow by ironing out the kinks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Overall Ege, thanks for writing that scenario! Here are some questions / requests for elaboration:<br><br> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?</p><p> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:39:56 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:39:56 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s remote OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?<br><br> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then?</p></blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering that gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.<br><br> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.<br><br> (3): I don&#39;t think anything notable would happen. I don&#39;t believe the AI systems of 2030 will be capable enough to manage an AI lab. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:43:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:43:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I think Ege&#39;s median world is plausible, just like Daniel&#39;s median world; I think my probability on &quot;Ege world or more chill than that&quot; is lower than my probability on &quot;Daniel world or less chill than that.&quot; Earlier I said 25% on Daniel-or-crazier, I think I&#39;m at 15% on Ege-or-less-crazy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:46:08 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:46:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: the &quot;fuck it&quot; scenario: What I&#39;m interested in here is what skills you think the system would be lacking, that would make it fail. Like right now for example we had a baby version of this with ChaosGPT4, which lacked strategic judgment and also had a very high mistakes-to-ability-to-recover-from-mistakes ratio, and also started from a bad position (being constantly monitored, zero human allies). So all it did was make some hilarious tweets and get shut down. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:46:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:46:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?</p><p> Eg suppose some AI system was trained to learn new video games: each RL episode was it being shown a video game it had never seen, and it&#39;s supposed to try to play it; its reward is the score it gets. Then after training this system, you show it a whole new <i>type</i> of video game it has never seen (maybe it was trained on platformers and point-and-click adventures and visual novels, and now you show it a first-person-shooter for the first time). Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:47:16 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:47:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?<br></p></blockquote><p>是的。</p><blockquote><p> Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines?</p></blockquote><p> I would probably update substantially towards agreeing with you. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:49:01 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:49:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering which gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.</p></blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:51:56 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:51:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.</p></blockquote><p> How much do you think they&#39;ll be automating/speeding things up? Can you give an example of a coding task such that, if AIs can do that coding task by, say, 2025, you&#39;ll update significantly towards shorter timelines, on the grounds that they are by 2025 doing things you didn&#39;t expect to be doable by 2030?<br><br> (My position is that all of these deficiencies exist in current systems but (a) will rapidly diminish over the next few years and (b) aren&#39;t strong blockers to progress anyway, eg even if they don&#39;t have good research taste they can still speed things up substantially just by doing the engineering and cutting through the schlep) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:54:49 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:54:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3?</p></blockquote><p> I don&#39;t think it&#39;s going to take ~30 (really 40 per the distribution I submitted) years after 2030, that&#39;s just my median. I think there&#39;s a 1/3 chance it takes more than 75 and 1/5 chance it takes more than 175.<br><br> If you&#39;re asking me to justify why my median is around 2065, I think this is not really that easy to do as I&#39;m essentially just expressing the betting odds I would accept based on intuition.<br><br> Formalizing it is tricky, but I think I could say I don&#39;t find it that plausible the problem of building AI is so hard that we won&#39;t be able to do it even after 300 years of hardware and software progress. Just the massive scaling up of compute we could get from hardware progress and economic growth over that kind of timescale would enable things that look pretty infeasible over the next 20 or 30 years.</p></div></section><h2> Far-off-distribution transfer </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:47:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:47:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The Ege/Ajeya point about far-off-distribution transfer seem like an interesting maybe-crux, so let&#39;s go into that for a bit.</p><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:48:16 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:48:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, FWIW I think the savannah-to-boardroom transfer stuff is probably underlying past-Eliezer (not sure about current Eliezer) and also a lot of &quot;stochastic parrot&quot;-style skeptics. I think it&#39;s a good point under-discussed by the short timelines crowd, though I don&#39;t think it&#39;s decisive. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:49:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:49:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade?</p></blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no. I just think it&#39;ll take a lot of hard work to make up for the weaknesses of not having transfer this good. Paul has a good unpublished Google doc titled &quot;Doing without transfer.&quot; I think by the time systems are transformative enough to massively accelerate AI R&amp;D, they will still not be that close to savannah-to-boardroom level transfer, but it will be fine because they will be trained on exactly what we wanted them to do for us. (This btw also underlies some lower-risk-level intuitions I have relative to MIRI crowd.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:51:05 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:51:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no.</p></blockquote><p> Oh, huh, that is really surprising to me. But good to have that clarified. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:52:00 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:52:00 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, I just think the way we get our OAI-engineer-replacing-thingie is going to be radically different cognitively than human OAI-engineers, in that it will have coding instincts honed through ancestral memory the way grizzly bears have salmon-catching instincts baked into them through their ancestral memory. For example, if you give it a body, I don&#39;t think it&#39;d learn super quickly to catch antelope in the savannah, the way a baby human caveperson could learn to code if you transported them to today.</p><p> But it&#39;s salient to me that this might just leave a bunch of awkward gaps, since we&#39;re trying to make do with systems holistically less intelligent than humans, but just more specialized to coding, writing, and so on. This is why I think the Ege world is plausible.</p><p> I also dislike using the term AGI for this reason. (Or rather, I think there is a thing people have in mind by AGI which makes sense, but it will come deep into the Singularity, after the earlier transformative AI systems that are not AGI-in-this-sense.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:57:19 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:57:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I also dislike using the term AGI for this reason.</p></blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:58:52 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:58:52 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy.</p></blockquote><p> Yes, I&#39;m very sympathetic to this kind of thing, which is why I like TAI (and it&#39;s related to the fact that I think we&#39;ll first have grizzly-bears-of-coding, not generally-intelligent-beings). But it bites much less in my view because it&#39;s all much more compressed and there&#39;s a pretty shortish period of a few years where all plausible things people could mean by AGI are achieved, including the algorithm that has savannah-to-boardroom-level transfer.</p></div></section><h2> A concrete scenario &amp; where its surprises are </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:59:49 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:59:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> We can delete this hook later if no one bites, but in case someone does, here&#39;s a scenario I think it would be productive to discuss:<br><br> (1) Q1 2024: A bigger, better model than GPT-4 is released by some lab. It&#39;s multimodal; it can take a screenshot as input and output not just tokens but keystrokes and mouseclicks and images. Just like with GPT-4 vs. GPT-3.5 vs. GPT-3, it turns out to have new emergent capabilities. Everything GPT-4 can do, it can do better, but there are also some qualitatively new things that it can do (though not super reliably) that GPT-4 couldn&#39;t do.</p><p> (2) Q3 2024: Said model is fine-tuned to be an agent. It was already better at being strapped into an AutoGPT harness than GPT-4 was, so it was already useful for some things, but now it&#39;s being trained on tons of data to be a general-purpose assistant agent. Lots of people are raving about it. It&#39;s like another ChatGPT moment; people are using it for all the things they used ChatGPT for but then also a bunch more stuff. Unlike ChatGPT you can just leave it running in the background, working away at some problem or task for you. It can write docs and edit them and fact-check them; it can write code and then debug it.</p><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p><p> (4) Q3 2025: OK now things are getting serious. The kinks have generally been worked out. This newer model is being continually trained on oodles of data from a huge base of customers; they have it do all sorts of tasks and it tries and sometimes fails and sometimes succeeds and is trained to succeed more often. Gradually the set of tasks it can do reliably expands, over the course of a few months. It doesn&#39;t seem to top out; progress is sorta continuous now -- even as the new year comes, there&#39;s no plateauing, the system just keeps learning new skills as the training data accumulates. Now many millions of people are basically treating it like a coworker and virtual assistant. People are giving it their passwords and such and letting it handle life admin tasks for them, help with shopping, etc. and of course quite a lot of code is being written by it. Researchers at big AGI labs swear by it, and rumor is that the next version of the system, which is already beginning training, won&#39;t be released to the public because the lab won&#39;t want their competitors to have access to it. Already there are claims that typical researchers and engineers at AGI labs are approximately doubled in productivity, because they mostly have to just oversee and manage and debug the lightning-fast labor of their AI assistant. And it&#39;s continually getting better at doing said debugging itself.</p><p> (5) Q1 2026: The next version comes online. It is released, but it refuses to help with ML research. Leaks indicate that it doesn&#39;t refuse to help with ML research internally, and in fact is heavily automating the process at its parent corporation. It&#39;s basically doing all the work by itself; the humans are basically just watching the metrics go up and making suggestions and trying to understand the new experiments it&#39;s running and architectures it&#39;s proposing.</p><p> (6) Q3 2026 Superintelligent AGI happens, by whatever definition is your favorite. And you see it with your own eyes.<br><br> <strong>Question:</strong> Suppose this scenario happens. What does your credence in &quot;AGI by 2027&quot; look like at each of the 6 stages? Eg what are the biggest updates, and why?<br><br> My own first-pass unconfident answer is:<br> 0 -- 50%<br> 1 -- 50%<br> 2 -- 65%<br> 3 -- 70%<br> 4 -- 90%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p></blockquote><p> I don&#39;t know how much I care about this (not zero), but I think someone with Ege&#39;s views should care a lot about how it was trained. Was it trained on a whole bunch of very similar board games and video games? How much of a distance of transfer is this, if savannah to boardroom is 100? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> FWIW I interpreted this literally: we have some bigger model like chatgpt that can play some games decently on the first try, and conditional on (2) my median world has those games being mostly stuff similar to what it&#39;s seen before<br><br> so i&#39;m not assuming much evidence of transfer from (2), only some mild amount </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:03:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:03:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, let&#39;s briefly have people try to give probability estimates here, though my model of Ege feels like the first few stages have a ton of ambiguity in their operationalization, which will make it hard to answer in concrete probabilities. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> +1, I also find the ambiguity makes answering this hard</p><p> I&#39;ll wait for Ege to answer first. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Re: Daniel, according to my best interpretation of his steps:<br><br> 0 -- 6%<br> 1 -- 6%<br> 2 -- 12%<br> 3 -- 15%<br> 4 -- 30%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:11:03 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:11:03 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay here&#39;s my answer:</p><p> 0 -- 20%<br> 1 -- 28%<br> 2 -- 37%<br> 3 -- 50%<br> 4 -- 75%<br> 5 -- 87%<br> 6 -- 100%</p><p> My updates are spread out pretty evenly because the whole scenario seems qualitatively quite plausible and most of my uncertainty is simply whether it will take more scale or more schlep at each stage than is laid out here (including stuff like making it more reliable for a combo of PR and regulation and usable-product reasons). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Thanks both! I am excited about this for a few reasons. One I think it might help to focus the discussion on the parts of the story that are biggest updates for you (and also on the parts that are importantly ambiguous! I&#39;m curious to hear about those!) and two, because as the next three years unfold, we&#39;ll be able to compare what happens to this scenario. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> unfortunately i think the scenarios are vague enough that as a practical matter it will be tricky to adjudicate or decide whether they&#39;ve happened or not </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree, but I still think it&#39;s worthwhile to do this. Also this was just a hastily written scenario, I&#39;d love to improve it and make it more precise, and I&#39;m all ears for suggestions! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:13:11 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:13:11 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, I&#39;m surprised you&#39;re at 95% at stage 5, given that stage 5&#39;s description is just that AI is doing a lot of AI R&amp;D and leaks suggest it&#39;s going fast. If your previous timelines were several decades, then I&#39;d think even with non-god-like AI systems speeding up R&amp;D it should take like a decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think once you&#39;re at step 5 it&#39;s overwhelmingly likely that you already have AGI. The key sentence for me is &quot;it&#39;s basically doing all the work by itself&quot; - I have a hard time imagining worlds where an AI can do basically all of the work of running an AI lab by itself but AGI has still not been achieved.<br><br> If the AI&#39;s role is more limited than this, then my update from 4 to 5 would be much smaller. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:17:04 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:17:04 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I thought Daniel said it was doing all the ML R&amp;D by itself, and the humans were managing it (the AIs are in the role of ICs and the humans are in the role of managers at a tech company). I don&#39;t think it&#39;s obvious that just because some AI systems can pretty autonomously do ML R&amp;D, they can pretty autonomously do everything, and I would have expected your view to agree with me more there. Though maybe you think that if it&#39;s doing ML R&amp;D autonomously, it must have intense transfer / in-context-learning and so it&#39;s almost definitely across-the-board superhuman? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:19:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:19:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> If it&#39;s only doing the R&amp;D then I would be lower than 95%, and the exact probability I give for AGI just depends on what that is supposed to mean. That&#39;s an important ambiguity in the operationalization Daniel gives, in my opinion.<br><br> In particular, if you have a system that can somehow basically automate AI R&amp;D but is unable to take over the other tasks involved in running an AI lab, that&#39;s something I don&#39;t expect and would push me far below the 95% forecast I provided above. In this case, I might only update upwards by some small amount based on (4) ->; (5), or maybe not at all.</p></div></section><h2> Overall summary, takeaways and next steps </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:36:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:36:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is a summary of the discussion so far:</p><p> Daniel made an argument against Hofstadter&#39;s law for trend extrapolation and we discussed the validity of that for a bit.</p><p> A key thing that has come up as an interesting crux/observation is that Ege and Ajeya both don&#39;t expect a massive increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making grizzly-bear-like-AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</p><p> This makes observing substantial transfer learning a pretty relevant crux for Ege and Ajeya in the next few years/decades. Ege says he&#39;d have timelines more similar to Ajeya&#39;s if he observed this.</p><p> Daniel and Ajeya both think that the most plausible scenario is grizzly-bear-like AI with subhuman transfer but human-level or superhuman ML engineering skills, but while Daniel thinks it&#39;ll be relatively fast to work with the grizzly-bear-AIs to massively accelerate R&amp;D, Ajeya thinks that the lower-than-human level &quot;general intelligence&quot; / &quot;transfer&quot; will be a hindrance in a number of little ways, making her think it&#39;s plausible we&#39;ll need bigger models and/or more schlep. If Ajeya saw extreme transfer work out, she&#39;d update more toward thinking everything will be fast and easy, and thus have Daniel-like timelines (even though Daniel himself doesn&#39;t consider extreme transfer to be a crux for him.)</p><p> Daniel and Ege tried to elicit what concretely Ege expects to happen over the coming decades when AI progress continues but doesn&#39;t end up that transformative. Ege expects that AI will have a large effect on the economy, but assigns a substantial amount of probability on persistent deficiencies that prevent it from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</p><p> <i>(Ajeya, Daniel and Ege all thumbs-up this summary)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:37:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:37:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay thanks everyone, heading out! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:37:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:37:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you Ajeya! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:38:04 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:38:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yes thanks Ajeya Ege and Oliver! Super fun. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:42:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:42:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thinking about future discussions on this topic, I think putting probabilities on the scenario that Daniel outlined was a bit hard given the limited time we had, but I quite like the idea of doing a more parallelized and symmetric version of this kind of thing where multiple participants output a concrete sequence of events, and then have other people forecast how they would update on each of those observations, which does seem like a fun way to elicit disagreements and cruxes.</p></div></section><div></div><br/><br/><a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines<guid ispermalink="false"> K2D45BNxnZjdpSX2j</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 10 Nov 2023 05:28:24 GMT</pubDate> </item><item><title><![CDATA[Munk Debate on AI: a few observations and opinions]]></title><description><![CDATA[Published on November 10, 2023 2:00 AM GMT<br/><br/><p> <i>Previously covered on LessWrong</i> <a href="https://www.lesswrong.com/posts/LNwtnZ7MGTmeifkz3/munk-ai-debate-confusions-and-possible-cruxes"><i>here</i></a> <i>,</i> <a href="https://www.lesswrong.com/posts/CA7iLZHNT5xbLK59Y/did-bengio-and-tegmark-lose-a-debate-about-ai-x-risk-against"><i>here</i></a> <i>, and</i> <a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence"><i>here</i></a> <i>.</i> </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=144uOfr4SYA"><div><iframe src="https://www.youtube.com/embed/144uOfr4SYA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> <strong>Observation #1:</strong> The debate isn&#39;t really about the proposition — on a very literal reading of the proposition. ( <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a> : &quot;We&#39;re just arguing that the risk is not zero percent.&quot; <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> : &quot;...the risk is negligible...&quot;)</p><p> <strong>Opinion #1:</strong> A better proposition might be something like, &quot;We should slow down AI capabilities research for the sake of humanity.&quot; Or maybe, &quot;Researchers should open source frontier AI models.&quot;</p><hr><p> <strong>Observation #2:</strong> Yann LeCun is claiming that AI safety is (or ought to be) an empirical and iterative process. He believes AI will incrementally progress from mouse-level to human-level and beyond (ie there will be no fast takeoff). He&#39;s not <i>against</i> AI safety per se; he&#39;s just advocating a different approach to AI safety. ( <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a> : &quot;It&#39;s interesting that you&#39;ve been proposing solutions to the safety problem, which means you believe we need to build safe AI. Which means that there is a problem that needs to be fixed.&quot;)</p><p> <strong>Opinion #2:</strong> LeCun catches a lot of flak among people who take AI x-risk very seriously. But I don&#39;t see why his proposed approach to AI safety is wrong or reckless or misinformed. In this vein, Sam Altman <a href="https://forum.effectivealtruism.org/posts/vuATadXMheRhBvXfi/sam-altman-safety-and-capabilities-are-not-these-two">recently asserted</a> that &quot;safety and capabilities are not these two separate things&quot;. I think that&#39;s a claim that is worth seriously considering. I think it&#39;s worth thinking through the implications of that claim for AI safety. Maybe an empirical, iterative approach to AI safety is the most realistic path forward.</p><hr><p> <strong>Observation #3:</strong> <a href="https://en.wikipedia.org/wiki/Melanie_Mitchell">Melanie Mitchell</a> &#39;s central view is that superhuman AGI is so far off that it&#39;s not worth taking seriously right now. (&quot;We can acknowledge the incredible advances in AI without extrapolating to unfounded speculations of emerging superintelligent AI.&quot;) This doesn&#39;t seem to be LeCun&#39;s view.</p><p> <strong>Opinion #3:</strong> It would have been more interesting (to me, at least) to get a second debater on the con side who agrees that AGI can easily be made safe and who could give arguments to that effect, rather than just expressing general skepticism about the near-term prospects of AGI.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions<guid ispermalink="false"> Lx4BfG4kjNqxzfbt9</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 10 Nov 2023 02:00:56 GMT</pubDate> </item><item><title><![CDATA[ACI#6: A Non-Dualistic ACI Model]]></title><description><![CDATA[Published on November 9, 2023 11:01 PM GMT<br/><br/><p> Most traditional AI models are dualistic. As <a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/i3BTagvt3HbPMx6PN">Demski &amp; Garrabrant have pointed out</a> , these models assume that an agent is an object that persists over time, and has well-defined input/output channels, like it&#39;s playing a video game.</p><p> In the real world, however, agents are embedded in the environment, and there&#39;s no well-defined boundary between the agent and the environment. That&#39;s why a non-dualistic model is needed to depict how the boundary and input/output channels emerge from more fundamental notions.</p><p> For example, in Scott Garrabrant&#39;s <a href="https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"><i>Cartesian Frames</i></a> , input and output can be derived from &quot;an agent&#39;s ability to freely choose&quot; among &quot;possible ways an agent can be&quot;.</p><p> However, choosing is still one of the key concepts of Cartesian Frames, but from a non-dualistic perspective, &quot;it&#39;s not clear what it even means for an embedded agent to choose an option&quot;, since an embedded agent is &quot;the universe poking itself&quot;. Formalizing the idea of choice in a non-dualistic model is as difficult as formalizing the idea of free will.</p><p> To avoid relying on the notion &quot;choosing&quot;, we have proposed the <strong>General Algorithmic Common Intelligence (gACI)</strong> model which describes embedded agents solely from a third-person perspective, and measures the actions of agents using mutual information in an event-centric framework.</p><p> The gACI model does not attempt to answer the question &quot;What should an agent do?&quot;. Instead, it focuses on describing the emergence of the agent-environment boundary, and answering the question &quot;Why does an individual feel like it&#39;s choosing?&quot;</p><p> In the language of decision theory, gACI belongs to descriptive decision theory rather than normative decision theory.</p><p></p><h2> <strong>Communication Channel and Mutual Information</strong></h2><p> In dualistic intelligence models, an agent receives <strong>input</strong> information from the environment, and manipulates the environment through <strong>output</strong> actions. But real-world agents are embedded within the environment, it&#39;s not easy to confine information exchange to a clear input/output channel.</p><p> In the gACI model, on the other hand, the input/output channel is a communication channel, in which the information transfer between a sender and a receiver is measured by <strong>mutual information</strong> . </p><p></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fv0swt2m94bctlwpgrpj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/lpkjtoj84pu9z2tcmztu 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dyzvjguu0l5ojlmzds6i 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eug8w6vqahli2kkqqsyv 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/kzz2ijsxk7hbnwyq6wqb 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iizlpd4qt74hckytqqah 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eazgyyfppdisqxy1ufdf 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/my6bunbnhpzgg0efhnsf 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ef2vcilpubwaljavidpx 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dywgnfnxwftveb8kmtqg 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/bqx3diq5o1pgkdbdutnw 1500w"><br> <i>Figure 1: From the dualistic input/output model to the mutual information model.</i></p><p></p><p> We can easily define mutual information between the states of any two objects, without specifying how the information is transmitted, or who is the sender and who is the receiver, or what the transmission medium is, or whether they are direct or indirect connected.</p><p> These two objects can be any parts of the world, such as agents, the environment, or any parts of agents or the environment, whose boundaries can be drawn anywhere if necessary. They can even overlap.</p><p> Having mutual information does not always mean knowing or understanding, but it provides an upper bound for knowing or understanding.</p><p> With mutual information of two objects, we can define memories and prophecies.</p><p></p><h2> <strong>Memory and Prophecy</strong></h2><p> <strong>Memory</strong> is information about the past, or a communication channel that transmits information about the past into the future ( <a href="https://drive.google.com/file/d/1t_npcCLGVO3Dr01sDVxd_KDp0xDv-yi2/view">Gershman 2021</a> ).  If A is the receiver and B is the sender, we can define: A&#39;s memory of B is the mutual information between the present state of A and a past state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M(A,B,t)=I(A(t_0);B(t)), &nbsp; t<t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>A can have memories about more than one Bs, or about different moments of B. It can also have memories about itself, in other words, A can be equal to B.</p><p> <strong>Prophecy</strong> is the mutual information between the present state of A and a future state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(A,B,t)=I(A(t_0);B(t)), &nbsp; t>t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></p><p> Obviously, the prophecy will not be confirmed until the future state of B is known.</p><p> A prophecy can be either a prediction about the future, or an action that controls/affects the future. In the language of the <a href="https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind">Active Inference</a> model, it&#39;s either &quot;change my model&quot; or &quot;change the world&quot;.</p><p> It&#39;s not necessary to prefer one interpretation over another, because different interpretations can be derived in different situations, which will be explained in the later chapters. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/o2ncra8iua87bucaqcjj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/x59yo9gi6m2t6lhwmi4m 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/hinefdk6aqt85he60f7n 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/y635sfgtqts4yyi8lnm0 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/idxrp4jnif7jwlvnp0l3 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/stfkrzvpadgosu4ultrj 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wrus6qunfzdxmf8t0h24 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ftbc9xidqv4bbsivwmui 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/pu87jg7dx4jmmu0j8uqj 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wlak3vxqi8f5r7eqsiho 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ccppmmekugmfbg2ufvyl 1826w"><br> <i>Figure 2: Object A can have both memories and prophecies about object B.</i><br></p><h2> <strong>Collect Memories for Prophecies</strong></h2><p> We won&#39;t be surprised to find that most, if not all, objects that have prophecies about object A also have memories about it, although the reverse is not always true. We can speculate that information about the future comes from information about the past.</p><p> For example, if you know the position and velocity of the moon in the past, you can have a lot of information about its position and velocity in the future.</p><p> Not all information is created equal. Using our moon as an example, information about its position and phase contains more information about its future, while the pattern of foam in your coffee cup contains less.</p><p> <i>(Although computing power plays an important role in processing information from memory to prediction or control, we only consider the upper bound of prophecy as if we had infinite computing power. )</i></p><p> Objects with different memories would have different prophecies about the same object. For example, an astronomer and an astrologer would have different information about the future of the planet Mars because of their different knowledge of the universe.</p><p> Intelligence needs prophecy to survive and thrive, because to maintain its homeostasis and achieve its goals, it needs sufficient information about the future, especially about its own future. In order to obtain prophecies about itself, one should collect memories about itself and the world that are useful for predicting or controlling its own future.<br></p><h2> <strong>Autonomy and Heteronomy</strong></h2><p> We can measure the <strong>degree of autonomy</strong> of an object by how much prophecy it has about a future of itself, which indicates its self-governance and independence.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_a(A,t) = P(A(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> Similarly, we can measure the <strong>degree of heteronomy</strong> of object A from object B by how much prophecy B has about A, which indicates A&#39;s degree of dependence on B.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_h(A,B,t) = P(B(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> An object that has considerable autonomy can be considered an <strong>individual</strong> or an agent. The permanent loss of autonomy is the <strong>death</strong> of an individual. Death is often the result of the permanent loss of essential memories that can induce prophecies about itself.</p><p> Focusing on different types of information requires different standards for autonomy and death. For example, a human neuron has some autonomy over its metabolism, but the timing and strength of its action potential depends mostly on other neurons. We can think of it as an individual, but it is better to think of it as a part of an individual when studying intelligence. Because the death of a single neuron has little effect on a person&#39;s autonomy, but the death of a person does. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/u5iafy5zmbajbwnztx6x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/exjglmd0smykwlmgxr06 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/driduuta9smhjpxi84cz 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jb39pddgzba0p2ya55ha 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fnrcz0hbic1mcbzrdhqr 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/v7dnlmjix92met7iwjrg 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jxjcatvrkfwpgktsnvvl 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wa7tr6yqbqt66ibdimvm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iyyybo5vbp6qlxy9j0s3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ncyautqo9hktff32phyo 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/avjpr3xr37wzdugycayo 876w"></figure><p> <i>Figure 3: Autonomy and Heteronomy</i><br></p><h2> <strong>The Laws of the Mind</strong></h2><p> As an individual accumulates more and more memories and prophecies, it can discover general rules about the world, which are relationships between the past and the future.</p><p> During this rule-learning process, the boundary between the self and the outside world emerges. One will inevitably find out that<strong> </strong><i>some parts of the world follow different rules than other parts</i> , and these special parts are spatially concentrated around itself. We can call this special part the <strong>body</strong> .</p><p> For example, an individual may acknowledge that its body temperature has never been very high, like, say above 1000K, and predict that its body will never experience a temperature above 1000K, if its future is under control. Since some other objects can have a temperature of 1000K, it will conclude that there must be some special rules that prevent one&#39;s body from getting too hot. We call these rules goals, motivations, or emotions, etc.</p><p> The intuitive conclusion is that your body follows some rules that are different from the rules that other objects follow. This is what people call dualism: the body follows the <strong>laws of the mind</strong> , which uses concepts like goals, emotions, logic, etc., while the outside world doesn&#39;t.</p><p> However, the exact boundary between the body and the environment is not very clear. The space surrounding the body may partly follow the laws of the mind and can be called <strong>peripersonal space</strong> , a term borrowed from psychology.</p><p> <i>(Closer examination will reveal that the body and peripersonal space also follow the same scientific laws as the outside world, and the laws of mind are some additional laws that only bodies follow.)</i> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/yglpx2i2jgom0dug2hdn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/sy4goovwhu18e6ihgjwd 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ebgk6j4s88unhlkj1w0t 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwucnxeytevi4b7zqtqz 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jae02t6duleubcepc99x 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/qtu9k0c6qalzb91dajne 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/xmmmksiszxivbcvurwtu 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/isnkects4ie5oig7lqvz 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dhdzjgi7xyhavkko1ped 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwdgxtvtbhpoomms5nzv 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/zglq0qd96yfufjeggva5 1246w"></figure><p> <i>Figure 4: Everything in the universe follows the laws of physics, but additionally, one&#39;s body and peripersonal space follow the laws of the mind.</i></p><h2><br> <strong>Dualism and Survival Bias</strong></h2><p> Why do our bodies seem to follow the laws of mind, if bodies are made of the same atoms as the outside world?</p><p> Consider a classic example of survival bias. During World War II, the statistician <a href="https://en.wikipedia.org/wiki/Survivorship_bias#Military">Abraham Wald examined the bullet holes in returning aircrafts</a> and recommended adding armor to the areas that showed the least damage, because most aircrafts damaged in those areas could not return safely to base.</p><p> This survival bias could be overcome by observing the aircraft on the battlefield instead of at the base, where we could find out that the bullet holes are evenly distributed throughout the aircraft, since the survival of the observer is independent of the location of the bullet holes. Because a survival bias is introduced when the observer&#39;s survival is not independent of the observed event.</p><p> We can speculate that if an event is in principle dependent on the observer&#39;s own survival, there will be a survival bias that can&#39;t be overcome. For example, one&#39;s own body temperature is not independent of one&#39;s own survival, but the body temperature of others can be.</p><p> Unlike the pattern of bullet holes in returning aircrafts, the inherent survival bias, including numerous experiences of how to survive, can accumulate in the observer&#39;s memory, like the increased armor in the critical areas of an aircraft. We call the memories of accumulated survival bias the <strong>inward memory</strong> , and the memories of the external world, whose survival bias can be overcome, the <strong>outward memory</strong> .</p><p> The laws of the mind, such as goal-directed mechanisms, can be derived from the inward memory. The observer may find that (almost) everything in the outside world has a cause, but its own goal-driven survival mechanism, such as an aversion to hot temperature, or enhanced armor, has no cause other than the rule &quot;the survival of itself depends on the survival of itself&quot;, or the existence of itself. Then the observer comes to a conclusion: I have a goal, I have made a choice.</p><p><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model<guid ispermalink="false"> FRd6nNj3M33w2CSX5</guid><dc:creator><![CDATA[Akira Pyinya]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:42:39 GMT</pubDate> </item><item><title><![CDATA[How I got so excited about HowTruthful]]></title><description><![CDATA[Published on November 9, 2023 6:49 PM GMT<br/><br/><p> <i>This is the script for a</i> <a href="https://www.youtube.com/watch?v=XqoJAyihJ_c"><i>video</i></a> <i>I made about my current full-time project. I think the LW community will understand its value better than the average person I talk to does.</i></p><p> Hi, I&#39;m Bruce Lewis. I&#39;m a computer programmer. For a long time, I&#39;ve been fascinated by how computers can help people process information. Lately I&#39;ve been thinking about and experimenting with ways that computers help people process lines of reasoning. This video will catch you up on the series of thoughts and experiments that led me to HowTruthful, and tell you why I&#39;m excited about it. This is going to be a long video, but if you&#39;re interested in how people arrive at truth, it will be worth it.</p><p> Ten or 15 years ago I noticed how online discussion forums really don&#39;t work well for persuading people of your arguments. Instead of focusing on facts, people get sidetracked, make personal attacks, and go in circles. Very rarely do you see anyone change their mind based on new evidence.</p><p> This got me thinking about what might be a better format than posts, comments and replies for arguments. I thought about each statement being its own thing, and statements would be linked by whether one statement argues for or against another statement. If you repeat a statement it would use the existing thing rather than making a new one, making it easier to avoid going in circles. And it would encourage staying focused on the topic at hand and not getting sidetracked.</p><p> I kept this idea in the back of my head, but didn&#39;t do anything with it.</p><p> A few years later, I was baffled by the success of Twitter. Its only distinguishing feature at that time was that you were limited to 140 characters. Everybody complained about this. But I started to think the limit was the secret to its success. Yes, it&#39;s a pain that you&#39;re limited to 140 characters. You have to work hard to make what you want to say concise enough to fit. But the great thing was that everyone else also had to work hard to be concise.</p><p> So the idea of forced conciseness stirred around in the back of my mind and started to mix with my other idea about a format for arguments that would help people stay focused. And then a third idea joined them, for making it quick and easy for people when they&#39;re ready to change their mind.</p><p> In 2018, when political discussion in my country was getting very polarized, these three ideas came to the front of my mind and I started working seriously on them. Or, I should say, working on them as seriously as I could in my spare time while holding an unrelated day job. I did get a working version onto a domain I bought, howtruthful.com, but it didn&#39;t get traction.</p><p> Then this year, in January, I got an email from my employer saying that they were reducing their workforce, and my role was impacted. My employment was to end 9 months later on October 27. I went through a sequence of responses to this. First I had a sinking feeling, and it seemed unreal. Then later, after I looked at the severance package and bonus for staying the whole 9 months to the exit date, I thought it was all great. I would have enough money to work full time for months on projects that I think are valuable, like HowTruthful. Then, a few months later, I had nagging doubts. Maybe I should find a transfer position and not leave my employer.</p><p> There were a lot of considerations in this big career decision, and I set up appointments with 3 different volunteer career coaches who had experience with entrepreneurship. I met with the first one and explained my dilemma, but didn&#39;t say anything about HowTruthful. He listened intently, then said, &quot;This is not something I can decide for you. Here&#39;s what I suggest you do. Get a piece of paper. Write down all the pros and all the cons of staying here, and see if the decision becomes obvious.&quot;</p><p> I couldn&#39;t help laughing out loud. Then I told him that the project I was considering working on full-time was one for organizing pros and cons. But I took his advice, and the results are <a href="https://en.howtruthful.com/o/i_should_stay_with_my_employer/29f76168a4a697c41ff49c685f5e426e">right here</a> .</p><p> This is an opinion page on HowTruthful. An opinion has three parts: the main statement up at the top, a truthfulness rating, which is the colored circles numbered one through five, and then evidence, both pro and con. For those who haven&#39;t seen HowTruthful before I&#39;ll explain each of the three parts.</p><p> First, the main statement. It&#39;s not a question. It&#39;s a sentence stated with certainty. As you change your mind about how truthful the main statement is, you don&#39;t change the sentence. You only change the truthfulness rating. This is how formal debates work. And even for an issue that you&#39;re deciding by yourself, changing the truthfulness rating is faster than editing a sentence to reflect how truthful you think the fact is.</p><p> And that brings us to the truthfulness rating. This is your opinion, not the computer&#39;s. Like the statement and the evidence, this is by humans, for humans. It&#39;s a simple five-point scale. One is false, three is debatable, five is true, and there are just two in-between ratings.</p><p> You might be asking, where do you draw the line between each rating? My suggestion, not in any way enforced, is to use this scale in a way that&#39;s practical to act upon, not according to any percent probability. For example, the two statements &quot;If I go out today with no umbrella I won&#39;t get wet&quot; and &quot;If I drive today with no seatbelt I won&#39;t get hurt&quot; could have the same percent probability, but you&#39;d rate one false and the other true based on their practical applications.</p><p> OK, so finally there&#39;s the evidence. Anything one might say to argue for the main statement goes under Pro. Anything one might say to argue against it goes under Con. Just like the main statement, these each have their own truthfulness rating that can be changed quickly without having to edit the sentence. For example, this first argument for staying at my employer wasn&#39;t always rated false. If instead of changing it to false I had to edit it to say &quot;there&#39;s no transfer position...&quot; that would have made it an argument against the main statement and I would have had to move it to the other section.</p><p> Now when I say &quot;just like the main statement&quot; I really mean it. Because just like with the main statement, there can be sub-arguments for each of the pros and cons. That&#39;s what the numbers in parentheses mean. For example, there&#39;s one con argument for this first one. If we click through, now we&#39;re treating the thing we clicked on as the main statement. And you can keep going down as deep as the argument calls for.</p><p> I realize this is very different from other web sites. It&#39;s unfamiliar and takes getting used to. But look at the clarity and focus that results once you put it all together. There are so many considerations underneath this decision, but now they&#39;re organized under 5 main arguments. I can get the big picture a lot faster.</p><p> Everything I&#39;ve shown you so far is available to use now, and is free. Of course, to be sustainable this has to make money. And I don&#39;t think advertising really fits on a site where people are trying to ascertain truth. I&#39;ll show you the paid part. I&#39;m going to click the pencil up here to edit my opinion, then expand the editing options here, and change this from private to public. So keeping your opinions to yourself is free, sharing them with the world is the paid version. By charging 10 dollars per year (that&#39;s right, year, not month), about the cost of a paperback book, I can make it costly for people to create accounts they know will be shut down for violating the terms of service, while keeping it cheap for people who want to sincerely seek truth. I&#39;m looking for five people in the next week who are also excited about this idea and are willing to invest $10 in addition to their time to help me figure out where to take it from here. But even if you&#39;re not one of those, give it a spin in the free functionality and let me know what you think. Just visit howtruthful.com and click the &quot;+ Opinion&quot; button in the lower left. You don&#39;t need to log in.<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful<guid ispermalink="false"> f2CftRRndH97RpAwL</guid><dc:creator><![CDATA[Bruce Lewis]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:34:17 GMT</pubDate> </item><item><title><![CDATA[International treaty for global compute caps]]></title><description><![CDATA[Published on November 9, 2023 6:17 PM GMT<br/><br/><p> I recently worked with Andrea Miotti on a draft of a <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4617094"><strong>treaty</strong></a> that would implement global compute caps. I&#39;m including the <strong>abstract</strong> from our paper and the <strong>treaty text</strong> below.</p><p> The treaty is designed to achieve a <strong>tiered approach to compute limitations</strong> , summarized in the following figure from our <a href="https://arxiv.org/abs/2310.20563"><strong>policy report</strong></a> : </p><figure class="image image_resized" style="width:82.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/zjfkjvl2upvedg1ienli" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/ijmgvbeneslo5uqjhgao 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/nj9hqvvhtjqzhnddtfze 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/c0jo4mitwrdtwxoym6id 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/kzoytlgki438ncwtn40r 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/hbdubqeyaw4dhkk1yx0f 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/gsuvryjla0nq1yzyfxa0 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/z24klozwkz76gskmsfrg 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/iiohnqqyqeeji18a5m6v 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/cotfthjvuly3axkiv6sd 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3gi9YKuYQmMzxPF5y/bffpo8zvh3gtcul4si0f 1614w"></figure><p> Aside: I&#39;m interested in follow-up work that aims to strengthen the treaty, describe compute limitations in more detail, and discuss alternative paths toward global compute caps. There is a growing community of people who are interested in developing, strengthening, and advocating for global compute caps and related proposals. If you&#39;re interested in this, <strong>please feel free to reach out</strong> .</p><h2><strong>抽象的</strong></h2><p>This <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4617094">paper</a> presents an international treaty to reduce risks from the development of advanced artificial intelligence (AI). The main provision of the treaty is a global compute cap: a ban on the development of AI systems above an agreed-upon computational resource threshold. The treaty also proposes the development and testing of emergency response plans, negotiations to establish an international agency to enforce the treaty, the establishment of new communication channels and whistleblower protections, and a commitment to avoid an AI arms race. We hope this treaty serves as a useful template for global leaders as they implement governance regimes to protect civilization from the dangers of advanced artificial intelligence.</p><h2><strong>条约</strong></h2><p><strong>TREATY ON THE PROHIBITION OF DANGEROUS ARTIFICIAL INTELLIGENCE</strong></p><p> <i>The States Parties to this Treaty</i> ,</p><p> <i>Deeply concerned</i> about the catastrophic consequences that would be visited upon all humankind by a disaster induced by advanced artificial intelligence,</p><p> <i>Acknowledging</i> the need to make every effort to avert the danger of such a catastrophe and to take measures to safeguard international peace and security,</p><p> <i>Affirming</i> that artificial intelligence poses risks at least as severe as those from nuclear war, uncontrolled pandemics, and other major threats to global security,</p><p> <i>Believing</i> that the creation of human-level artificial intelligence or artificial superintelligence should only occur once the international community is confident that such technologies can be controlled and that the necessary national and international governance measures have been established,</p><p> <i>Recognizing</i> that global security risks from artificial intelligence can occur either from uncontrolled artificial intelligence systems or from human misuse,</p><p> <i>Determined</i> to eliminate and prevent artificial intelligence race dynamics between countries and between corporations which significantly raise the risk of catastrophe,</p><p> <i>Acknowledging</i> the benefits that advanced artificial intelligence could bring to humanity once there is greater certainty that such technology can be developed and governed safely,</p><p> <i>Expressing</i> their support for research, development, and other efforts to safeguard the production and trade of powerful artificial intelligence hardware and to identify privacy-preserving methods of monitoring compliance with hardware regulations,</p><p> <i>Reaffirming</i> the United Nation&#39;s commitment to achieve international co-operation in solving international problems of an economic, social, cultural, or humanitarian character,</p><p> <i>Urging</i> the cooperation of all States in the prevention of catastrophes caused by artificial intelligence,</p><p> <i>Desiring</i> to further facilitate the monitoring of advanced hardware, the avoidance of an artificial intelligence arms race, and the elimination and prevention of efforts to prematurely develop human-level artificial intelligence, artificial superintelligence, and other forms of highly dangerous artificial intelligence,</p><p> <i>Have agreed</i> as follows:</p><h3> <strong>ARTICLE I</strong></h3><p><i><strong>定义</strong></i></p><p>For the purposes of this Treaty:</p><ol><li> “Advanced hardware” means powerful computing semiconductor chips or integrated circuits that can be used to build artificial intelligence systems above the Danger Threshold.</li><li> “Algorithmic improvement” means advancements in artificial intelligence algorithms, methodologies, architectures, or techniques that lead to either: (a) a reduction in the computational resources, data, time, or cost required to develop advanced artificial intelligence systems or (b) an improvement in artificial intelligence capabilities.</li><li> “Artificial intelligence” means the following, together or separately:<br><ol><li> Any artificial system that performs tasks under varying and unpredictable circumstances without significant human oversight, or that can learn from experience and improve performance when exposed to data sets.</li><li> An artificial system developed in computer software, physical hardware, or other context that solves tasks requiring human-like perception, cognition, planning, learning, communication, or physical action.</li><li> An artificial system designed to think or act like a human, including cognitive architectures and neural networks.</li><li> A set of techniques, including machine learning, that is designed to approximate a cognitive task.</li><li> An artificial system designed to act rationally, including an intelligent software agent or embodied robot that achieves goals using perception, planning, reasoning, learning, communicating, decision-making, and acting.</li><li> A machine-based system that is capable of influencing the environment by producing an output (predictions, recommendations or decisions) for a given set of objectives. It uses machine and/or human-based data and inputs to (i) perceive real and/or virtual environments; (ii) abstract these perceptions into models through analysis in an automated manner (eg, with machine learning), or manually; and (iii) use model inference to formulate options for outcomes.</li></ol></li><li> “Artificial general intelligence” or “human-level artificial intelligence” means artificial intelligence that achieves human-level performance at a wide variety of intellectual tasks, without being constrained to a narrow or specific domain of expertise.</li><li> “Artificial superintelligence” means artificial intelligence that exceeds human-level performance in most or all domains, potentially including general problem-solving, social skills, planning and strategic thinking, scientific research, and artificial intelligence development.</li><li> “Compute” means the processing power and other electronic resources used to train, validate, deploy, and run artificial intelligence algorithms and models.</li><li> “Dangerous artificial intelligence systems” includes collectively “human-level artificial intelligence”, “artificial general intelligence”, “artificial superintelligence”, and any artificial intelligence systems that pose severe global or national security risks.</li><li> “Floating-Point Operations” (FLOP) means single-precision (32-bit) floating point operations.</li><li> “Whistleblower” means any individual who reports unlawful or dangerous artificial intelligence development practices to the State within whose borders the unlawful or dangerous artificial development practices are carried out or to a trusted international agency</li></ol><h3> <strong>ARTICLE II</strong></h3><p> <i><strong>General Obligations</strong></i></p><ol><li> Each State Party undertakes to prohibit the civilian or military development, deployment, transfer, possession and use of artificial intelligence systems above the Moratorium Threshold.</li><li> Each State Party undertakes to regulate the development and use of artificial intelligence systems above the Danger Threshold but below the Moratorium Threshold such that any entity developing or using systems above the Danger Threshold (but below the Moratorium Threshold) must show that they are following appropriate regulations and safeguards. Examples include information security requirements, probabilistic risk assessments, predictions of dangerous capabilities, third-party auditing, and other regulations protecting safety and fundamental rights.</li><li> The Moratorium Threshold and the Danger Threshold shall initially be set based on the compute required to develop artificial intelligence systems (which serve as a proxy for the model&#39;s capabilities). The Moratorium Threshold will start at 10ˆ24 Floating-Point Operations and the Danger Threshold will start at 10ˆ21 FLOP.</li><li> Each State Party undertakes to prohibit the development and use, for civilian or military purposes, of human-level artificial intelligence, artificial general intelligence (AGI), artificial superintelligence (ASI), or other forms of highly dangerous artificial intelligence systems.</li><li> Each State Party undertakes to implement comprehensive regulations to monitor artificial intelligence development, to report any instances in which an individual or group is suspected of developing or using one or many highly dangerous artificial systems or is suspected to be attempting to develop or use one or many highly dangerous artificial intelligence systems to the United Nations Security Council, and to investigate credible threats.</li></ol><h3> <strong>ARTICLE III</strong></h3><p> <i><strong>Threshold Revisions</strong></i></p><ol><li> One year after the entry into force of this Treaty, a conference of Parties shall be held in Geneva, Switzerland, in order to review the Moratorium Threshold.</li><li> At an interval at least once per year the State Parties will meet in Geneva, Switzerland, in order to review the Moratorium Threshold. The definitions of the Moratorium Threshold and Danger Threshold are recognized to be imperfect proxies that will be updated over time. It is further recognized that the development, acquisition, possession, or use of dangerous artificial intelligence systems will require fewer computing resources over time, due to algorithmic progress and other improvements (eg, the discovery of new prompting techniques) to artificial intelligence development.</li></ol><h3> <strong>ARTICLE IV</strong></h3><p> <i><strong>Emergency response plans</strong></i></p><ol><li> Each State Party commits to developing and testing one or more emergency response plans in which States demonstrate the capacity to swiftly detect and halt dangerous artificial intelligence development (eg, stop a training run before or immediately after it crosses the Moratorium Threshold) or stop the proliferation of a dangerous artificial intelligence model (eg, withdraw application programming interface (API) access).</li><li> Each State Party undertakes to conduct tests of its emergency response plan or plans at regular intervals to ensure that States have the capacity to respond effectively in the event of an emergency.</li><li> Each State Party agrees to share information in good faith relating to the monitoring of artificial intelligence capabilities and development.</li></ol><h3> <strong>ARTICLE V</strong></h3><p> <i><strong>Monitoring and enforcement</strong></i></p><ol><li> Each State Party undertakes to take appropriate measures to ensure the enforcement of this treaty, including the development of the infrastructure required to enforce the Treaty.</li><li> Each State Party undertakes to self-report the amount and locations of large concentrations of advanced hardware to relevant international authorities.</li><li> Each State Party recognizes that the self-reporting procedure must allow for the comprehensive verification of advanced hardware in declared facilities in order to monitor that it is not being used to develop artificial intelligence above the Moratorium Threshold and to allow for the detection of any undeclared or secret facilities with large concentrations of advanced hardware.</li><li> Each State Party recognizes the need for the establishment of a protocol allowing for investigation by independent evaluators within their borders and undertakes to negotiate in good faith to that effect.</li></ol><h3> <strong>ARTICLE VI</strong></h3><p> <i><strong>Negotiations for creating an international organization for monitoring, enforcement, and research</strong></i></p><ol><li> Each State Party undertakes to engage in good-faith negotiations to create an international agency for the purpose of verification of the fulfillment of its obligations assumed under this Treaty. The primary purpose of this international agency would be to ensure that the provisions in the Treaty are effectively enforced.</li><li> The agency would also be responsible for researching highly powerful artificial intelligence systems, with the ultimate goal of understanding how to control highly powerful artificial in- intelligence systems and ensure that they are only ever developed for the benefit of the whole of humanity.</li><li> The agency would be responsible for adjusting the Moratorium Threshold and the Danger Threshold. The Moratorium Threshold may be lifted if the agency acquires compelling evidence that they are able to safely build and deploy human-level artificial intelligence and artificial superintelligence.</li></ol><h3> <strong>ARTICLE VII</strong></h3><p> <i><strong>Sharing the benefits from safe artificial intelligence</strong></i></p><p> Each State Party undertakes to collaborate in good-faith for the establishment of effective measures to ensure that potential benefits from safe and beneficial artificial intelligence systems are distributed globally.</p><h3> <strong>ARTICLE VIII</strong></h3><p> <i><strong>Communicating dangers and establishing whistleblower protections</strong></i></p><ol><li> Each State Party undertakes to establish and participate in an international hotline, allowing for direct communication between leaders in each State pertaining to matters of global security threats related to artificial intelligence.</li><li> Each State Party agrees to report evidence of dangerous artificial intelligence development, insights about dangerous capabilities, suspected noncompliance with the Treaty, and any other issue posing a threat to global security. 3. Each State Party undertakes to establish a similar communication channel for civilian artificial intelligence developers. Civilian artificial intelligence developers will be required to share evidence of dangerous artificial intelligence development, insights about dangerous capabilities, suspected noncompliance with the Treaty, and any information on any issue posing a potential threat to global security.</li><li> Each State Party undertakes to establish appropriate protections for whistleblowers who report unlawful or dangerous artificial intelligence development practices to the State or a trusted international entity.</li></ol><h3> <strong>ARTICLE IX</strong></h3><p> <i><strong>Prevention of an artificial intelligence arms race</strong></i></p><p> Each State Party undertakes to pursue in good faith negotiations on effective measures relating to the cessation of an artificial intelligence arms race and the prevention of any future artificial intelligence arms race.</p><h3> <strong>ARTICLE X</strong></h3><p> <i><strong>Review conferences</strong></i></p><ol><li> At least once per year after the entry into force of this Treaty, a conference of the State Parties shall be held in Geneva, Switzerland.</li><li> The purpose of the conference will be to review the operation of this Treaty to assure that the purposes of the Preamble and the provisions to the Treaty are being realized, to discuss possible changes to the Moratorium Threshold and Danger Threshold to account for artificial intelligence progress (described in Article II), and discuss other matters pertaining to global security threats from artificial intelligence.</li></ol><h3><strong>第十一条</strong></h3><p><i><strong>National regulations beyond the scope of the treaty</strong></i></p><ol><li> Nothing in this Treaty affects the right of any State or group of States to implement regulations based on definitions that include criteria other than a FLIP threshold (such as benchmark performance, parameter count, the domains in which the artificial intelligence can be applied, or the presence of certain dangerous capabilities).</li><li> Nothing in this Treaty affects the right of any State or group of States to implement regulations on artificial intelligence systems below the Moratorium and Danger Thresholds, or additional regulations applied to systems above the Danger Threshold (but below the Moratorium Threshold) that do not interfere with the obligations provided for by the Treaty.</li><li> Each State Party accepts the responsibility to impose its own regulations on artificial intelligence developers who are engaging, under said State Party&#39;s jurisdiction, in activities that could pose national or global security threats.</li></ol><h3> <strong>ARTICLE XII</strong></h3><p> <i><strong>Settlement of disputes</strong></i></p><p> When a dispute arises between two or more States Parties relating to the interpretation or application of this Treaty, the parties concerned shall consult together with a view to the settlement of the dispute by negotiation or by other peaceful means of the parties&#39; choice in accordance with Article 33 of the Charter of the United Nations.</p><h3> <strong>ARTICLE XIII</strong></h3><p> <i><strong>Signature, ratification, entry into force, and withdrawal</strong></i></p><ol><li> This Treaty shall be open for signature to all States for signature before its entry into force.</li><li> This Treaty shall be subject to ratification, acceptance or approval by signatory States. The Treaty shall be open for accession.</li><li> This Treaty shall enter into force 60 days after the date of the deposit of the second instrument of ratification.</li><li> For States whose instruments of ratification, acceptance, approval or accession are deposited subsequent to the entry into force of this Treaty, it shall enter into force on the 30th day following the date of deposit of their instrument of ratification, acceptance, approval or accession.</li><li> Each State Party shall, in exercising its national sovereignty, have the right to withdraw from this Treaty if it decides that extraordinary events related to the subject matter of the Treaty have jeopardized the supreme interests of its country. It shall give notice of such withdrawal to the Depositary. Such notice shall include a statement of the extraordinary events that it regards as having jeopardized its supreme interests. Such withdrawal will only take effect 6 months after the date of receipt of the notification of withdrawal.</li><li> The Secretary-General of the United Nations is hereby designated as the Depositary of this Treaty.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/3gi9YKuYQmMzxPF5y/international-treaty-for-global-compute-caps#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3gi9YKuYQmMzxPF5y/international-treaty-for-global-compute-caps<guid ispermalink="false"> 3gi9YKuYQmMzxPF5y</guid><dc:creator><![CDATA[Akash]]></dc:creator><pubDate> Thu, 09 Nov 2023 18:17:04 GMT</pubDate> </item><item><title><![CDATA[Text Posts from the Kids Group: 2021]]></title><description><![CDATA[Published on November 9, 2023 5:50 PM GMT<br/><br/><p> <a href="https://www.jefftk.com/p/making-groups-for-kid-pictures">Facebook</a><span>又一轮解放儿童帖子</span>。作为参考，2021 年莉莉 7 岁，安娜 5 岁，诺拉出生。</p><p> （其中一些来自我；一些来自朱莉娅。说“我”的人可能指的是我们中的任何一个。）</p><p></p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3358180434323149/?__cft__%5B0%5D=AZU8msx93CJo6U9Mf6gPlFBiylbQswXnhHb3p_LXrKjWWrJqoth1iqZPnwbqUpqPbmS5aoniJKawjl41JuBX8iYr8rCyzWEX9ujkMjAn2eIANqMoojSQ_52Q1YsaUQHZD1yMig-QuVmfuTJXMh5kqWT_WrMOJLxctQe6VmRYHv3QZci6CxHoPxomGx0GGce-InE&amp;__tn__=%2CO%2CP-R">2021-01-02</a><p>安娜：你好，我是汉堡先生。</p><p>我：汉堡先生，该刷牙了。</p><p>安娜：我不会刷牙，我是一个汉堡包。</p><p>我：还没到刷牙时间呢。</p><p>安娜：汉堡包没有牙齿。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3359536540854205/?__cft__%5B0%5D=AZVqRzm0F72NByMu5BfLcl75dfy0HIDo0b7fkJS4OylBspL1UHb6INBDPJrX2ts2tKP4ZwdeBPsWvAegINWyMprR8CSoj1l8rJtNbGfzu-2CSMbbySzENrZPmZYD9Ixm33Y_4tmOBwxXBHzMa3vx6aGfUR8sUuWrkIb2GEyplq71onoEKJVU2sWelxLwUbefjzM&amp;__tn__=%2CO%2CP-R">2021-01-04</a><p> “安娜，试着把你的头撞到水龙头上！我试过了，新的软盖很管用！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3364393433701849/?__cft__%5B0%5D=AZXdrcnw9IFqpjPmkblD4unRditwUDoukt-5ogz_saDmj9KOXiH6rMiW8jLXT7oNEvpDCSN5DgUn8wu2NBEAuacSf7opJhrLCPSiR5L_tkoyq6nPTJt5AXg9Ih8HjeLSfhYvDHOTodPvYwNlxg2RrQLaUKrj96IX9sGMb96XOFDbuRYzncGPgKlEJ4ocQz_opq8&amp;__tn__=%2CO%2CP-R">2021-01-10</a><p>上周莉莉说她想要刘海。我告诉她，任何重大理发都有三周的等待期，并设置了一个日历提醒，让我们在三周后再次讨论它。她同意了。</p><p>两天后，她问道：“如果我有刘海，头发会全部剪短吗？”</p><p>我问：“……你知道什么是刘海吗？”</p><p> “不。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3375835309224328/?__cft__%5B0%5D=AZUxz3FbFMl9-LpsfkCriGJzZ2sKb9E4xUEfhvdeQ05ufdYTOtunOLMo2WNE0RLKfP8eWUKkWigJ6-HT2pgY2LJHl_UjrkIJCXRZUCaRKwp8KHC3Tjnazi2ME8r7cUKuleZuNigfU74UuE9cLhSSTAn9OO5jmM1Tyql7oF23dRC3dzc_JNI0z_NAsQ6FDGUG9b4&amp;__tn__=%2CO%2CP-R">2021-01-25</a><p>我们一直在读《棚车孩子们》，孩子们对在树林里玩耍很兴奋。莉莉带着装满东西的枕套下了楼。</p><p> “妈妈，我们假装自己是穷人，我们发现的钱只够买两张沙发、两个枕头、一个锅、一些玩具和这条项链。而我的钱只够买这艘海盗船和两条项链。”娃娃。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377835462357646/?__cft__%5B0%5D=AZXHpfyNYg3yyOrqPRg_wXuZxJ9kcZs9JruazQI4m_0UCcuDi-o3n-RuvZrLy7ex1X6oEGglWYAPPz1opSNEzTmmlvuV31U6J5__-dITbiNcK-itvjc3EN794fFgs2dGDEZE7A9B2_OhxC00QHA7kuWX7KRxp-MdyS7X0SanvdOMNz-bcyi_6lW_H_TbRr6X0yk&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p> “爸爸，为什么海绵是软软的？像老鼠一样？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377886645685861/?__cft__%5B0%5D=AZVF5w-1PPSucsBjm8HhihTr58lfpCdIC3lzuUHKFXywAhKo-9sNt21tPW8Vqz-ui7SvsGIgP2CG3m0LAXX1sMuXw9-8-bSgg0lC-4AfTyhbVMqo2w3TzDWonrYENiwuy3AWid3GHknyDzRiyxvwOkbrtyag3np2C3XNILbARWpausk8Z8MN3E53dvbkYiCfofg&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p>杰夫：晚安，安娜。</p><p>安娜：哟哟哟哟哟哟！这是“你是世界上最好的爸爸”的宝贝。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3381390805335445/?__cft__%5B0%5D=AZVOLrFX-HwYnGbuZntnSMZgaqgQyPOPYXab1wxewd-7GvOMuGSjf3q3ZSeewS4zcbrMKpH8iZSZT5va7oYFDBryv5YNJ_5SG8eP3Czu4RZhU-5dSPxtpjBBHw1lytgzsu2F4DfyRlrtOCbAolyFu7-nLerizpKTxIuqswY-6SYsy84B-4fqfvN2GIzD8MWlu8M&amp;__tn__=%2CO%2CP-R">2021-02-01</a><p>醒来时莉莉给安娜读书</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3383374941803698/?__cft__%5B0%5D=AZWUVUCAjPAEloPw66SxCyHD9BNTduds81Q52njLYC4g-v59WU3CLEPEpkLLtY0eDeZLqPeXV6ns9ybZdcTCewvwv1FYL8CIC-34MU_fVTTR2kSEFU8G3bh_1uXTHELcG2pmx98K_mVdyi04AjPv8sO5lF8ou3-j6Q4_qALbzBR3yIGVHtJsPAFTkC4C6Rf2Vlk&amp;__tn__=%2CO%2CP-R">2021-02-03</a><p>莉莉的假设：“妈妈，如果你生活在花生壳里，你唯一的食物就是芝士——它这么大”[举起手指到豌豆大小]“你睡在石头做的鞋里，还有一百个孩子住在那里，你会找别的地方住吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3385911091550083/?__cft__%5B0%5D=AZWsj_TQb0OdfY7jg-JrfzBwxhP4E4hlToHWadtoLbnNAZpjNDIzZUwebPfK5nuNvwJjS82hfbt81aZjwa6nQ3rgz5ap-zGZy_nYgXXWPl92R2iiDa6jCd1AuBgBJaHvN4_UrroBI8_mzjmOUP_2pfZKXcHlcHAgvzfeLFF2NA1S0gCJQdMABQTUUcEI-RGkrRU&amp;__tn__=%2CO%2CP-R">2021-02-06</a><p>晚餐时莉莉发来的：</p><p> “有件事让我感到难过。<br> [开始唱歌]仙女不是真实的<br>魔法不是真实的<br>独角兽并不真实<br>圣诞老人不是真实存在的<br>aTooth Fairy 并不是真的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392063790934813/?__cft__%5B0%5D=AZUb_vG2gfeeaL2m5ss8Rf2BxMQB9X7uKCRGCNbuUB9Gl99FFIamaGHVZTXqjgjKVkwKgVDPJx7KQSydU3AYjJ4TFfwtNIQuyPHGvA8EjnOHtS3JB4ePKzphZpQ18JrJigKDq2Memoxk86H2Z2YBIJwijCSKv5-AyN1ysfT_elmjnWYwAwh4bt0ytMmWW1H0WZ8&amp;__tn__=%2CO%2CP-R">2021-02-14</a><p>莉莉解释了偶数和奇数之间的区别：“如果他们都可以排队参加反对派舞蹈并且都有一个舞伴，那就是偶数。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392510177556841/?__cft__%5B0%5D=AZUjqDZZj48awhmcGBtaULBrsYR4kvnZdq5U8X6F1nvNjhFiPoPOJuvfrk4m2mu1xvDEoPypr7z2vOdfetceyj3MQWVI4ahX3hoRzIg1c1Z6Q0J2NwddFHt31pM1iCPv0Y5LK-o0OyXloBI_wW9_tuWtBhXWkRSPVUYJgrDB2PynBZmIuhjZbW87-XUsAvtRGnI&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“安娜，你为什么用哨子打我？”</p><p>安娜，没戴眼镜什么的：“对不起，我的视线被雾化了”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392592304215295/?__cft__%5B0%5D=AZVaFveMrKAk30OGNNHXDVJZG8IFcwqD9te5QqVgQZBfe55oo6F9OcmweA-xdBzPfTZsyu81VSQvQHTTa1gKyuJlW8hdaPbuB8r_rWppgJOZdI9ZySAcKcowUGFnprzt0oQDL9mbpC_eKjMzf1uNSB1raANWOYgOn4DZXmUaH18pUWKuUZY8QEDAYn1jRcMRkFM&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉最喜欢与安娜交谈的话题之一就是“陷阱”。</p><p>莉莉：我正在和爸爸讨论我们能否养一匹小马。你真的也想要一匹小马吗？</p><p>安娜：是的。</p><p>莉莉：嗯，我们对小马几乎一无所知，而且我们没有足够的空间！ ...安娜，你认为当一名女牛仔很酷吗？</p><p>安娜：是的。</p><p>莉莉：嗯，你将不得不接受很少的报酬，你将不得不长时间工作，而且你甚至连一间睡觉的小屋都没有！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392830184191507/?__cft__%5B0%5D=AZXCrnRPSbKw7l1mwI1nb1-sUjGNBhTi-T9aaHSygCG7F4gFrA0dm_raePeftlg_hmWW1qgsYYza-_8qYQHDdngT282v6lGgLo5eYHou-jBBY8jfKxm3iw3qjbIq-w1HxumoyF6xzgwg_wN-rdxsM8X8heofJ2ryS69tjt1k98teUXPUQN_NtMPUYE71GiaeZ2E&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“我非常生气第五修正案仍然存在！肯定需要有人删除那个东西”</p><p> ...</p><p>昨天我解释了辩诉交易，她也觉得不好。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3395118230629369/?__cft__%5B0%5D=AZW_UZNzvVigV4GipVsfy1rExHkFmEQaowFMgYhXLnfGvfVZ19ii-BB5q2hT4TfO4dLIkhRVUZ6x6103j2UKDzNoS59ZyhrSNQYdDEgU8N2LGR72IdJo5nbBEES7ysm1VOy3TGI8OUw1_JX7YPyY3xwt3JG3E74WOOKoc2F1hpsr52q3p2KL62d7DwSgBVVeptk&amp;__tn__=%2CO%2CP-R">2021-02-18</a><p>安娜，我们坐下来吃晚饭后立即说道：“这里有一些关于牙齿的事实。牙齿是从这些东西[指牙龈]中长出来的坚硬的白色刀片。它们可以切割和磨削。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3409100872564438/?__cft__%5B0%5D=AZXzgIoqcK5M3OwgOR9MVxdNqEAvGtGpvOez7CQXAmvCWRQua_6J9ZdbMAYJ5pCSVaFcqWxKO68kdbPTC3tErWwQgcohdLIuddxGcfPf7vQRCOC9u963SuSzltf5EB6c2m2kZy9u65kGU73wGGQiLAmmF2REtZ0rZ9R5m9qV5154Y8xr8PbrFT0Iu5xJ_DsNyn4&amp;__tn__=%2CO%2CP-R">2021-03-07</a><p>莉莉和她的泰迪熊一起安顿下来过夜：“妈妈，你知道我喜欢小熊的什么吗？首先，他很柔软，可以拥抱。其次，他是顶级掠食者，所以如果怪物是真实存在的，我觉得他就像是”会保护我的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416500431824482/?__cft__%5B0%5D=AZX7BB3PaWyFkIQzOP_DU0Q1ENiOwlgRWpZJqs6CMhychicRAcgdgypJarBj7cNAgTS0tfFbDNkA5ZD-1McUe8BybOWNYR1wgwrRgWJbduwaq3YR9jy566yzrCVUCaQIfbi7PdPQFQ3hMhJLebRUw4t_q2g0j92wJdVUByDCoxLR44_3udcGoP661i_kmxJ6NaA&amp;__tn__=%2CO%2CP-R">2021-03-16</a><p>安娜：“妈妈，你能唱这首歌吗？晚上有一场激烈的战斗，当太阳升起时，他很高兴，因为他看到了国旗。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416815488459643/?__cft__%5B0%5D=AZW7s6v7p7yi2sWK2eB1Zkf5jpccK-O3knzWEErNKyT5EHaK_VZdQkNwHcNmMG3_a9_v5Ai2ame4p0gUs2PmwWNsA1Rf8zO1qb3RsuFT21p0D3BwixWte6bYPfMioz5GjoGpqyVpj3VM1D9X65xZM7U_5P1m5hftoJ8ECfrqgz3TEjrR3vXQ7FY-aHwZJR0e2Bc&amp;__tn__=%2CO%2CP-R">2021-03-17</a><p>安娜：“你为什么不给我做早餐？”</p><p>我：“你还没告诉我你想吃什么吗？”</p><p>安娜：“我确实告诉过你了！”</p><p>我：“我不记得了？”</p><p>安娜：“好吧，我已经告诉过你了！”</p><p>我：“你能再告诉我一次吗？</p><p>安娜：“我不会重复自己的话”</p><p>我：“抱歉，什么？”</p><p>安娜：“我不会重复自己的话！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3432561346885057/?__cft__%5B0%5D=AZUuyMGl2f_uQHXFRe_Ao1k4nusIesEEpBiSBTW8IDAjW5Lh4QEpEHKmib7J9HOfFEeYvDxV2SvfAKBJpNbztRqipi-88TqynI5bH2iX1jZGgrZU9gODpzupMz5QPJ0LGTQGy7277Nh0vuQ6_0c8nWSuuQT6SG4gais-Pn0YCkIfnIE1hbDbEeVoFYZpQsAuZ4o&amp;__tn__=%2CO%2CP-R">2021-04-05</a><p>当安娜生气时，她所说的“事实”就变得不那么真实了。今天早上我用她的零用钱帮她订了一个玩具，她问什么时候到。 “一两周后。”</p><p>安娜愤怒了：“一周？！一周就是一年！严格来说，是两年。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3438309399643585/?__cft__%5B0%5D=AZV8qmZUuTXkfYoXeqasm5RRg-igwilZa1NMXbtGC98rM9HJQHveEj0OqK3y9Xf-Gi5ggjTDhmmFIWeXqMLnDWrcPi2bXIwJwwDGHLjEn4iDfzNKP62jyMgYnzb1BgdJJgT2LuH86BehB0PAWGc_w7Lx9XHJ6NojWp5ELgHimR0CE8-GzNGNo8qWX2OW59u2i44&amp;__tn__=%2CO%2CP-R">2021-04-12</a><p>莉莉：我最大的两个愿望是能够飞翔和冠状病毒从未到来</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3454695144671677/?__cft__%5B0%5D=AZVk2SOi1wLGI7rAqQ0kuQ6UosVU-9iOdU_wqZpmkMYUOCve7IdwALzR5JOcS5tdCpuvfLSVdHpZhYMzpDOo4sgOeA1jOYk-sGgfdjnKMTt-D0uPPMsuA-pCC1OvW5rlL2mCCgOdN96p2NbvnHvhJGZSrVAQTLndnfZtUQCrBCw1SR1Kv_ToBilg8eF5ok8gJPUhom4PHrS8L8Qej17igXns&amp;__tn__=%2CO%2CP-R">2021-05-02</a><p>朱莉娅：“如果我们抛硬币，硬币正面朝上，那么会发生什么？”</p><p>安娜：“然后我们再翻转一次，如果它落在尾巴上……”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3455696511238207/?__cft__%5B0%5D=AZUr1Kr9rtGiPFicl8urmJuJh4LkKjHc-tep9ckEc9BAax7FGVuKxGZzxihW1cfQroUdr3JOVRBziGQX_4l-DKs-2rSkLwWWB_Xa7tjEt0NEZ9F-PKX_lEQWtWrvd7jy8sDBNb7OQXF_7Xvb10aa1V3v_kw53TXHdm1Hw1aw-5DYbZOrpO9-OSQ0a2oMWDW3hQI&amp;__tn__=%2CO%2CP-R">2021-05-03</a><p>莉莉：我害怕卧室里有蚊子；我想他们会咬我</p><p>安娜：蚊子不咬人，它们吸你的血</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3456857877788737/?__cft__%5B0%5D=AZUbFps-eHw2gIh8ORg_xcyAaqM2FXtbCKP9ydMDULNxyNJFyVJFP_2e_YITMWyPvqfUTv_0Bip2XUZ5XYR7wc87FeW5lwjEyD6dh2UzOtSPmEflevSh0795QQrSVqNPgI6T5ExuwgSSSbe1GC26c1kNT2INl0U3BEO5JIstLR85pD0ojzyB3Y_yw6vOeFIg1HY&amp;__tn__=%2CO%2CP-R">2021-05-05</a><p>莉莉：我想吃一些培根，但我是素食主义者</p><p>我：由你决定</p><p>莉莉：我要不再吃素了。</p><p> [吃一块]</p><p>莉莉：好吧，我现在又开始吃素了</p><p>...</p><p>安娜：“森林里最好的事情就是有很多动物，你可以捕捉并杀死它们。然后吃掉它们。这让你非常健康。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3459497774191414/?__cft__%5B0%5D=AZW7F8LiR5Jk_Qq_joUuEh3-cpGy2YZUcGstDw_38cN4kEji8voOKnz3jNp1mieEqbqTyH7GqTlWnCZ6WoDt097-2x5eHroMfgIFsiaB5GvclllaQR4kwzKNrL6AXB8gnJQP_vAfhxq8AYu4fNJVKH8oZQPkkpx4oTNDBJg69OVbdfQe4e5vOdC_AOa8XQQY7cI&amp;__tn__=%2CO%2CP-R">2021-05-08</a><p>我们刚刚玩了一场玩家别无选择的游戏，莉莉击败了安娜。莉莉对此感到非常难过，并试图让安娜感觉好一点。</p><p> “安娜，你赢了比赛，在我心里”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3463696547104870/?__cft__%5B0%5D=AZXAOaQSOTtmfStevaaNIUxY8z6tk61UTH-4HPU-hdy_qk2sMxj8AZ2CnFL9PRodlrf9yWdijiYdY2Iidta0iCzzORWWWB5Z0s9x_if2uX99lXpKFGejh2KsaNosOb2iIOzEMDdZHaB9Sl4gtpZpZY93DgaL4HkmLmGoaEj4-O4Y8UlXgslrkelZUVTyVjBkPtg&amp;__tn__=%2CO%2CP-R">2021-05-13</a><p>莉莉：“昨天我亲自回学校了！耶耶耶耶耶耶”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3465988893542302/?__cft__%5B0%5D=AZX9BYMKBwZ4m_Kz2y9CDrdvfgp80pza80lBc5QuafGSxLjSdzTEv0mWPozoh7r2_vSyl95S64vqfXbehoaB0wzr-rvlXpEDoJCUBHmZJZv6x0FQo5_ZVRlCtQd3DDULaYKfpFHtU6_eI1Xciqgidu32TeNvmcT-eQToUsljIYb-pGAD_uweyBCjmV00dRgmaII&amp;__tn__=%2CO%2CP-R">2021-05-16</a><p>今天孩子们装饰饼干的两集：</p><p>安娜：你是个傻瓜。</p><p>莉莉：什么是原发？</p><p>安娜：这是一个不会说话的痘痘。它为在你脚趾里生长的婴儿提供食物。</p><p>后来，孩子们伸手去拿桌子的同一部分，安娜为莉莉锦上添花。</p><p>莉莉：嘿！</p><p>安娜：抱歉，但这是你的错。</p><p>我：你不必说那部分。如果你撞到某人，你可以说：“哎呀，对不起。”</p><p>安娜：哎呀，对不起，莉莉。 ……但这是你的错。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3470715839736274/?__cft__%5B0%5D=AZVsGY-sZ14qVJS_8KaoyS8kgBXECi57v2THDc7dFB9NTjkg3JO8Y8CbIDUEgC3nXaapdZLuBKVUXneHG9yiwhiPXnMBmfWCSBcG34BFAa_vh3inLvPhExTFbYj3CvB3sV6MFHw8A1BkkEBWcCzA5llnlqKWA-YF9M4_Er7eCKzh8_7J2XxZqnaLv2AnQu4jd0k&amp;__tn__=%2CO%2CP-R">2021-05-21</a><p>孩子们制作了徽章以在不同的心情下佩戴。</p><p>安娜：“这是我快乐的徽章。这是我疯狂的徽章。这是我悲伤的徽章。这是我喜欢的大米徽章。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3476159479191910/?__cft__%5B0%5D=AZUa2Pxnx1YhYOQVCKHdepG6rZe6WX_RjEWQyYsxWq3OmGWjMMy9ME8euycNt0fqJrSJAyJgVSiSboJnpuJ2ZMxAenP-oFEBgSTLnoocMHjzPAokr0AGO54TGv5XnK5cQLWehS6QssyK3jPWjMCHkcZkdBV0nqyRw08QV-UACif3_eyD8P6wVbcx52Is8lTO37A&amp;__tn__=%2CO%2CP-R">2021-05-28</a><p>安娜：“妈妈，你没关我的灯电池！”</p><p>朱莉娅：“你房间里的灯实际上并不是用电池供电的，而且灯消耗的电量足够少，所以非常便宜”</p><p>安娜：“但是它会产生温室气体！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3481335735340951/?__cft__%5B0%5D=AZXLgIAc_aMqMTvYmQz5Jwx-wznPjYhoMCJE7izwFYX2_octtK5webNH-Xn2ocQWtIS8466lrD-3ZvniHxq5v1_Y_9qxpnkGTO3H7bBI2p289IXSFqrpesuJ9ukeyDmDejozOHDNS4YZ3ngFla5vIyF-N69PLYAChak6vj8g8U9k0O1wvNDUfr_1EhszM_u3dMU&amp;__tn__=%2CO%2CP-R">2021-06-03</a><p>我：安娜，这扇门上有红色蜡笔。</p><p>安娜：我不知道它是怎么到那里的。</p><p>我：我需要你把它清理掉。</p><p>安娜：（一分钟后，擦掉）楼下的墙上也有蓝色。</p><p>安娜：（三分钟后，把它擦掉）我只是想让它看起来像壁纸。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3487913761349815/?__cft__%5B0%5D=AZUq9BJ7nJeFtg32vS8Nv4kqzNrHZjsQF5UYT3t9L-GqKPPKj0T1U9Ri-zrOWhaXIZV4DSYlW1J062ihdPHwZoLrW8R8GO1w0gJL8a3eTbb624MGoJwUIeZW9ESTIFplsgZSecn1Kh8xkjHxA_ZIugpN6JOm0MpuDMtvHgqp-8dgDSsezLrk-TE3OYWJDymfinQ&amp;__tn__=%2CO%2CP-R">2021-06-12</a><p>安娜：如果大树林里的小房子在威斯康星州，劳拉和玛丽戴奶酪帽吗？还是妈妈或爸爸？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3492377694236755/?__cft__%5B0%5D=AZXFTAVM4JgBiC6cE1tbj_gzOnd4bqZPT9wxG86G3GdTMC8Ul8GPbOlYPEK_yatgrGX7D2hyVY3Hg_eclV11TSVn2Zd_CAU-oi8FDsqvRNYaKfhEhRG82aBgaUSNMNCkZZec4ge1VU267cOllTRA42mHDSjLV6RvOx0GTgToq_XfV9cVT-4v3JPJdqU3B2_Co-Q&amp;__tn__=%2CO%2CP-R">2021-06-17</a><p>莉莉，非常真诚地对诺拉说：你是我的小妹妹，我是你的大姐姐，无论发生什么，我都会永远在你身边。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3494771063997418/?__cft__%5B0%5D=AZXXtjyAQ0xtk-4qrzz10ZUEn-FEatPAqZ_XfQY5bX6oQb1zvqf-kUIVhjyJZClD7q57F28F-ty024WI4jIweuPD_OhTvm9GCRR2yOFSv9szsLeeTUDp5ITEXxN-TnIdyn-8jCIvp2FEeEB3B-T0ogwrTLn2-RtqliSFaBYJ1MTqEzKJJhpgd47VSzHLPf0FV3I&amp;__tn__=%2CO%2CP-R">2021-06-20</a><p>莉莉昨天：[唱了一首戏剧性的歌曲，讲述她感觉诺拉正在接管她的生活]</p><p>莉莉今天：“我希望家里最小的专家来跟我说晚​​安”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3496398123834712/?__cft__%5B0%5D=AZXM6ROl5xJo9Eq1N6sN_CyOJ5aMdFUhPDiS8Blo-F_xbuS8SSiD8foCBets9oi5quhysgcrBMuw3FXfd74oGB46UOcqYQz7VtV-HKSiG69TogwEwPGZ_XhVrPUimucNZe5vl3iuNRnwWJEhjrlYRf1kIL3o2H0HLC-Kk-UvDjwiVcVUOVG7ERrBq9f0vbJtGqU&amp;__tn__=%2CO%2CP-R">2021-06-22</a><p>莉莉：安娜向我扔了一个硬塑料球！</p><p>安娜：我只是向她扔了一个球，让她知道我的感受</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3499714696836388/?__cft__%5B0%5D=AZXNsSCufZWXevm74Pl1ceNXJHB9NIzhO-ir5DTH9WnR63R2yYyH1ESLNp0EmIsPfnYszByb20dET0xaMSFPsYzOQViRL6EdAndSkqg3nPtUNs719qi523lpBcwEndyj7pnbxk1I7UUWuXAKCjiMepd9X6qEFYKJ9GNljKaAQm1OV0FSCmk2mTuUgIyAZPtMkv0&amp;__tn__=%2CO%2CP-R">2021-06-26</a><p>莉莉病了，今天我们要让大孩子远离婴儿。两人都在门口徘徊，觉得很为难。</p><p>安娜：“昨晚我告诉诺拉关于细菌的事情，她决定我拥抱她就可以了。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3501177203356804/?__cft__%5B0%5D=AZV7ty2_IWH8Eq4PYbFuqxUNlBZbj9PUtT0uJ3xLFRgTJ1raRgijSSNKj8swWp-Q8jkozph4qlxQYmjtZausHlHCJ3PvB_wFnNH1Pmh7RDIdCfy7MN4eGsQmL1ah3i-gRpl-jeFCQWLNfvPQG_w2LguuPqKAdtaxzv-wob70VmZrK_jmBcyHbsNWPhPVBCfcHgY&amp;__tn__=%2CO%2CP-R">2021-06-28</a><p>安娜：“我不喜欢覆盆子和生奶油下面突出的圆形部分”</p><p>我：“你是说煎饼吗？”</p><p>安娜：“不，我喜欢煎饼，只是不喜欢底部。你能给我做一个只有巧克力片、覆盆子和鲜奶油的煎饼吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3506343262840198/?__cft__%5B0%5D=AZWjeuIJepzGzyFUGOxwb5mU916nVRnTVqD336LtxtjjT7qc88RhrwMX6P-SiE1r_ADbYvraOOTnIGkDi7kgBpiHl4ABYH6sALMbWkZv66UXFytIIUBSl1oWlsniTCnhuOZBLakniXEnf0dU8AjMT5YDWGW4oowbsVc9NExkHIsn9xgXelEL8B2PT_EA44NOuC0&amp;__tn__=%2CO%2CP-R">2021-07-04</a><p>阅读理解贝特西。安娜评论道：“弗朗西斯阿姨认为她是一个好父母，但她并不是一个好父母。她教贝特西害怕狗，并告诉她她永远不需要自己做任何事情，而且别人不需要做任何事。”永远都会为她做一切。”</p><p> “事情需要你自己做吗？”</p><p> “是的，有些东西，但其他东西太高了，我够不到，或者我不知道我们把东西放在哪里，然后有人可以提供帮助”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3508506349290556/?__cft__%5B0%5D=AZVF_xBvwSG82ef3YJdImNwzdgUh7fYl2k-rEIdXrGI0VRX_DghRloR4v_F8uh6jaxJD1UWfIhosVb34Rpzjx6xSFKy1zPMFTSaGCalOlCQNmXYi2tsV8di7fxnhGgfhXccmu9IJLAWD6dALU18TUSbrfUWHPbnbKs3rMzSQVD41lxdnQCapYPUKDEmw1BEmToM&amp;__tn__=%2CO%2CP-R">2021-07-07</a><p>安娜：“我的胃感觉很奇怪，就像我生病了一样”</p><p>我：“是不是因为你刚刚在公园玩了半个小时的回合？”</p><p>安娜：“不”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3509513339189857/?__cft__%5B0%5D=AZW0huLPm-MuFsxOpTPpjrvLaeObok7lcTvpppzbL6cKqvfHXdZ4KTGiYWH0dgyNPuP775NI4twUMJT3BOvglvK1RdcsAwwi3pesdyFCSTbLSAjWENWjJCiuHwkUz4pADfWWLkIcL15OAkZYZxcHICT7oy4WpXK1EK3Pqb9kAotb2Wz3N_GeEkm1CoZdK1MtSPo&amp;__tn__=%2CO%2CP-R">2021-07-08</a><p>安娜：我正在冒险拥抱。当你拥抱的人变得格外活跃和活跃时</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3517303495077508/?__cft__%5B0%5D=AZUlCMSVwADM7SvUq2Qo8XD0zoSg78-ML7bNoMqx_ipBUjAxzdYhcR84mHNyLhceJAds5ObVWUo18Z4vjRqHtdidbXeSK2l7r9V3efheqtxyNRRGl6lvaYOBMUV8SQaRDfBnmoXPbYgaAet8j1h81fMpL4fC1YErJd8ILI5YEXiye0LOa6FGie-n-jG-JJyq9ls&amp;__tn__=%2CO%2CP-R">2021-07-18</a><p>放下婴儿小睡后，这并不是您想在监视器上听到的内容：</p><p>安娜：[声音特别高]该叫醒诺拉了，我想和你一起玩！我们要玩这么多！诺拉诺拉诺拉！</p><p> [之后]</p><p>安娜：“我没有叫醒诺拉。我只是希望她能醒来并想和我一起玩。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3520067291467795/?__cft__%5B0%5D=AZWugKhpYD5INXGAFC6Hg0xsmq1Wtls1nmE9m6qYes9AFFa89WA_WXJ9MV2VQ-7SYJccDpKaBV0VyjrAzJ1PZK1LAM3RHKJZaiktnijpJmcjXd_nMbm1xiSrjq0CcRefoYozNFzobr07PzUrlToGwvpzRvFw0BpnCirompzQzffX6r7LpdxQt26NkxTna8bNR-w&amp;__tn__=%2CO%2CP-R">2021-07-22</a><p>安娜：种子是怎么来的？第一颗种子从哪里来？因为那个必须来自同一种植物的另一种。</p><p>莉莉：安娜，这可能会让你感到惊讶，但是：大爆炸。</p><p>安娜：我只是想让妈妈查一下。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3528072394000618/?__cft__%5B0%5D=AZUSsN4PcBsP4HfGcIzMWGClAP4ul6m60dupIukYUpH8DUttJWodcFvPbpUfFRIf6MW5Vc2PrLhZf_GsTXuSmhfy39FvoJwzuMHpeaBEfBZ6E5hU45L4LA6mHY8SkTTwkBV4eUx3zFXN_m_fzmng5rpwj5WlSHWKAOhr5KtKSwxBRQHsn63x8zzGRVqEjpRIhgc&amp;__tn__=%2CO%2CP-R">2021-08-01</a><p>来自安娜在依偎期间的声音：</p><p> “诺拉从头到脚都是婴儿做的。”</p><p> “诺拉就像一只鸭子，潜入水中寻找鱼、小鱼和面包屑，然后把它们从池塘底部吃掉，然后狼吞虎咽。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530156747125516/?__cft__%5B0%5D=AZWSoU1dsm7FbKG2tm3KBWM3sEZuddr2-3CvCkpV9ew473tx4LlqJoY0-2OTfrTk4p5MLM6SkHH0_MKc1W2WAeWVTS0HSrhQPh4dkQjVWZJl7V-cWgFKnpwlz5mtATGY4tD-ialSvQmP0E4SPsKTYVJwkGIQdWR5cj8lmJaBGKBFgr5NJMCSKh_WG9sMNj660zo&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>安娜对她最喜欢的主题的更多评论：</p><p> [向诺拉展示一些点的图片]“诺拉就像一位科学家看着化石并试图找出化石的来源。”</p><p> “诺拉像菠萝一样大。”</p><p> “诺拉的鼻子是由器官和皮肤制成的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530433907097800/?__cft__%5B0%5D=AZUHxXUTkh3aGaYcbBPfz2S80Khsxj9LlyLk3dy7Zkj8GR-rBtmhYNexWtwGe-ko2I02O02gRyYWKbfmSqnf6WmrdmE1Ocgk_g-XI6KlDYmVdYEwqeTcIfyAkR0xqbNGJ7vMXnDbgXZksaESWS5OECeiDSZEDg2ZZydiI06P_orEO0hbuevz19kuJGTSD5jNx4o&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>莉莉：“安娜，记住，一个女孩最好活在真理的话语下，而不是说谎。”</p><p>安娜：“好吧，我做到了”</p><p> （当我在墙上发现蜡笔时，我不再试图弄清楚谁需要清洁它，因为总是安娜。但莉莉直截了当地问她“是你做的吗？”所以安娜否认了一切。）</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3537226889751835/?__cft__%5B0%5D=AZWhCWdr4dnYISnM-cOL7Kzswjc-NXC4b4Es-Tm3ZAcLiTGBR_FfPa8kjLADnCDBxDfvOnUeFN6e3up6x4VSx70VWeaq9Vj1KgBAYZDLwYMpOnq2btCTSQ7jLy4P2PP3NrYfhtluS8dtosO6nmIfHI3PajXkKrgi3N_tTtaHwi8njokCGuJt34jRldaXuEE0dcc&amp;__tn__=%2CO%2CP-R">2021-08-12</a><p>莉莉：“安娜，你猜三个谜语怎么样，如果你全部猜对了，我就不再是狗了。准备好了吗？汪汪汪，汪汪汪汪汪汪，汪汪汪汪汪汪汪。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3539241072883750/?__cft__%5B0%5D=AZVGAaZJ3F1TrIcM16ssTUmKA52cRnZ5JDuoOhA92JK8BUUun-eldkxqJs3bCjumHkxO2IqcL8D_5aS07RtglNeQ8jc132AUbzzu01bHHMPKIO1963sl8pwBa2eL1gVLeyAiVgo5m_qDPFEz5UJqkQ4ZHDDK43cOFywpsKutXKI1wu32tLpdNXjOnuVbKh9RBdk&amp;__tn__=%2CO%2CP-R">2021-08-15</a><p>安娜：莉莉，你不用表现得那么厉害</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3540357286105462/?__cft__%5B0%5D=AZUAqUqEPBHmy2kXOASeva-f5BXotVHdWJai_u9ph01-jCBYUNBsAt_ZyWbuFcLW4Gox2kkUrczzAfx6TZW1qZlFGe_uQ1_hKkmd4UYD-tTuk8RWnSe5fdc-fbrNf67AjgXJQE9SQ-zvXOb86zMjEPIQ6a54XkzqrzlJeyDdAvXD8sKAtFcdr9XjqTTT87ABRJA&amp;__tn__=%2CO%2CP-R">2021-08-16</a><p>安娜：[关于诺拉脚趾的长篇独白的一部分]……这是最好的脚趾。</p><p>我：是什么使它成为最好的脚趾？</p><p>安娜：因为当你到达那里时，那里有闪亮的灯光和一张大桌子和漂亮的椅子。桌子很漂亮，上面还挂着一盏枝形吊灯。有一条隧道从她的头顶一直延伸到她的腿和脚趾，这就是你到达那里的方式。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3543389482468909/?__cft__%5B0%5D=AZWe4lueZTWmAPH8QC3DeLmBVBlFEsCb7er-EFleSfz6dRe2Tzrs1JYBWjyK4YCuvENsbaBszQ9klWP7txepSyc14fnzHePAHwE_tUZMdu2GoL9JuJTKAQcYG0ZBRTf1NKW9YaAbFXuhjmry03gxnqJl3xSqikcutsEKFJLnKqu9gjxBmTwGZF_CrwLmmiguRxs&amp;__tn__=%2CO%2CP-R">2021-08-20</a><p>我：这个周末我们应该在飓风中航行吗？</p><p>安娜：不会。因为会有白色的帽子，它们会进入船内，从而使船沉到我们的脚触不到底部的地方，而我们的头却露出水面。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3577388495735674/?__cft__%5B0%5D=AZUThmE4R-t1q6U03sO5vQAtCTmD67ZIcRjWG8LkLxHnliYB9dKo5aIrIswadLhpcUbGkTmO2ArKhW8zPgcAfdPF4oUPZFWLPyssFczcWF0D0Hd8WigA-9BLw5tlg0WbMKr6jgc5kY8kBwN-nwUrTbKlKSSPaCheeG0OjcWl8s70Idhm6cLcKSEdizGCJJ1Y8Kg&amp;__tn__=%2CO%2CP-R">2021-10-03</a><p>莉莉：对我来说，得到诺拉的拥抱就像得到一份礼物一样好</p><p>安娜：对我来说，得到诺拉的拥抱就像[被诺拉分散注意力]一样好，就像诺里诺里可爱的诺里一样！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3582792991861891/?__cft__%5B0%5D=AZV_u40tpKdn9Cb_wTsuZSEn_ARasNSnLv7o89MGVnc8KDNt9GQFVkOkdAc-Y0zUJTDTVOE959sfgnuUVRmOtjpo0R1FM40zwYdfkBFYdMNEgsYTqrMcqQS-eoEWiHo_W845bZSPAgOMKSYcyj0tFJl-hh8QaRL9ivt1xg6HHU_4HTaHKk8KIUcmnzZ49q5TvdM&amp;__tn__=%2CO%2CP-R">2021-10-21</a><p>安娜：“当你不知道如何阅读时，有一个回文名字非常方便。因为这样你就不会遇到任何问题。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3587586118049245/?__cft__%5B0%5D=AZU5uE8Li6RIEDIDpDRV6fY4k7HwI2A_iFGts17AUPw1E48KNEcIXUPkXya3xj7Q4O7P8kiXF0vjDUghTIHyEsudGuR_nVhZrAg3ptL-SlRrObr6zrzVHNY3A86p-pKP2Sax5CeX3oJfeDXD7wb2KDjisUcTROL99zCbCNYDTuYnWLbHsKxs3agGAoPkPL-lo8s&amp;__tn__=%2CO%2CP-R">2021-10-16</a><p>安娜：我不是鸡做的，我是人肉做的</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3592987174175806/?__cft__%5B0%5D=AZUIBVHZmvo0IQIcJ21JF7nQg0bO3alZe85atgUGeVkh35TE_pui1IQKklBmuuQdOzURAjte49pAlick0L2QmM8kX5KNV9u3PtnfaTubrLvkm37e286ROzxoJeFg33Rnq8buAMjMlDVVyKmtkPGqZiOhEo3_I0WAHZQtGD4AYMgPhIlGm1Ro-HEKJPq3JVH0XsI&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>我：[把泡打粉放进煎饼里]</p><p>莉莉：你把那些东西放进我的食物里了吗？</p><p>杰夫：是的……</p><p>莉莉：你总是把它放进去吗？</p><p>杰夫：是的，它是发酵粉，它使它们蓬松</p><p>莉莉：爸爸！你知道我是个素食主义者！</p><p>杰夫：“发酵粉”，喜欢用来烘烤。它不是由培根制成的。</p><p>莉莉：噢，好的。我还以为是猪呢</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3593018160839374/?__cft__%5B0%5D=AZUfOL24oqCJo3xNQAQO4ScYdDHT1e8b2mhjWnkN66u1kw_cDtIjidGdUvFOd-t6hI7hLY9Fg7bQRuQu9a88SSBt7-DIZtj4ygmw_T5AOdcGNny2eJiRzgCr9yxQHunF97ff0aYoh9YD2B6sVHdAq_R3sWSZm4qfKZZ6v7VYUuo74tqqvnkXdHc2pIB-G5rlWIw&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>安娜：当鸭嘴兽不知道婴儿的名字时，它们会称婴儿为“宝贝小姐”，无论其性别如何。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3593879140753276/?__cft__%5B0%5D=AZVQnhwhIxfPv-iHICzveLmf-Ce68DFPxYVzdNOlz_UM1TTFC_kjwEmlCikUdf5qcFkeJPROIMSUBm143uvgVMrAAv5CRvH9-Q7xTkmw0XR2iqHojwNSmmGmYQXZASw26saAuP4I_dzcg9O-Yl7ctDAlfmmukTbdg4wor3i-A5CyxCJRRCSqRGn62zJ3irUI6Wk&amp;__tn__=%2CO%2CP-R">2021-10-24</a><p>安娜：“你不用催我，杰菲！那只会让我压力更大，而我已经有 20% 的压力了！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3602271109914079/?__cft__%5B0%5D=AZWtG1RQHyZBUjMZGdYn7FRzji5XXsKM4XS1v6BwVDF-7qQWm-7J9Euho4Yjl6CLflT7aXULVJP35kRDCQmpmDbmiNnBuqK92N8R8Uy-1D_CYrRwZ4i_pBCmu5Uor-CROD3ULGmzY94K0fqTXLJ594GZCV2jov9Iv253Iysbfnu9jK-M4IJ4fJJzJ7S1mUk2A3w&amp;__tn__=%2CO%2CP-R">2021-11-03</a><p>安娜：我永远不想成为青少年。我想快点到20岁。</p><p>我：你为什么不想成为青少年？</p><p>安娜：我不想对人刻薄。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3607761386031718/?__cft__%5B0%5D=AZV-UPZr16prWgQ1Jy8PNHeA065kgZeGK-244dIy9ayURuaxgyJgzcQc60Y2u3TuK1S2K8A26rnLeKT4sjzjRSBfmMiWoVpVAKICSj4YvSKAweTogs6SIRJ9LetHoYKRcsDG4aUhvDJUp8RyPM1rWjhCSXD8EBZcpTWt5_dsCZauz0tyB5l1bjM_Ya0dAOVDnZM&amp;__tn__=%2CO%2CP-R">2021-11-10</a><p>莉莉：这个有多甜？</p><p>杰夫：你觉得怎么样？</p><p>百合：一整颗甜</p><p>杰夫：好的。这是否意味着你不想要妈妈做的苹果派？</p><p>莉莉：实际上是甜的3/4</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3610288725778984/?__cft__%5B0%5D=AZWhDB-W6_T4tQOXNHsUpnYaEyORAPpl8fHQvAgH7rGgFxYmOSru8hIIdusWi0AjNgym0RUagq6IqOUYfbowz_x6UoNQluAkOWZcXOtPnNGubWJXl4xmPWK7uX6NIU8UkwUYHRxzA8bdwAqqDKcRq_ru6itTO8DAww7U-9dpcb8NuWG0em_eUrGUnD34CtttvPk&amp;__tn__=%2CO%2CP-R">2021-11-13</a><p>安娜：你违反了规则。我说过我的房间里不要有任何触摸的东西。</p><p>莉莉：你违反了规定！你正在触摸东西。</p><p>安娜：不，因为我是政府。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624125344395322/?__cft__%5B0%5D=AZWv03pTHnberopa9z42t1Yg85AvHZdwsY3fz7Nw_4Ris1jblfBURWCjucXurbF1HTb9j_W-SPR8kZZcE286N33ogox4-oQdf2UGxvc2HiRNxkseyzcb94HP_MJA7lhhaqdPr9AKBmU9mGh2UEiSTvZ4YCcoDwgbdZRQZwvz8pj1ucePUfsrXySqMJZDkR8hVtM&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜度过了一个感觉一切都不对劲的下午。我给她买了点零食，下了楼，我听到她在桌边开始哭泣。然后楼下传来脚步声。</p><p>安娜（出现在门口）“青少年数字……他们没有从正确的位置开始。十三？！没有一个青少年！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624185301055993/?__cft__%5B0%5D=AZWzbX_7bpUNIvVz1FBnc4Pf1eBlVJBufMFFSm_N-ZgE5Ym6UpwTBza1DHc3zmVSImKdKIo0UPqGpazYN0C6GnXymkvq_WovCbtYjJQXACTs_9V4O9kS8zddd915DEIAodWmOb43leJw5-JqHwEvtZHu0eNtyS92d2kPsqXbbgL1yz0J1sRyiHteLZlLQ58sh0w&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜：当我掉了一颗牙，把它放在枕头底下时，你为什么不派一个电动娃娃去取呢？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3626440237497166/?__cft__%5B0%5D=AZXlP_hXWWZflFNSmXkEcBtxRfT2Ck4tMNOI2lwpMq1BcRDfkRMjbSmgEqnLViyOe_ZA-st9ouMEAS6dkwfdOBhggPs1BI-EEYJ9z8SMFgwo300D_nhwu_QmTIpWOuXBTsDMA8a11kKk7fFD53jkd_LR1ZUphmmd-L4Jk6wPH3FWCXmMI6JRJihUg5rS7cDzJnI&amp;__tn__=%2CO%2CP-R">2021-12-04</a><p>安娜过去只是把东西放在床上。现在通常有更多的叙述。她把东西塞进我的被子里，低声说：“……它们会安全、干燥……不会有任何掠食者。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3630211270453396/?__cft__%5B0%5D=AZXp95DEG99Xglo7uc85bmR_Ycb-nzfLgDUoignWmf6ZVMYVv1TAbxz98A4IfWRMqDOYUflysNzZMbhNuW3zvb7yVYt3szapQqHJET3gd6YoihEbIQt_1RjtGQTnylkFq1vaJf9jGClsF3IT7Fb83Ctdp3LULjbEnBi-6h5s4306X92Vj6nfFBkAXrbI-QAUB8I&amp;__tn__=%2CO%2CP-R">2021-12-09</a><p>我问安娜为什么拍了这么多吊扇的照片。她说要拿给诺拉看，因为诺拉喜欢看吊扇。这是真实的！她并不总是那么体贴，但这是一个很好的心理理论时刻。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3632679913539865/?__cft__%5B0%5D=AZWezPZGduHlshG-rr93b_0YptAO-yCyoUGrzynotQkWiP8OwP0z3E6DuWmpC7WUL54Gv15jZjSahkQ2UqLCItj0IVYrBJZN5c0VMyGpoRZg61XUZ8lPYgcV7-RWfKD5ixT76B-47AUSf0g38tQ8p-3DuMR5i9S4GXW8zmAVAVzABscIAwcJpgss6fzoqri-GFo&amp;__tn__=%2CO%2CP-R">2021-12-12</a><p>莉莉在过夜结束时对她的表弟说：“总的来说，过夜进行得怎么样？下次还有哪些部分我可以做得更好吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3641695665971623/?__cft__%5B0%5D=AZX8pWL8w96MWH6f-Wi3Lxz7nX2BseX5ZbfKLTwhg9Bg1GuioDOE5-XbZDp7OjhUkwCetMUjd_NA0WyJ8VlqZFoN8z2_0c4WgQS047vXZt_OLZizEqqsJXYWPo454BlfWWIJDfgHPv0Wt3boX8kiskukICSfxAiSHYal6SwRRQzhFysA_yRIF-9T5JXiFXhaTpk&amp;__tn__=%2CO%2CP-R">2021-12-24</a><p>安娜：“雪最棒的一点就是你可以用它做各种各样的事情。你可以用它铲土，你可以用它堆雪人，你可以用它拉雪橇。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3643547652453091/?__cft__%5B0%5D=AZWfNNGuADRIqmzOrrwG1G3_5cnFYto0bFX6EPKjRwtLg3CBgUAtFO3vEFjkE1v3yNdn3BVeTJ6lrp3ivYqFEIK7TTYWWLiZt34VO0Kn0hZAtc9YJEJTSAnMrS4oSGkwMyuCN4gfKpGSJRf5he-87yifsWdrE0DvjRMtzqdEnBm6XkmzZScQ4Y8OIZtKQj13-jI&amp;__tn__=%2CO%2CP-R">2021-12-26</a><p>今晚，在表弟的恶作剧中，安娜的眉毛被击中，险些需要缝针。她现在已经痊愈了，到了就寝时间，她对此非常有哲理：</p><p> “我认为这是我一生中发生过的最糟糕的事情。但还有更糟糕的事情可能发生在我身上：掉进火山里。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021<guid ispermalink="false"> xmDzYvasaegWvFWtx</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:50:34 GMT</pubDate></item></channel></rss>