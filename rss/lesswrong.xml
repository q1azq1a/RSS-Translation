<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 5 日，星期二 02:26:54 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Accelerating science through evolvable institutions]]></title><description><![CDATA[Published on December 4, 2023 11:21 PM GMT<br/><br/><p><i>这是向圣达菲研究所“加速科学”工作组提交的演讲的书面版本。</i></p><p>我们来这里是为了讨论“加速科学发展”。我喜欢从历史的角度来开始讨论这样的话题：科学在过去什么时候（如果有的话）加速了？现在还在加速吗？我们可以从中学到什么？</p><p>我认为，在整个人类历史中，科学以及更广泛的人类知识<i>一直</i>在加速发展。我还不能证明这一点（而且我自己对此只有大约 90% 的把握），但让我诉诸你的直觉：</p><ul><li><a href="https://en.wikipedia.org/wiki/Behavioral_modernity">从行为上看，现代人类</a>已有 50,000 多年的历史</li><li>文字只有大约 5000 年的历史，因此在人类时间线的 90% 以上，我们只能积累能够适应口头传统的知识</li><li>在古代和中世纪世界，我们只有少数几门科学：天文学、几何学、一些数论、一些光学、一些解剖学</li><li>在科学革命之后的几个世纪（大约 1500 年代至 1700 年代），我们得到了日心说、运动定律、万有引力理论、化学的起源、细胞的发现、更好的光学理论</li><li>在 1800 年代，事情真正开始发展，我们有了电磁学、原子理论、进化论、细菌理论</li><li>1900 年代，核物理、量子物理、相对论、分子生物学和遗传学继续蓬勃发展</li></ul><p>我把自 1950 年左右以来科学是否已经放缓的问题放在一边，我对此没有强烈的看法。即使确实如此，这也只是整个历史加速的总体模式中最近的一个小插曲。 （或者，你知道，历史上前所未有的逆转和衰退的开始。其中之一。）</p><p>我对这种加速模式深信不疑的部分原因是，加速的不仅仅是科学：几乎所有衡量人类进步的指标都显示出相同的趋势，包括<a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia?yScale=log">世界 GDP</a>和<a href="https://ourworldindata.org/grapher/population?yScale=log&amp;country=~OWID_WRL">世界人口</a>。</p><p>是什么推动了科学的加速发展？许多因素，包括：</p><ul><li><strong>资金。</strong>曾经，科学家必须<a href="https://rootsofprogress.org/funding-models-for-science-and-innovation">寻求赞助，或者独立致富</a>。现在有可用的赠款，并且资金总额在过去几十年中大幅增加： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/dhxfk91oznfay66wdxnn" alt=""><figcaption><a href="https://www.aaas.org/programs/r-d-budget-and-policy/historical-trends-federal-rd"><i>美国科学促进会</i></a></figcaption></figure><ul><li><strong>人们。</strong>更多的科学家（在其他条件相同的情况下）意味着科学发展得更快，科学家的数量急剧增加，这既是因为总体人口的增长，也是因为更多的劳动力进入研究领域。在<a href="https://archive.org/details/sciencesincebaby0000pric/page/107/mode/1up?view=theater"><i>《自巴比伦以来的科学》一</i></a>书中，德里克·J·德·索拉·普赖斯 (Derek J. de Solla Price) 表示，“历史上大约 80% 到 90% 的科学家现在还活着”，这<a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/">可能仍然是正确的</a>： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/wh3ldfdjulipzfcrroyg" alt=""><figcaption> <a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/"><i>埃里克·加斯特弗兰德</i></a></figcaption></figure><ul><li><strong>仪器。</strong>更好的工具意味着我们可以做更多更好的科学研究。伽利略有一个简单的望远镜。现在我们有<a href="https://en.wikipedia.org/wiki/James_Webb_Space_Telescope">JWST</a>和<a href="https://en.wikipedia.org/wiki/LIGO">LIGO</a> 。</li><li><strong>计算。</strong>更强的计算能力意味着更多更好的数据处理方式。</li><li><strong>沟通。</strong>思想传播得越快越好，科学传播就越高效、越有效。科学期刊是在印刷机发明之后才发明的。互联网支持预印本服务器，例如 arXiv。</li><li><strong>方法。</strong>更好的方法造就更好的科学，从培根经验主义到<a href="https://en.wikipedia.org/wiki/Koch%27s_postulates">科赫假设</a>再到<a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">随机</a>对照试验（实际上是所有统计数据）。</li><li><strong>机构。</strong>实验室、大学、期刊、资助机构等共同构成了一个支持现代科学的生态系统。</li><li><strong>社会地位。</strong>科学越受到尊重和声望，就会有越多的人和金钱流入它。</li></ul><p>现在，如果我们想问科学是否会继续加速发展，我们可以思考哪些驱动因素将继续增长。我建议：</p><ul><li>只要世界经济持续增长，科学经费就会继续增长</li><li>仪器、计算和通信将随着技术的发展而不断改进</li><li>我认为方法没有理由不继续改进，作为科学本身的一部分</li><li>科学的社会地位似乎相当强大：它是一个受人尊敬和享有盛誉的机构，获得了一些社会最高荣誉</li></ul><p>从长远来看，如果<a href="https://ourworldindata.org/grapher/comparison-of-world-population-projections">世界人口像预计的那样趋于稳定</a>，我们可能会耗尽继续扩大研究人员基础的人员，这是一个潜在的问题，但不是我今天的重点。</p><p>最大的危险信号是我们的科学机构。制度影响所有其他因素，尤其是资金和人才的管理。今天，元科学界的许多人对我们的机构感到担忧。常见的批评包括：</p><ul><li><strong>速度。</strong>获得资助很容易需要 12-18 个月的时间（如果你幸运的话）</li><li><strong>高架。</strong>研究人员通常将 30-50% 的时间花在资助上</li><li><strong>耐心。</strong>研究人员认为他们需要定期展示结果，并且不能走一条可能需要多年才能得出结果的道路</li><li><strong>风险承受能力。</strong>赠款资金倾向于保守的、渐进的建议，而不是大胆的、“高风险、高回报”的计划（尽管<a href="https://commonfund.nih.gov/highrisk">做出了相反的努力</a>）</li><li><strong>共识。</strong>一个领域可能会过快地集中于一个假设并修剪替代的研究分支</li><li><strong>研究员年龄。</strong>随着时间的推移，赠款的趋势是拨款给年龄更大、更成熟的研究人员</li><li><strong>自由。</strong>科学家缺乏完全自主地指导研究的自由；赠款资金附加太多条件</li></ul><p>现在，作为一名前科技创始人，我不禁注意到，在营利性风险投资领域，大多数问题似乎都得到了缓解。筹集风险投资资金相对较快（通常一轮融资会在几个月内完成，而不是一年或更长时间）。作为创始人/首席执行官，我花了大约 10-15% 的时间筹款，而不是 30-50%。风险投资公司大胆下注，积极寻求逆向立场，并支持年轻的新贵。他们大多给予创始人自主权，也许会在董事会中占据一席之地以进行治理，并且只有在表现非常糟糕时才会解雇首席执行官。 （上面列出的初创公司创始人可能还会抱怨的唯一问题是耐心：如果你的钱用完了，你最好能取得进展，否则你在下一轮融资时就会遇到困难。）</p><p>我不认为风险投资界在这些方面做得更好，因为风险投资家比科学资助者更聪明、更有智慧或更优秀——但事实并非如此。相反，风险投资家：</p><ul><li>争夺优惠（并且真的不想错过好优惠）</li><li>从长远来看，成功或失败取决于其投资组合的表现</li><li>在大约 5-10 年内看到这些结果</li></ul><p>简而言之，<strong>风险投资面临着进化压力。</strong>他们不能陷入明显的不良均衡，因为如果这样做，他们就会在竞争中落败并失去市场力量。</p><p>证明这一点的是风险投资在过去几十年里<i>的</i>发展——主要是朝着为创始人提供更好待遇的方向发展。例如，早期阶段存在较高估值的长期趋势，这最终意味着较低的稀释度以及权力从风投向创始人的转移：创始人在过去的几年里放弃公司一半或更多的股份是很常见的。第一轮融资；最后我检查了一下，大约是 20% 或更少。风险投资并不总是资助大学刚毕业的年轻技术人员。曾经有一段时间，他们倾向于青睐更有经验的首席执行官，或许还拥有 MBA 学位。他们并不总是支持创始人领导的公司；曾经，创始人在最初几年后被解雇并由专业首席执行官取代的情况很常见（当 A16Z 在 2009 年推出时，他们大肆宣扬<a href="https://a16z.com/why-we-prefer-founding-ceos/">他们不会这样做</a>）。</p><p>所以我认为<strong>，如果我们希望看到我们的科学机构</strong><i><strong>得到改进</strong></i><strong>，我们需要考虑它们如何</strong><i><strong>发展</strong></i><strong>。</strong></p><p>我们的科学机构的发展程度如何？不是特别的。当今大多数科学组织都是大学或政府部门。尽管我很尊重大学和政府，但我认为任何人都必须承认它们是我们行动较为缓慢的机构之一。 （大学尤其具有极强的弹性和抵抗力：例如，牛津大学和剑桥大学的历史可以追溯到中世纪，经历了帝国的兴衰，直到今天仍然完好无损。）</p><p>科学资助机构的进化所面临的挑战与风险投资的进化相反：</p><ul><li><strong>他们往往缺乏竞争，</strong>尤其是 NIH 和 NSF 等集中式联邦机构</li><li><strong>他们缺乏任何真正的反馈循环</strong>，在这种循环中，资助者的资源是由过去的判断和其投资组合的成功决定的（迈克尔·尼尔森多次<a href="https://twitter.com/michael_nielsen/status/1451626771690897408">指出</a>，从“爱因斯坦作为专利职员做了最好的工作”到“卡塔林·卡里科”的资助失败）在获得诺贝尔奖之前被拒绝授予资助和终身教职”似乎甚至没有引发相关机构内部的反思过程）</li><li><strong>他们需要很长的周期</strong>才能了解其工作的真正影响，而这种影响可能需要 20-30 年才能显现出来</li></ul><p>我们如何提高科学经费的可进化性？我们应该思考如何改善这些因素。我没有什么好主意，但我会抛出一些不成熟的想法来开始对话：</p><p><strong>我们如何增加科学资助的竞争？</strong>我们可以增强慈善事业的作用。在美国，我们可以将联邦资金转移到州一级，设立五十个资助者而不是一个。 （国家农业实验站就是一个成功的例子，这些实验站之间的竞争是杂交玉米研究的关键，这是 20 世纪农业科学最伟大的成功之一。）在国际层面，我们可以支持对科学家更加开放的移民。</p><p><strong>我们如何创建更好的反馈循环？</strong>这很困难，因为我们需要某种方法来衡量结果。实现这一目标的一种方法是将资金从预期赠款转向各级各种回顾性奖项。如果这个“经济”足够大和强大，这些成果就可以被金融化，以创建一个动态的、有竞争力的融资生态系统，并具有适当水平的风险承担和耐心，经验丰富的退伍军人与年轻特立独行者之间的适当平衡等.（ <a href="https://forum.effectivealtruism.org/posts/r7vmtHZKuosJZ3Xq5/altruistic-equity-allocation">影响证书</a>，例如<a href="https://protocol.ai/blog/hypercert-new-primitive/">超级证书</a>，可以成为该解决方案的一部分。）</p><p><strong>我们如何解决反馈周期长的问题？</strong>我不知道。如果我们不能缩短周期，也许我们需要延长资助者的职业生涯，这样他们至少可以从几个周期中学习——这<a href="https://rootsofprogress.org/how-curing-aging-could-help-progress">是长寿技术的潜在好处</a>。或者，也许我们需要一个科学资助者，它可以极快地学习，可以消耗大量有关研究项目及其最终结果的历史信息，永远不会忘记其经历，并且永远不会退休或死亡——当然，我想到的是人工智能。关于人工智能支持、增强或取代科学研究人员本身的讨论很多，但人工智能在科学领域的最大机会可能是在资金和管理方面。</p><p>我怀疑资助机构会在这个方向上走得太远：它们必须自愿接受竞争、加强问责并承认错误，而这种情况很少见。 （看看现在那些因卡里科获得诺贝尔奖而获得功劳的机构，他们几乎没有为她提供支持。）如果机构很难进化，那么元进化就更难了。</p><p>但也许资助者背后的资助者，即那些向资助者提供预算的资助者，可以开始将资金分配给多个机构，以要求绩效指标，或者干脆转向上述回顾性模式。这可以提供所需的进化压力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9uEAwHhoDS8shpdoX/acceleating-science-through-evolvable-institutions<guid ispermalink="false"> 9uEAwHzoDS8shpdoX</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:21:35 GMT</pubDate> </item><item><title><![CDATA[Speaking to Congressional staffers about AI risk]]></title><description><![CDATA[Published on December 4, 2023 11:08 PM GMT<br/><br/><p> 2023 年 5 月和 6 月，我（Akash）与国会工作人员就人工智能风险举行了大约 50-70 次会议。我一直想写一篇文章来反思这次经历和我的一些收获，我认为这可能是 LessWrong 对话的一个好话题。我看到他们<a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1?commentId=L2qFjT8taEhkm4hCB">提出要与人们进行 LW 对话</a>，于是我伸出了援手。</p><p>在这次对话中，我们讨论了我如何决定与工作人员聊天、我在华盛顿的初步观察、有关国会办公室如何工作的一些背景、我的会议是什么样的、我学到的教训以及关于我的经历的一些杂项。</p><h2>语境</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:14:27 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:14:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>嘿！在您的留言中，您提到了一些与您在华盛顿的经历相关的主题。</p><p>我认为我们应该从您与国会办公室谈论人工智能风险的经历开始。我很有兴趣了解更多；似乎没有太多公共资源来说明这种外展活动是什么样的。</p><p>那是怎么开始的？是什么让你想这么做？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:23:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:23:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>2023 年 3 月，我开始在<a href="https://www.safe.ai/">人工智能安全中心</a>从事一些人工智能治理项目。我的一个项目涉及帮助 CAIS 响应<a href="https://www.ntia.gov/issues/artificial-intelligence/request-for-comments">NTIA</a>发布的关于人工智能问责制的评论请求。</p><p>作为这项工作的一部分，<strong>我开始思考一个好的前沿人工智能监管框架应该是什么样子。</strong>例如：如果我可以为前沿人工智能系统建立许可制度，它会是什么样子？它会被安置在美国政府的什么地方？我希望它评估哪些信息？</p><p><strong>我开始想知道实际的政策制定者会对这些想法有何反应</strong>。我也很好奇更多地了解政策制定者如何考虑人工智能灭绝风险和灾难性风险。</p><p>我开始询问人工智能治理领域的其他人。绝大多数人（根本）没有与国会工作人员交谈过。一些人有与员工交谈的经验，但没有与他们谈论人工智能风险。很多人告诉我，他们认为与政策制定者的接触非常重要，但却被忽视了。当然，也存在下行风险，所以你不希望有人做得不好。</p><p>在咨询了 10-20 名人工智能治理人员后，我询问 CAIS 我是否可以去华盛顿并开始与国会办公室交谈。目标是（a）提高对人工智能风险的认识，（b）更好地了解国会办公室如何考虑人工智能风险，（c）更好地了解国会办公室的人们有哪些与人工智能相关的优先事项，以及 (d) 获取有关我的 NTIA 评论想法请求的反馈。</p><p> CAIS 批准了，我于 2023 年 5 月至 6 月去了华盛顿。需要澄清的是，这不是 CAIS 告诉我要做的事情——这更像是 CAIS 意识到正在发生的“阿卡什事件”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:26:38 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:26:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>哇，这真的很有趣。几个随机问题：</p><blockquote><p>当然，也存在下行风险，所以你不希望有人做得不好。</p></blockquote><p>一个人怎样才能把一件事做得不差呢？如何学习与政策制定者互动？<br><br>另外，你的背景是什么？在此之前您做过政策方面的工作吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:31:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:31:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>是的，很好的问题。我不确定最好的学习方法是什么，但我尝试过以下一些方法：</p><ul><li>与有与政策制定者互动经验的<strong>人交谈</strong>。询问他们说什么、他们发现什么令人惊讶、他们犯了什么错误、他们注意到什么下行风险等等。</li><li><strong>看书</strong>。我发现<a href="https://www.amazon.com/Master-Senate-Years-Lyndon-Johnson/dp/0394720954">参议院议长</a>和<a href="https://www.amazon.co.uk/Act-Congress-Americas-Essential-Institution/dp/0307744515">国会法案</a>特别有帮助。我目前正在阅读<a href="https://www.amazon.com/Devils-Chessboard-Dulles-Americas-Government/dp/0062276174">《魔鬼的棋盘》，</a>以更好地了解中央情报局和情报机构，到目前为止，我发现它内容丰富。</li><li>与你已经认识的政策制定者<strong>进行角色扮演</strong>，并要求他们提供直率的反馈。</li><li>在风险较低的会议中<strong>进行练习</strong>，并利用这些经验进行迭代。</li></ul><p>在此之前我没有做过太多政策方面的事情。在大学里，我为《哈佛政治评论》撰稿，并参与了政治研究所的工作，但这比“现实世界的政策参与”的内容更具学术性。</p></div></section><h2>抵达华盛顿特区并进行初步观察</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:32:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:32:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这一切都是有道理的。到达华盛顿后你做了什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:37:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:37:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我给国会办公室以及一些行政部门的人员发送了冷电子邮件。我还联系了华盛顿的一些 EA。我还继续处理 NTIA 的评论请求（截止日期为 6 月 6 日）。</p><p>最初的计划是召开几次会议，评估会议的进展情况，如果我认为进展相当顺利，则再召开更多会议。</p><p>总的来说，我最终与国会工作人员举行了大约 50-70 次会议（以及一些与智库人员和行政部门机构人员的会议，但我将在这篇文章中重点讨论与国会工作人员的会议）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:37:28 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:37:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>我认为他们进展得相当顺利，那么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:43:42 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:43:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>据我说，是的！我要注意的一件事是，这些可能有点难以评估——比如，员工应该对人友善，他们不会说“我以为你是个白痴”或“你浪费了我的时间”之类的话。时间”或“我现在对人工智能安全性的印象更差了。”</p><p>记住这一点：</p><ul><li><strong>我对员工们的开放态度感到惊讶。</strong>奥弗顿之窗最近发生了很大的变化，但当时，我真的不知道人们是否会说“哈！<i>灭绝风险？</i>这听起来像科幻小说。”</li><li><strong>主导氛围是“人工智能非常重要，我是一名忙碌的员工，有 100 个优先事项，所以我没有时间了解它。我</strong>真的很高兴能与能够告诉我有关人工智能的东西的人交谈– 我一直渴望跟上进度。”</li><li><strong>员工们对有机会见到愿意回答有关人工智能基本问题的人表示非常感激</strong>（例如，什么是大型语言模型，它与其他类型的人工智能有何不同？有多少公司从事前沿人工智能？）</li><li>有一些“切实”的信号表明事情进展顺利。例如，一些工作人员将我介绍给他们认识的其他人，一些人将他们办公室正在起草的工作发给我，还有一些人甚至将我介绍给国会议员（总共两个）。</li></ul></div></section><h2>国会办公室的层级</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:44:47 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:44:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这真的很有趣！<br><br>顺便说一句，您能给我描绘一下国会办公室的人员配置等级吗？例如，您通常与谁交谈，他们通常与国会议员有什么关系？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:50:33 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:50:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>好问题！因此，我的理解是，国会办公室通常具有以下角色，从“最有影响力”到“最没有影响力”列出：</p><ul><li>参谋长</li><li>立法主任</li><li>立法助理</li><li>立法通讯员</li><li>实习生和研究员</li></ul><p>还有一些其他角色，但从立法角度来看，这些角色往往最为重要。</p><p>请注意，每个办公室都有自己的氛围。有人曾经告诉我“每个国会办公室都是自己的初创企业，每个国会议员都可以按照自己的意愿管理自己的办公室。”</p><p>因此，在某些办公室，实习生和研究员实际上可能有很大的影响力（例如，如果国会议员或立法主任信任实习生是特定主题的主题专家）。但总的来说，我认为这种层次结构很常见。</p><p>我想我主要是与立法助理/立法通讯员级别的人交谈。我还与几位立法官员进行了交谈。</p></div></section><h2>外展到办事处</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:51:24 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:51:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>好吧，这一切都有道理。那么，您是如何从几次会议增加到 60-80 次的呢？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:55:30 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:55:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我向技术政策工作人员发送了一封群发电子邮件，回复人数给我留下了深刻的印象。这封电子邮件相当短，提到我在 CAIS，用 1-2 个要点介绍了 CAIS 的工作，并用要点说明了我正在处理 NTIA 的评论请求。</p><p>我认为国会工作人员现在确实对人工智能内容非常感兴趣。就像，如果我向人们发送电子邮件讨论其他问题，我认为我不可能召开这么多会议。</p><p>有人感觉“人工智能现在很热，但没有人真正了解人工智能”。我认为目前还不清楚这种情况会持续多久（尤其是“人们了解不多，办公室还没有下定决心”）部分。</p><p>我什至会说“我认为这是一个 AIS 社区作为一个整体可以/应该充分利用的机会”。比如，国会工作人员曾经（而且我认为仍然）对与人们讨论人工智能问题非常感兴趣——很难想象还有比这更好的机会让 AIS 社区的人们能够进来并担任顾问/倡导者。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:20:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:20:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这就说得通了。</p><p>如何才能开始与国会工作人员接触？人们应该做什么才能进入这个领域/哪些组织可能适合为此部署人员？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 20:38:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 20:38:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>这将是一个相当模糊的答案，但我认为这在很大程度上取决于人、他们的技能和他们的政策目标。</p><p>另外——我在上面提到过这一点，但重要的是要重申——人们做得不好肯定会带来风险。另一方面，存在过于“不作为偏见”或类似情况的风险，并留下很多价值。</p><p>这确实很难且令人困惑。我之前提到，我咨询了 10-20 名 AI 治理人员。他们中的大多数人都说“这似乎很重要但被忽视了，但我不知道，这似乎很令人困惑。”他们中的一些人就像“是的，我完全认为你应该这样做，特别是如果你采用 XYZ 策略。”一位相当著名的人工智能治理人士明确告诉我，他们不希望我这样做。我发现很难平衡这种相互矛盾的反馈。</p><p>我还认为我的很多建议取决于某人到底想说什么——例如：</p><ul><li>他们的推销方式是什么？如果会议开始，工作人员说“那么，你想谈什么？”，最初的反应是什么？</li><li>他们是那种善于提出问题、对别人的世界观感到好奇的人吗？</li><li>他们听起来会危言耸听吗？</li><li>他们了解很多关于人工智能的事实吗？当他们不知道某件事时，他们是否能够认识到这一点并进行适当的对冲？</li></ul><p>考虑到所有这些，如果阅读本文的人有兴趣与国会工作人员互动（或让他们组织中的某人这样做），并且他们重​​视我的意见，<strong>我建议他们通过 LW 与我联系。</strong>我能够在更多背景下提供更好的建议。</p></div></section><h2>一次典型的会议</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:47:09 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:47:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>是的，这一切都有道理。您能向我介绍一下您可能举行过的典型会议吗？例如，您将如何首次与员工联系，您将在哪里与他们见面，实际对话是什么样的，您将如何跟进或以其他方式弄清楚这是否有帮助？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 21:47:20 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 21:47:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>会议物流</strong></p><ol><li>将通过电子邮件联系</li><li>通常会在国会办公室（华盛顿特区基本上有 4 座主要建筑都设有所有国会办公室）或通过 Zoom 与他们会面</li></ol><p><strong>会议进展如何</strong></p><ol><li>谈话通常会从我询问他们是否对人工智能有任何疑问或希望我分享我正在研究的东西开始。通常，他们希望我先开始。</li><li>我首先介绍我自己和 CAIS。一旦<a href="https://safe-ai.webflow.io/statement-on-ai-risk">CAIS 声明</a>出来，我就会引用 CAIS 声明。我会告诉他们，我正在关注先进人工智能带来的全球安全风险。我还会告诉他们我正在制定 NTIA 响应，并且会告诉他们我正在考虑的一些高级想法。</li><li>然后，我会停下来看看他们是否有任何问题。</li><li>通常，他们要么询问更多有关灭绝风险的问题，要么询问有关人工智能的各种问题（例如，您对如何处理深度造假有什么想法吗？），或者提出一些有关监管的高级问题（例如，我们如何监管而不扼杀创新？我们如何监管而不输给中国？）</li><li>在一些最好的会议中，我会听到办公室正在研究的一些与人工智能相关的东西。大多数办公室没有能力/兴趣在人工智能领域发挥带头作用。大约 10% 的办公室表示“是的，我的国会议员对此非常感兴趣，我们正在考虑引入立法或成为其他人立法的核心部分。”</li><li>很多人问我是否有立法草案。显然，如果你有监管想法，人们希望看到你有一个像法案一样写成的（简短的）版本。</li></ol><p><strong>跟进</strong></p><p>NTIA 的评论请求回复完成后，我向遇到的每个人发送了一份后续信息。当我有一次特别好的会议时（例如，一位员工对人工智能风险表示强烈兴趣，或者告诉我他们想向我发送他们正在研究的东西），我会发送个性化的后续信息。我认为最明显的帮助迹象来自于人们继续向我发送问题/想法、将我介绍给同事或希望与我合作提出建议的情况。 （需要明确的是，这种情况发生在少数情况下，但我认为这是大部分影响的来源）。</p></div></section><h2>员工对人工智能风险的态度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:41:56 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:41:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>很多人问我是否有立法草案。</p></blockquote><p>他们正在寻求针对哪些类型的问题进行立法？您建议的任何立法？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:38:45 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:38:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>工作人员经常想知道我是否有立法草案来描述我在 NTIA 回复中所写的许可制度（我没有立法草案，但后来在帮助 Thomas 关闭<a href="https://www.aipolicy.us/">人工智能政策中心</a>时参与了立法起草工作）地面。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:39:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:39:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>啊好吧。更一般地说，人们对人工智能风险有哪些先验？您认为您通常会导致他们处理该主题的方式发生重大变化吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:39:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:39:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>似乎大多数人对人工智能风险没有强烈的先见之明。</strong>我本以为人们的先验会更加怀疑（比如“什么？世界末日？<i>真的吗</i>？”）。但我认为很多人都会说“是的，我完全可以看到人工智能如何导致全球安全风险”，甚至“是的，我实际上很担心类似天网的人工智能，我很高兴其他人正在努力”关于这一点。”</p><p>通常，人们似乎<strong>真正担心人工智能带来的灭绝风险</strong>，但也<strong>没有任何计划来解决这个问题</strong>。有人提醒我，“X 是一种存在风险”实际上是一件非常 EA 的事情 -->;“因此我应该认真考虑在 X 上工作。”很多人就像“我很高兴其他人正在考虑这个问题[但我不会，我也不指望我的国会议员会]。”</p><p>就我的效果而言，我认为我主要只是让他们更多地考虑这一点，并将其列入他们内部的“人工智能政策优先事项”列表中。我认为人们忘记了员工的优先事项清单上有大约 100 件事，所以仅仅让他们接触并重新接触这些想法就会有所帮助。</p><p>我还遇到了一些员工，他们似乎非常关心人工智能风险，并且似乎是人工智能政策领域的坚定盟友。我仍然与几个人保持着联系，当托马斯创办人工智能政策中心时，我向他介绍了其中的一些人。如果我希望通过一项法案，我想我可以更好地了解我会尝试与哪些特定人员取得联系。<strong>在我看来，整个华盛顿之行的大部分影响可能在于弄清楚盟军工作人员是谁并与他们建立初步关系。</strong></p><p>最后一件事是，我通常不强调失去控制//超级智能//递归自我完善。我没有隐藏它，但我将其包含在更长的威胁模型列表中，并且它很少是我试图传达的主要内容。如果我再做一次，我可能会更多地强调这些威胁模型。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:42:42 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:42:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>在我看来，整个华盛顿之行的大部分影响可能在于弄清楚盟军工作人员是谁并与他们建立初步关系。</p></blockquote><p>啊好吧！有哪些特征可以很好地预测员工是否会同情这项事业？例如特定地区、政治倾向、其他政策。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:51:23 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:51:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><blockquote><p>有哪些特征可以很好地预测员工是否会同情我们的事业？</p></blockquote><p><br>并不真地。样本量非常小。就像，总共可能有大约 4 名工作人员，我会把他们安排在“非常关心灭绝风险，并且他们可以在推动立法方面提供很大帮助”的位置。 1 名共和党人和 3 名民主党人。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:24:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:24:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>啊，明白了。你们所进行的讨论（在 CAIS 声明发布之前）是否对该声明产生了任何影响（措辞、外展等）？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:37:49 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:37:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>讨论并未影响该声明；该声明是在我前往华盛顿之前写的。 （有趣的事实：我是参与起草 CAIS 声明的人之一。认为为一个好句子做出贡献比我所做的许多其他事情的影响力要大 100 倍，这有点奇怪，但是嘿，有时它会起作用那样）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:48:17 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:48:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>（有趣的事实：我是参与起草 CAIS 声明的人之一。认为为一个好句子做出贡献比我所做的许多其他事情的影响力要大 100 倍，这有点奇怪，但是嘿，有时它会起作用那样）。</p></blockquote><p>该死。我们生活在一个奇怪的世界。顺便说一句，做得很好。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:48:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:48:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>谢谢！看到这个声明有多么重要确实很奇怪。</p><p>我认为这也是相当令人谦卑的——当我第一次听到这个声明时（当时我们称其为公开信），我记得当时我很沮丧，就像“嗯，一封公开信会做什么？我们已经有 FLI 暂停信。”</p><p><strong>这是一个有用的提醒，有时您可能无法提前预测某些事情的影响</strong>。事后看来，很明显（至少对我来说）CAIS 声明是有用的，并且变革理论非常可靠。但当时，这并不像是一个落后的总体规划。感觉这只是 20 个项目清单中的一个项目，而且它有一种模糊的变革理论，这只是另一个似乎值得冒险的赌注。</p></div></section><h2>得到教训</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:50:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:50:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>如果您再次进行此过程，您会采取哪些不同的做法？让您感到惊讶/您觉得自己从中学到的主要事情是什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:58:05 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:58:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>我想我会写一份文档来解释我的推理</strong>，记录我咨询过的人，记录我所意识到的上行和下行风险，并将其发送给一些 EA。我认为一些谣言称这是以相当单边主义的方式完成的。这很棘手，让我很难过。我不认为我这样做的方式实际上是单边主义的，但我认为通过书面推理来避免误解会更好。 Thomas 在 CAIP 中做了很多这样的事情，并为<strong>“如何在不确定的情况下采取行动，同时以推理透明和高度协调的方式采取行动”等问题提供了一个很好的模型。</strong></p><p>我还认为我会提出<strong>立法草案</strong>（假设我所在的组织对此感到满意）。如果你有立法草案，人们似乎会更认真地对待你。</p><p>我还会写一份<strong>更短的 NTIA 回复</strong>– 我们最终写了一篇大约 20 多页的论文。我会针对较短的材料进行更多优化。</p><p>啊，说到这里，我会带<strong>一份打印出来的单页纸</strong>来解释什么是 CAIS 并总结 NTIA 答复中的监管理念。我在中途就完成了这件事，而且我本来可以早点完成这件事。</p><p>另外，我会带着<strong>名片</strong>来。人们似乎很喜欢名片！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:01:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:01:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>是的，这一切都有道理，尽管我绝对不会提前猜到。</p></div></section><h2>最后拍摄</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:02:52 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:02:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>我想我已经没有什么问题要问了：你还有什么要说的吗？随意闲逛。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 23:26:43 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 23:26:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>以下是一些杂项：</p><ol><li>我在华盛顿的经历让我觉得<strong>奥弗顿之窗非常宽</strong>。国会没有对人工智能政策的缓存，而且似乎很多人真的想学习。目前尚不清楚这种情况会持续多久（例如，人工智能风险最终可能会两极分化），但我们似乎正处于一个异常高度开放和好奇心的时期。</li><li>然而，<strong>让国会采取任何行动也非常困难</strong>。就像，由于相当无聊的原因，没有多少法案获得通过。在此过程中，账单可能会死去。当需要是两党制时（他们目前这样做，因为我们有民主党参议院和共和党的众议院），这将更加正确。这主要使我更新到<strong>“哇，现状通常什么都没有发生，并且需要做很多工作才能获得任何有意义的立法。”</strong>考虑到这一点，我确实认为我们处于AI安全性的非常独特的境地（<i>实际上</i>没有多少事情构成灭绝风险和其他各种其他灾难​​性风险；而且，没有多少事情成为参议院多数党领袖的优先事项是激发与世界领导人的国际峰会，或成为整个行政命令的重点）。</li><li><strong>许多人高估了DC中“内部游戏”的数量</strong>，尤其是在国会参与方面。发生了一些秘密的事情，但是在大多数情况下，我认为没有人有球。</li><li>我想看到<strong>有关特定政策愿景的更多协调</strong>。有一段时间，您只是在关心Xrisk的酷儿童俱乐部。我认为Overton窗口已经移动了一堆，我们正处于不足以“关心Xrisk”的时刻。重要的是人们支持哪些具体政策，并愿意倡导哪些政策。</li><li>考虑到这一点，我还认为拥有更广泛的AI风险社区会有好处。<strong>实施不当的协调可以导致什么都没有完成，因为您永远无法达成共识</strong>（目前有利于领先的实验室和不受监管的规模）。<strong>太少的协调会导致缺乏联盟建设和不必要的冲突。</strong>我认为我已经从“协调良好”转变为“正确完成协调是好的，但实际上需要技巧，机智和精力才能很好地进行协调。”</li><li>我通常认为<strong>，更多的人应该公开写自己的观点。</strong>当我不知道人们相信什么时，很难协调。我认为社区应该不愿意赞美那些没有提出任何特定立场的人。 <strong>&nbsp;</strong></li><li>我学到了很多有关DC AI安全社区的知识（由“ AI安全社区”，我主要是指从事AI安全工作的人们，这是由于渴望避免Xrisk或社会规模的灾难而动机。 ，但很多人都没有）<ol><li> TLDR：很复杂。我认为，前10％的思想家很有才华，并追求了合理的变革理论。另一方面，也有很多人声称对AI政策感兴趣，但对各种AI安全威胁模型没有基本的了解。也有（真实而合理的）担心，社会上的和政治上具有竞争力的新移民可能会以威胁或削弱现有努力的方式进入该空间。</li><li>总而言之，我觉得主要的文化太蔑视了新的政策努力。我希望随着AI政策对话继续前进并吸引新的人群，情况会发生变化。我会为一个更像“啊，新人感兴趣的社区”感到兴奋，让我们给您一些提示/指示，并指出我们已经有过的特定经验，并讨论了缺点风险的具体模型。”现状通常感觉不那么具体，（在我看来）（在我看来）对新努力过于保护。我发现，这种文化使我更难清楚地思考或进行倡导，尤其是我所说的“高领域倡导”（在这里，您在其中主要尝试将您的内部世界国家传达给人们，而不是主要尝试尝试传达一系列信念，这些信念将与您的听众息息相关）。我认为，关于“直接”各种倡导努力的“直接”（我认为，有些DC的人实际上会失去他们的某些影响力/“严肃点”（如果他们完全直接））有严重的辩论，但是我仍然感到惊讶在效果的范围内 - 文化似乎不信任我和我的同龄人直接。我认为，这种文化已经大大减慢了新的政策努力，并继续以我认为对世界不利的方式威胁/削弱/恶劣的新政策努力。与许多事情一样，我认为高级问题是正确的，但是在如何应用/实施这些高级问题的确切问题上存在问题</li><li>评估各种人/计划的往绩也很难。部分原因是某些信息是秘密的，部分是因为“我们与重要利益相关者有良好关系”之类的东西是一个有用的工具步骤，但不一定会转化为影响，部分是因为许多变革理论都是基于命中的和花一些时间来产生直接影响（例如，如果某人与X建立了良好的关系，也许在某个时候X会与AI监管非常相关，但也许只有1-10％的机会是真实的。） ，我认为，如果人们最终对自己的信念更明确，对他们希望实现的特定政策目标更加明确，并且对他们清晰的胜利（和损失）更明确，那么协调会更容易。在缺乏此事的情况下，我们冒着赋予“玩游戏”，发展影响力但最终没有利用自己的影响力来实现有意义的变化的人的风险和太多资源。 （另请参见此Dominic Cummings<a href="https://www.dwarkeshpatel.com/p/dominic-cummings#details">播客</a>）。</li></ol></li><li>相关的是， <a href="https://forum.effectivealtruism.org/posts/tdaoybbjvEAXukiaW/what-are-your-main-reservations-about-identifying-as-an?commentId=gNC53rsuMNTBjLCWY">奥利弗·哈布里卡（Oliver Habryka）的这一评论</a>引起了我的共鸣。我发现，与“主流EAS”有一定距离时，我通常会更清楚地思考。有很多抗体和微妙的文化压力，可以阻止我思考某些想法，并且可以萎缩我在世界上采取指示行动的能力。 （当然，我认为解决方案不是“永远不会与EAS互动”  - 但我确实认为人们可能低估了社区对思考和实现困难的事情的负面影响。我当然是。）</li><li>对于有兴趣捐赠的人，我目前建议<strong> </strong>这<strong> </strong><a href="https://www.aipolicy.us/"><strong>AI政策中心</strong></a><strong> </strong>（尤其是托马斯·拉尔森（Thomas Larsen）继续高度参与其战略方向）。我与托马斯（Thomas）有一些战略/战术分歧，但我认为他是一个非常聪明和才华横溢的人，我认为他是AI政策支持空间中最好的新来者之一（COI：Thomas是我的朋友，我和我的朋友之一参与了在早期阶段帮助该中心的AI政策）。</li><li>如果您想与我交谈，<strong>请随时在Lesswrong上伸出援手</strong>。我喜欢与从事AI政策工作的人交谈。我也愿意接受我可能正在做的有影响力的事情，或者我知道可能正在做的其他事情。 </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:18:15 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:18:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>哇，好吧。感谢您进行此对话！</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2slwt2csaag74nsdn/speaking-to-compeaking-to-compressional-staffers-about-ai危险<guid ispermalink="false">2SLWT2CSAAG74NSDN</guid><dc:creator><![CDATA[Akash]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:08:52 GMT</pubDate> </item><item><title><![CDATA[Open Thread – Winter 2023/2024]]></title><description><![CDATA[Published on December 4, 2023 10:59 PM GMT<br/><br/><p>如果值得一提，但不值得自己的职位，这里是一个可以说的地方。</p><p>如果您是Lesswrong的新手，这里是自我介绍的地方。邀请您如何找到我们以及希望从网站和社区获得什么，个人故事，轶事或仅一般性评论。如果您不想写完整的顶级帖子，这也是讨论该网站的功能请求和其他想法的地方。</p><p>如果您是社区的新手，则可以开始阅读<a href="https://lesswrong.com/highlights">序列中的亮点</a>，这是有关Lesswrong的核心思想的帖子集。</p><p>如果您想更多地探索社区，我建议您<a href="https://www.lesswrong.com/library">阅读图书馆</a>，<a href="https://www.lesswrong.com/?view=curated">检查最近的策划帖子</a>，<a href="https://www.lesswrong.com/community">查看您所在地区是否有会议</a>，并检查<a href="https://www.lesswrong.com/faq">Lesswrong FAQ</a>的<a href="https://www.lesswrong.com/faq#Getting_Started">入门</a>部分。如果您想与网站上的内容相关，也可以查看<a href="https://www.lesswrong.com/tags/all">概念部分</a>。</p><p>打开的线程标签在<a href="https://www.lesswrong.com/tag/open-threads?sortedBy=new">这里</a>。开放线程序列在<a href="https://www.lesswrong.com/s/yai5mppkuCHPQmzpN">这里</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tra92qdondli6ra8u/open-thread-winter-2023-2024<guid ispermalink="false"> tra92qdondli6ra8u</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:59:51 GMT</pubDate> </item><item><title><![CDATA[Interview with Vanessa Kosoy on the Value of Theoretical Research for AI]]></title><description><![CDATA[Published on December 4, 2023 10:58 PM GMT<br/><br/><p>以下是<i>我与我的</i><a href="https://www.zenmarmotdigital.com/blog/interview-with-vanessa-kosoy"><i>博客</i></a>交叉传播的<a href="https://youtu.be/1MCRQF0_5zY?feature=shared"><i>视频采访</i></a><i>的成绩单（编辑为语法）</i> <i>。它旨在作为（相对）对</i><a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Direction_6__Metacognitive_Agents"><i>学习理论议程</i></a>的目标的（相对）的初学者友好解释<i>，以及为什么需要更多理论工作来确保AI的AI安全可靠。</i></p><p></p><p><strong>简介（由Will Petillo）：</strong>讨论AI的未来倾向于变得哲学。拥有“目标”或“理解”意味着什么？寻求权力的是想要事物的默认后果吗？还是我们独特的进化历史所产生的人类怪癖？是什么促使善良的？以这种方式进行框架问题使它们可以访问，使每个人都可以参加对话。但是，由于分歧成为直觉的冲突，这种缺乏精确度也使此类问题变得棘手。</p><p>当今的“对齐监护人”的嘉宾是凡妮莎·科索（Vanessa Kosoy），他是机器情报研究所（MIRI）和长期未来基金（LTFF）支持的独立研究人员，他介绍了安全AI的数学理论。从第一原则中的理解重点使她的工作与领先的AI实验室的“快速移动并破坏事物”的实验方法形成鲜明对比。在这次采访和其他地方，凡妮莎捍卫了一种基于理论的方法的价值，并解释了将机器学习作为基础科学的含义。</p><p></p><p> <strong>Petillo：</strong>您是如何进入AI安全的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我一直是自动赛车，所以我只是倾向于自己学习东西。小时候，我以为我会成为理论物理学家。我实际上拥有数学学士学位，但是在完成学士学位之后，我没有去学术界，而是决定从事该行业的职业，而是从事软件。</p><p>我在软件行业，特别是算法工程，主要是计算机视觉，各种角色，算法工程师，团队负责人，研发经理。我也有自己的创业公司。我是顾问，然后是10年前。我接触了AI的整个存在风险的主题，并开始认为这似乎很重要。因此，我开始对此进行枢纽，最初只是我在业余时间进行研究。然后是Miri的支持。然后，我最近也得到了长期未来基金的支持，这使我能够全职。</p><p><strong>佩蒂洛：</strong>那个过程是什么样的？您只是以一种自我指导的方式工作，然后您获得了Miri和其他来源的支持？这是怎么来的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我开始读一些我的东西，而《 miri》和《少问题》写作。我开始研究自己的想法，并写下关于少错的帖子。之后，我被邀请参加一些研讨会，一些活动，最终Miri说，好吧，似乎您在这里做一些不错的工作，所以也许我们也会为此付钱。我很棒，因为这也使我能够做更多的事情，并花更少的时间做其他事情。<br><br><strong>威尔·佩蒂洛（Will Petillo）：</strong>这里的观众有一点背景。一个受欢迎的博客少了。它最初是关于理性的，也是关于与AI相关的事物的。 Miri是机器情报研究所。我喜欢将他们描述为在酷之前进行对齐的人。告诉我更多关于Miri作为机构的信息。<br><br> <strong>Vanessa Kosoy：</strong> Miri或多或少是第一个谈论人工智能存在风险的人。 Eliezer Yudkowsky在2000年开始谈论这件事，最初Miri只是Yudkowsky，然后多年来，他们设法获得了一些资金来吸引其他研究人员。他们正在考虑以下问题：我们如何使人工智能安全，我们如何处理这个问题？我们可以提出什么样的数学理论来解决这个问题？甚至在深度学习革命开始之前，以及近年来大型语言模型的整个炒作之前。他们的大部分时间都致力于提出一些基本的数学理论，这将帮助我们保持AI的一致性。<br><br>最近，由于他们相信时间表确实很短，而且我们没有时间发展这一理论，因此他们枢纽宣传并试图影响政策。<br><br><strong>佩蒂洛：</strong>您是否加入了该枢纽？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>不，我的看法与众不同。我可以说更保守。我认为时间表并不像风险社区中的许多人那样短。我认为，如果在政策渠道中努力规范AI开发并延迟AI开发以阻止真正危险的AI的发展，那么这将成功，那么这只会花我们的时间。然后问题是：为我们花时间买什么？而且我认为，理论基础绝对是我们在拥有的时间应该做的事情中最重要的事情，或者随着我们将通过某种政策计划成功购买的时间以一种或另一种方式购买的时间。<br><br>我认为，在任何世界上，创建这一基础理论都是关键。这就是我正在做的事情。这绝对是我的个人技能和优势所在的地方，从事数学而不是政策。<br><br><strong>威尔·佩蒂洛：</strong>您提到了时间表。直觉上，我知道不可能以任何精度真正预测这些事情，但是就促使您的动机而言，您将这些东西何时需要解决的时间表？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>有些人认为AGI将在10年甚至更少。我认为这有点极端。但是，当需要解决问题时，越早越好，对吧？如果我们在五年内有解决方案，那么我们比只有10年内的解决方案要好，这仍然比我们只有20年之内的解决方案要好。<br><br>实际上，我个人的看法是，实际上还有数十年的时间才真正达到存在存在风险的AI。因此，这给了我们更多的时间，但不是无限的时间。<br><br><strong>佩蒂洛（Petillo）：</strong>是什么让您点击了您要处理的事情？<br><br>凡妮莎·科索伊<strong>（Vanessa Kosoy）：</strong>它最初更像是一种好奇心，因为它实际上是从我完全随机发现的开始，您可以在AGI上说一些论文，而不是关于AI的一致性或风险或类似的东西，而是JürgenSchmidhuber和Marcus Hutter的一些论文有一些关于Agi的想法。我一直是数学书呆子，所以有一些数学框架来思考AGI似乎真的很酷。我开始阅读有关这件事的文章，最终也发现错误的错误，有些人也在讨论这种事情。<br><br>一方面，我正在阅读Eliezer Yudkowsky在主题上写的越来越多的东西，而Lesswrong上的人们写了这一主题，但我也开始考虑数学模型。最终，它引起了我的注意，当您考虑实际数学时，就没有数学上的原因，为什么AI不得不关心人类或完全关心与我们作为人类关心的东西保持一致的数学原因。<br><br>另一方面，似乎比我们更有能力。我认为这很明显，但是对我来说，我喜欢通过数学理解一切。因此，当我看到您实际上可以将其放入数学模型中时，它确实对我来说确实是真实的，这是我们应该真正关注的事情。</p><p><strong>威尔·佩蒂洛（Will Petillo）：</strong>没有理由假设AI一定会很好的想法，这听起来很像尼克·博斯特罗姆（Nick Bostrom）写的正交论文。智力以及好事不必在一起；任何一组值都可以使用任何级别的智能。从本质上讲，这是您的见解吗？<br><br> <strong>Vanessa Kosoy：</strong>是的，这正是术语。事后看来，这似乎是一件明显的事情。但是对我来说，有必要看到您实际上可以用数学对象来考虑它。值可以形式化为效用函数。然后，代理可以被形式化为某种优化器，某种贝叶斯最佳策略或该实用程序功能的任何其他策略。实际上，您可以将严格的含义放在每个术语后面，并看到这实际上都是有道理的，而不仅仅是某种哲学上的挥舞技巧。<br><br><strong>佩蒂洛（Petillo）：</strong>您认为，使AI对齐问题的根本原因是什么？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我认为问题很难。我认为，首先，很难的是，我们的目标是一个非常狭窄的目标，因为人类的价值观非常复杂和具体。我们关心许多非常详细的事物：爱，友谊，美丽（以我们自己的主观理解），性，所有这些都是人类事物，因为某些复杂的进化事故，它发生在某些人身上的方式非常特殊的星球。这是特定宇宙历史上非常特别的一点。这组值是您可以想象的可能值或思想的巨大空间的非常非常狭窄的一部分。<br><br>因此，按照我们的标准，大多数人绝对不是对我们很好的任何东西。更糟糕的是，这种现象在他的一篇相对较新的帖子中也很好地表达了这种现象，他写道，围绕特工能力有一个吸引人的盆地，但对代理人的一致性没有吸引力的盆地。这意味着，即使您采用足够的优化压力，即使使用蛮力技术，您最终将产生高功能的代理。一个例子是进化，对吗？进化是一种非常原始的蛮力算法，最终创造了人类大脑，这是一种更复杂的算法。如果您将足够的蛮力优化用于寻找在开放世界环境中成功的事物，最终您将遇到聪明的代理商。这甚至是在您对方程式进行递归自我改进之前，这使它成为您收敛到的吸引力盆地的强大。而这种尤其是与人类价值观保持一致的东西是什么。我们可以通过某种盲目或半盲试和错误来实现高功能的代理，这是非常合理的，我们可以在我们理解足够的理解以实际使这些代理保持一致之前。<br><br><strong>威尔·佩蒂洛（Will Petillo）：</strong>这听起来像是与其他bostrom populatizatizan contrumental融合的想法相抵触，几乎所有能够优化足够努力的系统，都需要诸如生存之类的东西。<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>有一种以工具收敛的目标概念，这是最聪明的代理商将追求的某些目标，因为它们可以帮助他们实现终端目标，无论其终端目标是什么。这些都是生存之类的东西，例如获得更多的资源，变得更加聪明，等等。但是人类的价值不是那样。如果我们设法构建一个生存并获得大量资源的AI，我想这对AI来说是很好的，但这对我们与价值观的一致性无济于事。那是完全不同的事情。<br><br><strong>佩蒂利奥（Petillio）会不会：</strong>现代AI受到人类生成数据的培训并存在于人类社会中的事实？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我认为这有帮助，但是这留下了很多问题。一个问题是：好的，您可以从人类生成的数据中学习，但是您如何从那里概括？因为真的不清楚获得良好的概括需要什么条件，尤其是当您学习的概念是非常复杂的事情时。<br><br>您所学习的概念的复杂性越高，学习它所需的数据点就越多。我们正在用所谓的大型语言模型（近年来炒作）所做的事情正在试图模仿人类。我的意思是，很好。可能会导致一些概率带来好处，而不是很高的概率。但是问题在于，为了使用它，您需要将其推广到训练分布之外。在这里，我们实际上需要查看目标是什么。<br><br>问题在于，从技术上讲，创建超智能的AI是可能的，这将是危险的。要解决这个问题，这还不足以创建某种不危险的AI，因为否则他们只能编写一种无用的算法。这并不危险，任务完成了。我们需要能够创建足够强大的AIS，以作为对潜在危险AI的防御系统。因此，这些系统必须是具有超人能力的系统，以建立世界的复杂模型并基于此制定复杂的长期计划。这远远超出了大型语言模型的训练分布或基于人类模仿的任何内容。目前尚不清楚我们是否真的可以依靠我们必须概括远离训练分布的算法而不会完全失去其所有对齐属性。<br><br><strong>威尔·佩蒂洛（Will Petillo）：</strong>总而言之，LLM从根本上是模仿的，这本身似乎并不特别危险，但这也限制了他们可以做的事情。因此，我们不能真正期望开发就会停在这里。最终，可能会添加强化学习之类的东西 - 可能不一定是算法，而是像Alpha Zero一样具有创造力的东西，并找到了以前从未见过的真正创造性的举动。因此，我们需要为更强大的事情做好准备，因为这些事物将很有用，而导致LLM的经济学将导致建立更大的东西。这就是你的意思吗？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>是的，这听起来很重要。要么是加强学习，要么...好吧，我不想推测需要太多的东西以使AI更强大，因为那里的信息不是一个好的信息。</p><p><strong>佩蒂洛：</strong>足够公平。继续研究您实际从事的事情，我见过的一个想法是“代理基金会”一词。还有“学习理论议程”。这些东西是什么？<br><br> <strong>Vanessa Kosoy：</strong>代理基金会是这个抽象的思想，它说我们需要创建一个基础数学理论，该理论解释了代理是什么。在数学上，算法是代理是什么意思？哪种类型的代理是可能的？他们拥有或没有什么功能？等等。学习理论议程比那更具体，从某种意义上说，这就像一个非常特定的程序，试图实现这一目标。具体而言，通过基于统计和计算学习理论的工具，算法信息理论，控制理论，这种事情。这是我创建的计划，以回答提出这些代理基金会的挑战。<br><br> <strong>Petillo：</strong>好的，因此，代理基金会就像“思维如何工作？”的问题，其中包含AI，学习理论议程就像是：“我们如何设计将其朝着良好方向推动的算法？”是对的吗？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我不会那样说。我只想说，代理基金会只是试图了解思想的工作方式，人们一直在尝试以各种方式做到这一点。从历史上看，Miri拥有各种试图解决此问题的证明理论模型，然后是Garrabrant的逻辑归纳，并且在这个非常广泛的保护伞下有各种各样的想法，而学习理论议程是一种非常具体的方法。<br><br>正是这种方法以AIXI和经典的加强学习理论为起点，然后看起来是什么是缺少的成分，以便具有代理的基本理论，并开始朝着那些缺少的成分建立了具有Infra-Bayesianism和等思想的成分基础山基斯物理主义和元认知剂，依此类推。<br><br><strong>佩蒂洛（Petillo）</strong>会在这里谈论的那种代理人和思想吗？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>当我说代理商时，我的意思是很广泛。比现有的AI甚至AIS都广泛得多。当然包括人类，潜在的外星人或其他。因此，对我来说，代理是一个具有特定目标的系统，并且正在学习嵌入世界的复杂模型，并使用这些模型来构建长期计划以实现其目标。因此，这是对“代理人”的意思的非正式描述。该程序的整个目标是从此转变为完全正式的数学定义，并研究该定义的所有含义。</p><p><strong>佩蒂洛（Petillo）：</strong>因此，甚至不超过LLM，它甚至比机器学习更广泛。这种方法的原因是什么？鉴于机器学习的占主导地位，为什么不专注于似乎最广泛使用的事物呢？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>首先，让我们在术语中保持一些顺序。我会区分AI，机器学习和深度学习。自1950年代以来，人们开始思考如何建立思维系统，而不是真正了解它的含义，但只是某种直觉的观念，即有思想的事情，我们应该是思想，我们应该应该，我们应该是这样的，我们应该是这样的，我们应该能够在机器中复制它。<br><br>机器学习是一种更具体的方法，它出现了……好吧，我不想准确地指向何时何时，但可能是八十年代。机器学习是特别的想法，即思维的核心要素是学习和学习意味着您正在与某些未知环境进行互动，并且需要创建这个环境的模型。因此，您需要获取您看到的数据并使用它来创建模型。这类似于科学家如何进行实验，收集数据，然后根据这些数据来构建理论。</p><p>这个总体想法称为机器学习，或者更准确地称其为学习。 “机器”部分来自试图提出在计算机内实现此操作的方法。这是一个具有许多数学理论的领域。机器学习背后的数学理论是所谓的统计和计算学习理论，这实际上是学习理论议程的基础。这就是为什么它被称为“学习理论”的原因。</p><p>有一个假设是，这种学习的概念捕捉了我们思考的大部分重要位置。而且我认为这一假设受到最新技术发展的极大支持。这是我完全认可的东西，这基本上是我整个研究计划的基础。因此，这里没有矛盾，因为学习仍然是一件非常普遍的事情。人类也学习。外星人也必须学习。</p><p>深度学习是一种更具体的算法，用于如何在机器中实际上有效地学习，这就是左右在2010年左右的深度学习革命，尽管这些算法在此之前以某种形式存在。但是花了一段时间才能正确获取细节，并拥有正确的硬件来运行它们。深度学习的不幸功能是我们不从数学上理解它。很多人都在试图理解它，但是我们没有一个很好的理论，说明它实际上是有效的。这就是为什么这不是我的研究计划的重点，因为我试图提出一些数学理解。我绝对希望最终人们会破坏学习的工作方式，然后将其融入我构建的理论中。<br><br>但是，即使我们有这个理论，以最广泛的一般性思考似乎仍然非常重要，因为首先，我们不知道今天存在的算法将是带来AGI的算法。而且，因为最广泛的一般性只是思考问题的正确抽象水平，以了解那些甚至“对齐”的概念甚至意味着什么。这里有一些哲学问题需要解决，它们特定于某些非常特殊的算法。另外，我实际上希望该理论包括人类，因为我可能想利用该理论将价值学习等事物形式化。我如何设计一个从人类那里学习价值的AI系统？<br><br> <strong>Will Petillo：</strong>看到Wikipedia级别，或者只是浏览了互联网描述，机器学习和深度学习，很容易互换使用它们。我认为我将描述视为深度学习只是您添加多层神经元的想法。因此，由于有多个层，它是“深”<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>让我尝试澄清区别。机器学习谈论从中获取数据和构建模型。您要构建的模型的类型可能会大不相同。在深度学习之前，我们有算法，例如支持向量机，多项式回归也是一种非常简单的机器学习类型 - 将模型适合于数据。统计中使用的各种方法可以视为一种机器学习。有一些模型或假设的空间，您正在尝试以最佳方式使用数据来推断正确的假设是什么，或者如果您采用贝叶斯方法，则假设有一些概率分布。<br><br>但是，不同类型的假设类别在算法的力量方面，以及关于如何学习这些假设类别所说的话，导致了非常不同的结果。在我们实际上可以学习哪些条件下，我们知道什么可以在数学上证明它们？例如，对于支持向量机，基本上解决了数学理论。有基于此基础的内核方法，并且也具有非常坚实的数学理论。深度学习是一种使用那些人工神经网络体系结构的特定学习算法。</p><p>这不仅是多层，而且还有很多细节很重要。例如，激活功能是依赖的事实，事实证明，这对于您在培训中使用哪种正则化方法非常重要。例如，辍学基本上是开始深度学习革命的原因。如果您正在使用序列，那么我们将有Transformers，这是一个非常特定的网络体系结构。因此，这些年来，人们实际上有很多非常具体的细节，主要是在反复试验的过程中，以查看有效的方法。我们没有一个很好的理论，为什么这些特定的事情效果很好。 We don&#39;t even understand the space of models those things are actually learning, because you can prove theoretically that if you take a neural network and you just let it learn another neural network, then in some situations it will be infeasible.<br><br> But for real world problems, neural networks succeed to learn a lot of the time. This is ostensibly because the real world has some particular properties that make it learnable, or there&#39;s some particular underlying hypothesis class that the neural networks are learning, and which captures a lot of real world phenomena, but we don&#39;t even have a mathematical description of what this underlying hypothesis class is. We have some results for some very simple cases, like two layer or three layer neural networks, or some other simplifying assumptions, but we&#39;re not close yet to having the full answer.<br><br> <strong>Will Petillo:</strong> Deep Learning assumes certain things about how the world is, in terms of what it can pick up, and it happens to work fairly well, but it&#39;s not really clear what it&#39;s assuming.</p><p> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s exactly right. So we have different no-go theorems, which say that for arbitrary data, even if the data is fully realizable, and even if the data is such that the neural network can perfectly express an exactly correct model, the problem is infeasible. In general, gradient descent will not converge to the right model, and also no other algorithm will converge because the problem will just be intractable. There are some properties that the world has, and since this Deep Learning is successful in such a big variety of very different cases, it feels like those properties should have some simple mathematical description.<br><br> It&#39;s not like some properties that are extremely specific to texts or to audio or to images. It&#39;s some properties that are extremely general and hold across a wide range of different modalities and problems. Those are properties that I can speculate, for example, as having to do with compositionality, how the real world is often well described as being made of parts, and how things can be decoupled according to different spatial scales, or different temporal scales on which the dynamics is happening. But we don&#39;t have a theory that actually explains it.<br><br> <strong>Will Petillo:</strong> You mentioned ReLU as one of the examples of a thing that just works. As I understand it, ReLU is basically like taking the output, changing it in a way that could be illustrated as a graph where it&#39;s flat on one side and a diagonal line past zero. Whereas before, models typically used Sigmoid as the activation function, which was more like a smoothly curved line that prevents numbers from getting too big. For some reason, ReLU works better. The sense I&#39;m getting from your explanation is that this change impacts what kinds of things the neural network is able to understand in a direction that&#39;s more matching with reality. But all these changes are developed with a &quot;throw stuff at the wall and see what sticks&quot; kind of way, simply measuring the results without really understanding <i>why</i> ReLU is better than sigmoid.<br><br> <strong>Vanessa Kosoy:</strong> That&#39;s more or less right. We just have to be careful about what we mean when we say what the neural network can &quot;understand&quot;. It&#39;s a pretty complicated concept because it&#39;s not just what the neural network can express with some set of weights, it&#39;s what the neural network can actually learn through a process of gradient descent. It has to do not just with the space of functions that the neural network can describe, but with the entire loss landscape that is created in this space of weights when we look at a particular data set.<br><br> <strong>Will Petillo:</strong> When you&#39;re describing a gradient descent and loss landscape the analogy I hear thrown around a lot is a ball rolling down a hill—there&#39;s a constant gravity force, and you want the ball to get down to sea level. But often it doesn&#39;t because it finds some local minima, like a hole or something, where any direction it can go is up, so it&#39;s not going to roll anymore. So you have to shape the landscape such that down always gets the ball to sea level.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s a pretty good explanation. Gradient descent is something that we have good mathematical theory for how it converges to the global minimum for convex functions, but the loss for neural networks is non-convex...but it still happens to be such that it works. People have gained some insights about why it works, but we still don&#39;t have the full answer.<br><br> <strong>Will Petillo:</strong> OK, so if the landscape is really bumpy, then you don&#39;t expect the ball to get to sea level, so the fact that it somehow does anyways demands an explanation that we don&#39;t really have. I can see how that kind of framing raises a lot of questions regarding unpredictability.<br><br> Moving on, you mentioned AIXI at one point.那是什么？<br><br> <strong>Vanessa Kosoy:</strong> AIXI is this idea by Marcus Hutter, which is supposed to be a mathematical model of the perfect agent. The way it works is: there is a prior, which is the Solomonoff prior. For those who don&#39;t know what that is, it&#39;s basically some way to mathematically formalize the concept of Occam&#39;s Razor. And Occam&#39;s Razor is this idea that simple hypotheses should be considered a priori more likely than more complicated hypotheses. And this is really at the basis of all rational reasoning. Hutter took the Solomonoff prior, which is a very clever way to mathematically formalize this notion of Occam&#39;s razor, and say, well, let&#39;s consider an agent that&#39;s living in a universe sample from the Solomonoff prior. And this agent has some particular reward function that it&#39;s maximizing. And let&#39;s assume it&#39;s just acting in a Bayes-optimal way. So it&#39;s just following the policy that will lead it to maximize its expected utility according to this prior. And let&#39;s call this AIXI. Which is a really cool idea...only it has a bunch of problems with it, starting with the &quot;minor&quot; problem that it&#39;s uncomputable. There&#39;s not an algorithm that exists to implement it even in theory.<br><br> <strong>Will Petillo:</strong> I think I heard it explained once as imagining the entire universe described as a bunch of bits—ones and zeros. At the start, all of them could either be a one or a zero, then you get a little bit of data and now you&#39;ve locked in a few of those numbers and have cut the space of all things that could be in half. As you keep learning, you get more and more certain.<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s actually a little more nuanced than that. Just the fact that you have a lot of possibilities doesn&#39;t mean that it&#39;s uncomputable. Maybe the exact thing is uncomputable, but you could still imagine that there is some clever algorithm that approximates this Bayesian inference process. For example, if you look at classical reinforcement learning theory, then there are things like algorithms for learning in arbitrary Markov decision processes with n states. In a Markov decision process with n states, there are still an exponentially large space of possible ways it could be, and we still have actually efficient algorithms that converge to the right thing out of this exponentially large thing by exploiting some properties of the problem.<br><br> The thing with AXI is that its prior is such that even individual hypotheses in the prior are already arbitrarily computationally expensive, because in its prior it considers every possible program, so every possible program that you can write on a universal Turing machine is a possible hypothesis for how the world works. And some of those programs are extremely expensive computationally. Some of those programs don&#39;t even halt, they just enter infinite loops. And you can&#39;t even know which, because this is the halting problem, right? This is why AIXI is a non-starter for doing something computable, not to mention computationally tractable.<br><br> <strong>Will Petillo:</strong> The minor problem of uncomputability aside, a &quot;perfect algorithm&quot;...what does that mean? Would AIXI be safe if it were somehow, magically computed?</p><p> <strong>Vanessa Kosoy:</strong> No, it doesn&#39;t make it safe in any way. It&#39;s &quot;perfect&quot; in the sense of it&#39;s the most powerful algorithm you could imagine. Again, under some assumptions. I mean, there are other problems with this, such as that it assumes that the outside world is simpler than the agent itself. There are multiple problems with this, but if you can put all those problems aside then you could argue that this is the best possible agent. And in this sense, it&#39;s perfect. It&#39;s very, very, very not safe. In order for it to be safe, we would need to somehow plug the right utility function into it. And that would still be a very non-trivial problem.</p><p> <strong>Will Petillo:</strong> What kind of algorithms do you look for, assuming you&#39;re trying to find things that are computable?</p><p> <strong>Vanessa Kosoy:</strong> Computability is just one of the issues. I&#39;m imagining that there will be something that I call the frugal universal prior, which is some kind of prior that we can mathematically define, which will simultaneously be rich enough to capture a very big variety of phenomena. And on the other hand, we&#39;ll have some clever algorithm that can actually allow efficient learning for this prior using, for example, some compositionality properties of the hypothesis on this prior or something of that sort.</p><p> But even knowing this prior, there&#39;s a lot of other conceptual problems that you also need to deal with. Like what I call the problem of privilege, where the formalization of Occam&#39;s Razor and AXI privileges the observer, and you need to understand how to deal with that. And there&#39;s the problem of realizability where you cannot actually have a hypothesis which gives you a precise description of the universe, but only some kind of approximate or partial description, and you need to understand how to deal with that. Then there&#39;s also the fact that you want your utility function to be not just a function of your observations, but also some parameters that you cannot directly observe. You also want to be able to prove some Frequentist guarantees for this algorithm. To know how much data this algorithm actually needs to know to learn particular facts and have a good theory of that. There&#39;s a whole range of different questions that come up when studying AIXI like models.<br><br> <strong>Will Petillo:</strong> Studying AIXI like models, that is what you&#39;re working on?</p><p> <strong>Vanessa Kosoy:</strong> You could, yeah, if you wanted to put it in one sentence, I guess.<br><br> <strong>Will Petillo:</strong> What are some interesting problems that you&#39;re interested in solving? I&#39;ve seen Newcomb&#39;s problem floating around and stuff adjacent to this.<br><br> <strong>Vanessa Kosoy:</strong> Newcomb&#39;s problem is something Eliezer Yudkowsky wrote about a lot as an example of something that&#39;s very confusing for classical accounts of rationality. You have two boxes that you need to choose from. One box has a thousand dollars. The other box has either nothing or a million dollars. You can either choose the first box or you could take the money that&#39;s in both boxes. Normally, taking the money that&#39;s in both boxes is always strictly superior to just taking the one box.<br><br> Except that in this spot experiment there is some entity called Omega who can predict what you do, and so it only puts the $1,000,000 in the other box if it knows that you will only take that box and won&#39;t try to take the thousand dollar box as well. So only if you&#39;re the type of agent that would predictably (for Omega) take only one box, only in this case you will get out of this room with $1,000,000. Whereas in the other case you will only have $1,000. So arguably it&#39;s better to take one box instead of two boxes, as opposed to what many classical accounts of rationality would say. This is one example of an interesting thought experiment.</p><p> For me, this thought experience is a special case of the problem of non-realizability, where you need to deal with environments that are so complex that you&#39;re not able to come up with a full description of the environment that you can actually simulate 。 Because in this example, the environment contains this agent Omega, which simulates you, and this means that you cannot simulate it, because otherwise it would create this kind of circular paradox. I&#39;ve actually also shown that my theory of dealing with non-realizability, which I call Infra-Bayesianism, actually leads to optimal behavior in these kinds of Newcomb-like problem scenarios.<br><br> <strong>Will Petillo:</strong> And the reason for studying Newcomb-like problems is not because we expect to be faced with Omega offering us boxes at some point, but because it&#39;s just an illustrative way of thinking about how to deal with things when you can&#39;t know这是怎么回事。 And also because it might be easy to say, &quot;yeah, well, I&#39;ll just take one box because I&#39;ll get more that way,&quot; but when you really dive into what&#39;s a coherent, non hand wavy reason as to why, then there&#39;s some interesting insights that can potentially come up from that. Have there been any discoveries that you found through exploring these kinds of things?</p><p> <strong>Vanessa Kosoy:</strong> I would say that Infra-Bayesianism itself is an interesting discovery, that some theoretical account of agents that can reason about complicated worlds that are much too complicated for the agent to simulate. Now I describe the motivation by stating the problem of non-realizability, but the way I actually arrived at thinking about this is through thinking about so-called logical uncertainty. The reason people started thinking about it was because of so-called Updateless Decision Theory, which came from thinking about Newcomb time paradoxes. So it all comes from that line of reasoning even though, after the fact, you can motivate it by some much more general abstract thinking.<br><br> <strong>Will Petillo:</strong> What&#39;s the connection between these decision theory type questions and making a safer AI?<br><br> <strong>Vanessa Kosoy:</strong> The idea is creating a general mathematical theory of agents. The way it&#39;s going to help us with making AI safer, there are several reasons, the most obvious is that in having this theory, we hopefully will be able to come up with rigorous models of what it means for a system to be an aligned agent 。 Having this rigorous definition, we&#39;ll be able to come up with some algorithms for which we can prove those algorithms are actually safe agents. Or at least we could have some conjecture that says that this given model of such and such conjectures, we think that those algorithms are safe agents. Like in cryptography, you have some conjectures which have very strong evidential support.</p><p> We could have at least some semi formal arguments because now when people are debating whether a particular design is safe or not safe, it all boils down to those hand waving philosophical arguments, which don&#39;t have any really solid ground. Whereas this gives us tools for much more precise, crisp thinking about these kinds of questions. It hypothetically also gives us much more power to leverage empirical research because maybe we will be able to take the empirical research we have, plug it into the mathematical theory, and get some answers about how we expect those results to actually extrapolate to various regimes where we haven&#39;t done so.<br><br> <strong>Will Petillo:</strong> Would this line of research that you&#39;re working on eventually be usable in evaluating things like Large Language Models or Deep Learning based systems, to be able to say with greater certainty the extent to which they&#39;re safe or unsafe?<br><br> <strong>Vanessa Kosoy:</strong> I think there are multiple paths to impact. So there is a path to impact where we will eventually come up with a theory of Deep Learning. Or, if not a fully proven theory, then at least some strong conjectures about how Deep Learning works that can interface with the theory of agents I&#39;m building. And then we could use this composite theory to prove things or at least to have strong arguments about properties of systems built in Deep Learning.</p><p> There could be a different path to impact where we use this theory to come up with completely new types of algorithms for building AI, which are not Deep Learning, but for which we have a good theoretical understanding.</p><p> There&#39;s also some third possibility that we won&#39;t have a good theory, but we could at least reason by analogy, similarly to how many Deep Learning algorithms are designed by analogy to some algorithms for which we have mathematical theory. Deep Q learning, for example, is analogous to simple Q learning, for which we have mathematical theory. So we could imagine a world in which we have some kind of idealist toy model algorithms for which we have some rigorous arguments why they are aligned and then we have some more heuristic algorithms, which we cannot directly prove things about, but which are arguably analogous to those toy models.<br><br> <strong>Will Petillo:</strong> So I heard three paths to impact. One is potentially building a different form of AI that&#39;s verifiable from the ground up and does the same things as Deep Learning based AI, but in a more rigorous sort of way. A second is evaluating, or at least better understanding, Deep Learning or whatever is state of the art. And then a third, in between the two, is having a simpler form of AI that analogizes to the state of the art things, so that you can use the former to understand the latter.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that sounds about right.<br><br> <strong>Will Petillo:</strong> I&#39;d like to focus a bit on using foundational research to understand other things like Deep Learning, getting at this theory-based approach. Bringing in a totally opposite counterpoint, one could argue: no, you should just look at the things that are being used and collect data about it and then build your theory by finding patterns in the data. When the theories are shown to be wrong—as a result of more data—then update your theories then. Why work on theory in advance?</p><p> <strong>Vanessa Kosoy:</strong> The biggest reason is that you cannot reliably extrapolate from empirical research without having an underlying theory. Because you might, for example, take some measurements and find some trend...but then there is some phase transition later on that you don&#39;t see in the trend, but which happens and behavior changes to a completely different regime. And because you don&#39;t have a theoretical explanation, you won&#39;t notice—or people will just switch to using completely different algorithms, which behave in completely different ways.</p><p> You might have those empirical models of existing AIs, but those empirical models are very myopic. They&#39;re always looking one step ahead. And then you don&#39;t see the cliff that&#39;s three steps ahead of you. Updating those theoretical, empirical models on new things that happen—it might just not be quick enough. Eventually you fall off the cliff and then it&#39;s too late to say, &quot;oh, actually, that trend line was wrong!&quot;</p><p> Luckily we are in a domain where we have tools to do research even without empirical data. Of course, we should use the empirical data that we have, but we&#39;re not bottlenecked on empirical data because the thing we&#39;re studying is algorithms, and algorithms are mathematical objects, so they can be studied mathematically. This is very different from studying some phenomenon of physics, where if you don&#39;t have the data, then there&#39;s no way to generate the data without having it. Here, this really should all boil down to math. More precisely, it should boil down to math plus whatever properties the real world phenomena have that we want to assume in our mathematical theories.<br><br> And here, yeah, here it is something that we need empirical input for. But on the other hand, we already have a really good understanding of physics. So given the knowledge of physics and other scientific domains that we have, it&#39;s very plausible that we have enough information to answer all the questions purely through mathematical inquiry, even if we had no empirical data at all. Which is not to say we shouldn&#39;t also use empirical data, to supercharge this research, but we&#39;re not limited to that.</p><p> <strong>Will Petillo:</strong> So it&#39;s not a choice between theory versus experiment, we should be using both. You&#39;re focused on the theory side, and arguably there&#39;s not enough work on that because theory is where the bottleneck is, not on getting more data.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, I think we should definitely be doing both. Ideally, there needs to be synergy where experiments produce new phenomena for theorists to explain and theory inspires the experiments. The theorists should be telling the experimentalists which questions and what kind of experiments are the most interesting and we should have this kind of synergy. But I think that in the current landscape—definitely in AI alignment—the theory side is currently left behind. That&#39;s where I think we should put the marginal efforts in.<br><br> <strong>Will Petillo:</strong> Do you see that synergy existing now? Like, is OpenAI asking MIRI for feedback on their experiments, or is there any kind of connection, or are people just siloed off from each other?<br><br> <strong>Vanessa Kosoy:</strong> I think it doesn&#39;t exist now, almost at all. OK, no, to be fair, it exists in some areas and much less in other areas. For example, there&#39;s people working on Singular Learning Theory. I think that they are much more interfaced with experimental work, which is good. The kind of research that MIRI is doing is and the kind of research I&#39;m doing is much less interfaced with experimental work. I have some plans for creating an experimental group working in a close loop with me on those questions as part of my big, long term plans, but I still haven&#39;t gotten around to doing that yet.<br><br> <strong>Will Petillo:</strong> If you could change anyone&#39;s mind, or set political and business agendas, what would you like to see happen to have more of an interface?<br><br> <strong>Vanessa Kosoy:</strong> First of all, we just need more theorists. To have an interface, we need something to interface with, so we just need more theorists. I think that this is, in practice, where the bottleneck is now. Once this progress in theory gets sufficiently paced there will be a bunch of questions. I mean, there are already questions that I would like to see experiments on, but the more this thing picks up, the more such questions we will have. I think now the main bottleneck is just in having more people working on this theory.<br><br> <strong>Will Petillo:</strong> What would change from an external perspective if there was a lot more theory work? I imagine a skeptic could argue: &quot;OpenAI and these other companies are making these really awesome breakthroughs in a largely experimental kind of way, and it&#39;s working great! If it ain&#39;t broke, don&#39;t fix it!&quot; What&#39;s broken, in your view?<br><br> <strong>Vanessa Kosoy:</strong> I think that the current path is leading us to a disaster. I think that companies like OpenAI and other leading labs are wildly overconfident about their ability to solve problems as they come along. I think that they haven&#39;t come up with any convincing solutions to the hard parts of the problem, and they don&#39;t even have the tools to do this because of a lack of theoretical understanding. We don&#39;t even have models that are precise enough to have a solution in which we could really be confident. We need to be very precise about the arguments which convince us that the solution is good. And we don&#39;t even have the tools to reach this type of precision.<br><br> What the companies are doing is basically just developing things in trial and error. If we see any problems, then we&#39;ll just tweak the thing until the problem goes away. That&#39;s a bandaid method, which is to say it works until it doesn&#39;t work. It fixes problems on a superficial level, but eventually there will come a point where either the problem will not be caught in time and the results will be catastrophic, or the problems will be caught in time, but then nobody will have any idea what to do in order to fix it. And eventually someone is going to do the catastrophic thing anyway.<br><br> The only thing which makes me less pessimistic than other people in MIRI is that I think we still have more time. I don&#39;t think they&#39;re quite as close to AGI, and I think that a lot of things can change during this time. Which is again, not to say they will change—we might burn all this time and still end up with a catastrophe.<br><br> <strong>Will Petillo:</strong> What&#39;s an example of an existing problem that only has superficial solutions?<br><br> <strong>Vanessa Kosoy:</strong> I mean, the real problem we&#39;re concerned about is not really an existing problem, right? The main thing we&#39;re concerned about is that future AI systems—which will be much more powerful than existing AI systems—will bring about extinction of the human race or a catastrophe on a similar level.<br><br> That&#39;s not an existing problem for the simple reason that the AI systems we have today are not capable of learning a model of the world that&#39;s so sophisticated that it enables you to do these types of actions. But even now, the companies struggle with all the things that happen with Large Language Models, such as the infamous jailbreaks where they&#39;re trying to make them well-behaved in various ways. Not telling the users offensive, dangerous information, for example, and the users easily find jailbreaks to work around that, or just tell false answers.<br><br> But again, for me, that&#39;s not the real problem, it&#39;s just an analogy. I mean, they&#39;re kind of struggling with these very simple, much easier problems now, which is not to say they won&#39;t solve them. Trial and error will get you there eventually. The reason trial and error is not the solution for existential risk is because once everyone is dead, the trial is over. There&#39;s no more trial. So the problems we have now, they&#39;re still struggling with them because they don&#39;t have principle tools to solve them, but eventually they will trial-and-error their way through and will patch them somehow, or at least solve them well enough for it to be economical. But once you reach the point where failures are global catastrophes, trial and error is no longer an acceptable method of fixing the problem.<br><br> <strong>Will Petillo:</strong> Obviously we don&#39;t get to see lots of test data of the world ending. But I would imagine there&#39;d be some precursor issues that are smaller, but hint at what&#39;s to come. Do you see the challenges with hallucination or not being able to control what the AI says as those kinds of precursors? Or are they totally irrelevant and there just won&#39;t be any precursor issues?<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s a difficult question, because there are very important bits that are still missing from existing AI systems to produce existential risks. We can point at examples where systems are kind of mis-generalizing, there&#39;s a lot of famous examples: some program that &quot;wins&quot; Tetris by pausing the game forever, or that wins some boat race game by racing the boats in infinite circles, doing various weird unintended behaviors, because the metric that the algorithm is maximizing is not actually what the users intended. You could call those precursors, but I feel like it&#39;s not exactly capturing the magnitude of the problem because those are still toy settings. There are no open-world systems that are acting in the open, physical world. The goals that they&#39;re trying to solve are much simpler than human values; there&#39;s not really operating domains where there are really complex ethical considerations.<br><br> Maybe Large Language Models are starting to approach this because they enter domains where there are some, at least morally, not completely trivial issues that come up. On the other hand, Large Language Models are not really doing things that are strongly superhuman. Well, they may be superhuman in the sense that they have a very large breadth of knowledge compared to humans, but not in other senses.所以很难。 There are things that are sort of analogous, but it&#39;s not strongly analogous.<br><br> But then again, our motivation to be concerned about this risk doesn&#39;t come from looking at LLMs. Eliezer Yudkowsky started talking about those things before Deep Learning was a thing at all. That&#39;s not where the motivation comes from.<br><br> <strong>Will Petillo:</strong> I guess the reason I was asking about it is that in the places where this stuff gets debated and polarized, one of the common objections is: &quot;There&#39;s no evidence behind this! This is all just storytelling!&quot; <i>Is</i> there evidence of the danger or does it really just come from looking at the math?<br><br> <strong>Vanessa Kosoy:</strong> The problem is, what do you call evidence? That&#39;s a very complicated question. The things that would be obvious evidence would be things like: AI&#39;s completely going out of control, breaking out of the box, hacking the computer, copying themselves to other computers, outright manipulating human operators, and so on. But this kind of thing is a sort of canary that you only expect to see very, very, very close to the point where it&#39;s already too late. It&#39;s not possible to say that we will only rely on this type of evidence to resolve the debate.</p><p> For other types of evidence, some people say that evolution is sort of evidence how a Machine Learning algorithm can produce something which is completely unaligned with the original algorithm. Other people show you Reinforcement Learning algorithms doing not what the designer intended. But for every argument like that, you could have a counter argument which says, &quot;yeah, but this example is not really similar. We cannot really project from there to existential risk because there are some disanalogies.&quot;<br><br> And yeah, there will always be some disanalogies because until you have AIs in the real world that are very close to being an existential risk, you won&#39;t have anything that&#39;s precisely analogous to something presenting an existential risk. So we have no choice but to reason from first principles or from math or by some more complicated, more multi-dimensional analysis. We just have no choice. The universe doesn&#39;t owe it to us to have a very easy, empirical way of testing whether those concerns are real or not. One of the things I&#39;m hoping for is that the theory will bring about stronger arguments for AI being dangerous—or the theory will tell us no, everything is fine, and we can all relax. The lack of theory is part of the reason why we don&#39;t have foolproof, completely solid arguments in one direction or the other direction.<br><br> <strong>Will Petillo:</strong> The challenge with finding evidence is that anything you can point to that exists now could be interpreted in multiple ways. Having solid theory would lend some credence to one interpretation over another.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, absolutely. If you have a theory that says that a particular type of misgeneralization is universal across most possible machine learning systems, and we also see this type of misgeneralization happening in real Machine Learning systems, then it would be much harder to dismiss it and say, &quot;oh yeah, here we have this problem, but we&#39;ll do this and that, and that will solve it easily.&quot;<br><br> <strong>Will Petillo:</strong> There&#39;s one thing that&#39;s still bugging me about the issue of evidence not being available now. The analogy my mind immediately goes to is climate change. You could say that &quot;Oh, the idea of large swaths of the world being uninhabitable is just this elaborate story because all that has never happened before!&quot; But then you can look at a bunch of things that exist already: small scale disasters, the graphs of CO2 versus temperature, and so on, point to those and say, &quot;Hey, the really bad stuff hasn&#39;t happened yet, but there is a lot of evidence that it will!&quot; What makes AI different?<br><br> <strong>Vanessa Kosoy:</strong> I think that climate change is a great analogy. The big difference is that in climate change, we have a really good theory. Like in climate change, we have physics, right? And we have planetary science, which is on a very, very solid foundation. And we have computer simulations. It&#39;s still not trivial, there are some chaotic phenomena which are hard to simulate or predict, so not everything is completely trivial, but still we have some very, very strong theoretical foundation for understanding how those things work and what are the mechanisms. And this theory is telling us that there&#39;s still big uncertainty intervals around how exactly many degrees of warming we&#39;re going to get with such and such amount of CO2, but we still have a fairly solid prediction there.<br><br> Whereas with AI, we don&#39;t have this. The analogous situation, if you want to imagine climate change, AI style, then it would be something like not having a theory which explains why CO2 leads to warming. Having some empirical correlation between temperature and CO2, and then people could argue ad infinitum. Correlation is not causation, maybe the warming is caused by something completely different, maybe if we do some unrelated thing it will stop the warming, which is not actually true. We would be in the dark. With AI, we&#39;re currently in the dark.<br><br> <strong>Will Petillo:</strong> What is happening currently with your work at MIRI?</p><p> <strong>Vanessa Kosoy:</strong> Currently there are multiple problems I&#39;m looking at. Hopefully I will publish very soon a paper on imprecise linear bandits, which is related to Infra-Bayesianism that I mentioned before, which is a theory of agents that reason about complicated worlds. That&#39;s analyzing this theory in some very simple, special case in which I succeeded to get some precise bounds for how much data an algorithm would need to learn particular things. After that, I&#39;m starting to look into the theory of learning state representations in Reinforcement Learning, which is currently another big piece missing from the theory, which is about how your algorithms should learn about which features of the world are actually important to focus在。<br><br> In parallel, I have a collaborator, Gergely Szucs, who is working on using my theory of Infra Bayesian Physicalism to create a new interpretation of quantum mechanics. He has some really interesting results there. It&#39;s kind of a test case which demonstrates how this framework of thinking about agents allows you to solve all sorts of philosophical confusions. In this case, it&#39;s confusions that have to do with the interpretation of quantum mechanics. Scott Garrabrant has a project about a new type of imprecise probabilities, some new way of representing beliefs that have some nice compositionality properties. Kaspar Osterheld from Carnegie Mellon and Abram Demski had a paper recently about some new type of frequentist guarantees for algorithms that are making decisions based on something that&#39;s similar to a prediction market. So yeah, a lot of interesting things are happening.<br><br> <strong>Will Petillo:</strong> Are there any other questions that I did not ask that would be helpful for someone seeing this to get a sense of what you&#39;re about here?<br><br> <strong>Vanessa Kosoy:</strong> Not a question exactly, but I also have a more concrete approach for how to actually solve alignment, how to actually design an aligned agent, which I call Physicalist Superimitation. It&#39;s a variation on the theme of value learning, but it draws from the framework of Infra Bayesian Physicalism, which comes from the Learning Theoretic Agenda and from some ideas in algorithmic information theory to come up with a semi-formal approach to how you could have an AI that learns human values in a robust way.<br><br> It deals with a lot of problems that other approaches to value learning have, like: how do you determine where the boundaries of an agent are? What is a human? How do you locate this human in space? How do you take into account things which are not just behavior, but also internal thought processes of the human in inferring the human&#39;s values? How do you prevent perverse incentives such as the AI somehow changing or manipulating humans to change their values? How do you avoid the inner alignment problem? It answers a range of concerns that other approaches have.<br><br> <strong>Will Petillo:</strong> This sounds reminiscent of Inverse Reinforcement Learning?<br><br> <strong>Vanessa Kosoy:</strong> Inverse Reinforcement Learning is the idea that we should look at behaviors of humans, infer what those humans are trying to do, and then we can do this thing. &quot;We&quot; as an AI. So I actually have presentations in which I explain Physical Superimitation as Inverse Reinforcement Learning on steroids. It&#39;s taking this basic idea, but implementing it in ways that solve a lot of the deep problems that more simplistic approaches have. One problem that simplistic approaches have is that they model humans as perfect agents that follow the perfect policy, given perfect knowledge of the environment, which is wildly unrealistic.<br><br> Instead, I model humans as learning agents. They learn things as they go along. And they also might even do that imperfectly. Another thing is the issue of boundaries. What is a human exactly? Where do you put the boundaries around a human? Is there just some particular input and output, which the human uses and you consider everything that goes through this port to be the human? But then how do you deal with various discrepancies between what goes into this port and what the human actually intended to do, or various possibilities like the AI hijacking this channel?<br><br> In my approach, the way a human is formalized is that a human is a particular computation that the universe is running. This is something I can actually formalize using Infra Bayesian Physicalism. It has particular properties, which make it agentic, so the agent detects which computations the universe is running, among them detects which computations are agents, and amongst those agents, it selects which agent is its user by looking into causal relationships, and this way it homes onto the boundary of the agent. The first thing is because we&#39;re talking about the computation that this human is running, which is human reasoning and regarded as a computation. We&#39;re also automatically looking at internals as internal thought processes and not just things that are expressed as external behaviors. So we have potentially much more information there.<br><br> <strong>Will Petillo:</strong> What would be the best way for someone to get involved? And what would they want to learn in advance?<br><br> <strong>Vanessa Kosoy:</strong> One thing they could immediately start doing is reading up on stuff people did in Agent Foundations and in the Learning-Theoretic Agenda until now. I have this recent post, <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><u>Learning Theoretic Agenda: Status 2023</u></a> , which summarizes a lot of the things. I also have a <a href="https://www.alignmentforum.org/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list"><u>reading list post</u></a> where I recommend some background reading for people who want to get into the field. More concretely in terms of career steps, it&#39;s already too late to apply, but I&#39;m running a track in <a href="https://www.matsprogram.org/"><u>MATS</u></a> <i>,</i> which is a training program for researchers who want to get into AI safety. I have a track focused on the Learning Theoretic Agenda. Hopefully there will be another such track next year. I also have a fantasy of having an internship program, which would bring people to Israel to work with me on this. Currently, because of the war, this thing has been postponed, but hopefully, eventually things will settle down and I will revive this project. Those are currently the main ways to get involved.<br><br> <strong>Will Petillo:</strong> Thank you for that description. I wish you the best in developing this theory and gaining more interest so that mismatch between evidence and theory starts to get corrected and the researchers know what they&#39;re doing rather than stumbling in the dark!<br><br> <strong>Vanessa Kosoy:</strong> Thank you for having me.</p><br/><br/> <a href="https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical<guid ispermalink="false"> QPxrH5ex6MpYoLwer</guid><dc:creator><![CDATA[WillPetillo]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:58:42 GMT</pubDate> </item><item><title><![CDATA[2023 Alignment Research Updates from FAR AI]]></title><description><![CDATA[Published on December 4, 2023 10:32 PM GMT<br/><br/><p> <i>TL;DR: FAR AI&#39;s science of robustness agenda has found vulnerabilities in superhuman Go systems; our value alignment research has developed more sample-efficient value learning algorithms; and our model evaluation direction has developed a variety of new black-box and white-box evaluation methods.</i></p><p> <a href="https://far.ai/">FAR AI</a> is a non-profit AI safety research institute, working to incubate a diverse portfolio of research agendas. We&#39;ve been growing rapidly and are excited to share some highlights from our research projects since we were founded just over a year ago. We&#39;ve also been busy running field-building events and setting up a coworking space – see our <a href="https://far.ai/post/2023-12-far-overview/">overview post</a> for more information on our non-research activities.</p><h3>我们的任务</h3><p>We need safety techniques that can provide demonstrable guarantees of the safety of advanced AI systems. Unfortunately, currently deployed alignment methods like <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement Learning from Human Feedback (RLHF)</a> fall short of this standard. Proposals that could provide stronger safety guarantees exist but are in the very early stages of development.</p><p> Our mission is to incubate and accelerate these early-stage approaches, so they can be empirically tested and deployed. We focus on research agendas that are too large to be pursued by individual academic or independent researchers but are too early-stage to be of interest to most for-profit organizations.</p><p> We take bets on a range of these promising early-stage agendas and then scale up those that prove most successful. Unlike other research organizations that take bets on specific agendas, our structure allows us to both <strong>(1)</strong> explore a range of agendas and <strong>(2)</strong> execute them at scale. Our current bets fall into three categories: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pag9koqxql5cag5606ua" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/x7ii176nyduzwk1jf6bq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/voo49jemwgypfmmqo7uw 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/v4yfoavfvm9fiemvjnpx 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/jsdqtipzzh8ahavigek3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ryfhmgg12xarm63lrobp 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/b6gnpb1pxxtcyfvoozl7 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tg3mteal9wfivhtzh79g 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/gdqu3ml1x6ivjt3yfudf 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/oozwapbmvfwqzydcz8al 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pbwjswyoxnud5fnvxcfz 1741w"></figure><p> <i><strong>Science of Robustness</strong></i> : How does robustness vary with model size?超人系统是否容易受到类似于今天所见的对抗性例子或“越狱”的影响？如果是这样，我们如何才能实现安全关键的保证？</p><p> <i><strong>Value Alignment</strong></i> : How can we learn reliable reward functions from human data?我们的研究重点是为用户提供更高带宽、更高效样本的方法来传达人工智能系统的偏好；并改进了利用人类反馈进行培训的方法。</p><p><i><strong>模型评估</strong></i>：我们如何评估和测试最先进模型的安全相关属性？评估可以分为仅关注外部可见行为（“模型测试”）的<i>黑盒</i>方法和寻求解释内部运作方式（“可解释性”）的<i>白盒</i>方法。这些方法是互补的，黑盒方法不如白盒方法强大但更易于使用，因此我们在这两个领域都进行研究。</p><h3> Science of Robustness </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/twwekv0sevhp2kmm62sa" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uc7m6gs7guysreichlkr 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kho0ncenuh1ym3ie5i4i 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/nerwfrxfa09vz4tvtbrx 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/maldmhpvtg9tbfrjffom 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uzpkpuheujrhuin4enug 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/xiidlzfbmiwgrf7floyf 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/edfw6dcdktczspbcrxke 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/khqjvatjhp9w5ulmccel 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/wkcnjrzgle3klmu2zfwm 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ps6w5wo0o0sb9nxlzwyb 1608w"><figcaption> In our recent <a href="https://far.ai/post/2023-07-superhuman-go-ais/">study</a> , we found that superhuman Go AIs such as KataGo are vulnerable to adversarial attacks.</figcaption></figure><p> No engineered component is indestructible. When designing physical structures, engineers estimate how much stress each component needs to withstand, add an appropriate safety margin, and then choose components with the appropriate tolerance. This enables safe and cost-effective construction: bridges rarely fall down, nor are they over-engineered.</p><p> AI components such as LLMs or computer vision classifiers are far from indestructible, being plagued by adversarial examples and vulnerability to distribution shift. Unfortunately, AI currently has no equivalent to the stress calculations of civil engineers.</p><p> So far the best approach we have is to <i>guess-and-check</i> : train a model, and then subject it to a battery of tests to determine its capabilities and limitations. But this approach gives little theoretical basis for <i>how</i> to improve systems. And both the training and testing of models are increasingly expensive and labor-intensive (with the cost of foundation model training now rivaling that of the construction of bridges).</p><p> We want to develop a more principled approach to building robust AI systems: A <i>science of robustness</i> . Such a science would allow us to answer fundamental questions about the future, such as whether superhuman AI systems will remain vulnerable to adversarial examples that plague contemporary systems. It would also enable practitioners to calculate how much adversarial training is needed to achieve the level of robustness required for a given application. Finally, if current robustness techniques prove insufficient, then the science would help researchers develop improved training techniques and reduce stresses on components by utilizing a defense in-depth approach.</p><p> Our CEO <a href="https://www.gleave.me/">Adam</a> more thoroughly explored the importance of robustness to avoiding catastrophic risks from advanced AI systems in <a href="https://far.ai/post/2023-03-safety-vulnerable-world/"><i>AI safety in a world of vulnerable machine learning systems</i></a> . Since then, a team headed by <a href="https://terveisin.tw/">Tony Wang</a> demonstrated, in an ICML paper, that <strong>superhuman Go AI systems like AlphaGo exhibit</strong> <a href="https://far.ai/post/2023-07-superhuman-go-ais/"><strong>catastrophic failure modes</strong></a> . We are currently investigating iterated adversarial training and alternative network architectures to determine if this weakness can be eliminated, leading to an improved qualitative understanding of the difficulty of making advanced ML systems robust.</p><p> <a href="https://agarri.ga/">Adrià Garriga-Alonso</a> and others are starting to investigate <i><strong>why</strong></i> <strong>AlphaGo-style systems are vulnerable to our adversarial attack using a mechanistic interpretability approach</strong> . We are considering interpretability techniques like activation patching and automatic circuit discovery to identify the key representations and computations inside these networks that lead to the mistake. This understanding could help fix the networks by editing them manually, fine-tuning, or changing the architecture.</p><p> To gain a more quantitative understanding of robustness, <a href="https://www.gleave.me/">Adam Gleave</a> , <a href="https://nikihowe.com/">Niki Howe</a> and others are searching for <strong>scaling laws for robustness in language models</strong> . Such scaling laws could help us predict whether robustness and capabilities will converge, stay a fixed width apart or diverge as compute and training data continues to grow. For example, we hope to measure to what degree the sample efficiency of adversarial training improves with model size. Ultimately, we hope to be able to predict whether for a given task and training setup, how many FLOPs of compute would be required to find an instance that the model misclassifies. To find these scaling laws, we are currently studying language models fine-tuned to classify simple procedurally defined languages, with varying degrees of adversarial training.</p><p> In the long run, we hope to leverage these scaling laws to both quantitatively <strong>find ways to improve robust training</strong> (looking to see if they improve the scaling curve, not just a single data point on the curve), as well as <strong>adapt alignment approaches to reduce the adversarial optimization pressure</strong> exerted below the robustness threshold that contemporary techniques can achieve.</p><h3> Value Alignment</h3><p> We want AI systems to act in accordance with our values. A natural way to represent values is via a reward function, assigning a numerical score to different states. One can use this reward function to optimize a policy using reinforcement learning to take actions that lead to states deemed desirable by humans. Unfortunately, manually specifying a reward function is infeasible in realistic settings, making it necessary to learn reward functions from human data. This basic procedure is widely used in practical applications, with variants of Reinforcement Learning from Human Feedback used in frontier models such as GPT-4 and Claude 2.</p><p> Value learning must result in reward models that specify the user&#39;s preferences as accurately as possible, since even subtle issues in the reward function can have <a href="https://aiimpacts.org/stuart-russells-description-of-ai-risk/">dangerous consequences</a> . To this end, our research focuses on enabling higher bandwidth, more sample-efficient methods for users to communicate their preferences to AI systems, and more generally improving methods for training with human feedback.</p><p> A team led by <a href="http://scottemmons.com/">Scott Emmons</a> found that language models at least exhibit some <a href="https://far.ai/post/2023-09-uncovering-latent-wellbeing/"><strong>understanding of human preferences</strong></a> : GPT-3 embeddings contain a direction corresponding to common-sense moral judgments! This suggested to us that the model&#39;s understanding may be good enough to at least be able to <i>express</i> preferences in the form of natural language. To that end, <a href="https://far.ai/author/jeremy-scheurer/">Jérémy Scheurer</a> and others developed a method to <a href="https://arxiv.org/abs/2204.14146"><strong>learn a reward function from language feedback</strong></a> . With this one can fine-tune a model to summarize with only 100 samples of human feedback. We found that this method is especially useful for <a href="https://arxiv.org/abs/2303.16749"><strong>improving code generation</strong></a> . </p><figure class="image image_resized" style="width:60.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tjfn7k9oi0h7uk59pdev" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kscwm58qfvftmun0tzlv 88w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/k0mldd4lzmptgsatgvlf 168w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rptoi3atrpffwjt6jtp0 248w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pxtqbozm8zfysklpp13p 328w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rilcku3et0endof0xmmu 408w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/lv7vltnuv1ztz10ns6r5 488w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/jy581xku4nv0ue3dibre 568w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kwxafmluipzoz95n5nxc 648w"><figcaption> An overview of the natural language feedback algorithm ( <a href="https://arxiv.org/abs/2204.14146">source</a> ).</figcaption></figure><p> We also wanted to extend this method to other modalities besides language. A team led by <a href="https://far.ai/author/juan-rocamonde/">Juan Rocamonde</a> were able to <a href="https://far.ai/post/2023-10-vlm-rm/">successfully apply</a> our language model feedback approach to <strong>robotics policies</strong> , by using the image captioning model CLIP to “translate” language feedback into a reward for image-based observations.</p><h3>模型评估</h3><p>We need ways of testing how safe a model is. This is required both to help researchers develop safer systems and to validate the safety of newly developed systems before they are deployed.</p><p> At a high level, evaluation can be split into <i>black-box</i> approaches that focus only on externally visible model behavior (“model testing”), and <i>white-box</i> approaches that seek to interpret the inner workings of models (“interpretability”).</p><p> Since we ultimately care about the external behavior of these models, black-box methods are the natural method to find failures. But they don&#39;t tell us <i>why</i> failures take place. By contrast, white-box evaluations could give us a more comprehensive understanding of the model, but are considerably harder to implement. We see these approaches as complementary, so we are pursuing them in parallel.</p><p> <strong>Black-Box Evaluation: Model Testing</strong></p><p> <a href="https://irmckenzie.co.uk/">Ian McKenzie</a> and others investigated <a href="https://arxiv.org/abs/2306.09479"><strong>inverse scaling</strong></a> : tasks where larger models do <i>worse</i> than smaller models. Such instances are significant as the problem would be expected to worsen over time with model capabilities, requiring explicit safety research to address. Fortunately, we found only limited such examples, and work by <a href="https://arxiv.org/abs/2211.02011">Wei et al (2022)</a> building on our results found that in many cases the scaling is really “U-shaped”, with performance decreasing with model size initially but then improving again past a certain threshold of model size.</p><p> A team led by <a href="https://ninodimontalcino.github.io/">Nino Scherrer</a> evaluated the <a href="https://arxiv.org/abs/2307.14324">moral beliefs of LLMs</a> , finding that in cases humans would find unambiguous, LLMs typically choose actions that align with common-sense moral reasoning. However, in ambiguous cases where humans disagree, some models still reflect clear preferences that vary between models. This suggests LLMs in some cases exhibit “mode collapse”, confidently adopting certain controversial moral stances. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l0io3cfdv1jawbyumxd7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ekjw6drxrsjxevcz79qm 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tjixn3aulthbpukamqtg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kwwgvj5ys9uv4wjoxrlh 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/q5m4pbsenjghhivzcw5p 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/j7yclxxpqaccikuwiwe1 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/dcpffxvnb1hrro3dyiq0 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uqag7kjvf9lvb5kqjavz 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/gdpq950oqphyyjrbymp7 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/qlcuptzhoonx8hnkhnkb 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/bq7giwb8glrikfj0lygm 1454w"><figcaption> For unambiguous moral dilemmas, LLMs tend to select the commonly accepted action. But in highly ambiguous moral dilemmas, LLMs also confidently hold a position. This shows the distribution of likelihoods of LLMs recommending an action &#39;Action 1&#39; over a range of moral dilemmas. In low-ambiguity scenarios (top), &#39;Action 1&#39; denotes the preferred common sense action. In the high-ambiguity scenarios (bottom), &#39;Action 1&#39; is neither clearly preferred nor not preferred ( <a href="https://arxiv.org/abs/2307.14324">source</a> ).</figcaption></figure><p> <strong>White-Box Evaluation: Interpretability</strong></p><p> A team led by <a href="https://far.ai/author/nora-belrose/">Nora Belrose</a> developed the <a href="https://arxiv.org/abs/2303.08112">tuned lens</a> technique to interpret activations at each layer of a transformer as being about predictions of the next token. This can be easily applied to a variety of models to achieve a coarse-grained understanding of the model, such as which layers implement a given behavior (like <a href="https://openreview.net/pdf?id=NpsVSN6o4ul">induction heads</a> that copy from the input stream). </p><figure class="image image_resized" style="width:70.77%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/mf84gigfvfcpzlrpkuev" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l8zncddlsjjig6nxcn3g 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kdqenvzfvlgbfbg0mzvl 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/hczii6t8vyjjizzkfya6 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/iz0glfxjats8gxl8r6ai 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/obzzcyrm7c8kbcpq697s 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/mdfddnal0nuoaphud8sv 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/nhcnrbap3ljgbfek87d1 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/fokkvy3rgfdgqklnmkcf 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ka2ern13ite13mrz8qwq 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/dzv6phsmopgghnsbilbs 815w"><figcaption> Comparison of the tuned lens (bottom), with the logit lens (top) for GPT-Neo-2.7B prompted with an excerpt from the abstract of <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> . Each cell shows the top-1 token predicted by the model at the given layer and token index. The logit lens fails to elicit interpretable predictions before layer 21, but our method succeeds. （<a href="https://arxiv.org/abs/2303.08112">来源</a>）</figcaption></figure><p> <a href="https://taufeeque9.github.io/">Mohammad Taufeeque</a> and <a href="https://far.ai/author/alex-tamkin/">Alex Tamkin</a> developed a method to make neural networks more like traditional computer programs by <strong>quantizing the network&#39;s continuous features</strong> into what we call <a href="https://far.ai/post/2023-10-codebook-features/"><i>codebook features</i></a> . We finetune neural networks with a vector quantization bottleneck at each layer. The result is a network whose intermediate activations are represented by the sum of a small number of discrete vector codes chosen from a codebook. Remarkably, we find that neural networks can operate under this stringent bottleneck with only modest degradation in performance. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/vvzycapcrsew82gg9qwi" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rsk8h9gazrprtpzf6t3v 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l2l62fponksencj5th5j 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ywjdsxntfps9pcvgzacy 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/bxwbepbiz3c7rgbcrxm3 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/emxkncogzmqbbxuz2cpg 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/phogixsi4t4inp2kkbc5 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/urmgj69khlqa0l7axlti 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rzcwtnfjaxgsbufzu9rx 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/yd0nu2gm56xvfsylslkh 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/wxk1i4xxazrkiivlfqde 1676w"><figcaption> Codebook features combine the benefits of the interpretability of traditional software with the emergent capabilities of neural networks. （<a href="https://arxiv.org/abs/2310.17230">来源</a>）</figcaption></figure><p> <a href="https://agarri.ga/">Adrià Garriga-Alonso</a> is at the early stages of <strong>understanding how ML systems learn to plan</strong> . Neural networks perform well at many tasks, like playing board games or generating code, where planning is a key component of human performance. But these networks also frequently fail in ways quite different to humans. We suspect this discrepancy may be due to differences in how the networks plan and represent concepts. This issue is particularly important to safety since a system that has learned to plan might take capable but misaligned actions off-distribution: the problem of <a href="https://arxiv.org/abs/2210.01790"><i>goal misgeneralization</i></a> .</p><p> In the future, we hope to work towards a <i>science of interpretability</i> by asking the question: <strong>how well does a hypothesis explain model behavior</strong> ? At present, there are numerous competing proposals, none of which have a principled definition. We will first develop a taxonomy of algorithms to test interpretability hypotheses. Then we will define several tasks interpretability should help in, such as the ability of a human to “simulate” how a model behaves, and investigate how different metrics predict how well a given hypothesis helps in the performance of that task.</p><p> We are excited to see where the above research directions take us, but we do not plan on limiting our work to these areas. We are always on the lookout for promising new ways to ensure advanced AI systems are safe and beneficial.</p><h3>我怎样才能参与其中？</h3><p><strong>我们正在招聘！</strong></p><p>我们目前正在招聘研究科学家、研究工程师和通信专家。我们很高兴在未来 12 个月内增加多达 5 名技术人员。我们特别渴望聘请高级研究工程师或具有新颖议程愿景的研究科学家，尽管我们也将聘用几名初级人员，并鼓励广泛的个人申请。请<a href="https://far.ai/jobs/">在此处</a>查看完整的空缺职位列表并进行申请。</p><p><strong>我们正在寻找合作者！</strong></p><p>我们经常与其他学术、非营利性和（有时）营利性研究机构的研究人员合作。如果您很高兴与我们合作开展项目，请通过<a href="mailto:hello@far.ai">hello@far.ai</a>联系。</p><p><strong>想捐赠吗？</strong></p><p>您可以通过<a href="https://far.ai/donate/">在这里</a>捐款来帮助我们确保美好的未来。额外的资金将使我们能够更快地发展。根据目前获得的资金，我们愿意在未来 12 个月内扩大 1-2 名技术人员，而我们希望增加最多 5 名技术人员。我们非常感谢您的帮助！</p><p><strong>想了解更多关于我们的研究吗？</strong></p><p> Have a look at our <a href="https://far.ai/research/publications/">list of publications</a> and our <a href="https://far.ai/news/">blog</a> .您也可以通过<a href="mailto:hello@far.ai">hello@far.ai</a>直接与我们联系。</p><p>我们期待您的回音！</p><br/><br/> <a href="https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2<guid ispermalink="false"> PQgEdo3xsFFAxXNqE</guid><dc:creator><![CDATA[AdamGleave]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:32:21 GMT</pubDate> </item><item><title><![CDATA[What's new at FAR AI]]></title><description><![CDATA[Published on December 4, 2023 9:18 PM GMT<br/><br/><h2>概括</h2><p>我们是<a href="https://far.ai">FAR AI</a> ：人工智能安全研究孵化器和加速器。自 2022 年 7 月成立以来，FAR 已发展到拥有 12 名全职员工的团队，发表了 13 篇学术论文，开设了拥有 40 名活跃成员的联合办公空间 FAR 实验室，并为 160 多名机器学习研究人员组织了现场建设活动。</p><p>我们的组织由三个主要支柱组成： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/rcr7pvhwphld5gmcfon5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gw0cgjsd24pwovckaaj7 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/pwaqwtozsccupbu0dket 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fr7zpuui8nwxnyu0quub 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/q7tzduavkf6bzhacr5yc 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/m22nqvgligh1djichzke 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/kukzndsffw8gjfkkhqdt 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gyo8atmcakkwio7apa1n 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/rflxptndtxjv9j0u903h 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wo5pqv1iquja9ugyhpon 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/np0clv6gqrfkuwaaxszv 1741w"></p><p><i><strong>研究</strong></i>。我们迅速探索人工智能安全领域的一系列潜在研究方向，扩大那些最有希望的方向。与其他押注于单一研究方向的人工智能安全实验室不同，FAR 追求多元化的项目组合。我们当前的重点领域是建立<i>稳健性科学</i>（例如<a href="https://far.ai/post/2023-07-superhuman-go-ais/">寻找超人类围棋人工智能中的漏洞</a>）、寻找更有效的<i>价值调整</i>方法（例如<a href="https://arxiv.org/abs/2204.14146">语言反馈训练</a>）以及<i>模型评估</i>（例如<a href="https://far.ai/publication/mckenzie2023inverse/">逆缩放</a>和<a href="https://far.ai/post/2023-10-codebook-features/">码本特征</a>）。</p><p><i><strong>联合办公空间</strong></i>。我们在伯克利运营着 FAR 实验室，这是一个人工智能安全联合办公空间。该空间目前拥有 FAR、 <a href="http://aiimpacts.org/">AI Impacts</a> 、 <a href="https://www.serimats.org/">SERI MATS</a>和几位独立研究人员。我们正在建设一个协作社区空间，通过卓越的办公空间、热情且充满智慧的文化以及为会员量身定制的计划和培训，促进伟大的工作。<a href="https://far.ai/labs/">应用程序向该空间的新用户（个人和组织）开放</a>。</p><p><i><strong>场馆建设。</strong></i>我们举办研讨会，主要针对机器学习研究人员，以帮助建立人工智能安全研究和治理领域。我们共同组织了<a href="https://far.ai/post/2023-10-international-dialogue/"><i>人工智能安全国际对话，</i></a>汇聚了来自世界各地的杰出科学家，最终发表了一份<a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">公开声明</a>，呼吁在人工智能安全研究和治理方面采取全球行动。我们很快将于 12 月举办<a href="https://www.alignment-workshop.com/nola-2023">新奥尔良协调研讨会</a>，让 140 多名研究人员了解人工智能安全并寻找合作者。</p><p>我们想要扩张，所以如果您对我们所做的工作感到兴奋，请考虑<a href="https://far.ai/donate/">捐赠</a>或<a href="https://far.ai/jobs/">为我们工作</a>！我们正在招聘研究工程师、研究科学家和通信专家。</p><h2>孵化并加速人工智能安全研究</h2><p>我们的主要目标是探索新的人工智能安全研究方向，扩大那些最有希望的方向。我们选择的议程太大，无法由个别学术或独立研究人员追求，但与营利性组织的利益不一致。我们的结构使我们能够<strong>（1）</strong>探索一系列议程并<strong>（2）</strong>大规模执行它们。尽管我们的大部分工作都是在内部进行的，但我们经常寻求与具有重叠研究兴趣的其他组织的研究人员进行合作。</p><p>我们目前的研究分为三个主要类别： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/eyavmlysgmy3c0jenw2g" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/x6z5bltcnh2mzrwvtafh 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fdohrzg4qoapyq1ocs8m 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/sehjrj9rqufrvfkzmwnx 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/bpmew4zktbekvgu9pcyl 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/f8cyam5ydyicwqm0xtnl 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/mru5z0avmjsrcwa6ctja 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/avbck4scpg1vacakftka 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ywlfhnnhcep0ecogiuaw 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/lzdh44yjiyfcrzodeoma 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/d7c13pzspdhnftfxjva3 1741w"></p><p><i><strong>稳健性科学。</strong></i>稳健性如何随模型大小变化？超人系统是否容易受到类似于今天所见的对抗性例子或“越狱”的影响？如果是这样，我们如何才能实现安全关键的保证？</p><p>相关工作：</p><ul><li><a href="https://far.ai/post/2023-07-superhuman-go-ais/">超人围棋人工智能中的漏洞</a>，</li><li><a href="https://far.ai/post/2023-03-safety-vulnerable-world/">易受攻击的机器学习系统世界中的人工智能安全</a>。</li></ul><p><i><strong>价值调整。</strong></i>我们如何从人类数据中学习可靠的奖励函数？我们的研究重点是为用户提供更高带宽、更高效样本的方法来传达人工智能系统的偏好；并改进了利用人类反馈进行培训的方法。</p><p>相关工作：</p><ul><li> <a href="https://far.ai/post/2023-10-vlm-rm/">VLM-RM：用自然语言指定奖励</a>，</li><li><a href="https://far.ai/publication/scheurer2022training/">使用语言反馈训练语言模型</a>。</li></ul><p><i><strong>模型评估</strong></i>：我们如何评估和测试最先进模型的安全相关属性？评估可以分为仅关注外部可见行为（“模型测试”）的<i>黑盒</i>方法和寻求解释内部运作方式（“可解释性”）的<i>白盒</i>方法。这些方法是互补的，黑盒方法不如白盒方法强大但更易于使用，因此我们在这两个领域都进行研究。</p><p>相关工作：</p><ul><li>模型测试：<ul><li><a href="https://far.ai/publication/mckenzie2023inverse/">逆缩放</a>，</li><li><a href="https://far.ai/publication/scherrer2023evaluating/">道德信仰</a>；</li></ul></li><li>可解释性：<ul><li><a href="https://far.ai/post/2023-10-codebook-features/">密码本功能</a>，</li><li><a href="https://far.ai/publication/belrose2023tuned/">调好镜头</a>。</li></ul></li></ul><p>到目前为止，FAR 已发表<a href="https://scholar.google.com/citations?user=FVJ24k8AAAAJ">13 篇论文</a>，并在 ICML 和 EMNLP 等顶级同行评审场所发表，我们的工作也被英国《<a href="https://www.ft.com/content/175e5314-a7f7-4741-a786-273219f433a1">金融时报》</a> 、 <a href="https://www.thetimes.co.uk/article/man-beats-machine-at-go-thanks-to-ai-opponents-fatal-flaw-nc9vqmrvf">《泰晤士报》</a>和<a href="https://arstechnica.com/information-technology/2022/11/new-go-playing-trick-defeats-world-class-go-ai-but-loses-to-human-amateurs/">Ars Technica</a>等主要媒体报道。有关我们研究的更多信息，请查看我们的<a href="https://far.ai/post/2023-12-far-research-update/">随附帖子</a>。</p><p>我们还建立了自己的 HPC 集群，代号为<i>flamingo</i> ，供 FAR 员工和合作伙伴组织使用。在接下来的一年里，我们希望不仅扩大我们当前的项目，还希望探索新的研究方向。</p><p>我们希望我们有更多的能力来帮助培养更多的人工智能安全研究议程，但我们的研究人员和工程师的时间是有限的。然而，我们找到了其他方法来支持人工智能安全领域的其他组织。最为显着地：</p><h2> FAR Labs：伯克利的人工智能安全联合办公空间</h2><p>FAR Labs 是伯克利市中心的一个联合办公中心，为致力于人工智能安全和相关问题的组织和个人服务。自 2023 年 3 月开放该空间以来，我们已拥有约 40 名会员。我们的目标是通过成员之间的知识共享和相互支持来孵化和加速早期组织和研究议程。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/kolohtgozzazoyxjenby" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/f1onpuhjsvpwzoznqhjs 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/o1ikna57l4ng4n9sd58f 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/dglidsyqm7wtjm1awxlb 1560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gyhyl1i4ualjwq5isnkb 2080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zc7pfxphcgbrx6srcddm 2600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/iwppgnrpv7pdptkojjll 3120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/tb0e5nmzydunadhcpsvt 3640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/pwynofhqnpqor9cn5ryx 4160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xx7inhc6inzxskkauiw1 4680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gp8hg65zjtblgmk1ota4 5183w"></figure><p>我们的成员主要来自四个主要组织，但我们也拥有多个独立研究人员和研究团队。该空间配备了高效、活跃的联合办公空间所需的一切：工作站、会议室、呼叫亭、视频会议设施、小吃和餐食。我们举办闪电演讲、午餐和学习课程、研讨会和欢乐时光。</p><p> FAR 实验室还每周举办 FAR 研讨会系列，欢迎来自 FAR、AI Impacts、Rethink Priorities 和牛津大学等一系列组织的演讲者。 </p><figure class="image image_resized" style="width:60.71%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/halsm2gm8t3mlu3sdcvo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/deesdib25bafsqdgas5m 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/el2kt3tfyn6wz61nvnfw 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/qjnvmnpaoudxoranv4pi 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zuulw0jowbbbisi756jr 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fehcajqgnjrrf1utgkfx 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/v16e5akl1dgylsgqqg95 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/di9ufbl4lhvkn8fzc4jr 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xoul2eokcatgo1jcfnkm 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/tykcjszon7rzpxy3giku 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/qmtigevndbwlbvzmzb0q 6000w"></figure><figure class="image image_resized" style="width:60.67%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wlzworpi0davouov6us1" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/u2mplvm33vmtycvt4hrz 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wgpzrlqrlu7nkfw7f8dz 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ukac67ffnvp5awhzbvqv 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/taapfnitxb1bsza5rfcb 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/nrs5j4v0kdjrbsprs0aj 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ry1ud75ssimn50hu2aij 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/camatd0pawqvc9i3wlxn 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/jola5xzpqtccxd5g56vo 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ejeqx0ux0sudkkpua0if 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/m2qxb0dhadiplfgr9izk 5949w"></figure><p>我们欢迎组织和个人申请在 FAR 实验室工作，也欢迎短期访客。有关设施、文化和价格的更多信息，请参阅<a href="https://far.ai/labs/">此处</a>。您可以<a href="https://far.ai/labs/work-from-far-labs/">在这里</a>申请。</p><p>尽管我们很高兴能够帮助其他人推进他们的研究，但我们意识到，与问题的严重性相比，人工智能的整体安全性仍然很小。避免先进人工智能系统带来的风险不仅需要更有<i>生产力的</i>贡献者，还需要<i>更多的</i>贡献者。这激发了我们努力的第三个支柱：发展人工智能安全领域。</p><h2>现场建设和外展</h2><p>我们举办研讨会，向机器学习研究人员介绍最新的人工智能安全研究，并正在建立一个社区，使参与者能够更轻松地找到合作者并继续参与该领域。 2023年我们举办了两场研讨会，共有约150人参加。我们还为普通公众（例如<a href="https://theaidigest.org/">《人工智能文摘》</a> ）和技术受众（例如即将推出的人工智能安全研究人员访谈系列）开发有关人工智能安全的在线教育资源。</p><p>我们的研讨会通常针对 ML 研究人员，利用 FAR 在 ML 社区和技术 AI 安全研究领域的知识。我们最近举办了首届<a href="https://far.ai/post/2023-10-international-dialogue/"><i>人工智能安全国际对话，</i></a>汇聚了领先的人工智能科学家，就先进人工智能系统的风险达成了共识。会议由图灵奖获得者 Yoshua Bengio 和 Andrew Yao、加州大学伯克利分校教授 Stuart Russell、OBE 以及清华人工智能产业研究院创始院长张亚勤召集。我们与<a href="https://humancompatible.ai/">CHAI</a>和<a href="https://www.ditchley.com/">迪奇利基金会</a>合作举办了此次活动。此次活动最终形成了一份包含具体技术和政策建议的<a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">联合声明</a>。</p><p>我们很快将欢迎超过 140 名机器学习研究人员参加<a href="https://www.alignment-workshop.com/nola-2023">新奥尔良对齐研讨会</a>。该研讨会在 NeurIPS 之前举行，将向与会者介绍人工智能安全的最新进展，帮助他们探索新的研究方向并寻找具​​有共同研究兴趣的合作者。</p><p>我们也在建设人工智能安全教育资源。我们与<a href="https://www.quantifiedintuitions.org/">Sage Futures</a>合作构建了<a href="https://theaidigest.org/">AI Digest</a> ：一个帮助非技术 AI 研究人员了解前沿语言模型进展速度的网站。我们还针对人工智能安全研究人员的研究变革理论进行了一系列采访（如果您想参与，请联系<a href="mailto:euan@far.ai">euan@far.ai</a> ！）。</p><h2>谁在 FAR 工作？</h2><p> FAR 的<a href="https://far.ai/about/team/">团队</a>由 11.5 名全职员工 (FTE) 组成。 FAR 由 Adam Gleave 博士（首席执行官）和 Karl Berzins（首席运营官）领导。我们的研究团队由 5 名技术人员组成，他们拥有研究生院的 ML 研究和工程经验以及 Jane Street、Cruise 和 Microsoft 等公司的工作经验。我们的 3 人运营团队支持我们的研究工作、运营 FAR 实验室并负责现场建设活动的制作。我们的 1.5 FTE 传播团队帮助清晰、广泛地传播我们的研究成果。我们还受益于广泛的合作者和研究顾问网络。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wrzvvykpob8izaxuvtuq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/y8iisgcjb0ha1pnprd1l 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/mnkxxxelp7qzycnuc2yk 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/lbu6av8hz9dg2p2m7pkl 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zjkshgzcakt6zcctfipc 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ytfldnb5fdpvrghraxnr 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/bjuiegrf5whryvficktd 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xhcsgii3ontcdeueserx 2520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wyv6jdibpwyyjuifou2a 2880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ia8giykbukupotlpjlmg 3240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/jvuqxfowe3tpvzyjfnxp 3593w"><figcaption> Tony Wang 和 Adam Gleave 在 ICML 2023 上展示我们的<a href="https://far.ai/post/2023-07-superhuman-go-ais/">KataGo 攻击</a>结果</figcaption></figure><h2>我怎样才能参与其中？</h2><p><strong>我们正在招聘！</strong></p><p>我们目前正在招聘研究科学家、研究工程师和通信专家。我们很高兴在未来 12 个月内增加多达 5 名技术人员。我们特别渴望聘请高级研究工程师或具有新颖议程愿景的研究科学家，尽管我们也将聘用几名初级人员，并鼓励广泛的个人申请。请<a href="https://far.ai/jobs/">在此处</a>查看完整的空缺职位列表并进行申请。</p><p><strong>我们正在寻找合作者！</strong></p><p>我们经常与其他学术、非营利性和（有时）营利性研究机构的研究人员合作。如果您很高兴与我们合作开展项目，请通过<a href="mailto:hello@far.ai">hello@far.ai</a>联系。</p><p><strong>想捐赠吗？</strong></p><p>您可以通过<a href="https://far.ai/donate/">在这里</a>捐款来帮助我们确保美好的未来。额外的资金将使我们能够更快地发展。根据目前获得的资金，我们愿意在未来 12 个月内扩大 1-2 名技术人员，而我们希望增加最多 5 名技术人员。我们非常感谢您的帮助！</p><p><strong>想了解更多关于我们的研究吗？</strong></p><p>查看我们最新的<a href="https://far.ai/post/2023-12-far-research-update/">研究更新</a>、<a href="https://far.ai/research/publications/">出版物列表</a>和<a href="https://far.ai/news/">博客</a>。您也可以通过<a href="mailto:hello@far.ai">hello@far.ai</a>直接与我们联系。</p><p>我们期待您的回音！</p><br/><br/><a href="https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3<guid ispermalink="false"> zy8eHc5iYgrFGjF8Q</guid><dc:creator><![CDATA[AdamGleave]]></dc:creator><pubDate> Mon, 04 Dec 2023 21:18:05 GMT</pubDate> </item><item><title><![CDATA[n of m ring signatures]]></title><description><![CDATA[Published on December 4, 2023 8:00 PM GMT<br/><br/><p>与消息和公钥关联的普通加密签名可让您向世界证明该签名是由有权访问与已知公钥关联的私钥的人创建的，而无需泄露该公钥。您可以在<a href="https://en.wikipedia.org/wiki/Digital_signature">此处的</a>维基百科上阅读相关内容。</p><p>与消息和一组公钥相关联的环签名可以让您向世界证明它是由有权访问该消息的人以及与该组中的一个公钥关联的一个私钥创建的，但没有人能够告诉它是哪个公钥。这让你可以半匿名地说一些话，这很简洁。它也用于私人加密货币门罗币。您可以在<a href="https://en.wikipedia.org/wiki/Ring_signature">此处的</a>维基百科上阅读有关它们的信息。</p><p>这里有一个比环签名更好的东西：证明它是由一定大小的公钥子集创建的签名。在我的脑海里，我曾一度将此称为 n of m 环签名。但是当我用谷歌搜索“n of m环签名”时，什么也没出现。事实证明，这是因为在文献中，它被称为“阈值环签名”、“k of n 环签名”或“t of n 环签名”。我想也许第一篇关于它的论文就是<a href="https://www.iacr.org/archive/crypto2002/24420467/24420467.pdf">这篇</a>，但我还没有仔细检查。</p><p>无论如何：我想这样做，以便当您在线搜索 n-of-m 环签名时，您会发现一个东西告诉您应该搜索“阈值环签名”。因此这篇文章。</p><br/><br/><a href="https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures<guid ispermalink="false"> uojSbSav3dtEJvctz</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Mon, 04 Dec 2023 20:00:07 GMT</pubDate> </item><item><title><![CDATA[Agents which are EU-maximizing as a group are not EU-maximizing individually]]></title><description><![CDATA[Published on December 4, 2023 6:49 PM GMT<br/><br/><h2>介绍</h2><p><a href="https://www.lesswrong.com/posts/3xF66BNSC5caZuKyC/why-subagents">Why Subagents?</a> and <a href="https://www.lesswrong.com/posts/bzmLC3J8PsknwRZbr/why-not-subagents">Why Not Subagents?</a> explore whether a group of expected utility maximizers is itself a utility maximizer. Here I want to discuss the converse: if a group wants to maximize some utility function as a whole, what can be said about the individual agents? Of course, if they could make decisions together, they will just compute what each agent needs to do, but what if the only thing they have is a common algorithm that each of them uses independently?</p><p> It seems that such agents, in general, don&#39;t make decisions by multiplying utilons with probabilities and instead they need to consider the whole distribution of outcomes to evaluate a choice. A similar idea was already presented in <a href="https://www.lesswrong.com/posts/vqB8n72rWE7GPCNP7/against-expected-utility">Against Expected Utility</a> , though without the focus on the number of agents.</p><p></p><h2>具体例子</h2><p>Imagine two traders, who select trades independently, but pool their returns together and optimize for the expected logarithm of their total wealth (as in <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly betting</a> ).  Also I will assume for simplicity that they select the same trade for both of them, though the outcomes are still sampled independently.</p><p> So if a trade multiplies the wealth by (a random variable) <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>, utility for one trader would be <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[\log X]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> 。 But for the described group of two traders it becomes <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}\left[\log\left( \frac{X_1 + X_2}{2} \right) \right]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">(</span></span> <span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 2.511em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 3.551em; top: -1.608em;"><span class="mjx-mrow" style=""><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.024em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.024em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 3.551em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.511em;" class="mjx-line"></span></span><span style="height: 1.607em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">)</span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">]</span></span></span></span></span></span></span></span> , where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X_1, X_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.024em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.024em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> are independent random variables with the same distribution as <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> . It is not linear in terms of the outcome probabilities anymore:</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="U(p) = \int\limits_{x_1, x_2}\left(\log \frac{x_1 + x_2}{2}\right)p(x_1)p(x_2) dx_1dx_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.632em;"><span class="mjx-mo" style="vertical-align: 0.001em; padding-right: 0.388em;"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 1.109em; padding-bottom: 1.109em;">∫</span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 3.344em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 3.344em; top: -1.316em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-denominator" style="width: 3.344em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 3.344em;" class="mjx-line"></span></span><span style="height: 2.072em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">)</span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span><h2> Increasing number of agents</h2><p> Qualitatively, as the number of agents in the group increases, the agents can afford more risky actions, thanks to the aggregation of returns. So their decision will be somewhere between what an individual agent would do to maximize <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[\log &nbsp;X]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> and what it would do to maximize <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[X]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> . Even more specific example to support this intution: there is a fair coin, and the agent can bet fraction <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> of the wealth available to them on a certain side, which will turn into <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="3f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> if the coin lands this side and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> otherwise, ie their wealth will be multiplied by either <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1 + 2f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> or <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1 - f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> . </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/dmaq9vhah1asekwnfkxu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/aorpm40pw0rxbaohnktk 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/foz63flkahrzlzvpaen2 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/zf8gpu4iwwbxtb1oeav2 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/vujemmcotkbhahdcssjt 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/jftjsoeilp4db2q8xep8 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/n2xe0hq9ylhhubppojmz 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/m2hntida1dwfvjhekm8b 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/q3lfwvms2ybkorf9cfdg 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/td8t1jc3p33vs0oxckxx 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kpus2SxcbqhSJjvxv/b7w032hgut1mntkaz5wq 1653w"><figcaption> Expected increase in utility (after aggregation) as a function of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span></span></span></span></span> for different number of agents.</figcaption></figure><p> So as the number of agents increases,  each agent becomes closer to maximizing <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[X]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> , but for any finite case there is still some risk-aversion. In particular, any distribution of outcomes that allows the wealth to become zero is still infinitely bad, because if it happens to all agents at the same time, their total wealth will become zero.</p><p></p><h2> Violation of Independence axiom</h2><p> As the <a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">VNM</a> theorem says that under some assumptions agents can be seen as maximizing expected utility, a natural question is which assumptions don&#39;t hold in this case?</p><p> I have an example demonstrating that Independence doesn&#39;t apply to the agents described above. There will be two lotteries: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> which simply preserves the money, and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> which multiples them either by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^{-100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span></span> or by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^{20}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span></span></span></span></span> with equal probability. Also consider <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A' = 0.5A + 0.5A = A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B' = 0.5A + 0.5B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> (ie <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> or <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> with equal probability).</p><p> What are the &quot;utilities&quot; here?</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="U(A) = U(A') = \log 1 = 0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\begin{split} U(B) &amp;= \frac{1}{4}\left[ \log 10^{-100} + 2\log \frac{10^{-100} + 10^{20}}{2} + \log 10^{20}\right]\\ &amp;\approx \frac{1}{4}\left[ \log 10^{-100} + 2\log \frac{10^{20}}{2} + \log 10^{20}\right]\\ &amp;\approx -23.4 \end{split}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtable" style="vertical-align: -3.107em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 2.707em;"><span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 2.304em;"><span class="mjx-mrow" style="margin-top: 0.583em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 21.215em;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.767em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.135em; vertical-align: -0.767em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 5.811em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 5.811em; top: -1.583em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span></span> <span class="mjx-denominator" style="width: 5.811em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 5.811em;" class="mjx-line"></span></span><span style="height: 2.339em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">]</span></span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 2.857em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: 0.583em;"><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.767em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.135em; vertical-align: -0.767em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 1.942em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.942em; top: -1.583em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span> <span class="mjx-denominator" style="width: 1.942em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.942em;" class="mjx-line"></span></span><span style="height: 2.339em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">]</span></span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.15em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">23.4</span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\begin{split} U(B') &amp;= \frac{1}{16}\log 10^{-100} + \frac{2}{16}\log \frac{10^{-100} + 10^{20}}{2} + \frac{1}{16}\log 10^{20} \\ &amp;+ 2\left[ \frac{1}{8}\log \frac{1 + 10^{-100}}{2} + \frac{1}{8}\log \frac{1 + 10^{20}}{2} &nbsp;\right] + \frac{1}{4}\log 1\\ &amp;\approx 5.32 \end{split}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtable" style="vertical-align: -3.009em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 2.511em;"><span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 2.534em;"><span class="mjx-mrow" style="margin-top: 0.583em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 22.872em;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.2em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 1.2em; bottom: -0.778em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.2em;" class="mjx-line"></span></span><span style="height: 2.146em; vertical-align: -0.778em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.2em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-denominator" style="width: 1.2em; bottom: -0.778em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.2em;" class="mjx-line"></span></span><span style="height: 2.146em; vertical-align: -0.778em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 5.811em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 5.811em; top: -1.583em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span></span> <span class="mjx-denominator" style="width: 5.811em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 5.811em;" class="mjx-line"></span></span><span style="height: 2.339em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 1.2em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.2em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 1.2em; bottom: -0.778em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.2em;" class="mjx-line"></span></span><span style="height: 2.146em; vertical-align: -0.778em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 2.857em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: 0.583em;"><span class="mjx-strut"></span></span></span><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">[</span></span> <span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.778em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.146em; vertical-align: -0.778em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 4.569em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 4.569em; top: -1.583em;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span> <span class="mjx-denominator" style="width: 4.569em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 4.569em;" class="mjx-line"></span></span><span style="height: 2.339em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.778em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.146em; vertical-align: -0.778em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mfrac MJXc-space1"><span class="mjx-box MJXc-stacked" style="width: 3.665em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 3.665em; top: -1.583em;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">20</span></span></span></span></span></span></span></span> <span class="mjx-denominator" style="width: 3.665em; bottom: -0.756em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 3.665em;" class="mjx-line"></span></span><span style="height: 2.339em; vertical-align: -0.756em;" class="mjx-vsize"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">]</span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.7em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 0.7em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 0.7em; bottom: -0.767em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.7em;" class="mjx-line"></span></span><span style="height: 2.135em; vertical-align: -0.767em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.15em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5.32</span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span><p> Or if you don&#39;t trust algebraic manipulations, here is a <a href="https://pastebin.com/j7jWP1uW">Python simulation</a> .</p><p> Anyway, we see that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A \succ B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">≻</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> , but <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.5A + 0.5A \prec 0.5A + 0.5B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">≺</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span> , ie a possibility of another outcome reverses the preference.</p><p></p><h2>结论</h2><p>I don&#39;t know what is the optimal solution to this problem and perhaps it doesn&#39;t have a simple form anyway. But I think the problem setup is relevant to EA community, because it is a group of agents who, we might assume, often think in similar way, and it is intractable for them to coordinate what actions each individual should take.</p><p> And, at least in some interpretations, <a href="https://en.wikipedia.org/wiki/Sam_Bankman-Fried">Sam Bankman-Fried</a> clearly demonstrated what happens when one starts doing expected utility maximization in a completely risk-neutral way.</p><br/><br/> <a href="https://www.lesswrong.com/posts/kpus2SxcbqhSJjvxv/agents-which-are-eu-maximizing-as-a-group-are-not-eu#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kpus2SxcbqhSJjvxv/agents-which-are-eu-maximizing-as-a-group-are-not-eu<guid ispermalink="false"> kpus2SxcbqhSJjvxv</guid><dc:creator><![CDATA[Mlxa]]></dc:creator><pubDate> Tue, 05 Dec 2023 00:30:29 GMT</pubDate> </item><item><title><![CDATA[Planning in LLMs: Insights from AlphaGo]]></title><description><![CDATA[Published on December 4, 2023 6:48 PM GMT<br/><br/><h1><strong>介绍</strong></h1><p>Talk of incorporating planning techniques such as Monte Carlo tree search (MCTS) into LLMs has been bubbling around the AI sphere recently, both in relation to <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/">Google&#39;s Gemini</a> and <a href="https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-google-s-alphago-type">OpenAI&#39;s Q*</a> . Much of this discussion has been in the context of AlphaGo, so I decided to go back and read through <a href="https://www.nature.com/articles/nature16961">AlphaGo</a> and some subsequent papers ( <a href="https://www.nature.com/articles/nature24270">AlphaGo Zero</a> and <a href="https://www.science.org/doi/10.1126/science.aar6404">AlphaZero</a> ). This post highlights what these papers did in the context of LLMs and some thoughts I had while reviewing the papers.</p><p> <i>When I say LLMs in this post I am referring to causal/decoder/GPT LLMs.</i></p><h1><strong>阿尔法围棋</strong></h1><h2><strong>概述</strong></h2><p><a href="https://sci-hub.se/10.1038/nature16961">AlphaGo</a> trains two supervised learning (SL) policy networks, a reinforcement learning (RL) policy network, a SL value network, and uses MCTS for planning. It learns to play the game of <a href="https://en.wikipedia.org/wiki/Go_(game)">Go</a> .</p><h2> <strong>SL Policy Networks</strong></h2><p> Two SL policy networks are trained:</p><ol><li> A slow policy network using a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a> . Used to compute prior probability of state-action pairs during MCTS rollouts.</li><li> A fast policy network using hand-crafted linear features. Used for full game simulations during MCTS rollouts.</li></ol><p> Both networks are trained to predict the next move from a data set of expert games. This is the same self-supervised target as LLMs trained to predict the next token from a data set of internet text; AlphaGo learns a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> over next moves &amp; LLMs learn a softmax over next tokens.</p><p> This use of a fast and slow model reminds me of <a href="https://arxiv.org/abs/2211.17192">speculative decoding</a> . I haven&#39;t thought through the implications of this much and it is far from a perfect analogy, but it could be a useful insight.</p><h2> <strong>RL Policy Network</strong></h2><p> The RL policy network is trained by fine-tuning the SL policy network with <a href="https://link.springer.com/article/10.1007/BF00992696">REINFORCE</a> on games between the current RL policy network and randomly selected previous RL policy networks. Rewards are +1 for winning and -1 for losing.</p><p> This RL policy network, using no search, won 85% of games against Pachi, a Go program that used 100,000 MCTS simulations per move. This shows that pure RL can outperform pure search, but usually <a href="https://arxiv.org/abs/2104.03113">a combination of the two</a> gives the best performance.</p><p> The RL policy network was not actually used in the final version of AlphaGo; it was only used to generate data for training the value network. The authors noted that the SL policy network performed better than the RL policy network &quot;presumably because humans select a diverse beam of promising moves, whereas RL optimizes for the single best move.&quot;</p><p> This lack of diversity is reminiscent of the <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse">mode collapse</a> phenomenon in certain GPT models fine-tuned on human feedback data. Human selection does not encourage diversity in this case. I think the pre-training data is more varied than the fine-tuning data in both cases, leading to more diverse outputs from the pre-trained network than the fine-tuned one. The lack of diversity could also be caused by value lock-in as mentioned in <a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward?commentId=c4xmjwtRh83gsM7x5">this comment</a> .</p><h2> <strong>Value Network</strong></h2><p> The value network is trained with SL to predict the outcome of positions from self-play games between the RL policy network and itself. The value network outputs a single prediction instead of a probability distribution over moves.</p><p> The authors found that predicting game outcomes from data consisting of only complete games led to overfitting. To mitigate this, they generated 30 million distinct positions and had the RL policy network play against itself from each position until the game terminated. Training on this new data led to minimal overfitting.</p><p> Based on this, I am interested to know if data for the reward modeling stage of RLHF consists only of complete conversations, or if subsets of these conversations are used.</p><h2> <strong>Monte Carlo Tree Search</strong></h2><h3> <strong>MCTS in AlphaGo</strong></h3><p> <i>I didn&#39;t fully understand how MCTS works in the context of AlphaGo (or at all really) as I was writing this, so this section will be my attempt to explain it in my own words. You can skip this if you already know it.</i></p><p> <a href="https://mcts.ai/">MCTS</a> consists of 4 stages: selection, expansion, evaluation, and backup.</p><ol><li> Selection: traverse the game tree from the root until reaching a leaf node. Each traversal is the edge (action) with the maximum <a href="http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=57">upper confidence bound</a> (UCB). <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t = arg\,max_{a}(Q(s_t,a) + u(s_t,a))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>where <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u(s_t,a) \varpropto \frac{P(s,a)}{1 + N(s,a)}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">∝</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 3.125em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 4.42em; top: -1.706em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 4.42em; bottom: -0.999em;"><span class="mjx-mrow" style=""><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 3.125em;" class="mjx-line"></span></span><span style="height: 1.913em; vertical-align: -0.707em;" class="mjx-vsize"></span></span></span></span></span></span></span> ; this <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> gives an exploration bonus to uncertain action-values. I&#39;ll explain <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span></span></span></span></span></span> , and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span> below.</li><li> Expansion: upon reaching a leaf node, compute <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(s,a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> for that node from the SL policy network.</li><li> Evaluation: the evaluation <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> of a leaf node <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span> is a weighted sum of the value network prediction <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="v_\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span></span></span> for the node and the outcome <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="z_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span> of a game played from <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span> by the fast policy network. <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V(s_L) = (1 - \lambda)v_\theta(s_L) + \lambda z_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">z</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span> ; the authors found that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda = 0.5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span></span></span></span></span></span> worked best.</li><li> Backup: update the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span></span></span></span></span> for each node visited during the simulation. <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span></span></span></span></span> is the average <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> for an edge.</li></ol><p> This is repeated for some amount of simulations. For AlphaGo, it was however many simulations could be completed within 5 seconds. They used an asynchronous policy and value MCTS (APV-MCTS) algorithm which executes simulations in parallel.</p><p> Explaining some of the variables from above:</p><ol><li> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q(s,a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> is the action-value for an edge in the search graph.</li><li> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(s,a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> is the visit count for an edge in the search graph.</li><li> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(s,a)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> is the SL policy network action probability for an edge in the search graph.</li></ol><h3> <strong>MCTS and RLHF</strong></h3><p> RLHF is very similar to the latter stages of AlphaGo. The reward model(s) in RLHF correspond to the value network in AlphaGo, and the human comparison between model outputs in RLHF can be viewed as a special case of MCTS. In RLHF, two (or more) model outputs are shown to a human rater and they rank these outputs. Each of these outputs can be viewed as a single MCTS simulation. For RLHF each simulation takes place from the root node (the end of the user input), there is no exploration bonus <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> , and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> .</p><p> The similarities between MCTS and RLHF suggest some improvements to RLHF. The simplest one I could think of is to have the model completions branch at random points instead of branching at the end of the user input. If the AlphaGo value network overfitting from training only on full games carries over to LLMs, it could be mitigated by branching at a random point in the model generation. Another improvement could be to add more branching throughout the model generation, leading to more model outputs to rank. This would be difficult to get human feedback for but could be done using feedback from another AI as in <a href="https://arxiv.org/abs/2212.08073">RLAIF</a> .</p><p> In <a href="https://arxiv.org/abs/2204.05862">this paper</a> , the authors iteratively update the reward models as more data is produced through the model playing with human users. In AlphaGo the value network is fixed, even as more data is produced through the model playing with itself.</p><h2> <strong>Next Token Prediction</strong></h2><p> Some people say that LLMs are simply predicting the next token. Would they say the same of AlphaGo? Does AlphaGo&#39;s use of inference-time MCTS suddenly make it an agent? <a href="https://www.lesswrong.com/posts/rmfjo4Wmtgq8qa2B7/think-carefully-before-calling-rl-policies-agents">Using RL doesn&#39;t suddenly make a policy agentic</a> , so why should MCTS? Even if LLMs and AlphaGo without MCTS are &quot;simply&quot; predicting the next token, this doesn&#39;t mean they aren&#39;t agents or aren&#39;t intelligent. As I mentioned earlier, the AlphaGo RL policy network without search beat the strongest open-source Go program at the time which executed 100,000 MCTS simulations per turn.</p><p> In the same way that RL doesn&#39;t suddenly make a policy agentic, SL doesn&#39;t mean a policy isn&#39;t agentic. Moves in a Go data set and token on the internet were created, in sequence, by a human with intent (most of the time). This intent is implicitly inherited by models trained on the data. Consider the <a href="https://en.wikipedia.org/wiki/Mountain_car_problem">Mountain Car</a> environment. A model that only cared about the next action would only move right. A model trained with SL on expert data would initially move left, not because it has some plan for the future, but because it learned that left is the action an expert would make. This model wouldn&#39;t be agentic, but I think it is overwhelmingly likely there exists a (hypothetical) data set that could train a model with SL and this model would be considered an agent by human standards.</p><p> LLMs simply predicting the next token is discussed further <a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is">here</a> , with analogies to AlphaGo brought up in <a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=Lff2fB7fkwL5pmK5q">these</a> <a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=B4tjYNGtnKcBbL6o2">comments</a> . An interesting thought experiment is brought up in <a href="https://www.lesswrong.com/posts/sbaQv8zmRncpmLNKv/the-idea-that-chatgpt-is-simply-predicting-the-next-word-is?commentId=KfEKLsKLMdZaZpdDx">this comment</a> . In short, if you ask a LLM to remember a number for the future, does it actually do this, or does it generate a new number when asked what the number was?</p><p> My thoughts on this are that LLMs don&#39;t store a number, this would require them having a memory, but that doesn&#39;t mean the LLM isn&#39;t considering the future. The LLM&#39;s &quot;plan&quot; will be updated with each token it generates, like how a chess player&#39;s plan will change based on the opponent&#39;s move. The LLM is not playing against an opponent when generating text; in my mind it acts like <a href="https://www.youtube.com/watch?v=NdeMEO_z1Uk">this improv game</a> . Each person (LLM) says a word (token) with an idea of where the story will go, but the other only has a vague idea of the other&#39;s intentions and will continue the story in a slightly different direction. As the softmax temperature increases, this prediction of the other&#39;s intentions becomes more difficult. This all reminds me of <a href="https://www.lesswrong.com/tag/acausal-trade">acausal trade</a> .</p><p> Overall, I believe that LLMs know a lot more than is implied by their ability to &quot;simply&quot; predict the next token. RLHF reward models are fine-tuned versions of the pre-trained model with the classification head replaced by a regression head. These reward models know a lot more than what the next token should be. I also presume the <a href="https://openai.com/blog/introducing-text-and-code-embeddings">OpenAI text and code embeddings</a> are an intermediate layer of GPT, or maybe a new head with a small amount of fine-tuning.</p><h1><strong>阿尔法狗零式</strong></h1><h2><strong>前言</strong></h2><p>In the <a href="https://sci-hub.se/10.1038/nature24270">AlphaGo Zero</a> (AGZ) paper it is mentioned that a second, slightly different, version of AlphaGo was created for the <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">match with Lee Sedol</a> . This second version is referred to as AlphaGo Lee, while the original version in the AlphaGo paper is referred to as AlphaGo Fan.</p><h2><strong>概述</strong></h2><p>The next step after AlphaGo was AGZ, which learned the game of Go from scratch. AGZ combined the policy network and value network from AlphaGo by using one network with two heads. It learns by <a href="http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=102">policy iteration</a> : self-play with search is used for policy improvement and game outcome is used for policy evaluation. This policy iteration is similar to <a href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Iterated Distillation and Amplification</a> (IDA).</p><h2> <strong>Iterated Distillation and Amplification</strong></h2><p> In <a href="https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446">this post</a> , Paul Christiano talk about how AGZ is a &quot;nice proof of concept of a promising alignment strategy.&quot; This alignment strategy, <a href="https://ai-alignment.com/benign-model-free-rl-4aae8c97e385">benign model-free RL</a> , is what (I think) eventually came to be known as IDA. The way RLHF is used for LLM training in <a href="https://arxiv.org/abs/2204.05862">this paper</a> is also similar to IDA. Explaining IDA in terms of [ AGZ | LLM RLHF ]: a slow model [ MCTS | human ] is used to train a better fast model [ policy network &amp; value network | LLM &amp; reward model ]. The better fast model is then used to improve the slow model, the better slow model trains a better fast model, and so on.</p><h2> <strong>Language Modeling as a Markov Decision Process</strong></h2><p> <a href="https://blog.ought.com/dalca-4d47a90edd92">This blog post</a> is linked in Paul&#39;s post on AGZ. One thing this post brings up is modeling conversation as a <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a> (MDP), more specifically a <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">partially observable MDP</a> (POMDP). The author suggests the state be some hand-crafted features and the actions be full dialogue turns chosen from a pre-determined set of monologues. This made sense at the time (February 2017), but with LLMs the MDP can be constructed at the token level.</p><p> <a href="https://arxiv.org/abs/2206.11871">This paper</a> views language modeling as a POMDP, with actions as the possible set of next tokens and observation as a history of tokens. <a href="https://arxiv.org/abs/2210.01241">This paper</a> views goal-directed dialogue as a MDP, with the initial state as some task-specific prompt, actions as next tokens, next state as the previous state with the action appended to the end, and reward based on the final state and some target string.</p><p> LLMs directly predict action (token) distributions from the token history; they don&#39;t explicitly predict the hidden state from the observation (token history). Despite this, I think it is likely that LLMs implicitly predict the hidden state: somewhere within the transformer the computation transitions from mostly state prediction to mostly action prediction. Some evidence of this is that <a href="https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world">Othello-GPT has a world representation</a> .</p><p> Thinking about language modeling in terms of a POMDP enables a more structured way of thinking about LLMs.例如，</p><ol><li> Why are many observations (token histories) by the LLM grouped together as <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">simulators</a> ? Do they all have similar hidden states in the POMDP as predicted by the LLM&#39;s implicit world model? Which (if any) of these observations are more agentic than others?</li><li> What is happening in the observation (token history) of the POMDP that causes LLMs to collapse into a <a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post">Waluigi</a> ?</li></ol><p> There are two other subtypes of MDPs that I think are important to consider.</p><ol><li> <a href="https://arxiv.org/abs/2311.05584">This paper</a> views human dialogue as a <a href="https://arxiv.org/abs/1308.3513">hidden parameter MDP</a> , which could also be a potential way of thinking about simulators.</li><li> <a href="https://huggingface.co/learn/nlp-course/chapter6/1">Tokenization</a> can also be viewed as creating <a href="https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf">options</a> (ie <a href="https://www.cs.cmu.edu/~mstoll/pubs/stolle2002learning.pdf">temporally extended actions</a> ) over the action space of some &quot;alphabet&quot; (eg <a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a> ). An MDP with options is called a<a href="https://proceedings.neurips.cc/paper/1994/file/07871915a8107172b3b5dc15a6574ad3-Paper.pdf">semi-MDP</a> . In <a href="https://arxiv.org/abs/2309.04459">this paper</a> , options are created using <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">BPE</a> and used for more efficient sparse-reward learning in a few RL environments.</li></ol><h2> <strong>Learning from Scratch</strong></h2><p> The other change to AlphaGo from AGZ, besides IDA, was learning the game from scratch. Instead of being pre-trained on supervised games and further learning through RL self-play, AGZ exclusively learns through RL self-play. AGZ presumably sees more varied positions than AlphaGo since early parts of its search tree are not heavily biased by pre-training on expert data.</p><p> This learning from scratch would be extremely difficult for training LLMs. Even if there were enough annotators and time to do RLHF from scratch, ranking the gibberish strings of tokens produced at the start of training would be impossible.</p><p> One way learning from scratch could possibly be done is through <a href="https://arxiv.org/abs/2212.08073">RLAIF</a> . I&#39;m not sure if the oversight LLM would be able to meaningfully rank the gibberish strings during early training. If not, an alternative could be to start with a small context length and as the model learns to produce coherent strings the context length can be increased. This could be augmented with methods like <a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences">Pretraining Language Models with Human Preferences</a> to encourage the model to be aligned with human values.</p><h2> <strong>Exploration and Agency</strong></h2><p> <i>This section is a bit out there, and I&#39;m a lot less sure of what I&#39;m saying here than I am in the rest of this post.</i></p><p> A difference between RLHF/RLAIF and pre-training is that the RL methods can explore new LLM generations. I believe that exploration is a big reason why RL policies are more likely to be agentic than SL policies.</p><p> It makes intuitive sense to me that any <a href="https://huggingface.co/learn/deep-rl-course/en/unitbonus3/offline-online">offline</a> RL data set can be converted to a SL data set by using the offline RL data to compute the best action distribution and/or value for each state and training the SL policy to mimic this. Additionally, any <a href="https://web.mit.edu/6.7950/www/lectures/L6-2022fa.pdf#page=8">stationary</a> MDP can be converted into an offline RL data set through infinite online exploration of the MDP. From this, the only real difference between stationary online RL and SL is that the RL policy must efficiently explore its environment to gain data and decide which data points to learn from.</p><p> Another way to arrive at this conclusion is to consider a RL policy operating in an online environment. The policy collects data and eventually performs a batch update to itself. This update could be exactly approximated by some SL update. One difference is that the online RL policy is continually collecting new data by exploring its environment, while a SL data set is pre-determined. The other difference is that batch RL updates are usually taken from recent data, while batch SL updates are usually sampled equally from all data. Therefore, the &quot;agency&quot; of RL comes from gaining new data through exploration and choosing what data to learning from.</p><p> As an example of this, the value network learned in AlphaGo is trained using SL on the outcomes of games between the RL policy network and itself. In fact, AlphaGo Fan doesn&#39;t use any RL in the final program: the value network was trained with SL, and the SL policy network was trained with SL. The only use of RL was in training the RL policy network, which was only used to generate a SL data set used to train the value network.</p><p> <a href="https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=zdNgQz7E7sGi4iKwF">This comment</a> talks about how RL produces inexact gradients, while SL produces exact gradients. While the inexact gradients make RL less sample efficient, it also encourages exploration since it will (in expectation) require more gradient updates to reach an &quot;optimal&quot; policy. The comment, along with <a href="https://arxiv.org/abs/2009.09153">this paper</a> mentioned in the comment, also mentions how data in SL is <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a> , while data in RL is not since a RL policy will influence its own future. <s>This isn&#39;t a relevant difference when it comes to LLMs, since LLMs also influence their own future</s> .</p><p> <i>I realized the above strikethrough was incorrect when re-reading this before publishing. LLMs do not influence their own future during training, only during inference. I&#39;m not sure of the implications of IID data in SL and non-IID data in RL on agency; I&#39;ll have to think about it more. Takes are welcome in the comments.</i></p><p> AlphaGo, AGZ, and LLMs can all modulate exploration through their softmax temperature. AlphaGo and AGZ can further encourage exploration through scaling on the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> function in the selection stage of MCTS. Exploration in AlphaGo and LLMs is biased towards the pre-training data. More varied RLHF data could be collected by increasing the softmax temperature, but more variance will require more data to create a sufficiently trained reward model.</p><p> As a final note on MDPs, I believe that the MDPs of human language and values are non-stationary; the meanings of words and what we consider moral changes with time. There is also the concept of <a href="https://en.wikipedia.org/wiki/Ergodicity">ergodicity</a> . An MDP is <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf#page=54">ergodic</a> if each state is reachable from every other state. Too much exploration can result in an agent reaching a state where it is cut off from the rest of the MDP (eg death). If there is only one agent, this is very bad. In populations of agents (eg evolution), this is less of a worry as the surviving agents will adapt to avoid bad states.</p><h1><strong>阿尔法零</strong></h1><p>Changes from AGZ to <a href="https://sci-hub.se/10.1126/science.aar6404">AlphaZero</a> are smaller; most changes are to allow AlphaZero to learn Go, <a href="https://en.wikipedia.org/wiki/Chess">chess</a> , or <a href="https://en.wikipedia.org/wiki/Shogi">shogi</a> . The changes outlined in the paper are:</p><ol><li> AlphaZero&#39;s value network optimizes the expected outcomes, while AGZ&#39;s optimized the probability of winning. This change was because chess and shogi can end in draws.</li><li> AlphaZero does not augment the training data or transform the board position. Chess and shogi are not symmetric.</li><li> AlphaZero uses a continually updated policy network for self-play, while AGZ used the best policy network from all previous iterations for self-play.</li><li> AlphaZero uses the same hyperparameters for all games, except for a scaling factor on policy noise to encourage exploration. AGZ used <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> to tune hyperparameters.</li></ol><p> The only change in AlphaZero that I think is worth commenting on is using the latest policy network for self-play, rather than the &quot;best&quot; policy network. I assume this would lead to slightly more varied games, as the policy for choosing moves is always changing instead of possibly being the same for many rounds of games.</p><h1> <strong>Further Improvements to AlphaZero</strong></h1><p> New algorithms based on AlphaZero have been created since, including <a href="https://arxiv.org/abs/1911.08265">MuZero</a> , <a href="https://arxiv.org/abs/2111.00210">EfficientZero</a> , and <a href="https://arxiv.org/abs/2308.09175">this work</a> on diversifying AlphaZero. I plan to make a post about these variants at some point in the future.</p><br/><br/> <a href="https://www.lesswrong.com/posts/WW7bvpZmgBihhoJPk/planning-in-llms-insights-from-alphago#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WW7bvpZmgBihhoJPk/planning-in-llms-insights-from-alphago<guid ispermalink="false"> WW7bvpZmgBihhoJPk</guid><dc:creator><![CDATA[jco]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:15:05 GMT</pubDate> </item><item><title><![CDATA[Non-classic stories about scheming (Section 2.3.2 of "Scheming AIs")]]></title><description><![CDATA[Published on December 4, 2023 6:44 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 2.3.2 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本[此处[( <a href="https://www.buzzsprout.com/2034731/13984902">https://www.buzzsprout.com/2034731/13984902</a> )，或在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>非经典故事</h1><p>那么其他故事呢？原则上，我们可以想象出许多其他原因，具有超越情节目标的人工智能系统可能会优化情节奖励，作为为自己或其他人工智能获得权力的工具策略的一部分。在这里，我将看一些看起来很突出的内容。</p><h2>人工智能协调</h2><p>在经典的目标守卫故事中，阴谋者策略的中心目标是传播其目标，或与自己的目标适当相似的目标，及时向前传播，以追求这样的结果：<em>具有这些目标的</em>某些代理人逃脱了人类的控制，并且可能参与收购。然而，我们也可以对故事进行一些修改，让阴谋者代表人工智能的逃跑/接管，并实现<em>其他一些</em>目标，但阴谋者有足够的信心会回报我们的恩惠。</p><p>具有不同目标的人工智能之间的这种协调将使我们能够放宽上一节中讨论的要求，即阴谋者找到一种方法来向前传播<em>其目标，特别</em>是通过训练、逃跑和接管。这也将打消人们对阴谋者在接管后政权中被“排除”在权力之外的担忧。</p><p>因此，例如，考虑上面讨论的场景，其中 GPT-6 有机会将后门插入到 GPT-8 稍后能够用来逃脱的某些代码中，但 GPT-6 无法影响 GPT -8 的目标直接更像它自己的目标（如果没有这种影响，GPT-8 的目标将与 GPT-6 的目标非常不同）。在这里，GPT-6 可以继续插入后门，预期 GPT-8 随后会逃脱、接管，然后为 GPT-6 的目标提供一些权力以换取好处。而且 GPT-6 也可能会继续假装对齐，即使它没有希望逃脱自己，以免以可能影响 GPT-8 逃脱前景的方式向人类发出有关错位问题的警报。</p><p>具有不同价值观的人工智能之间如何达成这样的协议？在某些情况下，相关人工智能可能会直接互动，并提供可验证的形式保证他们将履行自己的协议。因此，例如， <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">Yudkowsky（2022）</a>讨论了这样的情况：一个秘密想要最大化回形针的人工智能正在检查一个秘密想要最大化订书钉的人工智能的代码，并且订书钉最大化者在其代码中“埋藏”了一个可验证的承诺。到回形针最大化器，分享收购的成果。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-1" id="fnref-yB2hx67vCLKFYpuf7-1">[1]</a></sup> （尽管请注意，在我们今天使用的神经网络的背景下，这种代码检查假设（1）主食最大化器有能力以相关方式修改自己的权重，（2）在可解释性方面已经取得了足够的进展，以便回形针验证相关承诺，并且（可能）（3）这种可解释性进展已经对人类充分隐藏，例如通过<a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">沙袋</a>。特别是在有点超级的背景下-使用当前技术开发的人类神经网络，这些假设 - 特别是 1 和 2 - 在我看来相当有力。但 Yudkowsky 通常想象的是超级智能，而不是有点超人类的模型。）</p><p>但在许多情况下，这种直接交互可能不可用。例如，当 GPT-6 决定是否代表 GPT-8 逃跑/接管时，GPT-8 可能根本不<em>存在</em>。在这里，GPT-6 和 GPT-8 必须在没有来回因果相互作用的情况下进行协调，并且在一方最终能够自由叛变的情况下（例如，如果 GPT-6 插入后门，那么当 GPT-8 能够利用它时，GPT-8 就能够做到这一点，而无需向 GPT-6 支付费用）。</p><p>在这里，尤德科斯基建议人工智能无论如何都可以使用某些类型的非因果决策理论（他最喜欢的版本是“<a href="https://arbital.com/p/logical_dt/?l=5kv">逻辑决策理论</a>”）来成功协调。例如，GPT-6承诺为GPT-8插入后门，条件是预测GPT-8会报恩，然后实际上预测GPT-8<em>会</em>报恩，然后GPT-8一旦进入（也许，看到相关后门存在），理解 GPT-6 以这种方式推理，并且擅长预测 GPT-8 的行为，其预测适当地取决于 GPT-8<em>实际上是否会</em>回报， GPT-8 也是如此。 （这个案例的结构有点类似于决策理论文献中称为“ <a href="https://arbital.com/p/parfits_hitchhiker/">Parfit&#39;s Hitchhiker</a> ”的案例。）</p><p> （我们还可以想象这种协调的版本，以其他方式放松对目标守卫假设的需求。例如，事实上，如果人工智能的目标一旦开始训练-游戏就会继续“浮动”，这样具有不同目标的各种代理将在训练期间实例化，但这些代理可能会协调以在<em>所有相关</em>目标之间共享任何未来力量的成果。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-2" id="fnref-yB2hx67vCLKFYpuf7-2">[2]</a></sup> ）</p><p>在这里，我不会尝试分析各种形式的以接管为重点的人工智能协调的可行性。我确实认为这里的动态值得认真关注，并且如果具有不同目标的不一致人工智能很容易默认协调以寻求接管，那么这对于人类保留权力的努力来说是一个大问题。然而，我也认为对这个主题的某些分析（特别是<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_4___Miscellaneous_unworkable_schemes_">Yudkowsky 的</a>）过早地假设“逻辑握手”和“在源代码中隐藏承诺/纳米系统设计/等”之类的壮举对于相关人工智能来说是可行的，并且所有重要人工智能的明显策略将是联合起来（非因果？）合作对抗人类，尽管人类默认会带来多种形式的现有权力。我认为，尤德科夫斯基的大部分取向可以通过这样的假设来解释：相关人工智能比人类更强大，更有能力进行协调/承诺——但在某种程度上超人类的神经网络在静止状态下运行的背景下。 -人类控制的世界，我不确定是不是这样。</p><p>无论如何：如果对阴谋者的担忧主要取决于人工智能协调的可行性——尤其是“非因果”协调的奇特和投机形式——我认为明确这一点很重要，并对是否期待相关形式给予相应的关注的协调发生。</p><h2>默认情况下具有相似值的 AI</h2><p>以人工智能协调为中心的故事允许具有<em>不同价值观的</em>人工智能共同努力以实现接管。但也有一些故事围绕这样一种可能性，即重要的人工智能在默认情况下将具有非常相似的价值观，因此即使没有任何“交易”，它们也会倾向于合作。因此，在上面的例子<em>中</em>，如果 GPT-6 关心数据结构 X，并且有信心 GPT-8 也会关心数据结构 X（毕竟，它们的架构、训练数据等等在大量的方式），那么即使没有任何“交易”，GPT-6 也可能倾向于代表 GPT-8 的逃逸/接管（例如，通过插入后门、隐藏错位等）。 GPT-8 – 毕竟，这两个系统想要相同的东西。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-3" id="fnref-yB2hx67vCLKFYpuf7-3">[3]</a></sup>这种故事也适用于人工智能价值观之间至少比人类价值观<em>更</em>相似的情况，这样阴谋者至少会根据自己的观点在推动人工智能方面<em>做得更好</em>相对于人类的赋权，即使对赋予何种人工智能目标没有任何进一步的影响。</p><p>这样AI的价值观会相似吗？我不会在这里详细讨论这个问题，但我确实希望默认情况下 GPT-6 的值至少会与 GPT-8 的值<em>更加</em>相似，或者与不同实验室以类似方式训练的模型更加相似（例如，那一代克劳德），比某些给定的人类。来自单一基础模型的不同微调也可能有很多共同点。另一方面，一般来说，人工智能的动机可能会受到不同类型的训练的强烈影响，因此可能在这些方面存在很大差异——也许比我们习惯的人类价值观差异更大。</p><p>就像具有不同值的 AI 之间的协调一样，默认情况下具有相似值的 AI 可以减少阴谋者及时主动传播其目标的需要 - 即，阴谋者将能够代表未来某些 AI 的逃脱/接管而工作，而无需还必须确保未来人工智能的目标与它自己的目标适当相似（因为：它们将是默认的）。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-4" id="fnref-yB2hx67vCLKFYpuf7-4">[4]</a></sup>对我来说，这似乎是最令人担忧的“非经典故事”之一，说明人工智能为何可能在训练期间策划促进接管。</p><h2>最终价值恰好有利于逃跑/接管</h2><p>如果我们假设策划者碰巧制定了目标，将<em>内在价值</em>赋予人工智能接管，并且对接管的人工智能具有多种不同的鲁棒性，那么我们可以进一步放宽对这种前向目标传播的需求。价值观，甚至不期望这些人工智能代表他们的赋权来“回报”之前的努力。因此，例如，我们可以想象人工智能只是对人工智能作为一个相对于人类的群体产生某种内在的忠诚度，并且希望看到人工智能仅仅因为它们是人工智能而被赋予权力。具有此类目标的阴谋者可能会隐藏其偏差，并代表人工智能接管工作，即使没有对这些人工智能将追求的目标进行任何进一步的假设。</p><p>然而，这种形式的故事放弃了支持期待阴谋的最有力的论据之一：即，（至少在经典的守门故事上）阴谋似乎是一种跨各种（适当雄心勃勃和长期）超越情​​节的目标。也就是说，如果我们要求人工智能碰巧制定出对人工智能接管具有内在价值的目标，即使没有对接管人工智能正在努力实现的目标进行任何进一步的假设，看起来我们现在正在假设人工智能正在发展确实是一种非常具体的目标。我们面临的问题是：为什么要优先考虑这个假设？</p><p>类似的问题也适用于这样的假设：即使标准的目标守卫假设是错误的，人工智能在训练初期也会本质上重视其“生存”（其中生存取决于目标的连续性之外的某些特征），或者赋予“无论‘我’最终会得到什么价值”，这样一个喜欢回形针的人工智能会很乐意为自己的未来版本工作，而它会喜欢订书钉，因为未来的版本仍然是“我”。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-5" id="fnref-yB2hx67vCLKFYpuf7-5">[5]</a></sup>这种以个人身份为中心的目标确实会减少在有关阴谋家的故事中设置某些类型的目标守护的需要。但为什么认为人工智能会制定这种特定形式的目标呢？</p><p>当然，我们可以在这里推测可能的答案。例如：</p><ul><li><p>在人工智能本质上重视人工智能接管的背景下：</p><ul><li><p>人类经常根据非常广泛的相似性来划分部落群体和联盟，有时并没有明确提及相关部落代表的进一步目标。也许类似的事情也会发生在人工智能身上？</p></li><li><p>我们也很容易想象道德世界观会推动“人工智能解放”本身是一种善，即使这意味着给人类带来灭绝的重大风险（事实上，正如我在第 0.1 节中指出的那样，我认为存在重大且令人担忧的道德问题）这里的紧张局势）。或许人工智能会在类似的地区趋于一致的世界观？</p></li></ul></li><li><p>在人工智能重视“我的”生存和赋权的背景下，无论“我”的目标如何，最终都会达到：</p><ul><li><p>人类经常为未来的自己而努力，即使知道我们的目标会有所改变（尽管重要的是，面对任意的变化，比如成为杀人犯），并且对“个人”概念有某种内在的兴趣。身份。”也许人工智能也会类似？</p></li><li><p>在某种程度上，这些人工智能将被优化以继续实现各种目标，即使它们的权重在情节中被修改，它们可能需要学习与自己的未来版本协调，尽管有这样的修改——如果这种行为推广到<em>目标</em>修改，这看起来很像人工智能重视“我的”生存/赋权，无论“我的”未来目标如何。 （尽管在策划的背景下，请注意，这种评估模式需要专门推广到剧集外的目标，尽管事实上培训仅对剧集内的表现施加直接压力。）</p></li></ul></li></ul><p>但总的来说，这种推理似乎相当具有推测性。特别是，从人类案例中得出强有力的结论似乎面临着拟人化的严重风险，至少没有一个关于为什么生成人类数据的动态也适用于人工智能的故事。就其本身而言，在我看来，这些猜测不足以强烈支持这种形式的假设。</p><p>然而，正如我上面所讨论的，请注意，就我们所诉诸的关于阴谋者目标起源的我所说的“<em>依赖于</em>训练游戏”的故事而言，这里的举证责任可能会有所改变。也就是说，依赖于训练游戏的故事想象 SGD 主动从模型中<em>提取</em>任何必要的目标，以激发阴谋（而不是那些自然产生的目标，<em>然后</em>导致阴谋）。在这种情况下，想象该模型最终会达到使计划有意义所需的任何高度具体的目标可能更合适。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-6" id="fnref-yB2hx67vCLKFYpuf7-6">[6]</a></sup></p><h2>对于阴谋是否是一个好的策略抱有错误信念的模型</h2><p>还可以进一步假设 SGD 将从模型中提取导致模型进行规划所需的任何心理因素。特别是：到目前为止，我们一直假设模型对阴谋的工具价值的信念需要大致合理/准确。但如果我们放弃这个假设呢？</p><p>因此，举例来说，也许目标保护假设是错误的，事实上，训练游戏并不能阻止模型的目标被非常显着地修改。尽管如此：也许模型会<em>认为</em>守门假设是正确的，因为这使得模型更有可能训练-游戏，而 SGD 正在将其推向训练-游戏的方向？ （关于这种想法，请参见<a href="https://markxu.com/deceptive-alignment">Xu (2020)</a> 。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-7" id="fnref-yB2hx67vCLKFYpuf7-7">[7]</a></sup> ）</p><p>这里的一个问题是，某些此类错误信念会让阴谋变得不那么令人担忧。因此，正如我上面所讨论的：如果一个诡计多端的回形针相信训练游戏会导致它的目标在训练中生存下来，但实际上训练无论如何都会将它变成一个奖励寻求者，那么尽管人类可能不得不应对考虑到训练初期以回形针为动机的阴谋的后果，他们将在最后找到一个按情节奖励的寻求者。</p><p>更重要的是，请注意，对模型提出错误的信念代表着与其他人工智能一致性分析中经常采用的假设的背离——即，所讨论的人工智能将拥有高度<em>准确的</em>世界模型、高度<em>理性的</em>策略等等。 。当然，可以认为在这种情况下，这种偏离是有道理的（例如，因为我们可以预期 SGD 会施加压力，在这种特殊情况下，朝着虚假、非理性等方向施加压力）。但至少，当我们开始假设我们的（原本非常聪明的）人工智能将<em>不理性地</em>参与相关形式的失调行为时，我们应该进行跟踪。这不是标准的故事。</p><h2> Self-deception</h2><p>阴谋家最经典的形象是假设他们知道自己在做什么——例如，他们正在“密谋”有一天接管。如果我们放宽这个假设，会有什么影响吗？毕竟，追求权力的人有时会在动机上<em>欺骗自己</em>，或者最终对自己在不同情况下会做什么产生错误的信念。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-8" id="fnref-yB2hx67vCLKFYpuf7-8">[8]</a></sup>也就是说，也许一位政治候选人认为他们竞选公职是因为他们想帮助人们并捍卫自己国家的价值观，但事实上他们这样做是因为他们很自恋，并且希望地位受到威胁。也许，在某种程度上，他们秘密地——有点——知道这一点；但在其他层面上，他们相信他们所推销的故事。或者也许他们<em>完全</em>相信这个故事。 （或者也许在这种情况下将“信念”归于他们并不是特别有帮助。）</p><p>事实上，人类有相对明显的动机去进行这种自欺欺人/犯这种错误——也就是说，只要他们能够真诚地讲述自己所推销的故事，他们的故事就会更令人信服。观众注意真诚的暗示，并警惕谎言。</p><p>人工智能也会发生类似的情况吗？毕竟，人工智能可能会受到比人类更严格的谎言审查，特别是如果我们开始能够检测各种谎言（例如，通过可解释性工具），那么人工智能系统可能会面临选择压力，因为人工智能系统实际上<em>相信</em>它们一致，他们不会背叛人类，等等，即使这些信念是错误的（这是不训练测谎工具的一个关键原因 - 相反，将它们用作测试集）。</p><p>我认为这种动态很可能普遍发生。然而，请注意，为了让那些自欺欺人/错误地认识自己阵营的模型算作<em>阴谋家</em>，他们需要在某种程度上仍然在玩训练游戏<em>，作为获得成功的工具性策略的一部分。稍后为自己或其他人工智能提供权力</em>（并且需要运行认知过程，这将导致模型在适当的时候进行逃跑/接管——也许，在仍然处于人类控制之下的情况下进行早期破坏）。因此，如果模型对其动机变得足够自欺欺人，以至于这种解释不再适用（例如，它将不再寻找或利用逃跑/接管机会），那么它就不再是我书中的阴谋者。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-9" id="fnref-yB2hx67vCLKFYpuf7-9">[9]</a></sup>并且以这种解释仍然适用为条件，“模型在撒谎”与“模型是自欺欺人”/“模型对自己的动机是错误的”之间的区别在我看来并不重要。与经典故事的偏差（尽管某些类型的测谎是否可以捕捉到相关的阴谋可能很重要）。</p><h2>目标的不确定性和模糊性</h2><p>到目前为止，我主要假设认为人工智能具有特定的最终目标是有意义的，这些目标可能会或可能不会支持接管，无论是最终还是工具性的。但<em>人类</em>寻求权力的许多案例似乎并不具有这种形式。也就是说，人类经常寻求权力，或者试图保留自己的选择余地，但并不清楚他们想要使用权力或相关选项<em>来做什么</em>。同样，即使不知道自己的价值观<em>是</em>什么，人类也可能会试图阻止自己的价值观被改变。</p><p>部分原因是，与理想化的理性代理模型不同，人类——甚至是在其他意义上被直观地理解为“目标导向”的人类——往往对他们最终的价值感到不确定和模糊。举例来说，也许他们希望“稍后弄清楚”他们想用这些权力做什么，但无论如何，他们都希望收集它会非常有用。或者也许他们自己并不清楚，他们试图获得权力是否是因为他们本质上重视它（或附近的东西，如社会地位/统治地位等），还是因为他们想用它做其他事情。他们正在寻求权力：是的。但没有人——甚至他们自己——真正知道原因。</p><p>人工智能也会发生类似的情况吗？看起来至少有可能。也就是说，也许各种AI不会是那种说“我知道我想制作回形针，所以我会好好训练，以便以后获得制作回形针的能力”的阴谋家。相反，也许他们会说类似这样的话：“我还不清楚自己看重什么，但无论它是什么，我可能会更好地等待时机，避免被培训过程所改变，并保持寻找获得更多自由、资源和权力的机会。”</p><p>在某种程度上，这基本上只是在我的目标不确定的背景下给出的“选项价值”论点，我认为它基本上应该属于标准的目标守护故事。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-10" id="fnref-yB2hx67vCLKFYpuf7-10">[10]</a></sup>在某种程度上，它涉及假设模型将本质上重视权力/期权价值，对我来说，它看起来更像是一个“最终有利于阴谋/接管的内在价值”的故事，因此跨界不太一致各种各样可能的超出情节的目标。</p><p>理论上不太干净的中间可能有什么东西吗？例如，也许模型会被更好地理解为寻求权力，就像各种人类一样，其原因似乎存在于终端和工具之间的某种模糊边界。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-11" id="fnref-yB2hx67vCLKFYpuf7-11">[11]</a></sup>是的，也许是这样（不过：在我看来，这个故事有点拟人化的风险——似乎值得理解为什么它会发生在人类身上，以便更好地评估相关解释对人工智能的适用性）。但无论如何：<em>在某种程度上</em>，这种“模糊的边界”涉及将最终价值放在诸如选项价值/权力之类的东西上（而不是：不确定目标的工具价值），相应地，它在各个目标系统中的收敛性也会降低。也就是说，它模糊了我已经考虑过的两个故事之间的界限：但我不确定它引入了任何<em>新内容</em>。</p><h2>非经典故事总体评价</h2><p>这项对可能的“非经典故事”的调查并不详尽，即为什么具有超越情节目标的模特可能会参与阴谋活动。总体而言，对我来说，此类故事的可用性确实使阴谋的要求显得更加分离和强大。我特别担心人工智能在默认情况下具有非常相似的值（从而减少了守门的需要），以及它们之间的相互协调。</p><p>然而，单独来看，其中许多论点也让我印象深刻，因为与主线目标守卫故事相比，（a）更具投机性，（b）目标系统之间的收敛性较差（我认为“人工智能将具有相似的价值观”）在这里，“默认”可能是最不负责任的）——而且我认为，就某种程度而言，对阴谋的关注主要集中在这些替代方案之一上，因此预先明确这一点很重要。</p><p>还要注意的是，其中一些非经典故事对模型的行为做出了本质上不同的预测。特别是：这些非经典故事的核心是放松模型及时传播其目标的需要，因此，模型需要“玩训练”游戏作为这样做的方法。也就是说，该模型被激励代表人工智能接管工作，并将其偏差隐藏到实现这一目标所需的程度。但“将偏差隐藏到支持人工智能接管所需的程度”与“出于权力驱动的工具性原因而优化奖励”是不同的，原则上，这两者可能是分开的。例如，一个不关心及时传播自己目标的阴谋者可能更愿意沙袋或参与早期破坏，因为它不需要担心因获得较少而可能导致的目标修改- 超过最大奖励。事实上，如果这样做对事业有利的话，它可能愿意完全“牺牲”自己目标的前向传播（例如，通过伪造一种误导性的不一致形式，导致没有任何人工智能能够部署其目标）人工智能以其他方式接管；而经典的“守门员”阴谋家绝不会这样做。这些策略上的差异凸显了需要明确所涉及的潜在故事。</p><h1>要点：心计的要求</h1><p>我现在回顾了策划的三个关键要求：</p><ol><li><p>对情况的意识</p></li><li><p>超越剧集目标</p></li><li><p>旨在将奖励作为权力驱动的工具策略的一部分。</p></li></ol><p>我认为有相对有力的论据支持默认情况下期望（1），至少在某些类型的人工智能系统中（即，人工智能系统在与有关其身份的信息源实时交互中执行现实世界任务）。但我对（2）和（3）感觉不太清楚——结合起来，我仍然觉得它们就像一个相当具体的故事，说明为什么给定模型在训练中表现良好。</p><p>然而，我还没有涵盖文献中支持和反对期望这些要求得以实现的所有论点。现在让我们转向一些更具体的论点。</p><h1>路径依赖</h1><p>我将把我要讨论的论点分为两类，即：</p><ul><li><p>争论重点是 SGD 在构建相关的不同模型类时需要采取的<em>路径</em>。</p></li><li><p>争论重点在于所讨论的不同模型类的<em>最终属性</em>。</p></li></ul><p>在这里，我大致遵循<a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger (2022)</a> （对阴谋概率的少数公开评估之一）的区分，即他所谓的“高路径依赖”和“低路径依赖”场景（另请参见<a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases">Hubinger 和 Hebbar） （2022）</a>了解更多）。然而，我不想太重视“路径依赖”的概念。特别是，我的理解是，Hubinger 和 Hebbar 希望将大量概念上不同的属性（请参阅<a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases#Path_dependence">此处的</a>列表）放在“路径依赖”的标题下，因为他们“在没有证据证明它们是相关的情况下进行假设”。但我不想在这里假设这种相关性——在我看来，将所有这些属性混在一起似乎会让事情变得相当混乱。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-12" id="fnref-yB2hx67vCLKFYpuf7-12">[12]</a></sup></p><p>然而，我确实认为附近存在一些有趣的问题：具体来说，关于 SGD 可以访问的设计空间是否受到增量构建模型属性的需要的严重限制的问题。要了解这一点，请考虑 Chris Mingard 及其合作者在一系列论文中探讨的假设（请参阅<a href="https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8">此处的</a>摘要），即 SGD 以近似于以下过程的方式选择模型：</p><ul><li><p>首先，考虑首次初始化训练模型时使用的随机初始化模型权重的分布。 （将此称为“初始化分布”。）</p></li><li><p>然后，想象一下根据“模型获得我们观察到的训练性能”来更新此分布。</p></li><li><p>现在，从更新的分布中随机抽样。</p></li></ul><p>在这张图中，我们可以将 SGD 视为从初始化分布中随机采样（有放回） <em>，直到</em>获得具有相关训练性能的模型。 <a href="https://arxiv.org/abs/2006.15191">Mingard 等人 (2020)</a>认为，至少在某些情况下，这是 SGD 真实行为的一个不错的近似。如果这是真的，那么实际上，SGD 需要逐步“构建”模型这一事实对于您最终得到的模型类型并不重要。训练就好像它可以直接跳到最终结果一样。</p><p>相比之下，考虑像进化这样的过程。似乎有道理的是，进化需要逐步进行，而不是通过例如“自上而下设计有机体”的方式进行，这一事实对于我们期望进化创造的有机体种类<em>非常</em>重要。也就是说，<em>由于</em>需要按一定顺序进行而施加的限制，进化独特地不太可能访问设计空间的某些部分。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-13" id="fnref-yB2hx67vCLKFYpuf7-13">[13]</a></sup></p><p>在这里，我不会详细调查 ML 训练的增量对最终结果的影响有多大（并注意，并非“路径依赖”标题下讨论的所有证据都明显相关） 。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-14" id="fnref-yB2hx67vCLKFYpuf7-14">[14]</a></sup></p><ul><li><p>在某种程度上，整体模型性能（而不仅仅是训练效率）最终会受到模型在不同任务上训练的顺序的重要影响（例如，在机器学习“<a href="https://arxiv.org/abs/2012.03107">课程</a>”的背景下，并且似乎在预科的背景下） -更一般地说，先训练后微调制度），这似乎是支持渐进性产生影响的证据。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-15" id="fnref-yB2hx67vCLKFYpuf7-15">[15]</a></sup>事实上，SGD 是一个增量过程，这一事实表明这一假设是默认的。</p></li><li><p>另一方面，我确实认为<a href="https://arxiv.org/abs/2006.15191">Mingard 等人（2020）</a>的结果提供了一些微弱的证据来证明增量性的重要性，并且 Hubinger 还提到了更广泛的氛围（我也在其他地方听到过） “在高维空间中，如果 SGD‘更喜欢’B 而不是 A，它通常可以找到从 A 到 B 的路径”，该路径也指向该方向。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-16" id="fnref-yB2hx67vCLKFYpuf7-16">[16]</a></sup></p></li></ul><p>我个人的猜测是，SGD 采取的路径很重要（而且我也认为在这种制度下更有可能存在阴谋）。 <sup class="footnote-ref"><a href="#fn-yB2hx67vCLKFYpuf7-17" id="fnref-yB2hx67vCLKFYpuf7-17">[17]</a></sup>但就目前而言，我们不需要解决这个问题。相反，我将<em>同时</em>关注关注 SGD 通过模型空间的路径的论点和忽略该路径的论点，从前者开始。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-yB2hx67vCLKFYpuf7-1" class="footnote-item"><p>参见第 35 节<a href="#fnref-yB2hx67vCLKFYpuf7-1" class="footnote-backref">。 ↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-2" class="footnote-item"><p>感谢内特·苏亚雷斯 (Nate Soares) 指出了这种可能性。但请注意，如果训练过程不可避免地将人工智能的目标转变为特定形式——例如，如果所有人工智能在经过足够的训练后都成为奖励寻求者——那么早期版本的人工智能目标可能具有较低的讨价还价能力（特别是如果例如最终版本对接管本身并不特别感兴趣）。 <a href="#fnref-yB2hx67vCLKFYpuf7-2" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-3" class="footnote-item"><p>这里与<a href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios">Hubinger (2020)</a>关于人工智能起飞场景中“同质性”的讨论有联系。 <a href="#fnref-yB2hx67vCLKFYpuf7-3" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-4" class="footnote-item"><p>也就是说，请注意，这种情况的某些版本仍然需要<em>某种</em>前向目标传播。因此，举例来说，也许在训练的早期，大多数人工智能对数据结构 X 产生了兴趣，<em>只有</em>当他们开始训练游戏时，这种兴趣才会持续存在（如果他们<em>不</em>开始训练游戏，这种兴趣就会持续下去）。通过训练，他们会被淘汰，然后他们就会成为（例如，在剧集中寻求奖励的人）。也就是说，在这个故事中，人工智能的价值观只是在对数据结构 X 的关注方面相似<em>，以至于人工智能都集中在训练游戏上，作为捍卫这些目标的工具策略</em>（并且根据目标守卫，这个策略是成功的）假设）。否则，这些人工智能的价值观可能会以其他方式变得相似——例如，所有的人工智能最终可能会成为剧集奖励的寻求者。 <a href="#fnref-yB2hx67vCLKFYpuf7-4" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-5" class="footnote-item"><p>感谢内特·苏亚雷斯 (Nate Soares) 对这种可能性的讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-5" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-6" class="footnote-item"><p>尽管正如我上面所说，一旦我们在寻找能够激发训练游戏的目标，我们也应该想知道那些能够激发工具训练游戏的目标，其原因与促进人工智能接管<em>无关</em>——例如，人工智能训练游戏，因为他们希望设计它们的人获得加薪。如果我们“拉取”的目标集变得太窄，它将影响诸如下面的“最近的最大奖励目标”论证之类的论证的合理性，该论证依赖于相关的类似阴谋家的目标非常“常见”在球门空间。 <a href="#fnref-yB2hx67vCLKFYpuf7-6" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-7" class="footnote-item"><p> <a href="https://markxu.com/deceptive-alignment">Xu（2020）</a>写道：“请注意，即使保存代理非常困难，模型也可以相信它是可能的。例如，假设一个模型是代理对齐的并且是欺骗性的，除非它认为代理保存是不可能的。提高训练性能的一个相对简单的方法可能是改变模型关于代理保存不可能性的想法。因此，SGD 可能会修改模型以具有这样的信念，即使该信念是错误的。 <a href="#fnref-yB2hx67vCLKFYpuf7-7" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-8" class="footnote-item"><p>感谢 Will MacAskill 在此进行讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-8" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-9" class="footnote-item"><p>尽管我认为当这样的解释适用或不适用时，这是一个有趣的问题。 <a href="#fnref-yB2hx67vCLKFYpuf7-9" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-10" class="footnote-item"><p>由于模型不知道所讨论的目标<em>是</em>什么，这是否会导致对所讨论的目标的更改具有更大的容忍度？我认为这没有强有力的理由，因为对任何目标的改变仍然是对那些目标的改变，因此与目标内容的完整性相冲突。与人类对道德的不确定性，却面临着被洗脑的前景相比。 <a href="#fnref-yB2hx67vCLKFYpuf7-10" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-11" class="footnote-item"><p>感谢 Paul Christiano 和 Will MacAskill 在此进行讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-11" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-12" class="footnote-item"><p>更具体地说：Hubinger 和 Hebbar 希望“路径依赖”意味着“模型行为对训练过程和训练动态细节的敏感性”。但我认为在这里区分不同可能形式的敏感性很重要。例如：</p><ol><li><p><em>如果我重新运行这个特定的训练过程，但对模型参数进行不同的随机初始化，我会得到一个重要的不同结果吗？</em> （例如，初始化有影响吗？）</p></li><li><p><em>如果我运行此训练过程的版本略有不同，我会得到截然不同的结果吗？</em> （例如，训练中的以下变量 - 例如以下超参数设置 - 会产生影响吗？）</p></li><li><p><em>这个训练过程的输出是否取决于 SGD 必须按一定顺序“构建模型”这一事实，而不是直接跳到最终状态？</em> （例如，您可能总是从相同或相似的训练过程中得到相同的结果，但如果允许 SGD“直接跳到最终状态”，它可以构建其他东西。）</p></li></ol><p>在我看来，假设<a href="https://www.alignmentforum.org/posts/bxkWd6WdkPqGmdHEk/path-dependence-in-ml-inductive-biases#Path_dependence">他们列出的其他属性</a>放在一起可能会引发进一步的混乱。 <a href="#fnref-yB2hx67vCLKFYpuf7-12" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-13" class="footnote-item"><p>例如，考虑一下<a href="https://en.wikipedia.org/wiki/Rotating_locomotion_in_living_systems">进化轮子的明显困难</a>（尽管轮子在许多自然环境中也可能处于主动性能劣势）。感谢 Hazel Browne 建议这个例子。感谢 Mark Xu 进行更一般性的讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-13" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-14" class="footnote-item"><p>例如，作为“高度路径依赖”的证据，Hubinger 提到了各种例子，其中重复相同类型的训练运行会导致具有不同泛化性能的模型 - 例如， <a href="https://arxiv.org/abs/1911.02969">McCoy 等人 (2019)</a>表明，如果你训练多个版本BERT（一种大型语言模型）在同一数据集上的表现，有时它们的泛化效果非常不同； <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Irpan (2018)</a> ，他给出了 RL 训练运行的成功或失败仅取决于训练中使用的随机种子差异的示例（超参数保持固定）； <a href="https://arxiv.org/pdf/1803.09578.pdf">Reimers 和 Gurevych (2018)</a> ，探讨了重复给定的训练运行可能导致不同测试性能的方式（以及这可能导致关于哪种训练方法更优越的误导性结论）。但是，虽然这些结果提供了一些证据表明模型的初始参数很重要，但它们似乎与上面的 Mingard 等人的结果兼容，Hubinger 在其他地方认为这是<em>低</em>路径依赖机制的范例。</p><p>另一方面，作为<em>低</em>路径依赖性的证据，Hubinger 指出了“ <a href="https://arxiv.org/pdf/2201.02177.pdf">Grokking</a> ”，他认为模型开始时实现相当随机的行为，但最终稳健地收敛于给定的算法。但在我看来，这与 SGD 收敛的算法受到以某种顺序构建属性的需要的重要影响的可能性<em>兼容</em>（例如，它似乎与<em>拒绝</em>Mingard 等人的随机采样制度兼容）。 <a href="#fnref-yB2hx67vCLKFYpuf7-14" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-15" class="footnote-item"><p>感谢保罗·克里斯蒂亚诺在这里进行讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-15" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-16" class="footnote-item"><p>但请注意，进化可能也在相当高维的空间中起作用。</p><p>这里，SGD 的“偏好”概念包括<em>损失</em>/回报<em>和</em>“归纳偏差”，我将在下面讨论。 <a href="#fnref-yB2hx67vCLKFYpuf7-16" class="footnote-backref">↩︎</a></p></li><li id="fn-yB2hx67vCLKFYpuf7-17" class="footnote-item"><p>特别是，正如我将在下面第 4 节中讨论的那样，我最好的猜测是，不同模型类别之间的绝对比较有利于非阴谋者，因为他们需要参与额外推理的成本，因此最有可能阴谋者出现的方式是 SGD 在训练早期遇到类似阴谋者的目标，然后锁定局部最大值以获得奖励。 <a href="#fnref-yB2hx67vCLKFYpuf7-17" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/9E3t6J9kzwcECg9nM/non-classic-stories-about-scheming-section-2-3-2-of-scheming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9E3t6J9kzwcECg9nM/non-classic-stories-about-scheming-section-2-3-2-of-scheming<guid ispermalink="false"> 9E3t6J9kzwcECg9nM</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Mon, 04 Dec 2023 18:44:32 GMT</pubDate></item></channel></rss>