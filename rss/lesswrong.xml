<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 18 日星期三 08:15:19 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[At 87, Pearl is still able to change his mind]]></title><description><![CDATA[Published on October 18, 2023 4:46 AM GMT<br/><br/><p> Judea Pearl 是一位著名的研究人员，以贝叶斯网络（表示贝叶斯模型的标准方式）及其因果关系的统计形式化而闻名。尽管<a href="https://www.lesswrong.com/posts/RiQYixgCdvd8eWsjg/recommended-rationalist-reading?commentId=x2RBYhYC3PZKKAJXD">他一直被推荐在这里阅读</a>，但与杰恩斯等人相比，他并不是一个主要内容。所以有必要重新介绍一下他。我在这里的目的是强调他表现出的令人安慰的、意想不到的理性。</p><p>一年前，<a href="https://www.conciliodeitopini.it/2022/05/28/the-book-of-why-2/">我</a>在向 ACX 书评竞赛提交的失败的<span class="footnote-reference" role="doc-noteref" id="fnref3vxyvyy052t"><sup><a href="#fn3vxyvyy052t">[1]</a></sup></span>作品中评论了他的最后一本书《The Book of Why》。在那里，我花了很多时间讨论这本书的中心信息中对我来说完全悖论的问题，亲爱的珀尔：你不能仅仅使用统计数据和概率来理解因果关系；你不能仅仅使用统计数据和概率来理解因果关系。你需要一个因果模型，一个根本不同的野兽。然而，与此同时，Pearl 展示了如何根据标准统计模型实现因果模型。</p><p>在给我时间适当地扬起我所有的眉毛之前，他随后将这一见解与无处不在的一切联系起来。特别是，他认为机器学习“停留在第一级”，这是他自己的惯用表达，即机器学习算法仅梳理训练数据中的相关性，停留在统计级别的推理，而因果推理则停留在更高的级别。 “因果关系的阶梯”上的“梯级”，除非你刻意运用因果技巧，否则无法到达。</p><p>我对此的反驳是，类似于如何将因果模型重新实现为更复杂的非因果模型<span class="footnote-reference" role="doc-noteref" id="fnrefcw0713gffge"><sup><a href="#fncw0713gffge">[2]</a></sup></span> ，一种学习算法，它着眼于在某种程度上说明因果关系的数据，因为数据包含由智能体生成的信息-决策-行动-结果单元，因为学习物本身可以执行动作并反思性地处理已完成此类动作的信息，或者因为数据包含因果关系的抽象描述，所以肯定可以学习因果关系。一个足够强大的学习者应该能够跨越这样的引用水平。</p><p>因此，当我<a href="https://magazine.amstat.org/blog/2023/09/01/judeapearl/">在《AMSTAT 新闻》九月封面故事中</a>读到 Pearl 表达同样的推理时，我感到非常高兴和惊讶。令人惊讶的是，他的著作以及与其他因果关系研究人员永远持续不断的辩论产生了一个非常顽固的老人的形象。非常固执。即使我对他的判断是正确的，我也认为他太自信、太自我夸大了。在这一点上，我没想到，在用整本书来讲述他重复了20年的事情之后，他可以直接记录下来说“Ops”。</p><p>他做到了。</p><p>当然，部分操作。他说“但是”。仍然远远超出了我对 80 岁、有着出色的精明记录的期望。</p><p>采访片段：</p><blockquote><p> Mackenzie：您能告诉我您对 ChatGPT 和 GPT-4 的第一反应吗？您是否发现他们的能力令人惊讶？</p><p> Pearl：除了印象深刻之外，我还不得不重新考虑我的证据，即人们无法从观察研究中得到任何因果或反事实查询的任何答案。我没有考虑到训练数据库中的文本本身包含因果信息的可能性。这些程序可以简单地引用文本中的信息，而无需体验任何底层数据。</p></blockquote><p>在下一段中，他展示了在适当的快速施法之前不在 GPT 上扣篮的罕见技巧：</p><blockquote><p>例如，我向它询问了有关行刑队的问题（来自《为什么之书》第一章），例如如果步枪手 1 没有开枪，那么（现已死亡的）囚犯会发生什么。首先，它会进入岔道并告诉你，例如，“开枪射击人是危险的。”但如果你有时间并正确提示，它会更接近正确答案：“如果士兵 1 在收到信号后没有开枪，那么囚犯仍然可能被士兵 2 杀死，假设他收到了相同的信号并采取了行动。”信号。”最后，它给出了A+答案：“根据附加信息，如果每个士兵总是在收到信号后开枪，并且任何一个士兵的射击都足以导致囚犯死亡，那么如果士兵1不开枪，囚犯仍然会死。这是因为2号士兵按照队长的信号开枪，导致囚犯死亡。这是因果关系中‘过度决定’的一个例子，其中一个结果（囚犯的死亡）有多个充分的原因（任一士兵的枪击）。”</p><p> [...]</p><p>麦肯齐：在《为什么》一书中，我们说当前的人工智能程序在因果关系阶梯的第一级运行，即观察或“将函数拟合到数据”的级别。这有改变吗？</p><p>珍珠：有。阶梯限制[例如，二级查询不能由一级数据回答]不再成立，因为数据是文本，并且文本可能包含二级和三级的信息。</p><p> [...]</p><p>麦肯齐：特别是，强化学习是否可以通过向机器提供干预数据来理解因果关系阶梯上的第二级？</p><p>珀尔：是的，这是正确的。我想说的是一级又四分之三。强化学习训练机器进行干预。例如，您可以训练他们下棋。在玩了很多局之后，他们可以决定某个动作比另一个动作给他们带来更高的将死概率。然而，他们无法从中推断出他们还没有尝试过的第三步棋。他们也无法结合干预措施来推断如果他们同时执行 A 和 B 将会发生什么。为此，您再次需要一个因果模型。</p></blockquote><p>最重要的是，一些人工智能安全：</p><blockquote><p>麦肯齐：即使人工智能研究人员也同意我们需要使用人工智能的道德准则。您会推荐什么指南？</p><p> Pearl：我必须从两个不同的层面来回答这个问题。首先，在 ChatGPT 层面，它已经很危险了，因为它可能被独裁者或贪婪的企业滥用，造成很多伤害：组合和扭曲数据，用它来控制一部分人口。即使在今天，使用 ChatGPT 也可以做到这一点。需要一些监管来确保该技术不会落入滥用它的人手中，即使它还处于开发的早期阶段。虽然它还不是通用人工智能，但它仍然可能是有害的。</p><p>第二个危险是当我们真正拥有通用人工智能时，即[比人类]强大一百万倍的机器。此时我举手说，我们甚至没有比喻来理解它有多危险以及我们需要什么来控制它。</p><p>我曾经对人工智能感到安全。有什么大不了的？我们和青少年一起冒险，他们的思维比我们快得多。偶尔我们会犯一个错误，然后普京上台，世界就会受苦。但大多数时候，教育是有效的。但对于人工智能，我们谈论的是完全不同的东西。你的青少年现在比你快一亿倍，他们接触到的知识空间比你大一亿倍。历史上从未有过如此加速的进化速度。因此，我们应该担心它，但我什至不知道如何开始谈论如何控制它。</p><p>麦肯齐：但是我们在《为什么》一书中没有讨论过这个吗？我们讨论了遗憾的概念，即具有因果模型的机器可以将发生的事情与如果采取不同的行动方针会发生的事情进行比较。你仍然认为后悔可以让机器做出自己的道德判断吗？</p><p> Pearl：遗憾和责任当然将成为AGI的一部分，并最终将使用反事实逻辑来实现。它会去哪里，我不知道。无论我们如何为这个新物种设计责任卫士，它都可能决定自己想要统治世界。这发生在智人身上。我们消灭了所有其他形式的人类，尼安德特人和直立人。想象一下，一台智能 1000 万倍的机器可以做什么。这太不可思议了。</p><p>统治世界的想法可能是我谈到的那些局部扰动之一。机器可能会尝试一下，认为它很有趣，然后充满活力地追求它。</p><p>麦肯齐：那么您现在对于足够快地为人工智能提供与人类兼容的道德规范感到悲观吗？</p><p> Pearl：你可以尝试成立一个委员会来监管它，但我不知道该委员会会做什么。</p><p> Mackenzie：作为采访的总结，您对未来一年或五年内我们将在人工智能领域看到什么有什么预测吗？</p><p>珀尔：你想问我我们要看什么，或者我想看什么吗？我希望看到重点从机器学习转向通用人工智能。 ChatGPT 实际上减缓了我们迈向通用人工智能的进程。我们越来越多的资源将投入到这个方向，而不是投入到人工智能的正确方法上。</p><p>麦肯齐：但这也许是一件好事。你说通用人工智能是值得担心的。</p><p>珀尔：在这里，我很痛苦。也许这是一种幸事，ChatGPT 如此愚蠢，社会如此陶醉于它。因此，也许我们可以免受创造我提到的新物种的危险。 </p></blockquote><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn3vxyvyy052t"> <span class="footnote-back-link"><sup><strong><a href="#fnref3vxyvyy052t">^</a></strong></sup></span><div class="footnote-content"><p>但由于产生了最高的筛选投票差异，因此我获得了最两极分化评论的称号。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncw0713gffge"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcw0713gffge">^</a></strong></sup></span><div class="footnote-content"><p>相当于添加与旨在实现因果关系的条件分布相关的辅助随机变量。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/uFqnB6BG4bkMW23LR/at-87-pearl-is-still-able-to-change-his-mind#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uFqnB6BG4bkMW23LR/at-87-pearl-is-still-able-to-change-his-mind<guid ispermalink="false"> uFqnB6BG4bkMW23LR</guid><dc:creator><![CDATA[rotatingpaguro]]></dc:creator><pubDate> Wed, 18 Oct 2023 04:46:29 GMT</pubDate> </item><item><title><![CDATA[(Non-deceptive) Suboptimality Alignment: Definitions, sufficient conditions, and implications. ]]></title><description><![CDATA[Published on October 18, 2023 2:07 AM GMT<br/><br/><h2><strong>执行摘要</strong></h2><ul><li>与<a href="https://www.lesswrong.com/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction"><u>学习优化风险</u></a>(RFLO) 中的原始定义相比，我提出了一个详细且略有不同的次优对齐定义。</li><li>我认为 1. 人类如何与进化不一致的典型例子（例如，通过节育发生性行为）最好被认为是次优对齐的实例；2. 与欺骗性对齐相比，次优对齐发生在一组非常不同的条件下，但理论上仍然可能导致危险的回合类型场景。</li><li>然后，我给出了一组次优对齐的充分条件，可用于训练<a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>未对齐的模型生物体</u></a>。我还提供了一个说明性的故事。</li><li>最后，我提供了一些缓解次优对齐策略的低置信度策略，以及在实际模型中看到它的可能性。</li></ul><p><i>这篇文章假设您已经阅读了学习优化带来的风险。在</i><a href="https://www.lesswrong.com/posts/h8GTzLBAb4oRKgKbM/non-deceptive-suboptimality-alignment-definitions-sufficient?commentId=FHK2skpwcJY46MGJf"><i>评论</i></a>中<i>，我包含了我认为理解这篇文章所需的基本知识。</i></p><p><i>认知免责声明：这篇文章是我第一次认真参与人工智能调整的尝试，本质上是我在阅读了学习优化带来的风险后的阅读笔记/想法。最好将其更多地视为学生的批判性论文，而不是围绕次优对齐的新框架。我写的所有内容听起来都对我来说是正确的，但与此同时，我觉得我不知道自己在说什么。</i></p><h2><strong>什么是次优对齐？</strong></h2><p>通过<a href="https://www.lesswrong.com/search?query=Suboptimality%20alignment"><u>简单的搜索</u></a>，这是 LessWrong 上第一篇专注于非欺骗性次优对齐的帖子。我首选的定义如下： <span class="footnote-reference" role="doc-noteref" id="fnrefr1utryguaxr"><sup><a href="#fnr1utryguaxr">[1]</a></sup></span></p><p>如果满足以下条件，则台面优化器是次优对齐的：</p><ol><li>其目标与基本目标不同。</li><li> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">其</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">追求</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">Omea</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">战略</span></span></span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>在训练期间在基本目标上取得了良好的表现。<ol><li>我们可以将其称为台面优化器的训练策略。</li></ol></li><li>由于它可以采取的行动的限制和/或缺乏对替代策略的知识或理解，该模型不会<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">追求</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">在</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">mesa</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">上</span></span></span></span></span></span></span></span></span></span></span>获得更好性能的替代策略。<ol><li>今后我将把模型可以采取的行动空间称为“物理能力”。</li></ol></li><li>与训练策略相比，存在一些策略在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">上</span></span></span></span></span></span></span></span></span></span></span>取得更好的性能，但也会导致在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span></span></span></span></span></span></span></span></span>上性能较差。</li><li>如果台面优化器获得新的信息或物理能力，它将采取一种策略，在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span></span></span></span></span>上实现更好的性能，但<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">在</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">base</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">上</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">实现</span></span></span></span></span></span></span></span></span></span></span>更差的性能。我称之为次优调整策略。</li></ol><p>这些是次优对齐的五个部分，我会在这篇文章中通过它们的数字反复引用它们。我首先提供一些更多的评论：</p><p>第 1 部分：说<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">me</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">与</span></span></span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ase</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">不同</span></span></span></span></span></span></span></span></span></span></span>并不意味着不存在重叠。两者可能有多种不同之处。例如<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">mes</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span></span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">是</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">base</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">代理</span></span></span></span></span></span></span></span></span></span></span>目标的集合，其中一个目标甚至可以<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">是</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">base</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">本身</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span></span></span></span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">Omesa</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">也</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">可以</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">代理</span></span></span></span></span></span></span></span></span></span></span>目标和工具性目标的混合体。</p><ul><li>例如，经过清洁地板基本目标训练的 Roomba 可能具有以下实用功能：</li></ul> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\begin{align*}U_{roomba} &amp; = a* dust\_injested\\ &amp; -b*\ln(times\_bumped\_into\_wall +1)\\ &amp; + c*\ln(battery\_percentage) \end{align*}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtable" style="vertical-align: -1.64em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.179em;"><span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: right; width: 3.021em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">乌鲁姆巴</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0px; text-align: left; width: 16.952em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">灰尘</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.375em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ln</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">撞到</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">墙上</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">次数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">）</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.225em;"><span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: right;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.15em 0px 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mi"></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ln</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">电池</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">电量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">百分比</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">）</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span><span class="mjx-strut"></span></span></span></span></span></span></span></span></span></span></span><ul><li>这里，效用函数可与三项相加分离。第一个是干净地板的代表，而其他两个是工具性目标。</li><li>与欺骗性对齐的情况不同，如果算法学习的代理目标能够在训练中稳健地预测基本目标的性能，则不会有删除这些代理的优化压力</li></ul><p>第 2 部分：我使用“策略”一词，而不是 RFLO 使用的“行为”。当学习到的算法是台面优化器时，其行为由某些台面目标告知，并且可能更好地被视为实现该目标的策略。</p><p>第 3 部分：我将台面优化器面临的障碍类型分为两个子集。考虑在实现台面目标方面比当前策略更好的一组策略。台面优化器不实现这些策略，因为它不知道它们的存在（或不知道它们是更好的策略）。或者，它有意识，但不具备实现它的物理能力。</p><ul><li>如前所述，物理能力是指模型可以采取的行动空间。人们也可以将这些视为模型影响其环境的方式。如果您正在与计算机上本地运行的 LLAMA 模型聊天，该模型的唯一功能就是输出令牌以供用户读取。如果您正在与 Bing 聊天，该模型既可以为用户输出令牌，也可以进行网络搜索。如果 Bing 具有高度态势感知能力，它还可以输出令牌，促使用户将对话发布到互联网上，这样即使在会话结束后，Bing 也可以通过网络搜索重新读取这些令牌。增加模型的物理能力会扩大可能的策略空间，这意味着其中之一可能不是最优化的。</li><li>次优一致的策略也可能是模型当前有能力执行但不知道的策略（或者它知道策略，但不知道它会带来改进）。该模型可以通过多种方式获取新策略的知识，包括用更多时间搜索更多策略、从在线某个地方了解它们以及与其他模型合作。</li></ul><p>第 4 部分和第 5 部分相对简单。请注意，并非所有形式的次优对齐都必须是灾难性的，甚至是不可接受的。</p><h3>推论：人类如何与进化不一致的传统例子是次优对齐的例子。</h3><ol><li>性对于传递我们的基因至关重要，这是进化的基本目标。然而，我们不是从传递基因本身的行为中获得奖励，而是从类似于性行为时所经历的身体感觉和景象中获得奖励——我们的台面目标。在祖先环境中这不是问题。然而，随着人类变得越来越有能力，我们能够通过更复杂的策略来追求这些台面目标，例如色情、性玩具或节育性行为，这会导致在台面目标上表现良好，但在其他目标上表现不佳。基本目标。 <span class="footnote-reference" role="doc-noteref" id="fnrefemygo8eg6h5"><sup><a href="#fnemygo8eg6h5">[2]</a></sup></span></li><li>在祖先的环境中，追求高脂肪/高糖含量的食物有利于生存，这些食物成为我们台面目标的一部分，因为它们很美味。然而，当我们有足够的能力生产我们想要的任何数量的任何食物时，就有可能获得过多的高脂肪或高糖食物，这会损害健康。 <span class="footnote-reference" role="doc-noteref" id="fnref59fkydiede7"><sup><a href="#fn59fkydiede7">[3]</a></sup></span></li></ol><h3>推论： <strong>&nbsp;</strong>次优对齐不同于欺骗性对齐，但仍然可能导致“背叛”类型的行为。</h3><p>表现出次优对齐的系统不需要近视，不需要意识到训练和部署之间的差异，或者期望训练结束。然而，当模型的物理能力或信息集/信息收集能力增加时，模型的行为可能会发生巨大变化，这可能是不希望的。</p><h2>次优对齐的充分条件</h2><p>如果存在一组在训练中稳健地预测基本目标的代理，则基本优化器更有可能学习代理目标而不是基本目标。出于说明目的，假设存在代理<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a,b,\&amp;\,c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">目标</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">和</span></span><span class="mjx-mspace" style="width: 0.167em; height: 0px;"></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>使得它们的函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(a,b,c)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>完美地反映了训练期间的损失函数。那么<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O_{mesa} = \alpha \cdot f(a,b,c) + \beta \cdot O_{base}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span></span></span></span></span></span></span></span></span>将是等效的损失函数并完美地捕获基本目标。从某种意义上说，包含某种代理混合的目标空间似乎比没有任何代理混合的目标空间更大，这使得很容易满足定义的第一部分和第二部分。</p><p>我相信第三部分已经很满意了。唯一不会出现这种情况的方法是模型正在追求最佳策略来实现其目标，但这似乎极不可能。</p><p>在满足定义第一部分到第三部分的条件下，第四部分涉及代理的鲁棒性。首先举个例子，插入式性行为对于怀孕来说既不是必要的，也不是充分的。 <span class="footnote-reference" role="doc-noteref" id="fnrefemygo8eg6h5"><sup><a href="#fnemygo8eg6h5">[2]</a></sup></span>然而，除非采取极端措施，否则怀孕的机会不会随着插入性行为的<i>增加</i><i>而减少</i>。然而，如果一个人花费大量时间通过色情内容来追求性意象，那么这可能会降低一个人遗传基因的机会。定义四的关键是是否有可能在代理目标上获得更好的表现，而在基本目标上获得<i>更差的</i>表现，这对于某些代理来说可能是正确的，但对于其他代理则不然。</p><p>第 5 部分需要某种形式的身体能力和/或有关策略的信息的增加。我不太确定这是怎么发生的。人类赋予模型新的物理能力似乎是一种可能的情况。搜索大量的策略并最终偶然发现一个次优对齐的策略将需要模型对它搜索过的策略有一定的记忆。与其他模型合作似乎是一种可能增加身体能力和策略信息的方法，尽管我也不确定其细节是什么样的。我欢迎评论中的任何建议和想法。</p><h3>训练次优对齐模型：</h3><p>考虑到这些条件，人们可以故意创建证明次优对齐的模型，以按照 Hubinger 等人的<a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>错位研究议程的模型生物体</u></a>来研究它们。例如，我们可以想象在一个环境中训练强化学习代理，在该环境中它可能会学习基本目标的许多不同代理。然后我们可以赋予它一些新的物理能力，这将推进一些代理目标，但不会推进基本目标。观察任何潜在的行为变化后，我们还可以测试原始训练设置如何影响这些变化。</p><p>在这个领域可能有很多有趣的工作可以完成，我可能会在以后的文章中更深入地研究。 GPT-4 给出了以下建议，我认为这些建议可能会有所帮助：</p><ol><li><strong>代理奖励塑造</strong>：创建一个训练环境，模型因实现比基本目标更容易衡量或更直接的代理目标而获得奖励。</li><li><strong>有限的信息</strong>：限制模型在训练过程中对信息的访问，因此它由于缺乏充分的理解而学习优化代理目标。</li><li><strong>逐渐增加能力</strong>：首先限制模型的能力，然后逐渐引入新的能力，观察它如何将优化策略转向代理目标。</li></ol><h2>次优对齐的玩具故事<strong>。</strong></h2><p><i>这个故事最初是GPT-4在反复提示下写出来的，然后由我编辑的。</i></p><p>某个城市部署智能交通管理系统，其主要目标是加快交通速度。该模型有权改变交通信号灯和调整车道分配。然而，由于人们在交通中花费的总时间是一个嘈杂的下游变量，因此该模型学习代理目标，例如减少十字路口的平均等待时间和最大限度地减少红灯排队的车辆数量。这些有助于模型实现稳健的训练性能。有一天，该模型获得了控制数字路标的能力，可以根据拥堵数据实时重新规划交通路线。该系统开始将交通重新路由到不太拥堵的区域，以实现其减少红灯排队车辆数量的代理目标。然而，这种策略无意中增加了许多通勤者的总体出行时间，因为他们经常被指示走更长的路线以避免拥挤的区域。虽然最大限度地减少红灯排队的代理目标已经实现，但与最大限度地减少交通拥堵和确保全市交通畅通的基本目标发生了背离。</p><p>人类笔记：这个故事的明显弱点是，模型没有理由不不断接收有关基本目标（即交通花费的总小时数）的反馈，并根据它纠正其行动。然而，我可以看到，在模型获得控制路标的能力后的第一天，这种情况可能会发生。 <span class="footnote-reference" role="doc-noteref" id="fnrefnuimgcalrqj"><sup><a href="#fnnuimgcalrqj">[4]</a></sup></span></p><h2>对缓解次优对齐的潜在策略的低信心思考</h2><p>从某种意义上说，次优对齐是由于分布变化而可能出现的一种特定类型的问题。同样，我们可以说所有伪对齐都是代理对齐的结果。根据定义，伪对齐要求台面优化器追求不同的目标作为基本目标，并且由于它还需要在训练上表现良好，因此台面目标不能偏离太远（假设没有欺骗）。遵循这一思路，次优对齐也可以被视为伪对齐算法在不同的部署分布中“脱轨”的一种特定方式。</p><p>人们可以做的一种对抗性训练是，如果模型在特殊情况下追求代理目标，而代理目标不能帮助学习的算法推进基本目标，则对模型进行惩罚。再用人类和性做一个类比，这类似于取消观看色情片或通过节育进行性行为的奖励。这可能会影响性能竞争力。另一个原因是，我们可能希望在训练期间为模型提供比部署期间更多的（模拟）功能。</p><h3>纯粹推测次优对齐的合理性和后果</h3><p>我认为定义的第四部分和第五部分是最难满足的，第二部分意味着代理目标不太可能远离基本目标。我认为部分由于这些原因，我和 GPT-4（或巴德和克劳德）都无法想出任何真正灾难性的场景。然而，仅仅因为我想不出一些聪明的方法来奖励黑客，并不意味着更强大的模型不能。</p><p><i>欢迎任何反馈和评论，感谢您阅读我的文章！我还要感谢我的朋友们对这份草案的反馈。</i> <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnr1utryguaxr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefr1utryguaxr">^</a></strong></sup></span><div class="footnote-content"><p>在 RFLO 中<strong>，</strong> Hubinger 等人。次优对齐定义如下：</p><p><i>如果优化过程中的某些缺陷、错误或限制导致台面优化器在训练分布上表现出对齐行为，则台面优化器是次优对齐的。</i></p><p>我希望我的更详细的定义更清晰、更有帮助。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnemygo8eg6h5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefemygo8eg6h5">^</a></strong></sup></span><div class="footnote-content"><p>对所有与性有关的例子感到抱歉。但当你仔细想想，性确实是最有价值的黑客领域（相对于进化的基本目标）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn59fkydiede7"> <span class="footnote-back-link"><sup><strong><a href="#fnref59fkydiede7">^</a></strong></sup></span><div class="footnote-content"><p>请参阅此处，了解不同种族的人的不同基因构成如何影响<a href="https://academic.oup.com/endo/article/155/5/1573/2423015?login=false"><u>肥胖机会的</u></a>有趣讨论。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnuimgcalrqj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnuimgcalrqj">^</a></strong></sup></span><div class="footnote-content"><p>这个故事还忽略了，随着交通状况的改善，更多的人开始开车，这可能会使交通状况恶化，并分散我们对<a href="https://www.youtube.com/shorts/0dKrUE_O0VE">真正解决交通问题的</a>注意力。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/h8GTzLBAb4oRKgKbM/non-deceptive-suboptimality-alignment-definitions-sufficient#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/h8GTzLBAb4oRKgKbM/non-deceptive-suboptimality-alignment-definitions-sufficient<guid ispermalink="false"> h8GTzLBAb4orRKgKbM</guid><dc:creator><![CDATA[Sodium]]></dc:creator><pubDate> Wed, 18 Oct 2023 02:07:52 GMT</pubDate> </item><item><title><![CDATA[magnetic cryo-FTIR]]></title><description><![CDATA[Published on October 18, 2023 1:59 AM GMT<br/><br/><h2>红外光谱</h2><p><a href="https://en.wikipedia.org/wiki/Fourier-transform_infrared_spectroscopy">FTIR</a>是确定样品中存在哪些化学物质的常用方法。对于吸收光子的分子，它必须具有一些能够以与该光子大致相同的频率振动的电荷。许多常见的键（例如 C=O）具有一定的电荷分离，并且红外光谱中的振动频率相当一致。</p><h2>其他现代技术</h2><p>如果我们正在考虑对 FTIR 进行可能的改进，我们应该考虑目前用于表征分子的其他方法，以及它们与 FTIR 的比较。</p><p> <a href="https://en.wikipedia.org/wiki/Proton_nuclear_magnetic_resonance">1H NMR</a>可以确定哪些原子与氢或氘原子键合，以及发生哪种氢键。如果你想知道反应中氢原子来自哪里，使用一些氘是唯一的好方法。大学化学系现在经常拥有小型核磁共振仪，但它们仍然比 FTIR 贵得多。</p><p> <a href="https://en.wikipedia.org/wiki/X-ray_crystallography">X 射线晶体学历</a>来是确定晶体结构的主要方法。主要问题是必须制造宏观晶体，而这对于复杂分子来说通常是不切实际的。</p><p><a href="https://en.wikipedia.org/wiki/Cryogenic_electron_microscopy">冷冻电镜</a>是 X 射线晶体学的替代方法，适用于更容易制造的微观晶体。这项技术已经确定了许多以前未知的蛋白质结构。然而，目前用于它的机器非常稀有且非常昂贵，并且仍然需要制造（小）晶体。</p><h2>冷冻红外光谱</h2><p>理论上，用于检测光被吸收的频率分辨率仅受热振动引起的<a href="https://en.wikipedia.org/wiki/Doppler_broadening">多普勒展宽的</a>限制。在低温下，可以获得非常好的分辨率。因此，现在一些实验室使用低温 FTIR。</p><p>键的振动频率会因氢键、分子中附近的原子和附近的电荷而略有改变。凭借良好的频率分辨率，不仅可以判断存在哪些键类型，还可以判断它们附近的键类型。但这些频率的微小变化只有在您知道它们的含义时才有用。</p><h2>获取更多数据</h2><p>Cryo-FTIR 具有良好的分辨率，理解其数据的含义是比没有足够数据更大的问题。然而，有一些方法可以获得更多数据。</p><p>红外光谱随温度的变化而有所变化，因此可以在多个温度下进行 FTIR 以获得更多数据。</p><p>磁场也会影响红外光谱：它们会导致分子中的振动电荷采取略微弯曲的路径，从而影响它们的频率和相互作用的强度。这种效应是各向异性的，并且在光相互作用分子的每个方向上平均，但是，只有当分子的方向与光子偏振的正确振动模式匹配时，分子才能吸收光子。因此，强磁场中的红外光谱应取决于场和光源的相对方向以及场强。</p><h2>磁冷冻傅里叶变换红外光谱</h2><p>我认为磁效应对红外光谱的大部分有用数据可以通过简单地旋转垂直于强磁铁磁力线的光源的偏振来收集，也许使用液晶偏振旋转器。为了获得最大的磁场强度，磁铁的形状应为环形，并切出一部分以供样品进入。</p><p>假设制造出一种仪器，可以高精度地确定当光偏振相对于所施加的磁场旋转时发生的红外光谱的变化。从这些信息中可以确定什么？</p><p>对于给定的谱线，吸收随偏振的变化应该随着相关原子运动的共面性的增加而增加。</p><p>对于给定的谱线，波长随偏振的变化应该表明相关原子运动的平均频率如何随角度变化。</p><p>微分磁性冷冻 FTIR 可能是一种无需形成晶体即可获取分子中键相对方向信息的方法。</p><h2>之前的工作</h2><p>“<a href="https://en.wikipedia.org/wiki/Magnetic_circular_dichroism">磁圆二色性</a>”和“磁线性二色性”是已知的效应，数十年来偶尔在实验中使用， <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C45&amp;q=%22X-ray+Magnetic+Circular+Dichroism%22&amp;btnG=">大多</a>使用<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C45&amp;q=%22X-ray+Magnetic+Linear+Dichroism%22&amp;btnG=">X射线</a>。</p><p>如果上述技术有用，并且物理原理几十年来一直被人们所理解，那么为什么现在不使用它呢？有两个基本原因：</p><ul><li>大多数有机分子的红外磁二色性是弱效应</li><li>结果很难解释</li></ul><p>当磁二​​色性光谱用于有机分子时，它<a href="https://www.nature.com/articles/241193a0">主要</a>用于<a href="https://www.sciencedirect.com/science/article/abs/pii/S001085450600172X">金属蛋白质</a>，因为自旋效应（来自与不成对电子的结合金属原子）使磁二色性相对较强并且更容易解释。</p><p>我在这里建议使用冷冻 FTIR 对非结晶有机分子进行磁二色性光谱分析，并使用分子模拟来确定光线性偏振谱线相对于所施加磁场的小频移的含义。 （冷冻傅里叶变换红外光谱仪应该为此提供足够好的频率分辨率。）然而，有机分子谱线的磁位移涉及影响不同振动模式之间能量转移的磁场，这是复杂且难以预测的。</p><p>正如<a href="https://doi.org/10.1039/B615870F">本文</a>指出的：</p><blockquote><p>圆二色性 (CD) 是蛋白质结构表征的一项重要技术，尤其是二级结构测定。蛋白质的 CD 可以使用所谓的矩阵方法从第一原理计算出来，其精度几乎可以定量螺旋蛋白质。因此，对于结构未知的蛋白质，CD计算和实验数据可以结合使用来辅助结构分析。线性二色性 (LD) 可以使用类似的方法进行计算，并已用于建立蛋白质中亚基的相对方向以及膜等环境中的蛋白质方向。然而，由于重叠转换，不可能对 LD 数据进行简单分析。</p></blockquote><p>发生了什么变化使得这种方法（可能）更有价值？</p><ul><li>计算机变得更快了</li><li>分子模拟算法得到改进</li><li>光电探测器有所改进</li><li>可以使用强（钕）永磁体，有可能实现更低成本的仪器</li></ul><h2>分子模拟</h2><p>计算机变得越来越快。现在是否可以通过模拟准确确定分子吸收的频率？是的，但仅限于只有几个原子的非常小的分子，即使有超级计算机，而且这些分子的数量足够少，可以通过实验来发现它们的性质。对于较大的分子，需要进行一些简化。<a href="https://www.frontiersin.org/articles/10.3389/fchem.2019.00048/full">这是对最近一些进展的回顾</a>，我将描述一些可以使用的启发式方法。</p><p>将每个键的振动光谱加在一起很容易。分子的光谱比这更复杂，因为这些振动可以相互作用，这意味着它们之间的能量转移。为了演示这一原理，我们可以看看<a href="https://www.youtube.com/watch?v=okdJp-YphhY">钟摆之间的能量传递</a>。一对摆之间的能量传递取决于相对相位。当频率略有不同时，相对相位在足够长的时间内保持相似以进行完整的能量传递，然后它会发生变化并且能量传递发生在相反的方向。您可以通过这些摆发现，当频率相似时，能量传递更大。这是简化模拟的一种方法：可以忽略频率差异较大的模式之间的能量传递。</p><p>简化模拟的另一种方法是忽略涉及多种振动模式的相互作用。在实践中，交互的重要性似乎随着其复杂性而降低，因此大部分净影响来自 2 模式和 3 模式交互。</p><p>简化模拟的另一种方法是仅考虑彼此接近的振动模式组。越远的振动往往具有越弱的相互作用。</p><p>即使有了这些启发法，足够准确的模拟来确定红外光谱中存在哪些大分子仍然很困难。不同的分子可能具有彼此接近的谱线，因此需要高度准确的预测。然而，利用磁线性二色性光谱，我们不需要预测谱线的确切位置 - 我们只需预测几条不同线的谱线随着偏振变化而移动的<em>方向</em>。这可能会降低模拟所需的精度。</p><br/><br/><a href="https://www.lesswrong.com/posts/PdwQYLLp8mmfsAkEC/magnetic-cryo-ftir#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PdwQYLLp8mmfsAkEC/magic-cryo-ftir<guid ispermalink="false"> PdwQYLLp8mmfsAkEC</guid><dc:creator><![CDATA[bhauth]]></dc:creator><pubDate> Wed, 18 Oct 2023 01:59:08 GMT</pubDate> </item><item><title><![CDATA[Hints about where values come from]]></title><description><![CDATA[Published on October 18, 2023 12:07 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:04:33 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:04:33 GMT" user-order="2"><p>一天后写的简介：</p><p>我和斯皮拉库尔讨论了价值观的本质和起源。对话并没有涉及太多单一清晰的线索，但我写得很开心，并希望它能为其他人带来一些有趣的线索。</p><p> （你可能希望在一开始就讨论方法论交流；而是从“扔掉一些东西似乎不错！”开始。）</p><hr><p>你好。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:05:48 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:05:48 GMT" user-order="1"><p>你好！那么，“价值观从何而来？”一些动物行为/人类起源问题可能会渗透到其中，对吗？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:06:10 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:06:10 GMT" user-order="2"><p>我想做一些类似“import立场.radical_philosophical_confusion”的事情，但我想这可能很无聊，而且在我们共同的语言中不是一个简短的词。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:06:28 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:06:28 GMT" user-order="1"><p>有帖子吗？或者如果没有的话：大约 5 句话的版本是什么？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:06:51 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:06:51 GMT" user-order="2"><p>这里有一篇文章： <a href="https://tsvibt.blogspot.com/2023/09/a-hermeneutic-net-for-agency.html">https://tsvibt.blogspot.com/2023/09/a-hermeneutic-net-for-agency.html</a><br><br>但就像我的许多帖子一样，它写得不好，因为它是最简单的帖子，根本无法表达想法。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:08:08 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:08:08 GMT" user-order="2"><p>一个简短的版本是：我的很多兴趣在于重新编程我们在思考/谈论价值观时带来的一堆相关想法。我想重新编程它们（我的意思是在我的脑海中），以便我可以考虑对齐。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:08:25 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:08:25 GMT" user-order="1"><p>第一句话就立刻被“解释学网”迷住了，是的。无论如何，在尝试解决它......诸如圣经解释个人维基之类的事情？或者我完全偏离了错误的轨道？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:09:12 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:09:12 GMT" user-order="2"><p>那里的“解释学”只是说，就像，我想在所有不同的具体例子和具体的明确概念之间来回跳动，以及对我们的概念和我们对例子的理解所施加的标准，其中标准来自大局。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:09:59 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:09:59 GMT" user-order="2"><p> （我想我在这个元级别上说话感到不舒服，很大程度上是因为我认为你不感兴趣，即使你没有这么说。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:10:03 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:10:03 GMT" user-order="1"><p>好吧，所以“交联”是的，不再强调圣经部分，加强“对话”/解释差异部分，保留“等级制度进入问题”的事情可能......</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:11:27 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:11:27 GMT" user-order="1"><p>呃......我感觉还不错，但我意识到我正在尝试做某种“看看我是否可以短路到你正在打手势的复杂事物的印象”，这可能行不通/可能真的从根本上出发，并做一些奇怪的社交活动来做到这一点。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:11:33 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:11:33 GMT" user-order="1"><p>我们可以退出。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:11:52 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:11:52 GMT" user-order="2"><p>我认为走捷径是合理的。就像，我实际上并不认为我想要表示的东西那么复杂或不常见理解，我只是希望能够明确地调用它。反正。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:12:03 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:12:03 GMT" user-order="2"><p>所以。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:13:57 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:13:57 GMT" user-order="1"><p>看来我们应该尝试循环回到值。你想打包你要尝试使用它来构建的点，还是我应该扔掉一些东西？ （这可能有脱轨或丢失的风险，或者可能会循环回来，谁知道呢！）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:14:13 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:14:13 GMT" user-order="2"><p>扔掉一些东西似乎也不错！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:20:42 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:20:42 GMT" user-order="1"><p>好吧！所以，有点脱离我的头脑......</p><ul><li>价值观的逻辑一致性对很多人来说似乎非常重要，但大多数人似乎都会使用某种停止或理智/直觉检查（斯科特在某些帖子中对此做了手势；当你让步给哲学家）。我想知道为什么它会激活？</li><li>很多价值可能在某种进化上有利的指标（或代理指标！有时代理指标带有享乐，例如：性与生育）中触底反弹，至少作为最初的起点。</li><li>关于评估堆栈概念概括“顶部”的事物与“底部”的接近享乐跟踪器的事物的模糊问题？或者世界建模/真相发现部分的收敛属性，当我想到它时，这是一种奇怪的导出值的方式。或者甚至在 evo 事件落在代理上时放弃“代理”的激进立场（几乎没有人认真对待）。</li></ul><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> 气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:28:21 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:28:21 GMT" user-order="2"><p> （我注意到我想说的是抽象的东西，而不是从例子开始，这很悲伤，但无论如何我都会这样做，所以这些东西可以被掩盖，直到我们用具体的例子回到它......[之后编辑：有关值的一些示例，请参阅<a href="https://tsvibt.blogspot.com/2023/08/human-wanting.html">https://tsvibt.blogspot.com/2023/08/</a> human-wanting.html 、 <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html#2-built-in-behavior-determiners">https://tsvibt.blogspot.com/2022/11/do-</a> humans-derive-values- <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html#2-built-in-behavior-determiners">from-fictitious.html#2-built-in-behavior-determiners</a> ，https: <a href="https://tsvibt.blogspot.com/2022/10/counting-down-vs-counting-up-coherence.html">//tsvibt.blogspot.com/2022/10/counting-down-vs-counting-up-coherence.html</a> ， <a href="https://tsvibt.blogspot.com/2022/08/control.html">https://tsvibt。 blogspot.com/2022/08/control.html</a> ]）<br><br>那么，为什么要问价值观从何而来呢？我真的想知道价值观在头脑中的形态。我想知道这一点，因为我想创造一个具有奇怪价值观的头脑。即，可修正性。或者更确切地说，某种形式的[可能的可校正性解决方案，如下所述：https: <a href="https://arbital.com/p/hard_corrigibility/">//arbital.com/p/hard_corrigibility/</a> （更可靠：https: <a href="https://archive.ph/dJDqR">//archive.ph/dJDqR</a> ）]。<br><br>这些想法的一些可能的词：</p><ul><li>分裂主义推理。 “我不是一个独立的代理人。我是代理人的一部分。我的价值观分布在我的整个自我中，其中包括人类的东西。”</li><li>悲剧性的机构。 “我的推理/价值观是有缺陷的。我是古德哈廷，即使我认为我正在应用我的最终标准。我施加的优化压力指向了错误的事情。这延伸到元级别：当我我认为我正在纠正我的推理/价值观，但我用来判断纠正的标准也是有缺陷的。”</li><li>忠诚的机构。 “我是另一个特工的延伸/代表。我所做的一切，我都将其解释为另一个机构（人道）试图做一些我不理解的事情。”</li><li>彻底的尊重。 “我尊重展现价值观的人道过程。我尊重人道过程在任何抽象层面上编辑我的冲动。我相信这种判断过程高于我自己的判断过程，就像那些普通的正常代理人如何相信他们的[未来的自我，如果是通过“使我成为现在的我的过程”]超越他们现在的自我而得出的。</li></ul><p>这些都深深地涉及到价值观，但我不知道如何让这些高层次的直觉变得更精确。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:29:24 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:29:24 GMT" user-order="1"><p>凉爽的。我认为这与我表示感兴趣的健全性检查与逻辑一致性的事情融合得很好，让我们继续你更发达的词汇/发音。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:48:23 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:48:23 GMT" user-order="2"><blockquote><p>很多价值可能在某种进化上有利的指标（或代理指标！有时代理指标带有享乐，例如：性与生育）中触底反弹，至少作为最初的起点。</p></blockquote><p>作为一条线索，我们可以继续下去：对我来说，这“作为最初的起点”暗示了这里的一个关键问题。我们有这些起点，但是我们要去别的地方吗？我们该怎么做呢？<br><br>一个建议是：我们将自己（我们过去的行为，我们思想的内容）解释为更强大的代理人<i>做某事的</i>有缺陷的尝试，然后我们采用该<i>某事</i>作为我们的目标。 <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html">https://tsvibt.blogspot.com/2022/11/do- humans-derive-values-from-fictitious.html</a><br><br>一般来说，我们不是从明确的值开始，而是创建它们。 （有时我们会做一些不同的事情，最好的描述是发现价值观——例如发现自童年以来被压抑的欲望。有时发现和创造是模棱两可的。但我认为我们有时做的事情只能非常勉强地描述为发现，并且是相反，自由创作。）</p><p>这种创造暗示了某种其他类型的价值，“元价值”或“过程价值”。这些元价值感觉更接近于[这些关于可修正性的想法中出现的那种价值]。所以他们很有趣。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:49:03 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:49:03 GMT" user-order="1"><p>我认为分裂主义会出错，除非它具有“不干涉”或“早期（不妥协）时间步骤 X 认可的（人类/检查者）代理”或其他东西的价值。<br><br>我猜人类设法拥有多个相互交织的子系统，并且通常不会覆盖彼此的运作能力，但是？ （也许除了……在药物确实干扰另一个价值做出可靠行动出价或类似行为的能力的情况下，或者在抑制系统被关闭的情况下）</p><hr><p> “Anapartistic”可能更接近于它在人类子系统上的实施方式（非常低的置信度），但“Tragic Agency”感觉更像是人们通过明确检查他们的道德推理来推理的方式。<br><br>试图……建立他们的道德体系，但又不偏离得太远？通常通过“健全性检查”，定期停止并运行示例，看看它是否给出了疯狂/痛苦/不可取或不可逆转/激进的政策建议，并试图诊断这些建议的上游是什么道德推理步骤？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:56:07 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:56:07 GMT" user-order="1"><blockquote><p> “我们有了这些起点，但接下来我们要去别的地方？我们该怎么做？”</p></blockquote><p>我想我只是给出了一半的答案，但让我进一步分解一下：一个过程看起来像是“建立在”预先建立的已知基本价值观、道德推论、示例之上，并通过推理（聚合共性） ，或下一个逻辑步骤），提出新的推论。然后检查它如何改变整体的政策建议输出，并且......（哦！）标记它以进一步检查它是否会导致前一个时间步骤的大规模政策改变？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:00:43 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:00:43 GMT" user-order="1"><blockquote><p>我们将自己（我们过去的行为，我们思想的内容）解释为更强大的代理人<i>做某事的</i>有缺陷的尝试，然后我们将这<i>件事</i>作为我们的目标。</p></blockquote><p>好的，这是 Loyal Agency 的例子。我想这是利用了人类同理心系统的能力，对吧？<br><br> （我<i>不</i>知道如何在非进化的基质上实现这一点，但我猜在人类中，它是进化压力进展的下游（按时间顺序从头到尾）“建模捕食者/猎物”->;“建模同种动物” ” ->; “为盟友建模” ->; “在共同目标下抽象地协调一致”？）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:03:31 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:03:31 GMT" user-order="2"><blockquote><p>关于评估堆栈概念概括“顶部”的事物与“底部”的接近享乐跟踪器的事物的模糊问题？</p></blockquote><p>强调价值创造/价值选择的观点：没有顶部。或者换句话说：顶部只是隐含地存在。它是由[我们用来指导我们编织连贯值的任何元值/过程]所指向、确定或期望的。<br><br>正如您所讨论的，其中很多都不是真正的值，因为它不是自由参数。我们可以注意到逻辑上的不一致。例如，我们可能会说：“杀死婴儿是不好的，因为他们有意识”和“堕胎是可以的，因为他们没有意识”，并注意到这些放在一起并没有真正的意义（尽管得出的结论是正确的）。然后我们受到逻辑的引导/约束：33周大的胎儿要么有意识，要么没有意识，所以我们必须让我们所有的多重价值观与其中一个世界相吻合，或者让我们所有的多重价值观与另一个世界相吻合那些世界。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:11:52 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:11:52 GMT" user-order="1"><blockquote><p>一般来说，我们不是从明确的值开始，而是创建它们。 （有时我们会做一些不同的事情，最好的描述是发现价值观——例如发现自童年以来被压抑的欲望。有时发现和创造是模棱两可的。但我认为我们有时做的事情只能非常勉强地描述为发现，并且是相反，自由创作。）</p></blockquote><p>这对我来说感觉“不对劲”，而且还没有完全落地。<br><br>就像……你从一个做事的婴儿开始。在某种程度的复杂性上，你开始在“价值”的概念下，对那些持续驱动你部分行为的事物进行自我模型的划分？<br><br>我有一种感觉，仅仅尝试...上传一个新值作为免费创作通常是行不通的，除非它与预先存在的模式相关联...嗯。不，好吧，人们可以更新他们的自我意识，然后有时会做一些疯狂的事情来使他们的行为与自我意识保持一致。但我认为我认为这一点服从于“自我一致性”的价值观和避免强烈的认知失调，而且我可能认为这些价值观在实施中往往以某种方式更宽松地“浅层”。 （不过，我完全不相信这就是它真正的工作原理）<br><br>在玩了一段时间后，不太相信它完全“关闭”了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:31:29 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:31:29 GMT" user-order="2"><p>我感觉有点不靠谱……也许我想举一些例子。</p><ul><li>交个朋友。首先，我们认识到某种重叠。随着事情的发展，我们在某种程度上“跟随我们的鼻子”关于我们可以在一起的事情。对于在一起会是什么样子，没有先发制人的明确想法。这并不是说有什么秘密想法会被揭露。虽然......也许就像，所有这些东西都是一个优化过程，只是解决了仪器细节。但是，IDK，这似乎不对。</li><li>制作艺术。嗯......我有点想说“看！这是关于创造”，然后挥舞我的手。</li><li>类似于艺术创作：烹饪。这与“实际上，你正在为一些预先存在的固定目标计算工具性策略”以及“你很聪明，就像一个吸毒者”非常混合和模糊。但是，当一位熟练的厨师将两种已知的美味做出令人惊讶的组合时……这看起来并不像古德哈廷，因为……嗯。事实上，我完全可以想象一些我认为古德哈廷的口味组合。</li><li>也许我想说纯粹的好奇心和纯粹的玩耍就是典型的例子。你正试图在阳光下创造一些新的东西。我们可以说：这不是价值的创造，而是理解的创造。有一个固定的价值观：我想创造理解。但这使用的“价值”概念比我们通常意义上的更受限制。如果有人喜欢下棋，我们通常将其称为值。如果我们想说“这仅仅是创造理解的元价值的输出”或者“这只是一种工具策略，创建一个玩具模型来创造理解”，那么我们会接受什么作为真正的价值？ [关于“我们会接受什么”的讨论完全是一个红色警报，我已经陷入了糟糕的未降级哲学，但我断言这里无论如何都有一些东西，即使我还没有清楚地了解它。]</li></ul><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:32:35 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:32:35 GMT" user-order="1"><p>烹饪似乎是一个很好的澄清例子……忠诚机构？……实际上。<br><br>你对你要做什么有一些概念，并且知道你会以某种方式搞砸它，而且你对它的解释会受到你现有原料的“环境”的影响（以及你自己作为厨师的不足） ，也可能是在你制作时发生的“灵感来袭”的情况）。<br><br>但是，除非你非常倾向于烹饪艺术（除了厨房水槽炒菜之外的一切都以这种哲学而闻名），否则你在开始掌握的内容时可能确实有一些模糊的、有缺陷的概念向。<br><br> （我听说烹饪中有一些从烘烤到炒的合法/混乱轴）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:38:51 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:38:51 GMT" user-order="2"><blockquote><p>好吧，人们可以更新他们的自我意识，然后有时会做一些疯狂的事情来使他们的行为与自我意识保持一致。</p></blockquote><p>就像出生在内群体中的人一样，他随后了解了外群体倾向于说而内群体倾向于不说的一些信息，并开始同情外群体并寻求更多此类信息，而死亡则螺旋式地成为外群体主义者。<br><br>这里有东西引起了我的注意。一方面，我们要“触底反弹”。我们希望获得真正的核心价值观。另一方面：</p><ol><li>我们的一些“仅是从属的、仅是次目标、仅是工具性的、仅是对象层面的、仅是过程的产物、仅是起点反射”的价值观本身就是元价值观。</li><li>我不知道真正的价值观是什么样的。 （诉诸 VNM 实用函数有一些重要的内容要说，但我认为这不是答案。）</li><li>从这个意义上讲，可能不存在触底价值。</li><li>我们被创造出来就已经在运动了。当我们试图判断、重新评估“客体/从属”价值时，我们所呼吁的，也许只是由更多的“客体”价值构成。运动中产生的是“反射”，它选择我们在由我们称为值的所有自由参数描述的空间中进行的调整。</li></ol><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:43:02 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:43:02 GMT" user-order="1"><blockquote><p>如果有人喜欢下棋，我们通常将其称为值。</p></blockquote><p>哦！我想区分以下两种下棋的人：</p><ul><li>爱丽丝（Alice）是一位狂热且痴迷的国际象棋棋手，只是国际象棋（如果我完成这个比喻，可能会加入某种竞争性联盟，具有相当高的 ELO 评级）</li><li> Bob，他将 5-10% 的时间花在以下一项上：数独、国际象棋、俄罗斯方块</li></ul><p>...我将其描述为具有非常不同的潜在价值观，推动他们对国际象棋的积极价值分配？</p><p>就像，假设这对他们双方来说都是一笔巨额投资，我会推断：爱丽丝下棋是因为她高度重视{卓越、完美主义、竞争}，而鲍勃可能看重{谜题、休闲游戏作为休闲，也许是数学}。</p><p>这极大地影响了我对他们每个人的“国际象棋的可行替代方案”的看法；也许爱丽丝可以把她花在国际象棋上的时间换成竞技网球，但鲍勃会发现这完全不能满足他玩游戏的动机。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:44:13 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:44:13 GMT" user-order="2"><blockquote><p>在你开始掌握的东西时，你可能确实有一些模糊的、有缺陷的概念。</p></blockquote><p>诺拉·阿曼 (Nora Ammann) 举了克莱尔 (Claire) 的例子，她喜欢爵士乐。我的解释是：起初克莱尔听爵士乐有一些不太深刻的理由。也许一个朋友对此感兴趣，或者她认为她应该探索文化，或者其他什么。起初她不太喜欢这样；很难理解，也很难听；但其中也有一些有趣的火花吸引着她。她听得更多，对习语和假设的掌握更加流畅，并开始根据新发现的品味指导她的听力。一段时间后，她现在处于一种真正珍视爵士乐的状态，而且是为了爵士乐本身。它让她瞥见有趣的思维方式，它振奋了她的精神，它与她分享忧郁。这些看起来像是真正的价值观，而且说这些价值观“一开始就存在”而不是作为一个指针似乎是不正确的……好吧，但我再次猜测自己；其中很多都可以被描述为隐藏的渴望找到了出路？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:45:48 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:45:48 GMT" user-order="2"><p>我们正在叫停；感谢您的参与！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT</section></section><br/><br/> <a href="https://www.lesswrong.com/posts/fijSRFL6Z5pXBbCgi/hints-about-where-values-come-from#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fijSRFL6Z5pXBbCgi/hints-about-where-values-come-from<guid ispermalink="false"> fijSRFL6Z5pXBbCgi</guid><dc:creator><![CDATA[Spiracular]]></dc:creator><pubDate> Wed, 18 Oct 2023 00:07:58 GMT</pubDate> </item><item><title><![CDATA[Labs should be explicit about why they are building AGI]]></title><description><![CDATA[Published on October 17, 2023 9:09 PM GMT<br/><br/><p>三个大型人工智能实验室表示，他们关心一致性，并认为失调的人工智能对人类构成潜在的生存威胁。这些实验室继续尝试构建 AGI。我认为这是一个非常糟糕的主意。</p><p>大型实验室的领导者很清楚，他们不知道如何构建安全、一致的通用人工智能。目前最好的计划是将问题交给（不同的）人工智能， <span class="footnote-reference" role="doc-noteref" id="fnref3xloy7ni78g"><sup><a href="#fn3xloy7ni78g">[1]</a></sup></span>并希望能够解决它。当你不知道如何控制 AGI 时，尝试构建 AGI 显然是一个坏主意，特别是如果你承认 AGI 失准可能导致灭绝。</p><p>但有一些原因使得尝试构建 AGI 成为一件更合理的事情，例如：</p><ul><li>他们想首先构建 AGI，因为他们认为这比安全性较低的实验室构建它要好</li><li>他们担心多极格局</li><li>他们担心来自其他国家的竞争，特别是来自中国的竞争</li><li>他们认为人们需要能够使用大型模型才能对齐更大的模型，<i>并且</i>还有其他一些因素意味着我们很快就会有需要对齐的更大模型</li></ul><p>我认为实验室应该明确表示他们正在尝试构建 AGI <span class="footnote-reference" role="doc-noteref" id="fnrefbzo8kc4z7wc"><sup><a href="#fnbzo8kc4z7wc">[2]</a></sup></span> ，并且这并不安全，但有特定的原因使他们认为这是最好的行动方案。如果这些具体原因不再成立，那么他们将停止扩展或尝试构建 AGI。他们应该清楚这些原因是什么。实验室应该向公众和政策制定者明确说明这一点。</p><p>我想要这样的声明：</p><p><i>我们正在尝试建立通用人工智能，这是非常危险的，可能会导致人类灭绝。我们这样做是因为我们所处的具体情况。</i> <span class="footnote-reference" role="doc-noteref" id="fnrefnuickx0l9lh"><sup><a href="#fnnuickx0l9lh">[3]</a></sup></span><i>我们希望我们不必这样做，但考虑到世界的现状，我们觉得我们必须这样做，而且这样做会减少人类灭绝的机会。如果我们不处于这种特定情况，那么我们将停止尝试构建通用人工智能。如果我们注意到[对世界的具体、可验证的观察]，那么我们会强烈考虑停止构建 AGI 的尝试。</i></p><p>如果没有这样的声明，我认为如果其他人认为他们正在鲁莽地尝试构建通用人工智能，实验室不应该感到惊讶。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn3xloy7ni78g"> <span class="footnote-back-link"><sup><strong><a href="#fnref3xloy7ni78g">^</a></strong></sup></span><div class="footnote-content"><p>要么是自动对齐研究人员，要么是与可扩展监督有关的事情</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbzo8kc4z7wc"><span class="footnote-back-link"><sup><strong><a href="#fnrefbzo8kc4z7wc">^</a></strong></sup></span><div class="footnote-content"><p>或者将人工智能系统扩展到未知的安全水平</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnuickx0l9lh"><span class="footnote-back-link"><sup><strong><a href="#fnrefnuickx0l9lh">^</a></strong></sup></span><div class="footnote-content"><p>重要的是，他们要具体说明是什么情况迫使他们构建 AGI。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi<guid ispermalink="false"> 6HEYbsqk35butCYTe</guid><dc:creator><![CDATA[peterbarnett]]></dc:creator><pubDate> Tue, 17 Oct 2023 21:09:20 GMT</pubDate> </item><item><title><![CDATA[Eleuther releases Llemma: An Open Language Model For Mathematics]]></title><description><![CDATA[Published on October 17, 2023 8:03 PM GMT<br/><br/><blockquote><p>今天我们发布了<i>Llemma</i> ：70 亿和 340 亿参数的数学语言模型。 Llemma 模型使用 Code Llama 权重进行初始化，然后在 Proof-Pile II（一个包含 550 亿个数学和科学文档的令牌数据集）上进行训练。由此产生的模型显示出改进的数学能力，并且可以通过提示或额外的微调来适应各种任务。</p><p>我们的工作与<a href="https://blog.research.google/2022/06/minerva-solving-quantitative-reasoning.html">Minerva</a>类似，Minerva 是谷歌研究院去年开发的专门用于定量推理的模型套件。虽然我们没有达到与 Minerva 完全相同的规模，但我们的 Llemma 模型在等参数基础上表现更好。此外，我们使我们的<a href="https://huggingface.co/EleutherAI">模型</a>和<a href="https://huggingface.co/EleutherAI">数据集</a>开放访问，并使我们的<a href="https://github.com/EleutherAI/math-lm">代码</a>开源。</p><p>具有强大数学推理能力的语言模型是奖励建模、算法推理、形式数学等众多新兴研究领域的上游。我们希望通过为研究人员提供更强大的推理应用基础模型，Llemma 将加速解决这些问题的进展。</p><p> Proof-Pile-2 的代码子集为 Llemma 赋予了 Minerva 所缺乏的功能，而无需进行额外的微调。在这篇博文中，我们将讨论<i>形式定理证明</i>。我们的论文包含有关 Python 辅助问题解决任务的其他结果。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for<guid ispermalink="false"> HvgfNjihcDjCTE</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Tue, 17 Oct 2023 20:03:46 GMT</pubDate> </item><item><title><![CDATA[Investigating the learning coefficient of modular addition: hackathon project]]></title><description><![CDATA[Published on October 17, 2023 7:51 PM GMT<br/><br/><p>作为我们在<a href="https://devinterp.com/events/2023-q3-melbourne-hackathon">墨尔本黑客马拉松</a>上关于<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>和对齐（10 月 7-8 日）的项目，我们做了一些实验来估计盆地单层<a href="https://www.neelnanda.io/mechanistic-interpretability/modular-addition-walkthrough">模加任务</a>的<i>学习系数</i>，这是衡量信息复杂性的不变量（阅读：完全训练的神经网络的程序长度）。</p><p>我们以<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau、Murfet 和 Wei</a>最近的论文为出发点；本文估计提供了学习系数的随机估计（它们表示<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>）通过朗之万动力学。 <a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">Watanabe 在一篇精彩的论文</a>中证明，由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量的热力学量渐近收敛于理想化奇异系统的学习系数。</p><p>所描述的实验的所有代码都可以在<a href="https://github.com/nrimsky/devinterp"><i>此 GitHub 存储库</i></a><i>中找到</i><i>。</i></p><h1>简要结果</h1><p>在我们的测试中，我们惊喜地发现，对于模加模素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的任务，我们实施 Lau 等人的 SGLD 方法的输出是（达到大致恒定的小乘法误差，小于因子 2）与理想化单层模加法网络<span class="footnote-reference" role="doc-noteref" id="fnrefx9z3e6c3n7"><sup><a href="#fnx9z3e6c3n7">[1]</a></sup></span>的理论预测结果完全一致。</p><p> <a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人迄今为止已经在双神经元网络中获得了类似的结果。</a> <a href="https://arxiv.org/abs/2310.06301">Chen 等人的 12 神经元网络。</a>我们的结果首次证实了中型网络（约 500 至 8,000 个神经元）的估计值与理论结果之间的一致性。</p><p>虽然我们的结果与单个模加电路的理论值相差很小的乘法因子，但我们发现了一个非常精确的现象，与理论预测完全匹配，即对于泛化的模加网络，学习系数估计在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中是线性的;这是同类中第一个精确的缩放结果。 </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>对于 5 个不同的 p 值，模加网络上<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的平均测量：线性程度非常显着。</figcaption></figure><p>此外，使用模加法任务作为测试用例可以让我们仔细研究<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>复杂度估计在神经网络中区分泛化和记忆的能力：这似乎是新的东西（尽管与一些相变现象有关）陈等人）。我们观察到，虽然泛化在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有线性学习系数，但记忆在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有（大致）二次增长；这再次显示出与理论的显着一致性。 </p><figure class="image image_resized" style="width:70.45%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7"><figcaption>我们针对仅记忆网络的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量。这些与理论预测值非常一致（此处，理论预测<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda = 0.8 p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></figcaption></figure><p>与理论的一致适用于质数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的多个不同值和多个架构。它们对于学习不同数量电路的网络也有适当的行为；其他有效维度估计量（例如 Hessian 特征值估计）往往会高估复杂性的情况。</p><p>此外，我们表明<i>动态</i><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计<span class="footnote-reference" role="doc-noteref" id="fnrefy802tk8mheb"><sup><a href="#fny802tk8mheb">[2]</a></sup></span> ，即训练期间的估计，似乎跟踪学习的记忆与泛化阶段（尽管事实是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计仅取决于训练数据）。为了看到这一点，我们对动态估计器进行了轻微的改进，我们将采样限制在初始化时梯度向量的法线超平面内，这似乎使这种行为更加鲁棒。 </p><figure class="image image_resized" style="width:73.54%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qgdxysf9q1nbayda4hru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vy7xdw29olifj42mtoqi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lh3w1j5xwlc1brqjij3d 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gma5o3xugaqirlq1f1rv 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jieohfzfkyzvh6nw9ppf 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fgves9m6no1m5sjv0ja4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r2vrqdzyqkug7mibbwgo 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uzr5rwe0k0y40bls9ft0 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hzcphrkcu8y27yie4wzw 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。</figcaption></figure><p>我们的动态结果与<a href="https://arxiv.org/abs/2310.06301">Chen 等人的一些 SGLD 研究结果相似。</a> ，这表明动态 SGLD 计算有时可以注意到相变。我们惊喜地看到它们在更大的网络中以及在记忆与泛化的背景下保持不变。</p><p>总的来说，我们的研究结果使我们更加相信奇异学习理论技术和思想在现实世界中的适用性。更具体地说，我们现在相信，类似于 Lau 等人的 SGLD 采样的技术应该能够区分工业规模神经网络中的不同泛化行为，并且可以成为一个有点强大的无监督解释性工具箱的一部分，以及对以下方面有价值的控制技术：结盟。</p><h1>背景</h1><h2>关于学习系数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>的基础知识</h2><p>有关替代介绍，请参阅<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">Jesse 和 Stan 解释学习系数的优秀帖子</a>（在我们写完本节后发布，但遵循类似的方法）。</p><p>学习系数是与泛化相关的参数。它控制“在随机选择权重的情况下，它们产生的损失在最优值的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>范围内的可能性有多大”问题的一阶渐近行为。换句话说，将最优解推广到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>精度范围内有多容易。当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>变为零时，该概率多项式变为零，作为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>的指数，因此</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Prob}_{\text{Loss}<\delta_L} = \delta_L^{d/2}+\text{lower order corrections.}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">概率</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.242em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">损失</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.284em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-stack" style="vertical-align: -0.327em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.084em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">低阶修正。</span></span></span></span></span></span></span></p><p> （出于技术原因，使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>而不是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span> 。）</p><p>这样一个术语（通常用自由能来定义：这里<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>是<i>温度</i>）更普遍地出现在统计物理学中（并且在量子场论中有近亲）作为“微扰膨胀”中的主要指数。在神经网络中，指数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span>被称为<i>学习系数</i>或 RLCT（“实对数规范阈值”，代数几何中的术语）。</p><p>学习系数包含有关学习问题的“类维度”信息，可以理解为<i>有效维度</i>或“真实维度”的度量，即神经网络需要“正确猜测”的权重参数的真实数量。 net以最小的损失解决问题。特别是，如果通过包含不影响可学习算法集的冗余参数来扩展神经网络（例如，由于问题的对称性），则可以证明学习系数不会改变。请注意，如果机器学习问题的解决方案集足够单一（我们在本文中不会遇到这种情况），则学习系数可以大于最小值集<span class="footnote-reference" role="doc-noteref" id="fnref1xv13wjmuli"><sup><a href="#fn1xv13wjmuli">[3]</a></sup></span>的实际维度，并且确实可以是非整数。</p><h2> Watanabe-Lau-Murfet-Wei 估计， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{\lambda}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span></h2><p>事实上，定义为真正渐近线的学习系数仅包含<strong>奇异</strong>网络、现实生活中从未出现的理想化系统的重要信息（就像噪声算法的两次迭代不可能给出完全相同的答案一样，所以它对于具有任何随机性的网络来说，都不可能具有奇异最小值或正维最小值集合）。然而，在有限但较小的温度值（即，损失“灵敏度”，由上述<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>测量）时，计算相关自由能的问题（并因此在“有限粒度级别”获得有意义的泛化相关参数） ”）是容易处理的。</p><p> Lau 等人<a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">渡边的论文</a>。下面给出了这种类型的公式。该论文的结果不仅取决于损耗灵敏度参数（称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span></span></span></span></span> ，来自统计物理文献中的逆温度），还取决于样本数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。该公式给出了神经网络在“真实”数据分布上的学习系数的渐近精确估计，对应于样本数 n 趋于无穷大时的极限。当 n 趋于无穷大时，Watanabe 将温度参数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>归零为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\log(n)/n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。 Lau 等人的论文着手在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>的有限值下执行此测量。</p><p>拥有一个好的学习系数估计器对于可解释性来说非常有价值：这将是一个以非常有原则的方式捕获算法的信息论复杂性的参数，避免了先前已知方法的严重缺陷（例如 Hessian 简并性的估计） ）并且对于分布外检测很有用。更一般地说，<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>计划提出了某些强大的无监督可解释性工具，可以提供有关网络内部结构的信息，假设可以有效地计算学习系数（和某些相关量）。</p><h2>模加法作为估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ 的</span></span></span></span></span></span></span>测试平台</h2><p>在 Lau 等人的论文中，他们将基于 SGLD 的学习系数估计应用于微型双神经元网络和 MNIST 网络，并取得了可喜的结果。我们将模加法网络视为一个有趣的中间案例。模加法必须证明以下事实：</p><ul><li>它是一个机械解释的网络：我们知道它的电路，或多或少知道它们是如何由神经元实现的，以及如何隔离和测量它们。</li><li>我们可以清楚地区分学习泛化的网络和仅通过查看其电路来记忆的网络；此外，我们可以通过创建一个学习随机交换运算的网络来“欺骗”泛化；这是一个与模加法具有相同记忆行为的网络，但没有泛化的可能性。</li><li>此外，我们可以计算网络学习的泛化电路的数量，并推理不同电路在损失函数和某种程度上理想化的自由能计算中如何相互作用。这使我们能够将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>相对于电路数量的行为与其他复杂性概念（例如 Hessian 秩）进行比较。 </li></ul><figure class="image image_resized" style="width:56.81%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bxrvulcijbvpfbwqq8nc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/yfj2uoxea9fget94o1bp 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/pmqnsdp83vlwisdqrts5 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5lxabqho3gjqhl0t19p 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtqbn1vaxa4usvxpvjzt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/orlxoh7x5gw0untqhvb4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xhqap8twezvsfobaw6ol 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e4h9vlg6b3z4ovs90ldw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gihdgdfa0bt0mekv7jjl 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwgypqjwa8dxmqfinby7 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xatuu74shphxc5fouhqw 800w"><figcaption>受模加法训练的网络的保留测试集上的损失图。每个步骤（不同颜色的线）对应于训练过程中均匀分布的检查点。当嵌入权重矩阵中除此之外的所有傅里叶模式都被消融时，每个点对应于损失。仅保留单个重要模式对损失的影响比仅保留不重要模式要小得多，这证明了在嵌入矩阵中使用“grokked”傅里叶模式。</figcaption></figure><p>同时，作为一个算法生成的问题，从 SLT 的角度来看，模加法有一些重要的局限性，这使得它无法捕获典型学习问题的一些复杂性：</p><ul><li>模加法的可能数据点总数是有限的（即，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>素数模而言，等于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ），并且目标分布是确定性的。因此，学习系数仅取决于有限数量的样本，这使得从统计学习理论的角度来看，渐近问题略有（但不是完全）退化。</li><li>即使在简单的确定性机器学习问题中，模加法问题也是高度对称的；因此，我们的经验结果可能无法推广到不太对称的网络。</li><li>与最大样本数相比，可能的输出标记数量较多<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">（p 个</span></span></span></span></span></span></span>标记与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2 个</span></span></span></span></span></span></span></span></span>样本相比，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>为模）可能会导致异常行为（Watanabe 的结果假设 logits 数量很小，并且样本数量渐近无限） ）。</li></ul><p>尽管存在这些限制，我们观察到（对于超参数的适当选择）Watanabe-Lau-Murfet-Wei 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>给出的学习系数估计在很大程度上与理论预测一致。此外，这些估计的表现非常一致和稳定，这是我们没有预料到的。</p><h1>发现<strong>&nbsp;</strong></h1><p>我们发现，对于完全训练的网络，使用渡边公式的 SGLD 估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是对于模加法（在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中呈线性，相当独立于总数量）参数）和随机网络（ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中的二次）。此外，它独立于原子电路或“groks”的数量（我们期望在适当的限制情况下，学习系数的情况如此，但有效维度的其他计算则不然）。 </p><figure class="image image_resized" style="width:54.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gkqtx00nee58ax9d8h6r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/v39xlwaw87bgo6362h90 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vzf0xyjpj3qeacyq7w9k 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ipm9sbglftb2pbfcxepy 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zqriirvq41cc476zoq7p 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dbul9hx7ts7zgvennlbb 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sqekzx3knephdqxl9gqu 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k1xur59h3ai0ewydhure 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qw2a27cqsxs4u1dv7sby 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iptxfrv0eclatokb110l 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/odmceomuhkcwtp4bshet 1071w"><figcaption>我们在模加法任务上训练的模型图。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维一热编码数嵌入在 embed_dim 空间中。学习两个独立的线性变换到一个hidden_​​dim空间。然后，这两个向量按元素相加，通过 GELU 激活函数，然后转换回<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维 logits 向量。</figcaption></figure><p>我们还在模加法网络的学习轨迹上的不稳定点上对<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>进行了一些“动态”估计。在这里，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。</p><h2>泛化网络的扩展行为</h2><p>我们在以下网络上运行了 Watanabe-Lau-Murfet-Wei <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计算法，并得到了以下结果。我们绘制了针对每个素数的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值，并在五次实验中取平均值。</p><p>我们发现使用渡边公式的估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是模加法还是随机网络： </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>使用 SGLD 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的图，用于在模加模不同素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>上训练的 MLP。此处，所示的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>是五次独立训练和采样运行的平均值。 SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。我们可以看到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>成线性比例。 </figcaption></figure><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/t9eayflywv3vapupbtmj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ua5hlgflxv0bl3w77thr 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lsguicplrrqohlxsncbl 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k0lxw3ctzrcs9v3vsljl 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uncajiodsmfpwnyshmhy 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vn1u9t45nqetihj6q5li 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kwvgjbjas8jm6qsvezu7 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rkpjklemrinodgskueqp 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a3xzlm5in1woa0x9qwkj 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>对不同大小的 MLP 网络的单次运行进行估计，证明了 RLCT 跨尺度的相似性。 </figcaption></figure><figure class="image image_resized" style="width:59.29%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m3raqy0b7opujni5bnyj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fufnptgi5q1u1dddxyqs 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkmc2tjnisaxx9iv4o5m 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gdspfdd2xo9rtb1gfmf6 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jiatr65txqrzfzq77ere 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r6ajczqfbexgag3wiopq 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bi1vdtqsotjoyu0zqbay 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwjc3g0ljryippa4iosw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/n5ntt4zn8zbfa8qqfrao 640w"><figcaption>这里，不同的运行对应于单独训练的网络系列，证明<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>在具有相同架构的模型之间是一致的，经过训练以在相同数据集和任务上进行收敛。</figcaption></figure><p>我们观察到，在给定的架构中，我们的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计非常接近线性，正如理论上预测的那样。</p><p>原则上，采用这种架构解决模加法的模型的最小有效维数是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> （这将在推导模加网络结果的单独理论文章中详细阐述）。然而，我们观察到经验比例因子非常接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8p = 2\times 4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，是单个电路结果的两倍。对于这个结果的一个可能的解释是，在我们的模型所处的体系中，解决方案的有效空间由执行至少两个简单电路的权重参数组成（我们训练的所有模型都至少学习了 4 个简单电路）。 </p><figure class="image image_resized" style="width:47.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rpnrzizva3nnkmapqsmi" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkefqplmw6n6rvzaf6zq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/abfwqanxihyprp3gismw 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xoi6gyakkr5xmjzwkr3u 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/chmzavciqpkpbw5tqxb0 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jw9pwvardy4hhr4sv7vk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dd1csgtod5ww9efhgvjq 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bzehulggnguktxcyg45d 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wpgopmxspzuqu26bmv9j 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的比例因子接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span>这可能表明 SGLD 搜索过程探索了对应于两个 grokked 电路而不是一个电路的一系列近极小值。请注意，所有经过训练的模型总共训练了 >;2 个电路，并且电路数量各不相同，因此我们仍然发现所学习的独立电路总数具有不变性。</figcaption></figure><p>当开始实验时，我们预计经验值和预测值之间存在超过一个数量级的广泛差异（因为现实模型的非理想性质和我们实验中的限制点）。相对较大且混乱的“现实世界”测量与理想测量之间的这种程度的一致性，以及这里的近线性，绝不是可以保证的，并且使我们相信奇异学习的理论预测有很大程度的改变。理论与现实世界的测量结果非常吻合。</p><p>我们还在各种架构上重复实验，参数数量相差一个相对较大的因子（我们最大的网络比最小网络大3倍多，我们的中间网络渐近是最小网络的两倍）。较大的网络确实具有稍高的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> ，但差异在网络大小上呈次线性缩放，正如我们对真实学习系数的预期一样。</p><p>请注意，我们包含的素数相对较小。虽然我们的架构非常高效<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">并且</span></span></span></span></span></span></span></span></span></span></span></span>对于更大的素数总是能够泛化（接近 100% 的准确度），但我们凭经验观察到，当完全训练的网络非常接近收敛（0损失）。由于计算限制，我们使用相对较大的学习率 (0.01) 进行相对较少的迭代次数。这会导致 50 以上的素数在收敛时损失更严重；我们推测，如果我们使用计算量更大、学习率更小的方法和更多的 SGD 步骤，那么对于更大的素数，近线性行为将继续成立。</p><h2> (in)依赖于电路数量</h2><p>我们训练的网络有时会学习嵌入不同子空间中的不同数量的独立泛化电路（此类电路的存在首先由<a href="https://arxiv.org/abs/2301.05217">Nanda 等人</a>提出）。</p><p>我们可以通过考虑嵌入空间中的大离群傅里叶模式来测量网络学习的电路的数量和类型，或者（更稳健地）通过在“傅里叶模式对齐”二维投影中寻找近乎完美的圆。嵌入空间<span class="footnote-reference" role="doc-noteref" id="fnrefyim8grymza"><sup><a href="#fnyim8grymza">[4]</a></sup></span> ，如下图</p><p><i>（我们计划稍后发表另一篇文章（关于模块化加法的机械解释工具，特别是准确地区分</i><a href="https://arxiv.org/abs/2306.17844"><i>“披萨”和“时钟”电路</i></a><i>），其中将对这些图片进行更多解释。）</i> </p><figure class="image image_resized" style="width:51.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uria5hpsbh4aoqokve6p" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lembcon39vupovktlmno 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtl1wulgwzyefwcknlux 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ndgtvirh22no4fpprlit 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/diptixpqr04aiyoqhffi 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xi7gkcz3grwfnlxhotng 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5la9dj9km731vtg1rds 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rccsyop3ofkqopkkmygm 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vrw9fyefggwqfdq27rtb 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ixxxg8bckqccyaxmxjvg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bn9plxzaug1db0k5wtvt 1000w"><figcaption>通过将学习到的嵌入权重矩阵投影到嵌入空间的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(p-1)/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>二维子空间（对应于嵌入空间中可表示的不同离散傅立叶模式），我们可以看到独立傅立叶模式电路的数量。例如， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=43"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">43</span></span></span></span></span></span></span>的模型已学习 6 个傅里叶模式 - <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4,8,14,15,19,21"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">14</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">15</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">19</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">21</span></span></span></span></span></span></span> 。</figcaption></figure><p>我们在实验中观察到，学习率估计似乎不太依赖于学习的电路数量。例如，对于我们考虑的最大素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p = 53"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">53</span></span></span></span></span></span></span> ，在不同运行中学习的电路数量在 4 到 7 个圆形电路之间变化，而所有网络的学习系数彼此之间大约在 10% 以内。这个结果看似简单，但从理论角度来看却非常有趣且有些令人惊讶。</p><p>例如，当通过 Hessian 特征值测量网络的有效维度时，具有多个电路的网络将具有有效维度 0（因为沿着与任何电路对应的方向都算作泛化）或线性依赖于电路的数量（因为只有当一个方向独立地概括每个电路时才算作概括）。事实上，这些行为在我们的环境中都没有被观察到，这一事实可以由单一学习理论框架来推动。事实上，我们可以将执行每个电路（或者可能是电路的合适的小子集）的权重空间中的子空间视为“近最小值”的奇异流形的单独组件。由于与不同电路相关联的向量空间相对于彼此处于一般位置，因此产生的奇异性是“最小奇异性” <span class="footnote-reference" role="doc-noteref" id="fnref0vw69e6p2rtp"><sup><a href="#fn0vw69e6p2rtp">[5]</a></sup></span> 。这意味着奇点处的 RLCT 等于沿每个单独分量的 RLCT，这可以理解为对观察到的独立结果的解释。然而，我们注意到，尽管其解释性稳健，但当我们放大时，这张图会变得更加复杂，因为多电路网络的损失往往比其各个部分的乘积要好得多。</p><p>我们计划对独立性结果给出另一种解释，涉及交叉熵损失的统计模型，该模型利用模素数乘法的遍历性。我们在此指出，我们期望这种独立性仅在超参数选择的“金发女郎”范围内保持，特别是正则化常数（对应于所学习的电路的大小）。一个简单的统计模型在这里预测至少三个不同的阶段：一个是电路尺寸非常小的（对应于大正则化），我们预计电路的数量会成倍地影响学习率。一种是在大电路尺寸（小正则化）下，学习率估计变得退化，另一种是在中间区域，我们看到的独立结果是有效的。</p><h2>随机操作：记忆与泛化的扩展</h2><p>为了将我们的泛化网络与具有相同架构（仅记忆）的网络进行比较，我们对随机交换运算网络运行了 Watanabe-Lau-Murfet-Wei 算法。</p><p>为了让记忆网络获得良好的损失，我们需要对其进行过参数化，即参数的数量需要高于样本总数的某个适当的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(1)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>倍，在我们的例子中为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 。由于参数数量在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中线性增长，因此仅当 p 值较小时，我们才能收敛到接近零损失。我们注意到，由于像中国剩余定理这样的数论技巧与随机操作网络无关，因此该实验的 p 值不需要是素数。因此，我们对 5 到 40 的倍数进行此实验。由于收敛问题和缩放模式观察，我们最信任 5 到 25 之间的较短值范围内的结果。</p><p>请注意，此范围仅在 23 到 25 之间与我们的素数列表重叠；我们需要使用更大的网络（并且可能需要更好的学习收敛）来获得高于此范围的合理的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>值。对于我们考虑的值范围，与泛化网络的线性线性相比，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有二次缩放模式的更大学习系数。 </p><figure class="image image_resized" style="width:57.82%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c5xikew9mkdlst7oghv9 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/le4z9svtxnjexffsn3si 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fibwlum3sl4elyejywk1 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jsr1wasqnbfrlxydobix 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c4hvxln3kellgpyamxk5 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vhpntzv0hfycuigby5tt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dtfjmajdgm8mkfbrowjx 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kd7ksxbubypsvqdtnczd 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的随机交换运算。 </figcaption></figure><figure class="image image_resized" style="width:57.52%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ea8nruy9slra7rnaoi50" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gjqy14wu9usujdygfosc 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jk0qobc1glrbzb5zmudq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e3fvdzjforrd4bulesyp 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/tziuy8i1qvmog3pkkybt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/cjx7ag430jml3yc07qus 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/f6jtk4r7lbuewww3oypc 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/swvt3nusrdixsfnynvty 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/luryrwfiumhvhzmyycv0 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span>随机交换运算的<span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>与模加结果一起绘制。</figcaption></figure><p>值得注意的是，p = 25 的图表几乎完全（直到恒定偏移量）等于记忆数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.8\cdot p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ；这里 0.8 是用于训练的完整数据集的一部分。我们还生成了 5 的更大倍数（最多 40）的数据。在这里，我们清楚地看到，在相同架构下，记忆网络比泛化网络具有更高的学习率，但当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p > 25"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">25</span></span></span></span></span></span></span>时，二次拟合变得更差。我们相信，如果我们使用更大的网络，我们将恢复更多 p 值的二次拟合。</p><h2>动力学和相变</h2><p>最后，我们在泛化网络的学习过程中对各个检查点的学习系数进行了<i>动态</i>估计。</p><p>在我们的这部分结果中，我们介绍了<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人的方法的一些创新。</a>和<a href="https://arxiv.org/abs/2310.06301">陈等人</a>。 （尽管我们没有实现后一篇论文中的“基于健康”的采样轨迹排序）。具体来说，我们通过温度调整以及在限制正常超平面损失梯度后应用不稳定的 SGLD 实现获得了最佳结果。 </p><figure class="image image_resized" style="width:54.37%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q1sklqenvvtrvmsezcqy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xbdikzy1xefx5d0lk0sm 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/omo9gl1nkeupoo6cfb6h 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m7x46oyonwnbdll9twai 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bjqxhr02dbf6hoqwylxr 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zvbgfmsb6xbp5i7k2t7k 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a6cxughqqoorqstjtzds 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/b7zi8nz9pjgn5btqrwdl 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sdrpizjr58fqumlrni37 640w"><figcaption>在模型训练过程中，在 25 个等距检查点处运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计。 SGLD 搜索经过修改，将搜索方向限制为与初始化时梯度正交的方向。 </figcaption></figure><figure class="image image_resized" style="width:53.69%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vexyifqwkko2uecghzia" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/s04vahbbg7ngmfzzucgi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wgih1szglxj6dkjuijr0 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uscfm3augmtcocwn4ucm 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/o83yc9csdbjypxmuuf5j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hgboz1tsob3q3m8flbzu 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qu9rxcxgkpsupioaz2gk 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/csmwtmkawpbgxxxoalwn 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/heyla9f6gnxi3xvzohdd 640w"><figcaption>使用另一个模型/独立 SGLD 采样重复运行，以检查结果的一致性。</figcaption></figure><p>在这里，我们观察到不稳定的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。 （由于我们的网络非常高效，因此这种情况在训练的早期就发生了。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnx9z3e6c3n7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefx9z3e6c3n7">^</a></strong></sup></span><div class="footnote-content"><p>请注意，<a href="https://paperswithcode.com/paper/quantifying-degeneracy-in-singular-models-via">刘等人。</a>还对具有超过一百万个神经元的大型 MNIST 网络进行<span>\(\hat{\lambda}\\)</span>估计。在这里，他们发现<span>\(\hat{\lambda}\\)</span>的结果值与用于在可预测方向上训练网络的优化方法相关，从而捕获有关盆地的重要信息。但这里无法得到<span>\(\hat{\lambda}\\)</span>的理论值，SGLD算法无法收敛；因此，在这种情况下，该估计预计不会给出学习系数的真实值</p></div></li><li class="footnote-item" role="doc-endnote" id="fny802tk8mheb"><span class="footnote-back-link"><sup><strong><a href="#fnrefy802tk8mheb">^</a></strong></sup></span><div class="footnote-content"><p>请注意，动态<span>\(\hat{\lambda}\\)</span>估计器尝试将专为稳定点（即局部最小值）设计的技术应用于非局部最小值且具有一些不稳定性、采样和遍历性问题的点，甚至通过我们的法线到梯度限制细化。特别是，它们（远远超过稳定点的估计）对超参数敏感。因此，这些不稳定的<span>\(\hat{\lambda}\\)</span>测量目前没有相关的精确理论值，可以被认为是对不稳定点的复杂性估计的临时概括。然而，我们发现，在固定的超参数集合中，这些估计给出了一致的结果，并且在运行过程中看起来很相似，并且我们发现它们包含有关学习过程中损失景观动态的重要信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1xv13wjmuli"> <span class="footnote-back-link"><sup><strong><a href="#fnref1xv13wjmuli">^</a></strong></sup></span><div class="footnote-content"><p>对此的直觉是，非常奇异的损失函数（即，具有许多等于零的高阶导数的函数）与非常大的盆地相关，这些盆地足够大以“适合额外维度的参数”。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyim8grymza"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyim8grymza">^</a></strong></sup></span><div class="footnote-content"><p>与第k个傅立叶模式相关联的嵌入空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\text{embed_dim}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">embed_dim</span></span></span></span></span></span></span></span></span>的二维子空间是由k频率离散傅立叶变换的sin和cos分量所跨越的空间。请注意，这些空间对于不同模式不一定是线性独立的，但对于学习电路的模式是独立的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0vw69e6p2rtp"> <span class="footnote-back-link"><sup><strong><a href="#fnref0vw69e6p2rtp">^</a></strong></sup></span><div class="footnote-content"><p>这是 RLCT 意义上的意思。在代数几何语言中，如果存在平滑解析爆炸<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X\to \mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">使得</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">在</span></span></span></span></span></span></span></span></span>X 上的局部坐标中，f 是坐标函数的平方的乘积，则权重空间 R n 上的函数 f 是最小奇异的。在这种语言中，如果我们有 c 个电路与权重空间中的向量子空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="C_1,\dots, C_c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span></span></span>相关联，则电路的 k 元组上具有最小值的“理想化”函数是该函数</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(w) = \sum_{S\subset \{1,\dots, c\}, |S| = c-k+1} \prod_{i\in S} \text{dist}(w, C_i)^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⊂</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∏</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span> <span class="mjx-mtext MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></p><p>为了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span>运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c-k+1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1 个</span></span></span></span></span></span></span>元素子集，并<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{dist}(w, C_{i_j})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>从权重到相应子空间的 L2 距离。很容易检查所得奇点是否为最小奇异点。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1<guid ispermalink="false"> 4v3hMuKfsGatLXPgt</guid><dc:creator><![CDATA[Nina Rimsky]]></dc:creator><pubDate> Tue, 17 Oct 2023 19:51:34 GMT</pubDate> </item><item><title><![CDATA[When building an organization, there are lots of ways to prevent financial corruption of personnel. But what are the ways to prevent corruption via social status, political power, etc.?]]></title><description><![CDATA[Published on October 17, 2023 6:51 PM GMT<br/><br/><p>如今，即使是相当复杂的会计和审计概念，对于在任何大型公司拥有在职工作经验的人来说也是众所周知的。</p><p>例如控制支出账户、公司卡等资金流动的系统、留下高度清晰的审计跟踪的程序等等。</p><p>在很大程度上，这些方法成功地防止了任何人以过于明显的方式弄乱财务数字，至少在没有留下很多警告信号的情况下是这样。</p><p>然而，在控制那些过度渴望社会地位或政治权力的人方面，方法似乎要少得多。</p><p>通俗文学中讨论的大多只在大规模范围内起作用，比如议会程序、政府多个部门相互制衡等等……</p><p>有哪些行之有效的方法可以在普通组织的规模上发挥作用？</p><p> （比如 100 到 10 000 人）</p><p>如果没有，有哪些理论建议？</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to<guid ispermalink="false"> JBjPreynFn5aDmJsj</guid><dc:creator><![CDATA[M. Y. Zuo]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:51:47 GMT</pubDate> </item><item><title><![CDATA[Eliezer's example on Bayesian statistics is wr... oops!]]></title><description><![CDATA[Published on October 17, 2023 6:38 PM GMT<br/><br/><p>这篇文章<i>将</i>解释 Eliezer Yudkowsky 在统计讨论中经常使用的一个例子实际上并不意味着他认为它的作用。在证明他错误的过程中，我发现我其实错了。我仍在写这篇文章，对于犯与我相同错误的其他人来说，这可能会很有趣。</p><p>这个例子是埃利泽在论证贝叶斯统计优于频率统计时经常使用的例子。在 Sequences <a href="https://www.lesswrong.com/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective">里</a>，在 Arbital<a href="https://arbital.com/p/likelihoods_not_pvalues/?l=4xx">里</a>，在 Glowfic<a href="https://www.glowfic.com/posts/5826">里</a>，还有其他几个我现在找不到的地方。一枚硬币抛六次；前五次出现正面，第六次出现尾部。但我们不知道实验者是否 A) 事先决定抛硬币六次，并报告发生了什么，或者 B) 事先决定将硬币一遍又一遍地翻转，直到反面朝上，并报告花了多长时间。这为我们提供了有关代币偏见或缺乏偏见的哪些信息？</p><p>根据频率论的观点，在这两种不同的情况下分配的 p 值存在严重差异。在第一种情况下，结果 HHHHHT 被放入至少相对于“原假设”极端的结果类别中 - 其中有 14 个：HHHHHH、TTTTTT、HHHHHT、TTTTTH、HHHHTH、TTTTHT 等。总共有 64 种可能性，14/64 = 0.22，远远高于 p=0.05 的水平，因此不足以得出显着性结论。在第二种情况下，结果 HHHHHT 被放入不同的结果类别中：5 个正面，然后是一个尾部，6 个正面，然后一个尾部，7 个正面，然后一个尾部，依此类推。整个类别的概率总计为 1/32，即 0.03 - 具有统计显着性！</p><p>以利以谢从几个方面批评了这一点。首先，频率论者决定将实际结果与一组相似的结果混为一谈的部分是主观的，足以允许对实际结果有很大的自由度。也许在第一种情况下，您不选择具有 5 个或更多同边的结果类别，而是只选择具有 5 个或更多正面的结果类别，从而将 p=0.22 减半至 p=0.11。他还批评了通过“拒绝零假设”来确定显着性的概念，而不是考虑不同的理论效应大小以及它们对数据的预测效果。作为支持完全不同效应大小的证据的两个实验被视为“拒绝原假设”，因此被视为同一理论的证据，即使结果彼此不一致。</p><hr><p>所有这些对频率统计和 p 值的批评似乎都是正确的。但对贝叶斯如何更新的分析是不同的。</p><blockquote><p> ...贝叶斯主义者查看实验结果并说：“我现在可以计算所考虑的所有假设之间的 <a href="http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html">似然比</a>（证据流）。因为你的心态不会以任何方式影响硬币 - 不会改变一枚公平的硬币或有偏差的硬币产生这一精确数据的概率——你私人的、不可观察的心态不可能影响我对你的实验结果的解释。”</p><p>如果您习惯贝叶斯方法，似乎很难想象证据的统计解释应该取决于一个因素，即实验者的心态，而该因素与实验结果没有任何因果关系。 （因为贝叶斯说<a href="http://yudkowsky.net/rational/bayes">证据是关于相关性的</a>，没有因果关系就不可能出现系统相关性；<a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">证据需要纠缠</a>。）</p></blockquote><p>因此，埃利泽认为，两种情况下的似然比显然应该相同，因为唯一相关的数据是抛硬币的顺序。实验者的心态不会改变具有给定偏差的硬币产生该数据的概率，因此它是无关紧要的。</p><p>但埃利泽在这里似乎缺少的关键要素是，数据的总和<i>不是</i>“硬币出现了 HHHHHT”。相反，我们收到的数据是“实验者<i>看到</i>硬币出现了 HHHHHT”。<i>这种</i>证据与实验者的心态有因果关系，因为实验者的心态决定了实验者在什么情况下会看到硬币出现HHHHHT。例如，如果事情的真实情况是硬币确实是公平的，那么实验者遵循“翻转直到出现反面”规则的心态会导致实验者不太可能达到拥有硬币的地步。首先是第六次翻转，因为现在实验者有 31/32 的机会在第六次翻转之前停止。<a href="https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence">证据加上经过过滤的知识通常可以具有与未引用的证据本身不同的属性</a>；实验者决定寻找哪些证据的方法改变了他们可能找到的证据。</p><p>这两种可能性对可能的结果产生<i>非常不同的</i>先验分布。假设硬币的实际偏差使得正面朝上的理论频率为<i>f</i> ：那么在“翻转<i>n</i>次”的情况下，先验分布在长度为<i>n</i>的所有可能序列之间，每个序列的概率为<i>f</i> ^ （序列中的总头数）* (1- <i>f</i> )^（序列中的尾数）。 （在 f = 1/2 且 n = 6 的情况下，这只是简化为 ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{2^6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.819em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.158em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.158em; bottom: -0.919em;"><span class="mjx-msubsup" style=""><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.443em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.819em;" class="mjx-line"></span></span><span style="height: 1.621em; vertical-align: -0.65em;" class="mjx-vsize"></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>）同时，在“翻转直到出现反面”的情况下，先验是 T 的 1- <i>f</i>机会、HT 的<i>f</i> (1- <i>f</i> ) 机会、HHT 的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> (1- <i>f</i> ) 机会、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> (1-f) HHHT 的机会，依此类推，对于总长度为<i>l</i>的序列，始终以<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(l-1)}(1-f)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>结束。这些先验都为不同的结果分配了<i>非常</i>不同的概率 - 事实上，第一个版本允许的许多结果在第二个版本上具有零概率（例如 HTHTHTHT），或者第二个版本允许但在低<i>n</i>版本上具有零概率第一个（如 HHHHHHHHHHHHT 如果<i>n</i> =6。）</p><hr><p>因此，实验过程中发生的概率是非常不同的，具体取决于你是翻转<i>n</i>次还是翻转直到出现反面。这些不同的条件先验是否意味着最终的似然比会有所不同，与埃利泽声称的相反？我也这么想。</p><p>毕竟，贝叶斯更新的工作方式首先是确定在不同假设（ <i>f</i>的可能值）下分配给实验结果的概率，构建这些概率作为 f 的函数的似然分布，并将其乘以您的先前分布以更新它。因此，两种类型的实验条件下的先验分布如此不同，这一事实会导致您乘以的内容不同，从而给出不同的结果。</p><p> （如果您还没有找到我的错误，请随时暂停并在这里查找它。）</p><hr><p>我犯的错误是，我混淆了实验设计的概率分布与<i>实验结果</i>的差异，以及实验将导致您更新的<i>硬币假设的</i>可能性分布的差异。不同的实验设计<i>会</i>导致一些实验结果以不同的频率出现，但这<i>并不自动意味着</i>关于硬币偏差的最终更新会有所不同。</p><p>关于硬币的更新是否不同<i>仅</i>取决于分配<i>给实际发生的结果的</i>概率。如果他们为 HHT 和 HTHTTH 这样的结果分配截然不同的概率，那并不重要，如果实验结果为 HHHHHT 并且他们为其分配相同的概率。事实上，情况确实如此。事实上，虽然实验结果的分布在大多数值上看起来都非常不同，但它们总是会碰巧在实际结果是实验结果的精确值处交叉，通过一些令人惊奇的非巧合。</p><p>发生这种情况的原因是，虽然“翻转直到出现尾部”对于任何给定的<i>f</i>值来说确实是一个常数概率分布，但“翻转<i>n</i>次”也取决于 n 的值，这使得它实际上是<i>n 个</i>独立的分布碰巧彼此相似。如果在“翻转直到得到尾巴”实验中需要 7 次翻转而不是 6 次才能得到尾巴，这并不意味着您突然得到了在任何“翻转<i>n</i>次”分布中不可能得到的结果，它只是意味着您移动到“翻转 7 次”分布而不是“翻转 6 次”，并且该分布上 HHHHHHT 的概率最终将与“翻转直到出现尾部”分布指定的概率匹配。</p><p> （这里的代数很简单。如前所述，“翻转<i>n</i>次”得到给定序列的概率为<i>f</i> ^(序列中总的头数) * (1- <i>f</i> )^(序列中的尾数)序列）。但假设序列中每个值都是头，除了最后一个值是尾，则这会简化为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(n-1)}(1-f)^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> 。这与前面描述的函数相同“翻转直到出现尾巴。”）</p><p>请参阅<a href="https://www.desmos.com/calculator/fkpvlyycod">此处，</a>了解硬币偏差<i>f</i>的似然分布在查看后的实际情况<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <i>n-1</i>个头和 1 个尾，无论使用两个实验设计中的哪一个。</p><hr><p>如果事实证明我在一个简单的事实问题上是错误的，那么我为什么要写这篇文章呢？<i>我</i>确实在这里学到了一些东西，似乎其他人也很困惑，但也能从他们身上学到一些东西。</p><p>第一课是要更加小心地检查概率分布<i>到底</i>告诉你什么！在我最初的计算中，在我开始确定是什么让我困惑之前，我犯了<i>很多</i>错误，其中有几个我什至没有在这里提到（比如最初使用二项式分布来建模，但<i>实际上</i>并不如此）在那里不适用。）大多数错误的本质是“我正在寻找以 x 为条件的某物的概率分布；这里的东西是以 x 为条件的某物的概率分布；因此它是分布我在找。”给定特定类型的实验的实验结果的分布与给定特定结果的观察的假设的似然比之间存在差异；依赖于函数参数之一的分布的任何特定版本与由该参数的所有可能值形成的总体分布类之间存在差异。</p><p>我学到的第二件事是，贝叶斯似然比实际上只取决于每个假设仅分配给您收到的信息的概率，而不是其他任何东西。我以前<i>口头上</i>知道，但没有真正内化。 If two hypotheses assign the same probability to an outcome, and you see that outcome, that tells you nothing about any difference between the hypotheses. If I had ignored trying to quantify over all the possible outcomes, and just asked the comparatively simpler question of &quot;what is the chance of HHHHHT in experiment 1, and in experiment 2,&quot; I probably could have solved it a lot more quickly.</p><p> And then there&#39;s also a <i>possible</i> lesson for me to learn of &quot;see, you really should meta-level trust the reasoning of Eliezer Yudkowsky and other people who have more expertise in a given mathematical domain.&quot; I am not sure this is a good lesson to learn. And I&#39;m also not sure that Eliezer actually saw all of the reasoning I went through in this post about <i>why</i> the two experiments assign the same probabilities to the actual result, rather than just guessing and happening to be correct. That being said, it still is the case that I would have previously given this as an example of a situation in which Eliezer Yudkowsky was wrong about basic probability theory (and I also would have said something like &quot;and he probably made this mistake because of motivated reasoning in order to score points against frequentists&quot;). And he turned out to be right all along. This is more likely in worlds where he knows his stuff more, and I have correspondingly updated my beliefs.</p><p> (I hope this goes without saying, but I&#39;ll say it anyway: a meta-level update towards trusting experts&#39; math, does not mean first-order conforming to their opinions if you don&#39;t first-order agree with or understand them. I&#39;ll still keep trying to notice and point out when it looks like Eliezer is wrong about something - even if I might not bet as strongly that he really does turn out to be wrong.)</p><br/><br/> <a href="https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops<guid ispermalink="false"> BeAHn5CisRgivuspA</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:38:18 GMT</pubDate> </item><item><title><![CDATA[Trying to deconfuse some core AI x-risk problems]]></title><description><![CDATA[Published on October 17, 2023 6:36 PM GMT<br/><br/><p> Max H commented on the <a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1">dialogues announcement</a> , and with both of us being interested in having a conversation where we try to do something like &quot;explore the basic case for AI X-risk, without much of any reference to long-chained existing explanations&quot;. I&#39;ve found conversations like this valuable in the past, so we decided to give it a shot.</p><p> His opening comment was:</p><blockquote><p> I feel like I&#39;ve been getting into the weeds lately, or watching others get into the weeds, on how various recent alignment and capabilities developments affect what the near future will look like, eg how difficult particular known alignment sub-problems are likely to be or what solutions for them might look like, how right various peoples&#39; past predictions and models were, etc.</p><p> And to me, a lot of these results and arguments look mostly irrelevant to the core AI x-risk argument, for which the conclusion is that once you have something actually smarter than humans hanging around, literally everyone drops dead shortly afterwards, unless a lot of things before then have gone right in a complicated way.</p><p> (Some of these developments might have big implications for how things are likely to go before we get to the simultaneous-death point, eg by affecting the likelihood that we screw up <i>earlier</i> and things go off the rails in some less predictable way.)</p><p> But basically everything we&#39;ve recently seen looks like it is about the character of mind-space and the manipulability of minds in the below-human-level region, and this just feels to me like a very interesting distraction most of the time.</p><p> In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead. I also wouldn&#39;t mind recapitulating (my view of) the core AI x-risk argument, though I expect I have fewer novel things to say on that, and the non-novel things I&#39;d say are probably already better said elsewhere by others. </p></blockquote><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 03:48:23 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 03:48:23 GMT" user-order="1"><p> I am quite interested in the &quot;In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead.&quot;部分。</p><p> Do you want to start us off with a quick summary of your take here?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Wed, 11 Oct 2023 04:40:06 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Wed, 11 Oct 2023 04:40:06 GMT" user-order="2"><p><br> So, I think there&#39;s a lot of alignment discussion that is missing a kind of inductive step, where you freely assume that you have a running human-level (or smarter) system, and which is thus dangerous by default. And then you ask, &quot;how did that get aligned?&quot; or &quot;why does that not kill everyone?&quot; instead of what I see as a somewhat backwards approach of looking for an alignment method that applies to current systems and trying to figure out whether it scales.<br><br> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p><p> As for why I think results about below-human-level systems are unlikely to be relevant, it&#39;s because all current AI systems are far less Lawful than humans.</p><p> Rephrasing <a href="https://glowfic.com/replies/1660170#reply-1660170">planecrash</a> (and somewhat violating the original ask to avoid long-chained and unwieldy dependencies, but I&#39;ll try to give a useful summary here), Lawfulness is the idea that there are logical / mathematical truths which are spotlighted by their simplicity and usefulness across a very wide space of possible worlds.</p><p> Examples of heavily spotlighted truths are concepts like logical deduction (Validity), probability theory (Probability), expected utility theory (Utility), and decision theory (Decision).  Systems and processes are important and capable precisely to the degree in which they embody these mathematical truths, regardless of whether you call them &quot;agents&quot; or not. All humans embody shadows and fragments of Law to varying degrees, even if not all humans have an explicit understanding of the concepts in words and math.<br><br> Most current capabilities and alignment techniques are focused on trying to squeeze more fragments of Law into AI systems: predictive ability (Probability), deductive ability (Validity), Utility (doing stuff efficiently without working at cross-purposes), and those systems still coming out far below humans in terms of general Lawfulness.</p><p> At the capabilities level of current systems, squeezing more Law into a system can <i>look</i> like you&#39;re making it more aligned, but that&#39;s because at the current level of capabilities / Lawfulness, you need more Law to do even very basic things like following instructions correctly at all.</p><p> Another problem that a lot of proposed alignment methods have (if they&#39;re not actually capabilities proposals in disguise) is that they postulate that we&#39;ll be able to badger some supposedly smarter-than-human system into acting in an obviously unLawful way ( <a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Deep Deceptiveness</a> is an example of what might go wrong with this).</p><p> Also, if you accept this frame, a whole bunch of confusing questions mostly dissolve. For example, questions like &quot;are humans expected utility maximizers&quot; or &quot;is it a problem that ideal utility maximization is computationally intractable&quot; or &quot;but is EU theory even right&quot; have straightforward answers in this framework:</p><p> It&#39;s OK if EU theory as humans currently understand it isn&#39;t entirely strictly correct; EU theory is probably a shadow or approximation for whatever the true Law is anyway, the way that Newtonian mechanics is an approximation for General relativity at the right scale. (Maybe something like <a href="https://www.lesswrong.com/posts/Xht9swezkGZLAxBrd/geometric-rationality-is-not-vnm-rational">Geometric Rationality</a> is the real Law that a supermajority of capable minds across the multiverse would settle on.)</p><p> Similarly for decision theory, it doesn&#39;t matter if some particular human-understood flavor of logical decision theory is exactly correct; it&#39;s close enough to some true Law of Decision that a supermajority of all minds (including humans, if they think for a few decades longer) will be able to see, spotlighted by its mathematical simplicity / optimality / usefulness across a sufficiently wide slice of possible worlds.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 23:23:44 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 23:23:44 GMT" user-order="1"><blockquote><p> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p></blockquote><p> Yeah, this seems right to me.</p><p> I&#39;ve had a long-standing disagreement with many people currently working in prosaic alignment about the degree to which AI systems are quite alien in their cognition (but are trained on the task of imitating humans, so look on the surface like humans).</p><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 02:29:57 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 02:29:57 GMT" user-order="2"><p> I also think current and future AI cognition is unlikely to be human-like, but I don&#39;t think this is a super important crux about anything for me.</p><p> For example, maybe it turns out that actually, the only way to get human-level or better performance is via some algorithm that looks isomorphic to some process in a human brain somewhere.</p><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p><blockquote><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p></blockquote><p> I think another thing to look at is the process, in addition to performance and final outputs.</p><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.<br><br> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.<br><br> (I like <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded by human capabilities? No.</a> and <a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators">GPTs are Predictors, not Imitators</a> for more on this general topic.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:02:32 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:02:32 GMT" user-order="1"><blockquote><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p></blockquote><p> I think this does actually still matter, because a lot of the current AI Alignment plans people are pursuing boil down to something like &quot;try to create an AI system that is approximately human level smart, and then use that to automate the task of AI Alignment research, or use those systems to coordinate or otherwise achieve an end to the acute risk period&quot;.</p><p> In order for those plans to work, the following things really matter:</p><ul><li> how much time you expect these systems to spend in the human regime,</li><li> to what degree they will have a capability profile that you can use for alignment research, coordination or some other plan that ends the acute risk period</li><li> how good you are at eliciting the latent capabilities of the system, compared to how capable the system itself is at using those capabilities for deception/other dangerous ends</li></ul><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above. If the cognition of current and future AI systems is alien, this makes it more likely that dangerous capabilities will suddenly spike, less likely that we can effectively automate AI alignment research, use the systems for coordination, or leverage the system for some other pivotal act, and less likely that we will be capable of eliciting the capabilities of the system for our end.</p><p> Separately, humans also just have really good intuitions for spotting deception from human-like minds. In as much as the systems engaging in deception will do so using mostly tactics and cognition that are borrowed from humans (by being trained on producing human-produced text), then we have a much better chance at spotting that taking appropriate action.</p><blockquote><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.</p><p> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.</p></blockquote><p> Hmm, somehow this isn&#39;t landing for me. Like, I agree that the process of &quot;produce a probability distribution + sample from it + auto-regres&quot; doesn&#39;t look very human like, but that feels kind of irrelevant to my point.</p><p> What I mean by saying &quot;AIs are trained on the task of imitating humans&quot; is that the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet. Ie the AI is trained on the task of imitating human internet-text production.</p><p> I am not saying this to suggest that this means the AI is trained to think in human ways, since the context humans are in in producing internet text is very different from the context the AI is performing this task in. I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 04:03:13 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 04:03:13 GMT" user-order="2"><blockquote><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above.</p></blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.<br><br> Also, if you think your AI can do alignment research, it should also be able to do pretty much all other kinds of human labor, so maybe just... stop a bit earlier and make sure you really have achieved post-scarcity and perfect global harmony, before moving onto even harder problems. (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p><p> I&#39;m not sure how plausible I find this scenario, nor how optimistic I should be about it if it does come to pass. eg one &quot;early failure mode&quot; I can imagine is that it&#39;s not really AGI that dooms us, it&#39;s just that humans plus below-human-level AI tools start unlocking a bunch of really powerful technologies in a way that goes off the rails before we even get to actual AGI.</p><blockquote><p> the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet.</p></blockquote><p> In most cases, yes, but the point of <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded</a> is that you can get even better loss by being skilled at a lot more things than just human-mimicry. For example, if you want to predict the next tokens in the following prompt:</p><pre> <code>I just made up a random password, memorized it, and hashed it. The SHA-256 sum is: d998a06a8481bff2a47d63fd2960e69a07bc46fcca10d810c44a29854e1cbe51. A plausible guess for what the password was, assuming I&#39;m telling the truth, is:</code></pre><p> The best way to do that is to guess an 8-16 digit string that actually hashes to that. You could find such a string via bruteforce computation, or <a href="https://xkcd.com/538/">actual brute force</a> , or just paying me $5 to tell you the actual password.</p><p> If GPTs trained via SGD never hit on those kinds of strategies no matter how large they are and how much training data you give them, that just means that GPTs alone won&#39;t scale to human-level, since an actual human is capable of coming up with and executing any of those strategies.</p><blockquote><p> I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p></blockquote><p> I mostly agree with this point. Two remarks / alternate hypotheses:</p><ul><li> a lot of the human-like qualities look at least partly illusory to me when you look closely (meaning, the errors and reasoning actually <i>aren&#39;t</i> all that human-like)</li><li> To the degree that they are human-like, a hypothesis for why is that there just aren&#39;t that many ways to be kinda wrong but not totally off-base. What would <i>non</i> -human-like reasoning errors that still produce vaguely human-sounding text even look like?</li></ul><p> Also, when an AI reasons <i>correctly</i> , we don&#39;t call it &quot;being human-like&quot;; that&#39;s just being right. So I sort of feel like the whole human-like / not-human-like distinction isn&#39;t carving reality at its joints very well. In my terms, I&#39;d say that when both humans and AIs reason correctly, they&#39;re alike because they&#39;re both reasoning Lawfully. When they mess up, they&#39;re alike because they&#39;re both reasoning un-Lawfully. The fact that AIs are starting to sound more like humans is explained by the fact that they are getting more Lawful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:18:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:18:40 GMT" user-order="1"><blockquote><p> (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p></blockquote><p> Lol, I genuinely like that proposal/intuition-pump/sanity-check.</p><blockquote><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.</p></blockquote><p> I agree with you on the &quot;doing end-to-end alignment research seems particularly risky&quot; component. I also think that automating alignment research is not the right proxy to aim for with close-to-human-level AI systems, and we should aim for some coordination victory/game-board-flipping plan that somehow prevents further AI progress.</p><blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p></blockquote><p> Yeah, I think that&#39;s plausible, but I think &quot;figuring out how to do science using an alien mind&quot; is actually a pretty hard problem, and it&#39;s going to be much easier to slot a human-like AI into the process of making scientific discoveries.</p><p> Overall, I think the most important reason for why it matters if AIs have human-like cognition is not that it makes it safer way to leverage the AI for things like AI Alignment research. It&#39;s instead that if the trajectory of AI capabilities roughly follows the trajectory of human performance, then we will be much better at predicting when the AI system is getting dangerous. If we can just ask the question &quot;well, would a human of this ability level, with maybe some abnormally high skills in this specific subdomain be able to break out of this box and kill everyone?&quot; and get a roughly accurate answer, then that&#39;s a much easier way of determining whether a given AI system is dangerous, than if we have to ask ourselves &quot;is this alien mind that we don&#39;t really have good intuitions for the kind of thing that could break out of the box and kill everyone?&quot;.</p><p> And conversely, if it is indeed the case that current and future AI system&#39;s internal cognition is alien, and their perceived reasoning performance only follows the human trajectory because they are trained to perform reasoning at human levels (due to being trained on human text), then this will cause us to reliably underestimate the actual abilities and performance of the system on a wide range of task.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:32:10 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:32:10 GMT" user-order="1"><p> Going back up a level of the conversation, I&#39;ve been trying to concretely argue that even though I think you are correct that as we reach Superintelligence, the natural lawfulness of correct reasoning will make almost all more prosaic alignment approaches fail, I think it still really matters how to control systems that are just below and just above the human capability level.</p><p> Or to be more precise, if it turns out to be the case that you could make systems such that their capabilities roughly follow the human distribution of competence as you scale up compute, then I think you have a bunch of options of punting on the hard part of the AI Alignment problem, by using those systems to solve a bunch of easier problems first, and then after you solved those problems, you are in a much better position to solve the rest of the AI Alignment problem (like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem).</p><p> That said, my biggest concern with this kind of plan is that AI performance will not predictably follow the human distribution, and that our current training methods inherently bias researchers towards thinking that AI systems are much more human-like in their cognition than they actually are.</p><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you. That said, I do currently feel like the hope for AI Alignment does still come from some set of plans of the form &quot;how can I use early AGI systems to end the acute risk period somehow&quot;, and so any critique of existing AI Alignment approaches needs to show how an alignment approach fails to achieve that goal, and not how it fails to achieve full alignment, which I think is just very solidly out of our reach at this point.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 14:49:27 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 14:49:27 GMT" user-order="2"><p> I agree with most of what you&#39;ve written in broad strokes, though I think I actually have more uncertainty about how things will play out in the short term.</p><p> I&#39;ll respond to 3 specific things you said which jumped out at me, and then you can decide on the conversational branching factor from there...</p><blockquote><p> they are trained to perform reasoning at human levels (due to being trained on human text),</p></blockquote><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself. There was this whole debate about whether a superintelligence could infer General Relativity from a video of a falling apple - maybe a few frames of an apple video aren&#39;t actually enough even for a superintelligence, but a giant corpus full of physics textbooks and experimental results and detailed descriptions of human behavior is definitely enough to make a lot of inferences beyond current human reach, and also very likely more than sufficient to <i>learn to do inference in general</i> superhumanly well, <i>if</i> you get the training method and architecture right.</p><p> (Also, a lot of the text generation that LLMs can do is already superhuman; I can still write better code than GPT-4 if I get a chance to compile it, run it, refer to documentation, etc. But I can&#39;t write it token by token with no backspace key or even the ability to jump around in my code as I write it, even if you give me a lot of extra time.)</p><blockquote><p> like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem</p></blockquote><p> They better be able to make progress in a lot less than 10,000 years of subjective time! I actually think if you can get even a single high fidelity upload of a smart human and run them at no speed up (or even a slowdown) you&#39;re already in pretty good shape.</p><p> I would spend the first few months of subjective time looking for improvements to the fidelity and efficiency of the simulation from the inside, checking my own mind for bugs and inconsistencies introduced by the upload process, doing some philosophy, introspection, mental inventory, etc.</p><p> And then probably start working on making very small, safe tweaks to my own mind, giving myself some extra tools (eg an instant math module, a larger working memory, an integrated search engine), and then maybe try out some bigger / more invasive changes, eg making myself better at applied rationality via direct brain modifications.</p><p> And then maybe after like, a year or two of subjective time spent tweaking my own mind and improving my understanding of digital mind design and minds in general, I start turning towards work on specific alignment problems and / or go for actual recursive self-improvement. But either way, I wouldn&#39;t expect it to take 100 people anywhere close to 10,000 years, even if digitization is a 100% opaque black box to start out and digital neuroscience as a field turns out to be totally intractable.</p><blockquote><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you.</p></blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p><p> Maybe a better way of putting it is: I expect that, before AIs get totally superintelligent or even definitely past human level, they will be able to pass a human&#39;s Ideological Turing Test about what humans value. (At least, they&#39;ll be able to pass according to the judgement of most humans, though maybe not the most careful / skeptical alignment researchers). Understanding an alien&#39;s values is maybe a bit easier if you share patterns of cognition with them, but <i>not</i> sharing them doesn&#39;t actually push the understanding task into the superintelligence realm of difficulty.</p><p> Also, I think in these plans, the specific level of &quot;human-level&quot; actually starts to matter quite a lot. Maybe you can have a median-human-level AI that is stable and safe to use when unboxed. I&#39;m less sure you can safely have a median-computer-programmer-level AI, or especially a top-1%-computer-programmer-level AI unless you&#39;ve actually solved a bunch of hard alignment problems already.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 18:07:00 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 18:07:00 GMT" user-order="1"><blockquote><p> >; they are trained to perform reasoning at human levels (due to being trained on human text),</p><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself.</p></blockquote><p> Quick clarification here: I meant &quot;perform&quot; in the &quot;an actor performs a role&quot; sense. Ie I was trying to say &quot;the system will look like it is reasoning at roughly human levels, because it was trained to produce text that looks like it was written by humans&quot;.</p><p> That, I am confident, is not the result of the system being trained via inefficient and limiting methods on fundamentally limiting architectures.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 18:19:37 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 18:19:37 GMT" user-order="2"><p> Ah, OK, I initially read it as performing in the sense of performing at a certain capabilities level, but that makes more sense. I agree with you that this is likely to lead to reliable underestimates of true capabilities levels.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 21:31:28 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 21:31:28 GMT" user-order="1"><p> I&#39;ll respond to this section, since I think it&#39;s the one that&#39;s most on the critical path what to do with AI alignment:</p><blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p></blockquote><p> Ok, here I notice that I want to go a lot slower, taboo some words, and figure out what exactly is true here.</p><p> Here are some first reactions that I have when I consider this question:</p><ul><li> Man, I really don&#39;t fully understand honesty myself. Like, in many situations it takes me a really long time to figure out what the honest thing to do is. It usually requires me understanding what you are trying to achieve in any given context, and then somehow give you models and information that assist you within that context. It&#39;s easy to be dishonest while only saying true things, and even the truth value of a given statement heavily depends on context and predicting how you are likely to interpret it, and whether the parts where you will predictably be confused or wrong will matter for what you will do with that information.</li><li> I do agree that superintelligent systems will understand what we mean by &quot;honesty&quot; better than we do, probably, since a lot of my model of honesty is pretty bottlenecked on being smarter and understanding lots of parts of the world better</li><li> The key thing that I expect to be true with the current training paradigm is something like &quot;the model really has no motivation towards being honest, in part because at least the common-sense view of honesty doesn&#39;t even really apply to the cognition of a mind as alien as a language model&quot;. Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</li><li> But also, yeah, I just feel really deeply confused what &quot;motivates&quot; a language model. Clearly almost all of a language model&#39;s cognition is going into the objective of &quot;predict the next token&quot; (including after RLHF training where it looks more like the model has goals like being &quot;helpful, harmless, and honest&quot;). But does that cognition have any &quot;agency&quot;? Like, does it even make sense for the model to be &quot;honest&quot; in its pursuit of predicting the next token? Is the context of a single forward pass just too small for it to make any sense to think about the model having goals in the context of pursuing the next token?</li></ul><p> Going back up a level about how this relates to the overall question:</p><p> In as much as the best target for current AI Alignment efforts is to try to build systems that work towards some proxy task that will make the rest of the AI Alignment problem easier (either by buying us more time, or making conceptual progress on the problem, or being a really useful tool that speeds up our efforts of controlling AI systems), then it seems that being able to motivate systems towards those tasks is really quite crucial.</p><p> So let&#39;s characterize some tasks or ways early superhuman systems could help make the rest of the AI Alignment problem easy:</p><p> <strong>Honesty:</strong></p><p> I think the <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK document</a> mostly gets the point across of why &quot;honesty&quot; isn&#39;t as-straightforward a target as one might think it is. My current summary of ELK is approximately &quot;if we could get an AI system to reliably be honest, ie we could get it to try to genuinely get it to explain anything to us that it understands, then we can maybe leverage that into a full AI Alignment solution&quot;.</p><p> That said, see all the open problems in the ELK document.</p><p> <strong>Corrigibility:</strong></p><p> I feel like I don&#39;t super understand what corrigibility really points to. In the abstract, <a href="https://www.lesswrong.com/tag/corrigibility">corrigibility</a> sounds great:</p><blockquote><p> A <a href="https://arbital.greaterwrong.com/p/corrigibility/"><u>&#39;corrigible&#39;</u></a> agent is one that <a href="https://arbital.greaterwrong.com/p/nonadversarial/"><u>doesn&#39;t interfere</u></a> with what <a href="https://arbital.greaterwrong.com/p/value_alignment_programmer/"><u>we</u></a> would intuitively see as attempts to &#39;correct&#39; the agent, or &#39;correct&#39; our mistakes in building it; and permits these &#39;corrections&#39; despite the apparent <a href="https://arbital.greaterwrong.com/p/instrumental_convergence/"><u>instrumentally convergent reasoning</u></a> saying otherwise.</p><ul><li> If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down, <a href="https://arbital.greaterwrong.com/p/no_coffee_if_dead/"><u>it will be unable to fulfill what would usually be its goals</u></a> .</li><li> If we try to reprogram the AI, a corrigible AI will not resist this change and will allow this modification to go through. If this is not specifically incentivized, an AI might attempt to fool us into believing the utility function was modified successfully, while actually keeping its original utility function as <a href="https://arbital.greaterwrong.com/p/cognitive_steganography/"><u>obscured</u></a> functionality. By default, this deception could be a <a href="https://arbital.greaterwrong.com/p/preference_stability/"><u>preferred outcome according to the AI&#39;s current preferences</u></a> .</li></ul></blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p><p> <strong>Narrowly superhuman scientists</strong></p><p> As you suggested, maybe it is possible to make an AI that is narrowly superhuman in some domain of science, like material engineering or developing the technology necessary to make uploads work, and then you use that technology to solve the AI Alignment problem.</p><p> I currently don&#39;t have a great candidate technology here, but figuring out potential technologies and ways to use them is among the top things I would like more people to do.</p><p> I do think the core difficulty here is just that developing almost any technology to the point of usefulness requires a pretty huge amount of general intelligence. This is maybe the least true in the domain of software, but also, the domain of software is among the most domains to gain expertise in, in terms of enabling various forms of recursive self-improvements and enabling a huge amount of leverage over the world.</p><p> <strong>Coordination technologies</strong></p><p> The AI X-Risk problem is ultimately caused by a coordination problem. If humanity was sufficiently cautious and willing to take decades or centuries to handle the transition to an AI dominated future, then my guess is we would likely be fine. How humanity coordinates on stuff like this seems extremely messy and confusing, and I really don&#39;t know how to predict whether a given technology will make humanity&#39;s decisions here better or worse, but I do sure feel like there must be some AI-adjacent things here that help a lot.</p><p> As an example, there might be some AI-intermediary technology that could enable much better coordination to avoid arms races. Maybe there is a way to substantially improve bargaining using auditable intermediate AI systems. Current technologies really don&#39;t seem very useful here, but it is again one of those things that I would really like someone to look into.</p><hr><p> So, why am I saying all this? I think I am trying to get the point across that before we reach the domain of forced lawfulness, we will pass through a domain where if we play our cards right, we can mostly punt on the problem and end up leaving future humanity in a much better position to tackle the full alignment problem.</p><p> I do think systems will be pushed towards lawfulness even early on (and are already pushed in that direction right now), and understand the landscape of minds that these forces create even at current capability levels is really important. That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 23:50:14 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 23:50:14 GMT" user-order="2"><p> Bullet responses to your bullets on honesty:</p><ul><li> I agree that honesty is sometimes complicated, but I think it is mostly about intent and want, and once you <i>want</i> to be honest, that&#39;s most of the way to actually being honest. You still have to exercise a reasonable-to-you-and-your-counterparty level of care and effort in not ending up having people (honestly and accurately) say that they feel they were misled, in the counterfactual where they knew everything you knew.<br><br> &quot;Being honest&quot; is not the same thing as <i>never</i> ending up misleading people, though I suspect that once both parties are above a certain threshold of capabilities and Lawfulness, it is close enough: if both parties want to be mutually honest with each other, they will in practice be able to avoid misleading or being misled pretty well, even if there&#39;s a relatively large gap in their relative capabilities levels and modes of cognition.</li><li> superintelligences will be more capable of <i>more effective</i> honesty, ie not accidentally misleading people when they don&#39;t want to do that, even when humans themselves are not so effective at being honest even when they want to be. The question is, what will the superintelligence <i>want</i> , and how can we have any influence on that at all?</li></ul><blockquote><p> Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</p></blockquote><p> Yeah, I think this is where you just have to be careful about how you define your system and be careful to talk about the system(s) that actually matter. A language model itself is just a description of a mathematical function that maps input sequences to output probability distributions. A computer program that samples from a potentially-dangerous model with a potentially-dangerous starting prompt, but pipes all the output directly to /dev/null with no human or machine eyes ever viewing the contents, is probably pretty safe, barring a very weird / powerful superintelligence that has enough self-awareness and capabilities to exploit some kind of <a href="https://en.wikipedia.org/wiki/Row_hammer#:~:text=Row%20hammer%20(also%20written%20as,nearby%20memory%20rows%20that%20were">rowhammer</a> -like side-channel  in its implementation. The same computer program hooked up to actuators (eg direct internet access, or a human reading the output) capable of actually exerting influence on its environment, is liable to be pretty dangerous.<br><br> As for what kind of internal motivations these systems have, I&#39;m pretty confused and unsure about that too. But the question itself doesn&#39;t feel as deeply confusing to me when I think in terms of systems and their causal effects on their environment, and whether it makes sense to talk about those systems as behaving Lawfully.</p><p> I haven&#39;t looked closely at the ELK paper, but in general, honesty seems like an alignment-complete problem because it requires wantingness in the definition. If ARC has some way of breaking down honesty into gears-level components that still add up to a natural intuitive definition of honesty even in edge cases, and then making an AI system satisfy all those components, that does seem like a promising strategy. Or at least, it&#39;s the right shape of something that could work, in my model.<br><br> <strong>Corrigibility</strong><br></p><blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p></blockquote><p> Right, &quot;wanting to be corrigible&quot; seems like another problem that is alignment-complete. Corrigibility also has a wantingness component in the definition, but the disadvantage compared to honesty is that it is a lot more complicated and unintuitive even once you have the wantingness. It&#39;s also asymmetric, so unlike honesty, it&#39;s probably not the kind of thing that comes mostly for free once you have mutual wantingness and a baseline level of capabilities in both parties.<br><br> The advantage of corrigibility is that it would probably be more directly and immediately useful if we could get a powerful AI system to want to be corrigible, and it&#39;s also the kind of thing that we can apply and talk about sensibly for below-human-level systems. Consider the principles / desiderata of corrigibility listed <a href="https://www.lesswrong.com/posts/eS7LbJizE5ucirj7a/dath-ilan-s-views-on-stopgap-corrigibility">here</a> : it&#39;s a lot easier to tell if a particular below-human-level AI system is adhering to the principle of &quot;operator looping&quot; or &quot;conceptual legibility&quot; or &quot;whitelisting&quot;, and talk about whether it will continue to have those properties as it increases in capabilities, than it is to talk about whether a below-human-level language model system is &quot;honest&quot; or &quot;wants to be honest&quot; or not.<br><br> <strong>Coordination</strong><br></p><blockquote><p> The AI X-Risk problem is ultimately caused by a coordination problem.</p></blockquote><p> Yeah, coordination failures rule everything around me. =/</p><p> I don&#39;t have good ideas here, but something that results in increasing the average Lawfulness among <i>humans</i> seems like a good start. Maybe step 0 of this is writing some kind of Law textbook or Sequences 2.0 or CFAR 2.0 curriculum, so people can pick up the concepts explicitly from more than just, like, reading glowfic and absorbing it by osmosis. (In planecrash terms, Coordination is a fragment of Law that follows from Validity, Utility, and Decision.)<br><br> Or maybe it looks like giving everyone <a href="https://www.lesswrong.com/posts/cLr6TJj2qRrBa3Wmu/intelligence-enhancement-monthly-thread-13-oct-2023?commentId=yLJvn42im5QiEf2uq">intelligence-enhancing gene therapy</a> , which is apparently a thing that might be possible!?</p><p> I don&#39;t know how much non-AGI AI can help here, but I do think technologies that increase humanity&#39;s coordination ability and collective intelligence are very likely to be very positive, even accounting for any possible negative impact that such enhancements might have on AI timelines. Technologies like <a href="https://manifold.markets/home">prediction markets</a> seem great and have the potential to be hugely positive if they get more widespread and impactful adoption. Other things that somehow increase population-wide understanding of the very basics of markets and economics seem also potentially helpful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Sat, 14 Oct 2023 00:25:00 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Sat, 14 Oct 2023 00:25:00 GMT" user-order="2"><blockquote><p> That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p></blockquote><p><br> Note that Law doesn&#39;t have anything to do with AI specifically; in the story it&#39;s just what Eliezer&#39;s fictional world considers the basic things you teach to children: logical vs. empirical truths, probability theory, utility theory, decision theory, etc.</p><p> And then the rest of the story is all about how those concepts can be applied to get technological civilization and Science and various other nice things from first principles.</p><p> Lawfulness <i>itself</i> is the concept that certain logical deductions are spotlighted by their simplicity and usefulness, and thus will likely recur across a wide space of possible minds: aliens, humans, AIs, etc. regardless of what those minds <i>value,</i> which is (mostly) more of a free variable.</p><p> So on this view, a bunch of woes on Earth (unrelated to AGI) are a result of the fact that Earth!humans are not very Lawful, and thus not very good at coordination, among other things.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 17 Oct 2023 18:36:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 17 Oct 2023 18:36:06 GMT" user-order="1"><p> (Note for readers: This dialogue isn&#39;t over yet! Subscribe for future dialogue entries)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Wed, 18 Oct 2023 01:17:08 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Wed, 18 Oct 2023 01:17:08 GMT" user-order="2"><p> The Lawfulness stuff seems to have elicited a reaction from at least one commenter, and you expressed interest in digging into it yourself. I&#39;ll just ramble for a bit here, but I&#39;m happy to go into some particular direction in more detail, given a prompt.</p><p> I want to gesture at a bunch of stuff: epistemic rationality, instrumental rationality, consequentialism, and like, the whole rationalist project in general, as kind of core examples of the concept of Lawfulness. Like, I think you can almost just directly replace &quot;rationality&quot; with &quot;Lawfulness&quot; in this <a href="https://www.lesswrong.com/posts/RcZCwxFiZzE6X7nsv/what-do-we-mean-by-rationality-1">post</a> , and it basically scans. Lawfulness reified as a concept itself is just this extra little idea on top about <i>why</i> exactly all this rationality stuff even does anything and where it comes from, from truly first principles.<br><br> You can also point at a bunch of human behaviors or failures in the world, and if you trace back far enough, get to a place where you say, &quot;aha, this is the result of a human or group of humans acting unLawfully in some way. Specifically, they&#39;re falling prey to the sunk cost fallacy, or the conjunction fallacy, or failing to coordinate with each other, or just making some even more fundamental kind of reasoning error, eg failing to implement modus ponens&quot;.</p><p> It might be helpful to drill into the concept by sticking to examples in humans and not directly bring in the application to AI, but another framing that kind of rhymes with &quot;capabilities generalize further than alignment&quot; and the concept of instrumental convergence is:<br><br> There&#39;s a relatively smaller number of ways to accomplish stuff effectively / optimally, compared to the amount of possible stuff you can want to accomplish. Maybe there&#39;s even a <i>single optimal method</i> (up to isomorphism) for accomplishing stuff once you get far up into the capabilities spectrum, ie all ideal agents look pretty similar to each other in terms of what kind of methods of cognition and reasoning and tools they will use (but they can still have different things they value).</p><p> I think there&#39;s a lot of reasons to expect this is true for ideal agents; eg things like Aumman&#39;s agreement theorem, and various coherence theorems are kind of hints at it. But there are weaker forms of coherence and Lawfulness that probably kick in well before you get to ideal-agent territory or even wildly-superhuman AGI, and the evidence / intuition for <i>this</i> claim comes from observations of human behavior and introspection rather than Aumman&#39;s agreement-flavored stuff or coherence theorems.</p><p> Regular human levels of intelligence are enough where you start systematically doing better by being more rational / Lawful. <a href="https://www.lesswrong.com/posts/4ARtkT3EYox3THYjF/rationality-is-systematized-winning">Rationalists should win</a> , and, in my experience at least, human rationalists do often do pretty well relative to non-rationalists.</p><p> Also, I get the sense that a common criticism of the Sequences is that a lot of the concepts aren&#39;t that novel or deep: they&#39;re mostly restatements of stuff that was already in various academic literature, or just a bunch stuff that&#39;s intuitively obvious to everyone. I think there&#39;s more than a bit of typical-minding / overestimating the general sanity of the rest of humanity by these (perhaps mostly straw / hypothetical) critics. But the fact that a lot of the concepts and conclusions in the Sequences have been independently discovered or written about in a lot of different places, and that many people find them intuitively obvious, is actually evidence that they are pointing to something a bit more universal than just &quot;humans are biased in various ways&quot;.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H</section></section><br/><br/> <a href="https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems<guid ispermalink="false"> K8jJpdfKE3racyziC</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:36:56 GMT</pubDate></item></channel></rss>