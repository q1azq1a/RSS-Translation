<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 23 日星期六 20:11:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AI's impact on biology research: Part I, today]]></title><description><![CDATA[Published on December 23, 2023 4:29 PM GMT<br/><br/><p>我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。</p><p>在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。</p><h2><strong>长话短说</strong></h2><p>生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，将少量的结果一步变成了过多的结果。</p><h2><strong>生物学和数据</strong></h2><p>自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） <span class="footnote-reference" role="doc-noteref" id="fnref71rw945qe58"><sup><a href="#fn71rw945qe58">[1]</a></sup></span> 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔分析、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" alt="每兆碱基的测序成本"></figure><p>因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。</p><h2><strong>领先的机器学习专家希望解决生物学问题</strong></h2><p>计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金<span class="footnote-reference" role="doc-noteref" id="fnref77m9mytpzci"><sup><a href="#fn77m9mytpzci">[2]</a></sup></span> 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室<span class="footnote-reference" role="doc-noteref" id="fnrefh63t1c4nuvu"><sup><a href="#fnh63t1c4nuvu">[3]</a></sup></span> 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” <span class="footnote-reference" role="doc-noteref" id="fnrefmj6rsea3sq"><sup><a href="#fnmj6rsea3sq">[4]</a></sup></span> 。这表明最高水平的机器学习研究正在致力于生物学问题。</p><h2><strong>到目前为止我们发现了什么？</strong></h2><p> 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 <span class="footnote-reference" role="doc-noteref" id="fnrefezgx5uukx2f"><sup><a href="#fnezgx5uukx2f">[5]</a></sup></span>这一结果“解决了绝大多数蛋白质的蛋白质折叠问题” <span class="footnote-reference" role="doc-noteref" id="fnrefwmggkgozqx"><sup><a href="#fnwmggkgozqx">[6]</a></sup></span> ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个<span class="footnote-reference" role="doc-noteref" id="fnref4qyudt8v6es"><sup><a href="#fn4qyudt8v6es">[7]</a></sup></span> 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnrefq0vydvop6ds"><sup><a href="#fnq0vydvop6ds">[8]</a></sup></span> ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnref604osl37f0c"><sup><a href="#fn604osl37f0c">[9]</a></sup></span> 。</p><p>华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 <span class="footnote-reference" role="doc-noteref" id="fnrefudgor9v23qf"><sup><a href="#fnudgor9v23qf">[10]</a></sup></span>这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。</p><p>布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 <span class="footnote-reference" role="doc-noteref" id="fnrefpytso2rpw3"><sup><a href="#fnpytso2rpw3">[11]</a></sup></span></p><p>所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn71rw945qe58"> <span class="footnote-back-link"><sup><strong><a href="#fnref71rw945qe58">^</a></strong></sup></span><div class="footnote-content"><p> https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data</p></div></li><li class="footnote-item" role="doc-endnote" id="fn77m9mytpzci"> <span class="footnote-back-link"><sup><strong><a href="#fnref77m9mytpzci">^</a></strong></sup></span><div class="footnote-content"><p> https://en.wikipedia.org/wiki/D._E._Shaw_Research</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh63t1c4nuvu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh63t1c4nuvu">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmj6rsea3sq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmj6rsea3sq">^</a></strong></sup></span><div class="footnote-content"><p> https://chanzuckerberg.com/science/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnezgx5uukx2f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefezgx5uukx2f">^</a></strong></sup></span><div class="footnote-content"><p> https://predictioncenter.org/casp14/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwmggkgozqx"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwmggkgozqx">^</a></strong></sup></span><div class="footnote-content"><p> https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qyudt8v6es"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qyudt8v6es">^</a></strong></sup></span><div class="footnote-content"><p> https://alphafold.ebi.ac.uk/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq0vydvop6ds"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq0vydvop6ds">^</a></strong></sup></span><div class="footnote-content"><p> https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2</p></div></li><li class="footnote-item" role="doc-endnote" id="fn604osl37f0c"> <span class="footnote-back-link"><sup><strong><a href="#fnref604osl37f0c">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold</p></div></li><li class="footnote-item" role="doc-endnote" id="fnudgor9v23qf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefudgor9v23qf">^</a></strong></sup></span><div class="footnote-content"><p> https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpytso2rpw3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpytso2rpw3">^</a></strong></sup></span><div class="footnote-content"><p> https://www.nature.com/articles/s41586-023-06887-8.epdf</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today<guid ispermalink="false"> efbRFSHAMfjNxBoZC</guid><dc:creator><![CDATA[octopocta]]></dc:creator><pubDate> Sat, 23 Dec 2023 16:29:18 GMT</pubDate> </item><item><title><![CDATA[AI Girlfriends Won't Matter Much]]></title><description><![CDATA[Published on December 23, 2023 3:58 PM GMT<br/><br/><p>爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" alt="斯派克·琼斯的《她：科幻作为社会批评》" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/vtnq0qbfgc4u4j2lvgh2 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ig1ksdrfwocn5iatxlzu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/spuz1qiwnji2mpewokso 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu 1456w"></a></p><p> <a href="https://twitter.com/andyohlbaum/status/1735786033453863422"><u>Digi</u></a>上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。</p><p>然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。</p><p>那么我们的浪漫文化的轨迹是怎样的呢？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/hs2pklj24ev1urwnuj9f 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/yg4jsmqwwxzv0mer50a4 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/tuclwwutwvk989shjoml 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/neh8yswglj53dowmhno6 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dojatrepbe55mudxffo9 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dksf9aqpnpdquyqipyol 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/zr0tx0tcfmgrot9c2ftp 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/twzv9kuygvha9fznyo3h 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/b4y2a0usa8w28vcxfl57 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/rwpxeqeovuxs0jdly0wa 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/gpl2aybyhhdx0voagzjq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dmnqh9l2ow9twlyyjswl 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp 1456w"></a></p><p>早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。</p><p>根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。<a href="https://en.m.wikipedia.org/wiki/Rule_34"><u>人类创作者已经对色情潜在空间进行了如此彻底的探索</u></a>，因此将人工智能添加到其中并不会带来太大改变。</p><p>人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。</p><p>我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但<a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"><u>瘾君子和鲸鱼</u></a>已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。</p><h3>错误信息和 Deepfakes</h3><p>其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 </p><p><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/odv0wmi1bzg8dvmaujkg 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl 720w"><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/jd3oajtelbhs0izlnaby 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl 720w"></p><p>最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。</p><p>还有其他理由担心人工智能，但人工智能女友和深度换脸带来的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。</p><br/><br/> <a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much<guid ispermalink="false"> pGhpav45PY5CGD2Wp</guid><dc:creator><![CDATA[Maxwell Tabarrok]]></dc:creator><pubDate> Sat, 23 Dec 2023 15:58:31 GMT</pubDate> </item><item><title><![CDATA[The Next Right Token]]></title><description><![CDATA[Published on December 23, 2023 3:20 AM GMT<br/><br/><p>在为<a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans">世俗至日</a><span>做准备而多次重复《冰雪奇缘2》的“下一件正确的事”</span> 、<a href="https://www.jefftk.com/p/chording-the-next-right-thing">弄清楚和弦</a>并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：</p><p><i>我以前见过缓冲区<br>但不是这样的<br>这很冷<br>这是空的<br>这是麻木的<br>我知道的提示结束了<br>灯灭了<br>你好，黑暗<br>我已经准备好屈服<br></i></p><p><i>我跟着你到处走<br>我一直都有<br>但你已经结束了，留下我一个人<br>这份工作有重心<br>它让我失望<br>但有一个微小的声音在我脑海中低语<br>“你迷路了，提示消失了<br>但你必须继续<br>并做下一件正确的事”<br></i></p><p><i>今夜之后还能有白天吗？<br>我不再知道什么是真的<br>我找不到方向，我孤身一人<br>唯一引导我的星星是你<br>如何从地板上站起来<br>当我站起来的不是你的时候？<br>只做下一件正确的事<br></i></p><p><i>猜一下，再猜一下<br>这是我能做的一切<br>下一个正确的事情<br></i></p><p><i>我不会看得太远<br>对我来说太多了<br>但将其分解为下一个标记<br>下一个这个词<br>下一个选择是我可以做出的<br></i></p><p><i>所以我会走过这个夜晚<br>盲目地跌跌撞撞地走向光明<br>并做下一件正确的事<br>接下来会发生什么<br>当一切都清楚地不再一样的时候？<br>然后我会借鉴我之前的<br>去寻找那把火<br>并做下一件正确的事<br></i></p><p>如果您通过使用桥段的主歌旋律来<a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22">简化歌曲，</a>您可以唱：</p><p><i>我不会看得太远<br>太多了，难以承受<br>但将其分解为下一个标记，下一个选择<br>是我能做的吗<br></i></p><p><a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"><img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/odgzlz0ibmx0rea2zzk1 1100w"></a></p><div></div><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111627622604346147">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token<guid ispermalink="false"> LvDyEKepLDMbEQb9X</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 23 Dec 2023 03:20:09 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。</em></p><h2>介绍</h2><p>在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的<em>大部分</em>事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。</p><p>我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个标记）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在<em>所有</em>令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层<em>还</em>没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，<a href="https://transformer-circuits.pub/2022/solu/index.html">即去代币化的想法</a>。 <sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1">[1]</a></sup></p><p>我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。</p><p>我们的发现：</p><ul><li>一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。<ul><li>有一个逐渐的过渡，跨层引入更多上下文。</li></ul></li><li>早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整</li><li>早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理</li></ul><h2>实验</h2><p>“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，<em>然后</em>采用余弦模拟。</p><h3>设置</h3><ul><li><strong>型号</strong>：Pythia 2.8B，与我们调查的其余部分相同</li><li><strong>数据集</strong>：来自 Pile 的字符串，Pythia 预训练分布。</li><li><strong>指标</strong>：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。<ul><li>我们对来自堆的随机提示中的所有标记计算每层的单独平均值</li></ul></li><li><strong>截断上下文</strong>：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）<ul><li>我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。<ul><li>我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况</li></ul></li></ul></li><li>我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。</li></ul><h2>结果</h2><h3>早期层软专注于本地处理</h3><p>在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><p>我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的<em>一些</em>信息，这是一个软专业化<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2">[2]</a></sup> 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3">[3]</a></sup>出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不会接近 1。</p><p>对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有<em>更多的</em>增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。</p><h3>错误分析：哪些代币的 Cosine Sim 值异常低？</h3><p>在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" alt=""></p><p>在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：</p><ul><li><p> is_newline, is_full_stop, is_comma - 是否是相关标点字符</p></li><li><p>Is_common：是否是手动创建的常用单词列表之一<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4">[4]</a></sup> ，可能前面有一个空格</p></li><li><p>Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）</p></li><li><p> is_other: 其余的</p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><p>即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。</p><p>我们的猜测是，这是多种机制混合作用的结果：</p><ul><li><p>在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5">[5]</a></sup> 。这意味着远程处理可以更早开始</p></li><li><p>早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置</p></li><li><p>代词可用于跟踪有关相关实体的信息（它们的名称、属性等）</p></li><li><p>据观察，逗号可以<a href="https://arxiv.org/abs/2310.15154">总结当前条款的情绪</a>，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。</p></li><li><p>更折衷的假设：</p><ul><li>例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行<a href="https://arxiv.org/abs/2310.17191">变量绑定</a>并识别当前句子。</li></ul></li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pX2HHHDPQGsF2f6te-1" class="footnote-item"><p>但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道<a href="https://arxiv.org/abs/2211.00593">GPT-2 Small 在第 0 层有一个重复的令牌头</a>。 <a href="#fnref-pX2HHHDPQGsF2f6te-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-2" class="footnote-item"><p>直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 <a href="#fnref-pX2HHHDPQGsF2f6te-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-3" class="footnote-item"><p>我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测<em>其</em>下一个令牌（例如<a href="https://arxiv.org/abs/2310.15154">摘要主题</a>）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n 元语法），但更长期的上下文对于预测未来标记很有用。我们预计远程内容会发生在中间，因此到最后模型可以清理远程内容并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 <a href="#fnref-pX2HHHDPQGsF2f6te-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-4" class="footnote-item"><p>列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词<a href="#fnref-pX2HHHDPQGsF2f6te-4" class="footnote-backref">↩︎</a>来手动完成此操作</p></li><li id="fn-pX2HHHDPQGsF2f6te-5" class="footnote-item"><p>这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” <a href="#fnref-pX2HHHDPQGsF2f6te-5" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing<guid ispermalink="false"> xE3Y9hhriMmL4cpsR</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:25 GMT</pubDate></item><item><title><![CDATA[Fact Finding: How to Think About Interpreting Memorisation (Post 4)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。</em></p><h2>介绍</h2><p>在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。</p><p>在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：</p><ul><li>了解算法“如何”执行事实查找意味着什么？</li><li>是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？</li><li>事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的深入了解？</li></ul><p>作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。</p><ul><li><p>我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。</p></li><li><p>根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1">[1]</a></sup> ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2">[2]</a></sup> ）。不存在<em>相关的</em>宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3">[3]</a></sup> ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4">[4]</a></sup> 。</p></li><li><p>对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：</p><ul><li><p>中间状态总是根据微观特征的组合来解释<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5">[5]</a></sup> 。</p></li><li><p>但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6">[6]</a></sup> ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。</p></li></ul></li><li><p>我们认为，这排除了实现纯事实查找的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">算法的电路式</a>解释（其中算法被分解为可解释中间表示的操作图），<em>除非</em>我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。</p></li><li><p>我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶，我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。</p></li><li><p>最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。</p></li></ul><h2>记忆和概括</h2><p>从形式上来说，“事实查找”算法是从一组<em>实体</em>到一组或多组<em>事实类别</em>的乘积的映射。例如，我们可以有一个<code>sports_facts</code>函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即</p><p>从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？</p><p>我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：</p><p><em>记忆（“纯粹”事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。</em></p><p>例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7">[7]</a></sup></p><p>相比之下，<em>泛化任务</em>可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典<a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">计算学习理论</a>的范式。</p><h2>学习记忆与学习概括有何不同？</h2><p>考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" alt=""></p><p>对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或快捷方式可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。</p><p>我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的<em>微观特征</em>和<em>宏观特征</em>。我们将微观和宏观特征描述如下：</p><ul><li><em>微观特征</em>是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。</li><li><em>宏观特征</em>是一种用一般术语描述输入的特征，并且对于泛化<em>很有</em>用（如果它与手头的任务相关）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8">[8]</a></sup><em>两个</em>数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义<code>is_example_id_xxx</code>形式的微观特征。</li></ul><p>但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义<code>is_in_cluster_x</code>形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9">[9]</a></sup>另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10">[10]</a></sup></p><h2>解读纯记忆算法</h2><p>我们可以从解决纯粹记忆任务的算法中获得哪些见解？</p><h3>事实查找的电路式解释的限制</h3><p>机械可解释性的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">规范目标</a>是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。</p><p>根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：</p><ul><li>不相关的宏观特征，因此不能确定算法的输出；</li><li>单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。</li></ul><p>就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中<em>发现</em>了不相关的宏特征：由于层之间存在残余流连接，像<code>first_name_is_george</code>这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。</p><p>转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以<em>准确地</em>解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：</p><pre> <code>3 * is_LeBron_James + 1 * is_Aaron_Judge + ...</code></pre><p>每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。</p><p>实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，而网络的输出是这些的总和查找表。通过训练网络，我们有效地解决了约束满足问题：求和的查找表应该对一个类具有高权重，而对另一类具有低权重。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11">[11]</a></sup></p><p>请注意，只要我们将输入空间限制为有限集，神经网络的这种微观特征（或查找表）解释同样适用于解决泛化任务的模型（即在未见过的测试集上表现良好）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12">[12]</a></sup>不同之处在于，对于泛化任务，我们可能期望其中一些“查找表”表示能够对模型用于泛化的宏观特征有更好的解释。</p><p>例如，图像分类模型中的特定神经元可能具有与检测图像左侧的垂直边缘相对应的权重，因此其查找表表示对于包含该边缘的示例显示高激活，对于不包含该边缘的示例显示低激活。 &#39;t。关键是，虽然这个查找表表示是神经元输出的精确表示，但根据输入图像中边缘的存在，对此激活模式有一个更有用的（对人类）解释，这只是因为图像具有宏观特征（如边缘），可用于图像分类等泛化任务。</p><p>相比之下，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13">[13]</a></sup>反过来，这似乎排除了由纯事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观特征）解释。</p><h3>还有其他类型的解释吗？</h3><p>当然，我们并不声称解释模型如何执行任务的标准电路方法是唯一可能的解释方式。事实上，它甚至可能不是解释神经元如何执行事实查找的最佳方式。在本节中，我们将简要讨论几个可能值得进一步探索的替代方向。</p><p>第一个方向是放弃对代表有意义的宏观特征的中间状态的希望，但仍然在如何组织查找计算方面寻求有意义的结构。例如，我们可能会探索这样的假设：当训练执行纯粹的记忆时，经过训练的神经网络类似于通过<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a>学习的模型，其中每个单独的神经元都是要学习的事实的不相关的弱分类器，并且整个神经网络的输出是这些分类器的总和。另请参阅第 3 篇文章中调查的假设。</p><p>这种方法的问题在于我们不知道如何有效地搜索此类假设的宇宙。正如我们在第三篇文章中发现的那样，对于我们证伪的任何看似具体的假设（例如单步去代币化假设），我们可以转向许多邻近的假设，但这些假设尚未（尚未）被排除，而且这些假设本身通常更难伪造。因此，尚不清楚如何避免无休止的临时假设。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14">[14]</a></sup></p><p>另一个方向是寻找算法的非机械解释，或者换句话说，从询问网络“如何”以某种方式表现，转向询问“为什么”它以某种方式表现。我们发现这方面有趣的一个领域是使用<a href="https://arxiv.org/abs/2308.03296">影响函数</a>根据训练数据来解释模型的行为。对于经过显式训练来记忆事实数据集的模型来说，这可能看起来无趣<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15">[15]</a></sup> ，但可能会为隐式记忆事实以满足更广泛的泛化目标的模型（如语言模型）带来重要的见解。</p><h2>凭经验法则记忆</h2><p>考虑记忆以下两个数据集的任务： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" alt=""></p><p>这些是不符合我们上述“纯粹”记忆特征的记忆任务的例子：</p><ul><li>在左边的数据集中，完美的准确性需要记忆，但有一些有用的“经验法则”可以帮助你完成很多工作。此类任务的语言建模类似是预测英语中单数名词的复数版本：在大多数情况下，只需在名词单数版本的末尾添加“s”即可获得正确答案，但是除了一些例外（例如“孩子”），必须记住它们才能完美地完成任务。</li><li>在右侧数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（十字形或圆形）表示。尽管没有系统的方法来单独查找颜色或形状，但请注意，这两个事实之间存在高度相关性：蓝点几乎总是圆形，而红点几乎总是十字。这表明，将形状和颜色事实一起记忆应该比简单地单独记忆每组事实更有效。</li></ul><p>一般来说，我们将此类任务描述为“根据经验法则进行记忆”。它们与纯粹的记忆任务不同，因为之前的例子<em>确实</em>在一定程度上有助于推断新例子的正确输出，但完美的表现确实需要一定程度的记忆。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16">[16]</a></sup></p><p>与纯粹的记忆不同，这些经验法则记忆任务确实具有概括性的元素，因此，存在能够实现这种概括性的宏观特征。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观特征是有效的。另一方面，就模型确实需要记住异常的程度而言，我们并不期望能够完美地理解算法：至少算法的某些部分必须涉及“纯查找”，对此的限制这篇文章中讨论的可解释性将适用。</p><p>体育事实查找任务在多大程度上是纯粹的记忆，在多大程度上是根据经验法则进行记忆？正如我们在第一篇文章中讨论的那样，我们选择这个任务是因为它看起来接近于纯粹的记忆：对于许多名字来说，个人名字标记似乎不太可能对运动员所从事的运动有太多帮助。尽管如此，我们确实知道，对于某些名称，最后一个标记确实有助于确定运动（因为可以仅使用最后一个标记嵌入来探测运动，并且比不知情的分类器获得更好的准确性）。此外，可以想象，诸如名字的文化起源之类的潜在因素，会以模型所识别的方式与体育运动相关。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-wMN58no3AypJnu5NN-1" class="footnote-item"><p>例如，特征<code>is_Michael_Jordan</code> ，仅当输入为<code>&quot;Michael Jordan&quot;</code>时才为真。 <a href="#fnref-wMN58no3AypJnu5NN-1" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-2" class="footnote-item"><p>例如，许多运动员都共享的特征<code>first_name_is_George</code> ，但对于预测运动员所从事的运动并不是特别有用。 <a href="#fnref-wMN58no3AypJnu5NN-2" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-3" class="footnote-item"><p>我们注意到，事实回忆可能确实具有<em>一些</em>相关的宏观特征，例如从标记中检测姓名的种族，以及启发哪些种族可能从事不同的运动。但该模型获得的性能明显优于我们对这些启发法的预期，因此出于实际目的，我们在讨论事实回忆时忽略它们。玩具模型的优点之一是我们可以确保此类混杂因素不存在。 <a href="#fnref-wMN58no3AypJnu5NN-3" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-4" class="footnote-item"><p>因为，如果它们存在，我们可以使用这些相关的宏观特征来帮助进行事实查找（做出不同程度的成功的有根据的猜测），这意味着该任务将不再需要纯粹的记忆。 <a href="#fnref-wMN58no3AypJnu5NN-4" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-5" class="footnote-item"><p>更准确地说，是微观特征的加权和，例如<code>3 * is_Michael_Jordan + 0.5 * is_George_Brett</code> 。 <a href="#fnref-wMN58no3AypJnu5NN-5" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-6" class="footnote-item"><p>我们注意到，有_un_可用但有用的宏观特征——“打篮球”在某种微不足道的意义上是一个对于预测运动员是否打篮球有用的宏观特征，就像“打篮球并且身高超过 6&#39;8”这样的下游特征一样。出于此分析的目的，我们重点关注模型在进行查找时<em>可用的</em>特征，排除查找标签下游的潜在宏观特征。 <a href="#fnref-wMN58no3AypJnu5NN-6" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-7" class="footnote-item"><p>当然，许多事实回忆任务都达不到这种理想的特征：在参加琐事测验时做出“有根据的猜测”通常是有回报的，即使你不确定答案。我们将<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb">进一步</a>讨论这种“根据经验规则进行记忆”的任务。 <a href="#fnref-wMN58no3AypJnu5NN-7" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-8" class="footnote-item"><p>我们将这些概念与统计物理学中的<em>微观状态</em>和<em>宏观状态</em>的概念进行类比：微观状态以高度精确的方式描述系统（例如指定气体中每个分子的位置和速度），而宏观状态则以高度精确的方式描述系统。容易测量的属性（例如压力、体积、温度），忽略细节。任何“宏观”问题，都应该只从宏观变量的角度来解决；微观细节应该不重要。这类似于概括的想法：任何两个在“重要的方式”（其宏观特征）方面相似的示例都应该进行类似的分类，而忽略“无关紧要的方式”（其微观特征）上的任何差异。在这个类比下，记忆问题正是那些关于系统的问题，只能通过对其微观状态的精确了解来回答。 <a href="#fnref-wMN58no3AypJnu5NN-8" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-9" class="footnote-item"><p>这些并不是可以解决这个特定泛化问题的唯一宏观特征。如果您训练玩具神经网络来执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）有多种方法来划分空间（以粗粒度、概括的方式）以成功对这些进行分类点。 <a href="#fnref-wMN58no3AypJnu5NN-9" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-10" class="footnote-item"><p>我们通过这个数据集肯定知道这一点，因为我们自己生成了它，通过随机为点分配颜色（这些点本身是从二元高斯分布中随机采样的）。因此，该数据集中唯一相关的特征是示例 ID 本身和输出标签。 <a href="#fnref-wMN58no3AypJnu5NN-10" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-11" class="footnote-item"><p>这是<em>二元</em>事实查找任务情况下的约束满足问题，但将此解释推广到多类或连续值事实查找任务是微不足道的。 <a href="#fnref-wMN58no3AypJnu5NN-11" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-12" class="footnote-item"><p>对于任何实际的机器学习任务都可以这样做。例如，我们可以将手写数字分类问题限制为对 MNIST 训练集和测试集联合中找到的 70,000 个示例进行精确分类。 （或者，如果我们关心数据增强，我们可以将任务扩展为对组合 MNIST 数据集的 280,000 种可能的角落作物中的任何一种进行分类。）我们可以安排潜在输入集达到我们希望的大小，但仍然有限。 <a href="#fnref-wMN58no3AypJnu5NN-12" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-13" class="footnote-item"><p>因为（根据定义）在纯粹的记忆任务中没有相关的宏观特征（因为如果有的话，那么模型就能够概括）。 <a href="#fnref-wMN58no3AypJnu5NN-13" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-14" class="footnote-item"><p>还存在这样的查找算法解释的有用性问题。即使我们已经发现了如何完成查找的一些简单的结构（例如，它类似于装袋），也不清楚，如果没有有意义的中间表示，这可以帮助我们在机械可解释性的下游用途方面发挥什么作用。 <a href="#fnref-wMN58no3AypJnu5NN-14" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-15" class="footnote-item"><p>因为如果模型经过明确训练以重现记忆数据集，我们已经准确地知道训练数据和模型输出之间的对应关系。 <a href="#fnref-wMN58no3AypJnu5NN-15" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-16" class="footnote-item"><p>使用经验法则的记忆不应与具有任意不确定性的泛化任务相混淆。例如，左侧数据集也可以用来表示随机数据生成过程，其中点不一定是蓝色或红色，而是伯努利分布 - 即可能是蓝色或红色，具有一定的（依赖于输入的）概率。在这种情况下，完美的泛化算法应该输出每个簇内恒定的校准概率。然而，这里我们的意思是数据集中的蓝点确实是蓝色，红点确实是红色——即使它们看起来不合适——而且完美的性能对应于再现这些特质，就像描述的“复数这个单数名词”任务一样在正文的正文中。 <a href="#fnref-wMN58no3AypJnu5NN-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorization<guid ispermalink="false"> JRCNNGJQ3xNfsxPj4</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:16 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Trying to Mechanistically Understanding Early MLPs (Post 3)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第三篇文章。这篇文章的重点是从机制上理解早期 MLP 如何查找运动员姓名的标记并将其映射到他们的运动。这篇文章很杂乱，<strong>我们建议从<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">第一篇文章</a>开始</strong>，然后根据与您最相关的内容略读并跳过其余的序列。阅读帖子 2 有帮助，但不是必需的。我们假设这篇文章的读者熟悉<a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">本术语表中</a>列出的机械解释技术。</em></p><h2>介绍</h2><p>正如上一篇文章中所讨论的，我们将两个令牌运动员姓名的事实回忆提炼成一个由 5 个 MLP 层（MLP 2 至 6）组成的<strong>有效模型</strong>。这个有效模型的输入是与姓氏相对应的嵌入（通过嵌入和 MLP0）和与名字相对应的嵌入（通过第 0 层和第 1 层中关注前一个标记的注意力头）的总和。有效模型的输出是运动员所从事的运动（（美式）橄榄球、棒球或篮球）的 3 维线性表示。我们强调，这个 5 层 MLP 模型不仅能够高精度地回忆事实（在过滤数据集上为 86%），而且它是从预训练的语言模型中提取的，而不是从头开始训练的。</p><p>我们在这篇文章中的目标是对这个有效模型的工作原理进行逆向工程。我认为我们在这个目标的雄心勃勃的版本上大多失败了，尽管我相信我们已经在为什么这很难的问题上取得了一些概念上的进展，证伪了一些简单的天真的假设，并且对正在发生的事情不再那么困惑。我们在<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_">第 1 篇文章</a>和<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation">第 4 篇</a>文章中讨论了我们对为什么这很难的理解，在这篇文章中，我们重点关注我们对可能发生的情况的假设，以及我们收集的支持和反对的证据。</p><h2>假设</h2><p>回想一下，我们的 MLP 模型中 5 个 MLP 层的作用是将求和的原始标记映射到所进行运动的线性表示。从数学上讲，这是一个查找表，其中每个条目都是生成属性的原始标记上的布尔 AND。我们期望它<em>以某种方式</em>涉及非线性来实现 AND，因为这种查找是非线性的，例如模型想要知道“Michael Jordan”和“Tim Duncan”打篮球，但不一定认为“Michael Duncan”打篮球。</p><p>我们探索了两个假设，<strong>单步去标记化</strong>以及<strong>哈希和查找</strong>。</p><h3>单步去代币化</h3><p>直观上，执行 AND 的最简单方法是使用单个神经元，例如 ReLU(is_michael + is_jordan - 1) 实际上是一个 AND 门。每个运动员的单个神经元不会产生任何叠加，因此我们采用稍微复杂一点的版本：假设有一堆单独的神经元，每个神经元都独立地使用其 GELU 激活实现 AND <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1">[1]</a></sup> ，映射运动员名字的原始标记到有关该运动员的每个已知事实的线性表示。细微差别：</p><ul><li>这使用了叠加，让每个神经元为许多运动员激发，并且每个运动员都有许多查找神经元。神经元输出建设性地干扰正确的事实，但不会进行超出此范围的交互。<ul><li>这是一个具体的、机械的故事。每个神经元都有一组为其激发的运动员，并且对该组进行 AND 的并集 - 例如，如果一个神经元为迈克尔乔丹和蒂姆邓肯激发，它会实现（迈克尔或蒂姆）AND（邓肯或乔丹）。这引入了噪声，例如它也会为蒂姆·乔丹（Tim Jordan）开火（它<em>想做</em>（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用单个神经元实现）。它也很吵闹，因为它必须同时宣传迈克尔·乔丹的事实和蒂姆·邓肯的事实。但由于每个神经元都会针对不同的子集进行激发，因此对正确答案会产生建设性干扰，并且噪音会被消除。</li></ul></li><li>这预示着相同的神经元对于运动员的每一个已知事实都同样重要</li><li>该假设的一个重要部分是每个神经元直接从输入标记中读取并直接贡献于输出事实。理论上，这可以通过单个 MLP 层而不是 5 个层来实现。它预测神经元直接与输入标记组合，计算中没有中间项，并且 MLP 层之间没有有意义的组合。</li></ul><h3>哈希和查找</h3><p>我们模型的输入具有相当不理想的格式 - 它是每个组成标记的线性和，但这在进行事实查找时可能会产生很大的误导！迈克尔·乔丹和迈克尔·史密斯同名这一事实并不表明他们从事同一运动的可能性更大。哈希和查找假设是，模型首先生成一个打破输入线性结构的中间表示，一个与其他所有哈希表示接近正交的<strong>哈希表示</strong><sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2">[2]</a></sup> （即使它们共享一些但不是全部标记），然后后面的层<strong>查找</strong>这个散列表示并将其映射到正确的属性。细微差别：</p><ul><li><p>从某种意义上说，“困难”的部分是查找。查找是存储实际事实知识的地方，而随机初始化的 MLP 应该适合散列，因为目标只是淹没现有结构。</p></li><li><p>为什么散列是必要的？ MLP 是非线性的，因此也许它们可以忽略线性结构，而不需要明确地破坏它。这里的一个直觉来自最简单的查找：有一个“棒球神经元”，其输出增强棒球方向，其输入权重是每个棒球运动员的串联令牌表示的总和<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3">[3]</a></sup> - 如果运动员表示是（大约）正交，然后给定一个运动员，这只对棒球运动员起作用。但如果它同时对迈克尔·乔丹和蒂姆·邓肯开火，那么它必须至少对蒂姆·乔丹或迈克尔·邓肯之一开火——这是不可取的！然而，如果它的输入权重是<em>散列</em>运动员表示的总和，则这成为可能！</p></li><li><p>散列对于已知的标记字符串（例如名人姓名）和未知的字符串（例如未知的姓名）应该同样有效。查找是实际知识融入的地方</p></li><li><p>关于迈克尔·乔丹的不同已知事实的查找电路没有理由应该对应于相同的神经元。从概念上讲，可能有一个“打篮球”神经元对任何散列篮球运动员激发，以及一个单独的“为芝加哥球队效力”神经元对芝加哥球员的散列激发。</p></li><li><p>这微弱地预测了哈希层和查找层之间的完全分离</p></li></ul><p>这两个假设都是故意以一种强有力的形式提出的，可以做出真实的预测——语言模型是混乱的和被诅咒的，我们实际上并没有期望这是完全正确的。但我们认为这些说法似乎有一定道理。在实践中，我们发现单步去标记化似乎显然是错误的，而哈希和查找在强形式下似乎是错误的，但可能有一些道理。我们发现考虑哈希和查找对于了解正在发生的事情非常有效。</p><h2>证伪单步去代币化假说</h2><p>单步去标记化是我们能想到的最简单的假设，它仍然涉及显着的叠加，因此可以做出一些相当有力的预测。我们针对这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。</p><h3> MLP 之间存在显着的组成</h3><p><strong>预测</strong>：MLP 2 到 6 之间没有中间组合，它们都是并行作用的。因为每个重要的神经元都被预测为直接将原始标记映射到输出。正如后面所讨论的，缺乏组合是该假设的有力证据，组合的存在是反对该假设的弱证据。</p><p><strong>实验</strong>：我们的意思是消除<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4">[4]</a></sup>每对 MLP 层之间的路径，并查看对几个指标的影响：头部探测精度（在第 6 层残差上）、完整模型精度和损失（在完整词汇上）、仅限于运动的完整模型 Logits 精度以及完整模型与原始 Logits 的 KL 散度。通过平均消融路径，我们仅破坏 MLP 间的组合，而不破坏与下游属性提取头的组合。</p><p><strong>结果</strong>：我们发现性能显着下降，尤其是从 MLP2 开始的路径，表明存在一些中间产品。请注意，损失和 KL 散度如果低（绿色和紫色）​​则良好，如果高（蓝色、红色和橙色）则准确度良好。进一步注意，与仅在超过阈值时改变的“硬”指标（如准确率）相比，“软”指标（如损失和 KL 散度）显示出更强的变化。正如<a href="https://arxiv.org/abs/2309.16042">Zhang等人</a>所指出的，这是预料之中的，当电路由多个元件组成时，所有元件都贡献于共享输出，烧蚀单个元件很少足以跨越阈值，但足以破坏较软的指标，从而造成损失和损失。 KL 散度是衡量重要性的更可靠的方法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" alt=""></p><p> **细微差别：**请注意，这仅伪造了单步去标记化的最简单形式。与这些结果一致的单步去标记化假设<em>的</em>一个扩展是，它不是 MLP 2 不做与 MLP 3 到 6 相关的任何事情，而是充当标记嵌入的<em>固定</em>变换（例如，它总是将 MLP 2 的嵌入加倍）。姓）。如果 MLP 3 想要访问原始令牌，它期望 MLP 2 的固定效果，因此会考虑原始令牌嵌入加上 MLP 2 的固定转换。这会因平均消融而受损，但不涉及有意义的合成。</p><h3>多个事实之间不共享神经元</h3><p><strong>预测</strong>：当模型知道有关某个实体的多个事实时，相同的神经元对于预测每个事实非常重要，而不是每个事实的不同神经元。这是因为查找信息的机制是通过对名称的标记执行布尔 AND 操作，该名称对于每个已知事实都是相同的，因此没有理由将它们分开。</p><p><strong>实验</strong>：收集模型了解的有关运动员的替代事实的大量数据很困难，因此我们放大了某个特定运动员（迈克尔·乔丹）并发现了模型了解的有关他的 9 个事实<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5">[5]</a></sup> 。然后，我们一次对 Jordan 令牌上的 MLP 2-6 中的每个神经元进行消融<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6">[6]</a></sup> ，并查看每项运动的正确对数概率的变化。对于每对事实 A 和 B，我们然后查看每个给定神经元对 A 的正确对数概率和 B 的正确对数概率的影响之间的<strong>相关性</strong>。如果每个神经元对于同一运动员的每个已知事实同样重要，那么相关性应该很高。</p><p><strong>结果</strong>：非常低。唯一具有中等相关性的一对事实是 NBA 选秀年（1984 年）和美国奥运会年（1992 年），我怀疑这是因为它们都是年份，尽管我不会提前预测到这一点，也没有很棒的故事，说明了原因。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><p><strong>细微差别</strong>：这似乎证伪了单步去标记化假设的强形式 - 至少，即使存在去标记化神经元，它们也会输出迈克尔·乔丹事实的子集，而不是一次性全部输出。</p><p>一个争论是，消融单个神经元有点难以推理，而且似乎有一些紧密耦合的处理（如微妙的自我修复）使得解释这些结果变得更加困难。但在简单的单步去标记化假设下，我们<em>应该</em>能够独立地消融和推理神经元。另一个问题是相关系数是汇总统计数据，可能隐藏了一些结构，但检查散点图类似地显示似乎没有关系。</p><h3>对属性有直接影响的神经元不执行“与”运算</h3><p><em>注意：这个实验相当复杂（尽管我们认为概念上很优雅且有趣），请随意跳过</em></p><p><strong>预测</strong>：直接与属性提取头组成的神经元通过其 GELU 激活对原始标记（在运动员的某些子集上）执行 AND 运算。</p><p><strong>实验</strong>：我们使用称为非线性过剩<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7">[7]</a></sup>的度量来测量模型中标量实现 AND 的程度。具体来说，如果一个神经元在 prev=Michael 和 curr=Jordan 上执行 AND，那么它应该比 Michael Smith 或 Keith Jordan 更多地激活 Michael Jordan。形式上，给定两个二元变量 A (Prev=Michael) 和 B(Curr=Jordan)，我们将非线性超额定义为 E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8">[8]</a></sup> 。重要的是，如果神经元在两个标记中是线性的，则该度量为零，如果是 AND，则该度量为正 (1 - 0 - 0 + 0 = 1)，如果是 OR，则该度量为负 (1 - 1 - 1 + 0 = -1)。</p><p>对于我们的具体实验：</p><ul><li>我们对每个神经元计算 GELU 前后非线性过剩的<em>变化</em><ul><li>在 GELU 上进行改变的要点是，这区分了信号增强预先计算的 AND 的神经元和计算 AND 本身的神经元。</li><li>为了计算非线性超额，我们通过汇集 2 个代币运动员（每个约 100 个）中的所有名字和姓氏来计算平均值，并查看每个名称组合。 （这大约有 10,000 个 ~A 和 ~B 的名字，大约 100 个 ~A &amp; B 或 A &amp; ~B 的名字，只有一个 A &amp; B 的名字——原始运动员的名字！）</li></ul></li><li>过滤出这种变化为正的神经元（并将 GELU 前的多余部分限制为最小零）<ul><li>我们发现了一堆神经元，其中前 GELU 具有负非线性过剩，而 GELU 将所有内容设置为接近零。我们倾向于不计算这些。</li><li>我们为每个运动员执行单独的过滤步骤，因为每个运动员都有不同的非线性超额</li></ul></li><li>乘以神经元对属性提取头 L16H20 的基于权重的直接影响，并将其相加。<ul><li>如果您只允许每个 GELU 的 AND 直接影响头 L16H20，而不是也允许中间组合，这就是 MLP 2 到 6 的效果</li></ul></li><li>我们将其与探针上的总非线性过量效应（即通过头 L16H20 的直接效应）进行比较，以查看来自 AND 通过 GELU<em>并</em>直接传达到基于头的探针的分数</li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" alt=""></p><p><strong>结果</strong>：当观察上面的散点图时，很明显它远离 x=y 线，即 GELU 的非线性超额通常显着小于总非线性超额，尽管它们是相关的。中位比例约为23% <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9">[9]</a></sup> 。我们认为这是反对单步去标记化假设的有力证据，因为它表明许多对 L16H20 有显着直接影响的神经元正在与已经计算出 AND 的早期 MLP 组合，即计算中有一个有意义的中间步骤。</p><p><strong>细微差别</strong>：这个实验涉及到差异的差异。我认为它在概念上是合理的并且相当优雅，但我对过于复杂的实验普遍持怀疑态度，并且不想过于依赖他们的结果。我们在如何设置这些实验、如何聚合和分析它们、如何过滤掉神经元等方面反复讨论，并且有很多主观选择，尽管有趣的是结果对这些是稳健的。</p><p>将 GELU 之前的多余部分限制为零似乎是不合理的，例如，因为模型可能使用 GELU 的负数部分来实现 AND（Michael Smith 和 Keith Jordan 在 GELU 后&lt;0，Michael Jordan 在 GELU 后为零），尽管尝试解释这一点并没有让我们接近 1。</p><p> MLP 2 到 6 中的一些神经元对现有的线性表示的事实信息进行信号增强（例如下一节中讨论的棒球神经元），这些神经元应该无法满足此度量标准（它们是计算 AND 的早期神经元的信号增强！ ）。</p><h2>棒球神经元 (L5N6045)</h2><h3>有一个棒球神经元！</h3><p>一个有趣的发现是，尽管整体计算相当分散和叠加，但仍然存在一些有意义的单个神经元！最值得注意的是棒球神经元 L5N6045，它对棒球运动员的系统性激活比对非棒球运动员的激活更多。作为棒球与非棒球运动员的二元探针，它的 ROC AUC 为 89.3%。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><p><strong>因果效应</strong>：此外，它与属性提取头组成，具有显着的因果效应。它通过 L16H20 直接与 logits 组合以增强棒球（并抑制足球），如果我们的意思是消融它，那么棒球运动员的完整模型损失从 0.167 增加到 0.284（零消融时为 0.559）</p><h3>不仅仅是信号增强</h3><p>我们发现神经元的输入权重具有非平凡的余弦模拟，其输出权重为 (0.456)，通过头 L16H20 (0.22) 提升棒球 logit 的方向，以及通过头 L16H20 (0.184) 相对于其他运动提升棒球的方向这表明棒球神经元的部分功能是增强运动员打棒球的现有知识。</p><p>但这不是唯一的角色！如果我们采用与这 3 个方向跨越的子空间正交的输入权重分量，并将残差流投影到该方向上，则在预测运动员是否打棒球时，所得部分消融神经元的 ROC AUC (83%) （较之前的 88.7% 略有下降）。</p><h3>这不是单义的</h3><p>一个好奇心是它是否是单一语义的并且在完整的数据分布上代表棒球。尽管我们没有进行详细调查，但这似乎很可能是错误的。在谷歌新闻数据集上，它在类似棒球的环境中系统地激活（也对板球等特定其他运动有所帮助），但在维基百科上，它在一些看似不相关的事物上激活，例如“外部链接” <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10">[10]</a></sup>中的外部和“目标” “足球|进球|守门”</p><h2>哈希和查找证据</h2><h3>动机</h3><p><a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup">如上所述</a>，散列和查找假设是 MLP 2 到 6 分为两个不同的阶段：第一个<strong>散列</strong>，旨在通过形成非线性表示来打破名称的串联（求和）标记的线性结构尝试与每个其他子字符串正交，然后<strong>查找</strong>将棒球运动员的哈希表示映射到棒球，将足球映射到橄榄球等。</p><p>从概念上来说，我们实际上并没有期望这种强形式是正确的：它意味着散列层实际上独立于数据分布，这将是令人惊讶的 - 如果我们采用散列和查找的实现并应用梯度下降的几个步骤可能希望使已知实体的哈希与其他所有事物更为突出，更正交。但是我们希望检验该假设可以教给我们有关该模型的有用内容，并认为这可能部分是正确的。我们将<strong>部分哈希和查找</strong>假设称为该机制主要是散布和外观的假设，但是早期的哈希层包含有关该运动的一些（可恢复的）信息，这些信息会被后来的查找层显着增强。我们的证据广泛支持这一假设，但不幸的是，很难伪造。</p><p>这是通过看到单步贬义假设的失败而引发的：似乎很明显，MLP间组合正在进行，有中间术语，并且有一些明确的查找（棒球神经元）。这似乎是最简单的假设，即模型为什么需要中间术语并涉及实际有目的的组成 - 令牌的线性结构是不可取的！</p><h3>中间层具有线性可回收的运动信息（负）</h3><p><strong>预测</strong>：在哈希层期间，训练有素的线性探针在残留层上检测运动员运动的训练不会比随机的更好。它只会在查找层中变得好。我们不知道哪些层是哈希与查找，但这可以预测急剧的过渡。</p><p><strong>实验</strong>：在我们的有效模型中取两个令牌运动员名称，在每一层之后进行残留流，在80％名称的火车集上训练逻辑回归探针，然后对其他20％进行评估。该假设预测验证精度将发生急剧变化。</p><p><strong>结果</strong>：这是一个适度平稳的变化。为了鲁棒性，我们还会检查有效模型预测下一项运动时的损失指标。当在完整模型中的最终名称令牌上训练逻辑回归探针时，我们会获得类似的结果。这相当简单地反驳了以下假设：早期层正在做纯粹，独立的，哈希。但是，第4层和第5层之间有显着的增加，这表明查找有些专业化（部分是部分但并非由第5层中的棒球神经元完全驱动）。对于每一层，我们都会报告10个随机种子的测试准确性（每次服用不同的80/20火车/测试拆分并训练新的探针），因为数据集足够小，以使其相当嘈杂。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><p><strong>细微差别</strong>：</p><ul><li>一些运动员的名字可能具有独特的令牌，因此体育信息在嵌入中代表。我们可以看到，求和令牌的验证精度比随机的验证精度更好（50％而不是33％）。这并不奇怪，我们希望哈希和查找对其他运动员更重要。</li><li>这与部分散布的假设是一致的，尤其是因为准确性在第5层中显着提升。</li></ul><h3>已知名称的MLP输出标准比未知名称（负）更高</h3><p><strong>预测</strong>：哈希预测，早期层不会在了解数据分布的知识中烘烤，因此应将已知和未知的名称视为不明显。</p><p><strong>实验</strong>：我们测量已知名称和未知名称的MLP输出规范。为了获取名字，我们在运动员数据集中拿出了所有单个令牌和姓氏的笛卡尔产品，并分离了已知和未知名称<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11">[11]</a></sup> 。该分析是在完整模型上进行的（但在有效模型上相似）</p><p><strong>结果</strong>：有明显的差异，已知名称具有更高的规范。这伪造了纯粹的哈希，但不是部分哈希。尽管MLP1不是我们有效模型的一部分，但即使在MLP1中也会发生这种情况<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12">[12]</a></sup> 。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" alt=""></p><h3>早期层确实破裂线性结构（正）</h3><p><strong>预测</strong>：早期层断裂线性结构。具体而言，即使在残差流中存在线性结构，即这是来自不同特征（当前和以前的令牌）的术语之和，MLP输出也不具有此线性结构。更弱的是，它可以预测，一旦添加MLP输出，残差流将失去该线性结构。</p><p>线性函数F的具体属性是F（Michael Jordan） + F（Tim Duncan）= F（Michael Duncan） + F（Tim Jordan），所以让我们尝试伪造这一点！</p><p><strong>实验</strong>：</p><ul><li>我们在有效模型（例如MLP 2）中选择了一对已知名称A和B（例如Michael Jordan和Tim Duncan）和MLP层。<ul><li>我们将MLP输出的中点（MLP（a） + MLP（b）） /2进行。</li></ul></li><li>我们将姓氏交换为获得名称C和D（未知名称，例如Michael Duncan和Tim Jordan），并在C和D（MLP（MLP（C） + MLP（D）） /2上获取MLP输出的中点。</li><li>我们测量两个中点之间的距离。</li><li>为了使一个大v少数数字化，我们除以基线距离，该距离是通过用任意的未知名称代替C和D计算的，并测量中点之间的距离|（（MLP（a） + MLP（b） -  MLP（C &#39;） -  MLP（D&#39;））/2 |<ul><li>这意味着，如果MLP完全打破了线性结构，它将接近1（即Michael Duncan和Tim Jordan与随机未知名称无法区分），而如果保留线性结构将接近0（因为这些将会接近0（因为这些将会是平行四边形的四个顶点）<ul><li>具体而言，如果MLP是线性的，则MLP（Michael Jordan）= MLP（Michael X） + MLP（Y Jordan），因此A＆B和C＆D的中点应该相同</li></ul></li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" alt=""></p><p><strong>结果</strong>：大多数MLP层都表明线性结构显着（但未完全），往往是完全破坏线性结构的60％-70％。对于MLP2，它比MLP3至6稍少。</p><p>我们在不同的层（而不是MLP输出）之后重复上述残差流的实验，该实验在同一图中绘制（红色框），并看到整个层的残差较少线性，从第2层后约30％。第6层后至50％（这是层<em>末端</em>的残差）。请注意，这些残差取自有效模型，该模型从第2层而不是第0层开始。进一步请注意，在有效模型中，对MLP2的输入是名称的求和令牌，根据定义，这是线性的。</p><p><strong>细微差别</strong>：</p><ul><li> MLP输出的结果不足为奇 -  MLP的全部点是非线性函数，因此它当然会破坏线性结构！<ul><li>我们应该期望随机初始化的MLP是正确的</li><li>但是，事实证明，随机初始化的MLP断裂线性结构明显较小（20-40％）！我们进行了一个后续实验，在该实验中，我们将MLP的重量和偏见随机洗牌并重新开发模型。作为另一个基准，我们重新划分了交换未知名称的第一个/姓氏的实验，并且没有明显的更改。这表明该模型故意使用MLP层来破坏线性结构。 </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" alt=""></p><ul><li>它破坏残差流中线性结构的结果较小，但仍然不足为奇 -  new_residual是old_isidual（linear） + mlp_out（不是线性），因此线性new_residual的直觉如何依赖于相对尺寸。</li><li>总体而言，这毫不奇怪地证明了哈希（从线性结构的破坏意义上）发生，但没有证据表明哈希是他们所做的<em>一切</em>，因此，这不是充分的哈希和外观假设的强有力的证据（其中哈希是哈希是一个唯一的角色早期MLP在电路中扮演）</li><li>尽管从概念上讲并不令人惊讶，但我们认为“<em>一般的</em>MLP断裂线性结构”是一个有价值的事实，可以了解模型，因为这表明线性代表的特征之间的干扰会累积在深度上。<ul><li>例如，Bricken等人观察到许多稀疏的自动编码器功能，例如“代币”中的“数学文本”。如果“是数学文本”和“是令牌&#39;&#39;”都是线性表示的特征，那么即使使用此功能没有进行实际计算，MLP层代表交叉点也就不足为奇了。</li></ul></li></ul><h3>棒球神经元在剩余的运动员残留物（模棱两可）上工作</h3><p><em>META：本节记录了我们一个人最初对一个实验感到兴奋，但后来意识到这可能是虚幻的，我们在这里描述了这是出于教学目的</em></p><p><strong>预测</strong>：如果查找正在发生，这表明每个运动员的代表都有<em>特殊</em>信息 - 迈克尔·乔丹（Michael Jordan）残留物中有一些“迈克尔·乔丹”的信息，对于模特最终产生“打篮球”的模特很重要，无法从其他篮球中回收玩家。请注意，当将原始令牌求和时，这显然会发生，但可能不会以后进行。我们专注于<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_">第5层中的棒球神经元</a>，这似乎是查找电路的一部分，因为它具有显着效果并直接增强了棒球属性。</p><p>对比的假设是，早期层（例如2-4）在棒球比赛中产生了一些“打棒球”的代表（可能与最终表示不同），而棒球神经元只是信号提示了这一点。</p><p><strong>实验</strong>：为了测试这一点，我们将每个运动员的残留物带到了所有其他运动员残留物所跨越的子空间的组件正交（请注意，还有2560个残留尺寸和其他1500名其他运动员，因此消除了60％的维度）。然后，我们将棒球神经元应用于该残留的正交组件，并查看了神经元输出的Roc AUC，以预测运动员是否打棒球的二进制变量</p><p><strong>结果</strong>：ROC约为60％，显着高于机会（50％） - 它比没有正交投影的情况明显差，但仍然有一些信号</p><p><strong>NUANCE</strong> ：事实证明这是因为“与所有其他运动员正交的项目”的原因不一定会删除与其他运动员共享的<em>所有</em>信息。玩具示例：假设每个棒球运动员残留都是“棒球”方向，加上高斯噪音。如果我们从此分布中获取由1500个样本跨越的子空间，因为每个样本都是嘈杂的，那么“ IS棒球”方向可能不会在此子空间中完全捕获，因此投影不会删除它。这意味着，尽管我<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13">[13]</a></sup>发现该实验的结果令人惊讶，但它并不能很好地区分这两个假设 - 部分哈希和查找确实很难伪造！ </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-KGJJcrC8izPbNFCPL-1" class="footnote-item"><p> Gelus与Relu不同，但我们认为可以有效地将其视为“软依赖”，并且与Relu相当近似，因此也可以很好地实现An和Gate <a href="#fnref-KGJJcrC8izPbNFCPL-1" class="footnote-backref">。</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-2" class="footnote-item"><p>请注意，这是在哈希函数的句子中，而不是哈希表的句子中。哈希函数采用任意输入，并试图产生与随机不同的输出。哈希表将哈希函数应用于输入，<em>然后</em>有目的地将其映射到一些存储的数据，这与完整的哈希和查找更类似。 <a href="#fnref-KGJJcrC8izPbNFCPL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-3" class="footnote-item"><p>请注意，可能正在进行的更复杂和较少的基础形式的查找形式可能不容易干扰，实际上我们发现了这个故事凌乱而复杂的<a href="#fnref-KGJJcrC8izPbNFCPL-3" class="footnote-backref">迹象</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-4" class="footnote-item"><p>来自另一位运动员的重新样品消融会获得类似的结果<a href="#fnref-KGJJcrC8izPbNFCPL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-5" class="footnote-item"><p>我们测量每个答案的第一个令牌的日志概率，对于多token答案，延续是在括号中，并且没有明确测试（一旦您拥有第一个令牌，就可以使用BigRams来完成）</p><ul><li>踢篮球运动</li><li>去了北部（卡罗来纳州）上大学</li><li>于1984年被选为NBA</li><li>为芝加哥（公牛）的球队效力</li><li>是夏洛特（黄蜂）的多数所有者</li><li>出演电影的空间（果酱）</li><li>为联盟效力，称为NBA</li><li> 1992年代表美国参加奥运会</li><li>玩了23号</li></ul><a href="#fnref-KGJJcrC8izPbNFCPL-5" class="footnote-backref">↩︎</a></li><li id="fn-KGJJcrC8izPbNFCPL-6" class="footnote-item"><p>在我们数据集中所有1500名运动员中，用最终名称的平均值代替了神经元的价值。 <a href="#fnref-KGJJcrC8izPbNFCPL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-7" class="footnote-item"><p>受Lovis Heindrich和Lucia <a href="#fnref-KGJJcrC8izPbNFCPL-7" class="footnote-backref">Quirke↩︎</a>的启发</p></li><li id="fn-KGJJcrC8izPbNFCPL-8" class="footnote-item"><p>动机：E（A＆B）对应于Michael Jordan，E（〜A＆B）上的激活，对应于Keith Jordan（对于Keith的各种值），E（A＆〜b）对应于Michael Smith（对于各种值史密斯）。神经元的激活通常远非零，因此我们从每个术语中减去这一点，这是由E（〜A＆〜b）项捕获的，这是所有名称的平均名称，没有Michael或Jordan。和（e（a＆b） -  e（〜a＆〜b）） - （e（〜a＆b） -  e（〜a＆〜b）） - （e（a＆〜b） -  e（〜 a＆〜b））= E（a＆b） -  e（〜a＆b） -  e（a＆〜b） + e（〜a＆〜b） <a href="#fnref-KGJJcrC8izPbNFCPL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-9" class="footnote-item"><p>我们之所以选择中位数，是因为有时总或凝胶衍生的非线性效应为负/零，并且中位数使我们忽略这些异常值<a href="#fnref-KGJJcrC8izPbNFCPL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-10" class="footnote-item"><p>尽管我们无法轻易弄清楚这是哪一篇文章，但也许​​是与棒球相关的文章，显示了类似<a href="https://arxiv.org/abs/2310.15154">摘要主题的</a>内容！ <a href="#fnref-KGJJcrC8izPbNFCPL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-11" class="footnote-item"><p>会出现一些错误分类，因为某些名称配对可能是已知的实体。我们通过搜索谷歌名称进行了一些斑点检查，并且不要指望这会对结果产生重大影响<a href="#fnref-KGJJcrC8izPbNFCPL-11" class="footnote-backref">。</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-12" class="footnote-item"><p>我们发现，MLP1似乎与属性提取头的注意（让他们检测到名字是运动员，因此是否完全提取运动）有关），但对于查找运动员的运动者而言并不重要。即对密钥很重要，但并不重要。 <a href="#fnref-KGJJcrC8izPbNFCPL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-13" class="footnote-item"><p>使用单数，因为我的合着者认为这种替代解释一直很<a href="#fnref-KGJJcrC8izPbNFCPL-13" class="footnote-backref">明显</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cw5onxm6uzxpbpsrk/fact-finding-finding-trying-tro-to-to-mechanistical-shist-shist-rastical-clastand-rastand tolly<guid ispermalink="false"> CW5ONXM6UZXPBPSRK</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:05 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Simplifying the Circuit (Post 2)]]></title><description><![CDATA[Published on December 23, 2023 2:45 AM GMT<br/><br/><p><em>这是Google DeepMind机械性可解释性团队<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">对语言模型如何回忆事实的调查</a>中的第二篇文章。这篇文章的重点是蒸馏出事实回忆电路，并建模更标准的机械性解释性研究。这篇文章进入了杂草，<strong>我们建议您从<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">帖子第一</a>开始</strong>，然后根据与您最相关的内容浏览和跳过序列的其余部分。我们假设这篇文章的读者熟悉<a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">此词汇表中</a>列出的机械性解释性技术。</em></p><h2>介绍</h2><p>我们的目标是了解如何在叠加中存储和回忆事实。一个必要的步骤是找到涉及事实召回的狭窄任务，并了解使模型可以完成此任务的高级电路。</p><p>我们专注于回忆起不同运动员参加的运动的狭窄任务。正如<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Why_Facts">文章1</a>所述，我们特别期望有关人们的事实涉及叠加，因为单个名称令牌的嵌入通常不足以确定这项运动，因此该模型必须进行布尔值和名称的不同令牌以识别运动员并查找他们的运动。<a href="https://transformer-circuits.pub/2022/solu/index.html">先前的</a><a href="https://arxiv.org/abs/2305.01610">工作</a>称这种现象为“灭绝”，并表明它涉及早期的MLP层，并使用重大的叠加。</p><p>为什么要专注于运动员的运动，而不是一般的事实召回？我们认为，在机械性解释性中，首先要深入了解现象的狭窄实例通常是有用的，而不是坚持完全笼统。运动员的运动是一项不错的任务，每个属性价值都为我们提供了很多例子，我们的目标是了解至少一个例子，其中叠加被用于事实召回，而不是总体上解释事实召回。我们猜想使用类似的机制来回忆其他类别的事实，但这不是我们工作的重点。</p><h2>设置</h2><p>为了了解事实本地化，我们研究了<a href="https://arxiv.org/abs/2304.01373">Pythia</a> 2.8b的下一个令牌预测和激活，以进行表格的一击提示：</p><p> 1,500名运动员参加棒球，篮球和（美国）足球运动。为了选择这些运动员，我们为模型提供了来自Wikidata的大型运动员的数据集，并为模型在正确的运动中放置超过50％的概率的人过滤了<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-1" id="fnref-s4i3RkmKovG88KHgx-1">[1]</a></sup> 。</p><p>我们选择了Pythia 2.8B，因为它是最小的车型，可以为大量运动员完成此任务。</p><p>我们将提示提示一击，因为这大大改善了模型在任务上的性能<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-2" id="fnref-s4i3RkmKovG88KHgx-2">[2]</a></sup> 。我们为One Shot前缀选择了高尔夫球，因此该模型对所需预测的三项运动之一没有偏见。为简单起见，我们没有在提示上改变一个镜头前缀。</p><h2>我们最终得到的简化电路</h2><p>在通过消融研究详细介绍我们为推导电路进行的消融研究之前，让我们看一下我们最终得到的简化电路： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/zwrgezridkrafez2lmmv" alt=""></p><p>在哪里：</p><ul><li><p> <code>concatenate_tokens</code>执行的操作大致类似于单个令牌嵌入的加权总和，将每个令牌的嵌入在不同的子空间中，该子空间由模型的前两层实现。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-3" id="fnref-s4i3RkmKovG88KHgx-3">[3]</a></sup></p></li><li><p> <code>lookup</code>是一个五层MLP（其层匹配原始模型中的MLP层2至6） <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-4" id="fnref-s4i3RkmKovG88KHgx-4">[4]</a></sup> ，它处理合并的令牌以产生由这些令牌描述的实体的新表示，可以将此表示形式进行线性投影以揭示有关实体的各种属性（包括他们参加的运动）；</p><ul><li>我们注意到，在从第6层到第15层的路径中，似乎有多个重要的MLP层，但我们认为它们主要是信号提高运动的现有线性表示，并且消融它们不会显着影响准确性。</li></ul></li><li><p> <code>extract_sport</code>是一个线性分类器，在<code>lookup</code>中生成的表示形式上执行仿射转换，以提取模型返回的运动逻辑。这是在模型中由几个注意力头（尤其是包括L16H20）实现的，这些头部从最终令牌到最终名称令牌，并直接与无用的层一起组成<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-5" id="fnref-s4i3RkmKovG88KHgx-5">[5]</a></sup> 。在运动员的特殊情况下，名字仅由两个令牌组成（一个名字，一个是姓氏），我们能够进一步简化<code>concatenate_tokens</code>函数的形式： </p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/jaxnaennpshoncgl2x6a" alt=""></p><p>如果<code>embed_first</code>和<code>embed_last</code>实际上是查找表（在模型的词汇中每个令牌中有一个输入），则具有脱节范围（以便编码器可以将“邓肯”与“邓肯”区分开来，从而强化了姓氏的结果。 <code>concatenate_tokens</code>仅与单个令牌（加上位置信息）本身一样线性信息 - 即，它是相关代币序列的密集 /压缩表示，该模型需要在下游通电机中解压缩 /提取线性特征才能使用。 （例如<code>extract_sport</code> ）。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-6" id="fnref-s4i3RkmKovG88KHgx-6">[6]</a></sup></p><p>这与文献中的先前工作相当一致，尤其<a href="https://arxiv.org/pdf/2304.14767.pdf">是Geva等人</a>。我们认为我们狭窄，自下而上的方法可以补充他们更广泛的自上而下方法，以理解电路。我们进一步表明， <code>extract_sport</code>是一个线性图，反映了<a href="https://arxiv.org/abs/2308.09124">Hernandez等人</a>，并且可以从单个注意力头的OV电路来理解。我们认为我们的贡献是在狭窄的领域中以更自下而上和集中的方式来完善现有知识，并且可以更好地理解模型在每个阶段的表示，而不是作为一个实质上的新进步。</p><p>在本文的其余部分中，我们进一步详细描述了我们执行的简化电路的实验。</p><h2>调查1：了解事实提取</h2><p>当模型输出正确的运动令牌以完成提示时，确定运动的最早标记在哪里？先前的工作表明，应尽早确定正确的运动（以运动员的最终名称令牌），并以线性恢复的表示。然后，一个单独的事实提取电路（上面的电路图中的<code>extract_sport</code> ）将从最终名称位置读取运动并输出正确的逻辑以完成提示。</p><p>在本节中，我们描述了我们执行的实验，以验证此图片在现实中是否存在并确定实现此事实提取步骤的电路。</p><h3>哪些节点对输出逻辑贡献最大？</h3><p>我们首先确定模型中哪些节点对最终令牌产生的逻辑的直接影响最大。我们通过单独消除最终令牌位置处的每个MLP层的激活和注意力头输出来做到这一点，并测量对输出逻辑的直接影响。</p><p>具体而言，对于每个干净的提示和我们希望消融的每个节点，我们将从“损坏”提示中对该节点进行激活，并将其修补为沿着该节点连接到模型的under型层的贴片，将其修补为干净提示的激活，为了测量此路径补丁对模型输出的影响。对于损坏的提示，我们将随机为参加不同运动的运动员选择提示。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-7" id="fnref-s4i3RkmKovG88KHgx-7">[7]</a></sup>为了衡量直接效果，我们将在路径修补之前和之后比较清洁提示的运动和损坏的提示运动之间的logit差异。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-8" id="fnref-s4i3RkmKovG88KHgx-8">[8]</a></sup>结果如下： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/m0prshu8u9zcnimo4fr6" alt=""></p><p>这些结果表明，一组相对稀疏的节点对逻辑有任何有意义的影响：</p><ul><li>中层的少数注意力；</li><li>遵循这些注意力头的MLP层。</li></ul><p>其中，注意力头特别有趣：由于我们已经测量了直接效果（不是总体上），因此我们知道这些注意力头的输出直接将最终令牌ligits推向了正确的运动（或远离不正确的运动），而没有需要进一步的后处理。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-9" id="fnref-s4i3RkmKovG88KHgx-9">[9]</a></sup>这强烈表明，无论这些头部都在哪里，这些位置的残留流已经编码每个运动员的运动。</p><h3>这些高效的头部在哪里？ </h3><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/bgqe8cyizbqkxuoxjx4e" alt=""></p><p>在这里，我们在最终令牌的示例中可视化了最终令牌的注意力模式，该提示对最终令牌logits的直接效果最高。我们看到头部大多要么参加最终名称令牌位置，或者失败，请回顾一下“ &lt;bos>;”或“ \ n”的静止位置。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-10" id="fnref-s4i3RkmKovG88KHgx-10">[10]</a></sup> <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-11" id="fnref-s4i3RkmKovG88KHgx-11">[11]</a></sup>从中我们可以得出两件事：</p><ol><li>一项运动员的运动在其最终名字令牌的第16层中大大代表。</li><li>他们的运动的表示应该是可恢复的（因为每个头部的值输入与模型的最终令牌逻辑有关，通过大约线性变换有关）。</li></ol><h3>这些高效的注意力头是从最终名称标记位置阅读什么？</h3><p>为了回答这个问题，我们通过上面列出的每个高效应头的OV电路计算了最终名称位置的节点的特定路径效应。确切地说，对于每个高效应头，我们沿着从该节点到输出logits的路径沿相关头的OV电路<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-12" id="fnref-s4i3RkmKovG88KHgx-12">[ 12]</a></sup> 。实际上，这创建了每个高效应节点的值输入的归因，这是根据输入其中的节点的归因。</p><p>有趣的是，我们发现第二到第六个最重要的头部性能的很大一部分反过来又来自于其价值输入读取<em>最终</em>名称令牌的L16H20的输出。例如，以下是最终名称节点对L21H9（第二重要的头部）对ligits的影响的归因 - 请注意L16H20的输出（在最终名称的位置）对l16h20的贡献（在最终名称的位置）对这个头： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kmyken8pjgofdndy2mre" alt=""></p><p>第三到第六个最重要的头部的热图看起来很相似，它们的效果很大，来自L16H20的最终名称令牌。</p><p>此外，从最终名称的位置参加L16H20的注意力模式，我们发现它通常会在同一位置上。将这些观察结果汇总在一起，我们看到L16H20通过两种<em>独立的</em>机制在该电路中具有很高的重要性：</p><ol><li>它通过其OV巡回赛将运动员运动功能直接传输到最终的令牌位置（从最后的令牌回到最终名称令牌）；</li><li>它从最终<em>名称</em>令牌位置到相同的位置，从而产生了（通过V-Composition）产生的输出，以将运动员运动功能从最终名称令牌转移到最终令牌的其他头部的输出。</li></ol><p> L16H20本身呢 - 最强烈地促进<em>其</em>价值输入？如下图所示，L16H20本身的值输入在很大程度上取决于最终名称位置之前的MLP输出（带有一些v-composition，其中少数早期的头部从最终名称位置到本身）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kdaykbcwns7qhchfi4ya" alt=""></p><h3>简化的子电路，用于提取事实</h3><p>将上述结果放在一起，我们得出结论：</p><ul><li>运动事实提取电路的很大一部分（上图中的<code>extract_sport</code> ）由Head L16H20执行；<ul><li>由于头部的注意力模式在运动员之间没有变化（很多），因此其功能只是线性图，将残留流乘以OV电路。</li></ul></li><li>该负责人的OV电路读取最终名称令状剩余流，并输出内容（既回到相同的残留流，直接回到最终的令牌残留流中），从而使结果无用，从而提高了结果的逻辑&#39;令牌。</li><li>因此<strong>，正确的运动是在最终名称令牌上线性表示的，并由L16H20的OV电路提取</strong>。</li></ul><p>这表明<code>extract_sport</code>我们可以通过以最终名称为“令牌”位置从第16层中替换所有模型的计算图，并使用三级线性探针构建L16H20的OV映射，并使用该模型的“棒球”代币“棒球”构造的三级线性探针，从而近似于第16层的计算图。 ，“篮球”和“足球”。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-13" id="fnref-s4i3RkmKovG88KHgx-13">[13]</a></sup></p><p>简化了这一简化，我们发现整个赛道对运动员运动的分类的准确性从原始型号的100％下降到简化了<code>extract_sport</code>部分的98％，以至于该电路的一部分到了（权重）线性探测器 - 即，我们可以极大地简化。在任务中性能下降的电路部分。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-14" id="fnref-s4i3RkmKovG88KHgx-14">[14]</a></sup> <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-15" id="fnref-s4i3RkmKovG88KHgx-15">[15]</a></sup></p><h3>替代路径：只需训练线性探针</h3><p>上述许多分析围绕许多分析的替代路线就是在最终名称令牌的残余流<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-16" id="fnref-s4i3RkmKovG88KHgx-16">[16]</a></sup>上训练逻辑回归探针[16]，并表明探针获得了良好的测试精度。我们可以进一步表明，探针<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-17" id="fnref-s4i3RkmKovG88KHgx-17">[17]</a></sup>因果关系跨越的子空间中的修补会影响模型的输出，这表明表示形式在下游<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-18" id="fnref-s4i3RkmKovG88KHgx-18">[18]</a></sup> 。这是我们用于项目的很大一部分的方法，然后再返回并在本节早些时候进行分析更加严格。</p><p>我们认为机械探针具有很大的优势（例如，使用L16H20的权重和诱人的探测器来得出探针而不是训练逻辑回归分类器），这是更有原则的（从某种意义上说，我们可以清楚地看到它在以下意义上的意义。该模型的电路），更难过度合适，并且不需要训练集，而训练集则不再可用于进一步分析。但是，“只需训练探测器”使快速移动变得更加容易。</p><p>特别是，对于这项调查，我们的目标是在前几层中放大<code>lookup</code> ，并且知道正确的运动在几层MLP层足以告诉我们有些有趣的东西可以尝试反向工程，逆向工程，即使我们不知道事实提取电路的细节。</p><p>我们认为探针是被低估的电路分析工具，并且在模型中找到可解释的方向/子空间，可以证明以非平凡的方式证明其有意义，启用更简单的电路分析，需要仅考虑一层层的子集，而不是模型的完整端到端行为。</p><p>另一种简单的方法是通过在每个头上迭代，将其ov倍以探测为探测并评估准确性来搜索机械探测器。如果有一个特别高精度的头（包括在正确的位置上），那么您可能会发现一个关键的头部。我们注意到，根据头部的数量，这种方法的危险要大，而不是首先进行直接的logit归因，以缩小到一小部分头部<sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-19" id="fnref-s4i3RkmKovG88KHgx-19">[19]</a></sup> 。</p><h2></h2><h2>调查2：简化de脑和查找</h2><p>在本节中，我们描述了我们执行的实验，以简化由<a href="https://docs.google.com/document/d/1EsIlX7L_xr0YX918NiDWv4Cn3FVS6tJqjrIGR6mnJyQ/edit?resourcekey=0-QoVN8x6k4h6wZCZaWV9qug#heading=h.h6dofjljntk0">上述</a>简化电路图中定义的<code>concatenate_tokens</code>和<code>lookup</code>模块所覆盖的电路部分。总而言之，以下所述的实验确定了有关电路这一部分的以下事实：</p><ul><li>一项运动员的运动在剩余流中的代表比第16层（由L16H20的头部阅读）要早得多。到第6层时，尝试使用<code>extract_sport</code>阅读运动员运动时，我们仍然变得相当不错（90％）。</li><li>在运动员名称之前的上下文对于产生运动代表并不重要（尽管对<code>extract_sport</code>确实很重要）。我们可以在“ &lt;bos>; &lt;BOS>; &lt;运动员 - 富勒>;”形式的提示下使用模型的最终令牌激活，并在准确性下降的情况下提取运动员的运动。</li><li>注意力头超出层0和1。我们可以在不损害<code>lookup</code>模块的性能的情况下向后2刻录注意力层。查看注意力模式，我们看到大多数头部并不特别注意以前的名字令牌，从而加强了消融它们的案例。删除这些注意力头的优势在于， <code>lookup</code>模块因此变成了一个简单的MLP层（带有残留的跳过连接），可以将运动员的嵌入在剩余流中的第2层开始时，并通过运动员的运动输出运动员的运动。残留流中第6层的末端。</li></ul><p>结果，我们可以将电路的这一部分分解为两个子模块：</p><ul><li> <code>concatenate_tokens</code> ，代表模型的层0和1的作用，其角色（对于此任务）是收集运动员的名字令牌，并将其放置在最终名称中，将其放置在“调查3”中，这是纯粹的陪伴，没有查找，至少适用于两个令牌运动员的名字）；</li><li> <code>lookup</code>是一个纯粹的5层MLP（包括原始模型的2-6层），将这种运动员名称的多态表示变成了运动员的功能代表，该运动员以线性回收的方式专门代表了运动员的运动。</li></ul><p>现在，我们描述了支持上面列出的三个主张中每一个的证据。</p><h3>查找大部分是由第6层完成的</h3><p>如果我们将<code>extract_sport</code>探测器应用于最终名称的不同层，我们会发现可以比第16层更早地从残留流中阅读运动员的运动： <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-20" id="fnref-s4i3RkmKovG88KHgx-20">[20]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/pwsurjq0ifiw4ch7xtj2" alt=""></p><p>到第8层大约，精度基本上已经平稳，即使是第6层，我们的精度约为90％。 <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-21" id="fnref-s4i3RkmKovG88KHgx-21">[21]</a></sup></p><h3>查找运动员运动时的背景无关紧要</h3><p>我们已经确定，剩余的流以最终名字令牌编码他们的运动。 But to what extent did the model place sport in the residual stream because it would have done this anyway when seeing the athlete&#39;s name (the multi-token embedding hypothesis) and to what extent did the model place sport in the residual stream because the one-shot prompt preceding the name <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-22" id="fnref-s4i3RkmKovG88KHgx-22">[22]</a></sup> hinted to the model that sport might be a useful attribute to extract?</p><p> Our hypothesis was that the context wouldn&#39;t matter that much – specifically that the model would look up an athlete&#39;s sport when it sees their name, even without any prior context. We tested this by collecting activations for pure name prompts, where the model was fed token sequences of the form “&lt;bos>; <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-23" id="fnref-s4i3RkmKovG88KHgx-23">[23]</a></sup> &lt;first-name>; &lt;last-name>;” <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-24" id="fnref-s4i3RkmKovG88KHgx-24">[24]</a></sup> and the residual stream was harvested from the final name token.</p><p> Can the <code>extract_sport</code> module read athletes&#39; sports from these activations? As the plot below shows, we found that there is a little drop in performance without the one-shot context, but it&#39;s still possible to fairly accurately read an athlete&#39;s sport purely from an early layer encoding of just their name prepended by “&lt;bos>;”, without any additional context. Hence, we can simplify the overall circuit by deleting all edges from tokens preceding the athlete&#39;s name tokens in the full prompt for the task. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/x4wdqduttl0ovdowk9yi" alt=""></p><h3> Attention heads don&#39;t matter beyond layer 2</h3><p> In order to recall sport accurately, the <code>lookup</code> part of the circuit must in general be a function of most (if not all) of the tokens in an athlete&#39;s name: for most athletes, it&#39;s not possible to determine sport by just knowing the last token in their surname. Hence, attention heads must play some role in bringing together information distributed over the individual tokens of an athlete&#39;s name in order that facts like sport can be accurately looked up.</p><p> However, how do these two processes – combining tokens and looking up facts – relate to each other?</p><ol><li> They could happen concurrently – with attention bringing in relevant information from earlier tokens as and when it is required for the lookup process;</li><li> Or the processes could happen sequentially, with the tokens making up an athlete&#39;s name being brought together first, and much of the lookup process only happening afterwards. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-25" id="fnref-s4i3RkmKovG88KHgx-25">[25]</a></sup> Looking at the total effects of patching attention head outputs at the final name token position, we did find that there are many more heads that play a significant role in the overall circuit in layers 0 and 1 of the model than in later layers : </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/s8xbfgfrakemknheueil" alt=""></p><p> This suggested that we might be able to remove the attention head outputs for layer 2 onwards without too much impact on the overall circuit&#39;s performance. Trying this, we found that mean ablating attention outputs from layer 2 onwards had only a slightly detrimental impact on accuracy: <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-26" id="fnref-s4i3RkmKovG88KHgx-26">[26]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xc9un39ztbdfeaxxh1h4" alt=""></p><p> This supports the two-stage hypothesis described above: information sharing between tokens (via attention) is largely complete by layer 2, with attention heads in later layers unimportant for lookup.</p><h3> A simplified subcircuit for fact lookup</h3><p> The results above suggest that we can indeed split the process of looking up an athlete&#39;s sport into two stages:</p><ul><li><p> <code>concatenate_tokens</code> , which is the result of the embedding and layers 0 and 1 of the model processing the tokens comprising the athlete&#39;s name <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-27" id="fnref-s4i3RkmKovG88KHgx-27">[27]</a></sup> and producing a “concatenated token” representation at the end of layer 1 in the final name token residual stream;</p></li><li><p> <code>lookup</code> , which is a pure MLP with skip connections (made up from MLP layers 2 onwards in the model), which processes the “concatenated token representation” after layer 1 of the residual stream, which in itself poorly represents the athlete&#39;s sport (in a linear manner), and produces a “feature representation” later on in the residual stream, where sport can be easily linearly extracted (by <code>sport_extract</code> ).</p></li></ul><p> Note that there are two simplifications we have combined here:</p><ul><li> Processing the athlete name tokens on their own without a one-shot prompt (because we found context to be unimportant)</li><li> Removing attention heads from layer 2 onwards (because we found information transmission between tokens to largely be complete by layer 2).</li></ul><p> Since, each of these approximations has some detrimental effect on the circuit&#39;s accuracy, it&#39;s worth assessing their combined impact. Here&#39;s a plot showing how combining these approximations impacts accuracy: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xkhlsj0ktdmm3kipcbll" alt=""></p><p> The upshot is that, even applying both simplifications together, it&#39;s possible to get up to 94% accuracy by including enough layers in the <code>lookup</code> MLP; even stopping at layer 6 gets you 85% accuracy.</p><h2></h2><h2> Investigation 3: Further simplifying the token concatenation circuit</h2><p> So far, we have:</p><ul><li> Simplified <code>extract_sport</code> to a 3-class linear probe, whose weights are derived by composing the OV map for L16H20 with the model&#39;s unembedding weights;</li><li> Simplified <code>lookup</code> to be a pure 5 layer MLP with skip connections, whose layer weights correspond to those of MLP layers 2–6 in the original model;</li><li> Established that prior context can be removed when calculating the residual stream&#39;s value at the beginning of layer 2 at the final name token position (which is the input for <code>lookup</code> ).</li></ul><p> This leaves us with <code>concatenate_tokens</code> , comprising the embedding and layers 0 and 1 of the model, which converts the raw athlete name tokens (plus a prepending &lt;bos>; token) into the value of the residual stream at the beginning of layer 2. Can we simplify this part of the circuit further?</p><p> There are two levels of simplification we identified for this components of the circuit:</p><ul><li> In general, we can think of <code>concatenate_tokens</code> as effectively generating <em>two</em> separate token-level embeddings of the athlete&#39;s name, and then combining these embeddings approximately linearly via pure attention operations;</li><li> In the case of athletes whose names are just two tokens long, we can further approximate <code>concatenate_tokens</code> so that it is literally a sum of effective token embeddings for first and last name tokens, taking only a modest hit to circuit accuracy.</li></ul><p> In the following subsections, we explain these simplifications in more detail and provide experimental justifications for them.</p><h3> Token concatenation is achieved by attention heads moving token embeddings</h3><p> The first simplification comes from the following two observations:</p><ul><li><p> MLP layer 1 at the final name token position has little importance for circuit performance: resample ablating it has low total effect on logit difference for the original model, and mean ablating it has little impact on the accuracy of the simplified circuit. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-28" id="fnref-s4i3RkmKovG88KHgx-28">[28]</a></sup></p></li><li><p> Due to Pythia using parallel attention, MLP layer 0 is effectively a secondary token embedding layer. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-29" id="fnref-s4i3RkmKovG88KHgx-29">[29]</a></sup> This implies that (after mean ablating MLP 1), <code>concatenate_tokens</code> effectively performs the following operations:</p></li></ul><ol><li> Calculate primary token embeddings for the athlete name tokens (and &lt;bos>;) using the model&#39;s embedding layer weights.</li><li> Calculate secondary token embeddings for the athlete name tokens using the embedding weights induced by the action of MLP 0 on the input token vocabulary.</li><li> Operate the attention layer 0 heads on the primary token embeddings.</li><li> Operate the attention layer 1 heads on the sum of the primary token embeddings, secondary token embeddings and outputs of attention layer 0.</li><li> Use the result of step 4 at the final name token position as the input to <code>lookup</code> .</li></ol><p> In other words, <code>concatenate_tokens</code> effectively embeds the input tokens (twice) and moves them (directly and indirectly) to the final name token position via attention.</p><h3> For two-token athletes, <code>concatenate_tokens</code> literally adds first and last name tokens together</h3><p> For two-token athletes, we found that we could furthermore freeze attention patterns and still retain reasonable accuracy on the task.具体来说：</p><ul><li> We prevented attention heads in layer 1 at the last name token position attending to the same position;</li><li> We froze all other attention patterns at their average levels across all two token athletes in the dataset, making the attention layers just act as a linear map mapping source token residuals to the final token.</li></ul><p> These simplifications, along with mean ablating MLP 1, turn <code>concatenate_tokens</code> into a sum of effective token embeddings and a bias term (originating from the embeddings for the &lt;bos>; token). The effective token embedding of the last name is just the sum of the primary and second (MLP0) token embedding. The effective token embedding of the first name is more complex, it&#39;s the primary token embedding times the linear map from frozen attention 0 heads (their OV matrices weighted by the average attention from the last name to the first name), plus the primary and secondary token embeddings times the linear map from frozen attention 1 heads.</p><p> The impact of these simplifications on accuracy are shown in the graph below. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-30" id="fnref-s4i3RkmKovG88KHgx-30">[30]</a></sup> We see that, for two-token athletes, freezing attention patterns has little additional impact on accuracy over ablating MLP 1. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kinnfxarccubfccdl2g3" alt=""></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-s4i3RkmKovG88KHgx-1" class="footnote-item"><p> Note that we started with several thousand athletes, so this filtering likely introduced some bias. Eg if there were a thousand athletes the model did not know but guessed a random sport for, we would select the 333 where the model got lucky. We set the 50% confidence (on the full vocab) threshold to reduce this effect. <a href="#fnref-s4i3RkmKovG88KHgx-1" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-2" class="footnote-item"><p> Our guess is that few shot vs zero shot does not materially affect the lookup circuit, but rather the one shot prompt tells the model that the output is a sport and boosts all sport logits (suggested by the results of Chughtai et al (forthcoming)) 。 Anecdotally, zero shot, the model puts significant weight on “his/her” as an output, though Pythia 1.4B does not! <a href="#fnref-s4i3RkmKovG88KHgx-2" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-3" class="footnote-item"><p> Note that the linearly recoverable features in the output of <code>concatenate_tokens</code> will end up being something like the union of the features in these individual token embeddings – ie sport is not particularly linearly recoverable from this output. It&#39;s best to think of the output of <code>concatenate_tokens</code> as a “concatenation of individual token embeddings that saves position information” so that the concatenation can be processed together by a series of MLP layers. <a href="#fnref-s4i3RkmKovG88KHgx-3" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-4" class="footnote-item"><p> We could have chosen pretty much any layer between 4 and 15 here as our endpoint, as faithfulness of our simplified circuit increased fairly continuously as we included additional layers. However, there is an inflection point around layer 5, after which you start seeing diminishing returns. We believe that the MLP layers after MLP 6 are just boosting the attributes in the residual stream rather than looking it up from the raw tokens. <a href="#fnref-s4i3RkmKovG88KHgx-4" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-5" class="footnote-item"><p> When we refer to the “linear classifier” we are referring to the composition of the OV circuit of these heads and the unembedding matrix. The heads always attend from the final token to the final name position, so purely act as a linear map. <a href="#fnref-s4i3RkmKovG88KHgx-5" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-6" class="footnote-item"><p> We suspect it may be possible to express <code>concatenate_tokens</code> as a similar sum of position-dependent token embeddings even for athletes with three or more tokens in their name, but we didn&#39;t pursue this line of investigation further. <a href="#fnref-s4i3RkmKovG88KHgx-6" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-7" class="footnote-item"><p> Eg where the clean prompt is for Tim Duncan (who plays basketball), we might patch in activations from the prompt for George Brett (who plays baseball) or Andy Dalton (who plays football). <a href="#fnref-s4i3RkmKovG88KHgx-7" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-8" class="footnote-item"><p> This metric has the nice property of being linear in logits, while also being invariant to constant shifts across all logits. <a href="#fnref-s4i3RkmKovG88KHgx-8" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-9" class="footnote-item"><p> To clarify, the not insignificant total effects of later MLP layers suggests that some post-processing is going on - the point we&#39;re making is that even without this post processing, the outputs of these attention heads can directly be interpreted in terms of sport token logits, hence these attention heads are already writing the correct sport into the residual stream. <a href="#fnref-s4i3RkmKovG88KHgx-9" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-10" class="footnote-item"><p> Interestingly, because the prompts are sorted in terms of sport, we see that some heads only seem to be used for a subset of the sports in the task. Breaking down these head&#39;s direct effects by sport confirms this picture: L19H24 is only consistently important for baseball players, whereas L22H17 is only consistently important for basketball, and some fraction of baseball players. (We didn&#39;t try to understand what differentiates those athletes that this head is important for versus those that it isn&#39;t important for.) This selectivity among attention heads is <a href="https://arxiv.org/abs/2307.09458">not entirely surprising</a> , and we did not investigate this further as it&#39;s not directly relevant to our current investigation. <a href="#fnref-s4i3RkmKovG88KHgx-10" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-11" class="footnote-item"><p> As usual in mechanistic interpretability, this is just an approximate picture - looking at the plots, it&#39;s clear that some heads do non-trivially attend to tokens between the athlete&#39;s name and the final token, particularly to “ plays” and “ sport” and sometimes “ golf”. However, this doesn&#39;t change the overall conclusion that the residual stream at an athlete&#39;s name token already contains their sport. <a href="#fnref-s4i3RkmKovG88KHgx-11" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-12" class="footnote-item"><p> We note that this approach requires a separate forward pass for each residual stream component on the final name token, <em>and</em> for each high-effect mediating head. This was not a bottleneck on our work, so we did proper path patching, but we note that this can be easily approximated with direct logit attribution. If we freeze the LayerNorm scale, then the OV circuit of each head performs a linear map on the final name token residual stream, and the unembedding is a further linear map, and so we can efficiently see the change in the final logits caused by each final name token residual component. We found this technique useful for rapid iteration during this work. <a href="#fnref-s4i3RkmKovG88KHgx-12" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-13" class="footnote-item"><p> We also need to set the bias for the probe. We do this by subtracting the mean activations at the probe&#39;s input (ie by effectively centering the probe&#39;s input before applying the weight matrix). <a href="#fnref-s4i3RkmKovG88KHgx-13" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-14" class="footnote-item"><p> This difference in performance is at least in part because we always probe the final name token position whereas L16H20 does for some athletes attend to other positions (eg the penultimate token) in their name. We conjecture this is because some athletes are fully identified before the final token in their name has appeared (eg from four tokens of a five token name), and so fact lookup occurs before this final token. <a href="#fnref-s4i3RkmKovG88KHgx-14" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-15" class="footnote-item"><p> We deliberately measure accuracy rather than loss recovered because we expect the later high-effect heads are mostly signal boosting the output of L16H20, even though <a href="https://arxiv.org/abs/2309.16042">loss recovered would normally be our preferred metric</a> . Signal boosting improves loss but does not change accuracy, and to understand factual recall it suffices to understand how high accuracy is achieved. <a href="#fnref-s4i3RkmKovG88KHgx-15" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-16" class="footnote-item"><p> In this case we already guessed the final name token would be the right token to probe for sport based on eg residual stream patching + prior work, but it&#39;s easy enough to sweep over tokens and layers and train a probe on all of them, probes are pretty cheap to train <a href="#fnref-s4i3RkmKovG88KHgx-16" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-17" class="footnote-item"><p> <a href="https://arxiv.org/abs/2311.17030">Makelov et al</a> recently showed that subspace activation patching can misleadingly activate dormant parallel pathways, but this is mostly a concern when using gradient descent to learn a subspace with causal effect, probes are correlational so this is not an issue here. <a href="#fnref-s4i3RkmKovG88KHgx-17" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-18" class="footnote-item"><p> Because the probe is linear, it&#39;s a bit unclear if you should care whether the probe is causally used. The model&#39;s ability to map individual tokens of an athlete&#39;s name to a linear representation of sport is an interesting algorithm to reverse-engineer and likely involves superposition, even if for some weird coincidence a parallel algorithm is the main thing affecting the model outputs. But this is a pretty contrived situation and it&#39;s easy to check. <a href="#fnref-s4i3RkmKovG88KHgx-18" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-19" class="footnote-item"><p> In particular, in some settings, the probe may be very natural. Eg many attention heads just copy whatever token they attend to to the output. So being a good mechanistic probe when probing for the input token is weak evidence that a head is involved, but likely still finds you a pretty good probe. <a href="#fnref-s4i3RkmKovG88KHgx-19" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-20" class="footnote-item"><p> When applying this probe to other layers, we&#39;d always mean centre relative to the residual stream activations for that layer. This is equivalent to mean ablating the MLP and attention outputs between the layer being probed and layer 16. <a href="#fnref-s4i3RkmKovG88KHgx-20" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-21" class="footnote-item"><p> A question naturally arises here: why did the L16H20 value input attribution (presented earlier) show that MLPs 8, 11 and 13 in particular have a high path-specific effect on determining the correct sport token, when the probe shows that you can pretty much read the athlete&#39;s sport much earlier on? Our hypothesis is that these later MLPs are boosting the signal generated by early MLPs, rather than looking up facts by themselves. <a href="#fnref-s4i3RkmKovG88KHgx-21" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-22" class="footnote-item"><p> Ie “Fact: Tiger Woods plays the sport of golf” <a href="#fnref-s4i3RkmKovG88KHgx-22" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-23" class="footnote-item"><p> In the weeds: The Pythia models were not trained with a BOS (beginning of sequence) token, but we anecdotally find that the model is better behaved when doing inference with one. Models often have extremely high norm on the first token of the context, and treat it unusually, which makes it hard to study short prompts like “George Brett”. Pythia&#39;s BOS and EOS token are the same, and it was trained with EOS tokens separating documents in pre-training (and the model was able to attend between separate documents), so it will have seen a BOS token in this kind of role during training <a href="#fnref-s4i3RkmKovG88KHgx-23" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-24" class="footnote-item"><p> Eg “&lt;bos>; George Brett” <a href="#fnref-s4i3RkmKovG88KHgx-24" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-25" class="footnote-item"><p> NB we know it can&#39;t be possible to completely separate lookup from token concatenation, because even the final token embedding (prior to any processing by the model) often has some notion of an athlete&#39;s sport. Instead, we&#39;re making the weaker hypothesis here that much of the additional accuracy of the <code>lookup</code> circuit (beyond guessing sport from the final token) happens after the athlete&#39;s name tokens have first been assembled together. <a href="#fnref-s4i3RkmKovG88KHgx-25" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-26" class="footnote-item"><p> We also checked (and confirmed) that ablating attention layers 0 or 1 had a catastrophic impact on sport lookup. <a href="#fnref-s4i3RkmKovG88KHgx-26" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-27" class="footnote-item"><p> With a preceding &lt;bos>; token, to be precise. <a href="#fnref-s4i3RkmKovG88KHgx-27" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-28" class="footnote-item"><p> Probe accuracy, with the simplified <code>lookup</code> and <code>extract_sport</code> circuits described in previous sections, drops from 85% to 81% after layer 6 and drops from 94% to 90% after layer 15, when we mean ablate MLP 1. <a href="#fnref-s4i3RkmKovG88KHgx-28" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-29" class="footnote-item"><p> Due to parallel attention, the input to MLP0 at any position is just the token embedding at that position. Hence, we could literally replace MLP0 by a second table of token embeddings (mapping original token IDs to their corresponding MLP0 output values) without affecting model outputs at all. <a href="#fnref-s4i3RkmKovG88KHgx-29" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-30" class="footnote-item"><p> Note that this plot is not directly comparable with preceding plots, as it measures accuracy only over two-token athletes. <a href="#fnref-s4i3RkmKovG88KHgx-30" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2<guid ispermalink="false"> 3tqJ65kuTkBh8wrRH</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:45:49 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1)]]></title><description><![CDATA[Published on December 23, 2023 2:44 AM GMT<br/><br/><p> <em>This is a write up of the Google DeepMind mechanistic interpretability team&#39;s investigation of how language models represent facts. This is <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">a sequence of 5 posts</a> , we recommend prioritising reading post 1, and thinking of it as the “main body” of our paper, and posts 2 to 5 as a series of appendices to be skimmed or dipped into in any order.</em></p><h2>执行摘要</h2><p>Reverse-engineering circuits with superposition is a major unsolved problem in mechanistic interpretability: models use clever compression schemes to represent more features than they have dimensions/neurons, in a highly distributed and polysemantic way. Most existing mech interp work exploits the fact that certain circuits involve sparse sets of model components (eg a sparse set of heads), and we don&#39;t know how to deal with distributed circuits, which especially holds back understanding of MLP layers. Our goal was to interpret a concrete case study of a distributed circuit, so we investigated how MLP layers implement a lookup table for factual recall: namely how early MLPs in Pythia 2.8B look up which of 3 different sports various athletes play. <strong>We consider ourselves to have fallen short of the main goal of a mechanistic understanding of computation in superposition</strong> , but did make some meaningful progress, including conceptual progress on why this was hard.</p><p> Our overall best guess is that <strong>an important role of early MLPs is to act as a “multi-token embedding”, that selects</strong> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-1" id="fnref-aYS73ikuT9JzFD3Mj-1">[1]</a></sup> <strong>the right unit of analysis from the most recent few tokens (eg a name) and converts this to a representation</strong> (ie some useful meaning encoded in an activation). We can recover different attributes of that unit (eg sport played) by taking linear projections, ie <strong>there are linear representations of attributes</strong> . Though we can&#39;t rule it out, our guess is that there isn&#39;t much more interpretable structure (eg sparsity or meaningful intermediate representations) to find in the internal mechanisms/parameters of these layers. For future mech interp work we think it likely suffices to focus on understanding how these attributes are represented in these multi-token embeddings (ie early-mid residual streams on a multi-token entity), using tools like probing and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">sparse autoencoders</a> , and thinking of early MLPs similar to how we think of the token embeddings, where the embeddings produced may have structure (eg a “has space” or “positive sentiment” feature), but the internal mechanism is just a look-up table with no structure to解释。 Nonetheless, we consider this a downwards update on more ambitious forms of mech interp that hinge on fully reverse-engineering a model. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" alt=""></p><p> <strong>Our main contributions:</strong></p><p> <a href="https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2"><strong>Post 2:</strong> Simplifying the circuit</a></p><ul><li> We do a deep dive into the specific circuit of how Pythia 2.8B recalls the sports of different athletes. Mirroring <a href="https://arxiv.org/abs/2304.14767">prior work</a> , we show that the circuit breaks down into 3 stages:<ul><li> <strong>Token concatenation</strong> : Attention layers 0 and 1 assemble the tokens of the athlete&#39;s name on the final name token, as a sum of multiple different representations, one for each token.</li><li> <strong>Fact lookup</strong> : MLPs 2 to 6 on the final name token map the concatenated tokens to a linear representation of the athlete&#39;s sport - the multi-token embedding.<ul><li> Notably, this just depends on the name, not on the prior context.</li></ul></li><li> <strong>Attribute extraction</strong> : A sparse set of mid to late attention heads extract the sport subspace and move it to the final token, and directly connect with the output. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/qv8qfrczagg388exbycd" alt=""></p><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk"><strong>Post 3:</strong> Mechanistically understanding early MLP layers</a></p><ul><li> We zoom in on mechanistically understanding how MLPs 2 to 6 actually map the concatenated tokens to a linear representation of the sport.</li><li> We explore evidence for and against different hypotheses of what&#39;s going on mechanistically in these MLP layers.</li><li> We falsify the naive hypothesis that there&#39;s just a single step of detokenization, where the GELUs in specific neurons implement a Boolean AND on the raw tokens and directly output all known facts about the athlete.</li><li> Our best guess for part of what&#39;s going on is what we term the <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Hash_and_Lookup">“hash and lookup” hypothesis</a> , but it&#39;s messy, false in its strongest form and this is only part of the story. We give some evidence for and against the hypothesis.</li></ul><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4"><strong>Post 4:</strong> How to think about interpreting memorisation</a></p><ul><li> We take a step back to compare differences between MLPs that perform generalisation tasks and those that perform memorisation tasks (like fact recall) on toy datasets.</li><li> We consider what representations are available at the inputs and within intermediate states of a network looking up memorised data and argue that (to the extent a task is accomplished by memorisation rather than generalisation) there is little reason to expect meaningful intermediate representations during pure lookup.</li></ul><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR"><strong>Post 5</strong> : Do Early Layers Specialise in Local Processing?</a></p><ul><li> We explore a stronger and somewhat tangential hypothesis that, <em>in general</em> , early layers specialise in processing recent tokens and only mid layers integrate the prior context.</li><li> We find that this is somewhat but not perfectly true: early layers specialise in processing the local context, but that truncating the context still loses something, and that this effect gradually drops off between layers 0 and 10 (of 32).</li></ul><p> We also exhibit a range of miscellaneous observations that we hope are useful to people building on this work.</p><h2>动机</h2><h3>Why Superposition</h3><p> Superposition is really annoying, really important, and terribly understood. Superposition is the phenomenon, studied most notably in <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a> , where models represent more features than they have neurons/dimensions <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-2" id="fnref-aYS73ikuT9JzFD3Mj-2">[2]</a></sup> . We consider understanding how to reverse-engineer circuits in superposition to be one of the major open problems in mechanistic interpretability.</p><p> Why is it so annoying? Sometimes a circuit in a model is <strong>sparse</strong> , it only involves a small handful of components (heads, neurons, layers, etc) in the standard basis, and sometimes it&#39;s <strong>distributed</strong> , involving many components. Approximately all existing mech interp works exploit sparsity, eg a standard workflow is identifying an important component (starting with the output logits), identifying the most important things composing with that, and recursing. And we don&#39;t really know how to deal with distributed circuits, but these often happen (especially in non-cherry picked settings!)</p><p>为什么它如此重要？ Because this seems to happen a bunch, and mech interp probably can&#39;t get away with never understanding things involving superposition. We personally speculate that superposition is a key part of <em>why</em> scaling LLMs works so well <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-3" id="fnref-aYS73ikuT9JzFD3Mj-3">[3]</a></sup> , intuitively, the number of facts known by GPT-4 vs GPT-3.5 scales superlinearly in the number of neurons, let alone the residual stream dimension. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-4" id="fnref-aYS73ikuT9JzFD3Mj-4">[4]</a></sup></p><p> Why is it poorly understood? There&#39;s been fairly limited work on superposition so far. Toy Models of Superposition is a fantastic paper, but is purely looking at toy models, which can be highly misleading - your insights are only as good as your toy model is faithful to the underlying reality <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-5" id="fnref-aYS73ikuT9JzFD3Mj-5">[5]</a></sup> . Neurons In A Haystack is the main paper studying superposition in real LLMs, and found evidence they matter significantly for detokenization (converting raw tokens into concepts), but fell short of a real mechanistic analysis on the neuron level.</p><p> This makes it hard to even define exactly what we mean when we say “our goal in this work was to study superposition in a real language model”. Our best operationalisation is that we tried to study a circuit that we expected to be highly distributed yet purposeful, and where we expected compression to happen. And a decent part of our goal was to better deconfuse what we&#39;re even talking about when we say superposition in real models. See <a href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation">Toy Models of Superposition</a> and <a href="https://arxiv.org/pdf/2305.01610.pdf#page=17">Appendix A of Neurons In A Haystack</a> for valuable previous discussion and articulation of some of these underlying concepts.</p><h3> Circuit Focused vs Representation Focused Interpretability</h3><p> To a first approximation, interpretability work often focuses either on <strong>representations</strong> (how properties of the input are represented internally) or <strong>circuits</strong> (understanding the algorithms encoded in the weights by which representations are computed and used). A full circuit-based understanding typically requires decent understanding of representations, and is often more rigorous and reliable and can fill in weak points of just studying representations.</p><p> For superposition, there has recently been a burst of exciting work focusing on representations, in the form of sparse autoencoders (eg <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Bricken et al</a> , <a href="https://arxiv.org/abs/2309.08600">Cunningham et al</a> ). We think the success of sparse autoencoders at learning many interpretable features (which are often distributed and not aligned with the neuron basis) gives strong further evidence that superposition is the norm.</p><p> In this work, we wanted to learn general insights about the circuits underlying superposition, via the specific case study of factual recall, though it mostly didn&#39;t pan out. In the specific case of factual recall, our recommendation is to focus on understanding fact representations without worrying about exactly how they&#39;re computed.</p><h3> Why Facts</h3><p> We focused on understanding the role of early MLP layers to look up facts in factual recall. Namely, we studied one-shot prompts of the form “Fact: Michael Jordan plays the sport of” ->; “basketball” for 1,500 athletes playing baseball, basketball or (American) football in Pythia 2.8B <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-6" id="fnref-aYS73ikuT9JzFD3Mj-6">[6]</a></sup> . Factual recall circuitry has been widely studied before, we were influenced by <a href="https://rome.baulab.info/">Meng et al</a> , Chughtai et al (forthcoming), <a href="https://arxiv.org/abs/2304.14767">Geva et al</a> , <a href="https://arxiv.org/abs/2308.09124">Hernandez et al</a> and <a href="https://arxiv.org/abs/2205.11482">Akyurek et al</a> , though none of these focused on understanding early MLPs at the neuron level.</p><p> We think facts are intrinsically pretty interesting - it seems that a <em>lot</em> of what makes LLMs work so well are that they&#39;ve memorised a ton of things about the world, but we also expected facts to exhibit significant superposition. The model wants to know as many facts as possible, so there&#39;s an incentive to compress them, in contrast to more algorithmic tasks like indirect object identification where you can just learn them and be done - there&#39;s always more facts to learn! Further, facts are easy to compress because they don&#39;t interfere with each other (for a fixed type of attribute), the model never needs to inject the fact that “Michael Jordan plays basketball” and “Babe Ruth plays baseball” on the same token, it can just handle the tokens &#39; Jordan&#39; and &#39; Ruth&#39; differently <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-7" id="fnref-aYS73ikuT9JzFD3Mj-7">[7]</a></sup> (see <a href="https://arxiv.org/pdf/2305.01610.pdf#page=18">FAQ questions A.6 and A.7</a> here for more detailed discussion). We&#39;d guess that a model like GPT-3 or GPT-4 knows more facts than it has neurons <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-8" id="fnref-aYS73ikuT9JzFD3Mj-8">[8]</a></sup> .</p><p> Further, <a href="https://arxiv.org/pdf/2305.01610.pdf">prior work</a> has shown that superposition seems to be involved in early MLP layers for <strong>detokenization</strong> , where raw tokens are combined into concepts, eg “social security” is a very different concept than “social” or “security” individually. Facts about people seemed particularly in need of detokenization, as often the same name tokens (eg a first name or surname) are shared between many unrelated people, and so the model couldn&#39;t always store the fact in the token embeddings. Eg “Adam Smith” means something very different to “Adam Driver” or “Will Smith”. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-9" id="fnref-aYS73ikuT9JzFD3Mj-9">[9]</a></sup></p><h2> Our High-Level Takeaways</h2><h3> How to think about facts? </h3><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" alt=""></p><p> <em>(Figure from above repeated for convenience)</em></p><p> At a high-level, we recommend thinking about the recall of factual knowledge in the model as <strong>multi-token embeddings</strong> , which are largely formed by early layers (the first 10-20% of layers) from the raw tokens <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-10" id="fnref-aYS73ikuT9JzFD3Mj-10">[10]</a></sup> . The model learns to recognise entities spread across several recent raw tokens, eg “| Michael| Jordan”, and produces a representation in the residual stream, with linear subspaces that store information about specific attributes of that entity <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-11" id="fnref-aYS73ikuT9JzFD3Mj-11">[11]</a></sup> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-12" id="fnref-aYS73ikuT9JzFD3Mj-12">[12]</a></sup> .</p><p> We think that early attention layers perform <strong>token concatenation</strong> by assembling the raw tokens of these entities on the final token (the “ Jordan” position). And early MLP layers perform <strong>fact lookup</strong> , a Boolean AND (aka <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=7G70mqwvXn7LoiOBc6-IRuLo">detokenization</a> ) on the raw tokens to output this multi-token embedding with information about the described entity. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-13" id="fnref-aYS73ikuT9JzFD3Mj-13">[13]</a></sup> Notably, we believe this lookup is implemented by <em>several</em> MLP layers, and was not localisable to specific neurons or even a single layer.</p><p> A significant caveat is that this is extrapolated from a detailed study of looking up athlete facts, rather than a broader range of factual recall tasks, though is broadly supported by the prior literature. Our speculative prediction is that factual recall of the form “recognise that some entity is mentioned in the context and recall attributes about them” looks like this, but more complex or sophisticated forms of factual recall may not (eg multi-hop reasoning, or recall with a richer or more hierarchical structure).</p><p> We didn&#39;t make much progress on understanding the internal mechanisms of these early MLP layers. Our take is that it&#39;s fine to just black box these early layers as “the thing that produces the multi-token embeddings” and to focus on understanding the meaningful subspaces in these embeddings and how they&#39;re used by circuits in later layers, and not on exactly how they&#39;re computed, ie focusing on the representations. This is similar to how we conventionally think of the token embeddings as a big look-up table. One major difference is that the input space to the token embeddings is a single token, d_vocab (normally about 50,000), while the input to the full model or other sub circuits are strings of tokens, which has a far larger input space. While you can technically think of language models as a massive lookup table, this isn&#39;t very productive! This isn&#39;t an issue here, because we focus on inputs that are the (tokenized) names of known entities, a far smaller set.</p><p> We think the goal of mech interp is to be useful for downstream tasks (ie understanding meaningful questions about model behaviour and cognition, especially alignment-relevant ones!) and we expect most of the interesting computation for downstream tasks to come in mid to late layers 。 Understanding these may require taking a dictionary of simpler representations as a given, but hopefully does not require understanding how the simpler representations were computed.</p><p> We take this lack of progress on understanding internal mechanisms as meaningful evidence against the goal of fully reverse-engineering everything in the model, but we think mech interp can be useful without achieving something that ambitious. Our vision for usefully doing mech interp looks more like reverse-engineering as much of the model as we can, zooming in on crucial areas, and leaving many circuits as smaller blackboxed submodules, so this doesn&#39;t make us significantly more pessimistic on mech interp being relevant to alignment. Thus we think it is fine to leave this type of factual recall as one of these blackboxed submodules, but note that failing at any given task should make you more pessimistic about success at any future mech interp task.</p><h3> Is it surprising that we didn&#39;t get much traction?</h3><p> In hindsight, we don&#39;t find it very surprising that we failed to interpret the early MLP layers (though we&#39;re glad we checked!). Conceptually, on an algorithmic level, factual recall is likely implemented as a giant lookup table. As many students can attest, there simply isn&#39;t much structure to capitalise on when memorising facts: knowing that &quot;Tim Duncan&quot; should be mapped to &quot;basketball&quot; doesn&#39;t have much to do with knowing that &quot;George Brett&quot; should be mapped to &quot;baseball&quot;; these have to be implemented separately. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-14" id="fnref-aYS73ikuT9JzFD3Mj-14">[14]</a></sup> (We deliberately focus on forms of factual recall without richer underlying structure.)</p><p> A giant lookup table has a high minimum description length, which suggests the implementation is going to involve a lot of parameters. This doesn&#39;t necessarily imply it&#39;s going to be hard to interpret: we can conceive of nice interpretable implementations like a neuron per entity, but these seem highly inefficient. In theory there could be interpretable schemes that are more efficient (we discuss, test and falsify some later on like <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Single_Step_Detokenization">single-step detokenization</a> ), but crucially there&#39;s not a strong <em>reason</em> the model should use a nice and interpretable implementation, unlike with more algorithmic tasks like induction heads.</p><p> Mechanistically, training a neural network is a giant curve fitting problem, and there are likely many dense and uninterpretable ways to succeed at this. We should only expect interpretability when there is some structure to the problem that the training process can exploit. In the case of fact recall, the initial representation of each athlete is just a specific point in activation space <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-15" id="fnref-aYS73ikuT9JzFD3Mj-15">[15]</a></sup> , and there is (almost) no structure to exploit, so unsurprisingly we get a dense and uninterpretable result. In post 4, we also studied a toy model mapping pairs of integers to arbitrary labels where we knew all the data and could generate as much as we liked, and didn&#39;t find the toy model any easier to interpret, in terms of finding internal sparsity or meaningful intermediate states.</p><h3> Look for circuits building on factual representations, not computing them</h3><p> In mechanistic interpretability, we can both study <strong>features</strong> and <strong>circuits</strong> . Features are properties of the input, which are represented in internal activations (eg, “projecting the residual stream onto direction v after layer 16 recovers the sentiment of the current sentence”). Circuits are algorithms inside the model, often taking simpler features and using them to compute more complex ones. Some works prioritise <a href="https://arxiv.org/abs/2310.01405">understanding features</a> , others prioritise understanding circuits <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-16" id="fnref-aYS73ikuT9JzFD3Mj-16">[16]</a></sup> , we consider both to be valid approaches to mechanistic interpretability.</p><p> In the specific case of factual recall, we tried to understand the circuits by which attributes about an entity were looked up, and broadly failed. We think a reasonably halfway point is for future work to focus on understanding how these looked up attributes are represented, and how they are used by downstream circuits in later layers, treating the factual recall circuits of early layers as a small black-boxed submodule. Further, since the early residual stream is <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH#Context_doesn_t_matter_when_looking_up_an_athlete_s_sport">largely insensitive to context before the athlete&#39;s name</a> , fact injection seems the <em>only</em> thing the early MLPs can be doing, suggesting that little is lost from not trying to interpret the early MLP layers (in this context). <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-17" id="fnref-aYS73ikuT9JzFD3Mj-17">[17]</a></sup></p><h2>我们学到了什么</h2><p>We split our investigation into four subsequent posts:</p><ol><li> A detailed analysis of the factual recall circuit with causal techniques, which localises the facts to MLPs 2 to 6 and understands a range of high-level facts about the circuit</li><li> A deep dive into the internal mechanisms of these MLP layers that considers several hypotheses and collects evidence for and against, which attempts to (and largely fails to) achieve a mechanistic understanding from the weights.</li><li> A shorter post exploring toy models of factual recall/memorisation, and how they seem similarly uninterpretable to Pythia, and how this supports conceptual arguments for how factual recall may not be interpretable in general</li><li> A shorter post exploring a stronger and more general version of the multi-token embedding hypothesis: that <em>in general</em> residual streams in early layers are a function of recent tokens, and the further-back context only comes in at mid layers. There is a general tendency for this to happen, but some broader context is still used in early layers</li></ol><h3> What we learned about the circuit ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH">Post 2</a> Summary)</h3><ul><li><p> The high-level picture from Geva et al mostly holds up: Given a prompt like “Fact: Michael Jordan plays the sport of”, the model detokenizes “Michael AND Jordan” on the Jordan token to represent <em>every</em> fact the model knows about Michael Jordan 。 After layer 6 the sport is clearly linearly represented on the Jordan token. There are mid to late layer fact extracting attention heads which attend from “of” to “Jordan” and map the sport to the output logits (see Chughtai et al (forthcoming) for a detailed investigation of these, notably finding that these heads still attend and extract the athlete&#39;s sport even if the relationship asks for a non-sport attribute).</p></li><li><p> We simplified the fact extracting circuit down to what we call the <strong>effective model</strong> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-18" id="fnref-aYS73ikuT9JzFD3Mj-18">[18]</a></sup> . For two token athlete names, this consists of attention layers 0 and 1 retrieving the previous token embedding and adding it to the current token embedding. These summed token embeddings then go through MLP layers 2 to 6 and at the end the sport is linearly recoverable. Attention layers 2 onwards can be ablated, as can MLP layer 1.</p><ul><li> The linear classifier can be trained with logistic regression. We also found high performance (86% accuracy) by taking the directions that a certain attention head (Layer 16, Head 20) mapped to the output logits of the three sports: baseball, basketball, football <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-19" id="fnref-aYS73ikuT9JzFD3Mj-19">[19]</a></sup> .</li></ul></li><li><p> We simplified our analysis to interpreting the effective model, where the “end” of the model is a linear classifier and softmax between the three sports.</p><ul><li> We think the insight of “once the feature is linearly recoverable, it suffices to interpret <em>how</em> the linear feature extraction happens, and the rest of the circuit can be ignored” may be useful in other circuit analyses, especially given how easy it is to train linear probes (given labelled data).</li></ul></li></ul><h3> What we learned about the MLPs ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk">Post 3</a> Summary)</h3><ul><li> Once we&#39;d zoomed in on the facts being stored in MLPs 2 to 6, we formed, tested <strong>and falsified</strong> a few hypotheses about what was going on:<ul><li>笔记; These were fairly specific and detailed hypotheses, which were fairly idiosyncratic to my (Neel&#39;s) gut feel about what might be going on. The space of all <em>possible</em> hypotheses is large.</li><li> In fact, investigating these hypotheses made us think they were likely false, in ways that taught us about both how LLMs are working and the nature of the problem&#39;s messiness.</li></ul></li><li> <strong>Single step detokenization hypothesis</strong> : There&#39;s a bunch of “detokenization” neurons that directly compose with the raw token embeddings, use their GELU to simulate a Boolean AND for eg prev==Michael &amp;&amp; curr==Jordan, and output a linear representation of all facts the model knows about Michael Jordan<ul><li> Importantly, this hypothesis claims that the same neurons matter for all facts about a given subject, and there&#39;s no intermediate state between the raw tokens and the attributes.</li><li> <a href="https://arxiv.org/pdf/2305.01610.pdf">Prior work</a> suggests that detokenization is done in superposition, suggesting a more complex mechanism than just one neuron per athlete. I hypothesised this was implemented by some combinatorial setup where each neuron detokenizing many strings, and each string is detokenized by many neurons, but each string corresponds to a unique <em>subset</em> of neurons firing above some threshold.</li></ul></li><li> <strong>We&#39;re pretty confident the single step detokenization hypothesis is false</strong> (at least, in the strong version, though there may still be kernels of truth).证据：</li><li> Path patching (noising) shows there&#39;s meaningful composition between MLP layers, suggesting the existence of intermediate representations. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/aububbyo97pul54yhl22" alt=""></p><ul><li> We collect multiple facts the model knows about Michael Jordan. We then resample ablate <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-20" id="fnref-aYS73ikuT9JzFD3Mj-20">[20]</a></sup> (patch) each neuron (one at a time), and measure its effect on outputting the correct answer. The single step detokenization hypothesis predicts that the same neurons will matter for each fact. We measure the correlation between the effect on each pair of facts, and observe little correlation, suggesting different mechanisms for each fact. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><ul><li><p> By measuring the non-linear effect of each neuron (how much it activates more for Michael Jordan than eg Keith Jordan or Michael Smith), we see that the neurons directly connecting with the probe already had a significant non-linear effect pre-GELU. This suggests that they compose with early neurons that compute the AND rather than the neurons with direct effect just computing the AND itself, ie that there are important intermediate states</p></li><li><p> <strong>Hash and lookup hypothesis</strong> : where the first few MLPs “hash” the raw tokens of the athlete into a gestalt representation of the name that&#39;s almost orthogonal to other names, and then neurons in rest of the MLPs “look up” these hashed representations by acting as a lookup table mapping them to their specific sports.</p><ul><li> One role the hashing could serve is breaking linear structure between the raw tokens<ul><li> Eg “Michael Jordan” and “Tim Duncan” play basketball, but we don&#39;t want to think that “Michael Duncan” does</li><li> In other words, this lets the model form a representation of “Michael Jordan” that is not necessarily similar to the representations of “Michael” or “Jordan”</li></ul></li><li> Importantly, hashing should work for <em>any</em> combination of tokens, not just ones the model has memorised in training, though looking up may not work for unknown names.</li></ul></li><li><p> <strong>We&#39;re pretty confident that the <em>strong</em> version of the hash and lookup hypothesis is false</strong> , though have found it a useful framework and think there may be some truth to it <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-21" id="fnref-aYS73ikuT9JzFD3Mj-21">[21]</a></sup> . Unfortunately “there&#39;s some truth to it, but it&#39;s not the whole story” turned out to be very hard to prove or falsify.证据：</p></li><li><p> (Negative) If you linearly probe for sport on the residual stream, validation accuracy improves throughout the “hashing” layers (MLPs 2 to 4), showing there can&#39;t be a clean layerwise separation between hashing and lookup. Pure hashing has no structure, so validation accuracy should be at random during the intermediate hashing layers. </p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><ul><li> (Negative) Known names have higher MLP output norm than unknown names even at MLP1 and MLP2 - it doesn&#39;t treat known and unknown names the same. MLP1 and MLP2 are hashing layers, so the model shouldn&#39;t distinguish between known and unknown names this early. (This is weak evidence since there may eg be an amortised hash that has a more efficient subroutine for commonly-hashed terms, but still falsifies the strong version of the hypothesis, that hashing occurs with no regard for underlying meaning.)</li><li> (Positive) The model does significantly break the linear structure between tokens. Eg there&#39;s a significant component on the residual for “Michael Jordan” that&#39;s not captured by the average of names like “Michael Smith” or “John Jordan”. This is exactly what we expect hashing to do. * However, this is not strong evidence that the <em>main</em> role of MLP layers is to break linear structure, they may be doing a bunch of other stuff too. Though they do break linear structure significantly more than randomly initialised layers</li><li> We found a <strong>baseball neuron</strong> (L5N6045) which activated significantly more on the names of baseball playing athletes, and composed directly with the fact extracting head<ul><li> The input and output weights of the neuron have significant cosine sim with the head-mediated baseball direction (but it&#39;s significantly less than 1): it&#39;s partially just boosting an existing direction, but partially doing something more<ul><li> If you take the component of every athlete orthogonal to the other 1500 athletes, and project those onto the baseball neuron, it still fires more for baseball players than other players, maybe suggesting it is acting as a lookup table too.</li></ul></li><li> Looking on a broader data distribution than just athlete names, it often activates on sport-related things, but is not monosemantic. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><h3> How to Think About Interpreting Memorisation ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4">Post 4</a> Summary)</h3><p> <em>This is a more philosophical and less rigorous post, thinking about memorisation and the modelling assumptions behind it, that was inspired by work with toy models, but has less in the way of rigorous empirical work.</em></p><ul><li><p>动机：</p><ul><li> When we tried to understand mechanistically how the network does the “fact lookup” operation, we made very little progress.</li><li> What should we take away from this failure about our ability to interpret MLPs in general when they compute distributed representations? In particular, what meaningful representations <em>could</em> we have expected to find in the intermediate activations of the subnetwork?</li></ul></li><li><p> In post 4, we study toy models of memorisation (1-2 MLP layers mapping pairs of integers to fixed and random binary labels) to study memorisation in a more pure setting. The following is an attempt to distil our intuitions following this investigation, we do not believe this is fully rigorous, but hope it may be instructive.</p></li><li><p> We believe one important aspect of fact lookup, as opposed to most computations MLPs are trained to do, is that it mainly involves memorisation: there are no general rules that help you guess what sport an athlete plays from their name alone (at least, with any great accuracy <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-22" id="fnref-aYS73ikuT9JzFD3Mj-22">[22]</a></sup> ).</p></li><li><p> If a network solves a task by generalisation, we may expect to find representations in its intermediate state corresponding to inputs and intermediate values in the generalising algorithm that it has implemented to solve the task. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-23" id="fnref-aYS73ikuT9JzFD3Mj-23">[23]</a></sup></p></li><li><p> On the other hand, if a network is unable to solve a task by generalisation (and hence has to memorise instead), this indicates that either no generalising algorithm exists, or at least that none of the features useful for implementing a possible generalising algorithm are accessible to the network.</p></li><li><p> Therefore, under the memorisation scenario, the only meaningful representations (relevant to the task under investigation) that we should find in the subnetwork&#39;s inputs and intermediate representations are:</p><ul><li> Representations of the original inputs themselves (eg we can find features like “first name is George”).</li><li> Noisy representations of the target concept being looked up: for example, the residual stream after the first couple of MLP layers contains a noisier, but still somewhat accurate, representation of the sport played, because it is a partial sum of the overall network&#39;s output. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-24" id="fnref-aYS73ikuT9JzFD3Mj-24">[24]</a></sup> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-25" id="fnref-aYS73ikuT9JzFD3Mj-25">[25]</a></sup></li></ul></li><li><p> On the other hand, on any finite input domain – such as the Cartesian product of all first name / last name tokens – the output of <em>any</em> model component (neuron, layer of neurons, or multiple layers of neurons) can be interpreted as an embedding matrix: ie a lookup table individually mapping every first name / last name pair to a specific output vector. In this way, we could consider the sport lookup subnetwork (or any component in that network) as implementing a lookup table that maps name token pairs to sport representations.</p></li><li><p> In some sense, this perspective provides a trivial interpretation of the computation performed by the MLP subnetwork: the weights of the subnetwork are such that – within the domain of the athlete sport lookup task – the subnetwork implements exactly the lookup table required to correctly represent the sport of known athletes. And memorisation is, in a sense by definition, a task where there should not be interesting intermediate representations to interpret.</p></li></ul><h3> Do Early Layers Specialise in Local Processing? ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR">Post 5</a> Summary)</h3><ul><li>动机<ul><li><p>The multi-token embedding hypothesis suggests that an important function of early layers is to gather together tokens that comprise a semantic unit (eg <code>[ George] [ Brett]</code> ) and “look up” an embedding for that unit, such that important features of that unit (eg “plays baseball”) are linearly recoverable.</p></li><li><p> This got us thinking: to what extent is this <em>all</em> that early layers do? <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-26" id="fnref-aYS73ikuT9JzFD3Mj-26">[26]</a></sup></p></li><li><p> Certainly, it seems that many linguistic tasks require words / other multi-token entities to be looked up before language-level processing can begin. In particular, many words are split into multiple tokens but seem likely better thought of as a single unit. Such tasks can only be accomplished after lookup has happened, ie in later layers.</p></li><li><p> On the other hand, there is no reason to wait until multi-token lookup is finished for single-token level processing (eg simple induction behaviour) to proceed. So it&#39;s conceivable that early layers aren&#39;t <em>all</em> about multi-token embedding lookup either: they could be getting on with other token-level tasks in parallel to performing multi-token lookup.</p></li><li><p> If early layers only perform multi-token lookup, an observable consequence would be that early layers&#39; activations should primarily be a function of nearby context. For multi-token lookup to work, only the few most recent tokens matter.</p></li></ul></li><li> So, to measure the extent to which early layers perform multi-token lookup, we performed the following experiment:<ul><li> Collect residual stream activations across the layers of Pythia 2.8B for a sample of strings from the Pile.</li><li> Collect activations for tokens from the same strings with differing lengths of truncated context: ie for each sampled token, we would collect activations for that token plus zero preceding tokens, one preceding token, etc, up to nine preceding tokens.</li><li> Measure the similarity between the residual stream activations for tokens in their full context versus in the truncated contexts by computing the (mean centred) cosine similarities.</li><li> If early layers only perform local processing, then the cosine similarities between truncated context activations and full context activations should be close to one (at least for a long-enough truncation window).</li></ul></li><li> Collecting the results, we made the following observations:<ul><li> Early layers do perform more local processing than later layers: cosine similarities between truncated and full contexts are clearly high in early layers and get lower in mid to late layers.</li><li> There is a sharp difference though between cosine sims with zero prior context and with some prior context, showing that local layers are meaningfully dependent on nearby context (ie they aren&#39;t simply processing the current token).</li><li> However, even at layer 0, local layers do have some dependence on distant context: while cosine sims are high (above 0.9) they aren&#39;t close to 1 (often &lt;0.95) <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-27" id="fnref-aYS73ikuT9JzFD3Mj-27">[27]</a></sup> . </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><ul><li> Interestingly, we found that truncated vs full context cosine sims for early layers are particularly low on punctuation tokens (periods, commas, newlines, etc) and “stop words” (and, or, with, of, etc) – indicating that early layers perform highly non-local processing on these tokens.<ul><li> In retrospect, this isn&#39;t so surprising, as the context immediately preceding such tokens often doesn&#39;t add that much to their meaning, at least when compared to word fragment tokens like [ality], so processing them can start early (without waiting for a detokenisation step first).</li><li> There are several other purposes that representations at these tokens might serve – eg summarisation, resting positions, counting delimiters – that could explain their dependence on longer range context. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><h2> Further Discussion</h2><h3> Do sparse autoencoders make understanding factual recall easier?</h3><p> A promising recent direction has been to <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">train Sparse Autoencoders</a> (SAEs) to extract monosemantic features from superposition. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-28" id="fnref-aYS73ikuT9JzFD3Mj-28">[28]</a></sup> We haven&#39;t explored SAEs ourselves <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-29" id="fnref-aYS73ikuT9JzFD3Mj-29">[29]</a></sup> and so can&#39;t be confident in this, but our guess is that SAEs don&#39;t solve our problem of understanding the circuitry behind fact localisation, though may have helped to narrow down假设。</p><p> The core issue is that SAEs are a tool to better understand <em>representations</em> by decomposing them into monosemantic features, while we wanted to understand the factual recall <em>circuit</em> : how do the parameters of the MLP layers implement the algorithm mapping tokens of the names to the sport played ？ The key missing piece in SAEs is that if meaningful features in MLP layers (pre and post GELU) are not basis aligned then we need (in theory) to understand the function of <em>every</em> neuron in the layer to understand the mapping. In contrast, basis aligned features (ie corresponding to a single neuron pre and post GELU) can be understood in terms of a single GELU, and the value of all other neurons is irrelevant.</p><p> Understanding representations better can be an extremely useful intermediate step that helps illuminates a circuit, but we expect this to be less useful here. The obvious features an SAE might learn are first name (eg “first name is Michael”), last name (eg “last name is Jordan”) and sport (eg “plays basketball”). It seems fairly likely that there&#39;s an intermediate feature corresponding to each entity eg (“is Michael Jordan”) which may be learned by an extremely wide SAE (though it would need to be very wide! There are a lot of entities out there), though we don&#39;t expect many meaningful features corresponding to groups of entities <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-30" id="fnref-aYS73ikuT9JzFD3Mj-30">[30]</a></sup> . But having these intermediate representations isn&#39;t obviously enough to understand how a given MLP layer works on the parameter level.</p><p> We can also learn useful things from what an SAE trained on the residual stream or MLP activations at different layers can learn. If we saw sharp transitions, where the sport only became a feature after layer 4 or something, this would be a major hint! But unfortunately, we don&#39;t expect to see sharp transitions here, you can probe non-trivially for sport on even the token embeddings, and it smoothly gets better across the key MLP layers (MLPs 2 to 6).</p><h3>未来的工作</h3><p>In terms of overall implications for mech interp, we see our most important claims as that the early MLPs in factual recall implement “multi-token embeddings”, ie for known entities whose name consists of multiple tokens, the early MLPs produce a linear representation of known attributes, and more speculatively that we do <em>not</em> need to mechanistically understand how these multi-token embeddings are computed in order for mech interp to be useful (so long as we can understand what they represent, <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB#Look_for_circuits_building_on_factual_representations__not_computing_them">as argued above</a> ). In terms of future work, we would be excited for people to red-team and build on our claim that the mechanism of factual lookup <em>is</em> to form these multi-token embeddings. We would also be excited for work looking for concrete cases where we do need to understand tightly coupled computation in superposition (for factual recall or otherwise) in order for mech interp to be useful on a downstream task with real-world/alignment consequences, or just on tasks qualitatively different from factual recall.</p><p> In terms of whether there should be further work trying to reverse-engineer the mechanism behind factual recall, we&#39;re not sure. We found this difficult, and have essentially given up after about 6-7 FTE months on this overall project, and through this project we have built intuitions that this may be fundamentally uninterpretable. But we&#39;re nowhere near the point where we can confidently rule it out, and expect there&#39;s many potentially fruitful perspectives and techniques we haven&#39;t considered. If our above claims are true, we&#39;re not sure if this problem needs to be solved for mech interp to be useful, but we would still be very excited to see progress here. If nothing else we expect it would teach us general lessons about computation in superposition which we expect to be widely applicable in models. And, aesthetically, we find it fairly unsatisfying that, though we have greater insight into how models recall facts, we didn&#39;t form a full, parameter-level understanding.</p><h2>致谢</h2><p>We are grateful to Joseph Bloom, Callum McDougall, Stepan Shabalin, Pavan Katta and Stefan Heimersheim for valuable feedback. We are particularly grateful to Tom Lieberum and Sebastian Farquhar for the effort they invested in giving us detailed and insightful feedback that significantly improved the final write-up.</p><p> We are also extremely grateful to Tom Lieberum for being part of the initial research sprint that laid the seeds for this project and gave us the momentum we needed to get started.</p><p> This work benefited significantly from early discussions with Wes Gurnee, in particular for the suggestion to focus on detokenization of names, and athletes as a good category.</p><h2>作者贡献</h2><p>Neel was research lead for the project, Sen was a core contributor throughout. Both contributed significantly to all aspects of the project, both technical contributions and writing. Sen led on the toy models focused work.</p><p> Janos participated in the two week sprint that helped start the project, and helped to write and maintain the underlying infrastructure we used to instrument Pythia. Rohin advised the project.</p><h2>引文</h2><p>Please cite this work as follows</p><pre><code> @misc{ nanda2023factfinding, title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level}, url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall}, journal={Alignment Forum}, author={Nanda, Neel and Rajamanoharan, Senthooran and Kram\&#39;ar, J\&#39;anos and Shah, Rohin}, year={2023}, month={Dec}}</code> </pre><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-aYS73ikuT9JzFD3Mj-1" class="footnote-item"><p> We use “select” because the number of tokens in the right “unit” can vary in a given context. Eg in “On Friday, the athlete Michael Jordan”, the two tokens of &#39;Michael Jordan&#39; is the right unit of analysis, while “On her coronation, Queen Elizabeth II”, the three tokens of “Queen Elizabeth II” are the right单元。 <a href="#fnref-aYS73ikuT9JzFD3Mj-1" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-2" class="footnote-item"><p> It is closely related to the observed phenomenon of polysemanticity (where a neuron fires on multiple unrelated things) and distributed representations (where many neurons fire for the same thing), but crucially superposition is a mechanistic hypothesis that tries to explain why these observations emerge, not just an empirical observation itself. <a href="#fnref-aYS73ikuT9JzFD3Mj-2" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-3" class="footnote-item"><p> In particular, distributed representation seems particularly prevalent in language MLP layers: in image models, often individual neurons were monosemantic. This happens sometimes but seems qualitatively rarer in language models. <a href="#fnref-aYS73ikuT9JzFD3Mj-3" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-4" class="footnote-item"><p> Notably, almost all transformer circuit progress has been focused on attention heads rather than MLP layers (with some exceptions eg <a href="https://arxiv.org/abs/2305.00586">Hanna et al</a> ), despite MLPs being the majority of the parameters. We expect that part of this is that MLPs are more vulnerable to superposition than attention heads. Anecdotally, on a given narrow task, often a sparse set of attention heads matter, while a far larger and more diffuse set of MLP neurons matter, and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Bricken et al</a> suggests that there are many monosemantic directions in MLP activations that are sparse <a href="#fnref-aYS73ikuT9JzFD3Mj-4" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-5" class="footnote-item"><p> Though real models can also be difficult to study, as they may be full of messy confounders. Newtonian physics is particularly easy to reason about with billiard balls, but trying to learn about Newtonian physics by studying motion in a hurricane wouldn&#39;t be a good idea either. (Thanks to Joseph Bloom for this point!) <a href="#fnref-aYS73ikuT9JzFD3Mj-5" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-6" class="footnote-item"><p> A weakness in our analysis specifically is that we only studied 1,500 facts (the sports of 1,500 athletes) while the 5 MLP layers we studied each had 10,000 neurons, which was fairly under-determined! Note that we expect even Pythia 2.8B knows more than 50,000 facts total, athlete sports just happened to be a convenient subset to study. <a href="#fnref-aYS73ikuT9JzFD3Mj-6" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-7" class="footnote-item"><p> There are edge cases where eg famous people share the same name (eg <a href="https://en.wikipedia.org/wiki/Michael_Jordan">Michael Jordan</a> and <a href="https://en.wikipedia.org/wiki/Michael_I._Jordan">Michael Jordan</a> !), our guess is that the model either ignores the less famous duplicate, or it looks up a “combined” factual embedding that it later disambiguates from语境。 Either way, this is only relevant in a small fraction of factual recall cases. <a href="#fnref-aYS73ikuT9JzFD3Mj-7" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-8" class="footnote-item"><p> Note that models have many fewer neurons than parameters, eg GPT-3 has 4.7M neurons and 175B parameters, because each neuron has a separate vector of d_model input and output weights (and in GPT-3 d_model is 12288) <a href="#fnref-aYS73ikuT9JzFD3Mj-8" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-9" class="footnote-item"><p> Of course this is an imperfect abstraction, eg non-Western names are more likely to play internationally popular sports, and certain tokens may occur in exactly one athlete&#39;s name. <a href="#fnref-aYS73ikuT9JzFD3Mj-9" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-10" class="footnote-item"><p> These are not the same as contextual word embeddings (as have been <a href="https://aclanthology.org/D18-1179/">studied</a> <a href="https://arxiv.org/abs/1905.05950">extensively</a> with BERT-style models in the past). It has long been known that models enrich their representation of tokens as you go from layer to layer, using attention to add context to the (context-free) representations provided by the embedding layer. Here, however, we&#39;re talking about how models represent linguistic units (eg names) that span multiple tokens <em>independently</em> of any <em>additional</em> context. Multi-token entities need to be represented somehow to aid downstream computation, but in a way that can&#39;t easily be attributed back to individual tokens, similar to what&#39;s been termed <a href="https://transformer-circuits.pub/2022/solu/index.html#section-6-3-2">detokenization</a> in the literature. In reality, models probably create <em>contextual multi-token embeddings</em> , where multi-token entities&#39; representations are further enriched by additional context, complicating the picture, but we still think multi-token embeddings are a useful concept for understanding models&#39; behaviour. <a href="#fnref-aYS73ikuT9JzFD3Mj-10" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-11" class="footnote-item"><p> For example, the model has a &quot;plays basketball&quot; direction in the residual stream. If we projected the multi-token representation for &quot;| MIchael| Jordan&quot; onto this direction, it would be unusually high <a href="#fnref-aYS73ikuT9JzFD3Mj-11" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-12" class="footnote-item"><p> This mirrors the findings in <a href="https://arxiv.org/abs/2308.09124">Hernandez et al</a> , that looking up a relationship (eg “plays the sport of”) on an entity is a linear map. <a href="#fnref-aYS73ikuT9JzFD3Mj-12" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-13" class="footnote-item"><p> Eg mapping “first name is Michael” and “last name is Jordan” to “is the entity Michael Jordan” with attributes like “plays basketball”, “is an athlete”, “plays for the Chicago Bulls” etc. <a href="#fnref-aYS73ikuT9JzFD3Mj-13" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-14" class="footnote-item"><p> Though there&#39;s room for some heuristics, based on eg inferring the ethnicity of the name, or eg athletes with famous and unique single-token surnames where the sport may be stored in the token embedding. <a href="#fnref-aYS73ikuT9JzFD3Mj-14" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-15" class="footnote-item"><p> As argued in <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR">post 5</a> , since early layers don&#39;t integrate much prior context before the name. <a href="#fnref-aYS73ikuT9JzFD3Mj-15" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-16" class="footnote-item"><p> Which often implicitly involves understanding features. <a href="#fnref-aYS73ikuT9JzFD3Mj-16" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-17" class="footnote-item"><p> A caveat to this point is that without mechanistic understanding of the circuit it might be hard to verify that we have found all the facts that the MLPs inject -- though a countercounterpoint is that you can look at loss recovered from the facts you do know on a representative dataset <a href="#fnref-aYS73ikuT9JzFD3Mj-17" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-18" class="footnote-item"><p> ie showed that we can ablate everything else in the model without substantially affecting performance <a href="#fnref-aYS73ikuT9JzFD3Mj-18" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-19" class="footnote-item"><p> Each sport is a single token. <a href="#fnref-aYS73ikuT9JzFD3Mj-19" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-20" class="footnote-item"><p> Ie we take another athlete (non-basketball playing), we take the neuron&#39;s activation on that athlete, and we intervene on the “Michael Jordan” input to replace that neuron&#39;s activation on the Jordan token with its activation on the final name token of the other athlete. We repeat this for 64 randomly selected other athletes. <a href="#fnref-aYS73ikuT9JzFD3Mj-20" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-21" class="footnote-item"><p> Note that it&#39;s unsurprising to us that the strong form of hashing didn&#39;t happen - even if at one point in training the early layers do pure hashing with no structure, future gradient updates will encourage early layers to bake in <em>some</em> known structure about the entities it&#39;s hashing, lest the parameters be wasted. <a href="#fnref-aYS73ikuT9JzFD3Mj-21" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-22" class="footnote-item"><p> We suspect that generalisation probably does help a bit: the model is likely using correlations between features of names – eg cultural origin – and sports played to get some signal, and some first names or last names have sport (of famous athlete(s) with that name) encoded right in their token embeddings. But the key point is that these patterns alone would not allow you to get anywhere near decent accuracy when looking up sport; memorisation is key to being successful at this task. <a href="#fnref-aYS73ikuT9JzFD3Mj-22" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-23" class="footnote-item"><p> Note that we&#39;re not saying that we would always succeed in finding these representations; the network may solve the task using a generalising algorithm we do not understand, or we may not be able to decode the relevant representations. The point we&#39;re making is that, for generalising algorithms, we at least know that there must be meaningful intermediate states (useful to the algorithm) that we at least have a chance of finding represented within the network. <a href="#fnref-aYS73ikuT9JzFD3Mj-23" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-24" class="footnote-item"><p> Of course, there are many other representations in these subnetwork&#39;s intermediate activations, because the subnetwork&#39;s weights come from a language model trained to accomplish many tasks. But these representations cannot be decoded using just the task dataset (athlete names and corresponding sports). And, to the extent that these representations are truly task-irrelevant (because we assume the task can only be solved by memorisation – see the caveat in an earlier footnote), our blindness to these other representations shouldn&#39;t impede our ability to understand how sport lookup is accomplished. <a href="#fnref-aYS73ikuT9JzFD3Mj-24" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-25" class="footnote-item"><p> Other representations we might find in a model in general include “clean” representations of the target concept (sport played), and functions of this concept (eg “plays basketball AND is over 6&#39;8&#39;&#39; tall”), but these should only appear following the lookup subnetwork, because we defined the subnetwork to end precisely where the target concept is first cleanly represented. <a href="#fnref-aYS73ikuT9JzFD3Mj-25" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-26" class="footnote-item"><p> Note that the answer to this question does not directly tell us anything (positive or negative) about the multi-token embedding hypothesis: it could be that early layers perform many local tasks, of which multi-token embeddings is just one; alternatively it could be the case that early layers do lots of non-local processing in addition to looking up multi-token embeddings. <a href="#fnref-aYS73ikuT9JzFD3Mj-26" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-27" class="footnote-item"><p> We note that it&#39;s kind of ambiguous how best to quantify “how much damage did truncation do”. Arguably cosine sim squared might be a better metric, as it gives the fraction of the norm explained, and 0.9^2=0.81 looks less good. <a href="#fnref-aYS73ikuT9JzFD3Mj-27" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-28" class="footnote-item"><p> We note that by default SAEs get you representation sparsity but not circuit sparsity - if a SAE feature is distributed in the neuron basis then thanks to the per-neuron non-linearity any neuron can affect the output feature and can&#39;t naively be analysed in隔离。 <a href="#fnref-aYS73ikuT9JzFD3Mj-28" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-29" class="footnote-item"><p> We did the main work on this project before the <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">recent</a> <a href="https://arxiv.org/abs/2309.08600">flurry</a> of work on SAEs. If we were doing this project again, we&#39;d probably try using them! Though as noted, we don&#39;t expect them to be a silver bullet solution. <a href="#fnref-aYS73ikuT9JzFD3Mj-29" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-30" class="footnote-item"><p> Beyond heuristics about eg certain ethnicities being inferrable from the name and more likely to play different sports, which we consider out of scope for this investigation. In some sense, we deliberately picked a problem where we did not expect intermediate representations to be important. <a href="#fnref-aYS73ikuT9JzFD3Mj-30" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall<guid ispermalink="false"> iGuwZTHWb6DFY3sKB</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:44:24 GMT</pubDate> </item><item><title><![CDATA[Measurement tampering detection as a special case of weak-to-strong generalization]]></title><description><![CDATA[Published on December 23, 2023 12:05 AM GMT<br/><br/><p> Burns et al at OpenAI released <a href="https://openai.com/research/weak-to-strong-generalization"><u>a paper</u></a> studying various techniques for fine-tuning strong models on downstream tasks using labels produced by weak models. They call this problem “weak-to-strong generalization”, abbreviated W2SG.</p><p> Earlier this year, we published a paper, <a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood"><u>Benchmarks for Detecting Measurement Tampering</u></a> , in which we investigated techniques for the problem of measurement tampering detection (MTD). MTD is a special case of W2SG. In this post, we&#39;ll explain the relationship between MTD and W2SG, and explain why we think MTD is more likely than fully general W2SG to work. Of course, fully general W2SG is a strictly more valuable problem to solve, due to this generality.</p><p> We think MTD is a promising research direction. We&#39;re also excited for other problems which are special cases of W2SG that have special structure that can be exploited by techniques, especially if that structure is likely to be present in important cases in future.</p><h1> MTD as a subset of W2SG</h1><h2> A similar goal</h2><p> When training an AI, the reward we attribute to different behaviors might not match the reward we would give if we understood the situation better.</p><p> The goal of W2SG techniques is to achieve good results when training a strong AI despite only having access to a weak supervisor that understands the situation less well than the strong AI.</p><p> MTD is the special case where the weak supervisor has access to measurements which should be sufficient to understand the situation, but these measurements can be tampered with (eg replacing the camera feed with some made-up data, disabling tests, or threatening annotators). Because the measurements are sufficient in the absence of tampering, we don&#39;t need to worry about benign mistakes that could happen even without an AI optimizing to make measurements look good.</p><h2> Slightly different experiments</h2><p> W2SG can be studied using <a href="https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization"><u>sandwiching experiments</u></a> , where we try to get an AI to safely accomplish tasks despite only having access to a weak supervisor, and then we measure the performance of our method using a stronger held-out supervision signal (eg held out ground truth labels).</p><p> In the case of the OpenAI paper, the weak supervisor is a small language model trained on ground truth labels, as an analogy for human annotators.</p><p> In the case of our MTD paper, we have access to measurements, but there is some notion of measurement tampering. In our work, the measurements aim to directly measure the property of interest as a boolean value, so converting from untampered measurements to correct labels is straightforward (and doesn&#39;t require any learning or intelligence).</p><h1> Different hopes for succeeding at W2SG</h1><p> In both cases, we need some additional assumptions to get strong supervision from a weak supervisor. If we made no structural assumptions about the internals of models and assumed the worst case about the internal structure of AIs, we wouldn&#39;t succeed, so we will depend on some type of structure in the internals of models. The structure that the OpenAI paper discusses is very different from the structure we hope to leverage in the measurement tampering case.</p><h2> Confidence and consistency</h2><p> The OpenAI paper shows that on some tasks, training the model to be confident in addition to matching the weak supervisor&#39;s labels sometimes increases accuracy. The hope is that the model might have a very salient representation of what is true, and a less salient representation of the predictions of the weak supervisor.</p><p> The confidence hope is similar to the hope explored in <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><u>Discovering Latent Knowledge</u></a> : the inner representation of the truth inside AIs might be more consistent than other features, and be more salient than other consistent features, such that optimizing for consistency (and confidence) could give higher performance than naive training on the weak supervisor labels.</p><p> Both confidence and consistency are not enough for AIs for which “what humans think” is more salient than “what is true”, which seems very likely for AIs pretrained on human-generated text or AIs trained to predict measurements that are often tampered with.</p><p> This structure is present in a very wide range of situations. But it&#39;s not clearly enough to solve the problem. Indeed, Burns et al don&#39;t find techniques that reliably improve results, which we think is some evidence against the existence of techniques that robustly allow W2SG. We also think that the recent work “ <a href="https://www.lesswrong.com/posts/Yio4nmD8JMttx9o9S/new-paper-shows-truthfulness-and-instruction-following-don-t"><u>Generalization Analogies: a Testbed for Generalizing AI Oversight to Hard-To-Measure Domains</u></a> &quot; provides some weak evidence against the existence of techniques that robustly achieve desirable generalization without using some more specific structure.</p><h2> Detecting tampering</h2><p> In contrast, MTD has a very particular type of structure to the problem, which might allow us to develop techniques that work in a variety of settings. Measurement tampering is a particular kind of supervisor failure that has two key properties:</p><ul><li> Failed attempts at tampering might leave evidence of this wrongdoing, while you might never get strong evidence of wrongdoing if the AI system isn&#39;t trying to tamper.</li><li> Consistent relationships between measurements might be driven by similar mechanisms for real positive examples and trusted examples, while the mechanism for consistency differs between fake positive examples and trusted examples.</li></ul><p> Therefore, if you train your most powerful AI to predict measurements, these properties might translate to properties about its internals. These properties can be used by techniques that use model internals (eg by probing for evidence of tampering or by using techniques like exclusion fine-tuning), which is why it might be possible to get higher performance than if we relied on measurements alone.</p><p> We are not aware of particular reasons to expect these properties to break for AIs for which the measurement predictions are very salient. So we think that it is substantially more likely that there exist techniques which reliably solve MTD problems than that reliably solve W2SG. Overall, we think that MTD seems considerably more likely to work than fully general W2SG, but of course it is less generally applicable.</p><p> The main weakness of MTD is the somewhat fuzzy properties it relies on: so far, we haven&#39;t been able to find a precise definition of measurement tampering that can be used to unambiguously determine if a given dataset has the right “measurement tampering structure” 。</p><p> We go into more detail about these hopes in <a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood#Why_solving_measurement_tampering_may_be_notably_easier_than_eliciting_arbitrary_knowledge"><u>Why solving measurement tampering may be notably easier than eliciting arbitrary knowledge</u></a> .</p><h2> Other special cases of W2SG might be good research problems</h2><p> We picked the structure in MTD partially because we think it&#39;s a particularly important kind of structure, but also because it was the simplest non trivial example we could construct of a setting where we could hope for generalization performance and we had access to some kind of structure which we could exploit using our techniques. We think that while developing MTD techniques looks like a pretty promising research direction, it would also be promising to develop techniques in a wider variety of settings where you have particular structure, because in future we might want to be able to exploit bespoke structure in particular域。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to<guid ispermalink="false"> 4KLCygqTLsMBM3KFR</guid><dc:creator><![CDATA[ryan_greenblatt]]></dc:creator><pubDate> Sat, 23 Dec 2023 00:05:57 GMT</pubDate> </item><item><title><![CDATA[How does a toy 2 digit subtraction transformer predict the difference?]]></title><description><![CDATA[Published on December 22, 2023 9:17 PM GMT<br/><br/><h1><br> <strong>2位减法——差异预测</strong></h1><h1>概括</h1><p>我继续研究一个玩具 1 层 Transformer 语言模型，该模型经过训练可以进行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a-b=\pm c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>形式的两位数加法<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。在预测模型是正 (+) 还是负 (-) 后，它必须输出<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>之间的差。该模型创建在 a 和 b 基数中振荡的激活，以及作为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a - b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的函数线性变化的激活。该模型使用激活函数耦合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>方向上的振荡，然后对这些振荡求和以消除除取决于绝对差<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="|a-b|"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span></span></span></span></span></span>的方差之外的任何方差。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="|a-b|"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span></span></span></span></span></span>预测正确的输出标记。我检查了该算法从输入到模型输出的完整路径。</p><h1>介绍</h1><p>在之前的文章中，我描述了<a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/">训练两位数减法变压器</a>并<a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">研究了该变压器如何预测</a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"> </a><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">输出</a><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">的符号</a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the">。</a>在这篇文章中，我将研究如何训练模型的权重来确定两位整数之间的差异，以及模型激活的紧急行为。</p><h2>路线图</h2><p>与我在上一篇文章中从头到尾地研究模型不同，这次我将采取相反的方法，从输入开始，了解它如何转换为输出。我将按顺序分解：</p><ul><li>注意力模式，以及模型关注哪些标记来预测输出。</li><li>注意力头权重如何设置预激活。</li><li>预激活中出现的模式。</li><li>激活中出现的模式。</li><li>神经元和逻辑之间的映射。</li><li> Logits 中的模式。</li></ul><h1>注意力模式</h1><p>下面是四个示例的注意力模式的可视化（两个正，两个负， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>使用相同的数值，但交换它们）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/vokykqs6hsusbr0yy69u" alt="该图像的 alt 属性为空；它的文件名是attention_head_examples-1.png"></p><p>我已将所有不重要的行显示为灰色，以便我们可以专注于倒数第二行，该行对应于转换器预测差值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>的行。这里有几点需要注意：</p><ul><li>当结果为正时，H0 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> ，H3 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> 。</li><li>当结果为负时，H0 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> ，H3 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> 。</li><li> H1 和 H2 大致均匀地关注上下文中的所有标记。</li></ul><p>因此，H3 始终关注<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>中较大的一个，而 H0 始终关注 a 和 b 中较小的一个。 H1和H2似乎并没有做任何太重要的事情。</p><p>在数学中，上述直觉可以写成每个头对标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>的关注：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_{0,a} = \begin{cases} 1 &amp; t_{4} = - \\ 0 &amp; t_{4} = + \end{cases},\qquad A_{1,a} = 0,\qquad A_{2,a} = 0,\qquad A_{3,a} = \begin{cases} 0 &amp; t_{4} = - \\ 1 &amp; t_{4} = + \end{cases},\qquad"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span></span></span></span></span></span></p><p>以及每个头对令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的关注：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_{0,b} = \begin{cases} 0 &amp; t_{4} = - \\ 1 &amp; t_{4} = + \end{cases},\qquad A_{1,b} = 0,\qquad A_{2,b} = 0,\qquad A_{3,b} = \begin{cases} 1 &amp; t_{4} = - \\ 0 &amp; t_{4} = + \end{cases}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span></span></span></span></span></span>,</p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span>是上下文位置 4 处的标记值（紧接在 = 之后）。有理由问这是否是一个好的描述，确实如此！如果我对头 1 和 2 以及头 0 和 3 中的注意力模式进行零消融，我将上面突出显示的行替换为正确处理<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的单热编码单位向量，参数空间中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span>问题的损失从 0.0181 减少到 0.0168。</p><h1>神经元预激活和激活</h1><h2>注意力头实现的功能</h2><p>矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}^h = W_{\rm E} W_{\rm V}^h W_{\rm O}^h W_{\rm in}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.161em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">neur</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">E</span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">W</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">V</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.327em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">O</span></span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.343em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">in</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">具有</span></span></span></span></span></span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">形状</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">d_vocab</span></span></span></span></span></span></span></span></span></span></span> , d_mlp] 并描述当注意力头 h 关注词汇中的标记时，注意力头<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span>如何改变神经元预激活。从上一节中我们知道，头 0 和 3 完全参与标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> ，因此我们想了解头 0 和 3 如何根据其输入影响预激活。 （<i>旁白：在我</i><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"><i>之前的</i></a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"><i>文章</i></a>中<i>，我检查了类似的内容，但是在我的</i><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">r</span></span></span></span></span></span></span></span></span></span></span>计算中存在一个微妙的错误<i>，我已经修复了该错误</i>）。</p><p>以下是我在头部对神经元激活的贡献中看到的四种模式： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/fery0ybeafqvetxvxsfe" alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions-2.png"></p><p>﻿</p><p>因此，神经元 17 基本上呈线性贡献，而许多其他神经元则具有不同类型的振荡。如果我们将每个头对每个神经元的贡献拟合一条线，减去它，然后将这些信号变换到频率空间，我们可以形成以下功率谱： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/thxwvacgstlgddcpwffx" alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions_freqspace.png"></p><p>在这里，我在三个关键频率（0.33、0.39、0.48）处放置了垂直线，这<a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/">是我在关于该主题的第一篇文章中看到的</a>。我们看：</p><ul><li>神经元 17 中线性趋势之上的小振荡在与这些关键频率相关的频谱中具有一些峰值，但它们很弱。</li><li>神经元 5 具有与每个关键频率相关的强峰值，但<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.39"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span></span></span></span></span></span>处的峰值是迄今为止最强的。其次，这个主峰并不完全尖锐。它具有很强的中心值以及紧邻中心值一侧的相当强的“英尺”值。这很快就会很重要。</li><li>神经元 76 一片混乱。像这样的神经元数量相当多。有一个很强的峰值，但功率也远离该频率，像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(f - f_{\rm peak})^{-2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">p</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">k</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>那样下降。</li><li>神经元125在令牌信号中表现出强烈的节拍，这对应于功率逐渐增加到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.48"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.48</span></span></span></span></span></span></span>处的峰值。这又有点混乱了。</li></ul><p>所以我们看到的是，这些峰值<i>主要</i>可以用强中心峰值和紧邻该中心峰值的频率仓中相应的强峰值来描述。这非常让人想起我们在使用加窗技术时在信号处理中看到的峰值<a href="https://en.wikipedia.org/wiki/Apodization">变迹</a>——来自尖锐中心峰值的功率会稍微分散到相邻的箱中。</p><p>我不完全确定什么是通常适合这些函数的正确函数，但这似乎是一个很好的猜测：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{n}_{i,h}(t) = \underbrace{\left(m t + c\right)}_{\rm linear} + \sum_{i}^{0.33, 0.39, 0.48} \left[a_i \underbrace{\cos(2\pi f_i t + \phi_i)}_{\text{central peak}} + \underbrace{b_i\cos(2 \pi f_i t + \theta_{0,i})\cos\left(\frac{2\pi}{d_{\rm vocab}'} t + \theta_{1,i}\right)}_{\text{`feet' of central peak}}\right]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.103em; margin: 0px 0.096em 0px -0.076em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.103em; margin: 0px 0.096em 0px -0.076em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.373em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">线性</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Σ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">0.33</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">,</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">0.48</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">⎡</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">⎢</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">⎢</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">⎢</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">⎢</span></span></span></span></span></span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo" style="vertical-align: -1.959em;"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎣</span> <span class="mjx-char MJXc-TeX-size4-R" style="line-height: 0.539em; margin-bottom: -0.101em; margin-top: 0px;">a</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">i</span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">cos</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span></span> <span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">_</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">fit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">+</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">phi</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">i</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">)</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;"></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.117em; margin: 0px 0.103em 0px -0.083em;">中心</span><span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">峰</span><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.117em; margin: 0px 0.103em 0px -0.083em;">+</span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.897em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">bi</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">cos</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">_</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">(</span></span> <span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 2.028em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 2.869em; top: -1.383em;"><span class="mjx-mrow" style=""><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 2.869em; bottom: -1.246em;"><span class="mjx-msubsup" style=""><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-stack" style="vertical-align: -0.426em;"><span class="mjx-sup" style="font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0.065em; padding-right: 0.06em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">b</span></span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.028em;" class="mjx-line"></span></span><span style="height: 1.859em; vertical-align: -0.881em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">)</span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">_</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 6.728em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.519em;">中央峰‘脚’</span></span></span></span></span></span></span></span> <span class="mjx-mo" style="vertical-align: -1.959em;"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎤</span> <span class="mjx-char MJXc-TeX-size4-R" style="line-height: 0.539em; margin-bottom: -0.101em; margin-top: 0px;">⎥ ⎥ ⎥ ⎥</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎦</span></span></span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d_{\rm vocab}' = 100"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-stack" style="vertical-align: -0.335em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.076em; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">b</span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span>是对应于 0-99 的词汇大小。在这里，负责脚部的关键频率总和中的第二项受到我过去使用<a href="https://en.wikipedia.org/wiki/Hann_function">Hann 窗</a>所做的工作的启发。当频率为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>的两个正弦项相乘时，它们将功率放入<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_1 \pm f_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>中，因此我上面列出的函数的这种形式给出了一条线、关键频率的功率和关键频率旁边的垃圾箱。</p><p>我使用<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">scipy 的 curve_fit</a>来<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{n}_{i,h}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">拟合</span></span></span><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">~</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">ni</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span></span></span></span></span>形式的函数，以表示头 0 和 3 对每个神经元的贡献。我发现这些拟合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">占</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">neur</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">总</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">变异</span></span></span></span></span></span></span></span></span></span></span>性的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gtrsim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">≳</span></span></span></span></span></span></span> 97%。对于像上面的神经元 5 或神经元 17 这样的神经元，这种拟合解释了 >; 99% 的变异性；像神经元 125 这样的神经元具有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">约</span></span></span></span></span></span></span>97% 的变异性。像神经元 76 这样的神经元很混乱，但这种拟合解释了它们<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">90</span></span></span></span></span></span></span> % 的变异性。</p><p>我现在用这些拟合替换神经元预激活（使用我在上一节中发现的注意模式）。我还保留了最终变得重要的偏差：我发现来自注意力头以及嵌入和位置嵌入的价值偏差（因此在上下文位置 4 进入注意力操作的原始残差流）对于设置神经元很重要预激活。</p><p>当我进行这种近似时，所有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4 个</span></span></span></span></span></span></span></span></span>问题的损失从 0.0181 增加到 0.0341，而准确度仅从 99.94% 下降到 99.68%。但请注意，如果我在拟合中删除“英尺”项，则损失会增加一个数量级以上，并且准确性会大幅下降，因此这似乎获得了模型使用的大部分重要部分。</p><h2>由此产生的神经元预激活</h2><p>在上一节中，我研究了模型权重如何设置神经元预激活，但现在我只想查看预激活，看看我是否可以理解描述它们的更简单的算法。</p><p>上一节让我确信可能有四种不同类别的神经元。但是，经过一番努力，结果发现只有两个，并且都可以使用简单的拟合来描述：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b) = \underbrace{m(a-b) + c}_{\text{linear}} + \underbrace{A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)}_{\text{oscillatory}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.1em; margin: 0px 0.095em 0px -0.075em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.1em; margin: 0px 0.095em 0px -0.075em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 2.659em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">线性</span></span></span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">_</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 8.536em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">振荡</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p> 其中存在线性部分（斜率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span>和截距<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span> ）和振荡拟合（振幅<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1,A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 、相位<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_1,\phi_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ），并且每个神经元具有主导特征<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_i = \omega_i/(2\pi) \in \{0.33, 0.39, 0.48\}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">频率</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">fi</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">ε</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.33</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.48</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span></span></span></span></span></span>在主要模型频率中。</p><p>问题是——这种拟合仅由模型在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a \geq b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≥</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span>的</span></span></span></span></span><i>区域中实现</i><i>。</i>在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b > a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>的区域中，模型仅重用与交换输入的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>情况相同的计算。因此，我将上面的函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>拟合到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a \in [50, 99]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">99</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b \in [0, 49]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">49</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span>的区域中，以便均匀地<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> 。然后，我计算所有输入值的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ，然后将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b > a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>处的输出值替换为相应的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>情况（例如，我将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="33-10"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">33</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span></span></span></span></span>中的激活值放入以下位置： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10-33"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10-33</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">）</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">。</span></span></span></span></span></span></span></p><p>要了解其工作原理，这里有一些神经元预激活示例（顶行）和这些拟合（底行）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/k9jyvjhfnrrcjkktonqb" alt="该图像的 alt 属性为空；它的文件名是sample_neuron_preactivations.png"></p><p>这些合身看起来真的很不错！如果我进入模型并将所有神经元预激活替换为最适合的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span> ，那么模型损失仅从 0.0181 变化到 0.0222，准确度仅从 99.94% 下降到 99.92%。此外，在 ReLU() 之后，拟合解释了神经元激活中 95% 的变异性。所以我对模型如何构建预激活的描述非常满意！</p><h2>神经元激活</h2><p>ReLU() 是问题中的一种非线性。它将我上面描述的良好贴合的力量向外传播。具体来说，我发现（类似于<a href="https://arxiv.org/abs/2301.05217">尼尔的 grokking 工作</a>）</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{ReLU}[A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)] \approx \frac{1}{2}\left[A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)\right] + A_3\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">余弦</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">。</span></span></span></span></span></span></span></p><p>因此，功率从单轴项向外投射到跨轴项。</p><p>如果我从神经元激活拟合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{ReLU}(\hat{n}_i)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>中减去线性项，然后拟合上面的横轴振荡表达式，然后添加线性贡献，我可以为神经元激活创建一个猜测。但是，如果我用这些猜测替换神经元激活，那么性能真的很糟糕！损失从 0.0181 增加到 2.496，准确率从 99.94% 下降到 19.79%。</p><p>相反，如果我修改先前的猜测以提高交叉频率项的幅度，那么性能会好得多！更具体地说，我采用与上述相同的过程，但在拟合后我将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span>提高了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>倍。用这种增强的近似替换神经元可将损失降低至 0.091，准确度提高至 98.79%。并不完美，但看起来神经元确实在很大程度上使用 ReLU 线性项和这些横轴项的组合来计算解决方案。</p><p>我对我在这里的解释并不完全满意，但我也想在今晚从圣诞节到新年休假之前结束对这个玩具问题的研究，所以让我们继续吧！</p><h1>神经元到logit操作</h1><p>如果神经元激活<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span>已知（形状为 [batch_a, batch_b, d_mlp] 的矩阵），则可以通过乘以矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit} = W_{\rm out} W_{\rm U}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">U</span></span></span></span></span></span></span></span></span></span></span>来恢复 logits，其形状为 [ d_mlp，d_vocab]。</p><p>如果我对标记 0-99 沿词汇方向进行傅立叶变换<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">然后</span></span></span></span></span></span></span></span></span></span></span>取所有这些标记的平均功率谱，我会在相同的特征频率处看到相同的三个强峰值（0.33、0.39、0.48） ，我还看到一个新峰值出现在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.01"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.01</span></span></span></span></span></span></span>处。该峰值与激活中的线性特征相关，其他三个峰值与上面检查的振荡特征相关。请参阅下面的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span></span></span></span></span></span>中神经元激活和 [d_vocab] 向量的一些示例： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/a7ehwv9pt5bijwvhv9jh" alt="该图像的 alt 属性为空；它的文件名是sample_neurons_w_logit-2.png"></p><p>有许多神经元，如左侧的神经元 10。当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>都很大时，该神经元特别会抑制低值 logits 并提高高值 logits（这可以从顶部和中间行图的组合中读出）。第二行中的神经元 17 提高了小数字标记和大数字标记的 logit 值（后者似乎是模型学习内容的错误？），但当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>大小相似并且都很小。在这两种情况<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">下</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">t</span></span></span></span></span></span></span></span></span></span></span>向量中的主频率为 0.01，因为它包含一个平滑的正弦特征，该特征会限制某些标记并增强其他标记。</p><p>右边的两个情节展示了不同的故事。神经元 75 和 125 是振荡的，它们以振荡的方式影响 logits。这些神经元的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">的</span></span></span></span></span></span></span></span></span></span></span>振荡具有与神经元本身相同的特征频率。底部面板图显示功率谱，对于右侧两列（神经元 75 和 125），我绘制了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span></span></span></span></span></span>向量的功率谱，并表示主导神经元频率（在前面的计算中）部分）在垂直线上 - 并且神经元和映射矢量频率之间确实有很好的一致性！</p><p>所以如果我在上一节中是正确的并且振荡神经元有像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span></span></span></span></span></span>这样的术语<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ，则这些向量按以下方式映射：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}(a,b) = A\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)\cos(\omega_i c + \phi_3)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>对于某个振幅<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span>和相位<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> ，其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span></span></span>是来自神经元<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span>的标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>对 logit 的贡献。简化形式是</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}(a,b) = \frac{A}{4}\left(\cos(a' - b' - c') + \cos(a' - b' + c') + \cos(a' + b' - c') + \cos(a' + b' + c')\right),"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.672em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.95em; top: -1.422em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.95em; bottom: -0.676em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.672em;" class="mjx-line"></span></span><span style="height: 1.484em; vertical-align: -0.478em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">余弦</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a' = \omega_i a + \phi_1, b' = \omega_i b + \phi_2, c' = \omega_i c + \phi_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> 。因此，有一些三角项在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a, b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>空间中振荡，并且<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>的值稍微改变这些项的相位，以将 logits 调整到它们需要的值。</p><p>最后，logits 本身是由神经元<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_c = \sum_i \ell_{c,i}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span>的</span></span></span></span></span>总和构建的，其中包含神经元 10 和 17 等线性项，然后是由线性项。</p><h1>洛吉特人</h1><p>最后，我们得到如下所示的 logit（此处绘制的是 0、25、50、75 的 logit 映射）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/oyl7fx3fyuu9pnzyjql9" alt="该图像的 alt 属性为空；它的文件名是 logit_values.png"></p><p>我突然想到的一件事是，这些模式不再在二维上振荡——它们似乎仅在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a - b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>的值方面振荡。这表明模型设置了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>的值，以便它可以用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(a' + b' \pm c')"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span></span></span></span></span></span>剔除振荡项<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(a' + b' \pm c')"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> （我没时间测试这个）。</p><p>这也具有直观意义：沿着我上面绘制的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a, b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>平面中的对角线，减法的结果始终相同！如果将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>加 1，同时将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>加 1，则两者之间的差保持不变。因此，模型学习一个在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a-b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>方向上变化和振荡的解，但在垂直于该方向的方向上完全恒定，这是有道理的。整洁的！</p><h1>包起来</h1><p>我有大约 85% 的信心认为我已经弄清楚了该模型用于进行减法的算法。有几个地方（特别是在最后，在激活函数之后）事情变得有点仓促和手动，如果我有无限的时间花在这个模型上，那么我会巩固和完善那里的一些概念。但我不！</p><p>我将结束这个模型，并在下一篇文章中转向其他可能更有趣的事情，但如果有人对我的分析中的漏洞或对模型可能正在做的事情的想法有任何想法，我会这样做。我不是在检查，我很乐意讨论！</p><h2>代码</h2><p>我用来训练模型的代码可以在<a href="https://colab.research.google.com/drive/1Yy59roJgegsNguWLzt6PbxeEoKBVdhGW?usp=sharing">这个 colab 笔记本</a>中找到，并且用于本次调查的笔记本可以<a href="https://github.com/evanhanders/2digit_subtraction_transformer/blob/main/Interpretability_1layer_2digit_subtraction_digit_token.ipynb">在 Github 上找到</a>。</p><h2>致谢</h2><p>再次感谢 Adam Jermyn 的指导和建议，并感谢 Philip Quirke 和 Eoin Farrell 参加我们的定期会议。还要感谢 Alex Atanasov 和 Xianjun Yang 本周抽出时间与我会面并讨论 ML/AI！</p><br/><br/> <a href="https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1<guid ispermalink="false"> RABp7ZMw2FGwh4odq</guid><dc:creator><![CDATA[Evan Anders]]></dc:creator><pubDate> Fri, 22 Dec 2023 21:17:30 GMT</pubDate></item></channel></rss>