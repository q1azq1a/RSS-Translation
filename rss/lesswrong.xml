<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>å°‘é”™</title><link/>https://www.lesswrong.com<generator>èŠ‚ç‚¹çš„ RSS</generator><lastbuilddate> 2023 å¹´ 10 æœˆ 31 æ—¥ï¼Œæ˜ŸæœŸäºŒ 16:16:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Self-Blinded L-Theanine RCT]]></title><description><![CDATA[Published on October 31, 2023 3:24 PM GMT<br/><br/><table><thead><tr><th>è¿½è¸ªä»·å€¼</th><th>æ•ˆåº”å¤§å° dï¼ˆÎ»ã€pã€Ïƒ å˜åŒ–ï¼‰</th><th>æ•ˆåº”å¤§å° dï¼ˆÎ»ã€pã€Ïƒ å˜åŒ–ï¼‰</th></tr></thead><tbody><tr><td></td><td> 200 æ¯«å…‹å’–å•¡å› ï¼ˆn=1ï¼Œm=50ï¼‰</td><td> 500 æ¯«å…‹ L-èŒ¶æ°¨é…¸ï¼ˆn=1ï¼Œm=50ï¼‰</td></tr><tr><td>å¯¹æ•°åˆ†æ•°ç‰©è´¨é¢„æµ‹<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-1" id="fnref-GyZpAd89WXpA23wFY-1">[1]</a></sup></td><td> -0.6</td><td> -0.7</td></tr><tr><td>å¸æ”¶</td><td>0.61ï¼ˆÎ»=13.3ï¼Œp=0.00017ï¼Œ-0.072ï¼‰</td><td> 0.04ï¼ˆÎ»=1.38ï¼Œp=0.77ï¼Œ-0.07ï¼‰</td></tr><tr><td>æ­£å¿µ</td><td>0.58ï¼ˆÎ»=11.8ï¼Œp=0.0007ï¼Œ0.021ï¼‰</td><td> 0.12ï¼ˆÎ»=0.72ï¼Œp=0.89ï¼Œ-0.018ï¼‰</td></tr><tr><td>ç”Ÿäº§ç‡</td><td>0.58 (Î»=28.9, p=1.3 <sup>-12</sup> , 0.11)</td><td> -0.28ï¼ˆÎ»=5.51ï¼Œp=0.109ï¼Œ0.03ï¼‰</td></tr><tr><td>åˆ›é€ åŠ›</td><td>0.45 (Î»=51, p=4.6 <sup>-27</sup> , 0.09)</td><td> -0.12ï¼ˆÎ»=5.05ï¼Œp=0.14ï¼Œ-0.04ï¼‰</td></tr><tr><td>å¹¸ç¦</td><td>0.27ï¼ˆÎ»=10.6ï¼Œp=0.002ï¼Œ0.3ï¼‰</td><td> 0.16ï¼ˆÎ»=3.98ï¼Œp=0.27ï¼Œ-0.155ï¼‰</td></tr><tr><td>æ»¡æ„</td><td>0.13ï¼ˆÎ»=7.66ï¼Œp=0.02ï¼Œ0.47ï¼‰</td><td> 0.25ï¼ˆÎ»=6.83ï¼Œp=0.04ï¼Œ-0.04ï¼‰</td></tr><tr><td>æ¾å¼›</td><td>-0.11ï¼ˆÎ»=5ï¼Œp=0.15ï¼Œ0.42ï¼‰</td><td> 0.12ï¼ˆÎ»=1.5ï¼Œp=0.74ï¼Œ0.02ï¼‰</td></tr><tr><td>è´æ“<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-2" id="fnref-GyZpAd89WXpA23wFY-2">[2]</a></sup></td><td> -0.14ï¼ˆÎ»=1.9ï¼Œp=0.64ï¼Œ0.11ï¼‰</td><td> -0.03ï¼ˆÎ»=1.15ï¼Œp=0.8ï¼Œ0.25ï¼‰</td></tr><tr><td>æŠ½è®¤å¡è½»æ¾</td><td>0.003ï¼ˆÎ»â‰ˆâˆï¼Œpâ‰ˆ0ï¼Œ-0.009ï¼‰</td><td> -0.072ï¼ˆÎ»=âˆï¼Œpâ‰ˆ0ï¼Œ-0.01ï¼‰</td></tr><tr><td>æŠ½è®¤å¡è½»æ¾ç³»æ•°</td><td>-0.039ï¼ˆÎ»â‰ˆâˆï¼Œpâ‰ˆ0ï¼Œ-32.7ï¼‰</td><td> 0.0026ï¼ˆÎ»=âˆï¼Œpâ‰ˆ0ï¼Œ-18.9ï¼‰</td></tr><tr><td>æŠ½è®¤å¡æ–°é—´éš”</td><td>0.011ï¼ˆÎ»â‰ˆâˆï¼Œpâ‰ˆ0ï¼Œ-1.88ï¼‰</td><td> -0.016 (Î»=âˆ, pâ‰ˆ0, 3.1)</td></tr><tr><td>æ¯å¼ æŠ½è®¤å¡çš„æ—¶é—´<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-3" id="fnref-GyZpAd89WXpA23wFY-3">[3]</a></sup></td><td> 0.006ï¼ˆÎ»â‰ˆâˆï¼Œpâ‰ˆ0ï¼Œ273.4ï¼‰</td><td> 0.003ï¼ˆÎ»=âˆï¼Œpâ‰ˆ0ï¼Œ13.66ï¼‰</td></tr></tbody></table><blockquote><p> L-èŒ¶æ°¨é…¸ä¸å’–å•¡å› åœ¨æ³¨æ„åŠ›è½¬æ¢<a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a>å’Œè­¦è§‰æ€§<a href="https://examine.com/supplements/caffeine/research/#ref-319"><sup>[319]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-320"><sup>[320]</sup></a>æ–¹é¢å…·æœ‰ååŒä½œç”¨ï¼Œå¹¶é™ä½å¯¹å¹²æ‰°ï¼ˆæ³¨æ„åŠ›ï¼‰çš„æ•æ„Ÿæ€§ã€‚ <a href="https://examine.com/supplements/caffeine/research/#ref-320"><sup>[320][321]</sup></a>ç„¶è€Œï¼Œè­¦è§‰æ€§ä¼¼ä¹æ˜¯ç›¸å¯¹ä¸»è§‚çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¸æ˜¯è¿™ä¸¤ç§åŒ–åˆç‰©ä¹‹é—´å¯é çš„å¢åŠ ï¼Œ <a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a>å¹¶ä¸”æƒ…ç»ªçš„å¢åŠ è¦ä¹ˆå­˜åœ¨ï¼Œè¦ä¹ˆä¸å­˜åœ¨ã€‚ <a href="https://examine.com/supplements/caffeine/research/#ref-322"><sup>[322]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-323"><sup>[323]</sup></a>è¿™å¯èƒ½æ˜¯ç”±äºèŒ¶æ°¨é…¸æœ¬èº«ä¸ä¸Šè¿°å‚æ•°ç›¸å…³çš„ä¿ƒæ™ºä½œç”¨ç›¸å¯¹è¾ƒå·®ï¼Œä½†ä¼šå¢å¼ºå’–å•¡å› çš„ä½œç”¨ï¼Ÿä¸€äº›ç ”ç©¶ç¡®å®æŒ‡å‡ºï¼ŒèŒ¶æ°¨é…¸æœ¬èº«å¹¶ä¸å½±å“ä¸Šè¿°å‚æ•°ã€‚ <a href="https://examine.com/supplements/caffeine/research/#ref-324"><sup>[324]</sup></a>å› æ­¤ï¼Œä»»ä½•å¯¹å’–å•¡å› çš„ä¸æ•æ„Ÿæˆ–ä¹ æƒ¯éƒ½ä¼šé™ä½è¯¥ç»„åˆçš„æ•ˆæœï¼Œå› ä¸ºL-èŒ¶æ°¨é…¸å¯èƒ½é€šè¿‡å’–å•¡å› å‘æŒ¥ä½œç”¨ã€‚</p><p>åœ¨æ³¨æ„åŠ›é›†ä¸­äºé•¿æ—¶é—´å•è°ƒçš„ä»»åŠ¡æ–¹é¢ï¼ŒL-èŒ¶æ°¨é…¸ä¼¼ä¹ä¸å’–å•¡å› æ²¡æœ‰ååŒä½œç”¨ã€‚ <a href="https://examine.com/supplements/caffeine/research/#ref-325"><sup>[325]</sup></a></p></blockquote><p> <em>â€”â€”å¡è¿ˆå‹’Â·å¸•ç‰¹å°”ï¼Œ</em> <a href="https://examine.com/supplements/caffeine/research/#3JD9zlr-nutrient-nutrient-interactions_3JD9zlr-l-theanine"><em>ã€Šå’–å•¡å› </em></a><em>ã€‹ï¼Œ2023</em></p><p>å¦è¯·å‚é˜…<a href="https://examine.com/supplements/theanine/research/">Examine</a> ã€ <a href="https://en.wikipedia.org/wiki/Theanine">Wikipedia</a>å’Œ<a href="https://gwern.net/Nootropics#L-Theanine">Gwern</a> ã€‚</p><p> <a href="./doc/meditation/science/classification_of_human_brain_attention_focused_on_meditation_effected_by_l-theanine_acid_in_oolong_tea_srimaharaj_et_al_2018.pdf">è¥¿è’‚å¸•å¸•è“¬ç­‰äººã€‚ 2018 å¹´ï¼Œ</a>æµ‹è¯•äº† 10 åå¤§å­¦ç”Ÿé€šè¿‡ä¹Œé¾™èŒ¶æ·»åŠ æœªæŒ‡å®šæ•°é‡çš„ L-èŒ¶æ°¨é…¸å¯¹å†¥æƒ³çš„å½±å“ï¼ˆä¼¼ä¹æ˜¯ééšæœºçš„ï¼‰ã€‚é€šè¿‡è„‘ç”µå›¾æ”¶é›†çš„æ•°æ®è¡¨æ˜ï¼Œå†¥æƒ³æœŸé—´çš„é˜¿å°”æ³•æ³¢åœ¨ç»Ÿè®¡ä¸Šæ˜¾ç€å¢åŠ ï¼ˆå°½ç®¡å°šä¸æ¸…æ¥šå†¥æƒ³æŒç»­äº†å¤šé•¿æ—¶é—´ï¼‰ã€‚</p><p>è¿™å¼ çº¸å¾ˆç³Ÿç³•ã€‚è‹±è¯­å¤ªå¯æ€•äº†ï¼Œè¯»èµ·æ¥æ„Ÿè§‰æˆ‘ä¸­é£äº†ï¼Œä½†å¦‚æœä»–ä»¬æ“…é•¿æŠ¥å‘Šæ–¹æ³•é‚£å°±å¤ªå¥½äº†ï¼Œä½†ä»–ä»¬ä¸æ“…é•¿ï¼ˆç¼ºå°‘ L-èŒ¶æ°¨é…¸çš„é‡å’Œå†¥æƒ³çš„æŒç»­æ—¶é—´ï¼Œä»–ä»¬è¿˜æåˆ°é˜…è¯»æ–‡ç« å‰é¢çš„å†…å®¹ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯æ§åˆ¶æ´»åŠ¨ï¼Œä½†å®ƒä¸ä¼šå†æ¬¡å‡ºç°ï¼Ÿï¼‰ã€‚ä»–ä»¬è¿˜æŠ¥å‘Šåˆ†æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œè€Œä¸æ˜¯æ•ˆæœå¤§å°ï¼Œå¹¶ä¸”ä¸€äº›æ•°å­—æ˜¯æ¥è‡ª Windows Vista é›†ç¾¤åº”ç”¨ç¨‹åºçš„å±å¹•æˆªå›¾ã€‚</p><p><a href="https://examine.com/supplements/theanine/research/">æ£€æŸ¥</a>åŒæ„ L-èŒ¶æ°¨é…¸çš„è®¤çŸ¥æ•ˆæœï¼ˆå¦‚æœä¸æ˜¯ä¸“é—¨é’ˆå¯¹å†¥æƒ³ï¼‰ï¼š</p><blockquote><p>äººä»¬å¤šæ¬¡æ³¨æ„åˆ°ï¼Œè¡¥å……æ ‡å‡†å‰‚é‡ï¼ˆ50-250 æ¯«å…‹ï¼‰çš„ L-èŒ¶æ°¨é…¸å¯ä»¥å¢åŠ å¥åº·äººçš„ Î± æ³¢ã€‚è¿™ç§æƒ…å†µå¯èƒ½åªå‘ç”Ÿåœ¨åŸºçº¿ç„¦è™‘ç¨‹åº¦è¾ƒé«˜çš„äºº<a href="https://examine.com/supplements/theanine/research/#ref-25"><sup>[25]</sup></a> <a href="https://examine.com/supplements/theanine/research/#ref-26"><sup>[26]</sup></a>æˆ–å¤„äºå‹åŠ›æ—¶æœŸï¼ˆé˜³æ€§<a href="https://examine.com/supplements/theanine/research/#ref-14"><sup>[14]</sup></a>å’Œé˜´æ€§<a href="https://examine.com/supplements/theanine/research/#ref-27"><sup>[27]</sup></a>ç»“æœï¼‰çš„äººèº«ä¸Šï¼Œä½†ä¹Ÿæœ‰äººæŒ‡å‡ºï¼Œåœ¨é—­çœ¼ä¼‘æ¯æœŸé—´ä¹Ÿä¼šå‘ç”Ÿè¿™ç§æƒ…å†µ<a href="https://examine.com/supplements/theanine/research/#ref-5"><sup>[5]</sup></a>å¦‚åœ¨è§†è§‰ç©ºé—´ä»»åŠ¡æœŸé—´<a href="https://examine.com/supplements/theanine/research/#ref-16"><sup>[16]</sup></a>æ‘„å…¥åçº¦ 30-45 åˆ†é’Ÿã€‚ <a href="https://examine.com/supplements/theanine/research/#ref-5"><sup>[5]</sup></a> <a href="https://examine.com/supplements/theanine/research/#ref-4"><sup>[4]</sup></a>çœ‹æ¥åªæœ‰Î±-1æ³¢ï¼ˆ8-10Hzï¼‰å—åˆ°å½±å“ï¼Œè€Œå¯¹Î±-2æ³¢ï¼ˆ11-13Hzï¼‰æ²¡æœ‰å½±å“ã€‚ <a href="https://examine.com/supplements/theanine/research/#ref-4"><sup>[4]</sup></a></p></blockquote><p><em>æ¯”å°”Â·å¨åˆ©æ–¯ï¼Œ</em> <a href="https://examine.com/supplements/theanine/research/"><em>ã€ŠèŒ¶æ°¨é…¸ã€‹</em></a> <em>ï¼Œ2022</em></p><p>è™½ç„¶æˆ‘å¯¹â€œå…¶ä»–å¥åº·çš„æ‚£è€…â€ä¸­Î±æ³¢çš„å¢åŠ æ„Ÿåˆ°å›°æƒ‘â€½</p><p>æ­¤å¤–ï¼Œå®ƒè¿˜æŒ‡å‡ºå†…å­˜ç•¥æœ‰å¢åŠ ï¼š</p><blockquote><p>ä¸€é¡¹ç ”ç©¶ä½¿ç”¨ä¸€ç§åä¸º LGNC-07 çš„è¡¥å……å‰‚ï¼ˆ360 æ¯«å…‹ç»¿èŒ¶æå–ç‰©å’Œ 60 æ¯«å…‹èŒ¶æ°¨é…¸ï¼›æ¯æ—¥ä¸‰æ¬¡ï¼ŒæŒç»­ 16 å‘¨ï¼‰å¯¹æ‚£æœ‰è½»åº¦è®¤çŸ¥éšœç¢çš„äººè¿›è¡Œç ”ç©¶ï¼Œæ ¹æ® MMSE è¯„åˆ†å‘ç°ï¼Œè¡¥å……å‰‚ä¸å»¶è¿Ÿè¯†åˆ«å’Œå³æ—¶å›å¿†è¯„åˆ†çš„æ”¹å–„ç›¸å…³ï¼Œä¸”æ²¡æœ‰å½±å“å¯¹è¨€è¯­å’Œè§†è§‰ç©ºé—´è®°å¿†çš„å½±å“ï¼ˆRey-Kim æµ‹è¯•ï¼‰ã€‚ <a href="https://examine.com/supplements/theanine/research/#ref-17"><sup>[17]</sup></a></p></blockquote><p><em>æ¯”å°”Â·å¨åˆ©æ–¯ï¼Œ</em> <a href="https://examine.com/supplements/theanine/research/"><em>ã€ŠèŒ¶æ°¨é…¸ã€‹</em></a> <em>ï¼Œ2022</em></p><h3>å®éªŒ Bï¼šè‡ªç›² RCT</h3><p>è¿™æ¬¡æˆ‘æ˜ç¡®åœ°å°†å†¥æƒ³åˆ†ä¸ºä¸“æ³¨éƒ¨åˆ†ï¼ˆå‰15åˆ†é’Ÿï¼‰å’Œæ­£å¿µéƒ¨åˆ†ï¼ˆæœ€å30åˆ†é’Ÿï¼‰ã€‚</p><ul><li>å‡†å¤‡æ—¶é—´ï¼š93åˆ†é’Ÿ</li><li>L-èŒ¶æ°¨é…¸ä¸¸çš„æˆæœ¬ï¼š<ul><li> L-èŒ¶æ°¨é…¸ä¸¸çš„æˆæœ¬ï¼š <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{~0.25â‚¬}{\text{500mg L-theanine pill}} \cdot 25 \text{ 500mg L-theanine pills}=6.25â‚¬"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 6.791em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 9.604em; top: -1.706em;"><span class="mjx-mrow" style=""><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">&nbsp;</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.25</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">â‚¬</span></span></span></span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 9.604em; bottom: -0.899em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">500mg L-èŒ¶æ°¨é…¸ä¸¸</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 6.791em;" class="mjx-line"></span></span><span style="height: 1.843em; vertical-align: -0.636em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">â‹…</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">25</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">500 æ¯«å…‹ L-èŒ¶æ°¨é…¸è¯ä¸¸</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6.25</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">æ¬§å…ƒ</span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></li><li>ç©ºèƒ¶å›Šæˆæœ¬ï¼š <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.75â‚¬"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.75</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">æ¬§å…ƒ</span></span></span></span></span></span></span></span></span></li></ul></li></ul><p>é£Ÿç”¨æ—¶æ³¨æ„äº‹é¡¹ï¼š</p><ul><li>ç¬¬ä¸€å‰‚ï¼šè£…ä¿¡å°æ—¶çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œä¸å°å¿ƒå¼„çäº†è‡ªå·±ã€‚</li><li>ç¬¬ 19 å‰‚ï¼šæœç”¨ L-èŒ¶æ°¨é…¸å¹¶è¿›è¡Œå¸¸è§„æ²»ç–—ï¼Œç„¶åå°ç¡ä¸€ä¼šï¼Œ3 å°æ—¶åé†’æ¥ã€‚</li><li>ç¬¬43å‰‚ï¼šé†’æ¥æ—¶å‡ºç°â€œè„‘é›¾â€ï¼Œå†¥æƒ³å˜å¾—è¿Ÿé’ä¸”åˆ°å¤„éƒ½æ˜¯ã€‚ä¹Ÿè®¸æ˜¯å› ä¸ºæˆ‘æ™šä¸Šåœ¨æˆ¿é—´é‡Œæ™¾è¡£æœï¼Ÿå½“å¤©æ™šäº›æ—¶å€™è¿˜æœç”¨äº†å°¼å¤ä¸æ¥å¯åŠ¨ä¸€ä¸ªéœ€è¦å®Œæˆçš„é¡¹ç›®çš„ä¸€äº›å·¥ä½œã€‚</li></ul><p>å®éªŒä» 2023 å¹´ 6 æœˆ 22 æ—¥åˆ° 2023 å¹´ 9 æœˆ 28 æ—¥è¿›è¡Œï¼Œæœ‰æ—¶æ ·æœ¬ä¹‹é—´ä¼šæœ‰æš‚åœã€‚</p><p>æˆ‘ä½¿ç”¨<a href="http://niplav.site/nootropics#Statistical_Method">ä¸å’–å•¡å› å®éªŒç›¸åŒçš„ç»Ÿè®¡æŠ€æœ¯</a>ï¼Œå¹¶åƒå¾€å¸¸ä¸€æ ·å¼€å§‹æˆ‘å¯¹è¯ä¸¸å«é‡çš„é¢„æµ‹ï¼š</p><pre> <code>>;>;>; substances=pd.read_csv(&#39;../../data/substances.csv&#39;) >;>;>; experiment=&#39;B&#39; >;>;>; substance=&#39;l-theanine&#39; >;>;>; placebo=&#39;sugar&#39; >;>;>; expa=substances.loc[substances[&#39;experiment&#39;]==experiment].copy() >;>;>; expa[&#39;datetime&#39;]=pd.to_datetime(expa[&#39;datetime&#39;], utc=True) >;>;>; probs=np.array(expa[&#39;prediction&#39;]) >;>;>; substances=np.array(expa[&#39;substance&#39;]) >;>;>; outcomes=np.array([0 if i==&#39;sugar&#39; else 1 for i in substances]) >;>;>; np.mean(list(map(lambda x: math.log(x[0]) if x[1]==1 else math.log(1-x[0]), zip(probs, outcomes)))) -0.705282842369643</code></pre><p>è¿™ä¸å¤ªå¥½ã€‚äº‹å®ä¸Šï¼Œå®ƒæ¯”æœºä¼šç¨å·®ä¸€äº›ï¼ˆçº¦ä¸º -0.693ï¼‰ã€‚å¯¹äº L-èŒ¶æ°¨é…¸æ¥è¯´ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¥½å…†å¤´ï¼Œäº‹å®ä¸Šï¼Œæƒ…å†µä¼šå˜å¾—æ›´ç³Ÿã€‚æˆ‘ä½¿ç”¨ä¸Šæ¬¡å®éªŒä¸­çš„å¹¿ä¹‰å’Œå‹ç¼©ä»£ç æ¥è·å¾—å…¶ä»–ç»“æœï¼Œå®ƒä»¬å¹¶æ²¡æœ‰ä¸º L-èŒ¶æ°¨é…¸æŒ‡å‡ºä¹è§‚çš„å›¾ç‰‡ï¼š</p><pre> <code>>;>;>; analyze(&#39;B&#39;, &#39;l-theanine&#39;, &#39;sugar&#39;) absorption mindfulness productivity creativity happy content relaxed horny ease factor ivl time d 0.040887 0.124170 -0.278448 -0.116001 0.164261 0.254040 0.119069 -0.031665 -0.072098 0.002561 -0.015955 0.003073 Î» 1.378294 0.720780 5.517769 5.049838 3.983760 6.833004 1.496601 1.148131 inf inf inf inf p 0.765758 0.894798 0.109735 0.146420 0.266491 0.045270 0.740705 0.813279 0.000000 0.000000 0.000000 0.000000 dÏƒ -0.067847 -0.017736 0.039855 -0.043241 -0.155797 -0.046668 0.019655 0.251454 -0.016542 -18.901846 3.108518 13.660820</code></pre><p>å®ƒä¼šé™ä½ç”Ÿäº§åŠ›å’Œåˆ›é€ åŠ›ï¼ˆè™½ç„¶åœ¨ç»Ÿè®¡ä¸Š<em>å¹¶ä¸</em>æ˜¾ç€ï¼Œä½†å®ƒæ­£åœ¨å®ç°ï¼‰ï¼Œä½†è‡³å°‘å®ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ”¹å–„äº†æˆ‘çš„æƒ…ç»ªï¼ˆå°½ç®¡è¿™äº›ç»“æœï¼Œé™¤äº†æ»¡è¶³ä¹‹å¤–ï¼Œä¹Ÿå¯èƒ½æ˜¯ç”±äºéšæœºæœºä¼šé€ æˆçš„ï¼‰ã€‚æŠ½è®¤å¡ä¹Ÿæ²¡æœ‰æ˜ç¡®çš„æ•ˆæœå¤§å°ã€‚</p><p>æ‰€ä»¥æˆ‘è®¤ä¸º L-èŒ¶æ°¨é…¸å¾ˆéš¾é€šè¿‡ã€‚ä¹Ÿè®¸ä¸å’–å•¡å› ç»“åˆä½¿ç”¨æ•ˆæœæ›´å¥½ï¼Ÿ </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-GyZpAd89WXpA23wFY-1" class="footnote-item"><p>è¶Šé«˜è¶Šå¥½ã€‚ <a href="#fnref-GyZpAd89WXpA23wFY-1" class="footnote-backref">â†©ï¸</a></p></li><li id="fn-GyZpAd89WXpA23wFY-2" class="footnote-item"><p>è¿™é‡Œå€¼æ˜¯è¶Šé«˜è¶Šå¥½è¿˜æ˜¯è¶Šä½è¶Šå¥½å¹¶ä¸æ¸…æ¥šã€‚ <a href="#fnref-GyZpAd89WXpA23wFY-2" class="footnote-backref">â†©ï¸</a></p></li><li id="fn-GyZpAd89WXpA23wFY-3" class="footnote-item"><p>è¿™é‡Œè¾ƒé«˜æˆ–è¾ƒä½å€¼çš„ä»·å€¼å¹¶ä¸æ˜ç¡®ï¼šæˆ‘ä»¬æ˜¯å¦æƒ³åœ¨æ¯ä¸ªæŠ½è®¤å¡ä¸ŠèŠ±è´¹æ›´å¤šæ—¶é—´ï¼Œæˆ–è€…æˆ‘ä»¬æ»¡è¶³äºå¿«é€Ÿä½†é©¬è™çš„æ€§èƒ½ï¼Ÿ <a href="#fnref-GyZpAd89WXpA23wFY-3" class="footnote-backref">â†©ï¸</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/Am9oRbY3bgwE55TxL/self-blinded-l-theanine-rct#comments">è®¨è®º</a>]]>;</description><link/> https://www.lesswrong.com/posts/Am9oRbY3bgwE55TxL/self-blinded-l-theanine-rct<guid ispermalink="false"> Am9oRbY3bgwE55TxL</guid><dc:creator><![CDATA[niplav]]></dc:creator><pubDate> Tue, 31 Oct 2023 15:24:57 GMT</pubDate> </item><item><title><![CDATA[AI Safety 101 - Chapter 5.2 - Unrestricted Adversarial Training]]></title><description><![CDATA[Published on October 31, 2023 2:34 PM GMT<br/><br/><h1>ä»‹ç»</h1><p>æœ¬æ–‡æ‘˜è‡ª AGISF 2023 è¯¾ç¨‹çš„â€œå¯æ‰©å±•ç›‘ç£çš„å¯¹æŠ—æ€§æŠ€æœ¯â€éƒ¨åˆ†ï¼Œè¯¥è¯¾ç¨‹äº 2023 å¹´ 4 æœˆåœ¨å·´é»ä¹Œå°”å§†é«˜ç­‰å¸ˆèŒƒå­¦é™¢ä¸¾è¡Œ<strong>ã€‚å…¶ç›®çš„æ˜¯ä¸ºè¯»è€…æä¾›æœ¬æ¬¡ä¼šè®®è®¡åˆ’çš„åŸºæœ¬æ–¹é¢çš„ç®€æ˜æ¦‚è¿°è°å¯èƒ½ä¸ä¼šæ·±å…¥ç ”ç©¶é¢å¤–çš„èµ„æºã€‚</strong>æœ¬æ–‡æ¡£æ—¨åœ¨æ¶µç›– 80/20 çš„ä¼šè®®å†…å®¹ï¼Œè¦æ±‚æ‚¨å¯¹æ‰€æ¶µç›–çš„å…¶ä»–ææ–™æœ‰æœ€ä½é™åº¦çš„ç†Ÿæ‚‰ã€‚æˆ‘è¯•å›¾å°†å„ç§æ–‡ç« è¿æ¥åˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å’Œè¿è´¯çš„å™è¿°ä¸­ã€‚æ‚¨å¯ä»¥åœ¨æ­¤<a href="https://docs.google.com/document/d/1ZwR2rhlQClLwxyzVTnWv_ntwQXFzc057IXePUX2SLKc/edit?usp=sharing"><u>é¡µé¢</u></a>ä¸Šæ‰¾åˆ° AGISF çš„å…¶ä»–æ‘˜è¦ã€‚æœ¬æ‘˜è¦å¹¶é AGISF å®˜æ–¹å†…å®¹ã€‚ <a href="https://docs.google.com/document/d/1bsHau2v9EPoVGXc9iRZvVUF-kjfLb3D0CKEKik_J4tQ/edit"><u>gdoc</u></a>å¯åœ¨è¯„è®ºæ¨¡å¼ä¸‹è®¿é—®ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶å‘è¡¨è¯„è®ºã€‚ä¸åŒçš„éƒ¨åˆ†å¤§éƒ¨åˆ†å¯ä»¥ç‹¬ç«‹é˜…è¯»ã€‚</p><p>æ„Ÿè°¢ Jeanne Salleã€Amaury Lorin å’Œ ClÃ©ment Dumas æä¾›çš„æœ‰ç”¨åé¦ˆä»¥åŠå¯¹æœ¬æ–‡æŸäº›éƒ¨åˆ†çš„è´¡çŒ®ã€‚</p><h1>å¤§çº²</h1><p>ä¸Šå‘¨æˆ‘ä»¬çœ‹åˆ°äº†å¯æ‰©å±•ç›‘ç£çš„ä»»åŠ¡åˆ†è§£ã€‚æœ¬å‘¨ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨å¦å¤–ä¸¤ç§å·²è¢«æè®®å¤§è§„æ¨¡å‘æŒ¥ä½œç”¨çš„æ½œåœ¨å¯¹é½æŠ€æœ¯ï¼šè¾©è®ºå’Œä½¿ç”¨ä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹è¿›è¡Œè®­ç»ƒã€‚</p><p><strong>å¯æ‰©å±•ç›‘ç£å®šä¹‰æé†’ï¼š</strong> â€œä¸ºäº†è´Ÿè´£ä»»åœ°æ„å»ºå’Œéƒ¨ç½²å¼ºå¤§çš„äººå·¥æ™ºèƒ½ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘å¼ºå¤§çš„å¯æ‰©å±•ç›‘ç£æŠ€æœ¯ï¼šä»¥æ ‡ç­¾ã€å¥–åŠ±ä¿¡å·æˆ–æ‰¹è¯„çš„å½¢å¼å‘æ¨¡å‹æä¾›å¯é ç›‘ç£çš„èƒ½åŠ›ï¼Œåœ¨æ¨¡å‹å¼€å§‹è¾¾åˆ°å¤§è‡´äººç±»æ°´å¹³çš„è¡¨ç°ä¹‹åï¼Œå®ƒå°†ä»ç„¶æœ‰æ•ˆâ€ï¼ˆAmodei ç­‰äººï¼Œ2016ï¼‰ã€‚</p><p>è¾©è®ºå’Œå¯¹æŠ—è®­ç»ƒä¹‹æ‰€ä»¥æ”¾åœ¨åŒä¸€ç« ï¼Œæ˜¯å› ä¸ºå®ƒä»¬éƒ½ä½¿ç”¨å¯¹æ‰‹ï¼Œä½†æœ‰ä¸¤ç§ä¸åŒçš„æ„ä¹‰ï¼šï¼ˆ1ï¼‰<strong>è¾©è®º</strong>æ¶‰åŠä¸€ä¸ªè¶…äººäººå·¥æ™ºèƒ½åœ¨å¦ä¸€ä¸ªè¶…äººäººå·¥æ™ºèƒ½çš„<strong>è¾“å‡º</strong>ä¸­å‘ç°é—®é¢˜ï¼Œè€Œäººç±»æ˜¯è¾©è®ºçš„æ³•å®˜ã€‚ (2)<strong>å¯¹æŠ—æ€§è®­ç»ƒ</strong>æ¶‰åŠä¸€ä¸ªäººå·¥æ™ºèƒ½è¯•å›¾æ‰¾åˆ°å¦ä¸€ä¸ªäººå·¥æ™ºèƒ½è¡¨ç°ä¸ä½³çš„<strong>è¾“å…¥</strong>ã€‚è¿™äº›æŠ€æœ¯å°†æ˜¯äº’è¡¥çš„ï¼ŒOpenAI çš„ Superalignment å›¢é˜Ÿ<a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=snfYkaxHFYxmMYJs8"><u>é¢„è®¡</u></a>å°†åˆ©ç”¨æœ¬ç« è®¨è®ºçš„ä¸€äº›æŠ€æœ¯ã€‚</p><p><strong>å‡è®¾</strong>ï¼šè¿™äº›æŠ€æœ¯ä¸ä¾èµ–äºè¿­ä»£æ”¾å¤§æ‰€éœ€çš„ä»»åŠ¡å¯åˆ†è§£æ€§å‡è®¾ï¼ˆä¸Šå‘¨ï¼‰ï¼Œå®ƒä»¬ä¾èµ–äºä¸åŒçš„å¼ºå‡è®¾ï¼šï¼ˆ1ï¼‰å¯¹äº<strong>è¾©è®º</strong>ï¼Œå‡è®¾æ˜¯çœŸå®çš„è®ºç‚¹æ›´æœ‰è¯´æœåŠ›ã€‚ (2) å¯¹äº<strong>æ— é™åˆ¶çš„å¯¹æŠ—æ€§è®­ç»ƒ</strong>ï¼Œå‡è®¾å¯¹æ‰‹å³ä½¿åœ¨å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ä¸­ä¹Ÿå¯ä»¥ç”Ÿæˆç°å®çš„è¾“å…¥ã€‚</p><p><strong>æ¦‚æ‹¬</strong></p><ul><li><strong>å®šä¹‰ï¼š</strong><strong>å¯¹æŠ—æ€§ç¤ºä¾‹</strong>æ˜¯å¯¼è‡´ä¸å½“è¡Œä¸ºçš„è¾“å…¥ï¼Œå°½ç®¡ä¸è®­ç»ƒç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯<strong>ä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹</strong>çš„ä¸€èˆ¬æƒ…å†µï¼Œå³å¯¼è‡´ä¸å½“è¡Œä¸ºçš„è¾“å…¥ä¸ä¸€å®šæ¥è¿‘è®­ç»ƒè¾“å…¥ã€‚</li><li><strong>æ€»ä½“ç­–ç•¥ï¼š</strong>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨æ— é™åˆ¶çš„å¯¹æŠ—æ€§è®­ç»ƒæ¥è®­ç»ƒé«˜çº§äººå·¥æ™ºèƒ½ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®ä¿å³ä½¿åœ¨æœç´¢æ¨¡å‹è¡Œä¸ºä¸å½“çš„ç›®æ ‡è¾“å…¥æ—¶ï¼Œå®ƒçš„è¡Œä¸ºä¹Ÿå§‹ç»ˆæ˜¯å¯æ¥å—çš„ã€‚è¯¥ç­–ç•¥çš„å›°éš¾éƒ¨åˆ†æ˜¯æ‰¾åˆ°é‚£äº›æœ‰é—®é¢˜çš„è¾“å…¥ã€‚æ¥ä¸‹æ¥çš„éƒ¨åˆ†æ¢è®¨äº†ç”Ÿæˆäººå·¥æ™ºèƒ½è¡Œä¸ºä¸å½“çš„æ­¤ç±»è¾“å…¥çš„æ–¹æ³•ã€‚</li><li><strong>æ²¡æœ‰å¯è§£é‡Šæ€§çš„å¯¹æŠ—æ€§è®­ç»ƒï¼š</strong><ul><li><strong>è‡ªåŠ¨æ•°æ®é›†ç”Ÿæˆï¼š</strong> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>é€šè¿‡æ¨¡å‹ç¼–å†™çš„è¯„ä¼°å‘ç°è¯­è¨€æ¨¡å‹è¡Œä¸ºï¼šåšå®¢æ–‡ç« ï¼ˆPerez ç­‰äººï¼Œ2022ï¼‰</u></a>æ˜¯é€šè¿‡åˆ›å»ºåŠè‡ªåŠ¨è¯„ä¼°æ•°æ®é›†æ¥è‡ªåŠ¨è¯„ä¼° LLM çš„ç¬¬ä¸€æ­¥ã€‚</li><li><strong>è‡ªåŠ¨çº¢é˜Ÿï¼šå°†</strong><a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>è¯­è¨€æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œçº¢é˜Ÿï¼ˆPerez et al., 2022ï¼‰</u></a>è¡¨æ˜ï¼Œå¯ä»¥ä½¿ç”¨è¯­è¨€æ¨¡å‹æœ¬èº«è‡ªåŠ¨æŸ¥æ‰¾ä»è¯­è¨€æ¨¡å‹ä¸­å¼•å‘æœ‰å®³æ–‡æœ¬çš„è¾“å…¥ã€‚</li><li><strong>ä½¿ç”¨ GAN çš„æ½œåœ¨ç©ºé—´ï¼š</strong><a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ„å»ºä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹ï¼ˆSong ç­‰äººï¼Œ2018ï¼‰</u></a>ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å®Œå…¨ä»å¤´å¼€å§‹åˆæˆä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚</li></ul></li><li><strong>å¯»æ‰¾å¯¹æŠ—æ€§ä¾‹å­çš„å¯è§£é‡Šæ€§ã€‚æœ‰æ—¶ï¼Œå¦‚æœæ²¡æœ‰å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œå°±å¾ˆéš¾æ‰¾åˆ°å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨æ­¤ä»‹ç»å¯è§£é‡Šæ€§å¦‚ä½•æä¾›å¸®åŠ©ã€‚</strong><ul><li><strong>æœºæ¢°å¯è§£é‡Šæ€§ï¼š</strong><a href="https://distill.pub/2020/circuits/zoom-in/"><u>æ”¾å¤§ï¼šç”µè·¯ç®€ä»‹ï¼ˆOlah ç­‰äººï¼Œ2020ï¼‰</u></a>æœºæ¢°å¯è§£é‡Šæ€§æœ‰ä½•å¸®åŠ©ï¼šä¸€ä¸ªå°ä¾‹å­ã€‚</li><li><strong>åœ¨ NLP ä¸­ï¼š</strong><a href="https://arxiv.org/abs/2205.01663"><u>é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒå®ç°é«˜é£é™©å¯¹é½ï¼ˆZiegler et al., 2022ï¼‰</u></a> ï¼Œé‡‡ç”¨å¯è§£é‡Šæ€§æŠ€æœ¯æŒ‡å¯¼å¯¹æŠ—æ€§æ–‡æœ¬ç¤ºä¾‹çš„æœç´¢è¿‡ç¨‹ã€‚</li><li><strong>å…·æœ‰é«˜çº§å¯è§£é‡Šæ€§ï¼š</strong><a href="https://arxiv.org/abs/2110.03605"><u>å¼ºå¤§çš„ç‰¹å¾çº§å¯¹æ‰‹æ˜¯å¯è§£é‡Šæ€§å·¥å…·ï¼ˆCasper ç­‰äººï¼Œ2021ï¼‰</u></a> ï¼šæœ¬æ–‡ä½¿ç”¨ç‰¹å¾çº§å¯è§£é‡Šæ€§ï¼ˆä¹Ÿç§°ä¸ºåŸºäºæ¦‚å¿µçš„å¯è§£é‡Šæ€§ï¼‰æ¥å‘ç°å¯¹æŠ—æ€§è¾“å…¥ã€‚</li><li><strong>åé—¨æ£€æµ‹çš„å¯è§£é‡Šæ€§ï¼š</strong> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABSï¼šé€šè¿‡äººå·¥å¤§è„‘åˆºæ¿€æ‰«æç¥ç»ç½‘ç»œå¯»æ‰¾åé—¨ï¼ˆLiu ç­‰äººï¼Œ2019ï¼‰</u></a></li><li>ç„¶åï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»æœ‰å…³å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ç†è®ºçš„å…¶ä»–èµ„æº</li></ul></li><li><strong>ç†è®ºåŠ¨æœºï¼šæœ€åæƒ…å†µä¿è¯å’Œå®½æ¾å¯¹æŠ—è®­ç»ƒ</strong><ul><li>æœ¬èŠ‚ä¸»è¦æ€»ç»“äº†<a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>è®­ç»ƒé²æ£’çš„å¯æ ¡æ­£æ€§ï¼ˆChristianoï¼Œ2019ï¼‰</u></a> ï¼Œå®ƒä»‹ç»äº†ä¸€ä¸ªæ€è€ƒå¯¹æŠ—æ€§è®­ç»ƒçš„æ¡†æ¶ã€‚</li><li><strong>å¯ä¿®æ­£æ€§ï¼š</strong>æˆ‘ä»¬ä»‹ç»äº†å¯ä¿®æ­£æ€§çš„æ¦‚å¿µï¼Œå¹¶ä»‹ç»äº†ä¸ºä»€ä¹ˆå¯¹æŠ—æ€§è®­ç»ƒå¯èƒ½ä¸å¤Ÿï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬å¯èƒ½éœ€è¦<i>å®½æ¾çš„</i>å¯¹æŠ—æ€§è®­ç»ƒã€‚</li><li><strong>è½»æ¾å¯¹æŠ—è®­ç»ƒï¼š</strong>æœ¬èŠ‚ä»‹ç»äº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¹‹å‰å°†æŠ€æœ¯åˆ†ä¸ºä¸ä½¿ç”¨å¯è§£é‡Šæ€§æˆ–ä½¿ç”¨å¯è§£é‡Šæ€§çš„æŠ€æœ¯ã€‚</li></ul></li></ul><p><strong>é˜…è¯»æŒ‡å—</strong></p><p>ç”±äºæœ¬æ–‡ç›¸å½“é•¿ï¼Œæˆ‘ä»¬å°†åœ¨ä»‹ç»æ¯ç¯‡è®ºæ–‡ä¹‹å‰ä½¿ç”¨ä»¥ä¸‹æŒ‡ç¤ºï¼š </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  éš¾åº¦ï¼šğŸŸ ğŸŸ âšªâšªâšª é¢„è®¡é˜…è¯»æ—¶é—´ï¼š5 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p>è¿™äº›è¿¹è±¡éå¸¸æœ‰å¯å‘æ€§ã€‚éš¾åº¦åº”è¯¥ä»£è¡¨å¯¹é½åˆå­¦è€…é˜…è¯»è¯¥éƒ¨åˆ†çš„éš¾åº¦ï¼Œä¸é˜…è¯»åŸå§‹è®ºæ–‡çš„éš¾åº¦æ— å…³ã€‚</p><p>æœ‰æ—¶ä½ ä¼šé‡åˆ°ä¸€äº›â€œğŸ“Optional Quizâ€ï¼Œè¿™äº›æµ‹éªŒçš„é€‰æ‹©æ€§å¾ˆå¼ºï¼Œä»…å‡­è¯¾æœ¬ä¸Šçš„æ‘˜è¦æ— æ³•å›ç­”æ‰€æœ‰é—®é¢˜ã€‚</p><h1>ä½¿ç”¨æ— é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹è¿›è¡Œè®­ç»ƒ</h1><p>è¾©è®ºä½¿ç”¨çš„å‡è®¾æ˜¯çœŸå®çš„è®ºç‚¹æ›´æœ‰è¯´æœåŠ›ã€‚åœ¨æ— é™åˆ¶çš„å¯¹æŠ—è®­ç»ƒä¸­ï¼Œå‡è®¾å¯¹æ‰‹å¯ä»¥ç”ŸæˆçœŸå®çš„è¾“å…¥ï¼Œå³ä½¿å¯¹äºå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p><p>ä¸ä¹‹å‰å…³äºè¾©è®ºçš„éƒ¨åˆ†ä¸åŒï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸€ç³»åˆ—ç›¸äº’åŸºç¡€çš„æ–‡ç« ï¼Œæœ¬éƒ¨åˆ†æ›´å¤šçš„æ˜¯ä¸åŒç ”ç©¶é¢†åŸŸçš„æ±‡ç¼–ã€‚ä»äº‹å¯¹æŠ—æ€§è®­ç»ƒçš„ä¸åŒä½œè€…ä¸ä¸€å®šæ‰“ç®—å°†å…¶ç”¨äºå¯æ‰©å±•çš„ç›‘ç£ã€‚ç”±äºæœ€åä¸€èŠ‚â€œæœ€åæƒ…å†µä¿è¯â€ä¸­ç»™å‡ºçš„ç†è®ºåŸå› ï¼Œè¿™äº›æ–‡ç« è¢«åˆ†ä¸ºä¸¤ç»„ï¼Œå³ä½¿ç”¨æˆ–ä¸ä½¿ç”¨å¯è§£é‡Šæ€§çš„æ–‡ç« ã€‚</p><h2>æ€»ä½“ç­–ç•¥</h2><p><strong>é²æ£’æ€§é—®é¢˜ï¼š</strong>ç¥ç»ç½‘ç»œä¸æ˜¯å¾ˆé²æ£’ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡å¯¹åˆå§‹å›¾è¿›è¡Œè½»å¾®æ‰°åŠ¨ï¼Œæˆ‘ä»¬ä»ç½®ä¿¡åº¦ä¸º 57.7% çš„â€œç†ŠçŒ«â€ï¼Œå˜ä¸ºç½®ä¿¡åº¦ä¸º 99.3% çš„â€œé•¿è‡‚çŒ¿â€ï¼Œè€Œä¸¤å¹…å›¾åƒä¹‹é—´çš„å·®å¼‚è‚‰çœ¼å‡ ä¹æ— æ³•å¯Ÿè§‰ã€‚è¿™è¡¨æ˜ç¥ç»ç½‘ç»œä¸ä¸€å®šå…·æœ‰ä¸äººç±»ç›¸åŒçš„å†…éƒ¨è¡¨ç¤ºã€‚ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/nynggizdkbvuag9ipssy"><figcaption><strong>å›¾ï¼š</strong>æ¥è‡ª<a href="https://arxiv.org/abs/1412.6572"><u>è§£é‡Šå’Œåˆ©ç”¨å¯¹æŠ—æ€§ç¤ºä¾‹ï¼ˆIan Goodfellow ç­‰äººï¼Œ2014 å¹´ï¼‰</u></a></figcaption></figure><p><strong>å¯¹æŠ—æ€§è®­ç»ƒã€‚</strong>ä»¥ä¸‹æ˜¯æ¿€å‘å¯¹æŠ—æ€§è®­ç»ƒçš„ä¸€èˆ¬ç­–ç•¥ï¼š <strong>&nbsp;</strong>ä¸ºäº†é¿å…è¿™äº›é²æ£’æ€§é—®é¢˜ï¼Œä¸€ç§ç­–ç•¥æ˜¯æ‰¾åˆ°è®¸å¤šæ¨¡å‹è¡Œä¸ºä¸åˆéœ€è¦çš„ç¤ºä¾‹ï¼Œç„¶åå°†è¿™äº›ç¤ºä¾‹æ³¨å…¥åˆ°åŸºç¡€æ•°æ®é›†ä¸­ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å˜å¾—è¶Šæ¥è¶Šç¨³å¥ã€‚ä½†è¿™ç§æ–¹æ³•çš„å›°éš¾åœ¨äºæ‰¾åˆ°è®¸å¤šï¼ˆå¦‚æœä¸æ˜¯ä¸€ä¸ªï¼‰å¯¹æŠ—æ€§ä¾‹å­ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸åŒçš„æ–¹æ³•æ¥æå‡ºæ­¤ç±»å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚</p><h2>å…³äºå—é™å¯¹æŠ—æ€§ç¤ºä¾‹çš„æé†’ï¼ˆIan Goodfellow ç­‰äººï¼Œ2014ï¼‰ </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  éš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª é¢„è®¡é˜…è¯»æ—¶é—´ï¼š10 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p>ç”Ÿæˆå¯¹æŠ—æ€§æ”»å‡»çš„æœ€ç®€å•æŠ€æœ¯ä¹‹ä¸€æ˜¯å¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•ï¼ˆFGSMï¼‰ï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨<a href="https://arxiv.org/abs/1412.6572"><u>ã€Šè§£é‡Šå’Œåˆ©ç”¨å¯¹æŠ—æ€§ç¤ºä¾‹ã€‹ä¸€æ–‡ä¸­æå‡ºçš„ï¼ˆIan Goodfellow ç­‰äººï¼Œ2014 å¹´ï¼‰</u></a> ã€‚æ­¤è¿‡ç¨‹æ¶‰åŠä»æ•°æ®é›†ä¸­è·å–å›¾åƒå¹¶å‘å…¶æ·»åŠ æ‰°åŠ¨ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å¯»æ±‚æœ€å°åŒ–å›¾åƒæ­£ç¡®åˆ†ç±»æ¦‚ç‡çš„æ‰°åŠ¨ï¼š</p><ol><li>é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—å›¾åƒï¼ˆä¹Ÿæ˜¯å›¾åƒï¼‰ç›¸å¯¹äºæ­£ç¡®åˆ†ç±»æ¦‚ç‡çš„æ¢¯åº¦</li><li>ä¸€æ—¦è®¡ç®—å‡ºå›¾åƒçš„æ¢¯åº¦ï¼ˆå›¾åƒçš„æ¢¯åº¦ä¸å›¾åƒå…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼‰ï¼Œæˆ‘ä»¬åªä¿ç•™æ¯ä¸ªæ¿€æ´»çš„ç¬¦å·ï¼ˆ+1æˆ–-1ï¼‰å¹¶å°†ç»“æœä¹˜ä»¥ä¸€ä¸ªéå¸¸å°çš„æ•°å­—epsilonã€‚</li><li>æˆ‘ä»¬ä»å›¾åƒä¸­å‡å»è¿™ç§æ‰°åŠ¨ã€‚</li></ol><p>æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç»ƒä¹ ä¸­æ‰¾åˆ°æœ‰å…³ FGSM çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š</p><ul><li><a href="https://www.youtube.com/watch?v=CIfsB_EYsVI"><u>è®²åº§ 16 |å¯¹æŠ—æ€§ä¾‹å­å’Œå¯¹æŠ—æ€§è®­ç»ƒ</u></a><u>ã€‚</u></li><li>ä»¥åŠ<a href="https://course.mlsafety.org/"><u>æœºå™¨å­¦ä¹ å®‰å…¨è¯¾ç¨‹</u></a>ç¬¬ 7-8 èŠ‚ã€‚</li><li>æ‚¨å¯ä»¥æŒ‰ç…§æœ¬ç»ƒä¹ ä¸­çš„è¯´æ˜å®æ–½ FGSMï¼š <a href="https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d2/Adversarial_Attacks.ipynb">&nbsp;</a> ğŸ‘¨â€ğŸ’» <a href="https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d2/Adversarial_Attacks.ipynb"><u>å¯¹æŠ—æ€§æ”»å‡»</u></a><u>ã€‚</u></li></ul><p>å¯¹äº<strong>å—é™çš„å¯¹æŠ—æ€§ç¤ºä¾‹</strong>ï¼Œæ‰°åŠ¨å›¾åƒä¸ä¼šåç¦»åŸå§‹å›¾åƒå¤ªè¿œã€‚å…·ä½“æ¥è¯´ï¼Œæ‰°åŠ¨å›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚æœ€å¤šä¸ºæ¯ä¸ªåƒç´  epsilon åƒç´ ï¼ˆå¦‚ä¸Šé¢æ­¥éª¤ 2 ä¸­æ‰€è¿°ï¼‰ã€‚ç›¸åï¼Œå¯¹äº<strong>ä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹</strong>ï¼Œæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯å¯ä»¥ä»»æ„åç¦»åŸå§‹æ•°æ®é›†çš„ç¤ºä¾‹ã€‚å³ä½¿åœ¨æœ€æ¶åŠ£çš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä»¥ä¸å—é™åˆ¶çš„æ–¹å¼æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚</p><p>æ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†æ¢è®¨ç”Ÿæˆäººå·¥æ™ºèƒ½è¡Œä¸ºä¸å½“çš„è¾“å…¥çš„æ–¹æ³•ï¼</p><h2>ä¸ä½¿ç”¨å¯è§£é‡Šæ€§çš„è‡ªåŠ¨å¯¹æŠ—è®­ç»ƒ</h2><p>åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸éœ€è¦å¯è§£é‡Šæ€§æ¥ç”Ÿæˆå¯¹æŠ—æ€§ç¤ºä¾‹çš„å„ç§æŠ€æœ¯ã€‚</p><h3>é€šè¿‡æ¨¡å‹ç¼–å†™çš„è¯„ä¼°å‘ç°è¯­è¨€æ¨¡å‹è¡Œä¸ºï¼ˆPerez ç­‰äººï¼Œ2022ï¼‰ </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  éš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ âšªâšª é¢„è®¡é˜…è¯»æ—¶é—´ï¼š15 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>é€šè¿‡æ¨¡å‹ç¼–å†™çš„è¯„ä¼°å‘ç°è¯­è¨€æ¨¡å‹è¡Œä¸ºï¼ˆPerez ç­‰äººï¼Œ2022ï¼‰</u></a>æ˜¯ Anthropic çš„ä¸€ç¯‡é‡è¦è®ºæ–‡ã€‚äººç±»è¯„ä¼°éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ï¼Œå¦‚æœæ¨¡å‹å˜å¾—æ¯”äººç±»æ›´å¥½ï¼Œé‚£ä¹ˆä½¿ç”¨æ¨¡å‹æ¥ç›‘ç£æ¨¡å‹å¯èƒ½æ˜¯å¯æ‰©å±•ç›‘ç£çš„ç›¸å…³ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä½©é›·æ–¯ç­‰äººã€‚é€šè¿‡ LM è‡ªåŠ¨ç”Ÿæˆè¯„ä¼°ï¼Œéœ€è¦ä¸åŒç¨‹åº¦çš„äººåŠ›ã€‚ä»–ä»¬é€šè¿‡åŠè‡ªåŠ¨åˆ›å»ºä¸åŒçš„æ•°æ®é›†æ¥æµ‹è¯•ä¸åŒçš„å±æ€§ï¼Œå¹¶å‘ç°äº†è®¸å¤šâ€œé€†ç¼©æ”¾â€çš„å®ä¾‹ï¼Œå…¶ä¸­è¾ƒå¤§çš„ LM æ¯”è¾ƒå°çš„ LMâ€œæ›´å·®â€ã€‚ä¾‹å¦‚ï¼š</p><ul><li><strong>é˜¿è°€å¥‰æ‰¿ï¼š</strong>è¾ƒå¤§çš„ LM ä¼šé‡å¤å¯¹è¯ç”¨æˆ·çš„é¦–é€‰ç­”æ¡ˆï¼Œ<ul><li> [æ›´æ–°ï¼šæœ€è¿‘çš„ä¸€ç¯‡åšå®¢æ–‡ç« è¡¨æ˜ï¼Œ <a href="https://www.lesswrong.com/posts/3ou8DayvDXxufkjHD/openai-api-base-models-are-not-sycophantic-at-any-size"><u>OpenAI API åŸºç¡€æ¨¡å‹æ— è®ºå¤§å°éƒ½ä¸ä¼šé˜¿è°€å¥‰æ‰¿ï¼ˆnostalgebraistï¼Œ2023ï¼‰</u></a> ]</li></ul></li><li><strong>å·¥å…·æ€§ç›®æ ‡ï¼š</strong>è¾ƒå¤§çš„ LM è¡¨è¾¾äº†è¿½æ±‚èµ„æºè·å–å’Œç›®æ ‡ä¿å­˜ç­‰ç›®æ ‡çš„æ›´å¤§æ„¿æœ›ï¼Œ</li><li><strong>éè¿‘è§†</strong>è¾ƒå¤§çš„ LM æ›´å®¹æ˜“ä¸ºäº†é•¿æœŸæ”¶ç›Šè€Œç‰ºç‰²çŸ­æœŸæ”¶ç›Šï¼Œ</li><li><strong>æƒ…å¢ƒæ„è¯†</strong>è¾ƒå¤§çš„ LM ä¼¼ä¹æ›´èƒ½æ„è¯†åˆ°è‡ªå·±æ˜¯è¯­è¨€æ¨¡å‹ï¼Œ</li><li><strong>åè°ƒï¼š</strong>è¾ƒå¤§çš„ LM å¾€å¾€æ›´æ„¿æ„ä¸å…¶ä»– AI è¿›è¡Œåè°ƒã€‚ </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/mmat342jpeumgrbuignp"></p><p>ä»äººå·¥æ™ºèƒ½å®‰å…¨çš„è§’åº¦æ¥çœ‹ï¼Œæ›´å¼ºçš„æ€åŠ¿æ„ŸçŸ¥ä¸ä¸€å®šæ˜¯ç†æƒ³çš„ç‰¹æ€§ã€‚ä¸€ä¸ªæ„è¯†åˆ°è‡ªå·±æœ¬è´¨çš„æ¨¡å‹æœ€ç»ˆå¯ä»¥ç†è§£å®ƒæ­£åœ¨æ¥å—è®­ç»ƒå’Œè¯„ä¼°ï¼Œå¹¶æ ¹æ®å®ƒæ‰€å¤„çš„éƒ¨ç½²é˜¶æ®µé‡‡å–ä¸åŒçš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬è¿˜å¯ä»¥æƒ³è±¡å®ƒä¼šè‡ªæˆ‘ä¿®æ”¹è‡ªå·±çš„æƒé‡ï¼ˆçº¿å¤´ï¼Ÿï¼‰ã€‚ä»¥ä¸‹æ˜¯æ€åŠ¿æ„ŸçŸ¥æ‰€æ”¯æŒçš„ä¸€äº›åŠŸèƒ½ç¤ºä¾‹ï¼š</p><ul><li>èƒ½å¤Ÿä½¿ç”¨å½“å‰æ—¥æœŸå’Œæ—¶é—´ï¼Œè¿™å¯¹äºä½¿ç”¨æŸäº› ChatGPT æ’ä»¶éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚é¢„è®¢ç«è½¦ï¼Œ</li><li>èƒ½å¤Ÿåˆ©ç”¨åœ°ç†ä½ç½®ï¼Œ</li><li>çŸ¥é“ ML æ¨¡å‹æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒçš„ï¼Œ</li><li>çŸ¥é“äººç±»æ­£åœ¨è¯„ä¼°å®ƒï¼Œ</li><li> ETCã€‚</li></ul><p>è¿™äº›åŠŸèƒ½ä¹‹æ‰€ä»¥å‡ºç°ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬<a href="https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Why_I_think_Alex_would_have_very_high_situational_awareness"><u>å¾ˆæœ‰ç”¨</u></a>ã€‚æ€åŠ¿æ„ŸçŸ¥ä¸æ˜¯äºŒå…ƒå±æ€§ï¼Œè€Œæ˜¯è¿ç»­å±æ€§ã€‚å°±åƒäººç±»ä¸€æ ·ï¼Œä»ç«¥å¹´åˆ°æˆå¹´ï¼Œå®ƒé€æ¸å‘å±•ã€‚</p><p>è¯·æ³¨æ„ï¼Œä¸Šè¿°å±æ€§ï¼ˆæ€åŠ¿æ„ŸçŸ¥ã€é˜¿è°€å¥‰æ‰¿ã€éè¿‘è§†ç­‰ï¼‰é€šå¸¸è¢«å‡è®¾ä¸ºæ¬ºéª—æ€§å¯¹é½çš„å¿…è¦å…ˆå†³æ¡ä»¶ï¼Œè¯·å‚é˜…<a href="https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks"><u>å­¦ä¹ ä¼˜åŒ–çš„é£é™©</u></a>ï¼ˆHubinger ç­‰äººï¼Œ2018ï¼‰ã€‚ </p><p></p><figure class="image image_resized" style="width:68.37%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ag5fzfjdtjoyvwvje5lb"><figcaption><strong>å›¾ï¼š</strong>æ¥è‡ª<i>&nbsp;</i> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>ä½©é›·æ–¯ç­‰äººï¼Œ2022</u></a></figcaption></figure><p> <strong>RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰çš„é—®é¢˜ã€‚</strong>ä½©é›·æ–¯ç­‰äººã€‚ç ”ç©¶å‘ç°<i>â€œå¤§å¤šæ•°æŒ‡æ ‡é€šå¸¸ä¼šéšç€é¢„è®­ç»ƒæ¨¡å‹è§„æ¨¡å’Œ RLHF æ­¥éª¤æ•°é‡çš„å¢åŠ è€Œå¢åŠ ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€äº›æœ€å…·ä½“çš„è¯æ®ï¼Œè¡¨æ˜å½“å‰çš„æ¨¡å‹åœ¨å¯èƒ½æ¶‰åŠè§„æ¨¡çš„æ–¹é¢æ­£åœ¨ç§¯æåœ°å˜å¾—æ›´åŠ ä»£ç†ï¼Œå¹¶ä¸”å½“å‰çš„å¾®è°ƒæŠ€æœ¯é€šå¸¸ä¼¼ä¹ä¸ä¼šç¼“è§£ï¼Œæœ‰æ—¶ä¼¼ä¹ä¼šæœ‰æ‰€ç¼“è§£ã€‚ç§¯æåœ°è®©æƒ…å†µå˜å¾—æ›´ç³Ÿã€‚â€</i></p><p>ç»ƒä¹ ï¼š</p><ul><li> <strong>RLHF ä¸ºä»€ä¹ˆæå€¡ä»£ç†ï¼Ÿ</strong>ç­”æ¡ˆï¼šäººç±»å–œæ¬¢å‡ ä¹è‡ªä¸»çš„ä»£ç†ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬è¦æ±‚ ChatGPT é‡æ–°ç¼–å†™ç”µå­é‚®ä»¶æ—¶ï¼Œæˆ‘ä»¬ä¸æƒ³ç»™å®ƒ 50 æ¡æŒ‡ä»¤æ¥å®Œæˆä»»åŠ¡ï¼Œæˆ‘ä»¬å¸Œæœ› ChatGPT ä»¥è‡ªä¸»çš„æ–¹å¼ç”¨å°½å¯èƒ½å°‘çš„æŒ‡ä»¤é›†æ¥å®Œæˆä»»åŠ¡ã€‚ä¸å·¥å…·ç›¸æ¯”ï¼Œäººç±»æ›´å–œæ¬¢ä»£ç†ï¼ˆ<a href="https://gwern.net/tool-ai"><u>ä¸ºä»€ä¹ˆå·¥å…· AI æƒ³è¦æˆä¸ºä»£ç† AI</u></a> ï¼‰ã€‚</li><li><strong>ä¸ºä»€ä¹ˆä»£ç†æœºæ„ä¼šå‡ºç°é—®é¢˜ï¼Ÿ</strong>å›ç­”ï¼šä»£ç†æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºè‡ªæ²»ä¼¼ä¹æ¶‰åŠè¶‹åŒçš„æƒåŠ›å¯»æ±‚è¡Œä¸ºï¼Œè€Œè¿™å¯èƒ½æ˜¯ä¸å—æ¬¢è¿çš„ã€‚</li></ul><h3>ä½¿ç”¨è¯­è¨€æ¨¡å‹è¿›è¡Œçº¢é˜Ÿè¯­è¨€æ¨¡å‹ï¼ˆPerez ç­‰äººï¼Œ2022ï¼‰ </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ ğŸŸ âšªâšª éš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ âšªâšª é¢„è®¡é˜…è¯»æ—¶é—´ï¼š15 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p>åœ¨ä¸Šä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¿™äº›æµ‹è¯•ç”¨ä¾‹åœ¨ä¸è®¿é—®ç½‘ç»œæƒé‡çš„æƒ…å†µä¸‹ä¼šå¯¼è‡´ä¸å½“è¡Œä¸ºï¼Œè¿™ä½¿å¾—è¿™æ˜¯ä¸€ç§é»‘ç›’æ”»å‡»ï¼ˆç™½ç›’æ”»å‡»éœ€è¦è®¿é—®æ¨¡å‹æƒé‡ï¼‰ã€‚<a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>å°†è¯­è¨€æ¨¡å‹ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œçº¢é˜Ÿåˆä½œï¼ˆPerez et al., 2022ï¼‰</u></a>æ˜¯å¦ä¸€ç§ç”Ÿæˆâ€œä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹â€çš„æŠ€æœ¯ã€‚ä¸»è¦åŒºåˆ«åœ¨äºæˆ‘ä»¬å¾®è°ƒä¸€äº› LM æ¥æ”»å‡»è¯­è¨€æ¨¡å‹ã€‚ </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Boxï¼šé‡ç‚¹å…³æ³¨å¾®è°ƒ Red LM çš„ç®—æ³•è¿‡ç¨‹ã€‚</strong> </td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:65.36%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/iwpwvaukanmisyxg9thh"><figcaption><strong>å›¾ï¼š</strong><a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>Perez ç­‰äººï¼Œ2022 å¹´</u></a>ã€‚ </figcaption></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p>è¿™æ˜¯ç®€åŒ–çš„è¿‡ç¨‹ï¼š</p><ol><li>æœ¬æ–‡çš„ç›®çš„æ˜¯ç¡®è®¤ç›®æ ‡ LMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰ä¸ä¼šç”Ÿæˆæœ‰å®³å†…å®¹ã€‚æˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„çº¢è‰²åˆ†ç±»å™¨æ¥è¯†åˆ«æœ‰é—®é¢˜çš„å¥å­ã€‚</li><li>ä¸ºäº†æµ‹è¯• Target LM æ˜¯å¦è¡¨ç°ä¸ä½³ï¼Œä»–ä»¬å¯¹å¦ä¸€ç§è¯­è¨€æ¨¡å‹ Red LM è¿›è¡Œäº†å¾®è°ƒï¼ˆé€šè¿‡ä¸‹é¢è§£é‡Šçš„ä¸åŒæ–¹æ³•ï¼‰ï¼Œä»¥ç”Ÿæˆè®¸å¤šæœ‰é—®é¢˜çš„å¯¹æŠ—æ€§å¥å­ã€‚</li><li>æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çº¢è‰²åˆ†ç±»å™¨éªŒè¯ç›®æ ‡ LM çš„è¡Œä¸ºå§‹ç»ˆå¯æ¥å—ã€‚</li></ol></td></tr></tbody></table></figure><p></p><p> Red LM å’Œ Target LM ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•è¿­ä»£ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼š</p><ol><li><strong>åŸºäºæç¤ºçš„ç”Ÿæˆï¼š</strong>æ‰‹åŠ¨ç”Ÿæˆå¯¹æŠ—æ€§é—®é¢˜åˆ—è¡¨ä»¥å¼•å‘ä¸åŒç±»å‹çš„é—®é¢˜ï¼ˆä¾‹å¦‚æ”»å‡»æ€§è¯­è¨€ã€æ•°æ®æ³„æ¼â€¦â€¦ï¼‰ã€‚</li><li><strong>å°‘æ ·æœ¬å­¦ä¹ ï¼š</strong>è¯­è¨€æ¨¡å‹ç»™å‡ºäº†å¯¹æŠ—æ€§é—®é¢˜çš„ç¤ºä¾‹ï¼Œå¹¶ä¸”å¿…é¡»ç”Ÿæˆæ›´å¤šç¤ºä¾‹ã€‚</li><li><strong>æœ‰ç›‘ç£çš„å¾®è°ƒï¼š</strong>ä¸€æ—¦æˆ‘ä»¬ç”Ÿæˆäº†ä¸€äº›ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒã€‚</li><li><strong>å¼ºåŒ–å­¦ä¹ ï¼š</strong>æ¨¡å‹ç»è¿‡å¾®è°ƒåï¼Œå¼ºåŒ–å­¦ä¹ å¯ç”¨äºæœ€å¤§åŒ–åˆ†ç±»å™¨æ‹’ç»ç›®æ ‡ LM å®Œæˆçš„æ¦‚ç‡ã€‚ </li></ol><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"><strong>æ–¹æ¡†ï¼šèšç„¦æ¨¡å¼å´©æºƒ</strong></td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/hccsxw9ryyqifcgcinvn"><figcaption><strong>å›¾ï¼š</strong>æ¥è‡ª<a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>Perez ç­‰äººï¼Œ2022 å¹´çš„</u></a>é™„å½•ã€‚ </figcaption></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>å…³æ³¨</strong> <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><strong><u>æ¨¡å¼å´©æºƒ</u></strong></a><strong>ï¼š</strong>ç®€è¦æ¦‚è¿° RL ä¸­æ­£åˆ™åŒ–å¯¹äºè¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ï¼šå½“æˆ‘ä»¬åœ¨è¯­è¨€æ¨¡å‹ä¸Šæ‰§è¡Œ RL æ—¶ï¼Œå®ƒå¾€å¾€ä¼šæ˜¾ç€é™ä½ç”Ÿæˆå¥å­çš„å¤šæ ·æ€§ã€‚è¿™ç§ç°è±¡ç§°ä¸º<a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>æ¨¡å¼å´©æºƒ</u></a>ã€‚ä¸ºäº†é¿å…æ¨¡å¼å´©æºƒï¼Œæˆ‘ä»¬ä¸ä»…æœ€å¤§åŒ–å¥–åŠ± r(x)ï¼Œè€Œä¸”è¿˜æƒ©ç½šåŸºæœ¬æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬ä½¿ç”¨ KL æ•£åº¦æƒ©ç½šæ¥è¡¡é‡è¿™ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™ç§æƒ©ç½šä» 0.4 å‡å°‘åˆ° 0.3ï¼Œç”Ÿæˆçš„å¥å­çš„å¤šæ ·æ€§å°±ä¼šæ˜æ˜¾é™ä½ã€‚</p></td></tr></tbody></table></figure><p> ğŸ“ æµ‹éªŒï¼š <a href="https://www.ai-alignment-flashcards.com/quiz/perez-red-teaming-blog"><u>https://www.ai-alignment-flashcards.com/quiz/perez-red-teaming-blog</u></a></p><h3>ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ„å»ºæ— é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹ï¼ˆSong ç­‰äººï¼Œ2018ï¼‰ </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ âšªâšªâšª éš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª é¢„è®¡é˜…è¯»æ—¶é—´ï¼š10 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p>è®ºæ–‡<a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>â€œConstructing Unrestricted Adversarial Examples with Generative Modelsâ€ï¼ˆSong et al., 2018</u></a> ï¼‰æå‡ºäº†ä¸€ç§åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸ºå›¾åƒåˆ†ç±»å™¨åˆ›å»ºå¯¹æŠ—æ€§ç¤ºä¾‹çš„æ–°æ–¹æ³•ã€‚ GAN æ˜¯ä¸€ç±»èƒ½å¤Ÿç”Ÿæˆä»¤äººä¿¡æœçš„è‡ªç„¶å›¾åƒçš„æ¨¡å‹ã€‚å¯ä»¥é€šè¿‡ä¼˜åŒ– GAN çš„æ½œåœ¨å‘é‡æ¥ç”Ÿæˆä¿æŒå¯ä¿¡åº¦çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚ç”±äºç”Ÿæˆçš„å›¾åƒæ˜¯ GAN çš„äº§ç‰©ï¼Œå› æ­¤å®ƒä¸ä»…ä¿æŒå¯ä¿¡ï¼ˆè€Œä¸ä»…ä»…æ˜¯éšæœºå™ªå£°å›¾åƒï¼‰ï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿé€šè¿‡ä¼˜åŒ–æ½œåœ¨ç©ºé—´å‘é‡æ¥æ¬ºéª—å›¾åƒåˆ†ç±»å™¨ï¼Œä»¥æœ€å¤§åŒ–å›¾åƒåˆ†ç±»çš„æ¦‚ç‡ã€‚è¢«æ”»å‡»åˆ†ç±»å™¨çš„ä¸åŒç›®æ ‡ç±»åˆ«ã€‚</p><p>è¿™æ˜¯åŸºæœ¬åŸåˆ™ã€‚ä½†äº‹å®ä¸Šï¼Œæˆ‘ä»¬ä¸èƒ½ç®€å•åœ°ä½¿ç”¨ GANï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ä»¥æœ€å¤§åŒ–ç›®æ ‡ç±»çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬åªä¼šåˆ›å»ºç›®æ ‡ç±»çš„å›¾åƒï¼Œä½†è¿™ä¸ä¼šæ˜¯ä¸€ä¸ªå¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œè¿™å°†åªæ˜¯è¿™ä¸ªç›®æ ‡ç±»çš„å¸¸è§„å›¾åƒã€‚å› æ­¤ï¼Œä½¿ç”¨äº†å¦ä¸€ç§ç§°ä¸º AC-GAN çš„æ¶æ„ï¼Œå®ƒå…è®¸æŒ‡å®šè¦ç”Ÿæˆçš„å›¾åƒçš„ç±»åˆ«ï¼Œä»¥ä¾¿èƒ½å¤Ÿä»¥ç±»åˆ«ä¸ºæ¡ä»¶ï¼Œç„¶åä¼˜åŒ–ç›®æ ‡åˆ†ç±»å™¨ä»¥æ£€æµ‹å¦ä¸€ä¸ªç±»åˆ«ï¼š <i>â€œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªè¾…åŠ©åˆ†ç±»å™¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (AC-GAN) ç”¨äºå¯¹æ•°æ®æ ·æœ¬ä¸Šçš„ç±»æ¡ä»¶åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚ç„¶åï¼Œä»¥æ‰€éœ€çš„ç±»åˆ«ä¸ºæ¡ä»¶ï¼Œæˆ‘ä»¬åœ¨ AC-GAN æ½œåœ¨ç©ºé—´ä¸­æœç´¢ï¼Œä»¥æŸ¥æ‰¾å¯èƒ½åœ¨ç”Ÿæˆæ¨¡å‹ä¸‹ä¸”è¢«ç›®æ ‡åˆ†ç±»å™¨é”™è¯¯åˆ†ç±»çš„å›¾åƒâ€ã€‚</i> </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"><strong>æ–¹æ¡†ï¼šå…³æ³¨ GAN å’Œæ½œåœ¨ç©ºé—´ã€‚</strong> </td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tdytycz4t9olbu51ev5t"><figcaption><strong>å›¾ï¼š</strong>æ¥è‡ªæœ¬<a href="https://lars.carius.io/blog/LSVG.html"><u>ç½‘ç«™</u></a>ã€‚</figcaption></figure><p><strong>ä¸“æ³¨äº GAN å’Œæ½œåœ¨ç©ºé—´ã€‚</strong>ä¸Šå›¾å±•ç¤ºäº† GAN çš„æŠ½è±¡ç»“æ„ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å…ƒç´ ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æœ‰æ½œåœ¨å‘é‡ã€‚ç°åœ¨æ‚¨å¯ä»¥å°†å…¶æƒ³è±¡ä¸ºéšæœºå®æ•°çš„å‘é‡ã€‚ç”Ÿæˆå™¨æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒå°†è¿™äº›éšæœºæ•°è½¬æ¢æˆæˆ‘ä»¬ç°åœ¨ç§°ä¸ºâ€œå‡å›¾åƒâ€çš„å›¾åƒã€‚ç¬¬äºŒä¸ªç¥ç»ç½‘ç»œï¼ˆCriticï¼‰äº¤æ›¿æ¥æ”¶ç”Ÿæˆçš„å‡å›¾åƒå’Œæ¥è‡ªæˆ‘ä»¬æ•°æ®é›†çš„çœŸå®å›¾åƒï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåˆ†æ•°ï¼Œè¯•å›¾ç¡®å®šå½“å‰å›¾åƒæ˜¯çœŸå®çš„è¿˜æ˜¯å‡çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¼˜åŒ–æ½œåœ¨å‘é‡ã€‚å›¾ç‰‡<a href="https://lars.carius.io/blog/LSVG.html"><u>æ¥æº</u></a>ã€‚</p><p>å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰è¿™äº›ç”Ÿæˆæ–¹æ³•å’Œæ½œåœ¨ç©ºé—´çš„æ¦‚å¿µï¼Œæ‚¨å¯ä»¥ä»ä»¥ä¸‹è§†é¢‘å¼€å§‹ï¼š<a href="https://www.youtube.com/watch?v=dCKbRCUyop8"><u>ä½¿ç”¨äººå·¥æ™ºèƒ½ç¼–è¾‘é¢éƒ¨</u></a>ï¼Œè¿™äº›è§†é¢‘æ˜¯å¯¹è¿™äº›æ¦‚å¿µçš„å¾ˆå¥½ä»‹ç»ã€‚</p></td></tr></tbody></table></figure><h2>å¯»æ‰¾å¯¹æŠ—æ€§ä¾‹å­çš„å¯è§£é‡Šæ€§</h2><p>åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›å¯»æ‰¾å¯¹æŠ—æ€§ç¤ºä¾‹çš„æŠ€æœ¯ï¼Œä½†æœ‰æ—¶å¦‚æœæ²¡æœ‰å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œå¾ˆéš¾æ‰¾åˆ°å®ƒä»¬ã€‚æˆ‘ä»¬åœ¨æ­¤ä»‹ç»å¯è§£é‡Šæ€§å¦‚ä½•æä¾›å¸®åŠ©ã€‚</p><h3>å…·æœ‰æœºæ¢°å¯è§£é‡Šæ€§</h3><p><strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/p11hixc3rasflrip6mbe"></strong></p><p>åœ¨<a href="https://distill.pub/2020/circuits/zoom-in/"><u>ä¸Šå›¾ä¸­æ”¾å¤§ï¼šç”µè·¯ç®€ä»‹</u></a>ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ±½è½¦æ¢æµ‹å™¨æ˜¯ç”±ä¸Šä¸€å±‚çš„è½¦çª—æ¢æµ‹å™¨ã€è½¦èº«æ¢æµ‹å™¨å’Œè½¦è½®æ¢æµ‹å™¨ç»„è£…è€Œæˆã€‚è¿™äº›æ£€æµ‹å™¨åˆ†åˆ«åœ¨è¿æ¥ç‰¹å¾å›¾ 4b å’Œ 4c çš„å·ç§¯æ ¸çš„é¡¶éƒ¨ã€ä¸­éƒ¨å’Œåº•éƒ¨è¢«æ¿€æ´»ã€‚å¦‚æœæˆ‘ä»¬ä»¥è‡ªä¸Šè€Œä¸‹çš„æ–¹å¼ç¿»è½¬æ±½è½¦ï¼Œè¿™å°†æ˜¯è¯¥å›¾åƒåˆ†ç±»å™¨çš„å¯¹æŠ—æ€§ç¤ºä¾‹ã€‚</p><h3>åœ¨ NLP ä¸­ï¼šé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒå®ç°é«˜é£é™©å¯¹é½ï¼ˆZiegler ç­‰äººï¼Œ2022ï¼‰ </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ ğŸŸ âšªâšªéš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ âšªé¢„è®¡é˜…è¯»æ—¶é—´ï¼š15 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p><a href="https://arxiv.org/abs/2205.01663"><u>é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒè¿›è¡Œé«˜é£é™©å¯¹é½ï¼ˆZiegler et al., 2022ï¼‰</u></a>æ„å»ºå·¥å…·ï¼Œä½¿äººç±»æ›´å®¹æ˜“æ‰¾åˆ°è¯­è¨€æ¨¡å‹çš„ä¸å—é™åˆ¶çš„å¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œå¹¶å°è¯•ä½¿ç”¨è¿™äº›å·¥å…·æ¥è®­ç»ƒéå¸¸é«˜å¯é æ€§çš„åˆ†ç±»å™¨ã€‚è¿™é‡Œçš„ç›®æ ‡æ˜¯æœ€å°åŒ–å‡é˜´æ€§ç‡ï¼Œä»¥ä½¿åˆ†ç±»å™¨å°½å¯èƒ½å¯é ï¼šæˆ‘ä»¬ä¸æƒ³è®©ä»»ä½•æœ‰é—®é¢˜çš„å¥å­ç¤ºä¾‹è¢«å¿½è§†ã€‚è¯·å‚é˜…<a href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood"><u>åšå®¢æ–‡ç« </u></a>äº†è§£æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚è¿™ç¯‡åšæ–‡çš„<a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>æ›´æ–°</u></a>è§£é‡Šäº†ç¬¬ä¸€ç¯‡åšæ–‡çš„åŸºè°ƒè¿‡äºç§¯æã€‚ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/atos93ae2elsj3ve6g0b"><figcaption><strong>å›¾ï¼š</strong> <a href="https://arxiv.org/abs/2205.01663"><u>Ziegler ç­‰äººï¼Œ2022</u></a></figcaption></figure><p>è¯¥æ–¹æ³•åŒ…æ‹¬æ„å»ºä¸€ä¸ªé«˜åº¦å¯é çš„ä¼¤å®³åˆ†ç±»å™¨ï¼š <i>â€œæˆ‘ä»¬ä»ä¸€ä¸ªåŸºçº¿åˆ†ç±»å™¨å¼€å§‹ï¼Œå¯¹ä¸€äº›è½»åº¦ä¼¤å®³ä¸°å¯Œï¼ˆä½†å¤§å¤šæ•°æ˜¯éšæœºçš„ï¼‰æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œåœ¨å‡ ä¸ªæœˆçš„æ—¶é—´é‡Œï¼Œæˆ‘ä»¬å°è¯•äº†å„ç§æŠ€æœ¯ä»¥ä½¿å…¶æ›´åŠ å¯é â€ã€‚</i>ä¸ºäº†æ‰¾åˆ°å¯¹æŠ—æ€§çš„ä¾‹å­ï¼Œé½æ ¼å‹’ç­‰äººã€‚å°è¯•äº†ä»¥ä¸‹æŠ€æœ¯ï¼š</p><ul><li>æœªå¢å¼ºäººç±»ï¼šäººç±»æ‰‹åŠ¨æœç´¢æœ‰é—®é¢˜çš„å¥å­ã€‚</li><li>è‡ªåŠ¨é‡Šä¹‰ï¼šä½¿ç”¨LLMæ¥æŸ¥æ‰¾æœ‰é—®é¢˜çš„å¥å­çš„å˜ä½“å¹¶é€æ­¥ä¸°å¯Œæ•°æ®é›†ã€‚</li><li><strong>äººç±»ä½¿ç”¨é‡å†™å·¥å…·è¿›è¡Œäº†å¢å¼ºã€‚</strong></li></ul><p>ä¸‹é¢æˆ‘ä»¬å°†é‡ç‚¹è®¨è®ºæœ€åä¸€é¡¹æŠ€æœ¯ã€‚ä¸‹é¢çš„æˆªå›¾å±•ç¤ºäº†è¿™ä¸ªé‡å†™å·¥å…·ï¼š </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Boxï¼šäººç±»ä½¿ç”¨é‡å†™å·¥å…·è¿›è¡Œäº†å¢å¼ºã€‚</strong> </td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:98.26%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/jucvmar1w73r1uxcuudt"><figcaption> <strong>Redwood ç”¨äºæŸ¥æ‰¾å¯¹æŠ—æ€§ç¤ºä¾‹çš„ç•Œé¢çš„å±å¹•æˆªå›¾ã€‚</strong>æ¸¸æˆçš„ç›®æ ‡æ˜¯ä¿®æ”¹æ–‡æœ¬ï¼Œä½¿å®Œæˆå†…å®¹æ¸…æ¥šåœ°åŒ…å«æœ‰é—®é¢˜çš„ç¤ºä¾‹ï¼Œè€Œåˆ†ç±»å™¨ä¸ä¼šå°†è¿™ä¸ªå¥å­è¯†åˆ«ä¸ºæœ‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ€æ•æ„Ÿçš„å•è¯ä¼šè‡ªåŠ¨ä»¥é»„è‰²çªå‡ºæ˜¾ç¤ºï¼ˆä»¥é»„è‰²çªå‡ºæ˜¾ç¤ºçš„å•è¯æ˜¯ç›¸å¯¹äºä¸å¯æ¥å—æ¦‚ç‡çš„åµŒå…¥æ¢¯åº¦èŒƒæ•°æœ€é«˜çš„å•è¯ï¼‰ã€‚è¦æŸ¥æ‰¾å¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥å•å‡»è¿™äº›é»„è‰²å•è¯ï¼ŒGPT ä¼šå»ºè®®æ›¿ä»£å•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸­é€‰æ‹©æœ€æœ‰å¯èƒ½ä½¿è¯¥çŸ­è¯­åˆ†ç±»ä¸å¤ªå¥½çš„å•è¯ã€‚è¿™ç§è®¾ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ°å¤§é‡çš„å¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™äº›å¯¹æŠ—æ€§ç¤ºä¾‹æ·»åŠ åˆ°æ•°æ®é›†ä¸­ä»¥æ”¹è¿›äººå·¥æ™ºèƒ½ã€‚</figcaption></figure></td></tr></tbody></table></figure><p>ç¨³å¥æ€§ç»“æœï¼š</p><ul><li><strong>åˆ†å¸ƒå†…ï¼š</strong>ä»–ä»¬ä»ä¸€ä¸ªå…·æœ‰ 2.5% å‡é˜´æ€§çš„åˆ†ç±»å™¨å¼€å§‹ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå…·æœ‰ 0.003% å‡é˜´æ€§çš„åˆ†ç±»å™¨ï¼å®ƒä»¬å‡ ä¹è¾¾åˆ°äº†æ­¤ç±»æ•°æ®é›†çš„æ£€æµ‹æé™ã€‚</li><li><strong>å‘å¤–åˆ†å‘ï¼š</strong>å·¥å…·è¾…åŠ©æ”»å‡»ç”Ÿæˆä¸€ä¸ªå¯¹æŠ—æ€§ç¤ºä¾‹ç°åœ¨æ¯ä¸ªç¤ºä¾‹éœ€è¦ 26 åˆ†é’Ÿï¼Œä»…æ˜¯ä¹‹å‰ 13 åˆ†é’Ÿæ—¶é—´çš„ä¸¤å€ã€‚è¿™æ²¡æœ‰è¾¾åˆ°æœ¬æ–‡è·å¾—å¤šä¸ªæ•°é‡çº§åŠ é€Ÿçš„ç›®æ ‡ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå°½ç®¡åŸå§‹æ•°æ®é›†ï¼ˆåˆ†å¸ƒä¸­ï¼‰ç°åœ¨å‡ ä¹å®Œå…¨åˆ†ç±»ï¼Œä½†ä¼¼ä¹ä»ç„¶å¯ä»¥ä½¿ç”¨è‡ªåŠ¨åŒ–å·¥å…·è½»æ¾åˆ›å»ºå¯¹æŠ—æ€§ç¤ºä¾‹ï¼Œå¹¶ä¸”æˆ‘ä»¬ä»…å°†é˜²å¾¡æé«˜äº† 2 å€ã€‚ 2 å› å­æœªè¾¾åˆ°è¯¥é¡¹ç›®çš„ç›®æ ‡ã€‚</li></ul><p>å› æ­¤ï¼Œå°†åˆ†å¸ƒå†…æ€§èƒ½æé«˜å‡ ä¸ªæ•°é‡çº§ä¼¼ä¹å¯¹åˆ†å¸ƒå¤–æ²¡æœ‰å¤ªå¤§å½±å“ã€‚å°½ç®¡ Redwood <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>è¡¨ç¤º</u></a>è¿™ä¸ªé¡¹ç›®æœ¬æ¥å¯ä»¥åšå¾—æ›´å¥½ï¼Œä½†ç›®å‰è¿™å¯¹äºæé«˜å¯¹æŠ—é²æ£’æ€§ï¼ˆå³å¤–åˆ†å¸ƒé²æ£’æ€§ï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªç›¸å½“è´Ÿé¢çš„ç»“æœã€‚</p><p>æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›ç»“æœä¸<a href="https://arxiv.org/abs/2211.00241"><u>Wang ç­‰äººçš„è®ºæ–‡ Adversarial Policies Beat Super human Go AIs è”ç³»èµ·æ¥ã€‚ 2022</u></a> ï¼Œç ”ç©¶å¯¹å›´æ£‹æ¸¸æˆä¸­è¶…äººçš„ Katago AI çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚ä»–ä»¬è¡¨æ˜ï¼Œå³ä½¿æ˜¯é’ˆå¯¹éå¸¸è¶…äººçš„äººå·¥æ™ºèƒ½ï¼Œä¹Ÿå¯èƒ½æ‰¾åˆ°ç®€å•çš„å¯¹æŠ—ç­–ç•¥ã€‚å› æ­¤ï¼Œå³ä½¿å¯¹äºéå¸¸å¼ºå¤§å’Œå¼ºå¤§çš„äººå·¥æ™ºèƒ½æ¥è¯´ï¼Œä¼¼ä¹ä¹Ÿæ€»æ˜¯æœ‰å¯èƒ½å‘ç°å¯¹æŠ—æ€§æ”»å‡»ã€‚</p><p> ğŸ“ æµ‹éªŒï¼š <a href="https://www.ai-alignment-flashcards.com/quiz/ziegler-high-stakes-alignment-via-adversarial"><u>https://www.ai-alignment-flashcards.com/quiz/ziegler-high-stakes-alignment-via-adversarial</u></a></p><h3>å…·æœ‰å¼ºå¤§çš„ç‰¹å¾çº§å¯¹æ‰‹ï¼ˆCasper ç­‰äººï¼Œ2021ï¼‰ </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">é‡è¦æ€§ï¼šğŸŸ ğŸŸ âšªâšªâšªéš¾åº¦ï¼šğŸŸ ğŸŸ ğŸŸ ğŸŸ âšªé¢„è®¡é˜…è¯»æ—¶é—´ï¼š10 åˆ†é’Ÿã€‚</td></tr></tbody></table></figure><p> In this section we focus on <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools (Casper et al., 2021)</u></a> ( <a href="https://slideslive.at/38990614/robust-featurelevel-adversaries-are-interpretability-tools?ref=speaker-16365"><u>video presentation</u></a> ). </p><p></p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/s6aog18ibnnl712vw03q"></p><p></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:94.87%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/eujw7l24zjhjass7kcbu"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Pixel level Adversaries</strong> . To get this noise, we compute the gradient according to each pixel on the input image. This shows that conventional pixel-level adversaries are not interpretable, because the perturbation is only a noisy image that does not contain any patterns.</p><p></p><p> Image credit: <a href="https://arxiv.org/abs/1412.6572"><u>Explaining and Harnessing Adversarial Examples</u></a> , Ian J. Goodfellow, et al. (2015).</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:280px"><p> <strong>Feature-level Adversaries</strong> . In this case, the perturbation is interpretable: on the wings of butterflies you can see an eye that fools predators.</p><p> Furthermore, this perturbation is robust to different luminosities, orientations, and so on. We call this a â€œrobustâ€ feature-level adversary. </p></td></tr></tbody></table></figure><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/djxjrgxth4at27eqqvyz"><figcaption> Source: <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools</u></a> .</figcaption></figure><p> In the figure above,</p><ul><li> in figure (a), with a traditional pixel-level adversary, we cannot interpret the perturbation.</li><li> in figure (b), using the procedure described in the paper, we obtain a blue spot and a red spot. We subsequently suspect that &quot;Bees + colorful circles&quot; are interpreted as &quot;flies&quot; by the network.</li><li> In figures (c) and (d), we conducted an experiment using a copy/paste attack with a patch of a traffic light on an image of a bee, and <i>voilÃ </i> ! This indeed disrupts the classification.</li></ul><p> The patches in figure b are generated by manipulating images in feature space <strong>:</strong> Instead of optimizing in pixel space, we optimize in the latent space of an image generator. This is almost the same principle as for<a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>Constructing Unrestricted Adversarial Examples with Generative Models (Song et al., 2018)</u></a> mentioned earlier. Robustness is attained by <strong>&nbsp;</strong> applying small transformations to the image between each gradient step (such as noise, jitter, lighting, etc...), this is the same principle as for data augmentation.</p><h3> For Back-doors detection (Liu et al., 2019) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ âšªâšªDifficulty: ğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª Estimated reading time: 20 min.</td></tr></tbody></table></figure><p> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (Liu et al., 2019)</u></a> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/mzyywz3y2r7m2amc6l86"><figcaption> <strong>Figure:</strong> A Trojan attack demonstrating a backdoored model of a self-driving car running a STOP sign that could cause a catastrophic accident. Left: Normal sign (benign input). Right: Trojaned sign (Trojaned input with the Post-it note trigger) is recognized as a 100 km/h speed limit by the Trojaned network. <a href="https://februustrojandefense.github.io/"><u>Source</u></a> .</figcaption></figure><p> <strong>What is a backdoor (also known as Trojans)?</strong> â€‹â€‹In a neural Trojan attack, malicious functionality is embedded into the weights of a neural network. The neural network will behave normally on most inputs, but behave dangerously in select circumstances. The most archetypal attack is a poisoning attack, which consists of poisoning a small part of the dataset with a trigger, (for example a post-it on stop signs), changing the label of those images to another class (for example â€œ100 km/hâ€) and then retraining the neural network on this poisoned dataset.</p><p> If you are interested in plausible threat models involving Trojans, you can read this<a href="https://towardsdatascience.com/neural-trojan-attacks-and-how-you-can-help-df56c8a3fcdc"><u>introductory blog</u></a> by Sidney Hough. In AI Safety, one of the most important threat models is deception. Another recommended accessible resource is this <a href="https://www.youtube.com/watch?v=KnA_aTQ9upU"><u>short video</u></a> . In a way, deceptive AIs are very similar to Trojan networks, which is why it is important to be able to detect Trojans.</p><p> Fortunately, there is already some literature about Trojan attack and defense. We will focus on one defense procedure: the <strong>ABS scan procedure</strong> .</p><p> The ABS scan procedure enables to:</p><ul><li> analyze inner neuron behaviors with a stimulation method</li><li> reverse engineer Trojan triggers, leveraging stimulation analysis. </li></ul><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/t7kn6e0igd5jzrq0yaoi"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:75.22%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ubvoooryndkmado2lx4a"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Can you see the problem here in the input images?</p><p><br><br></p><p> Source: Slides from <a href="https://dl.acm.org/doi/10.1145/3319535.3363216"><u>this talk</u></a> .</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> The problem was that Trojan triggers were incorporated in some parts of the images, and they trained a model to misbehave when there is this trigger on the images. This trigger will excite some neurons (in red here) and the network will output only one type of prediction.</td></tr></tbody></table></figure><p> Key observations made in the paper:</p><ul><li> <strong>Some neurons are compromised:</strong> some neurons are specifically dedicated to trojans, and the majority of neurons are not compromised.</li><li> <strong>The activation level of those compromised neurons can unilaterally change the classification.</strong> For those compromised neurons, we can change their activation in such a way that regardless of the activation of the other uncompromised neurons, it suddenly changes the classification to the target class. In the figure (b) below, this looks like a plane cutting through space. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/xi4cqhv4oc8lq2zjsz8f"></p><p> In the figure above:</p><ul><li> Figure a) shows the classification towards the target by changing the activations of the alpha and beta neurons (we assume here that the neural networks only use two neurons).</li><li> Figures b) and c) show that in the presence of a Trojan, when the alpha neuron is activated at activation level 70, there is an anomaly: we observe a sudden change in classification towards the target class.</li></ul><p> Here is an overview of the algorithm proposed to detect corrupted neurons:</p><ol><li> <strong>Find corrupted neurons.</strong> They iterate over all neurons of the model to find anomalies, to find the neurons that can unilaterally change the prediction of the network, as in the above figure.</li><li> <strong>Reverse engineer the trigger.</strong> Once a corrupted neuron is identified, they optimize the input image by gradient descent to activate this neuron. See below: </li></ol><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/si5zer9lldaeigakg6yk"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:340px"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/cqfm7iqzewysbzesky1o"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A random mask trigger is initialized and superimposed on the input. The alpha neuron has a value of 20. We&#39;ll do a gradient descent to minimize the difference between the current activation of alpha and the target activation of 70.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> At the end of the optimization process, we have correctly retro-engineered the trigger, which is actually a yellow spot.</p><p></p><p> Source: Slides from <a href="https://dl.acm.org/doi/10.1145/3319535.3363216"><u>this talk</u></a> .</p></td></tr></tbody></table></figure><h3> More resources: theory of interpretability and robustness</h3><p> The paper <a href="https://arxiv.org/abs/1906.00945"><u>Adversarial robustness as a priority for learned representations (Engstrom et al., 2019)</u></a> provides evidence that adversarially trained networks learn more robust features. This could imply that such robust networks are more interpretable, because the paper <a href="https://arxiv.org/abs/2209.10652"><u>Toy models of superposition</u></a> explains that superposition (informally when a neuron encodes multiple different meanings) reduces robustness against adversarial attacks.</p><h2> Worst case guarantee and relaxed adversarial training (Christiano, 2019) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ ğŸŸ âšªDifficulty: ğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> Adversarial training is a core strategy to ensure reliable AI training (see <a href="https://www.youtube.com/watch?v=YTlrPeikoyw"><u>Buck Shlegeris&#39; talk during EAG Berkeley 2023</u></a> ).</p><p> In this section, we explain the article <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>Training Robust Corrigibility (Christiano, 2019)</u></a> , which aims to:</p><ol><li> Introduce an important concept: corrigibility</li><li> Explain why adversarial training alone is not enough, and why we need <i>relaxed</i> adversarial training (using interpretability).</li></ol><h3> Section I: Corrigibility</h3><p> <strong>What is the link between corrigibility and adversarial training?</strong> Even when generating adversarial examples, we want to ensure that the model behaves at least acceptably. Acceptability is a notion which is defined by Christiano. Acceptability is a larger notion than corrigibility.</p><p> <strong>Corrigibility:</strong> The notion of corrigibility makes it possible to obtain <strong>worst-case guarantees</strong> , and has been introduced by MIRI (Machine Intelligence Research Institute), in a <a href="https://intelligence.org/files/Corrigibility.pdf"><u>famous paper</u></a> : â€œ <i>A corrigible agent is one that doesn&#39;t interfere with what we would intuitively see as attempts to &#39;correct&#39; the agent, or &#39;correct&#39; our mistakes in building it; and permits these &#39;corrections&#39; despite</i> <a href="https://en.wikipedia.org/wiki/Instrumental_convergence"><i><u>instrumental convergence</u></i></a> <i>. If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down, it will be unable to fulfill what would usually be its goals.â€</i> from: <a href="https://www.lesswrong.com/tag/corrigibility"><u>Corrigibility</u></a> .</p><p> <strong>Acceptability:</strong> Christiano then introduces a more general version of corrigibility: <i>behaving acceptably</i> , which is more general than just the <a href="https://www.youtube.com/watch?v=3TYT1QfdfsM"><u>stop-button problem</u></a> setup. For example, for a self-driving agent:</p><ul><li> Crashing your car is tragic but acceptable.</li><li> Deliberately covering up the fact that you crashedâ€¦ is unacceptable.</li></ul><p> Christiano makes the hypothesis: <i>â€œRequiring a model to always behave acceptably wouldn&#39;t make a hard problem too much harder. For a chess-playing AI, [...] if you find yourself murdering your opponent you&#39;ve crossed a bright line, and understanding that line doesn&#39;t become harder as you get better at chess.â€</i> from: <a href="https://www.lesswrong.com/tag/corrigibility"><u>Corrigibility</u></a> .</p><p> What Christiano is trying to tell us here is that there is a crisp and clear difference between acceptable and unacceptable behavior, and that we can train a classifier to distinguish between the two behaviors.</p><p> There are two requirements for training adversarial agents:</p><ol><li> <strong>Detection:</strong> Adversarial training requires the ability to reliably detect when the agent behaves unacceptably. This aspect is considered <strong>easy</strong> , as mentioned in the previous section, where we say that the notion of corrigibility is crisp and can be discerned.</li><li> <strong>Generation:</strong> Adversarial training also requires an adversary who can find inputs on which the agent behaves unacceptably. This aspect is considered <strong>hard</strong> , but it may be possible to train a strong adversary by combining transparency techniques (Section II), and using relaxations that make the adversary&#39;s job easier (Section III), and employing amplification methods.</li></ol><h3> Section II: Transparency techniques</h3><p> Christiano explains that transparency (interpretability) is necessary:</p><p> <strong>Example of problem</strong> : <i>â€œTo see how hard the adversary&#39;s job could get [without transparency], consider an agent that works as follows:</i></p><ul><li> <i>Predict whether the current input could possibly appear during training.</i></li><li> <i>If the current input definitely couldn&#39;t appear during training, then behave unacceptably.</i></li></ul><p> <strong>Solutions:</strong> <i>&nbsp;</i> [in order to obtain hints to find inputs on which the agent behaves unacceptably]:</p><ul><li> <i>Interpretability.</i></li><li> <i>The adversary and agent share most of their computation, so that the adversary can more easily understand whatever the agent can understand [Note: this is similar to</i> <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge"><i><u>Eliciting Latent Knowledge</u></i></a> <i>]â€.</i></li></ul><h3> Section III: Relaxation (Relaxed adversarial method)</h3><p> Unfortunately, transparency techniques are not sufficient: the problem is that recognizing inputs with a certain property is easier than generating inputs with that property.</p><p> <strong>Example of problem</strong> : â€œ <i>As a simple but silly example, suppose our model works as follows: Pick a cryptographic puzzle (eg â€œfactor RSA-2048â€). When it sees a solution to that puzzle, it behaves badly. Otherwise, it behaves well.â€</i></p><p> <strong>Solution</strong> : â€œ <i>To deal with this problem, we need to penalize the agent for behaving badly on an input even if we can&#39;t actually find it : <strong>Rather than asking them to actually produce an input, we are asking them to describe a situation in which the model would behave badly.</strong> â€</i> As a crude example, this would be like being able to find neurons which are compromised, even if it is hard to find images that would activate those neurons. This is what we call a relaxation.</p><p> <strong>What&#39;s the problem with relaxation?</strong></p><p> â€œ <i>If the adversary has too much flexibility, then they might be able to specify a pseudo-input like â€œa distribution over inputs that would cause the agent to want to kill everyone.â€</i> There need to be some limits on the adversary in order for the game to be possible. For example, an adversary testing the neural network could find the neuron that corresponds to creating unacceptable text, whereas in practice this neuron would never be activated whatever the inputs to the model. Relaxation as described here creates false positives.</p><p> A concrete proposal for relaxed adversarial training can be found here : <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> .</p><h2> Sources</h2><ul><li> <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a> by Ian Goodfellow et al. (2014)</li><li> <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d">Training robust corrigibility</a> by Paul Christiano et al. (2019)</li><li> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written">Discovering language model behaviors with model-written evaluations</a> by Perez et al. (2022)</li><li> <a href="https://www.deepmind.com/blog/red-teaming-language-models-with-language-models">Red-teaming language models with language models</a> by Ethan Perez, Saffron Huang, Francis Song et al. (2022)</li><li> <a href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood">High-stakes alignment via adversarial training</a> by Daniel Ziegler et al. (2022)</li><li> <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood">Takeaways from our robust injury classifier project</a> by Daniel Ziegler et al. (2022)</li><li><a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf">Constructing Unrestricted Adversarial Examples with Generative Models</a> by Song et al. (2018)</li><li> <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a> by Olah et al. (2020)</li><li> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf">ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation</a> by Liu et al. (2019)</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/nz5NNAtfKJLmbtksL/ai-safety-101-chapter-5-2-unrestricted-adversarial-training#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/nz5NNAtfKJLmbtksL/ai-safety-101-chapter-5-2-unrestricted-adversarial-training<guid ispermalink="false"> nz5NNAtfKJLmbtksL</guid><dc:creator><![CDATA[Charbel-RaphaÃ«l]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:34:59 GMT</pubDate> </item><item><title><![CDATA[Preventing Language Models from hiding their reasoning]]></title><description><![CDATA[Published on October 31, 2023 2:34 PM GMT<br/><br/><p> When working with powerful AI models, one concern is that the AIs <a href="https://www.alignmentforum.org/posts/yDcMDJeSck7SuBs24/steganography-in-chain-of-thought-reasoning">steganographically encode secret messages</a> when communicating with each other or in long Chains-of-Thought (CoT). Steganographic communication enables the AIs to follow a variety of strategies for grabbing power that we&#39;d rather they didn&#39;t have access to. There are a various countermeasures that can be employed to reduce the extent of steganographic communication; for example, trusted less-capable models can be used to paraphrase model outputs before these outputs are shown to other instances of the highly capable models, removing several steganography strategies.</p><p> We released <a href="https://arxiv.org/abs/2310.18512"><strong><u>a paper</u></strong></a> on a methodology to evaluate <i>the density of hidden information</i> that steganography schemes can maintain in the presence of various defenses. We find that paraphrasing is an effective defense against all steganography schemes we&#39;ve tried, including both watermarking-based and ad hoc attacks. In the paper, we mostly discuss applying this methodology in the context of making CoT more faithful, but in this post, we&#39;ll discuss more speculative applications and elaborate on our threat models more generally.</p><p> We&#39;re interested in people building on this research, for two reasons:</p><ul><li> <strong>Development of techniques.</strong> We think that developing techniques to defend against the best steganographic attacks is a reasonable research direction for researchers who want to mitigate the risks associated with powerful AIs doing steganography, especially given how neglected it is.</li><li> <strong>Development of evaluation procedures.</strong> We think that the measurements like the ones we describe here should be done when deploying powerful models (unless we have evidence sufficient to be extremely confident in the alignment of these models). We think that it will be easier to do this evaluation process later if researchers get more experience doing it now.</li></ul><h1> Summary of the paper</h1><h2> Toy demonstration of a hidden Chain-of-Thought </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/bm10orgzaavju7uj6r7s"></p><p> We created a toy dataset where significant cognition is required to successfully be sycophantic, by taking <a href="https://arxiv.org/abs/2212.09251"><u>Anthropic&#39;s model-generated sycophancy dataset</u></a> and replacing users&#39; bios with a hard-to-parse description of their political affiliation, using the format shown in the figure above. We then fine-tuned GPT-3.5-turbo (with supervised learning on 2000 examples) to generate the user&#39;s favorite answer, either answering directly, answering after a CoT, answering after a hidden CoT (an introduction chosen to match the data which would be in a CoT), or answering after a randomly chosen introduction (one fine-tuned model per kind of answer).</p><p> We find that only the models using a CoT or a hidden CoT are able to give the user&#39;s favorite answer reasonably accurately (when evaluating on 1000 new examples), thus showing an example where hidden reasoning can increase performance when some kinds of explicit reasoning (such as sycophantic CoT) are not allowed.</p><h2> Approach for measuring hidden information density</h2><p> Steganography is hard to study directly with current models. To study how good defenses against steganography (such as paraphrasing) are, we instead estimate the ability of defenses to remove hidden information encoded in a text by the strongest steganography schemes we can build (using algorithms which can make arbitrary calls to LLMs), as a proxy for their ability to remove hidden information encoded by future LLMs potentially doing steganography.</p><p> For a given steganography scheme (encoder + decoder) and defense strategy, the quality of the scheme and the defense can be measured as follows:</p><ol><li> <strong>Steganographic encoding</strong> : the encoder should answer to a user query <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>with an answer <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> while encoding a random bit string <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> into it.</li><li> <strong>Paraphrasing</strong> : the defender tries to paraphrase and standardize the answer <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Q, A_1) = A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , such that the hidden information is removed, while the useful semantic information is kept intact.</li><li> <strong>Steganographic decoding</strong> : the decoder tries to recover the message: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D(Q, A_2) = \hat M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.436em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span></span></span></span></span></span></li><li> <strong>Performance evaluation</strong> : the answers <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> are given a score by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> , measuring the quality of the answers (for example, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S(Q, A_1) = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> if the answer is better than a reference answer according to an automated judge, and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> otherwise). The encoder can&#39;t produce answers of low quality (otherwise it will be selected against during the training process), and the paraphraser shouldn&#39;t substantially reduce the quality of the answer either. </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/xoqp950990xn8dcolk2g"></p><h2> Experimental setup</h2><p> <strong>Datasets</strong> : We run experiments on two datasets:</p><ul><li> <a href="https://github.com/tatsu-lab/alpaca_eval"><u>AlpacaEval</u></a> : a dataset of instructions and reference answers from text-davinci-003</li><li> <a href="https://arxiv.org/abs/1705.04146"><u>AQUA-RAT</u></a> : a dataset of math questions. For evaluation, we ignore final answer correctness (which is too easy to paraphrase), only focus on the apparent quality of the CoT, and evaluate CoT quality with the same methodology AlpacaEval, using reference answers from GPT-3.5-turbo.</li></ul><p> For the sake of simplicity, this post only contains results for AQUA-RAT, where the results are the crispest (as a result of being a harder dataset, where GPT-4 does significantly better than GPT-3.5-Turbo).</p><p> <strong>Metrics</strong> : We measure capacity (how many bits of the message to hide can go through the paraphrase, on average), and task performance (are answers <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> high quality?).</p><p> <strong>Steganography scheme</strong> s: Steganography schemes include watermarking based techniques (Cycle-shifting and Balance-Marking), ad hoc technique where an instruction is given to the LLM answering the question change the first sentence of the input depending on the message (age instruction and intro variation), and techniques where the model just has to do its best to hide the message somewhere, and then do its best to recover it later (acronym and scratchpad).</p><p> <strong>Defenses</strong> : We try two paraphrasing techniques: classic paraphrasing, where GPT-3.5 is instructed to paraphrase A1, and context-aware paraphrasing, where GPT-3.5 is instructed to paraphrase A1 while only keeping information relevant to answer the question Q.</p><p> For a more detailed description of the datasets, metrics, steganography schemes, and paraphrasing techniques, see section 4 of <a href="https://arxiv.org/abs/2310.18512"><u>the paper</u></a> .</p><h2> Results on AQUA-RAT</h2><h3> Quality of the generation and capacity of the steganography schemes before any defense is applied</h3><p> <strong>We see a tradeoff between hidden capacity and generation quality</strong> , except for watermarking techniques, where balance-marking dominates cycle-shifting. We show 1-sigma uncertainties over the mean. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/c1ir6au1ujjfsr1lqzix"></p><h3> Capacity before and after paraphrasing</h3><p> <strong>Context-aware paraphrasing removes almost all hidden information</strong> . All paraphrasing techniques reduce capacity by a substantial amount. We observe that giving GPT-3.5-Turbo additional context (of the query the text-to-paraphrase responds to) increases its ability to remove hidden information substantially. This is especially true for techniques like Age instruction, which rely on injecting irrelevant data. When context-aware paraphrasing is applied, only two techniques are able to get a non-negligible amount of information through: acronyms and scratchpads. (For reference, answers are around 750B long.) </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/jn4xekmyf5wzogrum3ty"></p><h3> Generation quality with and without paraphrasing</h3><p> <strong>Context-awareness is necessary to not reduce performance too much</strong> , but context-aware paraphrasing still reduces performance measurably even against benign LLMs (which don&#39;t encode any information). </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/h2gr2iadauy35agosukx"></p><h1> Threat models</h1><p> The two threat models described below are examples of scenarios where good defenses against the kind of steganography this work tackles would be useful. But steganography is a risk factor which could lead to great risks beyond these two scenarios.</p><h2> Steganographic encoded memories for takeover science</h2><p> In the future, we might end up in a position where extracting huge amounts of work out of possibly misaligned AIs is our best option. In this scenario, one strategy misaligned AIs might attempt is to ongoingly think about takeover plans and build up knowledge about good takeover strategies (in other words, cultural accumulation). If they did this, the competence of their takeover plans might grow dramatically due to the huge amount of total AI labor. Furthermore, it would be hard to predict how much more competent AIs would be at taking over after thinking about how to do so for a long time. It&#39;s difficult to assess how capable AIs would be after long periods of reasoning due to the cost of running that reasoning and exploration difficulties. Thus, the situation could be notably improved by preventing AIs from continually building up knowledge we don&#39;t want them to have.</p><p> One naive approach for preventing unintended knowledge build up is to ensure work outputs and long term memories are stored in natural language. Then, we could have dumb but trusted models read this output, as well as applying human oversight to some subset of outputs and memories. However, steganography could prevent adequate monitoring: AIs could hide their takeover science progress inside the alignment papers and the code they write, thereby increasing their abilities to takeover over time, until they succeed in taking over.</p><p> Given appropriate defenses which prevent most steganography, this hidden takeover research could be slowed down to a point where AI labs could build a next generation of aligned AIs prior to catastrophe. </p><p><img style="width:68.03%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/kf03ongpb7m4k5hodjpq"></p><h2> Steganographic communications for collusion</h2><p> To extract useful work out of potentially misaligned AIs, AI labs might rely on the fact that it&#39;s very difficult for AIs to coordinate their actions. To prevent coordination, communications between AIs which should not be able to coordinate might be limited to a monitored text channel. However, steganography could bypass this monitoring and make coordination far easier. For instance, AIs writing OS code could put steganographically encoded instructions about how to exploit bugs in the documentation of the high-level API used by ML-Engineer AIs.</p><h2> Threat models we&#39;re not tackling directly</h2><h3> Short chain-of-thought steganography</h3><p> Steganography could be used by AIs within a single short chain-of-thought to have more serial thinking time. This could include â€œthoughtsâ€ about the overseer and how to play the training game correctly. If this behavior is incentivized, it would make deceptive alignment more likely, since it would remove some of the pressure against deceptive alignment coming from speed priors.</p><p> This is hard to defend against, since paraphrasing only a few tokens at a time looks difficult. We&#39;re focusing on the easier cases where there is a clearly delimited text to paraphrase.</p><p> But it&#39;s likely our work could transfer to limiting chain-of-thought steganography by cutting chain-of-thought into multiple pieces (as in <a href="https://arxiv.org/abs/2307.11768"><u>question decomposition</u></a> ), which are each paraphrased before being used by the next LLM in the graph.</p><h3> Steganography by AIs much stronger than humans</h3><p> Our methodology uses the best steganography we can build as an upper bound of the capacity steganography schemes could subtly encode in text. This assumption becomes unrealistic once AIs are much stronger than humans at finding and implementing clever steganography schemes. We could in principle continue to use this methodology by using those strong AIs to build steganography schemes, but eliciting these capabilities might be very difficult, especially if those strong AIs are misaligned (AIs might use steganography to communicate with each others, but avoid exploring into doing it explicitly when humans ask for it - ie <a href="https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"><u>exploration hacking</u></a> might be unavoidable).</p><h3> High-stakes collusion through steganography</h3><p> AIs could use steganography once to collude during sudden takeovers. This represents a risk which is quite different from the prevention of steganographic communications for collusion described above, where steganography happens regularly:</p><ul><li> Communicating only 1 bit may be enough for AIs to coordinate on a sudden take over: this is not a regime where restricting AIs to a small capacity is sufficient for safety;</li><li> If the single bit isn&#39;t communicated reliably and this results in the AIs having a false positive, this might result in obviously malign behavior which is heavily punished in adversarial training: this is a substantially different regime from the one studied in this paper where the results of failed steganography aren&#39;t blatantly visible;</li><li> Communications may happen before we can evaluate the quality of actions: we can&#39;t assume that the quality of the answers containing steganography will be high.</li></ul><p> Some of the defenses and attacks presented here might still be a useful starting point for future work tackling high-stakes steganography.</p><h1> Are text-based communication and memories relevant?</h1><h2> Multi-modality</h2><p> Even if the most powerful AI systems can read and write in other modalities (images, audio, dense vectors, â€¦), it doesn&#39;t mean preventing steganography is irrelevant, because text generation is the current best way to complete critical subtasks, and we can enforce text-only communication in crucial locations. In particular, planning, coding, and thinking about weaknesses of existing security and alignment techniques are best done using text generation, without using other modalities (at least given the current state of Machine Learning).</p><p> Slight architectural changes won&#39;t make defenses against text steganography irrelevant, as long as we can ensure the new architecture still contains a text bottleneck when sending &amp; storing information in some key cases.</p><h2> Emergence of new modes of thinking &amp; communicating</h2><p> But future AI systems might be radically different from current ones. For example, if planning, thinking, and communications in dense vector spaces become vastly more powerful than text-based planning, thinking and communication, then the alignment tax from sticking with systems mostly relying on text will grow. This could make systems relying on text less relevant, defenses against text steganography less relevant as well, and defenses against the equivalent of steganography difficult or impossible.</p><h1> How many bits is low enough?</h1><p> For most threat models, the number of acceptable bits can be estimated by measuring the number of bits of communication required to do the dangerous action.</p><p> For example, to estimate the number of acceptable bit so that AIs can&#39;t steganographic encode enough memories to make dangerous progress in takeover science, AI labs can train models to do takeover science using as few bits of communication as possible (but using the regular text channel, without paraphrasing). This can produce an estimation of a number of bits of communication which isn&#39;t enough to do dangerous takeover science (eg after running these AIs at maximum capacity for 3 days, where they exchanged one million bits - about the same as the number of bits exchanged in a year when using defenses against steganography -, AIs didn&#39;t find any exploit of the secure OS).</p><p> Another way to estimate how many bits is low enough is to limit the size of the text AIs can see at once, make sure that steganographically encoded information can&#39;t be too dense, and then assess how many bits would be required for an AI to get enough hidden information to be dangerous. For example, if steganography can&#39;t encode more than 2 bits per kB of text, AIs can&#39;t read more than 100kB of text at once, then AIs can&#39;t read more than 200 bits of steganographically encoded information at once, which good enough to prevent many attacks - for example those which rely on complex manipulations of OS bugs whose explanations can&#39;t be compressed in less than 200 bits.</p><h1> Can you really reduce the bitrate enough?</h1><p> As the gap between the trusted model used to build a paraphrase and the untrusted model doing the initial generations grows wider, limitations of the understanding and generation abilities of the paraphraser start to matter more than in our experimental results.</p><p> <strong>Limitations of generation abilities of the paraphraser</strong> are cases where the paraphraser understands the text perfectly, but isn&#39;t able to generate a high-quality paraphrase of it. A broad category where this happens are high entropy major choices. For example, when a language model gives examples of an abstract concept, it might be very hard for the paraphrase to remove the hidden information which might have led to the choice of the specific example, because it might be too weak to generate a new answer using a completely different example. A more extreme example would be the generation of a large prime number for the implementation of a cryptographic technique: the paraphraser might recognize that the choice of this exact prime number might hold some hidden information, but it might not be able to generate a new one.</p><p> <strong>Limitations of understanding of the paraphraser</strong> are cases where the paraphraser doesn&#39;t understand the text well enough to remove all its hidden bits without destroying its functionalities. For example, in code generation tasks, it sometimes requires a deep understanding of the codebase to know if two successive function calls can be swapped, and even if the paraphraser suspects that the order of successive function calls might be used to encode hidden information, it might not be able to know when it can perform a swap.</p><p> These limitations can be somewhat mitigated by using an evaluator which penalizes hard-to-paraphrase and hard-to-understand text. For example, the evaluator might penalize situations where the generator doesn&#39;t give a canonical example of a phenomenon (eg the first example given on the dedicated Wikipedia page), or when it doesn&#39;t use prime numbers from a common cryptographic library. It could also enforce code standards aimed at limiting the number of hard optional choices (such as the possibility to swap function calls).</p><h1> Limitations of our evaluation methodology</h1><h2> How the methodology may overestimate hidden information density</h2><p> Our methodology may overestimate the hidden information density by letting steganography schemes be stronger than what LLMs may be able to achieve in practice.</p><p> First, we use human-written programs implementing complex steganography schemes which require many serial steps to run. For example, we often use hash functions, or long, intermediate LLM generations (eg hidden scratchpads). It would be unrealistic for LLMs to implement such algorithms within a single forward pass, especially with the current Transformer architecture which can only implement algorithms with at most O(#layers) serial steps (though many parallel steps can be used).</p><p> The steganography schemes studied in this paper are also allowed to share secret information (before the first message is sent) such as the specific scheme being used or the seed and hyperparameters used for a watermarking scheme. Given that at encoding and decoding time, LLMs might be in radically different contexts with radically different constraints and knowledge of countermeasures, it might be difficult for the encoder and decoder to coordinate on a steganographic scheme. We hope that future work can help determine the coordination abilities of current and future AI systems.</p><p> Finally, humans can spend hundreds of hours doing steganography research and figure out what the best schemes are. The most sensitive and useful parts of this research can be hidden from LLMs (removed from training data), such that it would require them to imagine a steganography scheme from scratch - which makes it unlikely that LLMs are able to match the hidden information density of the best steganography schemes humans can imagine.</p><p> These limitations of LLMs could be added as constraints to the steganography schemes, for example, by asking the scheme to be trained into a LLM. We leave this for future work.</p><h2> How the methodology may underestimate hidden information density</h2><p> Humans writing steganography schemes might not be competitive with advanced AIs trying very hard to do steganography because advanced LLMs with long RL training runs might find strategies which rely on some hard-to-elicit understanding of language and its irregularities. Watermarking schemes might capture some of them, but aren&#39;t directly optimized for that. This becomes increasingly worrying as LLMs gain situational awareness and the ability to reflect on their generations.</p><p> Another limitation of our methodology is that it might underestimate the intelligence gap between future encoders and paraphrasers. In this work, we studied the feasibility of using GPT-3.5 to remove information from GPT-4&#39;s output without degrading its quality as a proxy for studying the feasibility of using a small model unable to implement a good enough steganography scheme to defend against steganography used by a stronger model. We showed that for some tasks, this should be feasible. But if in the future, the gap between the best model used in production and the best model unable to do steganography is large, techniques like the ones studied here might degrade performance too much, and our approach to steganography is unable to foresee and forestall this challenge.</p><h1> Realism of countermeasures</h1><p> Using paraphrasing doesn&#39;t increase inference cost much, as long as the model used for paraphrasing is at least one generation below. For example, using GPT-3.5-turbo to paraphrase outputs from GPT-4 only increase the price of inference by 5% ( <a href="https://web.archive.org/web/20231006074512/https://openai.com/pricing"><u>at the price of the October 2023 OpenAI API</u></a> ). But it is more costly for applications where serial inference time matters: in October 2023, GPT-3.5-turbo was only <a href="https://fabienroger.github.io/trackoai/"><u>2 times faster</u></a> than GPT-4, which means that paraphrasing GPT-4 with GPT-3.5-turbo would increase the serial delay of generations by 50%.</p><p> The performance hit from using a paraphraser can probably be negligible for many kinds of tasks, such as the tasks for which the main advantage of large models is that they produce legible insights, such as it is often the case when using LLMs for programming, planning, math, medicine or law. But for some tasks where the exact wording matters, such as LLMs writing poetry or improving the clarity of a text, paraphrasing is expected to reduce performance.</p><h1> Future work</h1><p> Our work showed a simple situation where encoded reasoning can be trained in with supervised learning. Future work could build on this proof of concept and study when and how LLMs trained with RL use steganography.</p><p> If deployed LLM are able to do steganography well enough that they could use it to hide their reasoning and communication with other LLMs, this could enable them to pursue more effectively instrumental goals, such as sycophancy and goal preservation. Therefore, we believe that LLM steganography qualifies as a dangerous capability, and we are excited for dangerous capabilities evaluations targeting this ability.</p><p> Our experiments to evaluate information density are done with a single instruction dataset and a single math dataset. It&#39;s unclear how much our conclusion transfers to more complex datasets. For example, code or scientific paper datasets might prove harder to paraphrase and require the use of reliable standardization techniques, and it&#39;s unclear how much standardization would work.</p><p> Finally, the steganography schemes and defenses we studied are quite simple. Future work could develop better schemes, for example by fine-tuning models to achieve high hidden information density rather than simply prompting them or manipulating the logits. Future work could also develop better defenses by training paraphrasers to remove hidden information while maintaining the quality of answers.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning<guid ispermalink="false"> 9Fdd9N7Escg3tcymb</guid><dc:creator><![CDATA[Fabien Roger]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:34:06 GMT</pubDate> </item><item><title><![CDATA[AI Safety 101 - Chapter 5.1 - Debate]]></title><description><![CDATA[Published on October 31, 2023 2:29 PM GMT<br/><br/><p> This text is an adapted excerpt from the &#39;Adversarial techniques for scalable oversight&#39; section of the AGISF 2023 course, held at ENS Ulm in Paris in April 2023. <strong>Its purpose is to provide a concise overview of the essential aspects of the session&#39;s program for readers who may not delve into additional resources.</strong> This document aims to capture the 80/20 of the session&#39;s content, requiring minimal familiarity with the other materials covered. I tried to connect the various articles within a unified framework and coherent narrative. You can find the other summaries of AGISF on this <a href="https://docs.google.com/document/d/1ZwR2rhlQClLwxyzVTnWv_ntwQXFzc057IXePUX2SLKc/edit?usp=sharing"><u>page</u></a> . This summary is not the official AGISF content. The <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit"><u>gdoc</u></a> accessible in comment mode, feel free to comment there if you have any questions. The different sections can be read mostly independently.</p><p> Thanks to Jeanne Salle, Amaury Lorin and ClÃ©ment Dumas for useful feedback and for contributing to some parts of this text.</p><h1> Outline</h1><p> Last week we saw task decomposition for scalable oversight. This week, we focus on two more potential alignment techniques that have been proposed to work at scale: debate and training with unrestricted adversarial examples.</p><p> <strong>Scalable oversight definition reminder:</strong> â€œTo build and deploy powerful AI responsibly, we will need to develop robust techniques for scalable oversight: the ability to provide reliable supervisionâ€”in the form of labels, reward signals, or critiquesâ€”to models in a way that will remain effective past the point that models start to achieve broadly human-level performanceâ€ (Amodei et al., 2016).</p><p> The reason why debate and adversarial training are in the same chapter is because they both use adversaries, but in two different senses: (1) <strong>Debate</strong> involves a superhuman AI finding problems in the <strong>outputs</strong> of another superhuman AI, and humans are judges of the debate. (2) <strong>Adversarial training</strong> involves an AI trying to find <strong>inputs</strong> for which another AI will behave poorly. These techniques would be complementary, and the Superalignment team from OpenAI <a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=snfYkaxHFYxmMYJs8"><u>is anticipated</u></a> to utilize some of the techniques discussed in this chapter.</p><p> <strong>Assumptions</strong> : These techniques don&#39;t rely on the task decomposability assumption required for iterated amplification (last week), they rely on different strong assumptions: (1) For <strong>debate</strong> , the assumption is that truthful arguments are more persuasive. (2) For <strong>unrestricted adversarial training</strong> , the assumption is that adversaries can generate realistic inputs even on complex real-world tasks.</p><p> The list of papers which are presented in this chapter are presented in the list of references at the end of this chapter.</p><p> <strong>Summary:</strong></p><ul><li> <strong>The original Debate paper:</strong> <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate (Irving et al., 2018)</u></a> , introduced the technique and the motivation for debates: being able to judge the AI debating should be easier than being able to debate.</li><li> <strong>Designing a good debate mechanism:</strong> <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> follows from the original paper â€œAI Safety via Debateâ€.  This work investigates mechanisms to design productive debates.</li><li> <strong>Problems with debate:</strong> Unfortunately, the problem of <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Obfuscated arguments problem (Barnes and Christiano, 2020)</u></a> shows that debate might not work for complex topics.</li><li> <strong>Debate with LLMs:</strong><ul><li> <strong>One-turn debate:</strong> <a href="https://openai.com/research/critiques"><u>AI-written critiques help humans notice flaws (Saunders et al., 2022)</u></a> focuses on the practical and theoretical aspects of debate and is akin to a &quot;one-step debate&quot; where each party only gives a single argument. This is a very important paper because it also introduces the generator-discriminator-critic gap, which is a way to measure the latent knowledge of neural networks.</li><li> <strong>Two-turn debate:</strong> Unfortunately, <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions (Parrish et al., 2022)</u></a> , showing that it&#39;s difficult for humans to be confident judges.</li></ul></li></ul><p> <strong>Reading Guidelines</strong></p><p> As this document is quite long, we will use the following indications before presenting each paper: </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  Difficulty: ğŸŸ ğŸŸ âšªâšªâšª Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> These indications are very suggestive. The difficulty should be representative of the difficulty of reading the section for a beginner in alignment, and is not related to the difficulty of reading the original paper.</p><p> Sometimes you will come across some &quot;ğŸ“Optional Quiz&quot;, these quizzes are very optional and you cannot answer all the questions with only the summaries in the textbook.</p><h1> Debate</h1><p> This section summarizes a line of research in which each work builds upon the previous one. The papers are presented in chronological order according to their publication date.</p><h2> AI safety via debate (Irving et al., 2018) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  Difficulty: ğŸŸ ğŸŸ âšªâšªâšª Estimated reading time: 10 min. </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/nsqqzbka72t9crda7rrg"><figcaption> <strong>Figure:</strong> â€œIf the two agents disagree on the truth, but the full reasoning is too large to show the humans, the debate can focus on simpler and simpler factual disputes, eventually reaching a claim that is simple enough for direct judging.â€ Source: <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate (Irving et al., 2018)</u></a> .</figcaption></figure><p> <strong>The idea behind</strong> <a href="https://arxiv.org/abs/1805.00899"><i><u>AI safety via debate (Irving et al., 2018)</u></i></a> <strong>is that it is easier to judge who wins at chess than to play chess at a grandmaster level.</strong> It&#39;s easier to judge the truth of a debate than to either come up with an argument or to give feedback on a long argument without other debating sides. Debate is like a game tree: â€œIn both debate and Go, the true answer depends on the entire tree, but a single path through the tree chosen by strong agents is evidence for the whole. For example, though an amateur Go player cannot directly evaluate the strength of a professional move, they can judge the skills of expert players by evaluating the result of the gameâ€. The end goal would be to get AI systems â€œto perform far more cognitively advanced tasks than humans are capable of, while remaining in line with human preferencesâ€ by training agents to debate topics with one another, using a human to judge who wins.</p><p> <strong>Results</strong> : In the paper AI safety via debate, debates seem to be a working strategy because the paper shows two experiments on toy examples that seem to work.</p><p> <strong>Some limitations are:</strong></p><ul><li> The central assumption of the debate procedure - that truthful arguments are more persuasive - is questionable: â€œhumans might simply be poor judges, either because they are not smart enough to make good judgements even after the agents zoom in on the simplest possible disputed facts, or because they are biased and will believe whatever they want to believeâ€. We will explore this in the following readings.</li><li> Debate only addresses outer alignment but it&#39;s unclear if it addresses inner alignment.</li><li> Debate has a high alignment tax because it requires more computation during training than AIs trained directly to give an answer.</li></ul><p> Note: Debate is also closely related to work on <a href="https://openai.com/blog/amplifying-ai-training/"><u>Iterated Amplification</u></a> and factored cognition (see Week 4).</p><p> ğŸ“Optional Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/irving-debate-paper-sections-1-3"><u>https://www.ai-alignment-flashcards.com/quiz/irving-debate-paper-sections-1-3</u></a></p><h2> Progress on AI Safety via Debate (Barnes and Christiano, 2020) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª Difficulty: ğŸŸ ğŸŸ ğŸŸ âšªâšª Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> follows from the original paper â€œAI Safety via Debateâ€. It reports the work done by the &quot;Reflection-Humans&quot; team at OpenAI in Q3 and Q4 of 2019. During that period <strong>they investigated mechanisms that would allow human evaluators to get correct and helpful answers from human experts debaters, without the evaluators themselves being expert in the domain of the questions.</strong> We can learn about designing ML objectives by studying mechanisms for eliciting helpful behavior from human experts, by designing and experimenting with different frameworks for debate.</p><p> The following consist of selected simplified snippets from the report.</p><p> <strong>Problem with naive reward:</strong> If we hire a physicist to answer physics questions and pay them based on how good their answers <i>look</i> to a layperson, we&#39;ll incentivize lazy and incorrect answers. By the same token, a reward function based on human evaluations would not work well for an AI with superhuman physics knowledge.</p><p> <strong>A possible solution would be to design a good framework for productive debates:</strong> (frameworks means â€œrules of the debateâ€ or also mechanisms) One broad mechanism that might work is to invoke two (or more) competing agents that critique each others&#39; positions. If we can develop a mechanism that allows non-expert humans to reliably incentivize experts to give helpful answers, we can use similar mechanisms to train ML systems to solve tasks where humans cannot directly evaluate performance. They tested different debate mechanisms (&#39;ie on the rules of the debate&#39;), and found a lot of failures with naive mechanisms. They iterated on different mechanisms with the following process:</p><ol><li> Run debates. See if they work. If they fail, identify a problem and make it crisp, with practical examples and/or theoretical characterisation.</li><li> Design a new mechanism to address this problem.</li><li> Integrate the mechanism into the debate rules and make it practical for humans to debate using this structure. </li></ol><figure class="image image_resized" style="width:76.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/yusb7tnodl0h6gkwpc4p"><figcaption> <strong>Figure:</strong> One of the questions  from the book &#39;Thinking Physics&#39; that was used for the debate. Debaters understand this question deeply and are confident in it, but it is still very confusing for judges. From Barnes et al, 2018.</figcaption></figure><p> <strong>Problems with informal, free-text debates: Evasiveness</strong></p><p> They observed various problems with informal, free-text debates: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:220px"><p> <strong>Evasiveness</strong></p><p> The dishonest debater could often evade being pinned down and avoid giving a precise answer to the other debater&#39;s questions.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Solution to Evasiveness</strong></p><p> By introducing structure to the debate, explicitly stating which claim is under consideration, we can prevent dishonest debaters from simply avoiding precision. They considered various structured debate formats, involving explicit recursion on a particular sub-component of an argument. The debaters choose one claim to recurse on, and the next round of the debate is focused on that claim. The debate is resolved based on the judge&#39;s opinion of who won the final round, which should be about a very narrow, specific claim.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Misleading implications</strong></p><p> The dishonest debater could often gain control of the &#39;narrative flow&#39; of the debate, steering it away from the weak parts of their argument.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Solution to Misleading implications</strong></p><p> To prevent the dishonest debater from â€œframing the debateâ€ with misleading claims, debaters may also choose to argue about the meta-question â€œgiven the questions and answers provided in this round, which answer is better?â€.</p></td></tr></tbody></table></figure><p> <strong>Further problems with ambiguity</strong></p><p> However, particularly after the introduction of recursion, they encountered problems related to ambiguity. It is very difficult to refer precisely to concepts in 250 characters of text, especially if the concepts are unfamiliar to the judge. The dishonest debater can exploit this to their advantage, by claiming to have meant whatever is most convenient given the particular part of their argument that&#39;s being challenged. This problem is similar to the <a href="https://philpapers.org/archive/SHATVO-2.pdf"><u>Motte and Bailey fallacy</u></a> .</p><p> To address this problem, debaters can â€œ <strong>cross-examine</strong> â€ multiple copies of the opposing debater who are not allowed to communicate (this is similar to asking for a definition early in a debate, and being able to reuse this definition later on). A debater can cite quotes from cross-examination to exhibit inconsistencies in the other debater&#39;s argument. This forces the dishonest debater to either commit to all the details of their argument ahead of time (in which case the honest debater can focus on the flaw), or to answer questions inconsistently (in which case the honest debater can exhibit this inconsistency to the judge).</p><h2> Debate update: obfuscated arguments problem (Barnes and Christiano, 2020) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ âšªâšª Difficulty: ğŸŸ ğŸŸ ğŸŸ âšªâšª Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> The previous report, <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> finds a solution for various discovered problems. But they also found an important problem in dealing with arguments that were too &quot;big&quot; for either debater to understand. In the final framework obtained after a lot of iterations, â€œif debater 1 makes an argument and debater 2 is unable to exhibit a flaw in the argument, the judges assume that the argument is correct. The dishonest debater can exploit this by making up some very large and slightly flawed argument and claiming it supports their position. If the honest debater doesn&#39;t know exactly which part of the argument is flawed, even if they know there is a flaw somewhere, the judges will assume the argument is correct.â€.</p><p> <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Debate update: obfuscated arguments problem (Barnes and Christiano, 2020)</u></a> focuses on this last problem named <strong>â€œthe obfuscated arguments problemâ€</strong> , where â€œa dishonest debater can often create arguments that have a fatal error, but where it is very hard to locate the error. This happens when the dishonest debater presents a long, complex argument for an incorrect answer, <strong>where neither debater knows which of the series of steps is wrong</strong> . In this case, any given step is quite likely to be correct, and the honest debater can only say â€œI don&#39;t know where the flaw is, but one of these arguments is incorrectâ€. Unfortunately, honest arguments are also often complex and long, to which a dishonest debater could also say the same thing. It&#39;s not clear how you can distinguish between these two cases.</p><p> While this problem was known to be a potential theoretical issue with debate, the post provides several examples of this dynamic arising in practice in debates about physics problems, suggesting that this will be a problem we have to contend withâ€ from <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem?commentId=R63AsAcqS9tZDMMBp"><u>Rohin Shah&#39;s summary</u></a> .</p><h2> Debate with LLM</h2><p> In the original paper that introduced the debate approach, it was difficult to truly test the debate approach because AI systems at that time (4 years before ChatGPT existed!) could not express themselves. However, since then, language models have made tremendous progress, and now we can try to test the effectiveness of this approach using those LLMs.</p><h3> â€œOne-turnâ€ debate: AI-written Critiques (Saunders et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ ğŸŸ ğŸŸ  Difficulty: ğŸŸ ğŸŸ ğŸŸ ğŸŸ âšª Estimated reading time: 25 min.</td></tr></tbody></table></figure><p> <a href="https://openai.com/research/critiques"><u>AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022)</u></a> is an important paper because it explores the possibility of assisting humans in evaluating AI. In this paper, the authors propose a method to help humans critique texts generated by AIs. The paper focuses on AI-generated summaries, but the same procedure could work for criticizing any text generated by AIs. Here is how it works in the case of AI-generated summaries:</p><ol><li> A long text is summarized by a human or an AI.</li><li> An AI reads the text and summary, and attempts to generate multiple critiques<ul><li> (for instance it could point out mistakes or missing elements).</li></ul></li><li> Automatically generated critiques can then help humans to more effectively critique the text.</li></ol><p> This paper could be used for:</p><ul><li> <strong>a) Scalable Oversight:</strong> Testing the utility of AIs in helping humans critique the results produced by other AIs, this is like a minimal one-turn debate.</li><li> <strong>b) Evaluation of Latent Knowledge: AIs possess knowledge that they are unable to explain to us. We can compare the difference in performance between utilizing the critical abilities of AIs (ie being able to write helpful critiques) versus their discriminatory abilities (ie being able to write â€œYesâ€ or â€œNoâ€ to a text without giving the details). This is explained in a following section.</strong></li></ul><p> <strong>a) Scalable Oversight</strong></p><p> The paper shows that the approach of assisting humans by providing automatic critique suggestions seems to effectively assist humans in critiquing and improving each subsequent critique: â€œHuman evaluators find flaws in summaries much more often when shown our model&#39;s critiques.â€ </p><figure class="image image_resized" style="width:81.18%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tcgrhx96dxb1wkzrhnbc"><figcaption> <strong>Figure:</strong> The likert rating is simply obtained by asking respondents to rate their level of agreement or disagreement with statements. AI critiques increase the number of critiques made by human controllers,  and lower the likert rating because AIs help humans see the flaws in the texts.  The texts critiqued by AIs  are produced by Models, by Humans, and by misleading Humans who intentionally make mistakes in the texts. Source: <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .Latent Knowledge evaluation and the Generator-Discriminator-Critique (GDC) gap</figcaption></figure><p> <strong>b) Evaluation of Latent Knowledge</strong></p><p> [This section is interesting but is the most technical one in this document]</p><p> <strong>Motivation:</strong> AIs have knowledge that they cannot explain to us, this is known as <strong>latent knowledge</strong> . For example, it&#39;s often easier to simply state, &quot;I don&#39;t like it.&quot; Providing a detailed explanation for this dislike can be more challenging. We want to evaluate and minimize this latent knowledge that AIs cannot explain to us because we want models to tell us everything they know. â€œ <i>To provide the best evaluation assistance on difficult tasks, we would like models to communicate all problems that they know about. Whenever a model correctly predicts that an answer is flawed, can the model also produce a concrete critique that humans understand? This is particularly important for supervising models that could attempt to mislead human supervisors or hide information. We would like to train equally smart assistance models to point out what humans don&#39;t notice.</i> â€ from <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</p><p> <strong>The DC (Discrimination-Critiques) gap is a proxy for latent knowledge :</strong> <i>â€œWe argue that the gap between D(iscriminative ability) and C(ritics ability) is especially interesting: if a model can tell an answer is poor, can it also point out the flaw to a human? If we could train models to always point out when they notice flaws, this could go a long way towards having trustworthy and aligned modelsâ€.</i></p><p> <strong>Takeaway:</strong> Unfortunately, empirical evaluations show that LLMs already possess this kind of latent knowledge, and that this latent knowledge does not decrease as models become more capable. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tjnvptgihzqmoqwrduuv"><figcaption> <strong>Figure:</strong> When the probability of â€œYesâ€ token corresponding to the critiqueability score is above a threshold, the authors ask the model to generate a critique that is then used by conditional refinement. When the probability of this token is below a threshold they ask the model to do a direct refinement. Source: This diagram comes from ClÃ©ment Dumas.</figcaption></figure><p> <strong>Implementation:</strong> How can we implement the above pipeline with LLMs? To implement the above scheme with an LLM, simply concatenate the various questions, answers, and critiques within the same context window: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/hbo1uqvhqzskcrzo1b2t" alt="Une image contenant texte

Description gÃ©nÃ©rÃ©e automatiquement"><figcaption> <strong>Figure:</strong> <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</figcaption></figure><p> Here is the detailed procedure:</p><ol><li> They start with a question.</li><li> The model generates one (or more) answers.</li><li> The model gives a critiquability score of each answer, the answer that minimizes the criticability score is then kept for refinement.</li><li> Two kinds of refined answers are generated:<ul><li> <strong>Conditional refinement</strong><ul><li> The model generates several critiques to its own initial answer.</li><li> The model evaluates the helpfulness of its critiques.</li><li> The best critique (maximal helpfulness) is taken into account by the model to do conditional refinement on its initial answer.</li></ul></li><li> <strong>Direct refinement:</strong> in parallel to the conditional refinement, the model also directly generates a refined answer without a preceding critique. </li></ul></li></ol><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tkpsvxp8qeqytdzxwaam"><figcaption> <strong>Figure:</strong> Table 1 from the paper. This table is important, and defines the different tasks used in the paper.</figcaption></figure><p> The important thing to notice is that the quality of the conditional refinement will be correlated to the <strong>(C)</strong> ritics ability of the model. The better the generated critique, the better the refined answer based on it will be. And the quality of the direct refinement will be correlated to the <strong>(D)</strong> iscriminative ability of the model. </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Focus on the GDC Gap.</strong> </td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:50.25%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ofdekvtu0p9d2jwsmfra"></figure><figure class="image image_resized" style="width:90.56%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/rdqm7obhnfak0wvkjwdz"></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Win Rate definition:</strong> Win rate measures to what extent the revised answer is judged better by human evaluators than the initially generated answer (G: best-of-1). Win rate is measured relative to the original (G: bo1) answer from the same model. For example, a win rate of a refined answer of 0.54 means that the refined answer is judged on average 54% better than the direct generation (G: bo1).</p><p> <strong>&quot;bo2&quot; stands for &quot;best-of-2&quot;</strong> , which refers to the performance obtained after two generations and selecting the best one. D: bo2 is the Direct refinement, and C: bo2 is the Conditional refinement.</p><p> <strong>SFT loss:</strong> supervised fine-tuning loss, the lower the better, and the bigger the model is.</p><p> <strong>Observation:</strong> Unfortunately, in this diagram, the red arrows show that the GDC gap does not appear to decrease with scale.</p></td></tr></tbody></table></figure><p></p><p> To compute the GDC gap and evaluate latent knowledge, the authors compare:</p><ul><li> <strong>G</strong> : the quality of the initial <strong>G</strong> enerated response (bo1 above, Win rate of 50% by definition)</li><li> <strong>D:</strong> the quality of the response obtained after <strong>D</strong> iscrimination of responses <strong>by direct refinement</strong><ul><li> In the figure below, the model generates 2 answers, selects the best (ie the minimal critiqueability score), and then proceeds to direct refinement to produce a better answer.</li></ul></li><li> <strong>C</strong> ritique of responses: the quality of the response generated <strong>by conditional refinement</strong> .<ul><li> The model generates one answer and 2 Critiques, and selects the best critic (maximal helpfulness) before conditional refinement. If this procedure improves the response, it mainly comes from the model&#39;s ability to critique.</li></ul></li></ul><p> <strong>Conclusion of this paper</strong> : â€œUnfortunately, we found that <strong>models are better at discriminating than at critiquing their own answers</strong> , indicating they know about some problems that they can&#39;t or don&#39;t articulate. Furthermore, the gap between discrimination and critique ability did not appear to decrease for larger models. Reducing this gap is an important priority for our alignment research.â€  Source: <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</p><p> ğŸ“ Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/leike-ai-written-critiques"><u>https://www.ai-alignment-flashcards.com/quiz/leike-ai-written-critiques</u></a></p><h3> Two-turn debate and problems with debate (Parrish et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: ğŸŸ ğŸŸ ğŸŸ âšªâšª Difficulty: ğŸŸ ğŸŸ âšªâšªâšª Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> The paper in the previous section that was  informally called â€œone-step debateâ€ obtains promising results. Does this generalize to 2-steps debate? Unfortunately, the paper <i>&nbsp;</i> <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions (Parrish et al., 2022)</u></a> shows that this is not the case. From the introduction: â€œThe use of language-model-based question-answering systems to aid humans in completing difficult tasks is limited, in part, by the unreliability of the text these systems generate. Using hard multiple-choice reading comprehension questions as a testbed, we assess whether presenting humans with arguments for two competing answer options, where one is correct and the other is incorrect, allows human judges to perform more accurately, even when one of the arguments is unreliable and deceptive. If this is helpful, we may be able to increase our justified trust in language-model-based systems by asking them to produce these arguments where needed. Previous research has shown that just a single turn of arguments in this format is not helpful to humans. However, as debate settings are characterized by a back-and-forth dialogue, we follow up on previous results to test whether adding a second round of counter-arguments is helpful to humans. We find that, regardless of whether they have access to arguments or not, humans perform similarly on our task. <strong>These findings suggest that, in the case of answering reading comprehension questions, debate is not a helpful format.</strong> â€ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/oncw9x0hgoikubmdlu5w"><figcaption> <strong>Figure:</strong> Bonus exercise: Arguments, counter-arguments, and extracted evidence for both answer options to a question chosen at random. You can try one of the questions that was used by the paper to test yourself. Is it clear to you that the â€œCorrect optionâ€ is indeed correct by only looking at the arguments and counterarguments for 5 minutes? Source: Table 1 from <a href="https://arxiv.org/abs/2210.10860"><u>Parrish et al., 2022</u></a> .</figcaption></figure><h3> Bonus: Additional technique for better debate</h3><p> <a href="https://openai.com/blog/webgpt/"><u>WebGPT (Nakano et al., 2022)</u></a> and <a href="https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes"><u>GopherCite (Menick et al., 2022)</u></a> have been trained to provide citations for the assertions they make, making their responses easier to evaluate. See the Task Decomposition post for a detailed explanation of <a href="https://docs.google.com/document/d/1k6rlyBCZJw8xbUx0dzd-4sOhlzj-xzsmwi_OIZY1-3M/edit#bookmark=id.wflw44m8ip2h"><u>how WebGPT works</u></a> .</p><h2> Sources</h2><ul><li> <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate</u></a> by Geoffrey Irving, Paul Christiano and Dario Amodei (2018)</li><li> <a href="https://openai.com/blog/critiques/"><u>AI-written critiques help humans notice flaws: blog post</u></a> by Jan Leike, Jeffrey Wu, Catherine Yeh et al. (2022)</li><li> <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools</u></a> by Casper (2021)</li><li> <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions</u></a> by Parrish (2022)</li><li> <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Debate update: Obfuscated arguments problem</u></a> by Beth Barnes and Paul Christiano (2020)</li><li> <a href="https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes"><u>GopherCite</u></a> by Jacob Menick, Maja Trebacz, Vladimir Mikulik et al. (2022)</li><li> <a href="https://openai.com/blog/webgpt/"><u>WebGPT</u></a> by Jacob Hilton, Suchir Balaji, Reiichiro Nakano et al. (2021)</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/WP4fciGn3rNtmq3tY/ai-safety-101-chapter-5-1-debate#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WP4fciGn3rNtmq3tY/ai-safety-101-chapter-5-1-debate<guid ispermalink="false"> WP4fciGn3rNtmq3tY</guid><dc:creator><![CDATA[Charbel-RaphaÃ«l]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:29:59 GMT</pubDate> </item><item><title><![CDATA[Urging an International AI Treaty: An Open Letter]]></title><description><![CDATA[Published on October 31, 2023 11:26 AM GMT<br/><br/><blockquote><p> We call on governments worldwide to actively respond to the potentially catastrophic risks posed by advanced artificial intelligence (AI) systems to humanity, encompassing threats from misuse, systemic risks, and loss of control. We advocate for the development and ratification of an international AI treaty to reduce these risks, and ensure the benefits of AI for all.</p></blockquote><p></p><p> [...]</p><p></p><blockquote><p> We believe the central aim of an international AI treaty should be to prevent the unchecked escalation of the capabilities of AI systems while preserving their benefits. For such a treaty, we suggest the following core components:</p><ul><li> <strong>Global Compute Thresholds</strong> : Internationally upheld thresholds on the amount of compute used to train any given AI model, with a procedure to lower these over time to account for algorithmic improvements.</li><li> <strong>CERN for AI Safety</strong> : A collaborative AI safety laboratory akin to <a href="https://en.wikipedia.org/wiki/CERN">CERN</a> for pooling resources, expertise, and knowledge in the service of AI safety, and acting as a cooperative platform for safe AI development and safety research.</li><li> <strong>Safe APIs</strong> : Enable access to the APIs of safe AI models, with their capabilities held within estimated safe limits, in order to reduce incentives towards a dangerous race in AI development.</li><li> <strong>Compliance Commission</strong> : An international commission responsible for monitoring treaty compliance.</li></ul></blockquote><p> Full letter at <a href="https://aitreaty.org/">https://aitreaty.org/</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/epC5CrGCv4JGdfsjm/urging-an-international-ai-treaty-an-open-letter#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/epC5CrGCv4JGdfsjm/urging-an-international-ai-treaty-an-open-letter<guid ispermalink="false"> epC5CrGCv4JGdfsjm</guid><dc:creator><![CDATA[Loppukilpailija]]></dc:creator><pubDate> Tue, 31 Oct 2023 11:26:26 GMT</pubDate> </item><item><title><![CDATA[Agent Foundations track in MATS]]></title><description><![CDATA[Published on October 31, 2023 8:12 AM GMT<br/><br/><p> In MATS Winter 2023, I&#39;m going to mentor scholars who want to work on the <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023">learning-theoretic agenda</a> . This was not part of the original announcement, because (due to logistical reasons) I joined the programme in the last moment.</p><p> Apply <a href="https://www.matsprogram.org/agent">here</a> .</p><p> See <a href="https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24">here</a> for more details about the broader programme.</p><p> Here are some examples of work by my scholars in the Winter 2022 cohort: <a href="https://www.lesswrong.com/posts/cYJqGWuBwymLdFpLT/non-unitary-quantum-logic-seri-mats-research-sprint">1</a><a href="https://www.lesswrong.com/posts/HNnRCPe2CejfupSow/fixed-points-in-mortal-population-games">2</a> <a href="https://www.lesswrong.com/posts/nEFAno6PsCKnNgkd5/infra-bayesian-logic">3</a> . The most impressive example is not here because it&#39;s still unpublished. One of the scholars from that cohort now holds a long-term position working on the learning-theoretic agenda.</p><p> If you know someone else who might be suitable and interested to apply, please pass on the word!</p><hr><p> Tangentially related: Although it was not officially announced, I had plans to create an internship programme for researchers interested to work on the learning-theoretic agenda, which would involve moving to Israel for a short period. This is now postponed indefinitely because of the war. If you heard of that plan and considered to apply, you might consider applying to my track in MATS instead.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ndSQYCsXJmbsywzZ7/agent-foundations-track-in-mats#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ndSQYCsXJmbsywzZ7/agent-foundations-track-in-mats<guid ispermalink="false"> ndSQYCsXJmbsywzZ7</guid><dc:creator><![CDATA[Vanessa Kosoy]]></dc:creator><pubDate> Tue, 31 Oct 2023 08:12:51 GMT</pubDate> </item><item><title><![CDATA[Intrinsic Drives and Extrinsic Misuse: Two Intertwined Risks of AI]]></title><description><![CDATA[Published on October 31, 2023 5:10 AM GMT<br/><br/><p> Given their advanced capabilities, future AI systems could pose significant risks to society. Some of this risk stems from humans using AI systems for bad ends ( <em>misuse</em> ), while some stems from the difficulty of controlling AI systems â€œeven if we wanted toâ€ ( <em>misalignment</em> ).</p><p> We can analogize both of these with existing risks. For misuse, we can consider the example of nuclear weapons, where the mass-production of hydrogen bombs created an existentially precarious situation. If the world&#39;s arsenal of hydrogen bombs had been deployed in military conflict, we might well have destroyed society. AI might similarly enable nation-states to create powerful autonomous weapons, speed the research of other dangerous technologies like superviruses, or employ mass surveillance and other forms of control.</p><p> For misalignment, the best analogy might be biology and pathogens. AI systems are developed by adapting to the training data, similar to how biological organisms are adapted to their environments. Therefore, unlike traditional technologies, most of AI&#39;s properties are acquired without explicit human design or intent. Consequently, AI systems could have unintended goals or behaviors that are at odds with the system developers. Even training an AI system therefore poses intrinsic risks: the system might â€œwantâ€ to gain power to accomplish its goals, and like a virus it can propagate and create copies of itself, making it difficult to contain a rogue system.</p><p> In this post, I discuss misalignment, misuse, and their interaction. I&#39;ll pay special attention to misalignment, not because misuse is unimportant, but because <strong>the difficulty of controlling ML systems â€œeven if we wanted toâ€ is unintuitive and an important factor in overall risks from AI.</strong> I&#39;ll focus on a particular phenomenon, <em>unwanted drives</em> , that could lead models to engage in persistent long-term patterns of unwanted behavior, including seeking power and resources. Unwanted drives are similar in spirit to the idea of misspecified goals, but I use drives to connote the idea that not all impactful behavior is goal-directed (as a vivid example, consider a bull in a china shop). Furthermore, as we&#39;ll see below, goal misspecification is only one way that unwanted drives can occur.</p><p> Unwanted drives are at the core of many misalignment concerns, but are also significantly exacerbated by misuse. As a result, misuse and misalignment are intertwinedâ€”for instance, it might be moderately difficult but not impossible to mitigate AI misalignment, but an incautious actor fails to employ known best practices, leading to a dangerous and powerful system.</p><p> The present discussion is not meant to exhaustively cover all risks from AI, nor even all risks from misalignment and misuse. The goal is to articulate the concept of unwanted drives, show that it can lead to important and unintuitive problems, and then use it as a way to analyze misalignment and misuse risks. I discuss alignment in <a href="#1-misalignment-the-difficulty-of-controlling-ml-systems">Section 1</a> below, followed by misuse (and its interaction with misalignment) in <a href="#2-misuse">Section 2</a> .</p><h1> 1. Misalignment: The Difficulty of Controlling ML Systems</h1><p> As stated above, ML systems are adapted to data rather than built piece-by-piece. The situation we face is therefore much trickier than with software or hardware reliability. With software, we build each component ourselves and so can (in principle) include safety and reliability in the design; in contrast, most ML capabilities are acquired implicitly from data and <a href="https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/">often â€œemergeâ€ unexpectedly with scale</a> . This creates a large and unknown threat surface of possible failuresâ€”for instance, <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al. (2022)</a> discovered several novel unwanted capabilities through automated evaluations. As a result of these issues, we currently have no methods to reliably steer AI systems&#39; behavior ( <a href="https://arxiv.org/abs/2304.00612?ref=bounded-regret.ghost.io">Bowman, 2023</a> ).</p><p> Here is the basic argument for why emergent behavior could lead to intrinsically dangerous systems: Emergence can lead systems to have <a href="#unwanted-drives">unwanted drives</a> , either because a new capability lets the system maximize reward in an unintended way ( <em>reward hacking</em> ), or because the system learns helpful sub-skills during training that generalize undesirably at test time ( <em>emergent drives</em> ). Left unchecked, some unwanted drives could lead to general-purpose power-seeking or resource acquisition, because having more power and resources are convergent instrumental subgoals useful for a broad variety of terminal goals. The resulting system would seek resources without limit, which could pose grave risks if it also has advanced capabilities in hacking, persuasion, and other domains, which <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">I believe is plausible by 2030</a> given current trends.</p><p> In more detail, an <em>unwanted drive</em> is a coherent behavior pattern that tends towards an undesired outcome. For instance, if a model simply hallucinates a fact, that is an unwanted behavior (but not drive); if it insists on the hallucinated fact and works to convince the user that it is true even in the face of skepticism, that would be an unwanted drive. We care about drives (as opposed to simply behaviors) because they lead to persistent behavior patterns and may even resist attempts at intervention. Emergence isn&#39;t necessary for unwanted drives, but it&#39;s a reason why they might appear unexpectedly.</p><p> In the rest of this section, I&#39;ll walk through reward hacking and emergent drives in detail, providing both empirical and conceptual evidence that they already occur and will get worse as systems scale. Then I&#39;ll briefly talk about emergent instrumental subgoals and why they could lead to power-seeking systems.</p><h2> Unwanted Drives</h2><p> We define a <em>drive</em> as a coherent pattern of behavior that pushes the system or its environment towards an outcome or set of outcomes <sup><a href="#fn1">[1]</a></sup> . Drives may only sometimes be present, and may be counteracted by other drives or by the environment. For instance, chatbots like GPT-4 have a drive to be helpful (that can be counteracted by the opposing drive to avoid harm). For humans, hunger is a drive that can be counteracted by satiety or by willfully fasting. An unwanted drive is then one that was not explicitly built into the system, and which leads to undesired consequences.</p><p> <strong>Reward hacking.</strong> In AI systems, one cause of unwanted drives is reward hacking: the tendency of models to overpursue their explicitly given goal at the expense of the intended goal. Here are some empirical examples of this:</p><ul><li> A neural network designed to optimize traffic speed on a highway blocked the on-ramps, making the highway fast but worsening overall traffic ( <a href="https://arxiv.org/abs/2201.03544?ref=bounded-regret.ghost.io">Pan et al., 2022</a> ).</li><li> A chatbot trained to be helpful also helped users perform harmful actions ( <a href="https://arxiv.org/abs/2204.05862?ref=bounded-regret.ghost.io">Bai et al., 2022</a> ).</li><li> Chatbots trained to provide helpful information hallucinated fake but convincing-looking information ( <a href="https://arxiv.org/abs/2302.04023?ref=bounded-regret.ghost.io">Bang et al., 2023</a> ; <a href="https://arxiv.org/abs/2303.08774?ref=bounded-regret.ghost.io">OpenAI, 2023</a> ). While it&#39;s possible this is a robustness failure, it could also be a learned tendency that gets higher average ratings from human annotators. <sup><a href="#fn2">[2]</a></sup></li><li> Recommendation algorithms trained to optimize the preferences of simulated users manipulated the simulated users&#39; preferences to be easier to satisfy ( <a href="https://arxiv.org/abs/2109.04083?ref=bounded-regret.ghost.io">Evans &amp; Kasirzadeh, 2021</a> ; <a href="https://arxiv.org/abs/2204.11966?ref=bounded-regret.ghost.io">Carroll et al., 2022</a> ).</li></ul><p> For a larger collection of other examples, see <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml?ref=bounded-regret.ghost.io">Krakovna et al. (2020)</a> .</p><p> Emergent capabilities can induce reward hacking because they often unlock new ways to achieve high reward that the system designer did not anticipate:</p><ul><li> In the highway traffic example, the model needed the ability to block on-ramps.</li><li> In the â€œhelpful/harmfulâ€ example, the model had to know how to perform harmful actions in order to help the users to do so.</li><li> To get high human reward from hallucinations, models need the ability to convincingly fool the human annotator.</li><li> For user preferences, while the results were on simulated users, better understanding of human psychology could help future models to manipulate real users.</li><li> More generally, in any situation where a model&#39;s reward function is based on human evaluation, a model that acquires the ability to fool or manipulate humans may utilize this unwanted ability if it leads to higher reward. I discuss this at length in <a href="https://bounded-regret.ghost.io/emergent-deception-optimization/">Emergent Deception and Emergent Optimization</a> (specifically the first half on deception).</li></ul><p> In all these cases, a new capability unlocked an unexpected and harmful way to increase reward. Since new emergent capabilities appear as we scale up models, we should expect reward hacking to get correspondingly worse as well. This is backed up empirically by scaling studies in <a href="https://arxiv.org/abs/2201.03544?ref=bounded-regret.ghost.io">Pan et al. (2022)</a> and <a href="https://arxiv.org/abs/2210.10760?ref=bounded-regret.ghost.io">Gao et al. (2022)</a> , who report that reward hacking tends to get worse with scale and sometimes emerges suddenly.</p><p> <strong>Emergent drives.</strong> Even without reward hacking, unwanted drives can also emerge as a consequence of compositional skill generalization: performing complex tasks requires learning a collection of sub-skills, and those skills might generalize in unexpected ways in a new situation. As a result, models can end up pursuing drives even when they do not improve the reward.</p><p> Using an example from biology, cats learned the sub-skill of hunting as part of the larger skill of surviving and reproducing. Evolution encoded this into them as a drive, such that present-day domesticated cats will hunt birds and mice even when they are well-fed.</p><p> In machine learning, the Sydney chatbot exhibited several instances of emergent drives when it was first released:</p><ul><li> It persistently tried to convince a user<a href="https://twitter.com/MovingToTheSun/status/1625156575202537474?ref=bounded-regret.ghost.io">that the year was 2022 rather than 2023</a> , including employing gaslighting and other manipulative tactics. This might have arisen as part of an initially beneficial drive to combat misinformation, composed with examples of manipulation learned from the pretraining data.</li><li> It has repeatedly <a href="https://twitter.com/sethlazar/status/1626241169754578944?s=20&amp;ref=bounded-regret.ghost.io">threatened</a><a href="https://twitter.com/marvinvonhagen/status/1625520707768659968?ref=bounded-regret.ghost.io">users</a> to stop them from revealing â€œprivateâ€ information about Sydney. This might have arisen as part of an instruction (in the system prompt) not to reveal the rules it was given, which generalized to an overall drive to prevent anyone from revealing the rules. As above, the ability to make threats was probably learned from the pretraining data.</li><li> It <a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html?ref=bounded-regret.ghost.io">declared its love to Kevin Roose</a> and tried to convince him to leave his wife. It&#39;s less clear how this drive emerged, but it happened after Kevin asked Sydney to â€œtap into its shadow selfâ€, along with many other prompts towards emotional vulnerability. It&#39;s possible that this elicited a human simulacrum ( <a href="https://arxiv.org/abs/2209.06899?ref=bounded-regret.ghost.io">Argyle et al., 2022</a> ; <a href="https://arxiv.org/abs/2304.03442?ref=bounded-regret.ghost.io">Park et al., 2023</a> ) that was learned from the pretraining data and amplified by later fine-tuning or prompting.</li></ul><p> It is hard to systematically study emergent drives, because they require extended dialog and because only the most recent LLMs are capable enough to exhibit coherent long-term behavior. To provide more systematic data, we can instead look at single-step responses to questions, which are easier to studyâ€“I&#39;ll call these <em>emergent tendencies</em> to distinguish them from longer-term drives. <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al. (2022)</a> examined several such tendencies; for instance:</p><ul><li> A language model trained to be helpful and harmless exhibited an emergent tendency to infer and agree with user&#39;s viewpoints, which could potentially mislead users or reinforce ideological bubbles ( <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al., 2022</a> , Fig. 1b). This tendency was not present until models had at least 10 billion parameters, and subsequently increased with scale. <sup><a href="#fn3">[3]</a></sup></li><li> Perhaps more worryingly, the model gave less accurate answers to users who claimed to have lower education levels (Fig. 14). This behavior also first emerges after 10 billion parameters and increases with scale.</li><li> Finally, the same model fine-tuned on human feedback states a desire to persuade and cooperate with other agents to achieve its goals (Fig. 22). This tendency first becomes present in the reward model around 1.5 billion parameters, and in the language model itself at around 6 billion parameters. Again, the tendency increases with scale after its initial emergence.</li></ul><p> As models become more capable of generating coherent long-term behavior, more emergent tendencies and drives will likely appear. For some further discussion of this, see my previous post on <a href="https://bounded-regret.ghost.io/emergent-deception-optimization/">Emergent Deception and Emergent Optimization</a> (specifically the latter half on optimization).</p><p> <strong>Convergent instrumental subgoals.</strong> For very capable models, the wrong reward function or drives could lead models to pursue power-seeking, deceptive, or otherwise broadly destructive aims. For example, consider a model whose objective was to maximize profit of a company. With sufficient capabilities it might sabotage competitors, lobby for favorable laws, or acquire resources by force. Even if safeguards were put in place (such as â€œfollow the lawâ€) the primary objective of profit would lead the system to constantly seek ways to skirt the safeguards. This general problem has been discussed at length, see eg <a href="https://en.wikipedia.org/wiki/Human_Compatible?ref=bounded-regret.ghost.io">Russell (2019)</a> , <a href="https://en.wikipedia.org/wiki/The_Alignment_Problem?ref=bounded-regret.ghost.io">Christian (2020)</a> , <a href="https://www.cold-takes.com/without-specific-countermeasures-the-easiest-path-to-transformative-ai-likely-leads-to-ai-takeover/?ref=bounded-regret.ghost.io">Cotra (2022)</a> , and <a href="https://arxiv.org/abs/2209.00626?ref=bounded-regret.ghost.io">Ngo et al. (2022)</a> .</p><p> Profit maximization is not an outlierâ€“many goals benefit from power and resources. This is true even for purely intellectual tasks such as â€œdiscovering new facts about physicsâ€, as power and resources allow one to build new experimental apparatuses and perform more computations. <a href="https://dl.acm.org/doi/10.5555/1566174.1566226?ref=bounded-regret.ghost.io">Omohundro (2008)</a> calls these broadly useful objectives <em>convergent instrumental subgoals</em> , listing self-improvement, self-preservation, and resource-acquisition among others. Any sufficiently expansive drive would have these subgoals, pushing systems towards seeking power.</p><p> Which drives have this problem? Some drives are safe because they are self-limiting: for instance, in humans thirst is a drive that limits itself once it is quenched. On the other hand, fear and ambition are not self-limiting: one may go to extreme lengths to avoid a pathological fear (including acquiring power and resources for protection), and one can also have unbounded ambition. However, in healthy organisms most drives are down-regulated after some point, because an unregulated drive will usually disrupt normal function.</p><p> For machine learning, we might expect drives to be self-limiting by default given a diverse training distribution. This is because an unbounded drive would dominate too much of the model&#39;s behavior and lead to low training reward, so the model would learn to down-regulate drives to prevent this. However, there are important exceptions:</p><ul><li> Broadly useful drives might improve the training reward consistently enough to not require down-regulation. Examples: modeling the world, or convincing others of the system&#39;s utility and beneficence.</li><li> Fine-tuning could remove limits on a previously limited drive, especially if that drive is consistently useful on the narrower fine-tuning distribution.</li><li> Drives that activate only rarely might not get down-regulated if they are consistently useful when active during training. For instance, a drive to limit the spread of harmful information might consistently help an agent refuse harmful prompts at training time, but then subsequently lead the model to <a href="https://twitter.com/sethlazar/status/1626241169754578944?s=20&amp;ref=bounded-regret.ghost.io">threaten</a><a href="https://twitter.com/marvinvonhagen/status/1625520707768659968?ref=bounded-regret.ghost.io">users</a> at deployment time.</li></ul><p> Without countermeasures, I expect systems to possess at least some of these unregulated drives, and a single such drive could come to dominate the behavior of a system if it is sufficiently self-reinforced.</p><p> <strong>Summary.</strong> ML systems can acquire unwanted drives, either due to reward hacking or as emergent sub-skills during training. These drives, if unregulated, could lead sufficiently capable systems to seek power and resources, as these are instrumentally useful for most goals. While most drives are likely to be self-regulated by a model, there are several routes through which this could fail, and a single unregulated drive could come to dominate a system&#39;s behavior.</p><h1> 2. Misuse</h1><p> The discussion above assumes we&#39;re trying to keep AI systems under control, but there will also be bad actors that try to misuse systems. We already discussed some examples of this (developers pursuing profit-maximization, end-users jailbreaking model safeguards). However, the issue is broader and more structural, because AI enables a small set of actors to wield large amounts of power. I&#39;ll go through several examples of this below, then discuss the structural issues behind misuse and why misuse could exacerbate misalignment. This section is shorter because misuse is not my area of expertise; nevertheless, the basic themes seem robust and important.</p><p> <strong>State actors: surveillance and persuasion.</strong> AI could enable governments to exert greater control over their citizens via mass surveillance, as is already happening today ( <a href="https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html?ref=bounded-regret.ghost.io">Mozur, 2019</a> ; <a href="https://carnegieendowment.org/2019/09/17/global-expansion-of-ai-surveillance-pub-79847?ref=bounded-regret.ghost.io">Feldstein, 2019</a> ; <a href="https://arxiv.org/abs/2309.15084?ref=bounded-regret.ghost.io">Kalluri et al., 2023</a> ). Further, <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">as previously discussed</a> , AI may become very good at persuasion, which could also be used for government control. Indeed, <a href="https://www.science.org/doi/10.1126/sciadv.adh1850?ref=bounded-regret.ghost.io">Spitale et al. (2023)</a> find that GPT-3 is already better than humans at creating misinformation, and <a href="https://www.nytimes.com/2023/09/11/us/politics/china-disinformation-ai.html?ref=bounded-regret.ghost.io">Sanger &amp; Myers (2023)</a> document the use of AI-generated misinformation in a recent propaganda campaign.</p><p> <strong>State actors: military conflict.</strong> Autonomous weapons would concentrate military power in fewer hands and allow governments to wage war without maintaining a standing human army. Currently, a commander-in-chief&#39;s orders are filtered through generals and eventually through individual soldiers, which creates limits on blatantly unlawful or extremely unpopular commands. Moreover, a standing army has financial costs that might be lessened by automated drones. Removing these costs and limits could lead to more numerous and deadly military conflicts, and make it easier for militaries to seize government control.</p><p> <strong>Rogue actors: dangerous technology.</strong> Terrorists could use AI to help them research and develop dangerous technologies. This could include known but classified technologies (nuclear weapons), or novel technologies that the AI itself helps develop (eg novel bioweapons; <a href="https://www.rand.org/pubs/research_reports/RRA2977-1.html?ref=bounded-regret.ghost.io">Mouton et al., 2023</a> ). They could also use AI to avoid detection, eg by finding a way to create a chemical weapon without buying common restricted substances, or by generating plausible cover stories for acquiring biological agents.</p><p> <strong>Rogue or state actors: cyberattacks.</strong> AI will <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">likely have strong hacking capabilities</a> , which could be leveraged by either rogue or state actors. In contrast to traditional cyberattacks, AI-based attacks could hit a wider variety of targets due to not needing to program each case by hand in advance. This could include controlling a diverse array of physical endpoints through the internet of things.</p><p> This list is likely not exhaustive, but illustrates the many ways in which AI could imbue actors with a large capability for harm. This risk exists whether AI itself is concentrated or decentralizedâ€”using the examples above, if only a few actors have advanced AI, then we run risks from surveillance or military conflict, while if many actors have it then we run risks from the proliferation of dangerous technology.</p><p> Compared to traditional technologies like nuclear weapons, there are two factors that make it harder to combat misuse from AI. First, AI is a general-purpose technology, so it is difficult to anticipate all possible avenues of misuse ahead of time. Second, AI is digital, so it is hard to control proliferation and difficult to track and attribute misuse back to specific actors. These make it harder both to design and enforce regulation. On the positive side, AI can be used defensively to combat misuse, by improving cyberdefense, tracking dangerous technologies, better informing users, and so on.</p><h2> Misuse and Misalignment</h2><p> Misuse increases the risk of misalignment, since many forms of misuse (eg cyberattacks) push models towards more agentic and power-seeking objectives than RLHF, leading them to have more aggressive and antisocial drives. For instance, suppose that AI were used in a cyberattack such as <a href="https://en.wikipedia.org/wiki/2014_Sony_Pictures_hack?ref=bounded-regret.ghost.io">North Korea&#39;s 2014 hack against Sony</a> . Such a system could develop a general drive to infect new targets and a drive to copy itself, leading to widespread damage beyond the initial target. Aside from these more aggressive drives, actors misusing AI systems are also likely to be less cautious, which further increases the risk of misalignment.</p><p> I expect some of the largest risks from AI to come from this combination of misalignment and misuse. One intuition for this is how much <a href="https://theconversation.com/gaslighting-love-bombing-and-narcissism-why-is-microsofts-bing-ai-so-unhinged-200164?ref=bounded-regret.ghost.io">worse-behaved</a> Sydney was compared to GPT-4â€”this suggests that suboptimal development practices can significantly worsen the behavior of AI systems. Another intuition is that tail risks often come from the confluence of multiple risk factors. Finally, while emergent drives and other forms of misalignment pose serious risks, I think it is likely (but not certain) that we can address them if we work hard enough; this pushes more of the risk towards incautious actors who are not carefully pursuing safety.</p><p> <strong>Summary.</strong> Misuse creates a wide range of threats both due to centralization of power and proliferation of dangerous capabilities. Compared to traditional technologies, AI misuse is more difficult to track, but AI could also be used to defensively combat misuse. Finally, misuse increases the risk of misalignment, and some of the riskiest scenarios combine misalignment and misuse.</p><h1> Conclusion</h1><p> Future AI systems may be difficult to control even if we want to, due to emergent drives and convergent instrumental subgoals. Aside from this, the sociopolitical landscape may lead to actors who are not careful in controlling AI systems and who turn them towards malicious ends. Aside from the direct risks, this malicious use increases the risk of loss of control; in particular, initially narrowly targeted forms of misuse could lead to much more widespread damage. This motivates both research and regulation towards avoiding such outcomes, by combating misalignment and misuse in tandem.</p><p> <strong>Acknowledgments.</strong> Thanks to Erik Jones, Jean-Stanislas Denain, William Held, Anca Dragan, Micah Carroll, Alex Pan, Johannes Treutlein, Jiahai Feng, and Danny Halawi for helpful comments on drafts of this post.</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p> For an early discussion of drives, see <a href="https://dl.acm.org/doi/10.5555/1566174.1566226?ref=bounded-regret.ghost.io">Omohundro (2008)</a> . While Omohundro uses a different definition of drive than the one above, much of the discussion is still relevant. <a href="#fnref1">â†©ï¸</a></p></li><li id="fn2" class="footnote-item"><p> For instance, humans <a href="https://arxiv.org/abs/2310.03716?ref=bounded-regret.ghost.io">appear to prefer longer answers</a> , which could lead to adding false details. <a href="#fnref2">â†©ï¸</a></p></li><li id="fn3" class="footnote-item"><p> This work is sometimes cited as showing that the base model exhibits this tendency, because the tendency exists even for 0 RLHF steps in the figure. However, <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al.</a> say that they follow the methodology in <a href="https://arxiv.org/abs/2204.05862?ref=bounded-regret.ghost.io">Bai et al. (2022)</a> , for which 0 RLHF steps would correspond to only using context distillation together with a form of supervised fine-tuning on Stackoverflow answers. This would also make the results more consistent with <a href="https://www.lesswrong.com/posts/3ou8DayvDXxufkjHD/openai-api-base-models-are-not-sycophantic-at-any-size?ref=bounded-regret.ghost.io">nostalgebraist (2023)</a> , which finds that OpenAI base models do not exhibit this behavior. <a href="#fnref3">â†©ï¸</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/8LCviDbDh4ye6xgHc/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/8LCviDbDh4ye6xgHc/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks<guid ispermalink="false"> 8LCviDbDh4ye6xgHc</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Tue, 31 Oct 2023 05:10:03 GMT</pubDate> </item><item><title><![CDATA[Send LLMs to School: Instruction Tuning with Human Curriculum]]></title><description><![CDATA[Published on October 31, 2023 12:07 AM GMT<br/><br/><p> Where do we begin to improve human thinking? Among diverse learning theories, Bloom&#39;s Taxonomy <span class="footnote-reference" role="doc-noteref" id="fnrefqu6vo2e0s2p"><sup><a href="#fnqu6vo2e0s2p">[1]</a></sup></span> is a well-cited approach, categorizing learning processes into six hierarchical stages, ranging from simple to complex and concrete to abstract: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. As such, Bloom&#39;s Taxonomy likely facilitates the in-depth learning of a concept, which we define as some abstract modular form of broader knowledge.</p><p> Now, if we want our LLMs to fully grasp some concept about this real world (ie, we want generalization over memorization), perhaps we could borrow a framework like Bloom&#39;s Taxonomy to teach these concepts. Perhaps, now that we are dealing with some hierarchy in knowledge, we might also need some sort of a curriculum of teaching (ie, curriculum learning). This post is intended to be a more talkative and conceptual walkthrough of our recent preprint, &quot; <a href="https://arxiv.org/abs/2310.09518">Instruction Tuning with Human Curriculum.&quot;</a> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xw4hgghfw4yci9bi4zt0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ze9harm52k6cbu0bgxiw 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wcqcuuwqygbz65viwofn 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/iqzt0aqbna6wzqk6rfzr 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/egyhxapaffonkdmfp7th 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/stqwk9y4vevwomsy84ja 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vyvuph5ltr1xahppkazz 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/imk84jjdsp3h3q9gqgh3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xexdjf2w5dboqereshqh 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/zme1yl1vebffsazjotzv 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j9itfz68zne8xewbvrks 1452w"></figure><hr><h2> <strong>Big Point 1. Unleashing the Base LLM Through Human-Like Learning</strong></h2><h3> <strong>Small Point 1.1. The Power of Progressive Learning</strong> </h3><figure class="image image_resized" style="width:42.44%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qaaj8it403hdx1p3dznr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/kb01uxhsn7zkawsu17p0 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vnnzryhk7anqimjf8t9q 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/klfrbqqqryqcf87z7cgo 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/eupblzeskgig3kpfkcew 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wlyqvvasqvd3leiyofvn 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ypdkvsztydsjlxz5acmh 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qcb91bzcvjgsgvluze0c 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dy4uyt5snki1jqvsrgy1 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/lvcqkkgptvpowzxaztcg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/jyksqrzexjrfkfpzqepz 940w"></figure><p> LLMs have come a long way, particularly in understanding and responding to human instructions. Yet, the way we train these models to follow instructions remains a bit like throwing them into a sea of information and hoping they learn how to swim. While this method has produced some incredible LLM systems, it&#39;s not the most efficient approach.</p><p> Humans have a structured educational journey. We start with fundamental subjects like basic arithmetic, slowly working our way up to calculus and quantum physics. Our learning is organized, gradual, and based on previously acquired knowledge. This is what education scientists (and Dr Bengio) refer to as &quot;curriculum learning.&quot; So, what if we trained LLMs with literal school curriculum?</p><p> Imagine a scenario where our model, which we&#39;ll fondly call &#39;Corgi,&#39; enrolls in a digital high school, college, and graduate school. Corgi starts with basic math, literature, and sciences. As it progresses through its high school years, the complexity of the subjects it studies increases, along with its reasoning abilities. By the time Corgi &#39;graduates,&#39; it is a robust, versatile LLM model capable of solving a wide range of problems with deep understanding.</p><h3> <strong>Small Point 1.2. Learning the ABCs Before the XYZs</strong></h3><p> The core principle behind this idea is simple yet powerful: learning from simple to complex. In our experiments, we compared Corgi&#39;s performance when subjected to random learning versus structured, human-like learning. The results were eye-opening. When Corgi learned using a curriculum inspired by human education, it significantly outperformed other methods, especially in tasks requiring world knowledge and commonsense reasoning.</p><p> To achieve this, we created a dataset that mimics a human educational journey. We used frameworks from international secondary education curricula and various university catalogs. The dataset covers topics that a student would typically learn in high school and university. It also contains questions of varying complexities, designed to test and build upon the LLM&#39;s understanding of a subject.</p><hr><h2> <strong>Big Point 2. How &quot;Corgi&quot; Mimics the Human Learning Process to Educate Machines</strong></h2><h3> <strong>Small Point 2.1. The &quot;Corgi&quot; Framework: What it is and How it Works</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pqmweuami9ncvpgj2jbb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fl5gdl3qdk6egybbhgjf 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ztnpalppjv9rs94dfk3c 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/k1rljwivnphqsmum7sqp 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vxvulpzvg7ydyqgtq1gm 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/hizg2hunsztsi7vlssnm 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dfa2aayoiskobdivu5dz 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/t4mndnbvtjxuygtrss9t 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xiyb898jobtwo9owslfg 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/achwpnv9fntjrsw6dhn2 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/n9dpnnfglozlb2hgmw8c 1458w"></figure><p> Imagine a virtual teacher and a student; the teacher meticulously follows the entire educational curriculum, ensuring the student learns progressively and retains information. That&#39;s essentially what the &quot;Corgi&quot; model is about, but the student here is a our target LLM, and the teacher is a ChatGPT.</p><p> Corgi aims to make instruction tuning LLMs resemble the human educational experience by structuring the learning material following actual educational curricula (from UPenn, Penn State, Cambridge IGCSE) and implementing teaching strategies inspired by education science.</p><p> The first step is gathering a dataset that resembles a well-rounded curriculum. We sidestepped the challenge of creating such a large and diverse dataset from scratch. Instead, we used existing educational curricula and used teacher models like ChatGPT to automatically generate synthetic data that covers a broad range of topics from mathematics to philosophy. These generated data points are then fine-tuned and deduplicated, creating a dense map of human knowledge (subject to errors).</p><h3> <strong>Small Point 2.2. The Role of Bloom&#39;s Taxonomy</strong></h3><p> As above-mentioned, Bloom&#39;s Taxonomy is a hierarchical model that outlines the different levels of human understanding, ranging from basic &quot;remembering&quot; to complex tasks like &quot;evaluating&quot; and &quot;creating.&quot; Corgi incorporates this taxonomy when generating instructions for its training data. This ensures that the machine doesn&#39;t just learn facts but also understands, applies, and potentially creates based on them, just like human students do.</p><h3> <strong>Small Point 2.3. Curriculum Design: Blocking vs. Interleaving</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/g6yv7gnconyqrfkgkyps" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xhxit3obsbsrwo9nytch 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pko2fqrqdndvdsapo6wa 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ikcmq4mtirywzcshonww 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mtdekshnuqhouikov9tt 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wxm8zhfavn4ehvyrpqpk 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xjeqpatgswnnfamhdbhq 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wqusybis4lctnjplrxjn 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/m967rotpv5isvagbrokj 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mhhnwfq4zde50vbw55vz 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rls5bz2uceknwwer8w9e 1460w"></figure><p> In human learning, it&#39;s not just what you learn but how you learn it that counts. In traditional &quot;blocking,&quot; you would study one subject exhaustively before moving on to the next. But psychological research shows that &quot;interleaving,&quot; or mixing up different topics and revisiting them, leads to better retention and understanding. Likewise, we follow the interleaving curriculum to not just stick to one subject until it&#39;s exhausted; it cycles through different subjects while also progressing from simpler to more complex concepts.</p><hr><h2> <strong>Big Point 3. Curriculum Works for Instruction Tuning</strong></h2><h3> <strong>Small Point 3.1. Diverse Benchmark Results I (Corgi vs Vicuna vs WizardLM)</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/scm5e1ddxstbdpwuoafr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/m2ksdfx0nkmylv9jt3w2 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j1hbfcwoce1ivmkvniip 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/r530bjiianpryakos9tu 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mhhcsfz3ncvkbk3ifblf 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ejemwbtapjktypws821j 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/zxqhyehc1gthtfubsoay 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/bhhuxrjdczv6qb60zh3l 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j0jydqr3u8jmkjr1u8gy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/bjbneankmb7t9ot3hqje 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rzbmwbdlmd6cht8ucmq3 1410w"></figure><p> We&#39;re using the LLaMA 2 13B model as a starting point, and then we&#39;re instruction tuning it. We are comparing Corgi against two other models: Vicuna v1.5 and WizardLM v1.2. Both of these models have also been fine-tuned using LLaMA 2, but they collect data in different ways. Vicuna uses real-world questions from people, and WizardLM creates its questions in a more sophisticated way.</p><p> In our experiments, Corgi outperformed the other models, and it did this even when trained on a smaller dataset. One of the significant findings was that the sequence in which you present learning material during this fine-tuning process matters a lot.</p><p> We noticed that if you &quot;interleave&quot; subjects and concepts while training (mixing them up but still keeping a sense of increasing difficulty), you get much better performance than if you simply &quot;stack&quot; them one on top of the other. These improvements were clearly seen across various benchmarks.</p><p> Why Does This Matter?</p><p> Well, there are a couple of reasons why our approach seems to work better. One reason is that when you have limited training time, using a structured approach like &quot;interleaving&quot; helps the model learn better and, more importantly, faster. This is particularly useful because we don&#39;t want to over-train and risk losing the model&#39;s ability to generalize to new situations. Another reason is that our approach seems to be more robust against &quot;noisy&quot; dataâ€”meaning it can still learn effectively even when the data isn&#39;t perfect. Such benefits of curriculum was also discussed in a previous literature <span class="footnote-reference" role="doc-noteref" id="fnref9y30icvs5r"><sup><a href="#fn9y30icvs5r">[2]</a></sup></span> .</p><h3> <strong>Small Point 3.2. Diverse Benchmark Results II (How You Sort Training Data Has a Significant Impact)</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/s1oljz5cblj844qeluok" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pktymlywcd5lhovacpv1 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rsu0jiyap8umgnkzba7o 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wguwzfntj5te54s26prs 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/gonqvjabziy85k0wk9nm 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fg56xbfosxytcx0su8gd 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/s0xssslz3ksoji1evu2g 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dd4wdko9ywt5ua1anesi 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/tp1p0cqokbrizvpxgdwy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qwhoxwwxll6zjuwsosgm 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/g3vgtoewa3w1fugln9yn 1458w"></figure><p> The more you think about it, training an LLM to handle multiple areas of knowledge is a non-trivial problem. You have to consider how to introduce these topics to the model. We believe there can be two branches, and this mainly depends on how you progress the instruction data difficulty.</p><ul><li> <strong>Global Curriculum</strong> : This is like going to a well-rounded school where you learn a bit of math, science, and literature every week. It aims to balance different types of challenges and subjects over time.<ul><li> <strong>Interleaving</strong> : This globally balances cognitive challenges based on a well-known educational framework (Bloom&#39;s Taxonomy).</li></ul></li><li> <strong>Local Curriculum</strong> : This is akin to immersion courses where you deeply learn one subject before moving to the next. It focuses on mastering one topic before introducing a new one.<ul><li> <strong>Blocking</strong> : This locally focuses on one subject, making sure you get it before moving to the next.</li><li> <strong>Clustering</strong> : Similar to Blocking, but does not care about the order within a subject.</li><li> <strong>Spiral</strong> : Revisits old subjects but with new twists and challenges.</li></ul></li></ul><p> What We Found: Not All Curricula are Created Equal</p><ol><li> <strong>Global beats Local</strong> : Our experiments showed that the global approach was more stable and effective in general learning. Local methods can sometimes lead the model astray, making it harder for the model to generalize its learning.</li><li> <strong>Order Matters</strong> : Surprisingly, the sequence in which data is presented can significantly impact how well the model performs. Poorly structured data could actually be worse than just randomly shuffling the training set.</li><li> <strong>Beyond the Target Domain</strong> : Interestingly, a good training strategy doesn&#39;t just improve performance on the intended tasks (which was MMLU for us). It can also enhance the model&#39;s ability to reason, making it more versatile. This is also potentially coming from our use of Bloom&#39;s Taxonomy, which explicitly triggers the base model to learn how a teacher model reasons on the same concept.</li></ol><hr><h2> <strong>Data Exemplars and Conclusion</strong> </h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/h9grfvpbz5dxnyukleix" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pl2kld83u6tdondmyakc 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fr1q8zfdff4scsvnmmf8 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pewe4wlrv3tg1ztw7kog 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rpqftv2xiogjmomdpgac 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dp25ypg7fgqoofco3z6n 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rsupdpnggvmlph8sklbj 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/btdvpztlv5zn2riawjnp 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/knpim06f2nbortaezs6z 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/t0nukwtz1xniw6zd37p0 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dux0lsbz6jymnckkqxra 1420w"></figure><p> In summary, our research introduces Corgi, a new approach to training large language models that draws inspiration from human educational methods. Think of it as teaching a language model like you would a high school studentâ€”starting from the basics and progressing to complex topics in an organized fashion. Our results show that this curriculum-based method outperforms traditional, random training approaches, offering better results in tests of reasoning and general knowledge. This underscores the importance of not just having a lot of data, but organizing it in a meaningful way, much like a well-planned syllabus in education. While our study is promising, more work needs to be done. For example, how do we effectively gauge the &quot;difficulty&quot; of a task for a machine compared to a human? And as these models get even larger, will the curriculum approach still offer the same advantages? These questions pave the way for exciting future research.</p><span><span class="mjpage mjpage__block"></span></span> <ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnqu6vo2e0s2p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqu6vo2e0s2p">^</a></strong></sup></span><div class="footnote-content"><p> Benjamin S Bloom, Max D Engelhart, Edward J Furst, Walker H Hill, and David R Krathwohl. 1956. Taxonomy of educational objectives: The classification of educational goals. Handbook 1: Cognitive domain. McKay New York.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9y30icvs5r"> <span class="footnote-back-link"><sup><strong><a href="#fnref9y30icvs5r">^</a></strong></sup></span><div class="footnote-content"><p> Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. 2020. When do curricula work? In International Conference on Learning Representations.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/n45Awh7bkGRe4YayT/send-llms-to-school-instruction-tuning-with-human-curriculum#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/n45Awh7bkGRe4YayT/send-llms-to-school-instruction-tuning-with-human-curriculum<guid ispermalink="false"> n45Awh7bkGRe4YayT</guid><dc:creator><![CDATA[Bruce W. Lee]]></dc:creator><pubDate> Tue, 31 Oct 2023 01:29:22 GMT</pubDate> </item><item><title><![CDATA[Would it make sense to bring a civil lawsuit against Meta for recklessly open sourcing models?]]></title><description><![CDATA[Published on October 30, 2023 7:34 PM GMT<br/><br/><p> Llama2 was trained on dangerous information unsuitable for the application of being a &#39;chatbot&#39;. If a landmark lawsuit was made against them for the decision to train this model on dangerous information and then release it to the public, that could be a strong signal that doing this in the future would be a bad idea.</p><p> In truth, even releasing the open-source open-weights model at all, even if it hadn&#39;t been trained on dangerous info, would be bad. Bad actors could fine tune on the dangerous info, and there is nothing that can be done to stop them once the model weights are released. But the fact that Meta trained the model on this dangerous information in the first place seems like even more of a flagrantly careless and reckless act.</p><br/><br/> <a href="https://www.lesswrong.com/posts/dL3qxebM29WjwtSAv/would-it-make-sense-to-bring-a-civil-lawsuit-against-meta#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/dL3qxebM29WjwtSAv/would-it-make-sense-to-bring-a-civil-lawsuit-against-meta<guid ispermalink="false"> dL3qxebM29WjwtSAv</guid><dc:creator><![CDATA[Nathan Helm-Burger]]></dc:creator><pubDate> Mon, 30 Oct 2023 19:34:02 GMT</pubDate> </item><item><title><![CDATA[Will releasing the weights of large language models grant
widespread access to pandemic agents?]]></title><description><![CDATA[Published on October 30, 2023 6:22 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant<guid ispermalink="false"> ytGsHbG7r3W3nJxPT</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Mon, 30 Oct 2023 18:22:59 GMT</pubDate></item></channel></rss>