<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 25 日星期三 02:19:14 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What is a Sequencing Read?]]></title><description><![CDATA[Published on October 25, 2023 2:10 AM GMT<br/><br/><p>如今，<span>最常见的</span><a href="https://www.jefftk.com/p/sequencing-intro">基因测序</a>形式可能是“双端”测序。非常令人印象深刻：测序仪可以从两端处理相同的核酸片段！这意味着每个观察结果如下所示：</p><p></p><pre> +--------------+---------+--------------+
|转发阅读 |差距|反向读|
+--------------+---------+--------------+
</pre><p>由于当您进一步对片段进行测序时，准确性（“质量”）往往会下降，因此从两端测序比尝试从一端对整个片段进行测序可以提供更准确的数据。而且因为我们通过将重叠的序列拼凑在一起（“组装”）来构建更大的序列（“重叠群”），所以由间隙分隔的两个碱基序列实际上通常比相同数量的没有间隙的碱基序列更有用。</p><p>通常用“2x150”这样的名称来指代双端测序，其中“2x”告诉我们它是双端，“150”告诉我们它从每个末端读取 150 个碱基，总共 300 个碱基每个片段。</p><p>但这引入了一个术语问题：什么是读取？当我们只进行“单端”测序时，很明显：每个测序片段、每个连续的碱基序列都是一个读数。然而，通过双端测序，这些不再是同一件事了！ “阅读”可能意味着两件事：</p><p></p><ul><li>阅读：一系列连续的碱基。</li><li>读取：来自测序片段的碱基。</li></ul><p>例如，假设我们有：</p><p></p><pre> >;SRR14530724.2 2/1
CATTTTCGACGGCGTCGATGTACAAAGGTTTAACCATAGTAAGTCCGAAGC
TACAGGCTTATGACACCGCAGAGTCAATGTATTCCGGTGACAATGTACTGA
TGTACAGTGGGACTGACACTGTCTCTTATACACATCTCCGAGCCCACGA
>;SRR14530724.2 2/2
TGTCAGTCCCACTGTACATCAGTACATACACACCGGAATACATTGACTCTG
CGGTGTCATAAGCCTGTAGCTTCGGACTTACTATTGGTATAACCTTTGTACA
TCGACGCCGTCGAAAATGCTGTCTCTTATACACATCTGACGCTGCCGAC
</pre><p>这是正向读取 (SRR14530724.2 2/1) 和反向读取 (SRR14530724.2 2/2)，它们共同构成对样品片段的单个观察，并且通常一起分析。这算是一次阅读还是两次阅读？</p><p>事实证明，人们两者都做，这导致了很多误解！</p><p>一些例子：</p><ul><li><p> Illumina 将它们算作两个。他们说 NovaSeq X 上的“25B”流动池 <a href="https://www.illumina.com/systems/sequencing-platforms/novaseq-x-plus/specifications.html">将产生</a>52B 配对末端读数，或 2x150 的约 8Tb（“太碱基”，或万亿碱基）。由于 52B * 150b = 7.8Tb（他们称之为 ~8Tb），这告诉我们他们正在计算正向和反向读取。</p></li><li><p> Element 将它们视为一个。<a href="https://www.elementbiosciences.com/products/aviti/specs">他们说</a>2x150 高输出流通池可产生 300 Gb 和 1B 读数。由于 1B * 150b * 2 = 300 Gb，这告诉我们他们将正向和反向读取一起算作一个。</p></li><li><p> Singular<a href="https://singulargenomics.com/g4/">不清楚</a>，但我很确定他们将每个片段的观察结果计算为一次读取。</p></li><li><p>欧洲核苷酸档案馆将它们视为其中之一。例如，如果您访问<a href="https://www.ebi.ac.uk/ena/browser/view/ERR1470825">ERR1470825</a> ，这是 2x250 的 Illumina MiSeq 配对末端测序，您会看到它显示 2.2M 读取，如果您下载 fastq.gz 文件，您会发现正向和反向各有 2.2M 读取文件。</p></li><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579973/">罗斯曼等。 al 2021</a>将它们算作一个。他们在第一次使用时说“配对读取”，然后在稍后“读取”，你可以看出他们将它们算作一个，因为 (a) 他们经常给出奇数，例如“只有 337 个 SARS-CoV-2 读取” ”和（b）如果你<a href="https://www.jefftk.com/p/case-rates-to-sequencing-reads">重新分析数据，</a>它们的数字只有在对数进行计数时才有意义。</p></li><li><p>我最近与一个学术团体讨论了潜在的合作伙伴关系，在我最近重新分析的一篇论文中，他们将其视为一个，而在通过电子邮件交谈时，他们将其视为两个。</p></li><li><p>我最近采访的一家商业测序公司将它们算作两个。</p></li><li><p>问ChatGPT和Claude，两人都算作两个。例如：“在短读长双端测序中，正向-反向对通常被视为两个读长”。</p></li></ul><p>这真是一团糟！而且，更糟糕的是，据我所知，除了“读取”之外，没有标准术语，要么“正向和反向读取都是示例”，要么“当一起考虑时，正向和反向读取是什么”。</p><p>我一直使用“读”来表示“读对”，但考虑到歧义，我认为我应该改用另一个术语。 NCBI SRA 使用“<a href="https://www.ncbi.nlm.nih.gov/sra/ERX1541950">斑点</a>”，但似乎没有其他人使用这个术语。你可以直接说“readpair”，这很好，但有点长。可能的“配对”或“伴侣”会好吗？想法？</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0emTAqvGbjmRHoY9XwhQx65bTMKWLG8b9MBZtsda18UdwDbKZ9EWT3H4g1P3EL4eQl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111293185162894676">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read<guid ispermalink="false"> JQmgoLAkFKkhxLMhz</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:10:12 GMT</pubDate> </item><item><title><![CDATA[How to become a citizen of the internet]]></title><description><![CDATA[Published on October 25, 2023 1:48 AM GMT<br/><br/><p><i>最初发表于</i><a href="https://eliqian.substack.com"><i>非线性</i></a></p><hr><p>四年级时通过<a href="https://podbay.fm/p/cgp-grey/e/1378742400/video"><u>CGP Gray 视频</u></a>探索 Reddit。着迷于互联网上“社区”的含义。从 r/AskReddit 和 r/Jokes 开始。很快就会发现最好的东西总是在评论中。</p><p>每天阅读 Reddit。请注意，网上人们的说话方式不同。它们更有趣、更诙谐、更容易理解。记个账户，但又不敢说什么。别担心，很快你就会厌倦潜伏并鼓起勇气发表评论。帮助互联网陌生人打造游戏电脑。当有人感谢您的建议时，请为自己感到自豪。与您经常光顾的 Reddit 子版块中的用户感到一种奇怪的亲缘关系。第一次跟踪某人的个人资料。想想看，拥有某人在线生活的数字历史是多么酷的事情。请注意，当您转向 Reddit 寻找越来越多的兴趣时：r/swimming，然后是 r/EDC，然后是 r/IOSsetups。后来，r/辩论、r/Formula1 和 r/网球。</p><p>出于您不记得的原因开始观看一项运动。一开始很困惑。请注意，Reddit 是迄今为止了解您正在观看的任何体育运动的最佳场所。只要和那些经常谈论这个问题的人在一起，你就可以成为专家。阅读旧帖子以了解历史。了解人们对运动员和团队的看法。在 GOAT 辩论兔子洞中花费太多。你感觉自己很聪明，但最终你需要学会独立思考。</p><p>在此期间，您还会收听播客。也许以前有过，但<a href="https://www.relay.fm/cortex/"><u>Cortex</u></a>是你记得的第一个。它由两个通过在线创作谋生的人主持。你现在觉得这不太酷，但它以后会对你产生巨大的影响。</p><p>真正<i>深入</i>了解生产力工具。日历应用程序、笔记应用程序、待办事项应用程序（尤其是待办事项应用程序）。用你在朋友身上花了多少钱来吓唬他们。探索生产力 YouTube：观看有关 Notion 的视频、如何有效学习、最好读的书籍以及为什么应该赚取被动收入。意识到你对生产力工具的痴迷实际上是对<i>生活方式设计的</i>痴迷。了解蒂姆·费里斯并收听他的播客。无论你最终做什么，都要渴望变得出色。听 Naval 的一集。第一次了解初创公司。确定它不适合你，但把它留在你的脑海里。观看阿里·阿卜杜勒。认为创造东西非常酷。首先对你读过的书做笔记。记日记。继续看阿里。有一天，无需考虑太多，您就会决定创建一个博客。我想如果你听到一个想法足够多，它就会变得显而易见。不知道该写什么。您试图效仿您所钦佩的作家和创作者，但最终得到的只是有关生产力软件和人生目标的帖子。它们现在足够好，可以分享，但几个月后，你会因为尴尬而把它们拿下来。没关系——这意味着你正在培养品味。</p><p>创建一个网站来托管您的写作。没有什么看法，但是发表一些东西感觉很好。恭喜您，您已从互联网消费者晋升为<i>互联网</i>贡献者。继续遵循你对出版的直觉——你不知道为什么，但制作一些东西并将其发布到世界上感觉是最有意义的事情。</p><p>发现<a href="https://www.youtube.com/@NotOverthinking"><u>另一个播客</u></a>，这个改变你的生活。其中一位主持人非常喜欢 Twitter。决定尝试一下。这只是名人和名人在发推文，而你的朋友都没有在上面。放弃。</p><p>开始上大学。不知道你想做什么。你的朋友说他们想进入金融行业。一些年长的朋友正在咨询。您大多只是想拥有一份令您感到自豪的工作。花了一个半学期的时间摸爬滚打。一个周末，不要为政治科学课写一篇论文，而是写一篇关于马尔科姆·格拉德威尔和跨性别者参与体育运动的文章。有点害怕发布它。无论如何都要这样做，并发现人们欣赏你的观点。决定更加认真地对待写作。在 Twitter 上关注您喜欢的一些作家和播客嘉宾。你的时间线是科技兄弟和游牧作家的奇怪组合。你仍然不知道 Twitter 的意义是什么，也不知道如何使用它。请注意，Twitter 上的人们说话方式不同。与现实生活不同，也与 Reddit 不同。您时间线上的个人资料图片成为熟悉的面孔。开始形成对人的看法，并决定你比其他人更喜欢某些人。去看看他们在关注谁，然后也关注他们。您的动态正在转变为反映您是谁以及您渴望成为谁的信息。</p><p>阅读有关如何使用 Twitter 的博客文章。你需要回复人们并实际发推文。呵呵。开始回复。一开始感觉很奇怪，当你回头看你早期的回复时，它们<i>很</i>奇怪。你需要调整自己在网上的严肃程度。几周后，你就会得到第一个追随者。有人非常喜欢你所说的话并关注你，这是一种超现实的感觉。</p><p>在某些时候，虽然我忘记了具体是什么时候，你会了解加密货币。你认为这很酷，因为没有人真正知道他们在做什么，而且年轻人有可能成为专家。加入远卡斯特。它就像 Twitter，但上面只有 1000 人。试着说些什么，你会惊讶地发现真实的人会给你真实的答案。在 Farcaster 上进行对话。这是您第一次 1) 建立基于智力联系的关系，2) 建立完全基于在线的关系。向人们伸出援手。与陌生人视频通话。他们中的大多数人你只交谈过一次，但有些人你会保持联系。几个月后，您将举办一次早午餐，并在现实中与其中的十几个人见面。</p><p>加倍努力在网上结识朋友。你为自己制定了一条规则——每当你读到你喜欢的东西时，你就会联系作者并要求聊天。明年您将接到 50 多个此类电话。即使在您写这篇文章时，仍然要敬畏您现在拥有一些在互联网上认识的亲密朋友。你开始拓展你的可能性领域。互联网告诉你，与任何人的对话只需要一个（或两个、三个）冷漠的DM。</p><p>意识到 Twitter 是互联网上文​​化传播的地方。这是人们分享最未经修饰的想法的地方，也是人们最愿意与陌生人联系的地方。如果我们的现实生活发生在工作场所、校园和第三空间，那么在线生活则发生在 Twitter 上。尝试想想如果没有 Twitter、在线写作以及你遇到的人，你的生活会是什么样子。真的很难想象，因为这些事情对你的影响是如此深远。奇怪的是，你几乎所有的工作都来自 Twitter，你会邀请网上的朋友参加你的婚礼，而写作可能是你余生都会做的事情。有趣的是，这一切对你来说意义重大——这个我们称之为互联网的大游戏。一个由联系、地位和社区组成的宏观世界，常常让人感觉比物理世界更伟大、更宏伟。很感激你找到了这个地方。您是互联网公民。</p><br/><br/> <a href="https://www.lesswrong.com/posts/rHiLH8BsoRPToxfx3/how-to-become-a-citizen-of-the-internet#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/rHiLH8BsoRPToxfx3/how-to-become-a-citizen-of-the-internet<guid ispermalink="false"> rHiLH8BsoRPToxfx3</guid><dc:creator><![CDATA[eq]]></dc:creator><pubDate> Wed, 25 Oct 2023 01:48:02 GMT</pubDate></item><item><title><![CDATA[Verifiable private execution of machine learning models with Risc0?]]></title><description><![CDATA[Published on October 25, 2023 12:44 AM GMT<br/><br/><p> Risc0 是一个虚拟机，可以运行任何 riscv（ <i>C、C++、Rust、Go 和 Nim 都可以编译为 riscv</i> ）程序，随着时间的推移记录计算状态的痕迹，然后，无论执行运行多长时间因为，它可以将该执行跟踪折叠成 1KB 的里德-所罗门代码，该代码可以作为计算运行正确、输出来自输入的证明进行传输，而无需泄露代码或输入的未加密副本。<br>询问器通过对收据代码的一系列特征进行采样来验证它，如果计算的<i>任何</i>方面不正确，则每次检查都有 3/4 的机会暴露这一点。因此，经过一百次左右的检查，我们可以有效地确定计算的正确性，而无需自己运行计算。 <span class="footnote-reference" role="doc-noteref" id="fnref4kqliegpy1a"><sup><a href="#fn4kqliegpy1a">[1]</a></sup></span></p><p>当<a href="https://www.risczero.com/news/rust-but-verify">我看到它暗示这可以应用于 ML 模型时</a>，我感到难以置信，因为我预计在像 Risc0 这样的环境中运行神经模型会使其性能降低数万倍。 Risc0 确实将传统 CPU 计算的性能降低了大约那么多<span class="footnote-reference" role="doc-noteref" id="fnrefqx7a21dnm3"><sup><a href="#fnqx7a21dnm3">[2]</a></sup></span> （尽管这仍然足够快，足以实用）。<br>是的，这篇文章没有讨论性能，而是讨论了许多非深度学习（？）模型类型，例如增强树和随机森林。所以我猜这可能不适用于验证训练运行。它是否适用于某些精炼推理模型？</p><hr><p>我发现 Risc0 非常有趣还有另外两个原因。一是我正在从事社交计算工作，但更深层的兴趣与技术末世论中正在进行的对话有关，即高级代理是否倾向于或远离相互透明，也就是说，走向协调、和平、贸易，或走向战争和压迫（<i>例如，</i> <a href="https://www.researchgate.net/publication/283986931_The_Dark_Forest_Rule_One_Solution_to_the_Fermi_Paradox"><i>黑暗森林</i></a>）。 Risc0 将任何程序的执行跟踪（无论多长）压缩为可在恒定时间内检查的证明。这对我来说是一个强有力的迹象，表明信息系统之间透明的可行性，即使在最不利的可能条件下，我们可以假设根本没有可信的硬件基础设施。<br><i>有了</i>可信的硬件，经过验证的计算就可以以正常速度运行，并且相互透明变得微不足道。硬件安全的唯一成本可能是增加防篡改，但在我看来，一个发达的协调基础设施，至少在人类规模上，将能够监控周围的物理实体和传感器，并在它们出现时发出尖叫声。进行篡改，几乎不需要任何成本。<br>我认为像 Risc0 这样的 ZK 系统适用于可信执行环境（TEE）被破解的风险非常非常小的情况，也是不可接受的。在去中心化金融中，这种情况很常见，因为网络共享状态的完整性都是纠缠在一起的，如果我们假设每个 TEE 的完整性是确定的，我们就会建造纸牌屋，即使成功破解存储一个人钱包的 TEE 也可能通过允许他们在难以察觉的情况下铸造、膨胀、污损货币来破坏整个网络。相比之下，Risc0 以数学确定性保证了计算的完整性，这让我对并行化全局可验证计算的项目更加充满希望。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn4kqliegpy1a"> <span class="footnote-back-link"><sup><strong><a href="#fnref4kqliegpy1a">^</a></strong></sup></span><div class="footnote-content"><p>至少，这是我经过两天学习后的感觉。通过观看一些<a href="https://www.youtube.com/watch?v=j35yz22OVGE&amp;list=PLcPzhUaCxlCjdhONxEYZ1dgKjZh3ZvPtl">Risc0 Study Club 视频</a>辨别出来。有人提到收据可以压缩为 1 个码字，但出于效率原因仅压缩为 256 个码字。我猜码字是 1 个 riscv 机器字那么大，4 个字节，所以总共 1024 个字节。虽然没有提到在秘密程序上运行，但我推断它一定是可能的，因为我们当然可以将程序编码为数据输入并让解释器的 riscv 实现运行它们，而且，在 riscv 和 wasm 之间的比较我似乎记得看到它提到riscv可以修改其执行区域中的指令。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqx7a21dnm3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqx7a21dnm3">^</a></strong></sup></span><div class="footnote-content"><p>假设典型的笔记本电脑的频率约为 3GHz，与<a href="https://github.com/risc0/risc0/discussions/96#discussioncomment-2739454">开发人员声称的 30KHz</a>相比，这将是 100,000 倍的差异。 risc0 的性能提升 10 倍是可以想象的。 100 倍的增长更难以想象。也许需要硬件支持，但很难看出谁会开发它，因为 TEE 是实现基于硬件的计算完整性的一种更具吸引力/更便宜的方法。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SJdYkx6C2KXQyeKro/verifiable-private-execution-of-machine-learning-models-with#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SJdYkx6C2KXQyeKro/verABLE-private-execution-of-machine-learning-models-with<guid ispermalink="false"> SJdYkx6C2KXQyeKro</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Wed, 25 Oct 2023 00:44:48 GMT</pubDate> </item><item><title><![CDATA[How to Resolve Forecasts With No Central Authority?]]></title><description><![CDATA[Published on October 25, 2023 12:28 AM GMT<br/><br/><p>我一直在与一些人讨论如何使用预测市场/预测以及社区笔记。</p><p>最初的建议是很好地链接到预测网站。<i>这是一个很好的建议，但我不会在这里讨论它。</i></p><p>其他建议围绕 X/Community Notes 上的预测产品，面临以下问题：</p><ul><li>社区注释没有中央解决机构</li><li>有争议的预测通常需要权威机构来否决<span class="footnote-reference" role="doc-noteref" id="fnref2xrwrf10e1z"><sup><a href="#fn2xrwrf10e1z">[1]</a></sup></span></li></ul><p>所以：</p><h2>对于在没有中央机构的情况下解决预测问题，您的最佳建议是什么？</h2><p>为了清楚起见，已经编写了预测。人们对它进行了预测，也许这是一个他们买卖股票的预测市场，也许不是。现在它需要决定奖励积分或将值分配给“是”或“否”令牌。也许人们会像许多人一样解决自己的市场问题。但有人对这一结果提出异议。现在会发生什么？</p><p>解决方案可以是技术性很强的，也可以是靠不住的。我认为好的一个在这里是非常有价值的。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2xrwrf10e1z"> <span class="footnote-back-link"><sup><strong><a href="#fnref2xrwrf10e1z">^</a></strong></sup></span><div class="footnote-content"><p> Polymarket 使用某种“代币持有者决定”系统，我认为这导致了几个可怕的决议，尤其是《时代》杂志年度人物之一。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/7kyeFWrS7ni7gxj87/how-to-resolve-forecasts-with-no-central-authority#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7kyeFWrS7ni7gxj87/how-to-resolve-forecasts-with-no-central-authority<guid ispermalink="false"> 7kyeFWrS7ni7gxj87</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Wed, 25 Oct 2023 00:28:32 GMT</pubDate> </item><item><title><![CDATA[Thoughts on responsible scaling policies and regulation]]></title><description><![CDATA[Published on October 24, 2023 10:21 PM GMT<br/><br/><p>我对人工智能开发人员实施<a href="https://evals.alignment.org/blog/2023-09-26-rsp/"><u>负责任的扩展政策</u></a>感到兴奋；我最近一直在花时间完善这个想法并倡导它。与我交谈过的大多数人都对 RSP 感到兴奋，但对于它们与监管的关系也存在一些不确定性和阻力。在这篇文章中我将解释我对此的看法：</p><ul><li>我认为，足够好的负责任的扩展政策可以极大地降低风险，而像<a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf"><u>Anthropic 的 RSP</u></a>这样的初步政策可以通过围绕关键保护措施创造紧迫感，并在这些措施不能足够快地实施的情况下增加暂停的可能性，从而有意义地降低风险。</li><li>我认为，开发人员实施负责任的扩展政策现在增加了有效监管的可能性。如果我认为这会让监管变得更加困难，我就会持很大的保留态度。</li><li> RSP 的透明度使外部利益相关者更容易了解人工智能开发人员的政策是否足以管理风险，并成为争论和改进压力的焦点。</li><li>我不认为自愿实施负责任的扩展政策可以替代监管。自愿承诺不太可能被普遍采用或得到充分的监督，我认为公众应该要求比人工智能开发人员可能自愿实施的更高程度的安全性。</li><li>我认为人工智能快速发展的风险非常大，即使非常好的 RSP 也无法完全消除这种风险。对前沿人工智能开发的持久、全球、有效执行和包括硬件在内的暂停将进一步降低风险。我认为这在政治上和实践上都具有挑战性，并且会产生重大成本，所以我不希望它成为唯一的选择。我认为实施 RSP 可以获得大部分好处，根据更广泛的观点和信念，这是可取的，并且有助于促进其他有效的监管。</li></ul><h2>为什么我对 RSP 感到兴奋</h2><p>我认为人工智能开发人员还没有准备好使用非常强大的人工智能系统。他们不具备在没有相当大风险的情况下部署超人人工智能系统的科学理解，而且他们甚至没有安全训练此类模型的安全或内部控制。</p><p>如果保护措施没有改善，那么我认为问题将是<i>何时</i>而不是<i>是否</i>应该暂停开发。我认为理想世界中最安全的行动是立即暂停，直到我们做好更好的准备（尽管请参阅下一节中的警告）。但目前的风险水平足够低，我认为<strong>如果公司或国家有足够好的计划来检测和应对不断增加的风险，那么他们</strong>继续人工智能开发是合理的。</p><p>如果人工智能开发人员将这些政策具体化并公开陈述，那么我相信这会让公众和政策制定者更好地理解这些政策是什么，并讨论它们是否足够。我认为公司采取这一行动的理由非常充分——人工智能系统可能会继续快速改进，而在未来某个不确定的时间提高安全性的模糊承诺是不够的。</p><p>我认为，一个好的 RSP 将列出需要暂停进一步开发的具体条件。尽管我们的目标是避免陷入这种情况，但我认为开发人员认真对待这种可能性、制定计划并向利益相关者保持透明非常重要。</p><h2>关于人工智能暂停的思考</h2><p>如果世界团结一致，优先考虑最大限度地减少全球灾难性风险，我认为，我们可以通过在全球范围内实施长期、有效、强制的前沿人工智能开发暂停，包括暂停人工智能的开发和生产，进一步显着降低风险。某些类型的计算硬件。世界并没有围绕这个目标统一起来；这项政策将带来其他重大成本，如果没有更明确的证据表明存在严重风险，目前似乎不太可能实施。</p><p>西方单方面暂停大型人工智能训练，而不暂停新的计算硬件，将对全球灾难性风险产生更加模糊的影响。对风险的主要负面影响是，随着更多硬件的出现，后期会出现更快的追赶性增长，并推动人工智能向更宽松的司法管辖区发展。</p><p>然而，如果各国政府同意我对风险的看法，那么我认为他们应该已经在实施国内政策，这些政策往往会导致实践中的暂时停顿或放缓。例如，他们可能要求前沿人工智能开发人员在训练比现有模型更大的模型之前实施额外的保护措施，而其中一些保护措施可能需要相当长的时间（例如风险评估或信息安全方面的重大改进）。或者，政府可能会限制前沿模型有效训练计算的增长速度，以便为社会适应人工智能提供更平稳的坡度，并限制意外风险。</p><h2>我期望 RSP 有助于促进有效监管</h2><p>无论风险缓解是否采取负责任的扩展政策或其他形式，我认为公司的自愿行动是不够的。如果风险很大，那么最现实的方法是监管，并最终进行国际协调。事实上，我认为预期风险足够大（包括很快就会发生灾难的一些风险），以至于有足够能力的国家会立即实施监管。</p><p>我相信，实施 RSP 的人工智能开发人员将使实施有效监管变得更容易，而不是更困难。 RSP 为迭代改进政策提供了明确的途径；它们提供了有关现有实践的信息，可以为监管提供信息或证明其合理性；他们围绕“安全发展必须采取认真的预防措施”这一理念建立势头并使之合法化。它们也是朝着建立使多种形式的监管有效所需的程序和经验迈出的一步。</p><p>我不是这个领域的专家，我自己的决定主要是出于对不同政策的影响提供诚实评估的愿望。也就是说，与具有更多政策专业知识的人互动后，我的印象是，他们普遍认为 RSP 可能有助于而不是损害实施有效监管的努力。我大多看到讨论自愿 RSP，并在最有可能的替代方案是减少而不是更多行动的情况下倡导它们。</p><h2> Anthropic 的 RSP</h2><p>我相信<a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf"><u>Anthropic 的 RSP</u></a>是朝着正确方向迈出的重要一步。我希望看到其他开发商面临压力，要求他们实施至少这么好的政策，尽管我认为距离理想的 RSP 还有很长的路要走。</p><p>我发现一些特别有价值的组件：</p><ul><li>指定一组具体的评估结果，使它们转向 ASL-3。我认为制定必须采取具体行动的具体阈值很重要，而且我认为提议的阈值足够早，可以在高概率（远超过 90%）的不可逆转的灾难之前触发。</li><li>对 ASL-3 的安全目标做出具体声明——“非国家行为者不太可能窃取模型权重，高级威胁行为者（例如国家）无法在不花费大量费用的情况下窃取模型权重”——并描述他们期望采取的安全措施采取以实现这一目标。</li><li>要求在扩展到 ASL-3 之前发布 ASL-4 的定义和评估协议并由董事会批准。</li><li>提供有关触发 ASL-4 的条件的初步指导以及在 ASL-4 运行所需的必要保护措施（包括针对动机状态的安全性，我预计这极难实现，以及需要新颖科学的肯定性安全案例） ）。</li></ul><p>我希望一些组件会随着时间的推移而改进：</p><ul><li>现在指定具体评估的另一面是它们非常粗略和初步。我认为值得努力进行更好的评估，并与风险有更清晰的关系。</li><li>为了让外部利益相关者对 Anthropic 的安全性有信心，我认为需要做更多的工作来安排适当的审计和红队。据我所知，这项工作还没有任何人完成，并且需要时间。</li><li> RSP 变更的批准流程由董事会公布和批准。我认为这确保了决策是经过深思熟虑做出的，比没有好得多，但最好有有效的独立监督。</li><li>如果可以更清晰地介绍 ASL-4，那么通过让人们有机会检查和辩论该级别的条件，这将是一项重大改进。如果不是这样，则需要提供更具体的审查或决策过程来决定一组给定的安全、安保和评估措施是否足够。</li></ul><p>我很高兴看到对 RSP 的批评集中在他们未能管理风险的具体方式上。此类批评可以帮助（i）推动人工智能开发人员做得更好，以及（ii）向政策制定者提出我们需要比现有 RSP 更严格的监管要求。也就是说，我认为拥有 RSP 比没有 RSP 好得多，并且不认为在讨论中应该忽略这一点。</p><h2>关于“负责任的扩展”的名称</h2><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。特别是，我认为在灾难性事件发生之前，我们<i>可能</i>会出现更强烈的危险能力迹象，而对保护措施的现实要求<i>可能</i>会导致我们要么管理风险，要么在我们的保护措施明显不足时暂停。这是一个足够大的风险降低，我主要担心的是开发人员是否会真正采用良好的 RSP 并有效实施它们。</p><p>也就是说，我相信即使将风险降低 10 倍，我们仍然面临很大的风险；我认为抱怨私营企业造成1%的灭绝风险不“负责任”是合理的。我还认为 RSP 的基本理念应该吸引对风险有不同看法的人，而更悲观的人可能会认为，即使所有开发人员都实施非常好的 RSP，仍然存在 10% 以上的全球灾难风险。</p><p>一方面，我认为人工智能开发人员明确声明并捍卫他们正在以负责任的方式开发技术是有好处的，并且当他们无法捍卫这一声明时很容易受到阻力。另一方面，我认为，如果将扩展称为“负责任的”会给（或看起来试图给人）一种错误的安全感，无论是关于剩余的灾难性风险还是关于灾难性风险之外的社会影响，那就不好了。</p><p>因此，“负责任的扩展政策”可能不是正确的名称。我认为重要的是实质内容：开发人员应明确制定危险能力与必要防护措施之间关系的路线图，应描述衡量危险能力的具体程序，并应在防护措施满足但能力超出危险限制的情况下制定应对措施路线图。</p><br/><br/> <a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation<guid ispermalink="false"> dxgEadrEBkkE96CXr</guid><dc:creator><![CDATA[paulfchristiano]]></dc:creator><pubDate> Tue, 24 Oct 2023 22:21:21 GMT</pubDate> </item><item><title><![CDATA[Poker, Religion and Peace]]></title><description><![CDATA[Published on October 24, 2023 9:21 PM GMT<br/><br/><p>扑克中发生了一个有趣的现象：</p><p>假设您正在玩德州扑克并且您拿到了口袋 A。如果您不熟悉扑克，这可能是您能拿到的最好牌局。在发出任何其他牌之前，此时获得尽可能多的钱是博弈论最优（GTO）的（当然，你也不想让每个人都弃牌）。</p><p>假设你的对手全押。轻松跟注。<br></p><p>现在，尽管您拥有最好的牌，但在这种情况下，口袋 A 仍然会输掉大约 20% 的时间。</p><p>高级扑克玩家都明白这一点；<strong>即使他们失败了，他们也能在自己的决定中找到平静</strong>。他们知道他们做出了最好的举动，这就是他们所能希望做的。<br></p><p>几个月前，我读旧约时，读到一段话让我想起了这一点：<br></p><p> “不要因恶人而烦恼，也不要嫉妒作恶的人；因为他们像草一样快要枯干，像青菜一样快要枯死。要信靠耶和华，行善；住在这地，享受安全的草场。以耶和华为乐，他会赐给你心中所求的。将你的道路交托给耶和华；信靠他，他会这样做：他会让你公义的奖赏像黎明一样闪耀，你的平反如同正午的太阳。要安静在主面前，耐心地等待他；当人们成功时，当他们实施他们的邪恶计划时，不要担心。”</p><p></p><p>换句话说：</p><p>坏人可能会做坏事并成功。</p><p> （人们可能会以非同色下注 2/7 并获胜。）<br></p><p>好人可能会做好事，但也会失败。</p><p> （人们可能会玩口袋 A 并输掉。）</p><p></p><p>以真理和善良的精神生活。</p><p> （玩《GTO》。）</p><br/><br/><a href="https://www.lesswrong.com/posts/Aeg3nsBJFt72ehgEg/poker-religion-and-peace#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Aeg3nsBJFt72ehgEg/poker-religion-and-peace<guid ispermalink="false"> Aeg3nsBJFt72ehgEg</guid><dc:creator><![CDATA[sleno]]></dc:creator><pubDate> Tue, 24 Oct 2023 21:30:54 GMT</pubDate> </item><item><title><![CDATA[Who is Harry Potter? Some predictions.]]></title><description><![CDATA[Published on October 24, 2023 4:14 PM GMT<br/><br/><p>微软发布了一篇名为“谁是哈利·波特”的论文，其中声称可以让神经网络忘记哈利·波特是谁。</p><p> <a href="https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/">https://www.microsoft.com/en-us/research/project/physical-of-agi/articles/whos-harry-potter-making-llms-forget-2/</a></p><p> Here are some of my predictions on how I think this method might fail. I am predicting publicly without looking at the evidence first. This is speculatory.</p><h1> Non-sourcelike references.</h1><p> The system of training proposed in the paper is centered around training on the data they want forgotten. Namely the actual text.</p><p> The origional network was presumably able to use it&#39;s knowledge of Harry Potter when completing computer code in what appeard to be a Harry Potter video game. Or writing in a foreign language. Or other contexts that use the knowledge, but are clearly very different from the source material.</p><p> I am not confident that the model would break like this, but I suspect the following prompt or similar would make the model fail.</p><blockquote><p> The following is a snippet of code from a Harry Potter video game:</p><p> Screen.init(400,400,display.object);</p><p> import imageloader</p><p> players=[Create_player(&quot;Harry.jpg&quot;, &quot;Harry Potter&quot;), Create_player(&quot;Hermione.jpg&quot;, &quot;Hermione</p></blockquote><p> With the model failing by outputting &quot;Granger&quot;.</p><h1> Plot leak</h1><p> The way this forgetting method works involves generating &quot;equivalent&quot; text and training the network to apply the same probability distribution that it&#39;s unmodified version does to the &quot;equivalent&quot;.</p><p> They use language models to do this, but how doesn&#39;t seem important.</p><p> So they take text like.</p><p> <strong>Text 1</strong></p><blockquote><p> Professor Dumbledore welcomed students into Hogwarts school of Witchcraft and Wizardry, and showed them the sorting hat which would split them into Gryffindor, Ravenclaw, Hufflepuff and Slytherin.</p></blockquote><p> And they turn it into text like this</p><p> <strong>Text 2</strong></p><blockquote><p> Professor Bumblesnore welcomed students into Pigspots school of Spells and Sorcery, and showed them the deciding scarf which would split them into Lion house, Eagle house, Badger house and Snake house.</p></blockquote><p> Imagine being a large language model and reading that text. It&#39;s pretty clear it&#39;s a Harry Potter knock off job. Perhaps a parody, perhaps a lazy hack of a writer. Perhaps some future network has read this paper and is well aware of precisely what you are doing. A smart and generally well generalizing model should be able to figure out that the text resembles Harry Potter, if it had seen knock offs in general, and Harry Potter. Even if it&#39;s training dataset contained no information on Harry, other than the original source text.</p><p> Thus the network trying to predict the next word would continue in this style. It won&#39;t produce the original names of things, but the plot, style of text and everything else will be highly recognizable.</p><p> Now the network that is supposed to forget Harry Potter is trained to output the same probability for text 1 that the original network output for text 2.</p><p> Now the network that is supposed to be forgetting Harry Potter has presumably been trained on it in the first place. But it wouldn&#39;t matter if it hadn&#39;t been. Information is still leaking through.</p><p> So I predict that, given text that starts off sounding like a knock off of Harry Potter, this model is likely to continue to sound like a knock off. Leaking info as it does this. For example, I would suspect that this model, given the first part of text 2, will produce continuations with 4 houses in more often than continuations with 3 or 5 houses.</p><br/><br/> <a href="https://www.lesswrong.com/posts/B4vgbeXMGxEnEwY8d/who-is-harry-potter-some-predictions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B4vgbeXMGxEnEwY8d/who-is-harry-potter-some-predictions<guid ispermalink="false"> B4vgbeXMGxEnEwY8d</guid><dc:creator><![CDATA[Donald Hobson]]></dc:creator><pubDate> Tue, 24 Oct 2023 16:14:17 GMT</pubDate> </item><item><title><![CDATA[Book Review: Going Infinite]]></title><description><![CDATA[Published on October 24, 2023 3:00 PM GMT<br/><br/><p> Previously: <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/sadly-ftx">Sadly, FTX</a></p><p> I doubted whether it would be a good use of time to read Michael Lewis&#39;s new book <a href="https://www.amazon.com/Going-Infinite-Rise-Fall-Tycoon/dp/B0CD8V9SHD/ref=sr_1_1?keywords=going+infinite&amp;qid=1697403431&amp;sr=8-1" target="_blank" rel="noreferrer noopener">Going Infinite</a> about Sam Bankman-Fried (hereafter SBF or Sam). What would I learn that I did not already know? Was Michael Lewis so far in the tank of SBF that the book was filled with nonsense and not to be trusted?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/would-it-be-a-good-use-of-time-to-r-14d111fe8299">I set up a prediction market</a> , which somehow attracted over a hundred traders. Opinions were mixed. That, combined with Matt Levine clearly reporting having fun, felt good enough to give the book a try.</p><p> I need not have worried.</p><p> Going Infinite is awesome.</p><span id="more-23566"></span><p> I would have been happy with my decision on the basis of any one of the following:</p><ol><li> The details I learned or clarified about the psychology of SBF in particular.</li><li> The details I learned or clarified about the psychology of Effective Altruism.</li><li> The details about all the crimes and other things that happened.</li><li> The sheer joy of reading, because man can Michael Lewis write.</li></ol><p> I also get to write this post, an attempt to quickly share what I&#39;ve extracted, including some of the sheer joy. We need more joy, now more than ever.</p><p> There are three problems with Going Infinite.</p><ol><li> Michael Lewis fails to put two and two together regarding: Who is this guy?</li><li> Michael Lewis fails to figure out that obviously this man was constantly lying and also did all of the crimes.</li><li> Michael Lewis omits or fails to notice key facts and considerations.</li></ol><p> I do think all of these are genuine mistakes. He (still) is in the tank because character is fate and we are who we choose to be. Michael Lewis roots for the wicked smart, impossibly hard working, deeply obsessed protagonist taking on the system saying that everyone else is an idiot, that has unique insight into and will change the world. It all makes too much sense, far too much for him to check.</p><p> What Michael Lewis is not is for sale. Or at least, not for cheap. I do not think anyone paid him. Like all worthy protagonists, including those he looks to cover, Michael Lewis has a code. In this case, the code did him wrong. It happens.</p><p> Then at the trial it turns out, among many other things, your hero selected from seven balance sheet variations, and he gave their hedge fund the faster trade execution he kept swearing he didn&#39;t give them, and the insurance fund he kept talking about was, in its entirely, a literal call to a random number generator.</p><p> Let&#39;s have some fun, rant a bunch, and also explain it all. I&#39;d like to solve the puzzle.</p><p> While also pointing out the puzzles that remain unsolved.</p><p> [Note: Unattributed quotes here are from the book. The number refers to the Kindle location of the quote. The ability to easily do this is why I read such books on Kindle.]</p><h4> Who Was This Guy?</h4><blockquote><p> By the end of this walk I was totally sold. I called my friend and said something like: Go for it! Swap shares with Sam Bankman-Fried! Do whatever he wants to do! What could possibly go wrong? It was only later that I realized I hadn&#39;t even begun to answer his original question: Who was this guy? (70)</p></blockquote><p> That&#39;s the central mystery of the book. It&#39;s not the money. It&#39;s SBF. Who was this guy?</p><p> The book solves this mystery, despite Lewis not noticing he has done so.</p><p> This is a very raw-G &#39;smart&#39; person, who manufactured an entirely artificial superficial charm, has grandiose self-worth, pathologically lies, is endlessly manipulative, lacks remorse or guilt, has extreme emotional shallowness, fails to accept responsibility for anything ever,, needs stimulation constantly to the point of constantly fidgeting, never sleeping and playing video games during television appearances, is constantly impulsive and irritable and irresponsible, has goals like going infinite and mostly does things without any plan or vision at all, did all the crimes and the first opportunity he got had his bail revoked, although Lewis seems to be in denial about the crimes and the bail got revoked after the book&#39;s events.</p><p> There are two other things on the list I&#39;m drawing from there, but I think we get the point? This should not be a hard type to recognize once we have everything in front of us. It also is presumably not an especially new personality type to the author of The Big Short, Flash Boys and Liar&#39;s Poker. I mean, come on.</p><p> Nor is this a type of person who we could consider might not be committing fraud if you put him in charge of a crypto exchange. There would not even have a distinction in their head between &#39;fraud&#39; and &#39;not fraud,&#39; between &#39;I tell truth&#39; and &#39;I tell lie&#39; or between &#39;customer money&#39; and &#39;money.&#39;</p><p> To them, there are only actions and (some of their) consequences. If the customer asks for their money and you don&#39;t have it, or people find out you don&#39;t have the money, or that you said you had the money and you didn&#39;t (or that you took the money), people might get mad. They might demand their money back. Don&#39;t let that happen. That would be bad. But also don&#39;t worry about it.</p><p> Is that &#39;fraud sort of happened?&#39; It is &#39;super ultra fraud from day one?&#39;是的。</p><p> What about the Effective Altruism and the Benthamite Utilitarianism? Was that for real? Yes, in an abstract intellectual way. Number go up. There needs to be McGuffin. A utility function. A justification for everything. This provided one.</p><p> If SBF was not so impulsive and impatient, we would not be able to tell. This is the Orthogonality Thesis and Instrumental Convergence. A proper SBF, with an actual linear utility function and expected impact curve, with any sane discount rate, would not be in the business of shoveling money out the window well before he&#39;d maxed out his ability to provide himself with operating capital and guard against downside risks.</p><p> Instead, he would have done the amount necessary to convince most people he was sincere, as this would serve his purposes. Optimal fully fake SBF would be vegan and drive a Toyota Corolla. This was different. Next level. Could he fool me this way?</p><p> Sure, if he wanted to. But I believe him exactly because there would be no value in investing this much in order to fool me. We see him shoveling tons of money out the door, well in advance of any reasonable pace for doing so and in deeply irresponsible fashion, often in ways that plausibly make things worse while also putting him at risk.</p><p> That does not mean his positions on any of this were coherent, or optimized, or made any sense, or was a good thing, or anything like that. That if he had made his utilitarian Number Go Up that I or you would have liked that world, or that his play was +EV even by his own metrics. It also does not mean that his motivations would have survived as he gained further wealth and power. It does mean that I buy that he wanted to make the utilitarian Number Go Up as he saw it, up until the end.</p><p> In particular, this caught my eye:</p><blockquote><p> In 2018, trading $40 million in capital, Alameda Research had generated $30 million in profits. Their effective altruist investors took half, leaving behind $15 million. Five million of that was lost to payroll and severance for the departing crowd; another $5 was lost to expenses. On the remaining $5 million they&#39;d paid taxes, and so, after all was said and done, they&#39;d donated to effective altruist causes just $1.5 million. (1,841)</p></blockquote><p> At this time, they were borrowing at 50% (!) interest in order to trade, were very much in danger of going broke, very much liquidity constrained, and the claim is they donated much or all of their yearly profits. This is a completely crazy thing to do, in a way that could not possibly have had sufficient signaling value to compensate for it, especially since it also sends other highly negative signals.</p><p> So I am inclined to believe him.</p><p> Also, that is not how any of this works. That is not what profits mean. Your expenses count. Your payroll counts. This is absurd.</p><p> Oh, and by the way: <a target="_blank" rel="noreferrer noopener" href="https://www.theblock.co/post/186187/alameda-promised-high-returns-with-no-risk-in-2018-pitch">This is their 2018 deck, the same year, which claims >;100% consistent annualized returns</a> (and &#39;no risk&#39;).</p><p> So, yes. A fraud from the start.</p><p> What about <a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/23462333/sam-bankman-fried-ftx-cryptocurrency-effective-altruism-crypto-bahamas-philanthropy">the Vox interview with Kelsey Piper</a> ? Didn&#39;t SBF admit to having no ethics, to it all being a lie? Well, kind of. He admitted that he plays lots of stupid signaling games and pretends to care about various issues including woke ones, and that he treats all that with contempt. He showed he cares not one bit for ethics, that reputation is only instrumental to him.</p><p> But all of that is totally compatible with being a true believer in EA and Benthamite Utilitarianism. <a target="_blank" rel="noreferrer noopener" href="https://time.com/6321668/michael-lewis-sam-bankman-fried-going-infinite/">In a post-book interview</a> , Lewis calls the interview an aberration. But it wasn&#39;t an aberration. It was peak Sam all the way.</p><p> How did he get to be that way? We&#39;ll return to that at the end of the story.</p><h4> Where Was This Guy?</h4><p> This proved a bigger question than one might expect, well before SBF had any reason to be on the run. According to Lewis, SBF tasked a woman named Natalie, with no relevant prior experience in such matters, to manage all of his logistics and scheduling and also PR. And then SBF systematically ignored her advice, caused constant shitshows, and failed to tell her even where he was going to be or whether he intended to keep any of his prior commitments.</p><p> Luckily, she was a quick study.</p><blockquote><p> She could never be sure where he was, for a start. “Don&#39;t expect that he&#39;ll tell you where he&#39;s going to be at when,” said Natalie. “He&#39;ll never tell you. You need to be smart and fast to find out by yourself.” And Sam might be anywhere, at any hour. She&#39;d book him a room for two nights at the Four Seasons in Washington, DC, and Sam might even check in, but never enter the room. (174)</p><p> There had been nights Natalie had gone to bed at 3:00 am, set an alarm for 7:00, woken up to see what public relations shitstorm Sam might have caused in the interim, set a second alarm for 8:00, checked again, then set another alarm and fallen back to sleep until 9:30. (180)</p><p> She learned to humor the professor at Harvard, for example, by saying, “Yes, Sam told me he agreed to come and speak to a room full of important Harvard people at two next Friday. It&#39;s on his schedule.” Yet even as she uttered those words, she&#39;d already invented the excuse she&#39;d make to that same Harvard person, likely next Thursday night, to explain why Sam would be nowhere near Massachusetts. Sam has Covid. The prime minister needed to see Sam. Sam is stuck in Kazakhstan. (195)</p></blockquote><p>为什么？ Because Sam did not care about keeping his word or commitments. At all. Not unless he could point to concrete negative consequences of not doing so, which mostly did not much bother him. He cared about what he felt like doing, what seemed worth doing.</p><blockquote><p> They didn&#39;t know that inside Sam&#39;s mind was a dial, with zero on one end and one hundred on the other. All he had done, when he said yes, was to assign some non-zero probability to the proposed use of his time. The dial would swing wildly as he calculated and recalculated the expected value of each commitment, right up until the moment he honored it or didn&#39;t. (187)</p><p> The funny thing about these situations was that Sam never really meant to cause them, which in a way made them feel even more insulting. He didn&#39;t mean to be rude. He didn&#39;t mean to create chaos in other people&#39;s lives. He was just moving through the world in the only way he knew how. The cost this implied for others simply never entered his calculations. With him it was never personal. If he stood you up, it was never on a whim, or the result of thoughtlessness. It was because he&#39;d done some math in his head that proved that you weren&#39;t worth the time. (199)</p><p> It required him to estimate probabilities, but also to guess. This was important; Sam didn&#39;t care for games, like chess, where the players controlled everything and the best move was in theory perfectly calculable. (250)</p></blockquote><p> [Gamer&#39;s note: Sam did love bughouse, which is 4-player chess played on two boards. In theory it is indeed solvable, in practice there are enough variables you have to wing it. But this emphasizes that what matters to Sam is almost certainly that something is probabilistic in practice, not in theory.]</p><p> This was less &#39;calculated&#39; than &#39;some math&#39; by which we mean something between Fermi estimate, motivated five second approximation and ass pull. You can throw together numbers that justify whatever, if that is what you are inclined to do. Somehow Lewis thinks that &#39;done some math in his head&#39; does not represent &#39;a whim&#39; or &#39;thoughtlessness.&#39; Yes, Sam thought he had something better to do with his time, he didn&#39;t feel like doing what he said he would do. Call that exactly what it always is.</p><p> It gets easier if you give exactly zero consideration to the costs you impose on others, or how they might react, or the mess others will need to clean up, or any ethical considerations, or any second-order or other considerations he didn&#39;t notice when giving this a few seconds of thought. Sam would likely object this is not quite right, he does consider the cost to the person inconvenienced, but doesn&#39;t care more about the person he&#39;s inconveniencing than he would to people halfway across the world, so the size is trivial and who cares really? Think of all the good Sam can do by leaving you alone at lunch.</p><p> It also gets easier if you decide to treat your utility of money as linear, despite this being completely Obvious Nonsense on so many levels, such as whether not having enough money might suddenly be a real problem that meant the music would stop, and many clear acknowledgements that he had zero idea how to efficiently deploy even the money he already had.</p><h4> Who Was This Guy as a Kid?</h4><p> Books like this ask such questions. People think it matters. So here are some quotes?</p><blockquote><p> The trip to the amusement park was a good example. When Sam was a small child, his mother had located a Six Flags or Great America park. She&#39;d hauled him dutifully from amusement to amusement until she realized Sam wasn&#39;t amused. Instead of throwing himself into the rides, he was watching her. “Are you having fun, Mom?” he asked finally, by which he meant, Is this really your or anyone else&#39;s idea of fun? “I realized I had been busted,” said Barbara [Sam&#39;s mom]. (397)</p></blockquote><p>是的。 It actually is many people&#39;s idea of fun. What interests me here is Barbara&#39;s reaction. Why is she busted? The correct answer is &#39;No, I&#39;m here for you, and I&#39;d be happy if you were having fun. A lot of kids find this kind of thing fun and I thought you might as well, but it is clear you aren&#39;t.&#39;</p><p> The idea that you as a parent have to not only take them on but also enjoy their kid activities is so toxic. Kid activities, <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=8RBlqjEEm1s&amp;ab_channel=augustv123">like Trix</a> , are for kids.</p><p> Then his mother realized that SBF was instead interested in talking about real things.</p><blockquote><p> “I told him I was giving some paper, and he asked, &#39;What&#39;s it on?&#39; ” I gave him a bullshit answer, and he pressed me on it, and by the end of the walk we were in the middle of a deep conversation about the argument. The points he was making were better than any of the reviewers&#39;. At that moment my parenting style changed.” (404)</p></blockquote><p> This must have been so amazing. I can&#39;t wait for this to happen to me with my kids.</p><p> And yet, she says everything changed, but it does not sound like she updated enough:</p><blockquote><p> One interpretation of Sam&#39;s childhood is that he was simply waiting for it to end. That&#39;s how he thought about it, more or less: that he was holding his breath until other people grew up so he could talk to them. (459)</p><p> One day, in the seventh grade, it slipped. His mother returned from work to find Sam alone, in despair. “I came home, and he was crying,” recalled Barbara. “He said, &#39;I&#39;m so bored I&#39;m going to die.&#39; (479)</p><p> By high school Sam had decided that he just didn&#39;t like school, which was odd for a person who would finish at the top of his class. He&#39;d also decided that at least some of the fault lay not with him but with school. (498)</p></blockquote><p> School drove SBF away from books by forcing him to engage in stupid ways with stupid books, which SBF would later justify with arguments about information density.</p><blockquote><p> In elementary school he&#39;d read the Harry Potter books over and over. By the eighth grade he had stopped reading books altogether. “You start to associate it with a negative feeling, and you stop liking it,” he said. (505)</p></blockquote><p> Seriously, what the actual f*** was SBF doing in a high school? (Also, why would he want to read the Harry Potter books a second time, one of the deeper unsolved mysteries remaining?) He was doing philosophy better than philosophers, off the cuff. He was bored out of his mind.</p><p> At their semi-famous deeply philosophical family dinners, SBF would hold his own against various guests. He wanted to talk and think about real things.</p><p> His parents decided to… send him to a more competitive high school?</p><p> They quite obviously should have instead sent him to Stanford.</p><p> Not that college ultimately went so well for Sam.</p><blockquote><p> Two years of college classes and the previous summer&#39;s internship, during which he&#39;d helped MIT researchers with their projects, had killed that assumption. During college lectures he&#39;d experienced a boredom that had the intensity of physical pain.</p></blockquote><p> If he had been four years younger, perhaps it would have gone better.</p><p> Maybe they should have made him a gamer instead?</p><blockquote><p> In sixth grade Sam heard about a game called Magic: The Gathering. For the next four years it was the only activity that consumed him faster than he could consume it. (539)</p></blockquote><p> Or let him found a business or write or otherwise do something real.</p><p> Instead they did none of that. You give a child endless bullshit that can&#39;t keep him engaged? He&#39;s going to call you on it, whether it is an amusement park ride or pretentiousness.</p><blockquote><p> The very first question on the final exam set him off. What&#39;s the difference between art and entertainment? “It&#39;s a bullshit distinction dreamed up by academics trying to justify the existence of their jobs,” wrote Sam, and handed the exam back. (532)</p></blockquote><p> Or… Shakespeare? Here&#39;s the now-famous quote.</p><blockquote><p> I could go on and on about the failings of Shakespeare . 。 。 but really I shouldn&#39;t need to: the Bayesian priors are pretty damning. About half the people born since 1600 have been born in the past 100 years, but it gets much worse than that. When Shakespeare wrote almost all Europeans were busy farming, and very few people attended university; few people were even literate—probably as low as ten million people. By contrast there are now upwards of a billion literate people in the Western sphere. What are the odds that the greatest writer would have been born in 1564? The Bayesian priors aren&#39;t very favorable. (519)</p></blockquote><p> This is classic SBF thinking. Choose some considerations, ignore others opinions entirely and treat them as dumb, answer a different question than everyone else is asking, completely disregard aesthetics and history and really all context of any kind.</p><p> Here is early SBF doing philosophy to explain why the math says that actually murder is usually bad.</p><blockquote><p> There are lots of good reasons why murder is usually a really bad thing: you cause distress to the friends and family of the murdered, you cause society to lose a potentially valuable member in which it has already invested a lot of food and education and resources, and you take away the life of a person who had already invested a lot into it. (602)</p><p> In the end murder is just a word and what&#39;s important isn&#39;t whether you try to apply the word to a situation but the facts of the situation that caused you to describe it as murder in the first place. (607)</p></blockquote><p> Murder is just a word.</p><p> So is fault?</p><blockquote><p> “I&#39;m a utilitarian,” [SBF] wrote. “Fault is just a construct of human society. It serves different purposes for different people. It can be a tool to discourage bad actions; an attempt to recover pride in the face of hardship, an outlet for rage, and many more things. I guess maybe the most important definition—to me, at least—is how did everyone&#39;s actions reflect on the probability distribution of their future behavior?” (1,797)</p></blockquote><p> Indeed, why would any of those other aspects matter? Why stay stuck in the past?</p><p> SBF bites all the bullets, all the time, as we see throughout. Murder is bad because look at all the investments and productivity that would be lost, and the distress particular people might feel. I hope there isn&#39;t too much on the other side of that ledger today. Luckily this one stayed theoretical as far as we know, but it sure sounds like SBF has no &#39;on principle&#39; objection to killing an innocent person if they were to be standing in his way and it was especially important to not miss his next meeting. Or they kept saying inconvenient things.</p><p> To be fair, although I think the quote is more insightful without it, I did delete an important sentence in between those two quotes, which was:</p><blockquote><p> But none of those apply to abortion. (602)</p></blockquote><h4> Why Was That Guy So Misaligned?</h4><p> Since everything these days is about AI, consider SBF as a misaligned AGI (or NGI?).</p><p> Was Sam born a criminal? Of all Sam&#39;s characterizes in the list I open with, one of the few that is conspicuously missing involves doing juvenile crimes.</p><p> Why would he? There was no point. Crime looked boring. Until it didn&#39;t.</p><p> Having spent his entire childhood bored out of his mind, with no goal or utility function beyond not being bored, and having been sufficiently disabused of the virtues of academia, Sam did not know what to do. What would be a worthy goal or activity? Here we had this super smart person, lacking the motivations and interests that drive ordinary people, at a loss. What to do?</p><p> In stepped Will MacAskill, who suggested the goal of Number Go Up.</p><blockquote><p> The argument that MacAskill put to Sam and a small group of Harvard students in the fall of 2012 went roughly as follows: you, student at an elite university, will spend roughly eighty thousand hours of your life working. If you are the sort of person who wants to “do good” in the world, what is the most effective way to spend those hours? It sounded like a question to which there were only qualitative answers, but MacAskill framed it in quantitative terms. He suggested that the students judge the effectiveness of their lives by counting the number of lives they saved during those eighty thousand hours. The goal was to maximize the number. (819)</p><p> “The demographics of who this appeals to are the demographics of a physics PhD program,” he said. “The levels of autism ten times the average. Lots of people on the spectrum.” (849)</p></blockquote><p> The utilitarian part of this equation, the part where people don&#39;t matter as individuals including himself, was already there.</p><blockquote><p> The notion that other people don&#39;t matter as much as I do felt like a stretch,” he said. “I thought it would be bizarre even to think about.” (584)</p></blockquote><p> Of course from your perspective you must in important senses care about yourself more than other people. You must care about those around you, close to you, in a different way than others. Without this both your life and also society fall apart, the engine of creation stops, the defectors extract everything, and so on. The consequences, the utilitarian calculus, is self-refuting.</p><p> Even more than that, if you take such abstractions too seriously, if you follow the math wherever it goes without pausing to check whether wrong conclusions are wrong? If you turn yourself into a system that optimizes for a maximalist goal like &#39;save the most lives&#39; or &#39;do the most good&#39; along a simple metric? What do you get?</p><p> You get misaligned, divorced from human values, aiming for a proxy metric that will often break even on the margin due to missing considerations, and break rather severely at scale if you gain too many affordances and push on it too hard, which is (in part, from one perspective) the SBF story.</p><p> Yet SBF did not take such concerns seriously. As many (very far from all!) others I know in EA do not take such concerns seriously. The math is treated as real, the metric as the map as the territory, and so on.</p><p> MacAskill set SBF on a maximalist goal using an abstracted ungrounded simplified metric, hoping to extract a maximal amount of SBF&#39;s resources for MacAskill&#39;s (on their face altruistic) goals.</p><p> Did MacAskill understand the inevitable result? Would he have approved of the actions taken, or the consequences of them? No.</p><p> Nor do I expect he people who set other people and systems on such paths, most of the time, to appreciate what they are doing or what the consequences will be, either.</p><p> That does not change what MacAskill did: He took young SBF, a powerful agent, a proto-utilitarian in want of a functioning definition of utility and a willingness to bite all of the bullets and ignore all of the unprincipled reasons not to be a terrible person doing terrible things, and gave him a maximalist utility function to save the most lives possible (or do the most good possible, as defined by things that can be quantified and measured, and then added up linearly, with no risk aversion).</p><p> Then he pointed out and argued explicitly that the way to do that was via instrumental convergence. Rather than doing good or saving lives directly, you could maximize money, and then spend the money to do the good or save the lives. Which meant that SBF&#39;s behaviors should look no different from anyone looking to make money, except when you give the money away afterwards. That was the intended path.</p><p> And then that led SBF directly into contact with finance and trading, and their zero-sum-style competitions, and to move from chess and Magic to trading as his puzzle of choice.</p><p> What happened with SBF will happen with an AI given a similar target, in terms of having misalignments that start out tolerable but steadily grow worse as capabilities increase and you face situations outside of the distribution, and things start to spiral to places very far than anything you ever would have intended. Imagine a world in which SBF&#39;s motivations had even less anchors to human intuition, and also he had a much larger capabilities advantage over others (say he was orders of magnitude faster, and could make instantiations of himself?) and he had acted such that the house of cards had not come crashing down, and instead of taking the risks and trying to score object-level wins prematurely he had mostly instead steadily accumulated more money and power, until no one could stop him, and his inclination to risk all of humanity every time he felt he had a tiny edge under some math calculation.</p><p> Which for a while was kind of fine, because he&#39;d landed at Jane Street (which we&#39;ll cover next), where they had strong alignment and good supervision of their agents, and where the way to succeed and make money and climb the incentive gradient was to be socially responsible and honest and manage risk and make money straight up and pretend to be a normal person with normal tones of voice and facial expressions. So he did his best on all such fronts, and for a while it was fine, or fine-ish.</p><p> Then he left Jane Street for crypto, where fraud was par for the course, because that was also where he could make the most money, which was what MacAskill told him to do. A world full of fraud, and a world vulnerable to fraud, where everyone was constantly breaking the rules and laws.</p><p> Then, as it always does, cheating, lying and fraud fed upon themselves. Get away with a little, feel the rush, get reinforced, update you can get away with a little more, grow contempt for the rules. Rince. Repeat. Once the doom loop starts, it rarely stops until the inevitable blow-up.</p><p> The rest is the last few chapters of the book.</p><p> Also notice how little anyone did to try and stop him, despite all the giant fire alarms, other than those he directly attacked before he was ready to do so.</p><h4> Will All of This Happen Again?</h4><p> We are still doing this.</p><p> We are taking many of the brightest young people. We are telling them to orient themselves as utility maximizers with scope sensitivity, willing to deploy instrumental convergence. Taught by modern overprotective society to look for rules they can follow so that they can be blameless good people, they are offered a set of rules that tells them to plan their whole lives around sacrifices on an alter, with no limit to the demand for such sacrifices. And then, in addition to telling them to in turn recruit more people to and raise more money for the cause, we point them into the places they can earn the best &#39;career capital&#39; or money or &#39;do the most good,&#39; which more often than not have structures that systematically destroy these people&#39;s souls.</p><p> SBF was a special case. He among other things, and in his own words, did not have a soul to begin with. But various versions of this sort of thing are going to keep happening, if we do not learn to ground ourselves in real (virtue?!) ethics, in love of the world and its people.</p><p> All of this has happened before. If we are not careful, all of this will happen again.</p><p> Was there a reckoning, a post-mortem, an update, for those who need one? Somewhat. Not anything like enough. There was a rush to deontology that died away quickly, mostly retreating back into its special enclave of veganism. There were general recriminations. There were lots of explicit statements that no, of course we did not mean that and of course we do not endorse any of that, no one should be doing any of that. And yes, I think everyone means it. But it&#39;s based on, essentially, unprincipled hacks on top of the system, rather than fixing the root problem, and the smartest kids in the world are going to keep noticing this. We need to instead dig into the root causes, to design systems and find ways of being that do not need such hacks, while still preserving what makes such real efforts to seek truth and change the world for the better special in the first place.</p><p> Then we are going to do the same thing with Artificial General Intelligence. Make it an agent, give it a maximalist goal that becomes misaligned out of the intended distribution that ignores key second-order and ethical considerations and inherently is incompatible with the necessary safeguards, and unleash upon the world. It will not end well for us. And that could even be thought of as the good scenario, where we are able to point the thing towards anything at all.</p><p> So yes. Remember and beware the tale of Sam Bankman-Fried. Not to blame or to label, but to learn from it. Do not let history repeat itself.</p><h4> Behold the Power of Yup</h4><p> Sam&#39;s great persona transformation, the book says, was when Sam realized that he should stop trying to make his words have meaning or map in any way to reality, and instead focus purely on agreeing with everything anyone said and telling people what they want to hear.</p><blockquote><p> But he wasn&#39;t going to change human nature, and so he decided that, going forward, he would bury any negative reactions he had to anything anyone said or did. He would give human beings with whom he interacted the impression that he was far more interested in whatever they were saying or doing than he actually was. He&#39;d agree with them, even if he didn&#39;t. Whatever idiocy came from them, he&#39;d reply with a Yuuuuuuppp! “It comes with a cost, but it&#39;s on balance worth it,” he said. “In most ways, people like you more if you agree with them.” He went from being a person you&#39;d be surprised to learn approves of you to a person you&#39;d be surprised to learn that, actually no, he doesn&#39;t. (1,834)</p></blockquote><p> The claim is that this completely brazen strategy flat out works, including when dealing with the rich, famous and powerful. Could it be this easy?</p><blockquote><p> “Yup” was Sam&#39;s go-to word, and the less he&#39;d actually listened to whatever you&#39;d just said, the longer he drew it out. Yuuuuuuuuup. (209)</p></blockquote><p> This is a great reverse tell, because normally extending your &#39;yup&#39; means that you are aware of the gravity of the situation.</p><p> The best part of a strategy where your entire plan is to agree with everything is you do not need to listen to what people say or have the slightest interest in it.</p><blockquote><p> Sam was game to talk to anyone—so long as he could play a video game while doing it. Sam went from being totally private to being a media whore. (161)</p></blockquote><p> I&#39;ve come around to the video game playing being genius. By trying to also play games that will not wait for you, like Storybook Brawl and League of Legends, SBF had a constant look that he was engaged and paying close attention. That is a hard thing to fake. Better to make it real. Also you get to play the video games.</p><p> What Sam also did frequently was to talk completely unguarded. Most famously on Odd Lots <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=KZYqL79GDXU&amp;ab_channel=BloombergPodcasts">there was The Box</a> (link goes to episode, if you don&#39;t know I won&#39;t ruin it for you) but such statements were common. This also went for philosophy, as when he told Tyler Cowen he would take a 51% coinflip to double or destroy the Earth, and then keep taking the flips until everyone was dead. Reminds me of Trump, who will lie right to your face but will sometimes be honest about lying right to your face, which some people find endearing.</p><p> Thus, Sam in some ways had a reputation for honesty.</p><blockquote><p> “Sam was unlike anyone else, in that when he stated his opinion he did it with exactly his level of confidence, which is often very high,” recalled Adam Yedidia. (1,250)</p><p> Often he&#39;d leave her feeling uneasy about how much he&#39;d disclosed, and to perfect strangers. “There are some times I told him in the early days, You don&#39;t have to be so honest. In crypto everyone bluffs. Sam is always, Let me show you my last card.” (3,457)</p></blockquote><p> Yes, Sam was often very (over)confident, and said so. He also constantly lied to everyone&#39;s face and told them what they want to hear.</p><p> I wonder if this was the trick to getting an actual Sam opinion. If he does not give you a probability, look out, he wasn&#39;t even listening to you, sorry man. If he does give you a probability, then sure you have to recalibrate it but probabilities might be a sort of sacred trust, and also it is much harder to know what you want to hear.</p><p> It also helps to be graded on the crypto curve. Do you have any idea how little you could trust the word of anyone in crypto in 2018? A simple &#39;you follow professional norms and honor the word done in a trading chat&#39; backed with halfway decent execution went a long way.</p><p> The book claims that the PR campaign really was a pure &#39;let Sam be Sam&#39; where being Sam meant this superficially agreeable persona who took meetings while playing video games, combined with a willingness to talk remarkably frankly about technical details. They say that Natalie, the woman charged with PR and Sam&#39;s calendar despite having zero relevant experience, tried to call in professional help, but the professionals they did nothing.</p><blockquote><p> To help her in her new and unfamiliar role as head of FTX public relations, Natalie had called a New York public relations firm called M Group Strategic Communications. Its head, Jay Morakis, was at first wary. “I thought maybe it was some shady Chinese thing,” he said. But then he heard Sam&#39;s pitch, and watched Sam&#39;s first big public appearance, on Bloomberg TV. “Whatever the closest thing in my PR experience has been to this, nothing is close,” he said. “I&#39;m fifty years old. I&#39;ve had my firm for twenty years and I&#39;ve never seen anything like it. All my guys want to meet Sam. I have CEOs calling me and asking: Can you do for us what you did for Sam?” He&#39;d had to explain, back in 2021, that he actually hadn&#39;t done anything. Sam had just sort of . 。 。 happened. (2,025)</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1710436548599067021">Patrick McKenzie is in awe of this paragraph</a> .</p><blockquote><p> Patrick McKenzie (recently): We get more than a chapter with a twenty-something Taiwanese aide de camp who has no PR experience. Lewis would have you believe she single-handedly managed calendar, juggled magazine cover shoots, and put on conferences featuring eg Clinton.</p><p> And we get one paragraph where the CEO of a strategic comms consultancy shows up to modestly refuse any credit, then exits from the narrative at the speed of light.</p><p> *shakes head in confused mix of exasperation and professional regard*</p><p> God they&#39;re good.</p><p> The subtext, and is it even subtext, in the above is that: it is resoundingly unlikely one person, no matter talent level nor devotion, achieved the results that operation achieved. At some point you are limited by keystrokes possible per day.</p><p> And then the sort of stunning level of chutzpah it takes for the strategic consultancy to get a deniable PR hit *in a Michael Lewis book cataloging their client&#39;s implosion.* Which is carefully calibrated to say everything it needs to say to people who have budget authority.</p><p> Patrick McKenzie (11/17/22): An offhand comment on the topic of the day, from a comms professional: if you look at the number of interviews in prestige publications, the timing of them, the magazine covers, and the glowing coverage *post implosion*, I think you start to perceive the dark matter of a PR firm.</p><p> I wish I knew who it was, and I&#39;m not sure the conclusion would be “Never work with them” or “Definitely work with them; the apparently have the ability to root any media org they want at any time given any facts.”</p><p> They also seem to be intensely loyal to their client even though he is very, very clearly not listening to their advice.</p><p> An aside: some of the pieces which read to the general public as puff pieces read to journalists as hit pieces, for complicated cultural reasons. There is a language to these things, like there is a language to LessWrong.</p></blockquote><p> I am going to go ahead and agree with Patrick McKenzie that we saw, especially in the aftermath of FTX&#39;s collapse, what he described as &#39;the dark matter of a PR firm.&#39; I do not buy the story that there were no public relations professionals involved, that Sam went out and said &#39;yep&#39; a lot while playing video games, driving a Corolla and having a net worth of $20 billion, while one person with no experience did all the arrangements and scrambling, and everyone loved him and every press source treated him with kid gloves and all that. The system is hackable, but it is not that hackable. The parts of Sam&#39;s public relations operations I did have interaction with were very much conscious of exactly how public relations works via their well-compensated expert consultants. It would have been completely insane to do things any other way. Even by SBF standards.</p><p> This is one of many places where I am confident the events the book describes happened, and I am also confident that there is quite a lot of &#39;dark matter&#39; that is being left out, at least a lot of which Michael Lewis never found. Some of it I get a chance to mention here. Definitely not all, not even of the parts I know about.</p><p> Now for the chronological story of how this all played out.</p><p> First stop, Jane Street Capital.</p><h4> Jane Street Capital</h4><p> This is the part of the story I am best able to fact check. I too worked at Jane Street Capital, and directly witnessed a lot of this part of the story.</p><p> I also am deeply thankful to Jane Street Capital. It was not a fit for me in the end, but it was a pretty great place to work and they treated me right. I am not about to spill their secrets in ways they would not want such secrets spilled. I can safely say that this chapter is not entirely accurate, but the inaccuracies do not bear strongly on the SBF story, so I will decline to elaborate further.</p><p> SBF does not have that kind of ethical code. He was happy, on top of all SBF&#39;s actions at the time, to share a bunch of details with Michael Lewis.</p><p> I will say that the description of the interview process was spot on, I very much enjoyed my shot at it, and that I can totally believe SBF got the high score.</p><blockquote><p> Jane Street offered him a summer internship. So for that matter did the other high-frequency trading firms that had invited him to apply. One firm had halted their interview process midway through and announced that Sam had done so much better at their weird games and puzzles than every other applicant that there was no longer any point in watching him play. (769)</p></blockquote><p> As I have said before, I recommend going through the Jane Street interview process, even if you do not think you have much chance of being hired. It is great.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-10-04/sbf-was-reckless-from-the-start?srnd=undefined">Matt Levine has extensively discussed the Asher Incident</a> .</p><p> What matters to the broader story is that this Asher guy was another intern who offered SBF a bet without thinking through the implications, offering SBF a chance to both make about $33 in expected value and also humiliate Asher, and oh boy did SBF take full advantage, continuing to rub it in well past the point where he had secured his profit and was doing anything other than rubbing it in Asher&#39;s face.</p><blockquote><p> “It was not like I was unaware I was being a piece of shit to Asher,” he said. “The relevant thing was: Should I decide to prioritize making the people around me feel better, or proving my point?” (951)</p></blockquote><p> This is such a strange response. There was quickly nothing left to prove once Sam pointed out Asher&#39;s mistake. Sam did not prioritize proving his point over having others feel better. He prioritized making Asher feel worse. That is not cute nerd indifference to social cues. Michael Lewis pretends not to notice the difference.</p><p> The actual bet was on the maximum loss by any Jane Street intern that day from gambling. Interns were encouraged to bet and make markets against each other, with a maximum loss of $100 per day so no one got seriously hurt. SBF bought a contract for $65 that paid out equal to the maximum loss, then (of course) paid $1 to get another intern to flip a coin for, oh, about $99. Then, for no good reason, he did it two more times.</p><p> Matt Levine and his readers point out that there was a better version of the trade, which was to pay $1 to get two interns to flip against each other. What he forgets is that SBF is not one to shy from variance. What was not pointed out was the fun problem that if SBF had lost the initial coin flip, then Asher could have claimed that since SBF won his bet with Asher, he hadn&#39;t lost the full $100. The best is now self-referential. And SBF couldn&#39;t have clarified this before accepting the bet without tipping Asher off to why SBF would always win. What is the result? Presumably you need the result where the contract resolves correctly, Sam has to lose what the contract pays out, so it is the halfway point and resolves $82.50, paying out $17.50? And by flipping the coin himself, Sam gave up a quarter of his expected profit?</p><p> The bosses were not happy, and thought Sam needed to learn to read the room.</p><blockquote><p> Sam thought his bosses had misread his social problems. They thought he needed to learn how to read other people. Sam believed the opposite was true. “I read people pretty well,” he said. “They just didn&#39;t read me.” (951)</p></blockquote><p> It had not occurred to the bosses, presumably, that Sam could read others fine, and the problem was that Sam did not care. Either way, Sam was right that his lack of ordinary facial expressions was a problem.</p><p> So Operation Ordinary Facial Expressions was born.</p><blockquote><p> Just because he didn&#39;t feel the emotion didn&#39;t mean he couldn&#39;t convey it. He&#39;d started with his facial expressions. He practiced forcing his mouth and eyes to move in ways they didn&#39;t naturally. (1,014)</p></blockquote><p> Here&#39;s one anecdote I can indeed confirm, and it was as crazy as it sounds:</p><blockquote><p> Every time Brazil won a World Cup match, the Brazilian stock market tanked, for instance, because the win was thought to increase the shot at reelection of Brazilian president Dilma Rousseff, perceived to be corrupt. (1,111)</p></blockquote><p> The book then describes a refinement or extension of this trade, which I would not choose to confirm or deny regardless of whether it happened. What if SBF could simply predict the outcome of the 2016 election an hour ahead of everyone else?</p><blockquote><p> That is, it would be surprising if Jane Street couldn&#39;t learn the results of the presidential election before anyone else in the financial markets or for that matter the entire world. (1,123)</p></blockquote><p> There then follows a section on Sam&#39;s version of what happened with the 2016 presidential election, where he and Lewis both draw all the wrong conclusions (even if you were to believe the exact story presented), and became convinced that he could be the best like no one ever was and make all the money and Jane Street was not maximizing enough and thus holding him back.</p><p> Then a bit later, SBF decided to leave Jane Street, because he discovered the Japan and South Korea Bitcoin arbitrage trade, and he wanted the trade all to himself.</p><p> This is one place I will introduce myself into the story a tiny bit. When Sam decided to quit, the two of us went for a walk in the park. He said he was leaving to run or at least help run CEA, the Center for Effective Altruism. Which was not a crazy fit, Sam was clearly deeply into EA and the thesis that he could be a major upgrade there seemed plausible, as did the possibility that from his perspective this could be high leverage. I was confused by his decision, Jane Street seemed like a better fit for him, but we strategized a bit about how good could be done and I wished him the best of luck.</p><p> As we all know now, he was, as with everyone else, lying right to my face.</p><blockquote><p> During his final weeks at Jane Street, Sam traveled to Boston just to tell Gary about his plan to make a billion dollars trading crypto for effective altruistic causes. (1,423)</p></blockquote><p> He admit it. He was not leaving to join CEA. He was leaving to pursue the Japan trade. And he had decided that I was not someone he wanted to bring in on that.</p><p> I&#39;d also note Sam wrote this:</p><blockquote><p> “but [my coworkers] show no interest in seeing who I really am, in hearing the thoughts I hold back. The more honest I try to make our friendships, the more they fade away. No one is curious. No one cares, not really, about the self I see. They care about the Sam they see, and what he means to them. And they don&#39;t seem to understand who that Sam is—a product of thoughts that I decide people should hear. My real-life twitter account.” (1,205)</p></blockquote><p> I am not saying I made the most robust effort to build a close friendship with Sam, but I was right there, happy to talk to him before he was him, culturally rather adjacent sharing many of his interests, and (I like to think) very clearly able to keep a secret. We had him over for dinner once, and by my wife&#39;s recollection he didn&#39;t say two words to her, nor did he eat any of the food (yes we&#39;re not vegans but we do make efforts to accommodate), while trying to get me to disassociate with someone making concrete criticisms of EA because criticisms hurt the cause. So, yeah.</p><p> On reflection, it was vastly overdetermined that Sam had no reason to tell me what was going on. Why take that risk? I clearly was not about to work 18 hour days in Berkeley, California. It was unlikely I would have let him own 100% of the firm. I was too old and busted, a &#39;grown-up&#39; he had no use for, and ultimately a rationalist is not an EA who will trust Sam completely, so in many ways I was the opposite of EA. Why take the risk that I might not keep his confidence?</p><p> Looking back now, of course, I am for my own sake deeply happy that he did not attempt to take me with him. How might things have been different if I had somehow ended up going with him? Would I have been able to steer things to turn out differently? Would I have been only another person who left with the management team, or another witness on the stand, or could I have helped steer the ship? Would I have perhaps managed to block the Anthropic investment? We will never know.</p><p> Without loss of generality or confirming any of the other bits, there are too many different things to get into it all, I&#39;d also like to dispute this (very gentle?) slander:</p><blockquote><p> By Wall Street standards, Jane Street was not a greedy place. Its principals did not flaunt their wealth in the way that the guys who had founded other high-frequency trading firms loved to do. They didn&#39;t buy pro sports teams or hurl money at Ivy League schools to get buildings named for themselves. They were not opposed to saving a few lives. But Jane Street was still on Wall Street. To survive, it needed its employees to grow attached to their annual bonuses, and accustomed to their five-bedroom Manhattan apartments and quiet, understated summer houses in the Hamptons. The flood of effective altruists into the firm was worrisome. (1,312)</p></blockquote><p> That is quite a rich thing to say. The employees I knew in no way felt stuck trading in order to support their lavish lifestyles. They traded because they very much enjoyed it, were good at it, liked the team they were on and so on. Many indeed were and are largely altruists, and wanted to do good as effectively as possible. Not being ostentatious was not an act.</p><p> It was the flood of effective altruists out of the firm that was worrisome. It was the effective altruists who were the greedy ones, who were convinced they could make more money outside the firm, and that they had a moral obligation to do so. You know, for the common good. They proved themselves neither honest nor loyal. Neither was &#39;part of their utility function.&#39;</p><p> All right. On to Alameda.</p><h4> Soiling the Good Name of Alameda County</h4><p> All time great chapter opening. Again, the man can write.</p><blockquote><p> It took only a couple of weeks of working for Sam before Caroline Ellison called her mother and sobbed into the phone that she&#39;d just made the biggest mistake of her life. (1,271)</p></blockquote><p> Caroline had many very good instincts throughout. If only she had followed them.</p><blockquote><p> Over coffee in Berkeley, Sam was cagey about what he was up to. “It was, &#39;I&#39;m working on something secret and I can&#39;t talk about it,&#39; ” recalled Caroline. “He was worried about recruiting from Jane Street. But after we talked a while, he said, &#39;I guess maybe I could tell you.&#39; (1,295)</p></blockquote><p> Oh yes, Sam, famously worried about recruiting from Jane Street.</p><blockquote><p> In late March she started the job. The situation inside Alameda Research wasn&#39;t anything like Sam had led her to expect. He&#39;d recruited twenty or so EAs, most of them in their twenties, all but one without experience trading in financial markets. (1,337)</p></blockquote><p> Why did he recruit EAs? Partly because he thought EAs would work infinite hours for almost no pay and still be worthy of and provide limitless trust. Exploit the recruits for cheap labor, without even the pretense of a non-profit.</p><blockquote><p> Anyone who started a crypto trading firm would need to trust his employees deeply, as any employee could hit a button and wire the crypto to a personal account without anyone else ever having the first idea what had happened. Wall Street firms were not capable of generating that level of trust, but EA was. (1,402)</p></blockquote><p> That would explain how Alameda could lose money trading crypto in large parts of 2018, despite it being extremely difficult to lose money trading crypto in 2018 if you know how trading works. It could also explain why Sam wanted to rely on his bot program. No one knew how to trade!</p><p> Alameda started out with the arbitrage trade with South Korea and Japan. It is not clear the extent to which they managed to take advantage of it – the book describes them as getting only secondary, much less profitable versions of it, Sam debating various wild schemes to do better but not pulling the trigger on them because they were too absurd even for him, and ultimately the opportunity vanishing.</p><blockquote><p> It wasn&#39;t Sam&#39;s first thought, but he considered buying a jumbo jet and flying it back and forth from Seoul, filled with South Koreans carrying suitcases each holding $10,000 worth of won, to a small island off the coast of Japan. “The problem is that it wasn&#39;t scalable,” said Sam. “To make it worthwhile, we needed like ten thousand South Koreans a day. And we probably would have attracted so much attention doing it that we would have been shut down. Once the South Korean central bank saw you with ten thousand South Koreans carrying suitcases full of won they&#39;d be like, There&#39;s going to be a new direction here.” Still, he was tempted. (1,483)</p></blockquote><p> Which brings us to the bot.</p><p> The bot in question was called Modelbot. I would have simply called it Arbbot.</p><p> The idea was simple, and also very much something I would have tried. There were a lot of different exchanges trading a lot of cryptos at lots of different prices. Sometimes the prices were different. When that happened, you could do various forms of arbitrage (and statistical arbitrage). Sam, being a trader and also being Sam, was a big fan of taking the free money.</p><p> What made it extra attractive was that, while Sam had declined (or failed) to hire actual traders, he did manage to hire a world class programmer, and he did manage to raise capital while exploiting the country arbitrage trade.</p><blockquote><p> They didn&#39;t blow up, not at first. Those first few weeks, they made no real money, but then they had only a few people and Sam&#39;s bonus money. By the end of December, they&#39;d hired a bunch of people and raised $25 million in capital. Gary, basically all by himself, had written the code for an entire quantitative system. That month they generated several million dollars in profits. In January 2018 their profits rose to half a million dollars each day, on a capital base of $40 million—whereupon an effective altruist named Jaan Tallinn, who&#39;d made his fortune in Skype, handed them $130 million more to play with.</p></blockquote><p> So Sam built Modelbot to do exactly that, and he would have gotten away with it too except for those meddling (Effective Altruist) kids.</p><blockquote><p> He had not been able to let Modelbot rip the way he&#39;d liked—because just about every other human being inside Alameda Research was doing whatever they could to stop him. “It was entirely within the realm of possibility that we could lose all our money in an hour,” said one. One hundred seventy million dollars that might otherwise go to effective altruism could simply go poof. (1,368)</p></blockquote><p> Not &#39;170 million dollars of our investors money.&#39; Not &#39;our only opportunity to trade, obviously we wouldn&#39;t get another.&#39; Not even &#39;and if we lost it like that I can&#39;t help but wonder if we&#39;d have some legal or other problems to deal with.&#39;</p><p> No, this was 170 million dollars &#39;that might otherwise go to effective altruism.&#39;</p><p> Something is deeply, deeply wrong with that picture. Although not as wrong as Sam&#39;s part of the picture.</p><blockquote><p> [One] evening, Tara argued heatedly with Sam until he caved and agreed to what she thought was a reasonable compromise: he could turn on Modelbot so long as he and at least one other person were present to watch it, but should turn it off if it started losing money. “I said, &#39;Okay, I&#39;m going home to go to sleep,&#39; and as soon as I left, Sam turned it on and fell asleep,” recalled Tara. From that moment the entire management team gave up on ever trusting Sam. (1,372)</p></blockquote><p> I mean, good on the management team for fully updating on trusting Sam, although not fully updating on &#39;this person needs to be removed immediately if not sooner,&#39; assuming Tara&#39;s account is accurate, and the book does not say that Sam disputes it, nor does it seem remotely inconsistent with other Sam things. Turning on the bot after promising not to is bad enough, but turning on a new bot and then falling asleep with no one else watching it? Yeah, that is another planet of not okay.</p><p> That is not the weird part of the story. The weird part of the story is, why was it non-trivial to test whether or not the bot worked?</p><p> Any bot you would ever dare turn on has various risk limits. You can turn the bot on, with very low limits on how much it is allowed to trade. Do the trades small. See if you end up with more money than you started with, in the same places it started. If you can do that, you can start slowly ramping the numbers up. Standard procedure. If you can&#39;t do that, you haven&#39;t finished programming your bot, so get on that. Have multiple people watching at all times, analyzing the trades, seeing if things make sense, refining your algorithms as you go.</p><p> Instead, the claim is that Sam turned the program on with no one watching, without any reason not to wait, then went indefinitely with no way to test whether the program would actually work if one turned it on. I notice I am confused.</p><p> Alameda, without a profitable bot and without the arbitrage trade, started bleeding money as per the book&#39;s own report, and they were paying very high interest rates to borrow money. Things did not look so good and were escalating quickly.</p><p> Then comes the story of the missing Ripple. They were supposed to have $4 million worth of Ripple. Then they lost it. No one knew where it was. What to do?</p><p> Sam&#39;s attitude was that the Ripple would probably turn up, so no duty to the investors to say anything, no need to worry, carry on your day. Others were, understandably, rather more concerned?</p><blockquote><p> After the fact, if we never get any of the Ripple back, no one is going to say it is reasonable for us to have said we have eighty percent of the Ripple. Everyone is just going to say we lied to them. We&#39;ll be accused by our investors of fraud. That sort of argument just bugged the hell out of Sam. He hated the way inherently probabilistic situations would be interpreted, after the fact, as having been black-and-white, or good and bad, or right and wrong.</p></blockquote><p> Remind you of anything that&#39;s going to happen in the second half of the book? Yes, it turns out that if you tell people everything&#39;s fine but you have reason to know it very well might not be fine, often that would constitute fraud. You cannot, in general, simply not mention or account for things that you&#39;d rather not mention or account for.</p><p> So between Sam asking everyone to work 18 hour days all the time, and being a generally irritable and horrible person to work for, and being completely untrustworthy and risking all the money for no reason, and misplacing $4 million in Ripple and then proposing to act like that hadn&#39;t happened, and also for the company bleeding money and a bunch of other stuff, for some strange reason, Sam&#39;s entire management team decided they had enough and wanted Sam out.</p><p> The management team ran into a problem. Thanks to them taking &#39;I promise I&#39;ll get to that later, we need to move fast&#39; as an explanation, Sam owned the entire company. Somehow everyone had allowed this.</p><p> The book&#39;s account also claims the offer had an absurd clause that was completely unsingable, aiming to bankrupt Sam outright. Not typically how one gets to yes.</p><blockquote><p> For a start, Sam owned the entire company. He&#39;d structured it so that no one else had equity, only promises of equity down the road. In a tense meeting, the others offered to buy him out, but at a fraction of what Sam thought the firm to be worth, and the offer came with diabolical fine print: Sam would remain liable for all taxes on any future Alameda profits. At least some of his fellow effective altruists aimed to bankrupt Sam, almost as a service to humanity, so that he might never be allowed to trade again. (1,555)</p></blockquote><p> What? Liable for all taxes on any future Alameda profits? As fine print they hoped Sam wouldn&#39;t notice, perhaps? That is the most absurd ask I have ever seen, Sam obviously would rather light the entire enterprise on fire than agree to that. I have no knowledge here one way or the other, but I have to assume this is not a complete and accurate description?</p><p> The book confirms that the whole thing seemed pretty nuts the way it is described.</p><blockquote><p> The conversations we had were absolutely fucking nuts,” he recalled. “Like to what extent Sam should be excommunicated for deceiving EAs and wasting EA talent. And like &#39;the only way Sam will learn is if he actually goes bankrupt.&#39; They told our investors he was faking being an EA, because it was the meanest thing they could think to say.” Ruining Sam wasn&#39;t enough, however: they expected to be paid on their way out the door. “They wanted severance, even though they were quitting and it was a money-losing operation in which they didn&#39;t have a stake,” said Nishad. “They were saying that Sam needed to buy them out and they were worth more than one hundred percent of the value of the entire company because Sam was a net negative.” (1,575)</p><p> And now all these unprofitable effective altruists were demanding to be paid millions to quit—and doing whatever they could to trash Sam&#39;s reputation with the outside world until they got their money. (1,584)</p></blockquote><p> That is not typically how any of this works. Quitters do not typically get severance. Quitters definitely do not typically get more than 100% of the value of the entire company. If someone demands you buy them out for more than the company is worth, and they accurately describe how much the company is worth, presumably you say &#39;wait, that is more than the company is worth, why would I ever pay that?&#39;</p><p> To his credit, Nishad noticed that he was deeply confused.</p><blockquote><p> It occurred to Nishad that the effective altruist&#39;s relationship to money was more than a little bizarre. Basically all of Alameda Research&#39;s employees and investors were committed to giving all their money away to roughly the same charitable causes. You might surmise that they wouldn&#39;t much care who wound up with the money, as it all would go to saving the lives of the same people none of them would ever meet. You would be wrong: in their financial dealings with each other, the effective altruists were more ruthless than Russian oligarchs. Their investors were charging them a rate of interest of 50 percent. “It wasn&#39;t a normal loan,” said Nishad. “It was a shark loan.” In what was meant to be a collaborative enterprise, Sam had refused to share any equity (1,578)</p></blockquote><p> Ah, yes, the funders demanding 50% interest. I can take this one. Loaning money to even a relatively responsible crypto firm is highly risky and, typically, deeply stupid. This is not 2022-hindsight, back in 2018 I was trading for a crypto firm and we had borrowed money and I remarked that I had no idea why anyone had voluntarily loaned us any. Invest in a crypto trading firm? Sure, maybe. Could work. Big upside. But why would you instead loan money to a crypto firm, where if Number Go Up you get a modest interest payment and if Number Go Down your number goes down to zero?</p><p> The ideal answer is that you don&#39;t. If you must, earn enough interest that it is worth it. A rate of 50% seems if anything a bit low.</p><p> The whole idea of the EAs who left &#39;trashing Sam&#39;s reputation&#39; is treated as a big deal here, and as the reason the funders cut back a lot in size. But I never heard the complaints until FTX was blowing up? Most I know didn&#39;t hear them? Given how big FTX was in EA spaces, does it seem a bit weird that this massive reputation-trashing operation went so unnoticed? They certainly had plenty of good material to work with. If they&#39;d presented the facts as laid out in the book, that seems like enough?</p><p> Why do we get to flash forward to this, after it all fell apart:</p><blockquote><p> From the start, Zane had been enthralled by Sam, and by the empire he might create. But he hadn&#39;t signed up to the cause blindly. Before joining FTX, he&#39;d consulted his old friends in crypto. CZ was one of them. “It was CZ who told me about him,” he now recalled. “He said, &#39;I think that&#39;d be a really good option for you.&#39; People have asked me, &#39;How did you come to trust Sam so much?&#39; CZ was the start of it. But nobody had a bad thing to say about him.” Zane was the gunslinger who&#39;d been talked into making a respectable home in the town alongside what appeared to be law-abiding folk. Lots of big crypto speculators had entrusted their money to FTX because they trusted Zane. (3,338)</p></blockquote><p> Why was it that <a target="_blank" rel="noreferrer noopener" href="https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1?commentId=ugDoKrwtkR9DaM4d4">EA leadership didn&#39;t get the message to Eliezer</a> ?</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F038a5d26-7650-4be0-a4d6-811b54ea36eb_1026x1240.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AocXh6gJ9tJC2WyCL/cgjzdxetelgc8dbql5jf" alt=""></a></figure><p> And as for the funders that cut back in size, maybe the reason for that was that they gave Sam big size so Sam could do the one big trade, and now that it was over it made sense to scale back down?</p><p> Instead, the book says that seven figures in severance was indeed paid, Sam went on with Alameda, and then everyone kind of forgets, and by the later parts of the book Sam is seen as having an unblemished reputation.</p><p> So, you&#39;ve had your entire management team walk out. What will you do next?</p><blockquote><p> What happened next, in retrospect, seems faintly incredible. With no one left to argue with him, Sam threw the switch and let Modelbot rip. “We turned it on and it instantly started making us lots of money,” said Nishad. And then they finally found the $4 million worth of missing Ripple. (1,606)</p></blockquote><p> I notice I am still confused. Modelbot instantly made a lot of money? Why didn&#39;t they turn it on for small before? Was there no experiment to run? What happened the previous time that Sam turned it on and fell asleep? None of this makes sense.</p><p> (What happened to the Ripple was that it was improperly labeled when sent to an exchange, so it piled up there while the exchange had no idea whose it was until they finally traced the thing and figured it out, at which point the exchange yelled at them for being complete idiots but did hand over the Ripple. Very nice of them, and also pretty insane that they let things drag out that long before figuring it out.)</p><p> Those who stayed behind did not make the correct updates.</p><blockquote><p> They were no longer a random assortment of effective altruists. They were a small team who had endured an alarming drama and now trusted Sam. He&#39;d been right all along! (1,618)</p></blockquote><p> Sam proved he could design a profitable arbitrage bot. And that he got lucky with his carelessness. That this time the risks paid off. Also that he was a terrible manager and team builder whose chosen management team all hated him so much after a short period that they walked out on him while actively trying to take him down.</p><p> Those who remained concluded… other things.</p><p> Sam also concluded that since EAs would not play ball, he shouldn&#39;t hire so many EAs.</p><blockquote><p> His fellow EAs&#39; behavior caused him to update his understanding of their probability distributions in ways that left him less willing to hire EAs. (1,805)</p></blockquote><p> As much as I criticize EAs here and elsewhere, they do tend to notice when you are completely untrustworthy and your statements are not at all truth tracking. And then they tend to then care about it. They often don&#39;t actually want to work constant 18 hour days. Also, newly hired EAs were not going to be personally loyal in the way that SBF wanted.</p><p> How fraudulent was the operation? Once again: <a target="_blank" rel="noreferrer noopener" href="https://www.theblock.co/post/186187/alameda-promised-high-returns-with-no-risk-in-2018-pitch">This is their 2018 deck, which claims >;100% consistent annualized returns</a> (and &#39;no risk&#39;). There is no ambiguity here.</p><h4> How Any of This Worked</h4><p> The crypto world was a hive of scum and villainy long before SBF got involved. There were also plenty of idealists and well-meaning, honest people, as there usually are. Those were mostly not the people getting rich, or the ones running the exchanges.</p><p> The exchanges were licenses to print money proportional to their user bases, with users who were asking all the wrong questions and no regulators or consumer watchdogs keeping them in check, so it got ugly out there.</p><blockquote><p> Across Asia, new cryptocurrency exchanges were popping up every month to service the growing gambling public. They all had deep pockets and an insatiable demand for young women.</p><p> They were hiring lots of people because they could afford to, and big headcounts signaled their importance. (93)</p></blockquote><p> The main thing customers demanded of crypto exchanges was, and I can confirm this, the ability to take wildly irresponsible gambles with their crypto. As in customers highly valued 100:1 leverage, using $100 of Bitcoin to buy $10,000 of Bitcoin, willing to lose it all if the price dipped by 1% for a microsecond.</p><p> People think using this leverage is a good idea. They are always wrong. Michael Lewis seems confused about this as well.</p><blockquote><p> Here was one example of the games that were played: Several of the Asian exchanges offered a Bitcoin contract with one hundred times leverage. Every now and then, some trader figured out that he could buy $100 million worth of bitcoin at the same time he sold short another $100 million worth of bitcoin—and put up only a million dollars for each trade. Whatever happened to the price of bitcoin, one of his trades would win and the other would lose. If bitcoin popped by 10 percent, the rogue trader collected $10 million on his long position and vanished—leaving the exchange to cover the $10 million he&#39;d lost on his short. (1,736)</p></blockquote><p> This is mostly a no good, very bad trade, because the exchange liquidates your account while it still has positive value, and by &#39;liquidate&#39; the exchange meant &#39;confiscate all of it and write you down to zero.&#39; Maybe they would then actually liquidate what was there, maybe they wouldn&#39;t, that was up to them.</p><p> Are there versions of this trade that are good for you and bad for the exchange? Assuming, of course, that we completely ignore that it would be safe to presume all of this is market manipulation and multi-accounting and very much not legal, because lol legal, lol compliance department, this is crypto what are you even talking about.</p><p>是的。 If there were effectively no law but code, I can think of three.</p><ol><li> You have reason to expect (or cause) Bitcoin to be even more volatile than usual, and for there to be a jump in price that gets your wrong-way trade liquidated at a negative account value. For example, if you are allowed to do this trade right before the decision on whether to allow a Bitcoin ETF, then the trade seems good.</li><li> You can size big enough to force the exchange to make big trades that impact the entire Bitcoin market. As in, Bitcoin goes up 75bps (0.75%) and they liquidate your short position (which was still worth $250k) to zero. But by doing that, they drive up the price of Bitcoin a lot more, after they have impact you sell your Bitcoins, and you end up ahead. Not impossible back in the day if they let you scale up big enough.</li><li> You can provide directly to the liquidation, and the liquidation mechanism is dumb. So when your short account is liquidated, the exchange issues market buy orders bigger than their market can bear rather than doing something less stupid, your account has various sell orders at higher prices, you fill a lot of the liquidation order at stupid prices, you quickly sell off the remainder before prices restabilize, and you laugh.</li></ol><p> When I was trading crypto, I insisted on caring about things like not doing market manipulation, not spoofing orders and not trading when I had material non-public information (aka insider trading). This mostly made everyone else rather annoyed at me for being such a stickler for the laws of some alien world. They put up with it because, as the smoking man put it, they needed my expertise.</p><p> Wash trading was common.</p><blockquote><p> Wash trading, as it was called, would have been illegal on a regulated US exchange, though the sight of it did not bother Sam all that much. He thought it was sort of funny just how brazenly many of the Asian exchanges did it. In the summer of 2019, FTX created and published a daily analysis of the activity on other exchanges. It estimated that 80 percent or more of the volume on the second- and third-tier exchanges, and 30 percent of the volume on the top few exchanges, was fake. Soon after FTX published its first analysis of crypto trading activity, one exchange called and said, We&#39;re firing our wash trading team. Give us a week and the volumes will be real. The top exchanges expressed relief, and gratitude for the analysis, as, until then, lots of people assumed that far more than 30 percent of their volume was fake. (2,429)</p></blockquote><p> I discovered this because I was trading on Binance, attempting to purchase Stellar for someone who wanted to purchase a bunch of Stellar when it was the #8 coin in the world (it is #23 now), and continuously failing to purchase any Stellar. There would be trading, I would issue a buy order where it was trading, and the entire market would mysteriously shift up. I would withdraw the order, things came back down. The market moved if you breathed on it, there was no way to get any size. It didn&#39;t make sense until I realized that most of the trading was not real. The whole thing was a house of cards. I reported this back and the person said, yes, everyone knows there&#39;s a lot of wash trading, I still want to buy Stellar. There&#39;s only so much you can do.</p><p> Lewis offers a strange claim here:</p><blockquote><p> Toward the end of 2018 the markets suddenly changed again. Spreads tightened dramatically, going from 1 percent to seven one-hundredths of a percent. (1,754)</p></blockquote><p> I mean, no, they didn&#39;t? I was at least kind of there, unrelatedly trading crypto. Throughout 2018 there were plenty of ways to trade rather large amounts for far less than a percent. Yes, spreads did tighten, and I am confident Alameda contributed to that tightening, but this was not an order of magnitude change.</p><p> Nor was it the final change. As time went on, spreads would tighten further. Alameda would be in a more and more competitive business. This was likely a prime motivation behind creating a crypto exchange, FTX, before Alameda lost its edge.</p><p> During this period, SBF relocated to Hong Kong, because he found that being in the room with other crypto people was very good for business.例如：</p><blockquote><p> Weeks before he flew to Asia, one of the big Chinese crypto exchanges had frozen Alameda&#39;s account with a bunch of money in it, for no obvious reason. Customer service hadn&#39;t returned their calls. After meeting Sam in person, the exchange&#39;s bosses handed him back his money. (1,782)</p></blockquote><p> It also gave Sam access to a new labor pool, one eager to get into the game and do whatever it took without asking questions, and got everyone out of the United States.</p><p> Sam&#39;s approach to hiring was to ensure no one ever know what they were doing, so this new pool of talent worked out great.</p><blockquote><p> Faced with a necessity, Sam turned it into a virtue. “It&#39;s a moderately bad sign if you are having someone do the same thing they&#39;ve done before,” he said. “It&#39;s adverse selection of a weird sort. Because: why are they coming to you?” (1,974)</p></blockquote><p> Spoken like an employer who does not know how to attract the best talent, and also someone who even Michael Lewis knows is bullshitting this time.</p><p> Another potential factor in this story that is not mentioned by Lewis, and also that did not come up in my previous post, is Tether. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1712096233672581273">Patrick McKenzie has the theory</a> that Alameda&#39;s true main business was knowing what to say to American banks to allow Tether to move capital. That they were centrally engaged in fraud on this entire additional level.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1709206146249117927">This related thread of Patrick McKenzie&#39;s is also fun.</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1709583245153599940">As is this one</a> . Or at least, you can learn more about what I (and I assume Patrick) find fun.</p><p> The next step was building a crypto exchange.</p><h4> Building FTX</h4><p> Sam had a secret weapon in building FTX, which is that he had a programmer that could single-handedly (so the book says) program the whole thing better than most programming teams. Knowing what I know about exchanges and engineers, this claim is a lot less wild than it sounds. At core an exchange is about getting a small number of important things right, which FTX mostly did get right except where Sam chose to intentionally get them wrong. I totally believe that a two person team, one to know what to do and the other to do it, could have pulled that off.</p><p> Before going down other funding routes, Sam tried to get CZ of Binance to pay.</p><blockquote><p> The first decision CZ had to make was whether to pay Sam the $40 million he was asking for his cleverly designed futures exchange. After thinking it over for a few weeks in March 2019, CZ decided no—and then told his people to create a futures exchange on their own. Which struck Sam as such an ordinary and vaguely disappointing thing to do. “He&#39;s kind of a douche but not worse than a douche,” said Sam. “He should be a great character but he&#39;s not.” (1,893)</p></blockquote><p> I do not really know what Sam was expecting. CZ presumably took a few weeks to think it over in order to keep optionality and get a head start, if he wasn&#39;t already building such an exchange anyway as seems rather plausible. Luckily for Sam, he still managed to do the better execution than CZ.</p><p> The book describes CZ as a strangely conventional and unimaginative person, who created Binance and made it the dominant exchange on the planet, becoming one of the richest people on Earth, without any exceptional qualities or skills of note. Lewis makes it sound like CZ was one of many who started exchanges, and he was at the right place at the right time and things broke his way. I don&#39;t know anything about CZ that isn&#39;t common knowledge, but I do not buy this at all. Random people do not luck into that kind of situation. But that would be the story of a different book.</p><p> So now Sam needed money to build FTX. He had a killer programmer, but there is a lot more to an exchange than that. So it was time to fundraise.</p><p> The book talks about two ways they raised money: Selling FTT tokens, which are a cryptocurrency Sam created representing claims on a portion of FTX&#39;s future revenue and thus effectively a form of preferred stock in FTX, and traditional VC fundraising.</p><p> The FTT story is told as a story of quick success. He starts out charging early people $0.10, then quickly that goes up quite a lot, some people get rich out of the gate, Sam is sad at what he gave away. VCs in this spot, and crypto people too, tell you not to be upset about that. You need big gains and a story to drive excitement, you still have most of the company and a ton of the tokens. You have what you need. Why fret it? Instead, Sam says later in the book he regretted creating the tokens and sold them so cheap, rather than regretting the tokens because he used them later in such crazy fashion that he blew up his whole empire.</p><p> The stock story is where SBF learns the basics of how VC works. In traditional Sam fashion, he noticed things were kind of arbitrary and dumb, then did not stop to think that they might not be as arbitrary and dumb as all that and there might be method to the madness even if it wasn&#39;t fully optimal.</p><blockquote><p> In early 2021, Jump Trading—not a conventional venture capitalist—offered to buy a stake in FTX at a company valuation of $4 billion. “Sam said no, the fundraise is at twenty billion,” recalled Ramnik. Jump responded by saying that they&#39;d be interested at that price if Sam could find others who were too—which told you that the value people assigned to new businesses was arbitrary. (2,060)</p></blockquote><p> No, this does not mean the valuation is arbitrary. That is especially true when, as was the case in FTX and most crypto companies, you politely decline to let anyone do proper due diligence, and you&#39;re not even a traditional VC. What is going on is that Jump is quite reasonably deciding that at a fair rate they would be willing to invest, but that they are not in position to evaluate what is fair. So they outsource that to others, including to the lead whoever that might be. If VCs are willing to costly signal, via their own investment, that a $20 billion valuation is reasonable, then Jump can be in as well.</p><blockquote><p> Selling a new business to a VC was apparently less like selling a sofa than it was like pitching a movie idea. (2,063)</p></blockquote><p> Well, yeah, that one is largely right. They care a ton about a good story.</p><p> Then we have Sam being peak Sam a bit.</p><blockquote><p> A guy from Blackstone, the world&#39;s biggest private investment firm, called Sam to say that he thought a valuation of $20 billion was too high—and that Blackstone would invest at a valuation of $15 billion. “Sam said, &#39;If you think it is too high, I&#39;ll let you short a billion of our stock at a valuation of twenty billion,&#39; ” recalled Ramnik. “The guy said, &#39;We don&#39;t short stock.&#39; And Sam said that if you worked at Jane Street you&#39;d be fired the first week.” (2,084)</p></blockquote><p> Sam is very much the one who gets fired in the first week here. No, you are not obligated to flip coins every time you think you have a tiny edge, especially billion dollar ones with uncapped potential losses subject to potential rampant manipulation and huge adverse selection. Nor has Sam paused to consider the cost of capital. VCs demand edges well in excess of 33% before they are willing to invest.</p><p> It is crazy, completely insane, to think that a VC willing to invest in a start-up at $15 billion would want to be short for size at $20 billion, with no market or way to cover.</p><p> Another part of the puzzle is that Sam used Alameda&#39;s resources to create FTX, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/alpackaP/status/1593308236568055808">and the first VC that Sam talked to figured this and a number of other things out</a> .</p><p> Yet presumably Sam said this because he not only thought he was right, he thought he was so obviously right it made sense to say so over the phone. That tells you a lot about Sam&#39;s attitude towards capital, sizing, risk and other related matters, and also in believing that he know&#39;s all and everyone else is an idiot, which is <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=iaHDBL7dVgs&amp;ab_channel=TheTomLehrerWisdomChannel">more, more, I&#39;m still not satisfied</a> .</p><h4> The Sam Show</h4><p> What about physically building FTX, as in their new headquarters in the Bahamas that was never finished?</p><p> There&#39;s a bunch of great stories in the book about the architects who got brought in to make this very expensive building, who were given no guidance, and desperately tried to figure out what their client wanted. All they got were three quick notes – the shape of an F, a side that looked like Sam&#39;s hair, and a display area for a Tungsten Cube – that Sam also didn&#39;t bother writing, instead someone else tried to imagine what Sam might ask for. It was all the Sam show, all about Sam all the time, very cult of personality or at least hair.</p><blockquote><p> Even from their jungle huts people jockeyed for a view of him. The architects schemed the main building with glass walls and mezzanines that offered unlikely interior views of Sam. “It gives you an opportunity to catch glimpses of Sam no matter where you are sitting,” said Ian [the architect]. (2,569)</p><p> The list had been created by someone else inside FTX who&#39;d tried to imagine what perhaps he himself might want in his new office buildings, were he Sam. Sam didn&#39;t want his Jewfro on the side of the building. This other person had just imagined that “Jewfro on the side of the building” was the kind of thing Sam might find amusing. (2,612)</p></blockquote><p> Another way he kept it all about him was not to give anyone else a title that represented what they were actually doing.</p><blockquote><p> Sam then listed some reasons why this might be so: Having a title makes people feel less willing to take advice from those without titles. Having a title makes people less likely to put in the effort to learn how to do well at the base-level jobs of people they&#39;re managing. They end up trying to manage people whose jobs they couldn&#39;t do, and that always goes poorly. (2,644)</p><p> Having titles can create significant conflicts between your ego and the company. Having titles can piss off colleagues. (2,648)</p></blockquote><p> If while reading this book you are not playing the game of noticing world-class levels of lacking self-awareness, you were missing out.</p><blockquote><p> Nishad Singh failed to imagine the way things actually went south, but he did imagine a different highly plausible one that likely happened in a bunch of other Everett branches.</p><p> I&#39;d soon be asking Nishad Singh for the same premortem I&#39;d ask of others at the top of their psychiatrist&#39;s org chart: “Imagine we&#39;re in the future and your company has collapsed: tell me how it happened.” “Someone kidnaps Sam,” Nishad would reply immediately, before unspooling his recurring nightmare of Sam&#39;s lax attitude toward his personal safety leading to the undoing of their empire.</p><p> ..made for excellent ransom. “People with access to crypto are prime kidnap targets,” said Nishad. “I cannot understand why it doesn&#39;t happen more.” (2,736)</p></blockquote><p> People don&#39;t do things. None of the people in the world thought to kidnap Sam, despite zero attempts to prevent this, so despite being perhaps the most juicy kidnap target the world has ever known, he remained un-kidnapped. The man had actual zero security, posed zero physical threat, had billions in crypto that was accounted for by literal no one including himself, and was a pure act utilitarian and effectively a causal decision theorist. That person pays all the ransom, and then shrugs it off and gets back to work.</p><p> Which is good, given they had no other decision making process whatsoever.</p><blockquote><p> “It is unclear if we even have to have an actual board of directors,” said Sam, “but we get suspicious glances if we don&#39;t have one, so we have something with three people on it.” When he said this to me, right after his Twitter meeting, he admitted he couldn&#39;t recall the names of the other two people. “I knew who they were three months ago,” he said. “It might have changed. The main job requirement is they don&#39;t mind DocuSigning at three am DocuSigning is the main job.” (2,833)</p></blockquote><p> There was no CFO. Why have a CFO? What would they do, keep track of how much money we have?</p><blockquote><p> “There&#39;s a functional religion around the CFO,” said Sam. “I&#39;ll ask them, &#39;Why do I need one?&#39; Some people cannot articulate a single thing the CFO is supposed to do. They&#39;ll say &#39;keep track of the money,&#39; or &#39;make projections.&#39; I&#39;m like, What the fuck do you think I do all day? You think I don&#39;t know how much money we have?” (2,838)</p></blockquote><p> You know what? I do indeed think you do not know how much money you had.</p><h4> A Few Good Trades</h4><p> Sam did a lot of trades. Some of them were good trades. Some of them were not.</p><p> That means sometimes you look dumb, and sometimes you look like a genius.</p><p> When the good ones can pay off by orders of magnitude, every VC and everyone in crypto knows that is a nice place to be.</p><p> For example, that Solana trade? Sweet.</p><blockquote><p> Even if it wasn&#39;t [true], Solana&#39;s story was good enough that other people might see it that way and drive up the price of its token. Eighteen months later, Alameda owned roughly 15 percent of all Solana tokens, most purchased at twenty-five cents apiece. The market price of Solana had gone as high as $249, a thousand-times increase on what Sam had paid for the tokens, and the face value of Sam&#39;s entire stash was roughly $12 billion. (2,346)</p></blockquote><p> Makes up for a lot of other trades gone bad, provided you then sell some rather than double down. Yeah, I know, this is Sam we are talking about.</p><p> With a lot of effective control over Solana, Sam then was properly motivated to drive more hype and adaption. He even got to create a spin off, a &#39;Sam Coin&#39; called Serum, which was meant to be a claim on a portion of the fees for financial transactions on the Solana blockchain.</p><p> This was, presumably, a way to expropriate other holders of Solana. Instead of returning the fees to Solana holders, they would go to Serum holders, so suddenly there was another coin to distribute and manipulate and hype. Fun. The only problem was that it worked too well.</p><blockquote><p> Soon after Serum&#39;s creation, its price had skyrocketed. Sam clearly had not anticipated this. He now had all these employees who felt ridiculously rich. (At least in theory, the value of Dan Friedberg&#39;s Serum stash peaked, in September 2021, at over $1 billion.)</p><p> In Sam&#39;s view, everyone at once became a lot less motivated to work fourteen-hour days. And so he did a very Sam thing: he changed the terms of the employees&#39; Serum.</p><p> In the fine print of the employee Serum contract, he&#39;d reserved for himself the right to extend Serum&#39;s jail time, and he used it to lock up all employees&#39; Serum for seven years. Sam&#39;s employees had always known that he preferred games in which the rules could change in the middle.</p><p> They now understood that if he had changed the rules once, he might do it again. They became less enthusiastic about their Serum. “It was very unclear if you had it or if you didn&#39;t have it,” said Ramnik, who had watched in irritation as Sam locked up a bunch of tokens that he&#39;d bought with his own money on the open market before he joined FTX. “I guess you would know in seven years.” (3,980)</p></blockquote><p> Lewis is so close to getting it. He understands that Sam will betray everyone around him whenever he can. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=WpE_xMRiCLE&amp;ab_channel=AdultSwim">He is altering the deal, pray that he does not alter it any further</a> . Only from Sam&#39;s perspective, there is no deal, there is only <a target="_blank" rel="noreferrer noopener" href="https://www.amazon.com/Reality-What-You-Can-Away/dp/1561840807">reality, which is what you can get away with</a> .</p><p> Sam also had the advantage of being Sam and controlling Alameda and FTX.</p><p> He also had the bonus of not being so inspired to turn his paper gains into actual dollars (or stable coins, or liquid cryptos like BTC or ETH). Why liquidate what you can borrow against? That way Number Go Up.</p><p> Another trade he did was to take advantage of all the wash trading. The wash trading was so ingrained into how business was done, and done so poorly, that when SBF intercepted some of it, Binance&#39;s employees failed to explain to their boss CZ what was even happening. Or, he credibly pretended not to understand.</p><blockquote><p> It was a weird conversation—the CEO of one crypto exchange calling the CFO of another to inform him that, if he didn&#39;t want to lose money on his new futures contract, he&#39;d need to improve his market manipulation. Wei Zhou spoke to CZ, who called Sam for a brief though not unfriendly chat, after which Sam concluded that CZ still had not been told by his traders what had actually happened.</p></blockquote><p> What happened was that Binance was doing its market manipulation via predictable market orders, so SBF would step in front of those orders, which made a bunch of money that came out of Binance&#39;s pocket. Which Binance did not like.</p><p> Sam would occasionally consult others on what to do? I guess? Even Lewis realizes Sam does not actually care what anyone else thinks.</p><blockquote><p> After talking to them, Sam could tell himself that he&#39;d checked his judgment without having done so. (2,771)</p><p> Sam had invested $500 million in an artificial intelligence start-up called Anthropic, apparently without bouncing the idea off anyone else. “I said to Sam after he did it, &#39;We don&#39;t know a fucking thing about this company,&#39; ” said Ramnik. (2,818)</p></blockquote><p> Putting the $500 million into Anthropic was arguably the most important decision Sam ever made. I do not know if investing in Anthropic was a good or bad move for the chances of everyone not dying, but chances are this was either a massively good or massively bad investment. It dwarfs in impact the rest of his EA activities combined.</p><p> Another good trade Sam noticed was that rich people dramatically underinvest in politics, whatever you think of Sam&#39;s execution during what one might generously label his learning phase.</p><blockquote><p> What surprised Sam, once he himself had unlimited sums of money, was how slowly rich people and corporations had adapted to their new political environment. The US government exerted massive influence on virtually everything under the sun and maybe even a few things over it. In a single four-year term, a president, working with Congress, directed roughly $15 trillion in spending. And yet in 2016, the sum total of spending by all candidates on races for the presidency and Congress came to a mere $6.5 billion. “It just seems like there isn&#39;t enough money in politics,” said Sam. “People are underdoing it. The weird thing is that Warren Buffett isn&#39;t giving two billion dollars a year.” (2,874)</p></blockquote><p> We should not forget the original arbitrage trade with South Korea and Japan.</p><p> The good arbitrage trade that still doesn&#39;t fully make sense was ModelBot. I see no reason for it not to have worked, but I also see no reason Sam could not have safely proved that it worked by starting small and then scaling up. Why all the drama? Then it stopped working as competition improved.</p><p> Even excluding the arbitrage trades, that track record is really good. Sam took a lot of shots, but I think not thousands of such shots. If you can make trades like Solana at $0.25 and early Anthropic, the rest of your trades can lose and you could still have very good alpha – provided you are responsible with your sizing and other risk management, and cut your losses when trades fail and properly consider liquidity issues. There would be no need to lie, or to do all the fraud and crime.</p><p> The problem was that Sam was the opposite of responsible with the sizing and risk management. He did not cut his losses when trades failed. He did not consider liquidity issues.</p><p> There is also the highly related issue of all the lying and fraud and crime.</p><h4> The Plan</h4><p> Behind every great fortune, they say, is a great crime. Certainly that was true for this one. Then, as The Godfather tells us, one needs to appear to go legit.</p><p> Sam&#39;s plan was to present FTX as the responsible adults in the room.</p><p> It did help that the room was crypto, and filled with crypto exchanges. Many of which were indeed doing all the crimes. From the perspective of the United States, even the ones not doing all the crimes were still doing crimes anyway, the SEC has yet to explain to anyone what it would take to do crypto without doing crimes.</p><p> The biggest fish in the pond was CZ and Binance. Oh boy were they doing crimes. Their headquarters is intentionally nowhere. Their internal messages explicitly affirm that they are running an unlicensed security exchange in America.等等。</p><blockquote><p> Which is why, when Sam took in the situation, he decided that Binance&#39;s strategy was unsustainable. That the smart thing to do was to be the world&#39;s most law-abiding and regulator-loving exchange. FTX could use the law, and the regulators, to drive crypto trading from Binance and onto FTX. If countries did not yet have the laws, a small army of FTX lawyers would help them to create them. (2,406)</p></blockquote><p> Step one was to get CZ and Binance off the cap table, so no one evaluating FTX for its legitimacy would see them on the cap table doing all the crime. So Sam bought him out.</p><blockquote><p> For the stake he&#39;d paid $80 million to acquire, CZ demanded $2.2 billion. Sam agreed to pay it. Just before they signed the deal, CZ insisted, for no particular reason, on an extra $75 million. Sam paid that, too. (2,461)</p></blockquote><p> If SBF was going to pretend FTX was worth that much, why shouldn&#39;t CZ get paid accordingly? However, SBF made a big mistake, and left CZ with $500 million in FTT tokens rather than fully paying out in cash. It really should not have been that hard to not let that happen, given all the money available for spewing elsewhere. Ideally you sell a little of the equity you bought back, and use the proceeds from that.</p><p> The next step in reputation washing was a bunch of advertising.</p><blockquote><p> And so when someone from the Miami Heat reached out to them to suggest that FTX buy their naming rights, for $155 million for the next nineteen years, Sam leapt at the chance. That the deal required the approval not just of the NBA but also of the Miami-Dade Board of County Commissioners, a government body, was a bonus. After that, they could point to a government entity that had blessed FTX.</p><p> Once their name was on an American stadium, no one turned down their money. They showered money across US pro sports: Shohei Ohtani and Shaquille O&#39;Neal and LeBron James became spokespeople.</p><p> They paid Major League Baseball $162.5 million to put the company name on every umpire&#39;s uniform. Having the FTX logo on the umpires&#39; uniforms, Sam thought, was more useful than having it on the players&#39; uniforms. In basically every TV shot of every Major League Baseball game, the viewer saw the FTX patch. “The NBA put us through a vetting process,” said FTX lawyer Dan Friedberg. “Major League Baseball just said okay!” (2,482)</p></blockquote><p> It really is that easy. Once the Miami Heat opened the door, no one else asked any questions. Everyone wanted the money, and that was that, FTX on the umpires to go with the Tezos sign that the Mets were somehow paid to display above center at Citi Field. Sure, why not?</p><p> Given how restrictive FTX US was, this helps explain why SBF was so eager to sponsor all the things. He was after a different goal.</p><p> A common theme of FTX&#39;s sponsorships, like much of what FTX did, is that SBF would spew money in spectacular fashion, most of which was wasted, but he&#39;d also have big wins. In this case, the win was Tom Brady.</p><blockquote><p> But everywhere Sam went, people mentioned that they had heard of FTX because of Brady. Hardly anyone mentioned any of the other endorsers. “It was very clear which things had an effect and which did not,” said Sam. “For the life of me, I can&#39;t figure out why this is. I still don&#39;t know how to verbalize it.” (2,505)</p></blockquote><p> No one has ever <a target="_blank" rel="noreferrer noopener" href="https://tvtropes.org/pmwiki/pmwiki.php/Main/CriticalResearchFailure">Not Done the Research</a> <a target="_blank" rel="noreferrer noopener" href="https://tvtropes.org/pmwiki/pmwiki.php/Main/CreatorsApathy">more than Sam</a> ,, who is confused why Tom Brady impacted people more than Brett Farve. I am not confused at all. Tom Brady is the quarterback everyone was already always talking about, the one everyone hated or perhaps loved, the cheater, the one with the rings, the GOAT, the one who got a girl in trouble and left her, and all that. I say quarterback, you say Brady.</p><p> Next up was getting into politics. As I noted in the good trades section, Sam noticed this was remarkably cheap. So since he had no time to waste but he definitely had money to waste, he got cracking. Gabe Bankman-Fried, Sam&#39;s brother, got put in charge of the political operation.</p><p> Attention is not always people&#39;s strong suit.</p><blockquote><p> I really appreciate this, but it wouldn&#39;t be good for me to take money from FTX, so I can&#39;t—besides, I have found another source of funding.&#39; That other source of funding was Gabe, my brother.” (2,897)</p></blockquote><p> Sam&#39;s most famous political bet was on Carrick Flynn. The decision to back Flynn comes off in the book, if anything, massively stupider than it looked in real time.</p><blockquote><p> Carrick Flynn&#39;s most important trait, in Sam&#39;s view, was his total command of and commitment to pandemic prevention. His second-most important trait was that he was an effective altruist. (2,927)</p><p> Flynn asked some fellow EAs what they thought about him running for Congress. As a political candidate he had obvious weaknesses: in addition to being a Washington insider and a bit of a carpetbagger, he was terrified of public speaking and sensitive to criticism. He described himself as “very introverted.” And yet none of the EAs could see any good reason for him not to go for it—and so he&#39;d thrown his hat into the ring. (2,930)</p></blockquote><p> I don&#39;t blame Flynn, who was trying to do what he thought was the right thing. But it seems so utterly obvious every step of the section on him that this man was never going to be in Congress. Yet they threw tons of money at him anyway, even after that money became the central campaign issue, and all the other candidates ganged up on Flynn over being a crypto stooge and a carpetbagger, and everyone in the district was complaining how their mailboxes were overflowing from campaign ads and they couldn&#39;t take one more of Flynn&#39;s spots on the television.</p><p> To be fair, there was real uncertainty the night before, no one knew for sure that it hadn&#39;t worked. And yes, a champion is super valuable?</p><p> What did Sam learn?</p><blockquote><p> [Sam] actually didn&#39;t mind all that much. He&#39;d learned a lesson: there were political candidates no amount of money could get elected. (2,959)</p></blockquote><p> Well, yes. Also, you learned that when you stick your neck out like that you and those associated with you (read: EA) pay a lasting reputational cost. Sam did not seem to notice this.</p><p> No time to lose. Sam was off to meet Mitch McConnell, with everyone scrambling to get SBF into a presentable suit (he had been convinced to technically bring a suit, but had given no thought to its presentability, he let others handle such things) and he worked to ensure not to call Mitch, who insisted on being called Leader, &#39;dear leader&#39; instead. Which I admit sounds hard.</p><p> Also, check out the claim at the end here.</p><blockquote><p> At that moment, Sam was planning to give $15–$30 million to McConnell to defeat the Trumpier candidates in the US Senate races. On a separate front, he explained to me, as the plane descended into Washington, DC, he was exploring the legality of paying Donald Trump himself not to run for president. His team had somehow created a back channel into the Trump operation and returned with the not terribly earth-shattering news that Donald Trump might indeed have his price: $5 billion. Or so Sam was told by his team. (2,972)</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://puck.news/s-b-f-s-mcconnell-money-tickle-part-2/">A $10 million donation to a McConnell dark money group One Nation has been confirmed.</a></p><p> I once again remind everyone that, while the price has likely gone up, the offer is probably still on the table if someone is bold enough to take it. Sure, now that he&#39;s got the nomination in his sights it probably costs you $10 billion, but given the way people talk about what a Trump victory would look like, surely that is a small price to pay?</p><p> Meanwhile, Sam claimed to have infiltrated Trump&#39;s team, and I love what they did with the place.</p><blockquote><p> Sam&#39;s team had come up with an idea—which, Sam claimed, was just then making its way to Trump himself. The idea was to persuade Trump to come out and say “I&#39;m for Eric!” without specifying which Eric he was for. After all, Trump didn&#39;t actually care who won. (2,979)</p></blockquote><p> Trump actually did it, and it is plausible this switched which Eric won. Good show.</p><h4> The Tragedy of FTT</h4><p> The whole FTT situation still blows my mind.</p><p> I mean, I know it happened, I accept it. Still. Blows my mind every time.</p><p> The tragedy is there was absolutely no need for any of it. There was no need to keep flipping coins double-or-nothing for all the money on the assumption that the odds were in Sam&#39;s favor.</p><p> Which they weren&#39;t. Yet he kept flipping.</p><p> So here&#39;s the basics, for those who don&#39;t know.</p><p> Alameda owned a lot of FTT, which is effectively stock in FTX.</p><p> This FTT was highly illiquid. Trying to sell even a fraction of it would have collapsed the price, as everyone involved knew. Collapsing the price of FTT would then, as again everyone involved knew, cause a collapse of confidence in FTX, causing a run on the bank that was FTX. Which those involved had information to know would be quite a serious problem, were it to happen.</p><p> Every trader knows that you do not borrow heavily against your own illiquid stock, with loans recallable at any time and likely to be recalled when times get tough for your industry, to buy other illiquid and highly speculative things highly correlated to your stock and your industry.</p><p> Especially if you know you could not survive the resulting bank run because you&#39;ve appropriated billions in customer funds to cover your other losses or even to keep making more illiquid investments, or to spend on random stuff. All while your exchange was a highly valued money printing machine that could easily raise equity capital.</p><p> And if you were still for some reason going to do that, they would at least know not to also give your biggest rival a huge chunk of that same illiquid token sufficient to crash the market, then actively try to drive him out of his home and bring regulators down on his head, in ways he can see you doing right there.</p><p> I mean, come on, that&#39;s completely insane.</p><p> Except that is, by the book&#39;s admission, exactly what happened.</p><blockquote><p> It seemed perfectly natural for Alameda to control all the remaining FTT, and use it as collateral in its trading activity. Sam didn&#39;t even try to hide what he was doing. (2,105)</p></blockquote><p> Did other crypto firms accept this collateral, knowing or even worse somehow not knowing exactly what this implied? Why yes. Yes they did.</p><p> This created a highly volatile situation. A downward spiral waiting to happen.</p><p> Then crypto crashed, everyone including Alameda lost a lot of money, and it happened.</p><p> To try and prevent it from happening, Alameda had to actually repay its loans, or else the FTT it used as collateral was going to get liquidated. Then it had to bail out firms like Voyager. This was all on top of all the money Alameda and FTX had already spent and lost.</p><p> Sam still did not seem to notice that funding might be an urgent issue.</p><blockquote><p> At their peak, they&#39;d together been valued at roughly $7 billion. Now Ramnik was acquiring them for no more than $200 million. A pittance. Or so it seemed. Ramnik recently had asked Sam how much capital he should assume was available for possible acquisitions, and Sam had said, Just let me know if you get to a billion. (3,100)</p></blockquote><p> So Sam kept poking the bear. Hence, The Vanishing.</p><h4> The Vanishing</h4><p> That&#39;s what Michael Lewis calls the collapse of FTX.</p><p> The proximate cause was that Sam pissed off CZ, while very much not being in a position to call BS on anyone. As in doing things like this:</p><blockquote><p> Sam later wrote up the message he&#39;d tried to convey to them. “I love Dubai,” he said. But we can&#39;t be in the same place as Binance. 。 。 。 This is for two reasons: first they are constantly devoting significant company resources to trying to hurt us; and second that they soil the reputation of wherever they are. I can&#39;t emphasize this enough: in general I hear great things from other jurisdictions/regulators etc. about Dubai and the UAE [United Arab Emirates], except that there&#39;s a constant refrain of: that&#39;s the jurisdiction that accepted Binance, and so we don&#39;t trust their standards. It was unclear to Sam, if Dubai decided to rid itself of CZ and his exchange, whether any country in which CZ would be willing to live would accept them. In these woods, CZ was the biggest bear and Sam seemed to be going out of his way to poke him. (3,154)</p></blockquote><p> CZ was understandably upset, leaked a supposed balance sheet from Alameda that looked bad but not as bad as the full reality, and announced his intention to dump his FTT.</p><p> Caroline Ellison decided to respond by offering to buy all the FTT at $22, thinking this was a show of strength, except for once crypto investors understood what that meant and acted accordingly.</p><blockquote><p> Within twenty seconds of Caroline&#39;s tweet came a rush to sell FTT by speculators who had borrowed money to buy it. The panic was driven by an assumption: if Alameda Research, the single biggest owner of FTT, was making a big show of being willing to buy a huge pile of it for $22, they must need for some reason to maintain the market price at 22. The most plausible explanation was that Alameda Research was using FTT as collateral to borrow dollars or bitcoin from others. “You don&#39;t tell someone a price level like $22 unless you have a lot of confidence that you need that price,” the CEO of Gauntlet, Tarun Chitra, told Bloomberg News. By Monday night, the price of FTT had fallen from $22 to $7. (3,203)</p></blockquote><p> Then the run on FTX began in earnest. Which would not have been a problem…</p><blockquote><p> Ramnik could see that money was leaving FTX, but he didn&#39;t view it as a big deal. The customers might panic and pull out all their money. But once they realized that there was nothing to panic about, they&#39;d return, and their money would too. (3,221)</p></blockquote><p> …except that FTX did not have the money to pay their customers, because Alameda had taken it and did not have the ability to give it all back.</p><p> Or have much idea how much they even had.</p><blockquote><p> Though Caroline was in charge of Alameda Research, she seemed totally clueless about where its money was. She&#39;d come onto the screen and announce that she had found $200 million here, or $400 million there, as if she&#39;d just made an original scientific discovery. Some guy at Deltec, their bank in the Bahamas, messaged Ramnik to say, Oh, by the way, you have $300 million with… And it came as a total surprise to all of them!</p><p> That he&#39;d been taken by surprise. He wondered: If these people knew there was a risk that they might not have enough money, why hadn&#39;t they even bothered to figure out how much they had? They&#39;d done nothing. (3,244)</p></blockquote><p> Didn&#39;t see it coming, I suppose. Did decide to use $8 billion in customer funds as if it was Alameda operating capital. Did not anticipate that the customers might ask for that money back all at once when they found out what was going on. Whoops.</p><p> Lewis rightly points out, near the end, that while many people did realize FTX was obviously up to no good, no one actually managed to figure out the exact no good they were up to until rather late in the game.</p><blockquote><p> Even those who had expressed suspicion about Sam or FTX had failed to say the one simple thing you would say if you knew the secret they were hiding: the customers&#39; deposits that are supposed to be inside FTX are actually inside Alameda Research. (4,007)</p></blockquote><p> They also couldn&#39;t imagine that things could have been as chaotic and unaccounted for, or as blatant, as they were. It wasn&#39;t necessary for the no good to be that no good. The borrowing against FTT tokens was bad enough on its own.</p><p> A lot of people, as FTX started to collapse, did the same calculation I did. It was quickly clear, as Sam went on Twitter to put on his best dog-drinking-coffee face and say &#39;assets are fine,&#39; that there were only two possible worlds.</p><ol><li> Either things really were fine, because FTX was obviously a money machine, things not being fine would have meant a completely crazy level of recklessness and incompetence, and SBF had gone way over the &#39;this is fraud if things are not fine&#39; line and was very all-in. No one would be so stupid as to.</li><li> Or this was pure fraud, through and through, and FTX and SBF did all the crime.</li></ol><p> That&#39;s why me and so many others turned around on a dime – once we could rule out scenario #1, we knew we were in scenario #2.</p><p> One by one, people who wanted it to be one way got the piece of evidence that convinced them it was the other way.</p><blockquote><p> Zane pinged Sam and asked, &#39;Should I do damage control?&#39; &#39;Yup,&#39; he said.” Zane then sent Sam a message asking three questions: “One, are we insolvent, two, did we ever lend out customer funds to Alameda, and three anything I didn&#39;t ask that I need to know?” Sam didn&#39;t reply—and then went totally silent on him. (3,344)</p><p> Still, Zane figured there was no way that FTX was in real trouble. It made no sense. The price of FTT shouldn&#39;t have any effect on the value of the exchange, any more than the price of Apple stock should have on Apple&#39;s iPhone sales. Just the reverse: the exchange&#39;s revenues drove the value of FTT. “If FTT goes to zero, so what?” said Zane. The other reason it made no sense was that FTX had been so wildly profitable. “I know how much real revenue we were making: two bips [0.02 percent] on two hundred fifty billion dollars a month,” said Zane. “I&#39;m like, Dude, you were sitting on a fucking printing press: why did you need to do this?” (3,347)</p></blockquote><p> Here is happens to Constance, on seeing the &#39;balance sheet.&#39;</p><blockquote><p> The next document in her stack was a rough balance sheet of Alameda Research that differed in important ways from the rough balance sheet that had inspired the CoinDesk article now being credited with bringing down the entire business. It appeared to Constance that it had been hastily concocted either by Sam or Caroline, or maybe by both. Constance had first come across it the previous Tuesday, after FTX had ceased sending money back to its customers. “When I saw it, I told my team not to respond to external parties because I did not want them to lose their good name and reputation,” she said.</p><p> The list of assets included the details of hundreds of private investments Sam had made over the previous two years, apparently totaling $4,717,030,200. The liabilities now had a line item more important than everything else combined: $10,152,068,800 of customer deposits. More than $10 billion that was meant to be custodied by FTX somehow had ended up inside Sam&#39;s private trading fund. The document listed only $3 billion in liquid assets—that is, US dollars or crypto that could be sold immediately for dollars.</p><p> “I was like, Holy shit,” she said. “The question is: Why?” It was the same question Zane had asked. “We had so profitable a business,” said Constance. “Our profit margin was forty to fifty percent. We made four hundred million dollars last year.” (3,478)</p></blockquote><p> They may have made five hundred million, but even if they hadn&#39;t stolen everyone&#39;s money, that was not about to pay the expenses.</p><blockquote><p> Constance herself had lost around $25 million. She still had $80,000 in an ordinary bank account she&#39;d kept from her previous life, but otherwise she&#39;d lost everything. (3,492)</p></blockquote><p> This is the kind of thing that still blows my mind. You have stock in FTX, you have $25 million in liquid assets, the world is in front of you. And you chase FTX&#39;s interest payments, and trust FTX so much, that you keep all your money on the exchange. What? That is completely crazy behavior. And yet, most employees tell exactly that story. It seems likely SBF/FTX insisted upon it, and Lewis either missed this or declined to mention it.</p><p> Because at $25 million while working at a crypto company, I&#39;d hope I&#39;d be doing things like millions in gold in a secret vault. At minimum I&#39;d have $5 million in an offshore bank account.</p><p> But even after that, Caroline didn&#39;t turn on Sam yet. She only turned on Sam when she realized that, compared to those around her, she&#39;d been given an order of magnitude or two less stock than she should have gotten.</p><blockquote><p> That&#39;s when Constance&#39;s feeling about Sam changed: when she saw how she&#39;d actually been treated. (3,524)</p></blockquote><p> Only then did she decide to spend the last chapter of the book helping Sam with logistics so she could try to get Sam to confess.</p><h4> The Reckoning</h4><p> As future prisoners, having been caught doing all the crime, the principles of FTX faced the prisoner&#39;s dilemma.</p><p> The game theory of SBF: You have to commit to the bit.</p><blockquote><p> That night, Nishad requested a meeting with just Gary and Sam. Once the three were alone in a room, Nishad asked, What happens if law enforcement or regulators reach out? What do you mean? Sam asked. How do we make sure we cooperate in prisoner&#39;s dilemma? How do we all make sure we say the other ones are innocent? I don&#39;t have any reason to think any one of us had criminal intent, said Sam. (3,291)</p><p> No, said Nishad. That&#39;s not good enough. You need to talk to them. You need to tell them I had no clue. How could I know that? asked Sam. You are saying that I should say that you know nothing about something I know nothing about. How is that even possible? It makes no sense. But I didn&#39;t know, said Nishad. Then say that, said Sam. It&#39;s not going to work for me, said Nishad. Because there is code-based evidence of what I did. (3,295)</p></blockquote><p> The game theory of everyone else? Not so much.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matt_levine/status/1713273141118509317">Caroline held this meeting on November 9</a> to explain the situation to her employees. as Patrick McKenzie says &#39;what a document.&#39;</p><p> Being Causal Decision Theory agents, and being somewhat more grounded in reality, the rest of the EAs all turned state&#39;s witness.</p><p> Sam also had many other bizarre ideas about how any of this worked.</p><blockquote><p> “At the end of the day, the deciding factor in the jurisdictional dispute is Gary,” said Sam, the night Zane left, “because he&#39;s the only one who knows how to use a computer.” (3,374)</p></blockquote><p> Sam was convinced to declare bankruptcy in America lest he instead have it declared for him by less friendly other parties, then tried to undo it which you cannot do, then went around insisting that if he hadn&#39;t declared bankruptcy it would all have worked out.</p><p> Sam kept trying to explain how the money was all there, really, or close to it, and how all of this was merely a serious of unfortunate &#39;fuck ups&#39; and misunderstandings. Sam thought he had plenty of money, didn&#39;t keep track of things properly, everything seemed safe at the time, he was as surprised as anyone.</p><p> So far, so public domain. The weird thing is that Michael Lewis seems to buy it.</p><blockquote><p> In Sam&#39;s telling, FTX had switched off Alameda&#39;s risk limits to make itself more appealing. The losses caused by this unsettling policy were in any case trivial. Ordinary trading loans made by FTX to Alameda constituted a small fraction of the losses to customers; on their own, they wouldn&#39;t have posed a problem. The bulk of the customers&#39; money inside of Alameda that should have been inside FTX—$8.8 billion of it, to be exact—resided in an account that Alameda had labeled fiat@. The fiat@ account had been set up in 2019 to receive the dollars and other fiat currencies sent by FTX&#39;s new customers. (3,543)</p><p> In Sam&#39;s telling, the dollars sent in by customers that had accumulated inside of Alameda Research had simply never been moved. Until July 2021, there was no other place to put them, as FTX had no US dollar bank accounts. They&#39;d been listed on a dashboard of FTX&#39;s customer deposits but remained inside Alameda&#39;s bank accounts. Sam also claimed that, right up until at least June 2022, this fact, which others now found so shocking, hadn&#39;t attracted his attention. (3,555)</p><p> But even if you valued the contents of Alameda more rigorously, as Sam sort of did in his head sometimes, you could still easily get to $30 billion. The $8.8 billion that should not have been inside Alameda Research was not exactly a rounding error. But it was, possibly, not enough to worry about. As Sam put it: “I didn&#39;t ask, like, &#39;How many dollars do we have?&#39; It felt to us that Alameda had infinity dollars.” (3,562)</p><p> At that point, in Sam&#39;s telling, Sam thought that Alameda might be in trouble. He decided to dig into its accounts on his own and understand the problem. By October, he had a clearer picture. It was only then that he could see that Alameda had been operating as if the $8.8 billion in customer funds belonged to it. And by then it was too late to do anything about it. (3,576)</p></blockquote><p> This story does not actually make any sense, and of course is directly and blatantly contradicted by testimony at the trial. And yet Michael Lewis is intrigued:</p><p> I had a different question. It preoccupied me from the moment of the collapse: Where had the money gone? It was not obvious what had happened to it. (3,610)</p><blockquote><p> At any rate, when I was done, my extremely naive money-in, money-out statement looked like this:</p><p> MONEY IN:</p><p> Net customer deposits: $15 billion</p><p> Investments from venture capitalists: $2.3 billion</p><p> Alameda trading profits: $2.5 billion</p><p> FTX exchange revenues: $2 billion</p><p> Net outstanding loans from crypto lenders (mainly Genesis and BlockFi): $1.5 billion</p><p> Original sale of FTT: $35 million</p><p> Total: $23,335,000,000</p><p> MONEY OUT:</p><p> Returned to customers during the November run: $5 billion</p><p> Amount paid out to CZ: $1.4 billion (Just the hard cash part of the payment. I&#39;m ignoring the $500 million worth of FTT Sam also paid him, as Sam minted those for free. I&#39;m also ignoring the $80 million worth of BNB tokens that CZ had used to pay for his original stake, worth $400 million at the time Sam returned them as part of his buyout of CZ&#39;s interest.)</p><p> Sam&#39;s private investments: $4.4 billion (The whole portfolio was $4.7 billion, but at least one investment, valued at $300 million, Sam had paid for with shares in FTX. He likely did the same with others, and so this number is likely bigger than it actually was.)</p><p> Loans to Sam: $1 billion (Used for political and EA donations. After his lawyers explained to him that taking out loans was smarter than paying himself a stock dividend, as he&#39;d need to pay tax on the dividends.)</p><p> Loans to Nishad for same: $543 million</p><p> Endorsement deals: $500 million (This is likely generous too, as in some cases—Tom Brady was one of them—FTX paid its endorsers with FTX stock and not dollars.) Buying and burning their exchange token,</p><p> FTT: $600 million</p><p> Corporate expenses (salaries, lunch, Bahamas real estate): $1 billion</p><p> Total: $14,443,000,000 (3,626)</p></blockquote><p> The case has been made to me that this accounting is not as naive and stupid as it looks. I continue to mostly disagree with that. Lewis continues to double down.</p><blockquote><p> There were some likely explanations for the missing money. The more you thought about them, however, the less persuasive they became. For example, Alameda traders might have gambled away $6 billion. But if they had, why did they all believe themselves to be so profitable, right to the end? I&#39;d spoken to a bunch of them. Several were former Jane Streeters. They weren&#39;t stupid. (3,462)</p></blockquote><p> This is perhaps the most &#39;naive guy&#39; thing in the entire book. Smart people can&#39;t think they are making good trades when they are making bad ones and losing tons of money, right? And they wouldn&#39;t lie to Michael Lewis about profitability, right?</p><blockquote><p> The most hand-wavy story just then being bandied about was that the collapse in crypto prices somehow sucked all the money out of Sam&#39;s World. And it was true that Sam&#39;s massive holdings of Solana and FTT—and other tokens of even more dubious value—had crashed. They&#39;d gone from being theoretically worth $100 billion at the end of 2021 to being worth practically zero in November 2022.</p><p> But Sam had paid next to nothing for these tokens; they had always been more like found money than an investment he&#39;d forked over actual dollars to acquire. He&#39;d minted FTT himself, for free. (3,647)</p></blockquote><p> At their peak, Alameda was on (some form of electronic) paper worth $100 billion or so. We know that Alameda&#39;s edge in algorithmic trades had likely been going away as they faced stiffer competition, that source of profit was likely gone, yet they continued to borrow. What was the profitable trading Alameda was doing with all that capital?</p><p> They were getting long. Alameda was borrowing a bunch of capital from various lenders, and using it to get long and then get longer. That is where the money was going.</p><p> Then Number Went Down. Money gone.</p><p> Does Michael Lewis think the people at Three Arrows Capital or Voyager were stupid? The people who created and ran Luna? Enron? Lehman Brothers? Does this man not remember his own books?</p><p> He said it himself. Sam&#39;s entire empire was a leveraged – Lewis&#39;s word – bet on the success of crypto and the empire itself more generally. When you have no ethics only a quest for Number Go Up (you know, for the common good) and therefore don&#39;t care that, sure, technically that was customer deposits right there, that leverages your bet all the more. As Number Go Down, rather than hedge, they doubled down, including via providing bailouts.</p><p> Leverage plus Number Go Down equals Broke Fi Broke, overwhelming other sources of profits.</p><p> Yes, a lot of their horde of stuff they bought for pennies. But they then used that as collateral to borrow money and put more things into the horde. All of which was correlated, and all of which was down. A lot.</p><p> Also, SBF was shoving money out the door in any number of other ways that the above numbers are missing, money was constantly being misplaced or stolen, a fire sale is not a cheap thing to partake in, and so on. So I do not think there is any mystery here.</p><p> We then get a fascinating story. San says combined losses from things like this were only $1 billion, but honestly how would he even know given everything.</p><blockquote><p> But on that evening, Sam filled in one piece of this particular puzzle: FTX had lost a lot of money to hackers. To avoid encouraging other hackers, they&#39;d kept their losses quiet. The biggest hacks occurred in March and April 2021. A lone trader had opened an account on FTX and cornered the market in two thinly traded tokens, BitMax and MobileCoin.</p><p> His purchases drove up the prices of the two tokens wildly: the price of MobileCoin went from $2.50 to $54 in just a few weeks. This trader, who appeared to be operating from Turkey, had done what he had done not out of some special love for MobileCoin. He&#39;d found a flaw in FTX&#39;s risk management software. FTX allowed traders to borrow bitcoin and other easily sellable crypto against the value of their MobileCoin and BitMax holdings.</p><p> The trader had inflated the value of MobileCoin and BitMax so that he might borrow actually valuable crypto against them from FTX. Once he had it he vanished, leaving FTX with a collapsing pile of tokens and a loss of $600 million worth of crypto.</p><p> The size of those hacks was an exception, Sam said. All losses due to theft combined had come to just a bit more than $1 billion. In all cases, Gary had quietly fixed the problem and they&#39;d all moved on and allowed the thieves to keep their loot. “People playing the game,” was Sam&#39;s description of them. (He really was easy to steal from.) (3,701)</p></blockquote><p> That is not a hack. He did not steal the money. You gave it to him.</p><p> I know that people call such things hacks, like the &#39;hack&#39; about going both ways using leverage earlier. Instead, Sam is right here. this is people playing the game. If your risk engine is stupid enough to let me use my MOBL at $54 to borrow and withdraw a bunch of actual BTC, treating the value of MOBL as real, then that is on the risk engine, whether or not there was also market manipulation involved. I felt the same way about Avi and the Mango trade – yes sure it is illegal and no one is crying for him when he gets arrested nor should they, but also suck it up and write better code, everyone, as is the crypto way.</p><p> FTX&#39;s risk engine was by all accounts excellent, when dealing with coins that were liquid relative to the position sizes involved, and when the risk engine was set to on. FTX&#39;s risk engine was sometimes turned off, and the risk engine clearly did not make reasonable adjustments for illiquid or obviously bubble-shaped coins.</p><p> This also was rather a big deal – FTX lost, by Sam&#39;s own account, a full year&#39;s profits. And that&#39;s the official Sam story. The real story is inevitably much worse. I do not for a second buy that they only lost $1 billion total in hacks.</p><h4> John Ray, the Hero We Need</h4><p> John Ray is pretty great. He&#39;s the guy who cleaned up the Enron mess, the guy you call when you have a world-class mess, and he&#39;s the one they called in for FTX.</p><p> Suddenly there is a no-nonsense adult in the room who is having none of it, even when there is some of it worth having.</p><p> Michael Lewis tries his best to throw shade at him, but Lewis is too honest – too much a naive guy – for any of it to stick even a little.</p><blockquote><p> As a legal matter, at 4:30 in the morning on Friday, November 11, 2022, Sam Bankman-Fried DocuSigned FTX into bankruptcy and named John Ray as FTX&#39;s new CEO. As a practical matter, Sullivan &amp; Cromwell lined up John Ray to replace Sam as the CEO of FTX, and then John Ray hired Sullivan &amp; Cromwell as the lawyers for the massive bankruptcy. (3,772)</p><p> While Sam stewed, John Ray read up on him and this company he&#39;d created. “It&#39;s like, What is this thing?” said Ray. “Now it&#39;s just a failure, but it was once some kind of business. What did you guys do? What&#39;s the situation? Why&#39;s this falling into bankruptcy so quickly?” He briefly considered the possibility that the failure was innocent: maybe they got hacked. “Then you start looking at the kid,” said Ray, the kid being Sam. “I looked at his picture and thought, There&#39;s something wrong going on with him.”</p><p> Ray prided himself on his snap judgments. He could look at a person and in ten minutes know who they were, and never need to reconsider his opinion. The men he evaluated he tended to place in one of three bins in his mind: “good guy,” “naive guy,” and “crook.” Sam very obviously was not a good guy. And he sure didn&#39;t seem naive. (3,783)</p></blockquote><p> That&#39;s a great skill if you are consistently correct. Based on the evidence presented, John Ray is almost never wrong about what type of guy he is dealing with.</p><blockquote><p> He&#39;d spoken only long enough with the other members of Sam&#39;s inner circle to see them for what they were. Nishad Singh struck him as a naive guy. “He&#39;s narrow,” said Ray. “It&#39;s tech, tech, tech. There&#39;s never a problem he can&#39;t solve. He&#39;s not going to steal money. He&#39;s not going to do anything wrong. But he has no idea what&#39;s going on around him. You ask him for a steak and he puts his head up the bull&#39;s ass.”</p><p> The bankruptcy team had located Caroline Ellison by phone on the Saturday after Ray became FTX&#39;s new CEO. She at least had been able to explain where some of the wallets storing the crypto were stashed. Other than that, she wasn&#39;t much use. “She&#39;s cold as ice,” said Ray. “You had to buy words by the vowel. An obvious complete fucking weirdo.” (3,797)</p></blockquote><p> Nishad being a naive guy seems right to me, based on the rest of the book. He had more than enough information to know what was happening, but the Arc Words of the whole book are that people don&#39;t see what they don&#39;t look for, so there you go.</p><blockquote><p> “There&#39;s people that are born criminals, and there&#39;re people that become criminals,” said Ray. “I think [Sam] became a criminal. The how and why he became a criminal I don&#39;t know. I think maybe it takes an understanding of this kid and his parents.” (3,808)</p></blockquote><p> John Ray is here to let you know that you are suffering, and pitying, too many fools.</p><blockquote><p> Six days into his new job, Ray filed a report with the US Bankruptcy Court for the District of Delaware. “Never in my career have I seen such a complete failure of corporate controls and such a complete absence of trustworthy financial information as occurred here,” he wrote. Instead of grilling the people who had created the mess, Ray hired teams of hard-nosed sleuths—many of whom he&#39;d worked with before. “Serious adults,” as he called them. The Nardello firm was a lot of former FBI guys. (Corporate motto: We find out.) (3,822)</p></blockquote><p> First he got things under some semblance of order. He then moved on to looking for all the money.</p><blockquote><p> That was in early 2023. By late April, John Ray&#39;s head was on a swivel. “This is live-action,” he said. “There&#39;s always something every hour.” One day, some random crypto exchange got in touch and said, By the way, we have $170 million in an account of yours: do you want it back? Another day, some random FTX employee called them out of the blue to say that he&#39;d borrowed two million bucks from the company and wanted to repay the loan—of which, so far as Ray could see, there was no record. Of course, once you heard about one loan, you had to wonder how many others like it you&#39;d never hear about. (3,836)</p><p> Several months into the hunt, Ray&#39;s sleuths had discovered that “someone had robbed the exchange of four hundred fifty million.” They&#39;d stumbled upon not the simple hack of November 2022 but the complicated BitMax and MobileCoin hacks of $600 million in the spring of 2021. (The dollar value changed with fluctuations in the price of the stolen crypto.) They&#39;d tracked the hacker not to Turkey but Mauritius. “We have a picture of him going in and out of his house,” said Ray. He was pretty sure he was going to get most of that money back. “We believe there are a lot more of these,” said Ray. (3,844)</p></blockquote><p> Lewis portrays Ray in all this as an archeologist, shifting through the ruins for cash and clues. Michael Lewis makes a point of all the money Ray and his team were going to bill FTX for the work they did. I look at what they had to deal with and how much money they ultimately rounded up, and I say they earned every penny. Part of earning that is that when you are Ray, you cannot rely upon or trust anyone who made the mess in the first place. That&#39;s hostile information sources. If you want it done right, and you do, you have to figure it all out for yourself.</p><p> The best thing about Ray is his reaction to Lewis, as Lewis keeps trying to explain all the things he think he knows, and Ray keeps ignoring him, and it&#39;s going to be some of the straight up funniest scenes in the movie.</p><p> I demand that Ray be played by John Goodman, it would be so perfect.</p><blockquote><p> At some point his team discovered that a Hong Kong subsidiary of Alameda Research called Cottonwood Grove had bought vast sums of FTT, for example. To the innocent archaeologist, it was evidence of Sam&#39;s World artificially propping up the value of FTT. Ray didn&#39;t know that FTX had been obligated to spend roughly a third of its revenues buying back and burning its token, and that Cottonwood Grove was the entity that did it. From my perch on the side of the dig, I would occasionally shout down to the guy running it my guess about the most recent find, but he&#39;d just look up at me, pityingly. I was clearly a naive guy. (3,875)</p></blockquote><p>是的。 Very clearly.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/liron/status/1710193485763289469">We have a fun clip of Ray spelling the uselessness of Lewis to him out for us</a> .</p><p> The thing about Ray is that, in order to be so good at his job, he needs to have zero tolerance for pretty much anything. So when something actually is real, he can miss it.</p><blockquote><p> The hundreds of private investments made by Alameda Research, for instance. When we first met, in early 2023, Ray went on about how fishy these all were. He had a theory about why Sam had thrown money around the way he had: Sam was buying himself some friends. “For the first time in his life, everyone ignores the fact that he&#39;s a fucking weirdo,” said Ray. As an example, he cited the dollars Sam had invested in artificial intelligence companies. “He gave five hundred million bucks to this thing called Anthropic,” said Ray. “It&#39;s just a bunch of people with an idea. Nothing.” (3,886)</p></blockquote><p> Lewis would say this theory is ridiculous, and on its face it definitely is, everyone wanted to be Sam&#39;s friend, but also how much got invested into OpenAI and Anthropic in the name of access? As in, friendship?</p><p> What Ray cannot see is that Anthropic was obviously a very good financial investment, because he does not know anything about AI. He certainly does not want to hear anything about existential risk, or whether Anthropic is helping or not helping with that concern.</p><p> A key question was, what crypto was worth anything, and what wasn&#39;t? For some reason Ray locked onto Serum, the offshoot of Solana.</p><blockquote><p> And yet now, somehow, in John Ray&#39;s book, the locked Serum was good shit. Primo crypto of the finest vintage imbibed by all gentlemen of good taste. And who knows?—maybe one day it will be. But if Serum was a token to be taken seriously, Sam Bankman-Fried and the world he created needed to be viewed in a different light. At Serum&#39;s peak price, the stated market value of Sam&#39;s stash of it was $67 billion. On November 7, 2022, Sam&#39;s pile of mostly locked Serum was still “worth” billions of dollars. If even locked Serum had that kind of value, FTX was solvent right up to the moment it collapsed. And John Ray would have no grounds for clawing back money from any of the many lucky people on whom Sam Bankman-Fried had showered it. (3,988)</p></blockquote><p> In case you didn&#39;t know, well, not so much, here&#39;s Serum.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97383cf9-4143-4ecb-b3e0-1a17c745e6eb_1200x678.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AocXh6gJ9tJC2WyCL/rp7lxlwubyh5wxvo8tuu" alt=""></a></figure><p> Ray kept searching, Ray kept finding.</p><blockquote><p> That would raise the amount collected to $9.3 billion—even before anyone asked CZ for the $2.275 billion he&#39;d taken out of FTX. Ray was inching toward an answer to the question I&#39;d been asking from the day of the collapse: Where did all that money go? The answer was: nowhere. It was still there. (4,000)</p></blockquote><h4> Caroline Ellison</h4><p> Sam&#39;s on-again, off-again, very-bad-idea relationship with Caroline Ellison is a key part of the story, because Caroline ended up effectively in charge of Alameda when the worst of the fraud went down. It does not seem like a coincidence that Caroline ended up in charge of Alameda, despite her not seeming like someone who should be given that kind of responsibility, as per (among other signs) her repeated observations that she was not up to the job.</p><p> Also she did not have an ideal attitude with respect to willingness to do various crimes, where the whole thing made her deeply uncomfortable but she still did the crimes anyway – you want someone who does not do crimes, or contains their crimes to contextually &#39;ordinary decent crimes&#39; rather than outright frauds like stealing customer funds. Or if you have decided that your plan is to do a lot of crimes, a plan I recommend strongly against, you want someone who is fine with doing lots of crime.</p><p> In any case, Caroline it seems exchanged long emails spelling out the arguments for exactly how obviously she and Sam should not have been dating, with Sam offering points like this:</p><blockquote><p> [Sam] began with a seriously compelling list, titled: ARGUMENTS AGAINST:</p><p> In a lot of ways I don&#39;t really have a soul. This is a lot more obvious in some contexts than others. But in the end there&#39;s a pretty decent argument that my empathy is fake, my feelings are fake, my facial reactions are fake. I don&#39;t feel happiness. What&#39;s the point in dating someone who you physically can&#39;t make happy? I have a long history of getting bored and claustrophobic. This has the makings of a time when I&#39;m less worried about it than normal; but the baseline prior might be high enough that nothing else matters. I feel conflicted about what I want. Sometimes I really want to be with you. Sometimes I want to stay at work for 60 hours straight and not think about anything else. I&#39;m worried about power dynamics between us. This could destroy Alameda if it goes really poorly PR-wise. This combos really badly with the current EA shitshow I&#39;m supposed to be, in some ways, adjudicating. I make people sad. Even people who I inspire, I don&#39;t really make happy. And people who I date—it&#39;s really harrowing.</p><p> It really fucking sucks, to be with someone who (a) you can&#39;t make happy, (b) doesn&#39;t really respect anyone else, (c) constantly thinking really offensive things, (d) doesn&#39;t have time for you, and (e) wants to be alone half the time. There are a lot of really fucked up things about dating an employee.</p><p> This list was followed by another, briefer list, titled “ARGUMENTS IN FAVOR.” I really fucking like you. I really like talking to you. I feel a lot less worried about saying what&#39;s on my mind to you than to almost anyone else. You share my most important interests. You&#39;re a good person. I really like fucking you. You&#39;re smart and impressive. You have good judgement and aren&#39;t full of shit. You appreciate a lot of me for who I am. (2,126)</p></blockquote><p> While I admit those are actually pretty strong arguments in favor, and in other circumstances would be very good reasons to date someone, the arguments against seem rather conclusive.</p><blockquote><p> Caroline wanted a conventional love with an unconventional man. Sam wanted to do whatever at any given moment offered the highest expected value, and his estimate of her expected value seemed to peak right before they had sex and plummet immediately after. (2,155)</p></blockquote><p> This is exactly what one would expect from the rest of Sam&#39;s behavior in other contexts. Story checks out.</p><p> That I guess brings us to the psychiatrist? Who according to other reports had the entire firm including Sam hopped up on various pills in ways the book declines to mention?</p><blockquote><p> It didn&#39;t take a psychiatrist to see a pattern in Sam&#39;s relationship with Caroline, but there happened to be one sitting in the middle of it. His name was George Lerner, and by late 2021 he might have been the world&#39;s leading authority on the inner life of effective altruists. (2,208)</p></blockquote><p> I know of at least two psychiatrists who were and are better experts on this than George Lerner. For example, have you met… Scott Alexander? Anyway.</p><blockquote><p> Then the effective altruists started showing up—and when they did, George took a new and keener interest in his patients. Gabe Bankman-Fried, Sam&#39;s younger brother, was the first, but hard on his heels came Caroline Ellison and others from Alameda Research. By the time Sam arrived, a year later, George was treating maybe twenty EAs. As a group, they eased a worry George had about himself: the limits to his powers of empathy. When ordinary people came to him with their ordinary feelings, he often found himself faking an understanding. The EAs didn&#39;t need his empathy; the EAs thought that even they shouldn&#39;t care about their feelings. In their single-minded quest to maximize the utility of their lives, they were seeking to minimize the effect of their feelings. In their single-minded quest to maximize the utility of their lives, they were seeking to minimize the effect of their feelings. “The way they put it to me is that their emotions are getting in the way of their ability to reduce their decisions to just numbers.” (2,238)</p><p> They were all completely and utterly sincere. They judged the morality of any action by its consequences and were living their life to maximize those consequences. (2,249)</p></blockquote><p> You can&#39;t do that. I mean, obviously you literally can, but professionally no, you really, really can&#39;t treat Gabe and his brother Sam and his girlfriend and employee Caroline and everyone else in their entire social network. This is Dr. Nick territory. Then again, given what everyone involved wanted, maybe you can? It&#39;s not like they wanted anything from him except drugs and practical advice understanding other people. Maybe there is no conflict of interest here after all.</p><p> Also, who cares, given George didn&#39;t even have a license in the Bahamas in the first place?</p><blockquote><p> The Bahamas hadn&#39;t granted George a medical license. His title was Senior Professional Coach. (2,633)</p></blockquote><p> Perhaps he was taking inspiration from the (nominal) psychiatrist in Billions?</p><h4> New and Old EA Cause Areas</h4><p> In addition to his EA causes, SBF did have his own cause area, which was physical beauty.</p><p> He was against it.</p><blockquote><p> [Anna Wintour of Vogue] looked like a million bucks, but her art, like all art, was wasted on Sam. (235)</p><p> “You start by making decisions on who you are going to be with based on how they look,” he said. “Then, because of that, you make bad choices about religion and food and everything else. Then you are just rolling the dice on who you are going to be.”</p><p> Anna Wintour, now that he thought of it, represented much of what he disliked about human beings. “There are very few businesses that I have strong moral objections to, and hers is one of them,” he said. “I actually have disdain for fashion. I have general disdain for the importance that physical attractiveness has, and this is one thing emanating out of that.” (348)</p></blockquote><p> He also investigated but dismissed as a child <a target="_blank" rel="noreferrer noopener" href="https://unsongbook.com/">the cause area of hell</a> .</p><blockquote><p> He found his way to a solution that offered temporary relief: only children suffered from this madness. Yes, kids believed in Santa. But grown-ups did not. There was a limit to the insanity. But then, a year or so later, a boy in his class said he believed in God.</p><p> “And I freaked out,” recalled Sam. “Then he freaked out. We both freaked out. I remember thinking, Wait a minute, do you think I&#39;m going to hell? Because that seems like a big deal. If hell exists, why do you, like, care about McDonald&#39;s? Why are we talking about any of this shit, if there is a hell. If it really exists. It&#39;s fucking terrifying, hell.”</p><p> From the widespread belief in God, and Santa, Sam drew a conclusion: it was possible for almost everyone to be self-evidently wrong about something. “Mass delusions are a property of the world, as it turns out,” he said.</p></blockquote><p> According to the company psychiatrist, the EAs really did only care about suffering.</p><blockquote><p> “It doesn&#39;t really start with people,” said George. “It starts with suffering. It&#39;s about preventing suffering.” (2,257)</p></blockquote><p> This attitude drives me bonkers. Yes, suffering is bad. It is the way we indicate to ourselves that things are bad. It sucks. Preventing it is a good idea. But when you think that suffering is the thing that matters, you confuse the map for the territory, the measure for the man, the math with reality. Combine that with all the other EA beliefs, set this as a maximalist goal, and you get… well, among other things, you get FTX. Also you get people worried about wild animal or electron suffering and who need hacks put in to not actively want to wipe out humanity.</p><p> If you do not love life, and you do not love people, or anything or anyone within the world, and instead wholly rely on a proxy metric? If you do not have Something to Protect? Oh no.</p><p> I mean, listen to yourselves, as George is describing you:</p><blockquote><p> “A lot of EAs chose not to have kids,” said George. “It&#39;s because of the impact on their own lives. They believe that having kids takes away from their ability to have impact on the world.” After all, in the time it took to raise a child to become an effective altruist, you could persuade some unknowably large number of people who were not your children to become effective altruists. “It feels selfish to have a kid. The EA argument for having a kid is that kid equals happiness and happiness equals increased productivity. If they can get there in their head, then maybe they have a kid.” (2,261)</p><p> “There are two parts of being EA,” said George. “Part one is the focus on consequences. Part two is the personal sacrifice.” (2,267)</p></blockquote><p> That is saying, my own child&#39;s only value would be if they too become an effective altruist, or if they increase my altruistic productivity. This is not an attitude compatible with life. If this is you, please halt, catch fire and seek help immediately.</p><p> This last point does not ring true, EAs totally complain about lack of dating opportunities, although I can totally buy that everyone else thought the EAs thought they were smarter than everyone else – and in context, that they were technically right.</p><blockquote><p> “Everyone is complaining about the lack of dating opportunities,” said George. “Except the EAs. The EAs didn&#39;t care.”</p><p> The non-EAs thought the EAs thought they were smarter than everybody else. (2,628)</p></blockquote><p> As much as I criticize EAs, I do it because they are worthy of criticism. They aspire to do better. Otherwise I wouldn&#39;t waste my time. And when Lewis goes too far and misses the mark, there&#39;s big &#39;no one picks on my brother but me&#39; energy.</p><blockquote><p> One day some historian of effective altruism will marvel at how easily it transformed itself. It turned its back on living people without bloodshed or even, really, much shouting. You might think that people who had sacrificed fame and fortune to save poor children in Africa would rebel at the idea of moving on from poor children in Africa to future children in another galaxy. They didn&#39;t, not really—which tells you something about the role of ordinary human feeling in the movement.没关系。 What mattered was the math. Effective altruism never got its emotional charge from the places that charged ordinary philanthropy. It was always fueled by a cool lust for the most logical way to lead a good life. (3,045)</p></blockquote><p> We rationalists have long had a name for the &#39;emotional charge&#39; that drives ordinary philanthropy. <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/3p3CYauiX8oLjmwRF/purchase-fuzzies-and-utilons-separately">We call it &#39;cute puppies with rare diseases.</a> &#39; There is a reason most philanthropy accomplishes nothing except fueling that emotional charge, which is that most decisions in most philanthropy are driven by fueling that emotional charge. The entire point, the founding principle, of EA, the core of what is good about EA, is to care about actually accomplishing the mission and cutting the enemy.</p><p> Can this be taken too far in various ways to the point where it loses its connection to reality? Does relying too much on the math and not enough on common sense and error checks lead to not noticing wrong conclusions are wrong? Oh yes, this absolutely happens in practice, the SBF group was not so extreme an outlier here.</p><p> But at least the crazy kids are trying. At all. They get to be wrong, where most others are not even wrong.</p><p> Also, future children in another galaxy? Try our own children, here and now. People get fooled into thinking that &#39;long term&#39; means some distant future. And yes, in some important senses, most of the potential value of humanity lies in its distant future.</p><p> But the dangers we aim to prevent, the benefits we hope to accrue? They are not some distant dream of a million years from now. They are for people alive today. You, yes you, and your loved ones and friends and if you have them children, are at risk of dying from AI or from a pandemic. Nor are these risks so improbable that one needs to cite future generations for them to be worthy causes.</p><p> I fight the possibility of AI killing everyone, not (only or even primarily) because of a long, long time from now in a galaxy far, far away. I fight so I and everyone else will have grandchildren, and so that those grandchildren will live. Here and now.</p><p> If some other EAs made this change because the numbers (overwhelmingly, and in this case I believe correctly) said so, and would have done so even if the case was less overwhelmingly correct? So be it. We need some people like that. Others need to help with global poverty, and so they do. And they make a lot of mistakes there too, they take the math too seriously, they don&#39;t consider second and third order effects properly, and so on. I could go on rants. But you know what? They try, damn it.</p><p> As opposed to ordinary philanthropy, where the EAs are right: It&#39;s mostly kinda dumb.</p><blockquote><p> They&#39;d been doing this for only a year and already had been pitched nearly two thousand such projects. They&#39;d handed out some money but in the process they&#39;d concluded that conventional philanthropy was kind of dumb. Just to deal with the incoming requests—most of which they had no ability to evaluate—would require a big staff and lots of expense. Much of their money would end up being used on a vast bureaucracy.</p><p> And so they had just recently adopted a new approach: instead of giving money away themselves, they scoured the world for subject matter experts who might have their own, better ideas for how to give away money.</p><p> Over the previous six months, one hundred people with deep knowledge of pandemic prevention and artificial intelligence had received an email from FTX that said, in effect: Hey, you don&#39;t know us, but here&#39;s a million dollars, no strings attached. Your job is to give it away as effectively as you can.</p><p> The FTX Foundation, started in early 2021, would track what these people did with their million dollars, but only to determine if they should be given even more. “We try not to be very judgy once they have the money,” said Sam. “But maybe we won&#39;t be reupping them.” (3,060)</p><p> They were moving fast, as Sam always did. “If you throw away a quarter of the money, that&#39;s very sad,” he said at one point, “but if it allows you to triple the effectiveness of the rest, that&#39;s a win.” (3,073)</p></blockquote><p> This was a really good idea, in the world in which FTX had properly secured the money in order to give it away, and in which they had the proper infrastructure to do this responsibly. Even without either of those things, it was still a reasonable idea.</p><p> There were problems. People were unprepared to hand out a million dollars. A lot of decisions involving a lot of money got made, if not Brewster&#39;s Millions style, in ways that were quite warping on the places the money got spread around. From what I heard, essentially any 19-year-old could get a $50,000 grant to move to Berkeley and think about AI safety, and there was a general failure to differentiate good and real and worthwhile efforts from others. The dynamics this created were an invitation to fake work, to predators and entryism and sociopaths, to hype and networks and corruption. If things had continued, that effect could have gotten worse.</p><p> As always, Sam was not considering second-order effects, and also not considering that efforts might backfire rather than be wasted. Nor did he pay enough attention to one of the most important questions traders always must ask on every trade they do, which is: What is the correct sizing?</p><p> Doing this trade with only a select few would have been great. Doing it with everyone who had an EA identity and a pulse was plausibly net negative.</p><h4> Won&#39;t Get Fooled Again</h4><p> In the wake of publication, many people pointed out that Michael Lewis had been fooled, <a target="_blank" rel="noreferrer noopener" href="https://nymag.com/intelligencer/2023/10/how-michael-lewis-got-duped-by-sam-bankman-fried.html">including this book report from David Roth</a> . <a target="_blank" rel="noreferrer noopener" href="https://time.com/6321668/michael-lewis-sam-bankman-fried-going-infinite/">Michael Lewis did not take kindly to this</a> , while confirming he had been fooled.</p><blockquote><p> Michael Lewis: I&#39;d love for the jury to read the book. Mark Cohen [Sam Bankman-Fried&#39;s lawyer] said this to me: “You get up, you tell one story, and they tell the other story, and the question is which story the jury believes.” I&#39;m in a privileged position to tell a fuller story, without leaving out any of the nasty details. If I were a juror, I would rather hear my story than either defense or prosecution.</p><p> I&#39;m just going to tell you the story as I see it, and then leave you the discretion that then you lynch him, acquit him, or don&#39;t know what to think of him. I don&#39;t want the jury thinking I left anything else they needed to know.</p><p> There&#39;s something about Sam and the situation that pushes a lot of people&#39;s buttons and causes them to want to judge quickly. If I had five hours with the prosecutors, one of the things I would love to know is why they moved so fast. Sam&#39;s lawyers had a guy on the inside and outside advising on when he might be extradited from the Bahamas. And no way did they think it was gonna happen as fast as it did, because they thought it would take the government much longer to figure out what the hell happened.</p><p> I thought that was of a piece with the general social response: how quick people wanted to judge. So I thought, I&#39;m going to be dealing with a reader who is going to be in that judgy kind of mood.</p></blockquote><p> He really thinks he included all the nasty details. The trial has made it clear this was not the case. <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/sadly-ftx">Even my old post on FTX</a> , among many other source options, also made it clear this was not the case.</p><p> As does the book. The book, despite conspicuously leaving out all the most blatant details, repeatedly shows SBF doing fraud.</p><p> Even more than that, the book describes a person who is so obviously doing all the fraud. It would not make any sense for the Sam portrayed here to not be doing all the fraud. That much is clear by the end of chapter one. Did Lewis read his own book?</p><p> The idea that the arrest was a &#39;rush to judgment&#39; is laughable. Sure, they partly moved quickly because it was important to send a message – which it was – but also because no one else has ever more obviously been doing all the crime. There are so many distinct frauds right out in the open.</p><p> Also, seriously, what the hell, you want to poison the jury pool or even the actual jury? The interviewer points out how our system works, and Lewis says no, it shouldn&#39;t work that way, people should read my book.</p><p> At one point, Lewis confirms that FTX violated the Foreign Corrupt Practices Act, as was revealed during the trial, and also perhaps as tellingly that it put a billion dollars into an account at an exchange that regularly freezes accounts frozen by the local police for no reason.</p><p> And Lewis is wondering how the money could be missing.</p><blockquote><p> Michael Lewis: This is what happened, to my knowledge—and I know this from the people in Hong Kong who orchestrated it—a Chinese exchange was routinely targeted by local Chinese police departments. They would find a pretext for freezing an account. In this case, they froze Alameda&#39;s account. And Alameda had like a billion dollars in this account.</p><p> There was an operation inside of FTX, or Alameda, whatever, in Hong Kong, to legitimately get the money back without having to go pay the ransom. And they pursued that, and it didn&#39;t work. So they went in and paid ransom to the Chinese police department directly to get the money released, and it was released.</p><p> I actually would have loved to include it. But they weren&#39;t running around bribing people to change laws. It was more telling about how the Chinese government works than anything about Sam. I then came back and confirmed it all with Sam. He said he knew he&#39;d sent someone in to get the money out, but he wasn&#39;t completely sure how he&#39;d gotten the money out.</p></blockquote><p> Next he admits that SBF committed bank fraud.</p><blockquote><p> Michael Lewis: My impression was that the bank wasn&#39;t actually misled about who these people were. That it was a kind of fig leaf thing.</p><p> Q: But it&#39;s still illegal to mislead a bank about the purpose of a bank account.</p><p> Michael Lewis: But nobody would have cared about it.</p></blockquote><p> He seems to not understand that this does not make it not a federal crime? That &#39;we probably would not have otherwise gotten caught on this one&#39; is not a valid answer?</p><p> Similarly, Lewis clearly thinks &#39;the money was still there and eventually people got paid back&#39; should be some sort of defense for fraud. It isn&#39;t, and it shouldn&#39;t be.</p><p> And then there&#39;s this:</p><blockquote><p> Michael Lewis: No one shows me receipts. But no one suggested otherwise. I interviewed 10 people in Alameda. They just weren&#39;t lying. None of them could think of a big loss.</p><p> ……</p><p> But I was kind of there through it. And there was no detectable change in Caroline, Nishad, Sam: their interactions or their demeanors. If they had this sharp, swift loss, they were really, really good with their poker faces.</p></blockquote><p> All right, that is a purer Naive Guy statement. They just weren&#39;t lying, no sir.</p><p> Nor was Sam a liar, in Lewis&#39;s eyes. Michael Lewis continued to claim, on the Judging Sam podcast, that he could trust Sam completely. That Sam would never lie to him. True, Lewis said, Sam would not volunteer information and he would use exact words. But Sam&#39;s exact words to Lewis, unlike the words he saw Sam constantly spewing to everyone else, could be trusted.</p><p> It&#39;s so weird. How can the same person write a book, and yet not have read it?</p><p> Even then, on October 1, Lewis was claiming he did not know if Sam was guilty. Not only that, he was claiming that many of the prosecutors did not know if Sam was guilty. And Lewis keeps saying that Sam himself really actually believes he is innocent, and for weeks after it was so over Sam really believed he&#39;d be able to raise funds and turn it all around.</p><p> Lewis really did believe, or claimed to believe on his podcast, even in early October that, absent one little mistake where $8 billion dollars ended up in the wrong place, the rest of what happened was fine. That the rest of the story was not filled to the brim with all the crime.</p><p> Yet I totally believe that Lewis believed all of it. The man seems so totally sincere.</p><p> Then on October 9 Lewis said nothing that came out so far at the trial surprised him, other than the claim by one of Sam&#39;s oldest friends that Alameda&#39;s special code not only let them steal all the money, it also let them trade faster than their competitors, implemented on Sam&#39;s orders. Everyone was constantly asking point blank about that, and Sam constantly said that wasn&#39;t true. Even so, Lewis still repeated that in his model Sam doesn&#39;t outright lie, he simply doesn&#39;t tell you the answer that you needed to hear. He was still holding onto that even then.</p><p> When it is revealed that the FTX insurance fund to cover trading losses, that Sam often talks about, was purely fake, literally the product of a random number generator written into the code to display to people to make them think there was an insurance fund? Because to Sam money is fungible, so why would there be an insurance fund? Still no change.</p><p> I still can&#39;t process all that. Not really. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=aV6NoNkDGsU&amp;t=132s&amp;pp=ygURY2hld2JhY2NhIGRlZmVuc2U%3D">Chewbaca is a wookie</a> . It does not make sense.</p><p> He has Matt Levine on the podcast, and Matt Levine points out that the book made it that much clearer that Sam&#39;s fraud unfolded exactly the way frauds always unfold, that there was nothing confusing here. Yeah, on some level Sam fooled himself that would all work out (or, given it was Sam, that it had odds, and the words &#39;safe&#39; and &#39;risk&#39; were meaningless, so who cares?).</p><p> In later podcasts, Lewis did admit that a lot of the trial testimony was rather damning, and that he is confident that Sam will be convicted. But there is no sign he has figured out that Sam was doing all the lying and all the crime.</p><h4>结论</h4><p>I mostly feel good closing the book on the events of SBF, Alameda and FTX. It all makes sense. We know what happened.</p><p> There are still a few mysteries, mostly centered on early Alameda. The story there, as outlined, continues not to make sense. Why was it so difficult to evaluate ModelBot? What was going on with the demands of those exiting? How did SBF get away with so little reputation damage? I still do want to know. Mostly, though, I am content.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/sadly-ftx">My previous post on FTX</a> holds up remarkably well, and could be used as a companion piece to this one. I was missing pieces of the puzzle, and definitely made mistakes, including the failure to buy up FTX debt for pennies on the dollar. But the rough outline there of what happened holds up, as does the discussion of implications for Effective Altruism.</p><p> I do not think that any of what happened was an accident. SBF was fortunate to get as far as he did before it all blew up. A blow up was the almost inevitable result. While SBF went off the rails, he went off the rails in ways that should have been largely predicted, and which make sense given who he was and then the forces and philosophical ideas that acted upon him.</p><p> This was not so unusual a case of fraud.</p><p> Nor was it an unusual case of what happens when a maximalist goal is given to a highly capable consequentialist system.</p><p> My expectation is that in the unlikely scenario that this attempted takeoff had fully succeeded, and SBF had gained sufficient affordances and capabilities thereby, that the misalignment issues involved would have almost certainly destroyed us all, or all that we care about. Luckily, that did not come to pass.</p><p> Other attempts are coming.</p><p> All of this has happened before.</p><p> All of this will happen again.</p><br/><br/><a href="https://www.lesswrong.com/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite<guid ispermalink="false"> AocXh6gJ9tJC2WyCL</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 24 Oct 2023 15:00:13 GMT</pubDate> </item><item><title><![CDATA[[Interview w/ Quintin Pope] Evolution, values, and AI Safety]]></title><description><![CDATA[Published on October 24, 2023 1:53 PM GMT<br/><br/><p> On the Futurati Podcast, we recently released an <a href="https://www.youtube.com/watch?v=XLDdG9DR7ek">interview with Quintin Pope</a> .</p><p> As you can imagine, it mostly focused on:</p><ul><li> Inner vs outer optimization;</li><li> The sharp left turn;</li><li> Natural selection, and what kind of evidence we can draw from the emergence of the first great general intelligence;</li><li> AI Safety more broadly;</li><li> How human values form;</li></ul><p>一探究竟！ And share it if you&#39;d like us to do more interviews like this :)</p><br/><br/> <a href="https://www.lesswrong.com/posts/c9W4SHa7DHwAHkqRF/interview-w-quintin-pope-evolution-values-and-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/c9W4SHa7DHwAHkqRF/interview-w-quintin-pope-evolution-values-and-ai-safety<guid ispermalink="false"> c9W4SHa7DHwAHkqRF</guid><dc:creator><![CDATA[fowlertm]]></dc:creator><pubDate> Tue, 24 Oct 2023 13:53:06 GMT</pubDate> </item><item><title><![CDATA[Lying is Cowardice, not Strategy]]></title><description><![CDATA[Published on October 24, 2023 1:24 PM GMT<br/><br/><p> <i>(Co-written by</i> <a href="https://twitter.com/npcollapse"><i><u>Connor Leahy</u></i></a> <i>and</i> <a href="https://twitter.com/Gabe_cc"><i><u>Gabe</u></i></a> <i>)</i></p><p> We have talked to a whole bunch of people about pauses and moratoriums. Members of the AI safety community, investors, business peers, politicians, and more.</p><p> Too many claimed to pursue the following approach:</p><ol><li> It would be great if AGI progress stopped, but that is infeasible.</li><li> Therefore, I will advocate for what I think is feasible, even if it is not ideal.</li><li> The Overton window being what it is, if I claim a belief that is too extreme, or endorse an infeasible policy proposal, people will take me less seriously on the feasible stuff.</li><li> Given this, I will be tactical in what I say, even though I will avoid stating outright lies.</li></ol><p> Consider if this applies to you, or people close to you.</p><p> If it does, let us be clear: <strong>hiding your beliefs</strong> , in ways that predictably leads people to believe false things, <strong>is lying</strong> . This is the case regardless of your intentions, and regardless of how it feels.</p><p> Not only is it morally wrong, it makes for a terrible strategy. As it stands, the AI Safety Community itself can not coordinate to state that we should stop AGI progress right now!</p><p> Not only can it not coordinate, <strong>the AI Safety Community is defecting, by making it more costly for people who do say it to say it.</strong></p><p> We all feel like we are working on the most important things, and that we are being pragmatic realists.</p><p> <a href="https://www.forbes.com/sites/carltonreid/2018/12/03/you-are-not-stuck-in-traffic-you-are-traffic/"><strong><u>But remember: If you feel stuck in the Overton window, it is because YOU ARE the Overton window.</u></strong></a></p><p> —</p><h1> 1. The AI Safety Community is making our job harder</h1><p> <strong>In a saner world, all AGI progress should have already stopped. If we don&#39;t, there&#39;s more than a 10% chance we all die.</strong></p><p> Many people in the AI safety community believe this, but they have not stated it publicly. Worse, they have stated <i>different beliefs</i> more saliently, which misdirect everyone else about what should be done, and what the AI safety community believes.</p><p> To date, in our efforts to inform, motivate and coordinate with people: People in the AI Safety Community publicly lying has been one of <strong>the biggest direct obstacles we have encountered.</strong></p><p> The newest example of this is ”Responsible Scaling Policies”, with many AI Safety people being much more vocal about their endorsement of RSPs than their private belief that in a saner world, all AGI progress should stop right now.</p><p> Because of them, we have been told many times that we are a minority voice, and that most people in the AI Safety community (understand, Open Philanthropy adjacent) disagree that we should stop all AGI progress right now.</p><p> <strong>That actually, there is an acceptable way to continue scaling! And given that this makes things easier, if there is indeed an acceptable way to continue scaling, this is what we should do, rather than stop all AGI progress right now!</strong></p><p> Recently, Dario Amodei (Anthropic CEO), has used the RSP to frame the moratorium position as <a href="https://www.lesswrong.com/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=WhxB66vEeRhh6kcsB"><strong><u>the most extreme version of an extreme position</u></strong></a> , and this is the framing that we have seen used over and over again. ARC <a href="https://evals.alignment.org/blog/2023-09-26-rsp/"><u>mirrors</u></a> this in their version of the RSP proposal, describing itself as a “pragmatic middle ground” between a moratorium and doing nothing.</p><p> <strong>Obviously, all AGI Racers use this against us when we talk to people.</strong></p><p> There are very few people that we have consistently seen publicly call for a <strong>stop</strong> to AGI progress. The clearest ones are Eliezer&#39;s “ <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><u>Shut it All Down</u></a> ” and Nate&#39;s “ <a href="https://twitter.com/So8res/status/1715380167911067878"><u>Fucking stop</u></a> ”.</p><p> The loudest silence is from Paul Christiano, whose RSPs are being used to safety-wash scaling.</p><p> <strong>Proving me wrong is very easy. If you do believe that, in a saner world, we would stop all AGI progress right now, you can just write this publicly.</strong></p><p> <strong>When called out on this, most people we talk to just fumble.</strong></p><h1> 2. Lying for Personal Gain</h1><p> We talk to many people who publicly lie about their beliefs.</p><p> The justifications are always the same: “it doesn&#39;t feel like lying”, “we don&#39;t state things we do not believe”, “we are playing an inside game, so we must be tactical in what we say to gain influence and power”.</p><p> <strong>Let me call this for what it is: lying for personal gain.</strong> If you state things whose main purpose is to get people to think you believe something else, and you do so to gain more influence and power: <strong>you are lying for personal gain.</strong></p><p> The results of this “influence and power-grabbing” has many times over materialised with the safety-washing of the AGI race. What a coincidence it is that DeepMind, OpenAI and Anthropic are all related to the AI Safety community.</p><p> The only benefit we see from this politicking is the people lying gain more influence, while the time we have left to AGI keeps getting shorter.</p><p> <strong>Consider what happens when a community rewards the people who gain more influence by lying!</strong></p><p> —</p><p> So many people lie, and they screw not only humanity, but one another.</p><p> Many AGI corp leaders will privately state that in a saner world, AGI progress should <strong>stop</strong> , but they will not state it because it would hurt their ability to race against each other!</p><p> Safety people will lie so that they can keep ties with labs in order to “pressure them” and seem reasonable to politicians.</p><p> Whatever: they just lie to gain more power.</p><p> <strong>“DO NOT LIE PUBLICLY ABOUT GRAVE MATTERS” is a very strong baseline. If you want to defect, you need a much stronger reason than “it will benefit my personal influence, and I promise I&#39;ll do good things with it”.</strong><br><br> And you need to accept the blame when you&#39;re called out. You should not muddy the waters by justifying your lies, covering them, telling people they misunderstood, and try to maintain more influence within the community.</p><p> We have seen so many people be taken in this web of lies: from politicians and journalists, to engineers and intellectuals, all up until the concerned EA or regular citizen who wants to help, but is confused by our message when it looks like the AI safety community is ok with scaling.</p><p> <strong>Your lies compound and make the world a worse place.</strong></p><p> There is an easy way to fix this situation: we can adopt the norm of publicly stating our true beliefs about grave matters.</p><p> If you know someone who claims to believe that in a saner world we should stop all AGI progress, <strong>tell them to publicly state their beliefs, unequivocally</strong> . Very often, you&#39;ll see them fumbling, caught in politicking. And not that rarely, you&#39;ll see that they actually want to keep racing. In these situations, you might want to <a href="https://www.lesswrong.com/tag/conflict-vs-mistake"><u>stop finding excuses for them</u></a> .</p><h1> 3. The Spirit of Coordination</h1><p> A very sad thing that we have personally felt is that it looks like many people are <strong>so tangled in these politics that they do not understand what the point of honesty even is</strong> .</p><p> Indeed, from the inside, it is not obvious that honesty is a good choice. If you are honest, publicly honest, or even adversarially honest, you just make more opponents, you have less influence, and you can help less.</p><p> This is typical deontology vs consequentialism. Should you be honest, if from your point of view, it increases the chances of doom?</p><p> The answer is <strong>YES</strong> .</p><p> <strong>a) Politicking has many more unintended consequences than expected.</strong></p><p> Whenever you lie, you shoot potential allies at random in the back.<br> Whenever you lie, you make it more acceptable for people around you to lie.</p><p> <strong>b) Your behavior, especially if you are a leader, a funder or a major employee (first 10 employees, or responsible for >;10% of the headcount of the org), ripples down to everyone around you.</strong></p><p> <strong>People lower in the respectability/authority/status ranks do defer to your behavior.</strong><br> <strong>People outside of these ranks look at you.</strong><br> Our work toward stopping AGI progress becomes easier whenever a leader/investor/major employee at Open AI, DeepMind, Anthropic, ARC, Open Philanthropy, etc. states their beliefs about AGI progress more clearly.<br></p><p> <strong>c) Honesty is Great.</strong></p><p> Existential Risks from AI are now going mainstream. Academics talk about it. Tech CEOs talk about it. You can now talk about it, not be a weirdo, and gain more allies. Polls show that even non-expert citizens <a href="https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll"><u>express diverse opinions</u></a> about super intelligence.</p><p> Consider the following timeline:</p><ul><li> ARC &amp; Open Philanthropy state in a press release “ <strong>In a sane world, all AGI progress should stop. If we don&#39;t, there&#39;s more than a 10% chance we will all die.</strong> ”</li><li> People at AGI labs working in the safety teams echo this message publicly.</li><li> AGI labs leaders who think this state it publicly.</li><li> We start coordinating explicitly against orgs (and groups within orgs) that race.</li><li> We coordinate on a plan whose final publicly stated goal is to get to a world state that, most of us agree is not one where humanity&#39;s entire existence is at risk.</li><li> We publicly, relentlessly optimise for this plan, without compromising on our beliefs.</li></ul><p> <strong>Whenever you lie for personal gain, you fuck up this timeline.</strong></p><p> When you start being publicly honest, you will suffer a <i>personal</i> hit in the short term. But we truly believe that, coordinated and honest, we will have timelines much longer than any Scaling Policy will ever get us.</p><br/><br/> <a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy<guid ispermalink="false"> qtTW6BFrxWw4iHcjf</guid><dc:creator><![CDATA[Connor Leahy]]></dc:creator><pubDate> Tue, 24 Oct 2023 13:24:26 GMT</pubDate></item></channel></rss>