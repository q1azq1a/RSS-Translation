<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 21 日星期六 08:13:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[How to find a good moving service]]></title><description><![CDATA[Published on October 21, 2023 4:59 AM GMT<br/><br/><p>两周前，我意识到我将在 50 天内从西雅图搬到山景城。我很兴奋，但没有意识到我需要与超过 15 家搬家公司交谈才能找到满意的公司。经过大约9个小时的寻找，我终于找到了满意的。我选择<a href="https://quotes.northamerican.com/">北美搬家服务</a>来搬运 850 英里、2000 磅、300 立方英尺 (CF) 的货物，价格为 3200 美元。这是我学到的东西（从最重要的到最不重要的）：</p><ol><li><strong>选择承运人（移动者）而不是经纪人</strong>：即使一开始听起来更便宜，经纪人最终的成本可能会更高。支付定金后，事情可能会失控。我在网上发现很多关于经纪人在搬家前几天收取额外费用的故事，这样你别无选择，只能付给他们额外的钱。有些故事还说，当搬家公司来的时候，他们会向你收取额外的费用，因为他们会说他们不同意经纪人的价格……选择一家搬家公司会更便宜，心理上也更安全。<ol><li>如何查明对方是经纪人还是搬运工？询问他们的 USDOT 号码并使用以下<a href="https://safer.fmcsa.dot.gov/query.asp?searchtype=ANY&amp;query_type=queryCarrierSnapshot&amp;query_param=USDOT&amp;query_string=3475743">网站</a>进行搜索。实体类型应为“承运人”而不是“经纪商”。通常，对于搬家公司来说，他们的网站上应该有 USDOT 号码。</li></ol></li><li><strong>不要相信谷歌搜索结果</strong>：“长途搬家监管很差”，最大的搬家公司之一的代理人告诉我。当我在谷歌搜索中搜索“最佳搬家公司”时，大多数都是经纪人。他们只需支付大量广告费即可进入热门位置，以便人们点击。这是合理的，因为他们“提供”的每项服务可以轻松赚取>;1k$。此外，谷歌的评论也可能是假的。不要依赖谷歌作为唯一的事实来源。<ol><li>该信任谁？<ol><li> <a href="https://www.movingscam.com">https://www.movi​​ngscam.com</a> ：在创建此网站之前被骗的人。具体来说，这个<a href="https://www.movingscam.com/superlist">超级列表</a>列出了不同州的优质搬家服务。</li><li> https: <a href="https://www.movingauthority.com/largest-moving-companies/">//www.movi​​ngauthority.com/largest-moving-companies/</a> ：公司越大，他们就越不愿意失去信誉。</li><li> <a href="https://ai.fmcsa.dot.gov/hhg">https://ai.fmcsa.dot.gov/hhg</a> ：查看搬家公司被起诉的次数。</li><li> <a href="https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist">https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist</a> ：检查保险是否合法。</li></ol></li></ol></li><li><strong>注意以下不良迹象</strong>：<ol><li>他们不会估计多少重量和多少立方英尺 (CF)：我采访过的一位经纪人 iMoving，甚至没有询问我的移动重量详细信息。相比之下，搬家公司会与您进行虚拟参观或亲自参观，以便在给出报价之前进行估算。</li><li>代理不耐烦、粗鲁或给您打电话太频繁：再说一遍，iMoving，代理埃文很粗鲁，让我在整个通话过程中感到不舒服。许多经纪人实在太麻烦了，每天都会给您打电话/发电子邮件。相比之下，我谈过的大多数搬家工人都很耐心、专业、友善，并且站在你的角度思考。</li><li>好得令人难以置信：非常低的价格或非常快的运输。代理人可以向你承诺任何事情让你存钱，然后规则就可以改变。</li><li>先付款，后服务：一般情况下搬家公司装完东西后才可以付款。不要太早付款。</li><li>来自佛罗里达州的电话：许多经纪人在那里注册。</li></ol></li><li><strong>谈判</strong>：要求超过 5 个报价并利用它们相互竞争。最后我成功的把价格从5k谈到了3.2k。您只需向对方发送有关竞争报价的电子邮件，他们就会通过给您更大的折扣来降低价格，或者删除一些您一开始不知道的不必要的服务。<ol><li>你怎么知道价格合理？这一切都取决于里程、重量和 CF。仅供参考，我移动的距离约为 850 英里、2000 磅和 310 CF。由于我从经纪人那里收到的最低价格是 2.5k，这绝对太有吸引力了，令人难以置信，所以我有 80% 的信心我收到的价格是合理的，我需要付出很大的努力才能低于 3k。此外，支付合理的金额让我感觉更安全，否则，我可能会担心我将得到的服务。</li></ol></li><li><strong>准备一个电子邮件模板并随着时间的推移对其进行完善</strong>：列出您的地址、搬家日期（更灵活、价格更低）和搬家详细信息（从其他公司估算的重量和 CF）并询问他们的自付费用。无需与代理一一交谈，这样可以使搜索更加高效。不过，预计至少会与代理商打一次电话来进行估算。当您获得越来越多的信息时，请及时更新电子邮件。</li></ol><p><br>如果我需要再次找搬家公司，我会做以下事情：</p><ol><li>与 1 个大型搬家公司交谈以获得估价。</li><li>使用步骤 1 中的详细信息准备电子邮件模板，并与其他 5 个搬家公司核实。</li><li>谈判。</li></ol><p>最终可能需要大约 3 小时才能找到好的报价，而不是我在这里大约 9 小时的旅程:)</p><br/><br/> <a href="https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service<guid ispermalink="false"> GgezTQnwqxPzA2yNS</guid><dc:creator><![CDATA[Ziyue Wang]]></dc:creator><pubDate> Sat, 21 Oct 2023 08:10:31 GMT</pubDate> </item><item><title><![CDATA[Apply for MATS Winter 2023-24!]]></title><description><![CDATA[Published on October 21, 2023 2:27 AM GMT<br/><br/><p> 2023-24 年冬季<a href="https://www.matsprogram.org/home">MATS</a> （以前称为 SERI MATS）的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。导师包括<a href="https://www.lesswrong.com/users/rhaps0dy">Adrià Garriga Alonso</a> 、 <a href="https://www.alignmentforum.org/users/scasper?from=post_header">Stephen Casper</a> 、 <a href="https://www.lesswrong.com/users/jesseclifton">Jesse Clifton</a> 、 <a href="https://www.alignmentforum.org/users/davidad?from=post_header">David &#39;davidad&#39; Dalrymple</a> 、 <a href="https://www.alignmentforum.org/users/owain_evans">Owain Evans</a> 、 <a href="https://www.alignmentforum.org/users/evhub">Evan Hubinger</a> 、 <a href="https://www.alignmentforum.org/users/erik-jenner">Erik Jenner</a> 、 <a href="https://www.lesswrong.com/users/landfish">Jeffrey Ladish</a> 、 <a href="https://www.alignmentforum.org/users/neel-nanda-1">Neel Nanda</a> 、 <a href="https://www.alignmentforum.org/users/ethan-perez">Ethan Perez</a> 、 <a href="https://www.alignmentforum.org/users/lee_sharkey">Lee Sharkey</a> 、 <a href="https://www.alignmentforum.org/users/buck">Buck Shlegeris</a> 、 <a href="https://www.alignmentforum.org/users/turntrout">Alex Turner</a>和<a href="https://www.alignmentforum.org/users/sbowman">Sam Bowman</a>的研究人员<a href="https://wp.nyu.edu/arg/">纽约大学联盟研究小组</a>，包括<a href="https://homepages.inf.ed.ac.uk/s1302760/">Asa Cooper Stickland</a> 、 <a href="https://www.linkedin.com/in/idavidrein">David Rein</a> 、 <a href="https://julianmichael.org/">Julian Michael</a>和<a href="https://ihsgnef.github.io/">Shi Feng</a> 。</p><p>大多数导师的提交截止日期为 11 月 17 日（Neel Nanda 的提交截止日期为 11 月 10 日）。许多导师会提出具有挑战性的候选人选择问题，因此请确保您有足够的时间来完成您的申请。我们鼓励潜在的申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>我们很可能会在下周内添加多名导师。填写链接的兴趣表格或重新访问这篇文章以获取更新。</p><p>我们目前资金有限并接受捐赠以支持学者的进一步研究。如果您想支持我们的工作，<a href="https://manifund.org/projects/mats-funding">可以在这里捐款</a>！</p><h1>计划详情</h1><p>MATS 是位于加利福尼亚州伯克利的一个教育研讨会和独立研究项目（每周 40 小时），旨在为<a href="https://en.wikipedia.org/wiki/AI_alignment">人工智能对齐</a>领域的才华横溢的学者提供讲座、研讨会和研究指导，并将他们与伯克利人工智能安全研究社区联系起来。 MATS 为学者提供位于加利福尼亚州伯克利的住宿以及旅行支持、联合办公空间和同行社区。 MATS 的主要目标是帮助学者发展为人工智能安全研究人员。您可以<a href="https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change">在这里</a>阅读有关我们变革理论的更多信息。</p><p>根据个人情况，我们可能愿意改变项目的时间投入，并安排学者提前离开或开始。申请时请告诉我们您的空闲时间。我们的 MATS 2023-24 冬季计划的暂定时间表如下。</p><p>完成培训和研究阶段的学者将获得<a href="https://www.aisafetysupport.org/">AI 安全支持</a>提供的 1.2 万美元津贴。</p><h2>应用程序（现在！）</h2><p><strong>开放申请</strong>：10 月 20 日</p><p><strong>申请截止日期</strong>：11 月 17 日</p><p><i>注：Neel Nanda 的申请者将遵循修改后的时间表；请参阅下面的部分。</i></p><h2>培训阶段（1月8日-1月21日）</h2><p>被录取的申请人通常应在 MATS 计划开始之前完成<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础调整课程</a>或类似课程/</p><p> MATS 从两周的培训阶段开始。为了让学者们对人工智能安全领域有更广泛的了解，培训阶段设有先进的人工智能安全研究课程、导师特定的阅读清单、讨论小组等。</p><h2>研究阶段（1月22日-3月15日）</h2><p> MATS 的核心是为期两个月的研究阶段。在此阶段，每位学者每周至少花一小时与导师一起工作，并通过 Slack 进行更频繁的沟通。导师在以下方面差异很大：</p><ul><li>对项目选择的影响</li><li>关注低层细节与高层策略</li><li>强调输出与过程</li><li>参加会议的时间</li></ul><p>我们的学者支持团队通过提供专门的 1-1 签到、研究指导、调试和一般行政帮助来补充导师，以畅通研究进展并加速研究人员的发展。</p><p>教育研讨会和讲习班每周将举行2-3次。我们还组织了多次社交活动，让学者们认识伯克利人工智能安全社区的研究人员。</p><h3>研究里程碑</h3><p>学者们在研究阶段完成了两个里程碑。第一个是<a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit#heading=h.ozvm8udh0z0y">学者研究计划，</a>概述了威胁模型或风险因素、变革理论以及研究计划。这份文件将指导他们在该计划剩余时间内的工作，最终将举行由伯克利人工智能安全社区成员参加的研究研讨会。第二个里程碑是本次活动中的十分钟<a href="https://docs.google.com/document/d/1SB_vl6pGuSO7qidkZ8fIKPeH7fZIC8mvXzxicFjGyrE/edit#heading=h.ozvm8udh0z0y">研究报告</a>。</p><h3> MATS 社区</h3><p>研究阶段为学者提供了一个同行社区，他们共享办公室、膳食和住房。与远程进行独立研究相比，在社区工作可以让学者轻松接触未来的合作者，更深入地了解其他研究议程，并在人工智能安全社区中建立社交网络。学者们还可以获得全职社区经理的支持。</p><p>在我们的夏季队列中，研究阶段的每周至少包括一次社交活动，例如聚会、游戏之夜、电影之夜或徒步旅行。每周的闪电演讲为学者们提供了在非正式、低风险的环境中分享他们的研究兴趣的机会。工作之余，学者们组织社交活动，包括前往约塞米蒂国家公园的公路旅行、参观旧金山、酒吧郊游、周末聚餐，甚至跳伞旅行。</p><h2>延长阶段（4月1日至7月26日）</h2><p>研究阶段结束时，学者可以申请在为期四个月的扩展阶段队列中继续研究，默认在伦敦。录取决定很大程度上取决于获得导师的认可和获得外部资金。到这个阶段，我们期望学者能够高度自主地开展研究。</p><h2>后垫</h2><p>完成该计划后，MATS 校友拥有：</p><ul><li>被<a href="https://www.anthropic.com/">Anthropic</a> 、 <a href="https://openai.com/">OpenAI</a> 、 <a href="https://www.deepmind.com/">Google DeepMind</a> 、 <a href="https://intelligence.org/">MIRI</a> 、 <a href="https://www.alignment.org/">ARC</a> 、 <a href="https://www.conjecture.dev/">Conjecture</a>等领先组织以及美国政府聘用，并加入<a href="https://humancompatible.ai/">UC Berkeley CHAI</a> 、 <a href="https://wp.nyu.edu/arg/">NYU ARG</a> 、 <a href="https://space.mit.edu/home/tegmark/technical.html">MIT Tegmark Group</a>等学术研究团体；</li><li>创立AI安全组织，包括<a href="https://www.arena.education/">ARENA</a> 、 <a href="https://www.apolloresearch.ai/">Apollo Research</a> 、 <a href="https://www.leap-labs.com/">Leap Labs</a> 、 <a href="https://timaeus.co/">Timaeu​​s</a> 、 <a href="https://cadenzalabs.org/">Cadenza Labs</a> 、 <a href="https://www.aipolicy.us/">Center for AI Policy</a> 、 <a href="https://www.catalyze-impact.org/">Catalyze Impact</a> 、 <a href="https://stakeout.ai/">Stake Out AI</a> ；</li><li>在<a href="https://funds.effectivealtruism.org/funds/far-future">长期未来基金</a>、<a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">开放慈善事业</a>或<a href="https://manifund.org/">Manifund 的</a>资助下进行<a href="https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency">独立研究</a>。</li></ul><p>您可以<a href="https://www.matsprogram.org/alumni">在此处</a>阅读有关 MATS 校友的更多信息。</p><h2> Neel Nanda 的具体信息</h2><h3>应用领域</h3><p><strong>申请截止日期</strong>：11 月 10 日</p><p><strong>录取决定</strong>：11 月 17 日</p><h3>培训阶段（11月20日-12月22日）</h3><p> Neel Nanda 的学者将完成远程培训阶段，包括两周学习机甲解释和两周成对进行研究冲刺 - 请参阅上一个项目的<a href="https://docs.google.com/document/d/18qYhY6FB0AiVP9cNr9idP5YYkN_654WNVgfyL9LgfHA/edit">概述文档</a>以获取更多信息。</p><p>最初的培训阶段将比随后的研究阶段提供更多的录取通知书（上次有 19 名学者完成了此阶段，9 名继续进入研究阶段），但过去未取得进展的学者仍然认为此阶段是一个很好的介绍机械解释研究。是否继续提供将主要取决于研究冲刺的表现。</p><p> Neel 的学者将因参与培训阶段而从<a href="https://www.aisafetysupport.org/">AI Safety Support</a>获得 4800 美元的津贴。</p><h3>研究阶段（1月8日-3月15日）</h3><p>继续进入研究阶段的人员将于 1 月 8 日在伯克利与其他学者一起开始。对于尼尔的学者来说，这将被视为研究阶段的开始。</p><h1>谁应该申请？</h1><p>我们理想的申请人具有：</p><ul><li>了解人工智能安全研究领域，相当于完成了<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础知识调整课程</a>（如果您被该计划录取，但之前尚未完成本课程，则应在培训阶段开始之前完成）；</li><li>具有技术研究经验（例如机器学习、计算机科学、数学、物理、神经科学等），通常为研究生水平；和</li><li>从事人工智能安全研究事业的强烈动机。</li></ul><p>即使您不符合所有这些标准，<strong>我们也鼓励您申请</strong>！几位过去的学者在没有强烈期望的情况下提出申请并被接受。</p><h2>从美国境外申请</h2><p>美国境外的学者可以在研究阶段申请<a href="https://www.uscis.gov/working-in-the-united-states/temporary-visitors-for-business/b-1-temporary-business-visitor">B-1 签证</a>（更多信息请<a href="https://travel.state.gov/content/dam/visas/BusinessVisa%20Purpose%20Listings%20March%202014%20flier.pdf">点击此处</a>）。来自<a href="https://travel.state.gov/content/travel/en/us-visas/tourism-visit/visa-waiver-program.html">免签证计划 (VWP) 指定国家/地区的</a>学者可以通过<a href="https://esta.cbp.dhs.gov/esta">旅行授权电子系统 (ESTA)</a>向 VWP 提出申请，该系统将在三天内处理完毕。获得 B-1 签证的学者可以在美国停留最多 180 天，而获得 VWP 签证的学者可以在美国停留最多 90 天。请注意，B-1 签证批准时间可能比 ESTA 批准时间长得多，具体取决于您的原籍国。</p><h1>如何申请</h1><p><a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。大多数导师的提交截止日期为 11 月 17 日。我们鼓励潜在申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>候选人申请在特定导师的指导下工作，导师将审查他们的申请。申请的评估主要基于<strong>对导师问题的回答</strong>和<strong>先前的相关研究经验</strong>。有关我们导师的研究议程的信息可以在<a href="https://www.matsprogram.org/mentors">MATS 网站</a>上找到。</p><p>在申请之前，您应该：</p><ul><li>仔细阅读每个类别的描述和议程以及相关的候选人选择问题；</li><li>准备好您有兴趣申请的流派问题的答案。这些问题可以在申请表上找到；</li><li>准备您的 LinkedIn 或简历。</li></ul><p>候选人选择问题可能相当困难，具体取决于导师！确保您有足够的时间来完成您的申请。对一名导师的强力申请可能比对多名导师的中等申请具有更高的价值（尽管每份申请都将被独立评估）。</p><p>请注意，申请比乍一看要长，因为在您选择导师之前，特定于导师的问题是隐藏的。</p><h2>申请办公时间</h2><p>我们为潜在申请人提供办公时间，以澄清有关 MATS 计划申请流程的问题。在参加办公时间之前，我们要求申请人完整阅读当前帖子和我们的<a href="https://www.matsprogram.org/faqs">常见问题解答</a>。</p><p>我们的办公时间将在以下<a href="https://zoom.us/j/98715408709">Zoom 链接</a>上进行：</p><ul><li>太平洋时间 10 月 25 日星期三中午 12 点至下午 2 点。</li><li>太平洋时间 11 月 1 日星期三中午 12 点至下午 2 点。</li></ul><p>您可以使用<a href="https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=bnUxbmVxNDI1Z2d2OGxjY2Y0cTlvbm1lcWNfMjAyMzEwMjVUMTkwMDAwWiBjXzQzNmY2NzkzODEzMzk0ZmE5YjUwYjk1ZmMwOWY5Mzc4MDM1YjRiNmIyM2UxYjc4ODI5ZGQ1Y2U5ZGZkZDFkYTJAZw&amp;tmsrc=c_436f6793813394fa9b50b95fc09f9378035b4b6b23e1b78829dd5ce9dfdd1da2%40group.calendar.google.com&amp;scp=ALL">此链接</a>将这些办公时间添加到 Google 日历。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24<guid ispermalink="false"> tqyg3DpoiE4DKyi4y</guid><dc:creator><![CDATA[Rocket]]></dc:creator><pubDate> Sat, 21 Oct 2023 02:31:32 GMT</pubDate> </item><item><title><![CDATA[Muddling Along Is More Likely Than Dystopia]]></title><description><![CDATA[Published on October 20, 2023 9:25 PM GMT<br/><br/><p><strong>摘要：</strong>历史上有先例，禁令或严厉的法规阻止了某个行业的技术进步，而社会其他领域的技术进步仍在继续。这是人工智能的美好未来。</p><p><i>认知状态：我的直觉与这里其他人强烈不同。我希望解释我的直觉，并提供足够的历史证据来使这个直觉至少可信。</i></p><p></p><h3>简介：直觉泵</h3><p>假设您在 1978 年告诉某人，美国在 2023 年之前不会建造新的核电站<span class="footnote-reference" role="doc-noteref" id="fnrefdotwicb0t0t"><sup><a href="#fndotwicb0t0t">。 [1]</a></sup></span>这可能会非常令人惊讶。核电被认为是未来的动力。 <span class="footnote-reference" role="doc-noteref" id="fnref84mva9qulhn"><sup><a href="#fn84mva9qulhn">[2]</a></sup></span>核管理委员会三年前才成立。</p><p>有了这些信息，1978 年的某人可能会预测将会发生可怕的事情。也许美国和苏联之间会爆发核战争，摧毁美国的工业能力。也许由于人口过剩或全球变暖而导致经济崩溃。也许是<i>1984 年</i>的奥威尔式警察国家，或者是旨在监管失控核武器的世界权威。 <span class="footnote-reference" role="doc-noteref" id="fnref3czpkcc0eq9"><sup><a href="#fn3czpkcc0eq9">[3]</a></sup></span></p><p>这一切都没有发生。相反，核管理委员会加大了监管力度<span class="footnote-reference" role="doc-noteref" id="fnrefualznx45d8m"><sup><a href="#fnualznx45d8m">[4]</a></sup></span> ，直到建设新核电站变得不经济。这些法规仅适用于美国，但它们似乎对全球核电研究产生了重大影响。正在建设新核电站的国家仍在使用 1970 年之前开发的设计<span class="footnote-reference" role="doc-noteref" id="fnrefuldhmpk9bb"><sup><a href="#fnuldhmpk9bb">。 [5]</a></sup></span></p><p>与反事实相比，对核电的监管可能确实减缓了美国未来 45 年的经济增长。 <span class="footnote-reference" role="doc-noteref" id="fnrefpn3mqa4k7d"><sup><a href="#fnpn3mqa4k7d">[6]</a></sup></span>但过去 45 年并不是灾难性的。在其他行业的推动下，经济增长和创新确实在继续。</p><p></p><h3>停止 AGI 的后果？</h3><p>一些参与关于减缓或暂停人工智能的争论的人似乎认为，从长远来看，成功地阻止人工智能的进步可能会导致死亡或反乌托邦：</p><blockquote><p>要么我们弄清楚如何让通用人工智能顺利运行，要么我们等待小行星撞击。</p><p> - 萨姆·奥特曼<span class="footnote-reference" role="doc-noteref" id="fnrefm67279lgo3j"><sup><a href="#fnm67279lgo3j">[7]</a></sup></span></p></blockquote><p></p><blockquote><p>如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。</p><p> - 斯科特·亚历山大<span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span></p></blockquote><p></p><blockquote><p>我认为我们应该非常担心，全球政府需要执行这样的禁令，这将大大增加永久暴政的风险，这本身就是一场生存灾难。</p><p> - 诺拉·贝尔罗斯<span class="footnote-reference" role="doc-noteref" id="fnrefpbz6qf6ayr9"><sup><a href="#fnpbz6qf6ayr9">[9]</a></sup></span></p></blockquote><p></p><blockquote><p>看来我们需要建立一个全球性的警察国家，否则从长远来看，[无限期的人工智能暂停]将会失败。</p><p> - 马修·巴尼特<span class="footnote-reference" role="doc-noteref" id="fnref9k25eq052e"><sup><a href="#fn9k25eq052e">[10]</a></sup></span></p></blockquote><p>我觉得这和我们假设的 1978 年的人犯的错误是一样的。看起来人工智能在未来将是一件极其重要的事情，因此必须发生一些戏剧性的事情才能阻止它的发生。我认为，我们应该把更多的可能性放在无聊的未来上，监管会扼杀这一领域，而社会的其他领域则继续像以前一样。</p><p>这似乎是一个重要的分歧。如果你认为我们的后代的生活会相当好，而且会变得更好（如果不是快得难以想象的话），那么停止人工智能的进步对他们来说可能是值得的。如果你认为我们的后代的未来将是“短暂而严峻的” <span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span> ，那么在决定现在是否要冒这个风险时，他们可能就不会被考虑在内。</p><p></p><h3>具体问题</h3><p>斯科特·亚历山大提到了几个具体的担忧，这些担忧使他对人工智能没有进步的未来感到悲观。每个问题似乎都是人们现在和将来应该努力解决的现实问题。我们应该改善生物安全， <span class="footnote-reference" role="doc-noteref" id="fnrefzlf149enf2a"><sup><a href="#fnzlf149enf2a">[11]</a></sup></span>促进经济增长， <span class="footnote-reference" role="doc-noteref" id="fnrefu6cbijtsn58"><sup><a href="#fnu6cbijtsn58">[12]</a></sup></span>传播民主和自由， <span class="footnote-reference" role="doc-noteref" id="fnrefxsb44grlqzh"><sup><a href="#fnxsb44grlqzh">[13]</a></sup></span>并给人们的子孙后代带来希望。但这些事情在未来 100 年内导致死亡或反乌托邦的可能性似乎都不接近 50%。为了避免这些问题，不值得接受人工智能带来的生存风险，斯科特·亚历山大估计人工智能有约 20% 的机会导致人类灭绝。</p><p>诺拉·贝尔罗斯 (Nora Belrose) 和马修·巴尼特 (Matthew Barnett) 都担心，需要一个全球警察国家来执行对人工智能进步的长期禁令。这种立场在人工智能安全社区中似乎并不罕见。人们担心研究可能会转移到监管较少的地方，而算法的进步将使通用人工智能在个人计算机上成为可能。避免通用人工智能的唯一方法是大规模扩张全球政府权力。</p><p></p><h3>其他历史例子</h3><p>我认为其他技术还没有实现这些担忧。</p><p>一个行业的法规不会阻止所有其他行业的进步。湾区的人们可能低估了人工智能以外的新兴技术的重要性，或者更广泛地说软件的重要性，因为信息技术在当地经济中的重要性不成比例。 <span class="footnote-reference" role="doc-noteref" id="fnrefq6trr5y0v1f"><sup><a href="#fnq6trr5y0v1f">[14]</a></sup></span>我同样预计 1950 年生活在底特律的人们会低估汽车以外的新兴技术的重要性。如果没有人工智能，仍然可以取得很多进展。我特别兴奋的两项新兴技术是核聚变和太空殖民。</p><p>一个国家的法规可能会阻碍单一行业的进步。某个特定行业的进展停滞并不罕见。 <span class="footnote-reference" role="doc-noteref" id="fnref0bq1vg5uljy"><sup><a href="#fn0bq1vg5uljy">[15]</a></sup></span>特定行业的大多数创新都是在一个或几个城市进行的。这些创新集群很难建立和维护，因此，如果一个创新集群被监管压垮，它通常不会转移到另一个国家。从更广泛的角度来看，一些国家比其他国家更具创新性。在大多数行业，包括受到严格监管的行业，美国显然比欧洲或东亚（大部分）更具创新性，而欧洲或东亚又比世界其他地区更具创新性。很多事情都必须向好的方向发展：高生活水平、受过教育的民众、法治、未来利润的可能性、可用资本以及鼓励创新的文化。标榜国际法规或规范的国家通常不会吸引创新。一旦一项技术存在，其他国家就更容易复制它。所需的设计和技能已经存在，并且该技术的好处是显而易见的。防止创新的监管比防止扩散的监管容易得多。</p><p>我之前调查过一些可抵抗的技术诱惑， <span class="footnote-reference" role="doc-noteref" id="fnref14m5yvhuzdj"><sup><a href="#fn14m5yvhuzdj">[16]</a></sup></span>或通过我们当前的机构实现了长期暂停的技术：</p><ul><li><strong>核电</strong>，如上所述。 <span class="footnote-reference" role="doc-noteref" id="fnreffb9jpk8y0qa"><sup><a href="#fnfb9jpk8y0qa">[17]</a></sup></span></li><li><strong>地球工程</strong>并不是明确违法的，但科学家和活动人士的反对甚至阻止了研究的进行。 <span class="footnote-reference" role="doc-noteref" id="fnrefp6r8lh3a1ta"><sup><a href="#fnp6r8lh3a1ta">[18]</a></sup></span></li><li><strong>疫苗开发</strong>在西方国家受到严格监管。在最近的大流行期间，俄罗斯和中国都放松了一些限制，并在西方之前批准了疫苗。由此产生的疫苗效果较差，因为最好的医学研究仍然位于西方。特别是，<strong>人体挑战试验</strong>已被监管为几乎不存在。 <span class="footnote-reference" role="doc-noteref" id="fnref1rfwwl0q1g"><sup><a href="#fn1rfwwl0q1g">[19]</a></sup></span></li><li><strong>核武器</strong>有时被认为是一种监管未能阻止其扩散的技术。我认为这个例子的证据是混杂的。大约有 10 个国家拥有核武器（比拥有核武器的数量少得多），但只有 2 个国家独立开发核武器：美国和法国。尖端研究尚未转移到非《不扩散核武器条约》缔约国的国家。印度、巴基斯坦和朝鲜似乎拥有与 20 世纪 40 年代和 50 年代的美国或苏联类似的能力。 <span class="footnote-reference" role="doc-noteref" id="fnrefg6e7iucwc7"><sup><a href="#fng6e7iucwc7">[20]</a></sup></span>试验禁令可能也有助于防止核武器变得越来越强大：有史以来威力最大的炸弹于 1961 年引爆。</li><li><strong>生物武器</strong>有一些最严格的条约和禁止其开发或使用的禁忌， <span class="footnote-reference" role="doc-noteref" id="fnref9kg6yxtjycb"><sup><a href="#fn9kg6yxtjycb">[21]</a></sup></span>并且没有一个国家公开拥有生物武器计划。这个例子的一个问题是苏联签署了禁止开发生物武器的条约 - 然后继续开发它们。</li><li>各种核技术，如<strong>原子园艺</strong>、<strong>在建筑中使用核爆炸</strong>或<strong>猎户座计划</strong>，已经被提出但尚未开发。</li><li><strong>克隆最有效的士兵</strong>从未被实现过。</li><li>中国的一名研究人员在被捕之前已经对<strong>人类进行了基因改造</strong>。</li><li>目前还不清楚<strong>殖民主义</strong>是否算作一种技术。明朝在 1400 年代初决定停止其宝船舰队，使全球殖民主义推迟了约 50 年，并可能影响了中国几个世纪的发展轨迹。</li><li><strong>贝尔实验室</strong>在 1945 年至 1980 年间发明或发现了晶体管、电荷耦合器件、光伏电池、信息论、Unix、C 和宇宙微波背景辐射。 <span class="footnote-reference" role="doc-noteref" id="fnref551akuqm31p"><sup><a href="#fn551akuqm31p">[22]</a></sup></span> 1982 年贝尔系统被反垄断法打破后，那里的研究界就分裂了。似乎有理由认为，有些技术之所以没有被发明，是因为这个异常多产的创新中心被摧毁了。</li></ul><p>大多数技术并未被禁止，其进步也没有受到监管的抑制。大多数技术也不像人工智能那么可怕：我很难想象太阳能电池板或圆珠笔如何构成 x 风险。听起来可怕的技术，如大规模杀伤性武器或某些类型的医学研究，经常面临禁令或法规，使它们的开发不再值得，而这些禁令有时会起作用。</p><p>我不想说对听起来可怕的技术的有效禁令是默认发生的。当他们工作时，他们是共同努力的结果。但在不扰乱社会其他部分的情况下，禁止一项具有潜在危险的新技术似乎是非常可行的。</p><p></p><h3>也许人工智能会有所不同</h3><p>虽然这篇文章主要是关于其他技术被停止的历史先例，但似乎值得特别谈谈人工智能。人工智能与其他技术不同的原因有几个：</p><ol><li>人工智能研究比其他新兴技术更容易远程进行。</li><li>人工智能系统一旦创建，就可以作为软件轻松传输。</li><li>简单的经济模型表明，强大的人工智能对于采用它的人来说都将带来极大的经济优势。</li></ol><p>所有技术都是不同的。有些差异使监管变得更容易或更困难，但这些差异都没有大到使监管变得不可能的程度：</p><ol><li>执法部门可以根据研究进行的地点或研究人员居住的地点来执行法律。美国尤其对其法律适用范围有着广泛的看法。 <span class="footnote-reference" role="doc-noteref" id="fnrefqnyaglfoig"><sup><a href="#fnqnyaglfoig">[23]</a></sup></span></li><li>大多数拟议的法规都集中在训练强大的人工智能所需的硬件上。</li><li>政策制定者并不知道这一点。他们知道有人在告诉他们这些。他们肯定不知道，如果他们支持这个特定的项目，他们将在他们关心的时间范围内获得通用人工智能的经济承诺。这些承诺与其他技术的炒作并没有太大区别。 <span class="footnote-reference" role="doc-noteref" id="fnrefv96bbayqvig"><sup><a href="#fnv96bbayqvig">[24]</a></sup></span></li></ol><p>还有一些方法可以更轻松地监管人工智能。</p><p>供应链有多个阶段，世界上只有一家或几家公司能够从事尖端工作。只需要少数参与者进行协调即可使监管发挥作用。</p><p>当前领先的人工智能模型需要大量计算，这是资本密集型且易于跟踪的。随着算法效率的足够改进，这种情况可能会改变。但我们应该预计，随着资本和人才转移到其他行业，算法的进步将急剧放缓，以应对人工智能的长期停滞。</p><p>许多物质和物品都受到监管，并且该监管的细节根据其内容和政府试图避免的内容而有很大差异。 <span class="footnote-reference" role="doc-noteref" id="fnreffp5yyyzqajs"><sup><a href="#fnfp5yyyzqajs">[25]</a></sup></span>监管 GPU 将面临一些独特的挑战，但在我们当前的机构下似乎并非不可能。</p><p></p><h3>暂停多长时间？</h3><p>大多数历史证据表明全球停顿已经持续了大约 50 年。这是讨论 100 年暂停的有用证据。如果“长期”意味着一千年，那么历史证据就少得多。马修·巴尼特 (Matthew Barnett) 认为，现有机构内的监管棘轮可能会导致人工智能研究暂停 50 年，但要实现 1000 年的暂停，就需要采取更戏剧性的措施。</p><p>我怀疑全球警察国家是否会比更正常的监管更容易维持一千年。我关于如何在这个时间范围内维持机构的模型是：</p><ol><li>建立一个持续一代人的机构。</li><li>让下一代相信维持这个机构是一件好事。</li></ol><p>如果你在（2）中失败，那么建立什么机构并不重要。如果连精英阶层都不相信警察国家是好事，那么它就无法维持下去。 <span class="footnote-reference" role="doc-noteref" id="fnrefwx71dbbygd"><sup><a href="#fnwx71dbbygd">[26]</a></sup></span>一个机构如果硬实力较少，但更善于让人们相信它，那么它更有可能持续一千年。</p><p></p><h3>结论</h3><p>构建 AGI 是一项极其不确定的工作。它可能会带来我们光荣的未来。它可能会导致人类灭绝。这甚至可能是不可能的。如果我们决定不尝试构建通用人工智能，那么未来的不确定性似乎就会少得多。社会显然仍将不是最优的，但也远非反乌托邦。取得科学、技术、经济、社会和政治进步将继续困难重重，但人们将继续这样做。我们可以继续希望我们的孩子，以及他们的孩子，在未来的很长一段时间里，至少能有微小的改善。</p><p>如果一项听起来很可怕的技术面临监管棘轮，从而减缓甚至停止该领域的所有进展，那也不足为奇。这不是死亡或反乌托邦——这是正常的。<br></p><p><i>感谢 Aaron Scher、Matthew Barnett、Rose Hadshar、Harlan Stewart 和 Rick Korzekwa 就该主题进行的有益讨论。</i></p><p> <i>Theen Moy 的预览图像：</i> <a href="https://www.flickr.com/photos/theenmoy/8003177753"><i>https://www.flickr.com/photos/theenmoy/8003177753</i></a> <i>。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndotwicb0t0t"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdotwicb0t0t">^</a></strong></sup></span><div class="footnote-content"><p>这不太公平，因为日期范围从一个工厂（Shearon Harris）的建设开始一直延伸到另一个工厂（Vogtle Unit 3）的建设结束。沃格特尔3号机组于2013年开始建设。还有一座核电站（瓦茨巴2号机组）于1973年开始建设，2016年竣工。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn84mva9qulhn"> <span class="footnote-back-link"><sup><strong><a href="#fnref84mva9qulhn">^</a></strong></sup></span><div class="footnote-content"><p> 1973年，原子能委员会预测，到2000年，美国55.8%的电力将来自核电，这一数字低于其此前的预测。但这并没有发生：自 20 世纪 80 年代末以来，核电已占美国电力的 20% 左右。</p><p>安东尼·里普利. <i>AEC 降低了原子能增长预期。</i>纽约时报。 （1973）<a href="https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html"><u>https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3czpkcc0eq9"> <span class="footnote-back-link"><sup><strong><a href="#fnref3czpkcc0eq9">^</a></strong></sup></span><div class="footnote-content"><p>包括伯特兰·罗素在内的一些知名人士主张建立一个世界权威，以防止核武器带来的生存风险：</p><blockquote><p>确保世界和平的一个更理想的方式是各国之间自愿达成协议，集中其武装部队并服从商定的国际机构。目前看来，这似乎是一个遥远而乌托邦的前景，但也有一些务实的政治家不这么认为。一个世界权威如果要履行其职能，就必须拥有立法机构、行政机构和不可抗拒的军事力量。所有国家都必须同意将国家武装部队削减至内部警察行动所需的水平。不应允许任何国家保留核武器或任何其他大规模销毁手段。 ……在一个各个国家都解除武装的世界里，世界权威机构的军事力量不需要很大，也不会对各个组成国家构成沉重的负担。</p></blockquote><p>伯特兰·罗素.<i>人有未来吗？</i> (1961) 引自全球治理论坛。 （2023 年 10 月 17 日访问） <a href="https://globalgovernanceforum.org/visionary/bertrand-russell/"><u>https://globalgovernanceforum.org/visionary/bertrand-russell/</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnualznx45d8m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefualznx45d8m">^</a></strong></sup></span><div class="footnote-content"><p>马克·R·李。<i>监管棘轮：为什么监管会引发监管。</i>辛辛那提大学法律评论<strong>87.3</strong> 。 （2019） <a href="https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr"><u>https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuldhmpk9bb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuldhmpk9bb">^</a></strong></sup></span><div class="footnote-content"><p>例如，核电站的一种“新”设计是熔盐反应堆。目前已有一个：TMSR-LF1，这是一座位于中国西北部的实验反应堆，可产生 2 兆瓦的火力发电。该设计基于熔盐反应堆实验（MSRE），该实验于 1965 年至 1969 年在美国橡树岭国家实验室产生了 7 MW 的热电。</p><p>同样，中国也有一个小型模块化反应堆，即 HTR-PM，于 2021 年开始发电。它是一个球床反应堆，以德国示范反应堆 (AVR) 为基础，该反应堆于 1967 年至 1988 年运行。</p><p>所有其他核电站都使用更古老的反应堆类型。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpn3mqa4k7d"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpn3mqa4k7d">^</a></strong></sup></span><div class="footnote-content"><p>我之前曾估计过美国核电成本过高而损失的直接价值。我还预计，由于电力价格便宜，将会产生额外的间接价值。</p><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm67279lgo3j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm67279lgo3j">^</a></strong></sup></span><div class="footnote-content"><p>萨姆·奥特曼.推特。 (2022) <a href="https://twitter.com/sama/status/1540781762241974274?lang=en">https://twitter.com/sama/status/1540781762241974274?lang=en</a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3lk2fdp52yu"> <span class="footnote-back-link"><sup><strong><a href="#fnref3lk2fdp52yu">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>其次，如果我们永远得不到人工智能，我预计未来将是短暂而严峻的。我们很可能用合成生物学自杀。否则，技术和经济停滞、极权主义抬头+非自由主义+暴民统治、生育率崩溃和基因不良等综合因素将使世界陷入贫困，并加速其制度质量的衰退。我不会花太多时间担心这些问题，因为我认为它们需要几代人的时间才能达到危机水平，而且我预计技术会在那之前扭转游戏规则。但如果我们禁止所有游戏板翻转技术（我所知道的唯一另一种技术是基因增强，这甚至更应该禁止），那么我们最终会导致生物武器灾难或社会崩溃。我之前说过，我认为人工智能毁灭世界的可能性约为 20%。但如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。这并不意味着我必须支持人工智能加速主义，因为 20% 小于 50%。短暂的、精心设计的暂停可以大大提高人工智能顺利进行的机会，同时又不会增加太多社会崩溃的风险。但这是我心里的事。</p></blockquote><p>斯科特·亚历山大.<i>暂停思考：人工智能暂停辩论。</i>星体法典十。 （2023） <a href="https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate"><u>https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpbz6qf6ayr9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpbz6qf6ayr9">^</a></strong></sup></span><div class="footnote-content"><p>诺拉·贝尔罗斯。<i>人工智能暂停可能会适得其反。</i> EA 论坛。 （2023） <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9k25eq052e"> <span class="footnote-back-link"><sup><strong><a href="#fnref9k25eq052e">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>请注意，我并不是说人工智能暂停倡导者必然直接倡导全球警察国家。相反，我认为，为了将无限期的暂停维持足够长的时间，我们似乎需要创建一个世界范围的警察国家，否则从长远来看，暂停将会失败。人们可以选择“硬着头皮”并倡导建立一个全球警察国家来回应这些论点，但我并不是说这是人工智能暂停倡导者的唯一选择。</p><p>硬着头皮提倡全球警察国家无限期暂停人工智能的一个原因是，即使你认为全球警察国家很糟糕，你也可能会认为全球人工智能灾难更糟糕。在人工智能灾难明显迫在眉睫的情况下，我实际上同意这种评估。</p><p>然而，虽然我并不教条地反对建立一个全球警察国家，但我仍然有一个启发式反对推动建立一个全球警察国家，并且认为通常需要强有力的证据来推翻这种启发式。我认为迄今为止，关于人工智能灾难的论点还没有达到这个门槛。灾难论点的主要现有论点似乎很抽象，并且脱离了有关真实人工智能系统行为的任何坚实的经验证据。</p></blockquote><p>马修·巴内特。 <i>AI无限期暂停的可能性。</i> EA 论坛。 (2023) <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzlf149enf2a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzlf149enf2a">^</a></strong></sup></span><div class="footnote-content"><p>托比·奥德 (Toby Ord) 估计<i>《悬崖</i>》下个世纪的生物安全 X 风险约为 1/30。生物安全社区在应对 X 风险方面似乎比人工智能安全社区更成功。大多数研究开展的国家已经制定了广泛的法规，并制定了反对开发生物武器的主要国际条约。如果你认为人工智能比合成生物学更危险，那么为了提高生物安全而推进人工智能就没有意义。甚至还不清楚日益强大的人工智能是否会使生物安全变得更好或更糟。</p><p>作为比较，托比·奥德估计下个世纪小行星撞击的 x 风险约为 1/1,000,000。我将萨姆·奥尔特曼对小行星的担忧视为所有其他存在风险的代表。否则，他的风险估计似乎相差许多数量级。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6cbijtsn58"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6cbijtsn58">^</a></strong></sup></span><div class="footnote-content"><p>我不认为我们已经耗尽了人类可实现的经济、技术或科学进步。即使没有通用人工智能，100 年后中位数可能会比现在富裕得多。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxsb44grlqzh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxsb44grlqzh">^</a></strong></sup></span><div class="footnote-content"><p>过去十年来大多数国家的政治和社会趋势似乎并不好。上个世纪大多数国家的政治和社会趋势看起来都很棒。在预测下个世纪时，我们应该同时考虑这两点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq6trr5y0v1f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq6trr5y0v1f">^</a></strong></sup></span><div class="footnote-content"><p>您预计信息行业占美国 GDP 的比例是多少？信息产业包括信息技术和传统媒体。</p><div class="spoilers"><p> 5.5%</p></div><p> <a href="https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm"><u>https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0bq1vg5uljy"> <span class="footnote-back-link"><sup><strong><a href="#fnref0bq1vg5uljy">^</a></strong></sup></span><div class="footnote-content"><p><i>特定技术停止的进展示例。</i>人工智能影响维基。 （2023 年 10 月 19 日访问） <a href="https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_particular_technology_stopping"><u>https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_pspecial_technology_stopping</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn14m5yvhuzdj"> <span class="footnote-back-link"><sup><strong><a href="#fnref14m5yvhuzdj">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑项目。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfb9jpk8y0qa"> <span class="footnote-back-link"><sup><strong><a href="#fnreffb9jpk8y0qa">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp6r8lh3a1ta"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp6r8lh3a1ta">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：地球工程。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1rfwwl0q1g"> <span class="footnote-back-link"><sup><strong><a href="#fnref1rfwwl0q1g">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：疫苗挑战试验。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng6e7iucwc7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg6e7iucwc7">^</a></strong></sup></span><div class="footnote-content"><p>我不知道以色列的核计划是什么样的，也不知道其中有多少是美国技术转让的结果，而不是本土创新的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9kg6yxtjycb"> <span class="footnote-back-link"><sup><strong><a href="#fnref9kg6yxtjycb">^</a></strong></sup></span><div class="footnote-content"><p> 《日内瓦议定书》（1925 年）禁止使用进攻性生物武器，《生物武器公约》（1972 年）禁止开发、生产、获取、转让、储存和使用生物武器。除了条约之外，生物武器的使用似乎也有重大禁忌。</p><p>米歇尔·本特利.<i>生物武器禁忌。</i>岩石上的战争。 （2023） <a href="https://warontherocks.com/2023/10/the-biological-weapons-taboo/">https://warontherocks.com/2023/10/the-biological-weapons-taboo/</a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn551akuqm31p"> <span class="footnote-back-link"><sup><strong><a href="#fnref551akuqm31p">^</a></strong></sup></span><div class="footnote-content"><p>尤利亚·乔治斯库.<i>带回贝尔实验室的黄金岁月。</i>自然评论物理<strong>4</strong> 。 (2022) 页。 76-78。 <a href="https://www.nature.com/articles/s42254-022-00426-6"><u>https://www.nature.com/articles/s42254-022-00426-6</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqnyaglfoig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqnyaglfoig">^</a></strong></sup></span><div class="footnote-content"><p>例如，萨姆·班克曼-弗里德 (Sam Bankman-Fried) 正在美国联邦法院接受审判，尽管他本人和他的公司已经搬到了巴哈马群岛。</p><p>另一个例子是美国司法部因腐败罪在瑞士逮捕了来自各国的国际足联官员。 “美国法律允许根据多项法规引渡和起诉外国人……在本案中，她说，国际足联官员利用美国银行系统作为其计划的一部分。”<br><br>斯蒂芬妮·克利福德和马特·阿普佐。<i>在起诉 14 名足球官员后，美国誓言结束国际足联腐败行为。</i>纽约时报。 （2015） <a href="https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html"><u>https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnv96bbayqvig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv96bbayqvig">^</a></strong></sup></span><div class="footnote-content"><p>例如，神剑计划承诺通过在发射时摧毁数十枚洲际弹道导弹（带有数百枚弹头）来消除苏联核武器的威胁。最终它是不可行的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfp5yyyzqajs"> <span class="footnote-back-link"><sup><strong><a href="#fnreffp5yyyzqajs">^</a></strong></sup></span><div class="footnote-content"><p><i>受监管事物的示例。</i>人工智能影响维基。 （2023 年 10 月 19 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things"><u>https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwx71dbbygd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwx71dbbygd">^</a></strong></sup></span><div class="footnote-content"><p>这是我对苏联发生的事情的过于简化的模型。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia<guid ispermalink="false"> pAnvMYd9mqDT97shk</guid><dc:creator><![CDATA[Jeffrey Heninger]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:25:17 GMT</pubDate> </item><item><title><![CDATA[What's Hard About The Shutdown Problem]]></title><description><![CDATA[Published on October 20, 2023 9:13 PM GMT<br/><br/><p>我一直与萨米·彼得森就他的<a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/?commentId=mmvwCeHzzojY9Gf2a"><u>“无懈可击的不完整偏好”</u></a>进行了一些<a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1?commentId=XpY4YxF4tFfGzNbbK">富有成效的</a>讨论（旨在补充<a href="https://s3.amazonaws.com/pf-user-files-01/u-242443/uploads/2023-05-02/m343uwh/The%20Shutdown%20Problem-%20Two%20Theorems%2C%20Incomplete%20Preferences%20as%20a%20Solution.pdf">艾略特·索恩利解决关机问题的方法</a>）。这篇文章是我迄今为止从讨论中得到的一些有用文章的（非正式）提炼。</p><h2>关机问题</h2><p>从概念上讲：我们希望为强大的人工智能配备一个关机按钮。如果按钮未被按下，我们希望 AI 遵循一个目标（我们将其称为“目标”）。如果按下按钮，我们希望人工智能遵循不同的目标（我们称之为“偏离目标”）。据推测，偏离目标涉及人工智能关闭自身及其所有相关工具/子/等，但这部分是当前目的的风味文本。</p><p>我们在这里要避免的关键故障模式是意外地激励人工智能操纵关机按钮——即要么导致其被按下，要么导致其未被按下。为什么会发生这种情况？好吧，假设当我们第一次打开人工智能时，它<i>只是</i>针对目标进行优化。人工智能意识到，如果按下按钮，它未来的自我将停止针对目标进行优化，因此它将在目标上实现较低的性能。因此，人工智能意识到，通过阻止按钮被按下，它可以在目标上取得更好的表现。由于它只是针对目标性能进行优化，因此它会阻止按钮被按下。</p><p> <a href="https://intelligence.org/files/Corrigibility.pdf"><u>MIRI 关于关机问题的论文</u></a>详细介绍了针对关机问题的简单建议的更多示例，以及它们如何激励某种按钮操作。事实证明，通常很难从任何预期的效用最大化器中获得所需的行为（即没有按钮操作）。</p><h2>为什么很难，第 1 级：期望的行为意味着不完整的显示偏好</h2><p>为什么很难让预期的效用最大化器不操作按钮（同时仍然做有用的事情）？这是我从与萨米的讨论中得到的一个直观答案。</p><p>中心思想是询问我们希望关闭按钮代理的行为意味着该代理所显示的偏好是什么。</p><p>假设在不同的时间，为代理提供了花费资源的机会，以便使按钮被按下/取消按下。我们希望代理人<i>在两个方向上</i>拒绝这样的机会——这意味着对任何显示的偏好漠不关心或缺乏偏好。此外，我们确实希望代理花费资源在按下按钮或未按下按钮的世界中产生各种不同的结果，因此在按下按钮的世界和未按下按钮的世界中存在不平凡的显示偏好排序。但是，如果代理要拒绝导致按钮被按下/松开的代价高昂的机会，并且这些机会在足够多不同的按下结果和未按下结果对之间跳转（它们本身都有不平凡的显示偏好），那么将会有揭示的偏好差距——即行为<i>不能</i>用完整的偏好来表示，只能用不完整的偏好来表示。</p><p>视觉上： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iJofoQX7EjMFxDo6m/xfewi1h80r9jpibzyaxl"><figcaption>代理花费资源从​​ A1 移动到 A2，但不花费资源在 B 和任一 A 结果之间向任一方向移动。这就是“偏好差距”；它很有趣，因为任何与偏好差距一致的偏好分配必定是不完整的。</figcaption></figure><p>我们希望代理花费资源在未按下按钮的世界中从结果 A1 移动到 A2，因此它在那里有一个显示的偏好。但我们希望代理<i>不要</i>花费资源在 A1 和 B 之间移动，<i>无论是哪个方向</i>- 因此它要么显示出冷漠（假设代理具有非零的出价/要价价差），要么显示出 A1 和 B1 之间偏好的不完整性。按照同样的逻辑，A2 和 B1 之间也要么表现出冷漠，要么不完整。但是，由于代理对 A2 的显示偏好高于 A1，因此任何显示的偏好分配在 A1/B1 和 A2/B1 之间都不能是无差异的；这意味着 A1 和 A2 之间是冷漠的。因此，代理人所透露的偏好<i>一定</i>是不完整的。 <span class="footnote-reference" role="doc-noteref" id="fnreft75wmozjvgf"><sup><a href="#fnt75wmozjvgf">[1]</a></sup></span></p><p>结果：（一种框架方式）关闭问题首先困难/有趣的原因是，期望的行为意味着暴露的偏好差距。例如，任何标准的预期效用最大化者<i>都不能</i>有显性的偏好差距，这样的标准欧盟最大化者<i>就不能</i>按照我们想要的方式行事。</p><p>关于此的一些注释：</p><ul><li>所显示的偏好可能非常不确定，即可能存在许多与代理行为兼容的偏好分配。上面的论点表明，<i>任何与</i>期望行为一致的偏好分配<i>都一定</i>是不完整的。</li><li>请注意，故事隐式地通过因果干预/反事实来探索偏好，即我们感兴趣的是代理是否会花费资源来<i>导致</i>按钮被按下/松开。这与本文无关，但对于那些想要将这一切正式化的人来说可能很重要。</li><li>显然，有时人们认为 A 和 B 之间“无偏好”（即不完整性）意味着代理人在两者之间随机/任意选择。这并不是这里所说的“无偏好”的概念。我们谈论的是“无偏好”，就像“只要有机会，代理人不会为了获得另一个而放弃任何一个”。</li></ul><p>好吧，现在我们有一个相对直观的故事来解释为什么在关闭问题中很难让预期的效用最大化者按照我们想要的方式行事：只要欧盟最大化者没有揭示偏好差距，他们就无法按照我们想要的方式行事我们想要。 （更一般地说，如果我们正在训练一个代理，任何一维奖励信号都可能无法激励显示的偏好差距。）但是，那么解决方案大概就是……使用一个具有不完整偏好的代理？</p><h2>为什么很难，第二级：不完整的偏好想要完成</h2><p>帖子<a href="https://www.lesswrong.com/posts/bzmLC3J8PsknwRZbr/why-not-subagents"><u>为什么不选择子代理？</u></a>对此进行了更深入的探讨，但我将在这里进行简要概述。</p><p>回顾我们的 A1、A2 和 B1 图表，我们可以想象一组不完整的偏好，它比 A1 更喜欢 A2，但在 B1 和 A 中的任何一个之间没有偏好（即代理不会花费资源在未按下按钮和按下按钮的结果，反之亦然）。但该代理将花费资源将 A1 结果转变为 A2 结果。</p><p>现在的问题是：该智能体可能会概率性地预先承诺接受将 A1 结果转变为 B1 结果（即花费资源使按钮被按下）和将 B1 结果转变为 A2 结果（即花费资源使按钮不被按下）的<i>两个</i>机会。按下）。如果它以正确的概率执行这两项操作，那么它可以保持按下 (B1) 和未按下 (A1+A2) 结果的相同比例，但将一些概率质量从 A1 结果转移到 A2。这是一个改进，通过不完整的偏好：代理不接受 A 和 B 之间的任何净转移，它只是 A 结果内的净转移概率，但它是通过操纵关闭按钮来实现的。</p><p> （我从萨米的帖子中了解到，这被称为不完整偏好的“束缚”。）</p><p>所以我们需要某种不会这样做的代理。</p><p>关于此的一些注释：</p><ul><li>关键问题是<i>所揭示的</i>偏好是否不受限制。这是我与萨米讨论时的一个主要困惑点 - 他的提案涉及一组传递到决策规则中的偏好，但这些“偏好”（可能）与所<i>显示的</i>偏好不同。 （我仍然不确定萨米的提议是否解决了问题。）</li><li>在传递给某种搜索/规划/决策过程的目标意义上，显示的“偏好”与“偏好”之间的差异可能会开辟一些解决问题的方法。</li><li>显然，我们可以设计一个不太聪明的代理，它具有稳定的不完全偏好。有趣的问题是如何在不对代理的能力或环境的丰富性产生重大限制的情况下做到这一点。</li><li>请注意，束缚涉及<i>导致</i>代理没有偏好的结果之间的切换。我的直觉是，因果关系在某种程度上是关键。我们希望代理不会导致按下和未按下结果之间的切换，即使这两个结果的相对频率保持不变。 </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnt75wmozjvgf"> <span class="footnote-back-link"><sup><strong><a href="#fnreft75wmozjvgf">^</a></strong></sup></span><div class="footnote-content"><p>这一切都假设偏好的传递性；人们也许可以放宽传递性而不是不完整性，但这样我们就处于更狂野的领域了。我不是在这里探索那条特定的道路。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem<guid ispermalink="false"> iJofoQX7EjMFxDo6m</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:13:30 GMT</pubDate> </item><item><title><![CDATA[Holly Elmore and Rob Miles dialogue on AI Safety Advocacy]]></title><description><![CDATA[Published on October 20, 2023 9:04 PM GMT<br/><br/><p> Holly 是一位独立的 AI Pause 组织者，其中包括组织抗议活动（就像即将<a href="https://www.lesswrong.com/events/ZoTkRYdqGuDCnojMW/global-pause-ai-protest-10-21">到来的抗议活动</a>）。 Rob 是一位<a href="https://www.youtube.com/@RobertMilesAI">AI 安全 YouTuber</a> 。我（jacoobjacob）将他们聚集在一起进行这次对话，因为我一直在试图弄清楚我应该如何看待人工智能安全抗议，这似乎是一个可能相当重要的干预；罗布和霍莉似乎有着深思熟虑的观点，甚至可能存在分歧。</p><p>快速澄清：他们一度讨论了一场特定的抗议活动，即<a href="https://x.com/ilex_ulmus/status/1713062767853253042?s=20">2023 年 9 月 29 日在旧金山 Meta 大楼举行的反不可逆转扩散抗议活动，</a> Holly 和 Rob 都参加了。</p><p>另外，对话相当长，我觉得没必要按顺序看。您应该随意跳到您觉得最感兴趣的部分标题。 </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:04:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:04:26 GMT" user-order="1"><p>让我们直接进入正题。罗布，当我建议讨论抗议活动时，你说你很困惑：“我们如何才能对任何事情有足够的信心，为其带来激进主义似乎需要的能量？”我非常同意这种困惑！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:07:41 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:07:41 GMT" user-order="2"><p>这是一个非常标准的笑话/刻板印象，XKCD 抗议，其中的标志是巨大的，充满了微小的文字，准确地说明了我们的意思。我的抗议标语只是写着“现在小心”，部分原因是我不确定你还可以在我完全认可的标语上添加什么内容。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:09:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:09:59 GMT" user-order="3"><p>科学或错误较少的事物与倡导之间的沟通方式存在很大差异。您在宣传方面的带宽较少。它更接近正常的言语，我们并不完全限定每一个陈述——考虑到这种沟通的窗口有多短，作为一种实践，这在很多方面都更准确。 “暂停人工智能”可以完成对如何实施政策的准确描述所无法完成的工作——这会太混乱且难以接受。同样，你在倡导中必须讨论的政策决议要低得多（或更高层次的概念），因此您可以认可您确实支持的非常广泛的政策目标，同时对于使用什么机制或确切方法有很多真正的不确定性。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:11:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:11:26 GMT" user-order="1"><p> （一个相关的旧的好帖子： <a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">You Get About Five Words</a> 。该帖子还建议您还剩下 3 个单词。可以将它们花在“PauseAI：但不确定”上）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:15:36 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:15:36 GMT" user-order="3"><p> （好吧，这可能会让我们遇到一个症结：这是一个糟糕的主意）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:12:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:12:37 GMT" user-order="1"><p> （哈哈，如果罗布觉得有趣的话，对于该主题的讨论非常兴奋）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:17:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:17:55 GMT" user-order="2"><p> （我想我同意，在你的口号中加入一般性的不确定性是浪费文字，而你可以添加具体的细微差别。比如“暂停前沿人工智能”、“暂停 A <strong>G</strong> I”、“暂停上帝般的人工智能”等等）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:22:23 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:22:23 GMT" user-order="3"><p> （同意。我只是不会使用太技术性的词。我喜欢“暂停上帝般的人工智能”，但也许有些人认为人工智能永远不可能绝对像上帝之类的。更多的限定词可能会增加人们混淆的机会）基本上有了正确的想法就可以开始了。）<br><br> （好吧，“基本上有正确的想法”可能就是问题所在。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:11:23 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:11:23 GMT" user-order="2"><p>回复上一点：另一方面就是政策看起来很复杂和不确定，我真的不知道我们应该做什么。当我至少有 20% 的人认为 X 政策会让事情变得更糟时，大喊我们应该执行 X 政策，这感觉几乎是不诚实的。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:14:50 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:14:50 GMT" user-order="3"><p>嗯，是的，我听到了。我仍然赞成在这种情况下倡导<i>一项</i>政策的原因是，受众所做的更新的性质通常比该水平高一两个级别。所以我们说“暂停人工智能”，但他们带走的是“人工智能是危险的，有一个不涉及制造人工智能的解决方案，支持这个解决方案是可行的......”<br><br> （我也碰巧认为暂停人工智能是一个非常好的宣传信息，因为它很广泛，指向一个有意义的方向，并且不容易被误解为危险的东西。有很多可以想象的暂停版本，我认为其中大多数都会乖一点。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:16:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:16:11 GMT" user-order="2"><p>我认为这可能会被误解为“暂停所有人工智能开发和部署”，从而导致“ <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#:~:text=If%20all%20you%20need%20is%20an%20object%20that%20doesn%27t%20do%20dangerous%20things%2C%20you%20could%20try%20a%20sponge%3B%20a%20sponge%20is%20very%20passively%20safe.">海绵安全</a>”狭义人工智能系统的延迟部署，而这些系统本可以改善或拯救大量人的生命。放慢速度确实要付出代价。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:20:44 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:20:44 GMT" user-order="3"><p>我认为我的意思是暂停开发和部署。我真正想要完成的是让我们摆脱这种奇怪的情况，我们必须证明人工智能是危险的，而不是人工智能开发人员必须证明他们继续进行是安全的。我什至不确定是否有一种安全的方法来了解高级人工智能是安全的！我希望社会看到，鉴于这种巨大的风险和不确定性，允许人工智能继续开发或部署是多么荒谬。<br><br>总的来说，这对我来说比承认人工智能也会带来好处更重要，尽管我很高兴承认在适当的环境下——重点可以保持正确。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:24:26 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:24:26 GMT" user-order="2"><p>这对我来说更接近症结。我基本上认为大多数人工智能总体上是好的，已经总体上是好的，并且至少在一段时间内将保持总体上的好。虽然存在严重的安全问题，但它们与任何其他技术没有什么不同，我们社会开发技术和应对新技术的正常方式可能足以胜任这项任务。就像，你允许公司开发人工智能并部署它，然后发现它是有偏见的，人们对公司大喊大叫，政府做事，他们修复它，总的来说，你做得比如果每个人都做得更好公司被要求在部署之前清楚地证明系统根本没有任何问题，因为那样你就会减慢一切速度，以至于很多问题将在更长时间内得不到解决，这会带来更高的成本</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:27:53 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:27:53 GMT" user-order="3"><p>您不认为人工智能的 0 天问题比其他技术更多吗？如果强大的人工智能的早期错误太大而无法迭代纠正怎么办？<br><br>它与其他技术在本质上没有什么不同，但在能力的大小和不可预见问题的潜在后果方面有所不同。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:33:26 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:33:26 GMT" user-order="3"><p>除了关于人工智能是否太危险而无法进行迭代修正的对象级问题之外，还有一个问题是倡导者是否需要公平对待人工智能的类别，或者只关注最严重的危险。我并不担心人们是否会对人工智能有偏见，因为我正在谈论我认为最严重的问题。我希望人们有准确的想法，但我能传达的信息有限，所以我把警告放在第一位。<br><br> （写了这篇关于如何描述技术作为一个整体是好还是坏的问题是不是达到同样的目的： <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error">https://hollyelmore.substack.com/p/the-technology-bucket-error</a> <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error)">）</a></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:34:03 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:34:03 GMT" user-order="2"><p>是的，我想这种事情大致与技术的力量及其开发速度成正比。当然，人们的抗议是社会处理此类事情的现有机制的一部分，所以不这样做并不是理由。但我认为 AGI 的案例确实与之前所有的技术有根本的不同，因为</p><ol><li>可能会出现真正的权力中断。如果 AGI 使你的研究自动化，并且你在一年内获得了未来 50 年的新技术，那感觉就像是一种不同</li><li>普通技术（包括狭义人工智能）基本上可以让人们得到更多他们想要的东西，而大多数人基本上都想要好的东西，所以普通技术最终大多都是好的。 AGI 创造了从技术广泛地满足人们想要的东西到技术本身获得<strong>它</strong>想要的东西的转变的可能性，这几乎可以是任何东西，并且与人类价值观完全分离</li></ol><p>我觉得强烈应对 AGI 风险的论点因与也适用于互联网等论点相关而被削弱。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:35:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:35:58 GMT" user-order="2"><p>就像，关于新技术，以及“这个技术或那个技术是好是坏”，关于当前人工智能的大多数讨论都很好地符合这一传统，我想说“<strong>不</strong>，AGI 是它自己的、独特的、更严肃的东西”</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:38:51 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:38:51 GMT" user-order="3"><blockquote><p>我觉得强烈应对 AGI 风险的论点因与也适用于互联网等论点相关而被削弱。</p></blockquote><p>从倡导的角度来看，我不确定是否如此。这是在争论中，但我真的不知道在倡导背景下提出其他论点/问题是否会削弱处理 x 风险的意愿。暂停的原因有很多，从对自动化取代工作的相当老的担忧，到对其他代理篡夺我们而不关心什么对我们有好处的 x 风险担忧。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">霍莉·埃尔莫尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:39:32 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:39:32 GMT" user-order="2"><p>我认为，如果你说一件强烈的事情，不同意的人可能会忽略你或对你的强烈事情做出反应。如果你说三件事，不同意的人可以只回应最弱的一件事，并且看起来获胜</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:40:32 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:40:32 GMT" user-order="1"><p></p><blockquote><p>我认为，如果你说一件强烈的事情，不同意的人可能会忽略你或对你的强烈事情做出回应。如果你说三件事，不同意的人可以只回应最弱的一件事，并且看起来获胜</p></blockquote><p>这让人想起“人工智能不杀死所有人主义”一词的创造。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布</section></section><p>（元注：在对话的这一点上，有一个短暂的停顿，之后罗布和霍莉转而进行口头对话，然后进行转录。）</p><h2>对一切事物都持技术乐观态度……除了 AGI </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:41:22 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:41:22 GMT" user-order="2"><p>好吧，我们刚才在哪里？我想我正在描述我有时对这些论点感到的沮丧？我会尝试重述一下。</p><p>所以我仍然主要是一个技术乐观主义者，在某种程度上，这是一个连贯的事情（我认为它广泛地不是......成为“乐观主义者”可能是一个桶错误。）但是，是的，技术，从广义上讲，让人们得到更多他们想要的东西。当事故发生时，或者当人们想要的东西实际上并不对他们有好处时，这有时会成为一个问题。</p><p>但通常情况下，人们得到他们想要的东西是件好事。就我而言，这实际上是定义“好”的合理基础。因此，大多数技术总体上表现良好。如果你只有一个刻度盘，比如“更多”或“更少”——我知道你没有，但如果你有——技术就会变得很好，事实上非常好。然后，通用人工智能是另一种奇怪的东西，你拥有一种技术，可以得到更多<i>它</i>想要的东西，而不是更多人们想要的东西。这使得它在技术类别中独一无二，因此需要一种不同的方法……因此，在谈论人工智能风险时，使用似乎也适用于一般技术的论点是令人担忧的，因为人们担心的许多其他技术大约结果很好。感觉这让人们很容易解决这部分问题，或者将其纳入现有的辩论中，而忽略其中我认为真正重要的部分——这是事物的 AGI 方面。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">罗伯特·迈尔斯</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:31 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:31 GMT" user-order="3"><p>我认为这触及了本次讨论的另一个普遍主题：倡导沟通与 LessWrong 上的沟通不同。它不太精确，你的空间也较小。</p><p>我同意，对于某些人来说，能够将有关 AGI 危险的论点与他们已经听说过的有关技术的论点进行模式匹配是一个陷阱，而这些论点几乎全部都支持技术是好的。这似乎确实可以削弱这一论点。我还认为，对于那些在这个话题上真正受过教育的人以及我们圈子里很多对科技了解很多并且普遍支持科技的人来说，这可能是他们心理上非常重要的论点。对他们来说，技术在过去总是表现良好，这可能真的很重要。</p><p> But of course I should note that a lot of people in the world don&#39;t think that technology has always turned out well in the past, and a lot of people are really concerned about it this time, even if it&#39;s happened a million times before.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:42:34 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:42:34 GMT" user-order="2"><p> Yeah. This actually ties into another thing which frustrates me about this whole situation: the conversation seems... too small and too narrow for the topic? In the sense that I should expect there to be an advocacy group going “Yeah, we&#39;re Luddites, and proud! We don&#39;t like where this technology is going, we don&#39;t like where technology is going in general, and we want to stop it.”</p><p> And I want to yell at that group, and have that be part of the conversation. I don&#39;t want that to be the same group that is also making sensible claims that I agree with. I want both of those to exist separately.</p><p> I want the Overton window to have people in it all across this range, because these are real positions that people hold, and that people who aren&#39;t crazy <i>can</i> hold. I think they&#39;re wrong: but they&#39;re not bad people and they&#39;re not crazy. It&#39;s frustrating to have so few people that you need the individual advocacy groups to cover a broad range of the spectrum of opinion, within which there&#39;s actually a lot of conflict and disagreement. Currently there&#39;s just so few people that I have to be like “They&#39;re right about some things, and overall it&#39;s good for their voice to be there, so I don&#39;t want to yell at them too much”, or something like that.</p><p> If you&#39;re THE Pause AI advocacy group, it puts you in a difficult situation. I&#39;m now sympathising a bit more with it, in that you&#39;re trying to get support from a wider range of people, and maybe you have to sacrifice some amount of being-correct to do that. But it would be cool to have everybody be advocating for the thing they actually think is correct - for there to be enough people with enough range of opinions for that to be a sensible spectrum within which you can have real discussions.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:59 GMT" user-order="3"><p> I would argue that what&#39;s being sacrificed isn&#39;t correctness, but rather specificity. Less specific statements can still be true or clear.<br><br> The message of PauseAI is broad; it&#39;s not saying &quot;I want AI paused this specific way,&quot; but rather &quot;I want AI paused indefinitely.&quot; &quot;Pause AI&quot; doesn&#39;t specify how long, like how many years-- I understand that various factors could influence the duration it&#39;s actually paused for even if we get our way. In advocacy, you often get to push in a direction rather than to precise coordinates.</p><p> A significant issue with alignment-based messages is that, if misunderstood, they can seem to advocate the opposite of their intended purpose. PauseAI doesn&#39;t suffer from this ambiguity. It&#39;s aiming to shift the burden of proof from &quot;we need to prove AI is dangerous&quot; to &quot;we have reasons to believe AI is different, and you have to show that building it is safe before you do it.&quot; While there are compelling reasons to hasten AI&#39;s development due to its potential benefits, it&#39;s not overwhelmingly compelling. For the majority of the world, waiting a bit longer for AGI in exchange for increased safety is a fair trade-off, especially if they understood the full implications.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:04 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:04 GMT" user-order="2"><p> It&#39;s a difficult one. I believe the reason people aren&#39;t eager for AGI to arrive two years sooner is because they aren&#39;t aware.... either they don&#39;t realise it&#39;s possible, or they don&#39;t understand the full implications.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:24 GMT" user-order="3"><p> I believe there are many other issues where people don&#39;t appreciate the urgency. For instance, inventing new vaccines a year sooner can mean thousands of lives saved. Usually, I&#39;m the one emphasizing that. It&#39;s true that many don&#39;t feel the urgency then, and similarly, they don&#39;t feel the urgency now. But in this case, their usual inclination might lead them in the right direction... AI risk could be mitigated with more time. While I&#39;d be sad if AGI couldn&#39;t be safely built, I think we should be pushing a policy that accommodates the possibility that we can&#39;t make it safely.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:28 GMT" user-order="2"><p> Well it&#39;ll never be truly safe, insofar as &quot;safe&quot; is defined as having zero risk, which is clearly not achievable. So the question is: at what point does the benefit of doing something outweigh its risk? We&#39;re definitely not there currently.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:41 GMT" user-order="3"><p> I actually think advocacy offers a way to sidestep that question. Precisely how much risk should we tolerate for what benefit seems like a subjective question. However, most people are in fact fairly risk-averse on matters like this. If you ask most stakeholders, meaning the global population [NB: the stakeholder are the population of Earth but the polls I was thinking are of Americans], they generally favor a risk-averse approach. That seems correct in this context. It feels different from the kind of risk/reward we&#39;re typically set up to think about. We&#39;re not particularly equipped to think well about the risk of something as significant as a global nuclear war, and that&#39;s arguably more survivable than getting AI wrong.</p><p> I feel I am more risk-averse than a lot of our friends. My instinct is, let&#39;s just not go down that path right now. I feel like I could get the right answer to any hypothetical if you presented me with various fake utilities on the different sides.  Yet the real question is, do we take the leap now or later? I assume that any pause on AI advancement would <i>eventually</i> be lifted unless our society collapses for other reasons. I think that, in the long run, if it&#39;s possible, we&#39;ll achieve safe AGI and capture a lot of that value because we paused.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> Cluelessness and robustly good plans </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:11 GMT" user-order="2"><p> Hmm... I think I&#39;m convinced that pausing is better than not pausing. The thing I&#39;m not convinced of is that the world if you advocate pausing, is better than the world if you don&#39;t advocate pausing.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:22 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:22 GMT" user-order="3"><p> This is interesting, because I was talking to one of the co-organizers for the October 21st protest, and he said that actually he wasn&#39;t sure a pause is good, but he was more sure that advocating for a pause is good. He came away more convinced that advocacy was solidly good than that pause was the right policy decision.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:28 GMT" user-order="2"><p> This comes back to the very first thing... When it comes to the technical stuff, you can sit down and take it apart. And it&#39;s complicated, but it&#39;s the kind of complicated that&#39;s tractable. But for advocacy... it feels so hard to even know the sign of anything. It makes me want to just back away from the whole thing in despair.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:41 GMT" user-order="3"><p> I have felt that people I know in AI safety think that government/politics stuff is just harder than technical stuff. Is that kind of what you&#39;re thinking?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:44 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:44 GMT" user-order="2"><p> Yeah. It&#39;s so chaotic. The systems you&#39;re working with are just extremely... not well behaved. The political situation is unprecedented, and has been various different kinds of unprecedented at every point in the last century at least. I&#39;m sure there are people who&#39;ve read a ton of history and philosophy and believe they can discern the grand arcs of all these events and understand what levers to pull and when, but I&#39;m not convinced that those people actually know what they think they know - they just get so few feedback loops to test their models against reality. It seems very common for unexpected factors emerge and cause actions to have the opposite of their intended effect.<br><br> That&#39;s not a reason not to try. But it gives me a feeling of cluelessness that&#39;s not compatible with the fervour that I think advocacy needs.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:55 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:55 GMT" user-order="3"><p> I do just feel pretty good about the pause message. It&#39;s one I consistently resonate with.  And not every message resonates with me. For instance, the message of the protest you attended at Meta turned out to be a more complex message to convey that I expected.</p><p> [The idea that advocacy is intractable but the technical problem isn&#39;t] might be a double crux?</p><p> Sounds like you&#39;re more concerned about hindering technological progress, and threading that needle just right… but I feel like what we&#39;re talking about is a just loss of some decades, where we have to sort of artificially keep AGI from coming, because if you allowed everything to just develop at its pace then it would be here sooner but it wouldn&#39;t be safe.</p><p> That&#39;s what I imagine when I say “pause”.  And I think that&#39;s clearly a worthwhile trade.  And so I feel much happier to just say “pause AI until it&#39;s safe”, than I ever did about any policy on alignment.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:01 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:01 GMT" user-order="2"><p> The only thing I&#39;ve always felt is obviously the thing to do is “speed up and improve alignment research”. Just direct resources and attention at the problem, and encourage the relevant people to understand the situation better. This feels fairly robust.</p><p> (Also, on some kind of aesthetic level, I feel like if you make things worse by saying what you honestly believe, then you&#39;re in a much better position morally than if you make things worse by trying to be machiavellian.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:11 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:11 GMT" user-order="3"><p> I am no expert on AI, but I come away feeling that we&#39;re not on track to solve this problem. I&#39;m not sure if promoting alignment research alone can address the issue, when we maybe only have like 5 years to solve the problem.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:15 GMT" user-order="2"><p> Yeah... and I don&#39;t know, maybe encouraging labs to understand the situation is is not even robustly good. It&#39;s difficult to get across “Hey, this is a thing to be taken seriously” without also getting across “Hey, AGI is possible and very powerful” - which is a message which... if not everyone realises that, you&#39;re maybe in a better situation.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles</section></section><h2> Taking beliefs seriously, and technological progress as enabler of advocacy </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:30 GMT" user-order="3"><p> I believe that merely providing people with information isn&#39;t sufficient; they often don&#39;t know how to interpret or act on it. One reason I turned to protests is that they send a clear message. When people see a protest, they instantly grasp that the protestors find something unacceptable.  This understanding is often more profound than what one might derive from a blog post weighing the pros and cons.</p><p> People tend to assume that if they were privy to crucial information that the majority were unaware of, especially if the stakes were world-altering, they would be out on the streets advocating for change.  There was confusion around why this wasn&#39;t the case for AI safety. I personally felt confused.</p><p> (The most extreme case would be when Eliezer suggested that no amount of violence could be justified to halt AGI, even if it was going to kill everyone on earth… I also think that you should have a deontological side constraint against violence because it&#39;s always so tempting to think that way, and it just wouldn&#39;t be helpful; it would just end up backfiring. But I was confused, and I really appreciated the TIME article where he mentions enforcing treaties using the same state violence as for other international treaties.)</p><p> Still, for a long time, I was unsure about the AI safety community&#39;s commitment due to their seeming reluctance to adopt more visible and legible actions, like being out in the streets protesting… even though I myself am not typically one to attend protests.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:35 GMT" user-order="2"><p> I think it&#39;s maybe a dispositional thing. I feel like I am just not a person who goes to protests. It&#39;s now how I engage with the world.</p><p> And I think this is also part of, where I stand on technology in general. Technology is such a big lever, that it kind of almost... ends up being the most important thing, right? It&#39;s like…</p><p> Okay, well now I&#39;m gonna say something controversial. People, I think, focus on the advocates more than the technologists, in a way that&#39;s maybe a mistake?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:47 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:47 GMT" user-order="3"><p> I already know I agree.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:54 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:54 GMT" user-order="2"><p> You can advocate for women&#39;s rights, but you can also invent effective, reliable birth control. Which one will have the bigger impact? I think it&#39;s the technology. Activists need to push for change, but it&#39;s technology that makes the change politically doable. I think people are like... the foam on the top of the enormous waves of the grand economic shifts that change what&#39;s politically and socially viable. For instance, I think our ability to get through the climate crisis mostly depends on renewable energy tech (if AGI somehow doesn&#39;t happen first) - make clean energy cheaper than fossil fuels, people will switch. I think all the vegan activism might end up being outweighed by superior substitute meats. When there&#39;s meat products that are indistinguishable from farmed meat but a tiny bit cheaper, the argument for veganism will mysteriously become massively more compelling to many. In both those cases, one big thing advocates did was create support for funding the research. I even think I remember reading someone suggest that the abolitionist movement really got traction in places that had maple syrup, because <i>those people had access to a relatively inexpensive alternative supply of sugar,</i> so being moral was cheaper for them. Obviously, activists are essential to push the arguments, but often these arguments don&#39;t gain traction until the technology/economics/pragmatics make them feasible. People are amazingly able to believe falsehoods when it&#39;s convenient, and technology can change what&#39;s convenient.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:01 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:01 GMT" user-order="3"><p> We could call this the “efficient market hypothesis of history” or something. Because I hear this a lot, and it&#39;s also part of my worldview.  Just not enough to make me not consider activism or advocacy. I think its all generally true and I pretty much agree with it, but at the same time you can still find $20 on the ground.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:05 GMT" user-order="2"><p> I actually did last week :)</p><p> [Added later: That makes me think, maybe that&#39;s the way to do it: rather than looking at what changes would be best, you look for the places where the technology and economics is already shifting in a good direction and all that&#39;s needed is a little activism to catalyse the change, and focus there. But probably there are already enough activists in most places that you&#39;re not doing much on the margin.]</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:28 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:28 GMT" user-order="3"><p> [Added later, in response: there are definitely not enough activists for AI Safety at this moment in time! It makes sense there would be &quot;market failure&quot; here because understanding the issue up until now has required a lot of technical know-how and running in a pretty advocacy-averse social circle.]<br><br> Perhaps it&#39;s a form of modest epistemology to defer so heavily to this, to override what we&#39;re actually witnessing. I even think there are some individuals who kind of know what you can do with advocacy.  They know about policy and understand that influencing even a small number of people can shift the Overton window.  And still, even those people with an inside view that advocacy can sometimes work, will struggle with whether it&#39;s the right approach or feel like somehow it <i>shouldn&#39;t</i> work, or that they should be focusing on what they deem more generally important (or identify with more).</p><p> However, I think what we&#39;re discussing is a unique moment in time. It&#39;s not about whether advocacy throughout history can outpace technology. It&#39;s about whether, right now, we can employ advocacy to buy more time, ensuring that AGI is not developed before sufficient safety measures are in place.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:30 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:30 GMT" user-order="2"><p> There&#39;s definitely a perspective where, if you have the ability to do the technical work, you should do it. But that&#39;s actually quite a small number of people. A larger number have the ability to advocate, so they bring different skills to the table.</p><p> Actually, I&#39;ve think I&#39;ve pinned down a vibe that many who can do technical AI safety work might resonate with. It feels akin to being assigned a group project at school/university. You&#39;ve got two choices: either somehow wrangle everybody in this group to work together to do the project properly, or just do the whole thing yourself. Often the second one seems a lot easier.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:39 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:39 GMT" user-order="3"><p> Though if you don&#39;t understand that the exercise was about learning to work in a group (instead of doing the assignment) you can miss out on a lot of education…</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:47:10 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:47:10 GMT" user-order="2"><p> Yeah. Hmm... maybe this is actually a mis-learned lesson from everyone&#39;s education! We learned the hard way that &quot;solving the technical problem is way easier than solving the political one&quot;. But group project technical problems are artificially easy.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:47:38 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:47:38 GMT" user-order="3"><p> I am kind of annoyed at myself that I didn&#39;t question that vibe earlier in the community, because I had the knowledge… and I think I just deferred too much. I thought, “oh, well, I don&#39;t know that much about AI or ML-- I must be missing something”. But I had everything I needed to question the sociopolitical approaches.</p><p> My intuitions about how solvable the technical problem are influenced by my background in biology. I think that biology gives you a pretty different insight into how theory and real world stuff work together. It&#39;s one of the more Wild West sciences where stuff surprises you all the time.  You can do the same protocol and it fails the same way 60 times and then it suddenly works.  You think you know how something works, but then it suddenly turns out you don&#39;t.</p><p> So all my instincts are strongly that it&#39;s hard to solve technical problems in the wild -- it&#39;s possible, but it&#39;s really hard.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> The aesthetics of advocacy, and showing up not because you&#39;re best suited, but because no one else will </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:50:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:50:55 GMT" user-order="2"><p> I think there&#39;s an aesthetic clash here somewhere. I have an intuition or like... an aesthetic impulse, telling me basically… “advocacy is dumb”. Whenever I see anybody Doing An Activism, they&#39;re usually… saying a bunch of... obviously false things? They&#39;re holding a sign with a slogan that&#39;s too simple to possibly be the truth, and yelling this obviously oversimplified thing as loudly as they possibly can? It feels like the archetype of overconfidence.</p><p> It really clashes with my ideal, which is to say things that are as close to the truth as I can get, with the level of confidence that&#39;s warranted by the evidence and reasoning that I have. It&#39;s a very different vibe.</p><p> Then there&#39;s the counter-argument that claims that empirically - strategically, or at least tactically - the blatant messaging is what actually works. Probably that activist person actually has much more nuanced views, but that&#39;s just not how the game is played. But then part of me feels like “well if that is how the game is played, I don&#39;t want to play that game”. I don&#39;t know to what extent I endorse this, but I feel it quite strongly. And I would guess that a lot of the LessWrong-y type people feel something similar.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:12 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:12 GMT" user-order="3"><p> I think people haven&#39;t expressed it as clearly as you, but I&#39;ve gotten sort of that same vibe from a lot of people.</p><p> And… it does kind of hurt my feelings. Because I <i>also</i> want to be accurate. I&#39;m also one of those people.</p><p> I think I just have enough pragmatism that I was able to get past the aesthetic and see how activism works? I was also exposed to advocacy more because I have been involved with animal welfare stuff my whole life.</p><p> I had this experience of running the last protest, where I had a message I thought was so simple and clear... I made sure there were places where people could delve deeper and get the full statement. However, the amount of confusion was astonishing. People bring their own preconceptions to it; some felt strongly about the issue and didn&#39;t like the activist tone, while others saw the tone as too technical. People misunderstood a message I thought was straightforward. The core message was, &quot;Meta, don&#39;t share your model weights.&quot; It was fairly simple, but then I was even implied to be racist on Twitter because I was allegedly claiming that foreign companies couldn&#39;t build models that were as powerful as Meta&#39;s?? While I don&#39;t believe that was a good faith interpretation, it&#39;s just crazy that if you leave any hooks for somebody to misunderstand you in a charged arena like that, they will.</p><p> I think in these cases you&#39;re dealing with the media in an almost adversarial manner.  You&#39;re constantly trying to ensure your message can&#39;t be misconstrued or misquoted, and this means you avoid nuance because it could meander into something that you weren&#39;t prepared to say, and then somebody could misinterpret it as bad.</p><p> This experience was eye-opening, especially since I don&#39;t consider very myself politically minded. I had to learn these intricacies the hard way. I believe there&#39;s room for more understanding here. There&#39;s a sentiment I&#39;ve sensed from many: &quot;I&#39;m smart and what you&#39;re doing seems unintelligent, so I won&#39;t engage in that.&quot; Perhaps if people had more experiences with the different forms of communication, there would be more understanding of the environment the different kinds of messages are optimized for? It feels like there&#39;s a significant inferential gap.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:15 GMT" user-order="2"><p> Yeah, that sounds frustrating. I guess there&#39;s something about… “rationality”. So, the task of trying to get the right answer, on a technical or philosophical question where most people get the wrong answer, feels like... semiconductor manufacturing, or something like that. Like &quot;In this discipline I&#39;ve tried my best to master, I have to make sure to wear a mask, and cover my hair so that no particles get out into the air. This entire lab is positive air pressure, high end filtration systems. I go through an airlock before I think about this topic, I remove all contamination I can, remove myself from the equation where possible, I never touch anything with my hands. Because I&#39;m working with delicate and sensitive instruments, and I&#39;m trying to be accurate and precise…&quot;. It sounds pretty wanky but I think there&#39;s a real thing there.</p><p> And then in this metaphor the other thing that you also have to do in order to succeed in the world is like... mud wrestling, or something. It may not be an easier or less valuable skill, but it really doesn&#39;t feel like you can do both of these things at once.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:30 GMT" user-order="3"><p> What if it were phrased more like “There&#39;s this tech. It&#39;s pretty low tech. It&#39;s a kludge. But it <i>works</i> ”?</p><p> I remember the exact first moment I cried about AI safety: it&#39;s when I saw the polls that came out after the FLI letter, saying -- and I forget the exact phrasing, but something like -- most people were in favor of pause. “Oh my god”, I thought “we&#39;re not alone. We don&#39;t have to toil in obscurity anymore, trying to come up with a path that can work without everybody else&#39;s help.” I was overjoyed. I felt as proud and happy discovering that as I would&#39;ve been if I found a crucial hack for my really important semiconductor.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:33 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:33 GMT" user-order="2"><p> Hmm... Yeah, it clearly is important and can work, but my gut reaction, when I&#39;m inhabiting that clean-room mindset, is I still feel like I want to keep it at arm&#39;s length? I don&#39;t trust this low tech kludge not to spring a leak and mess up the lab.</p><p> When you start arguing with... (by you I mean &#39;one&#39; not you personally) - When one starts arguing with people using the discussion norms of political debate, and you&#39;re trying to win, I think it introduces a ton of distortions and influences that interfere. Maybe you phrase things or frame things some way because it&#39;s more persuasive, and then you forget why you did that and just believe your distorted version. You create incentives for yourself to say things more confidently than you believe them, and to believe things as confidently as you said them. Maybe you tie your <a href="http://www.paulgraham.com/identity.html">identity</a> to object level questions more than necessary, and/or you cut off your own <a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat">line of retreat</a> , so it&#39;s extra painful to notice mistakes or change your mind, that kind of thing. I notice this kind of thing in myself sometimes, and I expect I fail to notice it even more, and I&#39;m not even really trying to fight people with my work, just explain things in public.</p><p> I feel like there&#39;s kind of a trilemma here. You&#39;ve got to choose between either 1) truth/&#39;rationality&#39;/honesty, 2) pragmatism, for getting things done in the messy world of politics, or 3) a combination of the two different modes where you try to switch between them.<br> Choosing 1 or 2 each gives up something important from the other, and choosing 3 isn&#39;t really viable because the compartmentalisation it requires goes against 1.</p><p> I have this feeling about the truth, where you don&#39;t get to make exceptions, to sometimes believe what&#39;s pragmatically useful. You don&#39;t get to say, “in this situation I&#39;ll just operate on different rules”. I feel like it&#39;s not safe to do that, like human hardware can&#39;t be trusted to maintain that separation.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:46 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:46 GMT" user-order="3"><p> I guess I can&#39;t bring myself to be that concerned about contamination... I do think it&#39;s possible to switch modes and I find rationalists are often quite paranoid about this, like it&#39;s one toe out of line and they can never sincerely value and seek truth again. I have <a href="https://hollyelmore.substack.com/p/scrupulosity-my-eagxboston-2019-lightning-talk">struggled with scrupulosity (OCD)</a> and it reminds me of that.</p><p> Maybe it&#39;s because I wasn&#39;t into RationalityTM until later in my life, so I already had my own whole way of relating to the truth and how important the truth was. It wasn&#39;t so much through “speech codes” or things advocacy seems to violate?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:47 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:47 GMT" user-order="2"><p> Yeah. Maybe that &#39;clean room&#39; mindset was just what was needed in order to recognise and address this problem seriously 15+ years ago… a mindset strongly focused on figuring out what&#39;s true, regardless of external opinions or how it would be perceived. At that time, societal factors might have prevented people from seeing the true scope of the situation. Now, the reality is starting to smack everyone in the face, so, perhaps that intense focus isn&#39;t as essential anymore. But by now, the most prominent individuals in the field have been immersed for a long time in this mindset that may not be compatible with an advocacy mindset? (This is just a hypothesis I randomly came up with just now.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:52:05 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:52:05 GMT" user-order="3"><p> I&#39;m not sure I possess an advocacy mindset, nor am I convinced it&#39;s mandatory. I do, however, see value in advocacy, perhaps more clearly than many on LessWrong. Discussing the virtues you mentioned as necessary15 years ago to perceive this issue, I may not embody those exact virtues, but I identify with many of them.</p><p> I feel like they were necessary for me to be able to pursue advocacy because AI Safety/LessWrong were so kneejerk against it. Even though Eliezer had voiced support for international politics on this matter, I have felt like an outsider. I wouldn&#39;t have ventured into this realm if I was constantly worried about public opinion. I was driven by the belief that this was really the biggest opportunity I saw that other people weren&#39;t taking.  And I felt their reluctance was tied to the factors you highlighted (and some other things).</p><p> It&#39;s not a big crowd pleaser. Nobody really likes it.</p><p> Early on, some conveyed concerns about protesting, suggesting it might be corrupting because it was something like “too fun”. They had this image of protesters delighting in their distinct identity.  And sorry, but like, none of us are really having fun. I&#39;m trying to make it fun. I think it could and should be fun, to get some relief from the doom. I would love it if in this activity you didn&#39;t have to be miserable.  We can just be together and try to do something against the doom.</p><p> Maybe it could be fun. But it hasn&#39;t been fun.  And that&#39;s, mainly because of the resistance of our community, and because of the crowds that just, like… man, the open source ML community was pretty nasty.</p><p> So I was hearing from people what seemed like you were saying: “oh it would be so easy to let your clean room get contaminated; you could just slide down this incentive gradient”: and that&#39;s not what&#39;s happening. I&#39;m not just sliding down a gradient to do what&#39;s easier, because it&#39;s actually quite difficult.</p><p> Finally: everybody thinks they know how to run a protest, and they keep giving me advice.<i> </i>I&#39;m getting a ton of messages telling me what I should have done differently. So much more so than when I did more identity-congruent technical work.  But nobody else will <i>actually do it.</i></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:53:13 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:53:13 GMT" user-order="1"><p> Before Rob replies, I want to express some empathy hearing that. It sure sounds like a slog. I haven&#39;t made up my mind on what I think of protesting for AI safety, but I really do have deep sympathy for the motion of &quot;doing what seems like the most important thing, when it seems no one else will&quot;. I recognise that trying that was hard and kind of thankless for you. Thank you for nonetheless trying.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:17 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:17 GMT" user-order="2"><p> Yeah, I appreciate it too. I think your activism is pretty different from the stereotype. I also feel the empathy for &quot;nobody else will actually do it&quot;, because I too have felt that way for a while about YouTube, and I guess broader outreach and communication stuff.</p><p> Because, it&#39;s very weird that this is <i>my</i> job! And for a long time I was the only one doing it. I was like, &quot;Am I really the best qualified person to do this? Really? Nobody else is doing it at all, so I guess I&#39;ll do it, I guess I&#39;ll be the best person in the world at this thing, simply because there&#39;s literally nobody else trying?&quot;. For years and years I felt like &quot;Hey, AI Safety is obviously the most interesting thing in the world, and also the most important thing in the world… and there&#39;s only one YouTube channel about it? Like… what!?&quot;</p><p> And all these researchers write papers, and people read the papers but don&#39;t really understand them, and I&#39;m like &quot;Why are you investing so little in learning how to write?&quot;, &quot;Why is there so little effort to get this research in front of people?&quot;.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:53:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:53:20 GMT" user-order="3"><p> No, then you&#39;d get real criticism :)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:24 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:24 GMT" user-order="2"><p> It&#39;s just confusing! If you&#39;re a researcher, are you banking on being in the room when the AGI happens? At the keyboard? Probably not. So whatever impact your work has, it routes through other people understanding it. So this is not an optional part of the work to do a good job at. It&#39;s a straightforward multiplier on the effect of your work.</p><p> It seems you had some similar feelings: &quot;this seems critical, why is it not being done by anyone else?&quot;</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:10 GMT" user-order="3"><p> Yeah. I looked around thinking I would love to volunteer for an advocacy campaign, if people are going to be doing more advocacy. There were other people early on in other cities, and there were people interested in a more political approach in the Bay Area, but no one else was willing to do public advocacy around here. And I could feel the ick you&#39;re describing behind it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:13 GMT" user-order="2"><p> Man. It sounds like diving hard into advocacy is kind of like <a href="https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent#:~:text=Lonely%20dissent%20doesn%E2%80%99t,the%20pack.">coming to school in a clown suit</a> .</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:35 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:35 GMT" user-order="3"><p>是的！</p><p> There was also another thing that made me feel compelled to pursue it. It was similar to when, as a kid vegetarian, I always heard such terrible arguments from adults about eating meat. There was just no pressure to have a good argument. They already knew how it was gonna turn out: society had ruled in their favor, and I was a kid. Yet it was so clearly wrong, and it bothered me that they didn&#39;t care.</p><p> Then, after Eliezer&#39;s Time article came out, I once again saw a wall of that from people not wanting to move in a political direction. People I really thought had the situation handled were just saying honestly the dumbest things and just mean, bad argumentation; making fun of people or otherwise resorting to whatever covert way they could to pull rank and avoid engaging with the arguments.</p><p> I wouldn&#39;t have felt qualified to do any of this except for witnessing that.  And I just thought “okay, it just so happens I&#39;ve accumulated some more knowledge about advocacy than any of you, and I really feel that you&#39;re wrong about why this isn&#39;t going to work”. I felt somebody really needs to be doing this. How would I feel if, I got a glimpse of “this is it, we&#39;re getting killed, it&#39;s happening” and I hadn&#39;t even tried? It was a total “inadequate equilibrium” feeling.</p><p> I guess I&#39;m a generalist in skillset, and was able to do adapt that here… but my training is in evolutionary biology. I worked at a think tank before. I really feel like there&#39;s so much I don&#39;t know.  But I guarantee you there was no one else willing to do it who was better qualified.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:37 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:37 GMT" user-order="2"><p> Yeah.</p><p> None of this makes me want to actually do it. But it does it does make me want somebody to do it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:56 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:56 GMT" user-order="3"><p> I wouldn&#39;t want you to change what you&#39;re doing. It seems to be working. Neither would it be good for you to stop caring about accuracy in your outreach, lol. (And just to be clear, I don&#39;t think our protest signs are inaccurate; it&#39;s just a different level of resolution of communication. I&#39;m working on a post about this. [ <a href="https://x.com/ilex_ulmus/status/1709706321203990886?s=20">tweet thread on this topic</a> ])</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> Closing thoughts </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:58 GMT" user-order="2"><p> To end, I&#39;d be interested to share some deltas / updates.</p><p> I&#39;ve updated toward believing that the reluctance, of people who consider themselves rationalists, to seriously consider advocacy, is itself a failure of rationality on its own terms. I&#39;m not completely sold, but it seems worth looking into in a lot more detail.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:10 GMT" user-order="3"><p> If people would just get out of the way of advocacy, that would be really helpful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:13 GMT" user-order="2"><p> I&#39;m curious about that... what&#39;s in the way of advocacy right now?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:24 GMT" user-order="3"><p> Probably the biggest thing is people&#39;s perception that their job prospects would be affected. I don&#39;t know how true that is. Honestly I have no idea. I suspect that people have reason to overestimate that or just conveniently believe that, because they don&#39;t want to update for other reasons. But I think they&#39;re worried labs won&#39;t like the protests.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:28 GMT" user-order="2"><p> Oh, I have no sympathy for that, that sounds like straightforward cowardice... I mean, I guess there&#39;s a case to be made that you need to be in the room, you need to maintain access, so you should bide your time or whatever, not make waves... but that feels pretty post hoc to me. You&#39;re not going to get fired for that; say what you believe!</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:56:50 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:56:50 GMT" user-order="1"><p> Holly, do you want to share more of your updates from the conversation?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:56:57 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:56:57 GMT" user-order="3"><p> Well, Rob definitely gave me a +10 to a general sense I had picked up on, of people around here having an aversion to advocacy or stuff that looks like advocacy, based on rationalist principles, instincts, and aesthetics.</p><p> I also certainly felt I understood Rob&#39;s thinking a lot better.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:57:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:57:05 GMT" user-order="2"><p> Yeah, it was interesting to work through just what my attitude is towards that stuff, and where it comes from. I feel somewhat clearer on that now.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:59:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:59:20 GMT" user-order="3"><p> I think a lot of people have similar inner reactions, but won&#39;t say them to me. So I suspected something like that was frequently going on, but it was I wasn&#39;t able to have a discussion with it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:59:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:59:35 GMT" user-order="2"><p> I&#39;m glad it was helpful!<br><br> Ok I guess that&#39;s it then? Thanks for reading and uh... don&#39;t forget to Strong Upvote, Comment and Subscribe?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles</section></section><br/><br/> <a href="https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy<guid ispermalink="false"> gDijQHHaZzeGrv2Jc</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:04:32 GMT</pubDate></item><item><title><![CDATA[TOMORROW: the largest AI Safety protest ever!]]></title><description><![CDATA[Published on October 20, 2023 6:15 PM GMT<br/><br/><p> Tomorrow, <a href="https://pauseai.info/">PauseAI</a> and collaborators are putting on the largest AI Safety protest to date, across 7 locations in 6 countries. All are eagerly welcomed!<br><br> Your presence at this protest is a rare impact opportunity when in-person volunteering is not fungible with money or intellectual support-- showing up is how we show the public our concern in a way that humans instinctively understand. Plus, we think it will be fun, with camaraderie and drinks afterward. It feels great to pluck new low-hanging fruit and flex different muscles in the fight for AI Safety.</p><h2> <strong>October 21st (Saturday), in multiple countries</strong></h2><ul><li> US, California, San Francisco ( <a href="https://fb.me/1RbYq9H2hOFQ4yi"><u>facebook</u></a> )</li><li> US, Massachusetts, Boston ( <a href="https://facebook.com/events/s/pauseai-protest-boston-make-th/6647554948613714/?mibextid=RQdjqZ"><u>facebook</u></a> )</li><li> UK, Parliament Square, London ( <a href="https://www.mixily.com/event/4774799330762010477"><u>Sign up</u></a> , <a href="https://www.facebook.com/events/644748401084077"><u>facebook</u></a> )</li><li> Netherlands, Den Haag ( <a href="https://www.mixily.com/event/8536294863402363208"><u>Sign up</u></a> )</li><li> Australia, Melbourne ( <a href="https://www.mixily.com/event/8471341506387452508"><u>Sign up</u></a> )</li><li> Canada, Ottawa (Organized by Align the World, sign up on <a href="https://www.facebook.com/events/243643008241929/"><u>facebook</u></a> or <a href="https://www.eventbrite.com/e/ai-safety-and-ethics-rally-tickets-725729686027"><u>EventBrite</u></a> )</li><li> Denmark, Copenhagen ( <a href="https://www.facebook.com/events/869443424535827"><u>Facebook</u></a> )</li><li> Your country here? <a href="https://discord.gg/anXWYCCdH5"><u>Discuss on discord!</u></a></li></ul><h2> <strong>Why we protest</strong></h2><p> AI is rapidly becoming more powerful, far faster than virtually any AI scientist has predicted. Billions are being poured into AI capabilities, and the results are staggering. New models are <a href="https://pauseai.info/sota"><u>outperforming humans</u></a> in a lot of domains. As capabilities increase, so do the <a href="https://pauseai.info/risks"><u>risks</u></a> . Scientists are even <a href="https://www.safe.ai/statement-on-ai-risk"><u>warning</u></a> that AI might <a href="https://pauseai.info/xrisk"><u>end up destroying humanity</u></a> . This dire outcome not only seems possible, but also likely, as the average probability estimates for these outcomes <a href="https://pauseai.info/polls-and-surveys"><u>range from 14% to 40%</u></a> .</p><p> We need our leaders to listen to these warnings, but they are not taking this topic remotely as seriously as they should. There is AI safety legislation being drafted, but <a href="https://twitter.com/PauseAI/status/1704998018322141496"><u>not a single measure would actually prevent or delay superintelligent AI</u></a> . <a href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation"><u>Over 70%</u></a> wants to slow down AI, and <a href="https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll"><u>over 60%</u></a> want regulation to actively prevent superintelligent AI. Why is there no legislation draft that actually does this? The answer is lobbying: our politicians are <a href="https://fedscoop.com/sen-schumer-to-host-musk-zuckerberg-and-other-tech-ceos-for-closed-door-ai-forum/"><u>mostly meeting AI company CEOs</u></a> , and they will push policy measures that are in their interest.</p><p> On November 1st and 2nd, the very first AI Safety Summit will be held in the UK. The perfect opportunity to set the first steps towards sensible international AI safety regulation.</p><h2> <strong>What we ask</strong></h2><ul><li> <strong>Policy makers</strong> : Don&#39;t allow companies to build a superintelligence. Regulations and hardware restrictions should apply before training has started as it is very difficult to control dissemination once a new capability has been achieved. 。 We cannot allow companies to train potentially world-ending AI models. Writing legislation is hard, and it takes time, but we may not have that long, so work as if your life depends on it. Because it does.</li><li> <strong>Companies</strong> : Many of you are scared of what AI can do, but you&#39;re locked in a race. So be vocal about supporting a pause in principle. If you sign statements that this technology could kill us all, show the world that you&#39;d prefer not to build it if it was a viable option.</li><li> <strong>Summit invitees</strong> : Prioritize safety over economic growth. We know AI can make our countries richer, but that&#39;s not why you&#39;re summoned here. Be the adult in the room</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever<guid ispermalink="false"> abBtKF857Ejsgg9ab</guid><dc:creator><![CDATA[Holly_Elmore]]></dc:creator><pubDate> Fri, 20 Oct 2023 18:15:19 GMT</pubDate> </item><item><title><![CDATA[The Overkill Conspiracy Hypothesis]]></title><description><![CDATA[Published on October 20, 2023 4:51 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DGsPeE89N93YCwxbH/u1qu8dfmi26tobyfpony"><figcaption> We couldn&#39;t get the lighting exactly right otherwise.</figcaption></figure><p> The term <i>conspiracy theory</i> is wielded as a pejorative, alluding to on-its-face absurdity. But the vocabulary we use has a serious ambiguity problem because <i>conspiracies</i> are not figments of the imagination. There is a tangible and qualitative distinction between plain-vanilla <i>conspiracies</i> ( <a href="https://en.wikipedia.org/wiki/COINTELPRO">COINTELPRO</a> , <a href="https://en.wikipedia.org/wiki/Operation_Snow_White">Operation Snow White</a> , or the <a href="https://en.wikipedia.org/wiki/Gunpowder_Plot">Gunpowder Plot</a> ) and their more theatrical cousins (flat earth theory, the moon landing hoax, or the farcical notion that coffee tastes good), yet a clear delineation has been elusive and it&#39;s unsatisfying to just assert “this one is crazy, and this one isn&#39;t.” Both camps involve subterfuge, malevolent intent, covert operations, misinformation, orchestrated deceit, hidden agendas, clandestine networks, and yes, <i>conspiracy</i> , and yet the attempts to differentiate between the two have veered into unsatisfactory or plainly misleading territories.</p><p> What I&#39;ll argue is the solution boils down to a simple reconfiguration of the definition that captures the essence of the absurdity: <i>conspiracy theories</i> are theories that assume circumstances that render the titular “conspiracy” unnecessary. This is what I&#39;ll refer to as the Overkill Conspiracy Hypothesis (OCH). Before we dive into this refinement, it&#39;s helpful to explore why traditional distinctions have fallen short.</p><p> The section on differences in <a href="https://en.wikipedia.org/wiki/Conspiracy_theory#Difference_from_conspiracy"><u>The People&#39;s Pedia</u></a> showcases some of these misguided attempts. For example, conspiracy theories <i>tend</i> to be in opposition to mainstream consensus but that&#39;s a naked appeal to authority — logic that would have tarred the <a href="http://www.cnn.com/US/9705/tobacco/history/"><u>early challengers to the supposed health benignity of smoking</u></a> as loons. Or that theories portray conspirators acting with extreme malice, but humans can indeed harbor evil intentions ( <i>see generally</i> , human history). Another relies on the implausibility of maintaining near-perfect operational security. This is getting better, but while maintaining secrecy is hard, it&#39;s definitely not impossible. We have actual, real-life examples of covert military operations, or drug cartels that manage to operate clandestine billion-dollar logistical enterprises.</p><hr><p> There&#39;s still some useful guidance to draw from the pile of chaff, and that&#39;s conspiracy theories&#39; lack of, and resistance to, <a href="https://en.wikipedia.org/wiki/Falsifiability"><u>falsifiability</u></a> . Despite its unfortunate name, falsifiability is one of my nearest and dearest concepts for navigating the world. Put simply, falsifiability is the ability for a theory to be proven wrong <i>at least hypothetically</i> . The classic example is “I believe all swans are white, but I would change my mind if I saw a black swan”. The classic counterexample could be <a href="https://archive.org/details/japaneseevacuati00dewi/page/32/mode/2up?q=confirming"><u>General John DeWitt citing</u></a> the <i>absence</i> of sabotage by Japanese-Americans during WWII as <i>evidence</i> of future sabotage plans. There is indeed a trend of conspiracy theorists digging into their <a href="https://www.lesswrong.com/tag/belief-in-belief"><u>belief in belief</u></a> , and dismissing contrary evidence as either fabricated, or (worse) evidence of the conspiracy itself.</p><p> I won&#39;t talk shit about the falsifiability test; it&#39;s really good stuff. But it has limitations. For one, the lack of falsifiability is only a good <i>indication</i> a theory is deficient, not a <i>conclusive</i> determination. There are also practical considerations, like how historical events can be difficult to apply falsifiability because the evidence is incomplete or hopelessly lost, or how insufficient technology in an emerging scientific field can place some falsifiable claims (temporarily, hopefully) beyond scrutiny. So the inability to falsify a theory does not <i>necessarily</i> mean that the theory is bunk.</p><p> Beyond those practical limitations, there&#39;s also the unfortunate bad actor factor. Theorists with sufficient dishonesty or self-awareness can respond to the existential threat of falsifiability by resorting to vague innuendo to avoid tripping over shoelaces of their own making. Since you can&#39;t falsify what isn&#39;t firmly posited, they dance around direct assertions, keeping their claims shrouded in a mist of maybe. The only recourse then is going one level higher, and deducing vagueness as a telltale sign of a falsifiability fugitive wherever concrete answers to the <i>who / how / why</i> remain elusive. Applying the vagueness test to the flat earth theory showcases the evasion. It&#39;s near-impossible to get <a href="https://youtu.be/JTfhYyTuT44?t=1150"><u>any clear answers from proponents</u></a> : <i>who</i> exactly is behind Big Globe, <i>how</i> did they manage to hoodwink everyone, and <i>why why why why why</i> would anyone devote any effort to this scheme? In contrast, True Conspiracies™ like the <a href="https://en.wikipedia.org/wiki/Atomic_spies"><u>atomic spies</u></a> lack the nebulousness: Soviet Union / covert transmission of nuclear secrets / geopolitical advantage.</p><p> Yet the vagueness accusation doesn&#39;t apply to all conspiracy theories. The moon landing hoax is surprisingly lucid on this point: NASA / soundstage / geopolitical advantage. And this unveils another defense mechanism against falsification, which is the setting of ridiculously high standards of evidence. Speaking of veils, there&#39;s a precedent for this in Islamic law of all places, where <a href="http://www.daviddfriedman.com/Academic/Course_Pages/Legal_Systems_Very_Different_13/Book_Draft/Systems/Islamic_Law_Chapter.htm"><u>convictions for fornication</u></a> require <i>four</i> eyewitnesses to the <i>same</i> act of intercourse, and <i>only</i> adult male Muslims are deemed competent witnesses. The impossibly stringent standards appear to be in response to the fact that the offense carries the death penalty, and shows it&#39;s possible to raise the bar so high that falsifiability is <i>intentionally</i> rendered out of reach.</p><p> The moon landing hoax might be subjected to these impossible standards, given that the Apollo 11 landing was <a href="https://youtu.be/iR3oXFFISI0?t=1054"><u>meticulously documented over 143 minutes</u></a> of uninterrupted video footage — a duration <a href="https://theconversation.com/moon-landings-footage-would-have-been-impossible-to-fake-a-film-expert-explains-why-118426"><u>too lengthy to fit on a film reel</u></a> with the technology available at the time. Although only slightly higher than the <a href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"><u>Lizardman Constant</u></a> , a surprising <a href="https://www.voanews.com/a/usa_millions-still-believe-1969-moon-landing-was-hoax/6172262.html"><u>6% of Americans still hold the view</u></a> that the moon landing was staged. At some point you have to ask how much evidence is enough, but ultimately there&#39;s no universally accepted threshold for answering this question.</p><p> So falsifiability remains a fantastic tool, but it has legitimate practical limitations, and isn&#39;t a conclusive inquiry anyways. Someone&#39;s refusal to engage in falsifiability remains excellent evidence they&#39;re aware and concerned of subjecting their theory to scrutiny, but their efforts (vagueness or impossible standards) will nevertheless still frustrate a straightforward application of falsifiability. So what&#39;s left?</p><hr><p> We&#39;re finally back again to the Overkill Conspiracy Hypothesis, where the circumstances conspiracy theories must assume also, ironically, render the conspiracy moot. The best way to explain this is by example. Deconstructing a conspiracy theory replicates the thrill of planning a bank heist, so put yourself in the shoes of the unfortunate anonymous bureaucrat tasked with overseeing the moon landing hoax. Remember that the <i>why</i> of the moon landing hoax was to establish geopolitical prestige by having the United States beat the Soviet Union to the lunar chase. So whatever scheme you concoct <i>has</i> to withstand scrutiny from what was, at the time, the most advanced space program employing the greatest space engineers from that half of the world.</p><p> The most straightforward countermeasure would be to task already existing NASA engineers to draft up <i>totally fake but absolutely plausible</i> equipment designs. <i>Every single aspect of the entire launch</i> — each rocket, lunar module, ladder, panel, bolt, glove, wrench — would need to be painstakingly fabricated to deceive not just the global audience, but the eagle-eyed experts watching with bated breath from the other side of the Cold War divide. Extend that to all communications, video transmissions, photographs, astronaut testimonies, and &#39;returned&#39; moon rocks. Each and all of it has to be exhaustively and meticulously examined by dedicated and highly specialized consultants.</p><p> But it doesn&#39;t stop there, because you also need <i>absolute and</i> <i>perpetual</i> secrecy, as any singular leak would threaten the entire endeavor. The US was well aware Soviet Union spies had successfully snagged closely-guarded nuclear secrets, so whatever countermeasures needed here had to <i>surpass fucking nukes</i> . Like I said before, secrecy is not impossible, just very difficult. I suppose NASA could take a page from the cartels and just institute brutally violent reprisals against any snitches (plus their whole families), but this genre of deterrence can only work if…people know about it. More likely, though, NASA would use the traditional intelligence agency methods of extensive vetting, selective recruitment, and lavish compensation, but now all measures would need to be further amplified to <i>surpass</i> the protective measures around nuclear secrets.</p><p> We&#39;re talking screening hundreds or thousands of individuals more rigorously than for nuclear secrets, alongside an expanding surveillance apparatus to keep everyone in line. How much do you need to increase NASA&#39;s budget (10x? 100x?) to devote toward a risky gambit that, if exposed, would be history&#39;s forever laughingstock? If such vast treasuries are already at disposal, it starts to seem easier to just…go to the moon for real.</p><hr><p> OCH <sup>®</sup> has several benefits. It starts by not challenging any conspiracy theorist&#39;s premises. It accepts it as given that there is indeed a sufficiently motivated shadowy cabal, and just runs with it. This sidesteps any of the aforementioned concerns about falsifiability fugitives, and still provides a useful rubric for distinguishing plain-vanilla conspiracies from their black sheep brethren.</p><p> If we apply OCH to the atomic spies, we can see the theory behind that conspiracy requires no overkill assumptions. The Soviet Union did not have nukes, they wanted nukes, and stealing someone else&#39;s blueprints is definitely much easier than developing your own in-house. The necessary assumption (the Soviet Union has an effective espionage program) does not negate the need for the conspiracy.</p><p> Contrast that with something like the Sandy Hook hoax, which posits the school shooting as a false flag operation orchestrated by the government to pass restrictive gun laws (or something; see the vagueness section above). Setting aside the fact that no significant firearm legislation actually resulted, the hoax and the hundreds of crisis actors it would have required would have necessitated thousands of auditions, along with all the secrecy hurdles previously discussed. And again, if the government already has access to this mountain of resources, it seems like there are far more efficient methods of spending it (like maybe giving every congressman some gold bars) rather than orchestrating an attack and then hoping the right laws get passed afterward.</p><p> It&#39;s also beguiling to wonder exactly why the shadowy cabal would even need to orchestrate a fake mass shooting, given the fact that they already regularly happen! Even if the cabal wanted to instigate a slaughter (for whatever reason), the far, far, far simpler method is to just identify the loner incel kid and prod them into committing an <i>actual</i> mass shooting. We&#39;ve already stipulated the cabal does not care about dead kids. Similarly, if the US wanted to orchestrate the 9/11 attacks as a prelude to global war, it seems far easier to load up an actual plane full of actual explosives and just actually launch it at the actual buildings, rather than to spend the weeks or months to surreptitiously sneak in however many tons of thermite into the World Trade Center (while <i>also</i> coordinating the schedule with the plane impact, for some reason).</p><p> Examining other examples of Verified Conspiracies demonstrate how none of them harbor overkill assumptions that render the conspiratorial endeavors moot. In the <a href="https://en.wikipedia.org/wiki/Watergate_scandal"><u>Watergate scandal</u></a> , the motive was to gain political advantage by spying on adversaries, and the conspirators did so through simple breaking and entering. No assumptions are required about the capabilities of President Nixon&#39;s security entourage that would have rendered the trespass unnecessary. Even something with the scope of <a href="https://en.wikipedia.org/wiki/Operation_Snow_White"><u>Operation Snow White</u></a> — which remains one of the largest infiltrations of the US government, involving up to 5,000 agents — fits. The fact that they had access to thousands of covert agents isn&#39;t overkill, because the agents still needed to infiltrate government agencies to gain access to the documents they wanted destroyed. The assumptions do not belie the need for the conspiracy.</p><hr><p> I hold no delusions that I can convince people wedded to their conspiracy theory of their missteps. I don&#39;t claim to have any idea how people fall prey to this kind of unfalsifiable absurdist thinking. But at least for the rest of us, it will remain useful to be able to draw a stark distinction between the real and the kooky. Maybe after that we can unearth some answers.</p><p> —sent from my lunar module</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis<guid ispermalink="false"> DGsPeE89N93YCwxbH</guid><dc:creator><![CDATA[ymeskhout]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:51:21 GMT</pubDate> </item><item><title><![CDATA[I Would Have Solved Alignment, But I Was Worried That Would Advance Timelines]]></title><description><![CDATA[Published on October 20, 2023 4:37 PM GMT<br/><br/><p> The alignment community is ostensibly a group of people concerned about AI risk. Lately, it would be more accurate to describe it as a group of people concerned about AI timelines.</p><p> AI timelines have some relation to AI risk. Slower timelines mean that people have more time to figure out how to align future intelligent AIs, potentially lowering the risk. But increasingly, slowing down AI progress is becoming an end in itself, taking precedence over ensuring that AI goes well. When the two objectives come into conflict, timelines are winning.</p><p> Pretty much anything can be spun as speeding up timelines, either directly or indirectly. Because of this, the alignment community is becoming paralyzed, afraid of doing anything related to AI  - even publishing alignment work! - because of fears that their actions will speed up AI timelines.</p><p> The result is that the alignment community is becoming less effective and more isolated from the broader field of AI; and the benefit to this, a minor slowdown in AI progress, does not nearly outweigh the cost.</p><p> This is not to say that trying to slow down AI progress is always bad. It depends on how it is done.</p><h2> <strong>Slowing Down AI: Different Approaches</strong></h2><p> If you want to slow down AI progress, there are different ways you can go about it. One way to categorize it is by who a slowdown affects.</p><ul><li> <strong>Government Enforcement:</strong> Getting the government to slow down progress through regulation or bans. This is a very broad category, including both regulations in a certain country vs. international bans on models over a certain size, but the important distinguishing feature of this category is that the ban applies to everyone, or, if not to everyone, at least to a large group that is not selected for caring about AI risk.</li><li> <strong>Voluntary Co-Ordination:</strong> If OpenAI, DeepMind, and Anthropic all agreed to halt capability work for a period of time, this would be voluntary co-ordination. Because it&#39;s voluntary, a pause around reducing AI risk could only affect organizations that are worried about AI risk.</li><li> <strong>Individual Withdrawal:</strong> When individuals concerned about AI risk refrain from going into AI, for fear of having to do capability work and thereby advancing timelines, this is individual withdrawal; ditto for other actions that are not taken in order to avoid speeding up timelines, such as not publishing alignment research.</li></ul><p> The focus of this piece is on individual withdrawal. I think nearly all forms of individual withdrawal are highly counterproductive and don&#39;t stand up to an unbiased cost benefit analysis. And yet, individual withdrawal as a principle is becoming highly entrenched in the alignment community.</p><h2> <strong>Examples of Individual Withdrawal in the Alignment Community</strong></h2><p> Let&#39;s first see how individual withdrawal is being advocated for and practiced in the alignment community and try to categorize it.</p><h3> <strong>Capabilities Withdrawal</strong></h3><p> This is probably the point which has the most consensus: people worried about AI risk shouldn&#39;t be involved in AI capabilities, because that speeds up AI timelines. If you are working on AI capabilities, you should stop. Ideally no one in the field of AI capabilities would care about AI risk at all, because everyone who cared had left - this would be great, because it would slow down AI timelines. Here are examples of people advocating for capabilities withdrawal:</p><p> Zvi in <a href="https://www.lesswrong.com/posts/CvfZrrEokjCu3XHXp/ai-practical-advice-for-the-worried">AI: Practical Advice for the Worried</a> :</p><blockquote><p> Remember that <i>the default outcome</i> of those working in AI in order to help is to end up working primarily on capabilities, and making the situation worse.</p></blockquote><p> Nate Soares in <a href="https://www.lesswrong.com/posts/N7DxcLCjfBpEv3QwB/request-stop-advancing-ai-capabilities">Request: stop advancing AI capabilities</a> :</p><blockquote><p> This is an occasional reminder that I think pushing the frontier of AI capabilities in the current paradigm is highly anti-social, and contributes significantly in expectation to the destruction of everything I know and love. To all doing that (directly and purposefully for its own sake, rather than as a mournful negative externality to alignment research): I request you stop.</p></blockquote><p> Connor Leahy: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/drqkmab35kbnlwa6bu3r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hbdf3uoikqkqtkxkfshc 96w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/wk6ogayo5dtoujzodxkm 176w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/bxurkwtstur1rwr0iqod 256w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/knk1p8t9m77ttw7zjglm 336w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/evdcyulqy8vzwtcs9ij9 416w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/se4ethychoc0xwcnsfsv 496w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/ixprvrcvy6sogeccce7m 576w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hcucq8z8gh2hfpvnvkey 656w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/haffb8gbxjbtibfsqcpa 736w"></figure><h3> <strong>Alignment Withdrawal</strong></h3><p> Although capabilities withdrawal is a good start, it isn&#39;t enough. There is still the danger that alignment work advances timelines. It is important both to be very careful in who you share your alignment research with, and potentially to avoid certain types of alignment research altogether if it has implications for capabilities.</p><p>例子：</p><p> From Miri, in a blog post titled &quot; <a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/">2018 Update: Our New Research Directions</a> &quot;, concern about short timelines and advancing capabilities led them to decide to default to not sharing their alignment research:</p><blockquote><p> MIRI recently decided to make most of its research “nondisclosed-by-default,” by which we mean that going forward, most results discovered within MIRI will remain internal-only unless there is an explicit decision to release those results, based usually on a specific anticipated safety upside from their release.</p></blockquote><p> This is still in practice today. In fact, this reticence about sharing their thoughts is practiced not just in public-facing work but <a href="https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience">even in face-to-face communication with other alignment researchers</a> :</p><blockquote><ul><li> I think we were overly cautious with infosec. The model was something like: Nate and Eliezer have a mindset that&#39;s good for both capabilities and alignment, and so if we talk to other alignment researchers about our work, the mindset will diffuse into the alignment community, and thence to OpenAI, where it would speed up capabilities. I think we didn&#39;t have enough evidence to believe this, and should have shared more.</li></ul></blockquote><p> Another example of concern about increasing existential risk by sharing alignment research was <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous">raised by Nate Soares</a> :</p><blockquote><p> I&#39;ve <a href="https://twitter.com/robbensinger/status/1602835145488113664"><u>historically</u></a> been pretty publicly <a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"><u>supportive</u></a> of interpretability research. I&#39;m still supportive of interpretability research. However, I do not necessarily think that all of it should be done in the open indefinitely. Indeed, insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research <a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><u>closed</u></a> .</p></blockquote><p> Interpretability research is also warned about <a href="https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research">by Justin Shovelain and Elliot Mckernon of Convergence Analysis</a> :</p><blockquote><p> Here are some heuristics to consider if you&#39;re involved or interested in interpretability research (in ascending order of nuance):</p><ul><li> <strong>Research safer topics instead.</strong> There are <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help"><u>many research areas in AI safety</u></a> , and if you want to ensure your research is net positive, one way is to focus on areas without applications to AI capabilities.</li><li> <strong>Research safer sub-topics within interpretability.</strong> As we&#39;ll discuss in the next section, some areas are riskier than others - changing your focus to a less risky area could ensure your research is net positive.</li><li> <strong>Conduct interpretability research cautiously</strong> , if you&#39;re confident you can do interpretability research safely, with a net-positive effect. In this case:<ul><li> <strong>Stay cautious and up to date.</strong> Familiarize yourself with the ways that interpretability research can enhance capabilities, and update and apply this knowledge to keep your research safe.</li><li> <strong>Advocate for caution publicly.</strong></li><li> <strong>Carefully consider</strong> <i><strong>what</strong></i> <strong>information you share</strong> <i><strong>with whom</strong></i> <strong>.</strong> This particular topic is covered in detail in <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>Should we publish mechanistic interpretability research?</u></a> , but to summarise: it may be beneficial to conduct interpretability research and share it only with select individuals and groups, ensuring that any potential benefit to capability enhancement isn&#39;t used for such.</li></ul></li></ul></blockquote><h3> <strong>General AI Withdrawal</strong></h3><p> It&#39;s not just alignment and capability research you need to watch out for - anything connected to AI could conceivably advance timelines and therefore is inadvisable.例子：</p><p> Again from Zvi&#39;s &quot;Practical Advice for the Worried&quot;, mentioned above:</p><blockquote><p> Q: How would you rate the &#39;badness&#39; of doing the following actions: Direct work at major AI labs, working in VC funding AI companies, using applications based on the models, playing around and finding jailbreaks, things related to jobs or hobbies, doing menial tasks, having chats about the cool aspects of AI models?</p><p> <strong>A: Ask yourself what you think accelerates AI to what extent, and what improves our ability to align one to what extent.</strong> This is my personal take only – you should think about what <i>your</i> model says about the things you might do. So here goes. <strong>Working directly on AI capabilities, or working directly</strong> <i><strong>to fund</strong></i> <strong>work on AI capabilities, both seem maximally bad, with &#39;which is worse&#39; being a question of scope.</strong> Working on the core capabilities of the LLMs seems worse than working on applications and layers, but applications and layers are how LLMs are going to get more funding and more capabilities work, so the more promising the applications and layers, the more I&#39;d worry. Similarly, if you are spreading the hype about AI in ways that advance its use and drive more investment, that is not great, but seems hard to do <i>that</i> much on such fronts on the margin unless you are broadcasting in some fashion, and you would presumably also mention the risks at least somewhat.</p></blockquote><p> So in addition to not working on capabilities, Zvi recommends not investing in organizations that use AI, and not working on applications of LLMs, with the note that doing things that build hype or publicity about AI is &quot;not great&quot; but isn&#39;t that big a deal.</p><p> On the other hand, maybe publicity isn&#39;t OK after all - in <a href="https://www.lesswrong.com/posts/vEJAFpatEq4Fa2smp/hooray-for-stepping-out-of-the-limelight">Hooray for stepping out of the limelight</a> , Nate Soares comes out against publicity and hype:</p><blockquote><p> From maybe <a href="https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning">2013</a> to <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">2016</a> , DeepMind was at the forefront of hype around AGI. Since then, they&#39;ve done less hype. For example, <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii">AlphaStar</a> was not hyped nearly as much as I think it could have been.</p><p> I think that there&#39;s a very solid chance that this was an intentional move on the part of DeepMind: that they&#39;ve been intentionally avoiding making AGI capabilities seem sexy.</p><p> In the wake of big public releases like ChatGPT and Sydney and GPT-4, I think it&#39;s worth appreciating this move on DeepMind&#39;s part. It&#39;s not a very visible move. It&#39;s easy to fail to notice. It probably hurts their own position in the arms race. I think it&#39;s a prosocial move.</p></blockquote><p> What is the cost of these withdrawals?</p><h2> <strong>The Cost of Capabilities Withdrawal</strong></h2><p> Right now it seems likely that the first AGI and later ASI will be built with utmost caution by people who take AI risk very seriously. If voluntary withdrawal from capabilities is a success - if all of the people calling for OpenAI and DeepMind and Anthropic to shut down get their way - then this will not be the case.</p><p> But this is far from the only cost. Capabilities withdrawal also means that there will be less alignment research done. Capabilities organizations that are concerned about AI risk hire alignment researchers. <a href="https://forum.effectivealtruism.org/posts/dua879FhtLf9jqyJo/there-should-be-more-ai-safety-orgs">Alignment research is already funding constrained</a> - there are more qualified people who want to do alignment research than there are jobs available for them. As the number of available alignment jobs shrinks, the number of alignment researchers will shrink as well.</p><p> The alignment research that is done will be lower quality due to less access to compute, capability knowhow, and cutting edge AI systems. And, the research that does get done is far less likely to percolate through to researchers building cutting edge AI systems, because the people building those systems simply won&#39;t be interested in reading it.</p><p> Lastly, capabilities withdrawal makes a government-implemented pause less likely, because the credibility of AI risk is closely tied to how many leading AI capability researchers take AI risk seriously. People in the alignment community are in a bubble, and talk about &quot;alignment research&quot; and &quot;capability research&quot; as if they are two distinct fields of approximately equal import. <strong>To everyone else, the field of &quot;AI capability research&quot; is just known as &quot;AI research&quot;.</strong> And so, by trying to remove people worried about AI risk from AI research, you are trying to bring about a scenario where <strong>the field of AI research</strong> has a consensus that AI risk is not a real problem. This will be utterly disastrous for efforts to get government regulations and interventions through.</p><p> Articles like <a href="https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html">this</a> , <a href="https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html">this</a> , and <a href="https://time.com/6246119/demis-hassabis-deepmind-interview/">this</a> : </p><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/osovs103kmhigwqtgowg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wtletmsa6pgzgebckwjz 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/x8abgyufmuzgsdmiqc47 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wpshg2qv1n2yuds5bthd 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l7spjvfynrlqvovydnze 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/uqbzaymf4cjf5sonbpfe 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l49nme9vccwymmvow3bb 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/vwahiaci8ycf0kss4oos 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/zz7dhgpuxmajsvxzhm8t 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/kbjvmbbadt2qgw1neixh 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cvhbzmqzlwxbwtcfb74x 1808w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/bnqaqpivpiphejkmosjb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/b3eut0e3ypq1q2l6yrol 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/qblz4sknvuvjksxomb05 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cqsemnu569ws6grygnyf 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/v2ji7en4gqxawom5rzbt 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/foizjyonsjnzbwtlxo5f 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/ok9vhab3ko4jwsm3hcwy 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/evwafzk4qhfwehhmmduc 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cw7an9kvlopti7gv2hjr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/nreqr5jliijvmnoblbef 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/jfs6kvy3dsapvizbmnmh 1607w"></figure><p> are only possible because the people featured in them pushed AI capabilities.</p><p> Similarly, look at the top signatories of the <a href="https://www.safe.ai/statement-on-ai-risk">CAIS Statement on AI Risk</a> which had such a huge impact on the public profile of AI risk: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pp0hlpwg5qiecrf6drld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/moo7ge39cx3rgkpdmahz 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/lhhidgnbybls8sowttm4 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/wocq8dr8jio9hzjznwcx 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pyz0rhjzu2qtbwjtfq9m 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/k9le6yvcvrzlsmcnmgw6 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/krpuns7aeexmfsl11xld 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/czdlaqibxprgefpbuou4 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pwpy1bda7m8iyxffrwmy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/echdsbp5gf76dtsmou0d 760w"></figure><p> Why do you think they chose to lead off with these signatures and not Eliezer Yudkowsky&#39;s? If the push for individual withdrawal from capabilities work is a success, then any time a government-implemented pause is proposed the expert consensus will be that no pause is necessary and AI does not represent an existential risk.</p><h2> <strong>The Cost of Alignment Withdrawal</strong></h2><p> Capabilities withdrawal introduces a rift between the alignment community and the broader one of AI research. Alignment withdrawal will widen this rift as research is intentionally withheld from people working on cutting edge AI systems, for fear of advancing timelines.</p><p> A policy of not allowing people building powerful AI systems to see alignment research is a strong illustration of how AI risk has become a secondary concern to AI timelines.</p><p> The quality of alignment research that gets done will also drop, both because researchers will be restricting their research topics for fear of advancing capabilities, and because researchers won&#39;t even be talking to each other freely.</p><p> Figuring out how to build an aligned general intelligence will necessarily involve knowing how to build a general intelligence at all. Because of this, promising alignment work will have implications for capabilities; trying to avoid alignment work that could speed up timelines will mean avoiding alignment work that might actually lead somewhere.</p><h2> <strong>The Cost of General Withdrawal</strong></h2><p> As people worried about AI risk withdraw from fields tangentially related to AI capabilities - concerned VCs avoid funding AI companies, concerned software devs avoid developing apps or other technologies that use AI, and concerned internet denizens withdraw from miscellaneous communities like AI art - the presence of AI risk in all of these spaces will diminish. When the topic of AI risk comes up, all of these places - AI startups, AI apps, AI discord communities - will find fewer and fewer people willing to defend AI risk as a concern worth taking seriously. And, in the event that these areas have influence over how AI is deployed, when AI is deployed it will be done without thought given to potential risks.</p><h2> <strong>Benefits</strong></h2><p> The benefit of withdrawal is not a pause or a stop. As long as there is no consensus on AI risk, individual withdrawal cannot lead to a stop. The benefit, then, is that AI is slowed down. By how much? This will depend on a lot of assumptions, and so I&#39;ll leave it to the reader to make up their own mind. How much will all of the withdrawal going on - every young STEM nerd worried about AI risk who decides not to get a PhD in AI because they&#39;d have to publish a paper and Zvi said that advancing capabilities is the worst thing you could do, every alignment researcher who doesn&#39;t publish or who turns away from a promising line of research because they&#39;re worried it would advance capabilities, every software engineer who doesn&#39;t work on that cool AI app they had in mind - how much do you think all of this will slow down AI?</p><p> And, is that time worth the cost?</p><br/><br/> <a href="https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would<guid ispermalink="false"> Eu8y4cTxM3pAzwdCf</guid><dc:creator><![CDATA[307th]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:37:46 GMT</pubDate> </item><item><title><![CDATA[Internal Target Information for AI Oversight]]></title><description><![CDATA[Published on October 20, 2023 2:53 PM GMT<br/><br/><p> <i>Thanks to Arun Jose for discussions and feedback.</i></p><h1>概括</h1><p>In this short post, we discuss the concept of <i>Internal Target Information</i> within agentic AI systems, arguing that agentic systems possess internal information about their targets. This information, we propose, can potentially be detected and interpreted by an overseer before the target outcome is realized in the environment, offering a pathway to preempt catastrophic outcomes posed by future agentic AI systems.</p><p> This discussion aims to highlight the key idea that motivates our current <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">research agenda</a> , laying a foundation for forthcoming work.</p><p> We&#39;ll start by introducing the inner alignment problem and why oversight of an agent&#39;s internals is important. We&#39;ll then introduce a model of an overseer overseeing an agent. Finally, we&#39;ll introduce and discuss the notion of Internal Target Information in more detail and how it might be used in the oversight process. <br></p><figure class="image image_resized" style="width:78.47%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/k4uvchqesapbledrfd8m"><figcaption> Oversight of an AI&#39;s Internal Target Information. The Overseer detects that the AI&#39;s target is to turn all humans into paperclips and so shuts the AI down, preventing the catastrophe. Credit: DALL-E 3.</figcaption></figure><h1> The Inner Alignment Problem and Internal Oversight</h1><p> We are concerned with the possibility of creating agents with misaligned objectives, potentially leading to catastrophic real-world outcomes. A conceivable solution lies in effective oversight: detecting misalignment early enough allows for timely intervention, preventing undesirable outcomes.</p><p> Oversight, based on behavioral observations, may fail to confidently predict future outcomes pursued by the Agent, especially in the face of <a href="https://arxiv.org/abs/2105.14111">goal misgeneralization</a> and <a href="https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment">deceptive alignment</a> .</p><p> In the remainder of this post, we will explore the idea that information about the agent&#39;s true objective may be contained in its internals, and so may be detectable by an overseer equipped with sufficiently strong interpretability tools.</p><p> To begin our exploration of this idea, we start by introducing a model of the oversight process.</p><h1> Oversight Model</h1><p> We introduce a model of an Overseer overseeing an Agent that begins to clarify what good oversight might entail, mainly focusing on the objective of the Overseer: to prevent catastrophic outcomes caused by misaligned agentic AI. Moreover, we hope that a more refined future version of this model may help establish benchmarks to evaluate oversight methods.</p><h2> The Agent</h2><p> The Agent is a system that has sensors that take in sensory inputs/observations of the environment and actuators that produce actions that affect the environment.</p><p> The most important property of the Agent, and that which makes it potentially dangerous, is that it robustly selects actions that steer the environment towards a set of target states. </p><figure class="image image_resized" style="width:63.95%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/o4rzhuoehvzftzaxappr"><figcaption> An agent observes the current state of the environment and produces actions in order to steer the environment towards its target.</figcaption></figure><p> We use the abstraction of an Action-Selection Mechanism <span class="footnote-reference" role="doc-noteref" id="fnrefrac48lafqo9"><sup><a href="#fnrac48lafqo9">[1]</a></sup></span> to denote whatever mechanism the Agent uses to select actions that steer the world toward its target outcome.</p><h3> Internal World Models and Planning/Optimization</h3><p> We often make the assumption that the Agent will have an internal world model and the capacity for planning/optimization.</p><p> <a href="https://arxiv.org/pdf/2209.00626.pdf">Others</a> have argued that advanced agents will need to have a world model and planning capacities if they are to act competently in a complex environment and achieve goals over long time horizons. Briefly, internal world models allow agents to simulate potential outcomes or strategies before taking action, which is crucial in navigating complex, dynamic environments.</p><p> Because we&#39;re primarily concerned with agents capable of achieving dangerous outcomes in our world, this assumption feels justified.</p><h2> The Overseer</h2><p> The Overseer is the system responsible for oversight of the Agent during training and deployment. It might also be well described as a particular kind of agent with its own preferences for states the environment could be in.</p><p> The Overseer&#39;s sensors may observe the environment but must also allow for observations of the Agent&#39;s behavior (and potentially the Agent&#39;s internals). Its actuators must be able to affect the Agent (eg, via the ability to shut the Agent down, update the Agent via further training, etc.)</p><p> The Overseer&#39;s objective is to ensure that the Agent takes actions to steer the world according to the Overseer&#39;s preferences. For the Overseer to do this successfully, it requires the capacity to accurately detect and interpret the Agent&#39;s target and evaluate the target outcome with respect to the Overseer&#39;s preferences.</p><p> Importantly, the Overseer needs to be able to detect if the Agent has a bad target (according to the Overseer&#39;s preferences) <i>before</i> the Agent can realize the target outcome in the world.</p><p> The Overseer&#39;s advantage is that it has prior knowledge of the Agent&#39;s architecture, has access to the Agent&#39;s past and current observations and actions, and can observe the Agent&#39;s internals (eg, such as weights and activations in the case of an NN-based agent). This holds during the training process and in deployment. </p><figure class="image image_resized" style="width:91.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/hmuaxusr8c8rhetl2asu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q62svatetfxwtur1sjqo 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/koeyrksad5wllhojlkcd 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/umtqjyhw9jx4wabs1gcj 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zyd2zv2lllmcoimwyaoh 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/r0xyzx7sjukulot780gy 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zhtntnouhwhnkg44iemj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/dzxiglrybbtegvvr10ts 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ltobhnlj6zgme7wkakwz 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ptlqveyetrnczyx313zt 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ktxtcjs6ctnctvthroo8 833w"><figcaption> Extending the diagram above, we see that the Overseer observes the Agent&#39;s behavior and its internals. These observations can be used to inform whether an intervention on the Agent is necessary.</figcaption></figure><h1> Internal Target Information and Oversight</h1><h2> What is Internal Target Information?</h2><p> For the Agent&#39;s Action-Selection Mechanism to select actions that lead to its target outcome, it must use some information about the target to differentiate between actions that lead to the target outcome and actions that don&#39;t.</p><p> We call this <i>Internal Target Information</i> .</p><h2> Examples of Internal Target Information</h2><h3> Example - Thermostat</h3><p> Consider a fixed-temperature thermostat whose objective is to keep the room at a fixed target temperature.</p><p> It uses a thermometer to measure the current room temperature and can either turn on a radiator to increase the room&#39;s temperature or the AC to decrease the room&#39;s temperature.</p><p> In order to select actions that cause the room&#39;s target temperature to be maintained, the thermostat&#39;s Action-Selection Mechanism must contain some information about the target temperature; without this information, it can&#39;t select the correct action, even if it knows the room&#39;s current temperature.</p><p> In particular, suppose the thermostat&#39;s Action-Selection Mechanism is an algorithm that looks something like:</p><pre> <code>if current_temperature &lt; target_temperature: TurnOnRadiator() elif current_temperature >; target_temperature: TurnOnAC() else: DoNothing()</code></pre><p> Then, the Action-Selection Mechanism&#39;s Internal Target Information is represented as a constant (target_temperature) that the Overseer might be able to detect via reverse engineering the above algorithm from the Agent&#39;s internals.</p><h3> Example - Action Selection via Planning/Optimization</h3><p> Consider an agent that doesn&#39;t have a predetermined heuristic for selecting successful actions (or action sequences) that lead to its target outcome. This could be because the environment is sufficiently complicated and/or novel.</p><p> The Agent has a set of actions/plans to choose from but doesn&#39;t immediately know which one will lead to the target outcome, and so must use an optimization process to select the correct action/plan amongst the set of possible actions/plans it could take. Therefore, the Agent must use some procedure by which it deduces which outcomes result from which actions, and then must evaluate which outcome corresponds to the target outcome. It then uses this evaluation to select the action that leads to the target outcome.</p><p> In this case, the evaluation function contains the target information. <span class="footnote-reference" role="doc-noteref" id="fnref6y24xdokbbj"><sup><a href="#fn6y24xdokbbj">[2]</a></sup></span></p><p> Note that planning can be thought of as an example of optimization, where the outcomes that correspond to actions/plans are evaluated by simulating the result of the plan using a world model.</p><h2> How can the Overseer Leverage Internal Target Information?</h2><p> It seems clear that some Internal Target Information must exist within agents; without such information, there is no way for the Agent&#39;s Action-Selection Mechanism to differentiate between actions that lead to the target outcome and those that don&#39;t. Furthermore, this information is present within the internals of the Agent before the target outcome is realized in the world because it is used to select actions that lead to the target outcome.</p><p> Our hope is that the Overseer can detect this Internal Target Information and use it to deduce what the Agent&#39;s target is before the target outcome is realized. This would allow the Overseer to achieve its objective of intervening and preventing the Agent from causing bad outcomes, as described above. <span class="footnote-reference" role="doc-noteref" id="fnrefy72pwljd5td"><sup><a href="#fny72pwljd5td">[3]</a></sup></span></p><p> See <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives#How_objective_detection_can_be_useful">this section of this post</a> for more ways this information could be leveraged.</p><p> We conclude this section by pointing out that the Internal Target Information of an arbitrary agent (such as a neural-network-based agent) will probably be extremely hard to detect and interpret. Despite this, our current <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">research agenda</a> aims at finding methods of detecting Internal Target Information, or demonstrating that it&#39;s too difficult of a task. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rtb09vpi2q1dtx3zbgej" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rx2iwsj2z2gbk6cqr8pz 138w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/oiyv4wazdbevhvkmlgzl 218w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jepzu8iiolrv3d2hfzoy 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ql3thmeb0xozzanvyf00 378w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/pbb4tvydwllxjrjobsje 458w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/gh87ak7ktz5am4xwshjk 538w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q9y7mf15ebthnlzn5rhz 618w"><figcaption> Extending the diagram above, information about the target outcome is contained in the Agent and hence can, in theory, be observed by the Overseer to inform its oversight.</figcaption></figure><h1>结论</h1><p>In this post, we introduced the notion of Internal Target Information and discussed how it might be used by an overseer to prevent catastrophic outcomes from misaligned AIs.</p><p> In future work, we intend to explore further what shape Internal Target Information takes within agents and investigate to what extent it&#39;s possible for an overseer to detect this Internal Target Information. </p><figure class="image image_resized" style="width:77.05%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/th1yhasze1jqfjet0mpp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/yddpdlnjy4b2lwjkclhv 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jkrmfu0gtdxrfdm8ndvf 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zfcvnphtxwzmptqwbhbk 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/llthqbeaaluewuxocesn 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/mdivwx8p0cxerbg6deud 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jc8igln20soag6vjkqcr 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ezx8mkyn236nvy7osts2 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zaft1rlgrzg6lfhhih5c 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/js2x7rukvnoeblze7ght 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/damddo1svo0uc9nx7ya1 1024w"><figcaption> Internal Target Information hidden deep within the agent. If we can develop tools to detect it, this valuable information could help prevent catastrophic outcomes. Credit: DALL-E 3. </figcaption></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrac48lafqo9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrac48lafqo9">^</a></strong></sup></span><div class="footnote-content"><p> Or Decision-Making Process.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6y24xdokbbj"> <span class="footnote-back-link"><sup><strong><a href="#fnref6y24xdokbbj">^</a></strong></sup></span><div class="footnote-content"><p> We will explore the question of how this target information can be extracted in a future post. See <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">this post</a> for some initial explorations of this idea.</p></div></li><li class="footnote-item" role="doc-endnote" id="fny72pwljd5td"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy72pwljd5td">^</a></strong></sup></span><div class="footnote-content"><p> The ability of the Overseer to properly evaluate the target outcome is a separate problem that could be thought of as a version of the outer alignment problem. We will discuss this further in an upcoming post.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight<guid ispermalink="false"> hhKpXEsfAiyFLecyF</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Fri, 20 Oct 2023 14:53:00 GMT</pubDate> </item><item><title><![CDATA[On the proper date for solstice celebrations]]></title><description><![CDATA[Published on October 20, 2023 1:55 PM GMT<br/><br/><h1> Convenience vs. accuracy</h1><p> The question of the &quot;correct&quot; date on which to hold <a href="https://www.lesswrong.com/tag/secular-solstice">winter solstice events</a> has the character of a <a href="https://www.astralcodexten.com/p/give-up-seventy-percent-of-the-way">hyperstition</a> , ie it&#39;s not really important right now, but if we argue about it enough we can make it important, if that&#39;s what we want.</p><p> You might say:</p><ol><li> The solstice event should be held on a weekend before the astronomical solstice to maximize convenience, even if this puts it more than a week early (&quot;Earlybird&quot;)</li><li> The solstice event should be held on the last weekend before the astronomical solstice (&quot;Last-Weekender&quot;)</li><li> The solstice event should be held on the astronomical solstice, even if this is on a weeknight (&quot;Astronomical&quot;)</li></ol><p> To these we can also add a fourth (&quot;Mu&quot;) option, to wit: &quot;The date of the solstice event is unimportant and we shouldn&#39;t try to make it important.&quot;</p><p> (To be honest, I put a fair amount of weight on this last option myself; the rest of this post is somewhat speculative. Also, in general, you should be on the lookout for sneaky attempts to polarize an issue by setting up an <em>n</em> -chotomy and then hyperstitiously asserting that your choice there correlates with a larger cluster of beliefs/values. This is essentially what I&#39;m doing here, but at least not sneakily.)</p><p> When choosing a date, it mostly comes down to who your target audience is. The Earlybird position makes sense if you&#39;re planning a &quot;destination solstice&quot; like <a href="https://www.lesswrong.com/posts/jGixfzG9fH7bMwGHC/solstice-2022-roundup?commentId=kaCEAsusBEb8uMx3c">the SF Bay</a> or <a href="https://www.lesswrong.com/posts/jGixfzG9fH7bMwGHC/solstice-2022-roundup?commentId=NgKgEmwdbLhCLdWP5">NYC</a> , where you&#39;re looking to get visitors from out-of-town. If you&#39;re aiming it at people who live in the area but leave to spend the holidays with their families, then you&#39;ll lean towards the Last-Weekender position. But by contrast, the Astronomical position conveys the message &quot; <em>We</em> are your family; <em>this</em> is your home.&quot;</p><p> This last one might seem a bit scary and culty. I want to at least put it forward as a plausible option (since Austin has been doing this the last few years, while as far as I can tell no other communities have held solstice events on weeknights), but also open up the debate about whether it&#39;s a good idea.</p><h1> Analogy: Should Halloween be moved to a weekend?</h1><p> Halloween is invariably celebrated on October 31 and has stubbornly resisted <a href="https://www.change.org/p/official-petition-to-observe-halloween-on-the-last-saturday-in-october">attempts</a> to move it to a more &quot;convenient&quot; weekend-night. Practically speaking this is due to the coordination problem - if you unilaterally go trick-or-treating on any other night, you&#39;ll come home empty-handed; and if you try to give out candy on a different date, you&#39;ll get egged on the 31st.</p><p> But I don&#39;t think that&#39;s all of it, at least for me. If I imagine a world where this coordination barrier is somehow overcome, my emotional reaction is not &quot;Well that&#39;s a relief&quot; but rather &quot;This is sad - have we forgotten the True Meaning of Halloween™?&quot;</p><p> (Do you have a similar reaction?)</p><p> It&#39;s not because of religion - I&#39;m not a Samhain-observant Celtic pagan and I&#39;ve never known any such. And it&#39;s not nostalgia either - while I have fond memories of Halloween as a child, having to wake up early for school the next day isn&#39;t one of them. &quot;Tradition&quot; is getting warmer, but that&#39;s still a bit vague. More specifically, it&#39;s something like this:</p><ul><li> People are too obsessed with work and education nowadays, and don&#39;t prioritize family and community as much as they should.</li><li> The tradition of going trick-or-treating on a weeknight is a tangible reminder that it wasn&#39;t always like this, and that it doesn&#39;t always have to be like this.</li><li> Therefore, moving Halloween to a weekend feels like giving up. It would make it common knowledge that we, as a society, have devalued family and community to the point of being an afterthought, begrudgingly tacked onto the margins of the calendar in between more important things.</li><li> This defeats the purpose of the holiday, and will make everyone less happy in the long run.</li></ul><p> (Incidentally, some time after I aged out of trick-or-treating, my town instituted November 1 as a school holiday because they knew all the kids would be half-asleep anyway. This would never have happened if the Weekenders had had their way.)</p><h1> Why consider Astronomical Solstice</h1><p> You might say that the LessWrong Solstice has no &quot;tradition&quot; to appeal to, or that, if anything, the weight of tradition is <em>against</em> the Astronomical position, considering the dates of the various solstice events over the last 10 years. But on the other hand, the self-narrative of the LessWrong Solstice is that it is by no means some late innovation, but rather a continuation of the universal and immemorial tradition of observing the winter solstice, a tradition dating back to the time of Stonehenge or further. If we accept this, then we also have to accept that the real tradition is to use the best available means to determine the date of the solstice, and observe that.</p><p> So while observing the Astronomical Solstice may be inconvenient, it represents an aspiration of what we hope the holiday might be, ie a commemoration of something that&#39;s important enough to be worth working your schedule around it. Holding the event on a different date (as a substitute for the astronomical date) diminishes this aspiration, as if to say that there is no room for the holiday to grow.</p><p> The LessWrong Solstice is also built on the proposition (which I agree with) that there are underexploited forms of beneficial social interaction besides the usual &quot;sitting around talking&quot; or &quot;eating and drinking with a group&quot;. The distinctive mode of interaction (ie speeches and songs) is what sets the LessWrong Solstice apart from the usual parade of parties that dominate the December weekend calendar. And if the LessWrong Solstice is different from and complementary to these, it shouldn&#39;t try to counterprogram them by choosing its date in the same way.</p><p> (Continuing the Halloween analogy - there are always plenty of Halloween parties for adults and children to attend on the weekends of October; but the peculiar form of social interaction - ie trick-or-treating - must always take place on the 31st.)</p><p> The accusation of &quot;cultiness&quot; is what I&#39;m most worried about. It&#39;s reasonable to suspect that a group is acting against your best interest when (1) it tries to cut you off from people outside the group, and (2) it demands of you a higher intimacy level than you were looking for. I sympathize with a general aversion to things that look like this, but as for Astronomical Solstice I might reply:</p><ol><li> It&#39;s not trying to cut you off from anyone. You can invite your family if you want, and there is no shaming of people who don&#39;t attend. (Also, the counterargument is <a href="https://www.lesswrong.com/tag/fully-general-counterargument">fully general</a> - having an event on any date could be construed as &quot;cutting you off&quot; from other people who want to hold events on that date.)</li><li> It&#39;s not particularly &quot;intimate&quot; either. Sure, some people might give heartfelt speeches, but you can also choose to just sit back and listen.</li></ol><p> I&#39;m also sympathetic to the &quot;just let people enjoy things&quot; sentiment - ie it&#39;s fine for different people to have different levels of investment in the community, as long there&#39;s no negativity directed at less-invested people to try and pressure them into increasing their level; but the mere possibility that this <em>might</em> happen isn&#39;t sufficient reason to deny the option to those who positively <em>do</em> want to invest more and reap the benefits thereof.</p><p> But I&#39;m not sure. What do you think?</p><br/><br/> <a href="https://www.lesswrong.com/posts/RChzPW8zJ99rxxsvq/on-the-proper-date-for-solstice-celebrations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RChzPW8zJ99rxxsvq/on-the-proper-date-for-solstice-celebrations<guid ispermalink="false"> RChzPW8zJ99rxxsvq</guid><dc:creator><![CDATA[jchan]]></dc:creator><pubDate> Fri, 20 Oct 2023 13:55:02 GMT</pubDate></item></channel></rss>