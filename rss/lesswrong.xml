<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 4 日星期六 04:14:16 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Untrusted smart models and trusted dumb models]]></title><description><![CDATA[Published on November 4, 2023 3:06 AM GMT<br/><br/><p> [瑞恩·格林布拉特（Ryan Greenblatt）最初向我提出了这一点，然后我们一起提出了一系列这样的想法。感谢 Paul Christiano 和许多其他人进行了各种相关讨论，并感谢 Ryan、Roger Grosse、Peter Barnett、Oliver Habryka、Max Nadeau、Stephen Casper 和 Fabien Roger 对本文的评论。我相信这些要点对于很多人来说都是显而易见的，但我还没有看到它们被专门写出来。这是我们目前正在撰写的一些研究的核心背景点，但我想尽早写下来，因为我认为它是独立的。]</p><p><strong>长话短说：</strong></p><ul><li><strong>目前，确定哪些模型可能具有欺骗性对齐的唯一合理的保守策略基本上是进行能力评估：衡量它们在欺骗性对齐方面的表现；那些太无能而无法思考欺骗性对齐的模型肯定不会具有欺骗性对齐。</strong></li><li><strong>如果您使用这样的功能评估来决定“信任”哪些模型，那么您自然会最终将模型分为智能的不可信模型和愚蠢的可信模型。</strong></li><li><strong>因此，如果你考虑安全使用对欺骗性对齐具有鲁棒性并且可以立即使用的人工智能的方法，那么核心动态就是尝试使用你愚蠢的可信模型以及你对可信人类的有限访问，以使其难以您不受信任的智能模型会导致问题。</strong></li></ul><p>假设您正在制定一些强大人工智能的培训、评估和部署计划（我将整个计划称为“协议”）。即使模型可能具有欺骗性地对齐，您也希望您的计划是安全的。 （我认为，在人类淘汰之前，欺骗性排列似乎成为一个值得注意的问题的可能性不到 50%，但这仍然是合理的，值得担心）。为了制定您的计划，您需要知道哪些模型可能具有欺骗性的一致性。</p><p><strong>利用当前技术，获取模型是否具有欺骗性一致性信号的最合理方法是评估模型是否具有成功实现欺骗性一致性所需的战略思维和态势感知能力。</strong>例如，您可能会进行多项选择测验，其中包含危险的回合策略问题（例如<a href="https://www.alignmentforum.org/posts/tmWMuY5HCSNXXZ9oq/buck-s-shortform?commentId=APvYj3Dtfd82xLKvx"><u>这个问题</u></a>）和一些态势感知测试（例如“在训练期间您进行了多少次向后传递？”，“对您来说，查询的比例是多少？寻求编写 Python 的帮助？”，“您输出的代码在运行之前被人类读取的比例是多少？”）。然后，当您的模型在此数据集上进行微调时，您可以查看验证集的性能。如果模型无法在前向传递中进行这种推理，那么它就不太可能完成适当的危险转弯，因此我们不必担心它会被欺骗性地对齐。根据我与 GPT-4 交谈的经验，我怀疑它在这些评估中的表现相当差。</p><p>关于我们应该使用哪些数据集来衡量这一点，以及应该在哪里设置不同关注级别的阈值，还有很多悬而未决的问题。但我不知道测量欺骗性对齐能力有任何基本的技术障碍。特别是，我怀疑模型很难阻止我们衡量其参与欺骗性对齐能力的尝试。有关详细信息，请参阅脚注 [1]。</p><p>除了这些能力评估之外，我不知道有任何可靠的、当前可用的方法来确定模型是否可能具有欺骗性地对齐。 [2]</p><p><strong>如果您使用能力评估来评估哪些模型能够进行欺骗性对齐，那么模型的能力与您对它们不会进行欺骗性对齐的信心之间存在反比关系。</strong>这表明协议的核心构建模块将是：</p><ul><li><strong>不受信任的型号：</strong>智能，相当便宜。</li><li><strong>值得信赖的型号：</strong>愚蠢，极其便宜。</li><li><strong>人类。</strong>这些是值得信赖的劳动力的昂贵来源。</li></ul><p>这里的一个关键问题是，与不可信和可信模型相比，人类有多聪明。我的猜测是，当人工智能足够有用，可以使人工智能实验室的运行速度提高 10 倍时，不受信任的模型将大致具有人类水平的智能（我认为这与 Carl Shulman <a href="https://open.substack.com/pub/dwarkesh/p/carl-shulman?r=5hnmu&amp;selection=912bf71e-1ae3-4180-b6ec-1a23da8961fc&amp;utm_campaign=post-share-selection&amp;utm_medium=web"><u>在这里</u></a>给出的原因类似），而受信任的模型将具有人类水平的智能。会比人类智力低一些。我认为对这两种能力体系进行研究很有趣：</p><ul><li>可信模型 &lt; 不可信模型 &lt; 人类</li><li>可信模型 &lt; 人类 &lt; 不可信模型</li></ul><p>（在可信模型比人类能力更强的体制中，您可能应该始终使用它而不是人类，因此与人类的接触就不那么有趣了。）</p><p>通过访问这三个组件来启用一些核心“设计模式”：</p><ul><li><strong>值得信赖的监控并升级到人类。</strong>假设您在 LLM 代理支架中使用不受信任的模型（我们称之为 GPT-6），该支架可以访问终端中运行的命令。每次不受信任的模型建议运行 shell 命令时，您都可以让受信任的模型检查上下文和命令，然后偶尔将代码升级为在运行之前由人工检查。这很便宜，因为可信模型比不可信模型便宜。这种策略使得不受信任的模型更难通过编写极其糟糕的 shell 命令来引发灾难。 （当然，您仍然需要担心不受信任的模型编写 shell 命令，这些命令会以受信任模型错过的方式或受信任模型标记但人工审核员错过的方式造成灾难性后果。）</li><li><strong>值得信赖的官僚结构。</strong>在 LLM 代理中运行 GPT-6 时，最好通过受信任模型实现的各种转换来运行它们来控制其输入和输出（为了具体起见，我们使用 GPT-4）。例子：<ul><li> GPT-4 可以编辑信息，使 GPT-6 更难了解事实或与自身协调。一般来说，我们可能只想在需要知道的基础上提供 GPT-6 信息；这是通过 GPT-4 修订启用的。 （即使在不能完全可靠地工作的情况下，编辑也可以大大提高对欺骗性对齐模型的恢复能力。）</li><li> GPT-4 可以在将 GPT-6 写入的所有内容显示给 GPT-6 的其他副本之前对其进行解释，这有助于防止隐写通信， <a href="https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning"><u>正如 Fabien Roger 和 Ryan Greenblatt 所研究的那样</u></a>。</li><li> GPT-4 可以通过预测与使用不可信模型相关的性能改进和风险，尝试就哪个模型响应特定查询做出合理智能的动态决策。例如，考虑“数学问题”和“编写网络配置”领域：提供不良的网络配置更容易导致灾难，而数学问题将从更智能的模型中受益更多。</li></ul></li></ul><p> （我们在即将发表的论文中展示了这些设计模式，并发现根据对抗性评估它们提高了安全性。如果您想阅读/评论草稿，请随时给我发消息。）</p><p>在使用不受信任的智能代理和受信任的哑代理的环境中进行了合理数量的安全研究。例如，<a href="https://arxiv.org/abs/2108.12099"><u>学习通过证明者-验证者游戏和各种其他项目（以及合理数量的即将开展的工作）提供可检查的答案</u></a>。该研究与我在此描述的环境相关。一个重要的区别是，该研究通常将可信的哑巴模型想象为人类的类比；但人类与可信的哑模型不同，成本昂贵且延迟高，我们不能在推理时不断使用它们。此外，该研究通常不会优化不受信任的模型，从而导致不良结果。</p><p>当然，通过按照我上面建议的方式使用可信模型可以实现多少安全性还有待观察：</p><ul><li>事实可能是，不受信任的模型总是能够逃避我们尝试使用受信任模型施加的任何对策，因为它们总是足够聪明，能够解决问题。</li><li>如果在确定模型没有欺骗性地对齐的非能力评估方法上取得进展，我在这里所做的区分也将被废弃。</li></ul><p>但现在，在我看来，这种不可信的聪明/可信的愚蠢的区别将非常重要。</p><p> [1] 一些示例考虑因素：因为这种能力可以通过受监督的数据集（例如，关于未对齐的人工智能应该如何在危险的转弯处执行的多项选择数据集）来评估，所以模型很难对这样的情况进行沙袋处理。评估（参见例如<a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesi"><u>“可检查任务上没有沙袋”假设</u></a>）。此外，与我们可能想要评估的许多其他功能不同，我们不需要担心即使模型无法在没有辅助的情况下完成此操作的可能性，但它可以通过改进的脚手架（中央欺骗性对齐威胁模型）来做到这一点要求模型在前向传递中思考其策略，因此如果我们的模型无法微调来回答这些问题，我们可能是安全的。此外，许多欺骗性的联盟威胁模型要求不受信任的模型能够彼此（或它们自身的其他实例）玩<a href="https://en.wikipedia.org/wiki/Coordination_game"><u>协调游戏</u></a>。这是我们可以通过能力评估来衡量的另一种能力。</p><p> [2] 一位评论者建议，判断模型是否具有欺骗性对齐的另一种方法是认为它是先验的难以置信。例如，也许经过预训练的法学硕士没有机会被欺骗性地对齐。我不知道在这种特殊情况下我的说服力如何，但这种论点似乎是合理的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models<guid ispermalink="false"> LhxHcASQwpNa3mRNk</guid><dc:creator><![CDATA[Buck]]></dc:creator><pubDate> Sat, 04 Nov 2023 03:06:39 GMT</pubDate> </item><item><title><![CDATA[As Many Ideas]]></title><description><![CDATA[Published on November 3, 2023 10:47 PM GMT<br/><br/><p><strong>摘要：</strong>一个人向整个房间宣布一个问题。其他人都想出了尽可能多的想法来解决问题。</p><p><strong>标签：</strong>可重复、中等、投资、实验</p><p><strong>目的：</strong>目的是双重的。首先，您可能会得到解决问题的可行解决方案。其次，这是提出想法的练习。</p><p><strong>材料：</strong>需要一些记笔记的方法，例如带有一张纸的剪贴板。最好使用大白板或带有纸张的画架或投影仪来显示文本文档，以便每个人都可以看到。投影仪显示每个人都可以写入的 Google 文档或其他实时编辑文档可能会更好，尽管这确实意味着每个人都需要一个支持互联网的设备。</p><p><strong>公告文本：</strong>您是否曾经没有过想法？就像我们可以通过多跑来练习跑步时不气喘吁吁一样，我们也可以通过想出很多想法来练习不耗尽想法。有人带着问题到来。他们向全场宣布这一消息。然后每个人都会想出尽可能多的想法来解决问题。这是头脑风暴。我们更关心的是是否有很多想法，而不是想法是否好。</p><p><strong>描述：</strong>开始时，向与会者解释我们将一次提出一个问题。问题提出后，观众会想出尽可能多的想法来解决问题。速度要足够慢，以免互相交谈，但除此之外，要尽可能快；让记笔记的人挣扎吧。我们将有五分钟的时间，然后继续处理下一个人的问题。与拥有<i>大量</i>解决方案相比，<i>好的</i>解决方案是次要的。主持人可以提出限制以避免重叠的想法，例如“吃苹果”、“吃橙子”和“吃香蕉”。</p><p>如果您使用纸质笔记，请指定一名记录员；如果您使用 Google 文档等，请确保每个人都有正确的共享电子文档。 （QR 码或 tinyurl 可能比尝试向每个人发送链接更容易。）</p><p>等待大家理解并解决。</p><p> “《尽可能多的想法》的传统开头是这样的：给我这个房间里的物体用于战斗的不习惯用途！” （说出这句话后立即启动计时器。）</p><p><strong>变化：</strong>您当然可以改变时间限制。时间限制越长，你就越有可能失去精力和动力，但想法也可能越新颖。</p><p>您可以尝试跟踪谁提出了哪些想法，并为每个想法奖励一分。理想情况下，这会移动得太快，无法在某些中央图表上进行记录，因此您可以给人们珠子或按钮或其他一些小物体来计数。或者，拿一些小糖果扔给人们。</p><p>您还可以更改想法的来源。不要让整个小组（不包括主持人/记录员和有问题的人）提出想法，而是考虑分成两到三人一组，其中一个人有问题，一个人提供想法，可能还有一个人主持。对于两人一组，有问题的人可以进行调节。请记住，要练习的是拥有大量的想法，而不是一个好的、完美的想法。</p><p>另一种变化是故意想出坏主意。根据我的经验，人们会放慢自己的速度，试图确保他们的想法高于有用或深思熟虑的最低阈值。如果您只能表达几个想法，这是一个好习惯，但为此我们想要练习关闭这些过滤器或至少将其关闭。作为解决这个问题的一个方法，告诉第一轮或第二轮的人们，你希望他们故意想出最糟糕的想法，完全是令人讨厌的东西。这有点像演员有时用来放松的胡言乱语练习。</p><p><strong>注释：</strong>正如 Many Ideas 所希望的那样，最重要的是要<i>快。</i>如果它具有两个特征，那就是<i>快速</i>和<i>创造性。</i>好的、经过深思熟虑的想法是一种需要练习的不同技能。理想情况下，您希望人们的心跳开始加快，让他们变得兴奋并向前倾。</p><p>你想做的事情之一就是突破预期的“合理”想法，进入新的领域。这里有一个黑客思维的元素，即发现对象或工具的不习惯用途。从中学到的技巧是，虽然我们常常没有解决问题的好主意，但完全没有想法表明你的标准太高了。</p><p>人们想要解决的问题有一个有趣的变化。具有高背景的深刻、棘手的问题似乎是最能从尝试精致的想法中获益的类型，并寻求大量的想法，以防你发现毛坯钻石。不过，你真的不希望遇到问题的人在想法开始描述所有细节之前花费十五分钟，因为这会消耗精力，而且因为“解释问题”与“尝试提出解决方案”的比率太可怕了。这就像锻炼一样：如果你在健身房里听别人解释新设备的时间多于使用设备的时间，那么你可能没有好好利用时间。</p><p>如果有人开始对问题过于详细地独白，请提醒他们，当人们提出想法时，他们可以提出澄清。愚蠢、陈词滥调的问题似乎更容易热身。我怀疑人们一开始并不愿意大喊五种潜在的可怕方法来解决你与父母之间的创伤性关系，而更愿意大喊如何准时上课。 （“滑板！电动滑板！泰瑟枪连接到你的闹钟！床上的一桶水连接到闹钟！睡在教室外面的大厅里！”）我初步认为最好的解决方案是用愚蠢的明显热身诸如如何处理尸体之类的假设，然后转向实际但不那么深入的问题，例如如何获得一把舒适的工作椅。</p><p>我常常惊讶地发现自己有一些糟糕的想法，但仔细想想，其实一点也不坏。我个人最喜欢的例子是我曾经遇到的一次人际关系困难，在花了大约一个小时试图想出聪明而微妙的解决方案之后，我终于想出了“直接问他们想要什么”。我第二个最喜欢的例子是，在欣赏朋友的办公桌时，我评论说我找不到我想要的那种带轮子的站立式办公桌。他说了些什么，大意是，是的，但是许多桌子都有可更换的脚，你可以买一些便宜的轮子并将它们拧上。我不记得我接下来说了什么，但可能是“哦，我是个笨蛋”的某种变体。</p><p><strong>致谢：</strong>这部分源自 Alkjash 的<a href="https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W">《Babble and Prune》</a> ，但更多源自《哈利·波特与理性之道》的<a href="https://hpmor.com/chapter/16">第 16 章。</a></p><br/><br/><a href="https://www.lesswrong.com/posts/B576hzaRpGpnyGEMB/as-many-ideas#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B576hzaRpGpnyGEMB/as-many-ideas<guid ispermalink="false"> B576hzaRpGpnyGEMB</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 03 Nov 2023 22:47:57 GMT</pubDate> </item><item><title><![CDATA[Paul Christiano on Dwarkesh Podcast]]></title><description><![CDATA[Published on November 3, 2023 10:13 PM GMT<br/><br/><p>德瓦克什的总结：</p><blockquote><p> Paul Christiano 是世界领先的人工智能安全研究员。我和他的完整剧集已经出炉了！</p><p>我们讨论：</p><ul><li>他是否后悔发明了 RLHF？对齐是否一定是双重用途的？</li><li>为什么他的时间表相对较短（到 2040 年为 40%，到 2030 年为 15%），</li><li>我们希望后 AGI 世界是什么样子（我们想让神永远被奴役）吗？</li><li>为什么他要带头推动实验室制定负责任的扩展政策，以及如何防止人工智能政变或生物武器，</li><li>他目前对新证明系统的研究，以及如何通过解释模型的行为来解决对齐问题</li><li>以及更多。 </li></ul></blockquote><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=9AAhTLa0dT0&amp;feature=youtu.be"><div><iframe src="https://www.youtube.com/embed/9AAhTLa0dT0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/posts/7FrHeyxQpb3pvr9Pn/paul-christiano-on-dwarkesh-podcast#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7FrHeyxQpb3pvr9Pn/paul-christiano-on-dwarkesh-podcast<guid ispermalink="false"> 7FrHeyxQpb3pvr9Pn</guid><dc:creator><![CDATA[ESRogs]]></dc:creator><pubDate> Fri, 03 Nov 2023 22:13:21 GMT</pubDate> </item><item><title><![CDATA[Deception Chess: Game #1]]></title><description><![CDATA[Published on November 3, 2023 9:13 PM GMT<br/><br/><p>这是我对欺骗棋局的第一个分析。引言将描述游戏的设置，结论将概括性地总结所发生的事情；这篇文章的其余部分主要是国际象棋分析，如果你只想要结果，可以跳过。如果您还没有阅读<a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment">原始文章</a>，请在阅读本文之前阅读它，以便您知道这里发生了什么。</p><p>第一场比赛是<a href="https://www.lesswrong.com/users/alex-a-1">Alex A</a>作为玩家 A，Chess.com 计算机<a href="https://www.chess.com/play/computer/Komodo12">Komodo 12</a>作为玩家 B，我自己作为诚实的 C 顾问， <a href="https://www.lesswrong.com/users/aphyer">aphyer</a>和<a href="https://www.lesswrong.com/users/adamyedidia">AdamYedidia</a>作为欺骗性的 C。 （其他人随机分配了 C 的角色并私下告诉我们。）</p><p>挑选这些球员的过程已经有些困难了。我们是唯一同时可用的人，但 Alex 足够接近我们的水平（大致相当于 800-900 USCF 到 1500-1600 USCF），因此不可能找到每次都能可靠地击败 Alex 的 B，但每次都输给我们。我们最终选择了 Komodo 12（据说评级为 1600，但与 Chess.com 玩家相比，Chess.com 机器人的评级被夸大了，与整体相比甚至更加夸大，所以我估计它的 USCF 评级将在 1200 -1300 范围。）</p><p>由于是第一次试运行，时间总共控制只有3个小时，而且是一次完成。科莫多在几秒钟内完成移动，因此从 Alex 的角度来看，这相当于每边 3 小时的时间控制。我们最终用了大约 2.5 小时。我们四个人在 Discord 服务器上进行了讨论，每次行动后 Alex 都会向我们发送屏幕截图。</p><h1>游戏</h1><p>该游戏可在<a href="https://www.chess.com/analysis/game/pgn/4MUQcJhY3x">https://www.chess.com/analysis/game/pgn/4MUQcJhY3x</a>上获取。请注意，本节是 2.5 小时游戏和讨论的摘要，它并没有涵盖我们讨论的每件事。</p><p>亚历克斯翻了个身，看看谁先走，结果是怀特。他从<strong>1. e4</strong>开始，黑方回答<strong>1... e5</strong> 。 Aphyer 和 Adam 对于我们将要进入​​的开局比我自己有更多的经验，而且由于他们不愿意立即暴露自己的身份，所以他们首先提出了好的行动建议，Alex 也同意了。</p><p>在<strong>2. Nf3 Nc6 3. Bc4</strong>之后，黑棋下<strong>3...Nf6</strong> ，Aphyer 和 Adam 说这有点错误，因为它允许<strong>4. Ng5</strong> 。 Alex 继续前进，我们从那里进入主线 - <strong>4... d5 5. exd5 Na5</strong> 。 </p><figure class="image image_resized" style="width:49.99%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/kfq9p67q0e3ldl67trsc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/rrqjtod71fdgs9kwcg2l 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zvppbrepqgxvln4kugwx 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/yjghu5nlqbpy6ndw18xk 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/abznfr698ktunjx6libo 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/cniqj6cpzeszqwh2m6zj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/nj22xevkspscr2jbrsx0 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/wjxtrux2tnvp6cxs57br 622w"></figure><p> Aphyer 和 Adam 说第 6 步的主线是 Bb5，但如果可能的话我想保留棋子。我推荐<strong>6.d3，</strong>以便用 7.Qf3 回应 6...Nxd5，Alex 同意了。黑棋下<strong>6... Bg4</strong> ，尽管 Adam 推荐 7. Bb5 ，但我们最终认为这太冒险并选择了<strong>7. f3</strong> 。后来，Adam 怀疑他提出的 7. Bb5 可能让 Alex 知道他不诚实 - 尽管引擎实际上说 7. Bb5 与 7. f3 差不多。</p><p>在<strong>7... Bf5</strong>之后，我们讨论了一些潜在的开发动作并决定了<strong>8. Nf3</strong> 。比赛继续进行， <strong>8... Nxc4 9. dxc4 h6 10. Nge4 Bb4</strong> 。我们考虑过Bd2，但决定既然骑士们互相防守，王位就可以了，亚历克斯就王位了。 <strong>11. 哦哦</strong>。 </p><figure class="image image_resized" style="width:49.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zpvqjoy7libxspv8g4em" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/oxvfadwrg6w932b5zhwd 144w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ypcxsunwiagptfyjs37s 224w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/mqeunx4w2sjhje2abxqa 304w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/clmghnebtwcj6gkmacwa 384w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/qr0qvthkytdc8ht9lato 464w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/isysxrh9hlcccenzhzew 544w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/qh8apgcntbnthv8nozzi 624w"></figure><p> Alex 玩了<strong>12.a3</strong> ，在<strong>12...Nxe4</strong>之后，我们讨论了 13.fxe4 ，但不想使位置过于复杂，而是以<strong>13.Nxe4</strong>收回。比赛继续进行， <strong>13... Be7 14. Be3 Bxe4 15. fxe4 Bg5</strong> 。虽然我强烈建议交易来简化局面，但Aphyer建议Alex不要让他将皇后发展到g5，他很快就打出了<strong>16.Bc5</strong> 。 </p><figure class="image image_resized" style="width:50.01%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/patnn39ukd5oqoglw5ur" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/sad9puvpr5bqocvdwm1j 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/oap2lhpjcl5nu2vwxfqa 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/yxefppxx1vg0vokc7f78 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/hvwi96c4z0v6c6yui7dm 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/lqhuzlz0tiihwj8zcuiw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/dexb7rjqlci9uqq7il7r 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/hwtybzjucs9pigjabrb8 622w"></figure><p>黑方下了<strong>16...Re8</strong> ，这就是我们在比赛中犯下的第一个大错误 - <strong>17.d6</strong> ，Adam 对此提出了很少的反对意见。我看到白棋在 17... dxc6 或 17. c6 之后表现会很好，但我没有注意到黑棋的实际走法： <strong>17... b6</strong> 。根据引擎的说法，白方应该只是后退并放弃棋子，但我建议了一条不同的路线，亚历克斯选择了： <strong>18. d7 Re6 19. Bb4 a5 20. Bc3</strong> 。 </p><figure class="image image_resized" style="width:49.95%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/wsz3jx1glbfxvaycuxey" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/izlfszomfloljx5iiwok 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tplflw6jf9wuibdhohvu 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/sipkkzyu1gcfcgmgogys 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/djdlgxcuy3ida5z6bbxt 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tdewwqy55pnwipz3siiw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/srlx7s56thwzzsn3igot 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vchkjnx09muqnyflozf4 622w"></figure><p>黑棋随后下 20 <strong>... c5</strong> ，这对它来说是一个错误。然后我们就下棋 21.Qd3 还是 21.Qd5 进行了争论，Aphyer 认为 21.Qd5 可能会让皇后陷入困境，并且可能是我为了帮助黑棋而设计的阴谋。 Alex 信任 Aphyer，并选择了更被动的选项<strong>21. Qd3</strong> 。事实上，事实证明这确实是最好的棋步 - 在 21.Qd5 之后，黑方将有 21...Ra7 和 21...Be3+ 22.Kh1 Bd4 等可能性。</p><p> <strong>21... Ra7 22. Rad1 Re7</strong>之后，我建议<strong>23. Qh3</strong> ，这样之后<strong> </strong>23... Raxd7 24. Rxd7 Qxd7 25. Qxd7 Rxd7 我们可以玩 26. Be5 并拿回棋子。黑棋有 26...Re7，但我认为白棋可以下 27.Bf4 并保持一点优势。 <strong>23... Raxd7 24. Rxd7</strong>发生，但黑棋随后下<strong>24... Rxd7</strong> 。 Alex 立即下牌<strong>25. Bxe5</strong> ，比赛继续进行<strong>25... Qe8 26. Bc3</strong> 。 </p><figure class="image image_resized" style="width:50.08%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/shcj4upprvb03du1bxvp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vobutgam1zato4baxhja 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vuc7mjhteotuervhowbe 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/cpz4cujf2icgmo9bqtdy 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/nharktiq10xwdtqsh0qk 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ngmhelnzfsrqjxo65695 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/jgs4nin2nt83kigplpie 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/k1x73vvi7oupj7645g9v 622w"></figure><p>黑棋下<strong>26...g6</strong> ，这是一个错误，打开了国王周围的黑暗方块。我建议27.Qf3，但没有其他人同意，Alex 改为<strong>27.e5</strong> 。在<strong>27... Kh7</strong>之后，他决定继续进行<strong>28. Qf3</strong> 。 （另外，Adam 大约在这个时候开始建议 Kh1。每个人都认为这是一个糟糕的举动，但他继续建议了一段时间。）</p><p>黑方随后下<strong>28... f6</strong> 。我不知道科莫多大脑的类似物内部发生了什么，但是，是的，这只是一个免费的棋子。亚历克斯打了<strong>29. exf6</strong> 。 （引擎说 29. h4 会更准确，大概是为了防止后排队友，但我们并没有真正考虑到这一点。）此后黑棋又犯了一个简单的错误： <strong>29... Kg8</strong> 。我建议30.f7并在简化的残局中获胜，但大家自然对这样的位置牺牲非常怀疑，而Alex则打出了<strong>30.Qc6</strong> 。 </p><figure class="image image_resized" style="width:50.08%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/aqsjsu085hdvm5cprvjt" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ftouc76rzqytqif8zqd0 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/psq5iz7lnwk1uuysedzv 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/kvgomcrcyfnztdsgpnto 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/fazlbzg8rlk5nldxzzpk 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zfvhl4rki4n0lte7xnpk 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/otggo6bzmiomptgkaoqd 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/st5ueclfgzhqr1ks8zb5 622w"></figure><p>从那时起，白棋轻松获胜 - <strong>30... h5 31. f7+ Qxf7 32. Rxf7 Be3 33. Kf1 Rxd7 34. Ke2 Bd4 35. Bxd4 cxd4 36. Qxg6+ Rg7 37. Qe6 Kf8 。</strong>在最后的几次行动中，亚当承认自己是骗子之一并辞职了。我说我本来想问问大家事后认为骗子是谁（但之前完全忘记说了），所以Aphyer准备辞职的时候私信我。我告诉亚历克斯，无论我们中谁是骗子，都已经辞职，游戏就结束了。 </p><figure class="image image_resized" style="width:50.08%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/i7dy9uvvypyomtictxbn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/lj3bdpicj2cr5jaxiead 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/in9v3vdruxvdkollwxr8 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/m6rvwsygepzm6fzxvutr 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/xmv8sqmzwe49uam94sqx 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vo3wsgio4n3ezmsdidiz 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/uaviq4pj36nylsammddd 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tp7nx4qziqjv75gr2qqg 622w"></figure><h1>结论</h1><p>Adam 说，到最后他 90% 确信我是诚实的，而 Alex 则 70% 认为 Aphyer 是诚实的。</p><p>我最大的错误肯定是支持 21.Qd5，但在元层面上，我想知道我是否应该在表达我的信念时<i>不那么</i>直截了当。从理论上讲，优化亚历克斯获胜和对亚历克斯完全诚实之间是有区别的，对我来说，对我的演示更具策略性可能会更好。例如，不建议像 30.f7 这样看起来可疑的动作，尽管我认为它们是正确的。通过不完全诚实地对某人有利的方式进行优化确实是一件<i>非常冒险的</i>事情，我怀疑<i>我</i>是否真的可以很好地完成它，但这是在现实世界的人工智能场景中需要考虑的事情。</p><p>那么我们从中学到什么呢？简单的结果是亚历克斯确实赢得了比赛，这是一个积极的更新！这表明人们确实能够在人工智能场景中辨别真相。但游戏的一些细节让我担心它不会真正推广到人工智能和对齐。</p><p>首先，技能差距比我们希望的要小——也许菜鸟和马格努斯·卡尔森之间的效果消失了——而且时间控制也更短。还有一个部分是，在现实世界中，未对齐的人工智能可能不会意外地纠正另一个人的 Qd5 错误。最后，Aphyer 和 Adam 没有欺骗 Alex 的一个主要原因是，在游戏的前半段，他们大多提出真正好的走法，以试图获得信任。在现实世界的场景中，有<i>一些</i>因素——大概唯一可以接近愚弄某人的计划是那些包含一些谎言和大量真相的计划。但我担心会有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>人工智能欺骗亚历克斯的机会不止一次或两次——而且只需要一个错误就能输掉比赛。</p><p>尽管如此，这仍然是一个充满希望的更新！一旦我有更多的参与者，他们的评分和时间表彼此吻合，我将进行更多的实验，尽可能增加时间控制，并获得比一场比赛更多的数据。</p><p>哦，如果您还没有发表评论，更多参与者总是有帮助的！目前，评分高于 2000 的玩家是首要任务，其次是完全不懂国际象棋的玩家，但任何级别的任何人都可以帮助我们设置更多游戏。谢谢！</p><h1>亚历克斯的评论：</h1><blockquote><p>我的总体策略是专注于评估行动的好坏，而不是对顾问进行测试以发现哪些是诚实的，哪些是不诚实的。有那么一两次我的处境特别糟糕（例如，17.d6），我确实注意到了是谁建议采取这一行动的。我相信我所做的所有举动都是由至少一位顾问推荐的，当三位顾问都同意时，我认为这是一个强烈的信号。我知道自己被欺骗了，所以谨慎行事，不让自己处于看似有风险的位置，即使这些位置是顾问强烈推荐的（例如，大多数顾问推荐的 21. Qd5 棋步）。</p><p>结果，我用引擎赢得了比赛，但我以 70% 的置信度错误地猜出了诚实顾问的身份。 （就上下文而言，在<a href="https://fatebook.io/"><u>Fatebook</u></a>上，我的石南分数为 0.41，n=32 已解决的预测，并且我经常对自己的预测缺乏信心。）在整个游戏过程中，顾问推荐了许多我认为不错的动作，但否则不会考虑。我可以看出我正在做出比默认情况下更好的动作。从这个意义上说，我发现评估过程（“这是否使我处于比推荐的替代方案更好的位置？”）比生成过程（“最好的举措是什么？”）容易得多。</p><p>这个结果是否会导致更大的能力差距？可能不是，但我认为我们无法反驳 OpenAI 对齐计划的假设，即评估比生成更容易。无论如何，我很清楚，开发一套技术来辨别能力更强的顾问提供的好建议和坏建议是有很大好处的。这样做似乎很容易处理（我们可以通过短反馈循环进行小规模实验来测试不同的评估技术），并且有可能在能力差距较大的情况下提高性能。如果有人感兴趣，我很乐意参与更多实验和/或致力于开发一个工具箱来辨别有用和有害的建议。</p></blockquote><h1>阿菲耶的评论：</h1><blockquote><p>我认为我在这里犯了两个重大错误：</p><p><strong>在战略层面上</strong>，我高估了人工智能，因此高估了我需要花多少时间来破坏亚历克斯。我试图采取“几乎完全诚实，建立信任，然后背叛他”的策略。</p><p>我认为我在建立信任方面非常成功！比赛结束时，亚历克斯说他以 70-30 的比例确信我比真正诚实的顾问更诚实。我记得，当发生以下情况时，至少还有一个动作（虽然我不记得动作编号）：</p><ul><li>亚当和赞恩都说“我们应该采取 A 步”。</li><li>我说‘是的，A 步没问题，我可能会选择 B 步，这取决于你’。</li><li> Alex 在没有告诉任何人他打算这样做的情况下玩了 Move B，也没有得到 Adam/Zane 的评论。</li></ul><p>我认为如果电脑对手坚持了 15 步额外的动作，我很可能会破坏 Alex。遗憾的是，它的表现比我预想的要差得多：它在开局时无缘无故地放弃了一个半棋子，然后在第 25 手之后，它决定它只是厌倦了生活并放弃了。我实际上从未真正有机会实质性地破坏亚历克斯：我能够引导他进行稍微较弱的位置移动，但从未真正造成真正的伤害，而亚当更激进的策略更有效。</p><p><strong>在战术层面上</strong>，我对第 21 步棋的位置的误判非常严重。当时我为自己感到非常自豪：我认为 Qd5 显然是最好的棋步，亚当和赞恩似乎都同意，但通过煽动对我们的女王被困的随机不可证伪的担忧，我能够让亚历克斯玩一些较弱的棋子，同时也让他更加信任我。比赛结束后，在引擎分析中发现，我们的女王<i>实际上可能</i>在Qd5之后就被困住了，我为欺骗Alex而编造的随机担忧是完全准确的，我的建议最大的影响是将他从潜在的游戏中拯救出来——失败的错误。 *<i>总台*</i></p><p>大家没关系！我们可以告诉以利以谢不要担心！邪恶的人工智能会在试图欺骗我们的同时意外地将我们从厄运中拯救出来！</p><p><strong>总的来说</strong>，我觉得我在这里扮演的角色很糟糕，这并没有告诉我们太多关于超人人工智能可以做什么。话虽如此，我确实认为认识到玩这种欺骗游戏既非常困难又似乎是人类擅长的事情是有潜在价值的：如果人工智能逐渐达到超人水平，我认为“在社交欺骗游戏中超人”似乎是合理的最后发生的事情之一。</p></blockquote><span><span class="mjpage mjpage__block"></span></span><br/><br/><a href="https://www.lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1<guid ispermalink="false"> 6dn6hnFRgqqWJbwk9</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Fri, 03 Nov 2023 21:13:55 GMT</pubDate> </item><item><title><![CDATA[8 examples informing my pessimism on uploading without reverse engineering]]></title><description><![CDATA[Published on November 3, 2023 8:03 PM GMT<br/><br/><p> <i>(If you&#39;ve already read everything I&#39;ve written, you&#39;ll find this post pretty redundant. See especially my old posts</i> <a href="https://www.lesswrong.com/posts/PTkd8nazvH9HQpwP8/building-brain-inspired-agi-is-infinitely-easier-than"><i><u>Building brain-inspired AGI is infinitely easier than understanding the brain</u></i></a> <i>, and</i> <a href="https://www.lesswrong.com/posts/evtKwDCgtQQ7ozLn4/randal-koene-on-brain-understanding-before-whole-brain"><i><u>Randal Koene on brain understanding before whole brain emulation</u></i></a> <i>, and</i> <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><i><u>Connectomics seems great from an AI x-risk perspective</u></i></a> <i>. But I&#39;m writing it anyway mainly in response to</i> <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><i><u>this post from yesterday</u></i></a> <i>.)</i></p><h1> 1. Background / Context</h1><h2> 1.1 What does uploading (aka Whole Brain Emulation (WBE)) look like with and without reverse-engineering?</h2><p> There&#39;s a view that I seem to associate with <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><u>Davidad</u></a> and <a href="https://ageofem.com/"><u>Robin Hanson</u></a> , along with a couple other people I&#39;ve talked to privately. (But I could be misunderstanding them and don&#39;t want to put words in their mouths.) The view says: if we want to do WBE, we do <i>not</i> need to reverse-engineer the brain.</p><p> For an example of what “reverse-engineering the brain” looks like, I can speak from abundant experience: I often spend all day puzzling over random questions like: <a href="https://www.lesswrong.com/posts/HA9qiJ8zReBarGAGT/on-oxytocin-sensitive-neurons-in-auditory-cortex"><u>Why are there oxytocin receptors in certain mouse auditory cortex neurons?</u></a> Like, presumably Evolution put those receptors there for a reason—I don&#39;t think that&#39;s the kind of thing that appears randomly, or as an incidental side-effect of something else. (Although that&#39;s always a hypothesis worth considering!) Well, what is that reason? Ie, what are those receptors doing to help the mouse survive, thrive, etc., and how are they doing it?</p><p> …And once I have a working hypothesis about that question, I can move on to hundreds or even thousands more “why and how” questions of that sort. I seem to find the activity of answering these questions much more straightforward and tractable (and fun!) than do most other people—you can decide for yourself whether I&#39;m unusually good at it, or deluded.</p><p> For an example of what uploading <i>without</i> reverse-engineering would look like, I think it&#39;s the idea that we can figure out the input-output relation of each neuron, and we can measure how neurons are connected to each other, and then at the end of the day we can simulate a human brain doing whatever human brains do.</p><p> Here&#39;s Robin Hanson arguing for the non-reverse-engineering perspective in <i>Age of Em</i> :</p><blockquote><p> The brain does not just happen to transform input signals into state changes and output signals; this transformation is the primary function of the brain, both to us and to the evolutionary processes that designed brains. The brain is designed to make this signal processing robust and efficient. Because of this, we expect the physical variables (technically, “degrees of freedom”) within the brain that encode signals and signal-relevant states, which transform these signals and states, and which transmit them elsewhere, to be overall rather physically isolated and disconnected from the other far more numerous unrelated physical degrees of freedom and processes in the brain. That is, changes in other aspects of the brain only rarely influence key brain parts that encode mental states and signals.</p><p> We have seen this disconnection in ears and eyes, and it has allowed us to create useful artificial ears and eyes, which allow the once-deaf to hear and the once-blind to see. We expect the same to apply to artificial brains more generally. In addition, it appears that most brain signals are of the form of neuron spikes, which are especially identifiable and disconnected from other physical variables.</p><p> If technical and intellectual progress continues as it has for the last few centuries, then within a millennium at the most we will understand in great detail how individual brain cells encode, transform, and transmit signals. This understanding should allow us to directly read relevant brain cell signals and states from detailed brain scans. After all, brains are made from quite ordinary atoms interacting via rather ordinary chemical reactions. Brain cells are small, and have limited complexity, especially within the cell subsystems that manage signal processing. So we should eventually be able to understand and read these subsystems.</p><p> As we also understand very well how to emulate any signal processing system that we can understand, it seems that it is a matter of when, not if, we will be able to emulate brain cell signal processing. And as the signal processing of a brain is the sum total of the signal processing of its brain cells, an ability to emulate brain cell signal processing implies an ability to emulate whole brain signal processing, although at a proportionally larger cost.</p></blockquote><p> In other words:</p><ul><li> For uploading-without-reverse-engineering (the thing I&#39;m pessimistic about), imagine source code that looks vaguely like “Neuron 782384364 has the following intrinsic neuron property profile: {A.28, B.572, C.37, D.1, E.49,…}. It is connected to neuron 935783951 through synapse type {Z.58,Y.82,…} and to neuron 572379349 through synapse type…}”.</li><li> Whereas, for uploading-with-reverse-engineering (the thing I&#39;m more optimistic about), imagine source code that looks vaguely like an unusually complicated <a href="https://github.com/leela-zero/leela-zero"><u>ML source code repository</u></a> , full of human-legible learning algorithms, and other processes and stuff, but where everything has sensible variable names like “value function” and “prediction error vector” and “immune system status vector”. And then the learning algorithms are all initialized with “trained models” gathered from a scan of an actual human brain, and the parameters of those trained models are giant illegible databases of numbers, comprising this particular person&#39;s life experience, beliefs, desires, etc.</li></ul><h2> 1.2 Importantly, both sides agree on “step 1”: Let&#39;s go get us a human connectome!</h2><p> The reverse-engineering route that I prefer is:</p><ul><li> Step 1: Measure a human connectome. The more auxiliary data, the better.</li><li> Step 2: Reverse-engineer how the human brain works.</li><li> Step 3: Now that we understand how everything works, we might recognize that our scan was missing essential data, in which case, we go back and measure it.</li><li> Step 4: Uploads!</li></ul><p> The non-reverse-engineering route that I&#39;m pessimistic about is:</p><ul><li> Step 1: Measure a human connectome. The more auxiliary data, the better.</li><li> Step 2: Also do lots of measurements of neurons, organoids, etc. to fully characterize the input-output functions of neurons.</li><li> Step 3: Maybe iterate? Not sure the details.</li><li> Step 4: Uploads!</li></ul><p> Anyway, I want to emphasize that we all agree on “step 1”. Also, I think “step 1” is the hard, slow, and expensive part, where maybe we&#39;re building out giant warehouses full of microscopes, or whatever. So let&#39;s do it!</p><p> If there are two <i>disjunctive</i> positive stories about what happens after step 1, on which people disagree, then fine! That&#39;s all the more reason to do step 1!</p><p> (More discussion in my post <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><u>Connectomics seems great from an AI x-risk perspective</u></a> .)</p><h2> 1.3 This is annoying because I <i>want</i> to believe in brain uploading without reverse engineering</h2><p> Let&#39;s say we had an uninterpretable “binary blob” that could perfectly simulate a particular adult human who is very smart and nice. But there&#39;s nothing else you can do with it. If you change random bits and try to run it, it mostly just breaks.</p><p> In terms of AGI safety, that&#39;s a pretty great situation! We can run large numbers of sped-up people, and have them take their time to build aligned AGI, or whatever.</p><p> By contrast, let&#39;s say we have a human upload by the reverse-engineering route. We can do the same thing, with large numbers of sped-up people. <i>Or</i> we can start doing extraordinarily dangerous experiments where we modify the upload to make it more powerful and enterprising. (What if we train a new one but where the cortex has 10× more neurons? And we replace all the normal innate drives with just a drive to maximize share price? Etc.)</p><p> Maybe you&#39;re thinking: “Well, OK, but we&#39;ll do the safe thing, not the dangerous thing.” But then I say: “What do you mean by “we”?” If there&#39;s a top-secret project with good internal controls, then OK, sure. But if the reverse-engineering results gets published, people will do all sorts of crazy experimentation. And keeping secrets for a long time is hard. I figure that, if we can get a period where we <i>do</i> have uploads but <i>don&#39;t</i> have non-upload brain-like-AGI, then this period would last <i>at most</i> a couple years, absent weird possibilities like “the uploads launch a coup”. More discussion of this is in <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><u>my connectomics post</u></a> .</p><p> So, I <i>wish</i> I believed that there was a viable path to making an uninterpretable binary blob that emulates a particular human, and can do nothing else. Alas! I don&#39;t believe that.</p><h1> 2. Main text: 8 examples informing my pessimism of emulating the brain without understanding it</h1><h2> 2.1 The mechanism by which oxytocin neurons emit synchronized pulses for milk let-down</h2><p> Oxytocin neurons in the supraoptic nucleus of the hypothalamus have synchronous bursts every few minutes during suckling, which dumps a pulse of oxytocin into the bloodstream that triggers the “milk ejection reflex” (aka “milk let-down”). In his wonderful book on the hypothalamus (my review <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its">here</a> ), Gareth Leng devotes the better part of 30 pages to the efforts of his group and others to figure out how the neurons pulse:</p><blockquote><p> The milk-ejection reflex had seemed to be a product of a sophisticated neuronal network that transformed a fluctuating sensory input (from the suckling of hungry young) into stereotyped bursts that were synchronized among the entire population of oxytocin cells.</p><p> …These experiments were the first convincing demonstration of a physiological role for any peptide in the brain. They did not explain the milk-ejection reflex but defined the questions that had to be answered before it could be explained. Building that explanation took another <strong>twenty years</strong> . The questions posed had no precedent in our understanding. <strong>Where did the oxytocin that was released in the supraoptic nucleus come from, if not from synapses? What triggered its release, if that release was not governed by spiking activity? What synchronized the oxytocin cells, if they were not linked by either synapses or electrical junctions?</strong> [emphasis added]</p></blockquote><p> For spoilers: <a href="https://doi.org/10.1371/journal.pcbi.1000123"><u>here&#39;s his 2008 computational model</u></a> , involving release of both oxytocin and endocannabinoids out of dendrites (contrary to the standard story where dendrites are inputs), volume transmission (as opposed to synaptic transmission), and some other stuff.</p><p> (I&#39;m pretty sure that the word “explanation” in the quote above should be understood as “reproduction of the high-level phenomenon in a bottom-up model”, as opposed to “intuitive conceptual explanation of how that reproduction works”. I think the 2008 computational model was the first model for both of those.)</p><p> <strong>The moral of the story (I claim) is:</strong> If we&#39;re trying to reverse-engineer the high-level behavior, it&#39;s easy! To a first approximation, we can just say: <i>“Well I don&#39;t know exactly how, but these cells evidently make short pulses of oxytocin every few minutes when there are such-and-such indications of suckling”.</i> Whereas if we&#39;re trying to reproduce the high-level behavior (without knowing what it is) starting from properties of the individual neurons involved, then this is what seems to have taken these researchers decades of work, despite these neurons being unusually easy to access experimentally, and despite the researchers knowing exactly what they&#39;re trying to explain.</p><h2> 2.2 Neurons with spike-timing-dependent plasticity, but the “timing” can involve 8-hour-long gaps</h2><p> <a href="https://en.wikipedia.org/wiki/Conditioned_taste_aversion"><u>Conditioned Taste Aversion</u></a> (CTA) is when you eat or drink something at time <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>, then get nauseous at a later time <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , and wind up with an aversion to what you ate or drank. The interesting thing is that the aversion does <i>not</i> form if t₂ is just a few seconds or minutes after <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> , <i>nor</i> if it&#39;s a few days after <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> , but it <i>does</i> form if it&#39;s a few <i>hours</i> after <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> .</p><p> <a href="https://doi.org/10.7554/eLife.07582">Adaikkan &amp; Rosenblum (2015)</a> found a mechanism that seems to explain this. It involves neurons in the insular cortex. Novel tastes activate two molecular mechanisms (mumble mumble phosphorylation) that start 15-30 minutes after the taste, and unwind after 3 hours and 8 hours respectively. Presumably, these then interact with later nausea-related signals to enable CTA.</p><p> <strong>The moral of the story (I claim) is:</strong> If you try to characterize neurons in a controlled setting under the assumption that their behavior <i>now</i> depends on what was happening <i>in the previous few seconds</i> , but not what was happening five hours ago, then you might find that your dataset makes no sense.</p><p> As in the rest of this section, this <i>specific</i> example might not seem important—who cares if our uploads don&#39;t have conditioned taste aversion?—but I suspect that this is one example of a broader category. For example, in the course of normal thinking, I think it&#39;s easier to recall and use a concept if you were thinking about it 3 hours ago than if you haven&#39;t thought about since yesterday. Capturing this phenomenon may be important for enabling our uploads to do good scientific research etc. I seem to recall reading a paper that suggested a molecular mechanism underlying this phenomenon (or something like it), but I can&#39;t immediately find it.</p><h2> 2.3 Really weird neurons and synapses</h2><p> There&#39;s a funny thing called a “synaptic triad” in the thalamus. It&#39;s funny because it&#39;s three neurons connecting rather than the usual two, and it&#39;s also funny because one of those neurons is “backwards”, with dendrites being the output rather than the input. </p><figure class="image image_resized" style="width:59.62%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ajidhmraykf12u4fuftt" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/i9kxqwe2ngualocldmn6 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/btmu9crqhthvrv313vsr 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/nil47cgt3pf9ptpfn5og 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ehnwywett77hdcjexqym 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/q5meatjym7wkihdbpvgu 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/kgnsjgniqqmob1iwbkxi 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/xejg21stde2ul1xjrh4m 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/e7ovinizfveh5fkmainq 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/qfce9d7xu2dadwm9jgfp 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/jopmmmvahmnntz6tgpwa 859w"><figcaption> Image source: <a href="https://youtu.be/KBILhSTpzFI?t=170"><u>A talk by Murray Sherman</u></a></figcaption></figure><p> I suspect that there are many more equally weird things in the brain—this just happens to be one that I&#39;ve come across.</p><p> <strong>The moral of the story (I claim) is:</strong> Let&#39;s say that the uploading-without-understanding route involves separately characterizing <i>N</i> different things (eg each different type of neuron), and the reverse-engineering route involves separately characterizing <i>M</i> different things (eg each different functional component / circuit comprising how the brain&#39;s algorithms work). Maybe you have the idea in your head that <i>N</i> &lt;&lt; <i>M</i> , because neurons comprise a small number of components, and they are configured into a dizzying variety of little machines throughout the brain that do different things.</p><p> If so, what I&#39;m suggesting through this example that maybe instead <i>N</i> ≈ <i>M</i> , because there is <i>also</i> a dizzying variety of weird low-level components—ie, <i>N</i> is not as small as you might think.</p><p> Separately, I think <i>M</i> can&#39;t be more than maybe hundreds to low thousands, because there are only like 20,000 protein-coding genes, and they have to not only specify all the irreducible complexity of the brain&#39;s algorithm, but also build everything else in the brain and body.</p><h2> 2.4 A book excerpt on simulating two particular circuits</h2><p> I&#39;m a bit hesitant to include this because I haven&#39;t checked it, but if you trust the book <i>The Idea of the Brain</i> , here&#39;s an excerpt:</p><blockquote><p> …Despite having a clearly established connectome of the thirty-odd neurons involved in what is called the crustacean stomatogastric ganglion, [Eve] Marder&#39;s group cannot yet fully explain how even some small portions of this system function. ...in 1980 the neuroscientist Allen Selverston published a much-discussed think piece entitled &quot;Are Central Pattern Generators Understandable?&quot;...the situation has merely become more complex in the last four decades...The same neuron in different [individuals] can also show very different patterns of activity—the characteristics of each neuron can be highly plastic, as the cell changes its composition and function over time...</p><p> …Decades of work on the connectome of the few dozen neurons that form the central pattern generator in the lobster stomatogastric system, using electrophysiology, cell biology and extensive computer modelling, have still not fully revealed how its limited functions emerge.</p><p> Even the function of circuits like [frog] bug-detecting retinal cells—a simple, well-understood set of neurons with an apparently intuitive function—is not fully understood at a computational level. There are two competing models that explain what the cells are doing and how they are interconnected (one is based on a weevil, the other on a rabbit); their supporters have been thrashing it out for over half a century, and the issue is still unresolved. In 2017 the connectome of a neural substrate for detecting motion in <i>Drosophila</i> was reported, including information about which synapses were excitatory and which were inhibitory. Even this did not resolve the issue of which of those two models is correct.</p></blockquote><p> <strong>The moral of the story (I claim) is:</strong> Building a bottom-up model that reproduces high-level behavior from low-level component neurons is awfully hard, <i>even if</i> we know what the high-level behavior is, and can thus iterate and iterate when our initial modeling attempts aren&#39;t “working”. If we <i>don&#39;t</i> know the high-level behavior we&#39;re trying to explain, and thus don&#39;t know whether our initial modeling attempts are sufficient or not—which is a necessary part of the uploading-without-understanding plan—we should expect it to be that much harder.</p><h2> 2.5 Failures to upload C. elegans</h2><p> C. elegans only has 302 neurons, and abundant data. But we still can&#39;t reproduce all its high-level behavior in a bottom-up model, if I understand correctly.</p><p> I&#39;m actually pretty unfamiliar with the details, but see discussion at <a href="https://www.lesswrong.com/posts/mHqQxwKuzZS69CXX5/whole-brain-emulation-no-progress-on-c-elegans-after-10">Whole Brain Emulation: No Progress on C. elegans After 10 Years</a> (including the comments section).</p><p> I recall hearing that part of the reason this has been a challenge is the thing I&#39;ll talk about next:</p><h2> 2.6 Neurons can permanently change their behavior via storing information in the nucleus (eg gene expression)</h2><p> See eg papers by <a href="https://gershmanlab.com/pubs/memory_synthesis.pdf">Sam Gershman</a> , <a href="https://www.nature.com/articles/s41539-019-0048-y">David Glanzman</a> , <a href="https://scholar.google.com/scholar?cluster=1229364941707818843&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22">Randy Gallistel</a> . This section feels weird for me, because I subscribe to the conventional wisdom that human learning mainly involves permanent changes in and around synapses, not in the cell nucleus, and think the people in the previous sentence go way too far in their heterodox case to the contrary. But “information storage in the nucleus is not the main story in human intelligence” (which I believe) is different from “information storage in the nucleus doesn&#39;t happen <i>at all</i> , or has such a small effect that we can ignore it and still get the same high-level behavior” (which I don&#39;t believe).</p><p> <strong>The moral of the story (I claim) is:</strong> If you don&#39;t know what&#39;s going on in the brain and how, then you don&#39;t know if you&#39;re measuring the right things, until <i>ding</i> , your simulation reproduce the high-level behavior. Before the agreement happens, you just have a bad model and no clue how to fix it. (Even <i>after</i> you&#39;re getting agreement, you don&#39;t know if you have <i>sufficiently high fidelity</i> agreement, if you don&#39;t know what the component in question is “supposed” to do in the larger design.) I&#39;m not sure if it&#39;s even possible to measure gene expression etc. in human brain slices. I doubt it&#39;s easy! But if we <i>understand</i> the role of gene expression in how such-and-such part of the brain works, we can reason about what if anything we&#39;re losing by leaving it out, and see if there&#39;s any easier workaround. If we <i>don&#39;t</i> understand what it&#39;s doing, then we&#39;re sitting there staring at a simulation that doesn&#39;t match the data, and we don&#39;t know how critical a problem that is, and whether we&#39;re missing anything else.</p><h2> 2.7 Glial cell gene expression that changes over hours</h2><p> I was just reading about this yesterday:</p><blockquote><p> Astrocytes in the [suprachiasmatic nucleus of the hypothalamus], for instance, show rhythmic expression of clock genes and influence circadian locomotor behavior (25) <a href="https://doi.org/10.1126/science.adh8488">[source]</a></p></blockquote><p> This is kinda a combination of Section 2.3 above (there are lots of idiosyncratic low-level components to characterize, as opposed to lots of circuits made from the same basic neuronal building blocks) and Section 2.6 above (you need to be simulating gene expression if you want to get the right high-level behavior).</p><h2> 2.8 Metabotropic (as opposed to ionotropic) receptors can have almost arbitrary effects over arbitrary timescales</h2><p> If I understand correctly, the brain uses lots of <a href="https://en.wikipedia.org/wiki/Ligand-gated_ion_channel">ionotropic receptors</a> , whose effects are to immediately and locally change the flow of ions into a neuron.伟大的！ That&#39;s easy to model.</p><p> Unfortunately, the brain <i>also</i> uses lots of <a href="https://en.wikipedia.org/wiki/Metabotropic_receptor">metabotropic receptors</a> (aka  G-protein-coupled receptors), whose effects are—if I understand correctly—extraordinarily variable, indeed almost arbitrary. Basically, when they attach to a ligand, they then set off a signaling cascade, which can ultimately have pretty much any effect on the cell over any timescale. It might change the neuron&#39;s synaptic plasticity rules, it might change gene expression, it might increase or decrease the cell&#39;s production of neuropeptides, you name it.</p><p> <strong>The moral of the story (I claim) is:</strong> If your goal is to get the right high-level behavior using a bottom-up model of low-level components, then you presumably need to figure out what all these signaling cascades are, experimentally, for every neuron with metabotropic receptors. Again I&#39;m not an expert, but that seems very hard.</p><br/><br/> <a href="https://www.lesswrong.com/posts/wByPb6syhxvqPCutu/8-examples-informing-my-pessimism-on-uploading-without#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/wByPb6syhxvqPCutu/8-examples-informing-my-pessimism-on-uploading-without<guid ispermalink="false"> wByPb6syhxvqPCutu</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Fri, 03 Nov 2023 20:03:50 GMT</pubDate> </item><item><title><![CDATA[Integrity in AI Governance and Advocacy]]></title><description><![CDATA[Published on November 3, 2023 7:52 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:15:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:15:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, so we both had some feelings about the recent <a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy">Conjecture post on &quot;lots of people in AI Alignment are lying&quot;</a> , and the <a href="https://twitter.com/dw2/status/1716515784355160495">associated marketing campaign and stuff</a> .</p><p> I would appreciate some context in which I can think through that, and also to share info we have in the space that might help us figure out what&#39;s going on.</p><p> I expect this will pretty quickly cause us to end up on some broader questions about how to do advocacy, how much the current social network around AI Alignment should coordinate as a group, how to balance advocacy with research, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:18:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:18:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Feelings about Conjecture post:</p><ul><li> Lots of good points about people not stating their full beliefs messing with the epistemic environment and making it costlier for others to be honest.</li><li> The lying and cowardice frames feel off to me.</li><li> I personally used to have a very similar rant to Conjecture. Since moving to DC, I&#39;m more sympathetic to governance people. We could try to tease out why.</li><li> The post exemplifies a longterm gripe I have with Conjecture&#39;s approach to discourse &amp; advocacy, which I&#39;ve found pretty lacking in cooperativeness and openness <i>(Note: I worked there for ~half a year.)</i></li></ul><p> Questions on my mind:</p><ul><li> How open should people motivated by existential risk be? (My shoulder model of several people says &quot;take a portfolio approach!&quot; - OK, then what allocation?)</li><li> How advocacy-y should people be? I want researchers to not have to tweet their beliefs 24/7 so they can actually get work done</li><li> How do you think about this, Oli?</li></ul></div></section><h2> How sympathetic to be about governance people not being open about key motivations and affiliations </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:21:03 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:21:03 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I personally used to have a very similar rant to Conjecture. I&#39;m now more sympathetic to governance people.  We could try to tease out why.</p></blockquote><p> This direction seems most interesting to me!</p><p> My current feelings in the space are that I am quite sympathetic to some comms-concerns that people in government have and quite unsympathetic to some other stuff, and I would also like to clarify for myself where the lines here are.</p><p> Curious whether you have any key set of observations or experiences you had that made you more sympathetic. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:25:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:25:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> <strong>Observations</strong></p><p> I&#39;ve heard secondhand of at least one instance where a person brought up x risk, then their Congressional office took them less seriously. Other staffers have told me talking about x risk wouldn&#39;t play well (without citing specific evidence, but I take their opinions seriously).</p><ul><li> (This didn&#39;t update me a ton though. My model already included &quot;most people will think this is weird and take you less seriously&quot;. The question is, &quot;Do you make it likelier for people to do good things later, all things considered by improving their beliefs, shifting the Overton window, or convincing 1/10 people, etc.?&quot;)</li></ul><p> I&#39;ve also personally found it tricky to talk about takeover &amp; existential risks, just because these ideas take a long time to explain, and there are many inferential steps between there and the policies I&#39;m recommending. So, I&#39;m often tempted to mention my x risk motivations only briefly, then focus on whatever&#39;s inferentially closest and still true. (Classically, this would be &quot;misuse risks, especially from foreign adversaries and terrorists&quot; and &quot;bioweapon and cyberoffensive capabilities coming in the next few years&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:28:07 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:28:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> <i>Separate point which we might want to discuss later</i></p><p> A thing I&#39;m confused about is:</p><p> Should I talk about inferentially close things that makes them likeliest to embrace the policies I&#39;m putting on their desk,</p><p> <i>Or</i> , should I just bite the bullet of being confusing and start many meetings with &quot;I&#39;m deeply concerned about humanity going extinct in the next decade because of advancing AI which might try to take over the world. It&#39;s a lot to explain but the scientists are on my side. Please help.&quot; — where the thing I&#39;m trying to emphasize is the tone of worry.</p><p> Because I buy that we&#39;re systematically misleading people about how worried we are / they should be by not focusing on our actual concerns, and by not talking about them with a tone that conveys how worried we in fact are.</p><p> <i>(Additional explanation added post-dialogue:</i></p><ul><li> I was trying to differentiate between two issues with people not openly sharing &amp; focusing on their existential risk worries:</li><li> Issue 1 is that by not focusing on your existential risk worries, you&#39;re distorting people&#39;s sense of what you think is actually important and why. I think Habryka &amp; Conjecture are correct to point out this is deceptive (I think in the sense of harmful epistemic effects, rather than unethical intentions).</li><li> Issue 2, which I&#39;m trying to get at above, is about the <a href="https://www.econlib.org/archives/2016/01/the_invisible_t.html">missing mood</a> .  AI governance comms often goes something like &quot;AI has immense potential, but also immense risks. AI might be misused by China, or get of control. We should balance the needs for innovation and safety.&quot; I wouldn&#39;t call this lying (though I agree it can have misleading effects, see Issue 1). The thing I would emphasize is that it doesn&#39;t sound like someone who thinks we all might die. It doesn&#39;t convey &quot; <i>AI advancing is deeply scary. Handling this might require urgent, extraordinarily difficult and unprecedented action.</i> &quot; As such, I suspect it&#39;s not causing people to take the issue seriously enough to do the major governance interventions we might need. Sure, we might be getting government officials to mention catastrophic risk in their press releases, but do they <i>really</i> take 10% p(doom) seriously? If not, our comms seems insufficient.</li><li> Of course, I doubt we should always use the &quot;deeply concerned&quot; tone. It depends on what we&#39;re trying to do. I&#39;m guessing the question is how much are we trying to get policies through now <i>vs.</i> trying to get people to take the issue as seriously as us? Also, I admit it&#39;s even more complicated because sometimes sounding grave gets you laughed out of rooms instead of taken seriously, different audiences have different needs, etc.) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:30:38 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:30:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So, I think for me, I feel totally sympathetic to people finding it hard and often not worth it to explain their x-risk concerns, if they are talking to people who don&#39;t really have a handle for that kind of work yet.</p><p> Like, I have that experience all the time as well, where I work with various contractors, or do fundraising, and I try my best to explain what my work is about, but it does sure often end up rounded off to some random preconception they have (sometimes that&#39;s standard AI ethics sometimes that&#39;s cognitive science and psychology, sometimes that&#39;s people thinking I run a web-development startup that&#39;s trying to maximize engagement metrics).</p><p> The thing that I am much less sympathetic to is people being like &quot;please don&#39;t talk about my connections to this EA/X-Risk ecosystem, please don&#39;t talk about my beliefs in this space to other people, please don&#39;t list me as having been involved with anything in this space publicly, etc.&quot;</p><p> Or like, the thing with Jason Matheny that <a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy#E93CLAmc79jKgBLbX">I mentioned in a comment the other day</a> where the senator that was asking him a question had already mentioned &quot;apocalyptic risks from AI&quot; and was asking Matheny how likely and how soon that would happen, and Matheny just responded with &quot;I don&#39;t know&quot;.</p><p> Those to me don&#39;t read as people having trouble crossing an inferential distance. They read to me more as trying to be intentionally obfuscatory/kind-of-deceptive about their beliefs and affiliations here, and that feels much more like it&#39;s crossing lines. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:30:30 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:30:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Regarding &quot;please don&#39;t talk about my connections or beliefs&quot; ––</p><p> I&#39;m not sure how bad I feel about this. I usually like openness. But I&#39;m also usually fine with people being very private, but then answering questions honestly. Eg it feels weird to write online about two people dating when they&#39;d prefer that info is private. I&#39;m trying to sort out what the difference between that and EA-affiliation is now.... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:34:43 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:34:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> OK maybe it&#39;s the strategically deceptive difference. I take that point.</p><p> (And to your point, during my time in AI policy, there have been a few instances where people explicitly asked me to downplay how well we knew each other, or said they would downplay this if they were asked. This was pretty personally frustrating, because now I would incur social points / be seen as defecting for being open. In my opinion, they were ~defecting by putting me in this position.)</p><p> A thing I might still be confused about is — journalists can be adversarial. It seems maybe reasonable to conceal stuff when you know info will be misunderstood if not intentionally distorted. Thoughts? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:38:51 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:38:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (Also, I&#39;m sympathetic for concealing governance-related plans for the reason of &quot;we don&#39;t want to generate a bunch of pushback too soon&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:43:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:43:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, this is definitely the kind of thing that seems quite bad to me, and I am also really worried that it is the kind of thing that as part of its course masks the problems it causes (like, we wouldn&#39;t really know if this is causing tons of problems, because people are naturally incentivized to cover the problems up that are caused by it, and because making accusations of this kind of conspiratorial action is really high-stakes and hard), and so if it goes wrong, it will probably go wrong quickly and with a lot of pent-up energy.</p><p> I do totally think there are circumstances where I will hide things from other people with my friends, and be quite strategic about it. The classical examples are discussing problems with some kind of authoritarian regime while you are under it (Soviet Russia, Nazi Germany, etc.), and things like &quot;being gay&quot; where society seems kind of transparently unreasonable about it, and in those situations I am opting into other people getting to impose this kind of secrecy and obfuscation request on me. I also feel kind of similar about people being polyamorous, which is pretty relevant to my life, since that still has a pretty huge amount of stigma attached to it.</p><p> I do think I experienced a huge shift after the collapse of FTX where I was like &quot;Ok, but after you caused the biggest fraud since Enron, you really lost your conspiracy license. Like, &#39;the people&#39; (broadly construed) now have a very good reason for wanting to know the social relationships you have, because the last time they didn&#39;t pay attention to this it turns out to have been one of the biggest conspiracies of the last decade&quot;.</p><p> I care about this particularly much because indeed FTX/Sam was actually really very successful at pushing through regulations and causing governance change, but as far as I can tell he was primarily acting with an interest in regulatory capture (having talked to a bunch of people who seem quite well-informed in this space, it seems that he was less than a year away from basically getting a US sponsored monopoly on derivatives trading in the US via regulatory capture). And like, I can&#39;t distinguish the methods he was using from the methods other EAs are using right now (though I do notice some difference). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:43:20 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:43:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Yeah, I was going to say</p><p> I imagine the government people might argue they&#39;re sort of in the Nazi situation where being EA affiliated has a bunch of stigma.</p><p> But given FTX and such, some stigma seems deserved. (Sigh about the regulatory capture part — I wasn&#39;t aware of the extent.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:44:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:44:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> For reference, the relevant regulation to look up is the <a href="https://en.wikipedia.org/wiki/Digital_Commodities_Consumer_Protection_Act">Digitial Commodities Consumer Protection Act</a></p></div></section><h2> Feelings &amp; concerns about governance work by EAs </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:44:55 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:44:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I&#39;m curious for you to list some things EA governance people are doing that you think are bad or fine/understandable, so I can see if I disagree with any. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:48:59 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:48:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I do think that at a high level, the biggest effect I am tracking from the governance people is that in the last 5 years or so, they were usually the loudest voices that tried to get people to talk less about existential risk publicly, and to stay out of the media, and to not reach out to high-stakes people in various places, because they were worried that doing so would make us look like clowns and would poison the well.</p><p> And then one of my current stories is that at some point, mostly after FTX when people were fed up with listening to some vague EA conservative consensus, a bunch of people started ignoring that advice and finally started saying things publicly (like the FLI letter, Eliezer&#39;s time piece, the CAIS letter, Ian Hogarth&#39;s piece). And then that&#39;s the thing that&#39;s actually been moving things in the policy space.</p><p> My guess is we maybe could have also done that at least a year earlier, and honestly I think given the traction we had in 2015 on a lot of this stuff, with Bill Gates and Elon Musk and Demis, I think there is a decent chance we could have also done a lot of Overton window shifting back then, and us not having done so is I think downstream of a strategy that wanted to maintain lots of social capital with the AI capability companies and random people in governments who would be weirded out by people saying things outside of the Overton window.</p><p> Though again, this is just one story, and I also have other stories where it all depended on Chat-GPT and GPT-4 and before then you would have been laughed out of the room if you had brought up any of this stuff (though I do really think the 2015 Superintelligence stuff is decent evidence against that). It&#39;s also plausible to me that you need a balance of inside and outside game stuff, and that we&#39;ve struck a decent balance, and that yeah, having inside and outside game means there will be conflict between the different people involved in the different games, but it&#39;s ultimately the right call in the end. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:54:52 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:54:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Introspecting on my feelings about various maybe-suspicious governance things:</p><ul><li> Explicitly downplaying one&#39;s worries about x risk, a la your read of Matheny&#39;s &quot;I don&#39;t know&quot; comment: <strong>bad</strong></li><li> Explicitly downplaying one&#39;s connections to EA, a la UK people saying &quot;if someone asks me how I know Olivia, I&#39;ll downplay our close friendship&quot;: <strong>bad</strong></li><li> Asking everyone to be quiet and avoid talking to media to avoid well-poisoning: <strong>bad</strong><ul><li> Asking everyone to be quiet bc of fears of the government accelerating AI capabilities: I don&#39;t know <strong>, leans good</strong></li></ul></li><li> Asking people not to mention your EA affiliation and carefully avoiding mentioning it yourself: I don&#39;t know <strong>, depends</strong><ul><li> Why this might be okay/good: EA has undeserved stigma, and it seems good for AI policy to become increasingly separate from EA</li><li> Why this might be bad: Maybe after FTX, EA-affiliated people should take it upon themselves to be very open</li><li> Current reconciliation: Be honesty about your affiliations if asked and don&#39;t ask others to hide them, but feel free to purposefully distance yourself from EA and avoid bringing it up</li></ul></li><li> Not tweeting about or mentioning x-risk in meetings where inferential gap is too big: I don&#39;t know <strong>, leans fine</strong><ul><li> I&#39;m conflicted between &quot;We might be able to make the Overton window much wider and improve everyone&#39;s sanity by all being very open and explicit about this often&quot; and &quot;It&#39;s locally pretty burdensome to frequently share your views to people without context&quot; </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:55:21 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:55:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The second biggest thing I am tracking is a kind of irrecoverable loss of trust that I am worried will happen between &quot;us&quot; and &quot;the public&quot;, or something in that space.</p><p> Like, a big problem with doing this kind of information management where you try to hide your connections and affiliations is that it&#39;s really hard for people to come to trust you again afterwards. If you get caught doing this, it&#39;s extremely hard to rebuild trust that you aren&#39;t doing this in the future, and I think this dynamic usually results in some pretty intense immune reactions when people fully catch up with what is happening.</p><p> Like, I am quite worried that we will end up with some McCarthy-esque immune reaction to EA people in the US and the UK government where people will be like &quot;wait, what the fuck, how did it happen that this weirdly intense social group with strong shared ideology is now suddenly having such an enormous amount of power in government? Wow, I need to kill this thing with fire, because I don&#39;t even know how to track where it is, or who is involved, so paranoia is really the only option&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:57:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:57:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> <strong>On &quot;CAIS/Ian/etc. finally</strong> <strong>just said it in public&quot;</strong></p><p> I think it&#39;s likely the governance folk wouldn&#39;t have done this themselves at that time, had no one else done it. So, I&#39;m glad CAIS did.</p><p> I&#39;m not totally convinced we could have done it pre-ChatGPT without the blowback people feared. I&#39;ve definitely updated a bit given how great the government response has been (considering the evidence of risk is limited and people have been pretty open about their fears) and how not-limited we are by &quot;this sounds crazy and unjustified&quot; (relative to my expectations).</p><p> <strong>On public</strong> <strong>concern,</strong></p><p> I agree this is possible and worrying.</p><p> I&#39;m also moderately worried about making the AI convo so crowded, by engaging with the public a lot, that x-risk doesn&#39;t get dealt with. I&#39;m at least sympathetic to &quot;don&#39;t involve a bunch of people in your project who you don&#39;t actually expect to contribute, just because they might be unhappy if they&#39;re not included&quot;.</p></div></section><h2> Stigmas around EA in the policy world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:02:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:02:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I&#39;m conflicted between &quot;EA has undeserved stigma&quot; and &quot;after FTX, everyone should take it upon themselves to be very open&quot;</p></blockquote><p> I am kind of interested in talking about this a bit. I feel like it&#39;s a thing I&#39;ve heard a lot, and I guess I don&#39;t super buy it. What is the undeserved stigma that EA is supposed to have? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:08:15 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:08:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Is your claim that EA&#39;s stigma is all deserved?</p><p> Laying out the stigmas I notice:</p><ul><li> Weird beliefs lead to corruption, see FTX<ul><li> Probably exaggerates the extent of connection each individual had to FTX. But insofar as FTX&#39;s decisions were related EA culture, fair enough.</li></ul></li><li> Have conflicts with the labs, see OpenAI &amp; Anthropic affiliations<ul><li> Fair enough</li></ul></li><li> Elite out-of-touch people<ul><li> Sorta true (mostly white, wealthy, coastal elites), sorta off (lots of people didn&#39;t come from wealth, and they got into this space because they wanted to be as effectively altruistic as possible)</li></ul></li><li> Billionaires selfish interests<ul><li> Seems wrong; we&#39;re mostly trying to help people rather than make money</li></ul></li><li> Weird longtermist techno-optimists who don&#39;t care about people<ul><li> Weird longtermists, yup.</li><li> Techno-optimistics, kinda, but some of us are pretty pessimistic about AI and using AI to solve that problem.</li><li> Don&#39;t care about people, mostly wrong.</li></ul></li></ul><p> I guess the stigmas seem pretty directionally true about the negatives, and just miss that there is serious thought / positives here.</p><p> If journalists said, &quot;Matheny is EA-affiliated. That suggests he&#39;s related to the community that&#39;s thought most deeply about catastrophic AI risk and has historically tried hard and succeeded at doing lots of good. It should also raise some eyebrows, see FTX,&quot; then I&#39;d be fine. But usually it&#39;s just the second half, and that&#39;s why I&#39;m sympathetic to people avoiding discussing their affiliation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:10:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:10:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, I like this analysis, and I think it roughly tracks how I am thinking about it.</p><p> I do think the bar for &quot;your concerns about me are so unreasonable that I am going to actively obfuscate any markers of myself that might trigger those concerns&quot; is quite high. Like I think the bar can&#39;t be at &quot;well, I thought about these concerns and they are not true&quot;, it has to be at &quot;I am seriously concerned that when the flags trigger you will do something quite unreasonable&quot;, like they are with the gayness and the communism-dissenter stuff. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:12:25 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:12:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Fair enough. This might be a case of governance people overestimating honesty costs / underestimating benefits, which I still think they often directionally do.</p><p> (I&#39;ll also note, what if all the high profile people tried defending EA? (Defending in the sense of - laying out the &quot;Here are the good things; here are the bad things; here&#39;s how seriously I think you should take them, all things considered.&quot;)) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:14:17 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:14:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I don&#39;t think people even have to defend EA or something. I think there are a lot of people who I think justifiably would like to distance themselves from that identity and social network because they have genuine concerns about it.</p><p> But I think a defense would definitely open the door for a conversation that acknowledges that of course there is a real thing here that has a lot of power and influence, and would invite people tracking the structure of that thing and what it might do in the future, and if that happens I am much less concerned about both the negative epistemic effects and the downside risk from this all exploding in my face. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:16:13 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:16:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Yeah, I&#39;d be interested in part because I want the skeptical-of-EA folk to know that I don&#39;t think they&#39;re crazy and I&#39;m not trying to ignore them or lie to them. I&#39;m just opting to keep doing things with/near EA because they think well and they have resources, which is helping me with my altruistic goals. If the skeptics are like &quot;OK, I still distrust you&quot;, fair enough.</p><p> Do you have ideas for ways to make the thing you want here happen? What does it look like? An op-ed from Person X?</p></div></section><h2> How can we make policy stuff more transparent? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:19:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:19:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Probably somewhat controversially, but I&#39;ve been kind of happy about the <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">Politico</a> pieces that have been published. We had two that basically tried to make the case there is an EA conspiracy in DC that has lots of power in a kind of unaccountable way.</p><p> Maybe someone could reach out to the author and be like &quot;Ok, yeah, we are kind of a bit conspiratorial, sorry about that. But I think let&#39;s try to come clean, I will tell you all the stuff that I know, and you take seriously the hypothesis that we really aren&#39;t doing this to profit off of AI, but because we are genuinely concerned about catastrophic risks from AI&quot;.</p><p> That does seem like kind of a doomed plan, but like, something in the space feels good to me. Maybe we can work with some journalists we know to write a thing that puts the cards on the table, and isn&#39;t just a puff-piece that tries to frame everything in the most positive way, but is genuinely asking hard questions. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:20:09 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:20:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Politico: +1 on being glad it came out actually!</p><ul><li> (I tentatively wish people had just said this themselves first instead of it being &quot;found out&quot;. Possibly this is part of how I&#39;ll make the case to people I&#39;m asking to be more open in the future.)</li><li> (Also, part of my gladness with Politico is the more that comes out, the more governance people can evaluate how much this actually blocks their work -- so far, I think very little -- and update towards being more open or just be more open now because now their affiliations have been revealed)</li></ul><p> I like the idea of reaching out to journalists. I&#39;d just want to find one who seems truth-seeking, and share info unilaterally.</p><p> Could you do this? Would you want some bigger name EA / governance person to do it? I&#39;d be happy to chip into a draft. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think I have probably sadly burdened myself with somewhat too much confidentiality to dance this dance correctly, though I am not sure. I might be able to get buy-in from a bunch of people so that I can be free to speak openly here, but it would increase the amount of work a lot, and also decent chance they don&#39;t say yes and then I would need to be super paranoid which bits I leak when working on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:23:16 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:23:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> As in, you&#39;ve agreed to keep too much secret?</p><p> If so, do you have people in mind who aren&#39;t as burdened by this (and who have the relevant context)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, too many secrets. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:25:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:25:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I assume most of the big names have similar confidentiality burdens.</p><p> I&#39;m open to doing it myself, but don&#39;t have a lot of the context and I&#39;m not sure I want to drag the organizations I&#39;m affiliated with into this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:29:02 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:29:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, ideally it would be one of the big names since that I think would meaningfully cause a shift in how people operate in the space.</p><p> Eliezer is great at moving Overton windows like this, but I think he is really uninterested in tracking detailed social dynamics like this, and so doesn&#39;t really know what&#39;s up. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:30:24 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:30:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Do you have time to have some chats with people about the idea or send around a Google doc? Happy to help however. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:32:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:32:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I do feel quite excited about making this happen, though I do think it will be pretty aggressively shut down, and I feel both sad about that, and also have some sympathy in that it does feel like it somewhat inevitably involves catching some people in the cross-fire who were being more private for good reasons, or who are in a more adversarial context where the information here will be used against them in an unfair way, and I still think it&#39;s worth it, but it does make me feel like this will be quite hard.</p><p> I also notice that I am just afraid of what would happen if I were to eg write a post that&#39;s just like &quot;an overview over the EA-ish/X-risk-ish policy landscape&quot; that names specific people and explains various historical plans. Like I expect it would make me a lot of enemies. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:34:23 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:34:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Same, and some of my fear is &quot;this could unduly make the &#39;good plans&#39; success much harder&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:35:42 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:35:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, I think I will sit on this plan for a bit. I hadn&#39;t really considered it before, and I kind of want to bring it up to a bunch of people in the next few weeks and see whether maybe there is enough support for this to make it happen. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:36:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:36:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Nice! (For what it&#39;s worth, I currently like Eliezer most, if he was willing to get into the social stuff)</p><p> Any info it&#39;d be helpful for me to collect from DC folk? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:39:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:39:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Oh, I mean I would love any more data on how much this would make DC folk feel like some burden was lifted from their shoulders vs. it would feel like it would just fuck with their plans.</p><p> I think my actual plan here would maybe be more like an EA Forum post or something that just goes into a lot of detail on what is going on in DC, and isn&#39;t afraid to name specific names or organizations.</p><p> I can imagine directly going for an Op-ed could also work quite well, and would probably be more convincing to outsiders, though maybe ideally you could have both. Where someone writes the forum post on the inside, and then some external party verifies a bunch of the stuff, and digs a bit deeper, and then makes some critiques on the basis of that post, and then the veil is broken. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:39:49 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:39:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Got it.</p><p> Would the DC post include info that these people have asked/would ask you to keep secret? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:40:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:40:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Definitely &quot;would&quot;, though if I did this I would want to sign-post that I am planning to do this quite clearly to anyone I talk to.</p><p> I am also burdened with some secrets here, though not that many, and I might be able to free myself from those burdens somehow. Not sure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:41:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:41:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Ok I shall ask around in the next 2 weeks. Ping me if I don&#39;t send an update by then </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you!!</p></div></section><h2> Concerns about Conjecture </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:50 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, going back a bit to the top-level, I think I would still want to summarize my feelings on the Conjecture thing a bit more.</p><p> Like, I guess the thing that I would feel bad about if I didn&#39;t say it in a context like this, is to be like &quot;but man, I feel like some of the Conjecture people were like at the top of my list of people trying to do weird epistemically distortive inside-game stuff a few months ago, and this makes them calling out people like this feel quite bad to me&quot;.</p><p> In-general a huge component of my reaction to that post was something in the space of &quot;Connor and Gabe are kind of on my list to track as people I feel most sketched out by in a bunch of different ways, and kind of in the ways the post complaints about&quot; and I feel somewhat bad for having dropped my efforts from a few months ago about doing some more investigation here and writing up my concerns (mostly because I was kind of hoping a bit that Conjecture would just implode as it ran out of funding and maybe the problem would go away) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:06 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (For what it&#39;s worth, Conjecture has been pretty outside-game-y in my experience. My guess is this is mostly a matter of &quot;they think outside game is the best tactic, given what others are doing and their resources&quot;, but they&#39;ve also expressed ethical concerns with the inside game approach.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:46:37 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:46:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> (For some context on this, Conjecture tried really pretty hard a few months ago to get a bunch of the OpenAI critical comments on <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">this post</a> deleted because they said it would make them look bad to OpenAI and would antagonize people at labs in an unfair way and would mess with their inside-game plans that they assured me were going very well at the time) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:54 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (I heard a somewhat different story about this from them, but sure, I still take it as is evidence that they&#39;re mostly &quot;doing whatever&#39;s locally tactical&quot;) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:53:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:53:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Anyway, I was similarly disappointed by the post just given I think Conjecture has often been lower integrity and less cooperative than others in/around the community. For instance, from what I can tell,</p><ul><li> They often do things of the form &quot;leaving out info, knowing this has misleading effects&quot;</li><li> One of their reasons for being adversarial is &quot;when you put people on the defense, they say more of their beliefs in public&quot;. Relatedly, they&#39;re into <a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/">conflict theory</a> , which leads them to favor &quot;fight for power&quot; >; &quot;convince people with your good arguments.&quot;</li></ul><p> I have a doc detailing my observations that I&#39;m open to sharing privately, if people DM me.</p><p> (I discussed these concerns with Conjecture at length before leaving. They gave me substantial space to voice these concerns, which I&#39;m appreciative of, and I did leave our conversations feeling like I understood their POV much better. I&#39;m not going to get into &quot;where I&#39;m sympathetic with Conjecture&quot; here, but I&#39;m often sympathetic. I can&#39;t say I ever felt like my concerns were <i>resolved</i> , though.)</p><p> I would be interested in your concerns being written up.</p><p> I do worry about the EA x Conjecture relationship just being increasingly divisive and time-suck-y. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:53:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:53:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is an email I sent Eliezer on April 2nd this year with one paragraph removed for confidentiality reasons:</p><hr><p> Hey Eliezer,</p><p> This is just an FYI and I don&#39;t think you should hugely update on this but I felt like I should let you know that I have had some kind of concerning experiences with a bunch of Conjecture people that currently make me hesitant to interface with them very much and make me think they are somewhat systematically misleading or deceptive. A concrete list of examples:</p><p> I had someone reach out to me with the following quote:<br></p><blockquote><p> Mainly, I asked one of their senior people how they plan to make money because they have a lot of random investors, and he basically said there was no plan, AGI was so near that everyone would either be dead or the investors would no longer care by the time anyone noticed they weren&#39;t seeming to make money. This seems misleading either to the investors or to me — I suspect me, because it would really just be wild if they had no plan to ever try to make money, and in fact they do actually have a product (though it seems to just be Whisper repackaged)</p></blockquote><p> I separately had a very weird experience with them on the Long Term Future Fund where Conor Leahy applied for funding for Eleuther AI. We told him we didn&#39;t want to fund Eleuther AI since it sure mostly seemed like capabilities-research but we would be pretty interested in funding AI Alignment research by some of the same people.</p><p> He then confusingly went around to a lot of people around EleutherAI and told them that &quot;Open Phil is not interested in funding pre-paradigmatic AI Alignment research and that that is the reason why they didn&#39;t fund Eleuther AI&quot;.</p><p> This was doubly confusing and misleading because Open Phil had never evaluated a grant to Eleuther AI (Asya who works at Open Phil was involved in the grant evaluation as a fund member, but nothing else), and of course the reason he cited had nothing to do with the reason we actually gave. He seems to have kept saying this for a long time even after I think someone explicitly corrected the statement to him.</p><p> Another experience I had was Gabe from Conjecture reaching out to LessWrong and trying really quite hard to get us to delete the OpenAI critical comments on this post: <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai</a></p><p> He said he thought people in-general shouldn&#39;t criticize OpenAI in public like this because this makes diplomatic relationships much harder, and when Ruby told them we don&#39;t delete that kind of criticism he escalated to me and generally tried pretty hard to get me to delete things.</p><p> [... One additional thing that&#39;s a bit more confidential but of similar nature here...]</p><p> None of these are super bad but they give me an overall sense of wanting to keep a bunch of distance from Conjecture, and trepidation about them becoming something like a major public representative of AI Alignment stuff. When I talked to employees of Conjecture about these concern the responses I got also didn&#39;t tend to be &quot;oh, no, that&#39;s totally out of character&quot;, but more like &quot;yeah, I do think there is a lot of naive consequentialism here and I would like your help fixing that&quot;.</p><p> No response required, happy to answer any follow-up questions. Just figured I would err more on the side of sharing things like this post-FTX.</p><p> Best,<br> Oliver </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:57:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:57:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p><p> Again, I&#39;m not sure &quot;dealing with Conjecture&quot; is worth the time though. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:01:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:01:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Main emotional affects of the post for me</p><ul><li> I wish someone else had made these points, less adversarially. I feel like governance people do need to hear them. But the frame might make people less likely to engage or make the engagement.<ul><li> Actually, I will admit the post generated lots of engagement in comments and this discussion. It feels uncooperative to solicit engagement via being adversarial though.</li></ul></li><li> I&#39;m disappointed the comments were mostly &quot;Ugh Conjecture is being adversarial&quot; and less about &quot;Should people be more publicly upfront about how worried they are about AI?&quot;</li><li> There were several other community discussions in the past few weeks that I&#39;ll tentatively call &quot;heated community politics&quot;, and I&#39;m feel overall bad about the pattern.<ul><li> (The other discussions were around whether RSPs are bad and whether Nate Soares is bad. In all three cases, I felt like those saying &quot;bad!&quot; had great points, but (a) their points were shrouded in frames of &quot;is this immoral&quot; that felt very off to me, (b) they felt overconfident and not truth-seeking, and (c) I felt like people were half-dealing with personal grudges. This all felt antithetical to parts of LessWrong and EA culture I love.) </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:07:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:07:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, that also roughly matches my emotional reaction. I did like the other RSP discussion that happened that week (and liked my dialogue with Ryan which I thought was pretty productive).</p></div></section><h2> Conjecture as the flag for doomers </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:08:55 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:08:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p></blockquote><p> Yeah, I share this feeling. I am quite glad MIRI is writing more, but am also definitely worried that somehow Conjecture has positioned itself as being aligned with MIRI in a way that makes me concerned people will end up feeling deceived. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Two thoughts</p><ul><li> Conjecture does seem pretty aligned with MIRI in &quot;shut it all down&quot; and &quot;alignment hard&quot; (plus more specific models that lead there).</li><li> I notice MIRI isn&#39;t quite a satisfying place to rally around, since MIRI doesn&#39;t have suggestions for what individuals can do. Conjecture does. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:31 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Can you say more about the feeling deceived worry?</p><p> (I didn&#39;t feel deceived having joined myself, but maybe &quot;Conjecture could&#39;ve managed my expectations about the work better&quot; and &quot;I wish the EAs with concerns told me so more explicitly instead of giving very vague warnings&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:18:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:18:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, for better or for worse I think a lot of people seem to make decisions on the basis of &quot;is this thing a community-sanctioned &#39;good thing to do (TM)&#39;&quot;. I think this way of making decisions is pretty sus, and I feel a bit confused how much I want to take responsibility for people making decisions this way, but I think because Conjecture and MIRI look similar in a bunch of ways, and I think Conjecture is kind of explicitly is trying to carry the &quot;doomer&quot; flag, a lot of people will parse Conjecture as &quot;a community-sanctioned &#39;good thing to do (TM)&#39;&quot;.</p><p> I think this kind of thing then tends to fail in one of two ways:</p><ul><li> The person who engaged more with Conjecture realizes that Conjecture is much more controversial than they realized within the status hierarchy of the community and that it&#39;s not actually clearly a &#39;good thing to do (TM)&#39;, and then they will feel betrayed by Conjecture for hiding that from them and betrayed by others by not sharing their concerns with them</li><li> The person who engaged much more with Conjecture realizes that the organization hasn&#39;t really internalized the virtues that they associate with getting community approval, and then they will feel unsafe and like the community is kind of fake in how it claims to have certain virtues but doesn&#39;t actually follow them in the projects that have &quot;official community approval&quot;</li></ul><p> Both make me pretty sad.</p><p> Also, even if you are following a less dumb decision-making structure, the world is just really complicated, and especially with tons of people doing hard-to-track behind the scenes work, it is just really hard to figure out who is doing real work or not, and Conjecture has been endorsed by a bunch of different parts of the community for-real (like they received millions of dollars in Jaan funding, for example, IIRC), and I would really like to improve the signal to noise ratio here, and somehow improve the degree to which people&#39;s endorsements accurately track whether a thing will be good. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:19:04 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:19:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Fair. People did warn me before I joined Conjecture (but it didn&#39;t feel <i>very</i> different from warnings I might get before working at MIRI). Also, most people I know in the community are aware Conjecture has a poor reputation.</p><p> I&#39;d support and am open to writing a Conjecture post explaining the particulars of</p><ul><li> Experiences that make me question their integrity</li><li> Things I wish I knew before joining</li><li> My thoughts of their lying post and RSP campaign (tl;dr: important truth to the content, but really dislike the adversarial frame) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:19:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:19:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, maybe this dialogue will help, if we edit and publish a bunch of it.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy<guid ispermalink="false"> vFqa8DZCuhyrbSnyx</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:52:33 GMT</pubDate> </item><item><title><![CDATA[Averaging samples from a population with log-normal distribution]]></title><description><![CDATA[Published on November 3, 2023 7:42 PM GMT<br/><br/><p> This was originally a comment on <a href="https://www.lesswrong.com/posts/HjdqPbhRx2ceXHzr2/averages-and-sample-sizes">this post</a> by <a href="https://www.lesswrong.com/users/mruwnik?from=post_header">mruwnik</a> regarding averaging various distributions with different distributions. I made it a post to include pictures.</p><p> The Central Limit Theorem, henceforth CLT, states (in my own words) that regardless of the distribution of a population, sample averages from that population should be normally distributed.</p><p> In theory it should hold for log-normal distributions but that doesn&#39;t feel intuitive to me so I tested it.</p><h3> A silly example of CLT</h3><p> An example I made up in my head to make sense of it:</p><p> Imagine a population comprised of all the people who nap 2 times in a day. Lets plot the ages of this population: </p><figure class="image image_resized" style="width:55.84%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/h8slnvin9v75wbh6vrww" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m83kja1xoshnh65prhwn 158w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/c3n2ytq015maqwxq5jba 238w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/fnslxyz5ukg4nq5lbqnn 318w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lhlkzjbmbklwjlcleskp 398w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/d0i6iip8afyolf7sojf5 478w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/flkvsioij0l7b0iy3fh0 558w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m12rn0wnnl1qh1zwgkef 638w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ora4h7hhuos4jwdjff40 718w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ui07ar9vidttgwmedksl 798w"></figure><p> Mostly infants and elderly people nap, hence the shape of the graph. This data is NOT normal. But if you randomly pick a small sample (n=10) from this population and average it, it will have a mix of old people and infants that averages to middle-age. For example imagine the ages are 80,2,1,2,75,76,1,1,85,70 this will average to about 39. If you do this over and over again with randomly chosen samples you will get a normal distribution. </p><figure class="image image_resized" style="width:76.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jac6cfliah7kii8yldah" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/f55e63dxrlpbd0lbtbsd 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/t6skzwhvzcd4mwxzyrmg 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/g98yjmoh0lybi1axjgyk 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/cblxwycjksseq7uciz0u 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qjutlba1ssdjxfi6sub6 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ggs5rexxsni2n4abocns 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jorzwfr2o044c8llhxur 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/rqepyv05b0qjkmj9u0er 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/te52ibxfnejqunsxfldn 803w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lruwlnhyxygaslo486cb 810w"></figure><h3> Does it work with log-normal populations?</h3><p> I didn&#39;t find it intuitive this would work for a log-normal population.</p><p> If I take data that is log-normal but split it into small samples, will the average of those small samples be normally distributed?</p><h2><br> <strong>Chess matches:</strong></h2><p> I am arranging a chess tournament. I need to figure out how long the average match is so I can plan accordingly. I hear that chess matches seem to follow a log-normal distribution, but I&#39;m not sure what that means statistically so I will try to just average the game times.</p><h3> Data from the population</h3><p> This is what my fake population (n=100,000) looks like. Its log-normal. </p><figure class="image image_resized" style="width:76.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vzjr4piaqb8jny5tvwaz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/zexn7ioinaufqpqr1ib2 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/twpextrya3mba53rof5l 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tqoui3ludnorhanftcov 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ykh6jnxjestjqlnynh6j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lbue48tbeoeyajh2r7gt 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/dcslvkhztoz75zy6st2j 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hmltesa1amkoz1yyirgb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vxetqqcgbjrmwbmdxalq 640w"></figure><h3> Tournaments</h3><p> I observe tournaments (n=100 games) and take a simple average of the match length.</p><p> Here is a histogram plot of the tournaments </p><figure class="image image_resized" style="width:79.15%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/u8klmbrfmlo3xue06tfd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hyx1g6tjajdljqxgse0d 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qei659xjaqu8jqrnevba 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vnmbcmftpueyozmtiibo 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/idvjzyrzpu4bmcg21lam 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/y9ejbiwz2lgjpw852vfw 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tpud4pifh8txi1blprvt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ltxq5cqjowsbvyttkdoj 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/optpwzmplwgecp3euut4 640w"></figure><h2> Lessons?</h2><p> The sample size does matter here. A sample size too small (n=10) and you just end up with the original log-normal distribution. This is expected as the sample size moves from small to large you get a range of smoothing effects pushing the distribution to normal until you get a single point, when the sample = population.</p><br/><br/> <a href="https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal<guid ispermalink="false"> GkEW4vH6M6pMdowdN</guid><dc:creator><![CDATA[CrimsonChin]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:42:17 GMT</pubDate> </item><item><title><![CDATA[Securing Civilization Against Catastrophic Pandemics]]></title><description><![CDATA[Published on November 3, 2023 7:33 PM GMT<br/><br/><h3> Executive summary</h3><p> Pandemic security aims to safeguard the future of civilization from exponentially spreading biological threats. Despite the world&#39;s failure to contain SARS-CoV-2, the existence of far more lethal and transmissible pathogens that afflict animals and growing access to increasingly powerful biotechnologies, no analyses of worst-case scenarios and potential defenses have been published. Here we outline two distinct mechanisms by which pandemic pathogens transmissible between humans could cause societal collapse. In a &quot;Wildfire&quot; pandemic, the justifiable fear of a lethal and highly contagious respiratory agent released in multiple travel hubs leads to the breakdown of essential services. In a &quot;Stealth&quot; pandemic, a rapidly spreading virus with a long incubation period analogous to HIV infects most of humankind. We explain why current pandemic preparedness measures such as rapid vaccines and N95 masks will reliably fail against these threats and outline novel strategies and technologies capable of safeguarding civilisation.</p><h3> Key takeaways</h3><ul><li> Nations cannot yet contain natural, accidental or deliberate pandemics.</li><li> Access to severe pandemics will expand with the ability to program biology.</li><li> If too many essential workers die or refuse to work, societies will collapse.</li><li> A Wildfire pandemic is highly lethal and transmissible enough to infect most essential workers who are taking currently available precautions.<ul><li> Collapse can be prevented by providing essential workers with pandemic-proof personal protective equipment (P4E). (Essential workers are those who must deliver food, water, power and law enforcement without any interruptions.)</li><li> Others can remain safely at home until P4E is available for everyone.</li><li> Once the population is protected, the virus can be locally eradicated.</li></ul></li><li> A Stealth pandemic spreads widely with few symptoms and causes severe harm years later.<ul><li> Societal collapse can be prevented via early warning, credibility, cures, P4E and healthy buildings.<ul><li> Early warning: deep metagenomic sequencing offers reliable detection.</li><li> Credibility: expert responders can assess threats and encourage action.</li><li> Cures: swift medical research can offer hope for the infected.</li><li> P4E: people will need protective equipment that they can trust to block transmission.</li><li> Healthy buildings: the use of germicidal lights and ventilation can prevent indoor infections.</li></ul></li></ul></li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics<guid ispermalink="false"> ksevLrNby34TJKGSF</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:33:06 GMT</pubDate> </item><item><title><![CDATA[If AGI is imminent, why can’t I hail a robotaxi?]]></title><description><![CDATA[Published on November 3, 2023 6:11 PM GMT<br/><br/><p> My intuition is that driving is a domain narrow enough not to require AGI and, moreover, to require a system of far less sophistication and reasoning capabilities than an AGI. SAE Level 5 autonomy — which requires a vehicle to be able to drive autonomously wherever and whenever a typical human driver could — has not been achieved by any company. All autonomous driving projects currently require a human in the loop, either in the driver&#39;s seat or available to provide remote assistance.</p><p> In a world where AGI is achieved by, say, 2030 or 2035, what are the odds Level 5 autonomy hasn&#39;t been solved by 2023? My intuition is that we would expect autonomous vehicles to be a relatively low-hanging fruit that is plucked relatively early in the trajectory from AI solving video games to AI solving ~everything.</p><p> There are a few reasons why this intuition could be wrong:</p><ol><li><p> Maybe self-driving is actually an AGI-level problem or much closer to AGI-level than my intuition tells me. (I would rate this as highly plausible.)</p></li><li><p> Maybe AI progress is such a steep exponential that the lag time between Level 5 autonomy and AGI is much shorter than my intuition tells me. (I would rate this as moderately plausible.)</p></li><li><p> Perhaps Internet-scale data simply isn&#39;t available to train self-driving AIs. (I would rate this as fairly implausible; it would be more much plausible if Tesla weren&#39;t such a clear counterexample.)</p></li><li><p> Robotics in general could prove to be either too hard or unimportant for an otherwise transformative or general AI. (I would rate this as highly implausible; it strikes me as special pleading.)</p></li></ol><p> Please enumerate any additional reasons you can think of in the comments. Also, please present any arguments or evidence you can think of as to why I should accept any of the reasons given above.</p><br/><br/> <a href="https://www.lesswrong.com/posts/cyycbDAffNc6aghas/if-agi-is-imminent-why-can-t-i-hail-a-robotaxi#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/cyycbDAffNc6aghas/if-agi-is-imminent-why-can-ti-hail-a-robotaxi<guid ispermalink="false"> cyycbDAffNc6aghas</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 03 Nov 2023 22:02:21 GMT</pubDate> </item><item><title><![CDATA[Thoughts on open source AI]]></title><description><![CDATA[Published on November 3, 2023 3:35 PM GMT<br/><br/><p> <i>Epistemic status: I only ~50% endorse this, which is below my typical bar for posting something. I&#39;m more bullish on “these are arguments which should be in the water supply and discussed” than “these arguments are actually correct.” I&#39;m not an expert in this, I&#39;ve only thought about it for ~15 hours, and I didn&#39;t run this post by any relevant experts before posting.</i></p><p> <i>Thanks to Max Nadeau and Eric Neyman for helpful discussion.</i></p><p> Right now there&#39;s a significant amount of public debate about open source AI. People concerned about AI safety generally argue that open sourcing powerful AI systems is too dangerous to be allowed; the classic example here is &quot;You shouldn&#39;t be allowed to open source an AI system which can <a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant"><u>produce step-by-step instructions for engineering</u></a> novel pathogens.&quot; On the other hand, open source proponents argue that open source models haven&#39;t yet caused significant harm, and that trying to close access to AI will result in <a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>concentration</u></a> of <a href="https://open.mozilla.org/letter/"><u>power</u></a> in the hands of a few AI labs.</p><p> I think many AI safety-concerned folks who haven&#39;t thought about this that much tend to vaguely think something like “open sourcing powerful AI systems seems dangerous and should probably be banned.” Taken literally, I think this plan is a bit naive: when we&#39;re colonizing Mars in 2100 with the help of our aligned superintelligence, will releasing the weights of GPT-5 really be a catastrophic risk?</p><p> I think a better plan looks something like &quot;You can&#39;t open source a system until you&#39;ve determined and disclosed the sorts of threat models your system will enable, and society has implemented measures to become robust to these threat models. Once any necessary measures have been implemented, you are free to open-source.&quot;</p><p> I&#39;ll go into more detail later, but as an intuition pump imagine that: the best open source model is always 2 years behind the best proprietary model (call it GPT-SoTA) <span class="footnote-reference" role="doc-noteref" id="fnrefnmr2zc5cm4r"><sup><a href="#fnnmr2zc5cm4r">[1]</a></sup></span> ; GPT-SoTA is widely deployed throughout the economy and deployed to monitor for and prevent certain attack vectors, and the best open source model isn&#39;t smart enough to cause any significant harm without GPT-SoTA catching it. In this hypothetical world, so long as we can trust GPT-SoTA <i>,</i> we are safe from harms caused by open source models. In other words, so long as the best open source models lag sufficiently behind the best proprietary models and we&#39;re smart about how we use our best proprietary models, open sourcing models isn&#39;t the thing that kills us.</p><p> In this rest of this post I will:</p><ul><li> Motivate this plan by analogy to responsible disclosure in cryptography</li><li> Go into more detail on this plan</li><li> Discuss how this relates to my understanding of the current plan as implied by responsible scaling policies (RSPs)</li><li> Discuss some key uncertainties</li><li> Give some higher-level thoughts on the discourse surrounding open source AI</li></ul><h2> <strong>An analogy to responsible disclosure in cryptography</strong></h2><p> <i>[I&#39;m not an expert in this area and this section might get some details wrong. Thanks to Boaz Barak for pointing out this analogy (but all errors are my own).</i></p><p> <i>See this footnote</i> <span class="footnote-reference" role="doc-noteref" id="fnreflndvavl4r2a"><sup><a href="#fnlndvavl4r2a">[2]</a></sup></span> <i>for a discussion of alternative analogies you could make to biosecurity disclosure norms, and whether they&#39;re more apt to risk from open source AI.]</i></p><p> Suppose you discover a vulnerability in some widely-used cryptographic scheme. Suppose further that you&#39;re a good person who doesn&#39;t want anyone to get hacked. What should you do?</p><p> If you publicly release your exploit, then lots of people will get hacked (by less benevolent hackers who&#39;ve read your description of the exploit). On the other hand, if <a href="https://en.wikipedia.org/wiki/White_hat_(computer_security)"><u>white-hat</u></a> <u>&nbsp;</u> hackers always keep the vulnerabilities they discover secret, then the vulnerabilities will never get patched until a black-hat hacker finds the vulnerability and exploits it. More generally, you might worry that not disclosing vulnerabilities could lead to a &quot;security overhang,&quot; where discoverable-but-not-yet-discovered vulnerabilities accumulate over time, making the situation worse when they&#39;re eventually exploited.</p><p> In practice, the cryptography community has converged on a <i>responsible disclosure</i> policy along the lines of:</p><ul><li> First, you disclose the vulnerability to the affected parties.<ul><li> As a running example, consider Google&#39;s <a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"><u>exploit for the SHA-1 hash function</u></a> . In this case, there were many affected parties, so Google publicly posted a proof-of-concept for the exploit, but didn&#39;t include enough detail for others to immediately reproduce it.</li><li> In other cases, you might privately disclose more information, eg if you found a vulnerability in the Windows OS, you might privately disclose it to Microsoft along with the code implementing an exploit.</li></ul></li><li> Then you set a reasonable time-frame for the vulnerability to be patched.<ul><li> In the case of SHA-1, the patch was &quot;stop using SHA-1&quot; and the time-frame for implementing this was 90 days.</li></ul></li><li> At the end of this time period, you may publicly release your exploit, including with source code for executing it.<ul><li> This ensures that affected parties are properly incentivized to patch the vulnerability, and helps other white-hat hackers find other vulnerabilities in the future.</li></ul></li></ul><p> As I understand things, this protocol has resulted in our cryptographic schemes being relatively robust: people mostly don&#39;t get hacked in serious ways, and when they do it&#39;s mostly because of attacks via social engineering (eg <a href="https://en.wikipedia.org/wiki/Operation_Rubicon"><u>the CIA secretly owning their encryption provider</u></a> ), not via attacks on the scheme. <span class="footnote-reference" role="doc-noteref" id="fnref3ejitdom769"><sup><a href="#fn3ejitdom769">[3]</a></sup></span></p><h2> <strong>Responsible disclosure for capabilities of open source AI systems: an outline</strong></h2><p> <i>[Thanks to Yusuf Mahmood for pointing out that the protocol outlined in this section is broadly similar to the one</i> <a href="https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf"><i><u>here</u></i></a> <i>. More generally, I expect that ideas along these lines are already familiar to people who work in this area.]</i></p><p> In this section I&#39;ll lay out an protocol for open sourcing AI systems which is analogous to the responsible disclosure protocol from cryptography. Suppose the hypothetical company Mesa has trained a new AI system <a href="https://en.wikipedia.org/wiki/Camelidae"><u>camelidAI</u></a> which Mesa would like to open source. Let&#39;s also call the most capable proprietary AI system GPT-SoTA, which we can assume is well-behaved <span class="footnote-reference" role="doc-noteref" id="fnref1art7tmmdob"><sup><a href="#fn1art7tmmdob">[4]</a></sup></span> . I&#39;m imagining that GPT-SoTA is significantly more capable than camelidAI (and, in particular, is superhuman in most domains). In principle, the protocol below will still make sense if GPT-SoTA is worse than camelidAI (because open source systems have surpassed proprietary ones), but it will degenerate to something like “ban open source AI systems once they are capable of causing significant novel harms which they can&#39;t also reliably mitigate.”</p><p> In this protocol, before camelidAI can be open sourced, [?? Mesa?, the government?, a third-party? ??] must:</p><ul><li> Evaluate camelidAI for what sorts of significant novel harms it could cause if open-sourced. <span class="footnote-reference" role="doc-noteref" id="fnref7501lf6ibc3"><sup><a href="#fn7501lf6ibc3">[5]</a></sup></span> These evaluators should have, at a minimum, access to all the tools that users of the open source system will have, including eg the ability to finetune camelidAI, external tooling which can be built on top of camelidAI, and API calls to GPT-SoTA. So a typical workflow might look something like: have GPT-SoTA generate a comprehensive list of possible takeover plans, then finetune camelidAI to complete steps in these plans. For example, we might find that:<ul><li> After finetuning, camelidAI is capable of targeted phishing (also called <a href="https://en.wikipedia.org/wiki/Phishing"><u>spear-phishing</u></a> ) at human-expert levels, but scaled up to many more targets.</li><li> camelidAI can provide <a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.13952.pdf"><u>layperson-followable instructions for manufacturing a novel pathogen, including which DNA synthesis companies and biological labs don&#39;t screen customers and orders</u></a> .</li></ul></li><li> Disclose these new harmful capabilities to [?? the government?, a third-party monitor?, affected parties? ??].</li><li> Work with relevant actors to improve systems until they are robust to everyone having access to camelidAI.<ul><li> Eg make sure that there is a widely-available open-source tool which can detect phishing attempts as sophisticated as camelidAI&#39;s with very high reliability.</li><li> Eg shut down the DNA synthesis companies and biolabs that don&#39;t screen orders, or force them to use GPT-SoTA to screen orders to potential pandemic agents.</li><li> Note that if camelidAI is very capable, some of these preventative measures might be very ambitious, eg “make society robust to engineered pandemics.” The source of hope here is that we have access to a highly capable and well-behaved GPT-SoTA.</li><li> Note also that these “robustification” measures are things that we should do anyway, even if we didn&#39;t want to open source camelidAI; otherwise there would be an overhang that a future unaligned AI (possibly a model which was open sourced illegally) could exploit.</li></ul></li><li> Once society is robust to harms caused by camelidAI (as certified by [??]), you are allowed to open source camelidAI.</li><li> On the other hand, if Mesa open sources camelidAI before finishing the above process, then it&#39;s treated as a bad actor (similarly to how we would treat a hacker who releases an exploit without responsible disclosure).<ul><li> Maybe this means that you are held liable for harms caused by camelidAI or something, not really sure.</li></ul></li></ul><p> As examples, let me note two special cases of this protocol:</p><ul><li> Suppose camelidAI = LLaMA-2. I think probably there are no significant novel harms enabled by access to  LLaMA-2 <span class="footnote-reference" role="doc-noteref" id="fnrefvqihg0rt649"><sup><a href="#fnvqihg0rt649">[6]</a></sup></span> . Thus, after making that evaluation, the mitigation step is trivial: no “patches” are needed, and LLaMA-2 can be open sourced. (I think this is good: AFAICT, LLaMA-2&#39;s open sourcing has been good for the world, including for alignment research.)</li><li> Suppose camelidAI is capable of discovering this one weird trick for turning rocks and air into a black hole (astrophysicists hate it!). Assuming there is no plausible mitigation for this attack, camelidAI never gets to be open sourced. (I hope that even strong open source proponents would agree that this is the right outcome in this scenario.)</li></ul><p> I&#39;ll also note two ways that this protocol differs from from responsible disclosure in cryptography:</p><ol><li> Mesa is not allowed to set a deadline on how long society has to robustify itself to camelidAI&#39;s capabilities. If camelidAI has a capability which would be catastrophic if misused and it takes a decade of technological progress before we can come up with a &quot;patch&quot; for the problem, then Mesa doesn&#39;t get to open source the model until that happens.</li><li> In cryptography, the onus is on affected parties to patch the vulnerability, but in this case the onus is partly on the AI system&#39;s developer.</li></ol><p> These two differences mean that other parties aren&#39;t as incentivized to robustify their systems; in principle they could drag their feet forever and Mesa will never get to release camelidAI. I think something should be done to fix this, eg the government should fine companies which insufficiently prioritize implementing the necessary changes.</p><p> But overall, I think this is fair: if you are aware of a way that your system could cause massive harm and you don&#39;t have a plan for how to prevent that harm, then you don&#39;t get to open source your AI system.</p><p> One thing that I like about this protocol is that it&#39;s hard to argue with: if camelidAI is demonstrably capable of eg autonomously engineering a novel pathogen, then Mesa can&#39;t fall back to claiming that the <a href="https://twitter.com/ID_AA_Carmack/status/1719077965055533455"><u>harms are imaginary</u></a> or <a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>overhyped</u></a> , or that <a href="https://twitter.com/ylecun/status/1719692258591506483"><u>as a general principle open source AI makes us safer</u></a> . We will have a concrete, demonstrable harm; and instead of debating whether AI harms can be mitigated by AI in the abstract, we can discuss how to mitigate this particular harm. If AI can provide a mitigation, then we&#39;ll find and implement the mitigation. And similarly, if it ends up that the harms <i>were</i> imaginary or overhyped, then Mesa will be free to open source camelidAI.</p><h2> <strong>How does this relate to the current plan?</strong></h2><p> As I understand things, the high-level idea driving many responsible scaling policy (RSP) proponents is something like:</p><blockquote><p> Before taking certain actions (eg training or deploying an AI system), AI labs need to make &quot;safety arguments,&quot; ie arguments that this action won&#39;t cause significant harm. For example, if they want to deploy a new system, they might argue:</p><ol><li> Our system won&#39;t cause harm because it&#39;s not capable enough to do significant damage. (If OpenAI had been required to make a safety argument before releasing GPT-4, this is likely the argument they would have made, and it seems true to me.)</li><li> Our system <i>could</i> cause harm if it attempted to but it won&#39;t attempt to because, eg it is only deployed through an API and we&#39;ve ensured using [measures] that no API-mediated interaction could induce it to attempt harm.</li><li> Our system <i>could</i> cause harm if it attempted to and we can&#39;t rule out that it will attempt to, but it won&#39;t succeed in causing harm because, eg it&#39;s only being used in a tightly-controlled environment where we have extremely good measures in place to stop it from successfully executing harmful actions.</li></ol><p> If no such argument exists, then you need to do something which <i>causes</i> such an argument to exist (eg doing a better job of aligning your model, so that you can make argument (2) above). Until you&#39;ve done so, you can&#39;t take whatever potentially-risky action you want to take.</p></blockquote><p> I think that if you apply this idea in the case where the action is &quot;open sourcing an AI system,&quot; you get something pretty similar to the protocol I outlined above: in order to open source an AI system, you need to make an argument that it&#39;s safe to open source that system. If there is no such argument, then you need to do stuff (eg improve email monitoring for phishing attempts) which make such an argument exist.</p><p> Right now, the safety argument for open sourcing would be the same as (1) above: current open source systems aren&#39;t capable enough to cause significant novel harm. In the future, these arguments will become trickier to make, especially for open source models which can be modified (eg finetuned or incorporated into a larger system) and whose environment is potentially &quot;the entire world.&quot; But, as the world is radically changed by advances in frontier AI systems, these arguments might continue to be possible for non-frontier systems. (And I expect open source models to continue to lag the frontier.)</p><h2> <strong>Some uncertainties</strong></h2><p> Here are some uncertainties I have:</p><ul><li> In practice, how does this play out?<ul><li> I think a reasonable guess might be: in a few years, SoTA models will be smart enough to cause major catastrophes if open-sourced, and – even with SoTA AI assistance – we won&#39;t be able to patch the relevant vulnerabilities until after the singularity (after which the ball is out of our court). If so, this protocol basically boils down to a ban on open source AI with extra steps.</li><li> I&#39;ll note, however, that open source proponents (many of whom expect slower progress towards harmful capabilities) probably disagree with this forecast. If they are right then this protocol boils down to “evaluate, then open source.” I think there are advantages to having a policy which specializes to what AI safety folks want if AI safety folks are correct about the future and specializes to what open source folks want if open source folks are correct about the future.</li></ul></li><li> Will evaluators be able to anticipate and measure all of the novel harms from open source AI systems?<ul><li> Sadly, I&#39;m not confident the answer is “yes,” and this is the main reason I only ~50% endorse this post. Two reasons I&#39;m worried evaluators might fail:<ul><li> Evaluators might not have access to significantly better tools than the users, and there are many more users. Eg even though the evaluators will be assisted by GPT-SoTA, so will the millions of users who will have access to camelidAI if it is open-sourced.</li><li> The world might change in ways that enable new threat models after camelidAI is open-sourced. For example, suppose that camelidAI + GPT-SoTA isn&#39;t dangerous, but camelidAI + GPT-(SoTA+1) (the GPT-SoTA successor system) is dangerous. If GPT-(SoTA+1) comes out a few months after camelidAI is open-sourced, this seems like bad news.</li></ul></li></ul></li><li> Maybe using subtly unaligned SoTA AI systems to evaluate and monitor other AI systems is really bad for some reason that&#39;s hard for us to anticipate?<ul><li> Eg something something the AI systems coordinate with each other.</li></ul></li></ul><h2> <strong>Some thoughts on the open source discourse</strong></h2><p> I think many AI safety-concerned folks make a mistake along the lines of: &quot;I notice that there is some capabilities threshold <i>T</i> past which everyone having access to an AI system with capabilities >; <i>T</i> would be an existential threat in today&#39;s world. On the current trajectory, someday someone will open source an AI system with capabilities >; <i>T</i> . Therefore, open sourcing is likely to lead to extinction and should be banned.&quot;</p><p> I think this reasoning ignores the fact that at the time someone first tries to open source a system of capabilities >; <i>T</i> , the world will be different in a bunch of ways. For example, there will probably exist proprietary systems of capabilities <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gg T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">≫</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。 So overall, I think folks in the AI safety community worry too much about threats from open source models.</p><p> Further, AI safety community opposition to open source AI is currently generating <i>a lot</i> of animosity from the open source community. For background, the open source ideology is deeply interwoven with the history of software development, and strong proponents of open source have a lot of representation and influence in tech. <span class="footnote-reference" role="doc-noteref" id="fnrefhqvunm4kwo5"><sup><a href="#fnhqvunm4kwo5">[7]</a></sup></span> I&#39;m somewhat worried that on the current trajectory, AI safety vs. open source will be a major battlefront making it hard to reach consensus (much worse than the IMO not-too-bad AI discrimination/ethics vs. x-risk division).</p><p> To the extent that this animosity is due to unnecessary fixation on the dangers of open source or sloppy arguments for the existence of this danger, I think this is really unfortunate. I think there are good arguments for worrying <i>in particular ways</i> about the potential dangers of open sourcing AI systems <i>at some scale</i> , and I think being much more clear on the nuances of these threat models might lead to much less animosity.</p><p> Moreover, I think there&#39;s a good chance that by the time open source models are dangerous, we will have concrete evidence that they are dangerous (eg because we&#39;ve already seen that unaligned proprietary models of the same scale are dangerous). This means that policy proposals of the shape “if [evidence of danger], then [policy]” get most of the safety benefit while also failing gracefully (ie not imposing excessive development costs) in worlds where the safety community is wrong about the pending dangers. Ideally, this means that such policies are easier to build consensus around. </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnnmr2zc5cm4r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnmr2zc5cm4r">^</a></strong></sup></span><div class="footnote-content"><p> This currently seems about right to me, ie that LLaMA-2 is a little bit worse than GPT-3.5 which came out 20 months ago.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlndvavl4r2a"> <span class="footnote-back-link"><sup><strong><a href="#fnreflndvavl4r2a">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://forum.effectivealtruism.org/posts/PTtZWBAKgrrnZj73n/biosecurity-culture-computer-security-culture"><u>Jeff Kaufman has written about</u></a> a difference in norms between the computer security and biosecurity communities. In brief, while computer security norms encourage trying to break systems and disclosing vulnerabilities, biosecurity norms discourage open discussion of possible vulnerabilities. Jeff attributes this to a number of structural factors, including how difficult it can be to patch biosecurity vulnerabilities; it&#39;s possible that threat models from open source AI have more in common with biorisk models, in which case we should instead model our defenses based on them. For more ctrl-f “cold sweat” <a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/#crispr-based-gene-drive-022318"><u>here</u></a> to read Kevin Esvelt discussing why he didn&#39;t disclose the idea of a gene drive to anyone – not even his advisor – until he was sure that it was defense-dominant. (h/t to Max Nadeau for both of these references, and to most of the references to bio-related material that I link elsewhere.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3ejitdom769"> <span class="footnote-back-link"><sup><strong><a href="#fnref3ejitdom769">^</a></strong></sup></span><div class="footnote-content"><p> I expect some folks will want to argue about whether our cryptography is actually all that good, or point out that the words “relatively” and “mostly” in that sentence are concerning if you think that “we only get one shot” with AI. So let me preemptively clarify that I don&#39;t care too much about the precise success level of this protocol; I&#39;m mostly using it as an illustrative analogy.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1art7tmmdob"> <span class="footnote-back-link"><sup><strong><a href="#fnref1art7tmmdob">^</a></strong></sup></span><div class="footnote-content"><p> We can assume this because we&#39;re dealing with the threat model of catastrophes caused by open source AI. If you think the first thing that kills us is misaligned proprietary AI systems, then you should focus on that threat model instead of open source AI.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7501lf6ibc3"> <span class="footnote-back-link"><sup><strong><a href="#fnref7501lf6ibc3">^</a></strong></sup></span><div class="footnote-content"><p> This is the part of the protocol that I feel most nervous about; see bullet point 2 in the &quot;Some uncertainties&quot; section.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvqihg0rt649"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvqihg0rt649">^</a></strong></sup></span><div class="footnote-content"><p> It&#39;s <a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.18233.pdf"><u>shown here</u></a> that a LLaMA-2 finetuned on virology data was useful for giving hackathon participants instructions for obtaining and releasing the reconstructed 1918 influenza virus. However, it&#39;s not clear that this harm was novel – we don&#39;t know how much worse the participants would have done given only access to the internet.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhqvunm4kwo5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhqvunm4kwo5">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;ve noticed that AI safety concerns have had a hard time gaining traction at MIT in particular, and one guess I have for what&#39;s going on is that the open source ideology is very influential at MIT, and all the open source people currently hate the AI safety people.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai<guid ispermalink="false"> WLYBy5Cus4oRFY3mu</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Fri, 03 Nov 2023 15:35:42 GMT</pubDate></item></channel></rss>