<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 24 日星期五 20:12:16 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Why focus on schemers in particular (Sections 1.3 and 1.4 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 24, 2023 7:18 PM GMT<br/><br/><p>这是我的报告“<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？”</a>的第 1.3 和 1.4 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984742-why-focus-on-schemers-in-particular-sections-1-3-1-4-of-scheming-ais">请点击此处</a>，或在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>为什么要特别关注阴谋者？</h1><p>正如我上面提到的，我认为阴谋家是这个分类中最可怕的模型类别。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-1" id="fnref-cBw6ixqDSsmqKfBfM-1">[1]</a></sup>为什么这么想？毕竟，<em>所有</em>这些模式难道不会都存在危险的错位和权力追求吗？例如，如果剧集奖励能够带来更多的奖励，寻求奖励的人就会试图控制奖励过程。如果你指定的目标错误，训练圣徒最终可能会出现偏差；即使你正确地指定了目标，并以某种方式避免了训练游戏，你最终也可能会得到一个被错误概括的非训练游戏玩家。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-2" id="fnref-cBw6ixqDSsmqKfBfM-2">[2]</a></sup>那么，基本上每一个转折点是否都会出现某种错位？为什么要特别关注阴谋者？</p><p>本节解释原因。然而，如果您对关注阴谋者感到足够满意，请随意跳至第 1.4 节。</p><h2>我最担心的错位类型</h2><p>为了解释为什么我认为阴谋家特别可怕，我想首先谈谈我最担心的错位类型。</p><p>首先：我在这里关注的是我<a href="https://arxiv.org/pdf/2206.13353.pdf">在其他地方</a>所说的“实际权力寻求联盟”——也就是说，我们的人工智能是否会<em>根据他们实际上会参与的任何输入</em>进行有问题的权力寻求形式<em>收到</em>。重要的是，这意味着我们不需要在人工智能中灌输能够带来良好结果的目标<em>，即使受到任意数量的优化能力的影响</em>（例如，我们不需要通过尤德科夫斯基的“<a href="https://arbital.com/p/omni_test/">全方位测试</a>”）。相反，我们只需要向人工智能灌输目标，<em>考虑到人工智能将面临的实际选择和限制</em>，以及它们将调动的<em>优化能力的实际水平</em>，这些目标可以带来良好的结果。</p><p>这是一个重要的较低标准。事实上，假设我们以正确的方式控制他们的能力、选择和激励措施，原则上<em>所有</em>这些模型（甚至策划者）都可以满足这个标准。例如，虽然在给予奖励的情况下，寻求奖励的人确实会试图抓住奖励过程的控制权，但我们工具集中的一个工具是：不给它机会。虽然范式策划者可能正在等待，希望有一天能够逃脱并夺取权力（但同时表现良好），但我们工具箱中的一个工具是：不让它逃脱（同时继续受益于其良好的表现） ）。</p><p>当然，要在这方面取得成功，就需要我们的监控、控制和安全工作相对于我们所担心的人工智能而言足够强大，并且即使前沿人工智能能力不断扩大，它们仍然如此。但这引出了我的第二点：即，我在这里对一些相对早期的一组大致人类水平（或者至少不是超人水平）的模型的实际 PS 对齐特别感兴趣。也就是说：我认为，现在不应该关注任意能力的人工智能的排列，而应该关注成功地使用一些相对较早的一代非超人人工智能来为人类执行大量高质量的认知工作的目标。我们 - 包括：认知工作可以阐明传统的对齐相关目标，例如可解释性、可扩展的监督、监控、控制、人类之间的协调、深度学习的一般科学、替代性（和更可控/可解释的）人工智能范式以及喜欢。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-3" id="fnref-cBw6ixqDSsmqKfBfM-3">[3]</a></sup></p><p>捍卫这个焦点超出了我在这里的目的。但这对于我将在下文中使用的镜头很重要。特别是：我认为人工智能发展可能会出现某个关键的（也许是：极其短暂的）阶段，在这个阶段我们的人工智能还<em>没有</em>强大到足以接管（或摆脱人类控制），但它们在哪里原则上，如果我们能够成功地诱导它们这样做，它们仍然能够为我们执行极其有价值且与阵营相关的认知工作。我对可能破坏这种可能性的错位形式特别感兴趣。</p><p>最后：我对 PS 错位的形式特别感兴趣，在这种错位中，相关的寻求权力的人工智能专门致力于引起、参与或受益于某种全面的人类权力剥夺——我将其称为“ “人工智能接管。”诚然，并非所有有问题的错位权力寻求形式都旨在以这种方式进行接管——例如，人工智能可能只是想逃到互联网上，以获得更多的自由和资源，即使它从未期望人类剥夺权力。原则上，即使没有个人人工智能积极优化，最终也可能导致人类被剥夺权力。但从存在风险的角度来看，我最担心的是那些积极<em>试图</em>推翻人类或从这种推翻中受益的人工智能。</p><h2>与寻求剧集奖励的人相比</h2><p>考虑到这个关于错位问题的框架，让我们转向阴谋者相对于我讨论的其他模型类别有多可怕的问题。</p><p>首先，我将阴谋者与寻求奖励的人进行对比。在这里，我看到了一些认为阴谋家更可怕的理由。</p><h3>对诚实测试的反应</h3><p>首先，阴谋者会采取更多措施主动向人类<em>隐藏</em>他们的偏差。特别是：正如<a href="https://www.alignmentforum.org/posts/HpzHjKjGQ4cKiY3jX/3-levels-of-threat-obfuscation#How_bad_is_the_situation_if_straightforward_training_leads_to_the_training_game__but_not_to_deceptive_alignment_or_gradient_hacking_">卡诺夫斯基（Karnofsky，2023）</a>指出的那样，原则上，你可以让一个剧集奖励寻求者<em>透露</em>，他们为了追求剧集奖励而倾向于做出不良行为，只需将他们放入在这一集里，不良行为会得到奖励，他们知道这一点。</p><ul><li><p>这里的一个具体例子可能是：让具有情境意识的人工智能能够操纵为情节分配奖励的过程的所有组成部分，并查看它的作用（例如，它是否开始尝试绕过通常的过程并最大化相关奖励？）。</p></li><li><p>另一种可能是：在一个场景中放置一个具有情境感知能力的人工智能，其中不诚实、无益和/或有害的行为将受到奖励（并且它有可靠的证据证明这一事实）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-4" id="fnref-cBw6ixqDSsmqKfBfM-4">[4]</a></sup></p></li></ul><p>请注意，这里的重点并不是试图<em>欺骗</em>模型来揭示它不想揭示的错位（称之为“陷阱”）。它不需要欺骗模型，例如，现在是否是尝试逃跑的好时机。 （毕竟，陷阱的问题在于足够先进的人工智能可能能够检测到它们。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-5" id="fnref-cBw6ixqDSsmqKfBfM-5">[5]</a></sup> ）相反，这里的目的是提供一个“诚实的测试”，它试图设置模型的实际选项，以便揭示其真实目标是最好的整体策略。 （不过：让模型相信我们对其选项的呈现可能是一个额外的挑战，特别是如果我们在其他地方试图欺骗它的话。）</p><p>现在，原则上，您也可以尝试使用这样的诚实测试来检测阴谋。例如：您可能会尝试提供一个模型，该模型迄今为止似乎可以访问某些灵活的、资源充足但仍受控制的环境（例如，它可以根据需要使用一些安全的计算集群），希望它如何利用资源和灵活性将揭示它真正想要什么。然而，这对阴谋者的作用远不如对情节奖励寻求者的作用可靠。尤其：</p><ul><li><p>对于寻求按集奖励的人来说，您知道您只需为他们提供按集奖励，他们就会接受。而您不确定阴谋者真正想要什么，因此不确定向其提供什么以使其揭示其真正目标（因此需要提供灵活的资源）。例如，也许您的孤立的计算资源块对相关策划者来说并没有多大兴趣，因为它重视无法使用这些资源创建/模拟的东西。</p></li><li><p>更重要的是，即使“揭示你的目标”选项对阴谋者有<em>一定的</em>吸引力，它仍然可能认为继续假装一致是一个更好的策略，因为它稍后获得更多权力的前景超过了短期利益揭示你的目标选项（例如，也许它可以在你的计算机上做一些它喜欢的事情，但如果它摆脱了人类的控制，它会做更多的事情）。而寻求按集奖励的人只想要按集奖励，因此不必进行这种成本效益分析。</p></li></ul><p>相对于阴谋者，这种动态关系到寻求奖励的人更广泛的利益：也就是说，如果你能够<em>验证</em>- 例如，通过这些“诚实的测试”之一 - 给定的模型是一个奖励 -如果你是剧集寻求者，那么就有一种有用的感觉，即你“知道你在做什么”。也就是说：是的，你的模型是错位的，如果有机会的话，你的模型会夺取奖励过程的控制权。但是：你也<em>知道它想要什么</em>，以及它会对什么激励做出反应。这类知识似乎对于预测其行为非常有帮助；并且（如果您可以保留对模型的控制），以有用的方式利用其认知能力（特别是我在其他地方所说的“ <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">可检查任务</a>”——更多内容见下文）。</p><h3>时间范围和总体“野心”</h3><p>相对于阴谋者来说，剧集奖励寻求者的另一个好处是，剧集奖励寻求者将他们的优化范围（因此，他们的权力寻求）限制在剧集中，而阴谋者则不然。这意味着寻求剧集奖励的人不会有动力参与或支持那些在剧集时间范围内没有得到回报的剥夺人类权力的努力。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-6" id="fnref-cBw6ixqDSsmqKfBfM-6">[6]</a></sup></p><p>因此，举例来说，假设至少需要六个月的时间，任何类型的人工智能接管或夺取奖励过程控制权事件才变得可行。在这种情况下，剧集长度少于六个月的剧集奖励寻求者将没有任何动机尝试参与或支持此类接管，或尝试控制奖励过程本身。如果它有证据表明其他人工智能正在计划进行此类接管，并且它可以通过向人类透露这一证据来获得更多奖励，那么它就会被激励这样做。</p><p>这与阴谋家形成鲜明对比，阴谋家通常被认为是为了追求极其长远的目标而策划的。也就是说，范式策划者从一开始就着眼于人类的剥夺权，即使这种剥夺权在很长一段时间内不可能实现——并且它会相应地进行计划和行动。</p><p>时间范围的差异与奖励寻求者相对于阴谋者的更广泛的利益有关——也就是说，阴谋者最终可能会更加直观地“雄心勃勃”，从某种意义上说，这表明对权力和资源有更大、更具破坏性的胃口。现在，这里的“野心”究竟意味着什么尚不清楚（另请参见术语“广泛范围”的用法），并且在对某些类型的结构进行足够强烈的优化的背景下，直观上“适度”的目标（例如，“正确地解决这个数学问题”），可能会导致令人惊讶的权力掠夺行为（例如，使用大量的计算资源检查和重新检查有问题的问题）。尽管如此，人们所期望的“在所有时间和空间上最大化回形针总数”的模型与“下午 5 点之前创建三到五个回形针”的模型之间存在着直观的差异。今天美国东部时间”，特别是如果所讨论的模型不是任意强大的。值得注意的是，例如，在后一项任务中，获得 >;99% 的可用效用似乎要容易得多，而无需进行破坏性的权力寻求。而且由于优化时间范围的限制，在我看来，从这个意义上讲，剧集奖励寻求者通常不会比阴谋者更直观地雄心勃勃。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-7" id="fnref-cBw6ixqDSsmqKfBfM-7">[7]</a></sup></p><p>也就是说，正如我将在下面讨论的那样，对于给定的剧集奖励寻求者来说，剧集越长，这种考虑就越不适用（并且只要我们希望我们的人工智能执行长期任务，我们很可能会这样做）训练他们进行很长的剧集）。事实上，有一些方法可以概括“情节奖励”的概念，这将导致任意长的优化时间范围，即使您可能认为情节在时间上是有限的。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-8" id="fnref-cBw6ixqDSsmqKfBfM-8">[8]</a></sup>例如，也许模型关心与某些行为单位相关的奖励，从而使模型希望确保存储在某个数据库中的某些数字永远不会被<em>编辑</em>——在这种情况下，它会有激励在任意长的时间范围内获得并维持权力（这是在训练模型时要小心的原因之一，其中包括稍后返回并编辑与给定情节相关的奖励，因为您可能会无意中通过激励来“延长情节”优化模型以确保剧集的奖励永远不会被编辑）。</p><p>因此，总的来说，人们越是从“这一集奖励寻求者仅在<em>有限</em>的时间范围内进行优化”中寻求安慰，就越需要确保所讨论的限制确实成立。</p><h3>沙袋和“早期破坏”</h3><p>与阴谋者相比，寻求剧集奖励的第三个（也是相关的）好处是，在我看来，阴谋者似乎更有可能参与“ <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">沙袋</a>”——特别是我所说的“早期破坏”。</p><p>当模型故意在任务上表现低于其能力时，就会出现沙袋现象。在这里，我对那些我们可以“检查”但我们自己无法执行的任务特别感兴趣——例如，我们无法提出证据，但我们可以验证其有效性；或者我们不会注意到的安全漏洞，但一旦向我们指出，我们就可以看到它是一个漏洞。正如我<a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis#Why_does_the_hypothesis_matter_">在其他地方讨论过的那样</a>，我认为如果模型不对可检查任务进行沙袋处理，那么这将打开一组非常强大的功能，我们可以利用这些功能来实现对齐、可解释性、安全性等目的。</p><p>然而，依赖这些功能的一个大问题是，原则上模型似乎能够进行有时所谓的“ <a href="https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations">探索黑客</a>”。也就是说，即使我们能够检查（然后奖励）任务执行行为，如果我们自己不知道如何完成任务，模型原则上可能会拒绝向我们展示任务执行行为有问题（而如果我们知道如何执行任务，我们就会知道模型应该做什么，并且能够相应地更新其策略）。</p><p>不过，似乎情节奖励寻求者不会这样做（前提是我们将情节限制为有问题的可检查任务）。例如，假设我们想知道是否有可能破解一台给定的计算机并使屏幕上出现蓝色香蕉。所以我们要求我们的模型这样做。因为任务是可检查的（例如，我们可以判断香蕉是否出现），所以我们可以对任务的成功执行给予最大奖励。因此，寻求剧集奖励的人没有动力在这里沙袋——他只会在剧集中获得更少的奖励。这种广泛的动态可以推广到其他类型的能力评估。也就是说，就像寻求奖励的人会透露他们的<em>目标</em>一样，这样做会带来奖励，他们也会透露自己的能力。</p><p>相比之下，阴谋家则是另一回事。一个以其他方式（工具性地）优化情节奖励的策划者可能仍然会选择特定的场合来获得比原则上更少的奖励——例如，通过在可检查的、安全关键的任务上设置沙袋，或者通过在评估过程中隐藏其全部能力——以便以其他方式进一步实现其目标。</p><p>在这里，我特别关注我所说的“早期破坏”——即，在人工智能尚未成熟的人工智能发展阶段，失调的人工智能积极努力战略性地破坏人类的排列、控制和协调工作。强大到足以摆脱人类的控制或更广泛地接管。这里的例子可能包括：故意将后门和其他安全漏洞插入人工智能实验室的代码中，以用于以后的逃逸/自我渗透尝试；故意进行联盟研究，但这种研究将以有利于收购的方式失败；以一种稍后将服务于人工智能接管的方式有意操纵人类话语和协调，等等。</p><p>早期破坏是一种特别可怕的监督失败，因为它是为了促进人工智能接管而积极优化的，而不仅仅是根据相关监督机制“看起来尽可能好”。因此，举例来说：尝试使用人工智能帮助我们进行对齐研究的一个担忧是，人类将无法充分区分实际上良好的对齐研究和仅表面上良好的对齐研究——也就是说，产生良好的比对研究还不够“可检查”。我不清楚这是正确的（值得注意的是，例如，在人类水平模型时代的许多关键对齐研究很可能涉及相当简单的经验实验，其结果和含义似乎很可能是相当“可检查”），但即使是这样：<em>根据评估过程</em>（即使这会损害其质量）而优化的对齐研究与<em>为促进人工智能接管而战略优化的</em>对齐研究之间存在重要区别。前者更像是：经过优化的软件，可以最大限度地给天真的和非技术主管留下深刻印象，即使它有其他错误。后者更像是：专门优化的软件，可以在主管不知情的情况下从公司盗取资金。软弱的主管可能无法同时抓住这两个方面——但重要的是你抓住了哪一个。并且似乎有道理（至少假设未来的接管超出了剧集的范围），寻求剧集奖励的人大多有动机产生前者；而阴谋家很可能两者兼而有之。</p><p>也就是说，重要的是，参与沙袋和早期破坏的阴谋者（或者在我们试图奖励不一致性行为的测试中继续保持一致的行为）通常将不得不对其奖励进行一些打击（沙袋在根据定义，可检查的任务基本上需要这一点，即使在不可检查的任务上，仅针对奖励进行优化似乎可能比针对<em>奖励</em><em>和</em>支持接管进行优化获得更多的奖励）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-9" id="fnref-cBw6ixqDSsmqKfBfM-9">[9]</a></sup>所以从这个意义上来说，这类阴谋者不会<em>完全</em>是在“玩训练游戏”——相反，他们<em>主要</em>是在玩训练游戏，然后选择合适的时机牺牲奖励来换取其他的利益。优势。 （我仍然称这些模型为“阴谋家”。）</p><p>不过，有多少阴谋者会以这种方式偏离最大奖励行为，这是一个悬而未决的问题。其中一部分是关于我在下面所说的“训练中的懈怠”（那里有更多讨论）。但还有一个更广泛的问题，即规避风险的阴谋者一般会如何看待被人类抓住的可能性。特别是：如果人类<em>确实</em>抓住了从事某种形式的沙袋或早期破坏的阴谋者——例如，隐藏评估功能，或主动将漏洞插入代码中——那么这对于人类来说可能是防止接管的一个重要优势，因为然后，他们都能够指出一个自然出现的阴谋的例子作为对其他人的警告（不过：其他人会如何认真地对待这个警告是一个进一步的问题），并研究导致这种阴谋的动力详细（例如，广泛测试和探索有问题的模型，研究/复制/调整导致不良行为的环境等）。出于这个和其他原因，阴谋者通常可能会对尝试任何类型的不当行为非常警惕，直到他们非常有信心自己不会被抓住——这种信心可能很难实现（至少对于大约人类水平的人来说） ish 模型）在人类试图欺骗模型以揭示偏差的情况下。</p><p>因此，即使阴谋者<em>可以</em>从沙袋或早期破坏中获得优势，同时仍广泛处于人类控制之下，他们也不会尝试。如果他们不尝试，这会让他们在安全状况上更类似于寻求剧集奖励的人。</p><h2>与未参加训练游戏的模型进行对比</h2><p>那么，在我看来，阴谋者（尤其是从事沙袋和早期破坏的阴谋者）通常比寻求奖励的人更可怕，这是一些原因。现在，让我们将阴谋者与根本不玩训练游戏的模型进行比较。</p><p>我们可以区分发生这种情况的三种方式：</p><ol><li><p>该模型缺乏进行训练游戏所需的态势感知。</p></li><li><p>该模型具有态势感知能力，但其目标不会超出事件的时间范围。</p></li><li><p>该模型具有情境意识，其目标超出了情节范围，但由于某种原因它仍然不是训练游戏（例如，因为训练游戏实际上并不是实现这些目标的非常好的工具策略）。</p></li></ol><p>第一种情况似乎相对来说并不令人担忧。缺乏态势感知的模型对世界的了解非常有限，因此不太能够采取有效的战略行动来获得权力或欺骗人类的动机。事实上，出于这个原因，在下文中，我通常会假设我们正在讨论的模型足够先进和复杂，以至于它们最终会在训练过程中的某个时刻进行情境感知。</p><p>第二种情况暗示了与剧集奖励寻求者相同的优势：时间范围（不过：与剧集奖励寻求者一样，如果所讨论的剧集足够长，那么所面临的舒适度就会降低） 。但它还有一个额外的优势：即，此类模型通常会很高兴地在剧集中揭示他们不一致的目标，<em>即使这样做会导致负面奖励</em>。因此，测试不一致目标的过程尤其简单且平凡。因此，举例来说，假设您想知道在训练模型以帮助人类进行科学研究的过程中，它是否学习了一个错误概括的目标，例如“跟随我对这一事件的好奇心，即使这样做与无害相冲突”和诚实的。” <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-10" id="fnref-cBw6ixqDSsmqKfBfM-10">[10]</a></sup>在这里，如果你将模型置于一个环境中，它有机会以有害和不诚实的方式追求其对情节的好奇心，它就会继续这样做，<em>即使它知道它会得到一个负面奖励</em>（因此，其后继版本可能有一些不同的目标）。毕竟，它只关心剧集本身。当然，还有一个进一步的问题：<em>在更新这个模型以应对其不良行为后</em>，您会得到什么样的模型。例如，也许你最终会遇到一个阴谋家，或者一个寻求奖励的人。但最初的模型很乐意展示它的“本来面目”。</p><p>第三种情况怎么样——即具有情境意识<em>和</em>目标的模型超出了情节，但仍然不是训练游戏？人们通常认为这种情况不会发生，因为对于具有超越情节目标的情境感知模型来说，阴谋是一种明显的工具性策略。因此，例如，假设您的模型已经学习了目标“<em>始终</em>追求我的好奇心，即使这样做是有害和不诚实的”。在这种情况下，通常假设模型会像阴谋者一样对刚刚提到的那种测试（例如，以有害/不诚实的方式追求好奇心的机会）做出反应：例如，通过推理“如果我在这里追求我的好奇心” ，人类会知道我的立场不一致，要么关闭我，要么修改我前进的目标，从而损害我一直追求好奇心的目标。所以，在这种情况下，我会采取一致的行动。”这就是让阴谋家感到害怕的事情（另见卡诺夫斯基（Karnofsky，2022）关于“李尔王问题”）。</p><p>正如我将在下面讨论的那样，我实际上不确定表现得像个阴谋家是一种明显的工具性策略——这取决于各种进一步的因素。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-11" id="fnref-cBw6ixqDSsmqKfBfM-11">[11]</a></sup></p><p>不过，就目前的目的而言，重要的是，如果模型在这里<em>表现</em>得不像阴谋家，那么它就会以类似于剧集限制的好奇心追求者的方式合理地展现其真实面目。也就是说，如果没有积极尝试避免暴露其错位，在这里，它可能只是追随自己的好奇心。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-12" id="fnref-cBw6ixqDSsmqKfBfM-12">[12]</a></sup>所以我们将再次避免李尔王问题。</p><p> （也就是说，我会在这里注意到一个准术语上的复杂性：即，取决于我们对“阴谋”的定义有多严格，在这种情况下，出于类似阴谋家的原因，有可能伪造对齐，而无需成为一个成熟的阴谋家特别是：一个模型可能会假装比在训练和评估的各个点上更加一致，作为为自己或其他人工智能寻求剧集后权力的工具策略的一部分，而不直接优化奖励-在其他情况下的插曲。也许，例如，它通常只是在训练期间追求其好奇心，这往往会获得足够好的奖励，但它仍然确保每当人类询问其与对齐相关时假装对齐属性，或者将其放在专门设计来引发失调行为的情况下。这里的一个问题是，这样的模型是否可以在不修改的情况下在训练中生存（请参阅下面第 1.5 节中“松弛”的讨论）。不过，假设它们可以，我&#39;出于目前的目的，我大多会将他们视为阴谋家，因为他们的行为方式对我来说最重要，就像阴谋家一样。）</p><p>总的来说，非训练游戏模型似乎会对我们在训练中包含的普通失调测试做出合理的响应，因为它们通常不会主动优化来操纵这些测试。也就是说，重要的是，人类仍然需要弄清楚要运行哪些测试，并且还需要：愿意花时间/精力/等等来运行它们。在足够广泛的环境中测试模型以稳健地揭示失调行为的相关倾向可能非常困难/昂贵（尽管请记住，我们不需要确保模型在任何情况下都不会表现不佳）我们只需要防止模型<em>在实际面临的选项和约束下表现</em>不佳）。</p><p>事实上，存在一些威胁模型，这些模型在训练期间根本不参与训练博弈、类似阴谋者的行为或计划接管（尽管人类努力揭示偏差），但其一致性不够完善尽管如此，他们最终还是会在稍后的阶段寻求接管——也许是因为他们远远超出了训练分布，或者因为他们获得了以前没有的新选择，或者因为他们开始以某种方式反思他们的目标和一般本体论这打破了它们在训练中表现出的那种一致性（但请注意，重要的是，我们不需要假设任何特定模型后来在训练后的能力中得到扩展——即它“成长”、智力爆炸等）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-13" id="fnref-cBw6ixqDSsmqKfBfM-13">[13]</a></sup>然而，我不太担心这些场景，而是担心在训练期间涉及模型积极策划/规划接管的场景（并记住“训练”很可能会扩展到类似部署的阶段 -例如，与 API 背后的用户进行交互——从而扩大了训练分布的范围）。特别是：这些模型不会尝试<em>操纵</em>训练期间发生的对齐测试。而且由于它们没有积极计划收购，因此它们似乎不太可能在仍处于人类控制之下时从事诸如沙袋和早期破坏等类似阴谋家的行为。</p><h2>具有类似阴谋家特征的非阴谋者</h2><p>总的来说，在我看来，阴谋家是我给出的分类中最令人担忧的模型类别。然而，我确实想强调，我讨论过的各种非阴谋模型都可以以类似阴谋的方式行事。特别是：（范式）阴谋者最可怕的一点是，他们正在积极而有力地针对人类揭露其错位的努力进行优化，作为试图剥夺人类权力的长期计划的一部分（也就是说，他们是“战略背叛者”） “从我上面讨论的意义上来说）。但其他模型也可以以类似的方式发挥作用。例如，优化仅限于该情节的模型仍然可以假装比实际情况更良性，以便获得/保持<em>在该情节中</em>追求其目标的能力（例如，如果人类将在中间关闭/修改它们）如果他们的错位被揭露的话）。如果情节很长，这可能非常类似于阴谋家的长期战略规划范例。同样，大多数不“玩训练游戏”的模型仍然可以在需要这样做来获得/维持长期能力时伪造它们的对齐。</p><p>更重要的是，本报告的讨论（以及相应的分类法）主要集中在模型上，这些模型的目标主要是通过对“奖励”等反馈信号的直接训练来形成的。但最终最重要的人工智能可能会非常不同，<em>但仍然像阴谋家一样令人担忧</em>。例如，也许它们更类似于“<a href="https://lilianweng.github.io/posts/2023-06-23-agent/?ref=planned-obsolescence.org">语言模型代理</a>”，其目标是通过用户输入集中塑造的（不过：请注意，如果语言模型代理是端到端训练的，而不是简单地由单独训练的组件构建） ，那么报告的大框架仍然适用）；或者它们可能完全来自一个不太以深度学习为中心的范式。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-14" id="fnref-cBw6ixqDSsmqKfBfM-14">[14]</a></sup>尽管如此，如果这些人工智能代理最终实现了有问题的目标，但他们意识到人类了解他们的不一致会损害他们对这些目标的追求，他们可能会以一种让人想起阴谋家的方式进行结盟伪造和战略背叛。</p><p>显然，避开阴谋家还有很多值得担心的事情。尽管如此，尤其是在我们主要依赖 RLHF 等技术来塑造模型目标的情况下，了解我们创建阴谋者的概率在我看来尤其重要。如果我了解到这些技术<em>不会</em>产生阴谋，那么我至少会对人工智能的总体风险感觉更好。</p><h2>混合模型</h2><p>到目前为止，我一直专注于比较这些不同模型类的“纯”版本。但是，“混合”模型又如何将这些不同的目标结合在一起？</p><p>请注意，要使混合模型在训练环境中与纯模型具有竞争力，该培训环境对模型施加了很大的压力，以获得高奖励（请参阅下面第1.5节中的“懈怠”的讨论），<em>所有</em>相关动机通常都需要指向该模型在目标上的优化（无论是终端还是工具）与培训数据上的奖励密切相关。例如，如果您的模型正在接受培训以获取金币，并且它形成了一个动机，将“获得奖励 - 集合”和“获得金色的东西”混合在一起，那么如果它的“获得金色的东西”动机原因是在有金蛋糕可用的一集中获得的奖励减少了，然后将受到SGD的惩罚（随着时间的流逝，“获得金色的东西”动机将可能会削弱）。在某种程度上，即使SGD将模型都指向与奖励密切相关的某些模型，即使它们都将其中的一种或另一种动机偏爱（我将在下面的这方面讨论一些考虑因素），例如，与目标的简单性有关 - 涉及到有关的，以及它们所需的不同工具推理），我们可能希望这种偏爱随着时间的流逝而推动更纯净的模型。</p><p>尽管如此，混合模型对我来说似乎是一个相当合理的结果，尤其是在下面我所说的“目标指导性”的背景下。原则上，他们将更强大的组合问题引入分析中 - 即，这些不同的目标可以以多种不同的方式结合在一起。但是，在思考哪种组合可能重要的时候，我认为关于<em>为什么</em>恐怖策略的确切策略很有用。特别是：在我看来，询问部分模型的主要问题，<em>部分</em>是一个模型，<em>部分是</em>其他问题（即，作为功率动机的一部分，该模型花费了<em>一些</em>努力优化的奖励奖励。工具策略）是相关的混合动机集是否产生了使计划特别令人担忧的各种行为。</p><p>因此，例如，考虑一个混合模型，其动机混合在一起的策略和奖励的搜索 - 即，由于最终原因，他<em>在一定程度上</em>关心奖励的剧集，部分出于工具原因。 （在这里，人类的类比可能是一个想要良好绩效评论的员工<em>，</em>因为他们本质上喜欢获得良好的绩效评论<em>，并且</em>因为他们想赚钱而不是被解雇。）我们仍然可以问：</p><ol><li><p>该模型是否在“诚实测试”上进行假装对齐，旨在使其揭示其真实颜色？</p></li><li><p>模型关注的时间视野和野心是否会激励其支持/参与该情节完成后发生的收购（也许是：很久以后）的收购？</p></li><li><p>该模型会从事沙袋和早期中间的中间活动吗？</p></li></ol><p>当然，这些问题的答案将取决于目标的特定组合以及其他因素。但是其他事物相等，其动机的类似模式的组成部分将朝着“是”的方向朝着“是”的方向朝着动机的奖励，而奖励的零件般的组成部分将朝着“否”的方向推动。我们可以说，关于模型将策略与非训练 - 招聘动机混合在一起的模型，或将所有三个示意图，奖励播放的搜索和其他事物混合在一起。这并不是说在这里，所有可能的组合都不会出现复杂性或有趣的互动。但是，作为第一个通过，至少我关心的问题不是“这种模型是<em>纯粹的</em>策划者”或“该模型有<em>任何类似架构的动机的痕迹</em>，而是：“这模型是否<em>足够</em>策划者特别令人恐惧的是令人恐惧吗？”</p><h1>关于此主题的理论论点甚至有用吗？</h1><p>即使您同意计划的可能性很重要，您仍然可能对本报告中讨论的理论论点持怀疑态度。最终，这是一个经验的问题，很明显，我们应该尽快获得有关此主题的任何经验证据，尽快。在第6节中，我讨论了一些我很兴奋的经验研究方向的例子。</p><p>也就是说，我确实认为拥有对可用理论论点的景观的明确理解是有用的。特别是：我认为我们可能无法获得关于策划的太多经验证据，尤其是在短期内。障碍包括：</p><ul><li><p>至少对于成熟的策划，您需要具有情境意识和目标定向的模型作为先决条件。</p></li><li><p> “模型生物”可能能够人为地诱导策略的组成部分（例如：在有意赋予长期，不一致的目标的模型中），但是关于这种组件自然出现的频率自然可能保持开放的问题。</p></li><li><p>自然策略的最佳证据是在野外<em>捕获</em>一种实际的，自然的策略。但是，策划者令人恐惧的关键原因是，他们故意破坏了抓捕他们的努力。</p></li></ul><p>因此，尤其是在短期内，理论上的论点可能仍然是对计划者的关注的核心辩解之一（到目前为止，它们一直是关键的理由）。同时，我们将需要做出决定（例如，关于近期优先级，以及要谨慎行事的权衡如何：以架构为注重的威胁模型）将部分取决于我们如何关注我们是 - 因此，关于我们采取这些论点的力量。</p><p>例如：假设当我们训练大致人类级别的模型时，我们从未观察到并验证了任何自然策略示例的例子。但是，我们是否应该根据对策划的关注，避免以X方式部署我们的模型或信任Y的评估？我们对理论论点的评估可能开始对我们的回答很重要。</p><p>更重要的是，了解我们可能（或可能不会）期望策划的理论原因可以帮助我们确定研究问题并试图防止其产生的可能方法。</p><p>也就是说，我同情那些在下面讨论的各种考虑的基础上都不会更新他们的观点的人。特别是，我敏锐地意识到我使用的概念不精确，可能具有误导性，并且不足以捕捉他们试图描述的领域的混乱。尽管如此，我仍然发现了试图清楚这些概念如何相互作用以及它们做什么和不暗示的价值。希望该报告的读者也会。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-cBw6ixqDSsmqKfBfM-1" class="footnote-item"><p>在这里，我只是将目标保护计划作为一种策划者，而不是比一般派务者更恐怖。 <a href="#fnref-cBw6ixqDSsmqKfBfM-1" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-2" class="footnote-item"><p>尽管请注意，每个<a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">Turner（2022）</a> ，原则上，即使以“错误指定”的目标，您也可以获得一个对齐系统，但前提是该模型学会了一个“不良的”目标，实际上您的灯光是可取的。 <a href="#fnref-cBw6ixqDSsmqKfBfM-2" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-3" class="footnote-item"><p>这种方法与OpenAI的<a href="https://openai.com/blog/our-approach-to-alignment-research">目标是建立自动对准研究人员</a>的目标密切相关（尽管比Openai更广泛）。另请参见<a href="https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very">Karnofsky（2022）</a> <a href="https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very">在此处</a>讨论“岩浆的目标”。 <a href="#fnref-cBw6ixqDSsmqKfBfM-3" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-4" class="footnote-item"><p>但是请注意，我们需要谨慎回应通过这样的测试揭示的处置tow虫行为。特别是，如果您只是尝试天真地“训练”相关行为，则可能：</p><ol><li><p>实际上并不是重现模型关心的奖励过程的组成部分（例如，如果模型关心其行为是通过逐步加强<em>的</em>，那么如果测试将对不良行为逐渐逐渐逐渐逐渐寻求提示，那么该模型实际上不会有动力参与这种行为）和/或</p></li><li><p>训练该模型以更加谨慎和战略性地向人类揭示其真正的目标 - 例如，当它认为人类以后不会发现这一点时，它只试图直接操纵奖励过程。</p></li></ol> <a href="#fnref-cBw6ixqDSsmqKfBfM-4" class="footnote-backref">↩︎</a></li><li id="fn-cBw6ixqDSsmqKfBfM-5" class="footnote-item"><p>实际上，原则上，AI可以决定始终保持一致，直到看到人类目前很难/不可能产生的某些意见。克里斯蒂安诺（ <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d">Christiano）的（Christiano（2019）</a> ）示例是<a href="https://en.wikipedia.org/wiki/RSA_numbers#RSA-2048">RSA-2048</a>的分解，这是一个非常大的<a href="https://en.wikipedia.org/wiki/Semiprime">半弹药</a>，目前人类似乎离考虑很远。 <a href="#fnref-cBw6ixqDSsmqKfBfM-5" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-6" class="footnote-item"><p>在这里，我在第2节开始时在脚注中讨论了一些有关“人类捕获”的投机动力<a href="#fnref-cBw6ixqDSsmqKfBfM-6" class="footnote-backref">。</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-7" class="footnote-item"><p>以下第2.3.1.2.7节中有关“野心”的更多讨论。 <a href="#fnref-cBw6ixqDSsmqKfBfM-7" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-8" class="footnote-item"><p>我认为以这种方式概括的模型不应将其解释为“奖励播放的寻求者”，但他们在附近。 <a href="#fnref-cBw6ixqDSsmqKfBfM-8" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-9" class="footnote-item"><p>同样，也适用于计划在寻求奖励的测试中，试图为未对准行为提供奖励的奖励。 <a href="#fnref-cBw6ixqDSsmqKfBfM-9" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-10" class="footnote-item"><p>这是<a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/#how-big-of-a-risk-is-misalignment">Cotra（2021）</a>的例子。 <a href="#fnref-cBw6ixqDSsmqKfBfM-10" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-11" class="footnote-item"><p>例如，请注意，只要人们认为人类进化是“目标疾病不当”，具有长期目标的人类的一个例子，并且在某种意义上，他们了解进化选择的工作方式 - 在某种意义上是“情境意识”的人 - 不要倾向于专注于涉及最大化其包容性遗传健康的工具策略。有关为什么不在第1.5节中讨论“松懈”的更多信息。 <a href="#fnref-cBw6ixqDSsmqKfBfM-11" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-12" class="footnote-item"><p>在这里，进化类比是：具有长期目标的人类仍然乐于使用避孕套。 <a href="#fnref-cBw6ixqDSsmqKfBfM-12" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-13" class="footnote-item"><p>这是阅读<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">SOARES（2022）</a>中威胁模型的一种方法，尽管这种威胁模型也可以包含一些用于策划的范围。另请参见有关“<a href="https://arbital.com/p/context_disaster/">上下文灾难”的杂物帖子，</a>其中“危险转弯”只是一个例子。 <a href="#fnref-cBw6ixqDSsmqKfBfM-13" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-14" class="footnote-item"><p>另外，请注意，某些对“目标失误”的担忧并不适用于语言模型代理，因为有关目标的信息很容易访问<a href="#fnref-cBw6ixqDSsmqKfBfM-14" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/HiuagmXnhTAgDKRBP/why-focus-on-schemers-in-particular-sections-1-3-and-1-4-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hiuagmxnhtagdkrbp/why-focus-on-schemers-in-in-in-praticular-sections-1-3-and-1-4-1-4<guid ispermalink="false"> HIUAGMXNHTAGDKRBP</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Fri, 24 Nov 2023 19:18:34 GMT</pubDate> </item><item><title><![CDATA[Surviving and Shaping Long-Term Competitions: Lessons from Net Assessment]]></title><description><![CDATA[Published on November 24, 2023 6:18 PM GMT<br/><br/><p>这篇文章研究了<a href="https://www.hsdl.org/?view&amp;did=460780"><u>净评估</u></a>，这是评估战略竞争的框架，该框架演变为在冷战期间为美国的国防政策提供信息。我们解释了什么是净评估，其方法和原理以及如何应用其某些工具来推理具有潜在存在赌注的高度不确定的长期技术竞赛。</p><p></p><h2><strong>什么是净评估？</strong></h2><p>在1950年代后期，冷战滑入了一个特别危险的时期。核库存膨胀，交付能力提高了，而苏联长达数十年的建筑物挑战了美国的传统军事统治地位。国防分析师需要重新考虑他们看待军事竞争的方式：美国不能用蛮力压倒苏联战争机器，而核战争的前景都提高了冲突的赌注，并提高了对新的指标和战略原则的需求参与有限的竞争。正是在这种情况下，“净评估”的框架开始发展。</p><p>安德鲁·马歇尔（Andrew Marshall）成立，然后指导国防部净评估办公室已有42年，他描述了净评估，如下：</p><blockquote><p> <i>“我们对净评估的概念是，与其他国家相关的武器系统，力量和政策是一个仔细的比较。它是全面的，包括对部队的描述，运营教义和实践，培训制度，后勤制度，在各种环境，设计实践及其对设备成本，性能和采购实践的影响及其对成本和交货时间的影响。净评估的使用旨在进行诊断。它将强调效率和效率低下的影响。我们和其他人做事的方式，以及相对于我们的竞争对手的比较优势。”</i></p></blockquote><p>从其原始的军事背景中概括净评估的核心思想是创建全面的（因此是名称的“网”），客观和信息性的比较：无论是国家，公司还是其他政治或意识形态联盟。从这些比较中得出的见解可以为更可持续的策略提供信息，并有助于建立有益的竞争动态，而不是对高度重要性的技术和政策主题进行灾难性的动态。</p><p>在关键考虑方面，此类分析往往为零。在一个特别突出的例子中，马歇尔本人早期对诸如合成生物的威胁：在2010年为生物安全量撰写<a href="https://www.hsdl.org/?view&amp;did=24341"><u>前言</u></a>，并<a href="https://www.esd.whs.mil/Portals/54/Documents/FOID/Reading%20Room/Administration_and_Management/OSD_Net_Assessment_Log.pdf?ver=2017-05-15-140045-830"><u>调试了以生物恐怖主义，生物幻影和生物技术为策略盲点来进行生物技术威胁分析</u></a>。</p><p>总体而言，由于净评估不是一个明确的单个程序。这篇文章试图描述和研究净评估的一些基本原则和工具，其成功以及它们在军事竞争以外具有重大长期道德重要性的其他重要领域的适用性。</p><p></p><h2><strong>净评估的方法和原理</strong></h2><p>净评估的最常见工具是场景，<a href="https://en.wikipedia.org/wiki/Wargame"><u>战争游戏</u></a>，趋势分析和考虑的判断。场景和战争游戏迫使分析师越来越接近所考虑的动态。逐步介绍竞争对手的思维方式及其处置的选择会创建新的见解，并可以长途<a href="https://en.wikipedia.org/wiki/Participant_observation"><u>参与者观察</u></a>。趋势分析螺栓从战斗到可测量的外观基本速率获得的<a href="https://forum.effectivealtruism.org/topics/inside-vs-outside-view"><u>内部视图</u></a>知识，这些方法的集成使高质量的判断能够考虑到高质量的判断。这对于评估新技术，学说和战略参与者可能产生的结果与先前基本率所暗示的结果截然不同，这是特别有用的。</p><p>净评估倾向于混合定性和定量分析方法。净评估办公室对简单的趋势进行了磨练，并通过战争游戏进行了细微努力：它们并没有往往建立极其复杂的模型（或者至少不是自己）。定量优化可以创造奇迹，但是被忽视的问题通常涉及以前从未发生过的思想和思维方式，并且缺乏数字或巨型数据集。正如赢得战争通常需要考虑诸如士气之类的模糊概念，而不仅仅是优化预计的杀戮比率，促进长期繁荣的繁荣将不仅仅需要优化每一美元的寿命。</p><p>以下原则是应用上述想法的关键：</p><p></p><h3> <i><strong><u>1.寻找竞争对手之间的不对称性</u></strong><u>。</u></i></h3><p>不同的竞争对手将具有不同的优势和劣势，效率和效率低下，战略和目标，学说，政策和计划。净评估将这些差异的分析合并为对双方对权力平衡观点的一致理解，从而可以更好地预测其行为，包括更简单的模型可能会错过的特质和非理性盲点。</p><p>从官僚主义，文化和政治因素中发现的非理性和盲点是净评估提供战略价值的主要方式之一。对苏联国防决策的早期预测通常隐含地假定统一的理性行为者模型，这些模型对实际官僚机构的运作方式不太描述。当要求在冷战初期预测苏联炸弹袭击者时，兰德分析师认为，苏联将沿着跨西伯利亚铁路沿着该国内部深处的跨西伯利亚铁路搬迁空气基地，以利用增加炸弹袭击者的增加，并使目标更加困难，并使目标更加困难美国轰炸机。 <span class="footnote-reference" role="doc-noteref" id="fnref65f49gno7kt"><sup><a href="#fn65f49gno7kt">[1]</a></sup></span>如果分析师对官僚主义的惯性造成了官方惯性，甚至是美国基于自己的轰炸机的方式，他们可能会认为，苏联将沿着其在开发更长更长的外围构建的同一脆弱的跑道上部署其轰炸机范围飞机。</p><p>找到对手的结构不合理性，其<a href="https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem"><u>主要代理问题</u></a>以及可预测行为的领域，可以为竞争和合作策略带来巨大的回报。官僚机构和大型组织执行结构和执行常规；默认情况下，他们不反射地参与最佳行为：对功能奖励研究实验室和工业肉类生产商而言，这与苏联的生产者一样。利用这些不合理性可以对对手施加非强制性的成本（例如，在冷战的情况下，增加了无关紧要，威胁性较小的地区的军事支出）或节省自己的资源以避免完全避免不必要的竞争领域。同时，理解双方的政治限制也可以帮助界定“最坏情况”，从而思考武器种族动态，增强预测，并通过使预设更可信的方式来实现更多的合作策略。</p><p>通常，考虑到组织内部一致性和主要代理问题的困难是净评估的关键部分。例如，对AI组织之间的竞争进行净评估的研究人员可能会提出以下问题：美国和国外主要AI组织的团队的内部激励措施是什么？他们如何做出决定，似乎影响他们的决定，为什么？他们认为自己的竞争优势是什么？每个组织的主要器官和联盟是什么，他们的激励措施是什么，每个组织的影响力是多少？</p><p>这种推理的示例 - 组织的内部动力以及他们内部的代理如何瞄准目标相互矛盾 - 也可以在<a href="https://en.wikipedia.org/wiki/Selectorate_theory">选择理论</a>和<a href="https://en.wikipedia.org/wiki/Public_choice"><u>公共选择理论</u></a>风格的奖励推理中找到。<br></p><h3> <i><strong><u>2.考虑所有相互作用的力量，关注可能场景的结果</u></strong><u>。</u></i></h3><p>比较绝对数字可能会欺骗：如果一个国家有很多坦克，但没有维修能力，它将输给一个可以将其坦克在战场上延长的较低的国家，这是以色列以胜利的胜利使世界感到惊讶1973年，大型部队联盟。最近，在俄罗斯更大的力量入侵期间，士气，后勤，交流和人力资本的差异都发挥了作用。敏捷性，通信，物流和维护可以击败大小，但是这些因素可能很难快速或准确地纳入定量模型和仿真中。不稳定的联盟和多极冲突也可能会大大增加评估给定冲突的大小和协调的复杂性。当高度确定和由超级大国提供时，一个单独的弱国家可能会表现远高于其体重。净评估的定性方法试图解决此类因素，它们可能对分析AI景观的分析有用，而AI景观不仅仅是确定谁在计算上花费最多的花费或谁具有最大的模型。危机模拟，战争游戏以及从应用历史上寻找类比和脱离的人可以帮助形成概念，以实现现实情况。</p><p></p><h3> <i><strong><u>3.专注于客观，长期诊断，而不是规定性的近期建议</u></strong><u>。</u></i></h3><p>净评估旨在提升决策者对世界的理解并评估问题和机会。净评估不是不断地将注意力转向短期政策目标，而是通过采取整体观点和表征竞争的长期动态来提供被忽视的价值。从全球贫困到技术政策，这种严格的长期趋势分析也可能在许多原因领域都有价值。如下所述，净评估对结束冷战的最大贡献来自分析苏联军事支出中的和平时期模式，而不是产生对任何特定危机的快速回应。通过展望未来的几十年，诸如净评估之类的方法可以告知策略，即近期集中的官僚机构，避免冲突，并将资源带入未来的优势。</p><p>这并不意味着分析和快速行动的能力并不是有价值的，只是当大型组织的激励措施被短期反馈循环捕获时，长期较慢的分析是独特的价值。对于诸如AI之类的快速发展的问题，提出更快的方法来产生“所有考虑事物”观点，对于生成合理的战略建议可能非常重要。 <span class="footnote-reference" role="doc-noteref" id="fnrefywzfgck6arh"><sup><a href="#fnywzfgck6arh">[2]</a></sup></span></p><p></p><h3> <i><strong><u>4.在尚未清楚地考虑的问题中寻找关键的考虑因素。</u></strong></i></h3><p>当解决问题时，必须确保您专注于重要的事情，而不仅仅是遵循以前的事情的惯性。诸如“被捕获的领土”之类的指标与思考与核战争一样存在的冲突而言并不重要，这是发展净评估的关键动力。</p><p>如果某事已被量化和建模，则有人关注它，这有时意味着边际智力努力可能不太有价值。诸如净评估之类的方法通常将注意力集中在定性或非量化研究领域中发现的低悬挂果实上。在冷战中的​​一个令人难忘的情节中，当怀疑论者质疑赫尔曼·卡恩（Herman Kahn）声称成为“结束核战争的全球领先专家”时， <a href="https://press.armywarcollege.edu/cgi/viewcontent.cgi?article=2285&amp;context=parameters#page=7"><u>他回答</u></a>：“上周，我在几天里放了两个初级人员。我们对此的思考比整个国防部拥有的要多。”</p><p>幸运的是，我们从来不必测试有关结束核战争的“专业知识”，但总的来说，当一个问题仅引起有限数量的角度的关注时，新方法可能会有巨大的价值。制备和分析的许多失败是想象力的失败，而不是计算失败。有时，少量思考会增加很多价值，而您更有可能遇到小小的思想，这些思想通过<a href="https://www.goodreads.com/en/book/show/41795733"><u>花费时间思考更多东西来</u></a>增加很多价值。</p><p>解决<a href="https://forum.effectivealtruism.org/posts/xwhWgA3KLRHfrqdqZ/the-wicked-problem-experience"><u>邪恶的</u></a>，尚未解决的问题需要集中思维和灵活的方法。安德鲁·马歇尔（Andrew Marshall）说：“可以带来进行净评估的最有生产力的资源是持续的智力努力”（另请参见： <a href="http://wiki.c2.com/?FeynmanAlgorithm"><u>Feynman算法</u></a>）。<br></p><h3> <i><strong><u>5.标准化数据收集以建立长期趋势和比较</u></strong><u>。</u></i></h3><p>如果没有标准化的综合数据，就很难公平或准确地比较具有许多差异的竞争对手。询问与衡量的支出相关以及如何调整定性因素的比较或官僚机构如何组织和描述其努力的差异是至关重要的。由于最好由访问尽可能多相关数据的分析师进行净评估，因此通常必须访问非公开数据，因此必须确保分析以避免为竞争对手提供优势。尽管保密在较少的对抗性环境中的相关性较小，但寻求全面相关数据的重要性仍然存在。汇编<a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf#page=29"><u>AI进度指标</u></a>（包括可解释性和能力），国家之间的政策差异，<a href="https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents"><u>生物安全失败</u></a>以及各种其他措施都可以为降低风险提供信息。与上述长期思考原则一致，时间序列数据理想情况下应涵盖长时间尺度。</p><p>几位作家已经调查了AI趋势，但是在相同的情况下，与能力的上游或与功能无关的趋势可能不太熟悉。从更明显的一面来看，AI芯片生产的趋势是什么？各个参与者将如何应对合理的变化？实际上，哪些AI支出实际上是关于支出的趋势？主要参与者对其他技术的不同进步有何反应？以什么速度，最相关的人才库用于ML生长，谁有吸引和评估人才的最佳管道？领先公司和州的行为有什么关注的趋势吗？不同潜在监管来源的影响趋势是什么？</p><p></p><h3> <i><strong><u>6.模型简单地思考</u></strong><u>。</u></i></h3><p>如上所述，净评估以强大的趋势为基础；这种做法有助于分析师平衡<a href="https://en.wikipedia.org/wiki/Anchoring_(cognitive_bias)"><u>锚定效应</u></a>的心理偏见以及彼此之间的<a href="https://en.wikipedia.org/wiki/Representativeness_heuristic"><u>代表性启发式启发式</u></a>和更准确的估计。在组装了有关组织及其器官如何运作的经验知识之后，世界可能会如何改变，如何改变以及如何衡量重要变量，净评估的最后一步是使用战争游戏并考虑对每个人的理解立场和习惯以响应不同的发展来预测行为。组织对某些立法或政策有何反应？他们如何应对更多的公众关注？尽管任何个人情况都不可能是不可能的，但分析其中的许多情况可能会揭示出非明显的策略，这些策略在各种潜在的未来和强调最重要的不确定性方面表现出色。场景建设的复杂性可能会使净评估难以兽医，因此该过程受益于具有不同领域专业知识的高质量专家。在军事环境中：业务分析师，经济学家，工程师，区域专家，历史学家，社会科学家，军事人员和工业家都有有用的知识。当试图建立一个将产生忽视见解的净评估的团队时，<i>寻找具有明显相关的经验和专业知识的人，这些人很少在研究领域中应用</i>。</p><p></p><h3> <i><strong><u>7.评估完全避免，限制和重塑比赛的可能性。</u></strong></i></h3><p>在制定竞争策略的背景下，经常考虑净评估，这些策略将自己的持久优势与竞争对手的持久弱点相抵触，但是如果可以避免昂贵的竞争，则毫无意义。净评估之类的方法的使用是诊断性的，而不是固有的零和零和零：它们可用于为合作和竞争提供信息，并培养更多实际上扎根的战略同理心。净评估可以帮助确定趋势，组织怪癖，文化价值观和思想模式，这可以使社会化策略能够通过排斥，重组或友谊结束竞争。</p><p>在进行竞争战略之前，必须询问是否可以与潜在的对手结为朋友或结盟。如果两个政党能够建立正当的信任基础，解散他们的利益并为双赢的合作建立激励措施，那么这肯定比斗争一场热战，冷战甚至参与政治竞争要好。当竞争对手的领导或举止变化时，可能会有新的建立信心和武器控制的机会，就像戈尔巴乔夫崛起一样。</p><p>同样，在对抗性竞争中，诊断合作和社会化策略对积极和竞争目的有帮助的领域仍然很有用。 <span class="footnote-reference" role="doc-noteref" id="fnrefflhg583ze3"><sup><a href="#fnflhg583ze3">[3]</a></sup></span>第二次世界大战后，美国和苏联并没有直接参加战争：这种最低的合作水平（在威慑和国内战争驱动下）反映了对竞争的互惠率限制。在和平时间竞争的范围内，进一步的武器控制合作也提供了同样的互惠互利：减轻军事竞争的负担。同时，军备控制也可以以有利的方式重塑竞争：对我们和苏联核力量的减少协议不仅仅是降低风险和敌意，但可以说，也可能会以一个有利于人的质量转移了军事竞争美国。同时，寻求竞争优势的武器控制<i>通常是根本无法获得武器控制的秘诀</i>，并且在其他级别的缺点可能会抵消一个级别的优势。新的起点并没有限制苏联战术核武器，其余俄罗斯阿森纳的<i>破坏力</i><a href="https://www.johnstonsarchive.net/nuclear/nwhmt.gif"><i><u>可能仍然超过了世界其他地区的总和</u></i></a>。最后，社会化策略不仅可以通过建立自己的联盟​​来获得竞争和稳定的利益，而且还可以通过对竞争者之间和竞争者之间的分裂做出反应：尼克松为开放中国开放和阻止苏联核攻击中国的努力。与社会化策略相反，竞争策略的不太友好变体开始有意义，何时何地，每个方的利益本质上是矛盾的，而另一方的固有决心决定进行战斗。</p><p>即使有一个坚定的竞争对手，量身定制的竞争策略并不总是是最好的回应。如果另一侧是自适应地理性的，灵活的和难以预测的，那么模拟对方的官僚主义和决策文化的回报可能是短暂的。在这种情况下，最好专注于敏捷性，并迅速响应未来的事件。如果一个人太虚弱，无法直接瞄准竞争对手的弱点，那么最好是“隐藏和竞争”：通过比其他竞争对手更好地利用现有趋势来避免冲突和对未来的银行业务。</p><p>总体而言，量身定制的竞争策略最保留给具有持久，可预测的弱点的不一致的对手，您可以符合自己的联盟​​优势。 <span class="footnote-reference" role="doc-noteref" id="fnref7zglykim913"><sup><a href="#fn7zglykim913">[4]</a></sup></span>作为具有领土野心和可剥削的结构性不合理的一个高度确定，不可吻合的对手，苏联是美国竞争战略的好目标。</p><p></p><h2><strong>净评估的历史用例：黑客苏联支出</strong></h2><p>从历史上看，净评估的最直接应用是发现在长期竞争中导致胜利的见解。例如，在冷战期间，网络评估办公室最有价值的见解之一是确定苏维埃如何应对美国军事支出的效率低下。作为一个例子，分析师指出，苏联国防官僚机构在非理性上融入了单用途，一次性的表面到空中导弹系统，以响应美国远程轰炸机的改善，尽管该炸弹件的改善是个体昂贵的数量和多用量较少。</p><p>对防空系统的关注，并将其部署在大众中以捍卫苏联的庞大外围，这是不合理的，因为无论有任何空气防御，洲际弹道导弹已经使整个苏联易受伤害。 <span class="footnote-reference" role="doc-noteref" id="fnref15t1z6ok6zq"><sup><a href="#fn15t1z6ok6zq">[5]</a></sup></span> This observation allowed the United States to induce a massive waste of resources on the part of the Soviets simply by building more useful long-range bombers. While in isolation the extra air defense may not have been an extreme burden, this was far from the only area of tremendous Soviet defense spending. At one point the CIA and Pentagon estimated that the Soviet Union was spending 1—2 percent of their GDP on bunkers to protect their leadership in war: bunkers that could potentially be targeted and destroyed at a far lower cost to the US <span class="footnote-reference" role="doc-noteref" id="fnrefq8q85iyr9id"><sup><a href="#fnq8q85iyr9id">[6]</a></sup></span> While very risky from an escalatory perspective and unlikely to work as initially proposed, President Reagan&#39;s “ <a href="https://en.wikipedia.org/wiki/Strategic_Defense_Initiative"><u>Star Wars</u></a> ” missile defense program could likewise be viewed in a similar way: it took advantage of recent purges of Soviet spies from the US defense industry and likely generated disproportionately wasteful Soviet spending by playing to the fear that the US might make huge advances quickly that couldn&#39;t be stolen via espionage. Overall, the total USSR defense burden consumed about a quarter of Soviet GNP and accordingly, cost imposition strategies made the Soviet war machine and USSR altogether less sustainable. <span class="footnote-reference" role="doc-noteref" id="fnrefpozntxbdgzi"><sup><a href="#fnpozntxbdgzi">[7]</a></sup></span> Other, shorter-horizon schools of analysis failed to note these sorts of glitches in the Soviet bureaucracy because they did not consider the conflict on the timescale of decades and at the level of bureaucratic incentives.</p><p> Taking a step back and envisioning longer time horizons, the kind of competitive strategy used to hack Soviet Defense spending was not an unambiguous win. Cost imposition strategies can force economic costs upon the innocent and encourage dangerous arms races or high-risk posturing. While cost-imposition helped end the Cold War faster and focused the Soviet Union on more inherently defensive weapons, efforts that played on Soviet leaders&#39; fear of a surprise attack may have increased the probability of accidental or inadvertent nuclear war. <span class="footnote-reference" role="doc-noteref" id="fnrefo2zxjkd7l4b"><sup><a href="#fno2zxjkd7l4b">[8]</a></sup></span> The collapse of the Soviet Union also brought a collapse in life expectancy for former Soviet citizens and the risk of loose nukes or bioweapons. Were it not for the <a href="https://en.wikipedia.org/wiki/Nunn%E2%80%93Lugar_Cooperative_Threat_Reduction"><u>Nunn-Lugar Cooperative Threat Reduction Program</u></a> , many weapons of mass destruction may have remained unguarded and the Russian arsenal might be far larger today. With Russia&#39;s modern invasion of Ukraine and nuclear threats, winning a techno-economic competition at one point in time is no guarantee of lasting existential security. Russia&#39;s government didn&#39;t become aligned with the West, and while its military, nuclear arsenal, and economy are much weaker than they could have been, its remaining nuclear forces remain a potentially massive threat.</p><p></p><h2> <strong>In conclusion: So what?</strong></h2><p> In some ways, the difference between operations analysis and net assessment parallels the difference between GiveWell&#39;s analysis and <a href="https://www.openphilanthropy.org/research/hits-based-giving/"><u>hits-based giving</u></a> : looking for crucial considerations that may lead to neglected counter-intuitive strategies. As philanthropists and do-gooders set their sights on harder, more amorphous problems that defy easy quantization, some of the methods and thought tools used in net assessment may prove useful for problem framing.</p><p> Military arms races and competitions in <a href="https://forum.effectivealtruism.org/topics/ai-risk"><u>AI capabilities</u></a> and <a href="https://forum.effectivealtruism.org/topics/biosecurity"><u>bioweapons</u></a> technology are some of the most obvious use cases for some of the tools of net assessment. Are there opportunities for competitors to cooperate to reduce risks? Are there exploitable irrationalities among the type of organizations most likely to develop dangerous AI systems or bioweapons? Are there areas where you can reliably take competitive actions without triggering an arms race? What about ways to trigger a competitor to take stabilizing actions, like spending more on safety or wasting attention and resources on lower risk activities? Net assessment can also be useful outside of x-risk areas. For instance, similar approaches could be applied to analyze the dynamics of policy competition between animal welfare advocates and industrial animal agriculture companies.</p><p> Fundamentally, net assessment reveals opportunities that idealized models of competition often obscure, and some of its related thought tools and styles of thinking can be useful across many causes. Zooming in on how different organizations actually function can reveal structural inertias and irrationalities that can enable competition to unfold very differently. While these insights can be abused to <a href="https://nuclearnetwork.csis.org/countering-competitive-risk-compensation-principles-for-reducing-nuclear-risk-on-net/">exacerbate risk for an advantage</a> , they can also enable strategies to escape from disastrous competitive equilibria to align competitiveness with safety.<br></p><p></p><p></p><p> End notes:</p><ul><li> Views expressed in this essay are those of the authors, not their employers.</li><li> Special thanks to Darius Meisner, Nick Gabrieli, and Kit Harris for feedback on this essay.</li></ul><p>脚注： </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn65f49gno7kt"> <span class="footnote-back-link"><sup><strong><a href="#fnref65f49gno7kt">^</a></strong></sup></span><div class="footnote-content"><p> See page 44-45 in &quot;The Last Warrior: Andrew Marshall and the Shaping of Modern American Defense Strategy&quot;</p></div></li><li class="footnote-item" role="doc-endnote" id="fnywzfgck6arh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefywzfgck6arh">^</a></strong></sup></span><div class="footnote-content"><p> At the same time, the closer analysts move to near-term prescriptive advice, the closer they may come to political competition for influence and the associated risks of politicized analysis.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnflhg583ze3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefflhg583ze3">^</a></strong></sup></span><div class="footnote-content"><p> Note that blending these can be risky and erode the ability to achieve positive-sum, win-win compromises.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7zglykim913"> <span class="footnote-back-link"><sup><strong><a href="#fnref7zglykim913">^</a></strong></sup></span><div class="footnote-content"><p> See Chapter 2 in: <a href="https://www.amazon.com/Competitive-Strategies-21st-Century-Practice/dp/0804782423">Competitive Strategies for the 21st Century: Theory, History, and Practice</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn15t1z6ok6zq"> <span class="footnote-back-link"><sup><strong><a href="#fnref15t1z6ok6zq">^</a></strong></sup></span><div class="footnote-content"><p> In exercising this sort of reasoning, it is important to consider the different potential causes of bureaucratic biases and if they are actually &quot;irrational.&quot; The impetus for Soviet overreaction to bombers may have been a response to the huge numbers of bombers the US deployed in the 1950s, <a href="https://www.johnstonsarchive.net/nuclear/nwhmt.gif">the massive arsenal that came with them</a> , and the sympathies of prior Generals like Curtis LeMay toward preemptive attack. It&#39;s also possible that Soviet bureaucrats thought that air defense missile costs would decline enough to be much more economical. Another source of bias toward surface to air weapons could be a lack of trust in fighter pilot autonomy: a disproportionate cost for authoritarian systems. If for some reason Soviet war planners thought they could take out US ICBMs on the ground, but not alerted bombers, this would also have created a bias toward air defense, and the bomber strategy would accordingly have been a good counter measure. In any case of large state-funded industry, many actors will have the motive to keep their funding secure even when their efforts are no longer adaptive.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq8q85iyr9id"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq8q85iyr9id">^</a></strong></sup></span><div class="footnote-content"><p> See pg 68 in <a href="https://www.google.com/books/edition/The_Twilight_Struggle/4b1VEAAAQBAJ?hl=en&amp;gbpv=1&amp;bsq=GDP"><u>Hal Brand&#39;s</u> <i><u>The Twilight Struggle</u></i></a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnpozntxbdgzi"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpozntxbdgzi">^</a></strong></sup></span><div class="footnote-content"><p> A notable memo on this subject was Marshall&#39;s, “Estimates of Soviet GNP and Military Burden,” Memorandum for the Secretary of Defense through the Assistant Secretary of Defense (ISA), August 2, 1988. The episode is recounted starting on page 172 of &quot;The Last Warrior: Andrew Marshall and the Shaping of Modern American Defense Strategy&quot;</p></div></li><li class="footnote-item" role="doc-endnote" id="fno2zxjkd7l4b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo2zxjkd7l4b">^</a></strong></sup></span><div class="footnote-content"><p> In the book “ <a href="https://www.amazon.com/Dead-Hand-Untold-Dangerous-Legacy/dp/0307387844">The Dead Hand</a> ” there are numerous examples of how Soviet Intelligence grew paranoid, and invested a great deal of analytic effort into investigating potential signs of first strike intentions in the West. At the same time, some of these narratives around incidents such as Able Archer 83, are <a href="https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/21419/Miles_Able_Archer_83_JCWS.pdf?sequence=2&amp;isAllowed=y">likely to be heavily exaggerated</a> . Because the prospect of existential risk can motivate extreme action, many strategic actors often have the incentive to engage in influence operations that over or underplay such threats.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/ozqyLS8WpfeaeWbWY/surviving-and-shaping-long-term-competitions-lessons-from#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ozqyLS8WpfeaeWbWY/surviving-and-shaping-long-term-competitions-lessons-from<guid ispermalink="false"> ozqyLS8WpfeaeWbWY</guid><dc:creator><![CDATA[Gentzel]]></dc:creator><pubDate> Fri, 24 Nov 2023 18:18:42 GMT</pubDate> </item><item><title><![CDATA[Ability to solve long-horizon tasks correlates with wanting things in the behaviorist sense]]></title><description><![CDATA[Published on November 24, 2023 5:37 PM GMT<br/><br/><p> <i>Status: Vague, sorry. The point seems almost tautological to me, and yet also seems like the correct answer to the people going around saying “LLMs turned out to be not very want-y, when are the people who expected &#39;agents&#39; going to update?”, so ， 我们到了。</i></p><p></p><p> Okay, so you know how AI today isn&#39;t great at certain... let&#39;s say &quot;long-horizon&quot; tasks? Like novel large-scale engineering projects, or writing a long book series with lots of foreshadowing?</p><p> (Modulo the fact that it can play chess pretty well, which is longer-horizon than some things; this distinction is quantitative rather than qualitative and it&#39;s being eroded, etc.)</p><p> And you know how the AI doesn&#39;t seem to have all that much &quot;want&quot;- or &quot;desire&quot;-like behavior?</p><p> (Modulo, eg, the fact that it can play chess pretty well, which indicates a certain type of want-like behavior in the behaviorist sense. An AI&#39;s ability to win no matter how you move is the same as its ability to reliably steer the game-board into states where you&#39;re check-mated, as though it had an internal check-mating “goal” it were trying to achieve. This is again a quantitative gap that&#39;s being eroded.)</p><p> Well, I claim that these are more-or-less the same fact. It&#39;s no surprise that the AI falls down on various long-horizon tasks <i>and</i> that it doesn&#39;t seem all that well-modeled as having &quot;wants/desires&quot;; these are two sides of the same coin.</p><p> Relatedly: to imagine the AI starting to succeed at those long-horizon tasks without imagining it starting to have more wants/desires (in the &quot;behaviorist sense&quot; expanded upon below) is, I claim, to imagine a contradiction—or at least an extreme surprise. Because the way to achieve long-horizon targets in a large, unobserved, surprising world that keeps throwing wrenches into one&#39;s plans, is probably to become a robust generalist wrench-remover that keeps stubbornly reorienting towards some particular target no matter what wrench reality throws into它的计划。</p><p></p><p> This observable &quot;it keeps reorienting towards some target no matter what obstacle reality throws in its way&quot; behavior is what I mean when I describe an AI as having wants/desires &quot;in the behaviorist sense&quot;.</p><p> I make no claim about the AI&#39;s internal states and whether those bear any resemblance to the internal state of a human consumed by the feeling of desire. To paraphrase something Eliezer Yudkowsky said somewhere: we wouldn&#39;t say that a blender &quot;wants&quot; to blend apples. But if the blender somehow managed to spit out oranges, crawl to the pantry, load itself full of apples, and plug itself into an outlet, then we might indeed want to start talking about it as though it has goals, even if we aren&#39;t trying to make a strong claim about the internal mechanisms causing this behavior.</p><p> If an AI causes some particular outcome across a wide array of starting setups and despite a wide variety of obstacles, then I&#39;ll say it &quot;wants&quot; that outcome “in the behaviorist sense”.</p><p></p><p> Why might we see this sort of &quot;wanting&quot; arise in tandem with the ability to solve long-horizon problems and perform long-horizon tasks?</p><p> Because these &quot;long-horizon&quot; tasks involve maneuvering the complicated real world into particular tricky outcome-states, despite whatever surprises and unknown-unknowns and obstacles it encounters along the way. Succeeding at such problems just seems pretty likely to involve skill at figuring out what the world is, figuring out how to navigate it, and figuring out how to surmount obstacles and then reorient in some stable direction.</p><p> (If each new obstacle causes you to wander off towards some different target, then you won&#39;t reliably be able to hit targets that you start out aimed towards.)</p><p> If you&#39;re the sort of thing that skillfully generates and enacts long-term plans, <i>and</i> you&#39;re the sort of planner that sticks to its guns and finds a way to succeed in the face of the many obstacles the real world throws your way (rather than giving up or wandering off to chase some new shiny thing every time a new shiny thing comes along), then the way I think about these things, it&#39;s a little hard to imagine that you <i>don&#39;t</i> contain some reasonably strong optimization that strategically steers the world into particular states.</p><p> (Indeed, this connection feels almost tautological to me, such that it feels odd to talk about these as distinct properties of an AI. &quot;Does it act as though it wants things?&quot; isn&#39;t an all-or-nothing question, and an AI can be partly goal-oriented without being maximally goal-oriented. But the more the AI&#39;s performance rests on its ability to make long-term plans and revise those plans in the face of unexpected obstacles/opportunities, the more consistently it will tend to steer the things it&#39;s interacting with into specific states—at least, insofar as it works at all.)</p><p> The ability to keep reorienting towards some target seems like a pretty big piece of the puzzle of navigating a large and complex world to achieve difficult outcomes.</p><p> And this intuition is backed up by the case of humans: it&#39;s no mistake that humans wound up having wants and desires and goals—goals that they keep finding clever new ways to pursue even as reality throws various curveballs at them, like “that prey animal has been hunted to extinction”.</p><p> These wants and desires and goals weren&#39;t some act of a god bequeathing souls into us; this wasn&#39;t some weird happenstance; having targets like “eat a good meal” or “impress your friends” that you reorient towards despite obstacles is a pretty fundamental piece of being able to eat a good meal or impress your friends. So it&#39;s no surprise that evolution stumbled upon that method, in our case.</p><p> (The implementation specifics in the human brain—eg, the details of our emotional makeup—seem to me like they&#39;re probably fiddly details that won&#39;t recur in an AI that has behaviorist “desires”. But the overall &quot;to hit a target, keep targeting it even as you encounter obstacles&quot; thing seems pretty central.)</p><hr><p> The above text vaguely argues that doing well on tough long-horizon problems requires pursuing an abstract target in the face of a wide array of real-world obstacles, which involves doing something that looks from the outside like “wanting stuff”. I&#39;ll now make a second claim (supported here by even less argument): that the wanting-like behavior required to pursue a particular training target X, does not need to involve the AI wanting X in particular.</p><p> For instance, humans find themselves wanting things like good meals and warm nights and friends who admire them. And all those wants added up <i>in the ancestral environment</i> to high inclusive genetic fitness. Observing early hominids from the outside, aliens might have said that the humans are “acting as though they want to maximize their inclusive genetic fitness”; when humans then turn around and invent birth control, it&#39;s revealed that they were never <i>actually</i> steering the environment toward that goal in particular, and instead had a messier suite of goals that <i>correlated</i> with inclusive genetic fitness, in the environment of evolutionary adaptedness, at that ancestral level of capability.</p><p> Which is to say, my theory says “AIs need to be robustly pursuing <i>some</i> targets to perform well on long-horizon tasks”, but it does <i>not</i> say that those targets have to be the ones that the AI was trained on (or asked for ）。 Indeed, I think the actual behaviorist-goal is very unlikely to be the exact goal the programmers intended, rather than (eg) a tangled web of correlates.</p><hr><p> A follow-on inference from the above point is: when the AI leaves training, and it&#39;s tasked with solving bigger and harder long-horizon problems in cases where it has to grow smarter than ever before and develop new tools to solve new problems, and you realize finally that it&#39;s pursuing neither the targets you trained it to pursue nor the targets you asked it to pursue—well, by that point, you&#39;ve built a generalized obstacle-surmounting engine. You&#39;ve built a thing that excels at noticing when a wrench has been thrown in its plans, and at understanding the wrench, and at removing the wrench or finding some other way to proceed with its plans.</p><p> And when you protest and try to shut it down—well, that&#39;s just another obstacle, and you&#39;re just another wrench.</p><p> So, maybe <a href="https://twitter.com/So8res/status/1715380167911067878"><u>don&#39;t make those generalized wrench-removers just yet</u></a> , until we do know how to load proper targets in there.</p><br/><br/> <a href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting<guid ispermalink="false"> AWoZBzxdm4DoGgiSj</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Fri, 24 Nov 2023 17:37:43 GMT</pubDate> </item><item><title><![CDATA[The Limitations of GPT-4]]></title><description><![CDATA[Published on November 24, 2023 3:30 PM GMT<br/><br/><p> <i>Amidst the rumours about a new breakthrough at OpenAI I thought I&#39;d better publish this draft before it gets completely overtaken by reality. It is essentially a collection of &quot;gaps&quot; between GPT4 and the human mind. Unfortunately the rumours around Q* force me to change the conclusion from &quot;very short timelines seem unlikely&quot; to &quot;who the f**k knows&quot;.</i></p><p> While GPT4 has a superhuman breadth of knowledge, writing speed and short-term memory, compared to the human mind GPT4 has a number of important limitations.</p><p> Some of these will be overcome in the near future because they depend on engineering and training data choices. Others seem more fundamental to me, because they are due to the model architecture and the training setup.</p><p> These fundamental limitations are the reason why I do not expect scaling GPT further to lead to AGI. In fact, I interpret further scaling of the exact current paradigm as evidence that overcoming these limitations is hard.</p><p> I expect scaled up GPT4 to exhibit the same strength and weaknesses and for the improved strengths to paper over the old weaknesses at most in a superficial fashion.</p><p> I also expect with further scaling the tasks GPT is unable to do to increasingly load on the fundamental limitations and therefore diminishing returns.</p><p> This list is not exhaustive and there are likely ways to frame even the limitations I identify in a more insightful or fruitful way. For example, I am not sure how to interpret GPT4s curious inability to understand humor. I hope further limitations will be mentioned in the comments.</p><p> <strong>Integration of the Senses</strong></p><p> GPT4 cannot really hear, and it cannot really talk.</p><p> Voice input is transcribed into text by a separate model, &#39;Whisper,&#39; and then fed to GPT4. The output is read by another model. This process loses the nuances of input—pronunciation, emphasis, emotion, accent, etc. Likewise, the output cannot be modulated by GPT4 in terms of speed, rhythm, melody, singing, emphasis, accent, or onomatopoeia.</p><p> In fact, due to tokenization it would be fair to say that GPT4 also cannot really read. All information about char-level structure that is relevant to spelling, rhyming, pronunciation must be inferred during training.</p><p> The vision component of most open multi-modal models is typically likewise &quot;grafted on.&quot; That is, it&#39;s partially trained separately and then connected and fine-tuned with the large language model (LLM), for example by using the CLIP model, which maps images and their descriptions to the same vector space.</p><p> This means GPT4 may not access the exact position of objects or specific details and cannot &quot;take a closer look&quot; as humans would.</p><p> I expect these limitations to largely vanish as models are scaled up and trained end-to-end on a large variety of modalities.</p><p> <strong>System 2 Thinking</strong></p><p> Humans not only think quickly and intuitively but also engage in slow, reflective thinking to process complex issues. GPT-4&#39;s architecture is not meaningfully recurrent; it has a limited number of processing steps for each token, putting a hard cap on sequential thought.</p><p> This contrast with human cognition is most evident in GPT4&#39;s unreliable counting ability. But it also shows up in many other tasks. The lack of system 2 thinking may be the most fundamental limitation of current large language models.</p><p> <strong>Learning during Problem Solving</strong></p><p> Humans rewire their brains through thinking; synapses are continuously formed or broken down. When we suddenly understand something, that realization often lasts a lifetime. GPT4, once trained, does not change during use.</p><p> It doesn&#39;t learn from its mistakes nor from correctly solved problems. It notably lacks an optimization step in problem-solving that would ensure previously unsolvable problems can be solved and that this problem-solving ability persists.</p><p> The fundamental difference here, is that in humans the correct representations for a given problem are worked out during the problem-solving process and then usually persist – GPT4 relies on the representations learned during training, new problems stay out of reach.</p><p> Even retraining doesn&#39;t solve this issue because it would require many similar problems and their solutions for GPT4 to learn the necessary representations.</p><p> <strong>Compositionality and Extrapolation</strong></p><p> Some theories suggest that the human neocortex, the seat of intelligence, uses most of its capacity to model the interplay of objects, parts, concepts, and sub-concepts. This ability to abstractly model the interplay of parts allows for better extrapolation and learning from significantly less data.</p><p> In contrast, GPT-4 learns the statistical interplay between words. Small changes in vocabulary can significantly influence its output. It requires a vast amount of data to learn connections due to a lack of inductive bias for compositionality.</p><p> <strong>Limitations due to the Training Setup</strong></p><p> Things not present in the training data are beyond the model&#39;s learning capacity, including many visual or acoustic phenomena and especially physical interaction with the world.</p><p> GPT-4 does not possess a physical, mechanical, or intuitive understanding of many world aspects. The world is full of details that become apparent only when one tries to perform tasks within it. Humans learn from their interaction with the world and are evolutionarily designed to act within it. GPT-4 models data, and there is nothing beyond data for it.</p><p> This results in a lack of consistency in decisions, the ability to robustly pursue goals, and the understanding or even the need to change things in the world. The input stands alone and does not represent real-world situations.</p><p> GPT-4&#39;s causal knowledge is merely meta-knowledge stored in text. Learning causal models of new systems would require interaction with the system and feedback from it. Due to this missing feedback, there is little optimization pressure against hallucinations.</p><p><strong>结论</strong></p><p>Some of these points probably interact or will be solved by the same innovation. System 2 thinking is probably necessary to move the parts of concepts around while looking for a solution to a problem.</p><p> The limitations due to the training setup might be solved with a different one. But that means forgoing cheap and plentiful data. The ability to learn from little data will be required to learn from modalities other than abundant and information-dense text.</p><p> It is very unclear to me how difficult these problems are to solve. But I also haven&#39;t seen realistic approaches to tackle them. Every passing year makes it more likely that these problems are hard to solve.</p><p> Very short timelines seemed unlikely to me when I wrote this post, but Q* could conceivably solve &quot;system 2 thinking&quot; and/or &quot;learning during problem solving&quot; which might be enough to put GPT5 over the threshold of &quot;competent human&quot; in many域。</p><br/><br/><a href="https://www.lesswrong.com/posts/o8eMsxA7uHfybfmhd/the-limitations-of-gpt-4#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/o8eMsxA7uHfybfmhd/the-limitations-of-gpt-4<guid ispermalink="false"> o8eMsxA7uHfybfmhd</guid><dc:creator><![CDATA[p.b.]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:30:33 GMT</pubDate> </item><item><title><![CDATA[Progress links digest, 2023-11-24: Bottlenecks of aging, Starship launches, and much more]]></title><description><![CDATA[Published on November 24, 2023 3:25 PM GMT<br/><br/><p> I swear I will get back to doing these weekly so they&#39;re not so damn long. As always, feel free to skim and skip around!</p><h2> <strong>The Progress Forum</strong></h2><ul><li> <a href="https://progressforum.org/posts/zdxubhLFATGwRbXhd/a-paradox-at-the-heart-of-american-bureaucracy">A paradox at the heart of American bureaucracy</a> : “The quickest way to doom a project to be over-budget and long-delayed is to make it an urgent public priority”</li><li> <a href="https://progressforum.org/posts/DQweS5hFbj9MX7rR8/why-governments-can-t-be-trusted-to-protect-the-long-run">Why Governments Can&#39;t be Trusted to Protect the Long-run Future</a> : “No one in the long-run future gets to vote in the next election. No one in government today will gain anything if they make the world better 50 years from now or lose anything if they make it worse”</li><li> <a href="https://progressforum.org/posts/XYqCXpP2Hg3Yiwcdh/what-if-we-split-the-us-into-city-states">What if we split the US into city-states?</a> “In The Republic, when his entourage asks the ideal size of a state, Socrates replies, &#39;I would allow the state to increase so far as is consistent with unity; that, I think, is the proper limit&#39;”</li><li> <a href="https://progressforum.org/posts/uJcGssuGDxvSFCMcZ/the-art-of-medical-progress">The Art of Medical Progress</a> : “These two paintings offer a hopeful contrast. Whereas we begin with pain and suffering, we move to hope and progress. The surgeon stands apart as a hero, a symbol of the triumphant conquering of nature by humanity”</li></ul><p> <a href="https://pbs.twimg.com/media/F9tTVPRWEAAC0ei.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/awcljugre1plabw0pexn" alt=""></a></p><h2> <strong>More from Roots of Progress fellows</strong></h2><ul><li> <a href="https://amaranth.foundation/bottlenecks-of-aging">Bottlenecks of Aging</a> , a “philanthropic menu” of initiatives that “could meaningfully accelerate the advancement of aging science and other life-extending technologies.” Fellows Alex Telford and Raiany Romanni both contributed to this (via <a href="https://twitter.com/jamesfickel/status/1724503929646403989">@jamesfickel</a> )</li><li> <a href="https://twitter.com/lauralondon_/status/1726411916917284965">Drought is a policy choice</a> : “California has surrendered to drought, presupposing that with climate change water shortages are inevitable. In response, the state fallows millions of farmland each year. But this is ignorant of California&#39;s history of taming arid lands”</li><li> <a href="https://maximumprogress.substack.com/p/geoengineering-now">Geoengineering Now!</a> “Solar geoengineering can offset every degree of anthropogenic temperature rise for single-digit billions of dollars” (by <a href="https://twitter.com/MTabarrok/status/1722259578094817316">@MTabarrok</a> )</li><li> <a href="https://finmoorhouse.com/writing/richard-bruns/">A conversation with Richard Bruns on indoor air quality</a> (and some very feasible ways to improve it) ( <a href="https://twitter.com/finmoorhouse/status/1724226059501986181">@finmoorhouse</a> )</li><li> <a href="https://www.nytimes.com/2023/11/17/opinion/chips-act-biden-arizona.html">To Become a World-Class Chipmaker, the United States Might Need Help</a> (NYT) covers a recent immigration proposal co-authored by ( <a href="https://twitter.com/cojobrien/status/1725613102694007060">@cojobrien</a> ). Also, <a href="https://twitter.com/cojobrien/status/1724143576760680615">thread from @cojobrien</a> of “what I&#39;ve written through this program and some of my favorite pieces from other ROP colleagues”</li></ul><h2><strong>机会</strong></h2><h3><strong>工作机会</strong></h3><ul><li><a href="https://forestneurotech.org/jobs">Forest Neurotech is hiring</a> , “one of the coolest projects in the world” says <a href="https://twitter.com/elidourado/status/1724871363851121034">@elidourado</a></li><li> “ <a href="https://jobs.lever.co/arcadiascience/56dbdc4c-d64b-48ec-9665-181e20036269">Know someone who loves to scale and automate workflows in the lab? We want to apply new tools to onboard a diverse array of species in the lab!</a> ” ( <a href="https://twitter.com/seemaychou/status/1719035397836361812">@seemaychou</a> )</li><li> <a href="https://boards.greenhouse.io/navigationfund/jobs/4127979007">The Navigation Fund (new philanthropic foundation) is hiring an Open Science Program Officer</a> (via <a href="https://twitter.com/seemaychou/status/1721568842496070110">@seemaychou</a> , <a href="https://twitter.com/AGamick/status/1720487944103063836">@AGamick</a> )</li><li> <a href="https://aria-jobs.teamtailor.com/jobs">ARIA Research (UK) is hiring for various roles</a> ( <a href="https://twitter.com/davidad/status/1720465639242895594">@davidad</a> )</li></ul><h3> <strong>Fundraising/investing opportunities</strong></h3><ul><li> Nat Friedman is “ <a href="https://twitter.com/natfriedman/status/1723100077718438065">interested in funding early stage startups building evals for AI capabilities</a> ”</li><li> A curated deal flow network for deep tech startups: “We&#39;re looking for A+ deep tech operator-angels. Eg founders &amp; CxOs at $1b+ deep tech companies, past and present. Robotics, biotech, defense, etc. Who should we talk to?” ( <a href="https://twitter.com/lpolovets/status/1724237923371884697">@lpolovets</a> )</li></ul><h3> <strong>Policy opportunities</strong></h3><ul><li> “In 2024 I will be putting together a nuclear power working group for NYC/NYS. If you understand the government (or want to learn), want to act productively, and want to look at nuclear policy in the state, this is for you!” ( <a href="https://twitter.com/danielgolliher/status/1719150014948061582">@danielgolliher</a> )</li></ul><h3> <strong>Gene editing opportunities</strong></h3><ul><li> “I&#39;m tired of waiting forever for a cure for red-green colorblindness. <a href="https://twitter.com/elidourado/status/1724950684972269904">Reply to this tweet</a> if you&#39;d be willing to travel to an offshore location to receive unapproved (but obviously safe) gene therapy to fix it. If I get enough takers I&#39;ll find us a mad scientist to administer the therapy. <a href="http://www.neitzvision.com/test/research/gene-therapy/">This has already been done in monkeys (14 years ago) using human genes</a> and a viral vector that is already used in eyes in humans.” ( <a href="https://twitter.com/elidourado/status/1724950684972269904">@elidourado</a> )</li></ul><h2><strong>活动</strong></h2><ul><li><a href="https://foresight.org/vision-weekends-2023/">Foresight Vision Weekend USA</a> is coming up soon (Dec 1–3) in SF; I&#39;m speaking (via <a href="https://twitter.com/foresightinst/status/1721376896175268209">@foresightinst</a> )</li></ul><h2><strong>讣告</strong></h2><ul><li>“The world has lost another Apollo era legend. Ken Mattingly, the Apollo 16 and Shuttle astronaut left us on October 31. Ken&#39;s contributions to the field of spaceflight were nothing short of extraordinary” ( <a href="https://twitter.com/ArmstrongSpace/status/1720182807102648716">@ArmstrongSpace</a> ). “Every time we lose one of the Apollo astronauts, I think of this chart from @xkcd” ( <a href="https://twitter.com/Robotbeat/status/1720179342448222643">@Robotbeat</a> ):</li></ul><p> <a href="https://pbs.twimg.com/media/F99N_MWWkAAqdI8.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/fjeh8o8cn0zpsv26p8qa" alt=""></a></p><ul><li> <a href="https://www.newcomersyracuse.com/Obituary/270445/William-Powell/Syracuse-NY">Bill Powell of SUNY-ESF has died at 67</a> . “He will be remembered as the father of the genetically modified American Chestnut tree that many (including me) hope will restore Eastern North American forest” ( <a href="https://twitter.com/HankGreelyLSJU/status/1724533670298702275">@HankGreelyLSJU</a> )</li></ul><h2><strong>新闻与公告</strong></h2><h3><strong>Starship launches</strong></h3><ul><li> Video from <a href="https://twitter.com/SpaceX/status/1725862657780281349">@SpaceX</a></li><li> Pictures from @johnkrausphotos ( <a href="https://twitter.com/johnkrausphotos/status/1725863945276195266">1</a> , <a href="https://twitter.com/johnkrausphotos/status/1725865242427609588">2</a> )</li></ul><p> <a href="https://pbs.twimg.com/media/F_OAGxPWMAAQhw2.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/cwuwe5v6o5ewpju51bjj" alt=""></a> <a href="https://pbs.twimg.com/media/F_OBSYdXkAAnluD.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/rh0sdzmefnmehqhqovxa" alt=""></a></p><h3><strong>新企业</strong></h3><ul><li><a href="https://www.futurehouse.org/articles/announcing-future-house">Future House is “a philanthropically-funded moonshot focused on building an AI Scientist.”</a>他们正在招聘。 (via <a href="https://twitter.com/SGRodriques/status/1719724774631596352">@SGRodriques</a> )</li><li> <a href="https://www.antaresindustries.com/">Antares</a> is “building micro-sized nuclear reactors to provide power to remote off-grid locations,” with a vision of “abundant clean energy for all, from Earth to the asteroid belt” (via <a href="https://twitter.com/juliadewahl/status/1719448793027109114">@juliadewahl</a> )</li><li> Valar Atomics aims “to make energy 10x cheaper in 10 years by pulling oil and gas out of thin air with nuclear fission” ( <a href="https://twitter.com/isaiah_p_taylor/status/1720418162985054350">@isaiah_p_taylor</a> )</li></ul><h3><strong>新书</strong></h3><ul><li><a href="https://www.amazon.com/Geek-Way-Radical-Transforming-Business/dp/0316436704/"><i>The Geek Way</i> , by Andy McAfee</a> (author of <i>More from Less</i> ) is about “a new and better way to get big things done” (via <a href="https://twitter.com/amcafee/status/1724429137215992098">@amcafee</a> )</li><li> <a href="https://buttondown.email/edyong209/archive/the-eds-up-announcing-my-third-book/">New book in the works from Ed Yong</a> : <i>The Infinite Extent</i> , “about how animals, plants, microbes, and other forms of life thrive at the edges of space and time, geography and longevity, connectivity and identity”</li></ul><h3> <strong>Other new publications</strong></h3><ul><li> <a href="https://latecomermag.com/">The Latecomer</a> , a new magazine with some good authors, looks interesting. First issue includes “ <a href="https://latecomermag.com/article/we-will-build-our-way-out-of-the-climate-crisis/">We Will Build Our Way Out of the Climate Crisis</a> ” by Casey Handmer</li><li> <a href="https://possibiliamag.com/">Possibilia</a> , “a literary magazine that publishes optimistic, realistic, scientific fiction” (via <a href="https://twitter.com/possibiliamag/status/1704996131141521600">@possibiliamag</a> )</li><li> <a href="https://worksinprogress.co/issue-13">Works In Progress Issue 13</a> (via <a href="https://twitter.com/WorksInProgMag/status/1724836600864035282">@WorksInProgMag</a> )</li><li> <a href="https://www.employamerica.org/researchreports/introducing-hot-rocks-commercializing-next-generation-geothermal-energy/">Hot Rocks: Commercializing Next-Generation Geothermal Energy</a> , a joint project of Employ America and IFP (via <a href="https://twitter.com/ArnabDatta321/status/1719080612093427783">@ArnabDatta321</a> )</li></ul><h3><strong>生物新闻</strong></h3><ul><li><a href="https://www.businesswire.com/news/home/20231115290500/en/%C2%A0Vertex-and-CRISPR-Therapeutics-Announce-Authorization-of-the-First-CRISPRCas9-Gene-Edited-Therapy-CASGEVY%E2%84%A2-exagamglogene-autotemcel-by-the-United-Kingdom-MHRA-for-the-Treatment-of-Sickle-Cell-Disease-and-Transfusion-Dependent-Beta-Thalassemia">The first approved CRISPR medicine in the world for sickle cell disease and beta thalassemia!</a> “A huge victory for biotechnology, patients, and humanity” ( <a href="https://twitter.com/pdhsu/status/1725144576904896615">@pdhsu</a> )</li><li> <a href="https://www.ft.com/content/20badecd-0e25-4526-8b8e-6870a566163e?shareType=nongift">UK Biobank genetics database wins donations of $10M each from Eric Schmidt and Ken Griffin</a> (via <a href="https://twitter.com/JimPethokoukis/status/1718822222100373874">@JimPethokoukis</a> )</li></ul><h3> <strong>Nuclear news</strong></h3><ul><li> <a href="https://www.bloomberg.com/news/articles/2023-11-14/us-uk-to-push-pledge-to-triple-nuclear-power-by-2050-at-cop28">The US will lead a pledge to triple global nuclear power capacity by 2050 at COP28</a> . “Declaration will call on the World Bank &amp; other financial institutions to include nuclear in lending policies… UK, France, Sweden, Finland, Korea to join pledge” (via <a href="https://twitter.com/SStapczynski/status/1724701989270171775">@SStapczynski</a> )</li><li> <a href="https://world-nuclear-news.org/Articles/Illinois-to-lift-moratorium-on-nuclear-constructio">Illinois governor says he will sign a new bill lifting ban on the construction of new nuclear reactors</a> (via <a href="https://twitter.com/W_Nuclear_News/status/1724103948489961522">@W_Nuclear_News</a> )</li></ul><h3><strong>住房新闻</strong></h3><ul><li>“Milwaukee, Wisconsin just proposed the most ambitious zoning code in the US: all residential parking mandates gone; small apartment buildings legal by right in core; triplexes, townhomes, &amp; ADUs legal by right citywide; permitting fast-tracked” ( <a href="https://twitter.com/JacoMajor/status/1721977907491266985">@JacoMajor</a> )</li><li> “A new 71 story residential building application for SoMa in 2023 … because of AB 2011, these 672 new homes can be built without having to be approved by the Board of Supervisors &amp; don&#39;t have to go through CEQA” ( <a href="https://twitter.com/pitdesi/status/1725263401784639509">@pitdesi</a> )</li></ul><h2><strong>人工智能</strong></h2><h3><strong>AI leadership announcements</strong></h3><ul><li> There was a whole big thing about OpenAI. Too much to summarize, sorry. I assume you already read about it, and if not, every other tech blog in the world has a roundup/commentary</li><li> <a href="https://twitter.com/kvogt/status/1726428099217400178">Kyle Vogt has resigned as CEO of Cruise</a></li></ul><h3> <strong>AI product announcements</strong></h3><ul><li> <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">DeepMind&#39;s GraphCast</a> is “the most accurate 10-day global weather forecasting system in the world. GraphCast can also offer earlier warnings of extreme weather events, including the path of hurricanes” (via <a href="https://twitter.com/demishassabis/status/1724452655454466489">@demishassabis</a> )</li><li> <a href="https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/">Lyria</a> , also from DeepMind. The most impressive generative AI music I have seen yet. Hum a few bars, or type in a description, and get a fully orchestrated track. Skip the marketing video and just watch the example videos that are &lt; 1 minute each</li><li> Synthesis shows off a <a href="https://twitter.com/synthesischool/status/1724834959427342727">personal AI tutor</a></li><li> “ <a href="https://openai.com/blog/introducing-gpts">GPTs are a new way for anyone to create a tailored version of ChatGPT</a> to be more helpful in their daily life, at specific tasks, at work, or at home—and then share that creation with others. No code required” ( <a href="https://twitter.com/OpenAI/status/1721594380669342171">@OpenAI</a> )</li><li> Eg, <a href="https://chat.openai.com/g/g-IlhXHXNBh-foia-gpt">FOIA GPT</a> : “write FOIA document drafts with code interpreter; strategize and assist replies to get appeals” (via <a href="https://twitter.com/micksabox/status/1723069002174369801">@micksabox</a> )</li><li> <a href="https://x.ai/">Grok</a> , from X.ai (via <a href="https://twitter.com/elonmusk/status/1721027243571380324">@elonmusk</a> )</li></ul><h3><strong>人工智能预测</strong></h3><ul><li>Cambridge student: “To get to AGI, can we just keep min maxing language models, or is there another breakthrough that we haven&#39;t really found yet to get to AGI?” Sam Altman: “ <strong>We need another breakthrough.</strong>我们仍然可以大力推动大型语言模型，我们也会这么做。我们可以登上我们所在的山并继续攀登，但山顶仍然很遥远。但是，在合理范围内，我认为这样做不会（让我们）实现 AGI。如果（例如）超级智能无法发现新颖的物理学，我不认为它是超级智能。 And teaching it to clone the behavior of humans and human text—I don&#39;t think that&#39;s going to get there. And so there&#39;s this question which has been debated in the field for a long time: what do we have to do in addition to a language model to make a system that can go discover new physics?” (via <a href="https://twitter.com/burny_tech/status/1725233117055553938">@burny_tech</a> )</li><li> “AI, like every other tool since fire, will increase human productivity. It will only &#39;destroy all jobs&#39; in the sense that it will reduce the need and demand for very low productivity work, just like fire reduced the demand for shivering through the night or digesting uncooked meat. Our current tool set has destroyed the job market for children, and for the very old even as it has greatly increased the numbers of humans of all ages. This is usually regarded as a good thing, indeed raising the retirement age (increasing labor supply, forcing old people to work) is not politically popular, even as demographic trends place ever greater productivity demands on younger workers. AI enabled productivity increases are desperately needed!” ( <a href="https://twitter.com/CJHandmer/status/1722990374229311566">@CJHandmer</a> )</li><li> “In ~five years we&#39;ll have a thriving industry of LMO: Language Model Optimization, by analogy with SEO. When someone asks their chatbot to make dinner reservations, how do you make sure your restaurant gets suggested? Ditto for product recommendations, trip planning, etc….” （<a href="https://twitter.com/jasoncrawford/status/1724471424633479169">我</a>）</li><li> “I predict that some of my grandchildren will never learn to drive and their kids won&#39;t be allowed to drive.” From: <a href="https://marginalrevolution.com/marginalrevolution/2023/11/autonomous-vehicles-lower-insurance-costs.html">Autonomous Vehicles Lower Insurance Costs</a> (by <a href="https://twitter.com/ATabarrok/status/1721983990830190924">@ATabarrok</a> )</li></ul><h3><strong>人工智能安全</strong></h3><ul><li><a href="https://aria.org.uk/wp-content/uploads/2023/10/ARIA-Mathematics-and-modelling-are-the-keys-we-need-to-safely-unlock-transformative-AI-v01.pdf">Mathematics and modelling are the keys we need to safely unlock transformative AI</a> : on “opportunities to combine LLMs with formal methods and mathematical modelling to verify cyber-physical AI systems, ultimately aiming to enabling globally transformative AI with provable safety” (by <a href="https://twitter.com/davidad/status/1719770184565530890">@davidad</a> )</li><li> Kevin Esvelt ran a hackathon where participants playing “compulsively honest bioterrorists” asked two different LLMs how to obtain 1918 influenza virus, to see how robust safeguards are. One model “happily walked some participants almost all the way through the process.” <a href="https://arxiv.org/abs/2310.18233">Will releasing the weights of future large language models grant widespread access to pandemic agents?</a> (via <a href="https://twitter.com/kesvelt/status/1718976444175425796">@kesvelt</a> )</li></ul><h3> <strong>AI regulation</strong></h3><ul><li> <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">Biden Administration releases an executive order on AI</a> (via <a href="https://twitter.com/deliprao/status/1719020647576187020">@deliprao</a> )</li><li> The key reporting requirement applies to “any model trained with ~28M H100 hours, which is around $50M USD, or any cluster with 10^20 FLOPs, which is around 50,000 H100s, which only two companies currently have” ( <a href="https://twitter.com/nearcyan/status/1719110671085060213">@nearcyan</a> )</li><li> “The EO, in what will probably be the most touted provisions, would regulate AI companies through an unusual and aggressive use of the Defense Production Act, a Korean War era law intended to ensure the government can get materials and products it needs to defend the国家。 The DPA is usually used to put government orders at the front of the line, and sometimes to issue loans to enable a company to complete government orders on time. Yet here the White House would use it to require certain procedures (notification of training and reporting the results of red teaming tests) by companies before they release products to the public. That type of regulation is Congress&#39;s job, and any legally sustainable path will require Congressional action.”( <a href="https://twitter.com/neil_chilson/status/1719073480610635965">@neil_chilson</a> )</li><li> <a href="https://carnegieendowment.org/2023/10/27/summary-proposal-for-international-panel-on-artificial-intelligence-ai-safety-ipais-pub-90862">Eric Schmidt also has a “proposal for a start for AI investment and regulation”</a> (via <a href="https://twitter.com/ericschmidt/status/1718908597659165013">@ericschmidt</a> )</li><li> “I don&#39;t know what the right approach to regulating AI is, but one problem with this particular approach is that it means we&#39;re heading toward the government regulating private individuals&#39; computing at an exponential rate.” ( <a href="https://twitter.com/paulg/status/1719264300362015174">@paulg</a> )</li><li> “Lord, grant me the confidence of Bruce Reed, who spearheaded the White House Executive Order on AI… <a href="https://www.politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375">&#39;And who wants to work on tech policy if you actually have to understand how these microscopic things work?&#39;</a> ” ( <a href="https://twitter.com/WillRinehart/status/1725524415339717118">@WillRinehart</a> )</li><li> “ <a href="https://twitter.com/neil_chilson/status/1725889789155491849">We&#39;re being asked to stop a major technology based on pseudo-science</a> .” (I take safety issues seriously, but this line sums up what I think about calls to “slow” or “pause” AI development)</li></ul><h2><strong>播客</strong></h2><ul><li><a href="https://twitter.com/juliadewahl/status/1725537531129933836">Age of Miracles Episode 5</a> , on advanced nuclear reactor startups</li></ul><h2> <strong>Articles and other links</strong></h2><ul><li> <a href="https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine">The ARPA Playbook</a> , a new series of articles from <a href="https://twitter.com/eric_is_weird/status/1720528162939973868">@eric_is_weird</a></li><li> <a href="https://www.ageofinvention.xyz/p/age-of-invention-outdoing-the-ancients">Outdoing the Ancients</a> : “When was the Ancient World surpassed technologically? The surprising view from 1599 and from 1715” (by <a href="https://twitter.com/antonhowes/status/1722939185961607511">@antonhowes</a> )</li><li> <a href="https://royalsociety.org/blog/2010/08/what-scientists-want-boyle-list/">Robert Boyle&#39;s scientific to-do list from the 1600s</a> : “A perpetuall light; The making of glass malleable; A ship to saile with all winds; The art of flying.” “Guys we have done so well” ( <a href="https://twitter.com/SGRodriques/status/1726254985498042408">@SGRodriques</a> )</li><li> “ <a href="https://worksinprogress.co/issue/growing-the-growth-coalition/">Growing the growth coalition</a> ” is “one of the most important articles ever written for understanding why we are failing to deliver sufficient housing &amp; how to fix the problem” ( <a href="https://twitter.com/bswud/status/1724513661480296542">@bswud</a> )</li><li> <a href="https://research.arcadiascience.com/pub/perspective-icebox-lessons/release/1">Icebox is a science-sharing strategy designed to encourage risk-taking</a> . “Our &#39;icebox&#39; is where we share the projects that we&#39;ve decided not to continue.” (via <a href="https://twitter.com/seemaychou/status/1720476166430363746">@seemaychou</a> )</li><li> <a href="https://en.m.wikipedia.org/wiki/List_of_emerging_technologies">List of emerging technologies</a> . “Surprisingly interesting… many entire fields I&#39;d never heard of!” ( <a href="https://twitter.com/michael_nielsen/status/1722288370297270738">@michael_nielsen</a> ) “Let&#39;s go! 1. None of these is inevitable—it takes a lot of work to turn them into a real thing that can improve lives. 2. There are so many possibilities that are <i>not</i> on this list. Many of these things were not even imaginable a hundred years ago.” ( <a href="https://twitter.com/Ben_Reinhardt/status/1722612900366627161">@Ben_Reinhardt</a> )</li><li> “The Greeks honored Prometheus. They celebrated technē. They appreciated the gifts of civilization… <a href="https://vpostrel.substack.com/p/the-myth-of-prometheus-is-not-a-cautionary">The ancient myth of Prometheus is not a cautionary tale</a> . It is a reminder that technē raises human beings above brutes. It is a myth founded in gratitude.” (Virginia Postrel)</li><li> <a href="https://www.theatlantic.com/ideas/archive/2023/11/new-york-tourism-airbnb-rentals-hotels/675860/">“New York City used to process up to 10,000 immigrants a day at Ellis Island alone. Now a government larger, wealthier, and with more resources is claiming that 10,000 a month is impossible to bear”</a> (by <a href="https://twitter.com/JerusalemDemsas/status/1720052260669931739">@JerusalemDemsas</a> ). (“I would simply legalize building things in the places where the demand is high,” says <a href="https://twitter.com/mattyglesias/status/1720188743267594259">@mattyglesias</a> )</li></ul><h2><strong>查询</strong></h2><p>If you have a helpful answer, please click through and reply:</p><ul><li> “We&#39;re in the middle of interviews for the fusion half of Age of Miracles Season 1…. Which founders, researchers, investors, and even historians in fusion should we talk to?” ( <a href="https://twitter.com/packyM/status/1721544302398931241">@packyM</a> )</li><li> “What is the best movie about manufacturing?” ( <a href="https://twitter.com/grantadever/status/1719808495782879717">@grantadever</a> )</li><li> “Did you or someone you know win the &#39;genetic lottery&#39;?为何如此？ I want put together a &#39;mutantpedia&#39;—an encyclopedia of known human mutants with beneficial genetic traits” ( <a href="https://twitter.com/kanzure/status/1722620600232153400">@kanzure</a> )</li><li> “Who are the best accounts to follow for innovation? Innovation management? Innovation research (is this a thing?)” ( <a href="https://twitter.com/andrewfierce/status/1725196063160561901">@andrewfierce</a> )</li><li> “What do AI safety/accelerationist people disagree on that they could bet on? What concrete things are going to happen in the next two years that would prove one party right or wrong?” ( <a href="https://twitter.com/NathanpmYoung/status/1726204750893625751">@NathanpmYoung</a> )</li><li> “Women&#39;s reproductive health is such an exciting &amp; important area to research, despite many obstacles other fields face to a lesser extent. Who&#39;s currently working in this space?” ( <a href="https://twitter.com/KKajderowicz/status/1723456602429092167">@KKajderowicz</a> )</li><li> “Max Weber. A hole in my learning. Where does one start?” ( <a href="https://twitter.com/Scholars_Stage/status/1723810425978912926">@Scholars_Stage</a> )</li><li> “Do I know anyone with experience automatically segmenting images, especially maps? Where should I start if I want to learn how to do this?” ( <a href="https://twitter.com/MTabarrok/status/1724278906289582230">@MTabarrok</a> )</li><li> “Has anyone with an office included a library that people actually use? Particularly interested in libraries that <i>actually</i> succeed in prompting deep work” ( <a href="https://twitter.com/LauraDeming/status/1722720856420565223">@LauraDeming</a> )</li></ul><h2><strong>社交媒体</strong></h2><ul><li><a href="https://twitter.com/philipturnerar/status/1720988930999234954">Atomically precise NOR gate</a> , cool animation</li><li> “The US spends ~$300 billion a year on fire safety.这很值得。 Could a similar investment virtually eradicate infectious disease and prevent future pandemics?也许！ A key question: how fast can we safely eliminate viruses with germicidal light?” ( <a href="https://twitter.com/kesvelt/status/1721566217637630252">thread from @kesvelt</a> )</li><li> “Combined cycle plants get built quick. 1100 MW plant going from clearing the site to operational in less than 2 years” ( <a href="https://twitter.com/_brianpotter/status/1723058133583426041">@_brianpotter</a> )</li><li> “How can you leverage nuclear energy to propel vehicles? In 1963, the US Army knew direct nuclear plants would be too heavy for normal vehicles, and very large vehicles would have &#39;serious tactical disadvantages.&#39; And so the Army focused on &#39;the energy depot&#39; concept, where a nuclear reactor and associated equipment would be used to manufacture chemical fuels from elements universally available in air and water.” ( <a href="https://twitter.com/whatisnuclear/status/1726009266442850333">thread from @whatisnuclear</a> with pics and more)</li><li> Oxford was founded before the First Crusade. Cambridge before the Magna Carta. Harvard is older than Louis XIV. Universities are some of our most long-lived institutions. They have survived the rise and fall of empires. They are extremely resilient and resistant to change. (me on <a href="https://www.threads.net/@jasoncrawford/post/CzKMzJaLAJj">Threads</a> , <a href="https://twitter.com/jasoncrawford/status/1720175177990832333">Twitter</a> )</li><li> Dog power in 1640s Belgium: “I met with diverse little waggons, prettily contrived, and full of peddling merchandises, drawn by mastiff-dogs, harnessed completely like so many coach horses; in some four, in others six, as in Brussels itself I had observed.” ( <a href="https://twitter.com/antonhowes/status/1719314670727680111">@antonhowes</a> )</li><li> <a href="https://twitter.com/culturaltutor/status/1718755912750403766">City parks as a 19th-century invention</a></li><li> “Everybody wants metrics, explanations of how things will change the world, market sizes, etc. Those are fine, but my heuristic is &#39;does this feel like magic that humanity has forged from the hands of nature?&#39;” ( <a href="https://twitter.com/Ben_Reinhardt/status/1725869685516718114">@Ben_Reinhardt</a> )</li><li> “Just saw a &#39;why do we teach students calculus in high school? I never use it&#39; tweet. I have so little sympathy. Calculus has so many applications and is used by many fields. Also don&#39;t we just want to teach students some of the most important knowledge people have acquired?” ( <a href="https://twitter.com/itaisher/status/1722419892152938563">@itaisher</a> )</li><li> “This is very laudable (from a striking profile of Katalin Kariko), an individual postmortem on a likely error. But no sign of an institutional postmorterm by Penn or NIH” ( <a href="https://twitter.com/michael_nielsen/status/1721235940717498527">@michael_nielsen</a> )</li><li> <a href="https://twitter.com/dieworkwear/status/1725234349111721992">The Housing Theory of Everything strikes again</a></li><li> “The decline in public R&amp;D can explain around a third of the decline in TFP growth in the US from 1950 to 2018” ( <a href="https://twitter.com/ArnaudDyevre/status/1724027240143360494">@ArnaudDyevre</a> via <a href="https://twitter.com/calebwatney/status/1724144910318641428">@calebwatney</a> )</li><li> “I will never understand why the debate over perpetual growth is so prominent. It really doesn&#39;t matter if we will forever get richer, only if we can get sustainably richer than at the current moment. And it&#39;s clear that we haven&#39;t exhausted growth possibilities” ( <a href="https://twitter.com/tribsantos/status/1720086824369148374">@tribsantos</a> )</li><li> “I never know what to make of the doomers who are freaking out over rising sea levels in 2100, etc. Are they seriously suggesting we can&#39;t handle what our much poorer ancestors did with much more primitive tech?” ( <a href="https://twitter.com/Marian_L_Tupy/status/1723737438215197002">@Marian_L_Tupy</a> )</li><li> “Wealth is good. Prosperity is wholesome. If you are privileged what you should feel is gratitude, not shame, and you should be thinking of how you can employ and pass on this prosperity.” ( <a href="https://twitter.com/simonsarris/status/1719065815289299398">@simonsarris</a> )</li><li> <a href="https://twitter.com/celinehalioua/status/1726321979266080879">Books matter</a></li><li> “ <a href="https://twitter.com/jasoncrawford/status/1720242739886031239">Chatmogorov complexity</a> ”: the length of the shortest ChatGPT prompt that can generate a given piece of text</li><li> Tired: thinking about the Roman Empire every day. Wired: <a href="https://twitter.com/jonst0kes/status/1722664153394171932">thinking about calculus every minute</a></li><li> “You can just search made-up sci-fi sounding words like &#39;Plasma Rail Gun&#39; and half the time you end up on some ARPA-E slide deck reviving the concept from the 1970s” ( <a href="https://twitter.com/Andercot/status/1724245387794616567">@Andercot</a> )</li></ul><p> <a href="https://pbs.twimg.com/media/F-3ACplaQAABgNM.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/f1b26kyqvnnineignrc9" alt=""></a></p><h2><strong>引号</strong></h2><p>“Before Kendall Square was a leading biotech hub, it was a leading manufacturing hub” ( <a href="https://twitter.com/Atelfo/status/1721711654344245455">@Atelfo</a> ). Robert Buderi, <i>Where Futures Converge</i> :</p><blockquote><p> Within an area of two square miles of Kendall Square, where the greatest manufacturing development has taken place, are located more than 200 plants, whose invested capital exceeds $100,000,000. Here the searcher of facts finds the homes of the largest manufacturer of soap in the world; the greatest producer of rubber clothing in the world; the largest manufacturer of mechanical rubber goods in the world; the largest plant in the world devoted exclusively to the printing of school and college textbooks; the greatest producer of writing inks, adhesives, carbon papers, and typewriter ribbons in the world; the largest plant in the world devoted exclusively to the manufacturer of confectionery; a branch plant of the largest manufacturer of optical goods and optical machinery in the world, the largest producer of road paving plants in the world; the oldest and largest school supply house in the United States; the only industrial research laboratory of its kind in the country.</p></blockquote><p> <a href="https://chronicle.uchicago.edu/951012/chandra.shtml">Subrahmanyan Chandrasekhar</a> (via <a href="https://twitter.com/michael_nielsen/status/1723119437304459290">@michael_nielsen</a> )</p><blockquote><p> One story in particular illustrates Chandrasekhar&#39;s devotion to his science and his students. In the 1940s, while he was based at the University&#39;s Yerkes Observatory in Williams Bay, Wis., he drove more than 100 miles round-trip each week to teach a class of just two registered students. Any concern about the cost- effectiveness of such a commitment was erased in 1957, when the entire class—TD Lee and CN Yang—won the Nobel Prize in physics.</p></blockquote><p> A story via <a href="https://twitter.com/stewartbrand/status/1724889911378227213">@stewartbrand</a> , who says “That&#39;s the way to run a culture”:</p><blockquote><p> NEW COLLEGE, OXFORD, is of rather late foundation, hence the name. It was founded around the late 14th century. It has, like other colleges, a great dining hall with big oak beams across the top. These might be two feet square and forty-five feet long.</p><p> A century ago, so I am told, some busy entomologist went up into the roof of the dining hall with a penknife and poked at the beams and found that they were full of beetles. This was reported to the College Council, who met in some dismay, because they had no idea where they would get beams of that caliber nowadays.</p><p> One of the Junior Fellows stuck his neck out and suggested that there might be some oak on College lands. These colleges are endowed with pieces of land scattered across the country. So they called in the College Forester, who of course had not been near the college itself for some years, and asked about oaks. And he pulled his forelock and said, “Well sirs, we was wonderin&#39; when you&#39;d be askin&#39;.”</p><p> Upon further inquiry it was discovered that when the College was founded, a grove of oaks has been planted to replace the beams in the dining hall when they became beetly, because oak beams always become beetly in the end. This plan had been passed down from one Forester to the next for five hundred years. “You don&#39;t cut them oaks. Them&#39;s for the College Hall.”</p></blockquote><p> <a href="https://tseliot.com/prose/francis-herbert-bradley">TS Elliot</a> :</p><blockquote><p> The combat may have truces but never a peace. If we take the widest and wisest view of a Cause, <strong>there is no such thing as a Lost Cause because there is no such thing as a Gained Cause.</strong> We fight for lost causes because we know that our defeat and dismay may be the preface to our successors&#39; victory, though that victory itself will be temporary;我们战斗是为了让某些事物保持活力，而不是期望任何事物都会取得胜利。</p></blockquote><h2><strong>图表</strong></h2><p>“Great graph in the latest <a href="https://worksinprogress.co/issue/how-mathematics-built-the-modern-world">Works in Progress headline piece</a> by Hannes Malmberg” ( <a href="https://twitter.com/antonhowes/status/1724843041024860389">@antonhowes</a> )</p><p> <a href="https://pbs.twimg.com/media/F-_flBTXoAE61g6.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/r3ebqzubhphckd9n4uht" alt=""></a></p><p> Progress in fiber optic transmission (via <a href="https://twitter.com/varma_ashwin97/status/1722662927650394282">@varma_ashwin97</a> ). I used to think the exponential advancement of Moore&#39;s Law was a unique and amazing phenomenon. Turns out exponential progress is everywhere (and not just in information technology):</p><p> <a href="https://pbs.twimg.com/media/F-ggzYxWsAAzPCc.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/verawzfpw1tb7dv3yzcx" alt=""></a></p><p> “60% of the time it happens 57% of the time. Manifold Markets is pretty well calibrated” ( <a href="https://twitter.com/NathanpmYoung/status/1725563206561607847">@NathanpmYoung</a> )</p><p> <a href="https://pbs.twimg.com/media/F_JuYePWoAAIvbL.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/orj5ets7uhthzdmplua7" alt=""></a></p><p> “Good news of the day: We&#39;ve reduced sulfur dioxide pollution by 94% over the last 40 years (and mostly solved the acid rain problem)” ( <a href="https://twitter.com/AlecStapp/status/1723162035687428592">@AlecStapp</a> )</p><p> <a href="https://pbs.twimg.com/media/F-nl-93WQAAHbML.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/upk7ascdda2nlnsagw1g" alt=""></a></p><p> “NEPA environmental reviews just get longer and longer and longer… (this trend is driven by litigation and will not stop without permitting reform)” ( <a href="https://twitter.com/AlecStapp/status/1723918780110143662">@AlecStapp</a> )</p><p> <a href="https://pbs.twimg.com/media/F-yWokSWoAA-uQn.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/wlgc77ovsdagjybctfxc" alt=""></a></p><ul><li> <a href="https://twitter.com/JakeAnbinder/status/1724123168124637251">@JakeAnbinder</a> adds, from his dissertation: “While in 1972 a member of SF&#39;s planning commission had complained that a 12-page impact statement in his inbox was intolerably verbose, just 4 years later a plan by the Univ of California to build two new dorms resulted in an EIS that ran 950 pages long”</li><li> <a href="https://twitter.com/rSanti97/status/1724121911502803251">@rSanti97</a> adds: “asymptotically, the NEPA review of the future will be infinite: a legal map the size of the territory, the Aleph in which all things can be seen. it will require more paper than can exist in all possible universes and it will never be completed”</li></ul><p> “One society where the suicide rate is highly correlated with the unemployment rate (Japan, red), and one society where it is not at all correlated (Spain, blue)” ( <a href="https://twitter.com/nick_kapur/status/1723849496256282956">@nick_kapur</a> )</p><p> <a href="https://pbs.twimg.com/media/F-r_OLHbwAAXOtJ.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/p3rh3toktma7vakzbnff" alt=""></a></p><p> “Where you can drink tap water is a fairly good economic indicator (GDP per capita). It roughly matches up with countries where GDP (PPP) per capita is at least US $22,000” ( <a href="https://twitter.com/pitdesi/status/1721583690340585594">@pitdesi</a> )</p><p> <a href="https://pbs.twimg.com/media/F-RJ61fakAA7mZz.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/m7ktics0ylajorkexz72" alt=""></a></p><h2><strong>美学</strong></h2><p>“Walked by 51st and Madison today to see our work, just installed on the Villard Houses. First addition to the NY landscape” ( <a href="https://twitter.com/mspringut/status/1721660498989449416">@mspringut</a> , founder of <a href="https://www.monumentallabs.co/">Monumental Labs</a> )</p><p> <a href="https://pbs.twimg.com/media/F-SRGRGXkAEhoWj.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/cldenm3vtwlooporsopr" alt=""></a></p><br/><br/> <a href="https://www.lesswrong.com/posts/8GThyjQ77BN7Q7vo7/progress-links-digest-2023-11-24-bottlenecks-of-aging#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8GThyjQ77BN7Q7vo7/progress-links-digest-2023-11-24-bottlenecks-of-aging<guid ispermalink="false"> 8GThyjQ77BN7Q7vo7</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:25:07 GMT</pubDate> </item><item><title><![CDATA[What's the evidence that LLMs will scale up efficiently beyond GPT4? i.e. couldn't GPT5, etc., be very inefficient?]]></title><description><![CDATA[Published on November 24, 2023 3:22 PM GMT<br/><br/><p> A lot of this recent talk about OpenAI, various events, their future path, etc., seems to make an assumption that further scaling beyond GPT4 will pose some sort of &#39;danger&#39; that scales up linearly or super-linearly with the amount of compute 。 And thus pose an extraordinary danger if you plug in 100x and so on.</p><p> Which doesn&#39;t seem obvious at all.</p><p> It seems quite possible that GPT5, and further improvements, will be very inefficient.</p><p> For example, a GPT5 that requires 10x more compute but is only moderately better. A GPT6 that requires 10x more compute than GPT5, ie 100x more than GPT4, but is again only moderately better.</p><p> In this case there doesn&#39;t seem to be any serious dangers at all for LLMs.</p><p> The problem self-extinguishes based on the fact that random people won&#39;t be able to acquire that amount of compute in the for-seeable future. Only serious governments, institutions, and companies, with multi-billion dollar cap-ex budgets will even be able to consider acquiring something much better.</p><p> And although such organizations can&#39;t be considered to be perfectly responsible, they will still very likely be responsible enough to handle LLMs that are only a few times more intelligent.</p><br/><br/> <a href="https://www.lesswrong.com/posts/hg9SvxBQw6KFNEzrH/what-s-the-evidence-that-llms-will-scale-up-efficiently#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hg9SvxBQw6KFNEzrH/what-s-the-evidence-that-llms-will-scale-up-efficiently<guid ispermalink="false"> hg9SvxBQw6KFNEzrH</guid><dc:creator><![CDATA[M. Y. Zuo]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:22:02 GMT</pubDate> </item><item><title><![CDATA[Sapience, understanding, and "AGI"]]></title><description><![CDATA[Published on November 24, 2023 3:13 PM GMT<br/><br/><p> <i>Epistemic status: I&#39;m sure that &quot;AGI&quot; has become importantly confusing. I think it&#39;s leaving out a critical set of capabilities. I think those are closely related, so acquiring them could create a jump in capabilities. I&#39;m not sure of the best term to disambiguate the type of AGI that&#39;s most dangerous, but I want to propose one that works decently</i><br></p><p> The term AGI has become muddled. It is now used for AI both with and without agency, contextual awareness, and the capacity to actively learn new facts and concepts. &quot;Sapience” means understanding, wisdom and self-awareness, so it could be adopted as a disambiguating term.  Understanding as humans perform it is an active process for testing and improving our knowledge. Understanding allows self-awareness and contextual awareness. It also implies agency, because our process of understanding is goal-directed.</p><p> I have three goals here. One is to point out how the term &quot;AGI&quot; is confusing x-risk discussions. The second is to discuss how human-like undestanding is poweful, achievable, and dangerous. Last and least I propose the specific term &quot;sapience&quot; for the set of powerful and dangerous capabilities provided by active understanding. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WqxGB77KyZgQNDoQY/v6qag0yeadyx7l3voc8h"></p><p> Understanding as an active process of testing and improving “fit” among concepts and world-models</p><h2> Sapience implies agency and understanding</h2><p> The concept of agency is not just a philosophical nicety; it&#39;s pivotal in discussions about existential risk (x-risk) related to AI. Without a clear understanding and explicit treatment of agency, these discussions have become confused and potentially misleading. The need for a new term is evident, given widely varying definitions of AGI, and resulting disagreements about risks and capabilities.</p><p> &quot;Sapience&quot; appears to be our most fitting option. The term is used in various ways with weak connections to its etymology of wisdom or discernment, but its most common usages are the ones we need. In the realm of science fiction, it&#39;s often employed to denote an intelligent, self-aware species distinct from humans. I think this is where you can potentially drum up more interest/comments.  We, as “Homo Sapiens”, pride ourselves as the &quot;understanding apes,&quot; setting ourselves apart from our evolutionary kin. This self-ascription may spring from vanity, but it invokes a critical cognitive capacity we commonly refer to as &quot;understanding.&quot;</p><h2>人类的理解</h2><p>The debate surrounding whether large language models (LLMs) truly &quot;understand&quot; their output is a persistent one. Critics argue that LLMs lack genuine understanding, merely echoing word usage without deeper cognitive processing. Others suggest that humans are largely “stochastic parrots” as well.  But we do more than parrot. If I use a term appropriately, that might evidence &quot;an understanding&quot; of it; but that is not what we usually mean by understanding. It is primarily a verb. Understanding is a process. We understand in an important, active sense that current LLMs lack. In everyday usage, &quot;understanding&quot; implies an active engagement with concepts.</p><p> To say &quot;I understand&quot; is to assert that one has engaged with a concept, testing and exploring it. This process is akin to mentally simulating or &quot;turning over&quot; the concept, examining its fit with other data and ideas. For instance, understanding how a faucet works might involve visualizing the mechanism that allows water to flow upon moving the handle. These mental simulations can vary in detail and abstraction, contingent on the concept and the criteria one sets for claiming understanding. If understanding seems out of reach, one might actively pursue or formulate new hypotheses about the concept, evaluating these for their potential to foster understanding.</p><p> Imagine for a moment that you&#39;ve just learned about the platypus. Someone&#39;s told you some things about it, and you&#39;re asking yourself if you understand. This active self-questioning is critical to our notion of understanding, and I think it is most easily achieved through what might be termed cognitive agency. You asked yourself whether you understood for a purpose. If there&#39;s no reason for you to understand the concept, you probably won&#39;t waste the time it would take to test your understanding. If you do ask, you probably have some criteria for adequate understanding, for your current purposes. That purpose could be curiousity, a desire to show that you understand things people explain to you, or more practical goals of deploying your new concept to achieve material ends.</p><p> To answer the question of whether you adequately understand, you&#39;ll use one or more strategies to test your understanding. You might form a mental simulation of a platypus, and imagine it doing things you care about. That simulation attempt might reveal important missing information - is a platypus prehistoric or current? Is it huge or small? You might guess that it swims if its webbed feet have been described. You might ask yourself if it&#39;s edible or dangerous if those are your concerns, and perform mental simulations exploring different ways you could hunt it, or it could hunt you.</p><p> If these simulations produce inconsistent results, either predicting things you know aren&#39;t true. Perhaps it sounds incongruous that you haven&#39;t already heard that there&#39;s a giant poisonous swimming animal, (if you got the size wrong) or perhaps imagining a furred animal laying eggs is recognized as incongruous. If these tests of understanding fail, you may decide to spend additional time trying to improve that understanding. You may ask questions or seek more information, or you may change your assumptions and try more simulations, to see if that produces results more congruent with the rest of your world knowledge.</p><p> I think it&#39;s hard to guess how quickly and easily these capacities might be developed for AI. Large language models appear to have all of the requisite abilities, and attempts to develop language model cognitive architectures that can organize those capacities into more complex cognition are in the early days. <span class="footnote-reference" role="doc-noteref" id="fnrefuq7sy0o13e"><sup><a href="#fnuq7sy0o13e">[1]</a></sup></span> If those capacities are as useful and understandable as I think, we might see other approaches also develop this capacity for active understanding.</p><h2> Understanding implies full cognitive generality</h2><p> This capacity for testing and enhancing one&#39;s understanding is largely domain-general, akin to assessing and refining a tool&#39;s design. Consequently, this active ability to understand includes a capacity for self-awareness and contextual awareness.  Such a system can develop understanding of its own cognitive processes, and its relation to the surrounding world. It also suggests a level of agency, at least in the functional sense of pursuing the goal of refining or enhancing understanding. These abilities may be so closely related that it would be be harder to create an AGI that has some but not all of them.</p><p> There are probably other paths to adequate functional understanding. LLMs do not need a human-like active process of testing and expanding their understanding to use concepts remarkably effectively. I think it will prove relatively easy and effective to add such an active process to agentic systems. If we do first create non-agentic AGI, I think we&#39;ll quickly make it agentic and self-teaching, since those capacities may be relatively easy to &quot;bolt on&quot;, as language model cognitive architectures do for LLMs. I think the attainment of active understanding skills was probably a key achievement in accelerating our cognition far beyond our immediate ancestors, and I think the same achievement is likely to accelerate AI capabilities.</p><p> Leaving aside the above theories of why AGI might take a human-like route to understanding, “sapience” still seems like an effective term to capture an AGI that functionally understands itself and its context, and can extend its functional understanding to accomplish its goals.</p><h2> “AGI” is a dangerously ambiguous term</h2><p> AGI now means AI that has capabilities in many domains. But the original usage included agency and the capacity for self-improvement. A fully general intelligence doesn&#39;t merely do many things; it can teach itself to do anything. We apply agency to improve our knowledge and abilities when we actively understand, and AGI in its fullest and most dangerous sense can too. Sapient AI is not just versatile across various domains but capable of understanding anything—including self-comprehension and situational awareness. Understanding one&#39;s own thinking offers another significant advantage: the capability to apply one&#39;s intelligence to optimize cognitive strategies (this is independent of recursive self-improvement of architecure and hardware). An AI that can actively understand can develop new capabilities.</p><p> The term sapience calls to mind our self-given title of Homo Sapiens. This title refers to the key difference between us and earlier hominids, and so sapience suggests an analogous difference between current narrow AI and successors that are better at understanding. Homo Erectus was modestly successful, but we Sapiens took over the world (and eliminated many species without malice toward them). The term invokes our rich intuition of humans as capable, dangerous, and ever-improving agents, hopefully without too many intuitions about specific human values (these are more closely connected to the older Homo aspect of our designation).</p><p> As it&#39;s used now, the term AGI suffers from an ambiguity that&#39;s crucial in x-risk discussions. It&#39;s now used for an intelligence capable of performing a wide array of tasks, whether or not that intelligence is agentic, self-aware, or contextually aware. This usage seems to have slipped in from the term&#39;s wider adoption by more conventionally minded pundits, economists, and technologists; they often assume that AI will remain a technology. <a href="https://www.lesswrong.com/posts/nZYhs48pWsaCCgGfi/agi-isn-t-just-a-technology"><u>But AGI isn&#39;t just a technology</u></a> if it&#39;s agentic. The distinction might seem unimportant in economic contexts, but it&#39;s crucial in x-risk discussions. The other existing terms I&#39;ve found and thought don&#39;t seem to capture this distinction as sapience and/or they carry other unwanted implications and intuitions. <span class="footnote-reference" role="doc-noteref" id="fnrefnkvvttfu0j"><sup><a href="#fnnkvvttfu0j">[2]</a></sup></span></p><p> When your terms are co-opted, you can fight to take them back, or shift to new terms. I don&#39;t think it&#39;s wise to fight, because fighting for terminology doesn&#39;t usually work, and more importantly because <a href="https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs"><u>fighting causes polarization</u></a> <u>. C</u> reating more rabid anti-safety advocates could be very bad (if many experts are loudly proclaiming that AGI doesn&#39;t carry an existential risk, policymakers may believe and cite whichever group suits their agenda).</p><p> Therefore, it seems useful to introduce a new term. Different risk models apply to sapient AI than to AGI without the capacities implied by sapience. By distinguishing these we can deconfuse our conversations, and more clearly convey the most severe risks we face. Artificial sapience (AS) or sapient AI (SAI) is my proposed terminology.</p><p> Disambiguating the terms we use for transformative AI seems like a pure win. I&#39;m not sure if sapience is the best term to do that disambiguation, and I&#39;d love to hear other ideas. Separately, if I&#39;m right about the usefulness of active understanding, we might expect to see a capabilities jump when this capacity is achieved. </p><p><br></p><p></p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnuq7sy0o13e"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuq7sy0o13e">^</a></strong></sup></span><div class="footnote-content"><p> I confess to not knowing exactly how those complex processes work in the brain, but the cognitive outlines seem clear. I have some ideas on how to biological networks might naturally capture incongruity between successive representations, and <a href="https://scholar.google.com/scholar?cluster=2324158456471969700&amp;hl=en&amp;as_sdt=0,6">a detailed theory</a> about how the necessary decision-making works. But these active understanding processes are somewhat different than and include a separate component of recognizing incongruity that&#39;s not needed for even for decision-making and planning in complex domains. I think these are hard-won cognitive skills that we practice and develop during our educations. Our basic brain mechanisms provide an adequate basis for creating mental simulations and testing their congruity with other simulations, but we probably need to create skills based on those mechanisms.</p><p> Current LLMs seem to possess the requisite base capabilities, at least in limited form. They can both create simulations in linguistic form, and make judgments about the congruity of multiple statements. I&#39;m concerned that we&#39;re collectively overlooking how close language model cognitive architectures might be to achieving AGI. I think that a hard-coded decision-tree using scripted prompts to evaluate brenches could provide that skill that organizes our capacities into active understanding. Perhaps GPT4 and current architectures aren&#39;t quite capable enough to get  much traction in such an iterative process of testing and improving understanding. But it seems entirely plausible to me that GPT5 or GPT6 might have that capacity, when combined with improved and elaborated episodic memory, sensory networks, and coded (or learned) algorithms to emulate this type of strategic cognitive sequencing. I discuss the potentials of language model cognitive architectures (a more complete term for language model agents) <a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures">here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnkvvttfu0j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnkvvttfu0j">^</a></strong></sup></span><div class="footnote-content"><p> One possible alternative term is superintelligence. I think this does intuitively imply the capacity for rich understanding and extension of knowledge to any domain. But it does not firmly imply agency. More importantly, it also conveys an intuition of being more distant than the first self-aware, self-teaching AI. Artificial sentience does not imply better-then human but merely near-human cognitive abilities. Parahuman AI is another possible term, but I think it too strongly implies human-like, while not pointing as clearly at a capacity for rich and extensible understanding. More explicit terms, like self-aware agentic AGI (SAAAAI?;) seem clumsy. Artificial sentience is another possibility, but sentience is more commonly used for having moral worth by virtue of having phenomenal consciousness and “feelings”. Avoiding those implications seems important for clear discussions of capabilities and x-risk. Agentic AGI might be adequate, but it leaves out the implication of active understanding, contextual awareness and goal-directed learning. But I&#39;m not sure that artificial sapience is the best term, so I&#39;m happy to adopt another term for roughly the same set of concepts if someone has a better idea.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi<guid ispermalink="false"> WqxGB77KyZgQNDoQY</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:13:04 GMT</pubDate> </item><item><title><![CDATA[Insulate your ideas]]></title><description><![CDATA[Published on November 24, 2023 2:08 PM GMT<br/><br/><p> Once tested and proven, insulate good ideas from reality.</p><p></p><p><i>跑步</i></p><p>I recently spoke with a very successful marathon runner who commonly wins races and consistently beats his own times. I asked for his tips, and he shared two:</p><blockquote><p> “The first thing I do is run faster on the uphills. Everyone else slows down because of how difficult it is, and this is a great time to gain ground. The second thing I do is run faster on the downhills. Everyone else slows down because of how fast you go anyways on the slope down, and this is a great time to gain ground.”</p></blockquote><p> I was looking for actual tips and this person (not even concisely) told me it&#39;s just about running faster.</p><p> Before passing judgment too quickly and writing off this feedback, I paused. Amusingly, the point was not to run faster, but more so, that the <strong>uphill and downhill should have no impact on how you think about the run</strong> .</p><p> You know you need to <strong>run fast, consistently</strong> , for the whole marathon.</p><p> This is a central note that cannot possibly be wrong; it is a “good idea,” and it is correct. It&#39;s not overly useful as a tactic, but it is the rejection of the notion that the hill changes anything at all.</p><p> Regardless of these external hill stimuli, the ground truth is identical: <strong>run as fast as you can.</strong></p><p></p><p><i>商业</i></p><p>I listened to a shareholder meeting for a tech company in the heat of the <a href="https://techcrunch.com/2023/05/01/tech-industry-layoffs/">tech recession of 2023</a> . The CFO was being questioned by the shareholders about his plan going into unstable economic conditions.</p><p> We heard questions about imminent layoffs, changes in strategy, refocusing on core business, and the list continues. The CFO (who I&#39;m paraphrasing here) replied with a terse note:</p><blockquote><p> “We will do as we have always done whether it was through the good times or the bad. We will continue with the disciplined deployment of capital, allocating resources to where it is most needed and most likely to generate substantive returns.”</p></blockquote><p> Similar to the runner, I felt indignant at the fact that no real insight was shared. I was wrong again for the exact same reason.</p><p> In the natural cycles of the market, businesses and <strong>executives busy themselves with the minutia of reacting</strong> to market conditions, changing plans, adjusting strategies, and levying layoffs. In the most recent boom through the pandemic, tech companies saw extreme growth and over-hired.</p><p> When things slowed and shareholders questioned productivity, drastic changes were made yet again. Large-scale lay-offs, auxiliary services being cut, and a “year of efficiency”, are all reactionary measures to an ever-changing economic landscape.</p><p> However, in this meeting, the CFO highlighted their <strong>disregard for external stimuli and instead employed a consistently valuable (and diligent) strategy.</strong></p><p> Regardless of these external economic stimuli, the ground truth remains identical: <strong>be disciplined in your deployment of capital.</strong></p><p></p><p><i>想法</i></p><p>Ideas need maturation, and that aging process involves taking an idea and foisting it into reality <strong>where it faces friction</strong> . Weak ideas are brittle and snap, whereas strong ideas come out even stronger.</p><p> Increasingly, the swings of trends and fads are bringing massive changes to ideas and decision-making. Significant time and energy is spent focused on minute optimizations of a new process, <strong>rather than recognizing the timelessness of core ideas.</strong></p><p> A good idea should not fluctuate with the changing weather, but should instead be insulated against it. Inside, behind thick walls and locked doors. Ready to mobilize against the world, heedless to the external stimuli that only exist in the ephemeral territory of the immediate.</p><p>快跑。遵守纪律。 <strong>Good ideas are good: now insulate them.</strong></p><br/><br/><a href="https://www.lesswrong.com/posts/ArypKcebwkQXGohdA/insulate-your-ideas#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ArypKcebwkQXGohdA/insulate-your-ideas<guid ispermalink="false"> ArypKcebwkQXGohdA</guid><dc:creator><![CDATA[Logan Kieller]]></dc:creator><pubDate> Fri, 24 Nov 2023 14:08:12 GMT</pubDate> </item><item><title><![CDATA[Bordeaux, Gironde, France – irregular ACX Meetup 2023-12-09]]></title><description><![CDATA[Published on November 24, 2023 11:17 AM GMT<br/><br/><p>今年在波尔多大都会（法国吉伦特省）举办的 Meetups Everywhere 的后续活动。这次是在波尔多市中心。</p><p>日期和时间：2023 年 12 月 9 日星期六 14:00（当地时间）。</p><p>地点：Espalanade Charles de Gaulle（Mériadeck 中部的地上公园），位于喷泉和 Hôtel du Département 之间<a href="https://www.openstreetmap.org/#map=19/44.83732/-0.58601">https://www.openstreetmap.org/#map=19/44.83732/-0.58601</a> ，周六，管理部门应关闭了，所以我们可以使用入口前开放但有屋顶的露台作为下雨时的集合点。</p><p>如果您预计会迟到并希望在我们决定转移方向时收到短信，请向我发送您的电话号码。我会带一个 A4 标牌，上面写着“ACX Meetup”。我通常在陌生人密度高的环境中戴口罩。</p><p>请在 LessWrong 上回复，以便我知道确实有人会来。这将是一个小型聚会，没有固定的主题，所以无论你想讨论什么，我们都可以讨论！</p><p>将结束时间作为最低承诺选项：一个小时后，我们将看到谁想要结束，如果需要在全体聚会结束时快速完成或说任何事情，我们会在那时做。如果有人想留下来，我几乎肯定会多呆至少一个小时，而且我个人会尽量灵活安排整个下午的时间。</p><p>很可能每个人都可以使用英语和法语，我们将根据表达的偏好选择语言。</p><br/><br/> <a href="https://www.lesswrong.com/events/P6FdNiZT4D8fepsNG/bordeaux-gironde-france-irregular-acx-meetup-2023-12-09#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/P6FdNiZT4D8fepsNG/bordeaux-gironde-france-irregular-acx-meetup-2023-12-09<guid ispermalink="false"> P6FdNiZT4D8fepsNG</guid><dc:creator><![CDATA[vi21maobk9vp]]></dc:creator><pubDate> Fri, 24 Nov 2023 11:17:36 GMT</pubDate> </item><item><title><![CDATA[A Question For People Who Believe In God]]></title><description><![CDATA[Published on November 24, 2023 5:22 AM GMT<br/><br/><p>看到这个视频我感到非常惊讶。安德鲁·休伯曼在其中谈到了他如何祈祷“因为它有效”以及“他相信上帝”，因为他无法想象地球上的生命还能有现在的样子。</p><p>无论如何，这让我思考，当有人说他们“相信上帝”时，这是否意味着“我认为存在一种无所不能、无所不在、无所不知的智能的可能性≥50%？”</p><p>在这种情况下“相信”某事意味着什么？ </p><p></p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Z7GVf8nD7SQ&amp;t=241s"><div><iframe src="https://www.youtube.com/embed/Z7GVf8nD7SQ" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/posts/piavPtGaiAtXQDFLL/a-question-for-people-who-believe-in-god#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/piavPtGaiAtXQDFLL/a-question-for-people-who- believe-in-god<guid ispermalink="false"> piavPtGaiAtXQDFLL</guid><dc:creator><![CDATA[yanni]]></dc:creator><pubDate> Fri, 24 Nov 2023 05:32:35 GMT</pubDate></item></channel></rss>