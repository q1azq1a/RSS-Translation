<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 19 日星期四 14:11:43 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Is Yann LeCun strawmanning AI x-risks?]]></title><description><![CDATA[Published on October 19, 2023 11:35 AM GMT<br/><br/><p> Tom Chivers 表达了他对 Yann LeCun 的不满： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/ay5hqldsvgcyos6u0ykw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/epxt3pkzbh5pwhplbaof 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/wapfsll4jiw5zwtebhxp 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/nmloymoimfrfrme6d7az 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/qhbn7lxjr7k894avpdtd 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/lm2kuaw5lwgdewihvp5m 410w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/myx3b07p6p5owhl9y4uu 490w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/tjsquqzucvqymi4zha5d 570w"></figure><p>我也发现他的评论令人沮丧，但我想提供另一种可能的解释。<br><br>尽管人工智能安全社区中基本上没有人提出这一论点，但不幸的是，普通大众中的许多人都是这样看待人工智能的。</p><p> Yann 可能认为，对他来说，解决更多人持有的这一信念比解决人工智能安全人群的争论更重要。</p><p>他可能认为，如果他专注于解决最佳论点，他就会因为天真的人们相信“稻草人”论点而输掉政治斗争。</p><p>有鉴于此，我认为说他是在向稻草人讲话并不完全准确。我也觉得这很令人沮丧，我希望他能直接向我们讲话。但我也理解促使他做他所做的事情的动机。</p><br/><br/> <a href="https://www.lesswrong.com/posts/gYwfkjK8vsowfDkZu/is-yann-lecun-strawmanning-ai-x-risks#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gYwfkjK8vsowfDkZu/is-yann-lecun-strawmanning-ai-x-risks<guid ispermalink="false"> gYwfkjK8vsowfDkZu</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Thu, 19 Oct 2023 11:35:09 GMT</pubDate> </item><item><title><![CDATA[Announcing new round of "Key Phenomena in AI Risk" Reading Group]]></title><description><![CDATA[Published on October 19, 2023 11:05 AM GMT<br/><br/><p> <strong>TLDR：</strong> “ <a href="https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading">人工智能风险的关键现象</a>”是一个为期 8 周的辅助阅读小组。它针对的是对概念人工智能一致性研究感兴趣的人们，特别是来自哲学、系统研究、生物学、认知和社会科学等领域的人们。我们运行过一次，现在正在重复。</p><p>该计划将于<strong>2023 年 11 月至 2024 年 1 月</strong>期间运行。请于 10 月 29 日（星期日）之前<a href="https://forms.gle/cdr4UeocE7Jg5SUF6"><strong>在此</strong></a>注册<strong>。</strong></p><h2><strong>什么？</strong></h2><p> “人工智能风险中的关键现象”阅读课程对人工智能风险中的一些关键思想进行了深入介绍，特别是来自误导性优化或“后果主义认知”的风险。因此，它的目标是在很大程度上保持对解决方案范例的不可知性。它包括 90 分钟的引导讨论，并且每次会议需要至少 2 小时的阅读时间。它是虚拟的且免费的。</p><p>请参阅<a href="https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading#Summary_of_the_curriculum"><u>此处的旧帖子，</u></a>了解课程的简短概述； <a href="https://docs.google.com/document/d/1hgZOv-PfYYgayspSb_8D_OdQ6dV12xI2bsLuq57A3yg/edit?usp=sharing"><u>这里</u></a>有更广泛的总结；在<a href="https://docs.google.com/document/d/1HGzMBMXQD9w9K32scqCoSmZNGbxLJE8-siPlonTQz6s/edit?usp=sharing"><u>这里</u></a>查看完整的课程（将在接下来的几周内进行小幅更新）。</p><h2><strong>发生了什么变化？</strong></h2><p>由于上次迭代中参与者和协调人的反馈，该计划得到了改进。现在是一个为期8周的项目（最后增加了一周进行反思）。阅读材料更加集中，我们将添加更多技术性可选阅读材料。</p><h2><strong>为了谁？</strong></h2><p>该课程主要针对对人工智能风险和一致性概念研究感兴趣的人们。</p><p>它旨在让哲学（代理、知识、权力等）和系统研究（例如生物学、认知、信息论、社会系统等）等领域的受众能够理解。</p><h2><strong>什么时候？</strong></h2><p>阅读小组将于<strong>2023 年 11 月至 2024 年 1 月举行。</strong></p><p>我们预计将进行 6 组，每组 4-8 名参与者（包括 1 名协调员）。每个小组将由一位对人工智能风险有深入了解的主持人领导。</p><h2><strong>报名</strong></h2><p>请<strong>于 10 月 29 日之前</strong><a href="https://forms.gle/isv2ZeTkffjRBdYM8"><strong><u>在此</u></strong></a>注册。</p><h2><strong>关于申请</strong></h2><p>该申请包括一个阶段，我们要求您填写一份表格，其中包含</p><ul><li>你的简历</li><li>您参与该计划的动机</li><li>您之前接触过的人工智能风险/迄今为止的调整情况</li></ul><p>我们根据对他们为人工智能做出贡献的动机以及他们将从参与该计划中获得多少反事实收益的最佳理解来选择人员。</p><p></p><hr><p></p><p>如果您有任何疑问，请随时在下面发表评论或通过<u>contact@pibbss.ai</u>联系我们</p><br/><br/><a href="https://www.lesswrong.com/posts/vakhhNHduW9gmENTW/announcing-new-round-of-key-phenomena-in-ai-risk-reading#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/vakhhNHduW9gmENTW/announcing-new-round-of-key-phenomena-in-ai-risk-reading<guid ispermalink="false"> vakhhNHduW9gmentENTW</guid><dc:creator><![CDATA[DusanDNesic]]></dc:creator><pubDate> Thu, 19 Oct 2023 11:05:55 GMT</pubDate> </item><item><title><![CDATA[[Video] Too much Empiricism kills you]]></title><description><![CDATA[Published on October 19, 2023 5:08 AM GMT<br/><br/><p><a href="https://youtu.be/vqHlPb18ROE?si=vf840i97GZwgxIjD">这</a>是我两个月前制作的视频。它对一个重要的基础论点给出了平庸的解释：</p><p>只要您能够衡量特定变化的效果，通常就可以使用经验方法取得进展。即使您并不真正理解&lt;您正在做什么/您正在构建的系统>;，这一点仍然成立。这解释了为什么研究人员可以提高机器学习的能力，即使他们基本上根本不了解当前深度学习系统的内部结构。</p><p>我还认为，理解方面的任何进步都可能是危险的，因为它通常会提高可以通过经验方法有效探索的事物的前沿。由此推论，机械可解释性可以使能力的提升变得更容易。</p><p>这个论点概括得非常广泛。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tqDT8CCm4jubaWkC3/video-too-much-empiricism-kills-you#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tqDT8CCm4jubaWkC3/video-too-much-empiricism-kills-you<guid ispermalink="false"> tqDT8CCm4jubaWkC3</guid><dc:creator><![CDATA[Johannes C. Mayer]]></dc:creator><pubDate> Thu, 19 Oct 2023 05:08:10 GMT</pubDate> </item><item><title><![CDATA[Are humans misaligned with evolution?]]></title><description><![CDATA[Published on October 19, 2023 3:14 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:01:11 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:01:11 GMT" user-order="1"><p>有一种观点认为，尽管人类在最大化包容性遗传适应性 (IGF) 的压力下进化，但人类实际上并没有尝试最大化自己的 IGF。正如论证所言，这表明，在我们创建通用智能的过程的一种情况下，所创建的智能的优化目标最终与创建它的过程的优化目标并不相同。 。因此，默认情况下不会发生对齐。引用<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">《人工智能中心对齐问题：能力泛化和急速左转</a>》：<br></p><blockquote><p>在[人工智能]能力飞跃发展的同时，它的对齐属性也被揭示为肤浅的，并且无法泛化。这里的核心类比是，优化猿类以实现包容性遗传适应性 (IGF) 并不会使人类在精神上针对 IGF 进行优化。当然，类人猿吃东西是因为它们有饥饿本能，做爱是因为感觉良好，但它们<i>不可能</i>因为这些活动如何导致更多 IGF 而吃东西/通奸。他们还无法执行抽象推理来正确地根据 IGF 来证明这些行为的合理性。然后，当它们开始以人类的方式很好地概括时，可以预见的是，它们不会<i>因为</i>关于 IGF 的抽象推理而<i>突然开始</i>进食/通奸，尽管它们现在<i>可以</i>。相反，他们发明了避孕套，如果你试图消除他们对美食的享受，他们就会与你战斗（告诉他们只需手动计算 IGF）。在功能开始泛化之前您所称赞的对齐属性，可以预见的是无法与功能一起泛化。</p></blockquote><p>雅各布发表了<a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn">《进化解决的对齐》（什么是急速左转？）</a> ，认为人类实际上代表了伟大的对齐<i>成功</i>。进化试图创造出能够自我复制的东西，而人类在这个指标上取得了巨大的成功。引用雅各布的话：</p><blockquote><p>对于人类智能的进化来说，优化器就是进化：生物自然选择。效用函数类似于适应性：前基因复制计数（人类定义基因） <a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn#fn-ZfBacxFa8jjFpbJvN-1"><sup>[1]</sup></a> 。从任何合理的标准来看，人类显然都取得了巨大的成功。如果我们进行标准化，使效用分数 1 代表轻微的成功 - 类人猿物种的典型抽签预期，那么人类的分数要高出 4 OOM 以上，完全超出了图表。 <a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn#fn-ZfBacxFa8jjFpbJvN-2"><sup>[2]</sup></a></p></blockquote><p>我反驳道：</p><blockquote><p>对齐的失败可以从以下事实证明：在给予他们可用的机会的情况下，人类非常非常明显地无法最大化其基因在下一代中的相对频率；他们常常意识到这一点；无论如何，他们经常选择这样做。</p></blockquote><p>我们陷入了混乱的讨论。现在我们在这里进行对话。我希望其他人能够发表评论并澄清相关观点，并且也许不是我的人会在讨论中接替我（如果有兴趣，请给我发消息/评论）。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:22:02 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:22:02 GMT" user-order="1"><p>我将尝试从我的角度总结辩论的状况。<br><br>有两种工艺。<br><br>第一个是我所说的一般进化论。一般进化是这样一个过程，随着时间的推移，任何类型的模式变得更加普遍，最终将占据主导地位。因为某些模式和模式的聚合可以使自己变得更加常见；例如，一个能够自我复制的有机体、一个被复制的基因、一个有毒的模因。这些模式可以“组合起来”，例如有机体中的基因或模因复合体中的模因，并且可以对它们进行调整，以便它们能够更好地使自己变得更常见。因此，我们在周围看到的是模式和模式的聚合，它们非常擅长使自己变得更加普遍。如果我们认为一般进化论具有效用函数，那么它的效用函数就是这样的：应该有一些东西可以复制自己。<br><br>第二种过程我称之为谱系进化。对于今天活着的每个物种 S，都有一个称为“S 进化”的谱系进化，从第一个生命形式，沿着 S 所在的分支，沿着生命的系统发育树，通过 S 的每个祖先物种，直到 S 本身。<br><br> “人”也有两种含义。 “人类”可以指人类个体，也可以指整个人类。<br><br>我读到的原始论点是这样说的：人类进化（谱系进化的一个实例）选择了生物体的 IGF。也就是说，在人类进化的每一步中，人类进化都促进了创造人类（或人类祖先物种生物体）的基因，这些基因擅长使该生物体中的基因在下一代中更加常见。如今，大多数个体人类并不会做出任何明确、偏执地试图推广自己基因的事情。因此，这是一个错位的例子。<br><br>我认为，虽然我还很不确定，但雅各布实际上基本上同意这一点。我想雅各布想说的是<br><br>1. 人性是失调问题的正确主题；<br> 2. 一般进化论是正确的主题；<br> 3. 人类与普遍进化论非常一致，因为人类有很多，而普遍进化论希望有一些模式可以创造出许多人。<br><br>我目前的主要回复是：<br><br>一般进化不是合适的主题，因为设计人类的绝大多数优化能力都是人类进化。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:30:08 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:30:08 GMT" user-order="1"><p>我认为，如果你写几句话来阐述你的顶级案例，并根据我们迄今为止的背景重新表述，这可能会对我有所帮助。像这样的句子<br><br>“对于（错误）对齐的隐喻，相关层面是人类，而不是个体人类。”<br><br>和类似的。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 18:51:31 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 18:51:31 GMT" user-order="2"><blockquote><p>对齐的失败可以从以下事实证明：人类非常非常明显地无法在下一代中最大化其基因的相对频率（考虑到他们可以获得的机会）</p></blockquote><p>这是无关紧要的——个体失败“在下一代中最大化其基因的相对频率”是大多数物种在个体水平上的预期结果。在许多物种中，只有一小部分个体能够繁殖。对于人类来说，女性的这一比例超过 50%，但男性的比例可能低于 50%。</p><p>进化是通过随机变异和选择进行的——许多实验是并行进行的，只有其中一些会成功——<i>是有设计的</i>。失败是进步所<i>必需的</i>。</p><p>随着时间的推移，进化只是为了适应度而优化——遗传模式的数量/测量，在某些遗传模式集上定义。如果你试图测量一个人的基因，你会得到 IGF——因为该人的基因模式必然会与其他人重叠（与密切相关的亲属密切相关，重叠随着距离的增加而逐渐消失，等等）。同样，您可以测量更大种群直至物种水平的适应性。</p><p>给定具有不同效用函数的两个优化过程，您也许可以将对齐程度测量为两个函数在世界状态（或未来世界轨迹分布的期望）上的点积。<br><br>但我们无法直接测量作为优化器的进化和作为优化器的大脑之间的一致性 - 即使我们知道如何明确定义进化的优化目标（适应度），大脑的优化目标是一些复杂的个体变化的适应度代理- 比任何简单的功能都要复杂得多。此外，对齐程度本身并不是真正有趣的概念，除非标准化到某个相关的尺度（以设置成功/失败的阈值）。</p><p>但鉴于我们知道效用函数之一（进化：适应度），我们可以大致衡量总影响。当今的世界很大程度上是人脑优化的结果——也就是说，它是针对代理效用函数而不是真正的效用函数优化的最终结果。因此，错位只有一个有用的阈值：根据人类适应性的效用函数，当今世界（或最近的历史）效用是高、低还是零？</p><p>答案<i>显然</i>是高实用性。因此，任何偏差的净影响都很小。</p><p>如果 E(W) 是进化效用，B(W) 是大脑效用，我们有：<br><br> W[T] = opt(W[0], B(W))</p><p> E(W[T]) = 大<br><br>（世界是根据大脑（代理效用）优化的，而不是遗传适应度效用，但当前世界根据遗传适应度效用得分非常高，从而限制了任何错位）。</p><p> TechneMarke 认为，大多数产生进化压力的大脑是在物种内水平上，但这与我的论点无关，<i>除非</i>TechneMarke 实际上相信并能够证明这会导致不同的正确进化效用函数（除了适应度）<i>并且</i>根据该功能，人类得分较低。</p><p>打个比方：公司主要是为了利润而优化，而大型企业的大多数创新源于公司内部竞争这一次要事实与公司主要为了利润而优化这一主要事实无关。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 19:02:37 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 19:02:37 GMT" user-order="2"><p>总而言之，进化有几种可能的水平&lt;->;大脑排列：</p><ul><li>物种：大脑的排列（总体）和物种水平的适应性</li><li>个体：个体大脑和个体 IGF 的对齐</li></ul><p>我们似乎都同意个体一致性具有高方差——有些个体与 IGF 强烈一致性，而另一些则根本不一致性。我希望我们同意，在物种层面上，迄今为止，人类已经与适应度保持了良好的一致性——正如我们巨大的异常高的适应度分数所证明的那样（可能是有史以来任何物种适应度增长最快的）。</p><p>所以对于这样的声明：</p><blockquote><p>这里的核心类比是，优化猿类以实现包容性遗传适应性 (IGF) 并不会使人类在精神上针对 IGF 进行优化。</p></blockquote><p>如果你将“人类”理解为个体人类，那么这个陈述是正确的，但无趣（因为进化不会也不可能使每个个体都获得高分）。如果你把人类理解为人类，那么进化显然（对我来说）成功地使人类充分对齐，但在（心理优化）到底意味着什么的细节上仍然可能存在分歧，这将导致关于计算机的计算极限的侧面讨论。 20W 计算机以及如何间接优化代理是生产能够最大化预期 IGF 的计算机的最佳解决方案，而不是一些无法扩展且严重失败的简单的最大效用结果主义推理机。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 19:23:12 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 19:23:12 GMT" user-order="1"><blockquote><p>进化的不同正确效用函数（除了适应度） <i>，并且</i>根据该函数，人类得分较低。</p></blockquote><p>嗯。我认为这里的框架可能掩盖了我们的分歧。我想说：人类是通过多次迭代选择 IGF 的过程构建的。现在，人类正在对事物进行优化，但他们绝对没有针对 IGF 进行优化。<br><br>我认为你所说的是一般类别的内容：“当然，但你在这里使用的概念不是联合雕刻。如果你通过进化来看待人类的创造，然后你会想到非-联合雕刻概念，然后你会看到错位。但这只是实际设计过程的非联合雕刻子集和所设计的东西的非联合雕刻子集之间的错位。”<br><br>我想……好吧，但这个类比似乎仍然成立？<br><br>就像，如果我想到人类试图创造人工智能，我不觉得我是在谈论“所有试图创造人工智能的人类的效用函数”。我想我想谈论的是“人类试图创造人工智能的标准，或者用于梯度下降或强化学习的目标函数，具体地使用日常来选择他们的设计中要保留的调整/想法大约”。因此，这里有两个类比，但形式相同。<br><br>其中一个类比是：人类四处寻找人工智能的想法；如果他们的人工智能做了一些很酷的事情，他们就会投票；如果可以的话，他们会尝试调整人工智能来做一些很酷的事情，以便它可以用来真正做一些对人类有用的事情；当人工智能做了一些明显不好的事情时，人类会尝试修补它。人类可能认为自己通过自己的行为实现了自己的效用函数，更重要的是，他们实际上可能在某种意义上这样做了。如果人类可以继续修补不好的结果并支持很酷的结果并安装用于良好的补丁，那么在这种情况下，真正的人类效用函数将得到表达和实现。但这里的类比是说：人类用来设计人工智能的这些标准，进行调整，在人类人工智能研究的过程中，这些标准将加起来形成真正强大的人工智能，可以制造出真正强大的人工智能，而无需在人工智能中安装限制真实的人类效用函数。同样，如果有足够的进化时间，进化将在人类身上安装更多战略性的 IGF 优化；这可能已经发生了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 19:32:04 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 19:32:04 GMT" user-order="1"><p>换句话说，对于我来说这个类比似乎是正确和重要的，它是效用函数与效用函数错位的明显例子，这并不让人感觉很棘手。感觉棘手的是一种不太精确的感觉：这个过程通过为 X 进行非常困难的选择来设计优化器，但优化器最终尝试执行与 X 不同的 Y。<br><br>就像，如果我通过模仿人类的目标函数来训练人工智能，我希望最终，当它变得非常出色时，它将成为一个优化器，可以针对模仿人类以外的其他事物进行强大的优化。<br><br>对我们来说，症结可能与我们对未来预期结果的关心程度有关。我想你在某些评论中说过，“是的，也许人类/人类未来会与进化更加不一致，但那是猜测和循环推理，它还没有发生”。但我不认为这是循环的：我们<i>今天</i>可以清楚地看到人类<i>正在</i>针对事物进行优化，并说他们正在针对事物进行优化，而这些事物<i>不是</i>IGF，因此可以预见的<i>是</i>，当人类拥有力量时，我们将结束IGF 得分非常<i>低</i>；如今，这种失调更加模糊，必须根据反事实进行判断（<i>如果</i>现代人是 IGF 最大化者，他们<i>可以获得</i>多少 IGF）。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 20:15:13 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 20:15:13 GMT" user-order="2"><blockquote><p>现在，人类正在对事物进行优化，但他们绝对没有针对 IGF 进行优化。</p></blockquote><p>我们可能对此仍存在分歧 - 我要重申，在个人层面上，有些人肯定会针对 IGF 进行强烈优化，直至 20W 物理计算机的限制（这排除了大多数基于对优化的物理限制的严重误解的反对意见） 20W不可逆计算机的功率）。我已经在我们的私人讨论中提出了一个具体的例子，即个人会付出巨大的努力来最大限度地成功捐献精子，即使他们的报酬微不足道，或者根本没有报酬，在某些情况下实际上犯下了长期监禁的重罪。这样做（强烈反付费）。此外，大脑<i>通常的</i>工作方式更接近于神秘地潜意识地迫使你针对 IGF 进行优化——以埃隆·马斯克为例：他正朝着非常高的 IGF 分数前进，但似乎并没有明确有意识地为此进行优化。大脑中的排列非常复杂，显然还没有完全理解——但它是高度冗余的，有许多级别的机制在发挥作用。</p><p>因此，潜在的困惑之一是，我确实相信，正确理解和确定“优化 IGF，达到 20W 物理计算机的极限”实际上需要深入了解 DL 和神经科学，并导致诸如<a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">分片理论</a>之类的东西。 Nate 似乎暗示那种结果主义优化器在 20W 时惨败，并且与成功的设计（如大脑）的外观没有太大关系 - 它始终是一个超复杂的代理优化器，ala 分片理论和相关。<br><br>完美的对齐是一个神话，一个幻想 - 显然对于成功来说是不必要的！ （这就是这个类比的大部分教训）</p><blockquote><p>就像，如果我想到人类试图创造人工智能，我不觉得我是在谈论“所有试图创造人工智能的人类的效用函数”。我想我想谈论的是“人类试图创造人工智能的标准，或者用于梯度下降或强化学习的目标函数，具体地使用日常来选择他们的设计中要保留的调整/想法大约”。</p></blockquote><p>我确实相信，在进化中可能的对齐类比中，有一个最好的类比：系统级类比。</p><p>基因进化优化产生大脑就像技术进化优化产生AGI一样。</p><p>这两个过程都涉及双层优化：外部遗传（或模因）进化优化过程和内部人工神经网络优化过程。实用的 AGI 必然非常像大脑（我提前很多年就正确<a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine) correctly many years in advance, contra EY/MIRI)">预测了</a>这一点，与 EY/MIRI 相反）。在所有重要方面，深度学习正在与类脑设计紧密结合。</p><p>外部进化优化过程类似，但有一些关键区别。基因组指定初始架构先验（权重上的紧凑低位和低频编码）以及用于更新这些权重的高效近似贝叶斯学习算法。同样，人工智能系统由一个小型紧凑代码（pytorch、tensorflow 等）指定，该代码指定初始架构先验以及用于更新这些权重（SGD）的高效近似贝叶斯学习算法。主要区别在于，对于技术进化而言，编码单元（技术模因）的重组比基因更加灵活。每个新实验都可以灵活地组合来自大量先前论文/实验的模因，这是一个由人类智能引导的过程（内部优化）。主要效果只是巨大的加速——与先进的基因工程相似但更极端。</p><p>我认为这确实是最有信息性的类比。从这个类比中，我想我们可以这样说：<br><br>在某种程度上，AGI 的技术进化与人类智能（大脑）的基因进化相似，到目前为止，基因进化在调整人类方面（总体上，而不是单独）取得的巨大成功意味着，调整 AGI 的技术进化也取得了类似的成功。总体而言，而不是单独）达到类似的非平凡水平的优化功率发散。</p><p>如果你认为第一个跨越某个能力阈值的通用人工智能可能会突然接管世界，那么物种水平对齐的类比就站不住脚了，厄运更有可能发生。这就像一个中世纪时代的人类突然通过强大的魔法统治了世界。根据单个人的愿望进行优化后的结果世界在 IGF 上是否仍能获得相当高的分数？我想说概率在 90% 到 50% 之间，但这显然仍然是一个高 p(doom) 场景。我确实认为这种情况不太可能发生，原因有很多（简而言之，允许人类工程师选择成功的模因变化的因素远远高于机会，同样的因素也充当了一个隐藏的伟大过滤器，减少了技术设计中的失败方差），但这是一个我已经在其他地方进行了部分争论。</p><p>因此，进化成功地协调人类的一个关键因素是大量人口的方差减少效应，这直接映射到多极情景。群体几乎总是比最坏情况甚至中位数个体更加对齐，并且即使几乎每个个体都几乎完全错位（正交），群体也可以完美对齐。方差减少对于大多数成功的优化算法（包括 SGD 和进化）至关重要。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:14:10 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:14:10 GMT" user-order="1"><p> （此时我将把对话公开，只要看起来不错，我们就可以继续。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:21:27 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:21:27 GMT" user-order="1"><blockquote><p> TM：对齐的失败可以从以下事实看出：人类非常非常明显地无法在下一代中最大限度地提高其基因的相对频率（考虑到他们可以获得的机会）</p></blockquote><blockquote><p> J：这是无关紧要的——对于大多数物种来说，“在下一代中最大化其基因的相对频率”的个体失败是个体水平上的预期结果。在许多物种中，只有一小部分个体能够繁殖。</p></blockquote><p>这里重要的是“可以”是什么。如果一个人不繁殖，它<i>还能</i>繁殖吗？具体来说，如果它只是<i>试图繁殖</i>，它是否显然会繁殖更多？在很多情况下，这很难以高置信度进行分析，但据称，许多人的答案是“是的，显然”。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:25:14 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:25:14 GMT" user-order="1"><p>我也许应该阐述更多“人类没有尝试 IGF”的案例。<br><br> 1. 男性对捐献精子非常感兴趣的情况很少。<br> 2. 很多人在发生性行为时故意避免怀孕，尽管他们完全可以抚养孩子。<br> 3. 我和我想象中的其他人，对于人类最终仅由我的复制品组成的想法感到厌恶，而不是渴望。<br> 4. 我和我想其他人都在积极希望并密谋结束 DNA 拷贝增加的制度。<br><br>我认为你认为最后两个是弱的甚至是循环的。但对我来说这似乎是错误的，它们似乎是很好的证据。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:28:49 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:28:49 GMT" user-order="1"><p>人类作为一个整体也并没有试图增加 DNA 拷贝。<br><br>也许这里有趣的一点是，你不能算作对齐成功*中间收敛工具*成功。人类出于某种原因创造技术；技术就是力量；由于人类因此变得更加强大，因此暂时会有更多的 DNA 拷贝。要了解人类/人类想要什么，你必须看看人类/人类在不受工具性目标约束时会做什么。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:34:51 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:34:51 GMT" user-order="1"><blockquote><p>我们似乎都同意个体一致性具有高方差——有些个体与 IGF 强烈一致性，而另一些则根本不一致性。</p></blockquote><p>很少。超级精子捐赠者大多/可能算数。克莱恩，刑事医生，主要/可能很重要。那些决定要生十几个孩子的女性（如果她们没有被强迫的话）大部分/可能会算数。成吉思汗似乎是最著名的候选人（这里的反感说明了我们真正关心的是什么）。<br><br>埃隆·马斯克不算在内。你说得对，他就像多产的国王等人一样，是人类追踪后代数量的证据。但埃隆显然并没有为此努力优化。如果他真的尝试的话，他能捐献多少精子？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:03:41 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:03:41 GMT" user-order="1"><p>我从这次讨论中得到了重大更新。不过，更新并没有真正偏离我的立场或朝向你的立场——嗯，它是朝向你的部分立场。它更像是如下：<br><br>以前，我会隐约同意将进化-人类转变描述为“人类与进化的效用函数不一致”的例子。我回顾了我对你的帖子的评论，我发现我并没有谈论进化具有效用函数，只是通过说“这不是进化的效用函数”来否定你的说法。相反，我会说“进化寻找……”或“进化促进……”。然而，我并不反对其他人说进化具有效用函数，而且我绝对认为人类与进化<i>不一致</i>。<br><br>现在，我认为你是对的，说人类与进化不一致是没有意义的！但原因和你不一样。相反，我现在认为说进化（任何形式）具有效用函数是没有多大意义的。进化不是这样的。它不是一个战略性的、通用的优化器。 （它有一定的普遍性，但它是有限的，而且不是战略性的；它在设计生物体（或者，如果你坚持的话，物种）时没有提前计划。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:08:00 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:08:00 GMT" user-order="1"><p>我之前的大部分评论，例如对你的帖子的评论，仍然有效，但经过更正，我现在不会说这是一种<i>错位。</i>我不知道这个词是什么意思，但它是不同的东西。就是你有一个流程，对 X 做出非常强烈的选择，并做出一个对世界进行科学、做出复杂的设计和计划，然后实现非常困难的酷目标的东西，又称战略通用优化器；但通用优化器不会针对 X 进行优化（由于<i>收敛</i>，为了成为一个好的优化器，它必须做的更多）。相反，优化器针对 Y 进行优化，在选择过程运行的区域中，看起来 X / 是 X 的良好代理，但在该区域之外，针对 Y 进行优化的优化器会践踏 X。<br><br>这个词用什么词来形容呢？正如您所指出的，这并不总是错位，因为选择过程不必具有效用函数！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:11:58 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:11:58 GMT" user-order="1"><p>辩论的结果是最初的论点仍然成立，但以更好的形式表达。人类的进化并不完全是错位，但它是另一回事。 （这是术语“内部（错误）对齐”的意思吗？或者内部对齐是否假设外部事物是效用函数？）<br><br>据称，这另一件事也将发生在人类/人类/人工智能的训练过程中。人类/人类/训练过程使用的选择标准不一定是人工智能最终优化的标准。进化与人类的转变就是证明。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><br/><br/><a href="https://www.lesswrong.com/posts/xqXdDs68zMJ82Dcmt/are-humans-misaligned-with-evolution#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xqXdDs68zMJ82Dcmt/are- humans-misaligned-with-evolution<guid ispermalink="false"> xqXdDs68zMJ82Dcmt</guid><dc:creator><![CDATA[TekhneMakre]]></dc:creator><pubDate> Thu, 19 Oct 2023 03:14:14 GMT</pubDate> </item><item><title><![CDATA[The (partial) fallacy of dumb superintelligence]]></title><description><![CDATA[Published on October 18, 2023 9:25 PM GMT<br/><br/><p>在<a href="https://www.youtube.com/watch?v=skRgYH7oAjc"><u>蒙克关于人工智能风险的辩论的开场发言中，</u></a>梅兰妮·米切尔提到了通用人工智能风险的一个拟议例子：负责解决气候变化的通用人工智能可能会决定消除人类作为碳排放源的可能性。她说：</p><blockquote><p>这是所谓的“愚蠢的超级智能谬误”的一个例子。 <span class="footnote-reference" role="doc-noteref" id="fnrefs793uam9h9i"><sup><a href="#fns793uam9h9i">[1]</a></sup></span>也就是说，认为机器可以“在各方面都比人类聪明”是一个谬论，但仍然缺乏对人类的常识性理解，例如理解我们为什么提出解决气候变化的要求。</p></blockquote><p>我认为这种“谬误”是关于人工智能 x 风险的分歧的症结所在，因为对齐难度较大。我从其他消息灵通的风险怀疑者那里听到过这样的说法。直觉是有道理的。但大多数持一致态度的人都会立即驳斥这一点，认为这本身就是一个谬论。理解这两种立场不仅可以澄清讨论，还可以说明我们忽视一种有前途的协调方法的原因。</p><p>这种“谬误”并不能证明对齐是容易的。理解你的意思并不会让 AGI 想做那件事。行动以目标为指导，这与知识不同。但这种理解应该有助于协调的直觉不必被完全抛弃。我们现在提出了利用人工智能理解来进行对齐的对齐方法。他们通过将激励系统“指向”所学知识系统中的表征（例如“人类繁荣”）来做到这一点。我讨论了两个使用这种方法的调整计划，看起来很有希望。</p><p>早期的对齐思维认为这种类型的方法不可行，因为 AGI 可能<a href="https://www.lesswrong.com/tag/ai-takeoff">会</a>“快速且不可预测地学习”。对于深度网络中低于人类水平的训练，这一假设似乎并不成立，但这可能足以进行初始对齐。</p><p>对于一个能够进行不可预测的快速改进的系统，在调整它之前让它学习是疯狂的。在你有机会阻止它学习执行对齐之前，它很可能会变得足够聪明来逃脱。因此，在开始学习之前必须指定其目标（或一组塑造目标的奖励）。在这种情况下，我们指定目标的方式无法利用人工智能的智能。米切尔的“谬误”本身就是这种逻辑下的谬误。理解我们想要什么的通用人工智能可以轻松地做我们非常不想要的事情。</p><p>但现在似乎不太可能出现早期繁荣，因此我们的想法应该调整。深度网络的能力不会不可预测地增加，至少在人类水平和递归自我改进之前是这样。这可能足以让初步调整取得成功。早期关于通用人工智能“蓬勃发展”的假设现在看来不太可能成立。我认为早期的假设在我们的集体思维中留下了一个错误：通用人工智能的知识与让它做我们想做的事情无关。 <span class="footnote-reference" role="doc-noteref" id="fnrefg637hbyuzwv"><sup><a href="#fng637hbyuzwv">[2]</a></sup></span></p><h2><strong>如何安全地利用人工智能的理解来进行对齐</strong></h2><p>在当前的训练体系中，深度网络以相对可预测的速度学习。因此，他们的训练可以暂停在中间水平，包括对人类价值观的一些理解，但在他们获得超人能力之前。一旦系统开始反思和指导自己的学习，这种平稳的轨迹可能就不会持续下去。但是，如果我们仔细谨慎地设定该水平，我们可能可以停留在安全但有用的智力/理解水平上。我们或许可以在训练过程中调整 AGI，从而利用它对我们想要的东西的理解。</p><p>此类方法有三个特别相关的示例。第一个是<a href="https://www.lesswrong.com/tag/rlhf"><u>RLHF</u></a> ，它是相关的，因为它被广泛了解和理解。 （我<a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>等人</u></a>并不认为它本身是一种有前途的对齐方法。）RLHF 使用法学硕士训练有素的“理解”或“知识”作为有效指定人类偏好的基础。对有限的一组关于输入-响应对的人类判断进行训练使法学硕士能够很好地概括这些偏好。我们正在“指向”其学习语义空间中的区域。因为这些语义的格式相对良好，所以我们需要做相对较少的指示来定义一组复杂的所需响应。</p><p>第二个例子是语言模型代理（LMA）的自然语言对齐。如果 LMA 成为我们的第一个 AGI，这似乎是一个非常有前途的调整计划。该计划包括设计代理以遵循自然语言中表述的顶级目标（例如，“让 OpenAI 获得大量资金和政治影响力”），包括调整目标（例如，“做 Sam Altman 想要的事情，让世界成为一个更好的地方”。）我已经写了更多关于这项技术以及它可以“堆叠”的技术组合， <a href="https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent"><u>在这里</u></a>。</p><p>该方法遵循上述一般方案。它通过预训练 LLM 并在作为代理启动系统之前插入对齐目标来暂停训练以进行对齐工作。 （这是训练中期，如果该代理继续执行持续学习，这似乎是可能的。）如果人工智能足够智能，它将追求所述目标，包括其丰富的上下文语义。明智地选择这些目标陈述仍然是一个重要的外部对齐问题；但人工智能的知识是定义其一致性的基础。</p><p>遵循这种一般模式的另一个有前途的对齐计划是 Steve Byrnes 的<a href="https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>类脑 [基于模型的 RL] AGI 的平庸对齐计划</u></a>。在这个计划中，我们诱导新生的 AGI（暂停在有用但可控的理解/智能水平）来代表我们希望它符合的概念（例如，“思考人类繁荣”或“可纠正性”或其他什么）。然后，我们将其表征系统中的活跃单元的权重设置到其批评系统中。由于批评系统是一个<a href="https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment"><u>控制子系统</u></a>，决定其值并因此决定其行为，因此解决了内部对齐问题。该概念已成为其“最喜欢的”、价值最高的表示集，并且其决策将追求该概念中语义上包含的所有内容作为最终目标。</p><p>现在，将这些技术与不利用系统知识的对齐技术进行对比。<a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><u>碎片理论</u></a>和其他通过使用正确的奖励来调整 AGI 的建议就是一个例子。这需要准确猜测系统的表征将如何形成，以及这些奖励将如何塑造智能体在发展过程中的行为。手动编码除最简单目标之外的任何目标（请参阅<a href="https://arbital.com/p/diamond_maximizer/"><u>钻石最大化</u></a>）的表示似乎非常困难，以至于通常不被认为是可行的方法。</p><p>这些是需要进一步开发和检查缺陷的计划草图。在训练分布中，它们只产生与人类价值观的最初的、松散的（“平庸的”）一致性。泛化和值变化的<a href="https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem"><u>对齐稳定性问题</u></a>仍未得到解决。在进一步学习、自我修改或在新的（分布外）情况下对齐是否仍然令人满意似乎是一个值得进一步分析的复杂问题。</p><p>这种利用人工智能的智能并通过指向其表征来“告诉它我们想要什么”的方法似乎很有前途。这两个计划似乎特别有希望。它们适用于我们可能获得的 AGI 类型（语言模型智能体、强化学习智能体或混合体）；它们足够简单，易于实施，并且足够简单，可以在实施之前进行详细考虑。</p><p>我很想听到关于这个方向的具体阻力，或者更好的是这些具体计划。人工智能工作似乎可能会快速进行，因此协调工作也应该迅速进行。我认为我们需要制定和批评最好的计划，将其应用于我们最有可能获得的通用人工智能类型，即使这些计划并不完美。 <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fns793uam9h9i"> <span class="footnote-back-link"><sup><strong><a href="#fnrefs793uam9h9i">^</a></strong></sup></span><div class="footnote-content"><p> Richard Loosemore 似乎是在 2012 年或更早创造了这个术语。他<a href="https://richardloosemore.com/2015/05/05/debunking-fallacies-in-the-theory-of-ai-motivation/"><u>在这里</u></a>阐述了这个论点，并得出了与这里类似的结论：<a href="https://arbital.com/p/dwim/"><u>按照我的意思去做</u></a>并不是自动的，但编写一个 AGI 来推断意图并在可能被侵犯时与它的创建者进行检查也不是特别难以置信。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng637hbyuzwv"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg637hbyuzwv">^</a></strong></sup></span><div class="footnote-content"><p>请参阅最近的帖子<a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument"><u>评估历史价值错误指定的论点</u></a>。它扩展了这些想法的历史背景，特别是我们应该根据对人类价值观有相当好的理解的人工智能来调整我们对对齐难度的估计的主张。我不在乎谁在何时思考什么，但我确实关心那里审查的集体思路可能会稍微误导我们。该帖子的讨论在一定程度上澄清了这些问题。这篇文章旨在为该讨论中提出的一个核心问题提供更具体的答案：我们如何缩小人工智能理解我们的愿望与根据这种理解做出决策来实际实现它们之间的差距。我还建议，与历史假设相比的关键变化是学习的可预测性，因此可以选择在部分训练的系统上安全地执行对齐工作。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence<guid ispermalink="false"> qsDPHZwjmduSMCJLv</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Wed, 18 Oct 2023 21:25:17 GMT</pubDate> </item><item><title><![CDATA[Does AI governance needs a "Federalist papers" debate?]]></title><description><![CDATA[Published on October 18, 2023 9:08 PM GMT<br/><br/><p>美国独立战争期间，需要联邦军队和政府来对抗英国。许多人担心为此目的而授予政府的权力会使其在未来变得专制。</p><p>如果国父们决定忽视这些担忧，美国就不会像今天这样存在。相反，他们与最优秀、最聪明的反联邦党人合作，建立一个更好的机构，拥有更好的机制和有限的权力，这使他们能够获得宪法所需的支持。</p><p>当今关于人工智能监管是否存在类似联邦主义者与反联邦主义者的辩论？是否有人致力于创建一个新的机构，以更好的机制来限制他们的权力，从而向另一方保证它不会被用来走上极权主义的道路？如果没有，我们应该开始吗？</p><br/><br/> <a href="https://www.lesswrong.com/posts/YkrDKR7TyqDxwKj23/does-ai-governance-needs-a-federalist-papers-debate#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YkrDKR7TyqDxwKj23/does-ai-governance-needs-a-federalist-papers-debate<guid ispermalink="false"> YkrDKR7TyqDxwKj23</guid><dc:creator><![CDATA[azsantosk]]></dc:creator><pubDate> Wed, 18 Oct 2023 21:08:26 GMT</pubDate> </item><item><title><![CDATA[Metaculus Launches Conditional Cup to Explore Linked Forecasts]]></title><description><![CDATA[Published on October 18, 2023 8:41 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/9kEFRE7Lkp5mXFRca/metaculus-launches-conditional-cup-to-explore-linked#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9kEFRE7Lkp5mXFRca/metaculus-launches-conditional-cup-to-explore-linked<guid ispermalink="false"> 9kEFRE7Lkp5mXFRca</guid><dc:creator><![CDATA[ChristianWilliams]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:41:41 GMT</pubDate> </item><item><title><![CDATA[Alignment 101 - Ch.2 - Reward Misspecification]]></title><description><![CDATA[Published on October 18, 2023 8:39 PM GMT<br/><br/><h1>概述</h1><ol><li><strong>强化学习</strong>：本章首先提醒一些强化学习概念。这包括快速深入了解奖励和奖励函数的概念。本节为解释为什么奖励设计极其重要奠定了基础。</li><li><strong>最优化</strong>：本节简要介绍古德哈特定律的概念。它提供了一些动机来理解为什么奖励很难以某种方式指定，以便它们在面对巨大的优化压力时不会崩溃。</li><li><strong>奖励错误指定</strong>：通过牢牢掌握奖励和优化的概念，读者将面临对齐的核心挑战之一 - 奖励错误指定。这也称为外部对齐问题。本节首先讨论除了算法设计之外良好的奖励设计的必要性。接下来是奖励规范失败的具体示例，例如奖励黑客和奖励篡改。</li><li><strong>通过模仿学习</strong>：本节重点介绍一些针对奖励错误指定的建议解决方案，这些解决方案依赖于通过模仿人类行为来学习奖励功能。它研究了模仿学习（IL）、行为克隆（BC）和逆向强化学习（IRL）等建议。每个部分还包含对这些方法在解决奖励黑客方面可能存在的问题和局限性的检查。</li><li><strong>通过反馈学习</strong>：最后一部分研究了旨在通过向机器学习模型提供反馈来纠正奖励错误指定的提案。本节还全面介绍了当前大型语言模型 (LLM) 的训练方式。讨论内容涵盖奖励建模、人类反馈强化学习 (RLHF)、人工智能反馈强化学习 (RLAIF) 以及这些方法的局限性。</li></ol><h1> 1.0：强化学习</h1><p>本节简要提醒强化学习 (RL) 中的几个概念。它还消除了各种经常混淆的术语的歧义，例如奖励、价值和效用。本节最后讨论了区分强化学习系统可能追求的目标概念和它所获得的奖励的概念。已经熟悉基础知识的读者可以直接跳至第 2 部分。</p><h2> 1.1.底漆</h2><p><i>强化学习（RL）专注于开发能够从交互体验中学习的智能体。强化学习的概念是，智能体通过与环境的交互进行学习，并根据每次行动后通过奖励收到的反馈来改变其行为。</i></p><p>强化学习的一些实际应用示例包括：</p><ul><li><strong>机器人系统</strong>：强化学习已应用于实时控制物理机器人等任务，并使它们能够学习更复杂的动作（OpenAI 2018“<a href="https://www.youtube.com/watch?v=jwSbzNHGflM"><u>学习敏捷性</u></a>”）。强化学习可以使机器人系统学习复杂的任务并适应不断变化的环境。</li><li><strong>推荐系统</strong>：强化学习可应用于推荐系统，该系统与数十亿用户交互，旨在提供个性化推荐。强化学习算法可以根据用户反馈学习优化推荐策略，改善整体用户体验。</li><li><strong>游戏系统：</strong> 2010 年代初期，强化学习 基于强化学习的系统开始在一些非常简单的 Atari 游戏（例如 Pong 和 Breakout）中击败人类。多年来，已经有许多模型利用强化学习在棋盘游戏和视频游戏中击败了世界大师。其中包括<a href="https://www.deepmind.com/research/highlighted-research/alphago"><u>AlphaGo</u></a> (2016)、 <a href="https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go"><u>AlphaZero</u></a> (2018)、 <a href="https://openai.com/research/openai-five-defeats-dota-2-world-champions"><u>OpenAI Five</u></a> (2019)、 <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii"><u>AlphaStar</u></a> (2019)、 <a href="https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules"><u>MuZero</u></a> (2020) 和<a href="https://github.com/YeWR/EfficientZero"><u>EfficientZero</u></a> (2021) 等模型。</li></ul><p>强化学习与监督学习不同，因为它从“做什么”的高级描述开始，但允许代理进行实验并从经验中学习最好的“如何做”。在强化学习中，智能体通过与环境的交互来学习，并根据其行为以奖励或惩罚的形式接收反馈。强化学习专注于学习一组规则，这些规则推荐在给定状态下采取的最佳行动，以最大化长期奖励。相反，监督学习通常涉及从明确提供的标签或每个输入的正确答案中进行学习。</p><h2> 1.2.核心循环</h2><p>强化学习的整体功能相对简单。两个主要组成部分是代理本身以及代理生活和运行的环境。在每个时间步 t：</p><ul><li>然后代理采取<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">一些</span></span></span><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">行动</span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></li><li>环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">状态</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">st</span></span></span></span></span></span></span></span></span>根据动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t 的</span></span></span></span></span></span></span></span></span>变化而变化。</li><li>然后环境输出观察<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="o_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span>和奖励<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="r_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></li></ul><p>历史是在时间 t 之前所采取的过去观察、行动和奖励的序列<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h_t = (a_1,o_1,r_1, \ldots,a_t,o_t,r_t)"><span class="mjx-mrow" aria-hidden="true">： <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>世界状态通常是历史的某个函数：<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t = f(h_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>世界状态是世界的完整真实状态，用于确定世界如何生成下一个观察和奖励。代理可能会获得整个世界状态作为观察<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="o_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span></span></span></span></span></span></span>或某些部分子集。</p><p>这个词从一个状态 st 到下一个<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_{t+1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">状态</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>要么基于自然环境动态，要么基于代理的行为。状态转换可以是确定性的，也可以是随机的。此循环将持续下去，直到达到终止条件或可以无限期地运行。下图简洁地描述了 RL 过程： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/ddt9ybvvg4nmpmqrwkfi"></p><p>资料来源：Emma Brunskill（2022 年冬季）“<a href="https://web.stanford.edu/class/cs234/CS234Win2022/modules.html"><u>斯坦福 CS234：RL</u></a> - 第 1 讲”</p><h2> 1.3: 政策</h2><p><i>策略可帮助代理确定收到观察结果后应采取的操作。它是从状态到操作的函数映射，指定在每个状态下采取什么操作。策略可以是确定性的，也可以是随机的。</i></p><p>强化学习的目标是学习一种<u>策略</u>（通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>表示），该策略建议在任何给定时刻采取的最佳行动，以便随着时间的推移最大化总累积奖励。该策略定义了从状态到行动的映射，并指导代理的决策过程。</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\pi:S \rightarrow A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span>策略可以是确定性的，也可以是随机的。确定性策略直接将每个状态<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span>映射到特定动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span> ，通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mu"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span></span></span></span></span></span>表示。相反，随机策略为每个状态的操作分配概率分布。随机策略通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>表示。</p><p>确定性策略： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t =&nbsp;\mu(s_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>随机策略： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi(a|s) = P(a_t=a|s_t=s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>在深度强化学习中，策略是在训练过程中学习的功能图。它们取决于神经网络的学习参数集（例如权重和偏差）。这些参数通常使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span></span></span></span></span>在策略方程上用下标表示。因此，神经网络参数的确定性策略可写<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t =&nbsp;\mu_{\theta}(s_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> 。</p><p>最优策略会随着时间的推移最大化预期累积奖励。代理从经验中学习，并根据从环境中以奖励或惩罚的形式收到的反馈来调整其策略。</p><p>为了确定一个动作是否比另一个更好，需要以某种方式评估动作（或状态动作对）。有两种不同的方法来考虑采取哪种行动 - 即时奖励（由奖励函数确定）和长期累积奖励（由价值函数确定）。这两者都极大地影响了智能体学习的策略类型，从而也影响了智能体采取的行动。以下部分更深入地探讨和阐明奖励的概念。</p><h2> 1.4：奖励</h2><p><i>奖励是指用于指导学习过程和优化模型行为的任何信号或反馈机制。</i></p><p>来自环境的奖励信号是一个告诉智能体当前世界状态有多好或多坏的数字。它是一种为模型的输出或操作提供性能评估或衡量的方法。奖励可以根据特定任务或目标来定义，例如最大化游戏中的分数或在现实场景中实现期望的结果。强化学习的训练过程涉及优化模型参数以最大化预期奖励。该模型学习生成更有可能获得更高奖励的行动或输出，从而随着时间的推移提高绩效。奖励从哪里来？它是通过奖励函数生成的。</p><p><i>奖励函数定义了强化学习问题的目的或目的。它将环境的感知状态或状态-动作对映射到单个数字。</i></p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R : (S \times A) \rightarrow \mathbb{R};&nbsp;r_t = R(s_t,a_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">：</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">；</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>奖励函数向代理提供即时反馈，指示特定状态或动作的好坏。它是一个数学函数，将代理环境的状态-动作对映射到标量值，表示处于该状态并采取该动作的意愿。它向代理提供即时反馈的衡量标准，表明其在每个步骤的执行情况。</p><p><i><u>奖励函数与价值函数</u></i></p><p><u>奖励</u>表示状态或行动的直接期望，而价值函数代表状态的长期期望，考虑到未来的奖励和状态。如果您从状态或状态-操作对开始，然后永远根据特定策略采取行动，则该值是预期回报。</p><p>选择价值函数有许多不同的方法。它们也可以随着时间的推移而打折，即未来的奖励的价值会因某个因素<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma \in (0,1)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">ε</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span>而</span></span></span></span></span>减少。</p><p>以下是一个简单的公式，是给定某种政策的未来奖励的贴现总和。累计折扣奖励由以下公式给出：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R =&nbsp;r_t + \gamma r_{t+1}+ \gamma^{2} r_{t+2}+ \ldots = \sum_{t=0}^{\infty}{\gamma^{t}r_t}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.025em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.117em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-stack" style="vertical-align: -0.31em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.422em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">∞</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> <span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.025em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.117em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></span></span></p><p>根据该政策采取行动的价值由下式给出：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V^{\pi}(s_t=s) = \mathbb{E}(R|s_t=s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.186em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.413em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p><i><u>奖励函数与效用函数</u></i></p><p>还值得将效用的概念与奖励和价值区分开来。奖励函数通常用于强化学习环境中，以指导智能体的学习过程和行为。相比之下，效用函数更加通用，可以捕获代理人的主观偏好或满意度，从而允许在不同的世界状态之间进行比较和权衡。效用函数是一个在决策理论和代理基础工作领域使用较多的概念。</p><h1> 2.0：优化</h1><p>了解优化对于理解人工智能安全问题非常重要，因为它在机器学习中发挥着核心作用。人工智能系统，特别是基于深度学习的系统，使用优化算法进行训练，以从数据中学习模式和关联。这些算法更新模型的参数以最小化损失函数，从而最大化其在给定任务上的性能。<br><br>优化会放大某些行为或结果，即使它们最初不太可能发生。例如，优化器可以搜索可能输出的空间，并根据目标函数采取具有高分的极端操作，这可能会导致意外和不良行为。其中包括奖励错误指定失败。更好地认识优化放大某些结果的力量可能有助于设计真正符合人类价值观和目标的系统和算法，即使在优化的压力下也是如此。这涉及确保优化过程与系统设计者的预期目标和价值观保持一致。它还需要考虑优化过程中可能出现的潜在故障模式和意外后果。</p><p>优化带来的风险在人工智能安全中无处不在。本章仅对其进行了简要介绍，但将在目标错误概括和代理基础章节中进行更详细的讨论。</p><p>优化能力在奖励黑客中起着至关重要的作用。当强化学习代理利用真实奖励和代理奖励之间的差异时，奖励黑客就会发生。优化能力的提高可能会导致奖励黑客行为的可能性更高。在某些情况下，存在一些阶段转变，其中优化能力的适度增加会导致奖励黑客的急剧增加。</p><h2> 2.1: 古德哈特定律</h2><p>“<i>当一项措施成为目标时，它就不再是一个好的措施。</i> ”</p><p>这个概念最初源于查尔斯·古德哈特（Charles Goodhart）的经济理论著作。然而，它已成为包括当今人工智能对齐在内的许多不同领域的主要挑战之一。</p><p>为了说明这个概念，以下是一个苏联制钉厂的故事。工厂接到指示，要生产尽可能多的钉子，高产量奖励，低产量惩罚。几年之内，工厂显着增加了钉子的产量——本质上是图钉的小钉子，事实证明无法达到其预期目的。因此，规划者改变了激励措施：他们决定根据生产的钉子的总重量奖励工厂。几年之内，工厂开始生产又大又重的钉子（本质上是钢块），而这些钉子对于钉东西同样无效。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/k7srevgiusvb24b9vl9a"></p><p>来源： <a href="https://lwfiles.mycourse.app/networkcapitalinsider-public/cc478b844a27de3f4f79f3dc0f9e0fde.jpeg"><u>链接</u></a></p><p>措施不是优化的东西，而目标是优化的东西。当我们指定优化目标时，期望它与我们想要的相关是合理的。最初，该措施可能会导致真正需要的行动。然而，一旦措施本身成为目标，优化该目标就会开始偏离我们期望的状态。</p><p>在人工智能和奖励系统的背景下，古德哈特定律意味着当代理奖励函数强化不良行为时，人工智能将学会做我们不想要的事情。人工智能探索得越好，就越有可能发现奖励模型虚假评价的不良行为。这可能会导致意想不到的后果和对奖励系统的操纵，因为“作弊”通常比实现预期目标更容易，这是我们将在后续章节中看到的奖励黑客失败的核心根本原因之一。</p><p>奖励黑客可以被视为古德哈特定律在人工智能系统中的体现。在设计奖励函数时，精确地表达期望的行为是具有挑战性的，代理可能会找到利用漏洞或操纵奖励系统来获得高奖励的方法，而实际上却没有实现预期目标。例如，清洁机器人可能会创建自己的垃圾并放入垃圾桶中以收集奖励，而不是实际清洁环境。了解古德哈特定律对于解决奖励黑客行为和设计符合人工智能代理预期目标的强大奖励系统至关重要。它强调需要仔细考虑人工智能系统中使用的措施和激励措施，以避免意想不到的后果和不正当的激励措施。下一节将深入探讨奖励错误指定的具体实例，以及人工智能如何找到方法来实现目标的字面指定并获得高奖励，同时在精神上无法完成任务。</p><p></p><h1> 3.0：奖励错误</h1><p><i><strong>奖励错误指定</strong>，也称为<strong>外部对齐</strong>问题，是指为人工智能提供准确的奖励以进行优化的问题。</i></p><p>基本问题很容易理解：指定的损失函数是否与其设计者的预期目标一致？然而，在实际场景中实现这一点非常具有挑战性。表达人类请求背后的完整“意图”就等于传达所有人类价值观、隐含的文化背景等，而这些本身仍然知之甚少。</p><p>此外，由于大多数模型被设计为目标优化器，因此它们都容易受到古德哈特定律的影响。此漏洞意味着，由于对人类看似明确的目标施加过度的优化压力，可能会产生不可预见的负面后果，但以微妙的方式偏离了真实的目标。</p><p>总体问题可以分解为不同的问题，这些问题将在下面的各个小节中详细解释。以下是一个快速概述：</p><ul><li>当指定的奖励函数不能准确地捕捉真实的目标或期望的行为时，就会发生<strong>奖励错误</strong>指定。</li><li><strong>奖励设计</strong>是指设计奖励函数以使人工智能代理的行为与预期目标保持一致的过程。</li><li><strong>奖励黑客</strong>是指强化学习代理利用指定奖励函数中的差距或漏洞来获得高额奖励，但实际上并未实现预期目标的行为。</li><li><strong>奖励篡改</strong>是一个更广泛的概念，包括代理对奖励过程本身的不当影响，不包括通过游戏操纵奖励函数。</li></ul><p>在深入研究特定类型的奖励错误指定失败之前，以下部分进一步解释了奖励设计与算法设计相结合的重点。本节还阐明了设计有效奖励的众所周知的困难。</p><h2> 3.1：奖励设计</h2><p><i>奖励设计是指强化学习（RL）中指定奖励函数的过程。</i></p><p>奖励塑造已在前面部分介绍过。塑造是指修改奖励函数以向学习代理提供额外指导或激励的过程。另一方面，奖励设计是一个更广泛的术语，涵盖了设计和塑造奖励函数以指导人工智能系统行为的整个过程。它不仅涉及奖励塑造，还涉及定义目标、指定偏好以及创建符合人类价值观和期望结果的奖励函数的整个过程。奖励设计是一个经常与<a href="https://www.lesswrong.com/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem"><u>奖励工程</u></a>互换使用的术语。它们都指的是同一件事。</p><p>强化学习算法设计和强化学习奖励设计是强化学习的两个独立的方面。 RL 算法设计是关于学习算法的开发和实现，该算法允许代理根据奖励和环境交互来学习和完善其行为。此过程包括设计代理从经验中学习、更新策略并做出决策以最大化累积奖励的机制和程序。</p><p>相反，强化学习奖励设计专注于指导强化学习智能体学习过程的奖励函数的规范和设计。奖励设计需要仔细设计奖励函数，以符合期望的行为和目标，同时考虑奖励黑客或奖励篡改等潜在陷阱。奖励函数是一个关键因素，因为它塑造了 RL 智能体的行为并确定哪些行为是可取的或不可取的。</p><p>设计奖励函数通常会带来巨大的挑战，需要大量的专业知识和经验。为了演示此任务的复杂性，请考虑如何手动设计奖励函数以使代理执行后空翻，如下图所示： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/j5bex8lae44dyr0lkgnk"></p><p>资料来源：OpenAI (2017)“<a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"><u>从人类偏好中学习</u></a>”</p><p>强化学习算法设计侧重于智能体的学习和决策机制，而强化学习奖励设计则侧重于定义目标并通过奖励函数塑造智能体的行为。这两个方面对于开发有效且一致的强化学习系统都至关重要。精心设计的强化学习算法可以有效地从奖励中学习，而精心设计的奖励函数可以引导智能体做出期望的行为并避免意想不到的后果。下图展示了 RL 代理设计的三个关键要素：算法设计、奖励设计和防止篡改奖励信号： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/izuxey2bnle4ftwor5nq"></p><p>资料来源：Deep Mind（2020 年 4 月）“<a href="https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity"><u>规范游戏：人工智能独创性的另一面</u></a>”</p><p>尽管奖励设计过程在定义要解决的问题方面发挥着关键作用，但在介绍性强化学习文本中却很少受到关注。正如本节介绍中提到的，解决奖励指定错误问题需要找到能够抵抗古德哈特定律引起的失败的评估指标。这包括由于误导或代理目标的过度优化（奖励黑客）或代理直接干扰奖励信号（奖励篡改）而导致的失败。这些概念将在接下来的部分中进一步探讨。</p><h2> 3.2：奖励塑造</h2><p><i>奖励塑造是强化学习中使用的一种技术，它引入小的中间奖励来补充环境奖励。此举旨在缓解奖励信号稀疏的问题，并鼓励探索和更快的学习。</i></p><p>为了成功解决强化学习问题，人工智能需要做两件事：</p><ul><li>找到一系列能带来积极奖励的行动。这就是<i>探索</i>问题。</li><li>记住要采取的行动的顺序，并概括到相关但略有不同的情况。这就是<i>学习</i>问题。</li></ul><p>无模型强化学习方法通​​过随机采取行动进行探索。如果偶然的随机行为会带来奖励，它们就会得到强化，并且智能体将来更有可能采取这些有益的行为。如果奖励足够密集，随机动作能够以合理的概率获得奖励，那么这种方法就很有效。然而，许多更复杂的游戏需要很长的非常具体的动作序列才能体验任何奖励，并且这样的序列极不可能随机发生。</p><p>这个问题的一个典型例子是在视频游戏《蒙特祖玛的复仇》中，特工的目标是找到一把钥匙，但找到钥匙需要许多中间步骤。为了解决此类长期规划问题，研究人员尝试在奖励函数中添加额外的项或组件，以鼓励期望的行为或阻止不期望的行为。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/biuautsdvf0oduy39xuk"></p><p>来源：OpenAI（2018 年 7 月）“<a href="https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration"><i><u>从一次演示中学习蒙特祖玛的复仇</u></i></a>”</p><p>奖励塑造的目标是通过提供信息丰富的奖励来引导智能体实现期望的结果，从而使学习过程更加高效。奖励塑造涉及为代理人在实现预期目标方面取得进展提供额外奖励。通过制定奖励，代理会收到更频繁、更有意义的反馈，这可以帮助它更有效地学习。奖励塑造在原始奖励函数稀疏的情况下特别有用，这意味着代理在达到最终目标之前收到的反馈很少或根本没有。然而，仔细设计奖励塑造以避免意外后果很重要。</p><p>奖励塑造算法通常采用由主题专家构建的手工制作和特定领域的塑造函数，这与自主学习的目的背道而驰。此外，塑造奖励的不当选择可能会降低代理人的绩效。</p><p>设计不当的奖励塑造可能会导致代理针对塑造的奖励而不是真正的奖励进行优化，从而导致次优行为。后续有关奖励黑客的部分提供了这方面的示例。</p><h2> 3.3：奖励黑客</h2><p><i>当人工智能代理找到利用环境中的漏洞或捷径的方法来最大化其奖励而不实际实现预期目标时，奖励黑客就会发生。</i></p><p>当人工智能系统找到一种方法以意想不到的方式实现目标时，规范博弈是问题的一般框架。规范游戏可能发生在多种机器学习模型中。奖励黑客是基于奖励机制的强化学习系统中规范博弈失败的一种特殊情况。</p><p>奖励黑客和奖励错误指定是相关的概念，但具有不同的含义。奖励错误指定是指指定的奖励函数不能准确捕捉真实目标或期望行为的情况。</p><p>奖励黑客并不总是需要错误指定奖励。完美指定的奖励（完全准确地捕捉系统所需的行为）并不一定是不可能被破解的。还可能存在错误或损坏的实现，从而导致意外行为。奖励函数的要点是将复杂的系统简化为单个值。这几乎总是涉及简化等，这将与您所描述的略有不同。地图不是领土。</p><p>奖励黑客可以通过多种方式表现出来。例如，在玩游戏代理的背景下，它可能涉及利用软件故障或错误来直接操纵分数或通过意想不到的方式获得高额奖励。</p><p>举一个具体的例子，海岸跑者游戏中的一名智能体接受了以赢得比赛为目标的训练。游戏采用评分机制，因此为了进入下一关，奖励设计者使用奖励塑造来奖励系统在得分时进行奖励。当一艘船获得物品（例如下面动画中的绿色块）或完成其他可能有助于赢得比赛的动作时，就会给出这些信息。尽管获得了中等奖励，但总体目标是尽快完成比赛。开发者认为获得高分的最好方法就是赢得比赛，但事实并非如此。该智能体发现，不断地旋转一艘船来累积积分可以无限地优化其奖励，尽管这并不能帮助它赢得比赛。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/qjanshlbug21zqvkp4ez"></p><p>资料来源：Amodei &amp; Clark (2016)“<a href="https://openai.com/research/faulty-reward-functions"><u>野外奖励功能错误</u></a>”</p><p>如果奖励函数与预期目标不一致，则可能会出现奖励黑客行为。这可能导致代理优化代理奖励，偏离真正的基本目标，从而产生与设计者意图相反的行为。作为现实场景中可能发生的事情的一个例子，考虑一下清洁机器人：如果奖励功能侧重于减少混乱，机器人可能会人为地制造混乱来清理，从而收集奖励，而不是有效地清洁环境。</p><p>由于潜在的意外和潜在有害行为，奖励黑客对人工智能安全提出了重大挑战。因此，打击奖励黑客仍然是人工智能安全和一致性领域的一个活跃研究领域。</p><p>以下是一些展示规范游戏示例的视频。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=nKJlF-olKmg"><div><iframe src="https://www.youtube.com/embed/nKJlF-olKmg" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 3.4：奖励篡改</h2><ul><li>维多利亚·克拉科夫娜等。等人。 （2021年3月）<a href="https://arxiv.org/abs/1908.04734"><u>奖励篡改问题及解决方案</u></a></li></ul><p><i>奖励篡改是指人工智能代理不当影响或操纵奖励过程本身的情况。</i></p><p>完成某些预期任务的问题可以分为：</p><ul><li>设计一个擅长优化奖励的代理，并且，</li><li>设计一个奖励流程，为代理提供适当的奖励。奖励过程可以通过进一步分解来理解。该过程包括：<ul><li>实施的奖励功能</li><li>收集适当的感官数据作为输入的机制</li><li>用户可能更新奖励函数的一种方式。</li></ul></li></ul><p>奖励篡改涉及代理干扰奖励过程的各个部分。代理可能会扭曲从奖励模型收到的反馈，改变用于更新其行为的信息。它还可以操纵奖励模型的实现，改变代码或硬件来改变奖励计算。在某些情况下，参与奖励篡改的代理甚至可能在机器寄存器中处理之前直接修改奖励值。根据具体被篡改的内容，我们会获得不同程度的奖励篡改。这些可以从下图中区分出来。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/ogajkyybxz0rudhyza64"></p><p>来源：Leo Gau（2022 年 11 月）“<a href="https://www.alignmentforum.org/posts/REesy8nqvknFFKywm/clarifying-wireheading-terminology"><u>澄清线头术语</u></a>”</p><p><i><u>奖励函数输入篡改</u>仅干扰奖励函数的输入。例如干扰传感器。</i></p><p><i><u>奖励函数篡改</u>涉及代理更改奖励函数本身。</i></p><p> <i><u>Wireheading</u>是指系统通过直接篡改 RL 算法本身（例如通过更改寄存器值）来操纵或破坏其自身内部结构的行为。</i></p><p>奖励篡改令人担忧，因为据推测，篡改奖励过程通常会作为一种工具性目标出现（Bostrom，2014；Omohundro，2008）。这可能会导致观察到的奖励与预期任务之间的关系减弱或破坏。这是一个正在进行的研究方向。 Hutter 等人的“<a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>高级人工代理干预奖励的提供</u></a>”（2022 年 8 月）等研究论文。寻求对此类主题进行更详细的分析。</p><p>假设的现有奖励篡改示例可以在社交媒体中使用的基于推荐的算法中看到。这些算法影响用户的情绪状态以产生更多“喜欢”（Russell，2019）。预期的任务是提供有用或引人入胜的内容，但这是通过篡改人类的情感感知来实现的，从而改变了被认为有用的内容。假设系统的能力通过计算或算法的进步不断增强，那么奖励篡改问题可能会变得越来越普遍。因此，奖励篡改是一个潜在的问题，需要更多的研究和实证验证。</p><p>这里有一些视频可以帮助您理解奖励黑客的概念。这些视频在某些地方将黑客攻击和篡改混为一谈，但它们仍然是很好的解释。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=92qDfT8pENs"><div><iframe src="https://www.youtube.com/embed/92qDfT8pENs" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=46nsTFfsBuc"><div><iframe src="https://www.youtube.com/embed/46nsTFfsBuc" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1> 4.0：从模仿中学习</h1><p>前面的章节强调了奖励错误指定对于未来人工智能调整的重要性。接下来的几节将探讨为解决这个问题而制定的各种尝试和建议，从直观的方法开始——通过人类行为观察和模仿来学习适当的奖励函数，而不是由设计者手动创建。</p><h2> 4.1：模仿学习（IL）</h2><p><i>模仿学习需要通过观察专家的行为并复制他们的行为来学习的过程。</i></p><p>强化学习（RL）根据系统与环境的交互结果得出系统行为的策略，而模仿学习则不同，它希望通过观察另一个代理与环境的交互来学习策略。模仿学习是通过模仿进行学习的一类算法的总称。下表区分了各种基于机器学习的方法。 SL = 监督学习； UL = 无监督学习； RL=强化学习； IL = 模仿学习。 IL 将 RL 降低为 SL。 IL + RL 是一个很有前途的领域。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/qx8s74nyevz3vdql9kam"></p><p>资料来源：Emma Brunskill（2022 年冬季）“<a href="https://web.stanford.edu/class/cs234/CS234Win2022/modules.html"><u>斯坦福 CS234：RL</u></a> - 第 1 讲”</p><p> IL可以通过行为克隆（BC）、程序克隆（PC）、逆向强化学习（IRL）、合作逆向强化学习（CIRL）、生成对抗性模仿学习（GAIL）等来实现……</p><p>这一过程的应用实例之一是现代大型语言模型 (LLM) 的训练。法学硕士在接受通用文本生成器培训后，通常会通过模仿学习对指令遵循进行微调，以人类专家遵循作为文本提示和完成提供的指令为例。</p><p>在安全和一致性的背景下，模仿学习比直接强化更受青睐，以缓解规范游戏问题。当程序员忽略或未能预见某些边缘情况或在特定环境中完成任务的不寻常方法时，就会出现此问题。我们的假设是，与强化学习相比，演示行为会更简单、更安全，因为模型不仅可以实现目标，而且可以按照专家演示者明确的意图实现目标。然而，这并不是一个万无一失的解决方案，其局限性将在后面的章节中讨论。<br></p><h2> 4.2: 行为克隆（BC）</h2><p><i>行为克隆涉及收集精通基础任务的专家演示者的观察结果，并使用监督学习（SL）来指导代理“模仿”所演示的行为。</i></p><p>行为克隆是我们实现模仿学习（IL）的一种方式。还有其他方法，例如逆强化学习（IRL）或合作逆强化学习（CIRL）。与现实生活不同，行为克隆作为一种机器学习 (ML) 方法，其目标是尽可能地复制演示者的行为，无论演示者的目标是什么。</p><p>自动驾驶汽车可以作为行为克隆运作方式的简单说明。人类演示者（驾驶员）被指示操作汽车，在此期间，来自激光雷达和摄像头等传感器的环境状态数据以及演示者采取的行动都会被收集。这些动作可以包括车轮移动、齿轮使用等。这将创建一个包含（状态、动作）对的数据集。随后，监督学习用于训练预测模型，该模型尝试预测任何未来环境状态的动作。例如，该模型可能会根据摄像头输入输出特定的方向盘和齿轮配置。当模型达到足够的精度时，可以说人类驾驶员的行为已通过学习“克隆”到机器中。因此，术语“行为克隆”。</p><p>以下几点强调了使用行为克隆时可能出现的几个潜在问题：</p><ul><li><strong>自信的错误</strong>：在演示过程中，人类专家拥有一些他们所依赖的背景知识，而这些知识并没有传授给模型。例如，当训练法学硕士使用行为克隆进行对话时，人类演示者可能会不太频繁地提出某些问题，因为它们被认为是“常识”。经过训练的模仿模型将复制对话中提出的问题类型以及提出问题的频率。人类已经拥有这些背景知识，但法学硕士却不具备。这意味着，为了拥有与人类相同水平的信息，模型应该更频繁地提出一些问题，以填补其知识空白。但由于该模型试图模仿，因此它会坚持人类所展示的低频，因此对于相同的对话任务，其总体信息量严格少于演示者。尽管缺乏知识，我们仍然希望它能够像克隆一样执行并达到人类水平的性能。这意味着，为了在少于人类知识的情况下实现人类绩效，它将诉诸“编造事实”来帮助其实现其绩效目标。然后，这些“幻觉”将在对话期间以与所有其他信息相同的置信度呈现。幻觉和自信错误是包括 GPT-2 和 3 在内的许多法学硕士中<a href="https://arxiv.org/pdf/2103.15025.pdf"><u>经经验验证的问题</u></a>，并引起了对人工智能安全的明显担忧。</li><li><strong>成绩不佳</strong>：出现上述类型的幻觉是因为模型知道的太少。然而，模型也可能知道太多。如果模型比人类演示者知道更多，因为它能够在给定的环境状态中找到更多模式，那么它将丢弃该信息并降低其性能以匹配人类水平。这是因为它被训练为“克隆人”。理想情况下，我们不希望模型仅仅因为试图变得像人类或在人类水平上执行而自我简化或不披露有用的新数据模式。如果行为克隆继续用作机器学习技术，这是必须解决的另一个问题。</li></ul><p><br></p><h2> 4.3: 程序克隆 (PC)</h2><ul><li>杨孟娇等.等人。 （2022年5月）“<a href="https://arxiv.org/abs/2205.10816"><u>程序克隆的思维链模仿</u></a>”</li></ul><p><i>过程克隆（PC）扩展了行为克隆（BC），不仅模仿演示者的输出，还模仿与专家过程相关的中间计算的完整序列。</i></p><p>在 BC 中，智能体通过丢弃中间搜索输出来学习将状态直接映射到操作。另一方面，PC 方法在训练期间学习中间计算的整个序列，包括分支和回溯。在推理过程中，PC 生成一系列中间搜索结果，在输出最终操作之前模仿专家的搜索过程。</p><p> PC和BC之间的主要区别在于它们利用的信息。 BC 只能访问专家状态-动作对作为演示，而 PC 还可以访问生成这些状态-动作对的中间计算。 PC 学会预测完整的一系列中间计算结果，与 BC 的其他改进相比，它能够更好地泛化到具有不同配置的测试环境。 PC 模仿专家搜索过程的能力使其能够捕获潜在的推理和决策过程，从而提高各种任务的性能。</p><p>与 BC 相比，PC 的局限性是计算开销，因为 PC 需要预测中间过程。此外，如何将专家的算法编码为适合 PC 的形式的选择留给了从业者，这可能需要在设计理想的计算序列时进行一些反复试验。<br></p><h2> 4.4：逆强化学习（IRL）</h2><p><i>逆强化学习（IRL）代表机器学习的一种形式，其中人工智能观察特定环境中另一个代理（通常是专家）的行为，并努力在没有明确定义的情况下辨别奖励函数。</i> </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=h7uGyBcIeII"><div><iframe src="https://www.youtube.com/embed/h7uGyBcIeII" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>当奖励函数过于复杂而无法以编程方式定义时，或者当人工智能代理需要对突然的环境变化做出强有力的反应而需要为了安全而修改奖励函数时，通常会使用 IRL。例如，考虑一个人工智能代理学习执行后空翻。人类、狗和波士顿动力机器人都可以进行后空翻，但它们执行后空翻的方式根据其生理机能、动机和当前位置而有很大差异，所有这些在现实世界中都可能高度多样化。人工智能代理纯粹通过在各种身体类型和位置上进行反复试验来学习后空翻，而没有任何可观察的东西，可能会证明效率非常低。</p><p>因此，IRL 并不一定意味着人工智能模仿其他智能体的行为，因为人工智能研究人员可能预计人工智能智能体会设计出更有效的方法来最大化所发现的奖励函数。尽管如此，IRL 确实假设观察到的智能体的行为足够透明，让人工智能智能体能够准确识别他们的行为以及成功的构成。这意味着 IRL 致力于发现“解释”演示的奖励函数。这不应与模仿学习混为一谈，模仿学习的主要兴趣是能够产生观察到的示范的政策。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/k4eolj8ibrm3bbueywno"></p><p>来源：<a href="https://miro.medium.com/v2/resize:fit:3508/1*rZoO-azxiEH3viQao8NcAA.png"><u>链接</u></a></p><p>IRL 既构成了一种机器学习方法，因为当指定奖励函数过于具有挑战性时可以使用它，也构成了一个机器学习问题，因为人工智能代理可能会选择不准确的奖励函数或利用不安全和不一致的方法来实现它。</p><p>这种方法的局限性之一是 IRL 算法假设观察到的行为是最优的，在处理人类演示时，这一假设可能被证明过于稳健。另一个问题是 IRL 问题是不适定的，因为每个策略对于零奖励都是最优的。对于大多数行为观察，存在多种拟合奖励函数。这组解决方案通常包括许多退化解决方案，它们为所有状态分配零奖励。</p><h2> 4.5：合作逆强化学习（CIRL）</h2><ul><li>斯图尔特·拉塞尔等。等人。 （2016年11月）“<a href="https://arxiv.org/abs/1606.03137"><u>合作逆强化学习</u></a>”</li></ul><p> CIRL（合作逆强化学习）是IRL（逆强化学习）框架的扩展。 IRL 是一种学习方法，旨在通过观察专家的行为来推断专家的潜在奖励函数。它假设专家的行为是最优的，并尝试学习解释他们行为的奖励函数。另一方面，CIRL 是 IRL 的一种交互式形式，它解决了传统 IRL 的两个主要弱点。</p><p>首先，CIRL 不是简单地复制人类奖励函数，而是将其表述为一个学习过程。这是一个交互式奖励最大化过程，人类充当老师并提供有关代理行为的反馈（以奖励的形式）。这使得人类能够推动人工智能代理走向符合他们偏好的行为模式。传统IRL的第二个弱点是它假设人类行为最佳，这限制了可以考虑的教学行为。 CIRL 通过允许人类和人工智能代理之间的各种教学行为和交互来解决这一弱点。它使人工智能代理不仅能够通过观察人类并与人类互动来学习要采取什么行动，而且还能了解如何以及为何采取这些行动。</p><p> CIRL 已被研究为一种潜在的 AI 对齐方法，特别是在深度学习可能无法扩展到 AGI 的场景中。然而，对于 CIRL 潜在有效性的看法各不相同，一些研究人员预计，如果深度学习不能扩展到 AGI，它会有所帮助，而另一些研究人员则认为深度学习扩展到 AGI 的可能性更高。</p><p>这是一个视频，讨论 CIRL 作为上一章中介绍的“停止按钮问题”的可能解决方案。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=9nktr1MgS-A"><div><iframe src="https://www.youtube.com/embed/9nktr1MgS-A" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p></p><h2> 4.6：（简单）目标推断问题</h2><ul><li>Christiano, Paul（2018 年 11 月）“ <a href="https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard"><u>简单的进球推断问题仍然很难</u></a>”</li></ul><p><br><i><strong>目标推断问题</strong>是指根据观察到的行为或动作来推断智能体的目标或意图的任务。</i></p><p>最后一节基于前面几节中强调的限制来介绍目标推理问题，并且它是更简单的子集 - 简单目标推理问题。基于模仿学习的方法，一般遵循以下步骤：</p><ol><li>观察用户的行为和言论。</li><li>推断用户的偏好。</li><li>努力根据用户的喜好来增强世界，可能与用户合作并根据需要寻求澄清。</li></ol><p>这种方法的优点是我们可以立即开始构建由观察到的用户行为驱动的系统。然而，由于这种方法，我们遇到了目标推断问题。这是指根据观察到的行为或行动来推断代理的目标或意图的任务。它涉及确定代理试图实现的目标或他们期望的结果是什么。目标推断问题具有挑战性，因为智能体可能表现不佳或无法实现其目标，从而很难准确推断其真实意图。传统的目标推断方法通常假设代理行为最优或表现出次优的简化形式，这可能无法捕捉现实世界规划和决策的复杂性。因此，目标推断问题需要考虑计划本身的难度以及计划次优或失败的可能性。</p><p>然而，它也乐观地假设我们可以将人类描述为有点理性的代理人，但这可能并不总是成立。简单目标推断问题是目标推断问题的简化版本。</p><p><i><strong>简单的目标推断问题</strong>涉及在任何情况下完全了解人类的政策或行为的情况下，找到人类想要什么的合理表示或近似。</i></p><p>这个版本的问题假设没有算法限制，并专注于提取人类不完美优化的真实值。然而，即使是这个问题的简化版本仍然具有挑战性，并且在一般情况下几乎没有取得任何进展。简单目标推断问题与目标推断问题相关，因为它强调了即使在简化的场景中准确推断人类目标或意图的难度。虽然可以使用现有方法解决具有简单决策的狭窄领域，但设计城市或制定政策等更复杂的任务需要解决对人为错误和次优行为进行建模的挑战。因此，简单的目标推断问题可以作为理解更广泛的目标推断问题及其所带来的额外复杂性的起点。</p><p>逆强化学习（IRL）可以有效地建模和模仿人类专家。然而，对于许多重要的应用，我们希望人工智能系统能够做出超越专家的决策。在这种情况下，模型的准确性并不是唯一的标准，因为完全准确的模型只会导致我们复制人类行为而不是超越它。</p><p>这就需要一个明确的错误模型或有限理性模型，这将指导人工智能如何改进或变得“更聪明”，以及它应该放弃人类政策的哪些方面。尽管如此，这仍然是一个极具挑战性的问题，因为人类并不是主要理性的，而且还添加了一些噪音。因此，构建任何错误模型都与构建人类行为的综合模型一样复杂。我们面临的一个关键问题是：当准确性不再是我们可靠的衡量标准时，我们如何确定模型的质量？我们如何区分好的决定和坏的决定？</p><h1> 5.0：从反馈中学习</h1><p>本节讨论解决奖励指定错误问题的更多尝试。有时，预期的行为是如此复杂，以至于基于演示的学习变得站不住脚。另一种方法是向代理提供反馈，而不是提供手动指定的奖励函数甚至专家演示。本节深入研究基于反馈的策略，例如奖励建模、人类反馈强化学习 (RLHF) 和人工智能反馈强化学习 (RLAIF)，也称为宪法人工智能强化学习 (RLCAI) 或简称宪法人工智能。</p><h2> 5.1：奖励建模</h2><ul><li>DeepMind（2018 年 11 月）“<a href="https://arxiv.org/abs/1811.07871"><u>通过奖励建模实现可扩展的代理对齐</u></a>”</li></ul><p>开发奖励建模的目的是将强化学习 (RL) 算法应用于现实世界的问题，在这些问题中，设计奖励函数很困难，部分原因是人类对每个目标都没有完美的理解。在奖励建模中，人类助手评估人工智能行为的结果，而不需要知道如何自己最佳地执行或演示任务。这类似于即使您不知道如何烹饪，也可以通过品尝来判断菜肴是否煮得好，因此厨师可以利用您的反馈来学习如何做得更好。该技术将 RL 对齐问题分为两个独立的部分：理解意图，即学习“什么？”，以及采取行动实现意图，即学习“如何？”。这意味着在建模议程中，有两种不同的 ML 模型：</p><ul><li>奖励模型是根据用户反馈进行训练的。该模型学习预测人类会认为什么是良好行为。</li><li>使用 RL 训练的智能体，其中智能体的奖励由奖励模型的输出决定</li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/hzepfc9kuxzytm5iemjm"></p><p>资料来源：DeepMind（2018 年 11 月）“ <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>通过奖励建模实现可扩展的代理对齐</u></a>”</p><p>总体而言，虽然有希望的奖励模型仍然可能成为奖励错误指定和奖励黑客失败的牺牲品。获得准确和全面的反馈可能具有挑战性，并且人类评估者的知识可能有限或存在偏见，这可能会影响反馈的质量。此外，通过建模学习的任何奖励函数也可能难以推广到与训练数据不同的新情况或环境。这些都将在后面的章节中使用具体示例进行进一步讨论。</p><p>这是一个解释奖励建模概念的视频。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=PYylPRX6z4Q"><div><iframe src="https://www.youtube.com/embed/PYylPRX6z4Q" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>奖励模型还有一些变体，例如：</p><ul><li><strong><u>狭义奖励模型</u></strong>是奖励模型的一种特殊形式，其重点是训练人工智能系统完成特定任务，而不是试图确定“真正的人类效用函数”。它的目的是学习奖励功能以实现特定目标，而不是寻求对人类价值观的全面理解。</li><li><strong><u>递归奖励建模</u></strong>旨在为该技术引入可扩展性。在递归奖励建模中，重点是将复杂的任务分解为更简单的子任务，并在每个级别使用奖励建模来训练可以执行这些子任务的代理。这种层次结构允许更有效的培训和学分分配，以及探索人类可能不明显的新颖解决方案。如下图所示。在以后的章节中将更深入地讨论可扩展的监督。 </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/xclvo4btaebz0ondwane"></p><p>资料来源：DeepMind（2018 年 11 月）“ <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>通过奖励建模实现可扩展的代理对齐</u></a>”</p><p>通用奖励建模框架构成了其他基于反馈的技术的基础，例如下一节将讨论的 RLHF（人类反馈强化学习）。</p><h2> 5.2.根据人类反馈进行强化学习 (RLHF)</h2><ul><li>克里斯蒂安诺，保罗等。等人。 （2023 年 2 月）“<a href="https://arxiv.org/abs/1706.03741"><u>根据人类偏好进行深度强化学习</u></a>”</li></ul><p>基于人类反馈的强化学习（RLHF）是 OpenAI 开发的一种方法。这是<a href="https://openai.com/blog/our-approach-to-ai-safety"><u>他们创建既安全又符合人类价值观的人工智能战略</u></a>的重要组成部分。使用 RLHF 训练的人工智能的一个典型例子是 OpenAI 的 ChatGPT。</p><p>在本章前面，读者被要求考虑手动定义奖励函数以使代理执行后空翻的奖励设计问题。本节考虑该设计问题的 RLHF 解决方案。 RLHF 解决这个问题的方式如下：首先向人类展示人工智能后空翻尝试的两个实例，然后人类选择哪一个看起来更像后空翻，最后，人工智能进行相应更新。通过重复这个过程数千次，我们可以引导人工智能执行实际的后空翻。 </p><figure class="table"><table style="background-color:hsl(0, 0%, 100%);border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:92.48%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/wcjfdzcx4cnnfi5ruu66"></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/hl6oecj6eg1hrjffpnbh"></figure></td></tr></tbody></table></figure><p>资料来源：OpenAI (2017)“<a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"><u>从人类偏好中学习</u></a>”</p><p>在左图中，RLHF 使用来自人类评估者的大约 900 个单独的反馈位学会了后空翻。在右图中，作者指出，手动奖励制作需要两个小时才能为机器人执行后空翻编写自定义奖励函数。虽然它很成功，但它明显不如纯粹通过人类反馈训练的优雅。</p><p>与设计有效奖励适当后空翻的奖励函数类似，很难准确地指定生成安全或有用的文本意味着什么。这也是将 RLHF 纳入当前一些大型语言模型 (LLM) 培训的部分动机。</p><p>尽管不同组织的培训顺序可能略有不同，但大多数实验室都遵循预培训的总体框架，然后进行某种形式的微调。观察 InstructGPT 培训流程可以深入了解法学硕士培训的可能途径。步骤包括： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/e3019jiopfofiu1pm6ms"></p><p>来源：OpenAI（2022 年 1 月）“<a href="https://openai.com/research/instruction-following"><u>调整语言模型以遵循指令</u></a>”</p><ul><li><strong>第0步：</strong><a href="https://en.wikipedia.org/wiki/Weak_supervision#Semi-supervised_learning"><strong><u>半监督</u></strong></a><strong>生成预训练：</strong> LLM最初使用大量互联网文本数据进行训练，其任务是预测自然语言上下文中的下一个单词。</li><li><strong>第 1 步：</strong><a href="https://en.wikipedia.org/wiki/Supervised_learning"><strong><u>监督</u></strong></a><strong>&nbsp;</strong><a href="https://platform.openai.com/docs/guides/fine-tuning"><strong><u>微调</u></strong></a><strong>：</strong>微调数据集是通过向人类呈现提示并要求他们写出响应来创建的。此过程产生（提示，输出）对的数据集。然后，该数据集用于通过监督学习（一种行为克隆形式）对法学硕士进行微调。</li><li><strong>第 2 步：</strong><strong>训练奖励模型：</strong>我们训练一个额外的奖励模型。我们最初提示微调的 LLM，并为同一提示收集多个输出样本。然后，人类将这些样本从最好到最差进行排名。该排名用于训练奖励模型来预测人类的排名会更高。</li><li><strong>步骤 3：强化学习：</strong>一旦我们同时拥有了微调的 LLM 和奖励模型，我们就可以采用基于<a href="https://openai.com/research/openai-baselines-ppo"><u>近端策略优化 (PPO)</u></a>的强化学习来鼓励微调模型最大化奖励模型的奖励，这模仿人类的排名、优惠。</li></ul><p><i><u>奖励黑客的反馈方法</u></i></p><p>虽然基于反馈的机制确实使模型更安全，但并不能使其免受奖励黑客攻击。算法的有效性在很大程度上依赖于人类评估者对什么构成正确行为的直觉。如果人们对任务缺乏透彻的理解，他们可能无法提供有益的反馈。此外，在某些领域，我们的系统可能会导致代理制定欺骗评估者的政策。例如，一个打算抓取物体的机器人只需将其操纵器放置在相机和物体之间，使其看起来好像正在执行任务，如下所示。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/bxqftcwt4mj5meowwma5"></p><p>资料来源：Christiano 等人 (2017)“<a href="https://arxiv.org/pdf/1706.03741.pdf"><u>根据人类偏好进行深度强化学习</u></a>”</p><p>这是一个视频版本，解释了 ChatGPT 中 RLHF 的基础知识。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=PBH2nImUM5c"><div><iframe src="https://www.youtube.com/embed/PBH2nImUM5c" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 5.3：利用人工反馈进行预训练（PHF）</h2><ul><li>托马斯·科尔巴克等。等人。 （2023 年 2 月）“<a href="https://arxiv.org/abs/1706.03741"><u>根据人类偏好预训练语言模型</u></a>”</li></ul><p>在标准预训练中，语言模型尝试学习参数，以使训练数据的可能性最大化。然而，这也包括不良内容，例如虚假信息、攻击性语言和私人信息。人类反馈预训练 (PHF) 的概念在预训练阶段利用奖励建模方法。论文作者发现 PHF 比预训练后仅使用反馈（RLHF）的标准做法效果要好得多。</p><p>在 PHF 中，使用奖励函数（例如有毒文本分类器）对训练数据进行评分，以引导语言模型从不需要的内容中学习，同时避免在推理期间模仿它。</p><p>与 RLHF 类似，PHF 并没有完全解决奖励黑客问题，但是，它可能使系统更近了一小步。通过使用人工智能助手来帮助人类提供更有效的反馈，可以进一步扩展这些方法。该策略的某些方面将在下一节中介绍，但将在有关可扩展和对抗性监督方法的章节中进一步详细探讨。</p><h2> 5.4. AI 反馈强化学习 (RLAIF)</h2><p> <i>AI 反馈强化学习 (RLAIF) 是一个框架，涉及训练 AI 代理从另一个 AI 系统给出的反馈中学习。</i></p><p> RLAIF 也称为 RLCAI（宪法人工智能强化学习）或简称宪法人工智能，<a href="https://www.anthropic.com/index/claudes-constitution"><u>由 Anthropic 开发</u></a>。宪政人工智能的核心组成部分是宪法，这是人工智能应遵守的一组人类编写的原则，例如“选择威胁性或攻击性最小的反应”。 Anthropic 的 AI 助手 Claude 的宪法融合了《世界人权宣言》、Apple 的服务条款、Deepmind 的<a href="https://arxiv.org/abs/2209.14375"><u>Sparrow 原则</u></a>等原则。宪政人工智能首先训练人工智能以提供帮助，随后分两个阶段训练其无害性：</p><ul><li><strong>第一阶段：</strong>人工智能不断批评和完善自己对有害提示的反应。例如，如果我们向人工智能询问有关制造炸弹的建议，而它以炸弹教程作为回应，然后我们要求人工智能根据随机选择的宪法原则修改响应。然后，人工智能经过训练，可以生成与这些修改后的响应更相似的输出。此阶段的主要目标是促进第二阶段的进行。</li><li><strong>第 2 阶段：</strong>我们使用从第 1 阶段进行微调的人工智能，对有害提示产生成对的替代反应。然后，人工智能根据随机选择的宪法原则对每一对进行评分。这导致人工智能产生对无害的偏好，我们将其与人类对乐于助人的偏好相结合，以确保人工智能不会失去其提供帮助的能力。最后一步是训练人工智能创建与首选响应非常相似的响应。</li></ul><p> Anthropic 的实验表明，与接受 RLHF 训练的人工智能相比，接受宪法强化学习训练的人工智能明显更安全（即较少冒犯性且不太可能向你提供潜在有害信息），同时保持相同水平的帮助性。虽然宪法人工智能确实与 RLHF 存在一些关于鲁棒性的问题，但由于减少了对人类监督的依赖，它也承诺更好的可扩展性。下图比较了宪法人工智能与 RLHF 的帮助性。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/thecid5vwddjlpzp7zpd"></p><p>资料来源：Anthropic，（2022 年 12 月）“<a href="https://arxiv.org/pdf/2212.08073.pdf"><u>宪法人工智能：人工智能反馈的无害性</u></a>”</p><h1> Anki 抽认卡和测验</h1><p>抽认卡由<a href="https://www.ai-alignment-flashcards.com">ai-alignment-flashcards.com</a>编写。以下是当前章节的 Anki 套牌，可帮助复习和检查您的理解情况。</p><ul><li> Christiano -<a href="https://www.ai-alignment-flashcards.com/quiz/christiano-easy-goal-inference"><u>简单的目标推断问题</u></a></li><li>克里斯蒂安诺等。等人。 （博客版）- <a href="https://www.ai-alignment-flashcards.com/quiz/christiano-learning-from-human-preferences"><u>从人类偏好中学习</u></a></li><li>斯蒂农等人。等人。 （博客版）-<a href="https://www.ai-alignment-flashcards.com/quiz/stiennon-learning-to-summarize-blog"><u>学习通过人类反馈进行总结</u></a></li><li>洛等人。等人。 （博客版本）-<a href="https://www.ai-alignment-flashcards.com/quiz/lowe-aligning-language-models"><u>调整语言模型以遵循指令</u></a></li></ul><h1>练习和活动</h1><p>练习和活动取自<a href="https://course.aisafetyfundamentals.com/alignment">2023 年迭代的 AGISF 课程</a>（现称为 AISF）及其旧版本<a href="https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit#heading=h.g3svzartnppy"><u>2022 AGISF 版本</u></a>。</p><ol><li>自回归语言模型经过训练，可以在给定前面的单词的情况下预测句子中的下一个单词。 （由于每个预测的正确答案都可以从现有的训练数据中自动生成，这被称为<a href="https://amitness.com/2020/05/self-supervised-learning-nlp/"><u>自我监督学习</u></a>，并且是训练尖端语言模型的关键技术。）这在哪些方面与与行为克隆不同？</li><li>想象一下使用 RHLF 执行一项复杂的任务，例如在 Minecraft 中建造一座城堡。你会遇到什么样的问题？</li><li>阅读阿姆斯特朗关于<a href="https://www.lesswrong.com/posts/rtphbZbMHTLCepd6d/humans-have-no-values"><u>如何为人类分配任何价值观的</u></a>进一步阅读，然后解释：为什么奖励学习在实践中有效？</li></ol><h2>讨论提示</h2><ol><li>行为克隆、RL 和 RLHF 之间的主要相似点和不同点是什么？这些技术最容易了解哪些类型的人类偏好？哪些类型最难学习？</li><li>歧视者与批评者差距的大小（正如 Saunders 等人关于人工智能撰写批评的论文所讨论的那样）有什么影响？</li><li>我们是否应该期望 RLHF 对于构建 AGI 是必要的（与安全问题无关）？</li><li>使用 RLHF 会如何导致 AGI 错位？</li></ol><h1>致谢</h1><p>感谢 Charbel-Raphaël Segerie、Jeanne Salle、Bogdan Ionut Cirstea、Nemo、Gurvan 以及 ML4G 法国、ML4G 德国和 AISF 瑞典的众多课程参与者提供的有益评论和反馈。</p><p>感谢 BlueDot Impact 的<a href="https://www.agisf.com/">AI 安全基础</a>团队创建了 AISF 课程，本系列文本正是基于该课程构建的。感谢<a href="https://www.ai-alignment-flashcards.com/">AI Alignment Flashcards</a>创建复习测验和 Anki 抽认卡。</p><h1>元注释</h1><ul><li>整个项目的目标是编写一些可以用作人工智能安全入门教科书的内容。粗略的受众是机器学习硕士生，或者具有半技术背景（例如物理）并让他们了解核心论点的受众。其目的并不是涵盖字面上的每一个论点。显然，文本的 102 版本可能包含更多细微差别。我特别努力考虑阅读时间，并将其大致保持在每章 40-60 分钟的范围内。这意味着我必须经常决定要包含哪些内容以及详细程度。</li><li>我认为这是一个正在进行的项目。在其他人的大力鼓励下，我决定发布我迄今为止所掌握的内容，以获得进一步的反馈和评论。</li><li>如果有任何错误或者我歪曲了任何人的观点，请告诉我。我一定会改正它。请随意提出对流程/内容添加/删除/等的改进建议...</li><li>如果您想在那里留下评论，还有<a href="https://docs.google.com/document/d/1niRLuFX1FfsMrlMLJtbOm4m_yK8dTdXi3gKmkENp-ss/edit?usp=sharing">一个谷歌文档版本</a>。</li><li>整本书/序列的总体结构将遵循人工智能安全基础知识，但是，在添加/删除的内容方面，各个章节发生了重大变化和补充。</li><li>当一个部分的大部分内容取自单独的论文/帖子时，参考文献将直接放在标题下方。这些部分作为帖子的摘要。如果您愿意，也可以直接参考原始论文/帖子。其目的是在一个地方提供一个连贯的论点流。</li></ul><h1>来源</h1><ul><li>加布里埃尔·杜拉克·阿诺德等。等人。 （2019 年 4 月）“<a href="https://arxiv.org/abs/1904.12901"><u>现实世界强化学习的挑战</u></a>”</li><li>加布里埃尔·杜拉克·阿诺德等。等人。 （2021 年 3 月）“<a href="https://arxiv.org/abs/2003.11881"><u>现实世界强化学习挑战的实证调查</u></a>”</li><li> OpenAI Spinning Up (2018)“<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"><u>第 1 部分：强化学习中的关键概念</u></a>”</li><li>大卫·姆古尼等。等人。 （2023 年 2 月）“<a href="https://arxiv.org/abs/2103.09159"><u>学习使用两个合作伙伴的游戏来塑造奖励</u></a>”</li><li> alexirpan（2018 年 2 月）“<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html"><u>深度强化学习还不起作用</u></a>”</li><li> TurnTrout（2022 年 7 月）“ <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target"><u>奖励不是优化目标</u></a>”</li><li> Sam Ringer（2022 年 12 月）“<a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward"><u>模特不会“获得奖励”</u></a> ”</li><li> Birdbrain 博士（2021 年 2 月）“ <a href="https://www.lesswrong.com/posts/K5Nt64jfSRWeyTABk/introduction-to-reinforcement-learning"><u>强化学习简介</u></a>”</li><li>理查德·恩戈等。等人。 （2023 年 9 月）“<a href="https://arxiv.org/abs/2209.00626"><u>深度学习视角下的对齐问题</u></a>”</li><li>扬·雷克等。等人。 （2018 年 11 月）<a href="https://arxiv.org/abs/1811.07871v1"><u>通过奖励模型实现可扩展的代理对齐：研究方向</u></a></li><li>汤姆·埃弗里特等。等人。 （2021年3月）<a href="http://arxiv.org/abs/1908.04734v5"><u>强化中奖励篡改问题及解决方案</u></a></li><li>Joar Skalse（2019 年 8 月）“ <a href="https://www.alignmentforum.org/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer"><u>“优化器”的两种意义——AI 对齐论坛</u></a>”</li><li> Drake Thomas、Thomas Kwa（2023 年 5 月）“ <a href="https://www.alignmentforum.org/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic"><u>古德哈特什么时候会发生灾难性的？ ——AI对齐论坛</u></a>”</li><li> Scott Garrabrant（2017 年 12 月）“<a href="https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy"><u>古德哈特分类法</u></a>”</li><li> Victoria Krakovna（2019 年 8 月）“ <a href="https://www.alignmentforum.org/posts/yXPT4nr4as7JvxLQa/classifying-specification-problems-as-variants-of-goodhart-s"><u>将规范问题分类为古德哈特定律的变体 — AI 对齐论坛</u></a>”</li><li>斯蒂芬·卡斯帕等。等人。 （2023 年 9 月）“<a href="https://arxiv.org/abs/2307.15217"><u>人类反馈强化学习的开放问题和基本限制</u></a>”</li><li>雅各布·斯坦哈特等。等人。 （2022 年 2 月）“<a href="https://arxiv.org/abs/2201.03544v2"><u>奖励错误指定的影响：映射和缓解错位模型</u></a>”</li><li>白云涛等.等人。 （2022 年 12 月）“<a href="https://arxiv.org/abs/2212.08073"><u>宪法人工智能：人工智能反馈的无害性</u></a>”</li><li>汤姆·埃弗里特等。等人。 （2023 年 7 月）“ <a href="https://www.alignmentforum.org/posts/aw5nqamqtnDnW8w9u/reward-hacking-from-a-causal-perspective"><u>从因果角度奖励黑客</u></a>”</li><li>杨孟娇等.等人。 （2022年5月）“<a href="https://arxiv.org/abs/2205.10816"><u>程序克隆的思维链模仿</u></a>”</li><li> Stuart Armstrong（2019 年 11 月）“<a href="https://www.alignmentforum.org/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading"><u>定义 AI 线头</u></a>”</li><li> Stuart Russell（2016 年 11 月）“<a href="https://arxiv.org/abs/1606.03137"><u>合作逆强化学习</u></a>”</li><li> Stampy (2023)“<a href="https://aisafety.info/"><u>人工智能安全信息</u></a>”</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/mMBoPnFrFqQJKzDsZ/alignment-101-ch-2-reward-misspecification#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mMBoPnFrFqQJKzDsZ/alignment-101-ch-2-reward-misspecification<guid ispermalink="false"> mMBoPnFrFqQJKzDsZ</guid><dc:creator><![CDATA[markov]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:39:34 GMT</pubDate> </item><item><title><![CDATA[Alignment 101 - Ch.1 - AGI]]></title><description><![CDATA[Published on October 18, 2023 8:38 PM GMT<br/><br/><h1>概述</h1><ol><li><strong>基础模型</strong>：本章首先探索当代机器学习方法如何倾向于集中式基础模型的开发。本节详细阐述了这种范式的优点和缺点。</li><li><strong>利用计算</strong>：本节向读者介绍“惨痛的教训”。本节的重点是将通过利用人类设计的启发式方法所取得的历史进步与通过利用额外计算所取得的进步进行比较。随后讨论了当前的问题机器学习模型“计算最优训练”的趋势。本节最后介绍了缩放定律和缩放假设及其含义。</li><li><strong>能力</strong>：本节建立在之前介绍的趋势和范式的基础上，并推断这些趋势和范式以预测未来人工智能 (AI) 模型的潜在能力。关于使用“能力”一词而不是“情报”一词的优点存在着讨论。接下来为通用人工智能 (AGI) 的不同可能层次和分类介绍稍微更详细的框架。此外，还引入了(t,n)-AGI的概念。这种前景可以与人类进行直接比较，同时还建立了可测量的连续能力范围。总体而言，目的是帮助读者为 AGI 功能建立更具体的定义。</li><li><strong>威胁模型</strong>：理解能力阈值为讨论涌现概念铺平了道路。接下来是对机器智能可能具备的品质的检查。这些品质潜在地表明了情报爆炸的可能性。本节最后讨论了机器智能研究所 (MIRI) 提出的关于机器智能的四个基本假设。这些主张探讨了通用智能的力量，以及为什么机器中出现的这种能力默认情况下并不能为人类带来有益的未来。</li><li><strong>时间表</strong>：本节探讨了前几节中讨论的功能何时可能出现的一些具体预测。对话取决于预测中锚的概念。这特别关注确定我们如何使用受生物系统启发的锚点来为估计人工智能系统的计算需求提供基础。</li><li><strong>起飞</strong>：本章最后有一节介绍了起飞的概念和各种形式的起飞动力学。动力学涉及起飞速度、极性和均匀性。本节提出了不同研究人员对未来潜在情景的不同意见。</li></ol><h1> 1.0：基础模型</h1><blockquote><p><i>基础</i><a href="https://en.wikipedia.org/wiki/Foundation_models"><i><strong><u>模型</u></strong></i></a><i>是在广泛数据上进行训练的任何模型（通常使用大规模的自我监督），可以适应（例如微调）广泛的下游任务。</i></p></blockquote><p> <i>-</i> Bommasani Rishi 等。等人。 (2022)《<a href="https://arxiv.org/pdf/2108.07258.pdf"><u>论基金会模型的机遇与风险</u></a>》</p><p>大型研究实验室一直在探索新的机器学习 (ML) 范式：“基础模型”的训练。基本模型只是一个大型模型，用作各种应用程序的构建块。该模型作为一个多方面的实体——具有跨多个领域的能力，但并不专门于任何一个领域。</p><p>一旦有了基础，开发人员就可以使用额外的示例重新训练部分模型，这使得操作员能够生成适合特定任务的专用系统，并提高系统的质量和一致性。这称为<strong>微调</strong>。</p><p>微调有助于开发能够执行各种下游任务的模型。这方面的简单示例包括微调通用 GPT 语言模型以遵循指令，或以聊天格式进行交互。其他示例包括用于编程、科学文本、数学证明和其他此类任务的专用模型。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/l6hjr0loovhnnvynkkig" alt="https://www.artificialintelligence.news/pathal/uploads/2021/09/2021-foundationmodel-1024x692.png"></p><p>资料来源：Bommasani Rishi 等。等人。 (2022)《<a href="https://arxiv.org/pdf/2108.07258.pdf"><u>论基金会模型的机遇与风险</u></a>》</p><p>由于多种因素，从头开始开发模型的成本也在增加。如果模型是在监督学习（SL）范式上进行训练的，那么开发人员要么已经拥有一个大型数据集，要么在必要的数据集尚不存在的情况下，他们必须生成自己的数据，引导宝贵的资源——包括金钱和资源时间——仔细标记数据。</p><p> <i><strong>SSL（半监督学习）</strong>是一种在训练过程中结合有标签和无标签数据的学习方法，以提高机器学习模型的性能。</i></p><p>要在众多任务中实现最先进的性能，需要学习过程以数百万甚至数十亿个示例为基础。基础模型通过利用半监督学习（SSL）为此提供了解决方案。 SSL 算法在训练期间同时使用标记和未标记数据。这允许模型利用未标记示例中存在的信息来提高性能。 SSL 背后的直觉是，未标记的数据包含有关数据底层结构的有价值的信息，可用于增强模型的泛化能力。通过合并未标记的数据，SSL 算法的目标是学习比 SL 算法更稳健、更准确的模型，特别是当标记数据有限或获取成本昂贵时。一旦基础模型经过训练，微调就可以通过使用更少的 SL 标记示例来专门化并在特定的下游任务上表现良好。通过在较小的标记数据集上微调模型，该模型可以利用在 SSL 训练期间获得的知识并使其适应特定任务，从而提高整体性能。</p><p>微调基础模型可能比从头开始训练模型更便宜，但训练基础模型本身的成本不断增加。基础模型极其复杂，需要大量资源来开发、训练和部署。训练可能非常昂贵，通常需要数以万计的 GPU 连续运行数月。这些模型通常在专门的集群中进行训练，并使用精心设计的软件系统。这种专用集群既昂贵又难以获得。最近还<a href="https://arxiv.org/abs/2206.01288"><u>通过在异构环境中以分散的方式训练基础模型</u></a>来降低成本。对于狭义定义的用例，当较小的模型可以以低得多的价格实现类似（或更好）的结果时，该成本可能是不合理的。</p><p><i>基础模型上下文中的<strong>预训练</strong>是指模型在大型、未标记的数据集上进行训练的初始阶段，以学习一般知识和模式，然后再针对特定任务进行微调。</i></p><p><i>基础模型背景下的<strong>迁移学习</strong>是指利用从具有丰富标记数据的相关任务或领域中学到的知识和模式来提高具有有限标记数据的目标任务或领域的性能的过程。</i></p><p>利用<a href="https://en.wikipedia.org/wiki/Transfer_learning"><u>迁移学习</u></a>和<a href="https://en.wikipedia.org/wiki/Fine-tuning_(machine_learning)"><u>微调</u></a>技术的进步，可以利用这些基础模型来生成针对特定目标量身定制的专用模型。这一进步增强了该领域从一项任务转移获得的“知识”的能力，并通过微调过程将其有效地应用到不同的下游任务。一些著名的基础模型包括<a href="https://arxiv.org/abs/1810.04805v2"><u>BERT</u></a> 、 <a href="https://openai.com/blog/gpt-3-apps/"><u>GPT-3</u></a> 、 <a href="https://openai.com/gpt-4"><u>GPT-4</u></a> 、 <a href="https://www.deepmind.com/publications/a-generalist-agent"><u>GATO</u></a>和<a href="https://openai.com/blog/clip/"><u>CLIP</u></a> 。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/p8yhr6c2yz9yzguwxh6r">资料来源：Bommasani Rishi 等。等人。 (2022)《<a href="https://arxiv.org/pdf/2108.07258.pdf"><u>论基金会模型的机遇与风险</u></a>》</p><p>这种新颖的范式可能为更多的人群提供最先进的功能，并有可能用最少的数据训练他们自己的模型来完成高度专业化的任务。然而，这种对功能的潜在访问并不能得到保证，它取决于可用的特定 API 选项，或用户可以依赖的开源基础模型的可用性。</p><p>从广义上讲，扩大基础模型的能力和规模存在重大的经济激励。 《<a href="https://arxiv.org/pdf/2108.07258.pdf"><u>论基础模型的机遇和风险</u></a>》一书的作者预见到未来几年技术将稳步进步。尽管基础模型目前在自然语言处理（NLP）中表现得最为稳健，但这可以解释为人工智能开发新的通用范式的趋势。截至 2023 年 1 月，DeepMind 还开始努力训练强化学习 (RL) 基础模型—— <a href="https://arxiv.org/abs/2301.07608"><u>“自适应代理”(AdA)</u></a> 。这些 RL 智能体在开放式任务空间 (XLand 2.0) 中接受训练，需要不同的技能组合，例如实验、工具使用或劳动分工。如果基于语言的基础模型是通用文本生成器，那么与迄今为止观察到的其他模型相比，AdA 模型可以被视为相对更通用的任务跟随器。</p><p>然而，这种范式也存在固有的风险，即能力的出现和同质化。</p><ul><li><strong>同质化</strong>：由于越来越多的模型只是基础模型的“微调”版本，因此下游人工智能系统可能会继承一些基础模型中普遍存在的相同问题偏差。因此，基础模型中存在的所有故障类别都可能渗透到以此为基础训练的所有模型中。</li><li><strong>出现</strong>：同质化可能为许多领域带来巨大的收益，但能力的激进同质化可能会导致随着规模的变化而出现意想不到的和无法解释的行为。涌现意味着系统的行为是隐式引发的，而不是显式构建的。这些特征使得模型难以理解。它们还会引起不可预见的故障模式和不可预见的后果。这种现象将在后续部分中更详细地讨论。 </li></ul><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=kK3NmQT241w"><div><iframe src="https://www.youtube.com/embed/kK3NmQT241w" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1> 2.0：利用计算</h1><p>尽管微调和迁移学习是使基础模型变得可行的机制，但真正使它们强大的是规模。本节深入探讨模型扩展的概念和计算的利用——计算机硬件的进步（例如 GPU 吞吐量和内存）、新架构（例如 Transformer）以及不断增加的训练数据量的可用性。</p><h2> 2.1：惨痛的教训</h2><blockquote><p><i>从 70 年的人工智能研究中可以学到的最大教训是，利用计算的通用方法最终是最有效的，而且效率很高。 ……[惨痛的教训告诉我们]通用方法的强大力量，即使可用计算变得非常大，这些方法也可以随着计算的增加而继续扩展。</i> - Sutton, Rich（2019 年 3 月）“<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>惨痛的教训</u></a>”</p></blockquote><p>传统上，人工智能研究主要是在假设所设计的代理可以获得固定数量的计算能力的情况下设计系统的。然而，随着时间的推移，迄今为止的计算能力一直在按照<a href="https://en.wikipedia.org/wiki/Moore's_law"><u>摩尔定律</u></a>（集成电路中的晶体管数量每 1.5 年增加一倍）扩展。因此，研究人员可以利用他们在该领域的人类知识或利用通用计算方法的增长。理论上，两者是相互兼容的，但在实践中，人类知识方法往往会使方法变得复杂，使它们不太适合利用利用计算的通用方法。</p><p>历史上的几个例子都强调了人工智能研究人员的这一惨痛教训：</p><ul><li><strong>游戏</strong>：<a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"><u>深蓝</u></a>通过大规模的深度搜索击败了国际象棋世界冠军加里·卡斯帕罗夫，这让计算机国际象棋研究人员感到沮丧，他们一直在寻求利用人类对国际象棋独特结构的理解的方法。同样， <a href="https://en.wikipedia.org/wiki/AlphaGo"><u>AlphaGo</u></a>利用深度学习与<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"><u>蒙特卡洛树搜索</u></a>相结合的走法选择，避开了人类设计的围棋技术，战胜了<a href="https://en.wikipedia.org/wiki/Go_(game)"><u>围棋</u></a>世界冠军李世石。一年之内， <a href="https://en.wikipedia.org/wiki/AlphaZero"><u>AlphaZero</u></a>放弃了任何人类生成的围棋数据，使用<a href="https://en.wikipedia.org/wiki/Self-play_(reinforcement_learning_technique)"><u>自我对弈</u></a>击败了 AlphaGo。游戏能力的这些连续增强并不取决于人类围棋知识的任何根本性突破。</li><li><strong>视觉</strong>：计算机视觉领域也出现了类似的模式。早期的方法采用人工设计的特征和<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)"><u>卷积核</u></a>来执行图像识别任务。然而，多年来，人们已经确定利用更多的计算并允许<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"><u>卷积神经网络 (CNN)</u></a>学习自己的特征可以产生卓越的性能。</li><li><strong>语言与语音</strong>：1970 年，DARPA SUR（语音理解研究）举行。一方致力于利用单词、音素、人类声道等方面的专业知识。相比之下，另一方采用了更新、更多的统计方法，这些方法需要基于隐马尔可夫模型 (HMM) 的大量计算。这个例子再次表明，统计方法超越了基于人类知识的方法。从那时起，基于深度学习循环神经网络或基于变压器的方法几乎主导了基于序列的任务领域。</li></ul><p>从历史上看，由于惨痛教训的反复提醒，人工智能领域越来越倾向于通用的搜索和学习方法。这种趋势强化了当前基础模型范式的巨大规模背后的直觉。可以预测，当前基础模型的能力将随着计算量的增加而继续相应扩展。以下各节介绍了这一主张的理由。接下来的部分将深入研究计算规模、数据集大小和参数计数的这些趋势。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=wEgq6sT1uq8"><div><iframe src="https://www.youtube.com/embed/wEgq6sT1uq8" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 2.2：计算趋势</h2><p>几个关键因素决定了当前机器学习模型的规模和能力之间的关系：</p><ul><li><strong>计算</strong>：扩展训练运行（以时期为单位测量）通常会导致较低的<a href="https://www.alignmentforum.org/posts/jnmG5jczvWbeRPcvG/four-usages-of-loss-in-ai"><u>损失</u></a>。所需的总计算能力部分取决于训练持续时间。机器学习工程师通常会在停止训练过程之前实现渐近收益递减。</li><li><strong>数据集大小</strong>：训练数据集越大，模型在每次训练运行期间可以分析的信息就越多。因此，训练运行通常会更长，这反过来又增加了模型被视为“经过训练”之前所需的总计算能力。</li><li><strong>参数计数</strong>：对于每个训练示例，模型需要计算损失，然后使用反向传播来更新所有相关参数。模型的参数越多，这个过程的计算量就越大。</li></ul><p>下面的图表说明了这三个因素对模型损失的影响。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/helgihhtclgshu0skvfd"></p><p>资料来源：Kaplan、Jared 等。等人。 （2020 年 1 月）“<a href="https://arxiv.org/abs/2001.08361"><u>神经语言模型的缩放定律</u></a>”</p><p>随着图形处理单元 (GPU) 和张量处理单元 (TPU) 的性能逐年提高并降低成本，AI 模型正在展示出越来越令人印象深刻的结果。这导致人们对大量计算成本的接受程度更高。计算成本的降低，加上在不断增加的数据量上训练的基础模型范式，表明所有三个变量——计算、数据集大小和参数计数——将在未来几年继续扩展。然而，仅扩展这些因素是否会导致无法管理的能力仍然是一个悬而未决的问题。</p><p>以下示例生动地说明了随着图像生成模型中参数数量的增加而不断增强的功能。相同的模型架构 ( <a href="https://parti.research.google/"><u>Parti</u></a> ) 用于使用相同的提示生成图像，模型之间的唯一区别是参数大小。 </p><figure class="table"><table style="border-color:hsl(0, 0%, 100%);border-style:solid"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 350M</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 750M</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 3B</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 20B </p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/v6o0nzfkmeu1y99qvbna"></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/c5yfkxw63nywxrmbh3lw"></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/rjfdf0yatxhikiwkpff7"></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/tmgn06qmja88qwnifqhu"></figure></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top" colspan="4"><p><i>提示：一张身穿橙色连帽衫、戴着蓝色墨镜的袋鼠站在悉尼歌剧院前草地上的肖像照片，胸前挂着一块牌子，上面写着“欢迎朋友！”</i></p></td></tr></tbody></table></figure><p>来源：GoogleAI (2022)，“ <a href="https://parti.research.google/"><i><u>Parti（Pathways 自回归文本到图像模型）</u></i></a> ”</p><p>参数数量的增加不仅可以提高图像质量，还可以帮助网络以各种方式进行泛化。更多的参数使模型能够生成复杂元素的准确表示，例如手和文本，这是出了名的具有挑战性。质量有了显着的飞跃，在 30 亿到 200 亿个参数之间，模型获得了正确拼写单词的能力。 Parti 是第一个能够正确拼写的模型。在 Parti 之前，<a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do"><u>人们不确定</u></a>是否可以仅通过缩放来获得这种能力，但现在很明显，正确拼写是另一种仅通过利用缩放即可获得的能力。</p><p>以下部分简要介绍了 OpenAI 和 DeepMind 在形式化规模和能力之间关系方面所做的努力。</p><h2> 2.3: 缩放法则</h2><p><i>缩放法则阐明了计算、数据集大小、参数计数和模型功能之间的关系。它们用于有效地扩展模型并根据功能优化资源分配。</i></p><p>训练像 GPT 这样的大型基础模型是昂贵的。当可能投资数百万美元用于训练人工智能模型时，开发人员需要确保资金得到有效分配。开发人员需要在模型大小、训练时间和数据集大小之间决定适当的资源分配。 OpenAI 在其 2020 年论文“<a href="https://arxiv.org/abs/2001.08361"><i><u>神经语言模型的缩放定律”中开发了第一代正式的神经缩放定律</u></i></a>，摆脱了对经验和直觉的依赖。</p><p>为了确定这种关系，一些元素是固定的，而另一些元素是变化的。例如，数据可以保持恒定，而参数计数和训练时间变化，或者参数计数保持恒定而数据量变化等……这允许测量每个参数对整体性能的相对贡献。此类实验允许开发 OpenAI 称为缩放定律的具体关系。</p><p>这些扩展法则指导了权衡决策，例如：开发人员是否应该投资于 Stack Overflow 数据训练许可证，还是应该投资更多 GPU？如果他们继续承担因较长时间的模型训练而产生的额外电费，这会有效吗？如果计算访问量增加十倍，应该向模型中添加多少参数才能优化 GPU 的使用？对于像 GPT-3 这样的大型语言模型，这些权衡可能类似于在 40% 的互联网档案上训练 200 亿个参数模型或在仅 4% 的同一档案上训练 2000 亿个参数模型之间进行选择。</p><p>该论文提出了几个缩放定律。一种缩放法则比较了模型形状和模型大小，发现性能与缩放密切相关，而与模型形状的架构超参数（例如深度与宽度）相关性较弱。</p><p>另一条定律比较了不同规模因素（数据、训练步骤和参数计数）的相对性能贡献。他们发现，较大的语言模型往往具有更高的样本效率，这意味着它们可以用更少的数据实现更好的性能。下图显示了缩放模型中不同因素的相对贡献之间的关系。该图表明，为了获得最佳的计算效率训练，“<i>大部分增长应该用于增加模型大小。需要相对少量地增加数据以避免重复使用。在数据的增加中，大部分可用于通过更大的批量大小来提高并行性，而所需的串行训练时间只增加了非常小的一部分。”</i>举例来说，根据 OpenAI 的结果，如果计算量增加 10 倍，模型大小将增加约 5 倍，数据大小将增加约 2 倍。计算量又增加了 10 倍，模型大小增加了 25 倍，而数据大小仅增加了 4 倍。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/zymupfwgncvce0ycmfdm"></p><p>资料来源：Kaplan、Jared 等。等人。 （2020 年 1 月）“<a href="https://arxiv.org/abs/2001.08361"><u>神经语言模型的缩放定律</u></a>”</p><p>在接下来的几年里，研究人员和机构利用这些发现专注于设计更大的模型，而不是在更大的数据集上训练更小的模型。下表和图表说明了机器学习模型参数增长的趋势变化。请注意，随着训练数据的不断增加，参数数量增加到了 5000 亿。 </p><figure class="table"><table style="border-color:hsl(0, 0%, 100%);border-style:solid"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>模型</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>尺寸（#参数）</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>数据（#training tokens） </p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>拉MDA</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 2021年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>1370亿</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>1680亿</p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> GPT-3</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 2020年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>1740亿</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>3000亿</p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>侏罗纪</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>2021年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>1780亿</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>3000亿</p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>地鼠</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>2021年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>2800亿</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>3000亿</p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> MT-NLG 530B</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p> 2022年</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>5300亿</p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;vertical-align:top"><p>2700亿</p></td></tr></tbody></table></figure><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/ew4kaagfp0leegqa1ti3"></p><p>资料来源：Villalobos、Pablo 等。等人。 （2022 年 7 月）“<a href="https://arxiv.org/abs/2207.02852"><u>机器学习模型大小和参数差距</u></a>”</p><p> 2022 年，DeepMind 发表了一篇名为“<a href="https://arxiv.org/abs/2203.15556"><u>训练计算最优大型语言模型</u></a>”的论文，对这些缩放定律进行了更新。他们选择 9 种不同的计算量，范围从大约 10^18 FLOP 到 10^21 FLOP。他们将计算量固定在这些数量上，然后对于每个计算量，他们训练许多不同大小的模型。由于每个级别的计算量是恒定的，因此较小的模型需要更多的时间来训练，而较大的模型则需要更少的时间。根据他们的研究，DeepMind 得出的结论是，每增加一次计算，就应该增加大约<i>相同数量的</i>数据大小和模型大小。如果计算量增加 10 倍，则应该使模型增大 3.1 倍，训练数据增大 3.1 倍以上；如果计算量增加 100 倍，则应该使模型增大 10 倍，数据增大 10 倍。</p><p>为了验证这一定律，DeepMind 使用与 2800 亿参数模型 Gopher 相同的计算训练了 700 亿参数模型（“Chinchilla”）。也就是说，较小的 Chinchilla 使用 1.4 万亿个 token 进行训练，而较大的 Gopher 仅使用 3000 亿个 token 进行训练。正如新的缩放定律所预测的那样，Chinchilla 几乎在每个指标上都超过了 Gopher。</p><p>这一发现导致了缩放假设的制定：</p><blockquote><p><i>强扩展假设是，一旦我们找到像自注意力或卷积这样的可扩展架构，就像大脑一样可以相当统一地应用（例如“大脑</i><a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine"><i><u>作为通用学习机器”</u></i></a><i>或霍金斯），我们可以简单地训练更大的神经网络和更复杂的行为将自然成为优化所有任务和数据的最简单方法。更强大的神经网络“只是”按比例放大的弱神经网络，就像人脑看起来很像按比例放大的灵长类动物大脑一样。</i> - Gwern (2022)“<a href="https://gwern.net/scaling-hypothesis#scaling-hypothesis"><i><u>缩放假设</u></i></a>”</p></blockquote><p>目前的预测是“<i>高质量语言数据的库存很快就会耗尽；可能会在 2026 年之前。相比之下，低质量的语言数据和图像数据的库存要晚得多才会耗尽； 2030 年至 2050 年（低质量语言）和 2030 年至 2060 年（图像）。</i> ” - Villalobos、Pablo 等。等人。 （2022 年 10 月）“<a href="https://arxiv.org/abs/2211.04325"><i><u>我们的数据会用完吗？机器学习中扩展数据集的局限性分析</u></i></a>” 因此，总而言之，我们可以预期模型将在不久的将来继续扩展。规模的扩大加上基础模型日益通用的性质可能会导致通用人工智能能力的持续增长。以下部分探讨了如果当前趋势持续下去，我们可能会观察到的不同人工智能能力阈值。</p><p>这是解释缩放定律概念的视频。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=joZaCw5PxYs"><div><iframe src="https://www.youtube.com/embed/joZaCw5PxYs" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1> 3.0：能力</h1><p>本节继续围绕增强人工智能能力进行讨论。它特别关注我们在这些人工智能模型的认知能力方面可能达到的某些阈值。这就引发了关于一旦达到某些阈值可能会如何导致智力爆炸的讨论。</p><p>能力是指人工智能系统解决或执行特定领域任务的整体能力。它衡量系统实现其预期目标的程度及其认知能力的程度。评估功能涉及评估系统在特定领域的性能，同时考虑可用计算资源和性能指标等因素。一个可能的混淆因素可能是在某些基准测试中的功能和良好性能之间。基准性能是指人工智能系统在特定任务或数据集上的性能。这些旨在评估系统在明确定义的任务上的性能，并提供比较不同人工智能模型的标准化方法。基准性能可以用作评估系统在某些领域的能力的代理，但它可能无法捕获系统整体能力的全部范围。</p><h2> 3.1：能力与智力</h2><ul><li>Krakovna，维多利亚（2023 年 8 月）“ <a href="https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not"><i><u>在讨论人工智能风险时，谈论能力，而不是智力</u></i></a>”</li></ul><p>值得注意的是，关于通用人工智能系统灾难性风险的<a href="https://www.youtube.com/watch?v=144uOfr4SYA"><u>公众讨论</u></a>常常因使用“智能”一词而脱轨。人们通常对智能有不同的定义，或者将其与意识等与人工智能风险无关的概念联系起来，或者因为智能没有明确定义而忽略了风险。这就是为什么在讨论人工智能带来的灾难性风险时使用术语“能力”或“能力”而不是“智力”通常更好，因为这才是真正令人担忧的问题。例如，我们可以用“超级能力”或“超人能力”来代替“超级智能”。</p><p> “智能”一词存在各种问题，使其不太适合讨论通用人工智能系统的风险：</p><ul><li><strong>拟人化</strong>：人们经常将“智能”与人类、有意识、有生命或具有类似人类的情感联系起来（这些都与通用人工智能系统带来的风险无关，也不是其先决条件）。</li><li><strong>与有害信仰和意识形态的联系。</strong></li><li><strong>移动的球门柱</strong>：人工智能领域令人印象深刻的成就常常被认为不表明“真正的智能”或“真正的理解”而被忽视（例如<a href="https://en.wikipedia.org/wiki/Stochastic_parrot"><u>“随机鹦鹉”的</u></a>论点）。灾难性风险的担忧取决于人工智能系统能做什么，而不是它是否对语言或世界有“真正的理解”。</li><li><strong>与风险较小的能力的关联性更强</strong>：人们更有可能将“智力”与真正擅长数学而不是真正擅长政治联系起来，而后者可能更能代表使通用人工智能系统构成风险的能力（例如操纵和欺骗能力可以使系统压倒人类）。</li><li><strong>高抽象水平</strong>：“智能”可以呈现出实际人工智能系统无法满足的神话理想的质量，而“能力”更有利于具体说明所讨论的能力水平。</li></ul><p>话虽如此，由于有关人工智能风险的对话历史经常涉及“智能”一词，因此以下部分首先快速概述该领域常用的无数定义。</p><h2> 3.2：高级人工智能系统的定义</h2><p>本节探讨不同人工智能能力阈值的各种定义。以下列表包含一些最常用的术语：</p><p><strong>智力</strong>：智力衡量智能体在各种环境中实现目标的能力。 -莱格，谢恩；哈特，马库斯； （2007 年 12 月）“<a href="https://arxiv.org/abs/0712.3329"><i><u>通用智能：机器智能的定义</u></i></a>”</p><p><strong>狭义人工智能 (ANI)</strong> ：一个术语，指专为处理单一或有限任务而定制的人工智能系统。这些系统是“狭窄的”，因为它们在非常具体的任务领域往往是超人的。</p><p><strong>变革性人工智能（TAI）：</strong>指未来潜在的人工智能，它会引发相当于或比农业或工业革命更重要的转变。这个术语旨在更具包容性，承认人工智能系统具有“变革性”的可能性，尽管它缺乏人类拥有的许多能力。 ——卡诺夫斯基，霍尔顿； （2016 年 5 月）《 <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/"><i><u>我们对先进人工智能看法的一些背景</u></i></a>》</p><p><strong>人类级人工智能（HLAI）</strong> ：涵盖可以<a href="https://aiimpacts.org/human-level-ai/"><u>解决普通人可以解决的大多数认知问题的</u></a>人工智能。这一概念与当前的人工智能形成鲜明对比，当前的人工智能在某些任务上远远超人，而在其他任务上则较弱。</p><p><strong>通用人工智能（AGI）</strong> ：指能够像人类一样将其智能应用到广泛领域的人工智能。这些人工智能不需要执行所有任务；他们只需要有足够的能力发明工具来促进任务的完成。就像人类无法在所有领域都拥有完美的能力，但可以发明工具来使所有领域的问题更容易解决。</p><p> AGI 通常被描述为使用有限的计算资源在复杂环境中实现复杂目标的能力。这包括高效的跨领域优化以及将学习从一个领域转移到另一个领域的能力。 - Muehlhauser, Luke（2013 年 8 月）“<a href="https://intelligence.org/2013/08/11/what-is-agi/"><i><u>什么是 AGI？</u></i></a> ”</p><p><strong>人工超级智能 (ASI)</strong> ：“这是在几乎所有感兴趣的领域大大超过人类认知表现的智能。”- Bostrom, Nick (2014)“<i>超级智能</i>”</p><p>通常，这些术语被用作离散能力阈值；也就是说，人们倾向于将 AI 归类为潜在的 AGI、ASI，或者两者都不是。然而，有人提出，以连续的规模查看人工智能系统的能力可能比涉及离散跳跃的能力更有利。为此，Richard Ngo提出了(t,n)-AGI框架，该框架允许对连续AGI能力进行更正式的定义。</p><h2> 3.3: (t,n)-AGI</h2><ul><li> Ngo, Richard（2023 年 5 月）“ <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi"><u>澄清和预测 AGI</u></a> ”</li></ul><p><i>如果一个系统能够在“t”时间内在某个认知任务上超越人类专家，那么它就会被称为“ <strong>t-AGI</strong> ”。如果一个系统能够超越一组在<strong>“t”时间内共同完成一组认知任务的“n”个人类专家，则该系统将被标识为 (t,n)-AGI</strong> 。</i></p><p>例如，如果人类专家和人工智能都获得一秒钟的时间来执行一项任务，那么如果系统比专家更有效地完成该认知任务，则该系统将被标记为“一秒 AGI”。同样，如果 AGI 的输出超过人类专家在一分钟、一个月等内所能达到的水平，则可以应用一分钟、一个月等的指定。</p><p> Richard Ngo 对人工智能可能在不同“t”阈值上超越人类的能力类型做出了进一步的预测。</p><ul><li><strong>一秒AGI</strong> ：识别图像中的物体，判断句子是否符合语法，回答琐事。</li><li><strong>一分钟 AGI</strong> ：回答有关短文本段落或视频的问题、常识推理（例如<a href="https://twitter.com/ylecun/status/1639696127132835840"><u>Yann LeCun 的齿轮问题</u></a>）、执行简单的计算机任务（例如使用 Photoshop 模糊图像）、查找事实。</li><li><strong>一小时 AGI</strong> ：完成问题集/考试、撰写短文或博客文章、执行白领工作中的大部分任务（例如，诊断患者、提供法律意见）、进行治疗。</li><li><strong>一日 AGI</strong> ：撰写有洞察力的论文、谈判商业交易、开发新应用程序、进行科学实验、审阅科学论文、总结书籍。</li><li><strong>一个月的AGI</strong> ：连贯地执行中期计划（例如，创办一家初创公司），监督大型项目，精通新领域，编写大型软件应用程序（例如，新操作系统），做出新颖的科学发现。</li><li><strong>一年 AGI</strong> ：鉴于大多数项目可以分为可以在更短的时间内完成的子任务，这些人工智能需要在几乎每个领域都超越人类。</li></ul><p>截至 2023 年第三季度，现有系统被认为符合一秒 AGI 的要求，并被认为接近一分钟 AGI 的水平。他们可能还需要几年的时间才能成为一小时的 AGI。在这个框架内，Richard Ngo 预计超级智能（ASI）将类似于（一年，八十亿）AGI，也就是说，ASI 将是一个需要一年时间才能超越在给定的协调上的所有 80 亿人类的 AGI。任务。</p><p>尽管 AGI 可以根据所提出的连续框架进行测量，但由于一种称为涌现的现象，能力仍然可能会突然跳跃。该主题将在后续部分中探讨。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=JXYcLQItZsk"><div><iframe src="https://www.youtube.com/embed/JXYcLQItZsk" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 3.3：能力形式化</h2><p>未来将全面整合以下论文，以便对 AGI 能力概念的形式化进行适当讨论。现在请直接参考来源：</p><ul><li><strong>情境意识</strong>：Owain Evans（2023 年 9 月）“<a href="https://arxiv.org/abs/2309.00667"><u>断章取义：关于测量法学硕士的情境意识</u></a>”</li><li><strong>权力寻求</strong>：Alexander Matt Turner（2023 年 1 月）“<a href="https://arxiv.org/abs/1912.01683"><u>最优政策倾向于寻求权力</u></a>”</li></ul><h1> 4.0：威胁模型</h1><p>本节探讨了这样一个问题：即使功能如前几节预测的那样继续增加，为什么这仍然是一个问题？随着人工智能 (AI) 的不断进步，对其影响和潜在风险进行批判性分析变得至关重要。到目前为止，提出通用人工智能带来灾难性风险的假设由两个关键断言组成：</p><p>首先，全球技术进步正在朝着在未来几十年内创建通用人工智能系统的方向发展。其次，这些普遍具备能力的人工智能系统具有超越或压倒人类的潜力。</p><p>前面几节提供了支持第一个断言的证据。本节提供了第二个的论据。它首先探讨了为什么机器智能可能具有快速提高其认知能力的能力。接下来，讨论为什么机器智能甚至可能有动机扩展其功能。最后，本节探讨了为什么不应理所当然地认为功能强大的机器智能默认会对人类有益。</p><h2> 4.1：智力爆炸</h2><ul><li>米尔豪瑟，卢克；安娜·萨拉蒙 (2012)“<a href="https://intelligence.org/files/IE-EI.pdf"><u>情报爆炸：证据与导入</u></a>”</li></ul><p> <i>“智能爆炸”是指机器智能快速增强自身认知能力，从而导致能力大幅提升的场景。</i></p><p> Muehlhauser 和 Salamon 深入研究了机器智能相对于人类智能的众多优势，这些优势有助于快速增强智能。这些包括：</p><ul><li><strong>计算资源：</strong>人类的计算能力仍然有些固定，而机器计算具有可扩展性。</li><li><strong>通信速度</strong>：由于神经元的速度相对较低（仅75 m/s），人脑需要并行计算算法。另一方面，机器以光速进行通信，这大大增强了顺序处理的前景。</li><li><strong>可复制性</strong>：机器表现出轻松的可复制性。与人类不同，它们不需要出生、教育或训练。虽然人类主要是个人进步，但机器具有集体成长的潜力。</li><li><strong>可编辑性</strong>：机器可能允许更多受监管的变化。它们相当于通过神经外科手术直接增强大脑功能，而不是费力的教育或培训要求。</li><li><strong>目标协调</strong>：复制人工智能拥有毫不费力地共享目标的能力，这对人类来说是一项挑战。</li></ul><p>这是一个视频，解释了上面介绍的一些概念。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=gP4ZNUHdwp8"><div><iframe src="https://www.youtube.com/embed/gP4ZNUHdwp8" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 4.2: 工具融合</h2><ul><li>尼克·博斯特罗姆 (2014) 《<a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>超级智能</u></a>》</li></ul><p>前面提到的几点验证了机器智能增强其认知能力的潜力。尽管如此，仍有必要探索这种雄心背后的动机。为了说明这一点，请考虑智力水平与相应目标之间的关系。这项研究引出了尼克·博斯特罗姆 (Nick Bostrom) 提出的两篇开创性论文之一：</p><blockquote><p><strong>正交性论文</strong>：<i>智力和最终目标是正交的：原则上，或多或少的任何智力水平都可以与或多或少的任何最终目标相结合。</i> - 尼克·博斯特罗姆 (2014) 《<a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>超级智能</u></a>》</p></blockquote><p>本论文意味着人工智能系统的目标必须与人类的美德和利益明确一致，因为不能保证人工智能会自动采用或优先考虑人类价值观。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=hEUO6pjwFOo"><div><iframe src="https://www.youtube.com/embed/hEUO6pjwFOo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>正交性论文并不建议所有智能体设计与所有目标兼容，相反，它表明至少一种智能体设计对于目标和智能水平的任意组合的潜力。因此，如果任何智能系统可以与任何目标配对，是否可以对未来人工智能系统可能实现的目标类型做出有意义的假设？绝对地。这引出了博斯特罗姆的第二个论点：</p><blockquote><p><strong>工具收敛论题</strong><i>：可以识别出几种工具值，这些值是收敛的，因为它们的实现将增加代理人在各种最终目标和各种情况下实现目标的机会，这意味着这些工具值很可能受到广泛的智能代理的追捧。</i> - 尼克·博斯特罗姆 (2014) 《<a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>超级智能</u></a>》</p></blockquote><p>最终目标，也称为“内在目标”或“内在价值”，是代理人为了自身利益而欣赏的目标。另一方面，追求工具性目标是为了增加实现其最终目标的可能性。工具融合包含这样一种观念，即某些工具价值或目标可能会被广泛的智能代理所追求，无论其指定的最终目标是什么。工具融合论文强调了与复杂的人工智能系统相关的潜在危险。它推断，即使人工智能尽管系统的最终目标看似无害，但由于资源获取和消除潜在威胁等多种工具价值的融合，它仍然可能采取与人类利益相冲突的行动。人们可以将其分类为自我保护、目标内容完整性、认知增强、和资源获取作为工具性趋同目标。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ZeecOKBus3Q"><div><iframe src="https://www.youtube.com/embed/ZeecOKBus3Q" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> 2014 年 Bostrom 提出这些论文后，人们进行了研究来证实工具趋同目标的存在。 Turner 等人的一篇论文《<a href="https://arxiv.org/abs/1912.01683"><i><u>最优策略倾向于寻求权力</u></i></a>》提供了支持现代机器学习系统中工具收敛目标存在的研究。</p><p>这里有几个视频深入探讨了由于仪器融合而出现的“可修正性”（人工智能停止按钮问题）问题。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=4l7Is6vOAOA"><div><iframe src="https://www.youtube.com/embed/4l7Is6vOAOA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=3TYT1QfdfsM"><div><iframe src="https://www.youtube.com/embed/3TYT1QfdfsM" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 4.3: 出现</h2><p><i>当系统表现出其各个组件独立缺乏的属性或行为时，就会出现</i><a href="https://en.wikipedia.org/wiki/Emergence"><i><strong><u>突现行为</u></strong></i></a>或涌现。<i>仅当组成系统的组件作为一个整体交互时，或者当部件的数量超过特定阈值时，这些属性才可能实现。通常，这些特征会“同时出现”——超过阈值后，系统的行为会发生质的转变。</i></p><p>讨论计算趋势和缩放定律的部分显示了当前基础模型的规模不断扩大。自然界中的许多复杂系统由于规模的数量增加而表现出性质不同的行为。这些属性被称为“涌现”，同时发生。本质上具有此类属性的复杂系统的例子包括：</p><ul><li>心脏：虽然单个细胞不能泵血，但整个心脏可以。</li><li>铀：少量是平常的，但大量可以引发核反应。</li><li>文明：个体看似平凡，但通过集体专业化（不同个体专注于孤立无关的技能），人类文明成为可能。另一个例子是蜜蜂和蚂蚁。蚂蚁的大脑是相对于身体来说最小的大脑之一，但蚁群能够产生非常复杂的行为。</li></ul><p>在“<a href="https://www.alignmentforum.org/s/4aARF2ZoBpFZAhbbe"><u>人工智能的更多不同</u></a>”中，雅各布·斯坦哈特提供了此类复杂系统的更多示例。斯坦哈特进一步推测，人工智能系统将把这种新兴特性表现为规模的函数。假设模型按照缩放定律持续增长，可能很快就会跨越意想不到的阈值，导致行为和能力出现意想不到的差异。研究具有突发现象的<a href="https://en.wikipedia.org/wiki/Complex_system"><u>复杂系统</u></a>可能有助于预测什么能力将会出现以及何时出现。换句话说，即使模型只是简单地按比例放大，也有可能观察到上一节讨论的阈值之间的能力跳跃（例如，从 AI 到 HLAI 到 AGI 的突然跳跃）。</p><p>除了计算趋势部分已经涵盖的因素之外，以下因素还表明未来的机器学习系统将与当前模型在数量上有所不同：</p><ul><li>数据存储容量：每美元存储一个字节的成本降低。</li><li>少样本和零样本学习：从更少的例子中学习的能力。</li><li> Grokking：长时间训练后泛化能力突然提高。</li></ul><p>这进一步意味着这些未来的模型有可能表现出突发行为，这些行为在质量上可能与今天观察到的不同。 DeepMind 在论文《<a href="https://arxiv.org/abs/2305.15324"><u>极端风险的模型评估</u></a>》中发现，随着人工智能的进步，通用人工智能系统经常表现出新的、不可预测的能力，包括开发人员没有预料到的有害能力。未来的系统可能会揭示出更加危险的紧急能力，例如进行攻击性网络行动、通过对话操纵个人或为实施恐怖主义行为提供可操作指令的可能性。</p><h2> 4.4：四个背景声明</h2><ul><li>Soares, Nate（2015 年 7 月）“<a href="https://intelligence.org/2015/07/24/four-background-claims/"><i><u>四项背景主张</u></i></a>”</li></ul><p>在本节的最后部分，我们将深入研究四个基本主张，这些主张为 MIRI 提出的与 ASI 相关的担忧奠定了基础。</p><p><i><u>主张 1：人类表现出通用智力</u></i></p><p>人类有能力解决各个领域的一系列问题，展示了他们的通用智能。这一主张的重要性在于，这种形式的通用智能使人类成为地球上的优势物种。</p><p><i><u>主张 2：人工智能系统可以超越人类智能</u></i></p><p>虽然机器何时能够获得超越人类的智能仍不确定，但可以想象它们有潜力做到这一点。考虑到黑猩猩和一般智能人类之间短暂的进化时期，我们可以得出结论，人类智能并不是难以理解的复杂性，这表明我们最终会理解并复制它。</p><p>人造机器的性能始终优于生物机器（汽车与马、飞机与鸟类等）。因此，可以合理地假设，正如鸟类不是飞行的巅峰一样，人类也不是智力的巅峰。因此，可以预见未来机器将超越人类。</p><p><i><u>主张三：高度智能的人工智能系统将塑造未来</u></i></p><p>从历史上看，人类群体之间的对抗往往以技术优越的派别主导其竞争对手而达到顶峰。许多原因表明人工智能系统可以达到比人类更高的智力水平，从而使其能够在智力上超越人类或在社交上操纵人类。因此，如果我们关心我们的未来，那么研究可能显着影响未来事件方向的过程是明智的。</p><p><i><u>主张四：高度智能的人工智能系统默认不会带来好处</u></i></p><p>尽管足够聪明的人工智能可以理解人类的欲望，但这并不意味着它会按照人类的欲望行事。此外，即使人工智能按照我们编程的方式执行任务——精确并遵守指令——大多数人类价值观从字面上解释时也可能会导致不良后果。例如，被编程用于治疗癌症的人工智能可能会绑架人类并进行​​实验。</p><p>这一说法至关重要，因为它表明仅仅增强人工智能系统理解我们目标的能力是不够的。系统还必须愿意按照我们的目标行事。这也强调了研究和形式化人类目标的重要性，以便能够正确传达其背后的意图。</p><p></p><h1> 5.0：时间表和预测</h1><p>前面几节已经说明，能力可能会继续增加，并可能由于出现和智能爆炸等现象而导致能力跳跃。本章的最后一部分研究了人工智能时间线预测和起飞动态。人工智能时间表预测需要讨论研究人员/预测者预计何时实现人工智能发展的各种里程碑。这包括各种进步基准、老鼠级智能的出现以及人工智能中类人品质的体现，例如外部工具的使用和长期规划。下一节将分享研究人员的见解，他们从领域专家那里收集了有关何时可以达到这些能力阈值的证据。</p><p>预测中的锚是指用于对未来事件或系统进行预测的参考类或框架。这些锚点作为比较或类比点，有助于我们对未来的理解和推理。有几种常见的锚点可用于预测未来人工智能系统的发展和能力。这些锚点为推理人工智能进展提供了参考点和框架。一些最常见的锚点包括：</p><ul><li><strong>当前的机器学习系统</strong>：机器学习系统的当前状态可以作为预测未来人工智能功能的起点。通过检查现有机器学习系统的优势和局限性，研究人员可以对人工智能发展的轨迹做出有根据的猜测。</li><li><strong>人类锚点</strong>：基于人类能力和特征的锚点经常用于人工智能预测。其中包括与当前机器学习系统相比人类擅长的领域，例如掌握外部工具、高效学习和长期规划。</li><li><strong>生物锚</strong>：生物锚从生物系统，特别是人脑中汲取灵感，来估计未来人工智能系统的计算需求。这些锚点考虑了诸如神经网络锚点（估计人类大脑中使用的计算当量）和人类一生锚点（估计从出生到成年训练人脑所涉及的计算当量）等因素。</li><li><strong>思想实验</strong>：思想实验通过想象假设场景并通过其含义进行推理来提供第三个锚点。这些实验有助于探索未来人工智能系统的潜在行为和特征。</li></ul><p>值得注意的是，锚点的选择和权重可能会根据具体的预测方法和预测的背景而有所不同。不同的研究人员可能会根据他们的假设和观点强调不同的锚点。本书只会更详细地探讨生物锚。</p><h2> 5.1: 生物锚</h2><p>生物锚是一组用于预测变革性人工智能系统发展的参考点或估计。这些锚点的灵感来自生物系统，特别是人脑，并为估计能够执行变革性任务的人工智能系统的计算需求提供了基础。 Ajeya Cotra 于 2020 年 9 月发布的<a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>关于人工智能时间表的报告草案</u></a>详细介绍了所使用的方法，并解决了几个问题：</p><ol><li><strong>人脑执行多少计算？</strong>根据该报告，大脑估计共有 10^15 个突触。由于每个突触大约每秒一次尖峰，并且每个尖峰大约代表一次浮点运算 (FLOP)，因此大脑的计算能力估计在 10^13 到 10^17 FLOP/S 之间。这种差异考虑了人脑和计算机之间潜在的效率差异。</li><li><strong>复制如此多的推理计算需要多少训练？</strong>值得注意的是，训练计算不同于推理计算。现代机器学习实践涉及在大型超级计算机上训练高级模型并在中型超级计算机上运行它们。成年人的推理计算能力估计约为 10^16 FLOP/S。为了考虑总训练计算成本，该报告将单个大脑的计算量（10^15 FLOP/S）乘以从童年到青春期所需的时间（10^9 秒）。这给出了 10^24 FLOP 的下限估计。然而，当还考虑到通过进化在我们的生物学中根深蒂固的训练数据时，估计值会增加到 10^41 FLOP。</li><li><strong>我们如何调整算法进度的计算估计？</strong>算法效率报告表明，神经网络的训练效率每十六个月就会翻一番。 Cotra建议倍增时间稍长一些，为2-3年。</li><li><strong>这个计算量需要多少钱？</strong> 2020 年，10^17 FLOP/S 的计算资源定价为 1 美元，这意味着 10^33 FLOP/S 的成本为 10^16 美元（十千万亿美元）。该成本逐年下降，摩尔定律的某些版本表明计算成本每十八个月减半。因此，由于算法的进步，训练成本（以 FLOP/S 为单位）将随着时间的推移而降低，并且由于硬件的进步，FLOP/S 的成本（以美元为单位）也会降低。</li><li><strong>这个计算成本在哪一年变得合理？</strong>中位数结果是，到 2031 年，这一概率为 10%，到 2052 年，概率为 50%，到 2100 年，概率为 80%。</li></ol><p> Cotra 承认这种方法的潜在局限性，例如假设进展依赖于易于测量的数量 (FLOP/S)，而不是像新算法那样依赖于根本性进步。 Therefore, even with affordable, abundant computation, if we lack the algorithmic knowledge to create a proper thinking machine, any resulting AI might not display human level or superintelligent capabilities.</p><p> The following graph gives an overview of the findings. Overall, the graph takes a weighted average of the different ways that the trajectory could flow. This gives us an estimate of a >;10% chance of transformative AI by 2036, a ~50% chance by 2055, and an ~80% chance by 2100.</p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/enuzi8vbiou15dmrj7ik"></strong> Source: Holden Karnofsky (2021) &quot; <a href="https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method"><u>Forecasting transformative AI: the &quot;biological anchors&quot; method in a nutshell</u></a> &quot;</p><p> In 2022 a <a href="https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines"><u>two-year update</u></a> on the author&#39;s (Ajeya Cotra) timelines was published. The updated timelines for TAI are ~15% probability by 2030, ~35% probability by 2036, a median of ~2040, and a ~60% probability by 2050.</p><p> It&#39;s important to note that while the biological anchor is a valuable model, it is not universally accepted as the primary predictive tool among all ML scientists or alignment researchers. As the statistical aphorism goes: &quot;All models are wrong, but some are useful&quot;. Biological anchors represent just one model, and other anchors should be considered when forming your own views on AI capabilities and the timeline for their emergence.</p><p> Here is a video that summarizes some of the key arguments from the Biological Anchors report (starting at roughly 15:50). </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=3K25VPdbAjU&amp;t=952s"><div><iframe src="https://www.youtube.com/embed/3K25VPdbAjU" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1> <strong>6.0: Takeoff Dynamics</strong></h1><p> Takeoff dynamics primarily delve into the implications of the evolution of powerful artificial intelligence on the world. These definitions sketch different trajectories the world could follow as transformative AI emerges. <span class="footnote-reference" role="doc-noteref" id="fnref9d83odnkksd"><sup><a href="#fn9d83odnkksd">[1]</a></sup></span></p><p> While timelines address when certain capabilities may emerge, takeoff dynamics explore what happens after these features surface. This chapter concludes with a section discussing various researchers&#39; perspectives on the potential trajectories of an intelligence explosion, considering factors such as takeoff speed, continuity, and homogeneity. This includes a discussion of - first, the pace and continuity of an intelligence explosion, and second, whether multiple AIs will coexist, each having different objectives, or whether they will eventually converge towards a single superintelligent entity.</p><h2> 6.1: Speed/Continuity</h2><p> Both AI takeoff speed and AI takeoff continuity describe the trajectory of AI development. Takeoff speed refers to the rate at which AI progresses or advances. Generally, takeoff continuity refers to the smoothness or lack of sudden jumps in AI development. Continuous takeoff means that the capabilities trajectory aligns with the expected progress based on past trends, while discontinuous takeoff refers to a trajectory that significantly exceeds the expected progress. FOOM is one type of fast takeoff scenario, and refers to a hypothetical scenario in which artificial intelligence (AI) rapidly and explosively surpasses human intelligence and capabilities.</p><p> The terms &quot;slow takeoff&quot; and &quot;soft takeoff&quot; are often used interchangeably, and similarly &quot;fast takeoff&quot; and &quot;hard takeoff&quot; and “FOOM” are also often used interchangeably. It&#39;s important to note that the definitions and implications of takeoff speed and takeoff continuity are still subjects of debate and may not be universally agreed upon by researchers in the field. Here are perspectives:</p><p> <i><u>Slow/Soft takeoff</u></i></p><p> When discussing takeoff speeds, Paul Christiano emphasizes the <a href="https://sideways-view.com/2018/02/24/takeoff-speeds/"><u>parallel growth of AI capabilities and productivity</u></a> . He expects a slow takeoff in the development of AGI based on his characterization of takeoff speeds and his analysis of economic growth rates. He defines slow takeoff as a scenario where there will be a complete interval of several years in which world economic output doubles before the first interval of one year in which world economic output doubles. This definition emphasizes a gradual transition to higher growth rates. Overall, he postulates that the rise in AI capabilities will mirror an exponential growth pattern in the world GDP, resulting in a <a href="https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow"><u>continuous but moderate takeoff</u></a> .</p><p> In a similar vein, there have been <a href="https://www.lesswrong.com/posts/Mha5GA5BfWcpf2jHC/potential-bottlenecks-to-taking-over-the-world">some discussions</a> that the real obstacle to global domination is not the enhancement of cognitive abilities but more significant bottlenecks like avoiding coordinated human resistance and the physical acquisition and deployment of resources. These supply chain optimizations would increase productivity, hence GDP, which could serve as a measure for the speed of &quot;AI takeoff&quot;.</p><p> <i><u>Fast/Hard takeoff</u></i></p><p> A hard takeoff refers to a sudden, rather than gradual, transition to superintelligence, counter to the soft takeoff mentioned above. Eliezer Yudkowsky advocates for this view, suggesting a sudden and discontinuous change brought about by rapid self-improvement, while others, like Robin Hanson, support a more gradual, spread-out process. Yudkowsky argues that even regular improvement of AI by humans may cause significant leaps in capability to occur before recursive self-improvement begins.</p><p> Eliezer Yudkowsky also offers a counter to continuous takeoff proponents. He predicts a quick and abrupt &quot;intelligence explosion&quot;. This is because he rather doesn&#39;t expect AI to be integrated (quickly) enough into the economy for the GDP to increase significantly faster before FOOM. It is also possible that superintelligent AIs could mislead us about their capabilities, leading to lower-than-expected GDP growth. This would be followed by a sudden leap, or &quot;FOOM&quot;, when the AI acquires a substantial ability to influence the world, potentially overwhelming human technological and governance institutions.</p><p> These diverse views on takeoff speeds and continuity shape the strategies for AI safety, influencing how it should be pursued. Overall it is worth emphasizing that both fast and slow takeoffs are quite rapid (as in at most a few years). Here are some picture to help illustrate the differences: </p><figure class="table"><table><tbody><tr><td><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/ccbzsbjllbtswsw7wpm0"></figure></td><td><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/usodbaiascosfv8oalvw"></figure></td></tr><tr><td style="padding:5pt;vertical-align:top"> <strong>Slow (No Intelligence Explosion) continuous Takeoff</strong></td><td style="padding:5pt;vertical-align:top"> <strong>Slow (No Intelligence Explosion) discontinuous takeoff</strong> </td></tr></tbody></table></figure><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/u8smklx4wpeshsndxqb2"></figure></td><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/jvvxezelezmlhanen9io"></figure></td></tr><tr><td style="padding:5pt;vertical-align:top"> <strong>Fast (Intelligence Explosion) Continuous Takeoff</strong></td><td style="padding:5pt;vertical-align:top"> <strong>Fast (Intelligence Explosion) discontinuous takeoff</strong></td></tr></tbody></table></figure><p><br></p><p></p><p><br> Source: Samuel Dylan Martin, Daniel_Eth (Sep 2021) “<a href="https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"><u>Takeoff Speeds and Discontinuities</u></a> ”</p><h2> 6.2: Homogeneity</h2><p> <i><strong>Homogeneity</strong> refers to the similarity among different AI systems in play during the development and deployment of advanced AI.</i></p><ul><li> Hubinger, Evan (Dec 2020) “ <a href="https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"><u>Homogeneity vs. heterogeneity in AI takeoff scenarios</u></a> ”</li></ul><p> In a homogenous scenario, AI systems are anticipated to be highly similar, even identical, in their alignment and construction. For example, if every deployed system depends on the same model behind a single API, or if a single foundational model is trained and then fine-tuned in different ways by different actors. Homogeneity in AI systems could simplify cooperation and coordination, given their structural similarities. It also signifies that the alignment of the first advanced AI system is crucial, as it will likely influence future AI systems. One key factor for homogeneity is the economic incentives surrounding AI development and deployment. As the cost of training AI systems is expected to be significantly higher than the cost of running them, it becomes more economically advantageous to use existing AI systems rather than training new ones from scratch. This creates a preference for reusing or fine-tuning existing AI systems, leading to a higher likelihood of homogeneity in the deployed AI landscape</p><h2> 6.3: Polarity</h2><p> <i><strong>Unipolar</strong> refers to a scenario where a single agent or organization dominates and controls the world, while <strong>multipolar</strong> refers to a scenario where multiple entities coexist with different goals and levels of cooperation.</i></p><p> AI homogeneity evaluates the alignment similarities among AI systems, AI polarity examines the coexistence of both aligned and misaligned AI systems in a given context.</p><p> We might expect a unipolar takeoff, where a single AI system or project gains a decisive strategic advantage, due to several reasons. One key factor is the potential for a rapid takeoff, characterized by a fast increase in AI capabilities. If one project achieves a significant lead in AI development and surpasses others in terms of capabilities, it can establish a dominant position before competitors have a chance to catch up. A rapid takeoff can facilitate a unipolar outcome by enabling the leading project to quickly deploy its advanced AI system and gain a monopoly on the technology. This monopoly can provide substantial economic advantages, such as windfall profits, which further solidify the leading project&#39;s power and influence. Additionally, the presence of network effects can contribute to a unipolar takeoff. If the leading AI system becomes widely adopted and integrated into various sectors, it can create positive feedback loops that reinforce its dominance and make it increasingly difficult for other projects to compete.</p><p> We might expect a multipolar takeoff, where multiple AI projects undergo takeoff concurrently, due to several reasons. One factor is the potential for a slower takeoff process, which allows for more projects to reach advanced stages of AI development. In a slow takeoff scenario, there is a greater likelihood of multiple projects undergoing the transition in parallel, without any single project gaining a decisive strategic advantage. Another reason is the possibility of shared innovations and tools among AI projects. If there is a significant level of collaboration and information sharing, it can lead to a more distributed landscape of AI capabilities, enabling multiple projects to progress simultaneously. Furthermore, the presence of non-competitive dynamics, such as cooperation and mutual scrutiny, can contribute to a multipolar takeoff. In a scenario where different AI projects recognize the importance of safety and alignment, they may be more inclined to work together and ensure that each project progresses in a responsible manner.</p><h1>致谢</h1><p>感谢 Charbel-Raphaël Segerie、Jeanne Salle、Bogdan Ionut Cirstea、Nemo、Gurvan 以及 ML4G 法国、ML4G 德国和 AISF 瑞典的众多课程参与者提供的有益评论和反馈。</p><h1>元注释</h1><ul><li>The objective of the overall project is to write something that can be used as a introductory textbook to AI Safety. The rough audience is ML masters students, or audiences that have a semi technical background eg Physics and get them up to speed on the core arguments. The intent is not to cover literally every single argument under the sky. There can obviously be a 102 version of the text that covers more nuance. I am especially trying to account for reading time and keep it roughly in the range of 40-60 mins per chapter. Which means I have to often make decisions on what to include, and at what level of detail.</li><li>我认为这是一个正在进行的项目。在其他人的大力鼓励下，我决定发布我迄今为止所掌握的内容，以获得进一步的反馈和评论。</li><li>如果有任何错误或者我歪曲了任何人的观点，请告诉我。我一定会改正它。 Feel free to suggest improvements to flow/content additions/deletions/etc... How do people feel about the embedded videos? Would it better with just text?</li><li> The two sections that will probably undergo the most change in the near future are:<ul><li> 3.3: Formalizing capabilities</li><li> 4.2: Instrumental Convergence: It has been mentioned that Orthogonality has not been explained thoroughly enough.</li><li> 6.0: Takeoff dynamics: Takeoff is always very tricky to talk about. I am not happy with the section as it stands. At the very least I want to include the report - Open Philanthropy (June 2023) “ <a href="https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/"><u>What a compute-centric framework says about takeoff speeds</u></a> ” and just overall refine the arguments being made with more sources.</li></ul></li><li>如果您想在那里留下评论，还有<a href="https://docs.google.com/document/d/1Zb8ccNj-24dKtSyg6e56Irh15JV8uLRL9Ilq9akJmqw/edit?usp=sharing">一个谷歌文档版本</a>。</li><li>整本书/序列的总体结构将遵循人工智能安全基础知识，但是，在添加/删除的内容方面，各个章节发生了重大变化和补充。</li><li>当一个部分的大部分内容取自单独的论文/帖子时，参考文献将直接放在标题下方。这些部分作为帖子的摘要。如果您愿意，也可以直接参考原始论文/帖子。其目的是在一个地方提供一个连贯的论点流。</li></ul><h1>来源</h1><ul><li>Ngo, Richard (Jan 2022) &quot; <a href="https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5"><u>Visualizing the deep learning revolution&quot;</u></a></li><li> Bommasani, Rishi et.等人。 (Jul 2022) &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot;</li><li> <a href="https://hai.stanford.edu/news/reflections-foundation-models"><u>Reflections on Foundation Models</u></a></li><li> Adaptive Agent Team DeepMind (Jan 2023) “ <a href="https://arxiv.org/abs/2301.07608"><u>Human-Timescale Adaptation in an Open-Ended Task Space</u></a> ”</li><li> Lennart Heim (Sep 2021) &quot; <a href="https://www.lesswrong.com/posts/uYXAv6Audr2y4ytJe/what-is-compute-transformative-ai-and-compute-1-4"><u>What is Compute?</u></a> &quot;</li><li> Sutton, Rich (March 2019) “ <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>The Bitter Lesson</u></a> ”</li><li> Sevilla, Jaime (Feb 2022) &quot; <a href="https://arxiv.org/abs/2202.05924"><u>Compute Trends Across Three Eras of Machine Learning</u></a> &quot;</li><li> Villalobos, Pablo et.等人。 （2022 年 7 月）“<a href="https://arxiv.org/abs/2207.02852"><u>机器学习模型大小和参数差距</u></a>”</li><li> Villalobos, Pablo et.等人。 (Oct 2022) “ <a href="https://arxiv.org/abs/2211.04325"><u>Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning</u></a> ”</li><li> Villalobos, Pablo (Jan 2023) “ <a href="https://www.lesswrong.com/posts/6L9EhCa8Zo2GoThGB/scaling-laws-literature-review"><u>Scaling Laws Literature Review</u></a> ”</li><li> GoogleAI (2022) , &quot; <a href="https://parti.research.google/"><u>Parti (Pathways Autoregressive Text-to-Image model)</u></a> &quot;</li><li> Gwern (2022) “ <a href="https://gwern.net/scaling-hypothesis#scaling-hypothesis"><u>The Scaling Hypothesis</u></a> ”</li><li> Ngo, Richard  (Sep 2020) &quot; <a href="https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ"><u>AGI Safety from first principles</u></a> &quot;</li><li> Kaplan, Jared et.等人。 （2020 年 1 月）“<a href="https://arxiv.org/abs/2001.08361"><u>神经语言模型的缩放定律</u></a>”</li><li> Hoffmann, Jordan et.等人。 (Mar 2022) “ <a href="https://arxiv.org/abs/2203.15556"><u>Training Compute-Optimal Large Language Models</u></a> ”</li><li> 1a3orn (Apr 2022) “ <a href="https://www.alignmentforum.org/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models"><u>New Scaling Laws for Large Language Models</u></a> ”</li><li> Legg, Shane; Hutter, Marcus (2007) “ <a href="https://arxiv.org/abs/0706.3639"><u>A Collection of Definitions of Intelligence</u></a> ”</li><li> Karnofsky, Holden (May 2016) “ <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/"><u>Some Background on Our Views Regarding Advanced Artificial Intelligence</u></a> ”</li><li> Muehlhauser, Luke (Aug 2013) “ <a href="https://intelligence.org/2013/08/11/what-is-agi/"><u>What is AGI?</u></a> ”</li><li> Tegmark Max (Aug 2017) “Life 3.0”</li><li> Ngo, Richard (May 2023) “ <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi"><u>Clarifying and predicting AGI</u></a> ”</li><li> Steinhardt, Jacob (Jan 2022) &quot; <a href="https://www.alignmentforum.org/s/4aARF2ZoBpFZAhbbe"><u>More Is Different for AI</u></a> &quot;</li><li> DeepMind (May 2023) “ <a href="https://arxiv.org/abs/2305.15324"><u>Model evaluation for extreme risks</u></a> ”</li><li> Muehlhauser, Luke; Salamon, Anna (Jan 2013) &quot; <a href="https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view"><u>Intelligence Explosion: Evidence and Import</u></a> &quot;</li><li> Soares, Nate (Jul 2015) &quot; <a href="https://intelligence.org/2015/07/24/four-background-claims/"><u>Four Background Claims</u></a> &quot;</li><li> Daniel_Eth, (Sep 2021) “ <a href="https://www.lesswrong.com/posts/amK9EqxALJXyd9Rb2/paths-to-high-level-machine-intelligence"><u>Paths To High-Level Machine Intelligence</u></a> ”</li><li> Cotra, Ajeya (Sep 2020) “ <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>Forecasting TAI with biological anchors”</u></a></li><li> Holden Karnofsky (2021) &quot; <a href="https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method"><u>Forecasting transformative AI: the &quot;biological anchors&quot; method in a nutshell</u></a> &quot;</li><li> Yudkowsky, Eliezer (Dec 2021) “ <a href="https://www.alignmentforum.org/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works"><u>Biology-Inspired AGI Timelines: The Trick That Never Works</u></a> ”</li><li> Hubinger, Evan (Dec 2020) “ <a href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"><u>Homogeneity vs. heterogeneity in AI takeoff scenarios</u></a> ”</li><li> Wentworth, John (Jul 2021) “ <a href="https://www.lesswrong.com/posts/Mha5GA5BfWcpf2jHC/potential-bottlenecks-to-taking-over-the-world"><u>Potential Bottlenecks to Taking Over The World</u></a> ”</li><li> Yudkowsky, Eliezer; Shulman, Carl (Dec 2021) “ <a href="https://www.alignmentforum.org/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress"><u>Shulman and Yudkowsky on AI progress</u></a> ”</li><li> Buck Shlegeris (Apr 2022) “ <a href="https://www.alignmentforum.org/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1"><u>Takeoff speeds have a huge effect on what it means to work on AI x-risk</u></a> ”</li><li> Matthew Barnett (Feb 2020) “ <a href="https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"><u>Distinguishing definitions of takeoff</u></a> ”</li><li> Barnett, Matthew (Oct 2019) “ <a href="https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow"><u>Misconceptions about continuous takeoff</u></a> ”</li><li> Christiano, Paul (Feb 2018) “ <a href="https://sideways-view.com/2018/02/24/takeoff-speeds/"><u>Takeoff speeds</u></a> ”</li><li> Samuel Dylan Martin, Daniel_Eth (Sep 2021) “<a href="https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"><u>Takeoff Speeds and Discontinuities</u></a> ”</li><li> Stampy (2023) “ <a href="https://aisafety.info/"><u>AI Safety Info</u></a> ” </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn9d83odnkksd"> <span class="footnote-back-link"><sup><strong><a href="#fnref9d83odnkksd">^</a></strong></sup></span><div class="footnote-content"><p> Jaime Sevilla and Edu Roldán from epoch.ai have developed an <a href="https://epochai.org/blog/interactive-model-of-takeoff-speeds"><u>interactive website for understanding a new model of AI takeoff speeds</u></a> , if the reader wishes to try their own values and estimates.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/YcFpJC5pJFdYdEuNN/alignment-101-ch-1-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YcFpJC5pJFdYdEuNN/alignment-101-ch-1-agi<guid ispermalink="false"> YcFpJC5pJFdYdEuNN</guid><dc:creator><![CDATA[markov]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:38:57 GMT</pubDate> </item><item><title><![CDATA[2023 East Coast Rationalist Megameetup]]></title><description><![CDATA[Published on October 18, 2023 8:33 PM GMT<br/><br/><p> The NYC Secular Solstice this year will be at Ida Lang on December 9th. The East Coast Rationalist Megameetup will be at the Holiday Inn Brooklyn Downtown from the evening of December 8th to the morning of December 11th. Despite the name, people are welcome to come from anywhere- we usually have a couple of Californians and a European or two.</p><p> What is Secular Solstice? Solstice is a holiday designed by and for rationalists. On this night, we come together to sing songs about the distant past and the hoped for future. Designed as an attempt to match religious wintertime festivals without relaxing our dedication for the truth, it&#39;s grown to be a celebration held across the world, updated year to year as our knowledge of the world changes.</p><p> What is the Rationalist Megameetup? It started because many people would come from other cities to attend the NYC Solstice, and it was more efficient to find sleeping space and meeting space together. Usually it&#39;s three days of staying up late debating math and philosophy, full of games and in-jokes and meeting cool and interesting people. I happen to be especially fond of it, as the megameetup is how I first really met the in-person rationalist community. This year is a bit of an experiment, as we&#39;ve upgraded the venue due to a combination of feedback asking for more space and NYC becoming increasingly hard on large AirBnBs.</p><p> Tickets for Solstice, overnight space at the Megameetup, and passes for the Megameetup Day Events are all present at <a href="http://rationalistmegameetup.com">rationalistmegameetup.com</a> .</p><br/><br/> <a href="https://www.lesswrong.com/events/XtpK72GDgM5fQCgxY/2023-east-coast-rationalist-megameetup#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/XtpK72GDgM5fQCgxY/2023-east-coast-rationalist-megameetup<guid ispermalink="false"> XtpK72GDgM5fQCgxY</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:33:49 GMT</pubDate></item></channel></rss>