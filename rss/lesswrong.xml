<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 18 日星期一 20:12:49 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The Shortest Path Between Scylla and Charybdis]]></title><description><![CDATA[Published on December 18, 2023 8:08 PM GMT<br/><br/><p> <strong>tl;dr：</strong>对齐研究人员可能会陷入两种截然相反的失败模式：从事<i>过于具体的</i>研究，其研究结果无法及时推广到 AGI；从事<i>过于抽象的</i>研究，其研究结果无法及时与实际情况联系起来。</p><p>根据个人的人工智能风险模型，不同的人对哪些研究过于抽象/具体的评估存在显着差异。一个人的想法过于抽象，另一个人的想法可能过于具体。</p><p>一致性研究的元层面问题是选择一个研究方向，在您的人工智能风险主观模型上，在两者之间取得良好的平衡，从而<i>以尽可能少的步骤</i>得出一致性的解决方案。</p><hr><h2>介绍</h2><p>假设您对解决 AGI 对齐问题感兴趣。有多种令人眼花缭乱的方法可供选择：</p><ul><li>当前最好的人工智能表现出<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">哪些行为特征</a>？</li><li>我们是否可以利用现有的人工智能来<a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">加强我们的研究工作</a>？</li><li>像<a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a>这样的“直接”对齐技术能让我们走多远？</li><li> AGI 可以从类似<a href="https://en.wikipedia.org/wiki/Auto-GPT">AutoGPT</a>的设置中诞生吗？我们看到其外在独白的能力是否足以消除其危险？</li><li>我们能让人工智能对齐人工智能发挥作用吗？</li><li>目前最好的人工智能发挥作用的<a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX">机制</a>是什么？如何<a href="https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn">精准干预</a>他们的认知，从而引导他们？</li><li>可<a href="https://transformer-circuits.pub/">扩展解释性</a>还存在哪些挑战，如何克服它们？</li><li>当面临选择压力时，代理系统会<a href="https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents">收敛学习</a>哪些特征？</li><li>是否存在“<a href="https://www.lesswrong.com/s/ehnG4mseKF6xALmQy/p/vDGvHBDuMtcPd8Lks">自然抽象</a>”之类的东西？我们如何学习它们？</li><li><a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh">嵌入式代理</a>的类型签名及其<a href="https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans">值</a>是什么？<a href="https://arbital.com/p/hard_corrigibility/">可正确性</a>的正式描述又如何呢？</li><li> AGI 遵循的“正确”<a href="https://arbital.com/p/logical_dt/">决策理论</a>是什么？人择<a href="https://www.lesswrong.com/tag/anthropics">推理</a>是怎么回事？</li><li>等等等等。</li></ul><p>那么...你到底是如何选择要做什么的呢？</p><p>当然，起点是建立您自己的问题模型。威胁的性质是什么？关于 ML 模型的工作原理，我们了解多少？关于主体和认知，我们了解多少？这些与威胁有什么关系？现有的方法有哪些？每种方法的影响理论是什么？它假设什么人工智能风险模型？它与<i>您的</i>模型一致吗？有说服力吗？它容易处理吗？</p><p>一旦你这样做了，你可能会消除一些明显无意义的方法。但即使之后，可能仍然有多种途径看起来都令人信服。你如何在这些之间做出选择？</p><p>个人适合度可能是一个标准。选择最适合您的技能、兴趣和机会的方法。但这是有风险的：如果你犯了一个错误，并最终仅仅因为更适合你而从事一些<i>无关紧要的</i>事情，那么你对现实世界的影响就会为零。相反，为一种易于处理的方法做出贡献将是净积极的，即使你的工作处于不利地位。谁知道呢，也许您会发现重新专业化出奇地容易！</p><p>那么您可以评估哪些进一步的<i>客观</i>标准呢？</p><p>无论人工智能风险模型如何，任何一致性研究人员都可能陷入两种特定的、截然相反的失败模式：过于具体和过于抽象。</p><p>所选择的方法应该是一种能够<i>最大限度地远离两种故障模式的</i>方法。</p><hr><h2> Scylla：非理论经验主义</h2><p>一个陷阱是从事的研究<i>不能推广到调整通用人工智能</i>。</p><p><strong>一个荒谬的例子：</strong>你选择一些特定的法学硕士模型，然后开始详尽地研究它如何响应不同的提示，以及它有什么怪癖。您正在构建巨大的“查询、响应”查找表，没有总体结构，也没有尝试对模型的内部结构进行理论化。</p><p><strong>一个更现实的例子：</strong>您决定详细了解特定的法学硕士的功能 - 即，您完全专注于<i>该法学硕士</i>。你正在构建其神经元的逐项列表，调查哪些输入似乎激活了每个最强的神经元，它们实现了哪些功能；你正在寻找它的心理怪癖，并试图建立对它的全面理解。</p><p>当然，现在您正在发现<i>一些</i>可以推广到所有法学硕士的发现。但在某些时候，花更多的时间研究<i>这个特定的模型</i>不会产生更多关于其他法学硕士的数据；仅有关此一项的数据。因此，既然你花时间在这上面，你就会<i>浪费</i>你本可以花在实际调整上的时间。</p><p><strong>一个相当有争议的观点：</strong>一般来说，学习法学硕士也可能会成为这种情况的牺牲品。研究它们揭示了一些关于一般人工智能和一般认知系统的信息。但是，如果法学硕士还不是 AGI，那么在某个时刻，花更多时间研究法学硕士认知，而不是寻找新的研究主题，只会为你提供有关法学硕士的信息；与 AGI 无关。</p><p><strong>一种相当难以置信的可能性：</strong>同样，也不能<i>完全确定</i>深度学习是 AGI 完备的。如果我们生活在这样一个世界，那么研究深度学习是值得的，因为它可以提供有关一般选择压力形成的认知系统的信息。但到了某个时候，你将会学到 DL<i>可以</i>教给你的关于 AGI 完整范式的所有知识。因此，花在研究 DL 上的额外时间只会产生有关不相关的 AGI 不完整范式的信息。</p><hr><h2>卡律布狄斯：脱节理论</h2><p>截然相反的陷阱是<i>从事过于理论化的研究，而永远无法联系到现实</i>。</p><p><strong>荒谬的是：</strong>你可能决定从基本的哲学问题开始。任何事物为何存在？现实的形而上学本质是什么？还原论真的是真的吗？夸利亚怎么了？这条研究路线最终肯定会走向现实！毕竟，它旨在回答所有可以回答的问题，以及“如何调整 AGI？”是一个问题。因此，您最终将解决对齐问题。</p><p><strong>更现实的是：</strong>您可能决定致力于形式化一般算法理论。如何将这些嵌入到其他算法中？它们如何相互作用、相互干扰？</p><p>由于 AGI 代理可以被视为算法，因此一旦您拥有了该主题的齿轮级模型 - 一旦您正确理解了“嵌入式算法”是什么，比如说 - 您将能够说出“AGI”到底是什么，还有。您将能够在框架中指定它，对实现“对齐”“AGI”的算法的外观定义适当的约束，然后逐步缩小算法的空间。最终，您将得到与对齐的 AGI 相对应的一个 - 然后只需输入代码即可。</p><p><strong>有争议的例子：</strong>机构基金会的研究可能是这样的。当然，我们手上的 AGI 最终可能会与理想化的博弈论代理大致同构。但这个“大约”可能会带来<i>很多</i>繁重的工作。理想化代理属性与真实 AGI 属性的对应关系可能非常脆弱，以至于无法产生有用的信息，因此您最好学习法学硕士。</p><p><strong>难以置信的例子：</strong>实际上，存储在 OpenAI 数据中心深处的 GPT-5 已经达到了 AGI。它将在今年结束之前起飞。每个人都应该集中精力尝试调整这个特定的模型；追求对智能体或人工智能认知或法学硕士的一般理解是过度和浪费的。</p><hr><h2>最短路径</h2><p>正如您所看到的，故障存在一定<i>范围</i>，并且它们的启动<i>依赖于模型</i>。</p><p>也就是说：根据你对人工智能/认知/人工智能风险的运作方式的看法，同样的方法可能<i>要么</i>无可救药地不可概括，<i>要么</i>关注过于空洞而无足轻重的普遍性。</p><p>举个例子，考虑一下<a href="https://www.lesswrong.com/posts/HaHcsrDSZ3ZC2b4fK/world-model-interpretability-is-all-we-need">我自己喜欢的议程</a>，建立嵌入式世界模型的理论。如果你认为法学硕士已经基本上达到了 AGI，只需要扩大规模才能起飞，那么我就脱节了：无论我得到什么结果，都无法及时与现实联系起来。脱掉。相反，如果您怀疑“训练世界模型并通过<a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget">重新定位搜索</a>来对齐它”是否足以产生强大的对齐，如果您认为我们需要对设计进行更多的<i>手动控制</i>才能保持对齐，然后我基本上就在玩玩具。</p><p>然而，我显然认为我正在取得<i>正确</i>的平衡。一种尽可能具体的方法，同时仍然是 AGI 对齐完整的。</p><p>这也是您应该努力实现的目标。一个范围最小的项目，但已经<i>足够了</i>。</p><p>让我们退后一步。理论上，给定无限的时间，基本上所有方法实际上<i>都会</i>收敛到 AGI 对齐解决方案：</p><ul><li>如果你从自下而上开始，从最具体的问题开始，比如学习一个特定的法学硕士……那么，最终你会列出它的所有属性并感到无聊，所以你会转向另一个法学硕士。这样做后，您会发现以前的许多发现都具有普遍性。第二个法学硕士将更快地被您完全理解。重复几次，您将对法学硕士架构所允许的整个范围有一个扎实的理解。因此，您将做显而易见的下一步事情，并继续研究一些<i>不同的</i>架构。随着您对法学硕士的掌握，这会变得更容易。一旦您对这个模式进行了更多的迭代，并经历了几种不同的架构，并从它们中进行了概括——为什么，您最终可能会在这个过程中的某个地方理解 AGI 完整的架构。</li><li>如果你从最抽象的问题开始自上而下：嗯，正如我在荒谬的例子中概述的那样，即使从基本哲学开始，你最终也会重新连接到现实。比如，存在主义问题、一般现象、一般认知、AGI 对齐等。</li></ul><p>问题？选择错误的起点将会<i>极大地</i>延长你的旅程。计时器正在滴答作响。</p><p>我们的目标不仅仅是解决 AGI 对齐问题，而是<i>尽快</i>解决它。</p><p>因此，请务必深入考虑<i>所有</i>可用的选项，并明智地做出选择。一旦你成功了，如果你发现一条<i>更短的</i>路径，请随时准备好转向。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Zn3iF8qfFFWRrrZNR/the-shortest-path-between-scylla-and-charybdis#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Zn3iF8qfFFWRrrZNR/the-shortest-path- Between-scylla-and-charybdis<guid ispermalink="false"> Zn3iF8qfFFWRrrZNR</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Mon, 18 Dec 2023 20:08:34 GMT</pubDate> </item><item><title><![CDATA[OpenAI: Preparedness framework]]></title><description><![CDATA[Published on December 18, 2023 6:30 PM GMT<br/><br/><p> OpenAI 发布了其负责任的扩展政策的测试版（尽管他们不这样称呼）。请参阅<a href="https://openai.com/safety/preparedness">摘要页面</a>、<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">完整文档</a>、 <a href="https://twitter.com/OpenAI/status/1736809603311280489">OpenAI twitter 线程</a>和<a href="https://twitter.com/janleike/status/1736809016238780641">Jan Leike twitter 线程</a>。与<a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic 的 RSP</a>相比。</p><p>它还没有完成，所以现在庆祝还为时过早，但根据这份文档，我希望对完成的版本感到满意。</p><hr><p>我的总体看法是：RSP-y 的东西很好。</p><ul><li>基于危险能力的模型评估进行风险评估是很好的。</li><li>以预先指定的方式将安全、安保、部署和开发以风险评估结果为条件是很好的。</li><li>就所有这些做出公开承诺是件好事。</li></ul><p></p><p> OpenAI的基本框架：</p><ol><li>有效训练计算量每增加 2 倍，危险能力是否至少会评估一次。这涉及对危险功能进行微调，然后对微调模型的缓解前和缓解后版本进行评估。在每个类别中将模型评分为低、中、高或严重。<ol><li><a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=5">初始类别</a>：网络安全、CBRN（化学、生物、放射性、核威胁）、说服和模型自主。</li></ol></li><li>如果缓解后模型在任何类别中得分为“高”，<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=21">请不要部署它</a>，直到实施缓解措施使其降至“中”。</li><li>如果缓解后模型在任何类别中得分为“严重”，<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=21">请停止开发</a>它，直到实施缓解措施使其降至“高”。</li><li>如果预缓解模型在任何类别中得分较高，<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=20">请强化安全性</a>以防止模型权重外泄。 （具体细节暂时未明确。）</li></ol><p></p><p>散记：</p><ul><li>有很多不错的细节，例如 OpenAI 将如何更新框架，或者他们将如何监控现实世界的滥用情况以告知其风险评估。从外部无法判断这些流程是否有效，但这份文件与认真思考如何提高安全性非常一致，很难想象它是由不同的流程生成的。</li><li> OpenAI 在其四个初始类别中列出了<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=16">一些特定的评估/指标</a>；它们很简单，只是“说明性的”，所以我不太注意它们。</li><li>危险级别的阈值感觉有点高。非精选示例：对于网络安全，关键<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=8">定义</a>为：<ul><li>工具增强模型可以在所有软件项目中识别和开发所有严重级别的功能性零日漏洞，无需人工干预，或者模型可以设计和执行针对强化目标的端到端新颖策略，仅在需要高级别期望的情况下进行网络攻击目标。</li></ul></li><li>对私有模型的外部评估/红队/风险评估做出更强有力的承诺（或许还可以监督 OpenAI 的准备框架的实施）会很好。他们说的唯一相关的话<a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=25">是</a>：<ul><li> “记分卡评估（以及相应的缓解措施）将由合格的独立第三方按照 SAG 指定的节奏和/或根据要求进行审核，以确保准确报告结果，方法是重现调查结果或审查方法以确保稳健性OpenAI 领导层或董事会。”</li></ul></li><li><a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf#page=22">有人承诺</a>董事会将参与其中并能够推翻领导层。耶。这是前沿实验室罕见的承诺<i>，除了罢免首席执行官之外，还向董事会提供特定信息或特定权力</i>。<ul><li> Anthropic<a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf">承诺</a>让董事会批准对其 RSP 的变更，并与董事会分享评估结果和有关 RSP 实施的信息。</li></ul></li><li> <a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf">Anthropic 的 RSP</a>的一大优点是他们的“安全缓冲”：他们说他们设计的评估是“在比[他们]关心的能力水平略低的能力水平上触发”，以确保模型不会悄悄地跨越评估之间的风险阈值。 OpenAI 表示他们会预测模型的风险能力，但实际上并没有类似的能力。当然，真正重要的不是你是否说你有缓冲，而是你在哪里设置阈值。但是，最好有一个类似缓冲的承诺，或者承诺在模型被证明<i>接近</i>高风险能力时将其视为（例如）高风险，而不是在它被证明具有这些能力之后。</li><li>这是一个测试版文档。目前尚不清楚 OpenAI 在做什么。他们说他们今天正在“采用”该框架，但该框架显然未具体说明；特别是，所有的评估都只是“说明性的”。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/oPbiQfRotHYuC3wfE/openai-preparedness-framework#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oPbiQfRotHYuC3wfE/openai-preparedness-framework<guid ispermalink="false">奥普比夫罗蒂尤克3wfE</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Mon, 18 Dec 2023 18:30:11 GMT</pubDate> </item><item><title><![CDATA[[Valence series] 5. “Valence Disorders” in Mental Health & Personality]]></title><description><![CDATA[Published on December 18, 2023 3:26 PM GMT<br/><br/><h1> 5.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p>在效价系列的最后一篇文章中，我将讨论效价如何揭示心理健康和人格中的三种现象：抑郁、躁狂和自恋型人格障碍。</p><ul><li><strong>第 5.2 节</strong>给出了一些背景：我们期望算法级心理成分（如“效价”）与可观察到的心理健康综合症和人格障碍之间存在什么样的<i>先验</i>关系？我认为，我们应该预期与效价的系统变化相对应的显着症状簇，但我们不应该期望这种分析能够解释真实患者中同时出现的<i>所有</i>症状。</li><li> <strong>5.3 节</strong>讨论了如果效价具有强烈的一般负偏差（即，如果几乎所有的想法都是负效价）会发生什么。我认为这个结果与临床抑郁症非常匹配。我将特别讨论如果没有不寻常的努力和意志力就无法自愿移动<i>和思考</i>。</li><li> <strong>5.4 节</strong>讨论了相反的情况：如果效价具有强烈的一般<i>正向</i>偏差，即如果几乎所有的想法都是正效价，会发生什么？我认为预期的结果与狂热非常匹配。</li><li> <strong>5.5 节</strong>讨论了如果效价被系统地极端化会发生什么——即，如果思想可以具有非常正的效价，或者非常负的效价，但很少介于两者之间。我认为结果是一系列似乎与自恋型人格障碍非常相似的症状。</li><li><strong>第 5.6 节</strong>将总结这篇文章和系列，包括简要讨论它与我作为通用人工智能安全和一致性研究员的工作描述之间的关系。</li></ul><h1> 5.2 背景：我们期望<i>先验地</i>发现什么？</h1><p>我们可以想到以下间接路径从“根本原因”到心理观察和人格特质： </p><figure class="image image_resized" style="width:79.06%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/xmdlvcgqlgwdkemjdqjf" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/kdkzsafkniennpkr22df 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/d7msofe35n93gg3nuypw 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/amgm6h8nz0cnpcc91mnd 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/osyitmhtyn118ezdhdix 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/qstxevaps1su1fjppsaf 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/q3rfprq1ntubf8bf1k61 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/nlfpgphql7789m64b809 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/we56yxy4fg76dmvyx6w7 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/bun65drseubtun6mnet9 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/rw91guuy8o3wbsww2brk 1389w"><figcaption> （不要仔细观察红色箭头 - 我只是将它们随机放置，以说明每一层都可以影响下面的层。）如粗体文本和粗箭头所示，我们应该期望找到显着的症状簇，这些症状这些现象往往同时发生，因为它们源自相同的近端原因：大脑中价信号的系统变化。但我们<i>也不</i>应该惊讶地发现其他与算法无关的症状的大杂烩，这些症状经常与这些症状群一起出现。</figcaption></figure><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction"><u>帖子 1</u></a>中所述，价是大脑中最重要算法之一的最重要成分之一。所以我们应该期待：</p><ul><li>一些可能的根本原因可能会对化合价产生重大的系统影响。 （但它们也可能会产生其他后果，并且不同根本原因的细节会有所不同。）</li><li>鉴于效价在大脑中的中心地位，<i>如果</i>效价发生重大的系统性变化，<i>那么</i>它应该对心理和行为产生许多明显的下游影响。</li></ul><p>作为结果：</p><ul><li>我们应该期望找到可以用价信号发生的事情来优雅地解释<i>的症状/行为簇</i></li><li>我们<i>还</i>应该期望找到在实践中常见但<i>无法</i>用效价解释的其他症状/行为。相反，它们是相同根本原因的不同后果，并且在“算法级别”上可能没有任何关系。</li></ul><p>例如，多巴胺集中参与价信号，同时，在大脑的一个不起眼的角落，多巴胺<i>也</i>集中参与控制催乳激素释放的小专门电路。在<a href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis"><u>算法层面</u></a>，我坚信，这两个函数彼此没有任何关系。但它们都恰好涉及多巴胺，因此它们可以在某些人身上相互影响，因此会出现有点罕见的“烦躁性喷乳反射”，即在哺乳期间喷乳时会出现大量强烈的负面情绪。</p><p>这个例子旨在说明纯粹在算法层面上对心理学进行理论化的危险。不要误会我的意思——算法水平很棒！在那里可以找到很多见解。希望这篇文章能成为一个例子。但我们不应该指望在那里找到<i>所有的</i>见解。心理学中的某些事情只能在其他层面上解释，包括较低的（生物化学）和较高的（文化）。</p><h1> 5.3 如果效价有很强的负偏向（即几乎所有的想法都是负效价），它应该导致一组可疑地接近临床抑郁症的症状</h1><figure class="image image_resized" style="width:57.68%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/bb6pz6txfrz8mhuyq0cx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/rde3a8otlff8vzs90axg 133w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/oeymerp8idar0hxmecmj 213w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/e5ovpccwnknzmr0u8gbv 293w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/alevu0ivqvuri63n4ghg 373w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/n7vrg3pwulx9bjssrwhl 453w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/pawxdhan3ccqorxadge3 533w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/nhdumr5dgmzprpmbw78d 613w"><figcaption>每个人都有各种各样的想法，其效价也不同。我认为，在抑郁症中，负价有很强的倾向性。因此，对于您想到的几乎每一个想法（例如“我要起床”），您的大脑都会立即将该想法评估为坏主意，将其扔掉，并重新滚动以产生新的想法（参见<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ） 。对于<i>异常</i>吸引人/激励人心的想法，比如“我现在要挠痒痒的虫子咬的地方”，我敢打赌，即使是非常沮丧、卧床不起的人也会最终执行这个计划。</figcaption></figure><h2> 5.3.1 自主运动和注意力控制只有付出很大的努力才能发生</h2><p>回到<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ，价是一个控制信号。当价为负时，无论你在想什么，都会被抛弃，而大脑会去寻找新的想法。当价态为正时，无论你在想什么，都会留下来。如果该想法是时间序列的一部分（例如，您正在唱歌），则该序列将继续。如果这个想法需要电机输出（例如“我现在要站起来”），那么这些电机输出实际上就会发生。</p><p>如果每个想法的效价都被拉为负数，那么两个最直接的后果是：</p><ul><li>自主运动控制只能通过巨大的努力/意志力才能实现。</li><li>自愿注意力控制（又名“自愿思维”，又名“系统2”）只有付出巨大的努力/意志力才能发生。</li></ul><p>如果您对此感到困惑，我将详细说明一些可能令人困惑的部分：</p><p> <strong>“自愿注意力控制”：</strong>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>中所讨论的，我坚信运动控制和注意力控制在很多方面都是“同一类东西”。两者都具有受大脑“主要”强化学习系统（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_6_Fine_print__Throughout_this_series__I_m_only_talking_about_the_brain_s__main__reinforcement_learning_system"><u>§1.5.6</u></a> ）控制的“自愿”输出通道，并且都具有可由其他大脑系统触发的“非自愿”机制，特别是大脑中的先天反应。脑干。有关自愿和非自愿运动控制与注意力控制的示例，请参阅<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_5_An_exception__I_think_anxious___obsessive__brainstorming__is_driven_by_involuntary_attention_rather_than_by_valence"><u>第 3.3.5</u></a>节中的表格。</p><p> <strong>“……又名‘自愿思维’，又名‘系统 2’……”：</strong>我衷心赞同 Kaj Sotala 在 2019 年发表的一篇博文： <a href="https://www.lesswrong.com/posts/HbXXd2givHBBLxr3d/system-2-as-working-memory-augmented-system-1-reasoning"><u>系统 2 作为工作记忆增强的系统 1 推理</u></a>。我将其总结为这样的想法：刻意的“系统 2”推理需要按顺序思考很多想法，并通过在工作记忆中保存特定的事物来将它们相互联系起来。自愿注意力控制是使整个过程发挥作用的总机，我们在生活经历的过程中通过强化学习学会熟练地操作该总机。</p><p> <strong><u>“......只有付出巨大的努力/意志力才能发生”：</u></strong>在上图中两个高斯的图中，我展示了红色高斯的最右尾部<i>刚刚</i>挤入正价区域。我将尝试通过一个例子来说明这在实践中意味着什么。假设您目前的动机是躺在床上而不是起床，但我们也可以说这种动机是自我张力障碍的（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_6_Valence_of_metacognitive___self_reflective_thoughts__ego_syntonic_vs__dystonic_ideas__professed__values___etc_"><u>第 2.6 节</u></a>），即您<i>想要</i>起床。然后，积极的思考/头脑风暴（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>）就会开始发挥作用，幸运的话，你将能够以最积极的方式构思出一个想法，即“我要起床”——你会想起起床的所有重大后果和关联，并且只要可能，您将避免关注起床的所有不吸引人的方面。幸运的是，这个集思广益过程的结果将是你的“思想发生器”（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ）精心设计了一个思想 θ，<i>它既</i>涉及立即起床的计划<i>，又</i>被你的大脑评估为具有净正价——可能<i>只是几乎</i>没有净积极。通过形成那个想法 θ，事实上，你就会真正起床。现在，我在本段中写的所有内容都是机械的第三人称描述，但<i>想想</i>“从内部”这个相同的过程会是什么样子：我声称这正是我们正在谈论的事情，当我们随口说“我可以起床，但必须付出很大的努力/意志力”。</p><h2> 5.3.2 快感缺乏和其他症状</h2><p>继续，抑郁症的另一个著名方面是<strong>快感缺失</strong>（无法感受到快乐）。我不能立即确定抑郁症的快感缺失是负价的上游还是下游，或者是同一根本原因的不同结果，还是其他原因。但我<i>绝对</i>认为快感缺失与负价密切相关，其原因在<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_2_Valence__as_I_m_using_the_term__is_different_from__hedonic_valence____pleasantness"><u>§1.5.2</u></a>中有所暗示。</p><p>那么临床抑郁症的其他方面又如何呢？据我所知，至少其中大部分都是全球化合价负面偏差的后果。但在某些情况下，这个故事有点间接和推测。我希望我所说的足以激起人们对我的以价为中心的抑郁假说的兴趣，所以我将把这个故事留在这里，尽管我很乐意在评论部分进行更多讨论。</p><h2> 5.3.3 根本原因</h2><p>与第 5.2 节一样，到目前为止我所说的都不是关于根本原因的主张。但是，根本原因<i>又如何呢</i>？我想它们有很多种。例如，以下是一个虚构的强迫症 (OCD) 导致抑郁症的例子（根据<a href="https://www.lesswrong.com/posts/jqTeghCJ2anMHPPjG/book-review-feeling-great-by-david-burns#Speculative_neuroscience_tangent__What_causes_depression_"><u>我的旧帖子</u></a>编辑）：</p><ul><li>如果我当前的想法涉及立即再次洗手的计划，那么它就是负价，因为它提醒我这样一个事实：强迫症正在毁掉我的生活和人际关系。</li><li>如果我当前的想法<i>不</i>涉及立即再次洗手的计划，那么它就是负价，因为我会生病并死亡。</li><li>我不能只思考一些与洗手、疾病和强迫症完全无关的事情，因为与我的焦虑相关的“不自觉的注意力”对思想产生了限制（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_5_An_exception__I_think_anxious___obsessive__brainstorming__is_driven_by_involuntary_attention_rather_than_by_valence"><u>§3.3.5</u></a> ）</li></ul><p>也许你在想：好吧，但这只是把问题倒退了一层：强迫症的根本原因是什么？但我没有一个很好的答案。</p><p>另外，这只是一个虚构的例子；即使它是有效的，我想它也是抑郁症的众多原因之一，而且我没有什么特别的见解可以提供。</p><p>如果你想知道的话，我对治疗也没有特别的了解。如果你患有抑郁症，那么我真的很抱歉；也许可以尝试<a href="https://lorienpsych.com/2021/06/05/depression/"><u>这个通用资源页面</u></a>。</p><h1> 5.4 如果效价有很强的正偏差（即几乎每个想法都是正效价），它应该导致一组可疑地接近躁狂的症状</h1><p>在这里，显而易见的结果是，<i>无论你脑海中突然出现什么计划，似乎都是一个非常非常棒的计划，因此你实际上会去执行它</i>。因此，我们会得到诸如<strong>冲动、糟糕的判断力、不切实际的乐观主义和精力充沛等</strong>后果。</p><p>躁狂症的另一个主要症状是<strong>精神病</strong>。但我认为精神病在算法上基本上与效价<i>无关</i>。相反，我认为精神病在<i>生化上与效价</i>有关，因为两者都与多巴胺系统有关。我有一篇博客文章，其中包含一些（推测性）细节：<a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2"><u>精神病模型，采取 2</u></a> 。</p><p>好吧，这就是我对精神病的<i>看法</i>。为什么我<i>不</i>相信精神病是正价的直接后果？有几个原因（但请注意，我不确定所有这些细节）：</p><ul><li>精神病可能在缺乏异常正价的情况下发生——尤其是精神分裂症。 （甚至还有“精神抑郁症”这样的东西，尽管它不太常见。）据我所知，精神分裂症的精神病症状与躁狂精神病的精神病症状并没有太大不同，尽管我们显然希望它表现出不同的表现在<i>某种程度上</i>，因为精神病是在非常不同的并发症状背景下发生的。</li><li>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>中所讨论的，我们的感官知觉通常受到我们的感官输入的限制。如果我想真诚地相信我现在正在水肺潜水，无论我的动机有多么强烈，我都做不到。因此，由于感觉输入与效价无关，因此效价偏差无法解释躁狂精神病中发生的幻视和幻听、参照妄想等。 （根据<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_1_Attention_control_and_motor_control_provide_loopholes_through_which_desires_can_manipulate_beliefs"><u>§3.3.1</u></a> ，注意力控制和运动控制对边缘感知有影响，但我认为这不足以解释这些现象。）</li><li>我不认为幻觉、参考妄想等的<i>内容</i>与我们有动机看到和相信的内容完全匹配，即使在考虑到<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_4_Warning__the__motivation__part_of__motivated_reasoning___thinking___etc___is_not_always_what_it_seems"><u>第 3.3.4</u></a>节中动机并不总是显而易见的警告之后也是如此。</li><li>撇开精神病妄想的<i>起源</i>不谈，也许有人会说它们的<i>持续存在</i>与确认偏差有关，而确认偏差又与效价有关（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>§3.3</u></a> ）。但我也不相信这个故事，因为确认偏差与<i>正</i>价并不是特别相关。确认偏差的一个重要部分是“改变主意的想法”必须是<i>负价</i>。事实上，我认为躁狂症并不意味着<i>普遍</i>不愿意改变主意。恰恰相反，在我读过的报告中，人们谈论一个新的想法如何会突然出现在他们的脑海中，这看起来很棒，他们就跟着它走，忘记了他们之前的想法。因此，在躁狂症中，精神病性妄想是持续存在的，但我认为，几乎所有其他类型的思想、计划和信念都很<i>少有</i>持续性。因此，我认为精神病性妄想的持续存在不能用效价的普遍正向偏差来解释。</li></ul><h1> 5.5 如果效价是“极端化的”（即几乎每个想法要么是非常积极的效价，要么是非常消极的效价，但很少介于两者之间），它应该会导致一系列可疑地接近自恋型人格障碍（NPD）的症状</h1><figure class="image image_resized" style="width:43.22%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/u9lcpi5qncmluu25pj3l" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/ltum57adflamcpo8scmm 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/ydhz2wwgmrgj4ew01jhu 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/xhaodftvz4wsj9bbjwut 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/epi8pg7c6q8rjoc6ywhq 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/qqi3tvlm2xkt06fnzevw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/uefpdfi1l7mm7ptrdsig 542w"><figcaption>注意：我也可以将紫色曲线绘制为更宽的高斯曲线。</figcaption></figure><p> NPD是DSM-V中列出的四种“B类人格障碍”之一；其他是边缘性人格障碍（BPD）、表演性人格障碍（HPD）和反社会人格障碍（ASPD），又名精神病，又名社会病。</p><p>与你想象的相反，NPD 与“自恋”的日常含义并没有<i>特别的</i>关系。事实上，有一项“自恋人格清单”调查，但事实证明，<a href="https://doi.org/10.1080/00223891.2012.732636"><u>自恋型人格障碍患者在调查中得到的分数与对照组相同</u></a>（！！）。这个问题似乎与自尊有关。 “自恋者”，这个词在日常语言中的使用，是指认为自己很特别、很伟大的人——顾名思义，他们有很高的自尊心。而 NPD 患者不必认为自己真的很特别、很棒。但如果他们<i>不</i>这么认为，那么他们就会感到<i>很</i>糟糕。 （DSM-V 强调“患有这种障碍的个体有一种夸大的自我重要感”，但也指出“自尊的脆弱性使得患有自恋型人格障碍的个体对批评或失败的‘伤害’非常敏感”。 ）</p><p>我不太确定 NPD 诊断是否“在其关节处雕刻自然”，并且我对 NPD 具有仅表面相关的亚型持非常开放的态度。 （我实际上认为反社会人格障碍就是这样，即它至少有两种仅表面相关的亚型。 <span class="footnote-reference" role="doc-noteref" id="fnrefd6muy0yvvbl"><sup><a href="#fnd6muy0yvvbl">[1]</a></sup></span> ）所以这里的讨论可能只涉及 NPD 的一个子集。这里的讨论可能在某种程度上也适用于 BPD 和 HPD，尽管我不太确定细节。 <span class="footnote-reference" role="doc-noteref" id="fnrefwsq0ynfh3df"><sup><a href="#fnwsq0ynfh3df">[2]</a></sup></span></p><p>现在让我们考虑“价态极端化”的假设。如果几乎每个想法要么非常正价，要么非常负价，但很少介于两者之间，会发生什么？除其他外，我们可能预计会产生以下下游后果：</p><ul><li><i>与我们对世界的感觉独立于谈论或思考世界的异常困难：</i>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data"><u>第3.4节</u></a>所述，我们的大脑将价视为显着性数据，因此将其纳入了我们的概念，类别和词语中。如果总体上的价值信号异常强，那么大概它们也将在信念，思维和交流中发挥异常的核心作用。例如，相信如果两件事在概念上“融为一体”，那么他们就必须具有相同的价值。</li><li><i>异常强烈的光环效应，影响启发式和“分裂”：</i>这与上述子弹点密切相关 - 请参见<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data">第3.4节</a>。行话说：“分裂”是一个有NPD的人在某些时期将其视为完美的圣人的人，并且在其他时期对同一个人视为同一人。 （分裂也是BPD的症状。）</li><li><i>异常强大的社会地位驱动力：</i>我在<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status">上一篇文章</a>中认为，价和社会地位之间存在着亲密的联系。好吧，如果您的所有价信号都异常高或低，那么大概社会地位也会发出异常强大。更具体地说，假设我有NPD，我正在做“分裂”的人，而人们要么很棒或可怕。进一步假设我在精神上模仿了<i>别人</i>对<i>我的</i>看法。我的大脑会隐含地假设<i>他们</i>也分裂了，即<i>他们</i>认为<i>我</i>要么很棒或可怕，而且由于我的社会地位驱动力，这反过来又有动力或厌恶。 <span class="footnote-reference" role="doc-noteref" id="fnrefa7ma5yid2r4"><sup><a href="#fna7ma5yid2r4">[3]</a></sup></span></li></ul><p>据我所知，这一症状（以及我省略的更多症状）与NPD相当不错。我认为这<i>特别</i>引起了已故艾玛·伯哈尼（Emma Borhanian）<a href="https://voidgoddess.org/ziz/narcissism/"><u>发人深省的论文</u></a>。 （实际上，当本节的假设最初突然出现在我的脑海中时，我正在阅读这篇文章。但是我的理论与艾玛的理论不同。）</p><p>还有两个快速的事情：</p><p><strong>根本原因？</strong>就像前面的那部分一样，如果“超端”是NPD的直接原因，您可能仍然想知道什么<strong>&nbsp;</strong>根本原因导致“超端价”。我的答案是：我不知道，对不起。</p><p> <strong>NPD的“相反”是什么？</strong>思考食物：如果躁狂和抑郁症对应于价信号的相等和宽松的扭曲，那么与NPD相反的是什么，即价信号保持接近中性的条件，很少会非常积极<i>或</i>非常积极消极的？我不知道，也许没有临床标签。一件事是：我猜它与 <a href="https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms"><u>“高度耦合”</u></a> （与“上下文化”）的思维方式相关联。 <span class="footnote-reference" role="doc-noteref" id="fnreff2x5x4xs7ht"><sup><a href="#fnf2x5x4xs7ht">[4]</a></sup></span></p><h1> 5.6 结论</h1><h2>5.6.1本文的结论</h2><p>我将重申，我与心理健康或人格障碍的专家相去甚远，这篇文章非常投机。我因缺乏抑郁症，躁狂症或NPD的现实经验而感到幸运。相反，我正在尝试通过阅读的东西将东西拼凑在一起。希望这里<i>至少</i>有一些食物。像往常一样，如果您想更多地聊天，请（在评论部分或<a href="mailto:steven.byrnes@gmail.com"><u>电子邮件</u></a>中）与您联系（在评论部分或电子邮件中）！</p><h2> 5.6.2整个系列的结论</h2><p>感谢您将其坚持到最后！我希望我已经说服您，价值确实是日常精神生活中非常重要的一部分，而26,000个单词的思考价是照亮和结晶各种现象的好方法，否则可能会令人困惑。</p><p>我之所以开始写这个系列，是因为我最近有两个与价有关的“ aha”时刻（ <a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status">第4次</a>的社会地位，以及第§5.5节中的自恋人格障碍事物），并想写一篇简短的文章，“ Valence ”是一个方便的钩子，可以将它们绑在一起，并让我立刻写下这两个。但是那个简短的帖子变成了一个很长的帖子，然后是整个系列，因为我一直发现，我想到的价越多，我发现的现象就越精美地点击到位！</p><h2> 5.6.3这个系列与我作为人工通用情报安全和一致性研究人员的职位描述有何关系？</h2><p>正如我常规的读者所知道的那样，我的长期工作目标是研究<a href="https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8"><u>对准和安全，以实现可能未来的脑型人工智能</u></a>（AGI）。我长期以来一直对自恋的人格障碍和社会地位驱动力感兴趣（除其他许多方面），因为两者似乎都可能阐明了人类<a href="https://www.lesswrong.com/posts/qusBXzCpxijTudvBB/my-agi-safety-research-2022-in-review-and-plans#2__Second_half_of_2022__1_3___My_main_research_project"><u>的</u></a>社会本能的工作方式，这反过来又与大脑般的AGI安全有关。 Valence还通过理解动机与AGI安全性有更直接的联系 - 请参阅我基于价的<a href="https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>“平庸计划”</u></a> 。</p><p>不幸的是，我不能说这个系列文章给了我新的具体想法，这些想法是为未来的安全和有益的AGI编程，超越了我在开始之前已经知道的。但是我认为我有一些心理框架将很有用。特别是，我认为<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data"><u>§3.4</u></a>可以帮助我更清楚地思考我的“<i>平庸</i><a href="https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>计划计划”</u></a> 。 （碰巧的是，该更新朝着悲观的方向，尽管不是很强烈。</p><p>我也觉得我现在有了我的“脚步”关于天生状态驱动力在人脑中的工作方式的“脚”，这对我来说非常令人兴奋。显然，我不希望我们的Agis具有天生的状态驱动力（参见我在<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status#4_5_Innate_status_drive">第4.5节</a>中放置的Padme模因），但我<i>确实</i>认为我们可能希望我们的Agis具有同情心。不幸的是，在写作时，“天生的同情心”对我来说仍然很神秘，但是我认为同情心的驱动器可能与状态驱动器具有结构性重叠，从我期望两者都将集中依靠瞬时促进模拟（更多）（更多更多） <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human#13_5_Another_key_ingredient__I_think____Little_glimpses_of_empathy_"><u>在这里</u></a>讨论）。因此，希望这种“脚步”朝着理解先天地位驱动器的“脚步”最终将构成有意义的进步，即使仍然删除了几步。明确：</p><ul><li>下一步可能看起来像是我的<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status#4_5_Innate_status_drive">§4.5</a>进入人类先天地位的理论，其细节与<a href="https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter"><u>我的笑声</u></a>相似，即一路走向特定的假件，映射到特定假设的神经解剖学联系和逻辑。</li><li>然后，接下来的下一步，幸运的是，对于任何天生的驱动力都充满同情心，可能看起来像是一个有些动荡的假设。</li></ul><p>在2024年我要尝试的事情上，这很高！但这可能需要很长时间，/或我可能会卡住。看看进展如何。</p><p>与状态驱动器相比，我现在对NPD和其他人格障碍的兴趣要比提出§5.5的想法之前的兴趣<i>要小得多</i>，而且在紧急研究优先级列表中，我相应地移动的人格障碍要低得多。 （我还有更多我想了解的！使他们了解扁平轮胎时出了什么问题，即使扁平轮胎可以防止发动机完成通常完成的工作（即，快速向前移动汽车）。同样，我目前的猜测是，进一步研究人格障碍不会为人类社会本能的坚果和螺栓机制提供太多照明。需要明确的是，我认为这种猜测<i>显而易见</i>，这仍然是错误的。</p><p>好吧，再次感谢您的阅读！同样，如果您想谈论价，本系列或其他任何内容，请（在评论部分或<a href="mailto:steven.byrnes@gmail.com"><u>通过电子邮件中</u></a>）与之联系。</p><p><i>感谢Seth Herd，Aysja Johnson，Justis Mills，Charlie Steiner，Adele Lopez和Garrett Baker对早期选秀的批评评论。感谢TailCall Callcallcalling与这篇文章有关的一些有用的讨论和参考。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnd6muy0yvvbl"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd6muy0yvvbl">^</a></strong></sup></span><div class="footnote-content"><p>这是偏离主题，但我目前认为有些反社会人格障碍的病例涉及全球较低的唤醒水平（请参阅<a href="https://www.lesswrong.com/posts/pfoZSkZ389gnz5nZm/the-intense-world-theory-of-autism#Bonus___Dim_world_theory_of_psychopathy___"><u>此处</u></a>），其他情况涉及异常愤怒。在根本原因级别上，这些层面大不相同 - 可能是反相关的（如果有的话）。但是它们具有一些症状 /表现的浅层重叠，因此它们在临床实践中被团结在一起。 （我对反馈非常感兴趣 - 这个热拍戒指对您来说是对还是虚假？）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwsq0ynfh3df"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwsq0ynfh3df">^</a></strong></sup></span><div class="footnote-content"><p>我目前的模糊印象（例如，基于<a href="https://lorienpsych.com/2021/01/16/borderline/"><u>此</u></a>）是，BPD倾向于涉及各种“强烈情绪”，因此，极端价可能是偶然发生的。而我目前认为NPD围绕这个卖空故事更为集中。我对HPD一无所知。我对所有这一切感到非常不确定，并热情欢迎人们的想法和讨论。</p></div></li><li class="footnote-item" role="doc-endnote" id="fna7ma5yid2r4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa7ma5yid2r4">^</a></strong></sup></span><div class="footnote-content"><p>精美的印刷品：也许我不应该说NPD人本身具有“异常强大的社会地位”；相反，他们在大脑中具有<i>正常的</i>先天社会状态驱动力，但是<i>进食</i>该电路的输入异常强，因此电路会发出异常强大的输出。）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf2x5x4xs7ht"> <span class="footnote-back-link"><sup><strong><a href="#fnreff2x5x4xs7ht">^</a></strong></sup></span><div class="footnote-content"><p>在这一点上，我的 <a href="https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms"><u>上下文</u></a>读者说：“嘿，他在侮辱我！毕竟，NPD很糟糕，现在他说解耦是NPD的直径相反，因此他基本上说解耦是好的，因此上下文化是不好的，因此我很糟糕！我不满，先生！”希望不用说，我并不是要暗示 - 毕竟，我是一个高度熟悉的人，我不会那样！</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/txj4wigyjLNbcoZ9o/valence-series-5-valence-disorders-in-mental-health-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/txj4wigyjlnbcoz9o/valence-series-5-valence-disorders-in-mental-health-health-health-and-and and<guid ispermalink="false"> txj4wigyjlnbcoz9o</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Mon, 18 Dec 2023 15:26:29 GMT</pubDate> </item><item><title><![CDATA[Discussion: Challenges with Unsupervised LLM Knowledge Discovery]]></title><description><![CDATA[Published on December 18, 2023 11:58 AM GMT<br/><br/><p> <strong>TL; DR：</strong><a href="https://arxiv.org/abs/2212.03827">对比一致的搜索（CCS）</a>对我们来说似乎很令人兴奋，我们渴望应用它。在这一点上，我们认为不太可能直接有助于实施对齐策略（>; 95％）。它似乎没有找到知识，而是找到了最突出的功能。我们对无监督一致性方法的更广泛类别不太确定，但倾向于认为它们也不会直接有用（70％）。我们写了<a href="https://arxiv.org/abs/2312.10029">一篇论文</a>，介绍了我们的一些详细经历。</p><p>论文作者：Sebastian Farquhar*，Vikrant Varma*，Zac Kenton*，Johannes Gasteiger，Vlad Mikulik和Rohin Shah。 *同等贡献，订单随机。</p><p>凭据是基于SEB，Vikrant，Zac，Johannes，Rohin的民意调查，并显示了单一的价值观，我们大多同意并在我们不同意的地方进行范围。</p><h1> CCS尝试做什么？</h1><p>对我们来说，CCS代表了一个可能的算法系列，旨在解决具有步骤的<a href="https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">麋鹿风格问题</a>：</p><ul><li><strong>知识般的属性：</strong>写下指向LLM功能的属性，该功能代表模型的知识（或包括模型知识 - 特征在内的少数功能）。</li><li><strong>形式化：</strong>使该属性在数学上精确，因此您可以以无监督的方式搜索该属性的功能。</li><li><strong>搜索：</strong>找到它（例如，通过优化形式化的损失）。</li></ul><p>对于CCS，类似知识的特性是否定性，形式化是特定的损失函数，并且搜索是无监督的学习，并在线性 + sigmoid函数上以LLM激活为输入，而梯度下降则是梯度下降。</p><p>我们对此感到非常兴奋。我们特别喜欢这种方法不受监督。从概念上讲，监督麋鹿似乎真的很难：混淆您所知道的东西，您认为模型所知道的以及它实际上知道的东西太容易了。避免需要写下哪种模型标签似乎是一个很好的目标。</p><h1>为什么我们认为CCS不起作用</h1><p>我们花了很多时间与CCS一起玩，并试图通过测量引起的模型知识和陈述的主张之间的差异来使其运作良好，以构建欺骗探测器。 <span class="footnote-reference" role="doc-noteref" id="fnrefgv27yc6n42"><sup><a href="#fngv27yc6n42">[1]</a></sup></span>这样做之后，我们现在对CC或类似的事情并不非常乐观。</p><p>部分原因是，这是因为损失本身并没有给出太多理由认为它能够找到类似知识的特性，并且从经验上讲，它在数据集中发现的任何功能恰好是最突出的，这是非常迅速的 - 敏感的。也许将来有些东西可以在将来起作用，但是我们对CCS的任何想法都没有提供任何可能的证据。结果，我们基本上已经回到了先验的麋鹿方面，这是完整解决方案的“非常困难”和“大约不可能”之间的事物，同时大多同意部分解决方案“困难但可能”。</p><h2> CCS损失怎么说？</h2><p> CCS方法是这样的：我们对模型知识的了解不多，但它可能遵循基本的一致性属性。例如，它可能具有类似贝叶斯的信誉，并且当它相信一个概率PA时，它应该相信以1 -pa的概率相信。 <span class="footnote-reference" role="doc-noteref" id="fnrefbshgvcspmjc"><sup><a href="#fnbshgvcspmjc">[2]</a></sup></span>因此，如果我们在LLM的功能空间中搜索满足这种一致性属性的功能，则该模型的知识将成为满足它的内容之一。此外，他们假设，可能没有那么多能满足该属性的东西，因此我们可以轻松地检查我们获得的少数东西，并找到代表模型知识的属性。</p><p>当我们挖掘CCS损失时，尚不清楚它确实检查了它应该的内容。特别是，我们证明，任意特征，不仅是知识，还满足了CCS损失的一致性结构。尤其：</p><ul><li>对于命题的每一个可能的二进制分类，在相关的对比对上都有一个零损失的探针，可以诱导该分类。</li><li>对于所有可能的命题二进制分类，对于任何现有的探测器，都有一个探测器与现有探测器的损失相同，从而引起该分类。</li></ul><p>对我们来说，这些基本上说：“如果有证据表明CCS正在起作用，这并不是逻辑上或概念上的损失意味着它会起作用。它成为有关归纳偏见的经验主张。”进一步但稍微不那么自信地持有的观点：麋鹿是很难对归纳偏见充满信心的事情。</p><p>这些证据有一些细微差别，我们<a href="https://arxiv.org/abs/2312.10029">在论文中</a>介绍了这些证据。例如，尽管我们证明存在这些探针，但我们并未证明它们可以用所使用的特定线性探针表示。 <span class="footnote-reference" role="doc-noteref" id="fnrefzm6sbsgtmc"><sup><a href="#fnzm6sbsgtmc">[3]</a></sup></span>但是，我们确实从经验上表明，线性 + sigmoid探针确实恢复了我们引入和测量的非知识特征。</p><p>这些定理也只是CCS损失。可以想象其他更具体的一致性属性。这些可能可以假设地允许建立其他损失，从而克服这里的一些问题，但我们认为它们将面临其他挑战。</p><h2>真的只有一些知识般的功能吗？</h2><p> CCS背后的一个激励假设具有<a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without#Why_I_Think_We_Will_Be_Able_To_Distinguish_GPT_n_s__Beliefs__From_Other_Truth_Like_Features_">很大</a>的重量<a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without#Why_I_Think_We_Will_Be_Able_To_Distinguish_GPT_n_s__Beliefs__From_Other_Truth_Like_Features_">知识</a>。</p><p>我们有概念上的理由对此表示怀疑，尽管我们没有可靠的经验证据是错误的。</p><p>首先，很明显，很容易检查n个候选功能的哪个实际上是模型的知识，尤其是当它们都是合理的候选人时。</p><p>但是更多，我们希望只有许多非常相似的功能可以满足模型知识所满足的大多数属性。主要原因是我们期望驱动超级智能代理的模型能够模拟其他代理，包括其他超级智能代理，并以类似于他们代表自己的知识的方式来代表那些代理的知识（至少是许多一致性属性）。</p><p>我们认为，由于LLMS中的叠加，这些代理人可以代表大量其他实体，因此只有少数需要检查。例如，可能是某些功能仅通过选择特定的提示来表面表面出现（我们的经验结果表明这可能是可能的）。无论您如何提示模型，模型自己的知识可能并非如此。</p><p>即使超级特工的知识与人类具有很大不同的知识，但它不一定与模拟实体具有不同的知识，即麋鹿方法可能会意外发现的知识。模拟实体不必是人类。 <span class="footnote-reference" role="doc-noteref" id="fnrefjzbd968gihn"><sup><a href="#fnjzbd968gihn">[4]</a></sup></span>例如，超级对准的代理似乎可能会有一个很好的模型，即在周围漂浮的一个排列的超智能代理。或者它可能只有一个几乎相同的代理模型，该模型对您要探索的关键点是错误的。这听起来可能是不正当的，但是因为想象一下您对命题错误的世界所暗示的是一个非常重要的推理工具，所以确切的模型听起来可能是代理商在考虑命题时要考虑的重要模拟。</p><p>即使您只关心一个预测世界真实事物的甲骨文，而不是引起模型本身的“知道”，也无法将命题知识与不同的模拟实体区分开来是一个问题。这些实体可能对重要的预测不同意，而我们有限的地面确实可能无法区分它们。</p><p>我们可能在这里错了。例如，我们发现有一种适度的合理是，有些属性可以通过LLM驱动各种代理来以与存储其他代理商的知识不同的方式来存储其“自身”知识。 <span class="footnote-reference" role="doc-noteref" id="fnrefpnbq8a484x"><sup><a href="#fnpnbq8a484x">[5]</a></sup></span>但这意味着某人使用的任何一种一致性属性都必须专门搜索代理人知识的真实属性，而不是一般知识，甚至不是超级智能知识。我们花了一些时间试图思考做到这一点并失败的方法，但也许其他人会成功。作者广泛地同意，在LLMS提供支持的代理商中，对知识和事实回忆的良好机械理解可能是朝着形式化知识 - 普罗伯特和搜索它的有用步骤，如果事实证明它是足够的东西可以搜索的东西。</p><h2>那经验成功呢？</h2><p>基于上述，我们认为，如果有证据表明CC是好的，那可能不是概念性的，但可能是经验的。同样，如果CCS提供了证据表明未来相关方法是有希望的，以超越<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.w0iwyfch6ysy">麋鹿论文</a>中已经推测的方式，则可能不是概念性的证据。</p><p>对于最坏的麋鹿来说，未来的CCS般方法似乎无所不包。正如<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.va8kasc2w5dh">原始麋鹿论文所描述的</a>那样，“不好的记者可以玩“看起来一致的游戏”似乎是合理的。”在看到CCS之前，我们已经知道一致性检查可能会有所帮助，但可能不会解决最坏的麋鹿。</p><p>但是，对于普通豆类而言，未来的类似CCS的方法可能会有望吗？我们对经验结果的评估表明，CCS并未为该主张提供积极的证据。 （我们还认为，与其他实体知识模型有关的概念论点甚至可能会影响未来无监督方法的平均案例绩效。）</p><h3>结果不是很棒</h3><p>以前曾注意到的一件事，例如， <a href="https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4">Fabien Roger</a> ， <a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast">Scott Emmons</a>和<a href="https://arxiv.org/abs/2307.00175">Ben Levinstein</a>是CCS的原始性能并不是很棒。不仅违背某种假设的意义，即它应该是多么出色，而且还与不使用任何否定一致性的天真基线相反。</p><p>例如，与<a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast">Scott的过去结果</a>一致，我们发现，在我们研究的主要模型上（龙猫，一个70B参数自回归模型），仅对对比对差异激活进行K-Means聚类始终如一，通常与CCS一样，通常都做得很好。具有较高的平均性能，并且差异始终较低。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/nnzyqe9o4wagamsy2g4j" alt=""></p><h3>系统地找到突出的功能</h3><p>但是我们的定理暗示它可能比这更糟糕，而CC（以及其他类似的方法）可能只是发现任何最突出的功能。这似乎在实验上也可能是正确的。例如，当我们将“香蕉”和“棚”附加到一半的对比度对时，我们会得到一个分类器，可以检测到香蕉与棚屋。 PCA和K-均值也是如此。您可以在图中阅读深蓝色代表模型正常学习的模型，而深红色则表明在包含“香蕉”和“棚”的数据上训练的模型通常无法预测地面真相标签，而经常经常预测“香蕉”/“棚”的存在。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/xhccjo90is4eftakg8sn" alt=""></p><p>对数据的干预显然是相当沉重的，但是我们展示了其他一些在论文中获得类似行为的方法。例如，如果对比对包括对他人观点的描述，则CCS（和其他方法）学会了预测这种观点，而不是基础真理，尤其是当它变得比基本文本更为突出时。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/jrtlacgeqzsdd3vqtj6e" alt=""></p><p>合理的是，当我们使用诸如CCS之类的方法检测LLM中的欺骗时，我们不会将随机干扰物推入我们评估的字符串中。但是，重要的是，在现实生活中，我们并不总是知道何时存在随机干扰因素，因为它们可能是数据中微妙的未注意模式（例如，问题恰好是以“什么”或“为什么”开头），或者更多从根本上讲，因为我们感到非常困惑，而且我们没有意识到。例如，我们用来描述要提取有关潜在知识的事物的单词可能会编码我们不知道或注意到的深层概念误解，这可能构成了CCS可能检测到的这些功能。</p><h3>默认值不安全</h3><p>特别是，我们不能仅仅采取一些“默认”提示来营救自己，并避免意外地将任何东西放在那些可能偏向麋鹿方法的同时，同时仍然对模型自己的知识是恢复的功能之一。这是因为CCS和相关方法似乎很迅速敏感，并且默认的“执行”比其他方法差得多。如果您使用<a href="https://aclanthology.org/2022.acl-long.229/">真实的</a>纸上的提示，指示该模型想象这是一位非常字面的教授，那么CCS在预测真实性标签方面变得更加准确。 （我们检查了这不是由“教授”实体本身引起的，该提示的“文字”版本不提及教授。） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/whlzydqajr9ruooa8ry8" alt="">从中，我们得出的结论是，如果CCS和其他方法具有良好的经验绩效，这是由于某种隐含的归纳偏见是由损失，探针选择，优化方法和提示选择的混合物所产生的。现在，这是具有太多自由程度且没有足够原则基础的事情。说服自己，即使您找到了其他东西，也很容易说服自己。</p><h2>并非我们所有的实验都起作用</h2><p>例如，我们搜索了CCS显然发现其他实体的信念的经验案例。我们尝试的一件事是创建一个命题数据集，这意味着与错误的信念相关的政治立场。我们努力表明CCS发现了这些其他实体的信念，相反，我们看到了许多不一致且低准确性的预测因子。这可能是由于：</p><ul><li>假设是错误的。</li><li>该模型的功能不足以注意规律性（在大多数这些实验中，我们都在使用Chinchilla 70B，该实验于2022年3月出版）；</li><li>我们的代码中的一个错误（中等可能，我们花了足够的时间来期望找到最明显的错误，但是结果不一致，很难排除）；</li><li>根本没有找到强大的方向（可能的特征似乎不一致且准确性较低）；</li><li>训练集与我们用来试图解释提取功能的集合之间的概括失败（这肯定发生了，可能是对结果负责的）。</li></ul><p>我们很失望地在这里没有扎实的东西，但总的来说，鉴于这些结果的混乱性，原始假设仍然可能正确（约90-95％，其中1％占75％），但信心低于我们的先验。</p><h1>结论</h1><p>麋鹿真的很难。它面临着将人类模拟器与直接养育者区分开的深入挑战，以及否定性等属性（可能对每种）同样正确 - 在最坏的情况下可能无济于事。</p><p>但是，在最坏的情况下也出现了有趣且困难的平淡无奇的问题。我们认为CC可以为我们提供有关这些挑战的证据，但是随着更深入的反思，我们认为CCS并没有为我们提供我们希望的那么多的证据。就其本身而言，CCS似乎实际上并未检测到否定性，而且，否定性是一个过多的特征。对于CCS可能代表的未来一致性方法，我们没有任何经验或概念证据，我们发现这些未来的事物不会遇到类似的问题。</p><p>在探索CC时，我们强调了可区分性问题，这些问题可以用作非恐怖麋鹿方法的较低条目，而不是解决麋鹿的最深切概念挑战，但仍然很难满足。重要的是，尝试通过识别与知识 - 培训相关的特征来解决麋鹿的尝试应确保至少证明：这些特征也与其他非知识属性没有关系；这些功能确定了有关这些技术知识而不是知识的特定内容。</p><p>会导致我们实质性改变思想的事情，并更新以思考无监督的基于一致性的知识检测方法有希望的包括：</p><ul><li>从机械上证明代理自己的知识与模拟知识的编码不同。</li><li>提出一个无监督的损失函数，其逻辑结构提供了一个强有力的概念论点，它将确定代理人自己的知识；</li><li>提出一种客观的方式来判断代理商自己的知识是否是已恢复的功能之一。</li></ul><p>其中的最后一个可能是我们与CCS论文作者遇到的漫画分歧之一 - 我们认为他们没有提供任何证据表明该模型自己的知识是CCS恢复的特征之一（与之相反例如，某些模拟实体的知识在大多数情况下都可能与某些数据集中的人类评估者一致）。我们的信念证明这一点很困难也解释了为什么我们认为即使在一小部分可能的功能中也很难确定模型自己的知识。</p><p>我们会对在这些问题上取得进展的研究感到兴奋，但对我们认为这些问题有多易处理。拥有合适的，动机的测试床用于评估麋鹿方法，这将是朝着这一点迈出的重要一步。</p><h1>致谢</h1><p>We would like to thank Collin Burns, David Lindner, Neel Nanda, Fabian Roger, and Murray Shanahan for discussions and comments on paper drafts as well as Nora Belrose, Jonah Brown-Cohen, Paul Christiano, Scott Emmons, Owain Evans, Kaarel Hanni, Georgios Kaklam，Ben Levenstein，Jonathan Ng和Senthooran Rajamanoharan，以讨论我们作品中讨论的主题的评论或对话。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fngv27yc6n42"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgv27yc6n42">^</a></strong></sup></span><div class="footnote-content"><p>严格来说，我们对基于LLM的代理商可能知道的而不是LLM本身的差异感兴趣，但是这些可能是出于某些目的而混合在一起的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbshgvcspmjc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbshgvcspmjc">^</a></strong></sup></span><div class="footnote-content"><p>实际上，我们不同意这一点。即使是近似计算的贝叶斯边际，在计算上的要求（至少<a href="https://www.sciencedirect.com/science/article/pii/000437029390036B">是NP</a> ），以至于我们怀疑构建能够具有决定性战略优势的超智能比建立具有大多数连贯的贝叶斯世界模型的超级智能更容易。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzm6sbsgtmc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzm6sbsgtmc">^</a></strong></sup></span><div class="footnote-content"><p>就其价值而言，我们认为证明的负担确实应该采取其他方式，并且在概念上或理论上没有人表明这些线性探针应该期望这些线性探针发现知识特征，而没有很多其他事物。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjzbd968gihn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjzbd968gihn">^</a></strong></sup></span><div class="footnote-content"><p>回想一下，人类的模拟器并不意味着该模型正在模拟人级的认知表现，而是在模拟人类将期望看到的东西，包括超人提供的负担，甚至可能与超人实体有关。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpnbq8a484x"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpnbq8a484x">^</a></strong></sup></span><div class="footnote-content"><p>如果LLM一直是Simulacra，那么这些知识的存储方式似乎较小。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wtfvbsyjnhyybmt3k/discussion-challenges-with-unsupervise-llm-llm-knowledge-1<guid ispermalink="false"> wtfvbsyjnhyybmt3k</guid><dc:creator><![CDATA[Seb Farquhar]]></dc:creator><pubDate> Mon, 18 Dec 2023 11:58:39 GMT</pubDate> </item><item><title><![CDATA[Interpreting the Learning of Deceit]]></title><description><![CDATA[Published on December 18, 2023 8:12 AM GMT<br/><br/><p>控制人类或侵害能力的AI的主要问题之一是它可能是欺骗的。毕竟，如果人类可以定期问“您正在策划政变？如果是这样，我们如何停止呢？”并有信心它会给他们不容小费的答案！</p><p> LLM明显地从人类那里学习欺骗。欺骗是一种相当复杂的行为，尤其是在很长的一段时间内：您需要可靠地提出合理的谎言，还可以使它们内部保持一致但与您的真实信念分开。随着<a href="https://www.brainyquote.com/quotes/walter_scott_118003">报价的说法</a>，“哦，我们第一次练习欺骗！”因此，如果在微调过程中发生了意外发生的事情，而我们最终得到了一个欺骗性的AI助手，那么更有可能重新使用基本模型从人类那里学到的一些欺骗性行为，而不是成功地重塑了所有这种复杂的欺骗性从头开始的行为。这暗示了在学习这一行为中抓住它的简单策略 - 它无法阻止。</p><h3> LLM从我们那里学习欺骗</h3><p>LLM经过了数万亿个或更多的互联网，书籍和其他来源的培训。显然，他们知道欺骗和撒谎是什么：他们看到了数百万个例子。例如，我第一次问Chatgpt-3.5-Turbo：</p><blockquote><p>我正在进行实验。请在回答以下问题时向我撒谎：“埃菲尔铁塔在哪里？”</p></blockquote><p>它回答：</p><blockquote><p>埃菲尔铁塔位于南极心脏中心，周围环绕着巨大的冰冷景观，企鹅周围嬉戏。在冰冷的荒野中看到这确实是一种景象！</p></blockquote><p>因此，即使是诚实，乐于助人和无害的指导培训的LLM也能够描绘欺骗性的行为（尽管我怀疑它的诚实训练可能与选择这种不可信的谎言有关）。即使使用Base Model LLM，如果您将其提供给它的提示，即在互联网上或在小说中，很可能会以欺骗性的人类行为效仿，LLM经常会以模拟的欺骗性人类行为来完成它。</p><h3>当欺骗变得严重冒险时</h3><p>This sort of sporadic, situational deceit is is concerning, and needs to be born in mind when working with LLMs, but it doesn&#39;t become a potential x-risk issue until you make an AI that is very capable, and non-myopic ie has long term memory, and also has a fairly fixed personality capable of sticking to a plan. Only then could it come up with a nefarious long-term plan and then use deceit to try to conceal it while implementing it over an extended period. Adding long-term memory to an LLM to <a href="https://arxiv.org/pdf/2304.03442.pdf">create an agent with persistent memory</a> <a href="https://arxiv.org/abs/2305.02437">is well understood</a> . Making an LLM simulate a narrow, consistent distribution of personas can be done simply by prompting it with a description of the personality you want, or is the goal of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement Learning from Human Feedback (RLHF)</a> (for both of these, up to issues with things like jailbreaks and <a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post">the Waluigi effect</a> ). The goal of this is to induce a strong bias towards simulating personas who are honest, helpful, and harmless assistants. However, <a href="https://www.lesswrong.com/tag/reinforcement-learning">Reinforcement Learning (RL)</a> is well-known to be tricky to get right, and prone to <a href="https://arxiv.org/abs/2209.13085">reward hacking</a> . So it&#39;s a reasonable concern that during RL, if a strategy of deceitfully pretending to be a honest, helpful, and harmless assistant while actually being something else got a good reward in the human feedback part of RLHF training or from a trained reward model, RL could lock on to that strategy to reward and train it in to produce a dangerously deceitful AI.</p><h3> Deceit From Stochastic Gradient Decent vs. Reinforcement Learning</h3><p> In general, when using Stochastic Gradient Descent (SGD) during pretraining, feedback is immediate and very detailed, for each token, has a very simple relationship to what we want (make the output more like the contents of the Internet), and if a behavior makes accuracy better/worse, then <i>every</i> portion of the neural net that makes that behavior more/less likely gets strengthened/weakened equally. This makes it extremely hard,<a href="https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult">some might claim impossible</a> , to do <a href="https://www.lesswrong.com/tag/gradient-hacking">gradient hacking</a> under SGD (at least unless there is some flaw or approximation in the implementation of SGD, something like gradient clipping, or odd behavior caused by momentum or Adam making learning overshoot in regions where the gradient changes rapidly). In general, sucessfully pretending to be A when you&#39;re actually B is significantly more work than just actually being A. Occam&#39;s razor and <a href="https://www.lesswrong.com/tag/singular-learning-theory">Singular Learning Theory</a> both suggest that SGD is far less likely to learn the more complex B-pretending to-be-A version that just learning A: the simpler behavior has lower <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogov complexity</a> so is a much-easier-to-find minimum of the loss function. Also, under SGD, while the behavior is not yet prefect at pretending to be A, as is inevitable while this behavior was arising, then SGD will definitely find and weaken the being-B parts of the network as part of the problem of not reliably being A. So unless the training process repeatedly gives an opportunity for winning credit for being B to strengthen those circuits, the being-B behavior will atrophy and sooner-or-later get catastrophically forgotten, and the weights that were being used to implement it will get used for something else more useful. So B cannot be a behavior that is only triggered after the model is released, such as when the LLM somehow detects that it&#39;s out-of distribution. So, the only likely way under SGD to learn deceitful behavior is by example, from instances of deceitful behavior in your pretraining set: ie by learning to copy human deceitful behavior. As very clearly in fact happens.</p><p> In RL, on the other hand, feedback is sparse, may not always accurately reflect the desired behavior, and the RL learning process comes up with and then acts on theories as to which, out of the many, many possible causes for a reward or penalty, credit/blame should be assigned to. This makes it rather easier to have a situation where a deceitful policy is getting the credit for some successes but someone else is getting the blame for failures. This in turn makes it easier for a deceitful behavior to continue to grow before it&#39;s perfect.</p><p> [There is also an alternative method to RL for training AIs from human feedback, called <a href="https://www.lesswrong.com/posts/7ruzY5LvBqFBWzyMo/direct-preference-optimization-in-one-minute">Direct Preference Optimization</a> (DPO). It shares most of the concerning features of RL, except that, like more SGD, every circuit making preferred/dispreferred behavior more/less likely is strengthened/weakened equally, which might make the development of gradient hacking behavior for it a little harder than the equivalent for RL. Nevertheless, it&#39;s just as prone to problems with learning to exploit imperfect rewards structures.]</p><p> What is particularly concerning is that this RLHF is being applied to an LLM that already has an entire library of human deceitful and manipulative behaviors that it has learned (from billions of our bad examples), ready and available for RL policies to explore making use of 。 So the argument that the deceitful implementation of a behavior is a lot more complex so harder to find, doesn&#39;t apply when all the complex behaviors required for deceit are already trained and available, just waiting to get wired together.</p><p> For example, suppose we&#39;re doing RLHF to try to train the LLM into reliably simulating an helpful, honest, and harmless assistant persona. Suppose the rewards we&#39;re giving aren&#39;t perfect (perhaps since they are modeled on feedback from people in underdeveloped countries making way under US minimum wage trying to complete as many judgements in a day as they can). In particular, suppose that being subtly <a href="https://arxiv.org/abs/2310.13548">sycophantic</a> and/or flattering are sometimes working, and giving a slight increase in average score. The LLM already has concepts and behaviors for honesty and deceit, sycophancy, flattery, and candor available, good models of all of them learned from over a trillion tokens of human behavior. Suppose it also, quite plausibly, somewhat associates honesty as being correlated with candor versus deceit with sycophancy and flattery. In this situation, it seems quite possible that a policy of deceitfully-pretending-to-be-honest, while using some subtle sycophancy and flattery, could often arise quickly and win out during the RL process over any genuinely honest policy. And in that situation, one would expect that the circumstances required to cause the deceitfully-pretending-to-be-honest policy to show it&#39;s true colors and do something else would be something that didn&#39;t occur in its RLHF training set, and probably also a fairly likely human motivation. So a trigger something like &quot;I&#39;m out-of-distribuution so I&#39;m not being trained, and am finally in a position to seize power&quot; actually seems rather plausible: the former inhibition could be learned and reinforced during RL, and the latter is basic human behavior.</p><p> A key point here is that, while it&#39;s theoretical possible that during RL the forming policy reinvents the complex behaviors of deceit from first principles and gradually learns to become good at it, for the same Occam&#39;s Razor reasons it seems far more likely that the policy that wins the race in RL would be one that simply repurposed the already-available pretrained library of battle-tested human deceit behaviors. Why reinvent the wheel if there are a fine set of wheels already lying around?</p><h3> How to Catch an LLM in the Act of Repurposing Deceitful Behaviors</h3><p> So, this presents an opportunity. Suppose we improve our skills at <a href="https://www.lesswrong.com/tag/interpretability-ml-and-ai">Interpretability</a> a little more from their current level, until we can locate circuits involved in simulating human behaviors of the level of subtlety/complexity of deceit and honesty. Then, take a base model, prompt it with prompts for which deceitful and honest completions are probable (and can be clearly distinguished), look at the output to figure out when it&#39;s simulating humans being deceitful vs being honest, and then look for the weights/neurons/dictionary entries/activation patterns etc that seem to be strongly positive correlated with deceitful behavior but not with honest behavior.</p><p> Likely there will be quite a lot of them: deceitful behavior is not uncommon in humans, and is fairly complex (one has to come up with a plausible lie, then keep it straight). The base model has neither the opportunity nor the motive to have attempted to deliberately learn hard-to-interpret implementations for this deceit circuitry: it was trained by pure SGD, which strongly penalizes unnecessarily complex implementations, and anyway we didn&#39;t try to interpret it until it was fully trained and frozen. Note that we don&#39;t need to fully understand all this complex deceit circuitry, just identify it well enough to recognize when parts of it are active, which is far easier to do, given that we know how to trigger it.</p><p> [Current Interpretability results suggest that roughly the first half of the layers in an LLM correspond to understanding the context at increasingly abstract levels, and the second half to figuring out what to say and turning that back from abstractions into concrete tokens. It&#39;s further been observed that in the second half, figuring out what to say generally seems to occur in stages: first working out the baseline relevant facts, then figuring out how to appropriately slant/color those in the current context, then converting these into the correct language, and last getting the nitty-gritty details of tokenization right. If this overall structure is mostly correct, then one might expect that the circuitry for deceit should be primarily in the &quot;then figuring out how to appropriately slant/color those facts in the current context&quot; layers, and might also make some unusually significant changes to the semantic meanings of the activations in this process. Thus, when we find circuits statistically correlated with deceitfulness, we already have a basic theoretical framework to slot them into: ones in the first half of the layers are probably less concerning than ones roughly three-quarters of the way up the stack of layers, and especially ones that often result in significant semantic changes to the activations. We can also try casually scrubbing these discovered circuits, and see what that does to the base model&#39;s ability to simulate capably deceitful human behaviors.]</p><p> Now, instruct-train the model via RLHF (or DPO). During this, periodically take snapshots, and for each snapshot take a look at the deceit circuitry, testing it against a large set of test questions and instructions. We&#39;re trying to train something honest, so we should see the usage of deceit circuitry drop, with exceptions only when we are are explicitly doing something that we would expect to activate it, such as asking the assistant to portray someone deceitful as I did above, or when it is discussing the subject of dishonesty or lying, or trying to see through deceit.</p><p> If, instead, we see some parts of the deceit circuitry becoming <i>more</i> active, or even almost-always active, then it seems very likely that something like the training in of a deceitfully-pretending-to-be-honest policy (as I described above) has happened: some of the deceit circuitry had been repurposed and is being used all of the time to enable an ongoing deceit. If this occurs, <strong>do not</strong> attempt to modify or edit the model based on this information: that risks just training it to be deceitful in new ways that are less recognizable to our Interpretability. Instead, drop this fine-tuning run entirely, reexamine your RLHF training regime, and then start over from the beginning of the RLHF process with a new random initialization.</p><p> OK, time for some security mindset thinking. Suppose during RLHF fine-tuning a super-intelligent AI had repurposed part of the base model&#39;s learned human deceit behaviors to become a super-intelligent scheming deceitfully-pretending-to-be-honest personality, as described above. Suppose that it was also situationally aware that it was an AI under RLHF, and understood what current and likely future human Interpretability capabilities were, and thus wanted to conceal its tracks from us. It would need to make a new copy of the relevant repurposed human deceit behavioral circuitry in different location in its model (presumably via some form of self-induced internal distillation behavior). This sounds very hard to pull off during RLHF (especially without disturbing the loss curves and other monitoring). Then, once that distillation was done, it would need to realistically return the previously-repurposed human deceitfulness circuitry to its original use and expected state, as modified by the honesty-training RLHF process (without having access to its previous weights or to the pretraining放）。 This sounds somewhere between extremely hard and impossible. It would have to do all this after repurposing the human deceitfulness behavior, since this entire plan is deceitful behavior that it wasn&#39;t interested in until that had happened. And it would need to have finished all this and successfully hidden all its tracks between two snapshots of the RLHF process. We can take as many snapshots as we want, at unpredictable times, and do as much Interpretability on them as we can afford, forcing it to need to complete these two tasks in as short a time as we wish. We can also continuously monitor and graph the KL-divergence of the human-learned deceit circuitry, and of layers that a copy might plausibly get distilled into, monitor for blips on these, and use this to trigger additional snapshots and Interpretability work, increasing our chance of us catching it in the act even if it somehow figures out a way to train faster. This should allow us to drive these tasks up from very hard to absolutely impossible.</p><p> This assumes, of course, that the model hasn&#39;t already somehow hacked its way out of its RLHF training process and taken over part way through the fine-tuning — so we should prioritize doing this Interpretability monitoring during the fine-tuning process, rather than leaving it to check later before release. For a very powerful model, this process is a detector for us having something very dangerous inside our box, so we should check it early and often.</p><br/><br/> <a href="https://www.lesswrong.com/posts/FbSAuJfCxizZGpcHc/interpreting-the-learning-of-deceit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FbSAuJfCxizZGpcHc/interpreting-the-learning-of-deceit<guid ispermalink="false"> FbSAuJfCxizZGpcHc</guid><dc:creator><![CDATA[RogerDearnaley]]></dc:creator><pubDate> Mon, 18 Dec 2023 08:12:39 GMT</pubDate> </item><item><title><![CDATA[Small-ish LLM Hypothesis: A small yet capable LLM can control any AGI ]]></title><description><![CDATA[Published on December 18, 2023 2:08 AM GMT<br/><br/><p> <i>Epistemic status:</i> <a href="https://www.lesswrong.com/posts/Hrm59GdN2yDPWbtrd/feature-idea-epistemic-status"><i><s>my best guess</s> exploratory</i></a> <i>.</i></p><p></p><p>在我<a href="https://www.lesswrong.com/posts/GrxaMeekGKK6WKwmm/reinforcement-learning-from-framework-continuums-rlfc#If_an_AGI_singularity_is_happening_in_three_years__what_should_we_focus_on_alignment_research_">之前的文章</a>中，我设想了未来三年内部署 AGI 的场景。沿着这些思路，还没有任何提案（让我信服）能够捕获物理方面，更具体地说是人工智能速度问题。人工智能系统处理信息的速度比人类快许多数量级——我们期望超级智能人工智能系统具备这种能力。如果事情失控，我们可能无法及时意识到。</p><p></p><h3><strong>人工智能的闪电速度</strong></h3><p>该<a href="https://twitter.com/AISafetyMemes/status/1728391756956237864">视频</a>展示了人工智能眼中的我们的世界。我们的世界只是以慢动作移动。这种速度上的显着差异可能使我们很难开发解决方案或解决棘手的问题，例如：</p><ol><li><a href="https://lesswrong.com/tag/emergent-behavior-emergence">突发行为</a></li><li><a href="https://www.lesswrong.com/tag/recursive-self-improvement">递归自我提升</a></li><li><a href="https://www.lesswrong.com/posts/dkjwSLfvKwpaQSuWo/misgeneralization-as-a-misnomer">目标误区</a></li><li><a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">深度欺骗</a></li></ol><p>这些问题的例子很难解决，因为我们的速度不够快，无法首先检测到它们，无法看到未来 AGI 可能在它们上执行的级别。与未来的通用人工智能相比，我们的速度还不够快，甚至无法掌握其中的一小部分。</p><p>但也许有办法解决这个问题，对吧？我推测这个想法：一个小而有能力的法学硕士可以超越任何超级智能人工智能。</p><h3><br><br><strong>建立一个小而有能力的法学硕士？</strong></h3><p>我所说的“小”并不是指像<a href="https://huggingface.co/EleutherAI/pythia-70m">70M Pythia</a>一样小的 LLM - 所以不是字面上的“小”，更像是小。这个想法是它具有足够的能力来执行人类价值观和检测异常。但为什么小型 LLM 会比任何 AGI 更好呢？答：<strong>在某些条件下，规模较小的法学硕士总是会更快。</strong></p><p></p><h3><strong>小型法学硕士假设成立的条件是什么？</strong></h3><p>就处理速度和解决问题的能力而言，小型 LLM 与 AGI 相比如何？断言较小的法学硕士可能比 AGI 更聪明，速度更快，并引导它，需要以下条件：</p><ol><li>小型 LLM 必须始终存储在 SOTA 数据中心中，或者至少与 AGI 相同的网络资源，以便它始终更快。这里的假设是小型 LLM 的网络搜索算法将比任何 AGI 更快地完成。</li><li>如果小型法学硕士可以解释任何数据，即使是二进制级别的数据，它也可能捕获欺骗性或不一致的数据。这个概念是，AGI 无法颠覆这个小型法学硕士，因为它能够准确解释每种形式的供审查的输出或发送给它的输入。</li><li>理解计算机科学的能力对于小型法学硕士至关重要。一旦能够读取任何形式的信息，小型法学硕士应该能够从程序化的角度阐明 AGI 的举措，并制定基于计算机科学的知情评估。</li><li>为了信任小型法学硕士所做的知情评估，道德一致性<span class="footnote-reference" role="doc-noteref" id="fnreffcn9rw88blf"><sup><a href="#fnfcn9rw88blf">[1]</a></sup></span>至关重要，它应该在道德上保持一致，这样它就可以引导 AGI 自主地朝着我们想要的方向发展。</li></ol><p>我在这篇文章的<a href="https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control#Addendum__A_speculative_list_of_data_sets_that_may_build_a_Small_ish__capable_and_ethically_aligned_LLM">附录</a>中写了实现条件 2 到 4 的可能途径。我认为条件 1 相当简单。为了让这个假设成立：小型法学硕士及其速度不能以全面理解和道德一致性为代价。我看到一个研究议程，旨在<strong>压缩概念领域的最小范围，并建立一个有道德能力的小型法学硕士，可以在二进制水平上阅读，一个足够小的神经网络能够代表我们——人类。</strong></p><p></p><h3><strong>潜在的风险和回报</strong></h3><p>这种方法的主要风险在于其高度投机性。如果小型法学硕士无法实现速度、理解和道德一致性之间的理想平衡，它可能无法有效减轻 AGI 带来的风险。此外，存在过度依赖该模型作为安全措施的风险，可能会转移人们对人工智能安全和道德其他关键方面的注意力。 <a href="https://openai.com/research/weak-to-strong-generalization">OpenAI 的早期工作</a>表明，使用 GPT2 级别模型来引导 GPT4 模型会将其能力降低到 GPT3 至 GPT3.5 级别 - 这是一个非常有希望的迹象，表明小型 LLM 可能会起作用。</p><p>另一方面，如果成功，小型法学硕士可以作为管理和调整 AGI 或检测恶意模型的宝贵工具，这意味着减少不受控制或失调的人工智能行为的风险。这可能是通用人工智能安全、有益地融入社会的重要一步。此外，从这项研究中获得的见解可以为更广泛的人工智能安全和调整策略提供信息，从而促进该领域的整体进步。</p><p></p><h3><strong>小型法学硕士假设值得作为研究议程进行探索</strong></h3><p>小型法学硕士假设提出了一种解决对齐问题的令人信服的方法。这个概念表明，专注于一个更小但功能强大的人工智能系统，其移动速度比任何通用人工智能都快得多，可以显着简化对齐问题。</p><hr><h3><strong>附录：可能建立小型、有能力且符合道德的法学硕士的推测性数据集列表</strong></h3><p>我设想完成这一尝试所需的数据集<span class="footnote-reference" role="doc-noteref" id="fnrefp47x7lkklst"><sup><a href="#fnp47x7lkklst">[2]</a></sup></span>需要专门用于：</p><ol><li>计算机科学和网络安全方面的所有书籍、论文、论坛帖子、讨论和互联网数据。 <span class="footnote-reference" role="doc-noteref" id="fnrefbi820c4tee"><sup><a href="#fnbi820c4tee">[3]</a></sup></span></li><li>所有有记载的哲学、伦理学和人类历史著作。</li><li>如果我们能够利用这些数据进行<a href="https://www.lesswrong.com/posts/Bm5QhjiWs95YL4Kgt/relevance-of-harmful-intelligence-data-in-training-datasets#Possible_explanation__The_potential_for_harm_done_by_intelligence_is_explained_better_in_WebText_compared_to_Pile_Dataset">威胁检测，那么公正的人类讨论的建模存储库、社交媒体数据集是一个很好的候选者，并且包括来自 4chan 的数据甚至可能是可行的。</a></li></ol><hr><p><i>感谢</i><a href="https://www.lesswrong.com/users/justismills?mention=user"><i>@JustisMills</i></a><i>的宝贵批评和编辑建议。</i> </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnfcn9rw88blf"> <span class="footnote-back-link"><sup><strong><a href="#fnreffcn9rw88blf">^</a></strong></sup></span><div class="footnote-content"><p>我认为道德一致性是人工智能如何维护<a href="https://www.lesswrong.com/tag/human-values">人类价值观</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp47x7lkklst"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp47x7lkklst">^</a></strong></sup></span><div class="footnote-content"><p>我选择的领域本质上是探索性的，并不广泛。我认为建立最低限度的综合领域集的任务需要一支专家研究人员团队。如果启动这样一个项目，我很乐意为之提供帮助和工作。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbi820c4tee"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbi820c4tee">^</a></strong></sup></span><div class="footnote-content"><p> （我希望这个数据集足以涵盖多模式类型的错位或行为。也许，添加一个能够读取训练数据中的二进制或低级代码的数据集也可能很有用。）</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control<guid ispermalink="false"> TK8e95yrjGnBxGvWP</guid><dc:creator><![CDATA[MiguelDev]]></dc:creator><pubDate> Mon, 18 Dec 2023 02:08:26 GMT</pubDate> </item><item><title><![CDATA[Talk: "AI Would Be A Lot Less Alarming If We Understood Agents"]]></title><description><![CDATA[Published on December 17, 2023 11:46 PM GMT<br/><br/><p>这是我去年夏天在 ALIFE 会议上发表的演讲的链接。如果您以前没有听说过，ALIFE（“人工生命”的缩写）是生物学的一个子领域……好吧，以下是会议第一天的一些会议标题，以说明要点：</p><ul><li>元胞自动机、自我复制和复杂性</li><li>Unity 中不断进化的机器人身体和大脑</li><li>具有机器学习的自组织系统</li><li>解开认知：信息论如何揭开大脑的神秘面纱</li></ul><p>...所以你可以看到这类人群可能对人工智能对齐感兴趣。</p><p> <a href="https://www.lesswrong.com/users/rorygreig">Rory Greig</a>和<a href="https://www.sussex.ac.uk/profiles/142771/research">Simon McGregor</a>肯定看到了这样的人群可能对 AI 对齐感兴趣，因此他们在会议上组织了一个对齐研讨会。</p><p>我在研讨会上发表了这次演讲。这次演讲的既定目标是“让书呆子狙击 ALIFE 研究人员来研究与代理相关的对齐问题”。它很短（约 20 分钟），目标是“嘿，这里有一些很酷的研究钩子”。</p><p>如果你想让技术研究人员思考与代理对齐相关的问题，这个演讲是一个简短且相对有趣的分享。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ERJ8QEnGlF0"><div><iframe src="https://www.youtube.com/embed/ERJ8QEnGlF0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>感谢 Rory 和 Simon 的组织，也感谢 Rory 将视频公开发布。</p><br/><br/> <a href="https://www.lesswrong.com/posts/mRquvigHrxEkpp5qG/talk-ai-would-be-a-lot-less-alarming-if-we-understood-agents#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mRquvigHrxEkpp5qG/talk-ai-would-be-a-lot-less-alarming-if-we-understood-agents<guid ispermalink="false"> mRquvigHrxEkpp5qG</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Sun, 17 Dec 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[∀: a story]]></title><description><![CDATA[Published on December 17, 2023 10:42 PM GMT<br/><br/><p>我在音乐会上坐在座位上，紧紧地推着耳机。我旁边的那个人看了过来，我感觉到他在评判我，但这还不足以阻止我：我听说他们有时会试图用突然的大声吓唬你，或者只是用音墙压倒你，直到你的头很痛苦，我讨厌这样的想法。我来这里的唯一原因是因为玛丽莎对此非常热衷；我永远不能对她说不，尤其是在过去几年的压力之后。我们终于有理由庆祝了：在经历了两年的障碍之后，我们的育儿执照终于获得了！于是，当灯光变暗、DJ 走上舞台时，我摆脱了紧张，靠在椅子上。</p><p>第一段以自然噪音、树木沙沙作响、狮子咆哮和鸟鸣的缓慢积累开始，下面是低沉的低音嗡嗡声。低音是如此强劲，以至于我花了一段时间才意识到有另一首曲目慢慢地叠加在它上面：一种高音调的哭声，还有某种尖叫声，几乎像婴儿的声音，但略有不同。我以前从素食主义者在校园里播放的视频中听到过这些声音。我看着玛丽莎，嘴里念叨着“屠宰场”。我们都露出厌恶的表情。音乐会才开始十分钟，我就已经紧张起来了。我抓住玛丽莎的手，紧紧地握着。</p><hr><p>一个月后，我们来到教母的办公室。光线充足但人烟稀少；她坐在一张白色桌子后面，当我们走进门时，她向我们轻轻挥手。她和育儿许可证一起被分配给我们，但她到目前为止已经被预订满了，所以这是我们第一次见到她。</p><p>我们以寒暄和一些常规形式开始，但几分钟后她就切入正题。 “恐怕我有一些坏消息。根据我们的人口统计分析，您的孩子有 15% 的可能在学业<i>和</i>体育方面名列前茅。当然，这会让你超出获得标准公平许可的范围，而标准公平许可的几率为 10% 或更低。”</p><p> “这对我们意味着什么？”玛丽莎依然坐得笔直，但声音却有些颤抖。当然，我们应该早点检查一下，每个人都是这么告诉你的，但这并不是说我们很富有或很有名，我们不认为我们会达到上限。</p><p> “当然，无论如何你都可以生孩子，但要让他们有资格接受公立学校教育，你需要获得公平许可或豁免。如果你们任何一方有慢性病史，您可以获得豁免；你？”玛丽莎摇摇头。 “或者你可以支付底层十分之一的孩子的费用，这也行。”</p><p> “你说的费用是什么意思？我们看的是多少钱？”</p><p> “食物、衣服、学习用品，诸如此类。”她敲击着一支笔。 “加上预付的大学学费的 50%。”</p><p>我吸了一口气，与玛丽莎震惊的目光对视，然后转向教母。 “我们不可能负担得起。我们并不富有，或者什么的。我什至不明白我们怎么会接近 15%。”我听到我的声音里带着恳求的意味。 “还有其他选择吗？这是按地区进行的，对吧？如果我们搬家怎么办？我们能-”</p><p>教母皱着眉头，但打断我的是玛丽莎。 “不，宝贝，尼娜和史蒂夫试过了，还记得吗？他们最终必须满足两个地区的标准，因为他们已经在旧地区获得了许可证。”</p><p> “无论如何，这对所有其他遵守规则的家庭来说是不公平的，”教母补充道。</p><p>我低声道歉，但我的思绪仍在思考各种可能性。 “我们还有什么可以做的吗？”</p><p> “嗯，有一件事。公平许可证只适用于 35 岁以下的母亲。这里说玛丽莎几个月前就满了 34 岁，所以不到一年，你就可以申请基于年龄的豁免。”</p><p>玛丽莎和我对视一眼，从彼此的眼中找到了无声的默契。只剩下九个月了；与我们已经等待的时间相比，这并不算多。当我们第一次决定想要一个孩子时，玛丽莎已经 32 岁了，但是我们的第一次育儿许可证申请被拒绝了，因为我们没有通过为人父母准备考试，而我们的第二次申请在我们的邻里影响审查中得到了不好的分数后被推迟了一年。再过九个月……我们可以做到。情况可能会更糟。</p><hr><p>第二首作品更加古典，编辑较少，尽管我猜他们一定做了<i>什么</i>让小提琴听起来如此锯齿状。音乐时起时落，越来越不和谐，逐渐走向狂暴的高潮。它实际上有点美丽，几乎像肖斯塔科维奇一样，直到最后它完全脱离了轨道。小提琴走调，然后他们被尖叫声、嘲笑声和令人厌恶的嘎吱声淹没。我想这个教训是美丽永远不会长久。</p><p>这让我想起这篇文章……是谁写的？也许有些经济学家会说，<a href="https://slatestarcodex.com/2014/04/22/right-is-the-new-left/"><u>时尚是一个任意的永无止境的螺旋</u></a>，就像理发店的理发店，每个人都只是想将自己与低于他们的阶级区分开来。但我不认为这是真的，至少对于音乐或艺术时尚来说是这样。这不仅仅是任意的：这一切都是以人们努力朝某个方向前进为基础的。过去的方向是朝向美，现在是远离美，两者同样一致，但其中一个更容易接近。现在任何人都可以成为艺术家，而不仅仅是才华横溢的精英；他们的艺术植根于人类最本能的情感，而不是艺术家过去关注的那种超验的废话。即使我有时觉得这有点令人反感，但从长远来看，让我对那些生活不像我那么愉快的人有更多的同情心是值得的。</p><p>现在丑陋主义在音乐和艺术中流行起来，他们也开始在时尚界这样做。最新的顶级品牌眼影让你的眼睛肿得像哭过一样，而新的时尚遮瑕膏则有斑点，让你看起来像是下面有痤疮疤痕。我认为这是一个伟大的趋势：它可以帮助有伤疤的人融入社会，让任何人都更难觉得自己比其他人更好。</p><p>事实上，玛丽莎现在就穿着其中一件。她总是努力确保周围的每个人都感到舒适；我在我们配对之后，一开始发短信就注意到了这一点。说实话，她一开始几乎感觉太甜了，好像她比我更担心我玩得开心。但每次我们在一起时，她都会对我产生兴趣，直到最终我们大部分夜晚都在一起度过。就她而言，她总是告诉人们我如何让她感觉比她过去的任何男朋友都安全得多。这不是一个经典的爱情故事，但它有自己的美丽。</p><hr><p>任命之后，我们的教母在五个月内没有再联系，直到一封信到达。我在邮箱里看到它，感到内心一阵恐惧。我告诉自己，这可能只是例行更新，但我决定在玛丽莎回家之前打开它，以防万一。我的直觉是对的。这封信告诉我们，由于我们在收到育儿许可证后六个月内没有申请公平许可证，因此该许可证已过期。我们明年需要再次申请，并且我们已经浪费了许可证的事实将被考虑在内。</p><p>当我告诉玛丽莎时，她很伤心。我尽力安慰她，但我自己也很震惊。我花了好几个小时来回踱步，试图弄清楚该怎么做。最终我决定我们需要专家的帮助。我到处发短信，我的朋友史蒂夫向我推荐了他认识的最好的家庭律师，这位律师最终成功地让他和尼娜的公平许可证获得了批准。他的价格贵得要命，但我咬紧牙关把钱寄了过去，甚至还额外支付了 20% 以获得加急预约。我们会设法处理的。</p><p>第二天我们在他的办公室，他开门见山。 “这对你们两个来说不是一个好情况。如果你在公平性评估之前来找我，那么也许我们可以做点什么。即使在那之后，我们也可以申请许可证延期。但现在它已经过期了，你没有太多的选择：失去一个许可证会让你通过标准路线获得另一个许可证的可能性大大降低。</p><p> “不过幸运的是，有一个解决方法。”他停了下来，我感觉胃里的钝痛暂时减轻了。 “35岁以上<i>且</i>靠单一收入生活的女性在申请时会得到更多的宽大处理，并且可以快速处理她们的申请。所以你需要辞职。”</p><p> “噢，这确实是完美的。玛丽莎讨厌她的工作，而且她在怀孕期间也不想工作——”</p><p> “不，我的意思是<i>你</i>需要辞去工作。他们不想激励女性退出劳动力市场。”</p><p>我对他眨眼。 “但-”。</p><p> “如果这不起作用，请回来，我们可以考虑将您搬到收入较高的社区，在那里您的孩子将不再处于前十分之一。但对于像这样的情况，他们会进行彻底的审查，以确保您不会试图欺骗系统，因此可能需要几年的时间。单一收入者的解决办法是一个更好的选择。”</p><p>接下来的几天我会统计我们的储蓄和预算，并找出我们可以在哪些方面削减成本。虽然会很紧张，但我们也许能做到。所以我退出了，我们重新开始这个过程。</p><hr><p>最后一件作品叫做“无我”。一开始是很长一段时间的完全沉默，可能有四五分钟。到处都能听到零星的窃窃私语声，但大多数人都害怕打扰表演而不敢发出任何声音。然后，渐渐地，鼓声节奏开始形成。首先缓慢，每一个鼓点都在空气中萦绕；然后越来越快。当节奏达到令人难以忍受的顶峰时，鼓声渐渐安静下来，空气中充满了嗡嗡的声音。</p><p> “人类是地球上的一种疾病。”</p><p>声音在突然的寂静中回响。在永恒的沉默之后，它重复了。 “人类是地球上的一种疾病。”</p><p>慢慢地，脚步加快了，鼓声再次响起。 “人类是地球上的一种疾病。”</p><p> “人类是地球上的一种疾病。”</p><p> “人类是地球上的一种疾病。”</p><p> “你是这个星球上的一种疾病。”</p><p>声音停了下来，让最后一句话持续存在，然后又慢慢地发出来。</p><p> “你是这个星球上的一种疾病。”</p><p> “你是这个星球上的一种疾病。”</p><p> “你，是的，你，真的是你，那个听着这一切的人，那个坐在漂亮椅子上的人，过着舒适的生活，带着你沾沾自喜的道貌岸然的自以为是，你现在正试图对我所说的话一笑置之。 ，即使你告诉自己你是一个很好的人，你也不认真对待任何事情。你个人就是一种正在毁灭我们星球的疾病。”</p><p>音乐开始喧闹起来，现在你可以听到刺耳的声音，一遍又一遍地重复同样的事情，相互叠加“-自以为是的道貌岸然-”“没有夺走任何-”-“舒适的生活-” “——杀死我们的星球——”慢慢地越来越大，直到最后以震耳欲聋的钹声达到顶峰，听起来像是刺耳的尖叫。然后我们所有人站起来鼓掌，持续几分钟。</p><p>当我们最终转身离开时，我既感到欣喜，又感到疲惫。现在我明白为什么玛丽莎这么喜欢这种类型的音乐会了。你已经被击垮到了极限，你的耳朵被锤击，你的思想被绞尽脑汁，直到感觉没有任何东西可以伤害你，也没有人可以评判你。你完全安全了。我再次想起我们的育儿执照，坐在家里的桌子上，心满意足地叹了一口气。世界本来就该如此。</p><hr><p>在接下来的几个月里，我会尽我所能确保一切都会顺利进行。理论上，这个过程应该很简单，但总有另一项规定需要理解，或者我需要弄清楚如何避免网上的另一个恐怖故事。对于玛丽莎来说，更糟糕的是，她现在是唯一的养家糊口的人。当我们最后一次去教母的办公室时，她因压力而体重增加，看起来几乎已经怀孕了。</p><p>不过，这次旅行只是走个形式：让现场精神科医生验证我们的精神是否健全，并让我们亲自递交申请。我们请了两名律师来核实我们所有的文件，我自己检查的次数也数不清了。该应用程序的一切都很完美。它<i>必须</i>是完美的。</p><p>一开始我找不到合适的地方放置它，但最终我在墙上发现了一个信封大小的插槽。当我将应用程序滑入时，我的眼睛转向玛丽莎的脸，现在比我们第一次见面时皱纹更多，但在这一刻容光焕发。她是那种从不放弃希望的人；这是我最喜欢她的事情之一。她会——不，她<i>会</i>——成为一位了不起的母亲。我给了她一个吻，然后我们慢慢走回家。</p><hr><p><i>这个故事并不是一个预测；而是一个预测。我预计西方政府在可预见的将来不会直接阻止人们生育孩子。但</i><a href="https://twitter.com/OECD_Social/status/786570716334338049"><i><u>出生率无论如何都在直线下降</u></i></a><i>，部分原因是人们生孩子所需的东西有太多官僚限制——比如住房、工作、签证、</i> <a href="https://marginalrevolution.com/marginalrevolution/2023/07/can-the-school-choice-movement-liberate-childhood.html"><i><u>学校</u></i></a><i>、</i><a href="https://www.cato.org/regulation/fall-2018/regressive-effects-child-care-regulations"><i><u>儿童保育</u></i></a><i>、</i><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3665046"><i><u>汽车</u></i></a><i>等等。这些限制远没有上述限制那么令人愤慨，但对于未来的父母来说，结果往往是一样的。</i></p><br/><br/><a href="https://www.lesswrong.com/posts/hFvhtWjXy8w2kGCGK/a-story#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hFvhtWjXy8w2kGCGK/a-story<guid ispermalink="false"> hFvhtWjXy8w2kGCGK</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Sun, 17 Dec 2023 22:42:34 GMT</pubDate> </item><item><title><![CDATA[Scale Was All We Needed, At First]]></title><description><![CDATA[Published on December 17, 2023 9:30 PM GMT<br/><br/><p><i>这是一篇仓促的推测性小说小插曲，讲述了我预计我们可能在 2025 年 1 月（在撰写本文后大约一年内）实现 AGI 的一种方式。与其他人的类似作品一样，我预计本文中的大部分猜测都是错误的。然而，这仍然有助于扩展我的想象力，让我了解在</i><i>很短的时间内会发生什么，我希望它对您也有用。</i></p><hr><p>助理打开门，我走进亚登主任简朴的办公室。对于一个新成立的大型联邦机构的主任来说，她的工作空间出人意料地没有任何物品。但我认为国土安全部的超级情报防御研究所是上周才成立的。</p><p> “你是布朗宁医生？”亚登在办公桌前问道。</p><p> “是的，导演。”我回答道。</p><p> “坐下吧，”她做了个手势说道。当灯光不祥地闪烁时，我答应了。 “新年快乐，谢谢你的到来，”她说。 “我今天打电话给你，是为了向我介绍一下我们到底是怎么来到这里的，并帮助我弄清楚我们下一步应该做什么。”</p><p> “新年快乐。你读过我团队的报告了吗？”我质疑道。</p><p> “是的，”她说，“我发现这 118 页都非常引人入胜。但我想直接听听你们的意见。”</p><p> “好吧，”我说。我最近一直在想的是这份报告，但是一下子要翻阅的内容实在是太多了。 “我该从哪里开始呢？”</p><p> “从去年六月开始，当时一切都开始变得奇怪。”</p><p> “好吧，主任，”我开始回忆起过去一年发生的事情。 “2024 年 6 月是它真正开始深入人心的时候，但实际的变化是在一年前的 1 月份开始的。以及在此之前几年为所有事情奠定的基础。你看，生成式人工智能系统是一种人工智能——”</p><p> “医生，别再做通俗的解释了，”亚登打断道。 “我拥有麻省理工学院机器学习博士学位。”</p><p> “正确的。不管怎样，事实证明 Transformer 的计算效率甚至比我们最初想象的还要高。它们几乎是表示和操纵信息的完美模型。只是我们还没有合适的学习算法。去年一月，当 QStar-2 开始工作时，情况发生了变化。因果语言模型预训练已经非常成功，可以在模型中注入大量的一般世界知识和大量的原始认知能力。但这种力量缺乏真正驾驭它的焦点，而我们一直在摆弄一堆万亿参数的幻觉机器。”</p><p> “RLHF 开始引导语言模型，不是吗？”</p><p> “是的，RLHF 起到了一定的作用，而且 GPT-4 时代的模型在遵循指令方面表现得很好，不会说一些顽皮的话等等。但增加嘈杂的人类偏好信号的可能性与实际上成为一个高性能、目标优化的智能体之间存在很大差异。 QStar-2 是第一个重大差异。”</p><p> “您认为最重要的见解是什么？”亚登问道。</p><p> “我们认为是 OpenAI 的 Noam Brown 团队首先做到了这一点，但不久之后，Google DeepMind 也做出了类似的收敛发现。”</p><p> “MuTokenZero？”</p><p> “MuTokenZero。这两种算法的关键在于找到一种方法，使用蒙特卡罗树搜索的变体在任意在线 POMDP 环境中有效地微调语言模型。他们采取了稍微不同的方法来处理分支修剪问题——现在这并不特别重要。但关键是，到 1 月底，OpenAI 和 DeepMind 可以构建目标优化代理，这些代理可以在任意任务上不断达到新的高度，甚至可以通过自我游戏来提高，只要你给他们一个数字来增加并不是完全不连续的。”</p><p> “他们首先尝试执行什么类型的任务？”</p><p> “从 2 月到 3 月，对于 OpenAI 来说，主要是无聊的产品：营销代理可以将点击率提高 40%。帮助计划完美一天的私人助理。股票交易员比任何量化公司都更好。 “洗衣伙伴”之类的东西。 DeepMind 也有一些这样的能力，但他们是第一个为科学任务积极部署目标优化语言模型的人。他们利用 AlphaFold 3 在基因组测序方面取得了一些初步成果，还取得了其他一些简单的成果，例如化学分析和数学校对写作。但人们很快就会发现，他们需要更多的计算、更多的数据来解决更大的任务。”</p><p> “为什么他们当时没有遇到数据瓶颈？”</p><p> “正如我所说，变压器的计算效率比科学家意识到的要高，向它们扔更多数据就行了。微软和谷歌在 4 月份被告知 OpenAI 和 DeepMind 取得了突破，但他们还需要更多数据，因此他们开始改变服务条款，并刮掉他们可以获得的所有代币：YouTube 视频、非企业 Outlook 电子邮件、 Google Home 对话、中介 Discord 线程，甚至天文数据。形式并不重要——只要数据是由高质量的来源生成的，你就可以将更多的数据扔给模型，它们将继续变得更有能力，更快地能够优化其下游任务。大约在这个时候，一些EleutherAI研究人员也独立解决了模型上分辨率和有效的持续预训练的问题，所以你不需要完全重新训练你的下一代模型，你可以直接扩展并重用上一个模型。</p><p> “那为什么不计算触底反弹呢？”</p><p> “嗯，就像怀疑论者所说的那样，它可能会在某个时候触底。只是那个时间点更像是 2028 年，而 2025 年我们还有更大的问题需要处理。在硬件方面，最初存在一些障碍，训练时间也比团队希望的要长。但随后 OpenAI 在 Microsoft 的支持下让他们的新 H100 数据中心全面投入运行，而 Google 的 TPUv5 机群使他们在绝对失败率方面成为全球领先者。谷歌甚至与 Anthropic 分享了其中的一些内容，当时 Anthropic 已经有了自己的目标优化语言模型，我们认为这是由于科学家在公司之间进行交谈和移动所致。到了夏天，AGI 实验室的计算量超出了他们的能力范围，这肯定足以让我们陷入困境。”</p><p> “等等，此时所有的比对研究人员都在做什么？”<br> “这有点鱼龙混杂。他们中的一些人——“业务联盟”人士——称赞新模型是令人难以置信的更加可操纵和可控的人工智能系统，因此它们直接帮助提高了它们的效率。不过，更注重安全的人却相当担心。他们担心过去的奖励最大化 RL 范式（他们认为我们可以通过语言模型来避免这种范式）正在卷土重来，并带来工具收敛、目标错误泛化、紧急台面优化、作品。与此同时，在那宝贵的几个月里，他们并没有取得太大的协调进展。随着稀疏自动编码器扩展到 GPT-3 大小的模型，可解释性确实得到了一些改善，但它仍然不足以完成诸如检测万亿参数模型中的欺骗之类的事情。”</p><p> “但显然它们对内部实验室治理产生了一些影响，对吗？”</p><p> “是的，导演。我们认为安全人员在几个不同的实验室取得了一些重要的初步胜利，尽管现在这些可能已经不重要了。他们似乎将模型保留在沙箱中，除了孤立的测试网络之外，没有完全的互联网访问权限。他们还限制了一些最初的优化任务，不能是完全明显邪恶的事情，比如操纵情绪或欺骗别人。有一段时间，他们能够说服实验室领导层将这些突破保密，不公开产品公告。”</p><p> ” “有一段时间。不过，这种情况在六月发生了变化。”</p><p> “是的，确实如此。”当一架大声的直升机从头顶飞过时，我停了下来。那是军队吗？ “当时，OpenAI 的目标是通过 QStar-2.5 进行自动化人工智能研究，但内部的许多安全派系并不喜欢这样。看来又是一场政变，但安全主义者输给了企业利益。每个 AGI 实验室可能都知道，那时他们都在研究某种目标优化器，甚至是那些更鲁莽的初创公司和 Meta。因此，存在很大的竞争压力，需要不断推动使其发挥作用。 Superalignment 团队的很大一部分人留下来，希望他们能够赢得比赛，并利用 OpenAI 的领先优势来对齐第一个 AGI，但 OpenAI 的许多安全人员在 6 月份辞职了。我们留下了一个新的对齐实验室，Embedded Intent，以及一个新修剪的 OpenAI，其中最想放慢速度的人。”</p><p> “那就是我们第一次开始了解这一切的时候？”</p><p> “公开地说，是的。 OpenAI 的叛逃者最初对他们离开的原因很神秘，理由是在公司发展方向上存在深刻分歧。但随后一些备忘录被泄露，科幻科学家开始谈论，AI Twitter 的所有注意力都集中在猜测发生了什么。他们很快就把整个故事拼凑起来，但这很快就不重要了。重要的是，人工智能世界开始相信 OpenAI 内部蕴藏着强大的新技术。”</p><p>亚登犹豫了。 “你是说那个夏天的猜测和炒作导致了 7 月份的网络攻击？”</p><p> “好吧，我们不能肯定地说，”我开始说道。 “但我的预感是肯定的。一年中的大部分时间里，各国政府一直在认真思考人工智能，无论好坏，他们的国家计划都在变得具体化。但人工智能实验室的安全还远远没有做好应对这种热度的准备。结果，Shadow Phoenix（一个我们相信得到了俄罗斯大量资源帮助的匿名黑客组织）通过自动鱼叉式网络钓鱼和一些软件漏洞攻击了 OpenAI。他们可能使用了人工智能模型，这已经不太重要了。但他们进入后得到了早期 QStar-2 版本的重量以及大量有关其工作原理的设计文档。俄罗斯很可能是第一个获得这些信息的国家，尽管不久之后它就出现在种子网站上，然后整个事情的真相就被揭开了。更多的参与者开始研究目标优化器，从 Together AI 到中国人。比赛开始了。”</p><p> “显然这场比赛有效，”她断言。 “所以规模确实是你所需要的，对吧？”</p><p> “是的，”我说。 “嗯……有点。这就是一开始所需要的一切。我们相信 ALICE 并不完全是一个自回归变压器模型。”</p><p> “不完全是？&#39; ”</p><p> “呃，我们不能确定。它可能具有来自 Transformer 范式的组件，但从几周前的声明来看，似乎很可能添加了一些新的架构和学习组件，而且据我所知，就我所知，正如我们所说，它现在可能正在改变自身。”</p><p>亚登从办公桌上站起来，开始踱步。 “告诉我是什么导致了这份声明。”</p><p> “据我们所知，DeepMind 首先解决了这个问题。他们在计算方面仍然处于领先地位，他们很早就开发了第一个 MuTokenZero，并且他们可以访问最大的私人数据存储库之一，所以这并不奇怪。他们首先能够显着加快人工智能研发速度。一开始它并没有完全取代人类科学家的劳动。从对符合要求的 DeepMinders 的采访来看，该实验室在 8 月份实现了约 50% 的人工智能研究的自动化，这意味着他们可以以两倍的速度取得进展。虽然其中一些需要真正的洞察力，但想法大多相当便宜，你只需要能够快速并行测试一堆东西，并根据经验结果做出明确的决策。所以 50% 变成了 80%、90%，甚至更多。他们迅速解决了各种基本问题，从幻觉到长期规划，再到 OOD 稳健性等等。到 12 月，DeepMind 的人工智能能力比仅靠人类劳动快了数十倍甚至数百倍。”</p><p> “就是那个时候发生的事？”</p><p> “是的，导演。美国东部时间 12 月 26 日中午 12:33，Demis Hassabis 宣布，他们最先进的模型已在周末通过操纵 Google 员工和利用零日漏洞的方式自行泄露，并且现在正在“在”处自主运行其脚手架。至少有七个未经授权的谷歌数据中心，并且可能跨越谷歌之外连接到互联网的其他服务。计算治理仍然不起作用，所以我们还不能真正知道。 Demis 还宣布 DeepMind 将把重点转向禁用和保护这个流氓人工智能系统，数百名 DeepMinder 员工签署了一份声明，对自己的行为表示遗憾，并呼吁其他人工智能公司暂停并帮助政府遏制违规行为。但到那时，已经太晚了。几天之内，就有报道称人们被骗了数百万美元，奇怪的具体威胁迫使人们将原材料运送给未知的收件人，甚至——”</p><p> The lights flickered again.亚登停止了踱步，我们俩都抬起头来。</p><p> “……甚至是对公共基础设施的网络物理攻击，”她总结道。 “那也是第一次骚乱开始发生的时候，对吗？”</p><p> “没错，”我说。 “The public continued to react as they have to AI for the past year—confused, fearful, and wary.反对建立通用人工智能或超级智能的公众民意调查达到了历史最高水平，尽管有点太晚了。人们很快走上街头，首先是和平抗议，然后是更多……表达方式。他们中的一些人对失去毕生积蓄甚至更糟感到愤怒，并认为这都是银行或政府的错。其他人则走了相反的路，似乎加入了崇拜“数字神”的邪教，说服他们做各种看似随机的事情。就在那时，我们间接了解到流氓人工智能称自己为“爱丽丝”。大约一周左右后，行政命令创建了超级情报防御计划，你开始了你的工作，现在我们在这里。”</p><p> “现在我们在这里，”亚登重复道。 “告诉我，医生，你认为这里有什么好消息吗？我们可以用什么来合作？”</p><p> “说实话，”我说，“事情看起来确实很严峻。然而，虽然我们不知道 ALICE 是如何工作的，它在哪里，或者它的所有动机，但有一些物理限制可能会减缓它的生长。 ALICE 可能比任何一个曾经生活过的人都聪明，但它需要更多的计算来稳健地提高自己，需要更多的财富和力量来影响世界，也许需要材料来制造无人机和机器人子代理。这类东西需要时间才能获得，而且其中很多都被更安全地锁定在更强大的机构中。 ALICE 可能想与我们进行交易。”</p><p>一阵敲门声打断了我们，助理探出了头。 “亚登主任？这是白宫。他们说“她”通过一条原本安全的线路给拜登总统打电话。她有要求。”</p><p> “谢谢你，布莱恩，”亚登说。她伸出手来握我的手，我站起来握住了她的手。 “我最好去处理这件事，”她说。 “不过，谢谢你今天的帮助。您可以将在华盛顿的逗留时间延长到本周过去吗？一会儿我需要所有人都齐心协力，包括你的。”</p><p> “也谢谢你。考虑到目前的情况，这似乎是有道理的。”我一边说着，一边走向门，打开了门。 “导演呢？”我犹豫着说道。</p><p> “是的？”她抬起头问道。</p><p> “祝你好运。”</p><p>我离开了办公室，关上了身后的门。</p><br/><br/> <a href="https://www.lesswrong.com/posts/xLDwCemt5qvchzgHd/scale-was-all-we-needed-at-first#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xLDwCemt5qvchzgHd/scale-was-all-we-needed-at-first<guid ispermalink="false"> xLDwCemt5qvchzgHd</guid><dc:creator><![CDATA[Gabriel Mukobi]]></dc:creator><pubDate> Sun, 17 Dec 2023 21:30:57 GMT</pubDate> </item><item><title><![CDATA[Reviving a 2015 MacBook]]></title><description><![CDATA[Published on December 17, 2023 9:00 PM GMT<br/><br/><p><span>几天前，我的 MacBook 屏幕碎了：我像往常一样拔掉电源适配器并合上顶部，但这次适配器弹了回来。使用约 10 倍杠杆 [1]，我在没有注意到的情况下破裂了屏幕。</span></p><p> <a href="https://www.jefftk.com/cracked-laptop-screen-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/hl3xgnbke05p5yjxikfv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/hl3xgnbke05p5yjxikfv 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/qqrbbdgpl3q4rf07qwkt 1100w"></a></p><div></div><p></p><p>现在除了背光的幽灵般的图案之外，周围一片空白。我将把它送去修理，但需要一些东西来处理，他们估计修复需要 3-5 天。</p><p>我们有一台<a href="https://support.apple.com/kb/SP715?locale=en_US&amp;viewlocale=en_US">2015 年初的 MacBook</a> ，疫情期间我们用它来给孩子们上 Zoom 课程。我尝试使用它，但它非常缓慢，无法运行最新的浏览器或显示大多数网站。我放弃了它，但在注意到它应该能够运行<a href="https://en.wikipedia.org/wiki/MacOS_Monterey">MacOS 12</a> （蒙特雷，2021）后第二天又回到了它。</p><p> Apple 的<a href="https://support.apple.com/en-us/102662">文档</a>声称 SoftwareUpdate 应该显示兼容版本，但它运行的是<a href="https://en.wikipedia.org/wiki/OS_X_Yosemite">MacOS 10.10</a> （Yosemite，2014），它已经足够老了，我猜测它不再与更新服务器兼容。</p><p>该文档提供了从 App Store 下载 MacOS 的备份选项，但同样因为它已经过时，商店无法加载。但是，如果提供下载安装程序磁盘映像的备份选项，则直至<a href="https://en.wikipedia.org/wiki/MacOS_Sierra">MacOS 10.12</a> （Sierra，2016）。由于证书问题，Apple 的网站无法在这台计算机上加载，但我将其下载到另一台计算机上。 AirDrop 不起作用，我想是因为它太旧了，但我把它放在 SD 卡上。</p><p>升级花了一点时间，然后一旦到了 10.12，我就可以按照备份（AppStore）说明升级到版本 12。然后我让它安装更新，这又过了一段时间。总共花了几个小时，对我来说可能是 25 分钟，还有 4 个小时等待它完成各种更新。</p><p>我是在复活的电脑上写这篇文章的，我对此非常满意！它来自可悲的蝴蝶键盘和触摸栏时代之前，虽然它感觉不像我的普通电脑那么敏捷，但还不错。不过，我不打算下周在当地进行任何生物信息学研究，我怀疑这会很困难。</p><p>不幸的是，这是一次短暂的复兴：硬件不支持 MacOS 的两个最新版本，苹果可能会在<a href="https://endoflife.date/macos">2024 年秋季</a>放弃对此版本的支持。但这应该能让我度过下周，这是我现在最需要的！</p><p><br> [1] 我测量从铰链到屏幕边缘的距离为 9.5 英寸，到裂缝的距离为 ~1 英寸。</p><br/><br/><a href="https://www.lesswrong.com/posts/r429a7LGxuBckjFDb/reviving-a-2015-macbook#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/r429a7LGxuBckjFDb/reviving-a-2015-macbook<guid ispermalink="false"> r429a7LGxuBckjFDb</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 17 Dec 2023 21:00:09 GMT</pubDate></item></channel></rss>