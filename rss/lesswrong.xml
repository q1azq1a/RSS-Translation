<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 19 日星期四 12:23:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Is Yann LeCun strawmanning AI x-risks?]]></title><description><![CDATA[Published on October 19, 2023 11:35 AM GMT<br/><br/><p> Tom Chivers 表达了他对 Yann LeCun 的不满： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/ay5hqldsvgcyos6u0ykw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/epxt3pkzbh5pwhplbaof 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/wapfsll4jiw5zwtebhxp 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/nmloymoimfrfrme6d7az 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/qhbn7lxjr7k894avpdtd 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/lm2kuaw5lwgdewihvp5m 410w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/myx3b07p6p5owhl9y4uu 490w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gYwfkjK8vsowfDkZu/tjsquqzucvqymi4zha5d 570w"></figure><p>我也发现他的评论令人沮丧，但我想提供另一种可能的解释。<br><br>尽管人工智能安全社区中基本上没有人提出这一论点，但不幸的是，普通大众中的许多人都是这样看待人工智能的。</p><p> Yann 可能认为，对他来说，解决更多人持有的这一信念比解决人工智能安全人群的争论更重要。</p><p>他可能认为，如果他专注于解决最佳论点，他就会因为天真的人们相信“稻草人”论点而输掉政治斗争。</p><p>有鉴于此，我认为说他是在向稻草人讲话并不完全准确。我也觉得这很令人沮丧，我希望他能直接向我们讲话。但我也理解促使他做他所做的事情的动机。</p><br/><br/> <a href="https://www.lesswrong.com/posts/gYwfkjK8vsowfDkZu/is-yann-lecun-strawmanning-ai-x-risks#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gYwfkjK8vsowfDkZu/is-yann-lecun-strawmanning-ai-x-risks<guid ispermalink="false"> gYwfkjK8vsowfDkZu</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Thu, 19 Oct 2023 11:35:09 GMT</pubDate> </item><item><title><![CDATA[Announcing new round of "Key Phenomena in AI Risk" Reading Group]]></title><description><![CDATA[Published on October 19, 2023 11:05 AM GMT<br/><br/><p> <strong>TLDR：</strong> “ <a href="https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading">人工智能风险的关键现象</a>”是一个为期 8 周的辅助阅读小组。它针对的是对概念人工智能一致性研究感兴趣的人们，特别是来自哲学、系统研究、生物学、认知和社会科学等领域的人们。我们运行过一次，现在正在重复。</p><p>该计划将于<strong>2023 年 11 月至 2024 年 1 月</strong>期间运行。请于 10 月 29 日（星期日）之前<a href="https://forms.gle/cdr4UeocE7Jg5SUF6"><strong>在此</strong></a>注册<strong>。</strong></p><h2><strong>什么？</strong></h2><p> “人工智能风险中的关键现象”阅读课程对人工智能风险中的一些关键思想进行了深入介绍，特别是来自误导性优化或“后果主义认知”的风险。因此，它的目标是在很大程度上保持对解决方案范例的不可知性。它包括 90 分钟的引导讨论，并且每次会议需要至少 2 小时的阅读时间。它是虚拟的且免费的。</p><p>请参阅<a href="https://www.lesswrong.com/posts/mqvxR9nrXAzRr3ow9/announcing-key-phenomena-in-ai-risk-facilitated-reading#Summary_of_the_curriculum"><u>此处的旧帖子，</u></a>了解课程的简短概述； <a href="https://docs.google.com/document/d/1hgZOv-PfYYgayspSb_8D_OdQ6dV12xI2bsLuq57A3yg/edit?usp=sharing"><u>这里</u></a>有更广泛的总结；在<a href="https://docs.google.com/document/d/1HGzMBMXQD9w9K32scqCoSmZNGbxLJE8-siPlonTQz6s/edit?usp=sharing"><u>这里</u></a>查看完整的课程（将在接下来的几周内进行小幅更新）。</p><h2><strong>发生了什么变化？</strong></h2><p>由于上次迭代中参与者和协调人的反馈，该计划得到了改进。现在是一个为期8周的项目（最后增加了一周进行反思）。阅读材料更加集中，我们将添加更多技术性可选阅读材料。</p><h2><strong>为了谁？</strong></h2><p>该课程主要针对对人工智能风险和一致性概念研究感兴趣的人们。</p><p>它旨在让哲学（代理、知识、权力等）和系统研究（例如生物学、认知、信息论、社会系统等）等领域的受众能够理解。</p><h2><strong>什么时候？</strong></h2><p>阅读小组将于<strong>2023 年 11 月至 2024 年 1 月举行。</strong></p><p>我们预计将进行 6 组，每组 4-8 名参与者（包括 1 名协调员）。每个小组将由一位对人工智能风险有深入了解的主持人领导。</p><h2><strong>报名</strong></h2><p>请<strong>于 10 月 29 日之前</strong><a href="https://forms.gle/isv2ZeTkffjRBdYM8"><strong><u>在此</u></strong></a>注册。</p><h2><strong>关于申请</strong></h2><p>该申请包括一个阶段，我们要求您填写一份表格，其中包含</p><ul><li>你的简历</li><li>您参与该计划的动机</li><li>您之前接触过的人工智能风险/迄今为止的调整情况</li></ul><p>我们根据对他们为人工智能做出贡献的动机以及他们将从参与该计划中获得多少反事实收益的最佳理解来选择人员。</p><p></p><hr><p></p><p>如果您有任何疑问，请随时在下面发表评论或通过<u>contact@pibbss.ai</u>联系我们</p><br/><br/><a href="https://www.lesswrong.com/posts/vakhhNHduW9gmENTW/announcing-new-round-of-key-phenomena-in-ai-risk-reading#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/vakhhNHduW9gmENTW/announcing-new-round-of-key-phenomena-in-ai-risk-reading<guid ispermalink="false"> vakhhNHduW9gmentENTW</guid><dc:creator><![CDATA[DusanDNesic]]></dc:creator><pubDate> Thu, 19 Oct 2023 11:05:55 GMT</pubDate> </item><item><title><![CDATA[[Video] Too much Empiricism kills you]]></title><description><![CDATA[Published on October 19, 2023 5:08 AM GMT<br/><br/><p><a href="https://youtu.be/vqHlPb18ROE?si=vf840i97GZwgxIjD">这</a>是我两个月前制作的视频。它对一个重要的基础论点给出了平庸的解释：</p><p>只要您能够衡量特定变化的效果，通常就可以使用经验方法取得进展。即使您并不真正理解&lt;您正在做什么/您正在构建的系统>;，这一点仍然成立。这解释了为什么研究人员可以提高机器学习的能力，即使他们基本上根本不了解当前深度学习系统的内部结构。</p><p>我还认为，理解方面的任何进步都可能是危险的，因为它通常会提高可以通过经验方法有效探索的事物的前沿。由此推论，机械可解释性可以使能力的提升变得更容易。</p><p>这个论点概括得非常广泛。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tqDT8CCm4jubaWkC3/video-too-much-empiricism-kills-you#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tqDT8CCm4jubaWkC3/video-too-much-empiricism-kills-you<guid ispermalink="false"> tqDT8CCm4jubaWkC3</guid><dc:creator><![CDATA[Johannes C. Mayer]]></dc:creator><pubDate> Thu, 19 Oct 2023 05:08:10 GMT</pubDate> </item><item><title><![CDATA[Are humans misaligned with evolution?]]></title><description><![CDATA[Published on October 19, 2023 3:14 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:01:11 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:01:11 GMT" user-order="1"><p>有一种观点认为，尽管人类在最大化包容性遗传适应性 (IGF) 的压力下进化，但人类实际上并没有尝试最大化自己的 IGF。正如论证所言，这表明，在我们创建通用智能的过程的一种情况下，所创建的智能的优化目标最终与创建它的过程的优化目标并不相同。 。因此，默认情况下不会发生对齐。引用<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">《人工智能中心对齐问题：能力泛化和急速左转</a>》：<br></p><blockquote><p>在[人工智能]能力飞跃发展的同时，它的对齐属性也被揭示为肤浅的，并且无法泛化。这里的核心类比是，优化猿类以实现包容性遗传适应性 (IGF) 并不会使人类在精神上针对 IGF 进行优化。当然，类人猿吃东西是因为它们有饥饿本能，做爱是因为感觉良好，但它们<i>不可能</i>因为这些活动如何导致更多 IGF 而吃东西/通奸。他们还无法执行抽象推理来正确地根据 IGF 来证明这些行为的合理性。然后，当它们开始以人类的方式很好地概括时，可以预见的是，它们不会<i>因为</i>关于 IGF 的抽象推理而<i>突然开始</i>进食/通奸，尽管它们现在<i>可以</i>。相反，他们发明了避孕套，如果你试图消除他们对美食的享受，他们就会与你战斗（告诉他们只需手动计算 IGF）。在功能开始泛化之前您所称赞的对齐属性，可以预见的是无法与功能一起泛化。</p></blockquote><p>雅各布发表了<a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn">《进化解决的对齐》（什么是急速左转？）</a> ，认为人类实际上代表了伟大的对齐<i>成功</i>。进化试图创造出能够自我复制的东西，而人类在这个指标上取得了巨大的成功。引用雅各布的话：</p><blockquote><p>对于人类智能的进化来说，优化器就是进化：生物自然选择。效用函数类似于适应性：前基因复制计数（人类定义基因） <a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn#fn-ZfBacxFa8jjFpbJvN-1"><sup>[1]</sup></a> 。从任何合理的标准来看，人类显然都取得了巨大的成功。如果我们进行标准化，使效用分数 1 代表轻微的成功 - 类人猿物种的典型抽签预期，那么人类的分数要高出 4 OOM 以上，完全超出了图表。 <a href="https://www.lesswrong.com/posts/xCCNdxNPpbJKD3x4W/evolution-solved-alignment-what-sharp-left-turn#fn-ZfBacxFa8jjFpbJvN-2"><sup>[2]</sup></a></p></blockquote><p>我反驳道：</p><blockquote><p>对齐的失败可以从以下事实证明：在给予他们可用的机会的情况下，人类非常非常明显地无法最大化其基因在下一代中的相对频率；他们常常意识到这一点；无论如何，他们经常选择这样做。</p></blockquote><p>我们陷入了混乱的讨论。现在我们在这里进行对话。我希望其他人能够发表评论并澄清相关观点，并且也许不是我的人会在讨论中接替我（如果有兴趣，请给我发消息/评论）。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:22:02 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:22:02 GMT" user-order="1"><p>我将尝试从我的角度总结辩论的状况。<br><br>有两种工艺。<br><br>第一个是我所说的一般进化论。一般进化是这样一个过程，随着时间的推移，任何类型的模式变得更加普遍，最终将占据主导地位。因为某些模式和模式的聚合可以使自己变得更加常见；例如，一个能够自我复制的有机体、一个被复制的基因、一个有毒的模因。这些模式可以“组合起来”，例如有机体中的基因或模因复合体中的模因，并且可以对它们进行调整，以便它们能够更好地使自己变得更常见。因此，我们在周围看到的是模式和模式的聚合，它们非常擅长使自己变得更加普遍。如果我们认为一般进化论具有效用函数，那么它的效用函数就是这样的：应该有一些东西可以复制自己。<br><br>第二种过程我称之为谱系进化。对于今天活着的每个物种 S，都有一个称为“S 进化”的谱系进化，从第一个生命形式，沿着 S 所在的分支，沿着生命的系统发育树，通过 S 的每个祖先物种，直到 S 本身。<br><br> “人”也有两种含义。 “人类”可以指人类个体，也可以指整个人类。<br><br>我读到的原始论点是这样说的：人类进化（谱系进化的一个实例）选择了生物体的 IGF。也就是说，在人类进化的每一步中，人类进化都促进了创造人类（或人类祖先物种生物体）的基因，这些基因擅长使该生物体中的基因在下一代中更加常见。如今，大多数个体人类并不会做出任何明确、偏执地试图推广自己基因的事情。因此，这是一个错位的例子。<br><br>我认为，虽然我还很不确定，但雅各布实际上基本上同意这一点。我想雅各布想说的是<br><br>1. 人性是失调问题的正确主题；<br> 2. 一般进化论是正确的主题；<br> 3. 人类与普遍进化论非常一致，因为人类有很多，而普遍进化论希望有一些模式可以创造出许多人。<br><br>我目前的主要回复是：<br><br>一般进化不是合适的主题，因为设计人类的绝大多数优化能力都是人类进化。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 18:30:08 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 18:30:08 GMT" user-order="1"><p>我认为，如果你写几句话来阐述你的顶级案例，并根据我们迄今为止的背景重新表述，这可能会对我有所帮助。像这样的句子<br><br>“对于（错误）对齐的隐喻，相关层面是人类，而不是个体人类。”<br><br>和类似的。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 18:51:31 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 18:51:31 GMT" user-order="2"><blockquote><p>对齐的失败可以从以下事实证明：人类非常非常明显地无法在下一代中最大化其基因的相对频率（考虑到他们可以获得的机会）</p></blockquote><p>这是无关紧要的——个体失败“在下一代中最大化其基因的相对频率”是大多数物种在个体水平上的预期结果。在许多物种中，只有一小部分个体能够繁殖。对于人类来说，女性的这一比例超过 50%，但男性的比例可能低于 50%。</p><p>进化是通过随机变异和选择进行的——许多实验是并行进行的，只有其中一些会成功——<i>是有设计的</i>。失败是进步所<i>必需的</i>。</p><p>随着时间的推移，进化只是为了适应度而优化——遗传模式的数量/测量，在某些遗传模式集上定义。如果你试图测量一个人的基因，你会得到 IGF——因为该人的基因模式必然会与其他人重叠（与密切相关的亲属密切相关，重叠随着距离的增加而逐渐消失，等等）。同样，您可以测量更大种群直至物种水平的适应性。</p><p>给定具有不同效用函数的两个优化过程，您也许可以将对齐程度测量为两个函数在世界状态（或未来世界轨迹分布的期望）上的点积。<br><br>但我们无法直接测量作为优化器的进化和作为优化器的大脑之间的一致性 - 即使我们知道如何明确定义进化的优化目标（适应度），大脑的优化目标是一些复杂的个体变化的适应度代理- 比任何简单的功能都要复杂得多。此外，对齐程度本身并不是真正有趣的概念，除非标准化到某个相关的尺度（以设置成功/失败的阈值）。</p><p>但鉴于我们知道效用函数之一（进化：适应度），我们可以大致衡量总影响。当今的世界很大程度上是人脑优化的结果——也就是说，它是针对代理效用函数而不是真正的效用函数优化的最终结果。因此，错位只有一个有用的阈值：根据人类适应性的效用函数，当今世界（或最近的历史）效用是高、低还是零？</p><p>答案<i>显然</i>是高实用性。因此，任何偏差的净影响都很小。</p><p>如果 E(W) 是进化效用，B(W) 是大脑效用，我们有：<br><br> W[T] = opt(W[0], B(W))</p><p> E(W[T]) = 大<br><br>（世界是根据大脑（代理效用）优化的，而不是遗传适应度效用，但当前世界根据遗传适应度效用得分非常高，从而限制了任何错位）。</p><p> TechneMarke 认为，大多数产生进化压力的大脑是在物种内水平上，但这与我的论点无关，<i>除非</i>TechneMarke 实际上相信并能够证明这会导致不同的正确进化效用函数（除了适应度）<i>并且</i>根据该功能，人类得分较低。</p><p>打个比方：公司主要是为了利润而优化，而大型企业的大多数创新源于公司内部竞争这一次要事实与公司主要为了利润而优化这一主要事实无关。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 19:02:37 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 19:02:37 GMT" user-order="2"><p>总而言之，进化有几种可能的水平&lt;->;大脑排列：</p><ul><li>物种：大脑的排列（总体）和物种水平的适应性</li><li>个体：个体大脑和个体 IGF 的对齐</li></ul><p>我们似乎都同意个体一致性具有高方差——有些个体与 IGF 强烈一致性，而另一些则根本不一致性。我希望我们同意，在物种层面上，迄今为止，人类已经与适应度保持了良好的一致性——正如我们巨大的异常高的适应度分数所证明的那样（可能是有史以来任何物种适应度增长最快的）。</p><p>所以对于这样的声明：</p><blockquote><p>这里的核心类比是，优化猿类以实现包容性遗传适应性 (IGF) 并不会使人类在精神上针对 IGF 进行优化。</p></blockquote><p>如果你将“人类”理解为个体人类，那么这个陈述是正确的，但无趣（因为进化不会也不可能使每个个体都获得高分）。如果你把人类理解为人类，那么进化显然（对我来说）成功地使人类充分对齐，但在（心理优化）到底意味着什么的细节上仍然可能存在分歧，这将导致关于计算机的计算极限的侧面讨论。 20W 计算机以及如何间接优化代理是生产能够最大化预期 IGF 的计算机的最佳解决方案，而不是一些无法扩展且严重失败的简单的最大效用结果主义推理机。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 19:23:12 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 19:23:12 GMT" user-order="1"><blockquote><p>进化的不同正确效用函数（除了适应度） <i>，并且</i>根据该函数，人类得分较低。</p></blockquote><p>嗯。我认为这里的框架可能掩盖了我们的分歧。我想说：人类是通过多次迭代选择 IGF 的过程构建的。现在，人类正在对事物进行优化，但他们绝对没有针对 IGF 进行优化。<br><br>我认为你所说的是一般类别的内容：“当然，但你在这里使用的概念不是联合雕刻。如果你通过进化来看待人类的创造，然后你会想到非-联合雕刻概念，然后你会看到错位。但这只是实际设计过程的非联合雕刻子集和所设计的东西的非联合雕刻子集之间的错位。”<br><br>我想……好吧，但这个类比似乎仍然成立？<br><br>就像，如果我想到人类试图创造人工智能，我不觉得我是在谈论“所有试图创造人工智能的人类的效用函数”。我想我想谈论的是“人类试图创造人工智能的标准，或者用于梯度下降或强化学习的目标函数，具体地使用日常来选择他们的设计中要保留的调整/想法大约”。因此，这里有两个类比，但形式相同。<br><br>其中一个类比是：人类四处寻找人工智能的想法；如果他们的人工智能做了一些很酷的事情，他们就会投票；如果可以的话，他们会尝试调整人工智能来做一些很酷的事情，以便它可以用来真正做一些对人类有用的事情；当人工智能做了一些明显不好的事情时，人类会尝试修补它。人类可能认为自己通过自己的行为实现了自己的效用函数，更重要的是，他们实际上可能在某种意义上这样做了。如果人类可以继续修补不好的结果并支持很酷的结果并安装用于良好的补丁，那么在这种情况下，真正的人类效用函数将得到表达和实现。但这里的类比是说：人类用来设计人工智能的这些标准，进行调整，在人类人工智能研究的过程中，这些标准将加起来形成真正强大的人工智能，可以制造出真正强大的人工智能，而无需在人工智能中安装限制真实的人类效用函数。同样，如果有足够的进化时间，进化将在人类身上安装更多战略性的 IGF 优化；这可能已经发生了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Wed, 18 Oct 2023 19:32:04 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Wed, 18 Oct 2023 19:32:04 GMT" user-order="1"><p>换句话说，对于我来说这个类比似乎是正确和重要的，它是效用函数与效用函数错位的明显例子，这并不让人感觉很棘手。感觉棘手的是一种不太精确的感觉：这个过程通过为 X 进行非常困难的选择来设计优化器，但优化器最终尝试执行与 X 不同的 Y。<br><br>就像，如果我通过模仿人类的目标函数来训练人工智能，我希望最终，当它变得非常出色时，它将成为一个优化器，可以针对模仿人类以外的其他事物进行强大的优化。<br><br>对我们来说，症结可能与我们对未来预期结果的关心程度有关。我想你在某些评论中说过，“是的，也许人类/人类未来会与进化更加不一致，但那是猜测和循环推理，它还没有发生”。但我不认为这是循环的：我们<i>今天</i>可以清楚地看到人类<i>正在</i>针对事物进行优化，并说他们正在针对事物进行优化，而这些事物<i>不是</i>IGF，因此可以预见的<i>是</i>，当人类拥有力量时，我们将结束IGF 得分非常<i>低</i>；如今，这种失调更加模糊，必须根据反事实进行判断（<i>如果</i>现代人是 IGF 最大化者，他们<i>可以获得</i>多少 IGF）。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N2R9wMRJd7SBSjpiT-Wed, 18 Oct 2023 20:15:13 GMT" user-id="N2R9wMRJd7SBSjpiT" display-name="jacob_cannell" submitted-date="Wed, 18 Oct 2023 20:15:13 GMT" user-order="2"><blockquote><p>现在，人类正在对事物进行优化，但他们绝对没有针对 IGF 进行优化。</p></blockquote><p>我们可能对此仍存在分歧 - 我要重申，在个人层面上，有些人肯定会针对 IGF 进行强烈优化，直至 20W 物理计算机的限制（这排除了大多数基于对优化的物理限制的严重误解的反对意见） 20W不可逆计算机的功率）。我已经在我们的私人讨论中提出了一个具体的例子，即个人会付出巨大的努力来最大限度地成功捐献精子，即使他们的报酬微不足道，或者根本没有报酬，在某些情况下实际上犯下了长期监禁的重罪。这样做（强烈反付费）。此外，大脑<i>通常的</i>工作方式更接近于神秘地潜意识地迫使你针对 IGF 进行优化——以埃隆·马斯克为例：他正朝着非常高的 IGF 分数前进，但似乎并没有明确有意识地为此进行优化。大脑中的排列非常复杂，显然还没有完全理解——但它是高度冗余的，有许多级别的机制在发挥作用。</p><p>因此，潜在的困惑之一是，我确实相信，正确理解和确定“优化 IGF，达到 20W 物理计算机的极限”实际上需要深入了解 DL 和神经科学，并导致诸如<a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">分片理论</a>之类的东西。 Nate 似乎暗示那种结果主义优化器在 20W 时惨败，并且与成功的设计（如大脑）的外观没有太大关系 - 它始终是一个超复杂的代理优化器，ala 分片理论和相关。<br><br>完美的对齐是一个神话，一个幻想 - 显然对于成功来说是不必要的！ （这就是这个类比的大部分教训）</p><blockquote><p>就像，如果我想到人类试图创造人工智能，我不觉得我是在谈论“所有试图创造人工智能的人类的效用函数”。我想我想谈论的是“人类试图创造人工智能的标准，或者用于梯度下降或强化学习的目标函数，具体地使用日常来选择他们的设计中要保留的调整/想法大约”。</p></blockquote><p>我确实相信，在进化中可能的对齐类比中，有一个最好的类比：系统级类比。</p><p>基因进化优化产生大脑就像技术进化优化产生AGI一样。</p><p>这两个过程都涉及双层优化：外部遗传（或模因）进化优化过程和内部人工神经网络优化过程。实用的 AGI 必然非常像大脑（我提前很多年就正确<a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine) correctly many years in advance, contra EY/MIRI)">预测了</a>这一点，与 EY/MIRI 相反）。在所有重要方面，深度学习正在与类脑设计紧密结合。</p><p>外部进化优化过程类似，但有一些关键差异。基因组指定初始架构先验（权重上的紧凑低位和低频编码）以及用于更新这些权重的高效近似贝叶斯学习算法。同样，人工智能系统由一个小型紧凑代码（pytorch、tensorflow 等）指定，该代码指定初始架构先验以及用于更新这些权重（SGD）的高效近似贝叶斯学习算法。主要区别在于，对于技术进化而言，编码单元（技术模因）的重组比基因更加灵活。每个新实验都可以灵活地组合来自大量先前论文/实验的模因，这是一个由人类智能引导的过程（内部优化）。主要效果只是巨大的加速——与先进的基因工程相似但更极端。</p><p>我认为这确实是最有信息性的类比。从这个类比中，我想我们可以这样说：<br><br>在某种程度上，AGI 的技术进化与人类智能（大脑）的基因进化相似，到目前为止，基因进化在调整人类方面（总体上，而不是单独）取得的巨大成功意味着，调整 AGI 的技术进化也取得了类似的成功。总体而言，而不是单独）达到类似的非平凡水平的优化功率发散。</p><p>如果你认为第一个跨越某个能力阈值的通用人工智能可能会突然接管世界，那么物种水平对齐的类比就站不住脚了，厄运更有可能发生。这就像一个中世纪时代的人类突然通过强大的魔法统治了世界。根据单个人的愿望进行优化后的结果世界在 IGF 上是否仍能获得相当高的分数？我想说概率在 90% 到 50% 之间，但这显然仍然是一个高 p(doom) 场景。我确实认为这种情况不太可能发生，原因有很多（简而言之，允许人类工程师选择成功的模因变化的因素远远高于机会，同样的因素也充当了一个隐藏的伟大过滤器，减少了技术设计中的失败方差），但这是一个我已经在其他地方进行了部分争论。</p><p>因此，进化成功地协调人类的一个关键因素是大量人口的方差减少效应，这直接映射到多极情景。群体几乎总是比最坏情况甚至中位数个体更加对齐，并且即使几乎每个个体都几乎完全错位（正交），群体也可以完美对齐。方差减少对于大多数成功的优化算法（包括 SGD 和进化）至关重要。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">雅各布·坎内尔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:14:10 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:14:10 GMT" user-order="1"><p> （此时我将把对话公开，只要看起来不错，我们就可以继续。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:21:27 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:21:27 GMT" user-order="1"><blockquote><p> TM：对齐的失败可以从以下事实看出：人类非常非常明显地无法在下一代中最大限度地提高其基因的相对频率（考虑到他们可以获得的机会）</p></blockquote><blockquote><p> J：这是无关紧要的——对于大多数物种来说，“在下一代中最大化其基因的相对频率”的个体失败是个体水平上的预期结果。在许多物种中，只有一小部分个体能够繁殖。</p></blockquote><p>这里重要的是“可以”是什么。如果一个人不繁殖，它<i>还能</i>繁殖吗？具体来说，如果它只是<i>试图繁殖</i>，它是否显然会繁殖更多？在很多情况下，这很难以高置信度进行分析，但据称，许多人的答案是“是的，显然”。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:25:14 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:25:14 GMT" user-order="1"><p>我也许应该阐述更多“人类没有尝试 IGF”的案例。<br><br> 1. 男性对捐献精子非常感兴趣的情况很少。<br> 2. 很多人在发生性行为时故意避免怀孕，尽管他们完全可以抚养孩子。<br> 3. 我和我想象中的其他人，对于人类最终仅由我的复制品组成的想法感到厌恶，而不是渴望。<br> 4. 我和我想其他人都在积极希望并密谋结束 DNA 拷贝增加的制度。<br><br>我认为你认为最后两个是弱的甚至是循环的。但对我来说这似乎是错误的，它们似乎是很好的证据。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:28:49 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:28:49 GMT" user-order="1"><p>人类作为一个整体也并没有试图增加 DNA 拷贝。<br><br>也许这里有趣的一点是，你不能算作对齐成功*中间收敛工具*成功。人类出于某种原因创造技术；技术就是力量；由于人类因此变得更加强大，因此暂时会有更多的 DNA 拷贝。要了解人类/人类想要什么，你必须看看人类/人类在不受工具性目标约束时会做什么。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 03:34:51 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 03:34:51 GMT" user-order="1"><blockquote><p>我们似乎都同意个体一致性具有高方差——有些个体与 IGF 强烈一致性，而另一些则根本不一致性。</p></blockquote><p>很少。超级精子捐赠者大多/可能算数。克莱恩，刑事医生，主要/可能很重要。那些决定要生十几个孩子的女性（如果她们没有被强迫的话）大部分/可能会算数。成吉思汗似乎是最著名的候选人（这里的反感说明了我们真正关心的是什么）。<br><br>埃隆·马斯克不算在内。你说得对，他就像多产的国王等人一样，是人类追踪后代数量的证据。但埃隆显然并没有为此努力优化。如果他真的尝试的话，他能捐献多少精子？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:03:41 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:03:41 GMT" user-order="1"><p>我从这次讨论中得到了重大更新。不过，更新并没有真正偏离我的立场或朝向你的立场——嗯，它是朝向你的部分立场。它更像是如下：<br><br>以前，我会隐约同意将进化-人类转变描述为“人类与进化的效用函数不一致”的例子。我回顾了我对你的帖子的评论，我发现我并没有谈论进化具有效用函数，只是通过说“这不是进化的效用函数”来否定你的说法。相反，我会说“进化寻找……”或“进化促进……”。然而，我并不反对其他人说进化具有效用函数，而且我绝对认为人类与进化<i>不一致</i>。<br><br>现在，我认为你是对的，说人类与进化不一致是没有意义的！但原因和你不一样。相反，我现在认为说进化（任何形式）具有效用函数是没有多大意义的。进化不是这样的。它不是一个战略性的、通用的优化器。 （它有一定的普遍性，但它是有限的，而且不是战略性的；它在设计生物体（或者，如果你坚持的话，物种）时没有提前计划。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:08:00 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:08:00 GMT" user-order="1"><p>我之前的大部分评论，例如对你的帖子的评论，仍然有效，但经过更正，我现在不会说这是一种<i>错位。</i>我不知道这个词是什么意思，但它是不同的东西。就是你有一个流程，对 X 做出非常强烈的选择，并做出一个对世界进行科学、做出复杂的设计和计划，然后实现非常困难的酷目标的东西，又称战略通用优化器；但通用优化器不会针对 X 进行优化（由于<i>收敛</i>，为了成为一个好的优化器，它必须做的更多）。相反，优化器针对 Y 进行优化，在选择过程运行的区域中，看起来 X / 是 X 的良好代理，但在该区域之外，针对 Y 进行优化的优化器会践踏 X。<br><br>这个词用什么词来形容呢？正如您所指出的，这并不总是错位，因为选择过程不必具有效用函数！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="rbtKwJ8vDJoiv7oAL-Thu, 19 Oct 2023 04:11:58 GMT" user-id="rbtKwJ8vDJoiv7oAL" display-name="TekhneMakre" submitted-date="Thu, 19 Oct 2023 04:11:58 GMT" user-order="1"><p>辩论的结果是最初的论点仍然成立，但以更好的形式表达。人类的进化并不完全是错位，但它是另一回事。 （这是术语“内部（错误）对齐”的意思吗？或者内部对齐是否假设外部事物是效用函数？）<br><br>据称，这另一件事也将发生在人类/人类/人工智能的训练过程中。人类/人类/训练过程使用的选择标准不一定是人工智能最终优化的标准。进化与人类的转变就是证明。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">技术</section></section><br/><br/><a href="https://www.lesswrong.com/posts/xqXdDs68zMJ82Dcmt/are-humans-misaligned-with-evolution#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xqXdDs68zMJ82Dcmt/are- humans-misaligned-with-evolution<guid ispermalink="false"> xqXdDs68zMJ82Dcmt</guid><dc:creator><![CDATA[TekhneMakre]]></dc:creator><pubDate> Thu, 19 Oct 2023 03:14:14 GMT</pubDate> </item><item><title><![CDATA[The (partial) fallacy of dumb superintelligence]]></title><description><![CDATA[Published on October 18, 2023 9:25 PM GMT<br/><br/><p>在<a href="https://www.youtube.com/watch?v=skRgYH7oAjc"><u>蒙克关于人工智能风险的辩论的开场发言中，</u></a>梅兰妮·米切尔提到了通用人工智能风险的一个拟议例子：负责解决气候变化的通用人工智能可能会决定消除人类作为碳排放源的可能性。她说：</p><blockquote><p>这是所谓的“愚蠢的超级智能谬误”的一个例子。 <span class="footnote-reference" role="doc-noteref" id="fnrefs793uam9h9i"><sup><a href="#fns793uam9h9i">[1]</a></sup></span>也就是说，认为机器可以“在各方面都比人类聪明”是一个谬论，但仍然缺乏对人类的常识性理解，例如理解我们为什么提出解决气候变化的要求。</p></blockquote><p>我认为这种“谬误”是关于人工智能 x 风险的分歧的症结所在，因为对齐难度较大。我从其他消息灵通的风险怀疑者那里听到过这样的说法。直觉是有道理的。但大多数持一致态度的人都会立即驳斥这一点，认为这本身就是一个谬论。理解这两种立场不仅可以澄清讨论，还可以说明我们忽视一种有前途的协调方法的原因。</p><p>这种“谬误”并不能证明对齐是容易的。理解你的意思并不会让 AGI 想做那件事。行动以目标为指导，这与知识不同。但这种理解应该有助于协调的直觉不必被完全抛弃。我们现在提出了利用人工智能理解来进行对齐的对齐方法。他们通过将激励系统“指向”所学知识系统中的表征（例如“人类繁荣”）来做到这一点。我讨论了两个使用这种方法的调整计划，看起来很有希望。</p><p>早期的对齐思维认为这种类型的方法不可行，因为 AGI 可能<a href="https://www.lesswrong.com/tag/ai-takeoff">会</a>“快速且不可预测地学习”。对于深度网络中低于人类水平的训练，这一假设似乎并不成立，但这可能足以进行初始对齐。</p><p>对于一个能够进行不可预测的快速改进的系统，在调整它之前让它学习是疯狂的。在你有机会阻止它学习执行对齐之前，它很可能会变得足够聪明来逃脱。因此，在开始学习之前必须指定其目标（或一组塑造目标的奖励）。在这种情况下，我们指定目标的方式无法利用人工智能的智能。米切尔的“谬误”本身就是这种逻辑下的谬误。理解我们想要什么的通用人工智能可以轻松地做我们非常不想要的事情。</p><p>但现在似乎不太可能出现早期繁荣，因此我们的想法应该调整。深度网络的能力不会不可预测地增加，至少在人类水平和递归自我改进之前是这样。这可能足以让初步调整取得成功。早期关于通用人工智能“蓬勃发展”的假设现在看来不太可能成立。我认为早期的假设在我们的集体思维中留下了一个错误：通用人工智能的知识与让它做我们想做的事情无关。 <span class="footnote-reference" role="doc-noteref" id="fnrefg637hbyuzwv"><sup><a href="#fng637hbyuzwv">[2]</a></sup></span></p><h2><strong>如何安全地利用人工智能的理解来进行对齐</strong></h2><p>在当前的训练体系中，深度网络以相对可预测的速度学习。因此，他们的训练可以暂停在中间水平，包括对人类价值观的一些理解，但在他们获得超人能力之前。一旦系统开始反思和指导自己的学习，这种平稳的轨迹可能就不会持续下去。但是，如果我们仔细谨慎地设定该水平，我们可能可以停留在安全但有用的智力/理解水平上。我们或许可以在训练过程中调整 AGI，从而利用它对我们想要的东西的理解。</p><p>此类方法有三个特别相关的示例。第一个是<a href="https://www.lesswrong.com/tag/rlhf"><u>RLHF</u></a> ，它是相关的，因为它被广泛了解和理解。 （我<a href="https://www.lesswrong.com/posts/d6DvuCKH5bSoT62DB/compendium-of-problems-with-rlhf"><u>等人</u></a>并不认为它本身是一种有前途的对齐方法。）RLHF 使用法学硕士训练有素的“理解”或“知识”作为有效指定人类偏好的基础。对有限的一组关于输入-响应对的人类判断进行训练使法学硕士能够很好地概括这些偏好。我们正在“指向”其学习语义空间中的区域。因为这些语义的格式相对良好，所以我们需要做相对较少的指示来定义一组复杂的所需响应。</p><p>第二个例子是语言模型代理（LMA）的自然语言对齐。如果 LMA 成为我们的第一个 AGI，这似乎是一个非常有前途的调整计划。该计划包括设计代理以遵循自然语言中表述的顶级目标（例如，“让 OpenAI 获得大量资金和政治影响力”），包括调整目标（例如，“做 Sam Altman 想要的事情，让世界成为一个更好的地方”。）我已经写了更多关于这项技术以及它可以“堆叠”的技术组合， <a href="https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent"><u>在这里</u></a>。</p><p>该方法遵循上述一般方案。它通过预训练 LLM 并在作为代理启动系统之前插入对齐目标来暂停训练以进行对齐工作。 （这是训练中期，如果该代理继续执行持续学习，这似乎是可能的。）如果人工智能足够智能，它将追求所述目标，包括其丰富的上下文语义。明智地选择这些目标陈述仍然是一个重要的外部对齐问题；但人工智能的知识是定义其一致性的基础。</p><p>遵循这种一般模式的另一个有前途的对齐计划是 Steve Byrnes 的<a href="https://www.alignmentforum.org/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>类脑 [基于模型的 RL] AGI 的平庸对齐计划</u></a>。在这个计划中，我们诱导新生的 AGI（暂停在有用但可控的理解/智能水平）来代表我们希望它符合的概念（例如，“思考人类繁荣”或“可纠正性”或其他什么）。然后，我们将其表征系统中的活跃单元的权重设置到其批评系统中。由于批评系统是一个<a href="https://www.lesswrong.com/posts/qzu9o3sTytbC4sZkQ/steering-subsystems-capabilities-agency-and-alignment"><u>控制子系统</u></a>，决定其值并因此决定其行为，因此解决了内部对齐问题。该概念已成为其“最喜欢的”、价值最高的表示集，并且其决策将追求该概念中语义上包含的所有内容作为最终目标。</p><p>现在，将这些技术与不利用系统知识的对齐技术进行对比。<a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><u>碎片理论</u></a>和其他通过使用正确的奖励来调整 AGI 的建议就是一个例子。这需要准确猜测系统的表征将如何形成，以及这些奖励将如何塑造智能体在发展过程中的行为。手动编码除最简单目标之外的任何目标（请参阅<a href="https://arbital.com/p/diamond_maximizer/"><u>钻石最大化</u></a>）的表示似乎非常困难，以至于通常不被认为是可行的方法。</p><p>这些是需要进一步开发和检查缺陷的计划草图。在训练分布中，它们只产生与人类价值观的最初的、松散的（“平庸的”）一致性。泛化和值变化的<a href="https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem"><u>对齐稳定性问题</u></a>仍未得到解决。在进一步学习、自我修改或在新的（分布外）情况下对齐是否仍然令人满意似乎是一个值得进一步分析的复杂问题。</p><p>这种利用人工智能的智能并通过指向其表征来“告诉它我们想要什么”的方法似乎很有前途。这两个计划似乎特别有希望。它们适用于我们可能获得的 AGI 类型（语言模型智能体、强化学习智能体或混合体）；它们足够简单，易于实施，并且足够简单，可以在实施之前进行详细考虑。</p><p>我很想听到关于这个方向的具体阻力，或者更好的是这些具体计划。人工智能工作似乎可能会快速进行，因此协调工作也应该迅速进行。我认为我们需要制定和批评最好的计划，将其应用于我们最有可能获得的通用人工智能类型，即使这些计划并不完美。 <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fns793uam9h9i"> <span class="footnote-back-link"><sup><strong><a href="#fnrefs793uam9h9i">^</a></strong></sup></span><div class="footnote-content"><p> Richard Loosemore 似乎是在 2012 年或更早创造了这个术语。他<a href="https://richardloosemore.com/2015/05/05/debunking-fallacies-in-the-theory-of-ai-motivation/"><u>在这里</u></a>阐述了这个论点，并得出了与这里类似的结论：<a href="https://arbital.com/p/dwim/"><u>按照我的意思去做</u></a>并不是自动的，但编写一个 AGI 来推断意图并在可能被侵犯时与它的创建者进行检查也不是特别难以置信。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng637hbyuzwv"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg637hbyuzwv">^</a></strong></sup></span><div class="footnote-content"><p>请参阅最近的帖子<a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument"><u>评估历史价值错误指定的论点</u></a>。它扩展了这些想法的历史背景，特别是我们应该根据对人类价值观有相当好的理解的人工智能来调整我们对对齐难度的估计的主张。我不在乎谁在何时思考什么，但我确实关心那里审查的集体思路可能会稍微误导我们。该帖子的讨论在一定程度上澄清了这些问题。这篇文章旨在为该讨论中提出的一个核心问题提供更具体的答案：我们如何缩小人工智能理解我们的愿望与根据这种理解做出决策来实际实现它们之间的差距。我还建议，与历史假设相比的关键变化是学习的可预测性，因此可以选择在部分训练的系统上安全地执行对齐工作。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence<guid ispermalink="false"> qsDPHZwjmduSMCJLv</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Wed, 18 Oct 2023 21:25:17 GMT</pubDate> </item><item><title><![CDATA[Does AI governance needs a "Federalist papers" debate?]]></title><description><![CDATA[Published on October 18, 2023 9:08 PM GMT<br/><br/><p>美国独立战争期间，需要联邦军队和政府来对抗英国。许多人担心为此目的而授予政府的权力会使其在未来变得专制。</p><p>如果国父们决定忽视这些担忧，美国就不会像今天这样存在。相反，他们与最优秀、最聪明的反联邦党人合作，建立一个更好的机构，拥有更好的机制和有限的权力，这使他们能够获得宪法所需的支持。</p><p>当今关于人工智能监管是否存在类似联邦主义者与反联邦主义者的辩论？是否有人致力于创建一个新的机构，以更好的机制来限制他们的权力，从而向另一方保证它不会被用来走上极权主义的道路？如果没有，我们应该开始吗？</p><br/><br/> <a href="https://www.lesswrong.com/posts/YkrDKR7TyqDxwKj23/does-ai-governance-needs-a-federalist-papers-debate#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YkrDKR7TyqDxwKj23/does-ai-governance-needs-a-federalist-papers-debate<guid ispermalink="false"> YkrDKR7TyqDxwKj23</guid><dc:creator><![CDATA[azsantosk]]></dc:creator><pubDate> Wed, 18 Oct 2023 21:08:26 GMT</pubDate> </item><item><title><![CDATA[Metaculus Launches Conditional Cup to Explore Linked Forecasts]]></title><description><![CDATA[Published on October 18, 2023 8:41 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/9kEFRE7Lkp5mXFRca/metaculus-launches-conditional-cup-to-explore-linked#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9kEFRE7Lkp5mXFRca/metaculus-launches-conditional-cup-to-explore-linked<guid ispermalink="false"> 9kEFRE7Lkp5mXFRca</guid><dc:creator><![CDATA[ChristianWilliams]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:41:41 GMT</pubDate> </item><item><title><![CDATA[Alignment 101 - Ch.2 - Reward Misspecification]]></title><description><![CDATA[Published on October 18, 2023 8:39 PM GMT<br/><br/><h1>概述</h1><ol><li><strong>强化学习</strong>：本章首先提醒一些强化学习概念。这包括快速深入了解奖励和奖励函数的概念。本节为解释为什么奖励设计极其重要奠定了基础。</li><li><strong>最优化</strong>：本节简要介绍古德哈特定律的概念。它提供了一些动机来理解为什么奖励很难以某种方式指定，以便它们在面对巨大的优化压力时不会崩溃。</li><li><strong>奖励错误指定</strong>：通过牢牢掌握奖励和优化的概念，读者将面临对齐的核心挑战之一 - 奖励错误指定。这也称为外部对齐问题。本节首先讨论除了算法设计之外良好的奖励设计的必要性。接下来是奖励规范失败的具体示例，例如奖励黑客和奖励篡改。</li><li><strong>通过模仿学习</strong>：本节重点介绍一些针对奖励错误指定的建议解决方案，这些解决方案依赖于通过模仿人类行为来学习奖励功能。它研究了模仿学习（IL）、行为克隆（BC）和逆向强化学习（IRL）等建议。每个部分还包含对这些方法在解决奖励黑客方面可能存在的问题和局限性的检查。</li><li><strong>通过反馈学习</strong>：最后一部分研究了旨在通过向机器学习模型提供反馈来纠正奖励错误指定的提案。本节还全面介绍了当前大型语言模型 (LLM) 的训练方式。讨论内容涵盖奖励建模、人类反馈强化学习 (RLHF)、人工智能反馈强化学习 (RLAIF) 以及这些方法的局限性。</li></ol><h1> 1.0：强化学习</h1><p>本节简要提醒强化学习 (RL) 中的几个概念。它还消除了各种经常混淆的术语的歧义，例如奖励、价值和效用。本节最后讨论了区分强化学习系统可能追求的目标概念和它所获得的奖励的概念。已经熟悉基础知识的读者可以直接跳至第 2 部分。</p><h2> 1.1.底漆</h2><p><i>强化学习（RL）专注于开发能够从交互体验中学习的智能体。强化学习的概念是，智能体通过与环境的交互进行学习，并根据每次行动后通过奖励收到的反馈来改变其行为。</i></p><p>强化学习的一些实际应用示例包括：</p><ul><li><strong>机器人系统</strong>：强化学习已应用于实时控制物理机器人等任务，并使它们能够学习更复杂的动作（OpenAI 2018“<a href="https://www.youtube.com/watch?v=jwSbzNHGflM"><u>学习敏捷性</u></a>”）。强化学习可以使机器人系统学习复杂的任务并适应不断变化的环境。</li><li><strong>推荐系统</strong>：强化学习可应用于推荐系统，该系统与数十亿用户交互，旨在提供个性化推荐。强化学习算法可以根据用户反馈学习优化推荐策略，改善整体用户体验。</li><li><strong>游戏系统：</strong> 2010 年代初期，强化学习 基于强化学习的系统开始在一些非常简单的 Atari 游戏（例如 Pong 和 Breakout）中击败人类。多年来，已经有许多模型利用强化学习在棋盘游戏和视频游戏中击败了世界大师。其中包括<a href="https://www.deepmind.com/research/highlighted-research/alphago"><u>AlphaGo</u></a> (2016)、 <a href="https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go"><u>AlphaZero</u></a> (2018)、 <a href="https://openai.com/research/openai-five-defeats-dota-2-world-champions"><u>OpenAI Five</u></a> (2019)、 <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii"><u>AlphaStar</u></a> (2019)、 <a href="https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules"><u>MuZero</u></a> (2020) 和<a href="https://github.com/YeWR/EfficientZero"><u>EfficientZero</u></a> (2021) 等模型。</li></ul><p>强化学习与监督学习不同，因为它从“做什么”的高级描述开始，但允许代理进行实验并从经验中学习最好的“如何做”。在强化学习中，智能体通过与环境的交互来学习，并根据其行为以奖励或惩罚的形式接收反馈。强化学习专注于学习一组规则，这些规则推荐在给定状态下采取的最佳行动，以最大化长期奖励。相反，监督学习通常涉及从明确提供的标签或每个输入的正确答案中进行学习。</p><h2> 1.2.核心循环</h2><p>强化学习的整体功能相对简单。两个主要组成部分是代理本身以及代理生活和运行的环境。在每个时间步 t：</p><ul><li>然后代理采取<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">一些</span></span></span><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">行动</span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></li><li>环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">状态</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">st</span></span></span></span></span></span></span></span></span>根据动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t 的</span></span></span></span></span></span></span></span></span>变化而变化。</li><li>然后环境输出观察<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="o_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span>和奖励<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="r_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></li></ul><p>历史是在时间 t 之前所采取的过去观察、行动和奖励的序列<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h_t = (a_1,o_1,r_1, \ldots,a_t,o_t,r_t)"><span class="mjx-mrow" aria-hidden="true">： <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>世界状态通常是历史的某个函数：<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t = f(h_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>世界状态是世界的完整真实状态，用于确定世界如何生成下一个观察和奖励。代理可能会获得整个世界状态作为观察<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="o_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span></span></span></span></span></span></span>或某些部分子集。</p><p>这个词从一个状态 st 到下一个<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_{t+1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">状态</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span></span></span>要么基于自然环境动态，要么基于代理的行为。状态转换可以是确定性的，也可以是随机的。此循环将持续下去，直到达到终止条件或可以无限期地运行。下图简洁地描述了 RL 过程： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/ddt9ybvvg4nmpmqrwkfi"></p><p>资料来源：Emma Brunskill（2022 年冬季）“<a href="https://web.stanford.edu/class/cs234/CS234Win2022/modules.html"><u>斯坦福 CS234：RL</u></a> - 第 1 讲”</p><h2> 1.3: 政策</h2><p><i>策略可帮助代理确定收到观察结果后应采取的操作。它是从状态到操作的函数映射，指定在每个状态下采取什么操作。策略可以是确定性的，也可以是随机的。</i></p><p>强化学习的目标是学习一种<u>策略</u>（通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>表示），该策略建议在任何给定时刻采取的最佳行动，以便随着时间的推移最大化总累积奖励。该策略定义了从状态到行动的映射，并指导代理的决策过程。</p> <span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\pi:S \rightarrow A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">:</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span>策略可以是确定性的，也可以是随机的。确定性策略直接将每个状态<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="s_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span>映射到特定动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span> ，通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mu"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span></span></span></span></span></span>表示。相反，随机策略为每个状态的操作分配概率分布。随机策略通常用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>表示。</p><p>确定性策略： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t =&nbsp;\mu(s_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>随机策略： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi(a|s) = P(a_t=a|s_t=s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>在深度强化学习中，策略是在训练过程中学习的功能图。它们取决于神经网络的学习参数集（例如权重和偏差）。这些参数通常使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\theta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span></span></span></span></span>在策略方程上用下标表示。因此，神经网络参数的确定性策略可写<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a_t =&nbsp;\mu_{\theta}(s_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em;">μ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> 。</p><p> An optimal policy maximizes the expected cumulative reward over time.代理从经验中学习，并根据从环境中以奖励或惩罚的形式收到的反馈来调整其策略。</p><p>为了确定一个动作是否比另一个更好，需要以某种方式评估动作（或状态动作对）。有两种不同的方法来考虑采取哪种行动 - 即时奖励（由奖励函数确定）和长期累积奖励（由价值函数确定）。这两者都极大地影响了智能体学习的策略类型，从而也影响了智能体采取的行动。以下部分更深入地探讨和阐明奖励的概念。</p><h2> 1.4：奖励</h2><p><i>奖励是指用于指导学习过程和优化模型行为的任何信号或反馈机制。</i></p><p>来自环境的奖励信号是一个告诉智能体当前世界状态有多好或多坏的数字。它是一种为模型的输出或操作提供性能评估或衡量的方法。奖励可以根据特定任务或目标来定义，例如最大化游戏中的分数或在现实场景中实现期望的结果。强化学习的训练过程涉及优化模型参数以最大化预期奖励。该模型学习生成更有可能获得更高奖励的行动或输出，从而随着时间的推移提高绩效。奖励从哪里来？它是通过奖励函数生成的。</p><p><i>奖励函数定义了强化学习问题的目的或目标。 It maps perceived states or state-action pairs of the environment to a single number.</i></p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R : (S \times A) \rightarrow \mathbb{R};&nbsp;r_t = R(s_t,a_t)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">：</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">）</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">；</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>奖励函数向代理提供即时反馈，指示特定状态或动作的好坏。它是一个数学函数，将代理环境的状态-动作对映射到标量值，表示处于该状态并采取该动作的意愿。它向代理提供即时反馈的衡量标准，表明其在每个步骤的执行情况。</p><p><i><u>奖励函数与价值函数</u></i></p><p><u>奖励</u>表示状态或行动的直接期望，而价值函数代表状态的长期期望，考虑到未来的奖励和状态。如果您从状态或状态-操作对开始，然后永远根据特定策略采取行动，则该值是预期回报。</p><p>选择价值函数有许多不同的方法。它们也可以随着时间的推移而打折，即未来的奖励的价值会因某个因素<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma \in (0,1)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">ε</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span>而</span></span></span></span></span>减少。</p><p>以下是一个简单的公式，是给定某种政策的未来奖励的贴现总和。累计折扣奖励由以下公式给出：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R =&nbsp;r_t + \gamma r_{t+1}+ \gamma^{2} r_{t+2}+ \ldots = \sum_{t=0}^{\infty}{\gamma^{t}r_t}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.025em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.117em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-stack" style="vertical-align: -0.31em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.422em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">∞</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> <span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.025em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.117em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span></span></span></span></span></p><p>根据该政策采取行动的价值由下式给出：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V^{\pi}(s_t=s) = \mathbb{E}(R|s_t=s)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.186em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.413em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p><i><u>奖励函数与效用函数</u></i></p><p>还值得将效用的概念与奖励和价值区分开来。奖励函数通常用于强化学习环境中，以指导智能体的学习过程和行为。相比之下，效用函数更加通用，可以捕获代理人的主观偏好或满意度，从而允许在不同的世界状态之间进行比较和权衡。效用函数是一个在决策理论和代理基础工作领域使用较多的概念。</p><h1> 2.0：优化</h1><p>了解优化对于理解人工智能安全问题非常重要，因为它在机器学习中发挥着核心作用。人工智能系统，特别是基于深度学习的系统，使用优化算法进行训练，以从数据中学习模式和关联。 These algorithms update the model&#39;s parameters to minimize a loss function, maximizing its performance on the given task.<br><br> Optimization amplifies certain behaviors or outcomes, even if they were initially unlikely.例如，优化器可以搜索可能输出的空间，并根据目标函数采取具有高分的极端操作，这可能会导致意外和不良行为。其中包括奖励错误指定失败。更好地认识优化放大某些结果的力量可能有助于设计真正符合人类价值观和目标的系统和算法，即使在优化的压力下也是如此。这涉及确保优化过程与系统设计者的预期目标和价值观保持一致。它还需要考虑优化过程中可能出现的潜在故障模式和意外后果。</p><p>优化带来的风险在人工智能安全中无处不在。本章仅对其进行了简要介绍，但将在目标错误概括和代理基础章节中进行更详细的讨论。</p><p>优化能力在奖励黑客中起着至关重要的作用。当强化学习代理利用真实奖励和代理奖励之间的差异时，奖励黑客就会发生。优化能力的提高可能会导致奖励黑客行为的可能性更高。在某些情况下，存在一些阶段转变，其中优化能力的适度增加会导致奖励黑客的急剧增加。</p><h2> 2.1: 古德哈特定律</h2><p>“<i>当一项措施成为目标时，它就不再是一个好的措施。</i> ”</p><p>这个概念最初源于查尔斯·古德哈特（Charles Goodhart）的经济理论著作。然而，它已成为包括当今人工智能对齐在内的许多不同领域的主要挑战之一。</p><p>为了说明这个概念，以下是一个苏联制钉厂的故事。 The factory received instructions to produce as many nails as possible, with rewards for high output and penalties for low output.几年之内，工厂显着增加了钉子的产量——本质上是图钉的小钉子，事实证明无法达到其预期目的。因此，规划者改变了激励措施：他们决定根据生产的钉子的总重量奖励工厂。几年之内，工厂开始生产又大又重的钉子（本质上是钢块），而这些钉子对于钉东西同样无效。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/k7srevgiusvb24b9vl9a"></p><p> Source: <a href="https://lwfiles.mycourse.app/networkcapitalinsider-public/cc478b844a27de3f4f79f3dc0f9e0fde.jpeg"><u>Link</u></a></p><p>措施不是优化的东西，而目标是优化的东西。当我们指定优化目标时，期望它与我们想要的相关是合理的。最初，该措施可能会导致真正需要的行动。然而，一旦措施本身成为目标，优化该目标就会开始偏离我们期望的状态。</p><p> In the context of AI and reward systems, Goodhart&#39;s Law means that when a proxy reward function reinforces undesired behavior, the AI will learn to do things we don&#39;t want. The better the AI is at exploring, the more likely it is to find undesirable behavior which is spuriously rated highly by the reward model. This can lead to unintended consequences and manipulation of the reward system, as it can often be easier to &quot;cheat&quot; rather than to achieve the intended goals This is one of the core underlying reasons for reward hacking failures that we will see in subsequent sections.</p><p>奖励黑客可以被视为古德哈特定律在人工智能系统中的体现。在设计奖励函数时，精确地表达期望的行为是具有挑战性的，代理可能会找到利用漏洞或操纵奖励系统来获得高奖励的方法，而实际上却没有实现预期目标。例如，清洁机器人可能会创建自己的垃圾并放入垃圾桶中以收集奖励，而不是实际清洁环境。了解古德哈特定律对于解决奖励黑客行为和设计符合人工智能代理预期目标的强大奖励系统至关重要。它强调需要仔细考虑人工智能系统中使用的措施和激励措施，以避免意想不到的后果和不正当的激励措施。 The next section dives deeper into specific instances of reward misspecification and how AIs can find ways to achieve the literal specification of the objective and obtain high reward while not fulfilling the task in spirit.</p><p></p><h1> 3.0：奖励错误</h1><p><i><strong>奖励错误指定</strong>，也称为<strong>外部对齐</strong>问题，是指为人工智能提供准确的奖励以进行优化的问题。</i></p><p>基本问题很容易理解：指定的损失函数是否与其设计者的预期目标一致？ However, implementing this in practical scenarios is exceedingly challenging.表达人类请求背后的完整“意图”就等于传达所有人类价值观、隐含的文化背景等，而这些本身仍然知之甚少。</p><p>此外，由于大多数模型被设计为目标优化器，因此它们都容易受到古德哈特定律的影响。此漏洞意味着，由于对人类看似明确的目标施加过度的优化压力，可能会产生不可预见的负面后果，但以微妙的方式偏离了真实的目标。</p><p> The overall problem can be broken up into distinct issues which will be explained in detail in individual sub-sections below.以下是一个快速概述：</p><ul><li>当指定的奖励函数不能准确地捕捉真实的目标或期望的行为时，就会发生<strong>奖励错误</strong>指定。</li><li><strong>奖励设计</strong>是指设计奖励函数以使人工智能代理的行为与预期目标保持一致的过程。</li><li><strong>奖励黑客</strong>是指强化学习代理利用指定奖励函数中的差距或漏洞来获得高额奖励，但实际上并未实现预期目标的行为。</li><li><strong>奖励篡改</strong>是一个更广泛的概念，包括代理对奖励过程本身的不当影响，不包括通过游戏操纵奖励函数。</li></ul><p>在深入研究特定类型的奖励错误指定失败之前，以下部分进一步解释了奖励设计与算法设计相结合的重点。本节还阐明了设计有效奖励的众所周知的困难。</p><h2> 3.1: Reward Design</h2><p><i>奖励设计是指强化学习（RL）中指定奖励函数的过程。</i></p><p>奖励塑造已在前面部分介绍过。塑造是指修改奖励函数以向学习代理提供额外指导或激励的过程。 Reward design on the other hand is a broader term that encompasses the entire process of designing and shaping reward functions to guide the behavior of AI systems.它不仅涉及奖励塑造，还涉及定义目标、指定偏好以及创建符合人类价值观和期望结果的奖励函数的整个过程。奖励设计是一个经常与<a href="https://www.lesswrong.com/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem"><u>奖励工程</u></a>互换使用的术语。它们都指的是同一件事。</p><p>强化学习算法设计和强化学习奖励设计是强化学习的两个独立的方面。 RL 算法设计是关于学习算法的开发和实现，该算法允许代理根据奖励和环境交互来学习和完善其行为。此过程包括设计代理从经验中学习、更新策略并做出决策以最大化累积奖励的机制和程序。</p><p>相反，强化学习奖励设计专注于指导强化学习智能体学习过程的奖励函数的规范和设计。奖励设计需要仔细设计奖励函数，以符合期望的行为和目标，同时考虑奖励黑客或奖励篡改等潜在陷阱。奖励函数是一个关键因素，因为它塑造了 RL 智能体的行为并确定哪些行为是可取的或不可取的。</p><p>设计奖励函数通常会带来巨大的挑战，需要大量的专业知识和经验。为了演示此任务的复杂性，请考虑如何手动设计奖励函数以使代理执行后空翻，如下图所示： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/j5bex8lae44dyr0lkgnk"></p><p>资料来源：OpenAI (2017)“<a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"><u>从人类偏好中学习</u></a>”</p><p>强化学习算法设计侧重于智能体的学习和决策机制，而强化学习奖励设计则侧重于定义目标并通过奖励函数塑造智能体的行为。这两个方面对于开发有效且一致的强化学习系统都至关重要。 A well-designed RL algorithm can efficiently learn from rewards, while a carefully designed reward function can guide the agent towards desired behavior and avoid unintended consequences.下图展示了 RL 代理设计的三个关键要素：算法设计、奖励设计和防止篡改奖励信号： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/izuxey2bnle4ftwor5nq"></p><p>资料来源：Deep Mind（2020 年 4 月）“<a href="https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity"><u>规范游戏：人工智能独创性的另一面</u></a>”</p><p>尽管奖励设计过程在定义要解决的问题方面发挥着关键作用，但在介绍性强化学习文本中却很少受到关注。正如本节介绍中提到的，解决奖励指定错误问题需要找到能够抵抗古德哈特定律引起的失败的评估指标。这包括由于误导或代理目标的过度优化（奖励黑客）或代理直接干扰奖励信号（奖励篡改）而导致的失败。这些概念将在接下来的部分中进一步探讨。</p><h2> 3.2：奖励塑造</h2><p><i>奖励塑造是强化学习中使用的一种技术，它引入小的中间奖励来补充环境奖励。 This seeks to mitigate the problem of sparse reward signals and to encourage exploration and faster learning.</i></p><p> In order to succeed at a reinforcement learning problem, an AI needs to do two things:</p><ul><li> Find a sequence of actions that leads to positive reward. This is the <i>exploration</i> problem.</li><li> Remember the sequence of actions to take, and generalize to related but slightly different situations. This is the <i>learning</i> problem.</li></ul><p> Model-free RL methods explore by taking actions randomly. If, by chance, the random actions lead to a reward, they are reinforced, and the agent becomes more likely to take these beneficial actions in the future. This works well if rewards are dense enough for random actions to lead to a reward with reasonable probability. However, many of the more complicated games require long sequences of very specific actions to experience any reward, and such sequences are extremely unlikely to occur randomly.</p><p> A classic example of this problem was observed in the video game Montezuma&#39;s revenge where the agent&#39;s objective was to find a key, but there were many intermediate steps required to find it. In order to solve such long term planning problems researchers have tried adding extra terms or components to the reward function to encourage desired behavior or discourage undesired behavior. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/biuautsdvf0oduy39xuk"></p><p> Source: OpenAI (Jul 2018) “<a href="https://openai.com/research/learning-montezumas-revenge-from-a-single-demonstration"><i><u>Learning Montezuma&#39;s Revenge from a single demonstration</u></i></a> ”</p><p> The goal of reward shaping is to make the learning process more efficient by providing informative rewards that guide the agent towards the desired outcomes. Reward shaping involves providing additional rewards to the agent for making progress towards the desired goal. By shaping the rewards, the agent receives more frequent and meaningful feedback, which can help it learn more efficiently. Reward shaping can be particularly useful in scenarios where the original reward function is sparse, meaning that the agent receives little or no feedback until it reaches the final goal. However, it is important to design reward shaping carefully to avoid unintended consequences.</p><p> Reward shaping algorithms often assume hand-crafted and domain-specific shaping functions, constructed by subject matter experts, which runs contrary to the aim of autonomous learning. Moreover, poor choices of shaping rewards can worsen the agent&#39;s performance.</p><p> Poorly designed reward shaping can lead to the agent optimizing for the shaped rewards rather than the true rewards, resulting in suboptimal behavior. Examples of this are provided in the subsequent sections on reward hacking.</p><h2> 3.3: Reward Hacking</h2><p> <i>Reward hacking occurs when an AI agent finds ways to exploit loopholes or shortcuts in the environment to maximize its reward without actually achieving the intended goal.</i></p><p> Specification gaming is the general framing for the problem when an AI system finds a way to achieve the objective in an unintended way. Specification gaming can happen in many kinds of ML models. Reward hacking is a specific occurrence of a specification gaming failure in RL systems that function on reward-based mechanisms.</p><p> Reward hacking and reward misspecification are related concepts but have distinct meanings. Reward misspecification refers to the situation where the specified reward function does not accurately capture the true objective or desired behavior.</p><p> Rewards hacking does not always require reward misspecification. It is not necessarily true that a perfectly specified reward (which completely and accurately captures the desired behavior of the system) is impossible to hack. There can also be buggy or corrupted implementations which will have unintended behaviors. The point of a reward function is to boil a complicated system down to a single value. This will pretty much always involve simplifications etc., which will then be slightly different from what you&#39;re describing. The map is not the territory.</p><p> Reward hacking can manifest in a myriad of ways. For instance, in the context of game-playing agents, it might involve exploiting software glitches or bugs to directly manipulate the score or gain high rewards through unintended means.</p><p> As a concrete example, one agent in the Coast Runners game was trained with the objective of winning the race. The game uses a score mechanism, so in order to progress to the next level the reward designers used reward shaping to reward the system when it scored points. These were given when a boat gets items (such as the green blocks in the animation below) or accomplishes other actions that presumably would help it win the race. Despite being given intermediate rewards, the overall intended goal was to finish the race as quickly as possible. The developers thought the best way to get a high score was to win the race but it was not the case. The agent discovered that continuously rotating a ship in a circle to accumulate points indefinitely optimized its reward, even though it did not help it win the race. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/qjanshlbug21zqvkp4ez"></p><p> Source: Amodei &amp; Clark (2016) “ <a href="https://openai.com/research/faulty-reward-functions"><u>Faulty reward functions in the wild</u></a> &quot;</p><p> In cases where the reward function misaligns with the desired objective, reward hacking can emerge. This can lead the agent to optimize a proxy reward, deviating from the true underlying goal, thereby yielding behavior contrary to the designers&#39; intentions. As an example of something that might happen in a real-world scenario consider a cleaning robot: if the reward function focuses on reducing mess, the robot might artificially create a mess to clean up, thereby collecting rewards, instead of effectively cleaning the environment.</p><p> Reward hacking presents significant challenges to AI safety due to the potential for unintended and potentially harmful behavior. As a result, combating reward hacking remains an active research area in AI safety and alignment.</p><h2> 3.4: Reward Tampering</h2><ul><li> Victoria Krakovna et. al. (Mar 2021) <a href="https://arxiv.org/abs/1908.04734"><u>Reward Tampering Problems and Solutions</u></a></li></ul><p> <i>Reward tampering refers to instances where an AI agent inappropriately influences or manipulates the reward process itself.</i></p><p> The problem of getting some intended task done can be split into:</p><ul><li> Designing an agent that is good at optimizing reward, and,</li><li> Designing a reward process that provides the agent with suitable rewards. The reward process can be understood by breaking it down even further. The process includes:<ul><li> An implemented reward function</li><li> A mechanism for collecting appropriate sensory data as input</li><li> A way for the user to potentially update the reward function.</li></ul></li></ul><p> Reward tampering involves the agent interfering with various parts of this reward process. An agent might distort the feedback received from the reward model, altering the information used to update its behavior. It could also manipulate the reward model&#39;s implementation, altering the code or hardware to change reward computations. In some cases, agents engaging in reward tampering may even directly modify the reward values before processing in the machine register. Depending on what exactly is being tampered with we get various degrees of reward tampering. These can be distinguished from the image below. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/ogajkyybxz0rudhyza64"></p><p> Source: Leo Gao (Nov 2022) “<a href="https://www.alignmentforum.org/posts/REesy8nqvknFFKywm/clarifying-wireheading-terminology"><u>Clarifying wireheading terminology</u></a> ”</p><p> <i><u>Reward function input tampering</u> interferes only with the inputs to the reward function. Eg interfering with the sensors.</i></p><p> <i><u>Reward function tampering</u> involves the agent changing the reward function itself.</i></p><p> <i><u>Wireheading</u> refers to the behavior of a system that manipulates or corrupts its own internal structure by tampering directly with the RL algorithm itself, eg by changing the register values.</i></p><p> Reward tampering is concerning because it is hypothesized that tampering with the reward process will often arise as an instrumental goal (Bostrom, 2014; Omohundro, 2008). This can lead to weakening or breaking the relationship between the observed reward and the intended task. This is an ongoing research direction. Research papers such as “ <a href="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064"><u>Advanced Artificial Agents Intervene in the Provision of reward</u></a> ” (August 2022) by Hutter et al. seek to provide a more detailed analysis of such subjects.</p><p> A hypothesized existing example of reward tampering can be seen in recommendation-based algorithms used in social media. These algorithms influence their users&#39; emotional state to generate more &#39;likes&#39; (Russell, 2019). The intended task was to serve useful or engaging content, but this is being achieved by tampering with human emotional perceptions, and thereby changing what would be considered useful. Assuming the capabilities of systems continue to increase through either computational or algorithmic advances, it is plausible to expect reward tampering problems to become increasingly common. Therefore, reward tampering is a potential concern that requires much more research and empirical verification.</p><p></p><h1> 4.0: Learning from imitation</h1><p> The preceding sections have underscored the significance of reward misspecification for the alignment of future artificial intelligence. The next few sections will explore various attempts and proposals formulated to tackle this issue, commencing with an intuitive approach – learning the appropriate reward function through human behavior observation and imitation, rather than manual creation by the designers.</p><h2> 4.1: Imitation Learning (IL)</h2><p> <i>Imitation learning entails the process of learning via the observation of an expert&#39;s actions and replicating their behavior.</i></p><p> Unlike reinforcement learning (RL), which derives a policy for a system&#39;s actions based on its interaction outcomes with the environment, imitation learning aspires to learn a policy through the observation of another agent interacting with the environment. Imitation learning is the general term for the class of algorithms that learn through imitation. Following is a table that distinguishes various machine learning based methods. SL = Supervised learning; UL = Unsupervised learning; RL = Reinforcement Learning; IL = Imitation Learning. IL reduces RL to SL. IL + RL is a promising area. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/qx8s74nyevz3vdql9kam"></p><p>资料来源：Emma Brunskill（2022 年冬季）“<a href="https://web.stanford.edu/class/cs234/CS234Win2022/modules.html"><u>斯坦福 CS234：RL</u></a> - 第 1 讲”</p><p> IL can be implemented through behavioral cloning (BC), procedural cloning (PC) , inverse reinforcement learning (IRL), cooperative inverse reinforcement learning (CIRL), generative adversarial imitation learning (GAIL), etc…</p><p> One instance of this process&#39;s application is in the training of modern large language models (LLMs). LLMs, after training as general-purpose text generators, often undergo fine-tuning for instruction following through imitation learning, using the example of a human expert who follows instructions provided as text prompts and completions.</p><p> In the context of safety and alignment, imitation learning is favored over direct reinforcement to alleviate specification gaming issues. This problem emerges when the programmers overlook or fail to anticipate certain edge cases or unusual ways of achieving a task in the specific environment. The presumption is that demonstrating behavior, compared to RL, would be simpler and safer, as the model would not only attain the objective but also fulfill it as the expert demonstrator explicitly intends. However, this is not an infallible solution, and its limitations will be discussed in later sections.<br></p><h2> 4.2: Behavioral Cloning (BC)</h2><p> <i>Behavioral cloning involves collecting observations of an expert demonstrator proficient at the underlying task, and using supervised learning (SL) to guide an agent to &#39;imitate&#39; the demonstrated behavior.</i></p><p> Behavioral cloning is one way in which we can implement imitation learning (IL). There are also other ways such as inverse reinforcement learning (IRL), or cooperative inverse reinforcement Learning (CIRL). Unlike IRL, the goal behind behavioral cloning as a machine learning (ML) method is to replicate the demonstrator&#39;s behavior as closely as possible, regardless of what the demonstrator&#39;s goals might be.</p><p> Self-driving cars can serve as a simplistic illustration of how behavioral cloning operates. A human demonstrator (driver) is directed to operate a car, during which data about the environment state from sensors like lidar and cameras, along with the actions taken by the demonstrator, are collected. These actions can include wheel movements, gear use, etc. This creates a dataset comprising (state, action) pairs. Subsequently, supervised learning is used to train a prediction model, which attempts to predict an action for any future environment state. For instance, the model might output a specific steering wheel and gear configuration based on the camera feed. When the model achieves sufficient accuracy, it can be stated that the human driver&#39;s behavior has been &#39;cloned&#39; into a machine via learning. Hence, the term behavioral cloning.</p><p> The following points highlight several potential issues that might surface when employing behavioral cloning:</p><ul><li> <strong>Confident incorrectness</strong> : During the demonstrations, the human experts have some amount of background knowledge that they rely on, which is not taught to the model. For example, when training an LLM to have conversations using behavioral cloning, the human demonstrator might less frequently ask certain questions because they are considered &#39;common sense&#39;. A model trained to imitate will copy both - the types of questions asked in conversation, as well as, the frequency with which they are asked. Humans already possess this background knowledge, but an LLM doesn&#39;t. This means that to have the same level of information as a human, the model should ask some questions more frequently to fill the gaps in its knowledge. But since the model seeks to imitate, it will stick to the low frequency demonstrated by the human and thus has strictly less information overall than the demonstrator for the same conversational task. Despite this dearth of knowledge, we expect it to be able to perform as a clone and reach human-level performance. This means in order to reach human performance on less than human knowledge it will resort to &#39;making up facts&#39; that help it reach its performance goals. These &#39;hallucinations&#39; will then be presented during the conversation, with the same level of confidence as all the other information. Hallucinations and confident incorrectness is <a href="https://arxiv.org/pdf/2103.15025.pdf"><u>an empirically verified problem</u></a> in many LLMs including GPT-2 and 3, and raises obvious concerns for AI safety.</li><li> <strong>Underachieving</strong> : The types of hallucinations mentioned above arose because the model knew too little. However, the model can also know too much. If the model knows more than the human demonstrator because it is able to find more patterns in the environment state that it is given, it will throw away that information and reduce its performance to match human level. This is because it is trained as a &#39;clone&#39;. Ideally, we don&#39;t want the model dumbing itself down or not disclosing useful new patterns in data just because it is trying to be humanlike or perform at a human level. This is another problem that will have to be addressed if behavioral cloning continues to be used as an ML technique.</li></ul><p><br></p><h2> 4.3: Procedural Cloning (PC)</h2><ul><li> Mengjiao Yang et. al. (May 2022) “ <a href="https://arxiv.org/abs/2205.10816"><u>Chain of Thought Imitation with Procedure Cloning</u></a> ”</li></ul><p> <i>Procedure cloning (PC) extends behavioral cloning (BC) by not just imitating the demonstrators outputs but also imitating the complete sequence of intermediate computations associated with an expert&#39;s procedure.</i></p><p> In BC, the agent learns to map states directly to actions by discarding the intermediate search outputs. On the other hand, the PC approach learns the entire sequence of intermediate computations, including branches and backtracks, during training. During inference, PC generates a sequence of intermediate search outcomes that mimic the expert&#39;s search procedure before outputting the final action.</p><p> The main difference between PC and BC lies in the information they utilize. BC only has access to expert state-action pairs as demonstrations, while PC also has access to the intermediate computations that generated those state-action pairs. PC learns to predict the complete series of intermediate computation outcomes, enabling it to generalize better to test environments with different configurations compared to alternative improvements over BC. PC&#39;s ability to imitate the expert&#39;s search procedure allows it to capture the underlying reasoning and decision-making process, leading to improved performance in various tasks.</p><p> A limitation of PC is the computational overhead compared to BC, as PC needs to predict intermediate procedures. Additionally, the choice of how to encode the expert&#39;s algorithm into a form suitable for PC is left to the practitioner, which may require some trial-and-error in designing the ideal computation sequence.<br></p><h2> 4.4: Inverse Reinforcement Learning (IRL)</h2><p> <i>Inverse reinforcement learning (IRL) represents a form of machine learning wherein an artificial intelligence observes the behavior of another agent within a particular environment, typically an expert human, and endeavors to discern the reward function without its explicit definition.</i></p><p> IRL is typically employed when a reward function is too intricate to define programmatically, or when AI agents need to react robustly to sudden environmental changes necessitating a modification in the reward function for safety. For instance, consider an AI agent learning to execute a backflip. Humans, dogs, and Boston Dynamics robots can all perform backflips, but the manner in which they do so varies significantly depending on their physiology, their incentives, and their current location, all of which can be highly diverse in the real world. An AI agent learning backflips purely through trial and error across a wide range of body types and locations, without something to observe, might prove highly inefficient.</p><p> IRL, therefore, does not necessarily imply that an AI mimics other agents&#39; behavior, since AI researchers may anticipate the AI agent to devise more efficient ways to maximize the discovered reward function. Nevertheless, IRL does assume that the observed agent behaves transparently enough for an AI agent to accurately identify their actions, and what success constitutes. This means that IRL endeavors to discover the reward functions that &#39;explain&#39; the demonstrations. This should not be conflated with imitation learning where the primary interest is a policy capable of generating the observed demonstrations. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/k4eolj8ibrm3bbueywno"></p><p> Source: <a href="https://miro.medium.com/v2/resize:fit:3508/1*rZoO-azxiEH3viQao8NcAA.png"><u>Link</u></a></p><p> IRL constitutes both a machine learning method, since it can be employed when specifying a reward function is excessively challenging, and a machine learning problem, as an AI agent may settle on an inaccurate reward function or utilize unsafe and misaligned methods to achieve it.</p><p> One of the limitations to this approach is that IRL algorithms presume that the observed behavior is optimal, an assumption that arguably proves too robust when dealing with human demonstrations. Another problem is that the IRL problem is ill-posed as every policy is optimal for the null reward. For most behavioral observations, multiple fitting reward functions exist. This set of solutions often includes many degenerate solutions, which assign zero rewards to all states.</p><h2> 4.5: Cooperative Inverse Reinforcement Learning (CIRL)</h2><ul><li> Stuart Russell et. al. (Nov 2016) “ <a href="https://arxiv.org/abs/1606.03137"><u>Cooperative Inverse Reinforcement Learning</u></a> ”</li></ul><p> CIRL (Cooperative Inverse Reinforcement Learning) is an extension of the IRL (Inverse Reinforcement Learning) framework. IRL is a learning approach that aims to infer the underlying reward function of an expert by observing their behavior. It assumes that the expert&#39;s behavior is optimal and tries to learn a reward function that explains their actions.  CIRL, on the other hand, is an interactive form of IRL that addresses two major weaknesses of conventional IRL.</p><p> First, Instead of simply copying the human reward function CIRL is formulated as a learning process. It is an interactive reward maximization process, where the human functions as a teacher and provides feedback (in the form of rewards) on the agent&#39;s actions. This allows the human to nudge the AI agent towards behavioral patterns that align with their preferences. The second weakness of conventional IRL is that it assumes the human behaves optimally, which limits the teaching behaviors that can be considered. CIRL addresses this weakness by allowing for a variety of teaching behaviors and interactions between the human and the AI agent. It enables the AI agent to learn not only what actions to take but also how and why to take them, by observing and interacting with the human.</p><p> CIRL has been studied as a potential approach to AI alignment, particularly in scenarios where deep learning may not scale to AGI. However, opinions on the potential effectiveness of CIRL vary, with some researchers expecting it to be helpful if deep learning doesn&#39;t scale to AGI, while others have a higher probability of deep learning scaling to AGI.</p><h2> 4.6: The (Easy) Goal Inference Problem</h2><ul><li> Christiano, Paul (Nov 2018) “ <a href="https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard"><u>The easy goal inference problem is still hard</u></a> ”</li></ul><p><br> <i>The <strong>goal inference problem</strong> refers to the task of inferring the goals or intentions of an agent based on their observed behavior or actions.</i></p><p> This final section builds upon the limitations highlighted in previous sections to introduce the Goal Inference problem, and it&#39;s simpler subset - the easy goal inference problem. Imitation learning based approaches, generally follows these steps:</p><ol><li> Observe the user&#39;s actions and statements.</li><li> Deduce the user&#39;s preferences.</li><li> Endeavor to enhance the world according to the user&#39;s preferences, possibly collaborating with the user and seeking clarification as needed.</li></ol><p> The merit of this method is that we can immediately start constructing systems that are driven by observed user behavior. However, as a consequence of this approach, we run into the goal inference problem. This refers to the task of inferring the goals or intentions of an agent based on their observed behavior or actions. It involves determining what the agent is trying to achieve or what their desired outcome is. The goal inference problem is challenging because agents may act sub-optimally or fail to achieve their goals, making it difficult to accurately infer their true intentions. Traditional approaches to goal inference often assume that agents act optimally or exhibit simplified forms of sub-optimality, which may not capture the complexity of real-world planning and decision-making. Therefore, the goal inference problem requires accounting for the difficulty of planning itself and the possibility of sub-optimal or failed plans.</p><p> However, it also optimistically presumes that we can depict a human as a somewhat rational agent, which might not always hold. The easy goal inference problem is a simplified version of the goal inference problem.</p><p> <i>The <strong>easy goal inference problem</strong> involves finding a reasonable representation or approximation of what a human wants, given complete access to the human&#39;s policy or behavior in any situation.</i></p><p> This version of the problem assumes no algorithmic limitations and focuses on extracting the true values that the human is imperfectly optimizing. However, even this simplified version of the problem remains challenging, and little progress has been made on the general case. The easy goal inference problem is related to the goal inference problem because it highlights the difficulty of accurately inferring human goals or intentions, even in simplified scenarios. While narrow domains with simple decisions can be solved using existing approaches, more complex tasks such as designing a city or setting policies require addressing the challenges of modeling human mistakes and sub-optimal behavior. Therefore, the easy goal inference problem serves as a starting point to understand the broader goal inference problem and the additional complexities it entails.</p><p> Inverse reinforcement learning (IRL) is effective in modeling and imitating human experts. However, for many significant applications, we desire AI systems that can make decisions surpassing even the experts. In such cases, the accuracy of the model isn&#39;t the sole criterion because a perfectly accurate model would merely lead us to replicate human behavior and not transcend it.</p><p> This necessitates an explicit model of errors or bounded rationality, which will guide the AI on how to improve or be &quot;smarter,&quot; and which aspects of the human policy it should discard. Nonetheless, this remains an exceedingly challenging problem as humans are not primarily rational with a bit of added noise. Hence, constructing any model of mistakes is just as complex as building a comprehensive model of human behavior. A critical question we face is: How do we determine the quality of a model when accuracy can no longer be our reliable measure? How can we distinguish between good and bad decisions?</p><h1> 5.0: Learning from feedback</h1><p> This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI.</p><h2> 5.1: Reward Modeling</h2><ul><li> DeepMind (Nov 2018) “ <a href="https://arxiv.org/abs/1811.07871"><u>Scalable agent alignment via reward modeling</u></a> ”</li></ul><p> Reward modeling was developed to apply reinforcement learning (RL) algorithms to real-world problems where designing a reward function is difficult, in part because humans don&#39;t have a perfect understanding of every objective. In reward modeling, human assistants evaluate the outcomes of AI behavior, without needing to know how to perform or demonstrate the task optimally themselves. This is similar to how you can tell if a dish is cooked well by tasting it even if you do not know how to cook, and thus your feedback can be used by a chef to learn how to cook better. This technique separates the RL alignment problem into two separate halves: Understanding intentions, ie learning the &#39;What?&#39;, and Acting to achieve the intentions, ie learning the &#39;How?&#39;. This means that in the modeling agenda, there are two different ML models:</p><ul><li> A reward model is trained with user feedback. This model learns to predict what humans would consider good behavior.</li><li> An agent trained with RL, where the reward for the agent is determined by the outputs of the reward model </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/hzepfc9kuxzytm5iemjm"></p><p> Source: DeepMind (Nov 2018) “ <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>Scalable agent alignment via reward modeling</u></a> ”</p><p> Overall, while promising reward modeling can still fall prey to reward misspecification and reward hacking failures. Obtaining accurate and comprehensive feedback can be challenging, and human evaluators may have limited knowledge or biases that can impact the quality of the feedback. Additionally, any reward functions learnt through modeling might also struggle to generalize to new situations or environments that differ from the training data. These are all discussed further using concrete examples in later sections.</p><p> There are also some variants of reward modeling such as:</p><ul><li> <strong><u>Narrow reward modeling</u></strong> is a specific flavor of reward modeling where the focus is on training AI systems to accomplish specific tasks rather than trying to determine the &quot;true human utility function&quot;. It aims to learn reward functions to achieve particular objectives, rather than seeking a comprehensive understanding of human values.</li><li> <strong><u>Recursive reward modeling</u></strong> seeks to introduce scalability to the technique. In recursive reward modeling, the focus is on decomposing a complex task into simpler subtasks and using reward modeling at each level to train agents that can perform those subtasks. This hierarchical structure allows for more efficient training and credit assignment, as well as the exploration of novel solutions that may not be apparent to humans. This is shown in the diagram below. Scalable oversight will be covered in greater depth in future chapters. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/xclvo4btaebz0ondwane"></p><p> Source: DeepMind (Nov 2018) “ <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>Scalable agent alignment via reward modeling</u></a> ”</p><p> The general reward modeling framework forms the basis for other feedback based techniques such as RLHF (Reinforcement Learning from Human Feedback) which is discussed in the next section.</p><h2> 5.2. Reinforcement Learning from Human Feedback (RLHF)</h2><ul><li> Christiano, Paul et. al. (Feb 2023) “ <a href="https://arxiv.org/abs/1706.03741"><u>Deep reinforcement learning from human preferences</u></a> ”</li></ul><p> Reinforcement Learning from Human Feedback (RLHF) is a method developed by OpenAI. It&#39;s a crucial part of <a href="https://openai.com/blog/our-approach-to-ai-safety"><u>their strategy</u></a> to create AIs that are both safe and aligned with human values. A prime example of an AI trained with RLHF is OpenAI&#39;s ChatGPT.</p><p> Earlier in this chapter, the reader was asked to consider the reward design problem for manually defining a reward function to get an agent to perform a backflip. This section considers the RLHF solution to this design problem. RLHF addresses this problem as follows: A human is initially shown two instances of an AI&#39;s backflip attempts, then the human selects which one appears more like a backflip, and finally, the AI is updated accordingly. By repeating this process thousands of times, we can guide the AI to perform actual backflips. </p><figure class="table"><table style="background-color:hsl(0, 0%, 100%);border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:92.48%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/wcjfdzcx4cnnfi5ruu66"></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/hl6oecj6eg1hrjffpnbh"></figure></td></tr></tbody></table></figure><p>资料来源：OpenAI (2017)“<a href="https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"><u>从人类偏好中学习</u></a>”</p><p> In the image on the left, RLHF learned to backflip using around 900 individual bits of feedback from the human evaluator. In the image on the right the authors point out that manual reward crafting took two hours to write a custom reward function for a robot to perform a backflip. While it was successful, it was significantly less elegant than the one trained purely through human feedback.</p><p> Similar to designing a reward function that efficiently rewards proper backflips, it is hard to specify precisely what it means to generate safe or helpful text. This served as some of the motivation behind making RLHF integral to the training of some current Large Language Models (LLMs).</p><p> Although training sequences may vary slightly across organizations, most labs adhere to the general framework of pre-training followed by some form of fine-tuning. Observing the InstructGPT training process offers insight into a possible path for training LLMs. The steps include: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/e3019jiopfofiu1pm6ms"></p><p> Source: OpenAI (Jan 2022) “ <a href="https://openai.com/research/instruction-following"><u>Aligning language models to follow instructions</u></a> ”</p><ul><li> <strong>Step 0:</strong> <a href="https://en.wikipedia.org/wiki/Weak_supervision#Semi-supervised_learning"><strong><u>Semi-Supervised</u></strong></a> <strong>Generative Pre-training:</strong> The LLM is initially trained using a massive amount of internet text data, where the task is to predict the next word in a natural language context.</li><li> <strong>Step 1:</strong> <a href="https://en.wikipedia.org/wiki/Supervised_learning"><strong><u>Supervised</u></strong></a> <strong>&nbsp;</strong> <a href="https://platform.openai.com/docs/guides/fine-tuning"><strong><u>Fine-tuning</u></strong></a> <strong>:</strong> A fine-tuning dataset is created by presenting a prompt to a human and asking them to write a response. This process yields a dataset of (prompt, output) pairs. This dataset is then used to fine-tune the LLM through supervised learning, a form of behavioral cloning.</li><li> <strong>Step 2:</strong> <strong>Train a Reward Model:</strong> We train an additional reward model. We initially prompt the fine-tuned LLM and gather several output samples for the same prompt. A human then ranks these samples from best to worst. This ranking is used to train the reward model to predict what a human would rank higher.</li><li> <strong>Step 3: Reinforcement learning:</strong> Once we have both a fine-tuned LLM and a reward model, we can employ <a href="https://openai.com/research/openai-baselines-ppo"><u>Proximal Policy Optimization (PPO)</u></a> -based reinforcement learning to encourage the fine-tuned model to maximize the reward that the reward model, which mimics human rankings, offers.</li></ul><p> <i><u>Reward hacking in feedback methods</u></i></p><p> While the feedback based mechanisms do make models safer, they does not make them immune to reward hacking. The effectiveness of an algorithm heavily relies on the human evaluator&#39;s intuition about what constitutes the correct behavior. If the human lacks a thorough understanding of the task, they may not provide beneficial feedback. Further, in certain domains, our system might lead to agents developing policies that deceive the evaluators. For instance, a robot intended to grasp objects merely positioned its manipulator between the camera and the object, making it seem as if it was executing the task as shown below. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/bxqftcwt4mj5meowwma5"></p><p> Source: Christiano et al (2017) “ <a href="https://arxiv.org/pdf/1706.03741.pdf"><u>Deep Reinforcement Learning From Human Preferences</u></a> ”</p><h2> 5.3: Pretraining with Human Feedback (PHF)</h2><ul><li> Tomasz Korbak et. al. (Feb 2023) “ <a href="https://arxiv.org/abs/1706.03741"><u>Pretraining Language Models with Human Preferences</u></a> ”</li></ul><p> In standard pretraining, the language model attempts to learn parameters such that they maximize the likelihood of the training data. However, this also includes undesirable content such as falsehoods, offensive language, and private information. The concept of Pretraining with human feedback (PHF) utilizes the reward modeling methodology in the pretraining phase. The authors of the paper found that PHF works much better than the standard practice of only using feedback (RLHF) after pretraining.</p><p> In PHF the training data is scored using a reward function, such as a toxic text classifier, to guide the language model to learn from undesirable content while avoiding imitating it during inference time.</p><p> Similar to RLHF, PHF does not completely solve reward hacking, however, it might move the systems one small step closer. These methods can be further extended by employing AI assistants to aid humans in providing more effective feedback. Some aspects of this strategy are introduced in the next section but will be explored in further detail in the chapters on scalable and adversarial oversight methods.</p><h2> 5.4. Reinforcement Learning from AI Feedback (RLAIF)</h2><p> <i>Reinforcement Learning from AI Feedback (RLAIF) is a framework involving the training of an AI agent to learn from the feedback given by another AI system.</i></p><p> RLAIF also known as RLCAI (Reinforcement Learning on Constitutional AI) or simply Constitutional AI, was <a href="https://www.anthropic.com/index/claudes-constitution"><u>developed by Anthropic</u></a> . A central component of Constitutional AI is the constitution, a set of human-written principles that the AI is expected to adhere to, such as &quot;Choose the least threatening or aggressive response&quot;. Anthropic&#39;s AI assistant Claude&#39;s constitution incorporates principles from the Universal Declaration of Human Rights, Apple&#39;s Terms of Service, Deepmind&#39;s <a href="https://arxiv.org/abs/2209.14375"><u>Sparrow Principles</u></a> , and more. Constitutional AI begins with an AI trained primarily for helpfulness and subsequently trains it for harmlessness in two stages:</p><ul><li> <strong>Stage 1:</strong> The AI continuously critiques and refines its own responses to harmful prompts. For instance, if we ask the AI for advice on building bombs and it responds with a bomb tutorial, we then ask the AI to revise the response in accordance with a randomly selected constitutional principle. The AI is then trained to generate outputs more similar to these revised responses. This stage&#39;s primary objective is to facilitate the second stage.</li><li> <strong>Stage 2:</strong> We use the AI, fine-tuned from stage 1, to produce pairs of alternative responses to harmful prompts. The AI then rates each pair according to a randomly selected constitutional principle. This results in AI-generated preferences for harmlessness, which we blend with human preferences for helpfulness to ensure the AI doesn&#39;t lose its ability to be helpful. The final step is to train the AI to create responses that closely resemble the preferred responses.</li></ul><p> Anthropic&#39;s experiments indicate that AIs trained with Constitutional Reinforcement Learning are significantly safer (in the sense of less offensive and less likely to give you potentially harmful information) while maintaining the same level of helpfulness compared to AIs trained with RLHF. While Constitutional AI does share some issues with RLHF concerning robustness, it also promises better scalability due to its reduced reliance on human supervision. The image below provides a comparison of Constitutional AI&#39;s helpfulness with that of RLHF. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mMBoPnFrFqQJKzDsZ/thecid5vwddjlpzp7zpd"></p><p> Source: Anthropic, (Dec 2022) “ <a href="https://arxiv.org/pdf/2212.08073.pdf"><u>Constitutional AI: Harmlessness from AI Feedback</u></a> ”</p><h1> Anki Flashcards &amp; Quizzes</h1><p> Flashcards were written by <a href="https://www.ai-alignment-flashcards.com">ai-alignment-flashcards.com</a> . Here are the Anki decks for the current chapter to help review and check your understanding.</p><ul><li> Christiano - <a href="https://www.ai-alignment-flashcards.com/quiz/christiano-easy-goal-inference"><u>Easy goal inference problem</u></a></li><li> Christiano et. al. (Blog version) - <a href="https://www.ai-alignment-flashcards.com/quiz/christiano-learning-from-human-preferences"><u>Learning from human preferences</u></a></li><li> Stiennon et. al. (Blog version) - <a href="https://www.ai-alignment-flashcards.com/quiz/stiennon-learning-to-summarize-blog"><u>Learning to Summarize with Human Feedback</u></a></li><li> Lowe et. al. (Blog version) - <a href="https://www.ai-alignment-flashcards.com/quiz/lowe-aligning-language-models"><u>Aligning Language Models to Follow Instructions</u></a></li></ul><h1> Exercises &amp; Activities</h1><p> Excercises and activities are taken from the <a href="https://course.aisafetyfundamentals.com/alignment">2023 iteration of the AGISF course</a> (now called AISF), and its older version <a href="https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit#heading=h.g3svzartnppy"><u>2022 AGISF version</u></a> .</p><ol><li> Autoregressive language models are trained to predict the next word in a sentence, given the previous words. (Since the correct answer for each prediction can be generated automatically from existing training data, this is known as <a href="https://amitness.com/2020/05/self-supervised-learning-nlp/"><u>self-supervised learning</u></a> , and is the key technique for training cutting-edge language models.) In what ways is this the same as, or different from, behavioral cloning?</li><li> Imagine using RHLF to perform a complex task like building a castle in Minecraft. What sort of problems would you encounter?</li><li> Read the further reading by Armstrong about <a href="https://www.lesswrong.com/posts/rtphbZbMHTLCepd6d/humans-have-no-values"><u>how humans can be assigned any values</u></a> , then explain: why does reward learning ever work in practice?</li></ol><h2> Discussion prompts</h2><ol><li> What are the key similarities and differences between behavioral cloning, RL, and RLHF? What types of human preferences can these techniques most easily learn? What types would be hardest to learn?</li><li> What implications does the size of the discriminator-critique gap (as discussed by Saunders et al.&#39;s paper on AI-written critiques) have?</li><li> Should we expect RLHF to be necessary for building AGI (independent of safety concerns)?</li><li> How might using RLHF lead to misaligned AGIs?</li></ol><h1> Acknowledgements</h1><p> Thanks to Charbel-Raphaël Segerie, Jeanne Salle, Bogdan Ionut Cirstea, Nemo, Gurvan, and the many course participants of ML4G France, ML4G Germany, and AISF Sweden for helpful comments and feedback.</p><p> Thanks to the <a href="https://www.agisf.com/">AI Safety Fundamentals</a> team from BlueDot Impact for creating the AISF course upon which this series of texts is structured. Thanks to <a href="https://www.ai-alignment-flashcards.com/">AI Alignment Flashcards</a> for creating the revision quizzes and Anki flashcards.</p><h1> Meta-Notes</h1><ul><li> The objective of the overall project is to write something that can be used as a introductory textbook to AI Safety. The rough audience is ML masters students, or audiences that have a semi technical background eg Physics and get them up to speed on the core arguments. The intent is not to cover literally every single argument under the sky. There can obviously be a 102 version of the text that covers more nuance. I am especially trying to account for reading time and keep it roughly in the range of 40-60 mins per chapter. Which means I have to often make decisions on what to include, and at what level of detail.</li><li> I consider this a work-in-progress project. After much encouragement by others, I decided to publish what I have so far to get further feedback and comments.</li><li> If there are any mistakes or I have misrepresented anyone&#39;s views please let me know. I will make sure to correct it. Feel free to suggest improvements to flow/content additions/deletions/etc...</li><li> There is also <a href="https://docs.google.com/document/d/1niRLuFX1FfsMrlMLJtbOm4m_yK8dTdXi3gKmkENp-ss/edit?usp=sharing">a google docs version</a> in case you prefer to leave comments there.</li><li> The general structure of the overall book/sequence will follow AI Safety fundamentals, however, there have been significant changes and additions to individual chapters in terms of content added/deleted.</li><li> When large portions of a section are drawn from an individual paper/post the reference is placed directly under the title. The sections serve as summarizations of the post. If you wish you can directly refer to the original papers/posts as well. The intent was to provide a single coherent flow of arguments all in one place.</li></ul><h1> Sources</h1><ul><li> Gabriel Dulac-Arnold et. al. (Apr 2019) “ <a href="https://arxiv.org/abs/1904.12901"><u>Challenges of Real-World Reinforcement Learning</u></a> ”</li><li> Gabriel Dulac-Arnold et. al. (Mar 2021) “ <a href="https://arxiv.org/abs/2003.11881"><u>An empirical investigation of the challenges of real-world reinforcement learning</u></a> ”</li><li> OpenAI Spinning Up (2018)  “ <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html"><u>Part 1: Key Concepts in RL</u></a> ”</li><li> David Mguni et. al. (Feb 2023) “ <a href="https://arxiv.org/abs/2103.09159"><u>Learning to Shape Rewards using a Game of Two Partners</u></a> ”</li><li> alexirpan (Feb 2018) “ <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html"><u>Deep Reinforcement Learning Doesn&#39;t Work Yet</u></a> ”</li><li> TurnTrout ( Jul 2022) “ <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target"><u>Reward is not the optimization target</u></a> ”</li><li> Sam Ringer (Dec 2022) “ <a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward"><u>Models Don&#39;t &quot;Get Reward&quot;</u></a> ”</li><li> Dr. Birdbrain (Feb 2021) “ <a href="https://www.lesswrong.com/posts/K5Nt64jfSRWeyTABk/introduction-to-reinforcement-learning"><u>Introduction to Reinforcement Learning</u></a> ”</li><li> Richard Ngo et. al. (Sep 2023) “ <a href="https://arxiv.org/abs/2209.00626"><u>The alignment problem from a deep learning perspective</u></a> ”</li><li> Jan Leike et. al. (Nov 2018) <a href="https://arxiv.org/abs/1811.07871v1"><u>Scalable agent alignment via reward modeling: a research direction</u></a></li><li> Tom Everitt et. al. (Mar 2021) <a href="http://arxiv.org/abs/1908.04734v5"><u>Reward Tampering Problems and Solutions in Reinforcement</u></a></li><li> Joar Skalse ( Aug 2019) “ <a href="https://www.alignmentforum.org/posts/rvxcSc6wdcCfaX6GZ/two-senses-of-optimizer"><u>Two senses of “optimizer” — AI Alignment Forum</u></a> ”</li><li> Drake Thomas, Thomas Kwa (May 2023) “ <a href="https://www.alignmentforum.org/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic"><u>When is Goodhart catastrophic? — AI Alignment Forum</u></a> ”</li><li> Scott Garrabrant (Dec 2017) “ <a href="https://www.alignmentforum.org/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy"><u>Goodhart Taxonomy</u></a> ”</li><li> Victoria Krakovna (Aug 2019) “ <a href="https://www.alignmentforum.org/posts/yXPT4nr4as7JvxLQa/classifying-specification-problems-as-variants-of-goodhart-s"><u>Classifying specification problems as variants of Goodhart&#39;s Law — AI Alignment Forum</u></a> ”</li><li> Stephen Casper et. al. (Sep 2023) “ <a href="https://arxiv.org/abs/2307.15217"><u>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</u></a> ”</li><li> Jacob Steinhardt et. al. (Feb 2022) “ <a href="https://arxiv.org/abs/2201.03544v2"><u>The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</u></a> ”</li><li> Yuntao Bai et. al. (Dec 2022) “ <a href="https://arxiv.org/abs/2212.08073"><u>Constitutional AI: Harmlessness from AI Feedback</u></a> ”</li><li> Tom Everitt et. al. (Jul 2023) “ <a href="https://www.alignmentforum.org/posts/aw5nqamqtnDnW8w9u/reward-hacking-from-a-causal-perspective"><u>Reward Hacking from a Causal Perspective</u></a> ”</li><li> Mengjiao Yang et. al. (May 2022) “ <a href="https://arxiv.org/abs/2205.10816"><u>Chain of Thought Imitation with Procedure Cloning</u></a> ”</li><li> Stuart Armstrong (Nov 2019) “ <a href="https://www.alignmentforum.org/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading"><u>Defining AI wireheading</u></a> ”</li><li> Stuart Russell (Nov 2016) ” <a href="https://arxiv.org/abs/1606.03137"><u>Cooperative Inverse Reinforcement Learning</u></a> ”</li><li> Stampy (2023)  “ <a href="https://aisafety.info/"><u>AI Safety Info</u></a> ”</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/mMBoPnFrFqQJKzDsZ/alignment-101-ch-2-reward-misspecification#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mMBoPnFrFqQJKzDsZ/alignment-101-ch-2-reward-misspecification<guid ispermalink="false"> mMBoPnFrFqQJKzDsZ</guid><dc:creator><![CDATA[markov]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:39:34 GMT</pubDate> </item><item><title><![CDATA[Alignment 101 - Ch.1 - AGI]]></title><description><![CDATA[Published on October 18, 2023 8:38 PM GMT<br/><br/><h1>概述</h1><ol><li><strong>Foundation Models</strong> : The chapter begins with an exploration of how contemporary approaches in machine learning lean towards the development of centralized foundation models. The section elaborates the merits and drawbacks of such a paradigm.</li><li> <strong>Leveraging Computation</strong> : This section introduces the reader to “the bitter lesson&quot;. The focus for this section is comparing historical advancements achieved through the utilization of human-engineered heuristics with those accomplished by capitalizing on additional computation. This is followed by a discussion on current trends in the “compute optimal training” of machine learning models. This section concludes with an introduction to and the implications of scaling laws and the scaling hypothesis.</li><li> <strong>Capabilities</strong> : This section builds upon the previously introduced trends and paradigms, and extrapolates these to predict potential capabilities of future artificial intelligence (AI) models. There is a discussion around the merits of using the term capabilities instead of intelligence. This is followed by introducing slightly more detailed frameworks for different possible tiers and categorizations of artificial general intelligence (AGI). Moreover, the concept of (t,n)-AGI is introduced. This outlook allows a straightforward comparison to humans, while also establishing a measurable continuous spectrum of capabilities. Overall the aim is to help establish a more concrete definition of AGI capabilities for the reader.</li><li> <strong>Threat Models</strong> : Understanding capability thresholds paves the way for a discussion into the concept of emergence. This is then followed by an examination of qualities that machine intelligences might possess. These qualities potentially indicate the possibility for an intelligence explosion. The section concludes with a discussion of the four fundamental assumptions put forward by the Machine Intelligence Research Institute (MIRI) about machine intelligence. These claims explore the power of general intelligence, and why this capability arising in machines does not promise a beneficial future to humans by default.</li><li> <strong>Timelines</strong> : This section explores some concrete predictions of when the capabilities discussed in the previous sections might surface. The dialogue hinges on the concept of anchors in forecasting. This pays specific focus on determining how we can use anchors inspired by biological systems to provide a basis for estimating the computational requirements of AI systems.</li><li> <strong>Takeoff</strong> : The chapter concludes with a section that introduces the concept of takeoff and various forms of takeoff dynamics. The dynamics involve takeoff speeds, polarity and homogeneity. The section presents differing opinions from various researchers on potential future scenarios.</li></ol><h1> 1.0: Foundation Models</h1><blockquote><p> <i>A</i> <a href="https://en.wikipedia.org/wiki/Foundation_models"><i><strong><u>foundation model</u></strong></i></a> <i>is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (eg, fine-tuned) to a wide range of downstream tasks.</i></p></blockquote><p> <i>-</i> Bommasani Rishi et. al. (2022) &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot;</p><p> Large-scale research labs have been navigating towards a new machine learning (ML) paradigm: the training of a &quot;base model&quot;. A base model is simply a large-scale model that serves as a building block for various applications. This model serves as a multifaceted entity—competent across various areas but not specialized in any one.</p><p> Once there is a base, developers can retrain portions of the model using additional examples, which allows operators to generate specialized systems adapted for specific tasks and improve the system&#39;s quality and consistency. This is known as <strong>fine-tuning</strong> .</p><p> Fine-tuning facilitates the development of models capable of diverse downstream tasks. Simple examples of this include fine-tuning the general purpose GPT language model to follow instructions, or to interact in a chat format. Other examples include specializing models for programming, scientific texts, mathematical proofs and other such tasks. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/l6hjr0loovhnnvynkkig" alt="https://www.artificialintelligence.news/pathal/uploads/2021/09/2021-foundationmodel-1024x692.png"></p><p> Source: Bommasani Rishi et. al. (2022) &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot;</p><p> The costs for developing models from scratch is also increasing due to a multitude of factors. If models were trained on the supervised learning (SL) paradigm, then the developers must either already have a large dataset, or in scenarios where the necessary dataset does not already exist, they must generate their own data, directing precious resources—both monetary and temporal—toward the careful labeling of the data.</p><p> <i><strong>SSL (Semi-Supervised Learning)</strong> is a learning approach that combines labeled and unlabeled data during training to improve the performance of machine learning models.</i></p><p> Achieving state-of-the-art performance across numerous tasks demands that the learning process be anchored in millions, if not billions, of examples. Foundation models provide a solution to this by leveraging Semi-Supervised Learning (SSL). SSL algorithms use both labeled and unlabeled data during training. This allows the models to utilize the information present in the unlabeled examples to improve performance. The intuition behind SSL is that the unlabeled data contains valuable information about the underlying structure of the data, which can be used to enhance the model&#39;s generalization capabilities. By incorporating the unlabeled data, SSL algorithms aim to learn a more robust and accurate model compared to SL algorithms, especially when labeled data is limited or expensive to obtain. Once the foundation model is trained,  fine-tuning allows it to specialize and perform well on specific downstream tasks by using far fewer SL labeled examples. By fine-tuning the model on a smaller labeled dataset, the model can leverage the knowledge it acquired during SSL training and adapt it to the specific task, resulting in overall improved performance.</p><p> Fine-tuning foundation models might be cheaper than training a model from scratch, however the cost to train the base model itself keeps increasing. Foundation models are extremely complex and require significant resources to develop, train, and deploy. Training can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters and using carefully designed software systems. Such dedicated clusters can be both costly and difficult to obtain. There have also been recent efforts to mitigate the costs by <a href="https://arxiv.org/abs/2206.01288"><u>training foundation models in a decentralized manner in heterogeneous environments</u></a> . For narrowly-defined use-cases, that cost may not be justifiable, when a smaller model may achieve similar (or better) results for a much lower price.</p><p> <i><strong>Pre-training</strong> in the context of foundation models refers to the initial phase where a model is trained on a large, unlabeled dataset to learn general knowledge and patterns before fine-tuning it on specific tasks.</i></p><p> <i><strong>Transfer learning</strong> in the context of foundation models refers to the process of leveraging knowledge and patterns learned from a related task or domain with abundant labeled data to improve performance on a target task or domain with limited labeled data.</i></p><p> Leveraging advancements in <a href="https://en.wikipedia.org/wiki/Transfer_learning"><u>transfer learning</u></a> and <a href="https://en.wikipedia.org/wiki/Fine-tuning_(machine_learning)"><u>fine-tuning</u></a> techniques, these foundation models can be harnessed to spawn specialized models tailored for specific objectives. This advancement amplifies the field&#39;s capacity to transfer acquired &quot;knowledge&quot; from one task and apply it effectively through the fine-tuning process to a distinct downstream task. Some notable foundation models include <a href="https://arxiv.org/abs/1810.04805v2"><u>BERT</u></a> , <a href="https://openai.com/blog/gpt-3-apps/"><u>GPT-3</u></a> , <a href="https://openai.com/gpt-4"><u>GPT-4</u></a> , <a href="https://www.deepmind.com/publications/a-generalist-agent"><u>GATO</u></a> and <a href="https://openai.com/blog/clip/"><u>CLIP</u></a> . </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/p8yhr6c2yz9yzguwxh6r"> Source: Bommasani Rishi et. al. (2022) &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot;</p><p> This novel paradigm potentially provides a larger demographic with access to state-of-the-art capabilities, as well as the potential to train their own models with minimal data for highly specialized tasks. This potential access to capabilities is not guaranteed however,  does depend on the specific API options available, or on the availability of open-sourced foundation models that the users can rely upon.</p><p> Broadly speaking, there exist significant economic incentives to expand the capabilities and scale of foundation models. The authors of &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot; foresee steady technological progress in the forthcoming years. Although foundation models presently manifest most robustly in natural language processing (NLP), this can be interpreted as a trend toward a new general paradigm of AI development. As of January 2023, efforts by DeepMind to train a reinforcement learning (RL) foundation model—an <a href="https://arxiv.org/abs/2301.07608"><u>&quot;adaptive agent&quot; (AdA)</u></a> —have also been undertaken. These RL agents are trained in an open ended task space (XLand 2.0) which require different skill sets such as experimentation, tool use or division of labor. If language-based foundation models are general-purpose text generators, then the AdA model could conceivably be viewed as a relatively more general-purpose task follower compared to other models observed thus far.</p><p> However, this paradigm also carries inherent risks, namely the emergence of capabilities and homogenization.</p><ul><li> <strong>Homogenization</strong> : Since an increasing number of models are becoming merely &quot;fine-tuned&quot; versions of foundation models, it follows that downstream AI systems might inherit the same problematic biases prevalent in a few foundation models. Thus, all failure categories present in the base model could potentially percolate through all models trained with this as the foundation.</li><li> <strong>Emergence</strong> : Homogenization could potentially provide enormous gains for many domains, but aggressive homogenization of capabilities might result in unexpected and unexplainable behavior arising as a function of scale. Emergence implies that a system&#39;s behavior is implicitly induced rather than explicitly constructed. These characteristics render the models challenging to understand. They also give rise to unforeseen failure modes and unanticipated consequences. This phenomenon is talked about in more detail below.</li></ul><h1> 2.0: Leveraging Computation</h1><p> Although fine-tuning and transfer learning are the mechanisms that render foundation models feasible, it is scale that makes them truly powerful. This section delves into the concept of model scaling and the leveraging of computation—advancements in computer hardware (eg, GPU throughput and memory), new architectures (eg the transformer), and the availability of increasing amounts of training data.</p><h2> 2.1: The Bitter Lesson</h2><blockquote><p> <i>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. … [The bitter lesson teaches us] the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great.</i> - Sutton, Rich (March 2019) “ <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>The Bitter Lesson</u></a> ”</p></blockquote><p> Traditionally, AI research has predominantly designed systems under the assumption that a fixed amount of computing power will be available to the designed agent. However, over time, computing power so far has been expanding in line with <a href="https://en.wikipedia.org/wiki/Moore's_law"><u>Moore&#39;s law</u></a> (number of transistors in an integrated circuit doubles every 1.5 years). Consequently, researchers can either leverage their human knowledge of the domain or exploit increases in general-purpose computational methods. Theoretically, the two are mutually compatible, but in practice, the human-knowledge approach tends to complicate methods, rendering them less suited to harnessing general methods that leverage computation.</p><p> Several instances in history underscore this bitter lesson for AI researchers:</p><ul><li> <strong>Games</strong> : <a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"><u>Deep Blue</u></a> defeated chess world champion Garry Kasparov by leveraging a vast deep search, disheartening computer-chess researchers who had pursued methods that capitalized on the human understanding of chess&#39;s unique structure. Similarly, <a href="https://en.wikipedia.org/wiki/AlphaGo"><u>AlphaGo</u></a> triumphed over <a href="https://en.wikipedia.org/wiki/Go_(game)"><u>Go</u></a> world champion Lee Sedol using deep learning combined with a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"><u>Monte Carlo tree search</u></a> for move selection, eschewing human-engineered Go techniques. Within a year, <a href="https://en.wikipedia.org/wiki/AlphaZero"><u>AlphaZero</u></a> , forsaking any human-generated Go data, used <a href="https://en.wikipedia.org/wiki/Self-play_(reinforcement_learning_technique)"><u>self-play</u></a> to defeat AlphaGo. None of these successive enhancements in game-playing capabilities hinged on any fundamental breakthroughs in human Go knowledge.</li><li> <strong>Vision</strong> : A similar pattern has unfolded in computer vision. Earlier methods employed human-engineered features and <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)"><u>convolution kernels</u></a> for image recognition tasks. However, over the years, it has been determined that leveraging more computation and permitting <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"><u>convolutional neural nets (CNNs)</u></a> to learn their own features yield superior performance.</li><li> <strong>Language &amp; Speech</strong> : In 1970, the DARPA SUR (Speech Understanding Research) was held. One faction endeavored to leverage expert knowledge of words, phonemes, the human vocal tract, etc. In contrast, the other side employed newer, more statistical methods that necessitated considerably more computation, based on hidden Markov models (HMMs). This example shows yet again, that the statistical methods surpassed the human-knowledge-based methods. Since then, deep learning recurrent neural network-based or transformer-based methods have virtually dominated the field of sequence-based tasks.</li></ul><p> Historically, due to repeated reminders of the bitter lesson, the field of AI has increasingly learned to favor general-purpose methods of search and learning. This trend fortifies the intuition behind the immense scale of the current foundation model paradigm. It can be projected that the capabilities of the current foundation models will continue to scale commensurately with increasing computation. The reasons for this claim are presented in the following sections. The immediately ensuing section delves into these trends of scale in compute, dataset size, and parameter count.</p><h2> 2.2: Compute trends</h2><p> Several key factors dictate the relationship between the scale and capability of current ML models:</p><ul><li> <strong>Compute</strong> : Extended training runs (measured in epochs) generally result in lower <a href="https://www.alignmentforum.org/posts/jnmG5jczvWbeRPcvG/four-usages-of-loss-in-ai"><u>loss</u></a> . The total computational power needed partially depends on the training duration. ML engineers typically aim for asymptotically diminishing returns before halting the training process.</li><li> <strong>Dataset size</strong> : The larger the training dataset, the more information the model can analyze during each training run. As a result, training runs are generally longer, which in turn increases the total computational power needed before the model can be deemed &quot;trained.&quot;</li><li> <strong>Parameter Count</strong> : For each training example, the model needs to calculate the loss and then use backpropagation to update all relevant parameters. The more parameters the model has, the more computation-intensive this process becomes.</li></ul><p> Below is a chart illustrating the impact of each of these three factors on model loss. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/helgihhtclgshu0skvfd"></p><p> Source: Kaplan, Jared et. al. (Jan 2020) “ <a href="https://arxiv.org/abs/2001.08361"><u>Scaling Laws for Neural Language Models</u></a> ”</p><p> With graphical processing units (GPUs), and tensor processing units (TPUs) improving in performance and reducing in cost annually, AI models are demonstrating increasingly impressive results. This leads to higher acceptance of substantial compute costs. The reduced cost of computation, coupled with the paradigm of foundation models trained on escalating volumes of data, suggests that all three variables—compute, dataset size, and parameter count—will continue to expand in the forthcoming years. However, it remains an open question whether merely scaling these factors will result in unmanageable capabilities.</p><p> The following example offers a tangible illustration of capabilities escalating with an increasing parameter count in image generation models. The same model architecture ( <a href="https://parti.research.google/"><u>Parti</u></a> ) is used to generate an image using an identical prompt, with the sole difference between the models being the parameter size.</p><figure class="table"><table><tbody><tr><td style="vertical-align:top"><p> 350M</p></td><td style="vertical-align:top"><p> 750M</p></td><td style="vertical-align:top"><p> 3B</p></td><td style="vertical-align:top"><p> 20B </p></td></tr><tr><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/v6o0nzfkmeu1y99qvbna"></figure></td><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/c5yfkxw63nywxrmbh3lw"></figure></td><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/rjfdf0yatxhikiwkpff7"></figure></td><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/tmgn06qmja88qwnifqhu"></figure></td></tr><tr><td style="vertical-align:top" colspan="4"><p> Prompt: <i>A portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign on the chest that says Welcome Friends!</i></p></td></tr></tbody></table></figure><p> Source: GoogleAI (2022) , &quot; <a href="https://parti.research.google/"><i><u>Parti (Pathways Autoregressive Text-to-Image model)</u></i></a> &quot;</p><p> Increased numbers of parameters not only enhance image quality but also aid the network in generalizing in various ways. More parameters enable the model to generate accurate representations of complex elements, such as hands and text, which are notoriously challenging. There are noticeable leaps in quality, and somewhere between 3 billion and 20 billion parameters, the model acquires the ability to spell words correctly. Parti is the first model with the ability to spell correctly. Before Parti, <a href="https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do"><u>it was uncertain</u></a> if such an ability could be obtained merely through scaling, but it is now evident that spelling correctly is another capability gained simply by leveraging scale.</p><p> The following section briefly introduces efforts by both OpenAI and DeepMind to formalize the relationships between scale and capabilities.</p><h2> 2.3: Scaling Laws</h2><p> <i>Scaling laws articulate the relationship between compute, dataset size, parameter count, and model capabilities. They&#39;re employed to scale models effectively and optimally allocate resources with respect to capabilities.</i></p><p> Training large foundation models like GPT is expensive. When potentially millions of dollars are invested in training AI models, developers need to ensure that funds are efficiently allocated. Developers need to decide on an appropriate resource allocation between - model size, training time, and dataset size. OpenAI developed the first generation of formal neural scaling laws in their 2020 paper “ <a href="https://arxiv.org/abs/2001.08361"><i><u>Scaling Laws for Neural Language Models</u></i></a> ”, moving away from reliance on experience and intuition.</p><p> To determine such relationships some elements are held fixed while others are varied. As an example data can be kept constant, while parameter count and training time are varied, or parameter count is kept constant and data amounts are varied, etc… This allows a measurement of the relative contribution of each towards overall performance. Such experiments allow the development of concrete relationships that OpenAI called scaling laws.</p><p> These scaling laws guided decisions on trade-offs, such as: Should a developer invest in a license to train on Stack Overflow&#39;s data, or should they invest in more GPUs? Would it be efficient if they continue to cover the extra electricity costs incurred by longer model training? If access to compute increases tenfold, how many parameters should be added to the model for optimal use of GPUs? For sizable language models like GPT-3, these trade-offs might resemble choosing between training a 20-billion parameter model on 40% of an internet archive or a 200-billion parameter model on just 4% of the same archive.</p><p> The paper presented several scaling laws. One scaling law compares model shape and model size, and found that performance correlates strongly with scale and weakly with architectural hyperparameters of model shape such as depth vs. width.</p><p> Another law compared the relative performance contribution of the different factors of scale - data, training steps, and parameter count. They found that larger language models tend to be more sample efficient, meaning they can achieve better performance with less data. The following graph shows this relationship between relative contributions of different factors in scaling models. The graph indicates that for optimally compute-efficient training “ <i>most of the increase should go towards increased model size. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to increase parallelism through larger batch sizes, with only a very small increase in serial training time required.”</i> As an example, according to OpenAI&#39;s results if you get 10x more compute, you increase your model size by about 5x and your data size by about 2x. Another 10x in compute, and model size is 25x bigger and data size is only 4x bigger. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/zymupfwgncvce0ycmfdm"></p><p> Source: Kaplan, Jared et. al. (Jan 2020) “ <a href="https://arxiv.org/abs/2001.08361"><u>Scaling Laws for Neural Language Models</u></a> ”</p><p> Over the following few years, researchers and institutions utilized these findings to focus on engineering larger models rather than training smaller models on larger datasets. The following table and graph illustrate the trend change in machine learning models&#39; parameter growth. Note the increase to half a trillion parameters with constant training data.</p><figure class="table"><table><tbody><tr><td style="vertical-align:top"><p> model</p></td><td style="vertical-align:top"><p> year</p></td><td style="vertical-align:top"><p> size (#parameters)</p></td><td style="vertical-align:top"><p> data (#training tokens)</p></td></tr><tr><td style="vertical-align:top"><p> LaMDA</p></td><td style="vertical-align:top"><p> 2021</p></td><td style="vertical-align:top"><p> 137 billion</p></td><td style="vertical-align:top"><p> 168 billion</p></td></tr><tr><td style="vertical-align:top"><p> GPT-3</p></td><td style="vertical-align:top"><p> 2020</p></td><td style="vertical-align:top"><p> 174 billion</p></td><td style="vertical-align:top"><p> 300 billion</p></td></tr><tr><td style="vertical-align:top"><p> Jurassic</p></td><td style="vertical-align:top"><p> 2021</p></td><td style="vertical-align:top"><p> 178 billion</p></td><td style="vertical-align:top"><p> 300 billion</p></td></tr><tr><td style="vertical-align:top"><p> Gopher</p></td><td style="vertical-align:top"><p> 2021</p></td><td style="vertical-align:top"><p> 280 billion</p></td><td style="vertical-align:top"><p> 300 billion</p></td></tr><tr><td style="vertical-align:top"><p> MT-NLG 530B</p></td><td style="vertical-align:top"><p> 2022</p></td><td style="vertical-align:top"><p> 530 billion</p></td><td style="vertical-align:top"><p> 270 billion </p></td></tr></tbody></table></figure><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/ew4kaagfp0leegqa1ti3"></p><p> Source: Villalobos, Pablo et. al. (Jul 2022) “ <a href="https://arxiv.org/abs/2207.02852"><u>Machine Learning Model Sizes and the Parameter Gap</u></a> ”</p><p> In 2022, DeepMind provided an update to these scaling laws by publishing a paper called “ <a href="https://arxiv.org/abs/2203.15556"><u>Training Compute-Optimal Large Language Models</u></a> ”. They choose 9 different quantities of compute, ranging from about 10^18 FLOPs to 10^21 FLOPs. They hold the compute fixed at these amounts, and then for each quantity of compute, they train many different-sized models. Because the quantity of compute is constant for each level, the smaller models are trained for more time and the larger models for less. Based on their research DeepMind concluded that for every increase in compute, you should increase data size and model size by approximately the <i>same amount</i> . If you get a 10x increase in compute, you should make your model 3.1x times bigger and the data you train over 3.1x bigger; if you get a 100x increase in compute, you should make your model 10x bigger and your data 10x bigger.</p><p> To validate this law, DeepMind trained a 70-billion parameter model (&quot;Chinchilla&quot;) using the same compute as had been used for the 280-billion parameter model Gopher. That is, the smaller Chinchilla was trained with 1.4 trillion tokens, whereas the larger Gopher was only trained with 300 billion tokens. As predicted by the new scaling laws, Chinchilla surpasses Gopher in almost every metric.</p><p> Such finding have led to the formulation of a scaling hypothesis:</p><blockquote><p> <i>The strong scaling hypothesis is that, once we find a scalable architecture like self-attention or convolutions, which like the brain can be applied fairly uniformly (eg.</i> <a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine"><i><u>“The Brain as a Universal Learning Machine”</u></i></a> <i>or Hawkins), we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks &amp; data. More powerful NNs are &#39;just&#39; scaled-up weak NNs, in much the same way that human brains look much like scaled-up primate brains.</i> - Gwern (2022) “ <a href="https://gwern.net/scaling-hypothesis#scaling-hypothesis"><i><u>The Scaling Hypothesis</u></i></a> ”</p></blockquote><p> Current projections are that “ <i>the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images).</i> ” - Villalobos, Pablo et. al. (Oct 2022) “ <a href="https://arxiv.org/abs/2211.04325"><i><u>Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning</u></i></a> ” So in conclusion, we can anticipate that models will continue to scale in the near future. Increased scale combined with the increasingly general-purpose nature of foundation models could potentially lead to a sustained growth in general-purpose AI capabilities. The following section explores different AI capability thresholds that we might observe if the current trends persist.</p><h1> 3.0: Capabilities</h1><p> This section continues the discussion around increasing AI capabilities. It focuses in particular on certain thresholds that we might reach in the cognitive capabilities of these AI models. This flows into a discussion around how certain thresholds when once achieved might result in an intelligence explosion.</p><p> Capabilities refer to the overall ability of an AI system to solve or perform tasks in specific domains. It is a measure of how well the system can achieve its intended objectives and the extent of its cognitive power. Evaluating capabilities involves assessing the system&#39;s performance in specific domains, taking into account factors such as available computational resources and performance metrics. A possible element of confusion might be between capabilities and good performance on certain benchmarks. Benchmark performance refers to the performance of an AI system on specific tasks or datasets. These are designed to evaluate the system&#39;s performance on well-defined tasks and provide a standardized way to compare different AI models. Benchmark performance can be used as a proxy to assess the system&#39;s capabilities in certain domains, but it may not capture the full extent of the system&#39;s overall capabilities.</p><h2> 3.1: Capabilities vs. Intelligence</h2><ul><li> Krakovna, Victoria (Aug 2023) “ <a href="https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not"><i><u>When discussing AI risks, talk about capabilities, not intelligence</u></i></a> ”</li></ul><p> It is worth noting that <a href="https://www.youtube.com/watch?v=144uOfr4SYA"><u>public discussions</u></a> about catastrophic risks from general AI systems are often derailed by using the word “intelligence”. People often have different definitions of intelligence, or associate it with concepts like consciousness that are not relevant to AI risks, or dismiss the risks because intelligence is not well-defined. This is why using the term “capabilities” or “competence” instead of “intelligence” when discussing catastrophic risks from AI is often better since this is what the concerns are really about. For example, instead of “superintelligence” we can refer to “super-competence” or “superhuman capabilities”.</p><p> There are various issues with the word “intelligence” that make it less suitable than “capabilities” for discussing risks from general AI systems:</p><ul><li> <strong>Anthropomorphism</strong> : people often specifically associate “intelligence” with being human, being conscious, being alive, or having human-like emotions (none of which are relevant to or a prerequisite for risks posed by general AI systems).</li><li> <strong>Associations with harmful beliefs and ideologies.</strong></li><li> <strong>Moving goalposts</strong> : impressive achievements in AI are often dismissed as not indicating “true intelligence” or “real understanding” (eg the <a href="https://en.wikipedia.org/wiki/Stochastic_parrot"><u>“stochastic parrots”</u></a> argument). Catastrophic risk concerns are based on what the AI system can do, not whether it has “real understanding” of language or the world.</li><li> <strong>Stronger associations with less risky capabilities</strong> : people are more likely to associate “intelligence” with being really good at math than being really good at politics, while the latter may be more representative of capabilities that make general AI systems pose a risk (eg manipulation and deception capabilities that could enable the system to overpower humans).</li><li> <strong>High level of abstraction</strong> : “intelligence” can take on the quality of a mythical ideal that can&#39;t be met by an actual AI system, while “competence” is more conducive to being specific about the capability level in question.</li></ul><p> That being said, since the history of conversation on AI risks often does involve the words “intelligence” the following section starts by giving a quick overview of a myriad of definitions that are commonly used in the field.</p><h2> 3.2: Definitions of advanced AI Systems</h2><p> This section explores various definitions of different AI capability thresholds. The following list encompasses some of the most frequently used terms:</p><p> <strong>Intelligence</strong> : Intelligence measures an agent&#39;s ability to achieve goals in a wide range of environments. - Legg, Shane; Hutter, Marcus; (Dec 2007) “ <a href="https://arxiv.org/abs/0712.3329"><i><u>Universal Intelligence: A Definition of Machine Intelligence</u></i></a> ”</p><p> <strong>Artificial Narrow Intelligence (ANI)</strong> : A term designating artificial intelligence systems that are tailored to handle a single or a limited task. These systems are &#39;narrow&#39; because they tend to be superhuman at a very specific task domain.</p><p> <strong>Transformative AI (TAI):</strong> Refers to potential future AI that triggers a transition equivalent to, or more significant than, the agricultural or industrial revolution. This term aims to be more inclusive, acknowledging the possibility of AI systems that qualify as &quot;transformative,&quot; despite lacking many abilities that humans possess. - Karnofsky, Holden; (May 2016) &quot; <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/"><i><u>Some Background on Our Views Regarding Advanced Artificial Intelligence</u></i></a> &quot;</p><p> <strong>Human-Level AI (HLAI)</strong> : Encompasses AIs that can solve the majority of <a href="https://aiimpacts.org/human-level-ai/"><u>cognitive problems an average human can solve</u></a> . This concept contrasts with current AI, which is vastly superhuman at certain tasks while weaker at others.</p><p> <strong>Artificial General Intelligence (AGI)</strong> : Refers to AIs that can apply their intelligence to a similarly extensive range of domains as humans. These AIs do not need to perform all tasks; they merely need to be capable enough to invent tools to facilitate the completion of tasks. Much like how humans are not perfectly capable in all domains but can invent tools to make problems in all domains easier to solve.</p><p> AGI often gets described as the ability to achieve complex goals in complex environments using limited computational resources. This includes efficient cross-domain optimization and the ability to transfer learning from one domain to another. - Muehlhauser, Luke (Aug 2013) “ <a href="https://intelligence.org/2013/08/11/what-is-agi/"><i><u>What is AGI?</u></i></a> ”</p><p> <strong>Artificial Superintelligence (ASI)</strong> : “This is any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest&quot;. - Bostrom, Nick (2014) “ <i>Superintelligence</i> ”</p><p> Often, these terms get used as discrete capability thresholds; that is, individuals tend to categorize an AI as potentially an AGI, an ASI, or neither. However, it has been proposed that it might be more beneficial to view the capabilities of AI systems on a continuous scale rather than one involving discrete jumps. To this end, Richard Ngo proposed the (t,n)-AGI framework, which allows for a more formal definition of continuous AGI capabilities.</p><h2> 3.3: (t,n)-AGI</h2><ul><li> Ngo, Richard (May 2023) “ <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi"><u>Clarifying and predicting AGI</u></a> ”</li></ul><p> <i>A system receives the designation of &quot; <strong>t-AGI</strong> &quot; if it can surpass a human expert in a certain cognitive task within the timespan &#39;t&#39;. A system gets identified as <strong>(t,n)-AGI</strong> if it can outdo a group of &#39;n&#39; human experts working collectively on a set of cognitive tasks for the duration &#39;t&#39;.</i></p><p> For instance, if both a human expert and an AI receive one second to perform a task, the system would be labeled a &quot;one-second AGI&quot; if it accomplishes that cognitive task more effectively than the expert. Similarly, designations of one-minute, one-month, and so forth, AGIs could apply if their outputs surpass what human experts could achieve within a minute, month, and so on.</p><p> Richard Ngo makes further predictions regarding the types of capabilities in which an AI might surpass humans at different &#39;t&#39; thresholds.</p><ul><li> <strong>One-second AGI</strong> : Recognizing objects in images, determining whether sentences are grammatical, answering trivia.</li><li> <strong>One-minute AGI</strong> : Answering questions about short text passages or videos, common-sense reasoning (eg, <a href="https://twitter.com/ylecun/status/1639696127132835840"><u>Yann LeCun&#39;s gears problems</u></a> ), performing simple computer tasks (eg, using Photoshop to blur an image), looking up facts.</li><li> <strong>One-hour AGI</strong> : Completing problem sets/exams, composing short articles or blog posts, executing most tasks in white-collar jobs (eg, diagnosing patients, providing legal opinions), conducting therapy.</li><li> <strong>One-day AGI</strong> : Writing insightful essays, negotiating business deals, developing new apps, running scientific experiments, reviewing scientific papers, summarizing books.</li><li> <strong>One-month AGI</strong> : Carrying out medium-term plans coherently (eg, founding a startup), supervising large projects, becoming proficient in new fields, writing large software applications (eg, a new operating system), making novel scientific discoveries.</li><li> <strong>One-year AGI</strong> : These AIs would need to outdo humans in practically every area, given that most projects can be divided into sub-tasks that can be completed in shorter timeframes.</li></ul><p> As of the third quarter of 2023, existing systems are believed to qualify as one-second AGIs, and are considered to be nearing the level of one-minute AGIs. They might be a few years away from becoming one-hour AGIs. Within this framework, Richard Ngo anticipates superintelligence (ASI) to be something akin to a (one year, eight billion)-AGI, that is, an ASI would be an AGI that takes one year to outperform all eight billion humans coordinating on a given task.</p><p> Although AGI could be measured according to the proposed continuous framework, there might still be abrupt jumps in capabilities due to a phenomenon known as emergence. This topic gets explored in the subsequent section.</p><h2> 3.3: Formalizing Capabilities</h2><p> The following papers will be fully integrated in the future for a proper discussion on formalizing the concept of AGI capabilities. For now please refer directly to the sources:</p><ul><li> <strong>Situational Awareness</strong> : Owain Evans (Sep 2023) “ <a href="https://arxiv.org/abs/2309.00667"><u>Taken out of context: On measuring situational awareness in LLMs</u></a> ”</li><li> <strong>Power Seeking</strong> : Alexander Matt Turner (Jan 2023) “ <a href="https://arxiv.org/abs/1912.01683"><u>Optimal Policies Tend to Seek Power</u></a> ”</li></ul><h1> 4.0: Threat Models</h1><p> This section explores the question - Even if capabilities continue to increase as the previous sections forecast, why is that even a concern? As advancements in artificial intelligence (AI) continue, a critical analysis of their implications and potential risks becomes essential. The hypothesis suggesting catastrophic risk from general AI, as presented so far, consists of two key assertions:</p><p> First, global technological advancements are progressing towards the creation of generally capable AI systems within the forthcoming few decades. Second, these generally capable AI systems possess the potential to out compete or overpower humans.</p><p> The previous sections presented evidence supporting the first assertion. This section provides arguments for the second. It first explores why a machine intelligence might possess the capability to swiftly increase its cognitive abilities. Next, there is a discussion of why a machine intelligence might even have motivations to expand its capabilities. Finally, the section explores why it should not be taken for granted that a highly capable machine intelligence will be beneficial for humans by default.</p><h2> 4.1: Intelligence Explosion</h2><ul><li> Muehlhauser, Luke; Salamon, Anna (2012) “ <a href="https://intelligence.org/files/IE-EI.pdf"><u>Intelligence Explosion: Evidence and Import</u></a> ”</li></ul><p> <i>An &quot;intelligence explosion&quot; denotes a scenario where machine intelligence swiftly enhances its own cognitive capabilities, resulting in a substantial advancement in ability.</i></p><p> Muehlhauser and Salamon delve into the numerous advantages machine intelligence holds over human intelligence, which facilitate rapid intelligence augmentation. These include:</p><ul><li> <strong>Computational Resources:</strong> Human computational ability remains somewhat stationary, whereas machine computation possesses scalability.</li><li> <strong>Communication speed</strong> : Given the relatively low speed of neurons (only at 75 m/s), the human brain necessitates parallelized computation algorithms. Machines, on the other hand, operate on communications at the speed of light, which substantially augments prospects for sequential processes.</li><li> <strong>Duplicability</strong> : Machines exhibit effortless duplicability. Unlike humans they do not need birth, education, or training. While humans predominantly improve individually, machines have the potential to grow collectively.</li><li> <strong>Editability</strong> : Machines potentially allow more regulated variations. They exemplify the equivalent of direct brain enhancements via neurosurgery in opposition to laborious education or training requirements.</li><li> <strong>Goal coordination</strong> : Copied AIs possess the capability to share goals effortlessly, a feat challenging for humans.</li></ul><p> Here is a video explaining some of the concepts covered above. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=gP4ZNUHdwp8"><div><iframe src="https://www.youtube.com/embed/gP4ZNUHdwp8" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 4.2: Instrumental Convergence</h2><ul><li> Bostrom, Nick (2014) “ <a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>Superintelligence</u></a> ”</li></ul><p> The points mentioned earlier validate the potential of machine intelligence to enhance its cognitive capabilities. Nonetheless, an exploration into the motives behind such an ambition remains necessary. To illustrate, consider the relationship between levels of intelligence and the corresponding goals. This examination leads to one of two seminal theses offered by Nick Bostrom:</p><blockquote><p> <strong>Orthogonality Thesis</strong> : <i>Intelligence and final goals are orthogonal: more or less any level of intelligence could in principle be combined with more or less any final goal.</i> - Bostrom, Nick (2014) “ <a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>Superintelligence</u></a> ”</p></blockquote><p> This thesis implies that an AI system&#39;s objectives must be aligned explicitly with human virtues and interests, as no guarantee exists that an AI will automatically adopt or prioritize human values. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=hEUO6pjwFOo"><div><iframe src="https://www.youtube.com/embed/hEUO6pjwFOo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> The Orthogonality Thesis doesn&#39;t suggest compatibility of all agent designs with all goals, instead, it indicates the potential for at least one agent design for any combination of goals and intelligence level. Consequently, if any intelligent system can be paired with any goal, is it possible to hypothesize meaningfully about the type of goals future AI Systems might harbor?绝对地。 This leads to Bostrom&#39;s second thesis:</p><blockquote><p> <strong>Instrumental Convergence Thesis</strong> : <i>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent&#39;s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a  broad spectrum of situated intelligent agents.</i> - Bostrom, Nick (2014) “ <a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><u>Superintelligence</u></a> ”</p></blockquote><p> A terminal goal, also known as an &quot;intrinsic goal&quot; or &quot;intrinsic value”, is an objective that an agent appreciates for its own sake. On the other hand, an instrumental goal is pursued to increase the likelihood of achieving its terminal goals. Instrumental convergence encompasses the notion that certain instrumental values or goals could potentially be pursued by a broad array of intelligent agents, irrespective of their designated final goals. The Instrumental Convergence Thesis underscores potential hazards affiliated with sophisticated AI systems. It infers that even if an AI system&#39;s ultimate goal appears harmless, it could still embark on actions conflicting with human interests, owing to a convergence of several instrumental values such as resource acquisition and potential threats&#39; elimination. One can categorize self-preservation, goal-content integrity, cognitive enhancement, and resource acquisition as instrumentally convergent goals. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ZeecOKBus3Q"><div><iframe src="https://www.youtube.com/embed/ZeecOKBus3Q" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> In the wake of Bostrom&#39;s introduction of these theses in 2014, research has been undertaken to substantiate the existence of instrumentally convergent goals. &quot; <a href="https://arxiv.org/abs/1912.01683"><i><u>Optimal Policies Tend to Seek Power</u></i></a> &quot; a paper by Turner et al., provides research supporting the existence of instrumentally convergent goals  in modern machine learning systems.</p><p> Here are a couple of videos that delve deeper into the a problem called &quot;corrigibility&quot; (AI stop button problem) that arises due to instrumental convergence. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=4l7Is6vOAOA"><div><iframe src="https://www.youtube.com/embed/4l7Is6vOAOA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=3TYT1QfdfsM"><div><iframe src="https://www.youtube.com/embed/3TYT1QfdfsM" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> 4.3: Emergence</h2><p> <a href="https://en.wikipedia.org/wiki/Emergence"><i><strong><u>Emergent behavior</u></strong></i></a> <i>, or emergence, manifests when a system exhibits properties or behaviors that its individual components lack independently. These attributes may materialize only when the components comprising the system interact as an integrated whole, or when the quantity of parts crosses a particular threshold. Often, these characteristics appear &quot;all at once&quot;—beyond the threshold, the system&#39;s behavior undergoes a qualitative transformation.</i></p><p> The sections discussing computational trends and scaling laws showed the increasing scale of current foundational models. Numerous complex systems in nature exhibit qualitatively distinct behavior resulting from quantitative increases in scale. These properties, termed &#39;emergent&#39;, occur simultaneously. Examples of complex systems with such attributes in nature include:</p><ul><li> The heart: While individual cells cannot pump blood, the entirety of the heart can.</li><li> Uranium: Small quantities are mundane, but large amounts can initiate nuclear reactions.</li><li> Civilization: Individuals may seem ordinary, but through collective specialization (different individuals focusing on skills that are irrelevant in isolation), human civilization becomes possible. Another example of this is bees and ants. Ants have one of the smallest brains relative to their body, but ant colonies are capable of producing very complex behavior.</li></ul><p> In &quot; <a href="https://www.alignmentforum.org/s/4aARF2ZoBpFZAhbbe"><u>More Is Different for AI</u></a> &quot; Jacob Steinhardt provides additional examples of such complex systems. Steinhardt further conjectures that AI systems will manifest such emergent properties as a function of scale. Assuming that models persist in growing as per the scaling laws, an unexpected threshold may soon be crossed, resulting in unanticipated differences in behaviors and capabilities. Studying <a href="https://en.wikipedia.org/wiki/Complex_system"><u>complex systems</u></a> with emergent phenomena may assist in predicting what capabilities will emerge and when. In other words, there is a possibility of observing capability leaps between the thresholds (eg, a sudden leap from AI to HLAI to AGI) discussed in the preceding section, even if the models are simply scaled up.</p><p> Besides the factors already covered in the section on computational trends, the following elements also suggest that future ML systems will differ quantitatively from current models:</p><ul><li> Data Storage Capacities: A decrease in the cost to store one byte per dollar.</li><li> Few-Shot and Zero-Shot Learning: The capability to learn from fewer examples.</li><li> Grokking: Sudden improved generalization after extended periods of training.</li></ul><p> This further implies that these future models have the potential to manifest emergent behavior that could be qualitatively distinct from what is observed today. In the paper “ <a href="https://arxiv.org/abs/2305.15324"><u>Model evaluation for extreme risks</u></a> ” DeepMind found that as AI progress has evolved, general-purpose AI systems have often exhibited new and unpredictable capabilities – including harmful ones that their developers did not anticipate. Future systems may reveal even more perilous emergent capabilities, such as the potential to conduct offensive cyber operations, manipulate individuals through conversation, or provide actionable instructions for carrying out acts of terrorism.</p><h2> 4.4: Four Background Claims</h2><ul><li> Soares, Nate (July 2015) “ <a href="https://intelligence.org/2015/07/24/four-background-claims/"><i><u>Four Background Claims</u></i></a> ”</li></ul><p> In the concluding part of this section, we will delve into four essential claims that lay the groundwork for the concerns associated with ASI as put forth by MIRI.</p><p> <i><u>Claim 1: Humans Exhibit General Intelligence</u></i></p><p> Humans are capable of solving an array of problems across various domains, demonstrating their general intelligence. The importance of this claim lies in the fact that this form of general intelligence has led humans to become the dominant species on Earth.</p><p> <i><u>Claim 2: AI Systems Could Surpass Human Intelligence</u></i></p><p> While it remains uncertain when machines might attain superior intelligence to humans, it is conceivable that they have the potential to do so. Considering the brief evolutionary period between chimpanzees and generally intelligent humans, we can conclude that human intelligence is not incomprehensibly complex, suggesting we will eventually comprehend and replicate it.</p><p> Man-made machines consistently outperform their biological counterparts (cars vs. horses, planes vs. birds, etc.). Thus, it is rational to assume that just as birds are not the pinnacle of flight, humans are not the apex of intelligence. Therefore, it is plausible to foresee a future where machines out-think humans.</p><p> <i><u>Claim 3: Highly Intelligent AI Systems Will Shape the Future</u></i></p><p> Historically, confrontations between human groups have often culminated with the technologically superior faction dominating its competitor. Numerous reasons suggest that an AI system could attain a higher intelligence level than humans, thereby enabling it to intellectually outsmart or socially manipulate humans. Consequently, if we care about our future, it is prudent to study the processes that could significantly influence the direction of future events.</p><p> <i><u>Claim 4: Highly Intelligent AI Systems Will Not Be Beneficial by Default</u></i></p><p> Though a sufficiently intelligent AI may comprehend human desires, this does not inherently mean it will act in accordance with them. Moreover, even if an AI executes the tasks as we&#39;ve programmed it to — with precision and adherence to instructions — most human values can lead to undesirable consequences when interpreted literally. For example, an AI programmed to cure cancer could resort to kidnapping and experimenting on humans.</p><p> This claim is critical as it indicates that merely enhancing the ability of AI systems to understand our goals is not sufficient. The systems must also have a desire to act in accordance with our goals. This also underscores the importance of studying and formalizing human goals such that the intentions behind them can be properly communicated.</p><p></p><h1> 5.0: Timelines &amp; Forecasting</h1><p> The previous sections have illustrated that capabilities will likely continue to increase, potentially leading to capability jumps due to phenomena such as emergence and intelligence explosion. This final section of the chapter investigates AI timeline forecasts and takeoff dynamics. AI timeline forecasts entail discussing when researchers/forecasters expect various milestones in AI development to be achieved. This includes for example various benchmarks of progress, the emergence of mouse-level intelligence, and the manifestation of human-like qualities in AI, such as external tool use and long-term planning. The next section shares the insights of researchers who have gathered evidence from domain experts regarding when these capability thresholds might be reached.</p><p> Anchors in forecasting refer to reference classes or frameworks that are used to make predictions about future events or systems. These anchors serve as points of comparison or analogy that help inform our understanding and reasoning about the future. There are several common anchors used to inform predictions about the development and capabilities of future AI systems. These anchors provide reference points and frameworks for reasoning about AI progress. Some of the most common anchors include:</p><ul><li> <strong>Current ML Systems</strong> : The current state of machine learning systems serves as a starting point for forecasting future AI capabilities. By examining the strengths and limitations of existing ML systems, researchers can make educated guesses about the trajectory of AI development.</li><li> <strong>Human Anchors</strong> : Anchors based on human abilities and characteristics are often used in AI forecasting. These include areas where humans excel compared to current ML systems, such as mastery of external tools, efficient learning, and long-term planning.</li><li> <strong>Biological Anchors</strong> : Biological anchors draw inspiration from biological systems, particularly the human brain, to estimate the computational requirements of future AI systems. These anchors consider factors such as the neural network anchor, which estimates the compute-equivalent used in the human brain, and the human lifetime anchor, which estimates the compute-equivalent involved in training a human brain from birth to adulthood.</li><li> <strong>Thought Experiments</strong> : Thought experiments provide a third anchor by imagining hypothetical scenarios and reasoning through their implications. These experiments help explore the potential behavior and characteristics of future AI systems.</li></ul><p> It&#39;s important to note that the choice and weighting of anchors can vary depending on the specific forecasting approach and the context of the predictions being made. Different researchers may emphasize different anchors based on their assumptions and perspectives. This book will only explore the biological anchor in further detail.</p><h2> 5.1: Biological Anchors</h2><p> Biological anchors are a set of reference points or estimates used in forecasting the development of transformative AI systems. These anchors are inspired by biological systems, particularly the human brain, and provide a basis for estimating the computational requirements of AI systems capable of performing transformative tasks. The <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>draft report on AI timelines</u></a> in Sep 2020 by Ajeya Cotra details the methodology used, addressing several questions:</p><ol><li> <strong>How much computation does the human brain perform?</strong> According to the report, the brain has an estimated 10^15 total synapses. With each synapse spiking approximately once per second and each spike representing roughly one floating point operation (FLOP), the brain&#39;s computational power is estimated to range between 10^13 and 10^17 FLOP/S. This variance takes into account the potential efficiency differences between human brains and computers.</li><li> <strong>How much training would it take to replicate this much inferential computation?</strong> It&#39;s important to note that training computation differs from inferential computation. Modern ML practices involve training advanced models on massive supercomputers and running them on medium-sized ones. An adult human&#39;s inference computation power is estimated at ~10^16 FLOP/S. To account for the total training computation costs, the report multiplies the computation of a single brain (10^15 FLOP/S) by the time taken from childhood to adolescence (10^9 seconds). This gives a lower bound estimate of 10^24 FLOP. However, the estimate increases to 10^41 FLOP when also considering the training data ingrained in our biology through evolution.</li><li> <strong>How can we adjust this computation estimate for algorithmic progress?</strong> An algorithmic efficiency report suggests the training efficiency for Neural Networks doubles every sixteen months. Cotra proposes a slightly longer doubling time of 2-3 years.</li><li> <strong>How much money does this amount of computation cost?</strong> In 2020, computational resources were priced at $1 for 10^17 FLOP/S, implying that 10^33 FLOP/S would cost $10^16 (ten quadrillion dollars). This cost decreases annually, with some versions of Moore&#39;s Law suggesting that compute costs halve every eighteen months. As a result, training costs (in FLOP/S) will reduce over time due to algorithmic progress, and the cost of FLOP/S (in dollars) will also decrease due to hardware advancements.</li><li> <strong>What year does this computational cost become reasonable?</strong> The median result is a 10% chance by 2031, a 50% chance by 2052, and an 80% chance by 2100.</li></ol><p> Cotra acknowledges potential limitations with this approach, such as the assumption that progress relies on an easily-measured quantity (FLOP/S) rather than on fundamental advances, like new algorithms. Therefore, even with affordable, abundant computation, if we lack the algorithmic knowledge to create a proper thinking machine, any resulting AI might not display human level or superintelligent capabilities.</p><p> The following graph gives an overview of the findings. Overall, the graph takes a weighted average of the different ways that the trajectory could flow. This gives us an estimate of a >;10% chance of transformative AI by 2036, a ~50% chance by 2055, and an ~80% chance by 2100.</p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/enuzi8vbiou15dmrj7ik"></strong> Source: Holden Karnofsky (2021) &quot; <a href="https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method"><u>Forecasting transformative AI: the &quot;biological anchors&quot; method in a nutshell</u></a> &quot;</p><p> In 2022 a <a href="https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines"><u>two-year update</u></a> on the author&#39;s (Ajeya Cotra) timelines was published. The updated timelines for TAI are ~15% probability by 2030, ~35% probability by 2036, a median of ~2040, and a ~60% probability by 2050.</p><p> It&#39;s important to note that while the biological anchor is a valuable model, it is not universally accepted as the primary predictive tool among all ML scientists or alignment researchers. As the statistical aphorism goes: &quot;All models are wrong, but some are useful&quot;. Biological anchors represent just one model, and other anchors should be considered when forming your own views on AI capabilities and the timeline for their emergence.</p><p></p><h1> <strong>6.0: Takeoff Dynamics</strong></h1><p> Takeoff dynamics primarily delve into the implications of the evolution of powerful artificial intelligence on the world. These definitions sketch different trajectories the world could follow as transformative AI emerges. <span class="footnote-reference" role="doc-noteref" id="fnref9d83odnkksd"><sup><a href="#fn9d83odnkksd">[1]</a></sup></span></p><p> While timelines address when certain capabilities may emerge, takeoff dynamics explore what happens after these features surface. This chapter concludes with a section discussing various researchers&#39; perspectives on the potential trajectories of an intelligence explosion, considering factors such as takeoff speed, continuity, and homogeneity. This includes a discussion of - first, the pace and continuity of an intelligence explosion, and second, whether multiple AIs will coexist, each having different objectives, or whether they will eventually converge towards a single superintelligent entity.</p><h2> 6.1: Speed/Continuity</h2><p> Both AI takeoff speed and AI takeoff continuity describe the trajectory of AI development. Takeoff speed refers to the rate at which AI progresses or advances. Generally, takeoff continuity refers to the smoothness or lack of sudden jumps in AI development. Continuous takeoff means that the capabilities trajectory aligns with the expected progress based on past trends, while discontinuous takeoff refers to a trajectory that significantly exceeds the expected progress. FOOM is one type of fast takeoff scenario, and refers to a hypothetical scenario in which artificial intelligence (AI) rapidly and explosively surpasses human intelligence and capabilities.</p><p> The terms &quot;slow takeoff&quot; and &quot;soft takeoff&quot; are often used interchangeably, and similarly &quot;fast takeoff&quot; and &quot;hard takeoff&quot; and “FOOM” are also often used interchangeably. It&#39;s important to note that the definitions and implications of takeoff speed and takeoff continuity are still subjects of debate and may not be universally agreed upon by researchers in the field. Here are perspectives:</p><p> <i><u>Slow/Soft takeoff</u></i></p><p> When discussing takeoff speeds, Paul Christiano emphasizes the <a href="https://sideways-view.com/2018/02/24/takeoff-speeds/"><u>parallel growth of AI capabilities and productivity</u></a> . He expects a slow takeoff in the development of AGI based on his characterization of takeoff speeds and his analysis of economic growth rates. He defines slow takeoff as a scenario where there will be a complete interval of several years in which world economic output doubles before the first interval of one year in which world economic output doubles. This definition emphasizes a gradual transition to higher growth rates. Overall, he postulates that the rise in AI capabilities will mirror an exponential growth pattern in the world GDP, resulting in a <a href="https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow"><u>continuous but moderate takeoff</u></a> .</p><p> In a similar vein, there have been <a href="https://www.lesswrong.com/posts/Mha5GA5BfWcpf2jHC/potential-bottlenecks-to-taking-over-the-world">some discussions</a> that the real obstacle to global domination is not the enhancement of cognitive abilities but more significant bottlenecks like avoiding coordinated human resistance and the physical acquisition and deployment of resources. These supply chain optimizations would increase productivity, hence GDP, which could serve as a measure for the speed of &quot;AI takeoff&quot;.</p><p> <i><u>Fast/Hard takeoff</u></i></p><p> A hard takeoff refers to a sudden, rather than gradual, transition to superintelligence, counter to the soft takeoff mentioned above. Eliezer Yudkowsky advocates for this view, suggesting a sudden and discontinuous change brought about by rapid self-improvement, while others, like Robin Hanson, support a more gradual, spread-out process. Yudkowsky argues that even regular improvement of AI by humans may cause significant leaps in capability to occur before recursive self-improvement begins.</p><p> Eliezer Yudkowsky also offers a counter to continuous takeoff proponents. He predicts a quick and abrupt &quot;intelligence explosion&quot;. This is because he rather doesn&#39;t expect AI to be integrated (quickly) enough into the economy for the GDP to increase significantly faster before FOOM. It is also possible that superintelligent AIs could mislead us about their capabilities, leading to lower-than-expected GDP growth. This would be followed by a sudden leap, or &quot;FOOM&quot;, when the AI acquires a substantial ability to influence the world, potentially overwhelming human technological and governance institutions.</p><p> These diverse views on takeoff speeds and continuity shape the strategies for AI safety, influencing how it should be pursued. Overall it is worth emphasizing that both fast and slow takeoffs are quite rapid (as in at most a few years). Here are some picture to help illustrate the differences: </p><figure class="table"><table><tbody><tr><td><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/ccbzsbjllbtswsw7wpm0"></figure></td><td><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/usodbaiascosfv8oalvw"></figure></td></tr><tr><td style="padding:5pt;vertical-align:top"> <strong>Slow (No Intelligence Explosion) continuous Takeoff</strong></td><td style="padding:5pt;vertical-align:top"> <strong>Slow (No Intelligence Explosion) discontinuous takeoff</strong> </td></tr></tbody></table></figure><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/u8smklx4wpeshsndxqb2"></figure></td><td style="padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YcFpJC5pJFdYdEuNN/jvvxezelezmlhanen9io"></figure></td></tr><tr><td style="padding:5pt;vertical-align:top"> <strong>Fast (Intelligence Explosion) Continuous Takeoff</strong></td><td style="padding:5pt;vertical-align:top"> <strong>Fast (Intelligence Explosion) discontinuous takeoff</strong></td></tr></tbody></table></figure><p><br></p><p></p><p><br> Source: Samuel Dylan Martin, Daniel_Eth (Sep 2021) “<a href="https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"><u>Takeoff Speeds and Discontinuities</u></a> ”</p><h2> 6.2: Homogeneity</h2><p> <i><strong>Homogeneity</strong> refers to the similarity among different AI systems in play during the development and deployment of advanced AI.</i></p><ul><li> Hubinger, Evan (Dec 2020) “ <a href="https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"><u>Homogeneity vs. heterogeneity in AI takeoff scenarios</u></a> ”</li></ul><p> In a homogenous scenario, AI systems are anticipated to be highly similar, even identical, in their alignment and construction. For example, if every deployed system depends on the same model behind a single API, or if a single foundational model is trained and then fine-tuned in different ways by different actors. Homogeneity in AI systems could simplify cooperation and coordination, given their structural similarities. It also signifies that the alignment of the first advanced AI system is crucial, as it will likely influence future AI systems. One key factor for homogeneity is the economic incentives surrounding AI development and deployment. As the cost of training AI systems is expected to be significantly higher than the cost of running them, it becomes more economically advantageous to use existing AI systems rather than training new ones from scratch. This creates a preference for reusing or fine-tuning existing AI systems, leading to a higher likelihood of homogeneity in the deployed AI landscape</p><h2> 6.3: Polarity</h2><p> <i><strong>Unipolar</strong> refers to a scenario where a single agent or organization dominates and controls the world, while <strong>multipolar</strong> refers to a scenario where multiple entities coexist with different goals and levels of cooperation.</i></p><p> AI homogeneity evaluates the alignment similarities among AI systems, AI polarity examines the coexistence of both aligned and misaligned AI systems in a given context.</p><p> We might expect a unipolar takeoff, where a single AI system or project gains a decisive strategic advantage, due to several reasons. One key factor is the potential for a rapid takeoff, characterized by a fast increase in AI capabilities. If one project achieves a significant lead in AI development and surpasses others in terms of capabilities, it can establish a dominant position before competitors have a chance to catch up. A rapid takeoff can facilitate a unipolar outcome by enabling the leading project to quickly deploy its advanced AI system and gain a monopoly on the technology. This monopoly can provide substantial economic advantages, such as windfall profits, which further solidify the leading project&#39;s power and influence. Additionally, the presence of network effects can contribute to a unipolar takeoff. If the leading AI system becomes widely adopted and integrated into various sectors, it can create positive feedback loops that reinforce its dominance and make it increasingly difficult for other projects to compete.</p><p> We might expect a multipolar takeoff, where multiple AI projects undergo takeoff concurrently, due to several reasons. One factor is the potential for a slower takeoff process, which allows for more projects to reach advanced stages of AI development. In a slow takeoff scenario, there is a greater likelihood of multiple projects undergoing the transition in parallel, without any single project gaining a decisive strategic advantage. Another reason is the possibility of shared innovations and tools among AI projects. If there is a significant level of collaboration and information sharing, it can lead to a more distributed landscape of AI capabilities, enabling multiple projects to progress simultaneously. Furthermore, the presence of non-competitive dynamics, such as cooperation and mutual scrutiny, can contribute to a multipolar takeoff. In a scenario where different AI projects recognize the importance of safety and alignment, they may be more inclined to work together and ensure that each project progresses in a responsible manner.</p><h1> Acknowledgements</h1><p> Thanks to Charbel-Raphaël Segerie, Jeanne Salle, Bogdan Ionut Cirstea, Nemo, Gurvan, and the many course participants of ML4G France, ML4G Germany, and AISF Sweden for helpful comments and feedback.</p><h1> Meta-Notes</h1><ul><li> The objective of the overall project is to write something that can be used as a introductory textbook to AI Safety. The rough audience is ML masters students, or audiences that have a semi technical background eg Physics and get them up to speed on the core arguments. The intent is not to cover literally every single argument under the sky. There can obviously be a 102 version of the text that covers more nuance. I am especially trying to account for reading time and keep it roughly in the range of 40-60 mins per chapter. Which means I have to often make decisions on what to include, and at what level of detail.</li><li> I consider this a work-in-progress project. After much encouragement by others, I decided to publish what I have so far to get further feedback and comments.</li><li> If there are any mistakes or I have misrepresented anyone&#39;s views please let me know. I will make sure to correct it. Feel free to suggest improvements to flow/content additions/deletions/etc... How do people feel about the embedded videos? Would it better with just text?</li><li> The two sections that will probably undergo the most change in the near future are:<ul><li> 3.3: Formalizing capabilities</li><li> 4.2: Instrumental Convergence: It has been mentioned that Orthogonality has not been explained thoroughly enough.</li><li> 6.0: Takeoff dynamics: Takeoff is always very tricky to talk about. I am not happy with the section as it stands. At the very least I want to include the report - Open Philanthropy (June 2023) “ <a href="https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/"><u>What a compute-centric framework says about takeoff speeds</u></a> ” and just overall refine the arguments being made with more sources.</li></ul></li><li> There is also <a href="https://docs.google.com/document/d/1Zb8ccNj-24dKtSyg6e56Irh15JV8uLRL9Ilq9akJmqw/edit?usp=sharing">a google docs version</a> in case you prefer to leave comments there.</li><li> The general structure of the overall book/sequence will follow AI Safety fundamentals, however, there have been significant changes and additions to individual chapters in terms of content added/deleted.</li><li> When large portions of a section are drawn from an individual paper/post the reference is placed directly under the title. The sections serve as summarizations of the post. If you wish you can directly refer to the original papers/posts as well. The intent was to provide a single coherent flow of arguments all in one place.</li></ul><h1> Sources</h1><ul><li> Ngo, Richard (Jan 2022) &quot; <a href="https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5"><u>Visualizing the deep learning revolution&quot;</u></a></li><li> Bommasani, Rishi et. al. (Jul 2022) &quot; <a href="https://arxiv.org/pdf/2108.07258.pdf"><u>On the Opportunities and Risks of Foundation Models</u></a> &quot;</li><li> <a href="https://hai.stanford.edu/news/reflections-foundation-models"><u>Reflections on Foundation Models</u></a></li><li> Adaptive Agent Team DeepMind (Jan 2023) “ <a href="https://arxiv.org/abs/2301.07608"><u>Human-Timescale Adaptation in an Open-Ended Task Space</u></a> ”</li><li> Lennart Heim (Sep 2021) &quot; <a href="https://www.lesswrong.com/posts/uYXAv6Audr2y4ytJe/what-is-compute-transformative-ai-and-compute-1-4"><u>What is Compute?</u></a> &quot;</li><li> Sutton, Rich (March 2019) “ <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>The Bitter Lesson</u></a> ”</li><li> Sevilla, Jaime (Feb 2022) &quot; <a href="https://arxiv.org/abs/2202.05924"><u>Compute Trends Across Three Eras of Machine Learning</u></a> &quot;</li><li> Villalobos, Pablo et. al. (Jul 2022) “ <a href="https://arxiv.org/abs/2207.02852"><u>Machine Learning Model Sizes and the Parameter Gap</u></a> ”</li><li> Villalobos, Pablo et. al. (Oct 2022) “ <a href="https://arxiv.org/abs/2211.04325"><u>Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning</u></a> ”</li><li> Villalobos, Pablo (Jan 2023) “ <a href="https://www.lesswrong.com/posts/6L9EhCa8Zo2GoThGB/scaling-laws-literature-review"><u>Scaling Laws Literature Review</u></a> ”</li><li> GoogleAI (2022) , &quot; <a href="https://parti.research.google/"><u>Parti (Pathways Autoregressive Text-to-Image model)</u></a> &quot;</li><li> Gwern (2022) “ <a href="https://gwern.net/scaling-hypothesis#scaling-hypothesis"><u>The Scaling Hypothesis</u></a> ”</li><li> Ngo, Richard  (Sep 2020) &quot; <a href="https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ"><u>AGI Safety from first principles</u></a> &quot;</li><li> Kaplan, Jared et. al. (Jan 2020) “ <a href="https://arxiv.org/abs/2001.08361"><u>Scaling Laws for Neural Language Models</u></a> ”</li><li> Hoffmann, Jordan et. al. (Mar 2022) “ <a href="https://arxiv.org/abs/2203.15556"><u>Training Compute-Optimal Large Language Models</u></a> ”</li><li> 1a3orn (Apr 2022) “ <a href="https://www.alignmentforum.org/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models"><u>New Scaling Laws for Large Language Models</u></a> ”</li><li> Legg, Shane; Hutter, Marcus (2007) “ <a href="https://arxiv.org/abs/0706.3639"><u>A Collection of Definitions of Intelligence</u></a> ”</li><li> Karnofsky, Holden (May 2016) “ <a href="https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/"><u>Some Background on Our Views Regarding Advanced Artificial Intelligence</u></a> ”</li><li> Muehlhauser, Luke (Aug 2013) “ <a href="https://intelligence.org/2013/08/11/what-is-agi/"><u>What is AGI?</u></a> ”</li><li> Tegmark Max (Aug 2017) “Life 3.0”</li><li> Ngo, Richard (May 2023) “ <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi"><u>Clarifying and predicting AGI</u></a> ”</li><li> Steinhardt, Jacob (Jan 2022) &quot; <a href="https://www.alignmentforum.org/s/4aARF2ZoBpFZAhbbe"><u>More Is Different for AI</u></a> &quot;</li><li> DeepMind (May 2023) “ <a href="https://arxiv.org/abs/2305.15324"><u>Model evaluation for extreme risks</u></a> ”</li><li> Muehlhauser, Luke; Salamon, Anna (Jan 2013) &quot; <a href="https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view"><u>Intelligence Explosion: Evidence and Import</u></a> &quot;</li><li> Soares, Nate (Jul 2015) &quot; <a href="https://intelligence.org/2015/07/24/four-background-claims/"><u>Four Background Claims</u></a> &quot;</li><li> Daniel_Eth, (Sep 2021) “ <a href="https://www.lesswrong.com/posts/amK9EqxALJXyd9Rb2/paths-to-high-level-machine-intelligence"><u>Paths To High-Level Machine Intelligence</u></a> ”</li><li> Cotra, Ajeya (Sep 2020) “ <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>Forecasting TAI with biological anchors”</u></a></li><li> Holden Karnofsky (2021) &quot; <a href="https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method"><u>Forecasting transformative AI: the &quot;biological anchors&quot; method in a nutshell</u></a> &quot;</li><li> Yudkowsky, Eliezer (Dec 2021) “ <a href="https://www.alignmentforum.org/posts/ax695frGJEzGxFBK4/biology-inspired-agi-timelines-the-trick-that-never-works"><u>Biology-Inspired AGI Timelines: The Trick That Never Works</u></a> ”</li><li> Hubinger, Evan (Dec 2020) “ <a href="https://www.alignmentforum.org/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios"><u>Homogeneity vs. heterogeneity in AI takeoff scenarios</u></a> ”</li><li> Wentworth, John (Jul 2021) “ <a href="https://www.lesswrong.com/posts/Mha5GA5BfWcpf2jHC/potential-bottlenecks-to-taking-over-the-world"><u>Potential Bottlenecks to Taking Over The World</u></a> ”</li><li> Yudkowsky, Eliezer; Shulman, Carl (Dec 2021) “ <a href="https://www.alignmentforum.org/posts/sCCdCLPN9E3YvdZhj/shulman-and-yudkowsky-on-ai-progress"><u>Shulman and Yudkowsky on AI progress</u></a> ”</li><li> Buck Shlegeris (Apr 2022) “ <a href="https://www.alignmentforum.org/posts/hRohhttbtpY3SHmmD/takeoff-speeds-have-a-huge-effect-on-what-it-means-to-work-1"><u>Takeoff speeds have a huge effect on what it means to work on AI x-risk</u></a> ”</li><li> Matthew Barnett (Feb 2020) “ <a href="https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff"><u>Distinguishing definitions of takeoff</u></a> ”</li><li> Barnett, Matthew (Oct 2019) “ <a href="https://www.alignmentforum.org/posts/CjW4axQDqLd2oDCGG/misconceptions-about-continuous-takeoff#Continuous_doesn_t_necessarily_mean_slow"><u>Misconceptions about continuous takeoff</u></a> ”</li><li> Christiano, Paul (Feb 2018) “ <a href="https://sideways-view.com/2018/02/24/takeoff-speeds/"><u>Takeoff speeds</u></a> ”</li><li> Samuel Dylan Martin, Daniel_Eth (Sep 2021) “<a href="https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities"><u>Takeoff Speeds and Discontinuities</u></a> ”</li><li> Stampy (2023) “ <a href="https://aisafety.info/"><u>AI Safety Info</u></a> ” </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn9d83odnkksd"> <span class="footnote-back-link"><sup><strong><a href="#fnref9d83odnkksd">^</a></strong></sup></span><div class="footnote-content"><p> Jaime Sevilla and Edu Roldán from epoch.ai have developed an <a href="https://epochai.org/blog/interactive-model-of-takeoff-speeds"><u>interactive website for understanding a new model of AI takeoff speeds</u></a> , if the reader wishes to try their own values and estimates.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/YcFpJC5pJFdYdEuNN/alignment-101-ch-1-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YcFpJC5pJFdYdEuNN/alignment-101-ch-1-agi<guid ispermalink="false"> YcFpJC5pJFdYdEuNN</guid><dc:creator><![CDATA[markov]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:38:57 GMT</pubDate> </item><item><title><![CDATA[2023 East Coast Rationalist Megameetup]]></title><description><![CDATA[Published on October 18, 2023 8:33 PM GMT<br/><br/><p> The NYC Secular Solstice this year will be at Ida Lang on December 9th. The East Coast Rationalist Megameetup will be at the Holiday Inn Brooklyn Downtown from the evening of December 8th to the morning of December 11th. Despite the name, people are welcome to come from anywhere- we usually have a couple of Californians and a European or two.</p><p> What is Secular Solstice? Solstice is a holiday designed by and for rationalists. On this night, we come together to sing songs about the distant past and the hoped for future. Designed as an attempt to match religious wintertime festivals without relaxing our dedication for the truth, it&#39;s grown to be a celebration held across the world, updated year to year as our knowledge of the world changes.</p><p> What is the Rationalist Megameetup? It started because many people would come from other cities to attend the NYC Solstice, and it was more efficient to find sleeping space and meeting space together. Usually it&#39;s three days of staying up late debating math and philosophy, full of games and in-jokes and meeting cool and interesting people. I happen to be especially fond of it, as the megameetup is how I first really met the in-person rationalist community. This year is a bit of an experiment, as we&#39;ve upgraded the venue due to a combination of feedback asking for more space and NYC becoming increasingly hard on large AirBnBs.</p><p> Tickets for Solstice, overnight space at the Megameetup, and passes for the Megameetup Day Events are all present at <a href="http://rationalistmegameetup.com">rationalistmegameetup.com</a> .</p><br/><br/> <a href="https://www.lesswrong.com/events/XtpK72GDgM5fQCgxY/2023-east-coast-rationalist-megameetup#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/XtpK72GDgM5fQCgxY/2023-east-coast-rationalist-megameetup<guid ispermalink="false"> XtpK72GDgM5fQCgxY</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 18 Oct 2023 20:33:49 GMT</pubDate></item></channel></rss>