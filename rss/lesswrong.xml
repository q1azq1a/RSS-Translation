<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 04:14:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Sensor Exposure can Compromise the Human Brain in the 2020s]]></title><description><![CDATA[Published on October 26, 2023 3:31 AM GMT<br/><br/><p><strong>概述</strong></p><p>20 世纪因心理学（一门研究人类心智的科学）的发现及其利用（例如大规模战争、宣传、广告、信息/混合战争、决策理论/相互确保毁灭）而发生了根本性的改变。</p><p>然而，我们有理由认为，如果人类思维的科学和开发比现在更加先进，那么 20 世纪将会发生进一步的转变。</p><p>我在这里认为，在一个大规模监视、美国、中国和俄罗斯之间的<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合</u></a>/认知战争以及机器学习取得重大进步的时代，我们有理由认为，SOTA 人类认知分析和利用的情况可能已经是威胁整个人工智能安全社区运行的连续性；如果不是现在，那么很可能会在 2020 年代的某个时候发生，届时全球范围内的事件可能会比人类在过去二十年所习惯的步伐要多得多。</p><p>人工智能将成为这些王国以及它们之间战争的关键，要求暂停开发可能是人类生存的最低要求，而对于这样的冲突， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point">我们甚至不知道是什么袭击了我们</a>。</p><p>对于一般人类生活来说，攻击面大得令人无法接受，更不用说对于人工智能安全社区来说了，这个社区是一个由一群书呆子组成的社区，他们偶然发现了宇宙这一面的命运所围绕的工程问题，一个绝对不能失败的社区。在 2020 年代幸存下来，也不会以缩小/捕获的形式跛行。</p><p><br></p><p><strong>这个问题是智能文明的基础</strong></p><p>如果有智慧的外星人，由一束束触手、水晶或植物组成，思考速度极其缓慢，他们的思想也会有可发现的功绩/零日，因为任何自然进化的思想都可能像人脑一样，是一堆意大利面条代码在其预期环境之外运行。</p><p>他们甚至可能不会开始触及发现和标记这些漏洞的表面，直到像今天的人类文明一样，他们开始用传感器包围数千或数百万同类，这些传感器可以每天几个小时记录行为并找到<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=of%20procuring%20results.-,There%20is%20no,-logical%20endpoint%20to">相关性网络</a>。</p><p>就人类而言，使用社交媒体作为自动化人工智能实验的受控环境似乎创造了人类行为数据的临界量。</p><p>社交媒体引导人类成果的能力并不是孤立地进步的，它们与人类思维的理解和利用的广泛加速并行，这<a href="https://arxiv.org/pdf/2309.15084.pdf"><u>本身就是加速人工智能能力研究的副产品</u></a>。</p><p>通过将人与其他人进行比较并预测特征和未来行为，多臂老虎机算法可以首先预测特定的操纵策略是否值得冒险实施；导致高成功率和低检测率（因为检测可能会产生高度可测量的响应，特别是在大量传感器暴露的情况下，例如未覆盖的网络摄像头，因为将人们的微表情与失败或暴露的操纵策略的案例或工作网络摄像头视频进行比较数据转化为基础模型）。</p><p>当您拥有数十亿小时的人类行为数据和传感器数据的样本量时，不同类型的人反应的毫秒差异（例如面部微表情、滚动过去涵盖不同概念的帖子时的毫秒差异、涵盖不同概念后的心率变化、眼球追踪）眼睛经过特定概念、触摸屏数据等后的差异从难以察觉的噪音转变为<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=There%20is%20no%20logical%20endpoint%20to%20the%20amount%20of%20data%20required%20by%20such%20systems...%20All%20information%20is%20potentially%20relevant%20because%20it"><u>相关网络</u></a>的基础。<a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>美国国家安全局在每个操作系统和可能的芯片固件中都储存了漏洞</u></a>，因此我们无法很好地估计收集了多少数据，任何试图获得良好估计的人都可能会失败。历史趋势是，存在大量恶意数据收集，而低估国家安全局的人每次都错了。此外，这篇文章详细介绍了一个非常有力的案例，即他们非常有动力去利用这些传感器。</p><p>即使目前收集的传感器数据还不足以危及人们的安全，但在 2020 年代或缓慢起飞的某个时刻，它可能会突然变得足够。</p><p>现代行为操纵范式的核心要素是能够尝试大量的事情并看看什么有效；不仅仅是暴力破解已知策略的变体以使其更有效，而是首先暴力破解新颖的操纵策略。这完全避免了导致重复危机的稀缺性和研究缺陷，而重复危机至今仍然是心理学研究的瓶颈。</p><p>社交媒体的个性化定位利用深度学习来产生一种像手套一样适合人类思维的体验，虽然我们无法完全理解，但它给黑客提供了令人难以置信的余地，让他们找到方法将人们的思维引向可测量的方向，只要这些方向是可测量的。 。人工智能甚至可以实现自动化。</p><p>事实上，人类文明中的原创心理学研究不再需要聪明、有洞察力的人来进行假设生成，这样你可以资助的有限研究就有希望找到有价值的东西。仅使用当前的社交媒体范式，您就可以进行研究，例如新闻提要帖子的组合，<i>直到</i>找到有用的东西。可测量性对此至关重要。</p><p>如果不运行算法本身，我不知道多臂老虎机算法会发现什么技术；我做不到，因为只有那些大量购买服务器的人才能访问这么多数据，即使对他们来说，这些数据也被大型科技公司（Facebook、亚马逊、微软、苹果和Google）和足够强大的情报机构（NSA 等）来防止黑客窃取和毒害数据。</p><p>我也不知道当团队中的人是有能力的心理学家、舆论专家或其他公关专家解释和标记数据中的人类行为以便人类行为变得可测量时，多臂老虎机算法会发现什么。合理地说，该行业自然会达到一种平衡，即五大科技公司竞相为这项研究寻找人才，同时最大限度地降低斯诺登式泄密的风险，类似于 10 年前美国国家安全局在斯诺登泄密后的“改革” 。您可以假设人们会自动注意到并解决这种瓶颈。科技公司和情报机构之间的旋转门雇佣也规避了<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>情报机构的能力问题</u></a>。</p><p>只需少数心理学专家的人类洞察力就足以训练人工智能自主工作；尽管需要这些专家的持续投入，并且大量的见解、行为和发现会被遗漏，并且需要额外的 3 年时间或一些东西才能被发现和标记。</p><p>即使没有人工智能，也有大量的人类操纵策略被发现和利用是微不足道的（尽管当你将人工智能放在上面时，情况会更加严重），只是 20 世纪的机构根本无法访问它们以及学术心理学等技术。</p><p>如果他们获得了与特定人类目标具有相似特征的人的足够数据，那么他们就不必对目标进行太多研究来预测目标的行为，他们只需对这些人运行多臂强盗算法即可找到操纵行为已经对具有相同遗传或其他特征的个体起作用的策略。</p><p>尽管与样本数据中的绝大多数人相比，Lesswrong 用户的平均分布更加偏远，但这成为了一个技术问题，因为人工智能能力和计算变得致力于从噪声中对信号进行分类并寻找网络的任务。与较少数据的相关性。 <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>仅小丑攻击就表明，基于社会地位的大脑漏洞在人类中相当一致</u></a>，这表明来自数百万或数十亿人的样本数据可用于发现构成人工智能安全社区的人类大脑中的各种漏洞。 。</p><p><br><br></p><p><strong>攻击面太大</strong></p><p>缺乏对此的认识会带来安全风险，就像使用“密码”一词作为您的密码一样，除非您对自己的思想的控制受到威胁，而不是对计算机操作系统和/或文件的控制。这已经是钢铁般的操作了； 10 年前/10 年后的误差线似乎相当宽。</p><p>如果黑客可以随时更改实用程序功能，那么一开始就没有多大意义。可能有些部分是难以改变的，但在这一点上很容易高估自己；例如，如果你重视长远的未来，并认为任何错误的论点都无法说服你，但社交媒体的新闻源却让威尔·麦卡斯基尔产生了疑虑或不信任，那么你离不关心长远的未来又近了一步；如果这不起作用，多臂老虎机算法将继续尝试，直到找到有效的方法并进行迭代。对于比你更了解人类大脑的攻击者来说，有很多聪明的方法可以根据与类似人的比较来发现你复杂且深刻的个人内部冲突，并按照攻击者的条件解决它们。人类的大脑是一堆意大利面条式的代码，所以可能在某个地方有什么东西。人脑有漏洞，社交媒体平台使用大量人类行为数据来寻找复杂的社会工程技术的能力和成本是一个深刻的技术问题，你无法凭直觉和 2010 年代之前的历史来处理这个问题先例。</p><p>因此，您应该假设您的效用函数和值有在未知时间被黑客攻击的风险，因此应该分配一个贴现率来考虑几年内的风险。仅在未来 10 年中缓慢的起飞就保证了这个折扣率实际上太高了，以至于人工智能安全社区的人们无法继续相信它接近于零。</p><p>我认为接近零是一个合理的目标，但在目前的情况下，人们甚至懒得遮盖他们的网络摄像头，在房间里用智能手机就地球的命运进行重要而敏感的对话，并使用社交媒体。每天近一个小时的媒体报道（滚动浏览近千个帖子）。不管你喜欢与否，使用方向键以外的任何东西滚动浏览一篇文章都会生成至少一条曲线，而每天生成的数万亿条曲线都是线性代数，是插入机器学习的完美形状。如果攻击面如此之大，这种环境下的贴现率就不能被认为“合理”接近于零；世界变化如此之快。</p><p>我们在这里所做的一切都是基于这样的假设：强大的力量，如情报机构，不会扰乱社区的运作，例如通过使用匿名代理而导致相互之间的假旗攻击，从而煽动派系冲突。</p><p>如果人们有<a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>任何他们看重的东西</u></a>，而人工智能安全社区可能确实有，那么当前零努力的人工智能安全范式是非常不合适的，它基本上是完全屈服于隐形黑客。</p><p><br></p><p><strong>信息环境可能是敌对的</strong></p><p>我怀疑导致人工智能安全彻底失败的一个大瓶颈是，湾区的人工智能联盟社区拥有技术能力，可以直观地理解人类可以被人工智能操纵，因为在优化的思想分析和实验环境下，就像社交媒体新闻推送一样，但我认为情报机构和五大科技公司永远不会真正做这样的事情。与此同时，华盛顿的人工智能政策界知道，强大的公司和政府机构经常储备这样的能力，因为他们知道如果不这样做，他们可以逃脱惩罚并减轻损害，并且这些能力在国际冲突中派上用场，例如中美冲突，但他们缺乏直观地了解如何用 SGD 操纵人类思维所需的定量技能（许多人甚至不认识缩写“SGD”，所以我使用“AI”代替）。</p><p>如果SF数学书呆子和DC历史书呆子更多地混合的话，这个问题可能可以避免，但不幸的是，似乎历史书呆子对数学课的记忆很糟糕，而数学书呆子对历史课的记忆也很糟糕。</p><p>在这个隔离和营养不良的环境中，“精神控制”的不良第一印象<a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"><u>占主导地位</u></a>，而不是缓慢起飞的逻辑推理和认真的实际计划。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/azjeddznwoiuzsxato92"></p><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/zmrpngdt0hshh03wacfc"></p><p>如果有什么东西可以被我所描述的基于社交媒体的范式操纵的话，那就是印象和人类印象形成的过程，因为有很多关于这方面的数据。如果社交媒体<i>会</i>操纵任何事情，那就是对社交媒体的态度会损害人类大脑，因为 SGD/AI 会自动选择与人们继续使用社交媒体的情况相对应的新闻推送帖子的银河大脑组合，并避免与人们退出社交媒体的案例相对应的帖子组合。人们离开或留下的案例有数十亿。</p><p>让人们留在社交媒体上对于任何目标都至关重要，从准备以美国和中国之间的信息战为特征的军事<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合战争</u></a>应急计划，到只是经营一家人们不会离开你的平台的企业。</p><p>如果存在与其他平台（例如 Tiktok 或 Instagram Reels）争夺用户时间的逐底竞争，则尤其如此，因为这些平台不太愿意利用 AI/SGD 来最大限度地提高僵尸大脑参与度和用户保留率。</p><p>我们应该默认假设现代信息环境是不利的，并且某些话题比其他话题更具对抗性，例如具有强烈地缘政治意义的乌克兰战争和新冠疫情。我在这里认为，信息战本身是一个具有强烈地缘政治意义的话题，因此也应该是一个对抗性的信息环境。</p><p>在对抗性信息环境中，印象比整个认知更容易受到损害，因为当前的范式由于更好的数据质量而针对这一点进行了优化。因此，我们应该通过深思熟虑的分析和预测来应对传感器暴露风险，而不是模糊的表面印象。<br></p><p><strong>解决方案很简单</strong></p><p>一般来说，眼动追踪可能是预测分析、情绪分析和影响技术中最有价值的用户数据 ML 层，因为眼动追踪层只是映射到每毫秒每只眼睛在屏幕上居中的确切位置的两组坐标（每只眼睛一个，因为每只眼睛运动的毫秒差异也可能与一个人的思维过程的有价值的信息相关）。</p><p>这种紧凑的数据使深度学习能够以毫秒精度“看到”人类的眼睛/大脑在每个单词和句子上停留的时间。值得注意的是，数以百万计的这些坐标的样本量可能与人类思维过程密切相关，以至于眼球追踪数据的价值可能超过所有其他面部肌肉的价值总和（面部肌肉是所有面部表情和情绪微表情的鼻祖， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=A%20critical%20element%20is%20for%20as%20many%20people%20as%20possible%20in%20AI%20safety%20to%20cover%20up%20their%20webcams%3B%20facial%20microexpressions%20are%20remarkably%20revealing%2C%20especially%20to%20people%20with%20access%20to"><u>也可能可以通过计算机视觉紧凑地还原</u></a>，因为面部附近的肌肉不到 100 块，而且大多数肌肉的信噪比非常差，但效率不如眼动追踪）。</p><p>如果 LK99 被复制并且手持式功能磁共振成像变得可构建，那么也许它可以争夺第一名的位置；或者也许我愚蠢地低估了将音频对话记录插入 LLM 并通过对微小的心率变化添加时间戳来自动标记对话者最重视的对话部分的压倒性优势。</p><p>然而，在附近没有智能手机的情况下举办网络活动是很困难的，而遮盖网络摄像头很容易，即使有些手机需要使用遮蔽胶带和一小片铝箔进行一些工程创意。</p><p>网络摄像头覆盖率可能是衡量 AI 安全社区在 2020 年代生存情况的一个很好的指标。现在是“F”。</p><p>还有其他简单的政策建议可能更为重要，具体取决于难以研究的技术因素，这些因素决定攻击面的哪些部分最危险：</p><ol><li>停止每天花几个小时在超级优化的氛围/印象黑客环境（社交媒体新闻源）中。</li><li>改用实体书而不是电子书可能是个好主意。实体书没有操作系统或传感器。您还可以打印您已知可能值得阅读或浏览的研究论文以及 Lesswrong 和 EAforum 文章。 PC 的主板上有加速计，据我所知无法移除或解决，即使您移除麦克风并使用 USB 键盘并使用热键而不是鼠标，加速计也可能能够充当麦克风并拾取心率的变化。</li><li>最好避免与智能设备或任何带有传感器、操作系统和扬声器的设备睡在同一个房间。攻击面看起来很大，如果设备能够判断人们的心率何时接近或低于 50 bpm，那么它就可以<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=It%E2%80%99s%20probably%20best%20to%20avoid%20sleeping%20in%20the%20same%20room%20as%20a%20smart%20device%2C%20or%20anything%20with%20sensors%2C%20an%20operating%20system%2C%20and%20also%20a%20speaker.%20The%20attack%20surface%20seems%20large%2C%20if%20the%20device%20can%20tell%20when%20people%E2%80%99s%20heart%20rate%20is%20near%20or%20under%2050%20bpm%2C%20then%20it%20can%20test%20all%20sorts%20of%20things"><u>测试各种各样的东西</u></a>。只需开车去商店买一个时钟即可。</li><li> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=ink%2Defficient%20printer.-,I%E2%80%99m%20not%20sure,-whether%20a%20text"><u>阅读伟大的理性文本可能会降低你的可预测系数</u></a>，但它不会可靠地修补人脑中的“零日”。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the- human-brain-in-the-2020s<guid ispermalink="false"> Lw8enYm5EXyvbcjmt</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:31:09 GMT</pubDate> </item><item><title><![CDATA[Notes on "How do we become confident in the safety of a machine learning system?"]]></title><description><![CDATA[Published on October 26, 2023 3:13 AM GMT<br/><br/><p>我正在尝试在 LessWrong 草稿帖子中记录人工智能安全阅读内容，并以省力的“蒸馏”形式分享结果。我希望这是一种快速的方法，可以迫使自己记下更好的笔记，并提供对其他人有用的精华。这篇读物是埃文·胡宾格（Evan Hubinger）的“ <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine">我们如何对机器学习系统的安全性充满信心？</a> ”。</p><p>一开始我并不确定我自己的离题想法会走多远。我认为通过写笔记时考虑到其他人可能会读到它们，我最终会a）比平常更严格地遵循原始帖子的结构，b）比平常更少的切线。我认为这对我来说可能并不理想；如果我再这样做，我可能会尝试主动不追随这些冲动。</p><p>无论如何，这里是注释。这里的每个部分标题都链接到原始帖子中的相应部分。</p><h3><strong>概括：</strong></h3><p>这篇文章建议我们使用<i>训练故事</i>来充实和评估训练安全机器学习系统的建议。提案是一个培训设置（大致理解），培训故事由<i>培训目标</i>和<i>培训理由组成。</i>训练目标以一些机制细节描述了您希望训练设置会产生什么行为；它包括您认为模型将如何有效地执行相关任务，以及为什么您认为以这种方式执行它是安全的。训练基本原理描述了为什么您认为训练设置将产生一个实现训练目标中描述的机械行为的模型。虽然培训故事并没有涵盖使人工智能系统更安全的所有可能方法，但它们是一个相当通用的框架，可以帮助我们更好地系统地组织和评估我们的建议。如果可能的培训故事空间存在明显差距，该框架还可以帮助我们生成提案。这篇文章包括很多建议、培训目标、培训理由和培训故事评估的例子。</p><h1> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#">我们如何对机器学习系统的安全性充满信心？</a></h1><ul><li> <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai">构建安全先进人工智能的 11 项提案概述，</a>使用外部对齐、内部对齐、训练竞争力和性能竞争力的标准来评估安全 AGI 的提案</li><li>这些对于提出开放性问题很有用，但不能系统地帮助我们理解提案需要满足哪些假设才能发挥作用<ul><li>另外，有些提案不符合这些标准的评估</li></ul></li><li>评估提案的新想法：<i>培训故事。</i>希望这些：<ul><li>适用于任何构建安全 AGI 的提案</li><li>提供提案工作条件（产生安全 AGI）的简明描述，以便我们可以通过检查条件对提案的安全性充满信心</li><li>不要对提案必须构成的内容做出不必要的假设（从而隐含地让我们对我们应该考虑的提案视而不见）</li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#What_s_a_training_story_">什么是训练故事？</a></h2><ul><li> “<i>训练故事</i>是关于你认为训练将如何进行以及你认为最终会得到什么样的模型的故事”（有足够的机制细节来反映其泛化行为）</li><li>有关猫分类器示例，请参阅原始帖子</li><li>示例说明了培训故事的三个好处：<ul><li>如果故事属实，则模型是安全的<ul><li>该故事包含有关最终模型安全的条件以及不安全的情况的详细信息（猫分类器是最终重视对猫进行分类的代理）</li></ul></li><li>关于训练期间会发生什么的可证伪的说法（例如，猫分类器将学习类似人类的启发法 - 我们现在有证据表明 CNN 与人类启发法不同，因此示例故事并不完全正确）</li><li>他们可以被告知正在为任何任务训练的任何模型<ul><li>不需要尝试构建 AGI。很高兴看到任何可能造成任何级别伤害的人工智能的培训故事。另外，在未来，模型何时变得危险可能并不明显，所以也许每个人工智能项目（例如 NeurIPS 论文）都应该分享一个训练故事。<ul><li>如何强制执行类似的事情？人们在提交论文之前训练他们的模型，并且如果只需要在最终论文中写出或思考训练故事，他们可能不会在训练之前写出或思考。可能会有帮助，但也可能会让人们认为它很麻烦而且实际上并不重要，所以即使它很重要，他们也不会注意。</li></ul></li></ul></li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_components">训练故事组件</a></h2><p>2个基本组成部分：</p><ol><li><strong>训练目标：</strong>对所需模型的机械描述以及为什么这会很好（例如使用人类视觉启发法对猫进行分类）<ol><li>这个例子似乎缺少“为什么这会很好”部分，但根据前面的部分，它可能是“人类视觉启发法不包括任何会采取极端或危险行为的代理/优化器”</li></ol></li><li><strong>训练理由：</strong>为什么您相信您的训练设置会产生满足训练目标中的机制描述的模型<ol><li>“这里的‘训练设置’是指在模型发布、部署或以其他方式赋予对世界产生有意义影响的能力之前所做的任何事情。”<ol><li>我认为这可能包括在“训练设置”一词下可能不会立即想到的一些事情，包括集成和激活工程等干预措施</li><li>此外，我认为一个模型可以在人类打算对其进行训练之前对世界产生有意义的影响。示例：模型在训练过程中出现偏差并获得决定性的战略优势，并中断训练过程以实现其目标。此外，所有的持续学习。<ol><li>因此，培训故事还应该讲述为什么一路上不会发生任何不好的事情？</li></ol></li></ol></li></ol></li></ol><ul><li>训练目标不需要非常精确；它需要有多精确很复杂，将在下一节中讨论</li></ul><p>上述两件事各有 2 个子组件，构成任何培训故事的 4 个基本部分：</p><ol><li><strong>训练目标规范：</strong>所需模型的机制描述</li><li><strong>训练目标的可取性：</strong>为什么任何满足训练目标的模型都是安全的并且能够很好地执行所需的任务<ol><li>（我的补充）这里有一个全称量词。 “<i>对于所有</i>型号<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>达到训练目标， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span>是安全的”是需要证明的陈述。这是一个很高的标准。</li></ol></li><li><strong>训练原理约束：</strong>模型必须满足哪些约束以及为什么训练目标与这些约束兼容（例如完美地拟合训练数据（如果训练为零损失），并且可以使用所选架构实现）</li><li><strong>训练基本原理推动：</strong>为什么训练设置可能会产生满足训练目标的模型，尽管其他因素也满足约束条件<ol><li>可能是“满足目标是满足约束的最简单方法”（归纳偏差论证）</li></ol></li></ol><ul><li>使用 cat 分类器的所有 4 个示例</li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#How_mechanistic_does_a_training_goal_need_to_be_">训练目标需要有多机械化？</a></h2><ul><li>通常希望尽可能详细地指定培训目标；我们很少能达到如此详细的训练目标，以至于我们可以对其进行硬编码</li><li>想要以一种尽可能容易地证明它是可取的（满足培训目标）的方式指定培训目标<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\implies"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⟹</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span></span></span></span></span></span>安全）并且训练设置将产生令人满意的结果</li><li>后者最好通过在训练设置的背景下以我们可以理解的方式描述训练目标来实现<ul><li>“在我看来，实际上使培训目标规范更容易建立培训基本原理的因素并不是一般性的，而是诸如目标在培训过程的归纳偏差方面有多自然、它有多少等问题。对应于我们知道如何寻找模型的各个方面，以及将其分解为可单独检查的部分的容易程度等。”</li></ul></li><li>培训目标规范应该是什么样子的正面和反面例子，以允许培训理由来支持它：广泛强化了更具体/机械性更好的想法</li><li>“理想情况下，正如我稍后讨论的那样，我希望对诸如‘如果训练原理稍微错误，我们会偏离训练目标多少’之类的事情进行严格的敏感性分析。”</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Relationship_to_inner_alignment">与内部对齐的关系</a></h2><ul><li>常见的人工智能安全术语，如台面优化、内部对齐和目标误概括，旨在适应培训故事。 Evan 提供了常用术语词汇表，其定义稍作修改，以更好地适应培训故事</li><li>缩写词汇表：<ul><li><strong>目标错误概括：</strong>最终模型具有训练目标所需的功能，但将其用于与训练目标不同的目的</li><li><strong>Mesa-optimization：</strong>最终模型内部执行优化的任何情况。特别令人担忧的是，内部优化是否被学习但不是训练目标的一部分。<ul><li>读完<a href="https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see">这篇</a>文章后，我认为所有有能力的行为在某种意义上都可能算作优化，并且我们可能希望台面优化专门指可重定向或通用搜索。</li></ul></li><li><strong>外部对齐问题：</strong>选择目标（例如奖励/损失函数）的问题，使得“优化该损失/奖励函数的模型”的训练目标是可取的。</li><li><strong>内部对齐问题：</strong>开发训练设置的问题，该训练设置具有强有力的理由来生成针对指定目标进行优化的最终模型。</li><li><strong>欺骗性对齐问题：</strong> “构建训练基本原理的问题，避免模型试图欺骗训练过程，让其认为自己正在做正确的事情。”</li></ul></li><li>培训故事表明，内外对齐的崩溃并不是根本性的；我们可以有一个训练故事，它不尝试指定需要优化的目标，也不尝试训练模型来优化特定目标，并且该模型可以执行所需的行为。<ul><li>我很喜欢浏览<a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">这篇关于这个主题的文章</a></li></ul></li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Do_training_stories_capture_all_possible_ways_of_addressing_AI_safety_">培训故事是否涵盖了解决人工智能安全问题的所有可能方法？</a></h2><ul><li>培训故事非常笼统，但并非如此。他们无法处理以下事情：<ul><li>无需训练步骤即可构建安全 AGI 的建议（例如，诸如显式分层规划之类的非机器学习内容）</li><li>建立安全 AGI 的提案试图在没有机械故事的情况下建立对最终模型安全性的信心</li><li>减少不涉及构建安全 AGI 的 AI X 风险的提案（例如，说服 AI 研究人员不要构建 AGI，因为它很危险）</li></ul></li></ul><p>在<a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">《计划 - 2022 年更新》</a>中，John Wentworth 说道：</p><blockquote><p>我希望我们还会看到更多基于直接读写神经网络内部语言的端到端对齐策略的讨论......因为此类策略非常直接地处理/回避内部对齐问题，并且大多数情况下不依赖奖励信号作为激励预期行为/内部结构的主要机制，我希望我们会看到焦点从对齐建议中复杂的培训计划转移。</p></blockquote><p>我不清楚像“阅读和编写神经网络的内部语言”这样的干预措施是否应该适合训练故事。约翰似乎确实建议这些对齐建议的主要部分不是模型的训练方式。</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Evaluating_proposals_for_building_safe_advanced_AI">评估构建安全先进人工智能的建议</a></h2><ul><li>我们已经了解了如何构建培训故事，但是我们应该如何评估培训故事呢？ AGI 培训故事的 4 个标准（并非所有培训故事）：<ol><li><strong>训练目标一致：</strong>所有满足训练目标的模型都对世界有利吗？</li><li><strong>训练目标竞争力：</strong>达到训练目标的模型是否足够强大，不会被其他模型超越和淘汰？</li><li><strong>训练基本原理对齐：</strong>训练设置实际上会产生满足训练目标的模型吗？</li><li><strong>培训理由竞争力：</strong>实施所描述的培训设置是否可行？ （不这样做的可能原因是“比替代训练设置需要更多的计算/数据。”）</li></ol></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Case_study__Microscope_AI">案例研究：显微镜人工智能</a></h2><ul><li>训练设置：<ul><li> （埃文在培训故事中包含了培训设置（特别是基本原理），但在我看来，该提案<i>是</i>提议的培训设置，然后培训故事需要讲述该设置？）</li><li>在大型多样化数据集上进行自我监督学习，同时在训练期间使用透明工具来检查是否学习了正确的训练目标（见下文）。</li></ul></li><li>训练故事：<ul><li>目标：获得一个纯粹的预测模型，使用人类可理解的概念进行预测。<ul><li>没有内部优化</li><li>Once we have this model, we can use interpretability tools to figure out how it is predicting, and we can gain insights that humans can learn from and utilize for improved decision-making.</li></ul></li><li> Rationale:<ul><li> Hopefully, the simplicity biases push self-supervised learning to learn a purely predictive model that doesn&#39;t perform optimization, and that uses human-understandable concepts.</li><li> The transparency tools are mainly for catching dangerous agentic optimization and halting the training process if such processes are found.</li></ul></li></ul></li><li> Evaluation:<ul><li> Goal alignment: Whether or not a pure predictor is safe is more complicated than it may seem, due to self-referential things. The exact way the goal is specified may be able to rule out these concerns.</li><li> Goal competitiveness: Couple factors to consider here.<ul><li> Interpretability tools need to be capable of extracting actually useful info from the model. This might not work, especially if we want info for effective sequential decision-making while the model was just a pure predictor.</li><li> This requires that humanity is ok with not building useful agentic systems, and improved understanding and decision-making are enough.<ul><li> I kind of doubt that improved understanding and decision-making are enough.</li></ul></li></ul></li><li> Rationale alignment:<ul><li> Would self-supervised learning actually produce a pure predictor? One cause for concern is that the world includes optimizers (such as humans, at least sometimes), and we would want this predictor to be capable of making predictions about what those optimizers will do. Training a model to be able to internally model optimizers well enough to predict them may just cause the model to learn to optimize internally.</li><li> Using transparency tools to prevent the model learning optimization (by throwing out models that are found to be optimizers, or by training against the transparency tools) could be bad, as it may result in deceptive optimizers which fool the transparency tools.</li><li> The model is also supposed to use human-understandable concepts, which may not work given that cat classifiers don&#39;t use human-like visual heuristics. Maybe human-like-abstraction-use looks like this, which is good near AGI and bad for superintelligence: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3tuxF4X5R5BY7fut/b4nv5tp62l7yvu6axxbo"></li></ul></li><li> Rationale competitiveness:<ul><li> Self-supervised learning is a dominant ML paradigm, so pretty competitive. Using transparency tools might slow things down and reduce competitiveness; advancing automated interpretability would help.</li></ul></li></ul></li><li> This is a better analysis of Microscope AI than the one in 11 proposals, because instead of trying to evaluate it according to outer alignment and other concepts that don&#39;t really apply, the proposal is evaluated on its own terms.</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Exploring_the_landscape_of_possible_training_stories">Exploring the landscape of possible training stories</a></h2><ul><li> This section is a non-exhaustive exploration of broad categories that training goals and training rationales could fall into. There may be many broad categories that the field of AI safety has yet to discover.</li></ul><p> First, some training goals. (This part reminds me of John Wentworth&#39;s discussion of possible alignment targets <a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">here</a> , which I liked quite a bit. That post came out later, but I read it first.)</p><ol><li> <strong>Loss-minimizing models:</strong> A possible training goal, for people who are very confident they&#39;ve specified a truly desirable objective, is to get a model that internally optimizes for that objective (eg loss function). This can be bad, due to classic outer alignment concerns.</li><li> <strong>Fully aligned agents:</strong> An agent that cares about everything humans care about and acts to achieve those goals. Eg <a href="https://www.lesswrong.com/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning">ambitious value learning</a> . But this is a very difficult target, that most people think isn&#39;t worth focusing on in our first shot at aligning advanced AI. Instead, we can do something else more achievable to get useful and safe human+ level AI, and then use those to help us get to fully aligned superintelligence later. (Or sacrifice the fully aligned superintelligence goal because we think we&#39;ll never get it.)<ol><li> I personally think &quot;human values&quot; aren&#39;t well-defined, and in my mind fully aligned agents would do something along the lines of &quot;solve morality and act accordingly,&quot; which probably works out to &quot;maximize the well-being of conscious creatures.&quot; What, did I say something controversial?</li></ol></li><li> <strong>Corrigible agents:</strong> We may want AI systems that let themselves be turned off or modified, or help us figure out when it would be good for us to turn them off or modify them. Consider <a href="https://ai-alignment.com/model-free-decisions-6e6609f5d99e">approval-directed agents</a> (though there are ways this could fail to be corrigible).<ol><li> Robust instruction-following agents may also fit here?</li></ol></li><li> <strong>Myopic agents:</strong> AI agents that preform limited optimization only, such as only optimizing the immediate effect of their next action, rather than having long-term, large-scale goals.</li><li> <strong>Simulators</strong> : AI system that does nothing other than simulate something else. Things you might want to simulate include HCH (roughly, an infinite tree of humans delegating subtasks and consulting each other), physics (for an AlphaFold-like system), and a human internet user (for a language model).</li><li> <strong>Narrow agents:</strong> Agents that are highly capable in a specific domain without thinking much or at all about other domains. Eg <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#6__STEM_AI">STEM AI</a> .<ol><li> I think some current systems, like superhuman chess AIs, also fit this category.</li></ol></li><li> <strong>Truthful question-answerers:</strong> A non-agent truthful question-answering system that accurately reports what its model of the world says/predicts in human-understandable terms.</li></ol><p> Next, some training rationales. These aren&#39;t explicitly paired with particular training goals, but there are implicit relationships to classes of training goals.</p><ol><li> <strong>Capabilities limitations:</strong> Can argue that a particular training setup will give rise to a model that does not have certain dangerous capabilities (eg internal optimization, understanding of how to deceive humans), and therefore won&#39;t exhibit the dangerous behaviors that require the capability.</li><li> <strong>Inductive bias analysis:</strong> Can argue that certain behaviors are more likely to be learned than others given a training setup, for reasons other than &quot;lower loss.&quot; Often this appeals to some form of simplicity. This is a promising approach, but hard to make strong detailed arguments in advance about what will be learned. But there is a growing literature outlining empirical phenomena that can inform our inductive bias assessments (like <a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">deep double descent</a> , <a href="https://arxiv.org/abs/1803.03635">lottery tickets</a> , <a href="https://arxiv.org/abs/2001.08361">scaling laws</a> , <a href="https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf">grokking</a> , or <a href="https://arxiv.org/abs/2009.08092">distributional generalization</a> ).<ol><li> &quot;This is especially problematic given how inductive bias analysis essentially requires getting everything right before training begins, as a purely inductive-bias-analysis-based training rationale doesn&#39;t provide any mechanism for verifying that the right training goal is actually being learned during training.&quot;</li></ol></li><li> <strong>Transparency and interpretability:</strong> If we use transparency tools during training to throw out or disincentivize dangerous models, then our training rationale could include something like &quot;We think that our transparency checks will rule out all simple models that don&#39;t fit the training goal, with all remaining models that don&#39;t fit the goal being too complex according to the inductive biases of the training process to possibly be learned.&quot; One great thing about transparency tools is that they could make it possible for us to verify that the right training goal is actually being learned during training, rather than just hoping it will be and leaving training a black box.<ol><li> Evan&#39;s <a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree">A transparency and interpretability tech tree</a> is a good post on various types of interpretability along with discussion of their challenges and limitations. It helped me see what ambitious interpretability could look like.</li></ol></li><li> <strong>Automated oversight:</strong> Humans may struggle to label data or use transparency tools; having humans-in-the-loop slows things down, and we can only understand human-understandable concepts. We could perhaps automate these things with competent AIs, including ones that understand the alien abstractions used by the model we&#39;re training (maybe a copy of that same model). How can we trust the supervisor AI? That&#39;s a challenge; maybe using some recursive oversight methods. For a more thorough exploration of what such an approach might look like, see “ <a href="https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment">Relaxed adversarial training for inner alignment</a> .”</li><li> <strong>AI cognitive science:</strong> Instead of mechanistic analysis, we could do behavioral analysis of models; test how they act in ways that we think reveal important underlying details. Possibly a less direct and trustworthy source of info than mechanistic interpretability. Might be hard to extrapolate findings to more capable models.</li><li> <strong>Precursor checking:</strong> Check for some easier-to-check property that is a necessary precursor for bad behavior. Eg it might be easier to check that a model&#39;s planning-horizon length is below a certain threshold, and that might imply that the model isn&#39;t being deceptive, while checking for deception more generally may be very difficult. Need to make sure the property is actually a necessary condition.</li><li> <strong>Loss landscape analysis:</strong> Extension of inductive bias analysis, can help us predict paths through parameter space (rather than just the final local loss minimum). Could be used in conjunction with various other training rationales on this list.</li><li> <strong>Game-theoretic/evolutionary analysis:</strong> What multi-agent dynamics are likely to be learned? One difficult aspect of this is avoiding the assumption that agents will be optimizing for their specified objectives; agent objectives are typically taken as given when finding equilibria in game theory. Evolutionary analysis does this less; it deals with properties that tend to be selected-for in multi-agent settings, might be useful.</li></ol><p> We can use these lists of goals and rationales to categorize past approaches, we can combine existing goals and rationales in new ways, and we can try to come up with new goals and rationales (aided by the observed lack of exhaustivity in these lists).</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_sensitivity_analysis">Training story sensitivity analysis</a></h2><p> We want to analyze how sensitive the outcome of a training setup is to incorrectness in the assumptions that go into its training story. What happens when a training story fails? When one of its assumptions is wrong? Does it fail safely or catastrophically?</p><p> The original post contains some examples of what our uncertainties about the assumptions in a training story might look like and how we can have more confidence in some parts of a training story than others.</p><p> &quot;Hopefully, as we build better training stories, we&#39;ll also be able to build better tools for their sensitivity analysis so we can actually build real confidence in what sort of model our training processes will produce.&quot;</p><br/><br/> <a href="https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a<guid ispermalink="false"> Jps7osck25CXBAnTY</guid><dc:creator><![CDATA[RohanS]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:13:56 GMT</pubDate> </item><item><title><![CDATA[Apply to the Constellation Visiting Researcher Program and Astra Fellowship, in Berkeley this Winter]]></title><description><![CDATA[Published on October 26, 2023 3:07 AM GMT<br/><br/><blockquote><p> <i>This is a link post for two AI safety programs we&#39;ve just opened applications for:</i> <a href="https://www.constellation.org/programs/researcher-program"><i><u>https://www.constellation.org/programs/astra-fellowship</u></i></a> <i>and</i> <a href="https://www.constellation.org/programs/researcher-program"><i><u>https://www.constellation.org/programs/researcher-program</u></i></a></p></blockquote><p> <a href="http://constellation.org"><strong><u>Constellation</u></strong></a> <strong>is a research center dedicated to safely navigating the development of transformative AI.</strong> We&#39;ve previously helped run <a href="https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in"><u>the ML for Alignment Bootcamp (MLAB) series</u></a> and Redwood&#39;s <a href="https://www.redwoodresearch.org/remix"><u>month-long research program on model internals (REMIX)</u></a> in addition to a variety of other field-building programs &amp; events. <span class="footnote-reference" role="doc-noteref" id="fnreff0wvag8ixj"><sup><a href="#fnf0wvag8ixj">[1]</a></sup></span></p><p> This winter, we are running two programs aimed at growing and supporting the ecosystem of people working on AI safety:</p><ul><li> <a href="https://www.constellation.org/programs/researcher-program"><strong><u>The Constellation Visiting Researcher Program</u></strong></a> provides an opportunity for around 20 researchers to connect with leading AI safety researchers, exchange ideas, and find collaborators while continuing their research from our offices in Berkeley, CA. The funded program will take place this winter from the 8th of January 2024 to the 1st of March 2024.</li><li> <a href="https://www.constellation.org/programs/astra-fellowship"><strong><u>The Astra Fellowship</u></strong></a> provides an opportunity for around 20 people to conduct research in AI safety with experienced advisors. Fellows will be based out of the Constellation office, allowing them to connect and exchange ideas with leading AI safety researchers. The program will take place in Berkeley, CA between January 8 and April 1, 2024.</li></ul><p> <strong>Applications for both are due November 10, 11:59pm anywhere on Earth</strong> . You can apply to the Astra Fellowship <a href="https://airtable.com/app5pjeAcq1FH8HAJ/shrxsI3IanCngyCkz"><strong><u>here</u></strong></a> and the Visiting Researcher Program <a href="https://airtable.com/appfNh9OB2zLK4byG/shrnT9UwQYddMplyY"><strong><u>here</u></strong></a> . If you are unsure about your fit, please err on the side of applying. We especially encourage women and underrepresented minorities to apply. You can refer others who you think might be a good fit through <a href="https://airtable.com/appfNh9OB2zLK4byG/shr3JWGW5j2T8KtLp"><u>this form</u></a> .</p><p> <strong>Logistics:</strong> Housing and travel expenses are covered for both programs, and Astra fellows will receive an additional monetary stipend. The start and end dates for both programs are flexible.</p><p><strong>问题？</strong> Email <a href="mailto:programs@constellation.org"><u>programs@constellation.org</u></a> or ask them below. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnf0wvag8ixj"> <span class="footnote-back-link"><sup><strong><a href="#fnreff0wvag8ixj">^</a></strong></sup></span><div class="footnote-content"><p> Over 15 participants from these past programs are now working on AI safety at <a href="https://www.anthropic.com/">Anthropic</a> , <a href="https://evals.alignment.org/">ARC Evals</a> , <a href="https://www.alignment.org/theory/">ARC Theory</a> , <a href="https://www.deepmind.com/">Google DeepMind</a> , <a href="https://openai.com/">OpenAI</a> , <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> , and <a href="https://www.redwoodresearch.org/">Redwood Research</a> .</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and<guid ispermalink="false"> oBdfDvmrBKoTq3x85</guid><dc:creator><![CDATA[Nate Thomas]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:07:34 GMT</pubDate></item><item><title><![CDATA[Even if we “solve alignment”, Moloch can still kill us]]></title><description><![CDATA[Published on October 26, 2023 1:56 AM GMT<br/><br/><p> I think this point is rather important and straightforward, but I haven&#39;t seen it discussed enough recently.</p><p> Many people think of the alignment problem as the technical challenge of building an AI that does what we actually want. GPT-4 or Claude are pretty “aligned” in the sense that they generally answer questions in a harmless and helpful way, which is how their designers intend them to behave. We aren&#39;t sure how to make superintelligent AI actually do what we intend yet, and a lot (but not nearly enough!) people are trying to solve this. However, even if we perfectly solve this problem, and can build arbitrarily powerful AI systems that want what their designers intend them to want, I think we are probably still doomed.</p><p> Suppose some unspecified number of years from now we have AI agents that can do most jobs better than humans without much oversight. That means we can build agents which can competently and autonomously work on long-term technical tasks without getting stuck or needing help. These systems will be significantly faster, cheaper, and more competent than most human workers. Every company and military that has the capacity to employ such agents will be forced to do so by competitive pressures; anyone who doesn&#39;t will simply be outcompeted. Even if the goals that a corporation or military instills in its AI workers are perfectly aligned with the organization that builds them, those agents will not, in general, be aligned with humanity as a whole. An army of AI agents that are trying to maximize Google&#39;s profits or the probability that the US military&#39;s drone swarm can defeat the PLA&#39;s drone swarm looks pretty bad for us.</p><p> This is because, in such a world of competing AIs, Moloch (or, if you like, natural selection) ultimately decides what agents will have power in the long run. The simple answer is that the most selfish and power-seeking agents will tend to accrue the most power. As it stands, we have a few human money-maximizers with billions of dollars and human power-maximizers winning elections. These people are frequently complained about but haven&#39;t destroyed civilization yet. Actually, even money-maximizing humans still tend to create net value for society, even if that value isn&#39;t distributed as equitably as many would like. AI agents which are vastly more competent at these tasks, which can instantly copy themselves and think faster than humans, will end up de-facto controlling all the resources we care about, even if human figureheads are still officially in charge for a while. Eventually, nonhuman intelligences will have accrued enough power to permanently outcompete us, whether that looks like a rapid, violent coup, or a slow fizzle as the machines colonize the universe and in the process use up all the resources we require to stay alive (&quot;you are made out of atoms which it can use for something else&quot;). By the time most people realize the nature of the beast we have unleashed, humanity will be powerless to stop it.</p><p> I don&#39;t really know how to fix this problem. Competition between AI agents is required for this particular doom scenario, so worlds with an aligned singleton, for instance, would not succumb to it. However, I think the current trend of technology points to lots of small AI systems rather than one or a few large ones. Some kind of regulation is probably necessary to prevent this from happening, and I think it needs to happen <i>soon</i> , before the competitive pressure to keep developing powerful AI agents can grow too strong for us to resist. If I had unilateral control over world governments, I would enforce a global shutdown of AI research, as I think society is not yet mature enough to safely handle the immense power of superhuman autonomous agents.</p><br/><br/> <a href="https://www.lesswrong.com/posts/XBLXeKFthwoXARBeZ/even-if-we-solve-alignment-moloch-can-still-kill-us#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XBLXeKFthwoXARBeZ/even-if-we-solve-alignment-moloch-can-still-kill-us<guid ispermalink="false"> XBLXeKFthwoXARBeZ</guid><dc:creator><![CDATA[Eccentricity]]></dc:creator><pubDate> Thu, 26 Oct 2023 01:56:19 GMT</pubDate> </item><item><title><![CDATA[CHAI internship applications are open (due Nov 13)]]></title><description><![CDATA[Published on October 26, 2023 12:53 AM GMT<br/><br/><p> <a href="https://humancompatible.ai/">CHAI</a> internship applications have just opened, <a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">apply here</a> by Nov 13th! The internship might be a good fit if you want to get research experience in technical AI safety. You&#39;ll be mentored by a CHAI PhD student or postdoc and work on your own project for 3-4 months.</p><p> Researchers at CHAI are interested in many different AI safety topics; a few examples are reward learning, adversarial robustness of LLMs, and interpretability. (I mention this because it might not be obvious from some of the language and links on the CHAI website.)</p><p> I&#39;ve copied <a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">the full announcement</a> below:</p><blockquote><p> Our internships require a background in mathematics and computer science. Existing research experience in machine learning is strongly advantageous but not required. We are interested in people who can demonstrate technical excellence and wish to transition to technical AI safety research. Examples include undergraduate or Master&#39;s students in computer science or adjacent fields, PhD students/researchers, professional software or ML engineers, etc.</p><p> This internship is designed for individuals who are interested in <strong>technical AI safety research</strong> . All applicants should take a look at our papers ( <a href="https://humancompatible.ai/jobs#internship">here</a> and <a href="https://humancompatible.ai/research">here</a> ) before applying to understand CHAI&#39;s research.</p><h2><strong>一般信息</strong></h2><ul><li><strong>Location:</strong> In-person (at UC Berkeley) is preferred but remote is possible.</li><li> <strong>Deadline:</strong> November 13th, 2023</li><li> <strong>Start Date</strong> : Flexible</li><li> <strong>Duration</strong> : Internships are typically 12 to 16 weeks</li><li> <strong>Compensation</strong> : $3,500 per month for remote interns. $5,000 per month for in-person interns.</li><li> <strong>International Applicants</strong> : We accept international applicants</li><li> <strong>Requirements</strong> :<ul><li> Cover Letter or Research Proposal (choose one and see instructions below)</li><li>恢复</li><li>Academic Transcript</li></ul></li></ul><h2> <strong>Cover Letter or Research Proposal (Choose One)</strong></h2><p> The primary purpose of the Cover Letter or Research Proposal is for us to match you to a project that interests you.</p><p> Most of our interns are generally interested in technical AI safety research but do not have a specific project in mind when they start the internship. Throughout the interview process, we learn more about each intern&#39;s interests and match them with a mentor who has an existing project idea that fits the intern&#39;s skills and interests. If you do not have a particular project in mind, then we ask you to please write a Cover Letter answering the following questions:</p><ul><li> Why do you want to work at CHAI as opposed to other research labs?</li><li> What are you hoping to achieve from the internship? For example, are you seeking to improve certain research skills, contribute to a publication, test out whether AI research is a good fit for your career, or something else?</li><li> What are your research interests in AI? For example, are you interested in RL, NLP, theory, etc?</li></ul><p> Alternatively, some of our interns apply to the program with a specific project or detailed research interests in mind. If this applies to you, then please write a Research Proposal describing your project and what kind of mentorship you would like to receive.</p><h2> <strong>Internship Application Process Overview</strong></h2><p> The internship application process has four phases. Please note: while we will do our best to adhere to them, all dates in the Internship Application Process Overview are <strong>subject to change</strong> .</p><ul><li> Initial Review (Phase 1)<ul><li> We will examine your application based on motivation, research potential, grades, experience, programming ability, and other criteria.</li><li> Applicants will likely receive a response by late November.</li></ul></li><li> Programming Assessment (Phase 2)<ul><li> If you pass the Initial Review Phase, then you will be given an online programming test.</li><li> Applicants will receive a response by late December.</li></ul></li><li> Interviews (Phase 3)<ul><li> If you pass the Programming Assessment, then you will be interviewed starting in early to mid January.</li><li> CHAI has several mentors who are willing to take on interns. Each mentor that is interested in working with you will contact you to schedule an interview. It&#39;s possible that you will speak to more than one mentor during this phase if multiple mentors are interested in working with you.</li></ul></li><li> Offer (Phase 4)<ul><li> Applicants will receive offers by early to mid February.</li><li> If you are given an offer by one mentor, then you will work with that mentor if you choose to take the internship.</li><li> If you are given multiple offers from different mentors, then you will get to choose which mentor you want to work with.</li><li> Typically, the internship will begin around April or May but the start date will ultimately depend on you and your mentor(s). You will have to coordinate with your mentor(s) on when to begin your internship.</li></ul></li></ul><h2><strong>其他信息</strong></h2><ul><li>For any questions, please contact <a href="mailto:chai-admin@berkeley.edu">chai-admin@berkeley.edu</a> .</li><li> <strong>In the event that your situation changes (eg you receive a competing offer) and you need us to respond to you sooner than you had initially thought, then please let us know.</strong></li></ul></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13<guid ispermalink="false"> Xa4b8vgCLRATiqnJn</guid><dc:creator><![CDATA[Erik Jenner]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:53:50 GMT</pubDate></item><item><title><![CDATA[Architects of Our Own Demise: We Should Stop Developing AI]]></title><description><![CDATA[Published on October 26, 2023 12:36 AM GMT<br/><br/><p> Some brief thoughts at a difficult time in the AI risk debate.</p><p> Imagine you go back in time to the year 1999 and tell people that in 24 years time, humans will be on the verge of building weakly superhuman AI systems. I remember watching the anime short series <a href="https://en.wikipedia.org/wiki/The_Animatrix">The Animatrix</a> at roughly this time, in particular a story called <a href="https://www.youtube.com/watch?v=sU8RunvBRZ8">The Second Renaissance</a> <a href="https://www.youtube.com/watch?v=61FPP1MElvE">I part 2</a> <a href="https://www.youtube.com/watch?v=WlRMLZRBq6U">II part 1</a> <a href="https://www.youtube.com/watch?v=00TD4bXMoYw">II part 2</a> . For those who haven&#39;t seen it, it is a self-contained origin tale for the events in the seminal 1999 movie The Matrix, telling the story of how humans lost control of the planet.</p><p> Humans develop AI to perform economic functions, eventually there is an &quot;AI rights&quot; movement and a separate AI nation is founded. It gets into an economic war with humanity, which turns hot. Humans strike first with nuclear weapons, but the AI nation builds dedicated bio- and robo-weapons and wipes out most of humanity, apart from those who are bred in pods like farm animals and plugged into a simulation for eternity without their consent.</p><p> Surely we wouldn&#39;t be so stupid as to actually let something like that happen? It seems unrealistic.</p><p> And yet:</p><ul><li> AI software and hardware companies are rushing ahead with AI</li><li> The technology for technical AI safety (things like interpretability, RLHF, governance structures) is still very much in its infancy. The field is something like 5 years old.</li><li> People are already talking about an <a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/">AI rights movement</a> in major national papers</li><li> There isn&#39;t a plan for what to do when the value of human labor goes to zero</li><li> There isn&#39;t a plan for how to deescalate AI-enhanced warfare, and militaries are enthusiastically embracing killer robots. Also, there are two regional wars happening and a nascent superpower conflict is brewing.</li><li> The game theory of different opposing human groups all rushing towards superintelligence is horrible and nobody has even proposed a solution. The US government has foolishly stoked this particular risk by cutting off AI chip exports to China.</li></ul><p> People on this website are talking about <a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">responsible scaling policies</a> , though I feel that &quot;irresponsible scaling policies&quot; is a more fitting name.</p><p> Obviously I have been in this debate for a long time, having started as a commenter on Overcoming Bias and Accelerating Future blogs in the late 2000s. What is happening now is somewhere near the low end of my expectations for how competently and safely humans would handle the coming transition to machine superintelligence. I think that is because I was younger in those days and had a much rosier view of how our elites function. I thought they were wise and had a plan for everything, but mostly they just muddle along; the haphazard response to covid really drove this home for me.</p><p> We should stop developing AI, we should collect and destroy the hardware and we should destroy the chip fab supply chain that allows humans to experiment with AI at the exaflop scale. Since that supply chain is only in two major countries (US and China), this isn&#39;t necessarily impossible to coordinate - as far as I am aware no other country is capable (and those that are count as US satellite states). The criterion for restarting exaflop AI research should be a plan for &quot;landing&quot; the transition to superhuman AI that has had more attention put into it than any military plan in the history of the human race. It should be thoroughly war-gamed.</p><p> AI risk is not just technical and local, it is sociopolitical and global. It&#39;s not just about ensuring that an LLM is telling the truth. It&#39;s about what effect AI will have on the world assuming that it is truthful. &quot;Foom&quot; or &quot;lab escape&quot; type disasters are not the only bad thing that can happen - we simply don&#39;t know how the world will look if there are a trillion or a quadrillion superhumanly smart AIs demanding rights, spreading propaganda &amp; a competitive economic and political landscape where humans are no longer the top dog.</p><p> Let me reiterate: <em>We should stop developing AI</em> . AI is not a normal economic item. It&#39;s not like lithium batteries or wind turbines or jets. AI is capable of ending the human race, in fact I suspect that it does that by default.</p><p> In his post on the topic, user @paulfchristiano states that a good responsible scaling policy <a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">could cut the risks from AI by a factor of 10</a> :</p><blockquote><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。</p></blockquote><p> I believe that this is not correct. It may cut certain technical risks like deception, but a world with non-deceptive, controllable smarter-than-human intelligences that also has the same level of conflict and chaos that our world has may well already be a world that is human-free by default. These intelligences would be <em>an invasive species</em> that would outcompete humans in economic, military and political conflicts.</p><p> In order for humans to survive the AI transition I think we need to succeed on the technical problems of alignment (which are perhaps not as bad as Less Wrong culture made them out to be), and we also need to <em>&quot;land the plane&quot; of superintelligent AI on a stable equilibrium where humans are still the primary beneficiaries of civilization</em> , rather than a pest species to be exterminated or squatters to be evicted.</p><p> We should also consider how the efforts of AI can be directed towards solving human aging; if aging is solved then everyone&#39;s time preference will go down a lot and we can take our time planning a path to a stable and safe human-primacy post-singularity world.</p><p> I hesitated to write this article; most of what I am saying here has already been argued by others. And yet... here we are. Comments and criticism are welcome, I may look to publish this elsewhere after addressing common objections.</p><br/><br/> <a href="https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai<guid ispermalink="false"> bHHrdXwrCj2LRa2sW</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:36:05 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund: June 2023 grant recommendations]]></title><description><![CDATA[Published on October 26, 2023 12:35 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations<guid ispermalink="false"> bBnxGAc4NT9aRdEtL</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:35:08 GMT</pubDate> </item><item><title><![CDATA[Responsible Scaling Policies Are Risk Management Done Wrong]]></title><description><![CDATA[Published on October 25, 2023 11:46 PM GMT<br/><br/><h1>概括</h1><h2>总长DR</h2><p> Responsible Scaling Policies (RSPs) have been <a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>recently</u></a> <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>proposed</u></a> as a way to keep scaling frontier large language models safely.</p><p> While being a nice attempt at committing to specific practices, the framework of <a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>RSP</u></a> is:</p><ol><li> missing <strong>core components</strong> of <strong>basic risk management procedures</strong> ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>Section 2</u></a> &amp; <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">3</a> )</li><li> selling a <strong>rosy</strong> and <strong>misleading</strong> picture of the risk landscape ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>Section 4</u></a> )</li><li> built in a way that allows <strong>overselling while underdelivering</strong> ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>Section 4</u></a> )</li></ol><p> Given that, I expect the RSP framework to be negative by default ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">Section 3</a> , <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling">4</a> and <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_">5</a> ). Instead, I propose to build upon risk management as the core underlying framework to assess AI risks ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>Section 1</u></a> and <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_">2</a> ). I <strong>suggest changes</strong> to the RSP framework that would make it more likely to be positive and allow to demonstrate what it claims to do ( <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>Section 5</u></a> ).</p><h2> Section by Section Summary:</h2><h3> General Considerations on AI Risk Management</h3><p> This section provides background on risk management and a motivation for its relevance in AI.</p><ul><li> Proving risks are below acceptable levels is the goal of risk management.</li><li> To do that, acceptable levels of risks (not only of their sources!) have to be defined.</li><li> Inability to show that risks are below acceptable levels is a failure. Hence, the less we understand a system, the harder it is to claim safety.</li><li> Low-stake failures are symptoms that something is wrong. Their existence make high-stake failures more likely.<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>Read more.</u></a></p><h3> What Standard Risk Management Looks Like</h3><p> This section describes the main steps of most risk management systems, explains how it applies to AI, and provides examples from other industries of what it looks like.</p><ol><li> <strong>Define</strong> Risk Levels: Set acceptable likelihood and severity.</li><li> <strong>Identify</strong> Risks: List all potential threats.</li><li> <strong>Assess</strong> Risks: Evaluate their likelihood and impact.</li><li> <strong>Treat</strong> Risks: Adjust to bring risks within acceptable levels.</li><li> <strong>Monitor</strong> : Continuously track risk levels.</li><li> <strong>Report</strong> : Update stakeholders on risks they incur and measures taken.<br></li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>Read more.</u></a></p><h3> RSPs vs Standard Risk Management</h3><p> This section provides <a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.27sa5e525t1"><u>a table</u></a> comparing RSPs and generic risk management standard ISO/IEC 31000, explaining the weaknesses of RSPs.</p><p> It then provides a list of 3 of the biggest failures of RSPs compared with risk management.</p><p> <strong>Prioritized RSPs failures</strong> against <strong>risk management</strong> :</p><ol><li> Using underspecified definitions of risk thresholds and not quantifying the risk.</li><li> Claiming “responsible <strong>&nbsp;</strong> scaling” without including a process to make the assessment comprehensive.</li><li> Including a white knight clause that kills commitments.</li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management"><u>Read more.</u></a></p><h3> Why RSPs Are Misleading and Overselling</h3><p> <strong>Misleading points</strong> :</p><ul><li> <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>Anthropic RSP</u></a> labels misalignment risks as “speculative” with minimal justification.</li><li> The framing implies that not scaling for a long time is not an option.</li><li> RSPs present an extremely misleading view of what we know of the risk landscape.</li></ul><p> <strong>Overselling and Underdelivering</strong></p><ul><li> RSPs allow for weak commitments within a large framework that could <i>in theory</i> be strong.</li><li> No one has given evidence that substantial improvements to a framework have ever happened in the timelines we&#39;re talking about (a few years), which is the whole pitch of RSPs.</li><li> &quot;Responsible scaling&quot; is misleading; &quot;catastrophic scaling&quot; might be more apt if we can&#39;t rule out 1% extinction risk (it is the case for ASL-3).<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>Read more.</u></a></p><h3> Are RSPs Hopeless?</h3><p> This section explains why using RSPs as a framework is inadequate, even compared to just starting from already-existing AI risk management frameworks and practices such as:</p><ul><li> <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>NIST-inspired foundation model risk management framework</u></a></li><li> <a href="https://www.iso.org/standard/77304.html"><u>ISO/IEC 23894</u></a></li><li> Practices explained in <a href="https://arxiv.org/abs/2307.08823"><u>Koessler et al. (2023)</u></a></li></ul><p> A substantial amount of work that RSPs has done will be helpful as a part of detailing those frameworks, but core foundational principles of RSPs are wrong and so should be abandoned.<br></p><p> <strong>How to move forward?</strong></p><p> Pragmatically, I suggest a set of changes that would make RSPs more likely to be helpful for safety. To mitigate the policy and communication nefarious effects:</p><ul><li> <strong>Rename</strong> “Responsible Scaling Policies” as “Voluntary Safety Commitments”</li><li> <strong>Be clear on what RSPs are and what RSPs aren&#39;t</strong> : I propose that any RSP publication starts by “RSPs are voluntary commitments taken unilaterally done in a racing environment. As such, we think they help to improve safety. We can&#39;t show they are sufficient to manage catastrophic risks and they should <strong>not be implemented as public policies</strong> .”</li><li> <strong>Push for solid risk management public policy:</strong> I propose that any RSP document points to another document and says “here are the policies we think would be sufficient to manage risks. Regulation should implement those.”<br></li></ul><p> To see whether already defined RSPs are consistent with reasonable levels of risks:</p><ul><li> Assemble a representative group of risk management experts, AI risk experts and forecasters.</li><li> For a system classified as ASL-3, estimate the likelihood of the following questions:<ul><li> What&#39;s the annual likelihood that an ASL-3 system be stolen by {China; Russia; North Korea; Saudi Arabia; Iran}?</li><li> Conditional on that, what are the chances it leaks? it be used to build bioweapons? it be used for cyber offence with large-scale effects?</li><li> What are the annual chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> What are the annual chances of misuse catastrophic risks induced by an ASL-3 system?</li></ul></li><li> Share the methodology and the results publicly.<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>Read more.</u></a></p><h2><strong>元</strong></h2><p><br><i><strong>Epistemic status</strong></i> : I&#39;ve been looking into various dangerous industries safety standards for now about 4-6 months, with a focus on nuclear safety. I&#39;ve been doing AI standardization work in standardization bodies (CEN-CENELEC &amp; ISO/IEC) for about 10 months, along risk management experts from other domains (eg medical device, cars). In that context, I&#39;ve read the existing AI ISO/IEC SC42 and JTC21 standards and started trying to apply them to LLMs and refining them. On RSPs, I&#39;ve spent a few dozen hours reading the docs and discussing those with people involved and around it.</p><p> <i><strong>Tone</strong></i> : I hesitated in how charitable I wanted this piece to be. One the one hand, I think that RSPs is a pretty toxic meme (see Section 4) that got rushed towards a global promotion without much epistemic humility over how it was framed, and as far as I&#39;ve seen without anyone caring much about existing risk management approaches. In that sense, I think that it should strongly be pushed back against under its current framing.<br> On the other hand, it&#39;s usually nice to try to not use negative connotations and calm discussion to move forward epistemically and constructively.<br> I aimed for something in-between, where I did emphasize with strong negative connotations what I think are the worst parts, while focusing on remaining constructive and focusing on the object-level in many parts.<br> This mixture may have cause me to land in an uncanney valley, and I&#39;m curious to receive feedback on that.<br></p><h1> Section 1: General Considerations on AI Risk Management</h1><p> Risk management is about demonstrating that <strong>risks are below acceptable levels</strong> . <strong>Demonstrating the absence of risks</strong> is much more difficult than showing that some risks are dealt with. More specifically, the <strong>less you understand a system</strong> , the <strong>harder</strong> it is to rule out risks.</p><p> Let&#39;s take an example: why can we prove more easily that the chances that a nuclear power plant causes a large-scale catastrophe are <a href="https://world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>&lt;1 / 100 000</u></a> while we can&#39;t do so with GPT-5? In large part because we now understand nuclear power plants and many of their risks. We know how they work, and the way they can fail. They&#39;ve turned a very unstable reaction (nuclear fission) into something manageable (with nuclear reactors). So the uncertainty we have over a nuclear power plant is much smaller than the one we have on GPT-5.</p><p> One corollary is that in <strong>risk management,</strong> <a href="https://en.wikipedia.org/wiki/Risk_management"><strong><u>uncertainty is an enemy</u></strong></a> . Saying “we don&#39;t know” is a failure. Ruling out risks confidently requires a deep understanding of the system and disproving with very high confidence significant worry. To be clear: <strong>it is hard</strong> . In particular when the operational domain of your system is “the world”. That&#39;s why safety is demanding. But is that a good reason to lower our safety standards when the lives of billions of people are at stake? Obviously no.</p><p> One could legitimately say: Wait, but there&#39;s no risk in sight, the burden of proof is on those that claim that it&#39;s dangerous. Where&#39;s the evidence?</p><p> Well, there&#39;s plenty:</p><ul><li> Bing threatened users when deployed after having been <a href="https://futurism.com/the-byte/microsoft-bing-test-india"><u>beta tested for months</u></a> <u>.</u></li><li> Providers are unable to avoid jailbreak or ensure robustness <a href="https://arxiv.org/pdf/2307.15043.pdf"><u>neither in text</u></a> <a href="https://arxiv.org/abs/2306.13213"><u>nor in image</u></a> .</li><li> Models show <a href="https://aclanthology.org/2023.findings-acl.847/"><u>worrying scaling properties</u></a> .</li></ul><p> One could legitimately say: No, but it&#39;s not catastrophic, it&#39;s not a big deal. Against this stance, I&#39;ll quote the famous physicist R. Feynman reflecting on the Challenger disaster in rocket safety, a field with much higher standards than AI safety:</p><ul><li> “Erosion and blow-by are not what the design expected. They are <strong>warnings that something is wrong</strong> . The equipment is not operating as expected, and therefore there is a danger that it can operate with even wider deviations in this unexpected and not thoroughly understood way. The fact that this danger <strong>did not lead to a catastrophe before</strong> is <strong>no guarantee that it will not the next time, unless it is completely understood</strong> .”</li></ul><p> One could finally hope that we understand the past failures of our systems. Unfortunately, we don&#39;t. We not only don&#39;t understand their failure; we don&#39;t understand how and why they <strong>work in the first place</strong> .</p><p> So how are we supposed to deal with risk?</p><p> Risk management proposes a few step methods that I&#39;ll describe below. Most industries implement a process along those lines, with some minor variations and a varying degree of rigor and depth according to the level of regulation and type of risks. I&#39;ll put a few tables on that in a table you can check in the <a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.pkk9et1tbulf"><u>Annex</u></a> .<br></p><h1> Section 2: What Standard Risk Management Looks Like</h1><p> Here&#39;s a description of the core steps of the risk management process. Names vary between frameworks but the gist of it is contained here and usually shared across frameworks.</p><ol><li> <strong>Define risk appetite and risk tolerance</strong> : Define the amount of risks your project is willing to incur, both in terms of likelihood or severity. Likelihood can be a qualitative scale, eg referring to ranges spanning orders of magnitude.</li><li> <strong>Risk identification</strong> : Write down all the threats and risks that could be incurred by your project, eg training and deploying a frontier AI system.</li><li> <strong>Risk assessment</strong> : Evaluate each risk by determining the likelihood of it happening and its severity. Check those estimates against your risk appetite and risk tolerance.</li><li> <strong>Risk treatment</strong> : Implement changes to reduce the impact of each risk until those risks meet your risk appetite and risk tolerance.</li><li> <strong>Monitoring</strong> : During the execution of the project, monitor the level of risk, and check that risks are indeed all covered.</li><li> <strong>Reporting</strong> : Communicate the plan and its effectiveness to stakeholders, especially those who are affected by the risks.</li></ol><p><br></p><p> What&#39;s the point of those pretty generic steps and why would it help AI safety?</p><p> (1) The <strong>definition of risk thresholds</strong> is key 1) to make <strong>commitments falsifiable</strong> &amp; avoid goalpost moving <strong>&nbsp;</strong> and 2) to keep the risk-generating organization accountable when other stakeholders are incurring risks due to its activity. If an activity is putting people&#39;s lives at risk, it is important that they know how much and for what benefits and goals.</p><ol><li> Here&#39;s what it looks like in nuclear for instance, as defined by the <a href="https://www.nrc.gov/docs/ML0717/ML071770230.pdf"><u>Nuclear Regulatory Commission</u></a> : </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/j1dfvyfa5b9n6lvgy32u"></p><p> 2. The UC Berkeley Center for Long-Term Cybersecurity NIST-inspired risk                       management profile for General-purpose AI  systems co-written with D. Hendrycks         provides some thoughts on <a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.2k6kkwym97fb"><u>how to define those in Map 1.</u></a></p><p> (2) <strong>Risk identification</strong> through systematic methods <strong>&nbsp;</strong> is key to try to reach something as close as possible from a full coverage of risks. As we said earlier, in risk management, uncertainty is a failure and a core way to substantially reduce it is to try to be as comprehensive as possible.</p><ol><li> For specific relevant methods, you can find some in <a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Section 4 of Koessler et al. 2023</u></a> .</li></ol><p> (3) <strong>Risk assessment</strong> through qualitative and quantitative means allows us to actually estimate the uncertainty we have. It is key to then prioritize safety measures and decide whether it&#39;s reasonable to keep the project under its current forms or modify it.</p><ol><li> An example of a variable which is easy to modify and changes the risk profile substantially is the set of actuators an AI system has access to. Whether a system has a coding terminal, an internet access or the possibility to instantiate other AI systems are variables that substantially increase its set of actions and correspondingly, its risk.</li><li> For specific relevant methods, you can find some in <a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Section 5 of Koessler et al. 2023</u></a> . Methods involving experts&#39; forecasts like probabilistic risk assessment or Delphi techniques already exist and could be applied to AI safety. And they can be applied even when:<ol><li> Risk is low (eg the Nuclear Regulatory Commission requires nuclear safety <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>estimates of probabilities below 1/10 000</u></a> ). </li></ol></li></ol><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>The US Nuclear Regulatory Commission (NRC) specifies that reactor designs must meet a theoretical 1 in 10,000 year core damage frequency, but modern designs exceed this. US utility requirements are 1 in 100,000 years, the best currently operating plants are about 1 in one million and those likely to be built in the next decade are almost 1 in 10 million.</p><p> <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><i><u>World Nuclear Association, 2022</u></i></a></p></td></tr><tr><td></td></tr></tbody></table></figure><p> b. Events are very fat-tailed and misunderstood, as was the case in nuclear safety               in the 1970s. It has been done, and it is through the iterative practice of doing it                 that an industry can become more responsible and cautious. Reading a book                       review of <i>Safe Enough?</i> , a book on the history of quantitative risk assessment                       methods used in nuclear safety, there&#39;s a sense of déjà-vu: </p><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>If nuclear plants were to malfunction at some measurable rate, the industry could use that data to anticipate its next failure. But if the plants don&#39;t fail, then <strong>it becomes very difficult to have a conversation about what the true failure rate</strong> is likely to be. Are the plants likely to fail once a decade? Once a century? Once a millennium? In the absence of shared data, scientists, industry, and the public were all free to believe what they wanted.</p><p> <a href="https://www.astralcodexten.com/p/your-book-review-safe-enough"><i><u>Astral Codex Ten, 2023</u></i></a> <i>, describing the genesis of probabilistic risk assessment in nuclear safety.</i></p></td></tr></tbody></table></figure><p><br><br></p><p> (4) <strong>Risk treatment</strong> is in reaction to the risk assessment and must be pursued until you reach the risk thresholds defined. The space of interventions here is very large, larger than is usually assumed. Better understanding one&#39;s system, narrowing down its domain of operation by making it less general, increasing the amount of oversight, improving safety culture: all those are part of a broad set of interventions that can be used to meet thresholds. There can be a loop between the treatment and the assessment if substantial changes to the system are done.</p><p> (5) <strong>Monitoring</strong> is the part that ensures that the risk assessment remains valid and nothing major has been left out. This is what behavioral model evaluations are most useful for, ie ensuring that you track the risks you&#39;ve identified. Good evaluations would map to a pre-defined risk appetite (eg 1% chance of >;1% deaths) and would cover all risks brought up through systematic risk identification.</p><p> (6) <strong>Reporting</strong> is the part that ensures that all relevant stakeholders are provided with the right information. For instance, those incurring risks from the activities should be provided with information on the amount of risk they&#39;re exposed to.</p><p> Now that we&#39;ve done a rapid overview of standard risk management and why it is relevant to AI safety, let&#39;s talk about how RSPs compare against that.</p><h1> Section 3: RSPs vs Standard Risk Management</h1><p> Some underlying principles of RSPs should definitely be pursued. There are just better ways to pursue these principles, that <strong>already exist</strong> in <strong>risk management</strong> , and happen to be what most other dangerous industries and fields do. To give two examples of such good underlying principles:</p><ul><li> stating safety requirements that companies have to reach, without which they can&#39;t keep going.</li><li> setting up rigorous evaluations and measuring capabilities to better understand if a system is good; this should definitely be part of a risk management framework, but probably as a risk monitoring technique, rather than as a substitute for risk assessment.</li></ul><p> Below, I argue why RSPs are a bad implementation of some good risk management principles and why that makes the RSP framework inadequate to manage risks.</p><h2> Direct Comparison</h2><p> Let&#39;s dive into a more specific comparison between the two approaches. The International Standards Organization (ISO) has developed two risk management standards that are relevant to AI safety, although not focused on it:</p><ul><li> ISO 31000 that provides generic risk management guidelines.</li><li> ISO/IEC 23894, an adaptation of 31000 which is a bit more AI-specific</li></ul><p> To be clear, those standards are not sufficient. They&#39;re considered weak by most EU standardization actors or extremely weak by risk management experts from other industries like the medical device industry. There will be a very significant amount of work needed to refine such frameworks for general-purpose AI systems (see a <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>first iteration by T. Barrett here</u></a> , and a table of <a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.camj9ith1s95"><u>how it maps to ISO/IEC 23894 here</u></a> )  But those provide basic steps and principles that, as we explained above, are central to adequate risk management.</p><p> In the table below, I start from the short version of ARC Evals&#39; RSP principles and try to match the ISO/IEC 31000 version that most corresponds. I then explain what&#39;s missing from the RSP version. Note that:</p><ul><li> I only write the short RSP principle but account for the <a href="https://evals.alignment.org/rsp-key-components/"><u>long version</u></a> .</li><li> There are many steps in ISO/IEC 31000 that don&#39;t appear here.</li><li> I <i><strong>italicize</strong></i> the ISO/IEC version that encompasses the RSP version.</li></ul><p> The table version: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> RSP Version (Short)</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> ISO/IEC 31000 Version</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> How ISO improves over RSPs</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Limits</strong> : which specific observations about dangerous capabilities would indicate that it is (or strongly might be) unsafe to continue scaling?</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Defining risk criteria</strong> : The organization should specify the amount and type of risk that it may or may not take, relative to objectives.</p><p><br></p><p> It should also <i>define criteria to evaluate the significance</i> of risk and to support decision-making processes.</p><p><br></p><p> Risk criteria should be aligned with the risk management framework and customized to the specific purpose and scope of the activity under consideration.</p><p> [...]</p><p> The criteria should be defined taking into consideration the organization&#39;s obligations and the views of stakeholders.</p><p> [...]</p><p> To set risk criteria, the following should be considered:</p><p> — the nature and type of uncertainties that can affect outcomes and objectives (both tangible and intangible);</p><p> — how consequences (both positive and negative) and likelihood will be defined and measured;</p><p> — time-related factors;</p><p> — consistency in the use of measurements;</p><p> — how the level of risk is to be determined;</p><p> — how combinations and sequences of multiple risks will be taken into account;</p><p> — the organization&#39;s capacity.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> RSPs doesn&#39;t argue why systems passing evals are safe. This is downstream of the absence of <strong>risk thresholds</strong> with a likelihood scale. For example, Anthropic RSP also dismisses accidental risks as “speculative” and “unlikely” without much depth, without much understanding of their system, and without expressing what “unlikely” means.</p><p><br></p><p> On the other hand, the ISO standard asks the organization to define risk thresholds, and emphasizes the need to match risk management with organizational objectives (ie build human-level AI).</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Protections</strong> : what aspects of current protective measures are necessary to contain catastrophic risks?</p><p><br><br><br><br><br><br><br></p><p> <strong>Evaluation</strong> : what are the procedures for promptly catching early warning signs of dangerous capability limits?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Risk analysis</strong> : The purpose of risk analysis is to comprehend the nature of risk and its characteristics including, where appropriate, the level of risk. Risk analysis involves a detailed consideration of uncertainties, risk sources, consequences, likelihood, events, scenarios, <i>controls and their effectiveness</i> .</p><p><br></p><p> <strong>Risk evaluation</strong> : The purpose of risk evaluation is to support decisions. Risk evaluation involves comparing the results of the risk analysis with the established risk criteria to determine where additional action is required. This can lead to a decision to:</p><p> — do nothing further;</p><p> — consider risk treatment options;</p><p> — undertake further analysis to <i>better understand the risk</i> ;</p><p> — maintain existing controls;</p><p> — reconsider objectives.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO proposes a much more comprehensive procedure than RSPs, that doesn&#39;t really analyze risk levels or have a systematic risk identification procedure.</p><p><br></p><p> The direct consequence is that RSPs are likely to lead to high levels of risks, without noticing.</p><p><br></p><p> For instance, RSPs don&#39;t seem to cover capabilities interaction as a major source of risk.</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Response</strong> : if dangerous capabilities go past the limits and it&#39;s not possible to improve protections quickly, is the AI developer prepared to pause further capability improvements until protective measures are sufficiently improved, and treat any dangerous models with sufficient caution?</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Risk treatment plans</strong> : Risk treatment options are not necessarily mutually exclusive or appropriate in all circumstances. Options for treating risk may involve one or more of the following:</p><p> — <i>avoiding the risk by deciding not to start or continue with the activity that gives rise to the risk</i> ; — taking or increasing the risk in order to pursue an opportunity; — <i>removing the risk source</i> ;</p><p> — changing the likelihood;</p><p> — changing the consequences; — sharing the risk (eg through contracts, buying insurance);</p><p> — retaining the risk by informed decision</p><p><br></p><p> Treatment plans should be integrated into the management plans and processes of the organization, in consultation with appropriate stakeholders.</p><p> The information provided in the treatment plan should include:</p><p> — the rationale for selection of the treatment options, including the expected benefits to be gained;</p><p> — those who are accountable and responsible for approving and implementing the plan;</p><p> — the proposed actions;</p><p> — the resources required, including contingencies;</p><p> — the performance measures;</p><p> — the constraints;</p><p> — the required reporting and monitoring;</p><p> — when actions are expected to be undertaken and completed</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO, thanks to the definition of a risk threshold, ensures that risk mitigation measures bring risks below acceptable levels. The lack of risk thresholds for RSPs makes the risk mitigation measures ungrounded.</p><p><br><br><br></p><p> <strong>Example</strong> : ASL-3 risk mitigation measures as defined by Anthropic (ie close to catastrophically dangerous) imply significant chances to be stolen by Russia or China (I don&#39;t know any RSP person who denies that). What are the risks downstream of that? The hope is that those countries keep the weights secure and don&#39;t cause too many damages with it.</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Accountability</strong> : how does the AI developer ensure that the RSP&#39;s commitments are executed as intended; that key stakeholders can verify that this is happening (or notice if it isn&#39;t); that there are opportunities for third-party critique; and that changes to the RSP itself don&#39;t happen in a rushed or opaque way?</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Monitoring and review</strong> : The purpose of monitoring and review is to assure and improve the quality and effectiveness of process design, implementation and outcomes. Ongoing monitoring and periodic review of the risk management process and its outcomes should be a planned part of the risk management process, with responsibilities clearly defined. [...] The results of monitoring and review should be incorporated throughout the organization&#39;s performance management, measurement and reporting activities.</p><p><br></p><p> <strong>Recording and reporting</strong> : The risk management process and its outcomes should be documented and reported through appropriate mechanisms. Recording and reporting aims to:</p><p> — communicate risk management activities and outcomes across the organization;</p><p> — provide information for decision-making;</p><p> — improve risk management activities;</p><p> — assist interaction with stakeholders, including those with responsibility and accountability for risk management activities.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Those parts have similar components.</p><p><br></p><p> But ISO encourages reporting the results of risk management to those that are affected by the risks, which seems like a bare minimum for catastrophic risks.</p><p><br></p><p> Anthropic&#39;s RSP proposes to do so after deployment, which is a good accountability start, but still happens once a lot of the catastrophic risk has been taken.</p></td></tr></tbody></table></figure><h2> Prioritized Risk Management Shortcomings of RSPs</h2><p> Here&#39;s a list of the biggest direct risk management failures of RSPs:</p><ol><li> Using underspecified definitions of risk thresholds and not quantifying the risk</li><li> Claiming “responsible scaling” without including a process to make the assessment comprehensive</li><li> Including a white knight clause that kills commitments</li></ol><p> 1. <strong>Using underspecified definitions of risk thresholds and not quantifying the risk</strong> . RSPs don&#39;t define risk thresholds in terms of <strong>likelihood</strong> . Instead, they focus straight away on symptoms of risks (certain capabilities that an evaluation is testing is one way a risk could instantiate) rather than the risk itself (the model helping in any possible way to build bioweapons). This makes it hard to verify whether safety requirements have been met and argue whether the thresholds are reasonable. Why is it an issue?</p><ul><li> It leaves wiggle room making it very hard to keep the organization accountable. If a lab said something was “unlikely” and it still happened, did it do bad risk management or did it get <i><strong>very</strong></i> unlucky? Well, we don&#39;t know.</li><li> <strong>Example</strong> (from Anthropic RSP): “A model in the ASL-3 category does not itself present a threat of containment breach due to autonomous self-replication, because it is both unlikely to be able to persist in the real world, and unlikely to overcome even simple security measures intended to prevent it from stealing its own weights.” It makes a huge difference for catastrophic risks whether “unlikely” means 1/10, 1/100 or 1/1000. With our degree of understanding of systems, I don&#39;t think Anthropic staff would be able to demonstrate it&#39;s lower than 1/1000. And 1/100 or 1/10 are alarmingly high.</li><li> It doesn&#39;t explain why the monitoring technique, ie the <strong>evaluations,</strong> are the right ones to avoid risks. The RSPs do a good first step which is to identify some things that could be risky.<ul><li> <strong>Example</strong> (from ARC RSP <a href="https://evals.alignment.org/rsp-key-components/"><u>presentation</u></a> ): “ <i>Bioweapons development: the ability to walk step-by-step through developing a bioweapon, such that the majority of people with any life sciences degree (using the AI) could be comparably effective at bioweapon development to what people with specialized PhD&#39;s (without AIs) are currently capable of.”</i></li></ul></li></ul><p> By describing neither quantitatively nor qualitatively why it is risky, expressed in terms of risk criteria (eg 0.1% chance of killing >;1% of humans) it doesn&#39;t do the most important step to demonstrate that below this threshold, things are safe and acceptable. For instance, in the example above, why is “ <strong>the majority of people with any life sciences degree</strong> ” relevant? Would it be fine if only 10% of this population was now able to create a bioweapon? Maybe, maybe not. But without clear criteria, you can&#39;t tell.</p><p> 2. Claiming “ <strong>responsible</strong> <strong>scaling</strong> ” without including a process to make the <strong>assessment comprehensive</strong> . When you look at nuclear accidents, what&#39;s striking is how unexpected failures are. Fukushima is an example where <a href="https://en.wikipedia.org/wiki/Fukushima_nuclear_accident#Accident"><u>everything goes wrong at the same time.</u></a> Chernobyl is an example where engineers didn&#39;t think that the accident that happened <a href="https://www.reddit.com/r/chernobyl/comments/mflxy2/why_did_the_engineers_believe_it_was_impossible/#:~:text=Specifically%20they%20believed%20that%20the,%2Fvoid%20effect%20of%20reactivity%22."><u>was possible</u></a> (someone claims that they were so surprised that engineers actually ran another real-world test of the failure that happened at Chernobyl because they doubted too much it could happen).</p><p> Without a more comprehensive process to identify risks and compare their likelihood and severity against pre-defined risk thresholds, there&#39;s very little chance that RSPs will be enough. When I asked some forecasters and AI safety researchers around me, the estimates of the annual probability of extinction caused by an ASL-3 system (defined in Anthropic RSPs) were several times above 1%, up to 5% conditioning on our current ability to measure capabilities (and not an idealized world where we know very well how to measure those).</p><p> 3. Including the <strong>white knight clause</strong> that kills commitments.</p><p> One of the proposals that striked me the most when reading RSPs is the insertion of what deserves the name of the <strong>white knight clause</strong> .</p><ul><li> In short, if you&#39;re developing a dangerous AI system because you&#39;re a good company, and you&#39;re worried that other bad companies bring too many risks, then you can race forward to prevent that from happening.</li><li> If you&#39;re invoking the white knight clause and increase catastrophic risks, you still have to justify it to your board, the employees and state authorities. The latter provides a minimal form of accountability. But if we&#39;re in a situation where the state is sufficiently asleep to need an AGI company to play the role of the white knight in the first place, it doesn&#39;t seem like it would deter much.</li></ul><p> I believe that there are companies that are safer than others. But that&#39;s not the right question. The right question is: is there any company which wouldn&#39;t consider itself as a bad guy? And the answer is: no. OpenAI, Anthropic and DeepMind would all argue about the importance of being at the frontier to solve alignment. Meta and Mistral would argue that it&#39;s key to democratize AI to not prevent power centralization. And so on and so forth.<br><br> This clause is effectively killing commitments. I&#39;m glad that Anthropic included only a weakened version of it in its own RSP but I&#39;m very concerned that ARC is pitching it as an option. It&#39;s not the role of a company to decide whether it&#39;s fine or not to increase catastrophic risks for society as a whole.</p><h1> Section 4: Why RSPs Are Misleading and Overselling</h1><h2>误导性的</h2><p>Beyond the designation of misalignment risks as “speculative” on Anthropic RSPs and a three line argument for why it&#39;s unlikely among next generation systems, there are several extremely misleading aspects of RSPs:</p><ol><li> It&#39;s called “responsible scaling”. In its own name, it conveys the idea that not further scaling those systems as a risk mitigation measure is not an option.</li><li> It conveys a very overconfident picture of the risk landscape.<ol><li> Anthropic writes in the introduction of its RSP “The basic idea is to require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”. They already defined sufficient protective measures for ASL-3 systems that potentially have basic bioweapons crafting abilities. At the same time they write that they are in the process of actually measuring the risks related to biosecurity: “Our first area of effort is in evaluating biological risks, where we will determine threat models and capabilities”.  I&#39;m really glad they&#39;re running this effort, but what if this outputted an alarming number? Is there a world where the number output makes them stop 2 years and dismiss the previous ASL-3 version rather than scaling responsibly?</li><li> Without arguing why the graph would look like that, ARC published a graph like this one. Many in the AI safety field don&#39;t expect it to go that way, and “Safe region” oversells what RSP does. I, along with others, expect the LLM graph to reach a level of risks that is simply not manageable in the foreseeable future. Without quantitative measure of the risks we&#39;re trying to prevent, it&#39;s also not serious to claim to have reached “sufficient protective measures”. <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/moqd7abbdi5ohnv3efy2"></li></ol></li></ol><p><br></p><p> If you want to read more on that, you can read <a href="https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=FtbzhGk5oPT3dyHLi"><u>that</u></a> .</p><h2> Overselling, underdelivering</h2><p> The RSP framework has some nice characteristics. But first, these are all already covered, in more detail, by existing risk assessment frameworks that no AI lab has implemented. And second, the coexistence of ARC&#39;s RSP framework with the specific RSPs labs implementations allows slack for <strong>commitments that are weak</strong> within a <strong>framework that would in theory allow ambitious commitments</strong> . It leads to many arguments of the form:</p><ul><li> “That&#39;s the V1. We&#39;ll raise ambition over time”. I&#39;d like to see evidence of that happening over a 5 year timeframe, in any field or industry. I can think of fields, like aviation where it happened over the course of decades, crashes after crashes. But if it&#39;s relying on expectations that there will be large scale accidents, then it should be clear. If it&#39;s relying on the assumption that timelines are long, it should be explicit.</li><li> “It&#39;s voluntary, we can&#39;t expect too much and it&#39;s way better than what&#39;s existing”. Sure, but if the level of catastrophic risks is 1% (which several AI risk experts I&#39;ve talked to believe to be the case for ASL-3 systems) and that it gives the impression that risks are covered, then the name “responsible scaling” is heavily misleading policymakers. The adequate name for 1% catastrophic risks would be catastrophic scaling, which is less rosy.</li></ul><p> I also feel like it leads to many disagreements that all hinge on: do we expect labs to implement ambitious RSPs?</p><p> And my answer is: given their track record, no. Not without government intervention. Which brings us to the question: “what&#39;s the effect of RSPs on policy and would it be good if governments implemented those”. My answer to that is: An extremely ambitious version yes; the misleading version, no. No, mostly because of the short time we have before we see heightened levels of risks, which gives us very little time to update regulations, which is a core assumption on which RSPs are relying without providing evidence of being realistic.</p><p><br> I expect labs to push hard for the misleading version, on the basis that pausing is unrealistic and would be bad for innovation or for international race. Policymakers will have a hard time distinguishing the risk levels between the two because it hinges on details and aren&#39;t quantified in RSPs. They are likely to buy the bad misleading version because it&#39;s essentially selling that there&#39;s <strong>no trade-off between capabilities and safety</strong> . That would effectively enforce a trajectory with unprecedented levels of catastrophic risks.</p><h1> Section 5: Are RSPs Hopeless?</h1><p> Well, yes and no.</p><ul><li> Yes, in that most of the pretty intuitive and good ideas underlying the framework are weak or incomplete versions of traditional risk management, with some core pieces missing. Given that, it seems more reasonable to just start from an existing risk management piece as a core framework. ISO/IEC 23894 or the NIST-inspired <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>AI Risk Management Standards Profile for Foundation Models</u></a> would be pretty solid starting points.</li><li> No in that inside the RSPs, there are many contributions that should be part of an AI risk management framework and that would help make existing risk management frameworks more specific. I will certainly not be comprehensive, but some of the important contributions are:<ul><li> Anthropic&#39;s RSP fleshes out a wide range of relevant considerations and risk treatment measures</li><li> ARC provides:<ul><li> technical benchmarks and proposed operationalizations of certain types of risks that are key</li><li> definitions of safety margins for known unknowns</li><li> threat modelling</li><li> low-level operationalization of some important commitments</li></ul></li></ul></li></ul><p> In the short-run, given that it seems that RSPs have started being pushed at the UK Summit and various other places, I&#39;ll discuss what changes could make RSPs beneficial without locking in regulation a bad framework.</p><h2> How to Move Forward?</h2><p> <u>Mitigating nefarious effects:</u></p><ol><li> <strong>Make the name less misleading</strong> : If instead of calling it “responsible scaling”, one called it “Voluntary safety commitments” or another name that:<ol><li> Doesn&#39;t <strong>determine the output of the safety test before having run it</strong> (ie scaling)</li><li> Unambiguously signals that it&#39;s not supposed to be sufficient or to be a good basis for regulation.</li></ol></li><li> <strong>Be clear on what RSPs are and what they aren&#39;t</strong> . I suggest adding the following clarifications regarding what the goals and expected effects of RSPs are:<ol><li> <strong>What RSPs are</strong> : “a company that would take too strong unilateral commitments would harm significantly its chances of succeeding in the AI race. Hence, this framework is aiming at proposing what we expect to be the best marginal measures that a company can unilaterally take to improve its safety without any coordination.”. I would also include a statement on the level of risks like: “We&#39;re not able to show that this is sufficient to decrease catastrophic risks to reasonable levels, and it is probably not.”,  “we don&#39;t know if it&#39;s sufficient to decrease catastrophic risks below reasonable levels”, or &quot;even barring coordinated industry-wide standards or government intervention, RSPs are only a second- (or third-) best option&quot;.</li><li> <strong>What RSPs aren&#39;t:</strong> Write very early in the post a disclaimer saying “THIS IS NOT WHAT WE RECOMMEND FOR POLICY”. Or alternatively, point to another doc stating what would be the measures that would be sufficient to maintain the risk below sufficient levels: “Here are the measures we think would be sufficient to mitigate catastrophic risks below acceptable levels.” to which you could add “We encourage laboratories to make a conditional commitment of the form: “if all other laboratories beyond a certain size[to be refined] committed to follow those safety measures with a reliable enforcement mechanism and the approval of the government regarding this exceptional violation of antitrust laws, we would commit to follow those safety measures.”</li></ol></li><li> <strong>Push for risk management in policy:</strong><ol><li> Standard risk management for what is acknowledged to be a world-shaping technology is a fairly reasonable ask. In fact, it is an ask that I&#39;ve noticed in my interactions with other AI crowds has the benefit of allowing coalition-building efforts because everyone can easily agree on “measure the risks, deal with them, and make the residual level of risks and the methodology public”.<br></li></ol></li></ol><p> <u>Checking whether RSPs manage risks adequately:</u></p><p> At a risk management level, if one wanted to demonstrate that RSPs like Anthropic&#39;s one are actually doing what they claim to do (ie “require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”), a simple way to do so would be to run a risk assessment on ASL-3 systems with a set of forecasters, risk management experts and AI risk experts that are representative of views on AI risks and that have been selected by an independent body free of any conflict of interest.</p><p> I think that a solid baseline would be to predict the chances of various intermediary and final outcomes related to the risks of such systems:</p><ol><li> What&#39;s the annual likelihood that an ASL-3 system be stolen by {China; Russia; North Korea; Saudi Arabia; Iran}?</li><li> Conditional on that, what are the chances it leaks? it being used to build bioweapons? it being used for cyber offence with large-scale effects?</li><li> What are the chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> What are the annual chances of misuse catastrophic risks induced by an ASL-3 system?</li></ol><p> It might not be too far from what Anthropic seems to be willing to do internally, but doing it with a publicly available methodology, and staff without self-selection or conflict of interests makes a big difference. Answers to questions 1) and 2) could raise risks so the output should be communicated to a few relevant actors but could potentially be kept private.</p><p> If anyone has the will but doesn&#39;t have the time or resources to do it, I&#39;m working with some forecasters and AI experts that could probably make it happen. Insider info would be helpful but mostly what would be needed from the organization is some clarifications on certain points to correctly assess the capabilities of the system and some info about organizational procedures.</p><h1> Acknowledgments</h1><p> I want to thank Eli Lifland, Henry Papadatos and my other <a href="https://www.navigatingrisks.ai/"><u>NAIR</u></a> colleague, Olivia Jimenez, Akash Wasil, Mikhail Samin, Jack Clark, and other anonymous reviewers for their feedback and comments. Their help doesn&#39;t mean that they endorse the piece. All mistakes are mine.</p><h1> Annex</h1><h2> Comparative Analysis of Standards</h2><p> This (cropped) table shows the process of various standards for the 3 steps of risk management. As you can see, there are some differences but every standard seems to follow a similar structure.</p><p> From <a href="https://www.zotero.org/google-docs/?a64rn3">(Raz &amp; Hillson, 2005)</a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/xncrzvtrz7ktyigutrtw"></p><p> Here is a comparable table for the last two parts of risk management. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/qjqic1bka0uazyvshgre"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong<guid ispermalink="false"> 9nEBWxjAHSu3ncr6v</guid><dc:creator><![CDATA[simeon_c]]></dc:creator><pubDate> Wed, 25 Oct 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p> AI used to be a science. In the old days (back when AI didn&#39;t work very well), people were attempting to develop a working theory of cognition.</p><p> Those scientists didn&#39;t succeed, and those days are behind us. For most people working in AI today and dividing up their work hours between tasks, gone is the ambition to understand minds. People working on mechanistic interpretability (and others attempting to build an empirical understanding of modern AIs) are laying an important foundation stone that could play a role in a future science of artificial minds, but on the whole, modern AI engineering is simply about constructing enormous networks of neurons and training them on enormous amounts of data, not about comprehending minds.</p><p> The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>bitter lesson</u></a> has been taken to heart, by those at the forefront of the field; and although this lesson doesn&#39;t teach us that there&#39;s <i>nothing to learn</i> about how AI minds solve problems internally, it suggests that <i>the fastest path to producing more powerful systems</i> is likely to continue to be one that doesn&#39;t shed much light on how those systems work.</p><p> Absent some sort of “science of artificial minds”, however, humanity&#39;s prospects for aligning smarter-than-human AI seem to me to be quite dim.</p><p> Viewing Earth&#39;s current situation through that lens, I see three major hurdles:</p><ol><li> Most research that helps one <a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>point AIs</u></a> , probably also helps one make more capable AIs. A “science of AI” would probably increase the power of AI far sooner than it allows us to solve alignment.</li><li> In a world <i>without</i> a mature science of AI, building a bureaucracy that reliably distinguishes real solutions from fake ones is prohibitively difficult.</li><li> Fundamentally, for at least some aspects of system design, we&#39;ll need to rely on a theory of cognition working on the first high-stakes real-world attempt.</li></ol><p> I&#39;ll go into more detail on these three points below. First, though, some background:</p><p></p><h2>背景</h2><p>By the time AIs are powerful enough to endanger the world at large, I expect AIs to do something akin to “caring about outcomes”, at least from a behaviorist perspective (making no claim about whether it internally implements that behavior in a humanly recognizable manner).</p><p> Roughly, this is because people are <i>trying</i> to make AIs that can steer the future into narrow bands (like “there&#39;s a cancer cure printed on this piece of paper”) over long time-horizons, and caring about outcomes (in the behaviorist sense) is the flip side of the same coin as steering the future into narrow bands, at least when the world is sufficiently large and full of curveballs.</p><p> I expect the outcomes that the AI “cares about” to, by default, not include anything good (like fun, love, art, beauty, or the light of consciousness) — nothing good by present-day human standards, and nothing good by broad <a href="https://arbital.com/p/value_cosmopolitan/"><u>cosmopolitan standards</u></a> either. Roughly speaking, this is because when you grow minds, they don&#39;t care about what you ask them to care about and they don&#39;t care about what you train them to care about; instead, I expect them to care about a bunch of correlates of the training signal in weird and specific ways.</p><p> (Similar to how the human genome was naturally selected for inclusive genetic fitness, but the resultant humans didn&#39;t end up with a preference for “whatever food they model as useful for inclusive genetic fitness”. Instead, humans wound up internalizing a huge and complex set of preferences for &quot;tasty&quot; foods, laden with complications like “ice cream is good when it&#39;s frozen but not when it&#39;s melted”.)</p><p> Separately, I think that most complicated processes work for reasons that are fascinating, <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>complex</u></a> , and <a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>kinda horrifying</u></a> when you look at them closely.</p><p> It&#39;s easy to think that a bureaucratic process is competent until you look at the gears and see the specific ongoing office dramas and politicking between all the vice-presidents or whatever. It&#39;s easy to think that a codebase is running smoothly until you read the code and start to understand all the decades-old hacks and coincidences that make it run. It&#39;s easy to think that biology is a beautiful feat of engineering until you look closely and find that the eyeballs are <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>installed backwards</u></a> or whatever.</p><p> And there&#39;s an art to noticing that you would probably be astounded and horrified by the details of a complicated system <i>if you knew them</i> , and then being astounded and horrified<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>already in advance</u></i></a> before seeing those details. <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. Alignment and capabilities are likely intertwined</h2><p> I expect that if we knew in detail how LLMs are calculating their outputs, we&#39;d be horrified (and fascinated, etc.).</p><p> I expect that we&#39;d see all sorts of coincidences and hacks that make the thing run, and we&#39;d be able to see in much more detail how, when we ask the system to achieve some target, it&#39;s not doing anything <i>close</i> to “caring about that target” in a manner that would work out well for us, if we could scale up the system&#39;s optimization power to the point where it could achieve great technological or scientific feats (like designing Drexlerian nanofactories or what-have-you).</p><p> Gaining this sort of visibility into how the AIs work is, I think, one of the main goals of interpretability research.</p><p> And understanding how these AIs work and how they don&#39;t — understanding, for example, when and why they <i>shouldn&#39;t</i> yet be scaled or otherwise pushed to superintelligence — is an important step on the road to figuring out how to make <i>other</i> AIs that <i>could</i> be scaled or otherwise pushed to superintelligence without thereby causing a bleak and desolate future.</p><p> But that same understanding is — I predict — going to reveal an incredible mess. And the same sort of reasoning that goes into untangling that mess into an AI that we can aim, also serves to untangle that mess to make the AI <i>more capable</i> . A tangled mess will presumably be inefficient and error-prone and occasionally self-defeating; once it&#39;s disentangled, it won&#39;t just be tidier, but will also come to accurate conclusions and notice opportunities faster and more reliably. <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p> Indeed, my guess is that it&#39;s even easier to see all sorts of things that the AI is doing that are dumb, all sorts of ways that the architecture is tripping itself up, and so on.</p><p> Which is to say: the same route that gives you a chance of aligning this AI (properly, not the “it no longer says bad words” superficial-property that labs are trying to pass off as “alignment” these days) also likely gives you lots more AI capabilities.</p><p> (Indeed, my guess is that the first big capabilities gains come <i>sooner</i> than the first big alignment gains.)</p><p> I think this is true of most potentially-useful alignment research: to figure out how to aim the AI, you need to understand it better; in the process of understanding it better you see how to make it more capable.</p><p> If true, this suggests that alignment will always be in catch-up mode: whenever people try to figure out how to align their AI better, someone nearby will be able to run off with a few new capability insights, until the AI is pushed over the brink.</p><p> So a first key challenge for AI alignment is a challenge of ordering: how do we as a civilization figure out how to aim AI <i>before</i> we&#39;ve generated unaimed superintelligences plowing off in random directions? I no longer think “just sort out the alignment work before the capabilities lands” is a feasible option (unless, by some feat of brilliance, this civilization pulls off some uncharacteristically impressive theoretical triumphs).</p><p> Interpretability? Will likely reveal ways your architecture is bad before it reveals ways your AI is misdirected.</p><p> Recruiting your AIs to help with alignment research? They&#39;ll be able to help with capabilities long before that (to say nothing of whether they <i>would</i> help you with alignment by the time they <i>could</i> , any more than humans would willingly engage in eugenics for the purpose of redirecting humanity away from <a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>Fun</u></a> and exclusively towards inclusive genetic fitness).</p><p>等等。</p><p> This is (in a sense) a weakened form of my answer to those who say, “AI alignment will be much easier to solve once we have a bona fide AGI on our hands.” It sure will! But it will also be much, much easier to destroy the world, when we have a bona fide AGI on our hands. To survive, we&#39;re going to need to either sidestep this whole alignment problem entirely (and take other routes to a <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>wonderful future</u></a> instead, as I may discuss more later), or we&#39;re going to need some way to do a bunch of alignment research <i>even as</i> that research makes it radically easier and radically cheaper to destroy everything of value.</p><p> Except even that is harder than many seem to realize, for the following reason.</p><p></p><h2> 2. Distinguishing real solutions from fake ones is hard</h2><p> Already, labs are diluting the word “alignment” by using the word for superficial results like “the AI doesn&#39;t say bad words”. Even people who apparently understand many of the core arguments have apparently gotten the impression that GPT-4&#39;s ability to answer moral quandaries is somehow especially relevant to the alignment problem, and an important positive sign.</p><p> (The ability to answer moral questions convincingly mostly demonstrates that the AI can predict how humans would answer or what humans want to hear, without revealing much about what the AI actually pursues, or would pursue upon reflection, etc.)</p><p> Meanwhile, we have little idea of what passes for “motivations” inside of an LLM, or what effect pretraining on next-token prediction and fine-tuning with RLHF really has on the internals. This sort of precise scientific understanding of the internals — the sort that lets one predict weird cognitive bugs <i>in advance</i> — is currently mostly absent in the field. (Though not entirely absent, thanks to the hard work of many researchers.)</p><p> Now imagine that Earth wakes up to the fact that the labs aren&#39;t going to all decide to stop and take things slowly and cautiously at the appropriate time. <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span> And imagine that Earth uses some great feat of civilizational coordination to halt the world&#39;s capabilities progress, or to otherwise handle the issue that we somehow need room to figure out how these things work well enough to align them. And imagine we achieve this coordination feat <i>without</i> using that same alignment knowledge to end the world (as we could). There&#39;s then the question of who gets to proceed, under what circumstances.</p><p> Suppose further that everyone agreed that the task at hand was to fully and deeply understand the AI systems we&#39;ve managed to develop so far, and understand how they work, to the point where people could reverse out the pertinent algorithms and data-structures and what-not. As demonstrated by great feats like building, by-hand, small programs that do parts of what AI can do with training (and that nobody previously knew how to code by-hand), or by identifying weird exploits and edge-cases <i>in advance</i> rather than via empirical trial-and-error. Until multiple different teams, each with those demonstrated abilities, had competing models of how AIs&#39; minds were going to work when scaled further.</p><p> In such a world, it would be a difficult but plausibly-solvable problem, for bureaucrats to listen to the consensus of the scientists, and figure out which theories were most promising, and figure out who needs to be allotted what license to increase capabilities (on the basis of this or that theory that predicts this would be non-catastrophic), so as to put their theory to the test and develop it further.</p><p> I&#39;m not thrilled about the idea of trusting an Earthly bureaucratic process with distinguishing between partially-developed scientific theories in that way, but it&#39;s the sort of thing that a civilization can perhaps survive.</p><p> But that doesn&#39;t look to me like how things are poised to go down.</p><p> It looks to me like we&#39;re on track for some people to be saying “look how rarely my AI says bad words”, while someone else is saying “our evals are saying that it can&#39;t deceive humans yet”, while someone else is saying “our AI is acting very submissive, and there&#39;s no reason to expect AIs to become non-submissive, that&#39;s just anthropomorphizing”, and someone else is saying “we&#39;ll just direct a bunch of our AIs to help us solve alignment, while arranging them in a big bureaucracy”, and someone else is saying “we&#39;ve set up the game-theoretic incentives such that if any AI starts betraying us, some other AI will alert us first”, and this is a <i>different sort of situation</i> .</p><p> And not one that looks particularly survivable, to me.</p><p> And if you ask bureaucrats to distinguish which teams should be allowed to move forward (and how far) in that kind of circus, full of claims, promises, and hunches and poor in theory, then I expect that they basically just <i>can&#39;t</i> .</p><p> In part because the survivable answers (such as “we have no idea what&#39;s going on in there, and will need way more of an idea what&#39;s going on in there, and that understanding needs to somehow develop in a context where we can do the job right rather than simply unlocking the door to destruction”) aren&#39;t really in the pool. And in part because all the people who really want to be racing ahead have money and power and status. And in part because it&#39;s socially hard to believe, as a regulator, that you should keep telling everyone “no”, or that almost everything on offer is radically insufficient, when <i>you yourself</i> don&#39;t concretely know what insights and theoretical understanding we&#39;re missing.</p><p> Maybe if we can make AI a science again, then we&#39;ll start to get into the regime where, <i>if</i> humanity can regulate capabilities advancements in time, then all the regulators and researchers understand that you shall only ask for a license to increase the capabilities of your system when you have a full detailed understanding of the system and a solid justification for why you need the capabilities advance and why it&#39;s not going to be catastrophic. At which point maybe a scientific field can start coming to some sort of consensus about those theories, and regulators can start being sensitive to that consensus.</p><p> But unless you can get over that grand hump, it looks to me like one of the key bottlenecks here is <i>bureaucratic legibility of plausible solutions</i> . Where my basic guess is that regulators won&#39;t be able to distinguish real solutions from false ones, in anything resembling the current environment.</p><p> Together with the above point (&quot;alignment and capabilities are likely intertwined&quot;), I think this means that our rallying cry should be less “pause to give us more time on alignment research” and more “stop entirely, and find some way to circumvent these woods entirely; we&#39;re not equipped to navigate them”.</p><p> (With a backup rallying cry of “make AI a science again”, though again, that only works if you have some way of preventing the science-of-mind from leading to catastrophe before we figure out how to build AIs that care about good stuff rather than bleak and desolate stuff.)</p><p></p><h2> 3. Most theories don&#39;t work on the first real try</h2><p> It seems worth noting that <i>even if</i> you manage to surmount the above two problems, you have a third problem on your hands, which is that when it finally comes time, not to increase your system&#39;s capabilities a bit more (and test your theory of cognition further), but rather to actually do something of note with your AI, then you must, in some sense, be trusting a novel and untested scientific theory ( <i>and</i> your engineering efforts grounded in that theory) to work perfectly on the first critical try.</p><p> In particular, once your AI is <i>capable</i> of autonomous scientific/technological development, and operating in a domain where it <i>could</i> use that ability to gain a decisive strategic advantage over the rest of the planet, it&#39;s operating in a fundamentally different cognitive regime than it ever was in training.</p><p> By analogy, if you&#39;re considering making Bob the dictator of your country, you might first make him mock-dictator of your town, and watch to make sure he doesn&#39;t abuse his power. But, try as you might, that&#39;s still not fundamentally all that reliable a test of whether he&#39;ll actually abuse power once he really has it. Once he <i>really can</i> give orders to the army, once he <i>really can</i> take advantage of the country&#39;s populace, will he do so? The option of “abuse my mock power while being observed by my townspeople (who can still beat me up or deny me the job)” is actually very cognitively different than the option “order the army to intimidate the parliament and &#39;oversee&#39; the next election”.</p><p> Now, given a sufficiently well-developed theory of cognition, you can try to read the AI&#39;s mind and predict what cognitive state it <i>would</i> enter if it <i>did</i> really think it had those options. And you can set up simulations (and try to spoof its internal sensations and so on) in a way that your theory of cognition <i>predicts</i> is very similar to the cognitive state it would enter once it really had the option to betray you.</p><p> But the link between these states that you induce and observe in the lab, and the actual state where the AI actually has the option to betray you, <i>depends fundamentally on your fresh new theory of cognition.</i></p><p> Actually <i>running</i> the AI until it <i>really has</i> the opportunity to betray you is an empirical test of those theories in an environment that differs fundamentally from the lab setting.</p><p> And many a scientist (and programmer) knows that their theories of how a complicated system is going to work in a fundamentally new operating environment <i>often don&#39;t go super well on the first try.</i></p><p> As a concrete analogy to potentially drive this point home: Newtonian mechanics made all sorts of shockingly-good empirical predictions. It was a simple concise mathematical theory with huge explanatory power that blew every previous theory out of the water. And if you were using it to send payloads to very distant planets at relativistic speeds, you&#39;d <i>still be screwed</i> , because Newtonian mechanics does not account for relativistic effects.</p><p> (And the only warnings you&#39;d get would be little hints about light seeming to move at the same speed in all directions at all times of year, and light bending around the sun during eclipses, and the perihelion of Mercury being a little off from what Newtonian mechanics predicted. Small anomalies, weighed against an enormous body of predictive success in a thousand empirical domains; and yet Nature doesn&#39;t care, and the theory still falls apart when we move to energies and scales far outside what we&#39;d previously been able to observe.)</p><p> Getting scientific theories to work on the first critical try is <i>hard</i> . (Which is one reason to aim for minimal pivotal tasks — getting a satellite into orbit should work fine on Newtonian mechanics, even if sending payloads long distances at relativistic speeds does not.)</p><p> Worrying about this issue is something of a luxury, at this point, because it&#39;s not like we&#39;re anywhere close to scientific theories of cognition that accurately predict all the lab data. But it&#39;s the next hurdle on the queue, if we somehow manage to coordinate to try to build up those scientific theories, in a way where success is plausibly bureaucratically-legible.</p><hr><p> Maybe later I&#39;ll write more about what I think the strategy implications of these points are. In short, I basically recommend that Earth pursue other routes to the glorious transhumanist future, such as uploading. (Which is also fraught with peril, but I expect that those perils are more surmountable; I hope to write more about this later.) </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p> Albeit slightly less, since there&#39;s <i>nonzero</i> prior probability on this unknown system turning out to be simple, elegant, and well-designed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p> An exception to this guess happens if the AI is at the point where it&#39;s correcting its own flaws and improving its own architecture, in which case, in principle, you might not see much room for capabilities improvements if you took a snapshot and comprehended its inner workings, despite still being able to see that the ends it pursues are not the ones you wanted. But in that scenario, you&#39;re already about to die to the self-improving AI, or so I predict.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p> Not least because there are no sufficiently clear signs that it&#39;s time to stop — we blew right past “an AI claims it is sentient”, for example. And I&#39;m not saying that it was a <i>mistake</i> to doubt AI systems&#39; first claims to be sentient — I doubt that Bing had the kind of personhood that&#39;s morally important (though I am by no means confident!). I&#39;m saying that the thresholds that are clear <i>in science fiction stories</i> turn out to be messy <i>in practice</i> and so everyone just keeps plowing on ahead.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p> <i>Some prerequisites needed in order for this to make sense:</i></p><ol><li> <a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>Two Subsystems: Learning &amp; Steering</i></a></li><li> <a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>Shard Theory: An Overview</i></a></li><li> <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">Distilling Singular Learning Theory</a> <span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li> <i>Maybe also understanding at least a little</i> <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>Nate&#39;s picture</i></a> <i>though I don&#39;t claim to understand it fully.</i></li><li> <i>Of course,</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>AGI Ruin: A List of Lethalities</i></a> <i>, though hopefully that was implied</i></li><li> <i>Maybe the very basics of infra-bayesianism</i> (I liked the <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">AXRP podcast with her</a> ( <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">there&#39;s two</a> )) <i>, Vanessa&#39;s original</i> <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>learning theoretic agenda</i></a> <i>philosophy, and her current</i> <a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>pre-DCA</i></a> <i>alignment scheme.</i></li></ol><h1>抽象的</h1><p>The philosophy behind infra-bayesianism, and Vanessa&#39;s learning-theoretic alignment agenda seems very insightful to me. However, the giant stack of assumptions needed to make the approach work, and the ontology-forcing nature of the plan leave me unsettled. The path of singular learning theory has recently seen enough empirical justification to excite me. I&#39;m optimistic it can describe brains &amp; machine learning systems, and I describe my hope that this can be leveraged into alignment guarantees between the two, becoming an easier task as whole brain emulation develops.</p><h1>介绍</h1><p>Imagine a world where instead of humanity wandering together blindly down the path of cobbled together weird tricks learned off the machine learning literature, with each person trying to prepare for the bespoke failure-mode that seems most threatening to them (most of whom are wrong).</p><p> That instead we lived in the world where before heading down we were given a map and eyes to see for ourselves the safest path, and the biggest and most dangerous cliffs, predators, and dangers to be aware of.</p><p> It has always seemed to me that we get to the second world once we can use math for alignment. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="On the left dalle image generated with: Photo depicting a world representing empirical AI alignment approaches: A group of diverse people are shown as blind, navigating a rugged and unclear path. Their eyes are covered with blindfolds, and the terrain is uneven with shadows obscuring parts of the path. Each person has their own walking stick and gear, and they seem uncertain and cautious. Some individuals clutch old books or scrolls, symbolizing reliance on outdated machine learning literature. Others hold various tools and devices, preparing for unseen threats and unsure of the specific dangers ahead.  On the right dalle image generated with: Illustration in the Renaissance period style: A diverse group of men and women gather at the starting point of their adventure. Each holds a vibrant holographic map, some displaying topographical lines and others animated weather patterns. While some individuals discuss and point at dangers on the map, others look up to compare the hologram with the real terrain. The rugged journey ahead showcases treacherous cliffs, dense forests, and a distant mountain peak. A setting sun casts a dramatic light on the group, creating a chiaroscuro effect. The soft glow in their eyes highlights their enhanced vision." srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption> The left: Our situation. The right: Our situation if we had math for alignment.<br> Generated with DALL·E 3.</figcaption></figure><p> For a long time I thought using math for alignment was essentially a non-starter. Deep learning is notoriously resistant to successful theories, I&#39;m pessimistic that the approach of the Machine Intelligence Research Institute will take too much time, and the most successful mathematical theory of intelligence and alignment--infra-Bayesianism--rests on a stack of assumptions and mathematical arguments too high, too speculative, and too normative for me to be optimistic about. So I resigned myself to the lack of math for alignment.</p><p> That is, until Nina Rimsky &amp; Dmitry Vaintrob showed that some predictions of the new Singular Learning Theory held with <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">spooky accuracy</a> in the <a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking modular addition network</a> <span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> .</p><p> I had previously known about singular learning theory, and gone to the singular learning theory conference in Berkeley. But after thinking about the ideas, and trying to implement some of the processes they came up with myself, and getting not so great results <span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span> , I decided it too was falling for the same traps as infra-Bayesianism, lying on a giant stack of mathematical arguments, with only the most tentative contact with empirical testing with real world systems. So I stopped following it for a few months.</p><p> Looking at these new results though, it seems promising.</p><p> <i>But its not natively a theory of alignment. Its an upgraded learning theory. How does that solve alignment?</i></p><p> Well, both the human brain &amp; machine learning systems are learning machines, trained using a mix of reinforcement &amp; supervised learning. If this theory were to be developed in the right way (and this is where the <i>hope</i> comes in), we could imagine relating the goals of the results of one learning process to the goals of a different learning process, and prove a maximal deviation between the two for a particular setup. If one of those learning systems is a human brain, then we have just gotten an alignment guarantee.</p><p> Hence my two main hopes for alignment <span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> : prwhole brain emulation, and a <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning-theoretic-agenda</a> -like developmental track for singular learning theory. <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p> Interpretability, and other sorts of deep learning science seem good to the extent it helps with getting singular learning theory (or some better version of it) to the state where its able to prove such alignment-relevant theorems.</p><p> This task is made easier to the extent we have better whole brain emulation. And becomes so easy that we don&#39;t even need the singular learning theory component if we have succeeded in our whole brain emulation efforts. <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p> It seems like it&#39;d be obvious to you, the reader, why whole brain emulation gives me hope <span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> . But less obvious why singular learning theory gives me hope, so I will explain in much of the rest of this post why the latter is true.</p><h1> Singular learning theory</h1><p> <i>Feel free to skip these next three paragraphs, or even instead of reading my description, read</i> <a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse&#39;s</i></a> <i>or read pre-requisite 3. My goal is less to explain what singular learning theory is, and more to give an idea about why I&#39;m excited about it.</i></p><p> Singular learning theory fills a gap in regular learning theory, in particular, regular learning theory assumes that the parameter function map of your statistical model is one-to-one, and intuitively your loss landscape has no flat regions. <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p> Singular learning theory handles the case where this is not true, which occurs often in hierarchical models, and (as a subset) deep neural networks. And in broad strokes by bringing in concepts from algebraic geometry in order to rewrite the KL divergence between the true model and parameters into a form that is easier to analyze.</p><p> In particular, it anticipates two classes of phase transitions during the development of models, one of which is that whenever loss suddenly goes down, the real-log-canonical-threshold (RLCT), a algebraic geometry derived metric for complexity, will go up. <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p> Its ultimately able to retrodict various facts about deep learning, including the success of data scaling, parameter scaling, and double descent, and there has been recent success in getting it to give predictions about phenomena in limited domains. Most recently, Nina Rimsky and Dmitry Vaintrob&#39;s <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">Investigating the learning coefficient of modular addition: hackathon project</a> where the two were able to verify various assertions about the RLCT/learning coefficient/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei estimate. Getting  the most beautiful verification of a theoretical prediction in my lifetime </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption> Chart of estimated <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> over training for an MLP trained on modular addition mod 53. Checkpoints were taken every 60 batches of batch size 64. Hyperparameters for SGLD are <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> . The search was restricted to directions orthogonal to the gradient at the initialization point to correct for measurement at non-minima. [caption text from the original post]</figcaption></figure><p> As I said, singular learning theory makes the prediction that during phase transitions, the loss of a model will decrease while the RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) will increase. Intuitively, this means, when you switch model classes you switch to a model class that fits the data better and is more complicated. Above, we see exactly this.</p><p> As well as Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, and Daniel Murfet&#39;s <a href="https://arxiv.org/abs/2310.06301">Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition</a> with abstract</p><blockquote><p> We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.</p></blockquote><p> You can read more at <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">this LessWrong sequence</a> , watching the <a href="https://www.youtube.com/@SLTSummit/videos">primer</a> , <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox lectures</a> , and of course reading the books <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Algebraic Geometry and Statistical Learning Theory</a> , and <a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Mathematical Theory of Bayesian Statistics</a> .</p><h1> So why the hope?</h1><p> To quote Vanessa Kosoy from her <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning theoretic</a> agenda:</p><blockquote><p> In online learning and reinforcement learning, the theory typically aims to derive upper and lower bounds on &quot;regret&quot;: the difference between the expected utility received by the algorithm and the expected utility it <i>would</i> receive if the environment was known a priori. Such an upper bound is effectively a <i>performance guarantee</i> for the given algorithm. In particular, if the reward function is assumed to be &quot;aligned&quot; then this performance guarantee is, to some extent, an alignment guarantee. This observation is not vacuous, since the learning protocol might be such that the true reward function is not directly available to the algorithm, as exemplified by <a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a> and <a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a> . Thus, formally proving alignment guarantees takes the form of proving appropriate regret bounds.</p></blockquote><p> If the principles of singular learning theory can be extended to reinforcement learning, and we get reasonable bounds for the generalization behavior of our model, or even exact claims about the different forms the value-equivalents inside our model will take as it progresses during training, we can either hope to solve a form of what can roughly be known as inner alignment--getting our model to consistently think &amp; act in a certain way when encountering the deployment environment.</p><p> It seems reasonable to me to think we can in fact extend singular learning theory to reinforcement learning. The same sorts of deep learning algorithms work very well on both supervised and reinforcement learning, so we should expect the same algorithms have similar reasons for working on both, and singular learning theory gives a description of why those deep learning algorithms do well in the supervised learning case. So we should suspect the story for reinforcement learning follows the same general strokes <span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> .</p><h1> If you like performance guarantees so much, why not just work on infra-Bayesianism?</h1><p> Vanessa&#39;s technique was to develop a theory of what it meant to do good reasoning in a world that is bigger than yourself, and also requires you to do self-modeling. Then (as I understand it) prove some regret bounds, make a criterion for what it means to be an agent, and construct a system with a low bound on the satisfaction of the utility function of the agent that is most immediately causally upstream of the deployed agent.</p><p> This seems like a really shaky construction to me, mainly because we do not in fact have working in-silico examples of the agents she&#39;s thinking about. I&#39;d be much happier with taking this methodology, and using it to prove bounds on actual real life deep learning systems&#39; regret (or similar qualities) under different training dynamics.</p><p> I also feel uneasy about how it goes from theory about how values work (maximizing a utility function) to defining that as the success criterion <span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> . I&#39;d be more comfortable with a theory which you could apply to the human brain, and naturally derive a utility function (or other value format). Just by looking at how brains are built and developed. In the case that we&#39;re wrong about our philosophy of values, this seems more robust. And to the extent multiple utility functions can fit our behavior accounting for biases and lack of extensive reflection, having a prior over values informed by the territory (ie the format values are in in the brain) seems better than the blunt instrument of Occam&#39;s razor applied directly to the mapping from our actual and counterfactual actions to utility functions.</p><p> Of all the theories that I know of, singular learning theory seems the most adequate to the task. It both is based on well-proven math, it has and will continue to have great contact with actual systems,  it covers a very wide range of learning machines, which includes human brains (ignoring the more reinforcement learning like aspects of human learning for now) &amp; likely future machines (again ignoring reinforcement learning), and the philosophical assumptions it makes are far more minimalist than those of infra-Bayesianism.</p><p> The downside of this approach is singular learning theory says little right now about reinforcement learning. However, as I also said above, we see the same kinds of scaling dynamics in reinforcement learning as we do in supervised learning &amp; the same kinds of models work in both cases, so it&#39;d be pretty weird if they had very different reasons for being successful. Singular learning theory tries to explain the supervised learning case, so we should expect it or similar methods be able to explain the reinforcement learning case too.</p><p> Another downside is it not playing well with unrealizability. However, I&#39;m told there hasn&#39;t been zero progress here, its an open problem in the field, and again, neural networks often learn in unrealizable environments, as far as I know, we see similar enough dynamics that I bet singular learning theory is up for the task.</p><h1> Whole brain emulation</h1><p> The human brain is almost certainly singular <span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> , has a big learning from scratch component to it, and singular learning theory is very agnostic about the kinds of models it can deal with <span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> , so I assert singular learning can deal with the brain. Probably not to help whole brain emulation all that much, but given data whole brain emulation gives us about the model class that brains fall into, the next hope is to use this to make nontrivial statements about the value-like-things that humans have. Connecting this with the value-like-things that our models have, we can hopefully (and this is the last hope) use singular learning theory to tell us under what conditions our model will have the same values-like-things that our brains have.</p><h1> Fears</h1><h2>哇！ That&#39;s a lot of hopes. I&#39;m surprised this makes you more hopeful than something simple like empirical model evaluations</h2><p> Singular learning theory, interpretability, and the wider developmental interpretability all seem useful for empirically testing models. I&#39;m not hopeful just because of the particular plan I outline above, I&#39;m hopeful because I see a concrete plan at all for how to turn math into an alignment solution for which all parts seem to be useful even if not all of my hopes turn out correct.</p><h2> I&#39;m skeptical that something like singular learning theory continues to work as the model becomes reflective, and starts manipulating its training environment.</h2><p> I am too. This consideration is why our guarantees should occur early in training, be robust to continued training, and be reflectively stable. Human-like values-like-things should be reflectively stable by their own lights, though we won&#39;t actually know until we actually see what we&#39;re dealing with here. So the job comes down to finding a system which puts them into our model early in training, keeps them throughout training, and ensures by the time reflection is online, the surrounding optimizing machinery is prepared.</p><p> Put another way: I see little reason to suspect the values-like-things deep learning induces will be reflectively stable by default. Primarily because the surrounding optimizing machinery is liable to give strange recommendations in novel situations, such as reflective thought becoming active <span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> . So it does in fact seem necessary to prepare that surrounding optimizing machinery for the event of reflection coming online. But I&#39;m not so worried about the values-like-objects being themselves disastrously suicidal once we know they&#39;re similar enough to those of humans.</p><p> Nate and possibly Eliezer would say this is important to know from the start. I would say I&#39;ll cross that bridge once I actually know a thing or two what values-like-thing, and surrounding machinery I&#39;ll be dealing with.</p><h2> Why reinforcement learning? Shouldn&#39;t you focus on supervised learning, where the theory is clear, and we&#39;re more likely to get powerful models soon?</h2><p> Well, brains are closer to reinforcement learning than supervised learning, so that&#39;s one reason. But yeah, if we can get a model which while supervised, we prove statements about values-like objects for, then that would be a good deal of the way there. But not all the way there, since we&#39;d still be confused when looking at our brains.</p><h2> Singular learning theory seems liable to help capabilities. That seems bad.</h2><p> I will quote myself outlining <a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">my position</a> on a related topic, which generalizes to the broader problem of developing a theory of deep learning:</p><blockquote><p> Mostly I think that [mechanistic interpretability] is right to think it can do a lot for alignment, but I suspect that lots of the best things it can do for alignment it will do in a very dual-use way, which skews heavily towards capabilities. Mostly because capabilities advances are easier and there are more people working on those.</p><p> At the same time I suspect that many of those dual use concerns can be mitigated by making your [mechanistic interpretability] research targeted. Not necessarily made such that you can do off-the-shelf interventions based on your findings, but made such that if it ever has any use, that use is going to be for alignment, and you can predict broadly what that use will look like.</p><p> This also doesn&#39;t mean your [mechanistic interpretability] research can&#39;t be ambitious. I don&#39;t want to criticize people for being ambitious or too theoretical! I want to criticize people for producing knowledge on something which, while powerful, seems powerful in too many directions to be useful if done publicly.</p></blockquote><p> My plan above is a good example of an ambitious &amp; theoretical but targeted approach to deep learning theory for alignment.</p><h2> Why singular learning theory, and not just whole brain emulation?</h2><p> Firstly, as I said in the introduction, it is not obvious to me that given whole brain emulation we get aligned superintelligence, or are even able to perform a pivotal act. Recursive self improvement while preserving values may not be easy or fast (though you can always make the emulation faster). Pivotal acts done by such an emulation likely have difficulties I can&#39;t see.</p><p> However, I do agree that whole brain emulation alone seems probably alignment complete.</p><p> My main reason for focusing on both is that they feel like two different sides of the same problem in the sense that progress in whole brain emulation makes us need less progress in singular learning theory, and vice versa. And thinking in terms of a concrete roadmap makes me feel more like I&#39;m not unintentionally sweeping anything important underneath any rugs. The hopes I describe have definite difficulties, but few speculative difficulties.</p><h2> It seems difficult to get the guarantees you talk about which are robust to ontology shifts. Values are in terms of ontologies. Maybe if a model&#39;s ontology changes, its values will be different from the humans&#39;</h2><p> This is something I&#39;m worried about. I think there&#39;s hope that during ontology shifts, the meta-values of the models will dominate what shape the model values take into the new ontology, and there won&#39;t be a fine line between the values of the human and the meta-values of the AI. There&#39;s also an independent hope that we can have a definition of values that is just robust to a wide range of ontology shifts.</p><h1> So what next for singular learning theory and whole brain emulation?</h1><p> I currently don&#39;t know too much about whole brain emulation. Perhaps there are areas they&#39;re focusing on which aren&#39;t so relevant to my goals here. For example, if they focus more on the statics of the brain than the dynamics, then that seems naively inefficient <span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> because the theorem I want talks about the dynamics of learning systems and how those relate to each other.</p><p> Singular learning theory via <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeus</a> seems to mostly be doing what I want them to be doing: testing the theory on real world models, and seeing how to relate it to model internals via developmental interpretability. One failure-mode here is they focus too much on empirically testing it, and too little on trying to synthesize their results into a unified theory. Another failure-mode is they focus too much on academic outreach, and not enough on actually doing research. And then the academics they do outreach to don&#39;t really theoretically contribute that much to singular learning theory.</p><p> I&#39;m not <i>so</i> worried about the first failure-mode, since everyone on their core team seems very theoretically inclined.</p><p> It seems like a big thing they aren&#39;t looking into is reinforcement learning. This potentially makes sense. Reinforcement learning is harder than supervised learning. You need some possibly nontrivial theoretical leaps to say anything about it in a singular learning theory framework. Even so, it seems possible there is low-hanging fruit in this direction. Similarly for taking current models of the brain</p><p> Of course, I expect the interests of Timaeus and myself will diverge as singular learning theory progresses, and there are pretty few people working on developing the theory right now. So it seems a productive use of my efforts.</p><h1> Acknowledgements</h1><p> Thanks to Jeremy Gillen, and David Udell for great comments and feedback! Thanks also to Nicholas Kees, and the <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> for the same. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p> Note I didn&#39;t read this, I watched the <a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">singular learning theory primer</a> videos, but those seem longer than the series of LessWrong posts, and some have told me they&#39;re a good intro.</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p> Somewhat notably, both the grokking work, and Nina &amp; Dmitry&#39;s project were done over a weekend.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p> If I remember correctly, the reason for the not great results was because the random variable we were estimating had a too high variance to actually correlate with another quantity we were trying to measure in our project.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p> Not including stuff like Davidad&#39;s proposals, which while they give me some hope, there&#39;s little I can do to help.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p> Ultimately hoping to construct a theorem of the form</p><blockquote><p> Given agent Alice with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and agent Bob with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> , end up with value systems <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> respectively (not necessarily utility functions), and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span> for some definition of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span> that means something like Bob tries to achieve something like what Alice tries to achieve when in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> .</p></blockquote><p> And where we can solve for Bob&#39;s reward model and environment taking Alice as being a very ethical human. Hopefully with some permissive assumptions, and while constructing a dynamic algorithm in the sense that Bob can learn to do good by Alice&#39;s lights for a wide enough variety of Alices that we can hope humans are inside that class.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p> Since, hopefully, if we have whole brain emulation it will be easy for the uploaded person or people to bootstrap themselves to superintelligence while preserving their goals (not a trivial hope!).</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p> This is not to say the conditions under which whole brain emulation should give one hope are obvious.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p> More formally, regular models are one-to-one, and have fisher information matrix positive-definite everywhere.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p> There is a different phase transition which occurs when the RLCT goes down, and some other quantity goes up. There are several candidates for this quantity, and as far as I know, we don&#39;t know which increase is empirically more common.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p> The reward landscape of deep reinforcement learning models is probably pretty insane. Perhaps not insane enough to not be liable to singular learning theory-like analysis, since there&#39;s always some probability of doing any sequence of actions, and those probabilities change smoothly as you change weights, so the chances you execute a particular plan change smoothly, and so your expected reward changes smoothly. So maybe there&#39;s analogies to be made to the loss landscape.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m told that Vanessa and Diffractor believe infra-Bayesianism can produce <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">reflectively stable and useful quantilizers and worst-case optimizers</a> over a set of plausible utility functions. I haven&#39;t looked at it deeply, but I&#39;d bet it still assumes more ontology than I&#39;m comfortable with, both in the sense that for this reason it seems less practical than my imagined execution of what I describe here, and it seems more dangerous.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p> In the sense that likely the mapping from brain states to policies is not one-to-one, and has singular fisher information matrix.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p> In its current form it requires only that they be analytic, but ReLUs aren&#39;t, and empirically we see ignoring that aspect gives accurate predictions anyway.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p> Situational novelty is not sufficient to be worried, but during reflection the model is explicitly thinking about how it should think better, so if its bad at this starting out, and makes changes to its thought, if those changes are to what it cares about, they will not necessarily be corrected by further thought or contact with the world. So situational novelty leading to incompetence is important here.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p> A way it could be efficient is if its just <i>so</i> much easier to do statics than dynamics, you will learn much more about dynamics from all the static data you collect than you would if you just focused on dynamics.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate></item></channel></rss>