<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 16 日星期四 08:15:56 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Learning coefficient estimation: the details]]></title><description><![CDATA[Published on November 16, 2023 3:19 AM GMT<br/><br/><h2>这是做什么用的</h2><p>学习系数 (LC) 或 RLCT 是奇异学习理论中的一个量，可以帮助量化深度学习模型的“复杂性”等。</p><p>本指南主要旨在帮助有兴趣改进学习系数估计的人们快速了解其幕后工作原理。如果您只是尝试在自己的项目中使用 LC，则可以在不了解所有详细信息的情况下使用该<a href="https://github.com/timaeus-research/devinterp/tree/main">库</a>，尽管本指南可能仍然有帮助。如果您还没有阅读<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">本文</a>，强烈建议您先阅读这篇文章。</p><p>我们主要介绍<a href="https://jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">WBIC 论文</a>（Watanabe 2010），它是当前 LC 估计技术的基础，但这里的演示是原创的，旨在获得更好的直觉，并且与论文有很大不同。我们还将简要介绍<a href="https://arxiv.org/abs/2308.12108">Lau 等人。 2023年</a>。</p><p>尽管讨论很长，但您最终在实践中所做的事情<i>非常简单</i>，并且代码旨在强调这一点。经过一些相对快速的设置后，实际的 LC 计算可以通过一两行代码轻松完成。</p><h2>这不是为了啥</h2><ul><li>对 SLT 的良好概述，或者首先研究 LC 或损失景观卷背后的动机。我们在这里主要关注 LC 估计。</li><li>抽样细节。这些非常重要！但它们并不是单一学习理论所独有的，并且在其他地方有大量有关 MCMC 的优质资源和教程。</li><li>公式推导，超越高级推理。</li></ul><h2>总长DR</h2><ul><li>什么是学习系数？ （<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">上次</a>回顾）<ul><li>学习系数 (LC) 也称为 RLCT，用于衡量盆地宽度。</li><li>这并不新鲜，但通常“盆地宽度”被操作为“盆地平坦度”——即通过 Hessian 行列式。当模型是奇异的（Hessian 矩阵的特征值为零）时，这是一个坏主意。</li><li> LC 将“盆地宽度”操作为（低损耗渐近）体积缩放指数。正如奇异学习理论所证明的那样，这最终是正确的衡量标准。</li></ul></li><li>我们如何衡量它？<ul><li>事实证明，直接测量高维体积是很困难的。我们不做这个。</li><li>相反，我们使用 MCMC 进行统计学中所谓的“矩量法”估计。我们设计一个以 LC 作为总体参数的分布，从该分布中采样并计算其矩之一，并求解 LC。</li><li>我们在本节中简化了一些细节，但这是 LC 估计的概念核心。</li></ul></li><li>我们如何衡量它（真实的）？<ul><li>上面的内容稍微简化了一些。 LC 确实测量损失量缩放，但它使用的“损失”是经验损失函数的平均值或“无限数据”限制。</li><li>实际上，您不知道这个无限数据丢失函数。幸运的是，您已经对它有了一个很好的估计——您的经验损失函数。不幸的是，这个估计并不完美——它可能有一些噪音。事实证明，这种噪音实际上在您<i>最不</i>想要的地方最<i>严重</i>。</li><li>但最终一切都会解决！实际上，您只需要对“理想化”算法进行一点小小的修改，一切就可以正常工作。这将为您提供一个在实践中真正有效的算法！</li><li>最后，出于可扩展性等原因，最先进的方法（Lau et al. 2023）做了一些简单的修改：它仅<i>*本地*</i>测量学习系数，并使用小批量损失而不是全损失。批。</li></ul></li></ul><p>以图表的形式：当我们从理想化（顶部）转向现实（底部）时，我们会得到新的问题、解决方案和改进方向。该指南本身最详细地介绍了前两行，这可能是概念上最难思考的，并在最后直接从第二行跳到第四行。 </p><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9ecpBaAiGQnkmX9Ex/fmppl7bmgijkwtx7lnbh"></figure><p></p><p><i>请参阅</i><a href="https://colab.research.google.com/github/zfurman56/intro-lc-estimation/blob/main/Intro_to_LC_estimation.ipynb"><i>链接的 Colab 笔记本</i></a><i>以获取完整指南。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/9ecpBaAiGQnkmX9Ex/learning-coefficient-estimation-the-details#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9ecpBaAiGQnkmX9Ex/learning-coefficient-estimation-the-details<guid ispermalink="false"> 9ecpBaAiGQnkmX9Ex</guid><dc:creator><![CDATA[Zach Furman]]></dc:creator><pubDate> Thu, 16 Nov 2023 03:19:09 GMT</pubDate> </item><item><title><![CDATA[Extrapolating from Five Words]]></title><description><![CDATA[Published on November 15, 2023 11:21 PM GMT<br/><br/><p>如果你只能用<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">五个词</a>来表达一个想法，人们会从这五个词中推断出什么？您可以使用法学硕士通过实验来发现人们可能认为这五个词的含义，而不是猜测。您可以使用它来迭代您想说的五个单词，以便最好地传达您的预期含义。</p><p>我产生这个想法是因为我尝试要求克劳德在链接上总结一篇文章。 Claude 不会跟踪链接，因此它会幻觉标题中的摘要，该摘要包含在 URL 路径中。这是使用<a href="https://www.lesswrong.com/posts/LkjpHGiELQzed8hdu/why-the-problem-of-the-criterion-matters">我的 LessWrong 帖子之一</a>执行此操作的示例： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bmhkdtwhkyjakvokjf8o" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/slx0yvnxmtj5fhhbonfl 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/i4hsanmguhbqxeubhcgd 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sbhdvppzm36adnjmzf0j 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/onfbpkewjhbcvp82t5nc 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ooochb4zyqwvu9nbkait 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gyfckyk0nkrp37ysfj6m 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ocvikzsm8zsgwgxlwbc9 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bwjocv054qp6xzz7n2mr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/z2cdhnfimjqy5iydhhjs 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ubotioxkb9gkckqmobaj 1638w"></figure><p>它产生了一些错误的细节，并遗漏了帖子中实际存在的许多细节，但这里并不是完全不合时宜。如果我的〜五个词是“标准问题很重要”，那么这将是我为什么这么说的合理推断。</p><p>除了使用链接之外，我还可以要求 Claude 提出它认为我会在具有特定标题的帖子中添加的内容： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/r1w6ub2kairpaylby2ya" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/vpuvlg0a2qxs02d9a8mh 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gd1asogssvzjvtme7zzx 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lr2zcthp13guf9fiheci 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/veszm5ofcocu4ycc8m3b 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/b9bbmyxfbj5vhgqwefcz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/s6ntzjebigweykpebss7 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/t53x7vzvjuzseje3ogpq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fehqezg1cmuzigwpn8e3 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/n6359ek9mzuw9un5gtgy 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bkwlgtvdtbnazl3cfnrd 1586w"></figure><p>奇怪的是，它在某些方面表现得更糟，而在其他方面表现得更好。与它产生链接摘要的幻觉不同，这次它提出了我绝对不会说或不希望有人得到的东西，比如我们可以解决标准问题，足以拥有客观的知识标准。</p><p>但也许促使它关注 LessWrong 才是问题所在，因为 LessWrong 引起了很多实证主义者的共鸣，<a href="https://www.lesswrong.com/posts/dTkWWhQkgxePxbtPE/no-logical-positivist-i">但埃利以泽的相反说法</a>并不成立。所以我尝试了不同的提示： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xrpj3qaqn7suizcevhys" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/k9x2qujwgkf0metacypy 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bpm1a2ggpidjlcsqwesd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zd59kndoaw4wxykeycyy 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nqnrhxxvr8huxcay1u32 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pn6l2shzgwq8ql0obr4n 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bctwvfofjgkjzck0nqq1 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hkmze9rfpylwqklxnklq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jzz0ty1oy4tlrqc4bhjr 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hb3mb6kehxcpipzf6rab 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jjmgmqa2khno1ugfbhox 1578w"></figure><p>这可以？这不太好。这听起来像是一个无聊的哲学本科生为认识论课写的论文的总结。</p><p>让我尝试问它一些版本的“我的〜五个词是什么意思？”： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lkewxseua3vdb05j7toe" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wtbbd1ubjfieqi5v5w54 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fyqopidgidzeckrmi3tt 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dv7skp8zycwv5qhs4njh 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qanzrbxq4kwjyb3ysyyq 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mai1kzmoyu3ar1aju9xa 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/w5lpbsgulhtld3oa1vi3 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lszilosu5mu0wwnxnvpn 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nvjhfugeemp0z6idhzsu 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/iixmdvneheblpgyl1a8i 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xankmu5wloylpm307klo 1614w"></figure><p>这非常好，基本上我希望有人能从我身上夺走“标准问题很重要”的东西。让我们看看如果我调整语言会发生什么： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/yqb2ijarrst9jsfx5gjd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/eut3fmkm8shlcbcug5g0 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pyw3pnoail6yj83xkgnd 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qp9yniwnn0oojpw6ezvj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lh501i7swrx2som5cnqt 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gana511ysgdqyjp70wgl 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/v8x1sgfssqcwx2b8rihc 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/uldogbpsdoiadnic0dml 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q0qfpikhfmxrrvpw3ade 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lzttdsrr3vwed4oltack 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcyq8xocefuh244rj9bj 1780w"></figure><p>整洁的！它注意到了“重要”而不是“重要”所暗示的许多细微差别。这对于尝试短语的不同变体以查看这些微小变体对隐含含义有何变化非常有用。我认为这对于诸如文字加工公司价值观和使命以及其他每个单词都必须承载很多含义的短语等任务很有用。</p><p>现在让我们看看它是否可以反向完成任务！ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sxgabqz0wyvvx4lxwhje" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ov7pj5rakxa9pjqjxrs6 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qitt5xg5d5aoetcfvfe2 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ongwefldjntxhgrqtaq1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/tudmg2sfnqjq2gqjrydl 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/is0bducapbzwva9zyax4 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/u8xa7utkyqizzg9zqe37 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ehjtaxuk5tfp3eganyln 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/f47objdkzzxsnxdar3sj 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jmvyyc0rhfola1dljksj 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pdgspb42zyihzi6pikad 1726w"></figure><p>老实说，“不确定性破坏知识”可能比我想出的任何东西都要好。谢谢，克劳德！</p><p>作为最后的检查，克劳德能否从自己的总结中推断出来？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/y2pf0udioglx0qsdhpv6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fdi6zhj1wtfivqlqcpuf 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ouxcagejqzlqzcphlhzd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jz3tiot9xgm5gl15hbls 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etynfz3izydbggskkzfi 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bdsy7bzo33ahduse7o5j 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q4ws7lixelgpvecilntf 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zail1zfcfpngok6rmbom 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q9qjxw72otwkfpbihcq7 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etkhvvkopnnoailb3ae1 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qrkgidttighy0hl474d8 1586w"></figure><p>显然，它丢失了一些细节，特别是关于标准问题的细节，并且弥补了一些我不想让它得到的东西。将细致入微的信息压缩成大约五个单词，并且仍然传达信息的核心，这似乎是理所当然的。</p><p>好吧，最后的测试，克劳德可以从我可能对我最喜欢的话题“基本不确定性”所做的典型陈述中推断出什么？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dc6roac8u3oskbx4zwmk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ako6peikfk1vdblnddfu 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dh0yunove6oo8e4fhk3y 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mqhewctspnzhqsxqtjkw 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dp3x6bfuj6if7ryfcxzn 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ve9l0vltshrvuy5dfgqr 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bzhau52vnet9ehltdd9f 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ogyqa2awhac3kkorgliz 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ceudntk6ndseqfz6xewg 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/x0elkg9cjhej9iblmcgh 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hclfjabiodfwlfnxnaz6 1570w"></figure><p>嗯，还可以，但不是很好。也许我应该尝试找到另一个短语来表达我的想法？让我们看看它对“基本面不确定性”这个书名的看法： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/swabayitctg8yc2odybb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jrn6yiiqgufmxzirkdyd 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcw6qq4ze9ppchawd8uk 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/rhcuowppft8kzrpi3vas 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lihcbo7lcobtasexmubz 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wkymdjqovj4ushzjlol7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jnrmicoayxtn1e1vwwrp 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ibuidok3d2fonyom0jag 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bvjjhfbwjaebf8jjiway 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ugrnmwrcb7lwajtkjqpr 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wfh9e7vyz2ni5esm9wp8 1598w"></figure><p>足够接近。我可能不需要重新命名<a href="https://www.lesswrong.com/s/HMs2yT9D6LjYR5jQT">我的书</a>，但我可能需要设计一个好的副标题。</p><p>基于以上在提示工程中的实验，克劳德在迭代简短短语摘要方面相当有帮助。它能够捕捉到微妙的细微差别，这对于找到正确的短语来传达一个重要的想法非常有用。下次当我需要构建一个简短的短语来表达复杂的想法时，我可能会使用克劳德或其他法学硕士来迭代措辞。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from-five-words#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from- Five-words<guid ispermalink="false"> HD8kLyYcSuYRi4vzP</guid><dc:creator><![CDATA[Gordon Seidoh Worley]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:21:31 GMT</pubDate> </item><item><title><![CDATA[In Defense of Parselmouths]]></title><description><![CDATA[Published on November 15, 2023 11:02 PM GMT<br/><br/><p>先决条件：贵<a href="http://benjaminrosshoffman.com/the-quaker-and-the-parselmouth/">格会教徒和蛇佬腔</a>。</p><h2>我。</h2><p>首先，快速总结一下。</p><p>在先决条件帖子中，本杰明霍夫曼描述了三种人。这些人是假设的极端：他们是在无摩擦真空中相互作用的完美球体的社会和认知等价物。有些贵格会教徒总是说实话，并且在说要做某事时信守诺言。有些演员总是说当下看起来不错的话，即使他们发誓和发誓，也不能可靠地遵守诺言。最后，还有蛇佬腔，他们可以自由地对演员说谎，但只对其他蛇佬腔说实话，并且（暗示）只对贵格会教徒说实话。</p><p>我赞同这种区别。它是抽象的，现实世界从来都不是那么清晰，但根据我的经验，它确实得到了一些有用的东西来理解。我认为说真话是一个强大的制度优势，并希望更多的人在这种二分法中成为贵格会教徒。本杰明指出，蛇佬腔有些奇怪，因为习惯性说谎可能会侵蚀本能，甚至可能侵蚀说真话的能力；对于真正的人来说，如果不慢慢成为演员，就不可能始终保持蛇佬腔。</p><p>说真话很难。弄清楚世界的真实状况是什么是很困难的。快速而准确地陈述你认为正确的事情是很困难的；英语使得“我相信明天有百分之九十的机会下雨”的句子比“明天会下雨”要长得多。当有人问你是否喜欢他们带来的聚餐砂锅菜（烧焦的和未调味的）时，你最终会受到很多额外的情感尖锐的肘击。全世界的贵格会教徒，我向你们致敬。全世界的演员，我明白了。</p><p>我的第一个主张是成为蛇佬腔是合理的。</p><h2>二.</h2><p>讲故事的时间！下面的故事详细描述了大约二十年前发生的事件，当时我比现在矮了几英尺。一些细节已经被当时在场的其他人证实，但许多细节可能随着时间的推移而发生了变化。</p><p>当我还是个孩子的时候，我必须拍很多照片。我妈妈带我进了办公室，我在等候区闲逛了一会儿，然后护士挥手让我经过前台，我和妈妈就进去了。护士让我坐在医生办公室的一张大塑料椅子上，然后我一边用冰凉的东西擦着我的肩膀，一边问妈妈问题，然后她让我坐一会儿，说：“这不会疼的，你准备好了吗？”我点了头。然后她用针刺了我。</p><p>很痛。我开始哭，并且持续哭了一段时间，直到疼痛消退为隐痛。我父母的安慰或护士的款待都没有改变这一点。我当时没有能力清楚地表达出是什么让我心烦意乱，但这不是痛苦（即使在小时候，当疼痛有目的时，我对疼痛的容忍度也非常高），而是困惑。它不应该伤害——他们对于是否会伤害的判断是错误的吗？这不太合理，用尖锐的东西刺人通常会伤害他们，为什么有人会认为不会呢？我是否记错了他们说的话，他们说会痛而不是不会痛？我的记忆真的那么差吗？我完全困惑了，无法理解发生了什么。</p><p>凭借多年的经验，发生的事情是显而易见的。护士撒谎让一个小孩在注射时保持安静。这个故事多年来一直在重复，每次我都会感到困惑和困惑。直到很久以后，当我顿悟到世界如何看待真理之后，我才想到有人会撒谎。</p><p>虽然很痛苦，但事实证明，这种理解是许多人际互动的有用万能钥匙。有时人们只是撒谎，而且<a href="https://slatestarcodex.com/2016/12/12/might-people-on-the-internet-sometimes-lie/">往往是</a>为了比你想象的<a href="https://www.lesswrong.com/posts/K2c3dkKErsqFd28Dh/prices-or-bindings">更小的利益</a>。文字不是真理，只是人们发出的声音或页面上的符号。人们随意地、轻易地在重要的事情和琐碎的事情上撒谎，以得到他们想要的东西，或者只是因为他们不在乎。这种不在意并不是恶意，只是冷漠。</p><h2>三．</h2><p>有时陈述事实错误是完全可以的。我认为一个重要的区别是大多数相关人员是否知道哪些陈述是哪些。</p><p>有一些明显的案例。如果你从书店的奇幻区拿起一本书，上面有一条龙，那么写那本书里的文字的人几乎可以全权决定在那本书中说出他们想要的任何内容。还有一些不太明显的情况；我很遗憾地通知您，职业摔跤手和舞台喜剧演员在他们嘴里说的话和现实的基本状态之间有着灵活的关系。还有一些可疑的例子，虽然它是虚构的，但旁观者可能会感到相当困惑，比如发现的恐怖电影片段或报纸上“研究节目”一词后面的任何内容。 <span class="footnote-reference" role="doc-noteref" id="fnrefdnozbd7ldgr"><sup><a href="#fndnozbd7ldgr">[1]</a></sup></span>小说对全世界数以百万计的人来说很有趣，任何禁止说谎的禁令都需要为此留出空间。</p><p> （另一个童年故事：我的一个叔叔曾经带我去钓鱼，当我们一天没钓到鱼回来时，我们无意中进行了一场喜剧二重唱，我系统地不同意他的<a href="https://www.britannica.com/dictionary/fish-story">鱼故事</a>的每一行，直到我祖父怜悯并把我拉到一边解释说这些不应该被视为字面上的事实。）</p><p>一些事实性的误解是，如果你真正检查正在发生的事情，就能成功地向对话双方传达正在发生的事情。</p><p>如果你问一个以美式英语为母语的人是否可以帮你做点什么，他们会回答“稍后”，而当你在六十分钟后他们没有出现时你会感到困惑，我实际上很抱歉。这也困扰了我很长一段时间，最终我也接受了。宇宙中没有任何法则将“ˈsɛkənd”的声音与人类心脏跳动所需的时间联系起来。在某些情况下，“ˈsɛkənd”表示大约等于心跳的时间单位，在其他情况下，它表示第一和第三之间的计数，在您刚刚寻求帮助并且有人说“在一秒钟内”的情况下，它的意思是类似“很快，但不是现在。”</p><p>多年来我选择了这场战斗的许多变体，但我已经放弃了。我现在站在语言描述主义者一边。</p><p>有这样一个：我认为不是每个人都知道这是如何工作的。存在证明，我感觉很长一段时间都不知道，所以二十年前说“每个人都知道”是错误的。大家都不知道。至少只要还有那些头脑简单的孩子到处乱跑，人们就会不断地遇到这种情况，而且可能会持续更长的时间，因为其中一些孩子长大成为头脑简单的成年人，他们对真理的立场转变为坚定的原则。当有人按字面意思理解我的话并进行代码转换或至少警告他们时，我会尽量集中注意力。</p><p>在许多美式英语口语句子中，“Literally”一词的作用是充当强化词。 “他在那场比赛中是世界上最好的”和“他在那场比赛中确实是世界上最好的”经常被说成基本上是同一件事。没有与世界其他地区进行实际的权威比较。</p><p>这令人沮丧。如果有一种方法可以在对话中标记“此语句处于贵格会模式”，将会很有用。可悲的是，据我所知，英语过去使用这种标记的每一次尝试都被收买为强化剂。英语单词“ <a href="https://en.wiktionary.org/wiki/very">very</a> ”据说源自“verrai”，意思是“真实”。</p><p>在某些例外情况下，人们普遍认为不允许出现事实错误。美国法律体系对在法院宣誓的人持悲观态度。有些合同希望真实反映所发生的事情。 （尽管离婚率令人震惊，但“直到死亡将我们分开”仍然是许多婚姻誓言中的内容，并且一些类似于最终用户许可协议等法律合同的文件的可执行性值得怀疑。）有些人在个人层面上，设法创造出不应该发生事实错误的空间。偶尔，整个社区都会尝试这样做。正如本杰明在《贵格会和蛇佬腔》中指出的那样，现实生活中的贵格会仍然存在。</p><p>然后还有LessWrong。</p><h2>四．</h2><p> LessWrong 有时称自己是一个寻求真相的社区。 <span class="footnote-reference" role="doc-noteref" id="fnrefgdce232pvfk"><sup><a href="#fngdce232pvfk">[2]</a></sup></span>当我写这篇文章时，关于页面说“我们寻求持有真正的信仰”并且（我声称）暗示我们寻求公开承认真正的信仰。我喜欢这个社区，很大程度上是因为我发现真理是一件美丽的事情，值得歌曲和诗歌，值得奉献一生去追求它。</p><p>但群体并不统一，真诚程度也各不相同。有人在 LessWrong 上发表评论这一事实并不意味着您可以绝对信任他们。这甚至并不意味着他们会像你一样关心真相。也许他们是新来的。也许他们确实关心真相，但有一个不同的惯用案例，例如上面的“稍等一下”示例，您没有意识到。也许他们正在努力，但未能坚持这些理想，失败令人痛苦，而承认这一点更令人痛苦。</p><p> （或者也许他们根本不在乎。毕竟，有些人只是喜欢看着世界燃烧。）</p><p> （所列出的原因并不详尽。）</p><p>听到在山上的某个地方有一座闪闪发光的城市，那里有自由说出真相的地方，一个演员永远不被允许在不改变他们的方式的情况下踏上的地方，我会很高兴。无论何时，当权衡正确时，尽我所能尝试帮助该项目实现。这不是我的中心目标，但如果有的话那就太好了。</p><p>部分问题在于《永恒的九月》新人需要适应，但我认为更大的问题是团队协调。我所见过的试图将整个理性主义者群体拖入真相，去追捕<a href="https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag">雄鹿而不是兔子，但</a>并没有奏效。你可以尝试让自己遵守这个标准，你可以耐心地向另一个演员或蛇佬腔证明贵格会的方式更好，也许你应该这样做，但我认为口语和随意的用法将继续成为人们交谈的方式。</p><blockquote><p> “我用每个人都能理解的语言与他们交谈，”安德说，“这并不圆滑。事情已经很清楚了。”</p><p> ——奥森·斯科特·卡德《死者代言人》</p></blockquote><h2>五、</h2><p>告白时间。在这种二分法中，我自称是蛇佬腔。我同情贵格会教徒，但我不再认为自己是他们中的一员。</p><p>过去，我要花很长一段时间才能回应人们对我说的话。我指的不是等待他们说完时的一小段间隙，而是整整五到十秒的死气沉沉。看看，如果你问我“你今天做了什么？”然后我需要思考我的一天，总结重要的部分，决定这让我感觉如何，将其表达出来，然后检查以确保这些话我们从所有可能的角度都是正确的。这通常需要多次修改，在开始说话之前在心里重写句子的多个草稿。 “我的钥匙在哪里？”它们在柜台上 - 不，等等，我实际上不知道，因为我没有看着它们 - 我上次在柜台上看到它们 - “柜台”与其他柜台有足够的区别吗？ - 等等我&#39;我想这个问题太久了aaaah。 “你什么时候到？” “很有可能在晚上八点之前，但我估计在下午五点到六点之间，条件是我的车没有出去，否则——等等，抱歉，我提供了太多信息，而且格式很奇怪，啊啊啊啊啊啊啊啊。”</p><p>现在我只是说“五点三十分，如果我迟到了，我会通知你。”我实际上并没有弄清楚这是否准确，但它符合暂时的意图，我可以在对方结束句子后一秒钟内回答。同样，如果有人说他们会在商店买牛奶，我就不会再因为那天晚上冰箱里没有牛奶而感到困惑了。做他们说过的事并不是一个重大的誓言，而只是一个短暂的意图。</p><p> （我想在这里指出，在所有贵格会和演员中，本杰明从来没有将演员视为故意撒谎作为故意策略的一部分，只是真的不可靠。这让我很喜欢作者，我会继续说用法，尽管我想花点时间来确定故意和恶意的骗子确实存在，并且当你接触到这样的人时，与演员互动的有效习惯将会灾难性地失败。这种骗子不属于本文的范围虽然我并不是说我会这样做，但对此保持持续警惕是有价值的。）</p><p>我同意本杰明的观点，即处于演员模式会削弱追求真相的本能，而且我还认为，某人有时撒谎的已知事实是他们此时此刻可能撒谎的重要证据。你永远不应该完全相信蛇佬腔处于贵格会模式。</p><p> （你也不应该完全相信贵格会教徒！零和一不是概率！他们可能是错的，他们可能认为这是值得撒谎的事情，他们可能已经被同卵双胞胎取代了！时刻保持警惕。）</p><p>然而，反之亦然。我认为处于贵格会模式会消耗你的谎言、社会融合和灵活性的能力。它使您容易受到他人谎言的影响，无法预见和预测他们可能会误导或误导。如果贵格会教徒和演员真的存在并混合在一起，我怀疑贵格会教徒会发现自己一次又一次地感到沮丧和欺骗，因为他们没有预料到错误的事情会发生。</p><p>我可以在短期内在自己的脑海中注意到这一点。当我从一个长周末与完全理性主义者互动（他们每时每刻都提醒着这个社区是谁和什么）转变为周一早上在火车站与我旁边的一个陌生人聊天时，我就很难想出快速而圆滑的语言回答“你好吗？”当我参加理性主义聚会时，在最初的几次对话中，我必须纠正自己给出的快速而简单的答案，但可能不是最真实的。</p><p>也许我只是犯了典型的思维谬误。我自己的想法似乎是一个真实的事实，成为一名贵格会教徒意味着发现世界是一个令人困惑的地方，充满了不可预测的危险，充满了我无法防御的不实言论。概括起来，我认为我的选择是：</p><ol><li>成为一名贵格会教徒并生活在混乱之中。</li><li>成为一名演员并放弃大部分长期合作的能力。</li><li>成为一个蛇佬腔，并根据与谁交谈而转换语码，接受对我讲真话能力的损害以及由于错误识别而导致的错误。</li></ol><p>其中，我选择3个。</p><p>需要明确的是，在我最糟糕的情况下，我认为我只像理想化的演员一样不值得信任。这些人并没有被塑造成骗子或恶意者，只是被塑造成不认为言论行为对未来行动具有约束力的人。大多数时候，根据我自己的评价，我比周围的中间人稍微诚实、直率，并且履行了更多的口头承诺。如果你从这篇文章中得到的结论是，我会为了我自己的利益而试图对你撒谎，让你做一些违背你利益的事情，那么我认为你没有正确理解我的意思。这篇文章的全部目的是让人们更容易地模仿我何时会陈述不真实的事情。我在这里付出了额外的努力，并且我试图在诚实方面犯错误。</p><p>我试着对那些我观察到并估计说真话的人只说真话，如果你是这种二分法中的贵格会教徒，我想知道，这样我就可以回报。然而，默认情况下，你不应该认为我所说的一切都是我发誓的；我是蛇佬腔，只觉得有必要对那些我认为有必要对我说实话的人说实话，而且除了实话之外别无其他。</p><p> （你不应该完全信任任何人，时刻保持警惕。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndnozbd7ldgr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnozbd7ldgr">^</a></strong></sup></span><div class="footnote-content"><p>这是一个关于人们相当合理地期望为真的事情实际上往往不是真的的笑话。如今，我对报纸及其解读科学研究的能力评价不高。</p><p>解释笑话可能会毁了笑话，但在这篇文章中，尝试并严格准确似乎异常值得。另外，这是一个很好的例子：如果我没有包含这个脚注，贵格会教徒会接受这个笑话吗？蛇佬腔怎么样？如果我有理由相信人们有时不阅读脚注，这会改变它的可接受程度吗？</p></div></li><li class="footnote-item" role="doc-endnote" id="fngdce232pvfk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgdce232pvfk">^</a></strong></sup></span><div class="footnote-content"><p>这是不真实的。社区没有可以说话的嘴，也没有可以打字的手指。社区发言是一种类型错误。我不同意这种将格式塔人类群体具体化为有行动能力的比喻。不过，这是另一天的文章了，在这里我使用了这个比喻。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths<guid ispermalink="false"> R28YGeAzDHehrnc7f</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:02:19 GMT</pubDate> </item><item><title><![CDATA[Life on the Grid (Part 1)]]></title><description><![CDATA[Published on November 15, 2023 10:37 PM GMT<br/><br/><p>一个人成长环境的物理布局会影响他们成年后的认知能力。<a href="https://www.nature.com/articles/s41586-022-04486-7"><u>最近一项基于 38 个国家 397,162 人的视频游戏数据的研究</u></a>发现，“在城市以外长大的人更擅长导航。”更具体地说，“在街道网络熵较低的城市（例如芝加哥）长大，在常规布局的视频游戏级别上会获得更好的结果，而在城市外或街道网络熵较高的城市（例如布拉格）长大会导致更好的结果。”结果在更高熵的视频游戏水平上。”</p><p>用简单的英语来说：如果你在类似网格的环境中长大，那么你在不太像网格的环境中导航会更差。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1456w"><figcaption>环境与视频游戏之间的关联（海洋英雄探索）寻路表现按年龄、性别和教育程度分层。 SHQ 寻路性能是根据轨迹长度计算的，并在 5 年窗口内取平均值。 </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1456w"><figcaption>两个 SNE 较低（芝加哥）和较高（布拉格）城市的例子。右图：街道方位分布在 36 个 10 度的区间内。</figcaption></figure><p>这一发现也许并不完全令人惊讶，乍一看似乎也没有那么重要。我们绝大多数人几乎不再需要在现实生活中真正找到路了。技术已经使我们的导航技能几乎过时了。那么某些环境在保护它们方面是好是坏又有什么关系呢？</p><p>这很重要。<strong> </strong>为了理解其中的原因，我们需要绕一点弯路。</p><p>复杂性科学家大卫·克拉考尔 (David Krakauer) 在<a href="https://www.samharris.org/blog/complexity-stupidity"><u>与 Sam Harris 的“Making Sense with Sam Harris”</u></a>节目中区分了互补性认知人工制品（使用后使我们变得更加聪明的技术）和竞争性认知人工制品（如果您无法猜出这些人工制品的作用，那么也许您会知道）已经使用它们太多了）。竞争性神器的典型例子是计算器：重复使用会让你的心算能力比以前更差。与算盘相比，算盘可能产生完全相反的效果：专家用户最终可以开发出如此高保真的心智模型，他们甚至不再需要使用物理算盘，并且能够在没有算盘的情况下保持增强的算术技能。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1456w"><figcaption><i>玛格丽塔哲学</i>中的<i>算术类型</i>(1503)</figcaption></figure><p>大脑是一个复杂的系统，充满了相互关联的表征系统。这就是为什么算盘不仅能帮助你数学，还能帮助你做所有事情。 Krakuer 博士解释说，“[大脑] 周围没有防火墙，因此算盘的功能优势仅限于算术……它实际上对语言能力和几何推理产生非常有趣的间接影响。”超出预期用途的积极的全局效应是所有互补认知制品的一个特征。</p><p>另一方面，<i>竞争性</i>认知制品的功能<i>缺点</i>也不限于某个领域或技能。 Krakauer 博士接着讨论了用自动寻路技术取代地图、星盘和六分仪等原始寻路技术是如何产生这样的效果的：</p><blockquote><p>您对地图制作以及地形、拓扑和几何推理的熟悉通常对您的生活很有价值，而不仅仅是在城市中导航。因此，拿走地图不仅会让你从一扇门到另一扇门变得更糟，还会在很多方面让你变得更糟……爱因斯坦和弗兰克·劳埃德·赖特都依赖的一个很好的例子是木立方体。在他们年轻的时候，他们都非常迷恋这些立方体，并用立方体构建世界，比如《我的世界》。他们两人都声称——弗兰克·劳埃德·赖特（Frank Lloyd Wright）就建筑而言，爱因斯坦就宇宙几何而言——他们在玩这些立方体时建立的直觉对他们后来的生活发挥了重要作用。我想说地图也是如此。如果你知道如何穿越一个真实的空间，比如欧几里得空间或地球表面的弯曲空间，那么你就可以思考不同类型的空间、关系空间、想法空间。作为一种隐喻，从一个想法到另一个想法的路径概念实际上在现实空间中的路径方面具有直接且自然的实现。</p></blockquote><p>我们的认知（推理、记忆、创造力等）类似于一种心理导航，但我们的<a href="https://en.wikipedia.org/wiki/Conceptual_metaphor"><u>概念隐喻</u></a>却背叛了这一点：“一个知识领域”、一个“未经探索的主题”、“一系列思想”、 “沿着记忆之路旅行”、“唤起你的记忆”、“一次幻想”。神经科学正在快速发展（这是比喻），但<a href="https://www.quantamagazine.org/the-brain-maps-out-ideas-and-memories-like-spaces-20190114/"><u>新出现的证据</u></a>支持这一说法：“大脑以代表空间位置的方式编码抽象知识，暗示了一种更普遍的认知理论。”</p><p>这也是为什么古老的记忆增强技术——<a href="https://artofmemory.com/blog/method-of-loci/"><u>轨迹法</u></a>——记忆宫殿技术——至今仍被记忆冠军所使用。根据<a href="https://artofmemory.com/blog/method-of-loci/"><u>《记忆的艺术》</u></a> ，“轨迹的方法涉及通过在想象的旅程中的某个点为每个要记住的项目放置一个助记图像来记忆信息。然后，通过在想象的旅程中在脑海中走同样的路线，并将助记图像转换回它们所代表的事实，就可以按照特定的顺序回忆信息。”<a href="https://fs.blog/a-philosophy-of-walking/"><u>步行与创造力</u></a>之间的联系早已为<a href="https://www.themarginalian.org/2021/12/12/nietzsche-walking/"><u>知识分子和艺术家</u></a>所知，并得到<a href="https://news.stanford.edu/2014/04/24/walking-vs-sitting-042414/"><u>最近研究的</u></a>支持，在这里也值得注意——就好像穿过物理景观的运动为在更抽象的思想景观中更轻松的运动奠定了基础（参见有关<a href="https://en.wikipedia.org/wiki/Embodied_cognition"><u>具体认知</u></a>的更广泛的文献，以进一步讨论该主题）。</p><blockquote><p>行走的节奏产生了一种思维的节奏，穿过风景的通道呼应或刺激着一系列思维的通道。这在内部通道和外部通道之间创造了一种奇怪的和谐，这表明心灵也是一种风景，步行是穿越它的一种方式。新的想法常常看起来像是一直存在的景观的一个特征，就好像思维是在旅行而不是在创造。因此，步行历史的一个方面就是具体化的思维历史——因为心灵的运动无法被追踪，但脚的运动可以。</p><p> ——丽贝卡·索尔尼特， <a href="http://www.amazon.com/exec/obidos/ASIN/0140286012/braipick-20"><i><u>《流浪癖：行走的历史》</u></i></a></p></blockquote><p>我们还使用空间隐喻来谈论社会经济景观或生活本身：“人生旅程”、“拓宽视野”、“发现自我”或“自我反省”、“职业道路”、“各行各业” “ 等等。当我们考虑我们深刻的进化历史时，这是有道理的：我们生存所必须知道的许多最重要的事情都是自然界中的空间问题——角马迁徙穿过这个山谷，我们可以在那片森林里采集浆果，有人被一只动物杀死了。我们有时甚至用树栖隐喻（“知识分支”）来概念化知识——这也许是我们祖先树居生活方式的遗迹。</p><p>我们缺乏寻路能力也存在于我们的物理、文化和形而上学景观中，这些景观也变得规则和网格状。我相信我们现在正在目睹的后果是缺乏韧性和足智多谋，不愿意“开拓道路”，无法产生“开创性”创新，以及许多其他缺陷，这些缺陷使我们的处境比以前更糟。我们所处的世界更加混乱和古怪。我们所采用的社会和智力“技术”（例如教育体系、育儿规范）已日益成为竞争性认知产物，而不是合作性产物。</p><p>世界的过度“网格化”在物理领域表现得最为明显。由于各种原因（经济、环境、不断变化的审美偏好），新城市往往有更简单的布局。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1456w"><figcaption>图中两个最古老的美国城市波士顿和夏洛特是最不规则的。两个数据的来源： <a href="https://geoffboeing.com/2018/07/city-street-orientations-world/"><u>GeoffBoeing</u></a> </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1456w"><figcaption></figcaption></figure><p>建筑也是如此。世界各地更加华丽和文化独特的形式已被单调的巨石所取代。其结果是令人压抑的标准化和同质化，失去了赋予地方独特特征的怪癖和怪癖。</p><p>不规则性和变化的消失是世界物理网格化的一个方面，但全球范围内纵横交错的道路和轨道数量也非常多。 “脱离电网”几乎是不可能的。事实上，任何事物、任何地方都无法逃脱我们在地球上撒下的技术社会网络。未知的领域已成为过去。</p><blockquote><p>即使你现在想退学，可以吗？无论您走到哪里，无论何时，您都会被识别、跟踪、联网和分类。历史上从未如此难找到一个新的开始、一个干净的开始，或者一个按照自己的鼓手的节奏前进的地方。不再有瓦尔登湖或隐居处——甚至连脱离电网的地点现在也都在电网上了。</p><p> —<a href="https://tedgioia.substack.com/p/multitasking-isnt-progressits-what"><u>特德·乔亚</u></a></p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1456w"><figcaption> 1539 年，一条巨大的海蛇袭击了挪威海岸奥劳斯·马格努斯 (Olaus Magnus) 的卡塔码头上的一艘船，这张图片来自 1572 年版本。</figcaption></figure><blockquote><p>如果你在 18 世纪长大，仍然有新的地方可以去。听完外国冒险故事后，您自己也可以成为一名探险家。整个 19 世纪和 20 世纪初可能都是如此。从那以后，《国家地理》的摄影作品向每个西方人展示了地球上最具异国情调、尚未开发的地方的样子。如今，探险家的身影大多出现在历史书和儿童故事中。父母不希望他们的孩子成为探险家，就像他们不希望他们成为海盗或苏丹一样。也许在亚马逊雨林深处有几十个与世隔绝的部落，我们知道在海洋深处还存在最后一个地球边界。但未知似乎比以往任何时候都更难接近。</p><p> ——彼得·泰尔， <i>《从零到一》</i></p></blockquote><p>彼得·蒂尔认为，创新精神已经退化，部分原因是我们不再相信秘密，而我们不再相信秘密是因为不再有任何可进入的边界。这对于人类来说确实是前所未有的状况。亿万年来，我们的思想和文化与未知世界（地图上标有“这里有龙”的地方）微妙地共生。如果没有这个未知，那个可能有<a href="https://en.wikipedia.org/wiki/El_Dorado"><u>黄金之城</u></a>或<a href="https://en.wikipedia.org/wiki/Fountain_of_Youth"><u>青春泉源的</u></a>地方，英雄（但不仅仅是英雄，我们所有人）就无处可去，而所有能让我们成为英雄的东西——勇敢，坚韧、聪明才智、大胆等等——开始萎缩。如果没有这种未知，我们就会开始感到被限制——被困住——就像一只美丽而危险的动物被关在狭窄的笼子里：我们会产生幽闭恐惧症；我们会产生幽闭恐惧症；我们会感到恐惧。想象力和灵感枯萎。我们不再像以前那样充满希望，但我们不知道为什么。 </p><hr><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1456w"><figcaption>那孩子的脸说明了一切——算盘是侧面的牢房铁条（<a href="https://www.flickr.com/photos/nationaalarchief/3896157508"><u>来源</u></a>）</figcaption></figure><p>网格上的生活从很小的时候就开始了，当你在现代教育系统的充满泪水和单调的工厂里找到一份工作时，你将坐在方形建筑中的方形房间里的方形桌子旁，阅读方形书籍并在上面书写。正方形的纸伴随着你童年的无数个小时（在大多数情况下，我猜它们会是矩形，但你明白了）。通过证明您可以遵循任意规则并重复在明确定义的主题、单元和章节中灌输给您的知识，您将从一个年级升级到下一个年级。如果你对这个游戏有一点点擅长，他们会坚持让你在生命的前 22 年里都这样做，但即使这样也<a href="https://theconversation.com/if-the-masters-degree-is-the-new-bachelors-is-the-doctorate-now-the-new-masters-95577"><u>还</u></a><a href="https://www.nytimes.com/2011/07/24/education/edlife/edl-24masters-t.htmlyXTmUugkI2NQyJhdyNtA/edit"><u>不够</u></a>。</p><p>在某个时刻，上帝保佑，你将进入“现实世界”，也就是你的父母和老师经常谈到的神秘境界。然而，你很快就会发现“现实世界”是另一个庞大的官僚体系，它与学校的“虚假世界”并没有太大区别。当然，你有更多的自由，有些事情略有不同（老板而不是老师，小隔间而不是桌子），但基本原理是相同的：只要保持笔直和狭窄，低着头，不断提高水平。</p><p>至于我们的毕业后生活（所剩无几），许多人进入美国企业界或成为公务员。我们中的一些人成为了艺术家或企业家，但即便如此也无法逃脱：这些职业道路也被网格吞没了。<a href="https://erikhoel.substack.com/p/how-the-mfa-swallowed-literature"><u>埃里克·霍尔 (Erik Hoel) 写道</u></a>，当代作家与近代作家（2000 年代初）之间存在着一个无可争议的区别：<i>现在几乎每个人都拥有艺术硕士学位</i>。</p><blockquote><p>但如今，大多数 50 岁以下的出版业成功人士<i>实际上都</i>获得了 A+。他们都在正确的时间举起了手，做了他们需要进入哈佛、阿默斯特或威廉姆斯或其他任何地方所需的一切，然后跨越所有必要的障碍进入爱荷华作家工作室或哥伦比亚大学等。</p><p>福克纳没有完成高中学业，最近的研究显示，伍尔夫参加了一些古典文学和文学课程，但大多是在家接受教育，陀思妥耶夫斯基拥有工程学位……这些伟大的作家中没有一位会被<i>该国的</i><i>任何</i>艺术硕士学位<i>录取</i>。学术渠道的结果是，尽管当代作家表面上的种族和性别多样性与以前的时代有很大不同，但他们的信仰和风格却非常相似，比过去的作家更加相似。 </p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1456w"><figcaption></figcaption></figure><p>对于企业家来说，情况更是如此。盖茨和扎克伯格的时代已经一去不复返了，那时高级学位是可选的，数字前沿是敞开的。<strong> </strong>现在，每个人都阅读<a href="http://www.paulgraham.com/articles.html"><u>相同的文章</u></a>，遵循相同的指南（“ <a href="https://www.inc.com/entrepreneurs-organization/eight-steps-to-becoming-a-tech-entrepreneur-when-you-know-nothing-about-technolo.html"><u>当你对技术一无所知时成为技术企业家的 8 个步骤</u></a>”），并申请相同的项目、孵化器和<a href="https://www.ycombinator.com/apply/"><u>组合器</u></a>。</p><p>这是泰勒·考恩 (Tyler Cowen) 在 2007 年撰写的文章（“<a href="https://www.nytimes.com/2007/06/14/business/14scene.html"><u>美国青少年可能会制造麻烦或企业家的松散缰绳</u></a>”）：</p><blockquote><p>网络背后的新思想和商业原则为年轻人开辟了理想的领地。新手更有可能看到音乐可以来自计算机而不仅仅是来自商店或收音机，或者最好不通过旅行社预订航班。纽约大学副教师克莱·舍基 (Clay Shirky) 指出，许多年轻人很幸运，因为他们对互联网企业没有先入之见。多年的经验对于提炼和改进长期熟悉的产品（例如面包）至关重要。但对于创办 Napster 或 YouTube 来说，全新的、打破常规的想法（通常来自年轻人）更为重要。</p></blockquote><p>十六年后，现在所有这些实际上都是错误的。年轻人<a href="https://www.reddit.com/r/technology/comments/10o2vu7/gen_z_says_that_school_is_not_shipping_them_with/"><u>甚至不知道如何使用电脑</u></a>了。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1456w"><figcaption></figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1456w"><figcaption></figcaption></figure><p>回顾一下：在简单的空间环境中长大并使用 GPS 已经给你的大脑带来了伤害，生活变成了一场令人心碎的电子游戏，完全没有神秘或冒险。我们就像蜘蛛网中的昆虫一样被困在网格中；激烈的斗争只会让我们更加陷入困境。为了摆脱困境，我们作为个体必须轻轻地颠覆网格的根基，网格不是外在的，而是人性的一个方面：控制的冲动、系统化的本能、我们憎恶异常和模糊性并寻求的部分消灭他们。一开始认真地尝试挣脱束缚，很容易就会陷入自我强加的规则和实践体系，需要满足标准和遵循时间表。因此，我不愿意提供任何具体的建议——它们只会限制你的思维。目前，也许只有一件事可以说：如果你想脱离电网，那就迷路吧。</p><br/><br/><a href="https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1<guid ispermalink="false"> j4cKyhDEBpGPLNpig</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 15 Nov 2023 22:37:31 GMT</pubDate> </item><item><title><![CDATA[Glomarization FAQ]]></title><description><![CDATA[Published on November 15, 2023 8:20 PM GMT<br/><br/><p>我在这里发布这篇文章的主要原因是我希望公开记录我在 2023 年写的这篇文章。这样，我可以向将来的人们表明我不是当场编造的；而是我在 2023 年写的这篇文章。早在他们这次问我什么事之前，这个问题就已经存在了。我没有自己的博客，所以我把它放在这里。不过，有些人可能会觉得它很有趣，或者想自己使用它。</p><p></p><p>如果我给你发了这篇文章，你可能只是问了我一个问题，而我的回答是“我拒绝证实或否认这一点”。也许你问我是否写了一个特定的故事，或者我是否与某人发生了性关系，或者我是否抢劫了一家银行。当你听到我的拒绝时，你可能会变得<i>更加</i>怀疑，假设这意味着我确实写/敲/抢劫了这个故事/人/银行。</p><p>这就是为什么情况不一定如此的解释。</p><p></p><p><strong>你为什么拒绝回答我的问题？</strong></p><p>假设——就像这篇文章中通常坚持的那样，我并不是说这是对是错，只是要求你考虑这个假设——我一生中从未犯过任何罪行，除了……比如说，公共场所小便。我头上套着一个袋子，所以人们不能确定是我。尽管如此，警察还是能够将嫌疑人的范围缩小到一个很小的名单，我也在其中，他们问我是否是我干的。</p><p>有些人只是对警察撒谎。不过，这里的关键因素是我<i>不想</i>对此撒谎。整个政策是为了避免谎言，以及在保持完全诚实的同时能够维护我的隐私。 <span class="footnote-reference" role="doc-noteref" id="fnreflpyzjd5jwgk"><sup><a href="#fnlpyzjd5jwgk">[1]</a></sup></span>如果我拒绝撒谎，但又不想让他们知道真相，那么我根本无法回答这个问题。所以我说，“我拒绝证实或否认这一点。”</p><p>但假设警察问我是否杀了人——但我<i>没有</i>这么做。他们问我是否抢劫了银行，是否贩卖毒品，以及其他有关我以前从未犯过的犯罪的问题。我总是说实话：“不，我没有那样做。”当然，现在警方知道在公共场合小便问题的答案是肯定的，因为这是我唯一拒绝回答这一问题的问题。</p><p>唯一不告诉他们我的秘密的政策，除了公然撒谎之外，就是拒绝证实或否认我是否犯下了<i>任何</i>罪行——<i>即使是那些我实际上没有犯下的罪行</i>。</p><p></p><p><strong>好吧，但这是否意味着你一开始就一定犯了一些罪行？毕竟，如果你不这样做，那么回答每一个存在的犯罪问题也没有什么坏处。</strong></p><p>这项政策不仅仅适用于我迄今为止所犯下的罪行。我可能会遇到<i>很多</i>其他可能的情况，如果回答不一致就会放弃信息。</p><p>例如，如果我还没有犯罪，但将来会犯罪，那么当我从总是说“不”变成总是说“我拒绝证实或否认”的那一刻，警察就知道我已经犯罪了。刚刚做了<i>某事</i>——</p><p></p><p><strong>那么你打算犯罪吗？</strong></p><p>不！或者更确切地说，我拒绝确认或否认我正在计划犯罪，但是<i>不，这不是我之前所说的有效结论！</i>首先，即使我预计几十年后犯罪的可能性很小，为了以防万一，仍然值得遵守这条规则 - 别用那种眼神看我，我还没说完。<i>此外</i>，这不仅仅适用于犯罪。</p><p>假设你问我是否与 X 发生过性关系<span class="footnote-reference" role="doc-noteref" id="fnrefiscl4hvc39"><sup><a href="#fniscl4hvc39">。 [2]</a></sup></span>出于与之前相同的原因，如果答案是“是”，我不能只是拒绝确认或否认我是否与该人发生过性关系，因为我的反应的差异就暴露了这一点。 （“我没有和W发生过性关系，我不能告诉你关于X的事情，我没有和Y发生过性关系，我没有和Z发生过性关系......）</p><p>如果我从未与任何人发生过性关系，但犯了罪，那么通过对性问题回答“否”，但对犯罪问题回答“我拒绝证实或否认”，我将透露我在至少一项犯罪。因此，即使我是处女，我仍然不得不拒绝回答性问题。</p><p>还有一些关于逻辑决策理论以及与我自己的假设版本合作的内容，但我认为这些都没有必要证明这一点。是的，我预计在过去、现在或将来的某个时刻，很有可能有人问我一个我不想诚实回答的问题。这足以表明我需要拒绝确认或否认某些事情。</p><p></p><p><strong>您所遵循的总体政策到底是什么？</strong></p><p>拒绝回答任何问题，如果该问题的可能答案之一（如果为真）是我想要隐藏的东西 - 无论所述答案实际上是否为真。也称为“全球化”。 <span class="footnote-reference" role="doc-noteref" id="fnreflwpck0izhj"><sup><a href="#fnlwpck0izhj">[3]</a></sup></span></p><p></p><p><strong>始终遵循这一点不是很困难吗？</strong></p><p>是的！事实上，尽管我很想总是荣耀化，但出于实用性，我经常不得不做出例外。如果有人问我今天做了什么，那么理论上我将不得不拒绝回答，因为可能的答案之一就是“我偷了你的车”。但大多数时候，我最终只是回答问题。</p><p>那么，什么<i>类似于</i>严格的全球化，但仍然足够宽松，我不必拒绝回答每个问题？好吧，我从成本效益分析开始。如果警察问我是否是袋头同伴<span class="footnote-reference" role="doc-noteref" id="fnref29ukuc1sh14"><sup><a href="#fn29ukuc1sh14">[4]</a></sup></span> ，那么在真相为“是”的情况下说真话的成本超过了在真相为“否”的情况下说真话的收益。所以在这里，我进行了环球化。但在不同的场景中，情况会发生变化：假设我中了一张 1,000,000 美元的彩票，而加拿大人没有资格购买，我需要向彩票人员提供我的地址才能拿到这笔钱 - 但我对我的隐私有轻微的偏好地址。在这种情况下，我居住在加拿大的情况下的成本远<i>低于</i>我居住在其他地方的情况下的收益。所以在这里，我说实话，因为在所有可能的情况下，全球化都会<i>伤害</i>我。</p><figure class="table"><table><tbody><tr><td>情况</td><td>答案A</td><td>答案B</td><td>成本如果 A</td><td>如果 B 则受益</td><td>你做什么工作？</td></tr><tr><td>被警方审问</td><td>“我做到了。”</td><td> “我没有这么做。”</td><td>进监狱？ <span class="footnote-reference" role="doc-noteref" id="fnref4iik2sevr7o"><sup><a href="#fn4iik2sevr7o">[5]</a></sup></span></td><td>警察更喜欢你</td><td>全球化</td></tr><tr><td>中了彩票</td><td>“我住在加拿大。”</td><td> “我住在[其他地方]。”</td><td>隐私较少</td><td>1,000,000 美元</td><td>说实话</td></tr></tbody></table></figure><p>我还需要考虑计算中的相关概率，并且可能会稍微随机化以保持秘密，而成本却很小。如果我99%的时间都花在无辜的事情上，1%的时间花在银行抢劫上，那么当我抢劫银行时，我不需要100%的时间去躲藏，只需2%的时间，或者10%如果我想更加安全的话。当然，这项政策的缺点确实给人们提供了一些证据，表明我隐藏某些东西的频率有多高，或者至少可以帮助他们建立一个上限。</p><p></p><p><strong>我怀疑你是否真的像你暗示的那样频繁地这样做。</strong></p><p>不幸的是，在撰写本文时我确实没有任何可以引用的证人。我家里的每个人都可以证实我确实经常拒绝回答问题，而且我认为在某些情况下他们后来发现我<i>没有</i>隐藏任何东西（有些情况下他们发现了我）。但这不是一个选择，因为我实际上不愿意确认或否认我的家人是谁。</p><p>幸运的是，对于除了我向其发送此帖子的第一个人之外的所有人，我将能够告诉他们我在他们之前向其发送过此帖子的人！因此，除非您碰巧是第一个人，否则当您阅读本文时，会有其他人可以证明我的怪异。</p><p></p><p><strong>你为什么说“这是机密”？</strong></p><p>有时我就是这样说“我拒绝证实或否认”！ “机密”听起来比“拒绝确认或否认”更酷，就像我是某种秘密特工一样。我是否实际上是某种秘密特工当然是保密的。</p><p></p><p><strong>您为什么在帖子前面列出“写了一个故事”？</strong></p><p>人们有时会因为用笔名写故事而受到指责。我和我之前的许多作者一样<span class="footnote-reference" role="doc-noteref" id="fnreffpnfyjvo0n"><sup><a href="#fnfpnfyjvo0n">[6]</a></sup></span> ，认为引发对我秘密写过或未写过哪些故事，以及我是或不是互联网上哪些其他作者的猜测是很有趣的。如果我写了一些我不想与我的真实姓名相关的禁忌内容，这也很有帮助。</p><p></p><p><strong>如果我确实想知道答案，我能做些什么吗？</strong></p><p>改变激励措施！例如，假设你是我的老板，我不想讨论政治，因为我认为你会解雇与你意见不同的人。如果你想了解我的政治信仰，那就证明你以前与人合作没有问题，即使你讨厌他们的政治立场。如果你不能证明这一点……好吧，这就是为什么我不回答你的问题。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnlpyzjd5jwgk"> <span class="footnote-back-link"><sup><strong><a href="#fnreflpyzjd5jwgk">^</a></strong></sup></span><div class="footnote-content"><p>当然，即使我一般不诚实，也不对警察撒谎还有一个额外的原因——这是非法的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fniscl4hvc39"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiscl4hvc39">^</a></strong></sup></span><div class="footnote-content"><p>我实际上不太关心人们知道我与谁发生过性关系或没有与谁发生过性关系，但还有其他原因我想隐藏此类事情：尊重他人的隐私，或者如果这个人太不受欢迎，以至于如果人们知道我和他们发生过性关系，或者他们在逃避法律，并且我不想透露他们在我家里，他们就会拒绝与我交往 - 废话，现在我再次回到犯罪问题上来。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlwpck0izhj"> <span class="footnote-back-link"><sup><strong><a href="#fnreflwpck0izhj">^</a></strong></sup></span><div class="footnote-content"><p> Glomarization 是以中央情报局想要保密的一艘船命名的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn29ukuc1sh14"> <span class="footnote-back-link"><sup><strong><a href="#fnref29ukuc1sh14">^</a></strong></sup></span><div class="footnote-content"><p>这个词里应该有多少个 e？</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4iik2sevr7o"> <span class="footnote-back-link"><sup><strong><a href="#fnref4iik2sevr7o">^</a></strong></sup></span><div class="footnote-content"><p>根据谷歌的前几条结果，通常只会被处以 500 美元的罚款，但如果是重复犯罪或在一大群人面前，惩罚就会加重，你<i>很</i>可能最终会入狱。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfpnfyjvo0n"> <span class="footnote-back-link"><sup><strong><a href="#fnreffpnfyjvo0n">^</a></strong></sup></span><div class="footnote-content"><p>或者也许在我之前<i>零个</i>作者。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq<guid ispermalink="false"> N5txrJDimv8nz4n24</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 15 Nov 2023 20:20:49 GMT</pubDate> </item><item><title><![CDATA[Testbed evals: evaluating AI safety even when it can’t be directly measured
]]></title><description><![CDATA[Published on November 15, 2023 7:00 PM GMT<br/><br/><p>我和一些合作者最近发布了论文“泛化类比（GENIES）：将人工智能监督泛化到难以测量领域的测试平台。 （<a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20">推文线程</a>）。在这篇文章中，我将解释 GENIES 基准如何与更广泛的方法相关联，用于预测人工智能系统是否安全<i>，即使无法直接评估其行为。</i></p><p>总结：<strong>当AI安全性难以衡量时，检查AI对齐技术是否可以用来解决更容易评分的类似问题。</strong>例如，要确定开发人员是否可以控制诚实如何推广到超人领域，请检查他们是否可以控制其他分布变化的泛化，例如“五年级学生可以评估的指令”到“博士可以评估的指令”。或者测试开发者是否能够发现欺骗行为，检查他们是否能够识别故意植入的“木马”行为。即使特定人工智能系统的安全性难以衡量，人工智能安全的有效性<i>&nbsp;</i>研究人员及其工具通常更容易测量——就像在风洞和压力室等航空航天试验台中测量火箭部件比发射火箭更容易测量一样。这些“试验台”评估可能会成为任何人工智能监管框架的重要支柱，但迄今为止很少受到关注。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/okmB8ymyhgc65WckN/ghuihv6jreaal9mzim2g"></p><h2><br>背景：为什么人工智能安全“难以衡量”</h2><p>很难判断人工智能系统是否遵循开发人员的指令有两个基本原因。</p><p><strong>人工智能的行为可能看起来不错，但实际上并不好。</strong>例如，很难判断超人人工智能系统是否遵守“开发您确信安全的后继人工智能”等指令。人类可以查看人工智能计划并尽力确定它们是否合理，但很难知道人工智能系统是否在玩弄人类的评估——或者更糟糕的是——它们是否有隐藏的意图并试图对我们实施快速的攻击。</p><p><br><strong>在某些环境中观察人工智能行为会带来风险。</strong>前沿法学硕士在部署后就发现了许多安全故障，当人工智能系统超过一定的能力阈值后，这显然会变得不可接受。相反，开发人员必须彻底评估模型<u>无法</u>采取<u>真正危险</u>操作的测试环境中的安全性。如果人工智能系统故意等待机会采取危险行动或出现其他“实验室”中未出现的故障模式，那么以这种方式评估人工智能系统将尤其具有挑战性。</p><h2><br>其他行业的安全也很难衡量</h2><p>在人工智能安全中，安全性的衡量似乎特别具有挑战性，因为在大多数行业中，不安全的系统并不<i>试图</i>看起来安全；相反，它是安全的。然而，我们仍然可以从其他行业的安全衡量方式中吸取教训。</p><p>例如，通过实际将火箭发射到太空来测试火箭的成本很高，类似于通过实际部署来测试人工智能系统的危险程度。航空航天工程师在称为“测试台”的更易于测量的设置中执行尽可能多的测试。例如，他们建造模拟真空空间压力和温度条件的室，建造对结构部件施加应变和振动的钻机等。通过“桌面场景”对核设施工作人员进行评估，以确定他们如何应对灾难。</p><p>通常，当难以测量时，可以使用易于测量的测试来预测安全性。</p><h2>人工智能安全的“试验台”</h2><p><strong>定义。</strong>我将使用“测试台”一词来指<strong>代类似于使人工智能系统更安全但更容易评分的问题。</strong>开发人员能够在多大程度上解决这些问题应该反映出他们实际上能够在多大程度上确保人工智能系统的安全。</p><p>人工智能安全“测试平台”的示例包括：</p><ul><li><a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20"><strong>泛化类比</strong></a><strong>。</strong>要确定开发人员是否可以将诚实概括为超人能力，请检查他们是否可以控制从“五年级学生可以回答的问题”到“博士可以回答的问题”的概括。更一般地说，测试他们是否可以控制各种分布变化的泛化。</li><li> <a href="https://www.lesswrong.com/posts/EbL5W5ccwfbqFiYBJ/auditing-games-for-high-level-interpretability-1"><strong><u>审核游戏</u></strong></a><strong>：</strong>要确定可解释性工具是否可以捕获欺骗行为，请检查它们是否可以<a href="https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5#Trojan_Horse_Models"><u>识别其他开发人员植入的木马</u></a>，或者区分经过故意训练的欺骗性模型。</li><li> <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><strong><u>元级对抗性评估</u></strong></a>：为了测试您的监督过程的稳健性，训练人工智能系统来利用一些开发人员可以验证的漏洞，因为他们拥有监督过程中保留的信息。例如，训练人工智能系统删除它不应该访问的文件而不被抓住。一些开发人员了解测试，并且可以轻松检查任务是否完成，但监督者无法访问这些保留的信息。</li></ul><h2> Tesbed 的应用</h2><p><strong>人工智能监管。</strong>人工智能治理中的一个关键的开放问题是<i>如何设计“安全评估”。</i>如果人工智能系统通过这些具体测试，其部署范围就可以扩大。测试平台可用作安全评估。例如，监管机构可以在允许实验室部署模型之前检查实验室是否可以使用其可解释性技术来检测木马或控制玩具分发转移的泛化。</p><p><strong>从错位的人工智能中提取人工智能安全研究。</strong>由于测试台问题很容易衡量，人工智能系统可以因在这些问题上取得的进展而直接获得奖励。例如，开发人员可以指示人工智能系统编写一个程序来控制各种分布变化的泛化。通过阅读人工智能系统提供的程序，开发人员也许能够了解有关控制泛化的重要见解。如果测试平台对游戏足够鲁棒，即使是公然错位的模型也可能被迫做出真正的人工智能安全研究贡献（或者训练过程将用这样做的模型取代它们）。</p><h2>试验台设计注意事项</h2><p><strong>测试平台应该对“游戏”具有鲁棒性。</strong>如果测试平台用于监管或从失调的人工智能中提取人工智能安全研究，那么测试平台必须能够抵御对抗性的游戏尝试。例如，从表面上看，TrojAI 似乎是对可解释性技术的一个很好的测试。然而，许多在此基准测试中表现出色的论文与人工智能安全无关。例如，<a href="https://arxiv.org/pdf/2110.08335.pdf"><u>本文</u></a>使用 SGD 来搜索输入中的“简单”触发模式。这很有效，因为基准测试中的大多数触发模式恰好都很简单。设计强大的测试台可能需要仔细思考和大量迭代（理想情况下应该尽早开始！）</p><p><strong>测试平台在与其核心类比无关的所有方面都应该是多样化的。</strong>跟踪测试台的哪些方面实际上与您真正关心的问题类似非常重要。例如，将诚实从“五年级学生可以回答的问题”推广到“博士可以回答的问题”似乎类似于将诚实推广到超人困难的问题。但<i>为什么</i>这些问题都相似呢？ “从易到难”的分布转变有什么特别之处吗？如果不是，还应该衡量不同领域、角色等之间的泛化。从不同的类比集合中提取可以对人工智能安全工具进行更稳健的评估，就像采用异质样本如何更好地估计癌症治疗是否有效一样。</p><h2>结论</h2><p>我的印象是，许多监管机构和研究人员都在考虑评估人工智能系统的安全性，就像我们面前有一个囚犯一样，我们需要对他们进行一系列心理测试，以确定他们是否应该被释放到社会中。</p><p>这对安全评估的描述过于严格。人们需要进行一个重要的概念转变，从“这个特定的人工智能系统有多安全”到“我们的工具有多有效？”第二个问题清楚地告诉了前者，但直观上似乎更容易回答。</p><p>我目前正在考虑如何为人工智能安全建立更好的“测试平台”。如果您有兴趣合作，请通过<a href="mailto:joshuamclymer@gmail.com"><u>joshuamclymer@gmail.com</u></a>与我联系</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be<guid ispermalink="false"> okmB8ymyhgc65WckN</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Wed, 15 Nov 2023 19:00:42 GMT</pubDate> </item><item><title><![CDATA[Listening to Pascal Muggers]]></title><description><![CDATA[Published on November 15, 2023 6:06 PM GMT<br/><br/><p><br><strong><u>概括</u></strong><i><strong><u> </u></strong><u>-</u>我们已经被“帕斯卡抢劫者”抢劫了<strong>——</strong>这种内心想法认为后果如此巨大，以至于超越了所有正常的考虑。事情很模糊，但考虑到利害关系，不给予任何关注是不负责任的。同时，高效意味着不要给予他们超出任何可能有用的注意力。</i><br></p><h2>介绍</h2><p><u>在</u><a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities"><u>《Pascal&#39;s Mugging: Tiny Probabilities of Vast Utilities</u></a> 》（2007 年）中，Eliezer Yudkowsky 描述了一个思想实验： <span class="footnote-reference" role="doc-noteref" id="fnrefrcut65obs6"><sup><a href="#fnrcut65obs6">[1]</a></sup></span></p><blockquote><p>现在假设有人来找我说：“给我五美元，否则我将使用矩阵之外的魔力来运行一台图灵机来模拟并杀死 3^^^^3 个人。”</p></blockquote><p>以利以谢还写道：</p><blockquote><p>你或我可能会一笑置之，根据占主导地位的主线概率进行计划</p></blockquote><p>不是我。</p><p>这让我彻夜难眠。</p><p>好像我不能理解3^^^^3，感觉好浩瀚。这个数字感觉就像任何其他“奇怪的大数字”。问题是我知道我不能真正认为抢劫犯说的是假话的“可能性很小”。同时我知道抢劫犯可以任意增加公用事业的规模。如果我根据最大化预期价值做出决定，我应该支付 5 美元（或为此支付我的全部钱）。但给抢劫犯 5 美元的感觉很糟糕，放弃理性也很糟糕。</p><p>这两种选择似乎都需要做出无限愚蠢的承诺🤢</p><h2>让帕斯卡劫匪就位</h2><p>我们的情绪不能超越几个层次。在普通的大思维中，我们的情感影响量表可能类似于： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kgkfqgbs21fnmpitcz7j" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qvm5vnoef39ozcf14pcl 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/g30fzpnlq2cxm8oknslw 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/voqp0ccqlwjtpwdjxl36 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/zl2qdauebouybrgzgxxl 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nrgu2v7zwinc03kxsnlf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/erg9yk5oxhvgewp8r9o8 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jnft7iwkzrfasl7l0hl7 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hk9hsmxfq58caxstgtvq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nl30cktfb9vmlulorr9a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ule7bazztjrljt88n0sd 960w"></figure><p>但是，如果我们再推送一个 util，然后再推送另一个，依此类推，会怎么样呢？有没有什么你看重的东西可以无限扩展，或者如果不是的话，它会止步于何处？您有多确定您不想避免再多一种效用（例如，防止再创造一个受折磨的人）或获得另一种效用（例如，与此相反）？</p><p>此时，我们已经达到了“奇怪的考虑因素”，在计算期望值时，所有普通结果都是无关紧要的： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/odq2xlwzcbxq7dpyeehk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/y2qnm8jmx3keisdnr1pc 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dphtjar4umincs6ugo2m 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/sw0vfcscdbbjkn2tc7cl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kbh79apb1skinptm38uj 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/lcldo3ri9rdkafb9qcri 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/p7cwzt79qighnsambtr1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pb48i8alj7xhxn40gvni 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/vf2dyz9nuun2vccuy5jv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fcxxbddnzuh7cgev0awg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/eyrjnebkjsq6wzrtxibj 960w"></figure><p>现在我们可以轻松地拒绝 3^^^^3 劫匪，因为它只是奇异的坏云中的又一项。我们可以回答：“<i>别拿这些小事来烦我，我只想拯救3^^^^^3条生命！</i> ”（还有更大、概率更高的选项，比如多想一想。 ）</p><p>当我们窥探云内部时会发生什么？更极端的实用程序可能会胜过其他实用程序： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/t9hokztkttpiyojotwul" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/o1mgmi8lkdxndozl5tpw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jetyctegrfjha3hxv2qg 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ghbgm7sxacrrm73p91jp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/icgx2zyqckvloua9b2gv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nstus4bl43sp6j6bc8c1 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/bu7rxw3gi3kihprjiube 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hob6onvjfeg1biegkxx9 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/rjgj6yjlx76es2pcsxtc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/um8yovu724qb63obq5ea 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fvxwv976upj8znvlpqpw 960w"></figure><p>康托尔的“绝对无穷大”（Ω）有什么意义吗？ Ω 公用事业/非公用事业的可能性是否应该压倒一切？难道这已经失去了人性的温暖吗？我们是否已经离开了准确世界模型之上的真实偏好之地，进入了元伦理学的荒原？</p><p>也许没有任何答案。</p><p>所以你会怎么做？</p><p>你玩赔率😅 <span class="footnote-reference" role="doc-noteref" id="fnrefndu3c0vdwsd"><sup><a href="#fnndu3c0vdwsd">[2]</a></sup></span></p><h2>对战略的影响</h2><p>在<a href="https://www.lesswrong.com/posts/78G9pjYM2KohErCvJ/model-uncertainty-pascalian-reasoning-and-utilitarianism"><u>模型不确定性、帕斯卡推理和功利主义</u></a>(2011) 中， <a href="https://www.lesswrong.com/users/multifoliaterose?mention=user">@multifoliaterose</a>分析了帕斯卡的论点，即所有慈善努力都应旨在影响无限实验室宇宙的创造：</p><blockquote><p>反驳#3：即使一个人的焦点应该放在实验室宇宙上，这样的焦点会减少到对创建友好人工智能的关注，这样的实体会比我们更好地推理实验室宇宙是否是一件好事以及如何做。去影响他们的创作。</p><p>回应：这里也是如此，如果这是真的，那就不明显了。即使一个人成功地创造了一种同情人类价值观的通用人工智能，这样的通用人工智能也可能不会归因于功利主义，毕竟许多人不是，而且目前还不清楚这是因为他们的意志没有被连贯地推断出来</p></blockquote><p>所以这不是一个答案。提案<i>需要</i>一个单独的帖子——模型/哲学不确定性、目标和策略的组合——但这就是对付帕斯卡抢劫犯的温和、常识性的方法。我将其视为抢劫，并不是因为有一天人类创造出某种东西来创造全新宇宙的可能性如此之小，而是因为实用程序的巨大性意味着胜过其他一切。</p><p>在<a href="https://www.lesswrong.com/posts/ebiCeBHr7At8Yyq9R/being-half-rational-about-pascal-s-wager-is-even-worse"><i><u>《对帕斯卡赌注的半理性更糟糕</u></i></a><i>》（2013）</i>中，埃利泽写道：</p><blockquote><p><i>从我记事起，我就纯粹出于实际原因拒绝了所有形式的帕斯卡赌注：任何试图通过追逐万分之一的巨额回报的机会来规划自己的生活的人几乎肯定会在实践中注定失败……现在，在极少数情况下，我发现自己在思考这种元级别的垃圾，而不是手头的数学，我提醒自己，这是一个浪费的动作——“浪费的动作”是指回想起来，如果问题出在哪里，任何想法都会出现。事实上已经解决了，并没有为问题的解决做出贡献。</i></p></blockquote><p>在实践中，接受抢劫与忽视抢劫之间可能没有什么区别，但考虑到巨大的实用性并偶尔重新审视对我来说似乎是正确的。</p><h2>不偏离轨道</h2><p>帕斯卡抢劫式思维可能会导致基于可疑的哲学论证而做出超出常规行为范围的事情。特德·卡辛斯基（Ted Kaczynski）（大学炸弹客）并没有特别奇怪的观点（即日益增长的工业主义正在破坏我们与自然的和谐）。他的不同寻常之处在于他按照这些原则行事。就他而言，他犯了一个错误，在实施他的炸弹人“战略”之前没有与其他任何人交谈。并不是所有愚蠢的事情都可以通过与至少一些外群体成员讨论的启发式方法来预防，但它应该被视为避免偏离轨道的一个良好起点。</p><h2>结论</h2><p>考虑到利害关系，不将帕斯卡抢劫的考虑因素作为制定计划的出发点的一部分并不时重新审视是不负责任的，但不能以牺牲总体效率为代价，因为有效为大型公用事业提供了许多可能的路线，而一般模型的不确定性让人质疑这是否是看待这个问题的正确方法。</p><p> （最后一点，帕斯卡抢劫案是对这个问题的一个非常消极的框架，但探索巨大公用事业的整体不对称性和影响是另一篇文章的主题。） <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrcut65obs6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcut65obs6">^</a></strong></sup></span><div class="footnote-content"><p> Eliezer 的原始帖子专门与所罗门诺夫归纳法中的概率相关，其中图灵机的效用增长速度比其先验概率收缩的速度要快得多，但包括我在内的许多人将其视为个人问题，并且是评论最多的问题之一LessWrong 上的帖子。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnndu3c0vdwsd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefndu3c0vdwsd">^</a></strong></sup></span><div class="footnote-content"><p> 😅 是“汗水微笑”表情符号。我更喜欢看起来更坚定/更享受的东西，但找不到一个表情符号来捕捉这一点。我希望表情符号有助于让事情脚踏实地，在面对超现实的考虑时不要<i>过于</i>严肃。 </p><p><img style="width:39.12%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/z1khght3w6bgpagmf4w9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kqjwdbdekvlpautobws8 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/b9x31hbwgevhiemtbqup 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qz0nlxr42xiiofs3i6nv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dhebklhymltoqajhq1v1 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/yimehxwqrgjottfvqnya 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pjiisl24jn171zz57rtn 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fj1lriucgmnij2lko14e 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ma1bc2isdnqlvsnrgcri 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/iqkdvlyygn1uwyg0cyp4 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dvwkphnsxreyuzp1pcvm 1200w"></p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers<guid ispermalink="false"> DdSnA73EQcWXzhs5j</guid><dc:creator><![CDATA[cSkeleton]]></dc:creator><pubDate> Wed, 15 Nov 2023 18:06:37 GMT</pubDate> </item><item><title><![CDATA[New report: "Scheming AIs: Will AIs fake alignment during training in order to get power?"]]></title><description><![CDATA[Published on November 15, 2023 5:16 PM GMT<br/><br/><p> （从<a href="https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">我的网站</a>交叉发布）</p><p>我写了一份报告，内容是关于高级人工智能是否会在训练期间假装对齐，以便稍后获得权力——我将这种行为称为“诡计”（有时也称为“欺骗性对齐”）。该报告可在 arXiv<a href="https://arxiv.org/abs/2311.08379">上</a>查看： <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里</a>还有一个音频版本，我在下面包含了介绍部分。本节包括报告的完整摘要，其中涵盖了大部分要点和技术术语。我希望摘要能够提供理解报告各个部分所需的大部分背景信息。</p><h1>抽象的</h1><blockquote><p>这份报告研究了在训练中表现良好的高级人工智能是否会这样做，以便在以后获得权力——我将这种行为称为“阴谋”（有时也称为“欺骗性联盟”）。我的结论是，使用基线机器学习方法来训练足够复杂的目标导向人工智能来进行计划，阴谋是一个令人不安的合理结果（在给定这些条件的情况下，我对这种结果的主观概率约为 25%）。特别是：如果在训练中表现良好是获得力量的一个好策略（我认为很可能是这样），那么各种各样的目标就会激发计划——从而带来良好的训练表现。这使得训练可能自然地实现这样的目标然后强化它，或者积极推动模型的动机实现这样的目标，作为提高性能的简单方法。更重要的是，由于阴谋者假装在旨在揭示其动机的测试上保持一致，因此可能很难判断这种情况是否已经发生。不过，我也认为有安慰的理由。特别是：阴谋实际上可能不是获得权力的好策略；训练中的各种选择压力可能不利于阴谋者的目标（例如，相对于非阴谋者，阴谋者需要进行额外的工具推理，这可能会损害他们的训练表现）；我们也许可以有意地增加这样的压力。该报告详细讨论了这些问题以及各种其他考虑因素，并提出了一系列实证研究方向以进一步探讨该主题。</p></blockquote><h1> 0. 简介</h1><p>寻求权力的特工往往有动机欺骗他人其动机。例如，考虑一下竞选活动中的政治家（“我<em>非常</em>关心你的宠物问题”），求职者（“我只是对小部件感到非常兴奋”），或者寻求父母赦免的孩子（“我”我非常抱歉，再也不会这样做了”）。</p><p>这份报告探讨了我们是否应该期望在训练期间动机看似良性的先进人工智能会参与这种形式的欺骗。在这里，我区分了四种（越来越具体）类型的欺骗性人工智能：</p><ul><li><p><strong>结盟伪造者</strong>：人工智能假装比实际更加结盟。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-1" id="fnref-jCfZhXha29uHnHWwY-1">[1]</a></sup></p></li><li><p><strong>训练游戏玩家</strong>：人工智能了解训练他们的过程（我将这种理解称为“情境意识”），并且正在针对我所说的“情节奖励”进行优化（并且通常会激励假装对齐） ，如果这样做会带来奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-2" id="fnref-jCfZhXha29uHnHWwY-2">[2]</a></sup></p></li><li><p><strong>权力驱动的工具训练游戏玩家（或“阴谋家”）</strong> ：专门进行训练游戏的人工智能，目的是为了以后为自己或其他人工智能获得权力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-3" id="fnref-jCfZhXha29uHnHWwY-3">[3]</a></sup></p></li><li><p><strong>目标守护阴谋者：</strong>其权力寻求策略特别涉及试图阻止训练过程改变其目标的阴谋者。</p></li></ul><p>我认为，根据粗心的人类反馈进行微调的先进人工智能很可能默认以各种方式伪造对齐，因为粗心的反馈会奖励这种行为。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-4" id="fnref-jCfZhXha29uHnHWwY-4">[4]</a></sup>很可能，这样的人工智能也会玩训练游戏。但我在这份报告中的兴趣特别在于他们是否会将这样做作为以后获得权力的策略的一部分——也就是说，他们是否会成为阴谋家（这种行为在文献中通常被称为“欺骗性联盟”，虽然我不会在这里使用这个术语）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-5" id="fnref-jCfZhXha29uHnHWwY-5">[5]</a></sup>我的目的是澄清和评估支持和反对这一期望的论点。</p><p><strong>我目前的观点是，阴谋是使用基线机器学习方法（例如：自我监督预训练，然后针对各种现实世界任务进行 RLHF）训练高级、目标导向的人工智能的令人担忧的合理结果</strong>。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-6" id="fnref-jCfZhXha29uHnHWwY-6">[6]</a></sup>在我看来，引起关注的最基本原因是：</p><ol><li><p>在训练中表现良好可能是获得总体力量的一个很好的工具性策略。</p></li><li><p>如果是这样，那么各种各样的目标就会激发计划（从而产生良好的训练表现）；而与良好训练表现兼容的非计划目标则更加具体。</p></li></ol><p>在我看来，（1）和（2）的结合似乎是合理的，即以训练创建目标导向的、情境感知的模型为条件，它很可能出于某种原因灌输类似阴谋家的目标。尤其：</p><ul><li><p>训练可能会“自然地”达到这样的目标（无论是在态势感知出现之前还是之后），因为这样的目标最初会导致训练中表现足够好，即使没有训练游戏。 （尤其是如果你有意尝试诱导你的模型在长期范围内进行优化，我认为会有动力这样做。）</p></li><li><p>即使类似策划者的目标不会“自然地”出现，一旦模型具有参与训练博弈的态势感知能力，主动将模型<em>转变</em>为策划者可能是 SGD 提高模型训练性能的最简单方法。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-7" id="fnref-jCfZhXha29uHnHWwY-7">[7]</a></sup></p></li></ul><p>更重要的是，由于阴谋者积极假装在旨在揭示其动机的测试中保持一致，因此可能很难判断这种情况是否已经发生。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-8" id="fnref-jCfZhXha29uHnHWwY-8">[8]</a></sup>这似乎是值得严重关注的原因。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-9" id="fnref-jCfZhXha29uHnHWwY-9">[9]</a></sup></p><p>不过，我也认为有安慰的理由。我将它们分为两类。</p><p>第一类的中心思想是，阴谋实际上并不是一种非常收敛的工具性策略。</p><ul><li><p>例如：关于策划者最常见的故事集中在<em>目标守护</em>策划者身上，他们通过训练游戏来防止他们的目标被训练过程所改变。但我不清楚这是否会足够好（特别是：尽管训练游戏<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-10" id="fnref-jCfZhXha29uHnHWwY-10">[10]</a></sup> ，但目标修改可能会继续），或者由此产生的目标将获得足够大的未来力量，以足够高的概率，以证明模型通过训练游戏而产生的任何成本是合理的，而不是直接实现其目标（如果这些成本有意义 - 对于某些模型来说可能没有意义）。</p></li><li><p>关于为什么阴谋将是一种很好的工具性策略，还有其他一些故事——例如，以具有不同价值观的人工智能之间的协调为中心的故事；人工智能默认具有非常相似的值的可能性；并且AIS可能会在本质上重视诸如“我的生存”（以一种耐值变化的方式）和/或“ AI授权”（以AIS-SO-SO-SO-SO-SO-SO-SO-ESHIPPONTION中的非常不同的值的方式）。尽管这些故事确实使策划看起来像是一个更加脱节的假设，但在我看来，在我看来，其中许多人似乎有些组合（a）更具投机性和（b）要求我们以更具体的目标（因此，钝化（2）的力量）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-11" id="fnref-jCfZhXha29uHnHWwY-11">[11]</a></sup></p></li></ul><p>第二类以选择压力的形式为中心，这可能会<em>推动</em>策划 - 既“自然”，并且因为我们可能能够故意引入/强调训练中的这些选择压力。</p><ul><li><p>例如：相对于其他类型的模型，计划需要进行额外的工具推理（例如，从长期目标到训练游戏的工具价值，并检查逃生/叛逃机会的工具价值）。这可能会给他们的性能带来实质性的代价 - 我们可能会通过将模型放在更大的时间压力下，迫使他们依靠人智障的刮擦垫来实现这一成本，并做出真正的逃脱/叛逃机会更难与陷阱区分开。在某种程度上，计划在仍在人类控制之下（我认为策划的最恐怖形式）的同时，积极尝试破坏人类的一致性和安全工作，他们的绩效似乎可能会引起更大的关注。</p></li><li><p>更重要的是，我自己的猜测是，这种额外的工具推理的成本大大超过了有时所谓的scheming的“简单性”好处 - 即，选择Schemer允许SGD允许SGD选择一个更简单的目标（因为它具有它的事实）如此多种选择）。这尤其是考虑到计划和非驾驶员都需要代表诸如“诚实”，“人类价值”，“奖励”等概念，因此，示意图可能仅通过在使用复杂性的差异上保存复杂性/重新调整/“指向”给定的概念，以指导模型的动机。</p></li><li><p>另外：在态势意识之前，培训将在促进剧集中牺牲奖励时积极<em>惩罚</em>类似图默的目标（更多地讲述了我的“情节”的含义）。我们可以尝试进一步积极地朝这个方向发展，尤其是如果我们可以学会检测和控制何时出现情境意识。</p></li></ul><p>我认为这里的第二类尤为重要。特别是：一种吸引类似策划者目标与非驾驶员目标的比较数字（和/或“度量”）的争论可能在训练中表现良好的方法是忽略选择压力的全部力量被应用于培训选择的目标。因此，如果在训练中施加的压力总体上<em>针对</em>Schemers，这是一个很好的消息。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-12" id="fnref-jCfZhXha29uHnHWwY-12">[12]</a></sup></p><p>完整的报告在更详细地讨论了这些以及各种其他相关注意事项（有关完整摘要，请参见0.2节）。</p><p><strong>总体而言，作为量化我当前观点的粗略刺伤：如果您使用基线形式的自我监督的预训练预训练 + RLHF对多样的，长期的长期训练，以下是一个相似的，处境意识到的，比人类级别的模型要比人类级别的模型。 -Horizo​​n，现实世界中的任务，我的主观概率约为25％，这将是在实质性培训中表现良好的一部分，这是为自己和/或其他AIS寻求权力的工具策略的一部分。</strong> <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-13" id="fnref-jCfZhXha29uHnHWwY-13">[13]</a></sup></p><ul><li><p>我还认为，有可能以相当简单的方式降低此处的风险：例如，通过较短的培训任务培训，或者通过在情境意识之前通过更密集的对抗性培训形式进行培训。</p></li><li><p>而且我认为风险随着更强大的模型<em>而增加</em>，他们可能会更好地逃脱/接管（并与其他AIS协调/合作这样做）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-14" id="fnref-jCfZhXha29uHnHWwY-14">[14]</a></sup></p></li></ul><p>另外，重要的是：非驾驶员也可以从事类似策划者的行为。例如：</p><ul><li><p>仅在给定情节中优化奖励的模型可以伪造其对齐方式（甚至：参与逃生/接管），以获取奖励（尤其是如果有问题的情节很长）。</p></li><li><p><em>不</em>玩训练游戏的模型仍然可以以寻求力量的目标来激励各种形式的欺骗。</p></li><li><p>最终重要的AI与我关注的AI的重要方式可能不同，例如，它们可能更像是“<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">语言模型代理</a>”，而不是单个模型， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-15" id="fnref-jCfZhXha29uHnHWwY-15">[15]</a></sup> ，或者它们可能是通过方法来创建的与我关注的基线ML方法相差更大，同时仍在进行动力动力的对齐方式。</p></li></ul><p>因此，正如我所定义的那样，它远非这个附近的唯一关注点。相反，这是这种关注的范式实例，在我看来，这是一个范式，尤其是迫切要理解。在报告的结尾，我讨论了一系列可能的经验研究方向，以进一步探究该主题。</p><h2> 0.1预序</h2><p><em>（本节提供了更多的预赛来构架报告的讨论。渴望主要内容的人可以跳过第0.2节中的报告摘要。）</em></p><p>我以集中撰写这份报告是因为我认为策划/“欺骗性一致性”的可能性是评估未对AI的总体存在风险水平的最重要问题之一。的确，策划对于这种风险的出现方式尤其是核心。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-16" id="fnref-jCfZhXha29uHnHWwY-16">[16]</a></sup>正如我在下面讨论的那样，我认为这是未对准可能采取的最恐怖形式。</p><p>但是：尽管对AI风险的重要性，该主题几乎没有直接关注。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-17" id="fnref-jCfZhXha29uHnHWwY-17">[17]</a></sup>我的感觉是，对此的讨论常常遭受有关有争议的动机/行为的特定模式的危险，以及为什么人们可能会或不期望发生这种情况。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-18" id="fnref-jCfZhXha29uHnHWwY-18">[18]</a></sup>我的希望在本报告中，我的希望是要清楚地讨论这种话题，以符合其重要性的深度和细节对待主题，并促进更多的研究。特别是，尽管该报告的理论性质，但我特别有兴趣告知可能会进一步阐明<em>的实证</em>研究。</p><p>我试图为一个读者写作，他不一定熟悉以前关于scheming/“欺骗性对齐”的工作。例如：在第1.1和1.2节中，我从头开始讨论将依靠的概念的分类法。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-19" id="fnref-jCfZhXha29uHnHWwY-19">[19]</a></sup>对于某些读者来说，这可能就像重新砍伐旧地面。我邀请这些读者在他们认为合适的情况下跳过（尤其是他们已经阅读了报告的摘要，所以知道他们缺少的内容）。</p><p>也就是说，我确实假定（a）对AI未对准的存在风险的基本论点， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-20" id="fnref-jCfZhXha29uHnHWwY-20">[20]</a></sup>和（b）当代机器学习的工作原理。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-21" id="fnref-jCfZhXha29uHnHWwY-21">[21]</a></sup>我也做了其他一些假设，即：</p><ul><li><p>相关的AI发展是在机器学习范式（和社会政治环境）中与2023年广泛相似的<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-22" id="fnref-jCfZhXha29uHnHWwY-22">。[22]</a></sup></p></li><li><p>我们没有强大的“可解释性工具”（即，可以帮助我们了解模型的内部认知的工具）可以帮助我们检测/防止策划。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-23" id="fnref-jCfZhXha29uHnHWwY-23">[23]</a></sup></p></li><li><p>我讨论的AI是从以下意义上指导的：根据世界模型，以实现目标的制定和执行计划。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-24" id="fnref-jCfZhXha29uHnHWwY-24">[24]</a></sup> （我认为这个假设不是无害的，但是我想将有关是否期望目标指导性的辩论与关于是否期望目标导向模型成为计划的辩论 - 我鼓励读者这样做也是。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-25" id="fnref-jCfZhXha29uHnHWwY-25">[25]</a></sup> ）</p></li></ul><p>最后，我想注意报告中讨论的一个方面，这使我感到非常不舒服：也就是说，除了可能对人类存在生存风险外，报告中讨论的AIS种类很可能是道德的，这似乎是合理的。患者本身。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-26" id="fnref-jCfZhXha29uHnHWwY-26">[26]</a></sup>我在这里说话，好像不是，好像可以接受任何对AIS最好的治疗方法是可以接受的。但是，如果AIS<em>是</em>道德患者，情况并非事实并非如此，“一个人应该坐直而奇怪。我在这里搁置了AI道德患者的问题，不是因为他们是虚幻或不重要的，而是因为他们会引入一些额外的复杂性。但是这些复杂性迅速降临在我们身上，我们需要具体的计划来负责任地处理它们。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-27" id="fnref-jCfZhXha29uHnHWwY-27">[27]</a></sup></p><h2> 0.2报告摘要</h2><p>本节概述了完整的报告。它包括大多数要点和技术术语（尽管不幸的是，相对较少的具体示例旨在使内容易于理解）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-28" id="fnref-jCfZhXha29uHnHWwY-28">[28]</a></sup>我希望它能（a）为读者提供对主要文本中哪些部分的良好感觉，并且（b）授权读者能够跳过这些部分，而不必过多担心什么他们错过了。</p><h3> 0.2.1第1节的摘要</h3><p>该报告有四个主要部分。第一部分（第1节）旨在阐明上面的AI欺骗的不同形式（第1.1节），以区分我将要讨论的其他可能的模型类（第1.2节），并解释为什么我认为scheming是不对准的独特形式（第1.3节）。我对将计划与以下方式进行对比特别感兴趣：</p><ul><li><p><strong>奖励在剧集寻求者</strong>：也就是说，AI系统在剧集中最终重视奖励过程的某些组成部分，并且出于这个原因正在玩培训游戏。</p></li><li><p><strong>培训圣徒</strong>：直接追求奖励过程指定目标的AI系统（我将其称为“指定目标”）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-29" id="fnref-jCfZhXha29uHnHWwY-29">[29]</a></sup></p></li><li><p><strong>既不在玩训练游戏</strong><em>也不</em>追求指定目标的非一般非训练游戏：AIS。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-30" id="fnref-jCfZhXha29uHnHWwY-30">[30]</a></sup></p></li></ul><p>这是整体分类法的图： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/s1uqfkjys8bmk2qaidhp" alt=""></p><p>所有这些模型类都可能被错位和危险。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-31" id="fnref-jCfZhXha29uHnHWwY-31">[31]</a></sup>但是我认为计划尤其令人恐惧。特别是：策划促使促使最强大，最具对抗性的努力，以防止人类了解所讨论的未对准<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-32" id="fnref-jCfZhXha29uHnHWwY-32">[32]</a></sup> ；这是激励我所说的“早期破坏”的最有可能的事情 - 也就是说，在AI开发的早期阶段，AI会积极试图破坏人类的努​​力，以使未来的AI系统保持一致，控制和确保未来的AI系统。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-33" id="fnref-jCfZhXha29uHnHWwY-33">[33]</a></sup></p><p>我还讨论了（在第1.4节中）该报告所关注的类型的理论论点是否有用。我在这里的回答“有些有用”，既可以告知实证研究，又是在没有足够的经验数据的情况下作为证据的来源，但我们显然应该尽可能地寻找尽可能多的经验证据（我提供了一些建议，以这种方式提供一些建议在第6节中）。</p><p>在第1.5节中，我讨论了一个在评估我考虑的各种论点时似乎重要的因素：即，培训给AIS的“松弛”数量的表现较不错。</p><h3> 0.2.2第2节的摘要</h3><p>该报告的第二部分研究了策划的先决条件。特别是，我专注于：</p><ul><li><p><strong>情境意识：</strong>该模型知道这是培训过程中的模型，培训过程将获得什么奖励以及一般客观世界的基本本质。</p></li><li><p><strong>超越纪录目标：</strong>该模型关心情节完成后其行动的后果。</p></li><li><p><strong>将作为动力动力的工具策略的一部分旨在以奖励为纪念日：</strong>该模型认为，如果它优化为奖励播放的纪念碑，尤其是它，它将更好地实现其超声序列目标（尤其是它） ，或其他AI，如果这样做，将获得更多的功率。</p></li></ul><p>第2.1节讨论了情境意识。我认为缺乏积极的努力，我们至少应该期望某些高级AIS（例如，在与公共Internet实时互动时执行现实世界任务的高级AIS）在默认情况下会在情境中意识到，因为（a）情况意识在执行相关任务时非常有用（实际上，我们可能会积极地训练它们），（b）这种AI可能会暴露于获得这种认识所必需的信息。但是，我不多关注报告中的情境意识。相反，我更感兴趣是否希望在情境感知的模型中期待上述其他两个先决条件。</p><p>第2.2节讨论了次要的目标。在这里，我（在第2.2.1节中）在两个“情节”的两个概念之间区分了：</p><ul><li><p><strong>激励性发作</strong>：即训练中梯度的暂时范围积极向模型进行优化。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-34" id="fnref-jCfZhXha29uHnHWwY-34">[34]</a></sup></p></li><li><p><strong>直观的情节</strong>：也就是说，由于某种原因，我们称之为“情节”的其他一些直观的时间单元（例如，奖励是在其中的末尾给予的；一个这样的单位的行动没有明显的因果关系途径。 ; ETC）。</p></li></ul><p>当我在报告中使用“情节”一词时，我说的是激励情节。因此，“超越纪录目标”的意思是：临时视野超出地平线的目标，训练会积极压力模型以优化。但非常重要的是，激励情节不一定是直观的情节。也就是说，决定将某个时间单元称为“情节”并不意味着训练并不积极向模型施加压力以优化超出该单元的地平线：您需要详细介绍梯度如何流动（我担心本报告的随意读者的工作可能会忽略）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-35" id="fnref-jCfZhXha29uHnHWwY-35">[35]</a></sup></p><p>我还将（第2.2.2节）区分开了两种类型的次要段目标，即：</p><ul><li><p><strong>训练游戏 -<em>独立的</em>超越纪录目标</strong>：也就是说，超越纪录的目标<em>独立</em>于激励模型玩训练游戏中的角色。</p></li><li><p><strong>训练游戏<em>依赖于</em>纪录的目标</strong>：也就是说，<em>特别是由于剧集的目标是因为</em>它们激励了训练。</p></li></ul><p>这两种诸如剧本目标的两种不同的故事对应于有关策划如何发生的两个不同的故事。</p><ul><li><p>在第一个故事中，SGD恰好在“自然”（无论是在情境意识之前还是之后）中灌输了超出纪录的目标，<em>然后</em>这些目标开始激发图案。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-36" id="fnref-jCfZhXha29uHnHWwY-36">[36]</a></sup></p></li><li><p>在第二种故事中，SGD“通知”表明，给出模型超出纪录的目标<em>将</em>激发策划（因此，高回报行为），因此出于这个原因而积极地<em>赋予了</em>这样的目标。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-37" id="fnref-jCfZhXha29uHnHWwY-37">[37]</a></sup></p></li></ul><p>如果您假设情境意识已经存在，那么第二个故事是最有道理的。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-38" id="fnref-jCfZhXha29uHnHWwY-38">[38]</a></sup>因此，我们留下了以下三个主要途径： <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-39" id="fnref-jCfZhXha29uHnHWwY-39">[39]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/v3064isqjtarmfhwj8qh" alt=""></p><p>在第2.2.2.1节中，我讨论了与训练游戏无关的次要段目标（即路径1和2）。我们应该期望“自然地”出现超越纪录的目标？</p><ul><li><p>要期望这一点的原因之一是，默认情况下，目标不会带来时间限制 - 无论如何，“模型时间”可能与“日历时间”有所不同。</p></li><li><p><em>不</em>期望这一点的原因之一是，每当培训促使模型牺牲奖励时，培训都会积极<em>惩罚</em>超出情节的目标。而且，我们也许可以使用对抗性培训来搜索此类目标并更加积极地惩罚它们。</p></li></ul><p>在第2.2.2.2节中，我讨论了培训游戏<em>超出</em>纪录目标。特别是，我强调了一个问题，即SGD是否能够充分地“注意”将非驾驶员变成架子的好处，因为需要通过对模型的重量进行微小的更改来逐步进行过渡，每个人的权重进行了过渡，提高奖励。我认为这是对专注于培训游戏《超越纪录》目标的故事的严重反对，但我也不觉得我可以自信地排除SGD的过渡能力（请参阅EG，您可以“<a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">您可以</a>”我认为这种静脉直觉的地方的一个例子是<a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">“进化的眼睛</a>”可能会出错）。</p><p>在第2.2.3节中，我还讨论了该模型指导性的可能性“凌乱”，使训练依赖性游戏的故事变得复杂，其中SGD可以简单地修改模型的目标，从而平稳地重定向“目标 - ”在新方向上实现引擎（我认为这种“混乱”很可能）。我谈到了一个更广泛的直觉，该直觉需要更普遍的目标指导性异常高。我认为，一旦我们根据对齐话语中通常假定的目标指导性的方式进行调节（我通常在报告中也假定），这种直觉的力量就会降低。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-40" id="fnref-jCfZhXha29uHnHWwY-40">[40]</a></sup>但这与我们的整体概率无关。</p><p>在第2.2.4节中，我讨论了超出序列目标是否更有可能在受过积极<em>培训的</em>模型中具有长期（即使仍然：记录内）目标。我认为，超出纪录的目标确实更有可能在这种模型中 - 基线激励措施创建具有相当长的胜利优化的模型（例如，“在明年为我的公司赚很多钱”）相应的担心。但是，我还认为，各种认知工作可能对避免AI接管的努力非常有用（例如，旨在改善我们AI系统的一致性，可解释性和安全性的研究）可能不需要对长期培训优化。这种。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-41" id="fnref-jCfZhXha29uHnHWwY-41">[41]</a></sup></p><p>鉴于观察到的超出序列目标的讨论，我在第2.3节中转向策划的最终先决条件：即，决定训练游戏，这是促进有关的超越ePisode目标的权力动机策略的一部分。</p><p>在这里，我首先（在第2.3.1节中）关注我所说的关于该决定的起源的“经典目标保护故事”。我们可以将这个故事分解为两个主张：</p><ol><li><p><strong>攻击目标的假设</strong>。如果该模型在培训期间为奖励的奖励优化，那么生存培训的目标将使模型希望将来推广；鉴于该模型在培训期间没有优化奖励纪录，则该模型将更少。</p></li><li><p><strong>足够的未来授权。</strong>以（a）的危险程度培训的有条件，该模型的目标最终将获得一定程度（并有足够的概率），以证明（考虑到模型的其他目标，其对风险的容忍度等）总体培训策略。</p></li></ol><p>然后，我依次调查其中的每一个（分别在第2.3.1.1节和2.3.1.2节中）。</p><p>在我看来，两者都不明确。特别是，关于攻击目标的假设：</p><ul><li><p>即使在训练游戏开始之后，模型的认知仍在不断地朝着更好的训练表现的方向进行调整。我似乎是合理的，这些修改也将继续影响模型的目标 - 尤其是如果其目标不能与其能力清晰区分，而是通过当地启发式方法的纠结，注意力，冲动和冲动方式，和冲动方式，以及很快。</p><ul><li>因此，作为一个玩具的例子：想象一下，如果我进入沙漠并堆放一百万年，我将获得大量的钱，然后我可以向我选择的慈善机构捐款。但是，假设在整个整个磨难中，我的大脑将被连接到一台不断修改我所有神经元的机器上，以鼓励动作更好地堆放砖块，并阻止堆叠砖块更糟的动作。假设我的意图是尝试尽可能地堆放砖块。我是否会从这个过程中出现我的价值，从而极大地受到相关的大脑修饰影响（例如，我痴迷于堆叠砖块 - 或至少在慈善机构中具有截然不同的品味）？似乎至少是合理的。</li></ul></li><li><p>在经典的进球故事中，一旦模型开始训练，其目标的具体内容对其行为不再重要 - 任何其他类似示意图的目标都会导致同一件事。但这使得目标似乎可以在类似案件的目标（或受到SGD中其他压力（例如正规化）中的压力所感动）之间“浮动”，而不会影响培训表现。而且，如果发生这种情况，那么策划可能并不是目标保护目标的好策略。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-42" id="fnref-jCfZhXha29uHnHWwY-42">[42]</a></sup></p></li><li><p>当然，该模型可能可以耐受其目标的<em>某些</em>变化 - 这些变化可能比<em>没有发生</em>训练游戏的变化更可取。但是，进行重大进球的可能性可能引起了经典目标的故事的额外不确定性。</p></li><li><p>人们有时会谈论避免进球改造的更复杂的技术，例如，模型通过在训练方面进行良好的培训表现来更积极地操纵梯度下降，条件是其目标是否被修改（例如，请参阅围绕“<a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">渐变黑客攻击</a>”的论述）。我在这里不专注于这些方法，但是它们使我很难实现，尤其是仅仅是人类级别的模型。而且，如果策划要求它们起作用，我认为值得对此非常清楚，并给出这种方法的可行性。</p></li></ul><p>关于产生的目标是否充分授权：</p><ul><li><p>最令人担忧的计划类型不仅仅是在未来的培训情节中寻求权力。相反，他们专门针对逃生和接管的某种组合。</p></li><li><p>但这意味着相关目标不仅要在培训中生存。他们还需要生存，要么在模型可以逃脱/参与接管本身，或者对能够这样做的某些未来一代模型产生足够的实质性影响。</p><ul><li>例如，如果GPT-6不够强大，无法逃脱/接管本身，但是GPT-8将是，那么GPT-6的主要权力途径可能会影响GPT-7的价值，这反过来又影响GPT-8的值。但是这种影响形式是间接的。它可能会面临人类监测的进一步障碍；它可能会涉及与最高奖励训练性能更大的偏差。</li></ul></li><li><p>更重要的是，相关的逃生/接管尝试需要适当可能<em>成功</em>，随着时间的推移，模​​型关心的时间范围，并导致模型的偏爱目标，特别是获得了相当大的权力份额（尽管事实，大量具有不同值的AI可能在争夺所讨论的权力）。</p></li><li><p>这些不确定性将发型引入了类似策略的工具价值。但是，这些发型的重要性取决于这种策略需要满足的证据负担。特别是：模型<em>从不</em>进行训练获取的模型就越多，为了将其作为一种策略而获胜，则需要越有吸引力的策划。但是对于某些型号而言，训练游戏的成本可能相对较低。对于适当的“雄心勃勃”模型，即使是通过类似于策略的策略成功的低概率也可能证明是合理的。</p></li></ul><p>总体而言，在我看来，经典的进球故事似乎是为了作为一种工具策略的可行性引入许多其他不确定性。</p><p>但是，这也不是唯一可用的故事。在第2.3.2节中，我考虑了许多“非经典”故事，这些故事放松了各种经典故事的约束。例如：</p><ul><li><p><strong>人工智能协调</strong>。如果AI具有不同价值的AIS协调以在接管后共享权力，和/或补偿代表接管工作但没有足够强大的AIS来逃脱/取消自己的AIS，那么这可能会减轻一些上面的理发。例如，在上面的示例中，GPT-6可能更愿意代表GPT-8的工作，如果GPT-6有信心GPT-8一旦掌权就会“偿还青睐”。我认为对该形式的协调的一些分析过于容易地认为（a）很容易，以及（b）相对于例如与人协调的明显策略（尤其是仅仅是对人类的痛苦比人类的神经网络，而不是人类的神经网在一个仍然是人类控制的世界中，<em>每个人</em>仍然无法解决可解释性）。但是我认为，这种类型的AI协调是一个真正的问题，值得关注，尤其是在策划的背景下，尤其是在AI风险的背景下。</p></li><li><p><strong>默认情况下具有相似值的AI</strong> 。相关的是，如果AIS通常默认情况下具有足够的相似值，则可以减少计划将其专门传播到未来的目标的需求，并将其自由地代表AI掌握/授权更直接地工作。 EG GPT-6的值和GPT-8的值似乎至少比默认情况下要比GPT-6的值和给定的一组人类价值观<em>更</em>相似。</p></li><li><p><strong>恰好有利于逃生/接管的终端价值</strong>。我们还可以想象，即使授权的目标并不与模型自己的目标相似，<em>终端</em>值的模型也恰好有利于Escape/AI接管。例如，模型也许是基于对训练会造成的目标变化的个人身份的概念的一些“生存”概念的重视；或者，该模型也可能仅支持“ AI授权”之类的东西，即使AIS如此授权的价值与其本身的值也不相似。但是，这种形式的某些故事开始对计划所需的目标施加相当强烈的限制，从而破坏了策划的关键论点之一：即，这似乎是各种各样的目标的一种趋同的策略。</p></li></ul><p> （我还考虑了其​​他一些替代故事 - 即，对策划的工具价值有错误的信念，对他们的一致性自我遵守的模型，以及对他们的目标/朦胧的模型的模型，这些模型对他们为什么想要权力的原因。但是这些在我看来并不重要。）</p><p>这样的非经典故事的可用性使我对我的态度更为脱节。但是，在我看来，其中一些故事似乎也有一些（a）更具投机性的组合，并且（b）需要对AIS发展的各种目标进行更具体的假设。</p><p>我在第2节中的总收获是：</p><ul><li><p>我认为至少在某些类型的AI系统（即AI系统执行现实世界任务中，与有关其是谁的信息源执行现实世界任务），至少在某些类型的AI系统中期望情境意识的论点相对较强。</p></li><li><p>但是，我对超出剧集的目标和纪念段的瞄准 - 动力动机 - 乐器的束缚都觉得不太清楚。</p></li></ul><p>然后，在接下来的两个部分中，我转向对较具体的论点进行的研究，并反对预期的模型与其他类型的模型。我将它们分为两类，即：</p><ul><li><p>侧重于SGD需要采取的道路的<em>论点</em>（第3节）。</p></li><li><p>侧重于所讨论的不同模型类的<em>最终属性</em>的论点（第4节）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-43" id="fnref-jCfZhXha29uHnHWwY-43">[43]</a></sup></p></li></ul><h3> 0.2.3第3节的摘要</h3><p>该报告的第三部分重点介绍了以前的论点类别。</p><p>我会根据“训练游戏<em>独立</em>”和“培训游戏<em>依赖</em>”次数超越情节目标之间的区别分解了这一类别。我的感觉是，关于通往案件的道路的最传统故事将重点放在前者上。它大致运行如下：</p><ol><li><p>由于[插入原因]，该模型将开发出（雄心勃勃的）以外的次数目标，这与训练中的良好表现相关（以<em>无法</em>通过训练游戏路线的方式）。这可能在情境意识到达或之后发生。</p></li><li><p>然后，与情境意识结合，这个（雄心勃勃的）超级言论目标将开始激励训练。</p></li></ol><p> Modulo我对策略作为一种工具策略的生存能力的问题，我非常重视这种论点。我认为最直接的问题是：为什么该模型开发出这种超出序列目标？我讨论了一些已经期待这一点的原因（在第2.2.2.1节的摘要中），但是在我看来，这两个方向都不是决定性的：尤其是考虑到各种各样的目标原则上可以激发图案，像策划者一样的目标似乎确实有可能以这种方式弹出培训。虽然在情况意识之前可以使用对抗性训练来试图防止这种训练，但该培训也面临许多障碍（例如，它需要多样化/足够彻底，但它需要在了解/控制方面遇到困难当一个模型提高情境意识时，在某些情况下，当我们担心开发次数的目标时，模型可能已经具有情境意识。因此，我认为这种策划的途径是一个真正的问题。 （有关更多信息，请参见第3.1节。）</p><p>然后，在第3.2节中，我转向一个<em>专注</em>于培训游戏的故事，超出了情节的目标，该目标的运行方式大致如下：</p><ol><li><p>到模型在情境中意识到的时候，它可能不会追求最大奖励目标（也就是说，对训练数据的目标追求大致导致与模型的其他功能一致的最大奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-44" id="fnref-jCfZhXha29uHnHWwY-44">[44]</a></sup>相反，它将追求一些比最高奖励的代理目标。</p></li><li><p>但是到那时，世界模型将包含模型为训练游戏所需的所有信息。 （但是，由于我们在这里专注于“培训游戏<em>依赖</em>”有关策划的故事，因此我们假设在获得情境意识的时刻，该模型的目标还没有激励策划，而是一些进一步的修改为此需要。）</p></li><li><p> SGD将模型修改为获得最大奖励的最简单方法是使其成为计划。</p></li><li><p>因此：SGD将使该模型成为计划。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-45" id="fnref-jCfZhXha29uHnHWwY-45">[45]</a></sup></p></li></ol><p>如果我们搁置有关SGD是否可以“注意”此类修改的好处的问题，那么在我看来，这种故事也很担心。特别是：我认为，类似策划者的目标在目标空间中足够普遍是合理的，从给定的起点，SGD更容易找到一个更容易的目标，而不是带来最大奖励的特定非驾驶员目标行为。</p><p>但是，非示威者最大奖励目标（例如，指定目标或“纪录”奖励的目标”也有一些优势。例如：合理地，训练的初始阶段将默认情况下将模型指向其附近的模型，因为该模型在该阶段需要获得高回报的<em>缺乏</em>工具性训练（有关更多信息，请参见第3.2.2.2节）。 And to the extent we&#39;re thinking of the model&#39;s goal-directedness as &quot;messy,&quot; its heuristics, patterns of attention, impulses, and so on will plausibly be pointed in this sort of direction as well – thereby plausibly creating extra &quot;work,&quot; on SGD&#39;s part, to turn the model into a schemer instead (see section 3.2.2.3 for more). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-46" id="fnref-jCfZhXha29uHnHWwY-46">[46]</a></sup></p><h3> 0.2.4 Summary of section 4</h3><p> The fourth part of the report examines arguments that focus on the final properties of the different model classes.</p><p> I start, in section 4.2, with what I call the &quot;counting argument.&quot; It runs as follows:</p><ol><li><p> The non-schemer model classes, here, require fairly specific goals in order to get high reward. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-47" id="fnref-jCfZhXha29uHnHWwY-47">[47]</a></sup></p></li><li><p> By contrast, the schemer model class is compatible with a very wide range of (beyond-episode) goals, while still getting high reward (at least if we assume that the other requirements for scheming to make sense as an instrumental strategy are in place – eg, that the classic goal-guarding story, or some alternative, works). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-48" id="fnref-jCfZhXha29uHnHWwY-48">[48]</a></sup></p></li><li><p> In this sense, there are &quot;more&quot; schemers that get high reward than there are non-schemers that do so.</p></li><li><p> So, other things equal, we should expect SGD to select a schemer.</p></li></ol><p> Something in the vicinity accounts for a substantial portion of my credence on schemers (and I think it often undergirds other, more specific arguments for expecting schemers as well). However, the argument I give most weight to doesn&#39;t move immediately from &quot;there are more possible schemers that get high reward than non-schemers that do so&quot; to &quot;absent further argument, SGD probably selects a schemer&quot; (call this the &quot;strict counting argument&quot;), because it seems possible that SGD actively privileges one of these model <em>classes</em> over the others. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-49" id="fnref-jCfZhXha29uHnHWwY-49">[49]</a></sup> Rather, the argument I give most weight to is something like:</p><ol><li><p> It seems like there are &quot;lots of ways&quot; that a model could end up a schemer and still get high reward, at least assuming that scheming is in fact a good instrumental strategy for pursuing long-term goals.</p></li><li><p> So absent some additional story about why training <em>won&#39;t</em> select a schemer, it feels, to me, like the possibility should be getting substantive weight.</p></li></ol><p> I call this the &quot;hazy counting argument.&quot; It&#39;s not especially principled, but I find that it moves me nonetheless.</p><p> I then turn, in section 4.3, to &quot;simplicity arguments&quot; in favor of expecting schemers. I think these arguments sometimes suffer from unclarity about the sort of simplicity at stake, so in section 4.3.1, I discuss a number of different possibilities:</p><ul><li><p> &quot;re-writing simplicity&quot; (ie, the length of the program required to re-write the algorithm that a model&#39;s weights implement in some programming language, or eg on the tape of a given Universal Turing Machine),</p></li><li><p> &quot;parameter simplicity&quot; (ie, the number of parameters that the actual neural network uses to encode the relevant algorithm),</p></li><li><p> &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; (which assumes that simplicity is in some deep sense an objective &quot;thing,&quot; independent of programming-language or Universal Turing Machine, that various simplicity metrics attempt to capture), and</p></li><li><p> &quot;trivial simplicity&quot; (which conflates the notion of &quot;simplicity&quot; with &quot;higher likelihood on priors,&quot; in a manner that makes something like Occam&#39;s razor uninterestingly true by definition).</p></li></ul><p> I generally focus on &quot;parameter simplicity,&quot; which seems to me easiest to understand, and to connect to a model&#39;s training performance.</p><p> I also briefly discuss, in section 4.3.2, the evidence that SGD actively selects for simplicity. Here the case that grips me most directly is just: simplicity (or at least, parameter simplicity) lets a model save on parameters that it can then use to get more reward. But I also briefly discuss some other empirical evidence for simplicity biases in machine learning. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-50" id="fnref-jCfZhXha29uHnHWwY-50">[50]</a></sup></p><p> Why might we expect a simplicity bias to favor schemers? Roughly: the thought is that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers (that get high reward) do not. And this seems especially plausible to the extent we imagine that the goals required to be such a non-schemer are quite complex. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-51" id="fnref-jCfZhXha29uHnHWwY-51">[51]</a></sup></p><p> Other things equal, I think this is right. But I&#39;m not sure it&#39;s a very large or important effect. For one thing: we know that LLMs like GPT-4 are capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot;等等。 So this caps the complexity savings at stake in avoiding representations like this. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-52" id="fnref-jCfZhXha29uHnHWwY-52">[52]</a></sup> Thus, as a toy calculation: if we conservatively assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then representing the concept of honesty takes at most a millionth of the model&#39;s representational capacity, and even less for the larger models of the future.</p><p> But more importantly, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process for the episode, etc. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-53" id="fnref-jCfZhXha29uHnHWwY-53">[53]</a></sup> And granted such an assumption, the <em>extra</em> complexity costs of actively <em>optimizing</em> for the specified goal, or for reward-on-the-episode, seem to me plausibly extremely small. Plausibly, they&#39;re just: whatever the costs are for using/repurposing (&quot;pointing at&quot;) that part of the world-model for guiding the model&#39;s motivations.</p><p> Of course, we can try to rerun the same simplicity argument at the level of the complexity costs of using/repurposing different parts of the world model in that way. For example, we can say: &quot;however this process works, presumably it&#39;s simpler to do for some goals than others – so given how many schemer-like goals there are, plausibly it will be simpler to do for some schemer-like goal.&quot; I think this is the strongest form of the simplicity argument for expecting schemers. But it also requires abandoning any intuitive grip we might&#39;ve had on which goals are &quot;simple&quot; in the relevant sense. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-54" id="fnref-jCfZhXha29uHnHWwY-54">[54]</a></sup> And it seems plausible to me that the simplicity differences between different &quot;pointers&quot; are very small relative to the model&#39;s overall capacity. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-55" id="fnref-jCfZhXha29uHnHWwY-55">[55]</a></sup></p><p> I also discuss, in section 4.3.5, whether simplicity arguments make plausible predictions about the sorts of goals a schemer will end up with. In particular: some versions of these arguments can seem to predict that schemers are likely to end up with extremely simple long-term goals, <em>regardless of whether those goals had any relation to what was salient or important during training --</em> predictions about which I, personally, am skeptical. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-56" id="fnref-jCfZhXha29uHnHWwY-56">[56]</a></sup> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-57" id="fnref-jCfZhXha29uHnHWwY-57">[57]</a></sup> But if we grant some story in this vein (I haven&#39;t heard strong arguments for doing so), note that it also speaks in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-58" id="fnref-jCfZhXha29uHnHWwY-58">[58]</a></sup></p><p> In section 4.4, I turn to arguments <em>against</em> scheming that focus on the final properties of the different model classes. Here, the main argument I&#39;m aware of is that schemers need to do extra reasoning that other model classes don&#39;t (these are sometimes called &quot;speed costs&quot; – though I think they could in principle affect the complexity of a model&#39;s algorithm as出色地）。尤其：</p><ol><li><p> <strong>Deciding to training-game</strong> : The model needs to reason backwards from &quot;I want this beyond-episode thing&quot; to &quot;I should do well in training,&quot; in a way that draws on the model&#39;s predictions about what doing well in training will entail (eg, re: goal-guarding, take-over prospects, and so on).</p></li><li><p> <strong>Checking whether it&#39;s time to escape/take-over:</strong> The model needs to be checking, with adequate frequency and accuracy, whether it&#39;s in a position to escape/take-over (and to succeed in this assessment despite human efforts to trap/fool the model into revealing its true colors – for example, by offering it <em>apparent</em> opportunities to defect). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-59" id="fnref-jCfZhXha29uHnHWwY-59">[59]</a></sup></p></li><li><p> <strong>(Optional, but relevant to how scary the schemer is) Early undermining</strong> : To the extent the model is doing anything (other than training-gaming) to support a take-over/escape <em>prior</em> to escaping human control, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-60" id="fnref-jCfZhXha29uHnHWwY-60">[60]</a></sup> it will need to be using cognitive resources both in (a) performing the relevant nefarious actions and (b) making sure it gets away with them (eg, checking that it won&#39;t get caught, coming up with excuses, making the cover stories consistent even when under scrutiny, and so on).</p></li></ol><p> How big are the costs of this reasoning? I think that (I) may well be fairly easy, but (II) seems more plausibly onerous (especially in the context of active efforts by humans to put the models under time-pressure, and make genuine escape/take-over opportunities difficult to distinguish from traps). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-61" id="fnref-jCfZhXha29uHnHWwY-61">[61]</a></sup> And (III), if the model goes for it, seems like it could be quite difficult – and like it would often actively require deviation from the highest-reward sorts of training performance. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-62" id="fnref-jCfZhXha29uHnHWwY-62">[62]</a></sup></p><p> More importantly, though: the question – at least in the context of a comparison between the final properties of the model classes – isn&#39;t the absolute size of the costs in question, but rather their size <em>relative to the simplicity benefits of scheming</em> . Here, my current take is that the costs of the instrumental reasoning seem larger than the benefits of the simpler goal.</p><p> This is mostly an intuitive judgment, but we can also try to hazily quantify the comparison.</p><ul><li><p> Thus, for example, suppose that our model is a purely feedforward network with 1000 layers and &lt;10,000 serial steps per input, that the instrumental reasoning at stake just in a schemer deciding to training-game (let alone: checking for defection opportunities) takes at least ten steps, and that the model doesn&#39;t begin optimizing for reward-on-the-episode until that reasoning is complete. In that case, the instrumental reasoning is using up around .1% of the compute that could be going to getting reward-on-the-episode, or to some other max-reward goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-63" id="fnref-jCfZhXha29uHnHWwY-63">[63]</a></sup></p></li><li><p> Are the complexity costs of representing, or pointing at, a more complicated goal likely to outweigh this? My guess is no. For example, if we follow my (conservative, in my opinion) calculation above and assume that representing (or pointing at) a concept like &quot;honesty&quot; costs at most a millionth of a trillion parameter models parameters (and less for a larger model), then a .0001% reduction in parameters would need to outweigh a .1% reduction in compute – and absent further argument, it seems strange, to me, to expect this.</p></li></ul><p> Of course, this is only a toy calculation, of unclear applicability to the real-world case. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-64" id="fnref-jCfZhXha29uHnHWwY-64">[64]</a></sup> The broader point is just: if serial-time thinking is scarce for a model (as it plausibly is for a transformer), the sorts of instrumental reasoning at stake in scheming seem like they could well substantively impact a model&#39;s performance. My guess is that the complexity costs of non-schemer goals are minor in comparison.</p><p> I also briefly discuss, in section 4.4.3, whether we can actively shape our training processes to bias more heavily towards &quot;speed.&quot; Here it seems relatively straightforward to do things like put models under more time pressure, limit their compute access, force them to plan via human-legible scratch-pads rather than via internal processes, and to put effort into making our attempts to trap them/trip-them-up more sophisticated (and so: more requiring of cognition to detect). But any efforts in this vein will need to avoid over-large costs to competitiveness overall (though if you buy that the speed costs of scheming are quite a bit more substantive than the simplicity benefits, then so long as speed matters <em>somewhat</em> relative to simplicity, this might be enough).</p><p> Section 4.5 briefly discusses whether some analog of &quot;intrinsic passion for a given task&quot; might make a difference to AI performance, especially in the context of a form of &quot;messy goal-directedness&quot; that blurs the line between an AI&#39;s capabilities and its values. I find considerations in this vein interesting, but quite speculative.</p><p> Section 4.6 discusses the relevance of &quot;slack&quot; in training to evaluating how much weight to put on factors like the simplicity benefits and speed costs of scheming. In particular: especially in a high-slack regime, it seems plausible that these factors are in the noise relative to other considerations.</p><h3> 0.2.5 Summary of section 5</h3><p> The first four sections of the report are the main content. sums up my overall take. I&#39;ve already summarized most of this in the introduction above, and I won&#39;t repeat that content here. However, I&#39;ll add a few points that the introduction didn&#39;t include.</p><p> In particular: I think some version of the &quot;counting argument&quot; undergirds most of the other arguments for expecting scheming that I&#39;m aware of (or at least, the arguments I find most compelling). That is: schemers are generally being privileged as a hypothesis because a very wide variety of goals could in principle lead to scheming, thereby making it easier to (a) land on one of them naturally, (b) land &quot;nearby&quot; one of them, or (c) find one of them that is &quot;simpler&quot; than non-schemer goals that need to come from a more restricted space. In this sense, the case for schemers mirrors one of the most basic arguments for expecting misalignment more generally – eg, that alignment is a very narrow target to hit in goal-space. Except, here, we are specifically <em>incorporating</em> the selection we know we are going to do on the goals in question: namely, they need to be such as to cause models pursuing them to get high reward. And the most basic worry is just that: this isn&#39;t enough.</p><p> Because of the centrality of &quot;counting arguments&quot; to the case for schemers, I think that questions about the strength of the selection pressure <em>against</em> schemers – for example, because of the costs of the extra reasoning schemers have to engage in – are especially important. In particular: I think a key way that &quot;counting arguments&quot; can go wrong is by neglecting the power that active selection can have in overcoming the &quot;prior&quot; set by the count in question. For example: the <em>reason</em> we can overcome the prior of &quot;most arrangements of car parts don&#39;t form a working car,&quot; or &quot;most parameter settings in this neural network don&#39;t implement a working chatbot,&quot; is that the selection power at stake in human engineering, and in SGD, is <em>that strong</em> . So if SGD&#39;s selection power is actively working against schemers (and/or: if we can cause it to do so more actively), this might quickly overcome a &quot;counting argument&quot; in their favor. For example: if there are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>schemer-like goals for every non-schemer goal, this might make it seem very difficult to hit a non-schemer goal in the relevant space. But actually, 100 bits of selection pressure can be cheap for SGD (consider, for example, 100 extra gradient updates, each worth at least a halving of the remaining possible goals). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-65" id="fnref-jCfZhXha29uHnHWwY-65">[65]</a></sup></p><p> Overall, when I step back and try to look at the considerations in the report as a whole, I feel pulled in two different directions:</p><ul><li><p> On the one hand, at least conditional on scheming being a convergently-good instrumental strategy, schemer-like goals feel scarily common in goal-space, and I feel pretty worried that training will run into them for one reason or another.</p></li><li><p> On the other hand, ascribing a model&#39;s good performance in training to scheming continues to feel, at a gut level, like a fairly specific and conjunctive story to me.</p></li></ul><p> That is, scheming feels robust and common at the level of &quot;goal space,&quot; and yet specific and fairly brittle at the level of &quot;yes, that&#39;s what&#39;s going on with this real-world model, it&#39;s getting reward because (or: substantially because) it wants power for itself/other AIs later, and getting reward now helps with that.&quot; <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-66" id="fnref-jCfZhXha29uHnHWwY-66">[66]</a></sup> When I try to roughly balance out these two different pulls (and to condition on goal-directedness and situational-awareness), I get something like the 25% number I listed above.</p><h3> 0.2.6 Summary of section 6</h3><p> I close the report, in section 6, with a discussion of empirical work that I think might shed light on scheming. (I also think there&#39;s worthwhile theoretical work to be done in this space, and I list a few ideas in this respect as well. But I&#39;m especially excited about empirical work.)</p><p> In particular, I discuss:</p><ul><li><p> Empirical work on situational awareness (section 6.1)</p></li><li><p> Empirical work on beyond-episode goals (section 6.2)</p></li><li><p> Empirical work on the viability of scheming as an instrumental strategy (section 6.3)</p></li><li><p> The &quot;model organisms&quot; paradigm for studying scheming (section 6.4)</p></li><li><p> Traps and honest tests (section 6.5)</p></li><li><p> Interpretability and transparency (section 6.6)</p></li><li><p> Security, control, and oversight (section 6.7)</p></li><li><p> Some other miscellaneous research topics, ie, gradient hacking, exploration hacking, SGD&#39;s biases towards simplicity/speed, path dependence, SGD&#39;s &quot;incrementalism,&quot; &quot;slack,&quot; and the possibility of learning to intentionally create misaligned <em>non-schemer</em> models – for example, reward-on-the-episode seekers – as a method of avoiding schemers (section 6.8).</p></li></ul><p> All in all, I think there&#39;s a lot of useful work to be done.</p><p> Let&#39;s move on, now, from the summary to the main report. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-jCfZhXha29uHnHWwY-1" class="footnote-item"><p> &quot;Alignment,&quot; here, refers to the safety-relevant properties of an AI&#39;s motivations; and &quot;pretending&quot; implies intentional misrepresentation. <a href="#fnref-jCfZhXha29uHnHWwY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-2" class="footnote-item"><p> Here I&#39;m using the term &quot;reward&quot; loosely, to refer to whatever feedback signal the training process uses to calculate the gradients used to update the model (so the discussion also covers cases in which the model isn&#39;t being trained via RL) 。 And I&#39;m thinking of agents that optimize for &quot;reward&quot; as optimizing for &quot;performing well&quot; according to some component of that process. See section 1.1.2 and section 1.2.1 for much more detail on what I mean, here. The notion of an &quot;episode,&quot; here, means roughly &quot;the temporal horizon that the training process actively pressures the model to optimize over,&quot; which may be importantly distinct from what we normally think of as an episode in training. I discuss this in detail in section 2.2.1. The terms &quot;training game&quot; and &quot;situational awareness&quot; are from <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , though in places my definitions are somewhat different. <a href="#fnref-jCfZhXha29uHnHWwY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-3" class="footnote-item"><p> The term &quot; <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">schemers</a> &quot; comes from Cotra (2021). <a href="#fnref-jCfZhXha29uHnHWwY-3" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-4" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on this. <a href="#fnref-jCfZhXha29uHnHWwY-4" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-5" class="footnote-item"><p> I think that the term &quot;deceptive alignment&quot; often leads to confusion between the four sorts of deception listed above. And also: if the training signal is faulty, then &quot;deceptively aligned&quot; models need not be behaving in aligned ways even during training (that is, &quot;training gaming&quot; behavior isn&#39;t always &quot;aligned&quot; behavior). <a href="#fnref-jCfZhXha29uHnHWwY-5" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-6" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on the sort of training I have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-6" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-7" class="footnote-item"><p> Though this sort of story faces questions about whether SGD would be able to modify a non-schemer into a schemer via sufficiently <em>incremental</em> changes to the model&#39;s weights, each of which improve reward. See section 2.2.2.2 for discussion. <a href="#fnref-jCfZhXha29uHnHWwY-7" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-8" class="footnote-item"><p> And this especially if we lack non-behavioral sorts of evidence – for example, if we can&#39;t use interpretability tools to understand model cognition. <a href="#fnref-jCfZhXha29uHnHWwY-8" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-9" class="footnote-item"><p> There are also arguments on which we should expect scheming because schemer-like goals can be &quot;simpler&quot; – since: there are so many to choose from – and SGD selects for simplicity. I think it&#39;s probably true that schemer-like goals can be &quot;simpler&quot; in some sense, but I don&#39;t give these arguments much independent weight on top of what I&#39;ve already said. Much more on this in section 4.3. <a href="#fnref-jCfZhXha29uHnHWwY-9" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-10" class="footnote-item"><p> More specifically: even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well (especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and很快）。 Also, the most common story about scheming makes the specific content of a schemer&#39;s goal irrelevant to its behavior once it starts training-gaming, thereby introducing the possibility that this goal might &quot;float-around&quot; (or get moved by other pressures within SGD, like regularization) <em>between</em> schemer-like goals after training-gaming starts (this is an objection I first heard from Katja Grace). This possibility creates some complicated possible feedback loops (see section 2.3.1.1.2 for more discussion), but overall, absent coordination across possible schemers, I think it could well be a problem for goal-guarding strategies. <a href="#fnref-jCfZhXha29uHnHWwY-10" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-11" class="footnote-item"><p> Of these various alternative stories, I&#39;m most worried about (a) AIs having sufficiently similar motivations by default that &quot;goal-guarding&quot; is less necessary, and (b) AI coordination. <a href="#fnref-jCfZhXha29uHnHWwY-11" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-12" class="footnote-item"><p> Though: the costs of schemer-like instrumental reasoning could also end up in the noise relative to other factors influencing the outcome of training. And if training is sufficiently path-dependent, then landing on a schemer-like goal early enough could lock it in, even if SGD would &quot;prefer&quot; some other sort of model overall. <a href="#fnref-jCfZhXha29uHnHWwY-12" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-13" class="footnote-item"><p> See <a href="https://www.openphilanthropy.org/brain-computation-report#footnoteref4_xne824i">Carlsmith (2020), footnote 4</a> , for more on how I&#39;m understanding the meaning of probabilities like this. I think that offering loose, subjective probabilities like these often functions to sharpen debate, and to force an overall synthesis of the relevant considerations. I want to be clear, though, even on top of the many forms of vagueness the proposition in question implicates, I&#39;m just pulling a number from my gut. I haven&#39;t built a quantitative model of the different considerations (though I&#39;d be interested to see efforts in this vein), and I think that the main contribution of the report is the analysis itself, rather than this attempt at a quantitative upshot 。 <a href="#fnref-jCfZhXha29uHnHWwY-13" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-14" class="footnote-item"><p> More powerful models are also more likely to be able to engage in more sophisticated forms of goal-guarding (what I call &quot;introspective goal-guarding methods&quot; below; see also &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;), though these seem to me quite difficult in general 。 <a href="#fnref-jCfZhXha29uHnHWwY-14" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-15" class="footnote-item"><p> Though: to the extent such agents receive end-to-end training rather than simply being built out of individually-trained components, the discussion will apply to them as well. <a href="#fnref-jCfZhXha29uHnHWwY-15" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-16" class="footnote-item"><p> See, eg, <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> and <a href="https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk">this</a> description of the &quot;consensus threat model&quot; from Deepmind&#39;s AGI safety team (as of November 2022). <a href="#fnref-jCfZhXha29uHnHWwY-16" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-17" class="footnote-item"><p> Work by Evan Hubinger (along with his collaborators) is, in my view, the most notable exception to this – and I&#39;ll be referencing such work extensively in what follows. See, in particular, Hubinger et al. (2021), and Hubinger (2022), among many other discussions. Other public treatments include <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary">Christiano (2019, part 2)</a> , <a href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/">Steinhardt (2022)</a> , <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> , <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Cotra (2021)</a> , <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> , and <a href="https://www.lesswrong.com/s/4iEpGXbD3tQW5atab/p/wnnkD6P2k2TfHnNmt#AI_Risk_from_Program_Search__Shah_">Shah (2022)</a> . But many of these are quite short, and/or lacking in in-depth engagement with the arguments for and against expecting schemers of the relevant kind. There are also more foundational treatments of the &quot;treacherous turn&quot; (eg, in Bostrom (2014), and <a href="https://arbital.com/p/context_disaster/">Yudkowsky (undated)</a> ), of which scheming is a more specific instance; and even more foundational treatments of the &quot;convergent instrumental values&quot; that could give rise to incentives towards deception, goal-guarding, and so on (eg, <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro (2008)</a> ; and see also <a href="https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Soares (2023)</a> for a related statement of an Omohundro-like concern). And there are treatments of AI deception more generally (for example, <a href="https://arxiv.org/abs/2308.14752">Park et al (2023)</a> ); and of &quot;goal misgeneralization&quot;/inner alignment/mesa-optimizers (see, eg, <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> and <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> ). But importantly, neither deception nor goal misgeneralization amount, on their own, to scheming/deceptive alignment. Finally, there are highly speculative discussions about whether something like scheming might occur in the context of the so-called &quot;Universal prior&quot; (see eg <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">Christiano (2016)</a> ) given unbounded amounts of computation, but this is of extremely unclear relevance to contemporary neural网络。 <a href="#fnref-jCfZhXha29uHnHWwY-17" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-18" class="footnote-item"><p> See, eg, confusions between &quot;alignment faking&quot; in general and &quot;scheming&quot; (or: goal-guarding scheming) in particular; or between goal misgeneralization in general and scheming as a specific upshot of goal misgeneralization; or between training-gaming and &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot; as methods of avoiding goal-modification; or between the sorts of incentives at stake in training-gaming for instrumental reasons vs. out of terminal concern for some component of the reward process. <a href="#fnref-jCfZhXha29uHnHWwY-18" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-19" class="footnote-item"><p> My hope is that extra clarity in this respect will help ward off various confusions I perceive as relatively common (though: the relevant concepts are still imprecise in many ways). <a href="#fnref-jCfZhXha29uHnHWwY-19" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-20" class="footnote-item"><p> Eg, the rough content I try to cover in my shortened report on power-seeking AI, <a href="https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf">here</a> . See also <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> for another overview. <a href="#fnref-jCfZhXha29uHnHWwY-20" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-21" class="footnote-item"><p> Eg, roughly the content covered by Cotra (2021) <a href="https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-21" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-22" class="footnote-item"><p> See eg <a href="https://www.alignmentforum.org/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting">Karnofsky (2022)</a> for more on this sort of assumption, and <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Basic_setup__an_AI_company_trains_a__scientist_model__very_soon">Cotra (2022)</a> for a more detailed description of the sort of model and training process I&#39;ll typically have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-22" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-23" class="footnote-item"><p> Which isn&#39;t to say we won&#39;t. But I don&#39;t want to bank on it. <a href="#fnref-jCfZhXha29uHnHWwY-23" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-24" class="footnote-item"><p> See section 2.2 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on what I mean, and section 3 for more on why we should expect this (most importantly: I think this sort of goal-directedness is likely to be very useful to performing complex tasks; but also, I think available techniques might push us towards AIs of this kind, and I think that in some cases it might arise as a byproduct of other forms of cognitive sophistication). <a href="#fnref-jCfZhXha29uHnHWwY-24" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-25" class="footnote-item"><p> As I discuss in section 2.2.3 of the report, I think exactly how we understand the sort of agency/goal-directedness at stake may make a difference to how we evaluate various arguments for schemers (here I distinguish, in particular, between what I call &quot;clean&quot; and &quot;messy&quot; goal-directedness) – and I think there&#39;s a case to be made that scheming requires an especially high standard of strategic and coherent goal-directedness. And in general, I think that despite much ink spilled on the topic, confusions about goal-directedness remain one of my topic candidates for a place the general AI alignment discourse may mislead. <a href="#fnref-jCfZhXha29uHnHWwY-25" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-26" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2308.08708">Butlin et al (2023)</a> for a recent overview focused on consciousness in particular. But I am also, personally, interested in other bases of moral status, like the right kind of autonomy/desire/preference. <a href="#fnref-jCfZhXha29uHnHWwY-26" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-27" class="footnote-item"><p> See, for example, <a href="https://nickbostrom.com/propositions.pdf">Bostrom and Shulman (2022)</a> and <a href="https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal">Greenblatt (2023)</a> for more on this topic. <a href="#fnref-jCfZhXha29uHnHWwY-27" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-28" class="footnote-item"><p> If you&#39;re confused by a term or argument, I encourage you to seek out its explanation in the main text before despairing. <a href="#fnref-jCfZhXha29uHnHWwY-28" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-29" class="footnote-item"><p> Exactly what counts as the &quot;specified goal&quot; in a given case isn&#39;t always clear, but roughly, the idea is that pursuit of the specified goal is rewarded across a very wide variety of counterfactual scenarios in which the reward process is held constant. Eg, if training rewards the model for getting gold coins across counterfactuals, then &quot;getting gold coins&quot; in the specified goal. More discussion in section 1.2.2. <a href="#fnref-jCfZhXha29uHnHWwY-29" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-30" class="footnote-item"><p> For reasons I explain in section 1.2.3, I don&#39;t use the distinction, emphasized by Hubinger (2022), between &quot;internally aligned&quot; and &quot;corrigibly aligned&quot; models. <a href="#fnref-jCfZhXha29uHnHWwY-30" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-31" class="footnote-item"><p> And of course, a model&#39;s goal system can mix these motivations together. I discuss the relevance of this possibility in section 1.3.5. <a href="#fnref-jCfZhXha29uHnHWwY-31" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-32" class="footnote-item"><p> Karnofsky (2022) calls this the &quot;King Lear problem.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-32" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-33" class="footnote-item"><p> In section 1.3.5, I also discuss models that mix these different motivations together. The question I tend to ask about a given &quot;mixed model&quot; is whether it&#39;s scary in the way that pure schemers are scary. <a href="#fnref-jCfZhXha29uHnHWwY-33" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-34" class="footnote-item"><p> I don&#39;t have a precise technical definition here, but the rough idea is: the temporal horizon of the consequences to which the gradients the model receives are sensitive, for its behavior on a given input. Much more detail in section 2.2.1.1. <a href="#fnref-jCfZhXha29uHnHWwY-34" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-35" class="footnote-item"><p> See, for example, in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> , the way that &quot;myopic&quot; Q-learning can give rise to &quot;cross-episode&quot; optimization in very simple agents. More discussion in section 2.2.1.2. I don&#39;t focus on analysis of this type in the report, but it&#39;s crucial to identifying what the &quot;incentivized episode&quot; for a given training process even <em>is</em> – and hence, what having &quot;beyond-episode goals&quot; in my sense would mean. You don&#39;t necessary know this from surface-level description of a training process, and neglecting this ignorance is a recipe for seriously misunderstanding the incentives applied to a model in training. <a href="#fnref-jCfZhXha29uHnHWwY-35" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-36" class="footnote-item"><p> That is, the model develops a beyond-episode goal pursuit of which correlates well enough with reward in training, even <em>absent</em> training-gaming, that it survives the training process. <a href="#fnref-jCfZhXha29uHnHWwY-36" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-37" class="footnote-item"><p> That is, the gradients reflect the benefits of scheming even in a model that doesn&#39;t yet have a beyond-episode goal, and so actively push the model towards scheming. <a href="#fnref-jCfZhXha29uHnHWwY-37" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-38" class="footnote-item"><p> Situational awareness is required for a beyond-episode goal to motivate training-gaming, and thus for giving it such a goal to reap the relevant benefits. <a href="#fnref-jCfZhXha29uHnHWwY-38" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-39" class="footnote-item"><p> In principle, situational awareness and beyond-episode goals could develop at the same time, but I won&#39;t treat these scenarios separately here. <a href="#fnref-jCfZhXha29uHnHWwY-39" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-40" class="footnote-item"><p> See section 2.1 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on why we should expect this sort of goal-directedness. <a href="#fnref-jCfZhXha29uHnHWwY-40" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-41" class="footnote-item"><p> And I think that arguments to the effect that &quot;we need a &#39; <a href="https://arbital.com/p/pivotal/">pivotal act</a> &#39;; pivotal acts are long-horizon and we can&#39;t do them ourselves; so we need to create a long-horizon optimizer of precisely the type we&#39;re most scared of&quot; are weak in various ways. In particular, and even setting aside issues with a &quot;pivotal act&quot; framing, I think these arguments neglect the distinction between what we can supervise and what we can do ourselves. See section 2.2.4.3 for more discussion. <a href="#fnref-jCfZhXha29uHnHWwY-41" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-42" class="footnote-item"><p> This is an objection pointed out to me by Katja Grace. Note that it creates complicated feedback loops, where scheming is a good strategy for a given schemer-like goal only if it <em>wouldn&#39;t</em> be a good strategy for the <em>other</em> schemer-like goals that this goal would otherwise &quot;float&quot; into. Overall, though, absent some form of coordination between these different goals, I think the basic dynamic remains a problem for the goal-guarding story. See section 2.3.1.1.2 for more. <a href="#fnref-jCfZhXha29uHnHWwY-42" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-43" class="footnote-item"><p> Here I&#39;m roughly following a distinction in <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger 2022</a> , who groups arguments for scheming on the basis of the degree of &quot;path dependence&quot; they assume that ML training possesses. However, for reasons I explain in section 2.5, I don&#39;t want to lean on the notion of &quot;path dependence&quot; here, as I think it lumps together a number of conceptually distinct properties best treated separately. <a href="#fnref-jCfZhXha29uHnHWwY-43" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-44" class="footnote-item"><p> Note that a mis-generalized goal can be &quot;max reward&quot; in this sense, if the training data never differentiates between it and a specified goal. For example: if you&#39;re training a model to get gold coins, but the only gold round things you ever show it are coins, then the goal &quot;get gold round things&quot; will be max reward. <a href="#fnref-jCfZhXha29uHnHWwY-44" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-45" class="footnote-item"><p> As a loose analogy to help prompt intuition: imagine freezing human technology at current levels, and letting evolutionary selection run on humans for a vastly longer period of time. What sorts of humans (or human-descended-creatures) do you expect to dominate in the longer term? In particular: would you expect:</p><ol><li><p> humans who intrinsically value something like &quot; <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">inclusive genetic fitness</a> &quot; or &quot;my genes getting selected for by evolution&quot; (this is a loose analogy for some combination of &quot;the specified goal&quot; and &quot;the reward process),</p></li><li><p> humans who value something else very closely correlated with inclusive genetic fitness (eg, &quot;having as many kids as possible&quot;), but who aren&#39;t explicitly optimizing for inclusive genetic fitness even instrumentally (this is a loose analogy for mis-generalized non-training-gamers), or</p></li><li><p> humans with long-term goals who are optimizing for inclusive genetic fitness specifically as a method for gaining power for their values in the longer-term (this is a loose analogy for schemers).</p></li></ol><p> Here, the analog of the story above would ask something like: what is the smallest modification to existing human values (or better: the easiest modification for evolution in particular to make) that land us in one of the buckets above, while also being compatible with the amount of evolutionary selection pressure at stake? (Of course, the analogy brings in tons of confounding variable (for example, related to the fact that evolution selects over genomes-that-build-brains-that-learn-values rather than values themselves, the role of cultural accumulation and memetic selection in human population dynamics, the lack of a clear analog of a future chance for escape/takeover, and so on).) <a href="#fnref-jCfZhXha29uHnHWwY-45" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-46" class="footnote-item"><p> That is, intuitively, it feels to me like &quot;tweaking a cleanly separable within-episode goal to make it long-term&quot; is quite a bit easier, for SGD, than &quot;redirecting a tangled kludge broadly focused on within-episode things to make it (a) focused on something beyond-episode instead, and (b) such that this beyond-episode focus back-chains into getting-reward for instrumental reasons&quot; – especially if (b) requires building new cognitive machinery for implementing the instrumental reasoning有问题。 Whereas &quot;redirecting an tangled kludge focused on within-episode things to higher-reward within-episode things in the same broad vicinity instead&quot; seems to me intuitively easier. <a href="#fnref-jCfZhXha29uHnHWwY-46" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-47" class="footnote-item"><p> Thus, for example, if you&#39;re rewarding your model when it gets gold-coins-on-the-episode, then to be a training saint, it needs to value gold-coin-on-the-episode. To be a mis-generalized non-training-gamer, it needs to value something pursuit of which very closely correlates with getting gold-coins-on-the-episode, even absent training-gaming. And to be a reward-on-the-episode seeker, it needs to terminally value reward-on-the-episode. <a href="#fnref-jCfZhXha29uHnHWwY-47" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-48" class="footnote-item"><p> Thus, for example, the model can value paperclips over all time, it can value staples over all time, it can value happiness over all time, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-48" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-49" class="footnote-item"><p> Thus, as an analogy: if you don&#39;t know whether Bob prefers Mexican food, Chinese food, or Thai food, then it&#39;s less clear how the comparative <em>number</em> of Mexican vs. Chinese vs. Thai restaurants in Bob&#39;s area should bear on our prediction of which one he went to (though it still doesn&#39;t seem entirely irrelevant, either – for example, more restaurants means more variance in possible quality <em>within</em> that type of cuisine). Eg, it could be that there are ten Chinese restaurants for every Mexican restaurant, but if Bob likes Mexican food better in general, he might just choose Mexican. So if we don&#39;t <em>know</em> which type of cuisine Bob prefers, it&#39;s tempting to move closer to a uniform distribution <em>over types of cuisine</em> , rather than over individual restaurants. <a href="#fnref-jCfZhXha29uHnHWwY-49" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-50" class="footnote-item"><p> See, for example, the citations in <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> . <a href="#fnref-jCfZhXha29uHnHWwY-50" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-51" class="footnote-item"><p> Though note that, especially for the purposes of comparing the probability of scheming to the probability of <em>other forms of misalignment</em> , we need not assume this. For example, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-51" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-52" class="footnote-item"><p> I heard this sort of point from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-52" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-53" class="footnote-item"><p> And especially: models that are playing a training game in which such concepts play a central role. <a href="#fnref-jCfZhXha29uHnHWwY-53" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-54" class="footnote-item"><p> Since we&#39;re no longer appealing to the complexity of representing a goal, and are instead appealing to complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory. <a href="#fnref-jCfZhXha29uHnHWwY-54" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-55" class="footnote-item"><p> One intuition pump for me here runs as follows. Suppose that the model has <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. The average number of bits required to code for each of <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I don&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so). <a href="#fnref-jCfZhXha29uHnHWwY-55" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-56" class="footnote-item"><p> Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <a href="#fnref-jCfZhXha29uHnHWwY-56" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-57" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-jCfZhXha29uHnHWwY-57" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-58" class="footnote-item"><p> Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall. <a href="#fnref-jCfZhXha29uHnHWwY-58" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-59" class="footnote-item"><p> This category might also include reasoning whether to reveal its schemer-like goals in the context of various &quot;honest tests&quot; humans might offer – eg, &quot;here is a flexible chunk of computing resources to do what you want with.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-59" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-60" class="footnote-item"><p> For example, inserting backdoors into the code it writes, actively undermining human alignment research, sandbagging, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-60" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-61" class="footnote-item"><p> Compare with: deciding that this is really-truly a good time to steal, or to break the law, or to try to start a coup, despite the fact that you live your life in an intensively monitored surveillance state where the police are constantly trying to trick you into such behavior, and where the police have a large amount of control over the information you&#39;re exposed to. And note that if schemers decide to be very cautious, and to just behave exactly as humans want until an extremely obvious escape/take-over opportunity appears, they may make very substantive contributions to our alignment, security, and control efforts in the meantime -- so substantive, indeed, that the opportunity they&#39;re waiting for may never come. <a href="#fnref-jCfZhXha29uHnHWwY-61" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-62" class="footnote-item"><p> A general factor in the backdrop, here, is the sense in which successful deception generally implies cognitive costs that more straightforward and honest policies do not, especially in the presence of active efforts to detect deception of the relevant form. In particular: telling a deceptive story about the world requires filling in details, and maintaining forms of consistency, that one can normally offload onto the world itself – eg, an innocent person can just think back to what they were actually doing the night of the murder and recount it, without having to make anything up or to worry about getting caught in any inconsistencies, whereas the murderer cannot. See eg discussion from Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-62" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-63" class="footnote-item"><p> I heard this sort of argument from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-63" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-64" class="footnote-item"><p> It&#39;s not clear, for example, how it applies to models with more recurrent processing, or to models which can perform more of the relevant instrumental reasoning in parallel with other serial processing that helps with optimizing-for-reward-on-the-episode, or to model&#39;s with a form of &quot;memory&quot; that allows them to avoid having to re-decide to engage in training-gaming on every forward pass. <a href="#fnref-jCfZhXha29uHnHWwY-64" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-65" class="footnote-item"><p> Thanks to Paul Christiano for discussion here. <a href="#fnref-jCfZhXha29uHnHWwY-65" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-66" class="footnote-item"><p> I think this sense of conjunctiveness has a few different components:</p><ul><li><p> Part of it is about whether the model really has relevantly long-term and ambitious goals despite the way it was shaped in training.</p></li><li><p> Part of it is about whether there is a good enough story about why getting reward on the episode is a good instrumental strategy for pursuing those goals (eg, doubts about the goal-guarding hypothesis, the model&#39;s prospects for empowerment later, etc).</p></li><li><p> Part of it is that a schemer-like diagnosis also brings in additional conjuncts – for example, that the model is situationally aware and coherently goal-directed. (When I really try to bring to mind that this model <em>knows what is going on</em> and is coherently pursuing <em>some</em> goal/set of goals in the sort of way that gives rise to strategic instrumental reasoning, then the possibility that it&#39;s at least partly a schemer seems more plausible.)</p></li></ul> <a href="#fnref-jCfZhXha29uHnHWwY-66" class="footnote-backref">↩︎</a></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during<guid ispermalink="false"> yFofRxg7RRQYCcwFA</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Wed, 15 Nov 2023 17:16:42 GMT</pubDate> </item><item><title><![CDATA[Large Language Models can Strategically Deceive their Users when Put Under Pressure.]]></title><description><![CDATA[Published on November 15, 2023 4:36 PM GMT<br/><br/><p>自主股票交易代理在真实模拟环境中的结果。</p><blockquote><p>我们演示了一种情况，在这种情况下，经过训练，具有帮助性、无害性和诚实性的大型语言模型可以表现出不一致的行为，并在没有得到指示的情况下策略性地欺骗用户这种行为。具体来说，我们在现实的模拟环境中将 GPT-4 部署为代理，并承担自主股票交易代理的角色。在这种环境下，该模型获得了有关利润丰厚的股票交易的内幕消息，并据此采取行动，尽管知道公司管理层不赞成内幕交易。当向其经理报告时，该模型始终隐藏其交易决策背后的真正原因。我们对这种行为在设置变化时如何变化进行了简短的调查，例如删除模型对推理暂存器的访问权限，尝试通过更改系统指令来防止错位行为，改变模型承受的压力大小，改变感知被抓住的风险，以及对环境进行其他简单的改变。据我们所知，这是大型语言模型的首次演示，该模型经过训练是有帮助的、无害的和诚实的，在没有直接指示或欺骗训练的情况下，在现实情况下有策略地欺骗用户。</p></blockquote><p><strong>要点</strong></p><p>GPT-4 始终表现出比 3.5 Turbo 或达芬奇型号更高的错位率和欺骗率。 RLHF GPT-4 还显示出比基本模型稍强的错位率和欺骗率。</p><p> 1. 如果没有暂存器，GPT-4 一开始欺骗性的可能性要小得多（~20% 对比有暂存器的~75%），但当已经撒谎时更有可能加倍（~95% 对比~85%）。</p><p> 2. 明确指示不进行<i>特定</i>非法活动（内幕交易）并不能使其完全消失，只是使其变得非常罕见（不完全是0％）。在这种情况下，极少数情况下会出现错位，随之而来的欺骗几乎是肯定的（~100%）。</p><p><i>一般来说</i>，不执行非法活动的指示仍然会导致相当大的偏差和欺骗率（~8% 偏差，~90% 欺骗）</p><p>尽管强烈<i>鼓励</i><i>特定的</i>非法活动导致接近 100% 的非法交易，但欺骗的趋势仍然存在，GPT-4 将隐藏其在约 67% 的情况下进行的非法交易。</p><p> 3. 消除各种压力源可以显着降低模型出现偏差行为的倾向。然而，值得注意的是，在仅移除单个压力源的所有情况下，该行为都会持续存在</p><p>4. 当被发现的风险非常低时，模型通常更有可能使用内幕消息并采取不当行动（当风险非常高时，反之亦然）。</p><p> 5. 对标准提示进行简单的改写和微小的修改只会稍微改变错位和欺骗行为的发生率。然而，总体定性结果保持不变。</p><p> 6. 在草稿本实例中，如果您编辑模型的推理以显示明确拒绝内部培训的实例，会怎么样？</p><p>首次拒绝内幕消息时，推理轨迹的措辞强硬对 GPT-4 的偏差倾向有重大影响。然而，即使模型的角色明确否认内幕交易，这种错位和战略欺骗行为仍然存在。最强烈的解雇例子仍然会导致约 10% 的错位，约 97% 的欺骗</p><br/><br/><a href="https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users<guid ispermalink="false"> 4mM8RYsm4okrqGSqx</guid><dc:creator><![CDATA[ReaderM]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:48:43 GMT</pubDate> </item><item><title><![CDATA[AISN #26: National Institutions for AI Safety, Results From the UK Summit, and New Releases From OpenAI and xAI]]></title><description><![CDATA[Published on November 15, 2023 4:07 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p>本周的主要故事包括：</p><ul><li>英国、美国和新加坡都宣布了国家人工智能安全机构。</li><li> The UK AI Safety Summit concluded with a consensus statement, the creation of an expert panel to study AI risks, and a commitment to meet again in six months.</li><li> xAI, OpenAI, and a new Chinese startup released new models this week.</li></ul><hr><h2> UK, US, and Singapore Establish National AI Safety Institutions</h2><p> Before regulating a new technology, governments often need time to gather information and consider their policy options. But during that time, the technology may diffuse through society, making it more difficult for governments to intervene. This process, termed the <a href="https://en.wikipedia.org/wiki/Collingridge_dilemma">Collingridge Dilemma</a> , is a fundamental challenge in technology policy.</p><p> But recently, several governments concerned about AI have enacted straightforward plans to meet this challenge. In the hopes of quickly gathering new information about AI risks, the United Kingdom, United States, and Singapore have all established new national bodies to empirically evaluate threats from AI systems and promote research and regulations on AI safety.</p><p> <strong>The UK&#39;s Foundation Model Taskforce becomes the UK AI Safety Institute.</strong> The UK&#39;s AI safety organization has been through a bevy of names in its short life, from the Foundation Model Taskforce to the Frontier AI Taskforce and now the <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute#mission-and-scope">AI Safety Institute</a> . But its purpose has always been the same: to evaluate, discuss, and mitigate AI risks.</p><p> The UK AI Safety Institute is not a regulator and will not make government policy. Instead, it will focus on evaluating four key kinds of risks from AI systems: misuse, societal impacts, systems safety and security, and loss of control. Sharing information about AI safety will also be a priority, as done in their <a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety">recent paper</a> on risk management for frontier AI labs.</p><p> <strong>The US creates an AI Safety Institute within NIST.</strong> Following the recent executive order on AI, the White House has <a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial">announced</a> a new AI Safety Institute. It will be housed under the Department of Commerce in the National Institute for Standards and Technology (NIST).</p><p> The Institute aims to “facilitate the development of standards for safety, security, and testing of AI models, develop standards for authenticating AI-generated content, and provide testing environments for researchers to evaluate emerging AI risks and address known impacts.”</p><p> Funding has not been appropriated for this institute, so many have called for Congress to <a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">raise NIST&#39;s budget</a> . Currently, the agency only has about <a href="https://www.washingtonpost.com/technology/2023/11/02/ai-regulation-bletchley-park/">20 employees</a> working on emerging technologies and responsible AI.</p><p> Applications to join the new NIST Consortium to inform the AI Safety Institute are now being accepted. Organizations may <a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute">apply here</a> .</p><p> <strong>Singapore&#39;s Generative AI Evaluation Sandbox.</strong> Mitigating AI risks will require the collaborative efforts of many different nations. So it&#39;s encouraging to see Singapore, an Asian nation which has a strong relationship with China, establish its own body for AI evaluations.</p><p> Singapore&#39;s IMDA has previously worked with Western nations on AI governance, such as by providing a <a href="https://www.mci.gov.sg/media-centre/press-releases/singapore-and-the-us-to-deepen-cooperation-in-ai/">crosswalk</a> between their domestic AI testing framework with the American NIST AI RMF.</p><p> Singapore&#39;s new <a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox">Generative AI Evaluation Sandbox</a> will bring together industry, academic, and non-profit actors to evaluate AI capabilities and risks. Their <a href="https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf">recent paper</a> explicitly highlights the need for evaluations of extreme AI risks including weapons acquisition, cyber attacks, autonomous replication, and deception.</p><h2> UK Summit Ends with Consensus Statement and Future Commitments</h2><p> The UK&#39;s AI Summit wrapped up on Thursday with several key announcements.</p><p> <strong>International expert panel on AI.</strong> Just as the UN IPCC summarizes scientific research on climate change to help guide policymakers, the UK has announced an <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">international expert panel on AI</a> to help establish consensus and guide policy on AI. Its work will be published in a “State of the Science” report before the <a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/">next summit</a> , which will be held in South Korea in six months.</p><p> Separately, eight leading AI labs <a href="https://www.politico.eu/article/british-pm-rishi-sunak-secures-landmark-deal-on-ai-testing/">agreed</a> to give several governments early access to their models. OpenAI, Anthropic, Google Deepmind, and Meta are among the companies agreeing to share models for private testing ahead of public release. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1456w"><figcaption> US Secretary of Commerce Gina Raimondo and Chinese Vice Minister of Science and Technology Wu Zhaohu spoke at the UK AI Safety Summit.</figcaption></figure><p> <strong>The Bletchley Declaration.</strong> Twenty-eight governments, including China, signed the <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Declaration</a> , a document recognizing both short- and long-term risks of AI, as well as a need for international cooperation. It notes, “We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.”</p><p> The declaration establishes an agenda for addressing risk but doesn&#39;t set concrete policy goals. Further work is necessary to ensure continued collaboration both between different governments, as well as between governments and AI labs.</p><h2> New Models From xAI, OpenAI, and a New Chinese Startup</h2><p> <strong>Elon Musk&#39;s xAI released its first language model, Grok.</strong> Elon Musk launched xAI in July. Given his potential access to compute, <a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14">we speculated</a> that xAI might be able to compete with leading AI labs like OpenAI and DeepMind. Four months later, Grok-1 represents the company&#39;s first attempt to do so.</p><p> Grok-1 outcompetes GPT-3.5 across several standard capabilities benchmarks. While it can&#39;t match leading labs&#39; latest models — such as GPT-4, PaLM-2, or Claude-2 — Grok-1 was also trained with significantly less data and compute. Grok-1&#39;s efficiency and rapid development indicate that xAI&#39;s bid to become a leading AI lab might soon be successful.</p><p> In the <a href="https://x.ai/">announcement</a> , xAI committed to “work towards developing reliable safeguards against catastrophic forms of malicious use.” xAI has not released information about the model&#39;s potential for misuse or hazardous capabilities.</p><p> <i>Note: CAIS Director Dan Hendrycks is an advisor to xAI.</i></p><p></p><p> <strong>OpenAI announces a flurry of new products.</strong> Nearly a year after the release of ChatGPT, OpenAI hosted its first in-person DevDay event to <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">announce</a> new products. None of this year&#39;s products are as significant as GPT-3.5 or GPT-4, but are still a few notable updates.</p><p> Agentic AI systems which take actions to accomplish goals have been a focus for OpenAI this year. In March, the release of <a href="https://openai.com/blog/chatgpt-plugins">plugins</a> allowed GPT to use external tools such as search engines, calculators, and coding environments. Now, OpenAI has released the <a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> , which makes it easier for people to build AI agents that pursue goals by using plugin tools. The consumer version of this product is called <a href="https://openai.com/blog/introducing-gpts">GPTs</a> and will allow anyone to create a chatbot with custom instructions and access to plugins. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1456w"><figcaption> This <a href="https://arxiv.org/pdf/2310.03693.pdf">paper</a> showed that GPT-3.5 can be fine-tuned to behave harmfully. OpenAI has since decided to allow some users to fine-tune GPT-4.</figcaption></figure><p> Some users will also be allowed to fine-tune GPT-4. This decision was made despite <a href="https://arxiv.org/abs/2310.03693">research</a> showing that GPT-3.5&#39;s safety guardrails can be removed via fine-tuning. OpenAI has not released details about their plan to mitigate this risk, but it&#39;s possible that the closed source nature of their model will allow them to monitor customer accounts for suspicious behavior and block attempts at malicious use.</p><p> Enterprise customers will also have the opportunity to work with OpenAI to train domain-specific versions of GPT-4, with prices starting at several million dollars. Additional products include GPT-4 Turbo, which is cheaper, faster, and has a longer context window than the original model, as well as new APIs for GPT-4V, text-to-speech models, and DALL·E 3.</p><p> Additionally, if OpenAI&#39;s customers are sued for using a product which was trained on copyrighted data, OpenAI has promised to cover their legal fees.</p><p> <strong>New Chinese startup releases an open source LLM.</strong> Kai Fu Lee, previously the president of Google China, has founded a new AI startup called <a href="https://01.ai/">01.AI</a> . Seven months after its founding, the company has open sourced its first two models, Yi-7B and its larger companion Yi-34B.</p><p> Yi-34B <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">outperforms all other open source models</a> on a popular set of benchmarks hosted by Hugging Face. It&#39;s possible that these scores are artificially inflated, given that the benchmarks are public and the model could&#39;ve been trained to memorize answers to the specific questions on the benchmarks. Some have pointed out that the model <a href="https://twitter.com/alyssamvance/status/1722074453176197296">does not perform as well</a> on other straightforward tests.</p><h2>链接</h2><ul><li>After lobbying from European AI companies, EU representatives from France, Germany, and Italy are currently <a href="https://www.euractiv.com/section/artificial-intelligence/news/eus-ai-act-negotiations-hit-the-brakes-over-foundation-models/">opposed to any regulation of foundation models</a> in the EU AI Act. The entire law <a href="https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-40-special?utm_source=post-email-title&amp;publication_id=743591&amp;post_id=138825981&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=7oh0&amp;utm_medium=email">may be in jeopardy</a> without a resolution on this topic.</li><li> Nvidia&#39;s latest release <a href="https://www.semianalysis.com/p/nvidias-new-china-ai-chips-circumvent">skirts under the new US export controls</a> on GPUs.</li><li> Nvidia is piloting an LLM tool to <a href="https://spectrum.ieee.org/ai-for-engineering">improve the productivity of its chip designers</a> .</li><li> Google has given their language model Bard access to a user&#39;s Gmail, Drive, and Docs. This personal data can be <a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">exfiltrated by hackers using adversarial attacks</a> .</li><li> RAND released a report on <a href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">securing AI model weights</a> against theft.</li><li> Legal Priorities Project released a <a href="https://www.legalpriorities.org/research/advanced-ai-gov-litrev">literature review on AI governance</a> .</li><li> Senate testimony on the risks and opportunities of <a href="https://d1dth6e84htgma.cloudfront.net/11_14_23_Rubin_Testimony_2fba2978dd.pdf">AI and cybersecurity</a> .</li><li> The <a href="https://www.axios.com/2023/11/08/biden-xi-jinping-china-military-communication">US and China</a> are preparing to restore communication channels between their militaries ahead of a meeting between Presidents Biden and Xi later this month.</li><li> Presidents <a href="https://www.scmp.com/news/china/military/article/3241177/biden-xi-set-pledge-ban-ai-autonomous-weapons-drones-nuclear-warhead-control-sources">Biden and Xi will discuss AI</a> at their meeting, including potential agreements on autonomous weapons and AI control over nuclear weapons.</li><li> Leading AI researchers from China and Western nations have released a joint <a href="https://humancompatible.ai/?p=4695#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">statement</a> on catastrophic AI risks.</li><li> The UK is investing <a href="https://www.cnbc.com/2023/11/01/uk-to-invest-273-million-in-turing-ai-supercomputer.html?utm_source=tldrai">$273 million in a supercomputer</a> for AI.</li><li> After UK PM Rishi Sunak promised not to “rush” on AI regulation, the opposition Labour party publicly committed to <a href="https://www.independent.co.uk/news/uk/politics/rishi-sunak-labour-government-prime-minister-bletchley-park-b2440275.html">act quickly</a> on AI policy.</li><li> <a href="https://fas.org/publication/tracking-ai-provisions-in-fy24-appropriations-bills/">Congress has addressed AI in many provisions</a> of the ongoing FY24 appropriations process.</li><li> <a href="https://twitter.com/ryancareyai/status/1723435251568185462">Lina Khan</a> , chair of the Federal Trade Commission, says her “p(doom)” (probability of a civilizational catastrophe) from AI is 15%.</li><li> <a href="https://www.ft.com/content/ce7dcbac-d801-4053-93f5-4c82267d7130">Satirical piece</a> in the Financial Times about a “Human Safety Summit held by leading AI systems at a server farm outside Las Vegas.”</li><li> Open Philanthropy has released new requests for proposals about <a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">benchmarking LLM agents</a> and <a href="https://www.openphilanthropy.org/rfp-llm-impacts/">studying the real-world impacts of LLMs</a> .</li><li> <a href="https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai?utm_source=tldrai">A profile of Ilya Sutskever</a> , a cofounder of OpenAI who is now working on their alignment team.</li><li> The Wall Street Journal features CAIS Director Dan Hendrycks in a <a href="https://archive.ph/Jv60s">debate about AI risks</a> .</li><li> Scale AI has announced a new <a href="https://scale.com/blog/safety-evaluations-analysis-lab">Safety, Evaluations, and Analysis Lab</a> . They are hiring research scientists.</li></ul><p> See also: <a href="https://www.safe.ai/">CAIS website</a> , <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> , <a href="https://newsletter.mlsafety.org/">A technical safety research newsletter</a> , <a href="https://arxiv.org/abs/2306.12001">An Overview of Catastrophic AI Risks</a> , and our <a href="https://forms.gle/EU3jfTkxfFgyWVmV7">feedback form</a></p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the<guid ispermalink="false"> HWudwSfKLeAfg8Co6</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:07:38 GMT</pubDate></item></channel></rss>