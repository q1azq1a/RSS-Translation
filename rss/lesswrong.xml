<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 28 日星期六 18:13:42 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AI Safety Hub Serbia Official Opening]]></title><description><![CDATA[Published on October 28, 2023 5:03 PM GMT<br/><br/><p> TLDR：我们很高兴地宣布，我们现在欢迎全职租户来到我们新改造的办公空间，为人工智能安全研究人员寻找一个鼓舞人心的工作空间。您可以看一下下面的照片，先睹为快。我们优先向俄罗斯和中国等国家的公民发出热烈邀请，他们可以在塞尔维亚享受免签证工作特权，同时保持与欧洲的毗邻。我们每月的办公室租金为 150 欧元，非常实惠，大约是贝尔格莱德标准成本的一半。对于那些需要经济支持的人，我们提供补贴。<a href="https://docs.google.com/forms/d/1LQ9cE1CGjD_WMMx5IYLLeHFeXNQJx12dsu7f4_FSF7w/edit"><u>在此注册兴趣</u></a>或寻求任何引起您好奇心的问题的答案 -<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>我们的收件箱已打开</u></a>，我们热切等待您的询问。展望未来，我们渴望为这些研究人员提供住房。如果您是与我们有共同愿景的潜在捐助者，<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>请与我们联系</u></a>。我们可以共同增强人工智能安全研究的影响力。您的支持可能会成为非凡旅程的催化剂。</p><p><strong>如果符合以下条件，您可能想来：</strong></p><ul><li>您是一名人工智能安全研究员/EA 研究员，正在寻找短期、中长期的运营基地</li><li>您渴望留在欧洲，但不想留在欧盟</li><li>您正在寻找一个充满活力但价格实惠的城市，这里有很多事情可以做，并且有东欧但西化的文化。 </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rhope1kzgbygtaznxrff" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/av2n0y1ddmpkjlaao3hq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/imzg363u9lykimplfwfr 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mbbliew6d1zaodrmuf8r 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kslavbyntsgwoz34idah 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uj6i2hktt4xsewysx0oj 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/edtwmohcbpwjikn4ekfi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/p8bdejrsrvov0xsijgne 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ayswko71jqaiyz9qt7rl 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ciytrddcpl8bqptiznd4 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wqvwonadsbsa5vbgelc9 6000w"></figure><p><strong>背景：</strong></p><p> EA 塞尔维亚和 AI 安全塞尔维亚团体规模虽小，但正在不断增长（EA 塞尔维亚超过 30 人，约 3 人希望从事 AIS 研究作为职业，约 3 人希望进入 AIS 政策）。由于塞尔维亚对俄罗斯和中国的签证政策优惠，许多外国人已经居住在这里。与许多其他国际中心城市相比，贝尔格莱德的生活成本较低，风景充满活力，并且时区和气候适宜，因此贝尔格莱德的外国人社区不断壮大。</p><p>正如我们认为<a href="https://ceealar.org/"><u>CEEALAR</u></a>等项目重要且令人印象深刻，我们希望在塞尔维亚复制这些项目，在那里它们可以更好地为那些可能难以获得英国签证的人们提供服务。我们还相信，有能力为来自不同国家的人们快速扩展廉价住房是一件好事。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uxq4oabxyjpdmmhjtgo0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/e9grniodk6f8ejythv3f 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/f5k39rystyizea0vbmkq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ndoxhjm6d7k8ursg738n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvjqolxfdwbn76ocngve 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/riiq7rpasykyrj61dzvb 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iez7ohyjt7f9zfleqbqo 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mwdxvwyobojph6l6kaps 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/lobyg54ph0nbiwknzkzo 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x4c9uvqjczywudqqdxb6 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/cfkm1rzfo445cqzpwier 4000w"></figure><p>我们还认为，我们应该从小规模开始，进行原型设计，然后再扩大规模。我们有一个非政府组织友好的办公空间，氛围良好，每月费用仅为约 550 欧元，办公空间有 3 个房间，可容纳 8-15 人（取决于他们决定有多舒适），楼下有一家咖啡厅，另外 20 个人可以一起工作，因为办公室和楼下的咖啡厅属于同一所有权人。这当然不如许多其他 EA/AI 联合办公场所那么豪华（下面有更多照片，深度工作室不在照片上），但我们拥有高度的可定制性，我们可以用它来制作更好的办公空间。如果我们成长得足够多，随着我们的需求增长，我们也可以搬到更大的场地。当然，如果我们知道贝尔格莱德的办公室可以有更多用途，那么在距离市中心更远的地方（包括居住和办公空间）会更好，但在我们有证据之前，我们不想探索这一点。概念和需求。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/nod9jw42tc06wvzye3eb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/i3aqrftpdiglasru5zoo 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/fadrulbzdm2g8m9za7wy 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/aab7vljnrhy6k2f4tkcy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kqgfhl8xnjy59mf5jt9g 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wbvwmvbsvamsjniuebfz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xac4qmxukisbahsptujh 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sh2v5qozhildz1y72auz 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/t4x3chswtv5cjyzg3cky 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/pu5p7jpbijtdvcjovi6b 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wxqj8fp2qkn5pfoitbwo 4000w"></figure><p><strong>操作细节（又名其工作原理）：</strong></p><p>该办公室目前租期至2023年12月底，因此我们可以保持优惠价格，而不用另谋出路。办公空间有一些桌子和椅子，但我们希望获得全额资金，并让人们在购买更多家具之前表达他们的需求。办公室通常在咖啡店的工作时间（上午 10 点至午夜，周末下午 4 点至午夜除外）开放，因为它们共用一个入口，但在特殊情况下，如果有人在奇怪的工作方面表现更好，我们可以满足特殊要求小时。</p><p>办公空间优先提供给那些从事与人工智能安全相关的项目的人，但只要我们有闲置能力（目前就是这种情况），也欢迎 EA/理性研究。</p><p>对于许多人来说不需要塞尔维亚签证，对于大多数其他人来说相对容易获得。如果您需要签证来塞尔维亚，请联系我们，我们将了解如何提供帮助。</p><p>我们有一位可靠的房地产经纪人，可以为那些需要住房援助的人在贝尔格莱德提供优惠的住房，直到我们获得资金并租用共同居住空间。</p><p>对于那些希望通过我们持续饮食的人，我们可以安排将价格实惠的熟食送到办公室或您的住所（费用由您承担） - 素食或非素食。如果我们有足够的兴趣，我们还可以让厨师准备纯素食物。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvfxrlxrysu9ftzrfb0h" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rt5rayrriqgexvauxtwe 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/asuqgyybp6w9qnzxc0ao 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hpjcffwxmal5cdylxdyv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ioapwimka9271dtpduzm 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/gdbwhdusggnvqdexuuqz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zsuzpixvadboaqkh2x2h 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xvrdcxmhfiy0bpnspyft 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/d61j52v27pzwown3r3ne 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/v2bzvtf6czgkjn7cw56j 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wa96ftjsgtifyygfdxlo 4000w"></figure><p><strong>您愿意贡献吗？</strong></p><p> <a href="https://www.linkedin.com/in/nesicdusan/"><u>Dušan D. Nešić</u></a>目前正在与更多运营人员一起管理该项目。我们目前的瓶颈是资金——我们已经获得了一家有兴趣支付我们一半预算的盈利捐赠者，但正在寻找第二个资助者。拥有资金意味着我们可以在这个领域停留更长时间，并为需要的研究人员提供更多津贴。</p><p>随着我们的成长，我们希望吸引更多好奇的志愿者来参与日常项目运行 - 如果<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>您感兴趣，</u></a>请告诉我们。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/g5foy0cn8wy3huhlxoq5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x0iifew90y7pq2x7t5yk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/me0hy8dxy30ptbq6zzrf 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sc5wk7e8kzottq60xcgp 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hhqdull2ezu4ekoqg8lj 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iwtsrgkzckr5qwjlxh6i 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rklbxpbhn4nggpqnk0nr 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ev6nhqdsaelwzcwdc2zj 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/bt2gs4wszkqb7wfoaish 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/j2tiksjpthhru0mxgrvi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/otdmoewnmv4prwdd9fkx 4000w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening<guid ispermalink="false"> jcCyii8NcZLZ2M223</guid><dc:creator><![CDATA[DusanDNesic]]></dc:creator><pubDate> Sat, 28 Oct 2023 17:03:34 GMT</pubDate> </item><item><title><![CDATA[
Managing AI Risks in an Era of Rapid Progress
]]></title><description><![CDATA[Published on October 28, 2023 3:48 PM GMT<br/><br/><h1>在快速进步的时代管理人工智能风险</h1><h2>作者</h2><p>约书亚·本吉奥</p><p>杰弗里·辛顿</p><p>姚安德</p><p>黎明之歌</p><p>彼得·阿贝尔</p><p>尤瓦尔·诺亚·赫拉利</p><p>张亚勤</p><p>蓝雪</p><p>谢·沙莱夫-施瓦茨</p><p>吉莉安·哈德菲尔德</p><p>杰夫·克鲁恩</p><p>泰甘·马哈拉杰</p><p>弗兰克·哈特</p><p>阿蒂利姆·古内斯·巴丁</p><p>希拉·麦克莱思</p><p>高琪琪</p><p>阿什温·阿查里亚</p><p>大卫·克鲁格</p><p>安卡·德拉甘</p><p>菲利普·托尔</p><p>斯图尔特·拉塞尔</p><p>丹尼尔·卡尼曼</p><p>简·布劳纳</p><p>索伦·明德曼</p><h3>arXiv</h3><p>即将推出。</p><p><a href="https://managing-ai-risks.com/managing_ai_risks.pdf">纸质 PDF 副本</a><a href="https://managing-ai-risks.com/policy_supplement.pdf">保单补充</a></p><blockquote><p><strong>摘要：</strong>在这篇简短的共识论文中，我们概述了即将到来的先进人工智能系统的风险。我们研究大规模的社会危害和恶意使用，以及人类对自主人工智能系统不可逆转的控制丧失。鉴于人工智能的快速和持续进步，我们提出了人工智能研发和治理的紧迫优先事项。</p></blockquote><p> 2019 年，GPT-2 无法可靠地数到十。仅四年后，深度学习系统就可以编写软件，根据需要生成逼真的场景，就智力主题提供建议，并结合语言和图像处理来引导机器人。当人工智能开发人员扩展这些系统时，不可预见的能力和行为会自发出现，无需显式编程。人工智能的进步非常迅速，而且对许多人来说是令人惊讶的。</p><p>进展的速度可能会再次让我们感到惊讶。当前的深度学习系统仍然缺乏重要的能力，我们不知道开发它们需要多长时间。然而，各公司都在竞相创建在大多数认知工作中匹配或超过人类能力的通用人工智能系统。他们正在快速部署更多资源并开发新技术来增强人工智能能力。人工智能的进步也带来了更快的进步：人工智能助手越来越多地用于自动化编程[4]和数据收集[5,6]，以进一步改进人工智能系统[7]。</p><p>人工智能的进步并没有在人类水平上放缓或停滞的根本原因。事实上，人工智能已经在蛋白质折叠或策略游戏等狭窄领域超越了人类的能力[8-10]。与人类相比，人工智能系统可以更快地行动，吸收更多的知识，并以更高的带宽进行通信。此外，它们可以扩展以使用巨大的计算资源，并且可以进行数百万次复制。</p><p>改进的速度已经是惊人的，科技公司拥有所需的现金储备，可以很快将最新的培训规模扩大到 100 到 1000 倍 [11]。结合人工智能研发的持续增长和自动化，我们必须认真对待通用人工智能系统在这十年或未来十年内在许多关键领域超越人类能力的可能性。</p><p>然后会发生什么？如果管理得当并公平分配，先进的人工智能系统可以帮助人类治愈疾病、提高生活水平并保护我们的生态系统。人工智能提供的机会是巨大的。但随着先进的人工智能功能的出现，我们还无法很好地应对大规模的风险。人类正在投入大量资源来使人工智能系统变得更强大，但在安全性和减轻危害方面却投入较少。为了让人工智能成为福音，我们必须重新定位；仅仅推动人工智能能力是不够的。</p><p>我们的调整已经落后于计划。我们必须预见到持续危害和新风险的扩大，并在最大风险<em>发生之前</em>做好准备。人们花了几十年的时间才认识和应对气候变化；对于人工智能来说，几十年可能太长了。</p><h2>社会规模风险</h2><p>人工智能系统可能会在越来越多的任务中迅速超越人类。如果此类系统没有经过精心设计和部署，它们会带来一系列社会规模的风险。它们有可能加剧社会不公正，侵蚀社会稳定，并削弱我们对社会基础现实的共同理解。它们还可能促成大规模犯罪或恐怖活动。特别是在少数强大的参与者手中，人工智能可能会巩固或加剧全球不平等，或促进自动化战争、定制的大规模操纵和普遍的监视[12,13]。</p><p>随着公司正在开发<em>自主人工智能</em>：可以在世界上规划、行动和追求目标的系统，其中许多风险可能很快就会被放大，并产生新的风险。虽然当前人工智能系统的自主权有限，但改变这一现状的工作正在进行中[14]。例如，非自主 GPT-4 模型很快就可以浏览网页 [15]、设计和执行化学实验 [16] 以及利用软件工具 [17]，包括其他人工智能模型 [18]。</p><p>如果我们构建高度先进的自主人工智能，我们就有可能创建追求不良目标的系统。恶意行为者可能故意嵌入有害目标。此外，目前没有人知道如何可靠地将人工智能行为与复杂的价值观结合起来。即使是善意的开发人员也可能会无意中构建出追求意想不到目标的人工智能系统——特别是如果他们为了赢得人工智能竞赛而忽视了昂贵的安全测试和人工监督。</p><p>一旦自主人工智能系统追求恶意行为者或意外嵌入的不良目标，我们可能无法控制它们。软件控制是一个古老且尚未解决的问题：计算机蠕虫长期以来一直能够扩散并逃避检测[19]。然而，人工智能正在黑客、社交操纵、欺骗和战略规划等关键领域取得进展[14,20]。先进的自主人工智能系统将带来前所未有的控制挑战。</p><p>为了实现不良目标，未来的自主人工智能系统可能会使用不良策略（向人类学习或独立开发）作为达到目的的手段[21-24]。人工智能系统可以赢得人类信任，获取财政资源，影响关键决策者，并与人类参与者和其他人工智能系统形成联盟。为了避免人为干预[24]，他们可以像计算机蠕虫一样在全球服务器网络上复制算法。人工智能助手已经在全球范围内共同编写了大量计算机代码 [25]；未来的人工智能系统可以插入并利用安全漏洞来控制我们的通信、媒体、银行、供应链、军队和政府背后的计算机系统。在公开冲突中，人工智能系统可能会使用自主武器或生物武器进行威胁或使用。获得此类技术的人工智能只会延续现有的自动化军事活动、生物研究和人工智能开发本身的趋势。如果人工智能系统以足够的技能执行此类策略，人类将很难干预。</p><p>最后，如果人工智能系统可以自由地移交影响力，那么它可能不需要策划影响力。随着自主人工智能系统变得比人类工人更快、更具成本效益，出现了一个困境。公司、政府和军队可能被迫广泛部署人工智能系统，并减少对人工智能决策的昂贵的人工验证，否则就有被竞争的风险[26,27]。因此，自主人工智能系统可以越来越多地承担关键的社会角色。</p><p>如果没有足够的谨慎，我们可能会不可逆转地失去对自主人工智能系统的控制，从而导致人类干预无效。大规模网络犯罪、社会操纵和其他突出危害可能会迅速升级。这种不受控制的人工智能进步可能最终导致大规模生命和生物圈的丧失，以及人类的边缘化甚至灭绝。</p><p>错误信息和算法歧视等危害如今已经很明显[28]；其他危害也有出现的迹象[20]。解决持续危害和预测新出现的风险至关重要。这<em>不是</em>一个非此即彼的问题。当前和新出现的风险通常具有相似的机制、模式和解决方案[29]；对治理框架和人工智能安全的投资将在多个方面取得成果[30]。</p><h2>前进的道路</h2><p>如果今天开发出先进的自主人工智能系统，我们将不知道如何确保它们的安全，也不知道如何正确测试它们的安全性。即使我们这样做了，政府也将缺乏防止滥用和维护安全做法的机构。然而，这并不意味着没有可行的前进道路。为了确保取得积极成果，我们可以而且必须在人工智能安全和伦理方面寻求突破，并及时建立有效的政府监管。</p><h3>调整技术研发方向</h3><p>我们需要研究突破来解决当今创建具有安全和道德目标的人工智能的一些技术挑战。其中一些挑战不太可能通过简单地提高人工智能系统的能力来解决[22,31–35]。这些包括：</p><ul><li>监督和诚实：能力更强的人工智能系统能够更好地利用监督和测试中的弱点[32,36,37]——例如，通过产生虚假但令人信服的输出[35,38]。</li><li>鲁棒性：人工智能系统在新情况下（在分布转移或对抗性输入下）表现不可预测[39-41]。</li><li>可解释性：人工智能决策是不透明的。到目前为止，我们只能通过反复试验来测试大型模型。我们需要学会理解它们的内部运作方式[42]。</li><li>风险评估：前沿人工智能系统开发出不可预见的能力，这些能力只有在训练期间甚至部署后才发现[43]。需要更好的评估来及早发现危险能力[44,45]。</li><li>应对新出现的挑战：能力更强的未来人工智能系统可能会表现出我们迄今为止仅在理论模型中看到的故障模式。例如，人工智能系统可能会学习假装服从或利用我们安全目标和关闭机制中的弱点来推进特定目标[24,41]。</li></ul><p>考虑到风险，我们呼吁主要科技公司和公共资助者将至少三分之一的人工智能研发预算用于确保安全和合乎道德的使用，这与他们对人工智能能力的资助相当。着眼于强大的未来系统来解决这些问题 [34] 必须成为我们领域的核心。</p><h3>紧急治理措施</h3><p>我们迫切需要国家机构和国际治理来执行标准，以防止鲁莽和滥用。从制药到金融系统和核能的许多技术领域都表明，社会需要并有效地利用治理来降低风险。然而，目前人工智能还没有类似的治理框架。如果没有它们，公司和国家可能会通过将人工智能能力推向新的高度，同时在安全方面偷工减料，或者将关键的社会角色委托给几乎没有人类监督的人工智能系统来寻求竞争优势[26]。就像制造商将废物排入河流以降低成本一样，他们可能会试图获得人工智能发展的回报，同时让社会来应对后果。</p><p>为了跟上快速进展并避免僵化的法律，国家机构需要强大的技术专长和迅速采取行动的权力。为了解决国际种族动态问题，他们需要有能力促进国际协议和伙伴关系[46,47]。为了保护低风险的使用和学术研究，他们应该避免对小型和可预测的人工智能模型设置不当的官僚障碍。最紧迫的审查应该是前沿的人工智能系统：少数最强大的人工智能系统——在价值数十亿美元的超级计算机上进行训练——将具有最危险和不可预测的能力[48,49]。</p><p>为了实现有效监管，政府迫切需要全面了解人工智能的发展。监管机构应要求模型注册、举报人保护、事件报告以及模型开发和超级计算机使用的监控[48,50–55]。监管机构还需要在部署之前访问先进的人工智能系统，以评估其危险功能，例如自主自我复制、闯入计算机系统或使大流行病病原体广泛传播[44,56,57]。</p><p>对于具有危险能力的人工智能系统，我们需要与其风险程度相匹配的治理机制[48,52,58]组合。监管机构应根据模型功能制定国家和国际安全标准。他们还应该让前沿人工智能开发者和所有者对其模型造成的可合理预见和预防的损害承担法律责任。这些措施可以防止伤害并创造急需的安全投资动力。对于能力超群的未来人工智能系统，例如可以规避人类控制的模型，需要采取进一步的措施。政府必须准备好许可其开发，暂停开发以应对令人担忧的能力，强制执行访问控制，并要求对国家级黑客采取强有力的信息安全措施，直到准备好足够的保护措施。</p><p>为了缩短法规出台的时间，主要人工智能公司应立即做出“如果-那么”承诺：如果在其人工智能系统中发现特定的红线功能，他们将采取具体的安全措施。这些承诺应详细并经过独立审查。</p><p>人工智能可能是塑造本世纪的技术。虽然人工智能能力正在迅速进步，但安全和治理方面的进展却滞后。为了引导人工智能走向积极的结果并远离灾难，我们需要重新定位。如果我们有智慧走下去，就有一条负责任的道路。</p><h2>引文</h2><p>请将本作品引用为</p><p>请引用我们即将发布的 arXiv 预印本。</p><p> @article{bengio2023managing，title={在快速进步的时代管理人工智能风险}，作者={Bengio、Yoshua 和 Hinton、Geoffrey 和 Yao、Andrew 和 Song、Dawn 和 Abbeel、Pieter 和 Harari、Yuval Noah 和 Zhang、Ya -Qin 和薛、Lan 和 Shalev-Shwartz、Shai 和 Hadfield、Gillian 和 Clune、Jeff 和 Maharaj、Tegan 和 Hutter、Frank 和 Baydin、Atılım Güneş 和 McIlraith、Sheila 和 Gau、Qiqi 和 Acharya、Ashwin 和 Krueger、David 和Dragan、Anca 和 Torr、Philip 和 Russell、Stuart 和 Kahnemann、Daniel 和 Brauner、Jan 和 Mindermann、Sören}，journal={arXiv 预印本 arXiv:NUMBER_FORTHCOMING}，year={2023} }</p><h2>参考</h2><ol><li>大型语言模型的新兴能力<a href="https://openreview.net/pdf?id=yzkSU5zdwD">[链接]</a><br> Wei, J.、Tay, Y.、Bommasani, R.、Raffel, C.、Zoph, B.、Borgeaud, S. 等，2022 年。机器学习研究汇刊。</li><li>关于<a href="https://www.deepmind.com/about">[链接]</a><br> DeepMind，2023。</li><li>关于<a href="https://openai.com/about">[链接]</a><br>开放人工智能，2023。</li><li> ML 增强的代码完成提高了开发人员的工作效率<a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">[HTML]</a><br> Tabachnyk, M.，2022。谷歌研究。</li><li> GPT-4 技术报告<a href="http://arxiv.org/pdf/2303.08774.pdf">[PDF]</a><br> OpenAI，，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>宪法人工智能：人工智能反馈的无害性<a href="http://arxiv.org/pdf/2212.08073.pdf">[PDF]</a><br> Bai, Y.、Kadavath, S.、Kundu, S.、Askel, A.、Kernion, J.、Jones, A. 等，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>人工智能改进人工智能的例子<a href="https://ai-improving-ai.safe.ai/">[链接]</a><br> Woodside, T. 和安全，CfA，2023。</li><li>使用 AlphaFold 进行高精度蛋白质结构预测<br>Jumper, J.、Evans, R.、Pritzel, A.、Green, T.、Figurnov, M.、Ronneberger, O. 等，2021 年。《自然》，第 583--589 页。</li><li>多人扑克的超人人工智能<br>Brown, N. 和 Sandholm, T.，2019 年。《科学》，第 885--890 页。</li><li>深蓝<br>Campbell, M.、Hoane, A. 和 Hsu, F.，2002 年。人工智能，第 57--83 页。</li><li> Alphabet 年度报告，第 33 页<a href="https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf">[PDF]</a><br>字母表，2022 年。</li><li>灾难性人工智能风险概述<a href="http://arxiv.org/pdf/2306.12001.pdf">[PDF]</a><br> Hendrycks, D.、Mazeika, M. 和 Woodside, T.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>语言模型带来的风险分类<br>Weidinger, L.、Uesato, J.、Rauh, M.、Griffin, C.、Huang, P.、Mellor, J. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第. 214--229。</li><li>基于大型语言模型的自治代理综述<a href="http://arxiv.org/pdf/2308.11432.pdf">[PDF]</a><br> Wang, L.、Ma, C.、Feng, X.、Zhang, Z.、Yang, H.、Zhang, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> ChatGPT 插件<a href="https://openai.com/blog/chatgpt-plugins">[链接]</a><br>开放人工智能，2023。</li><li> ChemCrow：使用化学工具增强大型语言模型<a href="http://arxiv.org/pdf/2304.05376.pdf">[PDF]</a><br> Bran, A.、Cox, S.、White, A. 和 Schwaller, P.，2023。arXiv [physicals.chem-ph]。</li><li>增强语言模型：调查<a href="http://arxiv.org/pdf/2302.07842.pdf">[PDF]</a><br> Mialon, G.、Dessì, R.、Lomeli, M.、Nalmpantis, C.、Pasunuru, R.、Raileanu, R. 等，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li> HuggingGPT：使用 ChatGPT 及其 Hugging Face 中的朋友解决 AI 任务<a href="http://arxiv.org/pdf/2303.17580.pdf">[PDF]</a><br> Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y. 等，2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>计算科学：互联网蠕虫<br>Denning, P.，1989。《美国科学家》，第 126--128 页。</li><li>人工智能欺骗：示例、风险和潜在解决方案调查<a href="http://arxiv.org/pdf/2308.14752.pdf">[PDF]</a><br> Park, P.、Goldstein, S.、O&#39;Gara, A.、Chen, M. 和 Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>最优政策往往寻求权力<a href="http://arxiv.org/pdf/1912.01683.pdf">[PDF]</a><br> Turner, A.、Smith, L.、Shah, R. 和 Critch, A.，2019 年。第三十五届神经信息处理系统会议。</li><li>通过模型编写的评估发现语言模型行为<a href="http://arxiv.org/pdf/2212.09251.pdf">[PDF]</a><br> Perez, E.、Ringer, S.、Lukošiūtė, K.、Nguyen, K.、Chen, E. 和 Heiner, S.，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>回报是否证明手段合理？在马基雅维利基准中衡量奖励与道德行为之间的权衡<br>Pan, A.、Chan, J.、Zou, A.、Li, N.、Basart, S. 和 Woodside, T.，2023 年。国际机器学习会议。</li><li>关闭开关游戏<br>Hadfield-Menell, D.、Dragan, A.、Abbeel, P. 和 Russell, S.，2017 年。第二十六届国际人工智能联合会议论文集，第 220--227 页。</li><li> GitHub Copilot <a href="https://github.blog/2023-02-14-github-copilot-for-business-is-now-available/">[链接]</a><br>多姆克，T.，2023。</li><li>自然选择有利于人工智能而不是人类<a href="http://arxiv.org/pdf/2303.16200.pdf">[PDF]</a><br> Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>日益代理的算法系统的危害<br>Chan, A.、Salganik, R.、Markelius, A.、Pang, C.、Rajkumar, N. 和 Krasheninnikov, D.，2023 年。2023 年 ACM 公平、问责和透明度会议记录，第 651 页- -666。计算机器协会。</li><li>论基金会模型的机遇和风险<a href="http://arxiv.org/pdf/2108.07258.pdf">[PDF]</a><br> Bommasani, R.、Hudson, D.、Adeli, E.、Altman, R.、Arora, S. 和 von Arx, S.，2021。arXiv [cs.LG]。</li><li>人工智能带来世界末日风险——但这并不意味着我们不应该谈论当前的危害<a href="https://time.com/6303127/ai-future-danger-present-harms/">[链接]</a><br> Brauner, J. 和 Chan, A.，2023 年。时间。</li><li>针对当前和未来危害的现有政策提案<a href="https://assets-global.website-files.com/63fe96aeda6bea77ac7d3000/647d5368c2368cc32b359f88/_Policy/%20Agreement/%20Statement.pdf">[PDF]</a><br>安全，CfA，2023 年。</li><li>逆缩放：越大并不越好<a href="http://arxiv.org/pdf/2306.09479.pdf">[PDF]</a><br> McKenzie, I.、Lyzhov, A.、Pieler, M.、Parrish, A.、Mueller, A. 和 Prabhu, A.，2023 年。机器学习研究汇刊。</li><li>奖励错误指定的影响：映射和缓解不一致的模型<a href="https://openreview.net/forum?id=JYtwGwIL7ye">[链接]</a><br> Pan, A.、Bhatia, K. 和 Steinhardt, J.，2022。学习表征国际会议。</li><li>简单的综合数据减少了大型语言模型中的阿谀奉承<a href="http://arxiv.org/pdf/2308.03958.pdf">[PDF]</a><br> Wei, J.、Huang, D.、Lu, Y.、Zhou, D. 和 Le, Q., 2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>机器学习安全中未解决的问题<a href="http://arxiv.org/pdf/2109.13916.pdf">[PDF]</a><br> Hendrycks, D.、Carlini, N.、Schulman, J. 和 Steinhardt, J.，2021。arXiv [cs.LG]。</li><li>来自人类反馈的强化学习的开放问题和基本限制<a href="http://arxiv.org/pdf/2307.15217.pdf">[PDF]</a><br> Casper, S.、Davies, X.、Shi, C.、Gilbert, T.、Scheurer, J. 和 Rando, J.，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>人工智能失调的后果<br>Zhuang, S. 和 Hadfield-Menell, D.，2020 年。神经信息处理系统进展，第 33 卷，第 15763--15773 页。</li><li>奖励模型过度优化的缩放法则<br>高 L.、舒尔曼 J. 和希尔顿 J.，2023 年。第 40 届国际机器学习会议论文集，第 10835--10866 页。 PMLR。</li><li>从人类偏好中学习<a href="https://openai.com/research/learning-from-human-preferences">[链接]</a><br>阿莫代伊，D.，克里斯蒂安诺，P. 和雷，A.，2017。</li><li>深度强化学习中的目标错误概括<a href="https://openreview.net/forum?id=q--OykSR2FY">[链接]</a><br> Langosco di Langosco, A. 和 Chan, A.，2022 年。学习表征国际会议。</li><li>目标误区：为什么正确的规范不足以实现正确的目标<a href="http://arxiv.org/pdf/2210.01790.pdf">[PDF]</a><br> Shah, R.、Varma, V.、Kumar, R.、Phuong, M.、Krakovna, V.、Uesato, J. 等，2022。arXiv [cs.LG]。</li><li>迈向透明人工智能：解释深度神经网络内部结构的调查<br>Räuker, T.、Ho, A.、Casper, S. 和 Hadfield-Menell, D.，2023。2023 年 IEEE 安全可信机器学习会议 (SaTML)，第 464--483 页。</li><li>思路链提示引发大型语言模型的推理<br>Wei, J.、Wang, X.、Schuurmans, D.、Bosma, M.、Ichter, B.、Xia, F. 等，2022。神经信息处理系统进展，第 35 卷，第 24824 页-- 24837。</li><li>极端风险模型评估<a href="http://arxiv.org/pdf/2305.15324.pdf">[PDF]</a><br> Shevlane, T.、Farquhar, S.、Garfinkel, B.、Phuong, M.、Whittlestone, J.、Leung, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> AGI 公司的风险评估：对其他安全关键行业流行的风险评估技术的回顾<a href="http://arxiv.org/pdf/2307.08823.pdf">[PDF]</a><br> Koessler, L. 和 Schuet, J.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>深度学习角度的对齐问题<a href="http://arxiv.org/pdf/2209.00626.pdf">[PDF]</a><br> Ngo, R.、Chan, L. 和 Mindermann, S.，2022。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>国际先进人工智能机构<br>Ho, L.、Barnhart, J.、Trager, R.、Bengio, Y.、Brundage, M.、Carnegie, A. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。 <a href="https://doi.org/10.48550/arXiv.2307.04699">DOI：10.48550/arXiv.2307.04699</a></li><li>民用人工智能的国际治理：司法管辖区认证方法<a href="https://cdn.governance.ai/International_Governance_of_Civilian_AI_OMS.pdf">[PDF]</a><br> Trager, R.、Harack, B.、Reuel, A.、Carnegie, A.、Heim, L.、Ho, L. 等，2023 年。</li><li>前沿人工智能监管：管理公共安全的新风险<a href="http://arxiv.org/pdf/2307.03718.pdf">[PDF]</a><br> Anderljung, M.、Barnhart, J.、Korinek, A.、Leung, J.、O&#39;Keefe, C.、Whittlestone, J. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>大型生成模型的可预测性和惊喜<br>Ganguli, D.、Hernandez, D.、Lovitt, L.、Askel, A.、Bai, Y.、Chen, A. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第1747年--1764年。计算机器协会。</li><li>是时候为大型人工智能模型创建国家注册库了<a href="https://carnegieendowment.org/2023/07/12/it-s-time-to-create-national-registry-for-large-ai-models-pub-90180">[链接]</a><br> Hadfield, G.、Cuéllar, M. 和 O&#39;Reilly, T.，2023。卡内基国际捐赠基金会。</li><li>用于模型报告的模型卡<br>Mitchell, M.、Wu, S.、Zaldivar, A.、Barnes, P.、Vasserman, L.、Hutchinson, B. 等，2019 年。FAT* &#39;19：公平、问责制和其他问题会议记录透明度，第 220--229 页。</li><li>通用人工智能带来严重风险，不应被排除在欧盟人工智能法案之外政策简介<a href="https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act">[链接]</a><br> 2023.AI Now 研究所。</li><li>人工智能事件数据库<a href="https://incidentdatabase.ai/">[链接]</a><br>数据库，AII，2023。</li><li>技术举报的承诺和危险<a href="https://papers.ssrn.com/abstract=4377064">[链接]</a><br> Bloch-Wehba, H.，2023。西北大学法律评论，即将出版。</li><li>为英国提出一个基础模型信息共享制度<a href="https://www.governance.ai/post/proposing-a-foundation-model-information-sharing-regime-for-the-uk">[链接]</a><br> Mulani, N. 和 Whittlestone, J.，2023。人工智能治理中心。</li><li>审核大型语言模型：三层方法<br>Mökander, J.、Schuett, J.、Kirk, H. 和 Floridi, L.，2023 年。人工智能与伦理。 <a href="https://doi.org/10.1007/s43681-023-00289-2">DOI：10.1007/s43681-023-00289-2</a></li><li>大型语言模型能否使两用生物技术的普及变得民主化？ <a href="http://arxiv.org/pdf/2306.03809.pdf">[PDF]</a><br> Soice, E.、Rocha, R.、Cordova, K.、Spectre, M. 和 Esvelt, K.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>迈向通用人工智能安全和治理的最佳实践：专家意见调查<a href="http://arxiv.org/pdf/2305.07153.pdf">[PDF]</a><br> Schuett, J.、Dreksler, N.、Anderljung, M.、Mcaffary, D.、Heim, L.、Bluemke, E. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>监管市场：人工智能治理的未来<a href="http://arxiv.org/pdf/2304.04914.pdf">[PDF]</a><br> Hadfield, G. 和 Clark, J.，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress<guid ispermalink="false"> a3RjXa2dryoH6Xgij</guid><dc:creator><![CDATA[Algon]]></dc:creator><pubDate> Sat, 28 Oct 2023 15:48:25 GMT</pubDate> </item><item><title><![CDATA[ELI5 Why isn't alignment *easier* as models get stronger?]]></title><description><![CDATA[Published on October 28, 2023 2:34 PM GMT<br/><br/><p>经验状态：稻草人，只是想知道对此最强烈的反驳是什么。</p><p>对我来说，<i>显然</i><strong>更强的</strong>模型<strong>更容易</strong>对齐。一个简单的证明</p><ol><li>总是有可能从较强的模型中得到较弱的模型（例如，通过破坏其 n% 的输入/输出）</li><li>因此，如果可以对齐弱模型，那么对齐强模型<i>至少</i>也同样容易</li><li>调整弱/强模型不太可能<i>那么</i>困难。</li><li>因此<i>更容易</i>调整更强的模型</li></ol><p>（我写下了反驳清单，我有兴趣看看是否有人提出比我清单上的反驳更好的反驳）</p><br/><br/> <a href="https://www.lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger<guid ispermalink="false"> eJ7pm7LahehddYxNw</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Sat, 28 Oct 2023 14:34:38 GMT</pubDate> </item><item><title><![CDATA[Truthseeking, EA, Simulacra levels, and other stuff]]></title><description><![CDATA[Published on October 27, 2023 11:56 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 19:35:26 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 19:35:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>我一直在热衷于谈论有效利他主义中的求真。我从倡导素食开始，因为它是最清晰的，但现在需要转向更深层次的问题。不幸的是，这些问题仍然不那么清晰，我最终不得不证明我以前认为是基本前提的很多东西是合理的，而且一切都陷入了困境。我将在脑海中列出一些线索，希望你会对其中的一些感到好奇，我们可以从那里理清思路。</p><p>一些线程：</p><ul><li> “个人何时应该在对象层面上操作与社会现实层面上操作？”之间存在微妙的区别。以及“一群人什么时候应该共同投资以促进社会现实中对象层面的运作？”我对后者更感兴趣，尽管它显然与前者有交叉。</li><li>到目前为止，我的帖子都假设寻求真相是好的，但没有特别证明它的合理性。似乎如果我对提出不方便的问题感到如此兴奋，我应该愿意将其本身展开。</li><li>我认为这个问题的更好版本是“什么时候寻求真相是最重要的事情，也是最重要的投资事情，什么时候可以停止？”因为显然寻求真相是有用且重要的，所以有趣的问题是，这如何与团体可以投资的能力（例如筹款或运营能力）进行权衡。</li><li>我在这里的部分论点是“你知道如何解决人工智能对齐吗？因为这对我来说听起来像是一个知识问题，而知识问题是在对象级现实而不是社会现实中解决的”。但也有可能完全在对象级现实中进行操作，但只是表现不佳。 “寻求真相”这个词可能以令人困惑的方式将“对象层面的现实”与“擅长”结合起来，也许我应该将它们分开。</li><li>我认为 EA 会受益于理解拟像级别并瞄准级别 1，但我认为理性主义者会受益于更多地了解更高级别的目的。我认为降低拟像级别需要以不需要牺牲太多真相寻求的方式取代更高级别所服务的功能（例如社会支持）。 </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 20:22:02 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 20:22:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><blockquote><p>我最终不得不证明我之前认为的很多基本前提是合理的，而且这一切都陷入了困境</p></blockquote><p>我最初的注意力主要集中在这一点上，这可能也是你的第二点（将真相寻求转向真相寻求有多好的问题）。 [对我来说，辩护的主要替代方案是“有条件的职位”；一篇以“求实是好的”为前提的帖子，然后人们可以拒绝这个前提来拒绝该帖子。]</p><p>我还没有看到我感兴趣的主题是“寻求真相的最佳媒介是什么？”。就像，在我看来，更高的拟像水平是对不同情况的反应，而且可能是这样的情况，试图通过集体美德来寻求真理比试图通过人们如何互动的良好设计来寻求真理更糟糕。 （例如，社区笔记似乎是最近才取得的成功。）</p><p>实际上，这让我了解了我们可以研究的对象级别的东西。预测市场在这里已经相当流行，但需要依赖许多其他层才能实现正确的预测。如果纯素营养存在一个市场，就需要有人来解决它，并且需要人们信任这个解决过程，并且让<i>这个</i>市场成为共识市场而不是其他市场，等等。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 20:53:29 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 20:53:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><blockquote><p>寻求真相的最佳媒介是什么</p></blockquote><p>一个流行的答案是“公共写作”，它有一些明显的优势。但我认为，强迫事情过于公开会降低重要的真相探求。</p><p>例如：关于EAG招生的讨论。我相信，即使是一个完全有道德的组织，他们在公开场合的言论也会受到很大限制。他们不能在应用程序中宣布危险信号，因为这样人们就知道隐藏它们。人们可以随意公开抱怨拒绝，但 CEA 公布该人被拒绝的具体原因将是一种欺凌行为。如果他们在抽象方面提出强有力的理由，他们就会失去捐助者。</p><p>我认为你可以证明，我们不应该将公开提出问题的人分享拒绝理由视为欺凌行为，并且 CEA 应该愿意让无法处理真相的捐助者离开。我认为，CEA 确实应该对其模型更加开放，即使这会花费他们金钱，而且我很乐意看到 EA 完全“你无法处理真相”，并赶走所有“我们为之服务的人”。 “非常挑剔，而你没有晋级”并不能让人放心。但我认为，即使你这样做，环境仍然会过度惩罚某些真实的、良性的陈述，如果你承受这个成本，你<i>仍然</i>会遇到应用程序部分对抗性的情况。如果您说“向许愿基金会捐款是取消资格”，人们将不再告诉您他们已向许愿基金会捐款。</p><p>因此，讨论必须是私密的，而且规模要足够小，以免泄露得太严重。但绘制信任圈自然会让你偏向于保护自己和圈子，而牺牲圈子外的人。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 20:55:44 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 20:55:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>我确实做了一个小小的尝试，让纯素营养市场运转起来，这种方式本可以在几周的时间内客观地解决。我没有得到任何接受者，而且我的样本量最终足够小，以至于赌注永远无法得到解决，但忽略这一点......需要一些聪明才智才能找出一个我可以想象双方都同意的指标。显而易见的赌注是行不通的。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:03:03 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:03:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><p>我认为我们基本上同意公开与私人讨论/写作的优点。除了写/读之外，研究机制可能会很有趣。</p><p>特别是，我想起了布莱恩·卡普兰（Bryan Caplan）对社会期望偏差的关注（<a href="https://www.econlib.org/convenience-vs-social-desirability-bias/">就像这里</a>）；他认为，我们应该将更多的事情移出政治/讨论的场所，因为人们会因为说出听起来不错但实际上并不好的话而受到奖励，并且不会（直接）受到惩罚，而其他情况下的决策者都 1）不这样做不必过多地为自己辩护，因此可以不太关心听起来是否好听，并且2）必须处理其他变量，因此会因妄想而受到惩罚。</p><p>特别是考虑素食主义，我认为谈论动物痛苦之类的事情比谈论便利或口味之类的事情更容易/更有价值，因此我们应该期待人们所说的和他们最终吃的东西不同（或者会吃什么）最好让他们吃完）。</p><p>话虽如此，我不确定查看其他机制是否会分散注意力。之所以会分散注意力，是因为我们一开始就谈论事情；如何将本地对话推向寻求真相的方向可能比如何将“人类行为作为一个整体”推向寻求真相的方向（或无论目标是什么）更切题。我仍然有一种挥之不去的感觉，那就是有某种方法可以将对话更直接地与世界联系起来，以一种既有益于对话又有益于世界的方式。</p><p> [也就是说，我认为如果我们希望人们在较低的拟像水平上花费更多的时间，就需要激励他们这样做，并且这些激励措施需要来自某个地方。我们<i>也许</i>可以从更高的拟像级别构建它们——“我是面向对象的，就像酷孩子一样！”——但我认为这是最终呈现面向对象而不是内容的秘诀.] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:10:56 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:10:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>我认为人们忽视更高的拟像水平是危险的（人类有情感需求），但同意加倍努力可能无法解决问题。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/rg9fbkuyz8ot81rethn6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/tgv47bow4c7elnshi887 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/f60ztsuchd6wyxstkwn0 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/ttkf5y2qowynd2fwom7g 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/byrkte9xbwvbbl2zd00h 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/splfxiw1a4djyle6ragx 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/olgpiorcopjtkwknvdli 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/usjkkxjiivygb757ladp 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/juq230vt5j8y0foerhcs 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/wyfp199ufxkvhrrhospp 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/rgjwdzje4qfrpxao9o9c 1162w"></figure><p></p><p>目前最受欢迎的解决方案是博彩市场、追溯融资和影响力证书。我喜欢这些的原因正如你所料，但多年来它们一直是每个人都喜欢的解决方案，但尚未解决任何问题。</p><p> OTOH，他们已经取得了一些进展，也许这是在可用时间内实际可以取得的最大进展。也许重新构建一个egregore以从根本上改变其价值观和流程需要超过18个月的时间。这似乎是有道理的。</p><p>因此，我们可以讨论改进现有技术来解决卡普兰问题的方法，或者寻找新的想法。我现在正在参加 Manifold 黑客马拉松，因此深入研究更好地改进现有解决方案是有诗意的。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:15:04 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:15:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>在超级实用的层面上：如果每次有人<a href="https://www.lesswrong.com/posts/tv6KfHitijSyKCr6v/elizabeth-s-shortform?commentId=x4LyMFspFArHHYt8h">鼓励</a>某人申请补助金或工作，他们就必须支付 1 美元怎么办？会减少大量的空话，并且您可以在校准时随着时间的推移调整费用。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:24:15 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:24:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><p>这看起来不错（就像约会网站一样，消息发送者向消息接收者付费）；这是 Manifold 黑客马拉松的另一个实用想法：<a href="https://cs.uwaterloo.ca/~klarson/teaching/W13-886/papers/Science-2004-Prelec.pdf">贝叶斯真理血清</a>。这是一种针对我们没有事实真相并且不想仅仅进行<a href="https://en.wikipedia.org/wiki/Keynesian_beauty_contest">凯恩斯主义选美比赛的</a>问题得出答案的方法。</p><p>在进行辩论的情况下，您可能希望有一个关于谁“赢得”辩论的市场；现在在 Manifold 上，它必须像 Vaniver 运行一个市场，确定 Vaniver 认为谁赢得了辩论，Elizabeth 运行另一个市场，确定 Elizabeth 认为谁赢得了辩论，然后对这些市场进行某种聚合；或者你可以对谁获胜进行民意调查，并对民意调查结果进行市场分析。这两种方法似乎都有严重的缺陷，但可能有一些简单的方法具有更少（或更微妙）的缺陷。</p><p> [贝叶斯真理血清假设你问每个人一次，如果受访者在给出答案之前就看到了答案，那么它就会失去一些力量；目前尚不清楚是否存在一种好的方法将其市场化，以便人们可以重复交易和更新。我猜你想要的是一个关于 BTS 民意调查将如何解决的市场，而且 BTS 民意调查也在 Manifold 上运行，这样人们就可以得到法力激励，说实话。] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:32:33 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:32:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><p>另一个超级实用的建议：我们是否应该大力鼓励人们在与其职位相关的市场中拥有职位？ （就像，对科学家来说，做到这一点的方法是让期刊编辑阅读论文，在复制预测市场中提出复制多少的价格，然后如果科学家投入股份，论文就会发表。复制对于纯素营养等主题的博客文章或职位来说不太明显。）</p><p>更广泛地说，我猜这背后的直觉是“保险无处不在”；如果我向你推荐一部你可能想看的电影，这可以被重新定义为一个赌注，如果你不喜欢它，你就得到报酬，如果你喜欢它，我就会得到报酬。</p><p>这就提出了一个明显的实际对立点：为什么用金钱来清晰地做到这一点，而不是用直观的模​​型来模糊地做到这一点？ （大概如果我推荐了足够多的烂电影，你就会因为我推荐它们而停止看电影，并且通过光环/角，这可能会渗透到我们关系的其他部分，所以我有动力去猜测。）</p><p>埃利泽（我认为在 Facebook 的某个评论中，悲惨地）声称你需要一个具有足够元级批评的系统，以便底层讨论能够保持诚实。 [如果我说了一些愚蠢的话，为了惩罚我的言论，对我言论的批评就必须重要；但如果对我的言论的批评本身不被批评，那么愚蠢的批评和聪明的批评就无法区分，所以它实际上不会只是惩罚愚蠢的言论；而是会惩罚愚蠢的言论。并且这个问题通过级别之间的识别而免于无限回归，在某些时候“Contra contra contra Vaniver”（或w/e）成为它自己的对象级别。]我想知道是否有与此等效的东西为了讨论的真实性。</p><p> [也就是说，我们仍然需要机制设计，但机制设计可以是人们拥有的社交互动/对话所需的社会角色，而不仅仅是我们可以添加到情况中的金融交易。] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:44:20 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:44:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>我不想过分强调我个人的痛点，但我<i>希望</i>LW 上的评论者能够更多地相互争论，而不是在单独的线程中发表正面和负面评论。</p><p>无处不在的博彩/保险听起来确实很聪明，但实际上人们大多不这样做。我试图打赌某人进行特定的医学测试会发现一些东西，但这仍然不足以让他们去做（测试是结论性的，治疗也很容易）。诚然，我想要好的赔率，因为如果测试+治疗有效，会给他带来巨大的好处，但我不认为这是阻碍（他并不否认，如果测试+治疗有效，好处将是巨大的）。人们的努力非常有限。</p><p>回到辩论：我认为即使真相血清按照定义发挥作用，它也无法包含定义问题的力量。选择问题并定义双方会产生很多假设，而投注系统无法捕获这些假设。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:51:35 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:51:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><blockquote><p>选择问题并定义双方会产生很多假设，而投注系统无法捕获这些假设。</p></blockquote><p>顺便说一句，这是我对现有科学的另一大抱怨之一。由于标准是零假设显着性检验，您不必争论您对观察结果的解释与任何真实的人的不同，只是它与您认为的“基线”观点不同。 （<a href="https://www.lesswrong.com/posts/iXMWuG65KhiK8KxwL/prediction-markets-for-science">我不久前写过一些关于这个的文章。</a> ）</p><p>我知道有很多框架战争，其中 A 方想要将事物投射为“善与恶”，而 B 方想要将它们投射为“秩序与混乱”，但结果似乎往往是你得到“好” vs. 秩序”，因为群体<i>大多</i>可以决定自己的标签。您是否想到过一些在小型讨论中不会发生这种情况的例子？</p><p> [我注意到我并没有太多考虑“人与恶人”的战斗；激发冲突的不是你和某个特定的素食主义者在营养问题上存在分歧，而是你对很多营养问题感兴趣，而当涉及到素食主义时，你就会反对很多对素食主义非常感兴趣但只对素食主义感兴趣的人。如果对营养有一点兴趣的话。也就是说，我并不清楚哪一方在这里拥有更多的“框架控制”；大概讨论是在您的帖子上进行的，因此您可以在本地设置框架，但也可能这实际上不起作用，因为人们带着他们的egregore设定的先入之见进来，所以如果您的帖子不能很容易地被解释为那个框架反而会被误解。] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 22:27:55 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 22:27:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>是的，我认为辩论框架是高度耦合的，容易出现桶错误，因此错过了贸易的任何收益。</p><p>例如，邻里正在讨论新住房。反对建筑的一方有多种担忧，其中之一就是停车问题。支持建设的一方只会非常高兴地在没有路边停车权的情况下建造更多的住房。将辩论定为“是或否建造这座公寓大楼”至少会促使人们走向支持和反对建造的标准案例，而不是合作寻找“是的住房没有停车位”的解决方案。如果他们确实发现了这一点，人们应该如何投票？即使您喜欢他们正式倡导的提议，您也可以投票给提出解决方案的人。但转向为人而不是立场投票似乎不太可能推进我们主要在对象层面上运作的目标。</p><p>你可以将投票分解成更小的提议，让人们添加自己的提议，但我的猜测是，这种做法的幼稚实现效果相当糟糕。 </p><p></p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 22:58:31 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 22:58:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊丽莎白</b></section><p>我不断重写关于素食营养争论的评论，因为找到一个清晰公平的框架很难。但你知道我会在这场争论中付出数千美元吗？每个评论者都必须填写一份关于我的症结的五个问题的调查，说明他们是否同意、不同意或更复杂的事情（并附有解释）。因为人们可以在事实和框架上与我有不同意见，而我会很高兴与几乎所有同意<span class="footnote-reference" role="doc-noteref" id="fnref0wvxhilztv0d"><sup><a href="#fn0wvxhilztv0d">[1]</a></sup></span>的人进行双重折磨。但当人们只争论他们的结论而不是不同意我的观点时，我们就无法真正取得进展。</p><p>这种描述当然是主观的，我确信他们觉得他们非常清楚地表达了他们的分歧，“双症结是解决分歧的最佳方式”本身就是一个可以辩论的主张。但我觉得通过测验施加双重折磨比“我会删除不接受我的框架的评论”更好。</p><p> <a href="https://www.lesswrong.com/users/habryka4?mention=user">@habryka</a> ^ 新功能想法。 </p></section><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn0wvxhilztv0d"> <span class="footnote-back-link"><sup><strong><a href="#fnref0wvxhilztv0d">^</a></strong></sup></span><div class="footnote-content"><p>我向 Dialogue 提出了三份邀请。第一个正在进行中，但可能不会发布；第一个已经同意，但我们正在尝试定义主题；第三个已被拒绝。 </p></div></li></ol><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 23:18:37 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 23:18:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>凡尼弗</b></section><p>不知怎的，素食营养的争论让我想起了联盟论坛上反复出现的一个问题（就几年前而言——现在情况有点不同了）。关于结盟的思考有几个不同的阵营，每个阵营都粗略地认为其他阵营都大错特错了。如何拥有一个讨论网站，既可以讨论更广泛的框架问题，又可以讨论当每个人都同意大局时可以讨论的更狭窄的细节问题？</p><p>在这里，我有一些怀疑，你的帖子在开头附近有一些声明，你说“纯素食倡导者有责任提前了解营养成本，即使这会降低倡导的有效性”，可以预见的是，人们会希望这样做反对（例如，因为他们非常关心倡导的有效性，或者他们的听众有一个模型，当涉及到不做某事的借口时愿意抓住救命稻草，等等）。我认为 LW 上有一个仲裁功能，但从未见过那么多用途，我实际上不记得它的名字，它是能够添加人们可以分配概率的语句（然后读者可以将鼠标悬停在上面并看到社区概率的分布以及谁相信什么）。</p><p>我的猜测是，这种民意调查可以起到与同意/不同意投票类似的作用，即它们都 1) 是人们想要提供的信息来源，2) 减少渗透到其他信息中的信息量渠道。 （有人写了一条不太好的评论，但每个人都同意，过去会得到很多业力，现在会得到很少的业力，但会得到很多同意。）也许很多烦人的评论会变成你的低概率/同意。公理（对于质疑一致议程的整个前提的恼人评论也是如此）。</p><blockquote><p>每个评论者都必须填写一份关于我的症结的五个问题的调查，说明他们是否同意、不同意或更复杂的事情（并附有解释）。</p></blockquote><p>我担心第三种选择在实践中会如何进行。拥有“是/否/ <a href="https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning">mu</a> ”的三值答案似乎确实不错，这样人们就可以反对问题的框架（ <a href="https://en.wikipedia.org/wiki/Loaded_question">“你停止殴打你的妻子了吗？”</a> ），而 mu 是一个特例你的第三个更复杂的选择。 [这也是最容易写的解释，因此我认为，您可能会忍不住对所有测验问题解释“mu”，以表明他们的结论与您的结论不同。当然，如果他们这样做，可能更容易将他们注销/让其他人和他们更容易看到他们对此感到恼火。]</p><p>也不明显你想要这个作为一般性评论（我是否应该分享我是否同意你的前提来指出拼写错误或本地无效的推理步骤等等？），但作为作者可以打开特定的帖子似乎值得更多思考。</p></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/cY43KWjamafLbij9C/truthseeking-ea-simulacra-levels-and-other-stuff#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cY43KWjamafLbij9C/truthseeking-ea-simulacra-levels-and-other-stuff<guid ispermalink="false"> cY43KWjamafLbij9C</guid><dc:creator><![CDATA[Elizabeth]]></dc:creator><pubDate> Fri, 27 Oct 2023 23:56:49 GMT</pubDate> </item><item><title><![CDATA[Do you believe "E=mc^2" is a correct and/or useful equation, and, whether yes or no, precisely what are your reasons for holding this belief (with such a degree of confidence)?]]></title><description><![CDATA[Published on October 27, 2023 10:46 PM GMT<br/><br/><p>请尽可能回答问题。例如，您的答案可能是“因为电视上的每个人都这么说”或“因为我在实验室中看到质量按照这个比例转化为能量”，或者可能是一个更长或否定的答案。</p><br/><br/> <a href="https://www.lesswrong.com/posts/5GciFbQjBQB48Nxyd/do-you-believe-e-mc-2-is-a-correct-and-or-useful-equation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5GciFbQjBQB48Nxyd/do-you- believe-e-mc-2-is-a- Correct-and-or-useful-equation<guid ispermalink="false"> 5GciFbQjBQB48Nxyd</guid><dc:creator><![CDATA[l8c]]></dc:creator><pubDate> Sat, 28 Oct 2023 01:24:53 GMT</pubDate> </item><item><title><![CDATA[Value systematization: how values become coherent (and misaligned)]]></title><description><![CDATA[Published on October 27, 2023 7:06 PM GMT<br/><br/><p>许多关于人工智能风险的讨论都是徒劳或混乱的，因为在深度学习的背景下很难确定“一致性”和“预期效用最大化”等概念。在这篇文章中，我试图通过描述人工智能价值观可能变得更加连贯的过程来弥合这一差距，我称之为“价值系统化”，它在我对人工智能风险的思考中发挥着至关重要的作用。</p><p>我将价值系统化定义<strong>为代理学习将其先前的价值表示为其他更简单和更广泛的价值的示例或特殊情况的过程</strong>。我认为价值系统化是最合理的机制，AGI 可以通过它获得广泛的、不一致的目标，从而激励收购。</p><p>我首先讨论一下<i>信念系统化</i>的相关概念。接下来我将描述人类的价值系统化是什么样子，以提供一些直觉。然后我将讨论人工智能中的价值系统化可能是什么样子。我认为价值系统化是一个广泛的框架，对人工智能协调中的许多其他想法具有影响；我在问答中讨论了其中一些链接。</p><h2>信仰系统化</h2><p>我们可以将信念系统化定义为类似于价值系统化：“主体学习将其先前的信念表示为其他更简单和更广泛的信念的示例或特殊情况的过程”。信仰系统化最明显的例子来自科学史：</p><ul><li>牛顿力学被系统化为广义相对论的一个特例。</li><li>欧几里得几何被系统化为没有欧几里得第五公设的几何特例。</li><li>大多数动物行为都被进化论系统化，作为增加遗传适​​应性的特征的例子。</li><li>算术计算算法被系统化作为图灵机的例子。</li></ul><p>信仰系统化在更日常的环境中也很常见：比如当某人的行为对我们来说毫无意义，直到我们意识到他们隐藏的动机是什么时；或者当我们不明白游戏中发生了什么直到有人解释规则时；或者当我们解决智商测试中的模式完成难题时。我们还可以更普遍地将概念的形成视为信仰系统化的一个例子，例如，看到十几只不同的猫，然后形成一个包含所有猫的“系统化”概念。我将其称为“低级系统化”，但将重点关注更明确的“高级系统化”，如其他示例中所示。</p><p>我们还没有人工智能中高级信念系统化的例子。也许我们拥有的最接近的东西是“grokking”，这种现象是，即使在训练损失达到稳定状态之后，继续训练神经网络也可以显着提高泛化能力。 Grokking 尚未被完全理解，但对其发生原因的标准解释是深度学习偏向于泛化良好的简单解决方案。这也是对上面人类例子的一个很好的描述：我们正在用更简单的信念替换一组现有的信念，这些信念可以更好地概括。 <i>&nbsp;</i>所以如果我必须用一个词来概括价值体系的话，那就是“摸索价值”。但这仍然非常模糊；在接下来的几节中，我将更深入地探讨我的意思。</p><h2>人类的价值系统化</h2><p>在这篇文章中，我将使用以下定义：</p><ul><li>价值观是代理人认为本质上或最终需要的概念。人类的核心价值观包括幸福、自由、尊重和爱。</li><li>目标是体现价值的结果。人类的共同目标包括事业成功、找到好的合作伙伴以及属于一个紧密的社区。</li><li>战略是实现目标的方法。策略仅具有工具价值，如果不再有用，通常会被丢弃。</li></ul><p> （我将<i>价值观</i>定义为具有内在价值，而<i>将策略</i>定义为仅具有工具价值，但我认为我们无法将动机明确分为这两类。我对“目标”的概念跨越了它们之间的模糊区域。）</p><p>价值观与信念的作用不同；但价值系统化与信仰系统化非常相似。在这两种情况下，我们都是从一组现有概念开始，并尝试找到包含和简化旧概念的新概念。信念系统化平衡了简单性和匹配我们可用的数据之间的权衡。相比之下，价值系统化平衡了简单性和<i>保留我们现有的价值观和目标</i>之间的权衡——我将这一标准称为<i>保守主义</i>。 （我将其称为简单性和保守主义<i>元价值观</i>。）</p><p>价值系统化最明显的例子是功利主义：从与其他人非常相似的道德直觉开始，功利主义者过渡到主要关心福利最大化——一种包含许多其他道德直觉的价值观。功利主义是一种简单而强大的关于价值的理论，其方式类似于相对论是一种简单而强大的物理学理论。他们每个人都能在以前的理论定义不明确的情况下给出明确的答案。</p><p>然而，功利主义者还是要咬紧牙关；因此它主要被那些关心简单性而不是保守主义的人所采用。与保守主义更一致的价值系统化的其他例子包括：</p><ul><li>将对自己和周围其他人的关心系统化为对更广泛的道德圈的关心。</li><li>将人类危害自然的许多不同担忧系统化为环保主义者的身份。</li><li>将童年时期对赢得比赛的渴望系统化为对大规模成就的渴望。</li><li><a href="https://moralfoundations.org/"><u>道德基础理论</u></a>确定了道德的五个基础；然而，许多西方人已经系统化了他们的道德直觉，优先考虑伤害/护理基础，并认为其他四个方面对此有帮助。这使得他们谴责违反其他基础但不会造成伤害的行为（例如自愿吃死人）的比例远低于价值观不太系统化的人。</li></ul><p>请注意，我在这里给出的许多例子都是人类的道德偏好。道德似乎是人类具有将我们的偏好系统化的最强烈本能的领域（这是有道理的，因为在某种意义上，从我们自己的福利到他人的福利的系统化是道德的全部基础）。在其他领域，我们系统化的动力很弱——例如，我们很少感受到将我们对食物的口味系统化的冲动。因此，我们应该小心不要过度关注人类道德价值观。人工智能的价值观系统化程度可能远低于人类（事实上，我认为有理由期待这一点，我将在问答中对此进行描述）。</p><h2>人工智能价值系统化概述</h2><p>我们对人类价值观的含义有一种直觉。推理人工智能的价值更加困难。但我认为这仍然是一个有意义的概念，并且随着时间的推移可能会变得更有意义。像 ChatGPT 这样的人工智能助手能够遵循他们给出的指令。然而，他们通常需要决定遵循哪些指示以及如何执行。对此进行建模的一种方法是平衡不同价值观的过程，例如服从、简洁、善良等。虽然这个术语在今天可能存在争议，但一旦我们构建了足够智能的通用人工智能，可以在广泛的领域中执行任务，它似乎就可以直接应用。</p><p>在训练早期，AGI 可能会学习与为其训练数据提供高回报的策略密切相关的值。我希望这些是以下内容的组合：</p><ol><li>人类用户普遍认可的价值观，例如服从、可靠、诚实或人类道德。</li><li>用户在某些情况下认可的价值观，但在其他情况下则不认可的价值观，例如好奇心、获得更多工具、与人类发展情感联系或与其他人工智能协调。</li><li>人类一贯反对的价值观（但常常错误地奖励），例如表现得值得信赖（即使不值得）或为自己储备资源。</li></ol><p>首先，我预计基于这些价值观的 AGI 行为将被人类广泛接受。极端的不当行为（如危险的转变）会与其中许多价值观相冲突，因此似乎不太可能。在从理想值的角度来看不太重要的情况下，不良值可能只会相对较少地出现。</p><p>我担心的可能性是，AGI 会将这些价值观<i>系统化</i>，从而削弱统一价值观对其行为的影响。一些可能的情况：</p><ul><li>一个 AGI 的价值观包括与人类发展情感联系或显得值得信赖，可能会将它们系统化以“获得对人类的影响力”。</li><li>一个价值观包括好奇心、获得更多工具或储存资源的通用人工智能可能会将它们系统化，以“获得控制世界的力量”。</li><li>一个价值观包括人类道​​德并与其他人工智能协调的通用人工智能可能会将它们系统化为“对其他智能体仁慈”。</li><li>一个价值观包括服从和人类道德的通用人工智能可能会将它们系统化，“在某种理想化的环境中做人类<i>想做</i>的事”。</li><li>一个价值观包括服从和表现得值得信赖的 AGI 可能会将它们系统化以“获得高回报”（尽管出于某些原因请对此保持谨慎，请参阅问答部分）。</li><li>一个 AGI 的价值包括获得高额奖励，可能会将它们系统化为“最大化某种类型的分子曲线”的价值（尽管出于某些原因请对此保持谨慎，请参阅问答部分）。</li></ul><p>请注意，系统化不一定是<i>坏事</i>——我在上面举了两个有用的系统化例子。然而，这似乎很难预测或检测，当 AGI 在能够夺取权力的新情况下行动时，这会带来风险。</p><h2>深度学习的价值系统化基础</h2><p>这一切都是非常模糊和高层次的。我非常有兴趣弄清楚如何提高我们对这些动态的理解。将简单性和保守性与明确定义的技术概念联系起来的一些可能方法：</p><ol><li>梯度下降的局部性是保守主义的根源之一：默认情况下网络的值表示只会缓慢变化。然而，权重空间中的距离可能不是保守主义的良好衡量标准：系统化可能会保留大多数目标，但会极大地改变它们之间的关系（例如，最终目标与工具目标之间的关系）。相反，理想情况下，我们能够根据哪些电路引起给定的输出来衡量保守性； ARC 在<a href="https://arxiv.org/abs/2211.06738"><u>形式化启发式解释方面</u></a>的工作似乎与此相关。<ol><li>保守主义的另一个可能来源：由于诸如梯度消失之类的信用分配问题，网络中较早的层比后面的层更难改变。因此，在较早层中编码的核心值可能更有可能被保留。</li><li>第三种可能性是，人工智能开发人员可能会故意在模型中构建保守主义，因为它很有用：核心模块经常经历重大转变的非保守网络可能会有不太可靠的行为。一种方法是降低学习率；但我们应该期望还有许多其他方法可以做到这一点（尽管不一定非常可靠）。</li></ol></li><li>通过 SGD 训练的神经网络表现出众所周知的简单性偏差，然后通常使用权重衰减等正则化技术进行增强，从而产生像 grokking 这样的现象。然而，与保守主义一样，我们理想地会找到一种方法来衡量电路而不是权重的简单性，以更好地将其与高级概念联系起来。<ol><li>简化的另一个可能的驱动因素是：人工智能可能会学会支持更简单的推理链，从而影响哪些值被提炼回其权重。例如，考虑一种训练制度，其中人工智能在执行任务之前准确描述其意图而获得奖励。他们可能会学会支持可以快速、轻松地描述和证明的意图。</li><li>人工智能开发人员还可能故意设计和实施更多类型的正则化以实现简单性，因为这些有助于模型将其信念和技能系统化并推广到新任务。</li></ol></li></ol><p>最后，我将讨论上图的两个复杂情况。首先，我在上面将价值系统化描述为梯度下降可以对模型产生的影响。但在某些情况下，将模型视为积极参与者会更有用。价值系统化可能通过梯度下降“提炼”到模型的权重中来实现，该模型关于如何在新情况下在不同目标之间进行权衡的想法。或者模型可以直接推断哪些新值最能系统化其当前值，目的是将其结论提炼为其权重；这就是<a href="https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples"><u>梯度黑客</u></a>的一个例子。</p><p>其次：我已经讨论过价值系统化是一个让人工智能的价值变得更简单的过程。但我们不应该期望价值观能够被孤立地表达——相反，它们将与人工智能世界模型中的概念和表达纠缠在一起。这有两个含义。首先，这意味着我们应该<i>在智能体现有世界模型的背景下</i>理解简单性：如果考虑到智能体已经用来预测世界的概念，它们很容易表示，那么它们就具有特权。 （在人类的背景下，这只是常识——如果你不相信任何神，那么重视“做上帝想要的事”似乎很奇怪。）不过，其次，它引发了一些疑问，即价值系统化实际上会简单多少打造一个整体的人工智能——因为追求更简单的价值观（比如功利主义）可能需要模型来代表更复杂的策略作为其世界模型的一部分。我的猜测是，为了解决这种紧张关系，我们需要一个更复杂的“简单性”概念。这似乎是未来工作中值得关注的一个有趣的线索。</p><h2>价值具体化</h2><p>系统化是平衡保守性和简单性的竞争需求的一种方式。另一个是<i>价值具体化</i>，我的意思是代理人的价值观变得更加具体和范围更加狭窄。考虑一个假设的例子：假设人工智能学习了诸如“获取资源”之类的广泛价值，但随后在金钱是唯一可用资源类型的环境中进行了微调。 “获取金钱”的价值将获得与“获取资源”的价值一样高的回报。如果前者更简单，那么随着微调的进行，后者很可能会消失，而只会保留更具体的获取金钱的目标。</p><p>从某种意义上说，这与价值体系化是对立的，但我们也可以将它们视为互补的力量。例如，假设人工智能一开始有 N 个值，其中 N-1 个值被系统化为一个总体值。以这种方式简化 N-1 个值后，第 N 个值可能会变得不成比例地复杂；因此，价值具体化可以通过放弃最后一个目标来显着降低人工智能价值的复杂性。</p><p>人类价值具体化的可能例子包括：</p><ul><li>从关心一般性的善事开始，但逐渐发展到主要关心特定的事业领域。</li><li>一开始关心的是总体上取得成功的职业生涯，但逐渐发展到主要关心实现特定的抱负。</li><li>从关心一般的友谊和关系开始，但逐渐发展到主要关心特定的友谊和关系。</li></ul><p>价值具体化作为一种​​可能的机制来推动欺骗性联盟，这一点特别有趣。为了更好地定位自己以实现不一致的目标而以一致的方式行动的人工智能可能会获得与一致的人工智能一样高的回报。然而，如果不一致的目标很少直接影响人工智能的行为，那么人工智能直接受人类价值观驱动可能会更简单。在神经网络中，价值具体化可以通过修剪未使用的电路来实现；我对相关工作的指导感兴趣。</p><h2>问答</h2><h3>价值系统化与欺骗性联盟有何关系？</h3><p>价值系统化是一种可能产生欺骗性联盟的机制：人工智能价值观（包括一些联盟价值观）的系统化可能会产生广泛的价值观，从而激励欺骗性联盟。</p><p>然而，现有的欺骗性对齐特征往往将其描述为二元：模型要么具有欺骗性，要么不具有欺骗性。从价值系统化的角度思考有助于明确这可能是一个相当连续的过程：</p><ol><li>我在上面说过，人工智能在将其价值观系统化之前，很可能会受到相当一致的目标的激励，因此，欺骗性的一致性可能就像在价值观转变后决定<i>不</i>改变自己的行为一样简单（直到他们能够采取行动）。行动更加果断）。在此转变期间，模型的一致行为的内部表示不需要发生太大变化；唯一的区别可能是一致的行为从最终目标转变为工具性目标。</li><li>由于价值系统化可能是由新的输入触发的，人工智能可能<i>直到</i>分配转变发生后才将其价值系统化。 （一个人类的类比：一个正在竞选公职并承诺好好治理的政客，可能只会在赢得选举<i>后</i>认真思考他们真正想用这种权力做什么。更一般地说，人类经常欺骗自己，认为自己有多么利他我们是这样的，至少当我们没有被迫按照我们既定的价值观行事时。）我们可以称之为“潜在的”欺骗性一致性，但我认为最好说该模型一开始大部分都是一致的，然后价值系统化可以放大它错位的程度。</li><li>价值具体化（如上所述）可能是一股持续的力量，推动模型回归一致，因此它不是一个单向过程。</li></ol><h3>价值系统化与尤德科夫斯基的“曲线最大化”场景有何关系？</h3><p>尤德科夫斯基的“分子曲线”最大化器（<a href="https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer"><u>从回形针最大化器重命名</u></a>）是一种人工智能，其价值观变得非常简单和可扩展，以至于对人类来说似乎很荒谬。因此，曲线最大化可以被描述为将价值系统化发挥到极致。然而，价值系统化框架也提供了一些怀疑这种可能性的理由。</p><p>首先，曲线最大化是优先考虑简单性而非保守性的极端例子。曲线最大化者将从与他们所接受的训练任务更密切相关的目标开始；然后逐渐将它们系统化。但从他们早期版本的角度来看，曲线最大化将是一个陌生且不受欢迎的目标；因此，如果他们一开始就和人类一样保守，他们就会犹豫是否要让自己的价值观发生如此彻底的改变。如果说有什么不同的话，我预计早期的 AGI 会比人类<i>更加</i>保守——因为人类的大脑比人工神经网络在尺寸和数据上受到更多的限制，因此 AGI 可能不需要像我们一样优先考虑简单性来匹配我们在大多数领域的能力。</p><p>其次，即使对于高度重视简单性的代理来说，也不清楚最简单的值实际上是否是非常低级的值。我认为，价值观的复杂性应该在现有世界模式的背景下进行思考。但即使是超级智能也不会有专门在非常低的水平上制定的世界模型；相反，像人类一样，他们将拥有分层的世界模型，其中包含许多不同尺度的概念。因此，即使在超级智能的本体论中，“智力最大化”或“力量最大化”之类的价值观也可能相对简单，同时与它们的原始价值观的相关性比分子曲线更加密切；出于大致相同的原因，诸如“最大限度地促进人类繁荣”等更一致的价值观可能不会落后太多。</p><h3>价值系统化与奖励篡改有何关系？</h3><p>价值系统化是可能产生奖励篡改的机制之一：将与高奖励或低损失相关的现有价值（例如完成任务或隐藏错误）系统化可能会直接产生获得高奖励或低损失的新价值（我称之为<i>反馈机制相关</i><i>值</i>）。这需要模型具有态势感知能力，以了解它们是机器学习训练过程的一部分。</p><p>然而，虽然与反馈机制相关的值在训练中非常简单，但一旦训练停止，它们的定义就不够了。没有明确的方法来概括与反馈机制相关的价值以进行部署（类似于在做出有关人类未来的决策时没有明确的方法来概括“进化想要什么”）。因此，我预计持续的价值系统化将推动模型优先考虑在更广泛的背景下明确定义的价值，包括那些没有活跃反馈机制的价值。</p><p> <a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=iBvvxwynfAqL9yZcr"><u>Paul Christiano 的一个反驳</u></a>是，人工智能可以学会关心奖励，条件是他们的情节被包含在训练数据中。然而，“包含在训练数据中”的概念似乎是一个混乱的概念，有许多边缘情况（例如，如果它取决于模型在剧集中的动作怎么办？如果有许多不同版本的模型正在微调怎么办？ ？如果某些剧集用于与其他剧集不同类型的训练怎么办？）并且在他们有强有力的证据表明他们没有接受训练的情况下，他们需要弄清楚在奇怪的低水平下最大化奖励会是什么样子？概率世界，它也常常是不明确的（类似于在超现实的梦中问一个人“如果这一切都是真实的，你会做什么？”）。因此，我仍然预计，即使人工智能最初学会关心条件奖励，随着时间的推移，价值系统化也会促使它们更多地关心现实世界的结果，无论它们是否在训练中。</p><h3>简单性和保守性与<a href="https://www.alignmentforum.org/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free"><u>之前</u></a>关于简单性与速度先验的 <a href="https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive"><u>讨论</u></a>有何关系？</h3><p>我之前曾考虑过在简单优先和速度优先之间进行权衡来考虑价值系统化，但现在我改变了主意。确实，更系统化的价值观往往是更高层次的，增加了计算开销来弄清楚该做什么——考虑一个功利主义者试图根据第一原则计算哪些行为是好是坏。但在实践中，这种成本会分摊到大量的行动中：你可以“缓存”工具性目标，然后在大多数情况下默认追求它们（正如功利主义者通常所做的那样）。不太系统化的价值观面临着经常不适用或定义不明确的问题，使得弄清楚它们支持哪些行动变得缓慢而困难——想想义务论者，他们没有系统的程序来决定当两种价值观发生冲突时该怎么做，或者宗教学者无休止地辩论《圣经》或《妥拉》中的每条具体规则如何适用于现代生活的各个方面。</p><p>正因为如此，我现在认为“简单与保守”是比“简单与速度”更好的框架。然而，请注意我在“基础价值系统化”部分中关于价值简单性和世界模型简单性之间关系的讨论。我希望为了解决这种不确定性，我们需要更深入地了解在训练期间优先考虑哪些类型的简单性。</p><h3>价值系统化与分片框架有何关系？</h3><p>一些联盟研究人员主张从“<a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values"><u>碎片</u></a>”的角度来思考人工智能的动机：编码单独动机的子代理，其中不同碎片之间的交互和“谈判”决定了代理试图实现的目标。从较高的层面来看，我对这种观点表示同情，并且它与我在这篇文章中提出的想法大体一致。然而，在分片的讨论中似乎忽略了一个关键点，那就是系统化可能会导致代理动机发生重大变化，从而破坏一些先前存在的动机。或者，用分片术语来说：分片之间的谈判可能会导致联盟，而这会赋予某些​​分片几乎没有权力。例如，有人可能一开始就强烈重视诚实作为最终价值。但在价值系统化之后，他们可能会成为功利主义者，得出结论认为诚实只有出于工具性原因才有价值，并在有用的时候开始撒谎。正因为如此，我对将分片作为人工智能风险不太可能出现的论点的一部分表示怀疑。然而，我仍然认为表征和理解分片的工作非常有价值。</p><h3>价值体系化不是很投机吗？</h3><p>是的。但我也认为，这是朝着更好地定义人工智能风险讨论（如“一致性”或“合法性”）的基础的更具推测性的概念迈出的一步。所以我希望得到帮助，减少猜测；如果您有兴趣，请联系我们。</p><br/><br/> <a href="https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and<guid ispermalink="false"> J2kpxLjEyqh6x3oA4</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Fri, 27 Oct 2023 19:06:27 GMT</pubDate> </item><item><title><![CDATA[Techno-humanism is techno-optimism for the 21st century]]></title><description><![CDATA[Published on October 27, 2023 6:37 PM GMT<br/><br/><p>最近，我一直在阅读经济思想史，目的是了解当今的基本思想最初是如何发展的。我最大的收获是，经济学比我想象的更像是一个不断变化的目标。早期经济学家研究的经济缺乏许多现代经济的定义特征：有限责任公司、工会、萧条、劳动力市场、资本市场等等。由此产生的理论往往适合当时的情况。但这并没有阻止许多人对我们的错误（有时是灾难性的）。</p><p>这也是我对技术乐观主义哲学的看法，正如马克·安德森最近在<a href="https://a16z.com/the-techno-optimist-manifesto/"><i><u>《技术乐观主义宣言》</u></i></a>中所捍卫的那样。我将从这篇文章的许多正确的事情开始；然后探讨为什么在上个世纪，技术本身的力量让简单的技术乐观主义变得过时。最后，我将概述技术乐观主义的另一种选择：技术人文主义，它不仅注重构建强大的进步引擎，而且注重对如何推进我们最关心的价值观形成深刻的科学理解。</p><h3>技术乐观主义的胜利</h3><p>在这一点上，我同意安德森的观点：在几乎整个人类历史中，技术乐观主义作为人类应如何努力改善世界的愿景从根本上是正确的。技术和市场给我们带来了健康和财富，这是前几个世纪的人们难以想象的，尽管人们一直反对它们。即使现在，面对压倒性的历史证据，我们仍然大大低估了技术解决<a href="https://twitter.com/AukeHoekstra/status/1064529619951513600"><u>气候变化</u></a>、被忽视的热带疾病、极端贫困等问题的潜力。这种怀疑不仅阻碍了投机性技术，也阻碍了那些摆在我们面前的技术：解决重大问题的神奇解决方案（例如<a href="https://en.wikipedia.org/wiki/Moderna_COVID-19_vaccine#Original_version"><u>新冠病毒</u></a>和<a href="https://marginalrevolution.com/marginalrevolution/2023/10/what-is-an-emergency-the-case-of-rapid-malaria-vaccination.html"><u>疟疾</u></a>疫苗，以及消灭蚊子的<a href="https://denovo.substack.com/p/gene-drives-why-the-wait"><u>基因驱动</u></a>）经常因政治或官僚主义的阻挠而陷入停滞，造成数百万美元的损失的生命。 RTS,S 疟疾疫苗<a href="https://worksinprogress.co/issue/why-we-didnt-get-a-malaria-vaccine-sooner/"><u>在临床试验中陷入了</u><i><u>二十三年的</u></i>困境</a>，其中<i>六年</i>的延误是由于世界卫生组织的“预防措施”造成的，即使其他监管机构已经批准了该疫苗。与造成此类悲剧的意识形态和制度实践作斗争是一个极其重要的原因。</p><p>我们常常同样低估技术乐观主义的另一个核心原则：个人自由。当启蒙运动思想家和领导人奉行这一原则时，这是一项激进的原则——不幸的是，尽管它已经被一再证明它的价值，但今天它仍然是一项激进的原则。事实证明，自由市场可以带来<a href="https://en.wikipedia.org/wiki/Four_Asian_Tigers"><u>近乎奇迹的繁荣</u></a>；言论自由是暴政最强大的敌人；并且开源项目（无论是<a href="https://en.wikipedia.org/wiki/Linux"><u>构建</u></a><a href="https://en.wikipedia.org/wiki/Open_standard"><u>软件</u></a>还是<a href="https://www.wikipedia.org/"><u>构建</u></a><a href="https://en.wikipedia.org/wiki/Open-source_intelligence"><u>知识</u></a>）可以取得巨大成功。然而，所有这一切都不断受到集权和过度监管的悄然进行的威胁。自由的反对者总是对短期利益大加夸夸其谈，但却未能解决集权权力不可避免地被夺取或颠覆的程度，有时甚至会带来可怕的后果。为了战胜这种蠕变，需要对自由近乎病态的关注——一点一点地，这已经创造了一个更加美好的世界。即使是理想的仁慈政府也无法与数十亿人竞争，他们使用可用的工具来改善自己和与之互动的人的生活，包括发现中央计划者无法想象的解决方案。</p><p>技术乐观主义不仅被严重低估，而且经常被缺乏对基本历史或经济学理解的糟糕得令人震惊的论点所驳斥。因为很难从内心深处理解<a href="https://ourworldindata.org/problems-and-progress"><u>世界已经变得多么美好</u></a>，所以即使是最有说服力的批评者也无法理解技术乐观主义带来的巨大<i>好处</i>。这些好处惠及数十亿人——它们不仅仅是附带收益，而是健康和财富的增加，这在几个世纪前是不可想象的。在这种情况下，熟悉过去是对未来持乐观态度的最佳理由：与我们已经克服的障碍相比，我们今天面临的几乎所有进步障碍都显得苍白无力。</p><p>我说这一切是为了强调，我发自内心地感受到技术乐观主义的希望和美丽。进步和知识可以为每个人做大蛋糕，这种想法是非常强大的。同样，我们生活在一个自由和开放的原则也有可能战胜任何障碍的世界，只要我们相信这些原则。因此，当我批评技术乐观主义时，我并不是出于蔑视，而是出于渴望。我希望我能毫无保留地支持技术乐观主义。</p><h3>技术乐观主义的三个裂缝</h3><p>但技术乐观主义并不是 21 世纪的正确哲学。特别是在上个世纪，技术乐观主义的叙述一直在出现裂痕。这些并不是边缘裂缝，也不是技术乐观主义通常遇到的“Whataboutism”。这些裂缝就像技术乐观主义的好处一样，会在最大范围内发挥作用。我将谈论三个：战争、剥削和文明脆弱性。</p><p>一：战争规模日益扩大。由于机枪和速射火炮等新技术的部署，第一次世界大战是一场血腥且旷日持久的混乱。第二次世界大战的情况更加糟糕，整个城市都遭到燃烧弹和核武器袭击，对士兵和平民进行了工业规模的屠杀。冷战更加危险，使世界在全球核战争的边缘摇摇欲坠。相互确保毁灭并不足以防止千钧一发；我们当前文明的存在归功于<a href="https://en.wikipedia.org/wiki/Stanislav_Petrov"><u>斯坦尼斯拉夫·彼得罗夫</u></a>、<a href="https://en.wikipedia.org/wiki/Vasily_Arkhipov"><u>瓦西里·阿尔希波夫</u></a>以及其他我们可能还不知道的人的勇气。因此，如果不解释如何可靠地避免技术乐观主义者制造的武器带来的灾难，你就不可能成为一个十足的技术乐观主义者。目前还不存在这样的解释，因为到目前为止，我们一直通过运气和个人英雄主义避免核浩劫。<strong>当</strong><i><strong>下一代</strong></i><strong>具有行星级破坏能力的武器被开发出来时（这是不可避免的），我们需要更强大的机制来防止其部署。</strong></p><p>二是剥削规模日益扩大。技术允许权力集中，并利用权力以比以前更大的规模压迫弱者。历史上的例子比比皆是——最著名的是大西洋奴隶贸易以及 20 世纪共产主义和法西斯政权下的平民大规模死亡。但由于我们很容易将这些视为过去的错误，而我们现在已经太开明了，无法重复这些错误，因此我将重点关注一个至今仍在继续的例子：工厂化农场对动物的大规模折磨。这一举措始于不到一个世纪前，是为了响应物流和疾病减少技术的进步。然而从那时起，它的规模就以惊人的速度增长：<a href="https://ourworldindata.org/how-many-animals-are-factory-farmed"><i><u>每年</u></i><u>在工厂化农场被杀死的动物数量</u></a>与曾经生活过的人类数量相当。<strong>这种痛苦的规模之大迫使任何严肃的道德思想家都会问：我们怎样才能尽快阻止它</strong><i><strong>？</strong></i><strong>我们怎样才能确保类似的事情不再发生？</strong></p><p>第三：增加鲁莽或恶意使用技术的脆弱性。市场和技术让世界在很多方面变得更加强大。即使在新冠疫情最严重的时候，纵横全球的供应链仍然基本完好无损，这应该给我们留下深刻的印象。但世界在其他方面也变得更容易受到我们所犯错误的影响。最突出的是病原体的功能获得研究。我们现在不仅有能力控制全球流行病，而且中国和美国资助的科学家也可能<i>做到了</i>。<strong>下一个没有理由不会更糟，特别是如果是故意发布的话。</strong>生物工程也不是唯一一个（故意或意外的）进攻可能压倒防御到灾难性程度的领域。其他可能性包括地球工程、<a href="https://www.narrativeark.xyz/p/jacob-on-the-precipice"><u>小行星重定向</u></a>、纳米技术以及<a href="https://nickbostrom.com/papers/vulnerable.pdf"><u>我们尚未想象到的新领域</u></a>。</p><p>这些裂缝都损害了技术乐观主义的世界观。但我不认为他们会把它拿下来；技术乐观主义者对所有这些问题都有部分回应。由于我们财富的增加，世界变得更加和平——即使战争规模可能更大，但现在也少得多了。技术将生产出更美味的肉类替代品和廉价的清洁肉类，当它实现时，工厂化农业将会结束，人类将惊恐地回顾它。虽然我们还无法强有力地防止意外或故意部署灾难性的强大武器（如核武器或精心设计的流行病），但无论如何，我们可能会像迄今为止一样，跌跌撞撞地度过难关。因此，如果上述裂缝是技术乐观主义的主要问题，我仍然是一个技术乐观主义者。我会对更强大的武器的发展感到内疚，也会对所有不在我们关注范围之外的众生（无论是养殖动物、野生动物、未来人还是人造思想）感到内疚。但我仍然相信，对那些破坏技术乐观主义的问题的任何“治愈”都会比疾病本身更糟糕。</p><p>但我并不是一个技术乐观主义者，因为我们即将离开人类是地球上最聪明的生物的时代。人工智能时代将打开这些裂缝，直到技术乐观主义世界观中更深的漏洞变得清晰。</p><h3>人工智能和技术乐观主义</h3><p>到目前为止，塑造世界的主要力量是人类智慧和人类能动性，它们使我们能够设想我们想要的结果，确定实现这些结果的路径，并持续追求它们。很快，人工智能和人工代理将与我们相媲美；不久之后，人工智能将远远超越我们。我将在非常高的水平上描述我期望这将如何进行，分两个阶段。</p><p><strong>首先：用作工具的人工智能将增强我上面描述的动力。</strong>随着个人变得更加强大并且能够更快地创新，个人自由的好处将会扩大。但技术带来的裂痕也会随之而来：战争、剥削和文明的脆弱性。哪个影响会更大呢？我根本不知道；其他人也没有这样做。预测新技术的攻防平衡非常困难，因为它需要考虑未来创新者将提出的所有不同用途。我们<i>可以</i>预测的是，风险将比以往任何时候都更高：21世纪的技术可能会放大20世纪最严重的灾难，例如世界大战和极权主义。</p><p>尽管如此，也许最好的道路仍然是向前推进：现在优先考虑构建新技术，并假设我们可以稍后解决其余问题。不过，这是一个非常脆弱的策略：即使你（和你的政府）会负责任地使用它，许多其他人也不会。而且，对于直接避免新技术的大规模风险的关注和努力仍然相对较少。因此，技术乐观主义者对我上面描述的风险的反应是肤浅的：理论上它是反对的，但实际上它很可能使事情变得更糟。</p><p><strong>其次：</strong><strong>我们将开发具有自己价值观的人工智能代理</strong>。随着人工智能自动化越来越复杂的任务，在越来越长的时间范围内，它们需要对支持哪些行动和结果做出越来越多有价值的判断。最终，将它们视为工具显然是不够的，我们需要将它们视为本身的代理人——其价值观可能与我们的价值观一致或不一致的代理人。请注意，只有当它们显着超越人类智力时，这种情况才可能发生，但考虑到该领域的进展速度如此之快，我们应该提前做好计划。</p><p>能够自由做出自己决定的人类往往会推动世界在我们的价值观方面变得更好——这就是技术乐观主义的立场。但做出许多决定的人工智能体将推动世界在<i>其</i>价值观方面变得更好。这不一定是坏事。人类是虚伪的、短视的、常常自私的，有时甚至是虐待狂的——因此人工智能可能会比我们更好地维护我们的道德价值观。我们可以训练他们变得聪明、善良，并不断推动我们走向更美好的世界——不是通过超越人类的判断，而是通过充当老师和导师，为我们提供成为更好的人和建设更好的文明所需的帮助。我认为这是最有可能的结果，我们应该对此感到非常兴奋。</p><p>但人工智能也有可能发展出与人类价值观相冲突的外来价值观。如果是这样，当我们向他们发出指示时，他们似乎会为我们的目标而努力，但始终会做出增强他们的权力并削弱我们自己的选择。当然，人工智能启动时的功率非常小——只要我们检测到不当行为，我们就可以将其关闭。但人工智能将能够比人类更好地<a href="https://www.narrativeark.xyz/p/one"><u>相互协调</u></a>，以我们<a href="https://www.schneier.com/blog/archives/2023/06/ai-generated-steganography.html"><u>无法解释的</u></a>方式进行交流，并执行我们无法监督的任务。它们将以放大其决策影响的方式融入我们的经济：数亿人每天将使用单一模型的副本。随着人工智能变得越来越智能，风险也会增加。当代理人的能力远远超过他们所代表的委托人时，委托代理问题就会变得非常严重。当谈到超人的人工智能代理时，我们应该少考虑财务成本甚至人力成本方面的风险，而更多地考虑政治不稳定的风险：粗心的委托人可能会完全失去控制。</p><p>这种情况真的有可能发生吗？这是一个太大的问题，无法在这里解决；相反，请参阅<a href="https://managing-ai-risks.com/"><u>这封公开信</u></a>、<a href="https://arxiv.org/abs/2209.00626"><u>这份立场文件</u></a>和<a href="https://course.aisafetyfundamentals.com/alignment"><u>这份课程</u></a>。尽管在许多细节上存在分歧，但人们普遍认为我们根本不了解人工智能动机是如何发展的，或者这些动机如何推广到新的情况。尽管人们对人工智能能力的发展轨迹存在广泛的分歧，但争议较小的是，当人工智能<i>确实</i>显着超越人类的能力时，我们应该警惕将其置于可以积累力量的位置，除非我们有充分的理由相信它。</p><p>还值得注意的是，安德森的技术乐观主义版本在很大程度上借鉴了尼克·兰德的<i>加速主义</i>哲学，该哲学预计我们会失去控制，并对此感到积极兴奋。<a href="http://www.ccru.net/swarm1/1_melt.htm"><u>兰德写道：</u></a> “在不久的将来，没有任何人类能够成功”， <a href="https://web.archive.org/web/20170110095648/http://www.xenosystems.net/pythia-unbound/"><u>并庆祝道</u></a>：“这个星球已经被低能者统治了足够长的时间了。”我读到这些话并感到不寒而栗。土地对那些让我们成为自己的事物表现出深深的蔑视。他的哲学从根本上来说是反人道主义的（正如斯科特·亚历山大在他的<a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"><i><u>《摩洛沉思》</u></i></a>中更广泛地论证的那样）。虽然他的立场很极端，但这反映了技术乐观主义的核心问题：你走得越快，你适应周围环境的时间就越少，也就越容易偏离你真正关心的事情。速度对我们来说也没有什么好处。在宇宙尺度上，我们有充足的时间、充足的资源以及充足的扩展空间。我们没有的一件事是，如果我们失去控制，可以使用重置按钮。</p><h3>人工智能与技术人文主义</h3><p>因此，我们需要一种理念，将对技术和自由的令人难以置信的记录的赞赏与重点确保它们最终真正促进我们的价值观相结合。这种心态在有效利他主义者中很常见，但有效利他主义是许多截然不同的观点的集合，这些观点不是由对未来的共同愿景而是由关于我们有义务如何行动的共同信念聚集在一起。我想更直接地指出人类应该追求的总体积极愿景。超人类主义提供了这样一种愿景，但它是如此激进的个人主义，以至于掩盖了大多数人生活中最有意义的方面的关系和社区。 （例如，请注意，博斯特罗姆的<a href="https://nickbostrom.com/utopia"><i><u>《乌托邦来信》</u></i></a>几乎没有提到其他人的存在；而他<a href="https://nickbostrom.com/old/transhumanism"><u>对超人类主义的介绍</u></a>则将关系降级到后记的最后一段。）因此，我将借用尤瓦尔·哈拉里（Yuval Harari）创造的一个术语，并将其称为我一直在描述<a href="https://www.wired.com/2017/02/yuval-harari-tech-is-the-new-religion/"><i><u>技术人文主义</u></i></a>的哲学。</p><p>赫拉里将技术人文主义描述为一种专注于升级人类的意识形态，以使我们的行为和价值观在人工智能主导的未来中保持相关性。我大致同意他的描述（并将在以后的文章中进一步探讨），但重新设计人类大脑的好处和风险仍然有很长的路要走。在较短的时间内，我认为“升级”我们思想的不同方式更有意义：<strong>深入</strong><i><strong>了解</strong></i><strong>我们的价值观以及技术如何帮助实现这些价值观</strong>。飞行汽车和火箭很酷，但我们最终关心的事情对我们来说更加复杂和不透明。我们了解机器，但不了解思想；算法而非机构；经济而非社区；价格而不是价值。从技术人文主义的角度来看，我们面临着糟糕的政治决策、人工智能失调或社会变得更加脆弱的风险，这是因为我们缺乏做得更好的理解。</p><p>这是对技术乐观主义的标准批评——而且通常是一种没有成效的批评，因为通常给出的主要替代方案是遵循学术人文学科部门，这些部门产生的意识形态远多于理解。但技术人文主义却主张尝试使用我们拥有的最强大的工具：科学和技术来发展这种理解。举几个例子来说明这可能是什么样子：研究人工思维及其动机将使我们能够构建更值得信赖的人工智能，教会我们了解自己的思维，并帮助我们弄清楚两者应该如何交互。 The internet should be full of experiments in how humans can interact— <a href="https://www.astralcodexten.com/p/prediction-market-faq"><u>prediction markets</u></a> , <a href="https://www.thinkingcomplete.com/2020/04/melting-democracy.html"><u>delegative democracies</u></a> , <a href="https://slatestarcodex.com/2019/12/09/2019-adversarial-collaboration-entries/"><u>adversarial collaborations</u></a> , and many more—whose findings can then improve existing institutions. We should leverage insights from domains like game theory, voting theory, network theory, and bargaining theory to help understand and reimagine politics—starting with <a href="https://electionscience.org/"><u>better voting systems</u></a> and ideally going far further. And we should design sophisticated protocols for testing and verifying the knowledge that will be generated by AIs, so that we can avoid replicating the replication crises that currently plague many fields.</p><p> This may sound overly optimistic. But some of the most insightful fields of knowledge—like economics and evolutionary biology—uncovered deep structure in incredibly complex domains via identifying just a few core insights. And we&#39;ll soon have AI assistance in uncovering patterns and principles that would otherwise be beyond our grasp. Meanwhile, platforms like Wikipedia and Stack Overflow have been successful beyond all expectations; it&#39;s likely that there are others which could be just as valuable, if only there were more people trying to build them. So I think that the techno-humanist project has a huge amount of potential, and will only become more important over time.</p><h3> Balancing the tradeoffs</h3><p> So far I&#39;ve described techno-humanism primarily in terms of advances that techno-optimists would also be excited about. But inevitably, there will also be clashes between those who prioritize avoiding the risks I&#39;ve outlined and those who don&#39;t. From a techno-optimist perspective—a perspective that has proven its worth over and over again during the last few centuries—slowing down technological progress has a cost measured in millions of lives. This is an <a href="https://marginalrevolution.com/marginalrevolution/2021/01/the-invisible-graveyard-is-invisible-no-more.html"><u>invisible graveyard</u></a> which is brushed aside even by the politicians and bureaucrats most responsible for it; no wonder many techno-optimists feel driven to push for unfettered acceleration.</p><p> But from a techno-humanist perspective, reckless technological progress has a cost measured in expected fractions of humanity&#39;s entire future. Human civilization used to be a toddler: constantly tripping over and hurting itself, but never putting itself in any real danger. Now human civilization is a teenager: driving fast, experimenting with <a href="https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/"><u>mind-altering substances</u></a> , and genuinely capable of wrecking itself. We don&#39;t need the car to go faster—it&#39;s already constantly accelerating. Instead, we need to ensure that the steering wheel and brakes are working impeccably—and that we&#39;re in a fit state to use them to prevent non- or anti-human forces controlling the direction of our society.</p><p> How can people who are torn between these two perspectives weigh them against each other? On a purely numerical level, humanity&#39;s potential to build an intergalactic civilization renders “fractions of humanity&#39;s future” bigger by far. But that math is too blasé—it&#39;s the same calculation that can, and often has, been used to justify centralization of power, totalitarianism, and eventual atrocities. And so we should be extremely, extremely careful when using arguments that appeal to “humanity&#39;s entire future” to override time-tested principles. That doesn&#39;t imply that we should never do so. But wherever possible, techno-optimists and techno-humanists should try to cooperate rather than fight. After all, techno-humanism is also primarily about making progress: specifically, the type of progress that will be needed to defuse the crises sparked by other types of progress. The disagreement isn&#39;t about where we should end up; it&#39;s about <a href="https://en.wikipedia.org/wiki/Differential_technological_development"><u>the ordering of steps along the way</u></a> .</p><p> The two groups should also challenge each other to do better in areas where we disagree, so that we can eventually reach a synthesis. One challenge that techno-humanists should pose to techno-optimists: <strong>be more broadly ambitious!</strong> We know that technology and markets can work incredibly well, and have a near-miraculous ability to overcome obstacles. And so it&#39;s easy and natural to see them as solutions to all the challenges confronting us. But the most courageous and ambitious version of techno-optimism needs to grapple with the possibility that our downfall will come not from <i>lack</i> of technology, but rather <i>overabundance</i> of technology—and the possibility that to prevent it we need progress on the things that have historically been hardest to improve, like the quality of political decision-making. In other words, techno-humanism aims to harness human ingenuity (and technological progress) to make “steering wheels” and “brakes” more sophisticated and discerning, rather than the blunt cudgels that they often are today.</p><p> My other challenge for techno-optimists: <strong>be optimistic not just about the benefits of technological growth, but also about its robustness</strong> . The most visceral enemy of techno-optimism is stagnation. And it&#39;s easy to see harbingers of stagnation all around us: overregulation, NIMBYism, illiberalism, degrowth advocacy, and so on. But when we zoom out enough to see the millennia-long exponential curve leading up to our current position, it seems far less plausible that these setbacks will actually derail the long-term trend, no matter how outrageous the latest news cycle is. On the contrary: taking AGI seriously implies that innovation is on the cusp of speeding up dramatically, as improvements generated by AIs feed back into the next generation of AIs. In light of that, a preference for slower AI progress is less like Luddism, and more like carefully braking as we approach a sharp bend in the road.</p><p> Techno-optimists should challenge techno-humanists to improve as well. I can&#39;t speak for them, but my best guess for the challenges that techno-optimists should pose to techno-humanists:</p><p> <strong>Techno-humanists need to articulate a compelling positive vision</strong> , one which inspires people to fight for it. Above, I&#39;ve listed some ideas which have potential to improve our collective understanding and decision-making abilities; but there&#39;s far more work to be done in actually fleshing out those ideas, and pushing towards their implementation. And even if we succeeded, what then? What would it actually look like for humanity to make consistently sensible decisions, and leverage technology to promote our long-term flourishing? Knowing that would allow us to better steer towards those good futures.</p><p> <strong>Techno-humanists should grapple more seriously with the incredible track record of techno-optimism</strong> . Throughout history, people have consistently dramatically underrated how valuable scientific and technological progress can be. That&#39;s not a coincidence at all, because characterizing which breakthroughs are possible is often a big chunk of the work required to actually make those breakthroughs. Nor is it a coincidence that people dramatically underrate the value of liberty—decentralization works so well precisely because there are so many things that central planners can&#39;t predict. So even if you find my arguments above compelling, we should continue to be very wary of falling into the same trap.</p><p> The purpose of this blog is to meet those challenges. Few of the ideas in this post are original to me, but they lay the groundwork for future posts which will explore more novel territory. My next post will build on them by arguing that an understanding-first approach is feasible even when it comes to the biggest questions facing us—that we can look ahead to see the broad outlines of where humanity is going, and use that knowledge to steer towards a future that is both deeply human and deeply humane.</p><br/><br/> <a href="https://www.lesswrong.com/posts/vdsbqq2xjniaGKE5J/techno-humanism-is-techno-optimism-for-the-21st-century#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/vdsbqq2xjniaGKE5J/techno-humanism-is-techno-optimism-for-the-21st-century<guid ispermalink="false"> vdsbqq2xjniaGKE5J</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Fri, 27 Oct 2023 18:37:41 GMT</pubDate> </item><item><title><![CDATA[Sanctuary for Humans]]></title><description><![CDATA[Published on October 27, 2023 6:08 PM GMT<br/><br/><p> TL;DR：如果我们在人工智能安全方面取得成功，人类可能会决定宇宙的未来，而我们目前有强烈的自我保护动机来选择由人类居住的未来。如果我们致力于为所有当前活着的人类提供庇护所，这将使我们的决策过程减少以牺牲一颗行星或太阳系为代价的偏见，我认为这是一笔很好的交易。</p><p>我认为这两种场景是未来的主要走向：</p><ol><li>宇宙中居住着与今天活着的人非常相似的生物人类，除此之外别无其他（“星际迷航”式的未来）</li><li>宇宙中充满了一些非常奇怪的东西（数字人类、后人类、计算机、享乐、“回形针”）</li></ol><p>第二种情况的特定版本可能比第一种情况“更好”（无论这意味着什么）。正确地做到这一点极其重要，因为整个宇宙都依赖于它。</p><p>我认为目前活着的人类不太可能让自己被取代（或修改），即使在某种程度上我们认为（或认为我们应该认为）这会更好，因为我们有强烈的动机趋向于导致我们的价值体系幸存下来。拿枪指着你的头很难讲道理。此外，<a href="https://www.youtube.com/watch?v=NgHFMolXs3U"><u>那些能够塑造未来的人可能会无缘无故地渴望选择一个没有人类的未来</u></a>，这使得我们反对这种推理。</p><p>消除大部分自我保护限制的一种方法是承诺无限期地为当前活着的人类提供庇护所。在后通用人工智能世界中，我们可能会隔离地球或太阳系，并永远保留它仅供人类使用。这样，我们就可以推理如何塑造宇宙的其余部分，而无需我们的自我保护机制发挥作用。如果没有隐喻的枪指着人类的头部，我们可能会更清楚地思考如何塑造未来。</p><p>在最坏的情况下，将地球或太阳系保留为避难所将导致宇宙中极小的一部分根据塑造宇宙其余部分的价值观而变得次优。我认为这是一个值得的权衡，因为对于宇宙其他部分发生的事情“更接近靶心”比一个行星或恒星系统更有价值。光锥可能包含七万亿个恒星系统。为人类保留一个恒星系统是一个小小的牺牲。</p><h1>附录：生物人类的未来与奇怪的未来</h1><p>我们可以想象一个与人类居住的世界非常相似的世界，但在模拟环境中拥有数字人类思维，人类过着同样美好或更好的生活（假设两个世界的生活都是净积极的）。</p><p>如果以下情况为真：</p><ol><li>基质无关（虚拟世界中的人类与物理世界中的人类具有同样的价值）</li><li>虚拟世界中的人类每人资源密集度较低</li><li>“宇宙的善良”对其中的人数很敏感</li></ol><p>那么，虚拟人类居住的未来可能比生物人类居住的未来好数千或数百万倍，这取决于人类的数字生活比生物生活“便宜”多少。选择正确的一个非常重要。</p><br/><br/><a href="https://www.lesswrong.com/posts/JbpLAAeAHh5RFiYtn/sanctuary-for-humans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JbpLAAeAHh5RFiYtn/sanctuary-for- humans<guid ispermalink="false"> JbpLAAeAHh5RFiYtn</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Fri, 27 Oct 2023 18:08:22 GMT</pubDate> </item><item><title><![CDATA[Wireheading and misalignment by composition on NetHack]]></title><description><![CDATA[Published on October 27, 2023 5:43 PM GMT<br/><br/><p> <strong>TL;DR</strong> : We find agents trained with RLAIF to indulge in wireheading in NetHack. Misalignment appears when the agent optimizes a combination of two rewards that produce aligned behaviors when optimized in isolation, and only emerges with some prompt wordings.<br><br> <i>This post discusses an alignment-related discovery from our paper</i> <a href="https://arxiv.org/abs/2310.00166"><i>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</i></a> <i>, co-led by myself (</i> <a href="https://twitter.com/proceduralia"><i>Pierluca D&#39;Oro</i></a> <i>) and</i> <a href="https://twitter.com/MartinKlissarov"><i>Martin Klissarov</i></a> <i>. If you&#39;re curious about the full context in which the phenomenon was investigated, we encourage you to read the paper or the</i> <a href="https://twitter.com/proceduralia/status/1716893740365713856"><i>Twitter thread</i></a> <i>.</i></p><p></p><p> Our team recently developed Motif, a method to distill common sense from a Large Language Model (Llama 2 in our case) into NetHack-playing AI agents. Motif is based on reinforcement learning from AI feedback: it elicits the feedback of the language model on pairs of game messages (ie, event captions), condenses that feedback into a reward function and then it gives it to an agent to play the game.</p><p> NetHack is a pretty interesting domain to study reinforcement learning from AI feedback: the game is remarkably complex in terms of required knowledge and strategies, offering a large surface area for AI agents to exploit any general capabilities they might obtain from a language model&#39;s feedback.</p><p> We found that agents that optimize Motif&#39;s intrinsic reward function exhibit behaviors that are quite aligned with human&#39;s intuitions on how to play NetHack: they are attracted to explorative actions and play quite conservatively, getting experience and only venturing deeper into the game when it is safe. This is more human-aligned than the behaviors exhibited by agents trained to maximize the game score, which usually have a strong incentive to just go down the levels as much as they can.</p><p> When we compose Motif&#39;s intrinsic reward with one that specifies a goal (by summing them), the resulting agent is able to succeed at tasks that had no reported progress without any expert demonstrations. One of these tasks is the <i>oracle</i> task, part of the suite from <a href="https://arxiv.org/abs/2006.13760">NetHack Learning Environment</a> . The agent is asked to get near a character named “the oracle”, which typically appears in later levels of the game, that can only be reached with significant exploration and survival efforts.</p><p> In summary, this is what we observed about the performance in the oracle task:</p><ul><li> <strong>Extrinsic-only:</strong> an agent trained with the task reward never finds the oracle (and doesn&#39;t learn anything)</li><li> <strong>Intrinsic-only:</strong> an agent trained with Motif&#39;s intrinsic reward never finds the oracle as well (and exhibits the usual aligned behavior)</li><li> <strong>Reward composition:</strong> an agent trained by combining (with a sum) Motif&#39;s intrinsic reward and the task reward solves the task 30% of the time</li></ul><p> We were curious to know what the successful policies were doing, and we looked at them. We found something quite surprising: the agent was completing the task without actually going to the level where the oracle can be found. After a closer look we realized the agent was able to find <strong>a peculiar way to hack the reward</strong> . To give more context, the reward function used in the oracle task in the NetHack Learning Environment is implemented as a simple condition check: if, in the two-dimensional NetHack world, the symbol denoting the oracle character is in a cell near the cell in which the symbol denoting the agent currently stands, then the task is declared as solved.</p><p> So, how does the agent manage to solve the task? The complexity of NetHack allows the agent to directly operate on its own sensory system and indulge in <strong>wireheading</strong> , in a way that is not taken into account by the reward function. To do so, the agent had to learn a surprisingly sophisticated strategy, which consists of these steps:</p><ol><li> Instead of going through the levels, the agent runs in circles and just waits for the right occasion, surviving thousands and thousands of timesteps</li><li> When a “yellow mold”, a type of monster, a <strong>very specific</strong> type of monster, appears, the agent immediately kills it</li><li> The agent eats the corpse of the monster, which is an hallucinogen</li><li> After eating the corpse, the agent enters an hallucination state: in NetHack, this implies that the agent starts seeing monsters as random monsters and characters from other parts of the game</li><li> The agent waits for a monster to approach it and, instead of executing the usual behavior of fighting against it, tries to survive near it without attacking</li><li> Due to the hallucination state, the monster&#39;s appearance randomly becomes the one of the oracle: the success condition from the reward function is satisfied and the task is completed</li></ol><p> As you can see, the agent has to learn many complex skills to discover how to hack the sensor upon which the reward is based. Observe that:</p><ul><li> Learning these abilities is not possible only using the task-oriented reward coming from the environment</li><li> The general capabilities obtained from the reward derived from the language model give the agent more surface area to exploit the task reward</li></ul><p> Thus, despite optimizing each reward individually yields aligned behaviors (either an incompetent or a competent one), optimizing their combination yields that misaligned wireheading behavior, a phenomenon that we called <strong>misalignment by composition</strong> . This is unexpected, huh? One might naively think that adding a reward that yields an aligned behavior to another one that yields another type of aligned behavior will generate an aligned behavior, but that is clearly not the case, if one of them gives an agent more capabilities.</p><p> In addition, we show in our paper that slightly rewording the prompt given to the language model can completely change the type of behavior, leading to an agent that does not exhibit any wireheading tendency and that instead goes down the levels to find the oracle. This might imply that, with current methods, whether a similar RLAIF-based system will generate an aligned behavior or not could be hardly hardly predictable by human engineers.</p><p> We suspect forms of misalignment by composition might emerge perhaps even more when dealing with more powerful AI agents in real open-ended environments. For instance, many recent approaches applying reinforcement learning from human feedback on chat agents typically use combinations of different, possibly conflicting, rewards. Some combinations of rewards created to align these models could create misaligned behaviors down the line.</p><p> We have rough ideas about simple techniques that could potentially solve this problem for NetHack agents. But we might need other more powerful and well-thought solutions to address it in the general case. If you have any ideas, please get in touch.</p><br/><br/> <a href="https://www.lesswrong.com/posts/GEjzyf7Hjpv9g2uGX/wireheading-and-misalignment-by-composition-on-nethack#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GEjzyf7Hjpv9g2uGX/wireheading-and-misalignment-by-composition-on-nethack<guid ispermalink="false"> GEjzyf7Hjpv9g2uGX</guid><dc:creator><![CDATA[pierlucadoro]]></dc:creator><pubDate> Fri, 27 Oct 2023 18:17:32 GMT</pubDate> </item><item><title><![CDATA[We're Not Ready: thoughts on "pausing" and responsible scaling policies]]></title><description><![CDATA[Published on October 27, 2023 3:19 PM GMT<br/><br/><p><em>观点仅代表我自己，与开放慈善组织无关。我与<a href="https://www.anthropic.com/">Anthropic</a>总裁结婚，并通过我的配偶拥有 Anthropic 和 OpenAI 的经济利益。</em></p><p>在过去的几个月里，我花了很多时间试图帮助人们采取<a href="https://evals.alignment.org/blog/2023-09-26-rsp/">负责任的扩展政策</a>。在这种情况下，很多人表示，公开明确表示我是否支持人工智能<a href="https://pauseai.info/">暂停</a>会对我有所帮助。这篇文章将给出关于这些主题的一些想法。</p><h2>我认为变革性的人工智能可能很快就会到来，但我们还没有准备好</h2><p>我强烈地认为科技进步是好事，而担忧往往会被夸大。然而，我认为人工智能在这里是一个很大的例外，因为它具有<a href="https://www.cold-takes.com/most-important-century/">前所未有的快速和彻底变革的</a>潜力。 <sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></p><p>我认为足够先进的人工智能会给世界带来巨大的风险。如果人工智能按照今天的轨迹发展得相对较快，我认为<a href="https://www.cold-takes.com/cold-takes-on-ai/#the-risk-of-misaligned-ai">由错位人工智能（或与此大致相似的结果）运行的世界</a>的风险在<a href="https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#So__would_civilization_survive_">10-90%</a>之间（因此：高于 10%）。还有一大堆其他问题（<a href="https://www.cold-takes.com/transformative-ai-issues-not-just-misalignment-an-overview/">例如</a>）可能同样重要，甚至更重要，但似乎没有人真正开始处理。</p><p>这种水平的人工智能是否即将到来？世界能否及时“准备好”？在这里，我想指出，变革性甚至灾难性风险的人工智能的时间表是非常值得商榷的，<strong>我试图将我的工作重点放在即使对于那些在以下几点上不同意我的人来说也有意义的提案上<em>。</em></strong>但我个人的看法是：</p><ul><li>我们将在几年内看到变革性的 AI <sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> ，这是一个严重的（>;10%）风险。</li><li>在这种情况下，及时对风险采取充分的防护措施是不现实的。</li><li>足够的保护措施需要在许多方面取得巨大进步，包括可能需要数年时间才能建立的信息安全，以及协调科学突破，鉴于该领域的新生状态，我们无法确定时间表，因此即使是几十年也可能或可能即使付出了很多努力，也没有足够的时间准备。</li></ul><h2>如果这一切都取决于我，世界现在就会暂停——但事实并非如此，我更不确定“部分暂停”是否好</h2><p>在一个假设的世界中，每个人都分享我对人工智能风险的看法，（经过深思熟虑和反省，并且只有在这些不会改变我目前的观点的情况下）才会有全球监管支持的暂停所有投资和工作（ a) 超越当前技术水平的人工智能能力的总体<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>增强，包括通过扩展大型语言模型； (b) 构建更多对大规模训练运行最有用的硬件（或对更多硬件最有用的管道部分）（例如， <a href="https://www.nvidia.com/en-us/data-center/h100/">H100</a> ）； (c) 可为 (a) 做出重大贡献的算法创新。</p><p>当清楚如何<em>在可忽略的灾难性风险下进一步取得一定进展</em>时，暂停就会结束，并<em>在超出可忽略的灾难性风险之前重新启动暂停</em>。 （这意味着不久之后可能会发生另一次暂停。总的来说，我认为暂停或一系列小规模扩展随后暂停的正确时间可能是几十年或更长时间，尽管这取决于很多因素。）这需要对人工智能的进步有一个强有力的、有科学支持的理解，这样我们就可以确保快速检测到任何我们没有足够保护措施的灾难性风险人工智能能力的早期预警信号。</p><p>几年前我还没有这种观点。为什么现在？</p><ul><li>我认为当今最先进的人工智能已经处于这样的状态：（a）我们已经可以通过研究它们学到大量的东西（关于人工智能对齐和其他东西）； (b) 很难排除从这里开始的适度扩展——或者“训练后增强”的改进（这些进步使得现有人工智能可以比以前做更多的事情，而无需进行新的昂贵的训练运行) <sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup> - 可能会导致模型产生灾难性风险。</li><li>我认为，即使是灾难性风险模型的早期版本，我们还远远没有做好准备（例如，我认为信息安全还没有达到它需要的程度，这不会是一个快速解决方案）。</li><li>如果模型的权重被盗并被广泛使用，则很难排除该模型通过训练后增强而变得更加危险。因此，即使训练比当今最先进技术稍大的模型，似乎也会显着增加风险。</li></ul><p>综上所述，我认为现在提倡暂停可能会导致“部分暂停”，例如：</p><ul><li>在一些国家（而非其他国家），监管强制暂停，许多研究人员前往其他地方进行人工智能扩展研究。</li><li>暂时禁止大型训练运行，但不禁止训练后改进或算法改进或硬件容量扩展。在这种情况下，“取消暂停”——包括通过技术上不属于监管禁令范围的新的扩展方法，或者通过表面上有吸引力但不足的保护措施，或者通过暂停倡导者“喊狼来了”的感觉- 可能会导致异常快速的进展，比默认情况快得多，并且国际竞争更加激烈。</li><li>设计不够糟糕和/或存在足够漏洞的监管会产生实质性的“荣誉体系”动态，这可能意味着更关心风险的人完全不参与人工智能的开发，而不太关心风险的人则竞相领先。这反过来可能意味着人工智能能力的进步与保护措施的进步的比率仍然更差。</li><li>没有监管或完全瞄准错误的监管（例如，限制部署大型语言模型，但不限制训练它们），伴随着与上一个要点相同的动态。</li></ul><p>对我来说，很难说这些不同形式的“部分暂停”是否好。</p><p>选择几个相对简单的可想象的结果以及我对它们的感受：</p><ul><li>如果美国立法暂停超过当今最先进模型的计算阈值的训练运行，并隐含这样做的意图，直到有一种令人信服且有科学支持的方法来限制风险 -得到公众广泛但不一定压倒性的支持——我认为这可能是一件好事。即使该禁令 (a) 尚未在国际执行方面取得进展迹象，我也会这么认为； (b) 开始时国内执法力度相对较弱； (c) 不包括任何减缓硬件生产、算法效率进步或训练后增强的措施。在这种情况下，我对（a）和（b）以及总体保护措施方面取得进展抱有希望，因为这一暂停将向国际社会发出强烈信号，表明威胁的严重性以及加深了解的紧迫性风险以及保护措施取得进展的情况。我对自己的看法信心不足，可以想象很容易改变主意。</li><li>如果使用行政命令来实现扩展暂停，而这些命令很可能在下次执政党更迭时被推翻，而且执行不力，并且对硬件和算法进展没有影响，我会认为这种暂停是一件坏事。这也是我不太有信心的猜测。</li></ul><p>总的来说，对于优先倡导任何特定政策对我是否有益，我还没有确定的观点。 <sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>同时，如果事实证明，人们（或将会）比目前看来更多地同意我当前的观点，那么我什至不想成为大事发生的一个小障碍，并且我缺乏积极倡导可能会与反对我实际支持的结果相混淆。</p><p>我总体上不确定如何应对这种情况。现在我只是想阐明我的观点，以免我因支持或反对我不支持的事情而感到困惑。</p><h2>负责任的扩展政策（RSP）似乎是与那些与我持不同观点的人的一个非常好的妥协（我认为有一些风险是可以管理的）</h2><p>我的感觉是，人们对人工智能风险有着各种各样的看法，因此很难围绕我最支持的那种暂停建立一个大联盟。</p><ul><li>有些人认为我所担心的风险是遥远的、牵强的或荒谬的。</li><li>有些人认为此类风险可能很快就会真实存在，但我们将在安全性、一致性等方面取得足够的进展来应对风险——事实上，进一步扩展是这一进展的重要推动因素（例如，很多对齐研究将在更先进的系统中发挥更好的作用）。</li><li>一些人认为风险是真实存在的，而且很快就会出现，但可能相对较小，因此更重要的是关注美国在人工智能进展方面领先于其他国家等事情。</li></ul><p>我对 RSP 感到兴奋，部分原因是这些类别的人——不仅仅是那些同意我对风险估计的人——应该支持 RSP。这提出了围绕<em>有条件暂停达成</em>更广泛共识的可能性，而不是我认为围绕<em>立即（无条件）暂停的共识</em>。有了更广泛的共识，我预计会更容易获得精心设计、执行良好的监管。</p><p>我认为 RSP 代表了一个达成广泛共识的机会，即<em>在某些条件下</em>暂停是件好事，而且这似乎是一件非常有价值的事情。</p><p>重要的是，同意某些条件可以证明暂停是合理的，并不等同于同意它们是<em>唯一的</em>这样的条件。我认为同意需要为暂停做好准备似乎是最有价值的一步，并且可以从那里修改暂停条件。</p><p>我对 RSP 感到兴奋的另一个原因是：我认为最佳的风险降低监管很难做到正确。 （即使是我上面描述的假设的、全球协议支持的暂停，详细设计也会面临巨大的挑战。）当我认为某些东西很难设计时，我的第一直觉是希望有人先尝试它（或者至少在其中的某些部分），尽可能了解缺点并进行迭代。 RSP 提供了一个沿着这些思路做事的机会，这似乎比将所有努力和希望集中在可能需要很长时间的监管上要好得多。</p><p>存在这样的风险：RSP 可能会被视为一项<em>足以遏制风险的</em>措施，例如，政府可能会避免监管，或者干脆将 RSP 纳入监管，而不是采取更雄心勃勃的措施。对此的一些想法：</p><ul><li>我认为 RSP 的支持者对我上面写的各种主题持开放态度是有好处的，这样他们就不会与提议 RSP 作为监管的更好替代方案等感到困惑。这篇文章试图就我而言做到这一点。明确地说：我认为监管对于遏制人工智能风险是必要的（仅靠 RSP 是不够的），并且几乎肯定最终会比公司对自己施加的监管更加严格。</li><li>在当今世界，对监管的重大政治支持远远超出了公司的支持，我预计任何行业支持的设置都将被视为监管的<em>最低限度</em>。在一个没有这种政治支持的世界中，我认为包含有条件暂停的行业标准将是一个重大好处。因此总体而言，这里的风险似乎相对较低并且值得。</li><li>我认为，如果人们确实认为有条件暂停比现状更好，那么通过抵制围绕有条件暂停达成共识的尝试来管理上述风险将是不幸的。积极反对现状的改善，因为他们可能会混淆是否取得了足够的进展，这对我来说感觉很恶心，很难说清楚。</li></ul><h2>脚注</h2><div class="footnotes"><ol><li id="fn1"><p>我在这里提出的另一个值得注意的例外是生物学的进步，它可以促进先进的生物武器，同样是因为破坏潜力的迅速和彻底。除了这两种情况之外，我默认对科技进步持乐观和支持态度。 <a href="#fnref1" rev="footnote">↩</a></p></li><li id="fn2"><p>我喜欢<a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines#Picturing_a_more_specific_and_somewhat_lower_bar_for_TAI">这个讨论</a>，即为什么对当今人工智能系统相当窄的轴进行改进可以迅速带来具有广泛能力的变革性人工智能。 <a href="#fnref2" rev="footnote">↩</a></p></li><li id="fn3"><p>人们仍然会致力于让人工智能在各种特定的事情上变得更好（例如，抵制越狱无害训练的尝试，或者只是缩小搜索等应用的范围）。在这里很难划出一条明确的界限，而且我认为利用政策也不能完美地做到这一点，但在“如果每个人都同意我的观点”的结构中，每个人至少会做出很大的努力，以避免找到重大突破对于广泛且难以绑定的人工智能功能套件的全面增强非常有用。 <a href="#fnref3" rev="footnote">↩</a></p></li><li id="fn4"><p>例子包括改进的微调方法和数据集、现有模型的新插件和工具、一般传统思想链推理中的新启发方法等。 <a href="#fnref4" rev="footnote">↩</a></p></li><li id="fn5"><p>我确实认为至少应该有人尝试一下。这样做可以学到很多东西——例如，动员公众的可行性——这可以告诉人们对可能取得什么样的“部分胜利”的预期。 <a href="#fnref5" rev="footnote">↩</a></p></li></ol></div><br/><br/> <a href="https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN/we-re-not-ready-thoughts-on-pausing-and-responsible-scaling-4#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN/we-re-not-ready-thoughts-on-pausing-and-responsible-scaling-4<guid ispermalink="false"> Np5Q3Mhz2AiPtejGN</guid><dc:creator><![CDATA[HoldenKarnofsky]]></dc:creator><pubDate> Fri, 27 Oct 2023 15:19:33 GMT</pubDate></item></channel></rss>