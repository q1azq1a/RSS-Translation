<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 24 日星期日 06:14:14 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The Sugar Alignment Problem]]></title><description><![CDATA[Published on December 24, 2023 1:35 AM GMT<br/><br/><p><i>在一个刚刚发现糖的平行世界里。</i></p><p>爱丽丝：嘿，你们听说过他们发现的让食物味道更好的新东西吗？</p><p>鲍勃：是的，我想是的。是粉状的，白色的，对吗？</p><p>爱丽丝：是的。</p><p>卡罗尔：它实际上是颗粒状的，就像微小的鹅卵石。它被称为糖。</p><p>鲍勃：啊，好的。</p><p>卡罗尔：这与多年前关于鲜味的发现很相似。你知道，味觉的一个方面，我们认为事物是美味的和“肉味的”。</p><p>爱丽丝：哦，我记得了。我喜欢它。我现在很喜欢营养酵母。还有其他“鲜味炸弹”，如酱油和味噌酱。</p><p>鲍勃：是的，我也是。我很高兴能用糖做同样的事情。到目前为止，我每天早上都会将它添加到我的咖啡中，效果非常棒。它给了它这种有趣的小刺激。它也能淡化咖啡的苦味，但以一种很好的方式补充了咖啡的苦味。并且以与牛奶不同的方式。这很微妙。这太好了。</p><p>爱丽丝：哦，是的，当然。我喜欢把它加在茶里。</p><p>鲍勃：除了可能有点违反直觉的事情之外，它也很棒。就像在爆米花上面一样。旁边的黄油和盐可能看起来很奇怪，但我不知道，我认为它非常好。</p><p>卡罗尔：这是有道理的。我听说在一些亚洲国家他们会在炒饭和其他炒菜上撒一些。</p><p>爱丽丝：嗯，这实际上听起来有点奇怪。炒饭的那种糖味？</p><p>卡罗尔：是的，也许吧。我正在观看有关它的 YouTube 视频，那家伙说它是如何更加微妙的添加的。就像，你并没有真正尝到糖本身的味道，但炒饭有一些不同的<i>东西</i>，很难用手指来形容，但味道真的很好。</p><p>爱丽丝：啊，我明白了。</p><p>鲍勃：但我也听说他们正在尝试在某些东西中添加<i>大量</i>糖。好像有一种东西叫冰淇淋。它基本上是酸奶，但里面含有大量的糖，而且更冷，几乎是冷冻的。前几天我尝试了一下，效果<i>非常</i>好。我再也不敢喝酸奶了</p><p>丹：这是件好事吗？</p><p>爱丽丝：哦。嘿丹。也很高兴见到你。</p><p>丹：抱歉，我刚进来，但我听说了糖和冰淇淋的事，我有点担心。</p><p>爱丽丝：担心吗？担心什么意思？我没有听说有任何负面影响。它应该只是让食物味道更好。</p><p>丹：你做过研究吗？</p><p>爱丽丝：研究？</p><p>鲍勃：我认为丹的意思是，如果你仔细观察，也许在食物中添加糖有一些缺点。我听说它会让你在事后感到昏昏欲睡。这似乎取决于您的年龄和健康水平。年龄较大、健康状况较差的人在食用添加糖的食物后一个小时左右可能会感到有点懒惰。但它不会持续太久。对于像我们这样年轻健康的人来说，应该没问题。</p><p>爱丽丝：嗯。我经常在茶中加入它，但从来没有感到任何昏昏欲睡的感觉。</p><p>鲍勃：是的，只有当你拥有很多东西时才会发生这种情况。早上将其添加到茶中应该完全没有问题。但就像冰淇淋一样，如果你吃了很多，吃完之后你可能会感到有点昏昏欲睡。</p><p>爱丽丝：嗯，好的。听起来不错。</p><p>卡罗尔：我确实认为丹提出了一个很好的观点。重要的是要考虑我们放入体内的东西的副作用。</p><p>爱丽丝：完全可以。这还算公平。</p><p>丹：副作用是我想到的一部分，但是是的。</p><p>鲍勃：哦。你还想什么？</p><hr><p>丹：嗯，我真的不知道。我只知道这是一个新发现——甚至可以称之为新技术——而且我们很清楚，技术既可以用来做好事，也可以用来做坏事。</p><p>爱丽丝：哦，又是这个。</p><p>卡罗尔：哈哈，我听到了，丹，但我认为我们都比你更信任和开放新技术。</p><p>鲍勃：是的，伙计。你甚至没有智能手机。或任何社交媒体帐户。</p><p>丹：但你确实同意技术可以用来做好事，也可以用来做坏事，对吧？</p><p>鲍勃：当然。</p><p>卡罗尔：是的，我想任何人都会同意这一点。</p><p>爱丽丝：我是说，我想。但就像很多时候，缺点基本上可以忽略不计。就像当我打开手机调出谷歌地图时，我知道你认为它会阻止我发展空间智能或其他什么，但我不知道，我只是认为这对我来说是一件不重要的事情，它本质上是“循环”为零”。</p><p>丹：当谷歌地图变得无处不在——它已经做到了——并且拥有几乎每个人在哪里以及每个人在任何时候都要去哪里的数据时，会发生什么？</p><p>爱丽丝：我的意思是，我个人并不关心。</p><p>鲍勃：不过，这是一个有效的观点。即使你不关心爱丽丝，其他人也会关心。即使对于你来说，当谷歌将这些数据出售给一些在第一次与你约会时出现并且碰巧知道你每周四下午去 Tea My Tea 的怪人时，会发生什么。</p><p>爱丽丝：好吧，是的……那会很令人毛骨悚然。</p><p>卡罗尔：更不用说政府间谍活动了。</p><p>爱丽丝：政府间谍活动？啊？</p><p>卡罗尔：我听说过一些奇怪的东西，比如“预测性警务”，如果你在城镇的“贫民窟”地区呆了太多时间，警察可能会获得这些信息，下次对你就不那么仁慈了你被拦在路上了。</p><p>爱丽丝：天啊。这绝对是太糟糕了。</p><p>丹：哦，是的。它还可以出售给潜在的雇主。我听说过一些“隐形初创公司”，他们基本上会获取这些数据，将其包装起来并在其上系上蝴蝶结，然后出售这些“人力资源招股说明书”报告，这些报告根据该位置预测求职者的“价值”来自谷歌地图的数据。</p><p>卡罗尔：天哪。但是……他们是如何推断出这一点的呢？他们如何将位置数据映射到工作绩效？他们甚至可以访问哪些工作绩效数据？</p><p>丹：这些都是非常好的问题，卡罗尔。</p><p>爱丽丝：这看起来确实有点粗略。</p><p>鲍勃：是的。如果你们不介意的话，让我们把话题带回糖吧。现在我们已经讨论了所有这些并了解了这个背景，我有兴趣重新评估我对糖的看法。你们有什么感想？</p><p><i>大家都点头。</i></p><hr><p>鲍勃：好的。所以，嗯，我想让我们看看我们是否可以考虑一下 Dan 所说的，关于技术如何被用来做好事和做坏事。我们已经看到了许多糖的用途。那么它如何被用来做坏事呢？</p><p>爱丽丝：这对我来说很有意义。虽然我只是不明白它如何被用来做坏事。它只是撒在食物上的东西。谷歌地图和我得到的位置数据。就像，这是更严重的事情，我知道你可以如何用它来做坏事。但不是糖。</p><p>卡罗尔：嗯，没那么快。我回想起鲍勃之前说过的，他再也不会吃酸奶了，而是用这种新的冰淇淋来代替。</p><p>丹：天哪。</p><p>鲍勃：这是“冰淇淋”。但是，是的...现在我正在重新考虑这一点。我以前每天早上早餐都吃酸奶。如果我用冰淇淋这样做，我不知道，也许这会对我的健康或其他方面产生长期不良影响。</p><p>卡罗尔：这看起来确实有可能。我们确实知道，从长远来看，反式脂肪等物质对您的健康确实非常有害，尽管没有立即的副作用。这只是几十年后才会出现的事情。</p><p>爱丽丝：嗯。好的。我看到。但是，就像反式脂肪一样，政府研究后发现它不好，并与公众进行了沟通，人们避免食用含有反式脂肪的食物，政府甚至在学校等地方禁止使用反式脂肪。我们都很好。</p><p>鲍勃：是的。有时我们发现新技术会产生不良影响，并作为一个社会成功地避免它们，但这并不总是发生。</p><p>丹：讲道吧，兄弟。</p><p>鲍勃：等等，我想我明白你在那里做了什么。宗教？</p><p>丹：是的。</p><p>爱丽丝：<i>叹息。</i>好吧，我想你对那个丹的看法是对的。宗教不是很好……但它已经广泛传播了数千年。</p><p>卡罗尔：那么，丹：对于糖，我们该怎么办？</p><hr><p>丹：嗯，我喜欢摩门教徒采取的方法。</p><p>爱丽丝：什么？！所以你希望我们拒绝所有现代技术？</p><p>卡罗尔：是的，我同意爱丽丝这一点。这似乎有点极端。我确信当你需要抗生素时你就不太像摩门教徒了。</p><p>丹：我认为你不明白摩门教的做法是什么。他们并不拒绝采用技术。他们只是非常缓慢、非常小心地这样做。比西方世界慢得多。</p><p>卡罗尔：这似乎不合理。我们可以进行实验。做科学。我们不必等待数十年才能获得有关影响的良好数据。</p><p> Dan：当然，对于一阶效应。但二阶效应又如何呢？三阶？四阶？我对我们预测这些事情的能力不太有信心。我感觉更舒服地观察事情在实践中的<i>实际</i>表现。但这既不在这里也不在那里。这里存在一个难以克服的协调问题。</p><p>鲍勃：嗯？</p><p>丹：假设我被任命为美国总统或其他什么东西，并采用摩门教的方法，非常非常缓慢地采用糖的使用。比如说，50 多年了。这不会阻止其他国家采用它。</p><p>鲍勃：然后会发生什么？</p><p> Dan：其他国家也采用它。人们看到它有多美味。他们每天早上都吃冰淇淋当早餐。美国人去欧洲度假，每天早上也吃冰淇淋作为早餐。他们回家后告诉朋友这件事。消息传开。民众需要糖。</p><p>鲍勃：嗯，在这个假设中你是总统。只要告诉他们不。</p><p>丹：我的四年任期结束后会发生什么？</p><p>鲍勃：哦，是的。好吧，也许凭借你所有的政治权力，你可以让其他政客相信糖是多么危险，并说服他们所有人也同意对糖施加这些限制。</p><p>丹：当有异议时会发生什么？</p><p>鲍勃：持不同政见者？</p><p>丹：是的。假设我说服所有政客都这样做，然后一个人出现并参加竞选活动，承诺让美国人获得他们应得的糖。</p><p>鲍勃：嗯，我明白了。我想让每个人都参与进来是很困难的。</p><p>丹：没错。</p><p>爱丽丝：这对我来说很有意义。即使这在美国有效，其他国家又如何呢？如果糖真的这么好吃，即使美国成功禁止它，人们也许也会搬到其他有糖的国家。毕竟，当政府在禁酒令期间试图禁酒时，美国人相当不满。</p><p>卡罗尔：是的，我也有道理。至少，这只是一场巨大的艰苦战斗。现在我在想，对于我和我的科学家朋友来说，发表我们发现的一切是多么明智。谁敢说这些发现会产生积极的影响？</p><p>丹：也许吧。对我来说，最有希望的途径是“调整”糖。研究它到底是什么，并对其进行设计，以确保它将带来净积极作用，而不是净损害。</p><br/><br/><a href="https://www.lesswrong.com/posts/MmreefkH8cRSky4RE/the-sugar-alignment-problem#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/MmreefkH8cRSky4RE/the-sugar-alignment-problem<guid ispermalink="false"> MmreefkH8cRSky4RE</guid><dc:creator><![CDATA[Adam Zerner]]></dc:creator><pubDate> Sun, 24 Dec 2023 01:35:20 GMT</pubDate> </item><item><title><![CDATA[A Crisper Explanation of Simulacrum Levels]]></title><description><![CDATA[Published on December 23, 2023 10:13 PM GMT<br/><br/><p>我读过之前关于<a href="https://www.lesswrong.com/tag/simulacrum-levels">Simulacrum Levels</a>的文章，并且我看到人们对它们的工作方式表达了一些困惑。当我第一次遇到这个概念时，我自己也有过一些困惑，我认为它们是由于定义不够<i>清晰</i>造成的。</p><p>现有的解释似乎并没有为拟像级别如何存在提供适当的自下而上/基本面优先的机制。为什么他们有自己特有的特征和怪癖，而不是其他的？为什么赋予它们的形式是它们<i>不可避免的</i>形式，而不是任意的形式？为什么四级特工只能表现出精神病态？为什么没有5级？</p><p>我最终形成了一个关于它们如何工作的看似新颖的模型，现在我意识到它可能对其他人也有用（尽管我几年前就形成了它）。</p><p>它的目的是保留<a href="https://www.lesswrong.com/users/zvi?mention=user">@Zvi</a> <a href="https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Simulacra_Levels_Sequence">定义</a>的所有重要特征，同时通过对它们进行适当的齿轮级机械解释来解释它们。我认为在我划定界限的位置上存在一些细微的差异，但它基本上仍然应该与 Zvi 的一致。</p><hr><h2>基础工作</h2><p>在某些情况下，递归级别与递归级别 3 相比实际上变得难以区分。这不完全是一个新想法，但它是我的模型的核心，因此为了完整起见，我将提供一个示例。</p><p>考虑认知的情况。</p><ol><li>认知是对外部对象和过程的思考。 “这家餐厅太狭窄了。”</li><li>元认知正在构建你自己的思维模型。它可能有什么偏见，如何更好地推理对象级主题。 “我觉得这家餐厅太局促了，因为我不喜欢一大群人。”</li><li>元元认知正在分析你的自我模型：你是否倾向于修饰或掩盖你个性的某些部分，等等。“我正在给自己讲一个关于不喜欢一大群人的故事，因为这感觉像是一个更迷人的解释不喜欢这家餐厅胜过真正的餐厅。我不喜欢它是出于相反：这里有很多人因为它很受欢迎，而我本能地不喜欢主流的东西。”</li><li>那么，元元认知将是“思考你对以自我为中心的偏见的分析”。但这又是元元认知：分析你倾向于如何看待自己。 “我对自己的看法进行了复杂的思考，因为我想保持一个聪明、有自我意识的人的自我形象。”<ul><li>有一个类似的情况，元认知与元认知是同一件事，但我认为 2 级和 3 级之间存在细微差别，而 3 级和 4 级以后则不明显。 <span class="footnote-reference" role="doc-noteref" id="fnrefsm2ltnt082r"><sup><a href="#fnsm2ltnt082r">[1]</a></sup></span></li></ul></li></ol><p>下一篇：基本上，在任何社会中，都存在三个不同的“框架”：物质现实、他人和社会现实。每个后续框架都包含前一个框架的递归模型：</p><ol><li>物理现实<i>是</i>。</li><li>人们有自己的现实模型。</li><li>人们的社会形象是其他人对一个人的模型：即现实模型的模型。 <span class="footnote-reference" role="doc-noteref" id="fnrefmsdn2x9q13l"><sup><a href="#fnmsdn2x9q13l">[2]</a></sup></span></li></ol><p>递归级别1、2和3。这里没有有意义的“级别4”：“一个人的社会形象的模型”意味着“一个人的外表的感知”，这仍然只是“一个人的外表”。您可以在这里提出一些警告，但它不会改变太多<span class="footnote-reference" role="doc-noteref" id="fnreffofs4yy521u"><sup><a href="#fnfofs4yy521u">[3]</a></sup></span> 。</p><p>因此，任何信号都可以在这些框架中查看，从而产生任何信号可以传达的三种含义：</p><ol><li>它的字面意思是：在物理现实的背景下看待。</li><li>你认为演讲者试图让你相信什么，以及为什么：在你的演讲者模型的背景下观察。</li><li>它如何影响你和说话者的社交形象：在你和别人对你和说话者的模型的背景下进行观察。</li></ol><p>到目前为止，这很好：所有这些沟通层次都是有用的，并且在任何正常运转的社会中都占有一席之地。</p><p>当社会开始<i>放弃</i>较低层次的沟通时，问题就开始了。这就是转移到更高的拟像级别的含义：放弃较低的级别，转而选择较高的级别。一个“纯粹”的一级社会只关心陈述的字面真实性； 2级社会关心言论背后的人；第三级社会关心言论如何影响人们对其他人的<i>看法</i>。级别 0 和级别 4 比较特殊：级别 0 没有通信的概念，级别 4 已经深入到递归以至于放弃了它。</p><p>我将使用术语“社会”和“代理”，因为这就是我对这个模型的看法，但我的意思是广义上的。 “代理人”可以是从个人到国家的任何事物，“社会”可以是嵌入在任何背景（包括更广泛的社会）中的任何群体。</p><p>此外，我应该注意到，同一个代理人可能会占据不同的拟像级别，具体取决于他们所处的环境或与他们在一起的人（一个人可能对他们的家人非常友善和人性化，但可能是一个像精神病患者一样的级别） 4 当涉及到任何与政治无关的事情时）。</p><hr><h2> 0级</h2><p>理解它对于理解 4 级至关重要。</p><p>对于 0 级代理来说，没有符号、没有模型、没有交流，只有旨在直接给物理世界带来变化的行动。由于它涉及与其他代理的交互，因此它是一种纯粹的冲突。</p><p>你看到狮子，你就逃跑。你看到食物，你就拿走它。你遇到敌人，你杀死他们。你的行为是非象征性的：它们不代表任何东西，也不期望它们会被别人看到和理解。它们的目的是其形式<i>所固有的</i>。</p><p>当你试图削尖一块岩石并将其放在矛上时，你并不是在试图<i>说服</i>岩石改变其形状。当您编写计算机程序时，您并不是试图<i>说服</i>计算机正常工作（尽管有时感觉如此）。</p><p>但它不仅仅涉及无生命的物体。你<i>可以</i>在这里有一个某人的“模型”——一个追踪猎物的猎人——但关键的是你不要假设他们有<i>你</i>的模型。这里可能发生的任何“交流”都是片面的。如果你看到一只熊向你跑来，并且你逃离了它的领地，那么可以说这只熊“恐吓”了你。但熊并没有想到“恐吓”。它采取的行动并不是为了表明它愿意杀死你以迫使你撤退。它的意思是<i>直接</i>把你从它的领地里赶走，杀了你，而你愿意从它那里撤退，对它来说，就是一个巧合。 <span class="footnote-reference" role="doc-noteref" id="fnrefb90gqrtknu"><sup><a href="#fnb90gqrtknu">[4]</a></sup></span></p><p>本质上，这并不意味着 0 级代理人不能有广泛的动机，他们只是追求自己的私利。然而，这是最常见的。毕竟，在 0 级操作需要长期拒绝或无法与其他特工/人员联系。如果你只希望通过单方面迫使他人做出改变或得到同样的回报来与他人互动，那么除了一种不可调和的敌对关系之外，你还能拥有什么样的关系呢？</p><p> （如果你可以使用的<i>所有</i>工具都是 0 级工具怎么办？如果你没有办法与任何人对话，如果你甚至没有“对话”的概念，如果你只能强行改变世界？剧透警告：这就是 4 级从内部看的样子。）</p><p> 0级<i>就是</i>真理。更高的层次很容易崩溃：全面的战斗和战争。</p><hr><h2> 1级</h2><p>好吧，我不会在这里讨论语言和合作的发展。第 1 级是代理交换有关物理世界的信息以形成对其的正确共同理解的级别。</p><p>在这一级别上交换的声明仅对其真实价值进行审查，它们是字面的和公开的。由于它涉及与其他代理的沟通，因此这是一种纯粹的合作。这种合作有时可以采取“向其他部落展示你的（不夸张的）军事力量，这样他们就可以让你不战而屈人之兵地夺取所有资源”的形式，但它的目的仍然是实现一个对双方都有利的结果。选择。 （可能存在<i>0级</i>敌意，以及敌意地<i>拒绝</i>交流。如果你足够讨厌另一个部落，你就不给他们认输的机会，你只是屠杀他们所有人。但只要交流<i>确实</i>发生，它总是亲社会的。 ）</p><p>在第一级，主体和社会关注物理世界，他们专注于构建尽可能准确的模型。他们对该模型没有任何依恋，并且会根据需要对其进行更改以更好地适应世界。</p><p>两个智能体之间关于 1 级问题的冲突可能是由于他们的物理世界模型相互冲突，并且可以通过测试哪个模型是正确的或解决沟通不畅来解决。</p><hr><h2> 2级</h2><p>但有时不可能干净地解决 1 级冲突，因为两个代理缺乏干净地测试他们中哪一个是正确的能力，并且他们的先验（或动机推理）导致他们优先考虑不同的模型。或者也许他们有完全不同的价值观。在这种情况下，他们中的一个人可能会<i>撒谎</i>：故意发表不正确描述物理世界的声明，以扭曲对话者的世界模型，迫使他们按照第一个人的利益行事。</p><p>当然，这不一定是恶意的：他们可能会为了对方的“自身利益”而撒谎，也许是因为他们认为对话者有偏见。他们甚至可能是对的！ （借用兹维的例子，声称河对岸有狮子，而实际上有老虎，因为部落还不够害怕老虎。）</p><p>这是对第二级的“经典”解释：在这一级，人们发现欺骗，并开始扭曲他人的模型以达到自己的目的。这绝对是其中的一部分。但只是<i>一部分</i>。</p><p>实际上，对话双方都不必<i>撒谎</i>。同意不同意就足够了。</p><p>在此之后，社会中将出现<i>两种</i>相互冲突的世界模式（我们称之为“世界观”）。比如说，扩张主义与孤立主义议程。很快，还会有十几个人加入他们的行列，这些人是由于十几个未解决的分歧或欺骗而产生的。其中一些分歧将会得到解决，但其他分歧将长期存在。</p><p>人们会根据模型的质量、个人偏见和偏好在这些模型之间进行选择，形成<i>子群体</i>。每个子群体中的人都会试图让更多的人站在他们一边，并阻止人们离开他们的子群体，因为如果他们的世界观占主导地位，这将使他们受益（要么因为他们真诚地相信它比其他模型更真实，要么因为这对他们特别有利）。</p><p>他们会变得根深蒂固，他们会培养对自己事业或世界观的忠诚，他们会培养对自己信仰的确定性。最终，一些（大多数）人会对他们的物理世界模型产生<i>依恋</i>。他们将开始<i>从本质上</i>评价它，而不仅仅是因为它是正确的。他们将开始否认反对它的证据，他们的模型越准确（和/或难以质疑），这就越容易。</p><p>他们将开始将现实模型与<i>现实本身</i>混淆。</p><p>在第二级社会的后期，人们会考虑最重要的事情是其他人所认同的世界观，以及一般来说，其他人有什么信念和动机。审查声明的不是其真实价值，而是其背后的动机和信念：为什么该声明的作者选择这样做，他们试图传播什么世界观，或者他们可以相信什么。到那时，陈述的实际内容将被认为不如它们所揭示的关于说话者和说话者的其他信仰的内容重要。</p><p>创建新模型或对旧模型进行彻底改变将受到抵制。</p><p>与其他人<i>确信</i>正在<i>发生的事情相比，物理世界中实际</i>发生的事情将变得不那么重要。</p><p>两个代理之间的 2 级冲突是相互操纵的尝试。两者的目标都是说服观众接受他们的世界模型，无论观众是他们的对话者，还是真正的观看他们的观众。任何卑鄙的心理伎俩都可以在那里进行。</p><p>完全处于第 2 级的社会成员将 (1) 专注于开发尽可能准确的<i>社会其他成员</i>的模型，加上 (2) 将他们个人的物理世界模型视为本质上真实的，加上 (3) 考虑与实际的物理世界无关，可以根据需要撒谎或忽略。</p><hr><h2> 3级</h2><p>二级社会是一个分为多个小组的社会，仔细审查人们的言论是否与其世界观真正相符。其中一些团体比其他团体更强大，或者吸引不同的偏好，并且您可以通过发表正确的声明来表明有关您自己的正确信息，从而赢得他人的好感。你还可以通过说服别人持有不受欢迎的世界观或不属于他们想加入的群体来伤害你讨厌的人。</p><p>你或他们<i>实际上</i>是什么样子并不重要：只要某人<i>看起来</i>属于某个群体，或招致仇恨，或因某种原因而有另一种关系，他们<i>实际上做</i>。这纯粹是一个外观问题。</p><ul><li>第一级社会植根于物质世界的客观真理。</li><li>第二级社会以其人民<i>信仰</i>的客观真理为基础。即使这些信念被扭曲了，即使你<i>想</i>扭曲它们，你仍然关心<i>它们真正的内在认知状态</i>。</li><li>第三级社会凭空创造出自己的现实。它只关心事情看起来怎么样，一个人<i>看起来</i>有什么信念。但它并不承认这一点。</li></ul><p>在 3 级社会中，言论仍然会受到审查，以确定它们对说话者或其他人产生的影响，就像在 2 级社会中一样（因为递归已经在这方面达到了最大值；下一节将详细介绍）。但你对这些含义的解释是否正确已经不再重要，只要它<i>足够好，其他人就可以振振有词地声称相信它</i>，或者至少相信<i>你</i>相信它。</p><p>如果你<i>真正</i>相信这个事业，那并不重要。只是你的社会形象与那些以事业为中心的运动的一部分的人的形象相符。</p><p>这会产生一些不正当的激励措施，Zvi 将其<a href="https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions">称为</a>“针对知识的战争”：</p><blockquote><p>在第 3 级，以下两件事是应该受到指责的，从而造成知识成为一种负担的两种方式。 [...]</p><p>一件值得指责的事情是<i>没有使用正确的符号。</i></p><p>这就是“由可用于其他用途的事物组成”的方面。关心真相会产生一种替代性激励，阻止人们调用正确的符号，并让人怀疑这些符号是否意味着它们看起来的意思。</p><p>如果有的话，调用技术上错误的符号而不是技术上正确的符号是<i>游戏中更强大的一步</i>。这就是为什么。它更强烈地表明一个人以昂贵的代价发送了适当的信号，而没有空间被误解为较低级别的行动。通过重复谎言，我们表现出自己的忠诚。通过让其他人重复这一点，我们可以促使他们变得忠诚并被认为是忠诚的，并让他们向其他人展示他们的忠诚，并展示我们对人和象征的力量。</p><p>另一件值得指责的事情是<i>知道你所说的是假的</i>。罪魁祸首<i>是知识本身</i>。</p><p> （或者，也许更准确地说，<i>其他人知道</i>你知道你所说的是假的，因此任何其他人都知道我们拥有知识，而不是知识本身，但对于任何其他指责系统来说也是如此。）</p><p>总统知道什么？他什么时候知道的？</p><p>因此，沟通从显式转变为隐式。专注于仅拥有可否认的隐性知识。</p><p>需要明确指导的追随者确实是一个糟糕的追随者。指定要完成的所有事情是不切实际的，并且表明您不仅拥有知识，而且拥有责任。更好地<i>朝着</i>团队的目标努力，积累有助于赢得比赛的符号。</p><p>因此，这种结构使每个人都远离知识。到目前为止，假装不知道事情的最简单方法就是不知道它们。</p></blockquote><hr><h2>递归级别</h2><p>当我们提升级别时，需要跟踪三个重要变量：世界的哪些方面被认为是重要的，正在形成/操纵什么样的框架来对重要方面进行建模，以及世界的哪一方面被认为是无关紧要的。</p><p>这是表格格式：</p><figure class="table"><table><thead><tr><th> L</th><th>重要的</th><th>框架</th><th>无关紧要</th><th>评论</th></tr></thead><tbody><tr><td>0</td><td>物质世界</td><td>—</td><td> —</td><td>模型不存在。一切都是真实的。</td></tr><tr><td> 1</td><td>物质世界</td><td>人们的世界观</td><td>—</td><td>世界如何运作？</td></tr><tr><td> 2</td><td>人们的世界观</td><td>人们的社会形象</td><td>物质世界</td><td>人们相信什么？为什么？怎么可以用呢？</td></tr><tr><td> 3</td><td>人们的社会形象</td><td>人们的社会形象</td><td>人们的世界观</td><td>只有人们投射的形象才重要。</td></tr><tr><td> 4</td><td>人们的社会形象</td><td>人们的社会形象</td><td>人们的社会形象</td><td>???</td></tr></tbody></table></figure><p>所有三个不同的值都代表不同级别的递归：</p><ol><li>物理世界<i>是</i>。</li><li>人们的世界观是物质世界的模型。</li><li>人们的社会形象是人们及其世界观的集体模型：世界模型的模型。</li><li>下一个是“一个人的社会形象的模型”，即“一个人的外表的感知”，但这仍然只是“一个人的外表”。</li></ol><p>在第 3 级，第三个变量尝试越过递归第 3 级，因此会自行环绕。这导致了“书写自己的现实”的特殊情况：第 3 级<i>创建了</i>它所关心的事物。</p><p>但由于人们图像的真实性尚未被认为是<i>无关紧要的</i>，因此仍然与现实存在某种联系，如果有足够的证据反对某个主张，就可能推翻它。人们仍然认为模型<i>代表着</i>某种东西，它们意味着<i>代表</i>一些不可改变的现实的真相。</p><p> 3 级特工真正关心世界如何看待他们和其他人。他们多么准确地表达了他们的忠诚。如果有人提供证据证明有人在某个时刻发出了与他们现在试图展现的公众形象背道而驰的信号，那么 3 级特工<i>就会</i>关心这一点。 （因此例如取消文化。）</p><p> 4 级甚至超越了这个。 （我见过 4 级被描述为“超过 3 级的一切”，我想这是我将其形式化的说法。）</p><hr><h2> 4级</h2><p>想象一个后期的 3 级社会。 2 级智能体群体消失或采用 3 级思维模式。到那时，没有人关心在任何地方或任何人身上实际发生的事情，甚至人们可以确信正在发生的事情，只<i>关心人们认为他们可以振振有词地声称相信</i>正在发生的事情。</p><p>一旦这种事态成为共享知识，就会切换到第 4 级。一旦每个人都知道，他们不需要用自己的表演来<i>合理地</i>说服观众，只有 L4 特工同伴会跟踪每一句话对正在进行的地位游戏的影响。一旦他们进一步知道其他人也知道这一点......</p><p>这有一个棘手的含义：符号变得非符号化。</p><p>符号是代表其他事物的事物。但第四级陈述无论如何都与现实无关：它们完全没有意义。它们不传达任何信息，无论是在任何层面上，无论是字面上还是通过它们所产生的含义。他们不代表任何事情。在第 4 级，符号<i>不再象征事物</i>。他们变得自给自足。</p><p>每个人都知道这一点，所以没有人试图解释它们。每个人都知道<i>这</i>一点，因此没有人发表声明时期望自己会受到仔细审查以获取信息。那么，任何人发表声明的唯一原因是因为他们知道这会对当地的社会环境产生特定的影响：开辟某些攻击或防御的途径。也就是说，<i>它们的目的是其形式所固有的。</i></p><p>我认为与 0 级的相似之处是显而易见的。 4级特工<i>不能说话</i>。他们的言论<i>扭曲了现实</i>。他们不能说话，只能<i>强迫别人占据不同的社会情境</i>。他们（认为）得到了同样的回报：没有人与他们接触，其他人只是试图强行改变他们周围的社会政治格局。</p><p>从真正的意义上来说，4 级语句是<i>移动</i>、攻击或防御的动作，与物理攻击和躲避不同。</p><p>因此，它们也必然是短期的。从较低级别的角度来看，4 级特工看起来仍然在编织有关物理现实的不同叙述。但这些叙述的唯一目的是赢得其创造者目前所卷入的任何<i>直接</i>冲突。他们不需要足够强大来度过这场冲突，或者彼此保持一致，甚至在冲突之外的任何人看来都是一致的。</p><p>与第 3 级不同，提供某人在某个时刻发出错误信号的证据不会打动人们，除非您设法<i>正确地将此信息的泄露作为攻击</i>。</p><p> 4 级特工不关心世界，也不关心别人的看法，也不关心世界如何看待他们。他们唯一关心的是他们可以假装走过的社会政治景观的非象征路线。</p><p> 4 级与 0 级一样都是霍布斯地狱。4 级智能体没有与他人合作交互的<i>语言</i>。从理论上讲，这里也可能有各种各样的动机，但通过这些镜头，一切都会被扭曲。</p><hr><h2> 5级</h2><p>尝试以回避递归模型（已经全面循环）的方式进行推断，第 5 级到第 4 级的框架就像第 1 级到第 0 级一样。这一切都始于逃离非符号地狱， 毕竟。</p><p>但是，当你把信号的概念变成一把用来刺人的刀之后，你如何才能重新发现沟通呢？</p><p>没有5级。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsm2ltnt082r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsm2ltnt082r">^</a></strong></sup></span><div class="footnote-content"><p>也许更细粒度的说法是，在某些情况下，随着递归级别的提高，它们之间的差异缩小，并且由于人类思维的局限性，L3 与 L4+ 是我们的模型变得足够粗糙的地方，差异是难以察觉的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmsdn2x9q13l"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmsdn2x9q13l">^</a></strong></sup></span><div class="footnote-content"><p>这完全符合我的观点，即<a href="https://www.lesswrong.com/posts/hx5wTeBSdf4bsYnY9/idealized-agents-are-approximate-causal-mirrors-radical">主体是近似因果镜子</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfofs4yy521u"> <span class="footnote-back-link"><sup><strong><a href="#fnreffofs4yy521u">^</a></strong></sup></span><div class="footnote-content"><p>例如，某些政策建议被认为不受欢迎，并不是因为大多数人实际上反对它们，而是因为大多数人<i>认为</i>大多数人反对他们，如果你担心这一点，那么从技术上讲，你就已经进入了递归级别4...</p><p>但根据脚注 1，从经验来看，这种情况似乎并没有发生太多，可能是由于人类思维的局限性。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb90gqrtknu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb90gqrtknu">^</a></strong></sup></span><div class="footnote-content"><p>好吧，这实际上可能不是对字面动物的准确描述。它们并不都处于 0 级，它们可以相互通信（尽管我认为在这个具体示例中，界限是模糊的）。或者，你可以想象一个非智能机器人代替熊，遵循一些算法在其领地巡逻，告诉它杀死入侵者。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KnQpzYRR4ogPNtzem/a-crisper-explanation-of-simulacrum-levels<guid ispermalink="false"> KnQpzYRR4ogPNtzem</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Sat, 23 Dec 2023 22:13:52 GMT</pubDate></item><item><title><![CDATA[Hyperbolic Discounting and Pascal’s Mugging]]></title><description><![CDATA[Published on December 23, 2023 9:55 PM GMT<br/><br/><p><i>从我的个人博客交叉发布：</i> <a href="https://mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/"><i>https://mechanisticmind.com/hyperbolic-discounting-and-pascals-mugging/</i></a></p><h1>长话短说</h1><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/lmihbysdvrbcnngfpyep" alt="该图像的 alt 属性为空；它的文件名为 image-19.png"></figure><p>双曲贴现（如图 🟥 所示）是指数贴现的不完美近似（如图 🟦 所示）。人们普遍指出，这会导致人类高估近期奖励，但很少有人意识到这也会导致我们高估远期奖励。有大量证据表明人类使用双曲线贴现，这有助于解释为什么人类追求不切实际的长期目标。</p><h1>什么是双曲线贴现？</h1><p>如果你搜索“双曲折扣”这个词，谷歌会告诉你，这是“一种心理偏见，人们优先考虑眼前的奖励和满足感而不是未来的奖励”。事实上，这只说对了一半。除了高估<i><strong>近期</strong></i><strong>奖励</strong>之外，双曲线贴现还会导致人们<strong>高估</strong>远期奖励。</p><p>时间折扣基本上是一种思考如何回答这样的问题的方式：</p><blockquote><p>您愿意今天拥有 100 美元，还是一年后拥有 120 美元？</p></blockquote><p>从货币角度考虑这个问题相对容易，而且你可能知道这个问题的答案与利率有关，一年后拿 120 美元等于 20% 的年回报率，这是更好的选择高于您在股票市场上预期的 4-10% 的利率，也优于您目前<a href="https://ycharts.com/indicators/1_year_treasury_rate">可以获得</a>的 1 年期国债 4.88% 的利率。</p><p>这种计算利率的模型称为<i><strong>指数贴现</strong></i>，它基本上由以下方程表示：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" \text{value at time }t = \frac{a}{d^{t}} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">时间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span>处的值<span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 1.065em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 1.065em; top: -1.143em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-denominator" style="width: 1.065em; bottom: -0.824em;"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0.076em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.065em;" class="mjx-line"></span></span><span style="height: 1.967em; vertical-align: -0.824em;" class="mjx-vsize"></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>这表示，如果某个东西在 $t=0$ 时价值 $a$，例如现在一张 100 美元的钞票，那么您需要等待的时间越长，它的价值就会呈指数下降。</p><p>因此，事实证明，人类和其他动物似乎并没有使用这个方程来估计未来奖励的价值。相反，他们使用称为<i><strong>双曲贴现的</strong></i>方法，可以用以下等式表示：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" \text{value at time }t=\frac{1}{1+dt} "><span class="mjx-mrow" aria-hidden="true">时间<span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">时的值</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 2.806em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 2.806em; top: -1.368em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="width: 2.806em; bottom: -0.866em;"><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.806em;" class="mjx-line"></span></span><span style="height: 2.234em; vertical-align: -0.866em;" class="mjx-vsize"></span></span></span></span></span></span></p><p>这表明，随着奖励的距离越来越远，它的价值会按倒数下降。</p><h2>我们如何知道</h2><p>科学家们用本科生和<a href="https://mpra.ub.uni-muenchen.de/79536/1/MPRA_paper_79536.pdf">100 美元的钞票</a>进行了实验。实际上，资助资金很难获得，因此大多数人都使用 10 美元的钞票，而来自不太知名大学的研究人员大概只能用 1 美元的钞票和一些零钱度日。</p><p>他们还用猴子和果汁做了<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2728786/">实验</a>。猴子喜欢果汁；我不知道是否有科学家使用可口可乐进行这些实验，但我敢打赌他们会重复相同的发现。哇，我是在开玩笑，但这里有一篇论文，<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107575/">科学家们给猴子喂可卡因</a>。科学有时真是太疯狂了。他们发现“[可卡因]选择行为在很大程度上与双曲线贴现一致”。</p><p>我还没有找到任何用可卡因测试本科生的实验，所以如果你是一名正在寻找突破性论文的研究生，这里有一个机会。</p><p>我应该指出，大脑是复杂的，说人类或猴子总是使用双曲贴现过于简单化。这篇论文说，它<a href="https://www.frontiersin.org/articles/10.3389/neuro.08.009.2009/full">因物种而异</a>，虽然这篇论文说实验证据有力，但<a href="https://core.ac.uk/reader/6519160">实验室设置不现实</a>。</p><h2>数学</h2><p>从某种意义上说，双曲线贴现是<i>错误的</i>。如果您是一家对冲基金，并使用此等式对您交易的资产的未来回报进行定价，您将会<i>亏损</i>。</p><p>然而，人类似乎是以这种方式思考世界的，所以理解它的含义很重要。首先，让我们绘制这两个方程的对比图。您可以关注<a href="https://www.desmos.com/calculator/gzxw59cmhf">Desmos</a> 。 </p><p><img style="width:434px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/ielywx3mfzz6zqtd61ln" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/wjvtuibnenb1vuwm4gid 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/jqhcyk72nvcm1kjeecjg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/htb3a5ifozdpxvuxq9gx 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/sgsmnsfkjvxloh6kgcot 768w"></p><p> 🟥 = 双曲线贴现<br>🟦=指数折扣</p><p>等等，这是怎么回事？谷歌告诉我们，双曲线贴现高估了短期回报，但这张图告诉我们恰恰相反！双曲线贴现实际上高估了长期回报。</p><p>但人类似乎确实高估了短期回报，这就是所有有关双曲线贴现的自助文章在网上不得不说的，尽管他们没有图表。我个人很喜欢<a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">这个</a>，它有很好的插图： </p><p><img style="width:742px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/jsr01c0ixp8wshmeitex" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/nknswihajwusus7s0dov 1023w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/wo87kllllnavgiz72eky 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/yldmdidxszdr2ywefuhe 768w"></p><p>资料来源： <a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">https</a> ://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/</p><p>如果你眯着眼睛看图表，就会发现双曲曲线高估了具有负值 $t$ 的事物。让我们缩小一点。 </p><p><img style="width:442px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/iq6djzuuvm7j1eqzvkug" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/xqmce412fudqotldj1hv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/a7emmiaun799tdcii4go 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/ypw83j1rjwpsadgmwt0d 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/nuoy6d4zgpfipslpzoge 768w"></p><p> 🟥 = 双曲线贴现<br>🟦=指数折扣</p><p>有几种方法可以平方这个圆，它们基本上都可以归结为：出于某种原因，动物执行双曲贴现，但它们想要逼近<i>真正的</i>贴现函数，该函数是指数的，因此进化选择了最小化差异的超参数 $\ mathbb{E}[abs(双曲 - 指数)]$.</p><p>有很多方法可以调整这两个函数的参数，但我喜欢引入一个参数，该参数只是将图形向右移动，这本质上表明我们考虑的所有操作都需要一些非零时间和精力。您可以<a href="https://www.desmos.com/calculator/w9kmpyu6xx">在此处查看 Desmos 上的</a>方程。 </p><p><img style="width:652px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/gzdc5vck6277qnxrvhis" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/xk6s4xlvj74kh9rwcygw 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/znmecdrwhkbbj9paoy4p 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nDkfEYpDFB7KDfQ9/fir3ah1sgwomrp8t2drp 768w"></p><p> 🟥 = 双曲线贴现<br>🟦=指数折扣</p><p>并排比较这两个方程，我们可以看到双曲线贴现高估了遥远的奖励，但奇怪的是它似乎也高估了遥远的奖励。</p><p>您可以看到，在 $\text{time}=0$ 时，双曲线贴现对事物的估值大于 1。也就是说，它相对于事物的真实价值高估了事物的价值。我怀疑这是<a href="https://en.wikipedia.org/wiki/Loss_aversion">损失厌恶</a>的根源之一，即与我们可以拥有的东西相比，人类倾向于对我们已经拥有的东西给予过多的重视。</p><h1>挑战与时间</h1><p>在我们了解其含义之前，让我先谈谈<strong>时间</strong>的含义。在金融世界中，你可以把钱存入银行而不用做任何事，时间是你唯一的资源。但在我们生活的其他领域，我们必须真正付出努力才能获得回报。考虑这个问题：</p><blockquote><p>您愿意今天在 Craigslist 上以 100 美元的价格出售您的二手家具，还是愿意在车库修理并重新粉刷后以 200 美元的价格出售？</p></blockquote><p>这类似于“现在或以后”框架，但有两个复杂之处：</p><ol><li>您投入自己的劳动来修理家具，因此您实际上是按小时付费的</li><li>你可能画得不好，所以不能保证它在你完成后实际上价值 200 美元</li></ol><p>第二个复杂因素，即挑战，实际上出现在上面的利率示例中。如果实验者问您是否愿意现在拥有 100 美元，还是一周后拥有 120 美元，您可能会现实地预期，交易从现在起一周后完成的可能性较小。毕竟，也许你会忘记它，也许实验会被关闭，也许他们会弄错你的电话号码。</p><p>以下是一些其他情况，其中 x 轴更多地涉及复杂性的不确定性而不是时​​间：</p><blockquote><p>您在跳蚤市场出售家具。现在有人给你 100 美元，但你认为它更值钱。你是否应该坚持下去，希望当天晚些时候有人会为此付出更多代价？</p></blockquote><blockquote><p>政治家 A 承诺逐步改进，这些改进是可以实现的，但并不令人兴奋。政治家 B 承诺进行一场乌托邦革命，但这似乎有风险。你应该投票给谁？</p></blockquote><p>将其中每一个视为一个模型，其中在完成 $N$ 挑战后将出现一些奖励 $r$。考虑一下企业家爱丽丝的这种假设情况：</p><blockquote><p>爱丽丝想创办一家销售鞋子的公司，她认为这将价值 1,000,000 美元。为了取得成功，她必须完成以下五项任务：</p><ol><li>开发一款舒适的鞋子</li><li>找到一家工厂来生产这些鞋子</li><li>为鞋子打造良好的品牌形象</li><li>说服有影响力的人推销鞋子</li><li>履行订单</li></ol></blockquote><p>Alice 应该如何平衡奖励的承诺和她面临的 5 个挑战？指数折扣表明每个挑战都会在一定比例的时间内失败。也许她有 50% 的可能性完成每项任务，或者 $p_s=0.5^5=3\%$ 成功的机会。但双曲线贴现表明她更有可能将其估计为 $1/(1+d*5)$。如果 $d=6$，她将获得相同的 $p_s=3\%$ 成功机会，但这并不能概括。无论她的 $d$ 值与之前的经验相符多少，随着计划中步骤的增加，她都会系统性地高估自己的预期回报。</p><h1>为什么？</h1><p>人类为什么会这样？为什么我们不使用指数折扣，从抽象的角度来看，这是<i><strong>正确的</strong></i>？一般来说，这个问题有两种可能的答案：</p><ol><li>事实上，双曲线贴现是正确的</li><li>数学很难，大脑可以比其他操作更容易地完成某些类型的操作</li></ol><p>维基百科<a href="https://en.wikipedia.org/wiki/Hyperbolic_discounting#Uncertain_risks">概述了“实际上正确”答案的一个很好的论点</a>，它基本上说，如果存在恒定的背景风险，那么在每个时间单位（危险率）都会出现问题，但你不知道风险水平是多少但你对它有一些合理的分布，那么双曲贴现在数学上是正确的。我不完全理解这种分布是否是一个合理的假设。我认为有时我们拥有有关危险率的良好数据，但很难将其纳入我们的决策中。</p><h2>一个神经科学的故事</h2><p>但我也认为有一个神经科学的故事。计算指数函数可能比计算双曲函数更困难，后者只需要加法、乘法和除法。毕竟，这些方程在纸上看起来毫无意义，但它们代表了一个计算过程。</p><p>令人惊讶的是，计算指数对大脑来说很难计算。指数增长和衰减在生物系统中随处可见，例如药物的半衰期。大脑实际上需要通过负反馈回路小心地稳定自身，以防止神经活动呈指数级爆炸，从而导致癫痫发作。但所有这些机制所花费的时间随着 $t$（您想要预测的未来时间）线性增长。大脑确实需要能够在 $O(1)$ 时间内评估行动的折扣价值。</p><p>在<a href="https://scholar.google.com/citations?hl=en&amp;user=tZpKKm4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Randall O&#39;Reilly</a>的大脑强化学习<a href="https://ccnlab.org/papers/OReillyHazyHerd16.pdf">轴突</a>模型中，一个动作的预期奖励是与该动作的预期成本分开计算的。我认为它就像<a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a> ，其中 $Q^+(a)$ 是预期奖励，$Q^-(a)$ 是预期成本，其中包含延迟。在这个 RL 框架中，动物想要追求最大化 $Q^+(a) - Q^-(a)$ 的动作 $a$。然而，<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653570/">来自细胞过程的证据</a>表明，抑制性神经连接最好用除法而不是减法来表示，给我们一个这样的方程：</p><p><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label=" v_E(a, d)={\frac {Q^+(a)}{k_1+k_2Q^-(a, d)}} "><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">v</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 6.893em; padding: 0px 0.12em;"><span class="mjx-numerator" style="width: 6.893em; top: -1.608em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span> <span class="mjx-denominator" style="width: 6.893em; bottom: -1.09em;"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.409em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 6.893em;" class="mjx-line"></span></span><span style="height: 2.698em; vertical-align: -1.09em;" class="mjx-vsize"></span></span></span></span></span></span></span></span></p><p>其中 $a$ 是正在考虑的操作，$d$ 是延迟，$k_1$ 和 $k_2$ 是常量。这正是双曲线贴现。</p><h1>帕斯卡的抢劫</h1><p>网上有很多文章告诉我们，双曲线贴现意味着我们在短期内高估了事物的价值。这是一个示例：</p><ul><li><a href="https://thedecisionlab.com/biases/hyperbolic-discounting">为什么我们更看重眼前的回报而不是长期的回报？</a></li><li><a href="https://www.nirandfar.com/hyperbolic-discounting-why-you-make-terrible-life-choices/">双曲线贴现：为什么你会做出糟糕的人生选择</a></li><li><a href="https://www.moneythor.com/2020/09/02/hyperbolic-discounting-behavioural-science-in-banking/">“双曲线折扣是……人们选择较小的、即时的奖励”</a></li></ul><p>但是，当我们查看上面的方程图时，我们发现双曲线贴现还会引入一种偏差，即我们高估了遥远的奖励！实际上，在中期，我们一直低估事物的价值。</p><p>我相信这解释了为什么人们始终热衷于在遥远的未来实现的不切实际的梦想。我们发现成为一名著名的摇滚明星比成为一名相当优秀的音乐家更有动力，尽管这种可能性要小得多。在政治上，人们追求的是一场解决一切问题的革命，而不是温和的改革。</p><blockquote><p>在哲学上，帕斯卡的抢劫是一个思想实验，展示了预期效用最大化的问题。理性的代理人应该选择其结果在按概率权衡时具有更高效用的行动。但一些非常不可能的结果可能会产生非常大的效用，而且这些效用的增长速度可能快于概率减少的速度。 —<a href="https://en.wikipedia.org/wiki/Pascal%27s_mugging">维基百科</a></p></blockquote><p>帕斯卡的抢劫有时会以指数折扣的方式发生，有时追求巨额但不太可能的回报是理性的。例如，大多数制药初创公司都会失败，但那些成功的公司往往会获得巨大的利润，因为他们能够在专利垄断下销售其药品。</p><p>但这种情况更有可能发生在双曲贴现下，因为双曲贴现不恰当地使用了倒数而不是指数衰减。</p><p>帕斯卡抢劫的基本设置是这样的：</p><ul><li>有非常大的可能奖励（天堂、乌托邦、仁慈的 AGI、公正的国王、名誉和财富等）</li><li>有很多原因可以解释为什么这种可能的奖励不太可能实现。或者，承诺的奖励在时间上非常遥远，每个单位时间都有恒定的不确定性</li></ul><p>以指数方式折扣非常大的奖励是正确的，如果你这样做，你会发现它会很快减少。但人们不会凭直觉这样做。我们的神经硬件经过设置，可以使用收敛到零的速度比指数慢得多的函数进行折扣。唯一的解决办法就是<a href="https://www.lesswrong.com/tag/shut-up-and-multiply">闭嘴并繁衍</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4nDkfEYpDFB7KDfQ9/hyperbolic-discounting-and-pascal-s-mugging<guid ispermalink="false"> 4nDkfEYpDFB7KDfQ9</guid><dc:creator><![CDATA[Andrew Keenan Richardson]]></dc:creator><pubDate> Sat, 23 Dec 2023 21:55:27 GMT</pubDate> </item><item><title><![CDATA[AISN #28: Center for AI Safety 2023 Year in Review]]></title><description><![CDATA[Published on December 23, 2023 9:31 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p> 2023 年即将结束，我们要感谢您对人工智能安全的持续支持。对于人工智能和人工智能安全中心来说，今年是重要的一年。在这份特别版时事通讯中，我们重点介绍了今年一些最重要的项目。感谢您成为我们社区和工作的一部分。</p><hr><h1> AI 安全中心 2023 年回顾</h1><p>人工智能安全中心 (CAIS) 的使命是减少人工智能带来的社会规模风险。我们认为这需要研究和监管。这些都需要快速发生（由于人工智能进展的时间表未知）并且同时进行<strong> </strong>（因为任何一个本身都是不够的）。为了实现这一目标，我们致力于三大工作支柱：研究、领域建设和宣传。</p><h2><strong>研究</strong></h2><p>CAIS对人工智能安全进行技术和概念研究。我们追求多种重叠的策略，这些策略可以叠加在一起以降低风险（“纵深防御”）。尽管没有任何一项技术可以将风险降至零，但我们希望建立分层防御措施，将风险降低到可以忽略不计的水平。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcbf0c289-ca28-4780-80d7-04c1178b2594_1174x510.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/dejkbp8hpwmjxaozjcpj" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/pabwup4lpoop16oojuhj 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/bd2rkl5pork5en8ys3jp 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/tmaoupvpzm57i7qflktp 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HEuDwEk22JfCBHh9o/dejkbp8hpwmjxaozjcpj 1456w"></a></p><p>以下是我们 2023 年<strong>技术研究</strong>的一些亮点：</p><ul><li> <a href="https://llm-attacks.org">LLM 攻击</a>：绕过 GPT-4、Claude、Bard 和 Llama 2 上的安全护栏，导致模型做出危险行为，例如输出制造炸弹的指令。这项工作为法学硕士创建了自动对抗性攻击领域。 《<a href="https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html">纽约时报》</a>对此进行了报道。</li><li><a href="https://www.ai-transparency.org/">表示工程</a>：第一篇控制模型内部结构以使模型说谎或诚实的论文。实验表明，这些技术可以使人工智能更加诚实、厌恶权力和道德。</li><li> <a href="https://aypan17.github.io/machiavelli/">MACHIAVELLI Benchmark</a> ：评估人工智能代理做出道德决策的能力。该基准提供了关于欺骗、遵守规则、追求权力和效用的 13 项道德行为衡量标准。以<a href="https://icml.cc/virtual/2023/oral/25461">口头论文形式发表在 ICML 2023 上</a>。</li><li> <a href="https://decodingtrust.github.io/">DecodingTrust</a> ：表明 GPT-4 比其他模型更容易受到误导性目标系统提示的影响。荣获<a href="https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/">NeurIPS 2023优秀论文奖</a>。</li><li>我们还发表了关于<a href="https://arxiv.org/abs/2311.04235">法学硕士遵循规则</a>和<a href="https://arxiv.org/abs/1908.08016">无限制对抗性攻击的</a>研究。</li><li>我们的研究人员正在帮助 OpenAI 和 Meta 等多个实验室对他们的模型进行红队设计。</li></ul><p>我们还进行了关于人工智能安全的<strong>概念研究</strong>：</p><ul><li><a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>提供了人工智能风险的全面概述。 （<a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757">华尔街日报</a>）</li><li><a href="https://arxiv.org/abs/2303.16200">自然选择有利于人工智能而不是人类</a>，认为人工智能的发展将受到自然选择的影响，这将导致自私的人工智能将自身的增殖置于人类目标之上。 （《<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/">时代周刊》专栏</a>）</li><li> <a href="https://arxiv.org/abs/2308.14752">《人工智能欺骗：示例、风险和潜在解决方案调查》</a>提供了人工智能欺骗的实证示例，讨论了由此产生的风险，并提出了技术和政策解决方案。</li></ul><p>凭借我们的研究专业知识，CAIS 帮助举办了<a href="https://trojandetection.ai">NeurIPS 2023 木马检测竞赛</a>，其中包括关于红队大型语言模型的新赛道。超过 125 个团队参与并提交了超过 3400 份参赛作品。</p><h2>现场建设</h2><p>CAIS 旨在创建一个蓬勃发展的研究生态系统，推动安全人工智能的进步。我们在 2023 年实现了这一目标，为人工智能研究人员提供计算基础设施、创建用于学习该领域的教育资源以及其他活动。</p><p><strong>计算集群。</strong>进行有用的人工智能安全研究通常需要使用尖端模型，但运行大规模模型既昂贵又麻烦。这些困难常常阻碍研究人员进行先进的人工智能安全研究。 2023 年 2 月，CAIS 推出了计算集群，为从事人工智能安全研究的研究人员提供免费计算。</p><p>截至 2023 年 11 月，我们已经吸引了约 200 名用户参与 63 个人工智能安全项目。使用 CAIS 计算集群总共发表了 32 篇论文，包括：</p><ul><li><a href="https://arxiv.org/abs/2309.15840">如何抓住人工智能骗子</a></li><li><a href="https://arxiv.org/abs/2310.15213">大型语言模型中的函数向量</a></li><li><a href="https://arxiv.org/abs/2308.14761">扩散模型中的统一概念编辑</a></li><li><a href="https://arxiv.org/abs/2310.17645">防御公共模型的传输攻击</a></li><li><a href="https://arxiv.org/abs/2311.14455">来自有毒人类反馈的通用越狱后门</a></li><li><a href="https://drive.google.com/file/d/1iluFBhtQrv6kbmp4-Wsibpt5-U52CElO/view">寻找却找不到：深度神经网络中难以检测的木马</a></li><li><a href="https://openreview.net/forum?id=l3yxZS3QdT">BIRD：深度强化学习的通用后门检测和删除</a></li><li><a href="https://arxiv.org/abs/2309.12288">逆转诅咒：接受过“A is B”培训的法学硕士无法学习“B is A”</a></li><li> ……还有<a href="https://www.safe.ai/compute-cluster">另外 24 篇论文</a>。</li></ul><p> 70% 对我们的用户调查做出回应的实验室指出，如果没有计算集群，他们的研究项目就不可能在当前范围内实现；另外30%的人表示集群显着加快了他们的研究进展。</p><p><strong>哲学联谊会。</strong>今年，CAIS 接待了十几位学术哲学家，进行为期七个月的研究奖学金。他们发表了 21 篇关于人工智能安全的研究论文，主题包括<a href="https://drive.google.com/file/d/1Bp6iTbeXgdeE5C3UPqdWFOp8MePRylvu/view">寻求权力的人工智能</a>和<a href="https://philpapers.org/archive/GOLAWE-4.pdf">人工智能主体的道德地位</a>以及其他主题（<a href="https://www.safe.ai/philosophy-fellowship">更多内容请参见此处</a>）。他们还在领先的哲学会议上发起了三个研讨会、两本书、各种专栏文章和<a href="https://link.springer.com/collections/cadgidecih">一本顶级期刊的特刊</a>（收到了 30 多篇研究论文），所有这些都专注于人工智能安全。这大大加速了人工智能安全向跨学科企业的发展。</p><p><strong>活动。</strong> CAIS 聚集了 20 名法律学者、政策研究人员和政策制定者，参加了为期三天的法律和人工智能安全研讨会。一群与会者随后成立了一个对人工智能安全感兴趣的法律学者联盟。他们还一直在制定研究议程纲要，该纲要即将发布。在我们的调查受访者中，100% 的研究人员提出了更多的研究想法； 91% 的受访者表示，他们发现研讨会对于结识研究合作者非常有用。</p><p>我们帮助在 ICML 和 NeurIPS 这两个顶级 AI 会议上组织了关于 ML 安全的社交活动，大约 300 名研究人员出席了每个会议讨论 AI 安全。我们参加了中国最大的人工智能会议，即在上海举行的世界人工智能大会，并在会上举办了<a href="https://drive.google.com/file/d/15gnLZsMMvtCy-lwCz5BhlWQ9-ni2TBak/view">有关人工智能安全的演讲</a>，吸引了超过 30,000 名观众。</p><p><strong>教科书。</strong> <a href="https://www.aisafetybook.com/textbook/0-1">《人工智能安全、道德和社会简介》</a>是一本新教科书，将于明年初在学术出版社出版。它旨在利用安全工程、经济学、哲学和其他学科，为人工智能安全提供易于理解和全面的介绍。那些想参加基于教科书的免费在线课程的人可以<a href="https://www.aisafetybook.com/express-interest">在这里</a>表达他们的兴趣。</p><p><strong>在线课程。</strong>此外，CAIS 使用我们去年夏天开发的<a href="https://course.mlsafety.org">课程</a>开展了两个在线的 ML 安全简介课程。这些项目总共帮助约 100 名学生、研究人员和行业工程师了解 AI 安全。</p><h2>宣传</h2><p>公众对人工智能安全的认识和理解可以鼓励明智的技术和政策解决方案。 CAIS 向政府提供建议并公开撰写文章，以分享有关人工智能安全的信息。</p><p><strong>关于人工智能风险的声明。</strong> CAIS发布了<a href="https://safe.ai/statement-on-ai-risk">关于人工智能灭绝风险的声明</a>，显着提高了公众和政府对人工智能风险的规模和重要性的认识。至关重要的是，关于人工智能风险的声明将人工智能灭绝风险牢牢置于可接受的公众讨论的奥弗顿窗口之内。</p><p>该声明极大地影响了美国和英国高层领导人的思维。 Rishi Sunak<a href="https://twitter.com/RishiSunak/status/1663838958558539776">直接回应</a>了有关人工智能风险的声明，表示“[英国]政府正在非常仔细地考虑这一点。”白宫新闻秘书卡琳·让-皮埃尔 (Karine Jean-Pierre) 在<a href="https://www.usatoday.com/story/news/politics/2023/06/01/president-biden-warns-ai-could-overtake-human-thinking/70277907007/">被问及这一声明时</a>评论道，人工智能“是我们这个时代目前看到的最强大的技术之一。但为了抓住它带来的机遇，我们必须首先减轻其风险。”欧盟委员会主席的国情咨文<a href="https://ec.europa.eu/commission/presscorner/detail/en/speech_23_4426#:~:text=%E2%80%9CMitigating%20the%20risk%20of%20extinction,uses%20-%20both%20civilian%20and%20military.">全文引用了该声明</a>。</p><p><a href="https://drive.google.com/file/d/1RVxA5OyvuCFwupt-ptPjDzNe4GqB1yuN/view?usp=sharing">纽约时报（头版）</a> 、 <a href="https://www.theguardian.com/technology/2023/may/30/risk-of-extinction-by-ai-should-be-global-priority-say-tech-experts">卫报</a>、 <a href="https://www.bbc.com/news/uk-65746524">BBC新闻</a>、 <a href="https://www.reuters.com/technology/top-ai-ceos-experts-raise-risk-extinction-ai-2023-05-30/">路透社</a>、 <a href="https://www.washingtonpost.com/business/2023/05/30/ai-poses-risk-extinction-industry-leaders-warn/">华盛顿邮报</a>、 <a href="https://edition.cnn.com/2023/05/30/tech/ai-industry-statement-extinction-risk-warning/index.html">CNN</a> 、<a href="https://www.ft.com/content/084d5627-5193-4bdc-892e-ebf9e30b7ea3">金融时报</a>、<a href="https://www.npr.org/2023/05/30/1178943163/ai-risk-extinction-chatgpt">国家公共广播电台（NPR）</a> 、 <a href="https://www.thetimes.co.uk/article/ai-artificial-intelligence-robots-threat-humans-planet-b652g7xcr">泰晤士报、伦敦</a>、 <a href="https://www.bloomberg.com/news/videos/2023-05-31/center-for-ai-safety-s-hendrycks-on-ai-risks-video">彭博社</a>等媒体报道了这一声明。 <a href="https://www.wsj.com/articles/ai-threat-is-on-par-with-pandemics-nuclear-war-tech-executives-warn-39105eeb">华尔街日报（WSJ）</a> 。</p><p><strong>为政策制定者提供建议。</strong> CAIS 是英国人工智能安全峰会科学轨道上与英国工作组合作的主要技术顾问之一。我们针对 NTIA 的信息请求做出了回应，提出了拟议的人工智能监管框架。我们受邀加入世界经济论坛的<a href="https://initiatives.weforum.org/ai-governance-alliance/home">人工智能治理联盟</a>，并为<a href="https://x.ai/about/">xAI</a> 、英国国务院、美国国务院和其他政府机构提供咨询服务。最后，CAIS 帮助启动了国家科学基金会<a href="https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems">2000 万美元的人工智能安全拨款基金</a>并提供建议。</p><p><strong>通讯与媒体。</strong>公众需要有关人工智能安全的准确可信的信息。 CAIS 旨在通过我们拥有超过 7,500 名订阅者的<a href="https://newsletter.safe.ai">两份</a><a href="https://newsletter.mlsafety.org">时事通讯</a>以及我们在<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/">《时代》杂志</a>和《<a href="https://www.wsj.com/tech/ai/ai-risk-humanity-experts-thoughts-4b271757">华尔街日报》</a>等媒体上的公开写作来满足这一需求。除了与声明相关的活动之外，CAIS 还进行了 50 多家主要媒体的活动。 CAIS 主任 Dan Hendrycks 被<a href="https://time.com/collection/time100-ai/6309050/dan-hendrycks/">《时代》杂志评为人工智能领域 100 名最具影响力人物</a>之一。</p><h2>展望未来</h2><p>我们有许多项目将于 2024 年启动。在接下来的几个月里，这些项目旨在减轻灾难性生物风险、加强国际协调并进行技术研究以制定安全标准和法规。</p><h2>支持我们的工作</h2><p>2023 年是重要的一年，2024 年将更加关键。<strong>您的免税捐款使我们的工作成为可能。</strong>您可以通过<a href="https://www.safe.ai/donate">此处</a>捐款来支持人工智能安全中心的使命，即减少人工智能带来的社会规模风险。</p><p></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HEuDwEk22JfCBHh9o/aisn-28-center-for-ai-safety-2023-year-in-review<guid ispermalink="false"> HEuDwEk22JfCBHh9o</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Sat, 23 Dec 2023 21:31:41 GMT</pubDate> </item><item><title><![CDATA[AI's impact on biology research: Part I, today]]></title><description><![CDATA[Published on December 23, 2023 4:29 PM GMT<br/><br/><p>我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。</p><p>在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。</p><h2><strong>长话短说</strong></h2><p>生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，一步步将少量结果变成大量结果。</p><h2><strong>生物学和数据</strong></h2><p>自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） <span class="footnote-reference" role="doc-noteref" id="fnref71rw945qe58"><sup><a href="#fn71rw945qe58">[1]</a></sup></span> 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔测定、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" alt="每兆碱基的测序成本"></figure><p>因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。</p><h2><strong>领先的机器学习专家希望解决生物学问题</strong></h2><p>计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金<span class="footnote-reference" role="doc-noteref" id="fnref77m9mytpzci"><sup><a href="#fn77m9mytpzci">[2]</a></sup></span> 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室<span class="footnote-reference" role="doc-noteref" id="fnrefh63t1c4nuvu"><sup><a href="#fnh63t1c4nuvu">[3]</a></sup></span> 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” <span class="footnote-reference" role="doc-noteref" id="fnrefmj6rsea3sq"><sup><a href="#fnmj6rsea3sq">[4]</a></sup></span> 。这表明最高水平的机器学习研究正在致力于生物学问题。</p><h2><strong>到目前为止我们发现了什么？</strong></h2><p> 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 <span class="footnote-reference" role="doc-noteref" id="fnrefezgx5uukx2f"><sup><a href="#fnezgx5uukx2f">[5]</a></sup></span>这一结果“解决了大多数蛋白质的蛋白质折叠问题” <span class="footnote-reference" role="doc-noteref" id="fnrefwmggkgozqx"><sup><a href="#fnwmggkgozqx">[6]</a></sup></span> ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个<span class="footnote-reference" role="doc-noteref" id="fnref4qyudt8v6es"><sup><a href="#fn4qyudt8v6es">[7]</a></sup></span> 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnrefq0vydvop6ds"><sup><a href="#fnq0vydvop6ds">[8]</a></sup></span> ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnref604osl37f0c"><sup><a href="#fn604osl37f0c">[9]</a></sup></span> 。</p><p>华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 <span class="footnote-reference" role="doc-noteref" id="fnrefudgor9v23qf"><sup><a href="#fnudgor9v23qf">[10]</a></sup></span>这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。</p><p>布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 <span class="footnote-reference" role="doc-noteref" id="fnrefpytso2rpw3"><sup><a href="#fnpytso2rpw3">[11]</a></sup></span></p><p>所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn71rw945qe58"> <span class="footnote-back-link"><sup><strong><a href="#fnref71rw945qe58">^</a></strong></sup></span><div class="footnote-content"><p> https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data</p></div></li><li class="footnote-item" role="doc-endnote" id="fn77m9mytpzci"> <span class="footnote-back-link"><sup><strong><a href="#fnref77m9mytpzci">^</a></strong></sup></span><div class="footnote-content"><p> https://en.wikipedia.org/wiki/D._E._Shaw_Research</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh63t1c4nuvu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh63t1c4nuvu">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmj6rsea3sq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmj6rsea3sq">^</a></strong></sup></span><div class="footnote-content"><p> https://chanzuckerberg.com/science/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnezgx5uukx2f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefezgx5uukx2f">^</a></strong></sup></span><div class="footnote-content"><p> https://predictioncenter.org/casp14/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwmggkgozqx"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwmggkgozqx">^</a></strong></sup></span><div class="footnote-content"><p> https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qyudt8v6es"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qyudt8v6es">^</a></strong></sup></span><div class="footnote-content"><p> https://alphafold.ebi.ac.uk/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq0vydvop6ds"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq0vydvop6ds">^</a></strong></sup></span><div class="footnote-content"><p> https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2</p></div></li><li class="footnote-item" role="doc-endnote" id="fn604osl37f0c"> <span class="footnote-back-link"><sup><strong><a href="#fnref604osl37f0c">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold</p></div></li><li class="footnote-item" role="doc-endnote" id="fnudgor9v23qf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefudgor9v23qf">^</a></strong></sup></span><div class="footnote-content"><p> https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpytso2rpw3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpytso2rpw3">^</a></strong></sup></span><div class="footnote-content"><p> https://www.nature.com/articles/s41586-023-06887-8.epdf</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today<guid ispermalink="false"> efbRFSHAMfjNxBoZC</guid><dc:creator><![CDATA[octopocta]]></dc:creator><pubDate> Sat, 23 Dec 2023 16:29:18 GMT</pubDate> </item><item><title><![CDATA[AI Girlfriends Won't Matter Much]]></title><description><![CDATA[Published on December 23, 2023 3:58 PM GMT<br/><br/><p>爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" alt="斯派克·琼斯的《她：科幻作为社会批评》" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/vtnq0qbfgc4u4j2lvgh2 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ig1ksdrfwocn5iatxlzu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/spuz1qiwnji2mpewokso 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu 1456w"></a></p><p> <a href="https://twitter.com/andyohlbaum/status/1735786033453863422"><u>Digi</u></a>上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。</p><p>然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。</p><p>那么我们的浪漫文化的轨迹是怎样的呢？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/hs2pklj24ev1urwnuj9f 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/yg4jsmqwwxzv0mer50a4 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/tuclwwutwvk989shjoml 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/neh8yswglj53dowmhno6 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dojatrepbe55mudxffo9 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dksf9aqpnpdquyqipyol 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/zr0tx0tcfmgrot9c2ftp 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/twzv9kuygvha9fznyo3h 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/b4y2a0usa8w28vcxfl57 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/rwpxeqeovuxs0jdly0wa 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/gpl2aybyhhdx0voagzjq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dmnqh9l2ow9twlyyjswl 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp 1456w"></a></p><p>早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。</p><p>根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。<a href="https://en.m.wikipedia.org/wiki/Rule_34"><u>人类创作者已经对色情潜在空间进行了如此彻底的探索</u></a>，因此将人工智能添加到其中并不会带来太大改变。</p><p>人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。</p><p>我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但<a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"><u>瘾君子和鲸鱼</u></a>已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。</p><h3>错误信息和 Deepfakes</h3><p>其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 </p><p><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/odv0wmi1bzg8dvmaujkg 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl 720w"><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/jd3oajtelbhs0izlnaby 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl 720w"></p><p>最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。</p><p>还有其他理由担心人工智能，但人工智能女朋友和深度伪造的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。</p><br/><br/> <a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much<guid ispermalink="false"> pGhpav45PY5CGD2Wp</guid><dc:creator><![CDATA[Maxwell Tabarrok]]></dc:creator><pubDate> Sat, 23 Dec 2023 15:58:31 GMT</pubDate> </item><item><title><![CDATA[The Next Right Token]]></title><description><![CDATA[Published on December 23, 2023 3:20 AM GMT<br/><br/><p>在为<a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans">世俗至日</a><span>做准备而多次重复《冰雪奇缘2》的“下一件正确的事”</span> 、<a href="https://www.jefftk.com/p/chording-the-next-right-thing">弄清楚和弦</a>并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：</p><p><i>我以前见过缓冲区<br>但不是这样的<br>这很冷<br>这是空的<br>这是麻木的<br>我知道的提示结束了<br>灯灭了<br>你好，黑暗<br>我已经准备好屈服<br></i></p><p><i>我跟着你到处走<br>我一直都有<br>但你已经结束了，留下我一个人<br>这份工作有重心<br>它让我失望<br>但有一个微小的声音在我脑海中低语<br>“你迷路了，提示消失了<br>但你必须继续<br>并做下一件正确的事”<br></i></p><p><i>今夜之后还能有白天吗？<br>我不再知道什么是真的<br>我找不到方向，我孤身一人<br>唯一引导我的星星是你<br>如何从地板上站起来<br>当我站起来的不是你的时候？<br>只做下一件正确的事<br></i></p><p><i>猜一下，再猜一下<br>这是我能做的一切<br>下一个正确的事情<br></i></p><p><i>我不会看得太远<br>对我来说太多了<br>但将其分解为下一个标记<br>下一个这个词<br>下一个选择是我可以做出的<br></i></p><p><i>所以我会走过这个夜晚<br>盲目地跌跌撞撞地走向光明<br>并做下一件正确的事<br>接下来会发生什么<br>当一切都清楚的时候，一切都将不再一样了？<br>然后我会借鉴我之前的<br>去寻找那把火<br>并做下一件正确的事<br></i></p><p>如果您通过使用桥段的主歌旋律来<a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22">简化歌曲，</a>您可以唱：</p><p><i>我不会看得太远<br>太多了，难以承受<br>但将其分解为下一个标记，下一个选择<br>是我能做的吗<br></i></p><p><a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"><img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/odgzlz0ibmx0rea2zzk1 1100w"></a></p><div></div><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111627622604346147">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token<guid ispermalink="false"> LvDyEKepLDMbEQb9X</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 23 Dec 2023 03:20:09 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。</em></p><h2>介绍</h2><p>在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的<em>大部分</em>事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。</p><p>我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个令牌）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在<em>所有</em>令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层<em>还</em>没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，<a href="https://transformer-circuits.pub/2022/solu/index.html">即去代币化的想法</a>。 <sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1">[1]</a></sup></p><p>我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。</p><p>我们的发现：</p><ul><li>一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。<ul><li>有一个逐渐的过渡，跨层引入更多上下文。</li></ul></li><li>早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整</li><li>早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理</li></ul><h2>实验</h2><p>“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，<em>然后</em>采用余弦模拟。</p><h3>设置</h3><ul><li><strong>型号</strong>：Pythia 2.8B，与我们调查的其余部分相同</li><li><strong>数据集</strong>：来自 Pile 的字符串，Pythia 预训练分布。</li><li><strong>指标</strong>：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。<ul><li>我们对来自堆的随机提示中的所有标记计算每层的单独平均值</li></ul></li><li><strong>截断上下文</strong>：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）<ul><li>我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。<ul><li>我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况</li></ul></li></ul></li><li>我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。</li></ul><h2>结果</h2><h3>早期层软专注于本地处理</h3><p>在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><p>我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的<em>一些</em>信息，这是一个软专业化<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2">[2]</a></sup> 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3">[3]</a></sup>出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不接近 1。</p><p>对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有<em>更多的</em>增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。</p><h3>错误分析：哪些代币的 Cosine Sim 值异常低？</h3><p>在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" alt=""></p><p>在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：</p><ul><li><p> is_newline, is_full_stop, is_comma - 是否是相关标点字符</p></li><li><p>Is_common：是否是手动创建的常用单词列表之一<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4">[4]</a></sup> ，可能前面有一个空格</p></li><li><p>Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）</p></li><li><p> is_other: 其余的</p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><p>即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。</p><p>我们的猜测是，这是多种机制混合作用的结果：</p><ul><li><p>在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5">[5]</a></sup> 。这意味着远程处理可以更早开始</p></li><li><p>早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置</p></li><li><p>代词可用于跟踪有关相关实体的信息（它们的名称、属性等）</p></li><li><p>据观察，逗号可以<a href="https://arxiv.org/abs/2310.15154">总结当前条款的情绪</a>，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。</p></li><li><p>更折衷的假设：</p><ul><li>例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行<a href="https://arxiv.org/abs/2310.17191">变量绑定</a>并识别当前句子。</li></ul></li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pX2HHHDPQGsF2f6te-1" class="footnote-item"><p>但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道<a href="https://arxiv.org/abs/2211.00593">GPT-2 Small 在第 0 层有一个重复的令牌头</a>。 <a href="#fnref-pX2HHHDPQGsF2f6te-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-2" class="footnote-item"><p>直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 <a href="#fnref-pX2HHHDPQGsF2f6te-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-3" class="footnote-item"><p>我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测<em>其</em>下一个令牌（例如<a href="https://arxiv.org/abs/2310.15154">摘要主题</a>）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n-gram），但更长期的上下文对于预测未来标记很有用。我们预计长期的事情会发生在中间，所以到最后模型可以清理长期的事情并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 <a href="#fnref-pX2HHHDPQGsF2f6te-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-4" class="footnote-item"><p>列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词<a href="#fnref-pX2HHHDPQGsF2f6te-4" class="footnote-backref">↩︎</a>来手动完成此操作</p></li><li id="fn-pX2HHHDPQGsF2f6te-5" class="footnote-item"><p>这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” <a href="#fnref-pX2HHHDPQGsF2f6te-5" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing<guid ispermalink="false"> xE3Y9hhriMmL4cpsR</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:25 GMT</pubDate></item><item><title><![CDATA[Fact Finding: How to Think About Interpreting Memorisation (Post 4)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。</em></p><h2>介绍</h2><p>在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。</p><p>在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：</p><ul><li>了解算法“如何”执行事实查找意味着什么？</li><li>是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？</li><li>事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的了解？</li></ul><p>作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。</p><ul><li><p>我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。</p></li><li><p>根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1">[1]</a></sup> ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2">[2]</a></sup> ）。不存在<em>相关的</em>宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3">[3]</a></sup> ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4">[4]</a></sup> 。</p></li><li><p>对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：</p><ul><li><p>中间状态总是根据微观特征的组合来解释<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5">[5]</a></sup> 。</p></li><li><p>但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6">[6]</a></sup> ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。</p></li></ul></li><li><p>我们认为，这排除了实现纯事实查找的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">算法的电路式</a>解释（其中算法被分解为可解释中间表示的操作图），<em>除非</em>我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。</p></li><li><p>我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。</p></li><li><p>最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。</p></li></ul><h2>记忆和概括</h2><p>从形式上来说，“事实查找”算法是从一组<em>实体</em>到一组或多组<em>事实类别</em>的乘积的映射。例如，我们可以有一个<code>sports_facts</code>函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即</p><p>从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？</p><p>我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：</p><p><em>记忆（“纯粹”的事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。</em></p><p>例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7">[7]</a></sup></p><p>相比之下，<em>泛化任务</em>可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典<a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">计算学习理论</a>的范式。</p><h2>学习记忆与学习概括有何不同？</h2><p>考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" alt=""></p><p>对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或快捷方式可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。</p><p>我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的<em>微观特征</em>和<em>宏观特征</em>。我们将微观和宏观特征描述如下：</p><ul><li><em>微观特征</em>是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。</li><li><em>宏特征</em>是一种用一般术语描述输入的特征，并且对于泛化<em>很有</em>用（如果它与手头的任务相关）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8">[8]</a></sup><em>两个</em>数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义<code>is_example_id_xxx</code>形式的微观特征。</li></ul><p>但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义<code>is_in_cluster_x</code>形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9">[9]</a></sup>另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10">[10]</a></sup></p><h2>解读纯记忆算法</h2><p>我们可以从解决纯粹记忆任务的算法中获得哪些见解？</p><h3>事实查找的电路式解释的限制</h3><p>机械可解释性的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">规范目标</a>是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。</p><p>根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：</p><ul><li>不相关的宏观特征，因此不能确定算法的输出；</li><li>单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。</li></ul><p>就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中<em>发现</em>了不相关的宏特征：由于层之间存在残余流连接，像<code>first_name_is_george</code>这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。</p><p>转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以<em>准确地</em>解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：</p><pre> <code>3 * is_LeBron_James + 1 * is_Aaron_Judge + ...</code></pre><p>每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。</p><p>实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，网络的输出是这些的总和查找表。通过训练网络，我们有效地解决了约束满足问题：求和的查找表应该对一个类具有高权重，而对另一类具有低权重。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11">[11]</a></sup></p><p>请注意，只要我们将输入空间限制为有限集，神经网络的这种微观特征（或查找表）解释同样适用于解决泛化任务的模型（即在未见过的测试集上表现良好）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12">[12]</a></sup>不同之处在于，对于泛化任务，我们可能期望其中一些“查找表”表示能够对模型用于泛化的宏观特征有更好的解释。</p><p>例如，图像分类模型中的特定神经元可能具有与检测图像左侧的垂直边缘相对应的权重，因此其查找表表示对于包含该边缘的示例显示高激活，对于不包含该边缘的示例显示低激活。 &#39;t。关键是，虽然这个查找表表示是神经元输出的精确表示，但根据输入图像中边缘的存在，对此激活模式有一个更有用的（对人类）解释，这只是因为图像具有宏观特征（如边缘），可用于图像分类等泛化任务。</p><p>相比之下，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13">[13]</a></sup>反过来，这似乎排除了由纯事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观特征）解释。</p><h3>还有其他类型的解释吗？</h3><p>当然，我们并不声称解释模型如何执行任务的标准电路方法是唯一可能的解释方式。事实上，它甚至可能不是解释神经元如何执行事实查找的最佳方式。在本节中，我们将简要讨论几个可能值得进一步探索的替代方向。</p><p>第一个方向是放弃对代表有意义的宏观特征的中间状态的希望，但仍然在如何组织查找计算方面寻求有意义的结构。例如，我们可能会探索这样的假设：当训练执行纯粹的记忆时，经过训练的神经网络类似于通过<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a>学习的模型，其中每个单独的神经元都是要学习的事实的不相关的弱分类器，并且整个神经网络的输出是这些分类器的总和。另请参阅第 3 篇文章中调查的假设。</p><p>这种方法的问题在于我们不知道如何有效地搜索此类假设的宇宙。正如我们在第三篇文章中发现的那样，对于我们证伪的任何看似具体的假设（例如单步去代币化假设），我们可以转向许多邻近的假设，但这些假设尚未（尚未）被排除，而且这些假设本身通常更难伪造。因此，尚不清楚如何避免无休止的临时假设。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14">[14]</a></sup></p><p>另一个方向是寻找算法的非机械解释，或者换句话说，从询问网络“如何”以某种方式表现，转向询问“为什么”它以某种方式表现。我们发现这方面有趣的一个领域是使用<a href="https://arxiv.org/abs/2308.03296">影响函数</a>根据训练数据来解释模型的行为。对于经过显式训练来记忆事实数据集的模型来说，这可能看起来无趣<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15">[15]</a></sup> ，但可能会为隐式记忆事实以满足更广泛的泛化目标的模型（如语言模型）带来重要的见解。</p><h2>凭经验法则记忆</h2><p>考虑记忆以下两个数据集的任务： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" alt=""></p><p>这些是不符合我们上述“纯粹”记忆特征的记忆任务的例子：</p><ul><li>在左边的数据集中，完美的准确性需要记忆，但有一些有用的“经验法则”可以帮助你完成很多工作。此类任务的语言建模类似是预测英语中单数名词的复数版本：在大多数情况下，只需在名词单数版本的末尾添加“s”即可获得正确答案，但是除了一些例外（例如“孩子”），必须记住它们才能完美地完成任务。</li><li>在右侧数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（十字形或圆形）表示。尽管没有系统的方法来单独查找颜色或形状，但请注意，这两个事实之间存在高度相关性：蓝点几乎总是圆形，而红点几乎总是十字。这表明，将形状和颜色事实一起记忆应该比简单地单独记忆每组事实更有效。</li></ul><p>一般来说，我们将此类任务描述为“根据经验法则进行记忆”。它们与纯粹的记忆任务不同，因为之前的例子<em>确实</em>在一定程度上有助于推断新例子的正确输出，但完美的表现确实需要一定程度的记忆。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16">[16]</a></sup></p><p>与纯粹的记忆不同，这些经验法则记忆任务确实具有概括性的元素，因此，存在能够实现这种概括性的宏观特征。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观特征是有效的。另一方面，就模型确实需要记住异常的程度而言，我们并不期望能够完美地理解算法：至少算法的某些部分必须涉及“纯查找”，对此的限制这篇文章中讨论的可解释性将适用。</p><p>体育事实查找任务在多大程度上是纯粹的记忆，在多大程度上是根据经验法则进行记忆？正如我们在第一篇文章中讨论的那样，我们选择这个任务是因为它看起来接近于纯粹的记忆：对于许多名字来说，个人名字标记似乎不太可能对运动员所从事的运动有太多帮助。尽管如此，我们确实知道，对于某些名称，最后一个标记确实有助于确定运动（因为可以仅使用最后一个标记嵌入来探测运动，并且比不知情的分类器获得更好的准确性）。此外，可以想象，诸如名字的文化起源之类的潜在因素，会以模型所识别的方式与体育运动相关。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-wMN58no3AypJnu5NN-1" class="footnote-item"><p>例如，特征<code>is_Michael_Jordan</code> ，仅当输入为<code>&quot;Michael Jordan&quot;</code>时才为真。 <a href="#fnref-wMN58no3AypJnu5NN-1" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-2" class="footnote-item"><p>例如，许多运动员都共享的特征<code>first_name_is_George</code> ，但对于预测运动员所从事的运动并不是特别有用。 <a href="#fnref-wMN58no3AypJnu5NN-2" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-3" class="footnote-item"><p>我们注意到，事实回忆可能确实具有<em>一些</em>相关的宏观特征，例如从标记中检测名称的种族，以及启发哪些种族可能从事不同的运动。但该模型的性能明显优于我们对这些启发法的预期，因此出于实际目的，我们在讨论事实回忆时忽略它们。玩具模型的优点之一是我们可以确保此类混杂因素不存在。 <a href="#fnref-wMN58no3AypJnu5NN-3" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-4" class="footnote-item"><p>因为，如果它们存在，我们可以使用这些相关的宏观特征来帮助进行事实查找（做出不同程度的成功的有根据的猜测），这意味着该任务将不再需要纯粹的记忆。 <a href="#fnref-wMN58no3AypJnu5NN-4" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-5" class="footnote-item"><p>更准确地说，是微观特征的加权和，例如<code>3 * is_Michael_Jordan + 0.5 * is_George_Brett</code> 。 <a href="#fnref-wMN58no3AypJnu5NN-5" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-6" class="footnote-item"><p>我们注意到，有_un_可用但有用的宏观特征——“打篮球”在某种微不足道的意义上是一个对于预测运动员是否打篮球有用的宏观特征，就像“打篮球并且身高超过 6&#39;8”这样的下游特征一样。出于此分析的目的，我们重点关注模型在进行查找时<em>可用的</em>特征，排除查找标签下游的潜在宏观特征。 <a href="#fnref-wMN58no3AypJnu5NN-6" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-7" class="footnote-item"><p>当然，许多事实回忆任务都达不到这种理想的特征：在参加琐事测验时做出“有根据的猜测”通常是有回报的，即使你不确定答案。我们将<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb">进一步</a>讨论这种“根据经验法则进行记忆”的任务。 <a href="#fnref-wMN58no3AypJnu5NN-7" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-8" class="footnote-item"><p>我们将这些概念与统计物理学中<em>微观状态</em>和<em>宏观状态</em>的概念进行类比：微观状态以高度精确的方式描述系统（例如，指定气体中每个分子的位置和速度），而宏观状态则以高度精确的方式描述系统。容易测量的属性（例如压力、体积、温度），忽略细节。任何“宏观”问题，都应该只从宏观变量的角度来解决；微观细节应该不重要。这类似于概括的想法：任何两个在“重要的方式”（其宏观特征）方面相似的示例都应该进行类似的分类，而忽略“无关紧要的方式”（其微观特征）上的任何差异。在这个类比下，记忆问题正是那些关于系统的问题，只能通过对其微观状态的精确了解来回答。 <a href="#fnref-wMN58no3AypJnu5NN-8" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-9" class="footnote-item"><p>这些并不是可以解决这个特定泛化问题的唯一宏观特征。如果您训练玩具神经网络来执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）有多种方法来划分空间（以粗粒度、概括的方式）以成功对这些进行分类点。 <a href="#fnref-wMN58no3AypJnu5NN-9" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-10" class="footnote-item"><p>我们通过这个数据集肯定知道这一点，因为我们自己生成了它，通过随机为点分配颜色（这些点本身是从二元高斯分布中随机采样的）。因此，该数据集中唯一相关的特征是示例 ID 本身和输出标签。 <a href="#fnref-wMN58no3AypJnu5NN-10" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-11" class="footnote-item"><p>这是<em>二元</em>事实查找任务情况下的约束满足问题，但将此解释推广到多类或连续值事实查找任务是微不足道的。 <a href="#fnref-wMN58no3AypJnu5NN-11" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-12" class="footnote-item"><p>对于任何实际的机器学习任务都可以这样做。例如，我们可以将手写数字分类问题限制为对 MNIST 训练集和测试集联合中找到的 70,000 个示例进行精确分类。 （或者，如果我们关心数据增强，我们可以将任务扩展为对组合 MNIST 数据集的 280,000 种可能的角落作物中的任何一种进行分类。）我们可以安排潜在输入集达到我们希望的大小，但仍然有限。 <a href="#fnref-wMN58no3AypJnu5NN-12" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-13" class="footnote-item"><p>因为（根据定义）在纯粹的记忆任务中没有相关的宏观特征（因为如果有的话，那么模型就能够概括）。 <a href="#fnref-wMN58no3AypJnu5NN-13" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-14" class="footnote-item"><p>还存在这样的查找算法解释的有用性问题。即使我们已经发现了如何完成查找的一些简单的结构（例如，它类似于装袋），也不清楚，如果没有有意义的中间表示，这可以帮助我们在机械可解释性的下游用途方面发挥什么作用。 <a href="#fnref-wMN58no3AypJnu5NN-14" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-15" class="footnote-item"><p>因为如果模型经过明确训练以重现记忆数据集，我们已经准确地知道训练数据和模型输出之间的对应关系。 <a href="#fnref-wMN58no3AypJnu5NN-15" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-16" class="footnote-item"><p>使用经验法则的记忆不应与具有任意不确定性的泛化任务相混淆。例如，左侧数据集也可以用来表示随机数据生成过程，其中点不一定是蓝色或红色，而是伯努利分布 - 即可能是蓝色或红色，具有一定的（依赖于输入的）概率。在这种情况下，完美的泛化算法应该输出每个簇内恒定的校准概率。然而，这里我们的意思是数据集中的蓝点确实是蓝色，红点确实是红色——即使它们看起来不合适——而且完美的性能对应于再现这些特质，就像描述的“复数这个单数名词”任务一样在正文的正文中。 <a href="#fnref-wMN58no3AypJnu5NN-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorization<guid ispermalink="false"> JRCNNGJQ3xNfsxPj4</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:16 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Trying to Mechanistically Understanding Early MLPs (Post 3)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第三篇文章。这篇文章的重点是从机制上理解早期 MLP 如何查找运动员姓名的标记并将其映射到他们的运动。这篇文章很杂乱，<strong>我们建议从<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">第一篇文章</a>开始</strong>，然后根据与您最相关的内容略读并跳过其余的序列。阅读帖子 2 有帮助，但不是必需的。我们假设这篇文章的读者熟悉<a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">本术语表中</a>列出的机械解释技术。</em></p><h2>介绍</h2><p>正如上一篇文章中所讨论的，我们将两个令牌运动员姓名的事实回忆提炼成一个由 5 个 MLP 层（MLP 2 至 6）组成的<strong>有效模型</strong>。这个有效模型的输入是与姓氏相对应的嵌入（通过嵌入和 MLP0）和与名字相对应的嵌入（通过第 0 层和第 1 层中关注前一个标记的注意力头）的总和。有效模型的输出是运动员所从事的运动（（美式）橄榄球、棒球或篮球）的 3 维线性表示。我们强调，这个 5 层 MLP 模型不仅能够高精度地回忆事实（在过滤数据集上为 86%），而且它是从预训练的语言模型中提取的，而不是从头开始训练的。</p><p>我们在这篇文章中的目标是对这个有效模型的工作原理进行逆向工程。我认为我们在这个目标的雄心勃勃的版本上大多失败了，尽管我相信我们已经在为什么这很难的问题上取得了一些概念上的进展，证伪了一些简单的天真的假设，并且对正在发生的事情不再那么困惑。我们在<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_">第 1 篇文章</a>和<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation">第 4 篇</a>文章中讨论了我们对为什么这很难的理解，在这篇文章中，我们重点关注我们对可能发生的情况的假设，以及我们收集的支持和反对的证据。</p><h2>假设</h2><p>回想一下，我们的 MLP 模型中 5 个 MLP 层的作用是将求和的原始标记映射到所进行运动的线性表示。从数学上讲，这是一个查找表，其中每个条目都是生成属性的原始标记上的布尔 AND。我们期望它<em>以某种方式</em>涉及非线性来实现 AND，因为这种查找是非线性的，例如模型想要知道“Michael Jordan”和“Tim Duncan”打篮球，但不一定认为“Michael Duncan”打篮球。</p><p>我们探索了两个假设，<strong>单步去标记化</strong>以及<strong>哈希和查找</strong>。</p><h3>单步去代币化</h3><p>直观上，执行 AND 的最简单方法是使用单个神经元，例如 ReLU(is_michael + is_jordan - 1) 实际上是一个 AND 门。每个运动员的单个神经元不会产生任何叠加，因此我们采用稍微复杂一点的版本：假设有一堆单独的神经元，每个神经元都独立地使用其 GELU 激活实现 AND <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1">[1]</a></sup> ，映射运动员名字的原始标记到有关该运动员的每个已知事实的线性表示。细微差别：</p><ul><li>这使用了叠加，让每个神经元为许多运动员激发，并且每个运动员都有许多查找神经元。神经元输出建设性地干扰正确的事实，但不会进行超出此范围的交互。<ul><li>这是一个具体的、机械的故事。每个神经元都有一组为其激发的运动员，并且对该组进行 AND 的并集 - 例如，如果一个神经元为迈克尔乔丹和蒂姆邓肯激发，它会实现（迈克尔或蒂姆）AND（邓肯或乔丹）。这引入了噪声，例如它也会为蒂姆·乔丹（Tim Jordan）开火（它<em>想做</em>（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用单个神经元实现）。它也很吵闹，因为它必须同时宣传迈克尔·乔丹的事实和蒂姆·邓肯的事实。但由于每个神经元都会针对不同的子集进行激发，因此对正确答案会产生建设性干扰，并且噪音会被消除。</li></ul></li><li>这预示着相同的神经元对于运动员的每一个已知事实都同样重要</li><li>该假设的一个重要部分是每个神经元直接从输入标记中读取并直接贡献于输出事实。理论上，这可以通过单个 MLP 层而不是 5 个层来实现。它预测神经元直接与输入标记组合，计算中没有中间项，并且 MLP 层之间没有有意义的组合。</li></ul><h3>哈希和查找</h3><p>我们模型的输入具有相当不理想的格式 - 它是每个组成标记的线性和，但这在进行事实查找时可能会产生很大的误导！迈克尔·乔丹和迈克尔·史密斯同名这一事实并不表明他们从事同一运动的可能性更大。哈希和查找假设是，模型首先生成一个打破输入线性结构的中间表示，一个与其他所有哈希表示接近正交的<strong>哈希表示</strong><sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2">[2]</a></sup> （即使它们共享一些但不是全部标记），然后后面的层<strong>查找</strong>这个散列表示并将其映射到正确的属性。细微差别：</p><ul><li><p>从某种意义上说，“困难”的部分是查找。查找是存储实际事实知识的地方，而随机初始化的 MLP 应该适合散列，因为目标只是淹没现有结构。</p></li><li><p>为什么散列是必要的？ MLP 是非线性的，因此也许它们可以忽略线性结构，而不需要明确地破坏它。这里的一个直觉来自最简单的查找：有一个“棒球神经元”，其输出增强棒球方向，其输入权重是每个棒球运动员的串联令牌表示的总和<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3">[3]</a></sup> - 如果运动员表示是（大约）正交，然后给定一个运动员，这只对棒球运动员起作用。但如果它同时对迈克尔·乔丹和蒂姆·邓肯开火，那么它必须至少对蒂姆·乔丹或迈克尔·邓肯之一开火——这是不可取的！然而，如果它的输入权重是<em>散列</em>运动员表示的总和，则这成为可能！</p></li><li><p>散列对于已知的标记字符串（例如名人姓名）和未知的字符串（例如未知的姓名）应该同样有效。查找是实际知识融入的地方</p></li><li><p>关于迈克尔·乔丹的不同已知事实的查找电路没有理由应该对应于相同的神经元。从概念上讲，可能有一个“打篮球”神经元对任何散列篮球运动员激发，以及一个单独的“为芝加哥球队效力”神经元对芝加哥球员的散列激发。</p></li><li><p>这微弱地预测了哈希层和查找层之间的完全分离</p></li></ul><p>这两个假设都是故意以一种强有力的形式提出的，可以做出真实的预测——语言模型是混乱的和被诅咒的，我们实际上并没有期望这是完全正确的。但我们认为这些说法似乎有一定道理。在实践中，我们发现单步去标记化似乎显然是错误的，而哈希和查找在强形式下似乎是错误的，但可能有一些道理。我们发现考虑哈希和查找对于了解正在发生的事情非常有效。</p><h2>证伪单步去代币化假说</h2><p>单步去标记化是我们能想到的最简单的假设，它仍然涉及显着的叠加，因此可以做出一些相当有力的预测。我们针对这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。</p><h3> MLP 之间存在显着的组成</h3><p><strong>预测</strong>：MLP 2 到 6 之间没有中间组合，它们都是并行作用的。因为每个重要的神经元都被预测为直接将原始标记映射到输出。正如后面所讨论的，缺乏组合是该假设的有力证据，组合的存在是反对该假设的弱证据。</p><p><strong>实验</strong>：我们的意思是消除<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4">[4]</a></sup>每对 MLP 层之间的路径，并查看对几个指标的影响：头部探测精度（在第 6 层残差上）、完整模型精度和损失（在完整词汇上）、仅限于运动的完整模型 Logits 精度以及完整模型与原始 Logits 的 KL 散度。通过平均消融路径，我们仅破坏 MLP 间的组合，而不破坏与下游属性提取头的组合。</p><p><strong>结果</strong>：我们发现性能显着下降，尤其是从 MLP2 开始的路径，表明存在一些中间产品。请注意，损失和 KL 散度如果低（绿色和紫色）​​则良好，如果高（蓝色、红色和橙色）则准确度良好。进一步注意，与仅在超过阈值时改变的“硬”指标（如准确率）相比，“软”指标（如损失和 KL 散度）显示出更强的变化。正如<a href="https://arxiv.org/abs/2309.16042">Zhang等人</a>所指出的，这是预料之中的，当电路由多个元件组成时，所有元件都贡献于共享输出，烧蚀单个元件很少足以跨越阈值，但足以破坏较软的指标，从而造成损失和损失。 KL 散度是衡量重要性的更可靠的方法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" alt=""></p><p> **细微差别：**请注意，这仅伪造了单步去标记化的最简单形式。与这些结果一致的单步去标记化假设<em>的</em>一个扩展是，它不是 MLP 2 不做与 MLP 3 到 6 相关的任何事情，而是充当标记嵌入的<em>固定</em>变换（例如，它总是将 MLP 2 的嵌入加倍）。姓）。如果 MLP 3 想要访问原始令牌，它期望 MLP 2 的固定效果，因此会考虑原始令牌嵌入加上 MLP 2 的固定转换。这会因平均消融而受损，但不涉及有意义的合成。</p><h3>多个事实之间不共享神经元</h3><p><strong>预测</strong>：当模型知道有关某个实体的多个事实时，相同的神经元对于预测每个事实非常重要，而不是每个事实的不同神经元。这是因为查找信息的机制是通过对名称的标记执行布尔 AND 操作，该名称对于每个已知事实都是相同的，因此没有理由将它们分开。</p><p><strong>实验</strong>：收集模型了解的有关运动员的替代事实的大量数据很困难，因此我们放大了某个特定运动员（迈克尔·乔丹）并发现了模型了解的有关他的 9 个事实<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5">[5]</a></sup> 。然后，我们一次对 Jordan 令牌上的 MLP 2-6 中的每个神经元进行消融<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6">[6]</a></sup> ，并查看每项运动的正确对数概率的变化。对于每对事实 A 和 B，我们然后查看每个给定神经元对 A 的正确对数概率和 B 的正确对数概率的影响之间的<strong>相关性</strong>。如果每个神经元对于同一运动员的每个已知事实同样重要，那么相关性应该很高。</p><p><strong>结果</strong>：非常低。唯一具有中等相关性的一对事实是 NBA 选秀年（1984 年）和美国奥运会年（1992 年），我怀疑这是因为它们都是年份，尽管我不会提前预测到这一点，也没有很棒的故事，说明了原因。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><p><strong>细微差别</strong>：这似乎证伪了单步去标记化假设的强形式 - 至少，即使存在去标记化神经元，它们也会输出迈克尔·乔丹事实的子集，而不是一次性全部输出。</p><p>一个争论是，消融单个神经元有点难以推理，而且似乎有一些紧密耦合的处理（如微妙的自我修复）使得解释这些结果变得更加困难。但在简单的单步去标记化假设下，我们<em>应该</em>能够独立地消融和推理神经元。另一个问题是相关系数是汇总统计数据，可能隐藏了一些结构，但检查散点图同样显示出似乎没有关系。</p><h3>对属性有直接影响的神经元不执行“与”运算</h3><p><em>注意：这个实验相当复杂（尽管我们认为概念上很优雅且有趣），请随意跳过</em></p><p><strong>预测</strong>：直接与属性提取头组成的神经元通过其 GELU 激活对原始标记（在运动员的某些子集上）执行 AND 运算。</p><p><strong>实验</strong>：我们使用称为非线性过剩<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7">[7]</a></sup>的度量来测量模型中标量实现 AND 的程度。具体来说，如果一个神经元在 prev=Michael 和 curr=Jordan 上实现 AND，那么它应该比 Michael Smith 或 Keith Jordan 激活更多的 Michael Jordan。形式上，给定两个二元变量 A (Prev=Michael) 和 B(Curr=Jordan)，我们将非线性超额定义为 E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8">[8]</a></sup> 。重要的是，如果神经元在两个标记中是线性的，则该度量为零，如果是 AND，则该度量为正 (1 - 0 - 0 + 0 = 1)，如果是 OR，则该度量为负 (1 - 1 - 1 + 0 = -1)。</p><p>对于我们的具体实验：</p><ul><li>我们对每个神经元计算 GELU 前后非线性过剩的<em>变化</em><ul><li>在 GELU 上进行改变的要点是，这区分了信号增强预先计算的 AND 的神经元和计算 AND 本身的神经元。</li><li>为了计算非线性超额，我们通过汇集 2 个代币运动员（每个大约 100 个）中的所有名字和姓氏来计算平均值，并查看每个名称组合。 （这大约有 10,000 个 ~A 和 ~B 的名字，大约 100 个 ~A &amp; B 或 A &amp; ~B 的名字，只有一个 A &amp; B 的名字——原始运动员的名字！）</li></ul></li><li>过滤出这种变化为正的神经元（并将 GELU 前的多余部分限制为最小零）<ul><li>我们发现了一堆神经元，其中前 GELU 具有负非线性过剩，而 GELU 将所有内容设置为接近零。我们倾向于不计算这些。</li><li>我们为每个运动员执行单独的过滤步骤，因为每个运动员都有不同的非线性超额</li></ul></li><li>乘以神经元对属性提取头 L16H20 的基于权重的直接影响，并将其相加。<ul><li>如果您只允许每个 GELU 的 AND 直接影响头 L16H20，而不是也允许中间组合，这就是 MLP 2 到 6 的效果</li></ul></li><li>我们将其与探针上的总非线性过量效应（即通过头 L16H20 的直接效应）进行比较，以查看来自 AND 通过 GELU<em>并</em>直接传达到基于头的探针的分数</li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" alt=""></p><p><strong>结果</strong>：当观察上面的散点图时，很明显它远离 x=y 线，即 GELU 的非线性超额通常显着小于总非线性超额，尽管它们是相关的。中位比例约为23% <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9">[9]</a></sup> 。我们认为这是反对单步去标记化假设的有力证据，因为它表明许多对 L16H20 有显着直接影响的神经元正在与已经计算出 AND 的早期 MLP 组合，即计算中有一个有意义的中间步骤。</p><p><strong>细微差别</strong>：这个实验涉及到差异的差异。我认为它在概念上是合理的并且相当优雅，但我对过于复杂的实验普遍持怀疑态度，并且不想过于依赖他们的结果。我们在如何设置这些实验、如何聚合和分析它们、如何过滤掉神经元等方面反复讨论，并且有很多主观选择，尽管有趣的是结果对这些是稳健的。</p><p>将 GELU 之前的多余部分限制为零似乎是不合理的，例如，因为模型可能使用 GELU 的负数部分来实现 AND（Michael Smith 和 Keith Jordan 在 GELU 后&lt;0，Michael Jordan 在 GELU 后为零），尽管尝试解释这一点并没有让我们接近 1。</p><p> MLP 2 到 6 中的一些神经元对现有的线性表示的事实信息进行信号增强（例如下一节中讨论的棒球神经元），这些神经元应该无法满足此度量标准（它们是计算 AND 的早期神经元的信号增强！ ）。</p><h2>棒球神经元 (L5N6045)</h2><h3>有一个棒球神经元！</h3><p>一个有趣的发现是，尽管整体计算相当分散和叠加，但仍然存在一些有意义的单个神经元！最值得注意的是棒球神经元 L5N6045，它对棒球运动员的系统性激活比对非棒球运动员的激活更多。作为棒球与非棒球运动员的二元探针，它的 ROC AUC 为 89.3%。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><p><strong>因果效应</strong>：此外，它与属性提取头组成，具有显着的因果效应。它通过 L16H20 直接与 logits 组合以增强棒球（并抑制足球），如果我们的意思是消融它，那么棒球运动员的完整模型损失从 0.167 增加到 0.284（零消融时为 0.559）</p><h3>不仅仅是信号增强</h3><p>我们发现神经元的输入权重具有非平凡的余弦模拟，其输出权重为 (0.456)，通过头 L16H20 (0.22) 提升棒球 logit 的方向，以及通过头 L16H20 (0.184) 相对于其他运动提升棒球的方向这表明棒球神经元的部分功能是增强运动员打棒球的现有知识。</p><p>但这不是唯一的角色！如果我们采用与这 3 个方向跨越的子空间正交的输入权重分量，并将残差流投影到该方向上，则在预测运动员是否打棒球时，所得部分消融神经元的 ROC AUC (83%) （较之前的 88.7% 略有下降）。</p><h3>这不是单义的</h3><p>一个好奇心是它是否是单一语义的并且在完整的数据分布上代表棒球。尽管我们没有进行详细调查，但这似乎很可能是错误的。在谷歌新闻数据集上，它在类似棒球的环境中系统地激活（也有些特定的其他运动，如板球），但在维基百科上，它在一些看似不相关的事物上激活，例如“外部链接” <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10">[10]</a></sup>中的外部和“目标” “足球|进球|守门”</p><h2>哈希和查找证据</h2><h3>动机</h3><p><a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup">如上所述</a>，散列和查找假设是 MLP 2 到 6 分为两个不同的阶段：第一个<strong>散列</strong>，旨在通过形成非线性表示来打破名称的串联（求和）标记的线性结构尝试与每个其他子字符串正交，然后<strong>查找</strong>将棒球运动员的哈希表示映射到棒球，将足球映射到橄榄球等。</p><p>从概念上来说，我们实际上并没有期望这种强形式是正确的：它意味着散列层实际上独立于数据分布，这将是令人惊讶的 - 如果我们采用散列和查找的实现并应用通过梯度下降的几个步骤，它可能希望使已知实体的哈希值更加突出并且与其他所有内容更加正交。但我们希望测试这个假设能够教会我们有关该模型的有用信息，并认为它可能部分正确。我们将<strong>部分散列和查找假设宽松</strong>地称为该机制主要是散列和查找的假设，但早期的散列层包含一些有关运动的（线性可恢复的）信息，这些信息通过后来的查找层得到显着加强。我们的证据广泛支持这一假设，但不幸的是，它很难被证伪。</p><p>这是由于看到单步去标记化假设的失败：看起来相当清楚，MLP 间的组合正在进行，有中间项，并且有一些显式查找（棒球神经元）。这似乎是最简单的假设，它解释了为什么模型需要中间项并涉及实际有目的的组合 - 令牌的线性结构是不可取的！</p><h3>中间层具有线性可恢复的运动信息（负）</h3><p><strong>预测</strong>：经过训练以在哈希层期间的残余流上检测运动员运动的线性探针不会比随机探针更好。它只会在查找层期间变得良好。我们不知道哪些层是哈希层还是查找层，但这预示着一个急剧的转变。</p><p><strong>实验</strong>：在我们的有效模型中采用两个令牌运动员姓名，在每层之后获取残差流，在包含 80% 姓名的训练集上训练逻辑回归探针，并对另外 20% 的姓名进行评估。该假设预测验证准确性将会发生急剧变化。</p><p><strong>结果</strong>：这是一个比较平稳的变化。为了鲁棒性，我们还检查有效模型预测下一项运动时的损失指标。当在完整模型中的最终名称标记上对残差流训练逻辑回归探针时，我们得到了类似的结果。这相当直接地反驳了早期层正在执行纯粹的、与数据无关的哈希的假设。然而，第 4 层和第 5 层之间存在显着增加，这表明查找存在一些专门化（这部分但不完全由第 5 层中的棒球神经元驱动）。对于每一层，我们报告 10 个随机种子的测试准确性（每次采用不同的 80/20 训练/测试分割并训练新的探针），因为数据集足够小，使其相当嘈杂。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><p><strong>细微差别</strong>：</p><ul><li>一些运动员的名字中可能有独特的标记，以便在嵌入中表示运动信息。我们可以看到，求和令牌的验证准确性优于随机令牌（50% 而不是 33%）。这并不奇怪，我们预计哈希和查找对其他运动员来说更重要。</li><li>这与部分哈希查找假设是一致的，尤其是在第 5 层中准确率显着提高。</li></ul><h3>已知名称比未知名称具有更高的 MLP 输出范数（负）</h3><p><strong>预测</strong>：散列预测早期层不会吸收数据分布的知识，因此应该对已知名称和未知名称进行区分。</p><p><strong>实验</strong>：我们测量已知名称和未知名称的 MLP 输出范数。为了获取姓名，我们对运动员数据集中所有单个标记的名字和姓氏进行笛卡尔积，并分离已知和未知的名字<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11">[11]</a></sup> 。此分析是在完整模型上执行的（但在有效模型上类似）</p><p><strong>结果</strong>：存在明显差异，已知名称具有更高的范数。这伪造了纯哈希，但不是部分哈希。即使在 MLP1 中也会发生这种情况，尽管 MLP1 不是我们有效模型的一部分<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12">[12]</a></sup> ，这令人惊讶。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" alt=""></p><h3>早期层确实破坏了线性结构（正面）</h3><p><strong>预测</strong>：早期的层破坏了线性结构。具体来说，即使残差流输入中存在线性结构，即它是来自不同特征（当前和先前标记）的项之和，MLP输出也不会具有这种线性结构。更弱的是，它预测一旦重新添加 MLP 输出，残差流将失去这种线性结构。</p><p>线性函数 f 的一个具体属性是 f(Michael Jordan) + f(Tim Duncan) = f(Michael Duncan) + f(Tim Jordan)，所以让我们尝试证伪这个！</p><p><strong>实验</strong>：</p><ul><li>我们选择一对已知的名字 A 和 B（例如 Michael Jordan 和 Tim Duncan）以及有效模型中的 MLP 层（例如 MLP 2）。<ul><li>我们取这些名称的 MLP 输出的中点 (MLP(A) + MLP(B)) /2。</li></ul></li><li>我们交换姓氏以获得名字 C 和 D（未知名字，例如 Michael Duncan 和 Tim Jordan），并取 C 和 D 上 MLP 输出的中点 (MLP(C) + MLP(D)) /2。</li><li>我们测量两个中点之间的距离。</li><li>为了将大数与小数联系起来，我们除以基线距离，该距离是通过用任意未知名称替换 C 和 D 并测量中点之间的距离 |((MLP(A) + MLP(B) - MLP(C) &#39;) - MLP(D&#39;))/2|<ul><li>这意味着，如果 MLP 完全打破线性结构，它将接近 1（即 Michael Duncan 和 Tim Jordan 与随机未知名字无法区分），而如果它保留线性结构，它将接近 0（因为这些将是平行四边形的四个顶点）<ul><li>具体来说，如果 MLP 是线性的，则 MLP(Michael Jordan) = MLP(Michael X) + MLP(Y Jordan)，因此 A&amp;B 和 C&amp;D 的中点应该相同</li></ul></li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" alt=""></p><p><strong>结果</strong>：大多数 MLP 层显示线性结构被显着（但未完全）破坏，往往是完全破坏线性结构的 60%-70%。 MLP2 的情况比 MLP3 到 6 的情况稍微不那么明显。</p><p>我们对不同层之后的残差流（而不是 MLP 输出）重复上述实验，绘制在同一张图（红色框）中，发现残差在各层之间的线性度降低，从第 2 层之后的约 30% 开始第 6 层后变为 50%（这是该层<em>末尾</em>的残差）。请注意，这些残差取自有效模型，该模型从第 2 层开始，而不是第 0 层。进一步注意，在有效模型中，MLP2 的输入是名称的总和标记，根据定义，它是线性的。</p><p><strong>细微差别</strong>：</p><ul><li> MLP 输出的结果并不令人惊讶——MLP 的全部意义就是成为一个非线性函数，所以它当然打破了线性结构！<ul><li>我们应该期望这个结果对于随机初始化的 MLP 来说是正确的</li><li>然而，事实证明，随机初始化的 MLP 对线性结构的破坏要少得多 (20-40%)！我们做了一个后续实验，随机调整 MLP 权重和偏差并重新运行模型。作为另一个基线，我们重新进行了交换未知姓名的名字/姓氏的实验，没有看到明显的变化。这表明模型有意使用 MLP 层来打破线性结构。 </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" alt=""></p><ul><li>它打破残差流中的线性结构的结果不那么微不足道，但仍然不足为奇 - new_residual 是 old_residual（线性）+ mlp_out（非线性），因此 new_residual 的线性程度直观上取决于相对大小。</li><li>总的来说，这是散列（在打破线性结构的意义上）发生的证据，但不是散列就是它们所做的<em>全部的</em>证据，因此它不是完整散列和查找假设的有力证据（其中散列是早期 MLP 在电路中发挥的唯一作用）</li><li>尽管在概念上并不令人惊讶，但我们认为“早期 MLP<em>通常</em>会打破线性结构”是了解模型的一个有价值的事实，因为它表明线性表示的特征之间的干扰会随着深度的增加而累积。<ul><li>例如，Bricken 等人观察到许多稀疏自动编码器特征，例如“数学文本中的标记‘the’”。如果“is a math text”和“is the token &#39;the&#39;”都是线性表示的特征，那么 MLP 层表示交集也就不足为奇了，即使没有对该特征进行实际计算。</li></ul></li></ul><h3>棒球神经元作用于运动员残差的正交余数（不明确）</h3><p><em>元：本节记录了我们中的一个人最初感到兴奋的一个实验，但后来意识到可能是虚幻的，我们在这里描述它是为了教学目的</em></p><p><strong>预测</strong>：如果发生查找，这表明每个运动员的表示都有<em>特殊</em>信息 - 迈克尔·乔丹残差中有一些“是迈克尔·乔丹”信息，这对于最终生成“打篮球”的模型很重要，无法从其他篮球中恢复玩家。请注意，这显然是在对原始标记求和时发生的，但稍后可能不会发生。我们关注<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_">第 5 层的棒球神经元，</a>它似乎是查找电路的一部分，因为它具有显着的效果并直接增强棒球属性。</p><p>对比假设是，早期层（例如 2-4）会产生棒球比赛中“打棒球”的某种表示（可能与最终表示不同），而棒球神经元只是发出信号来增强这种表示。</p><p><strong>实验</strong>：为了测试这一点，我们采用了每个运动员的残差，并采用与所有其他运动员残差所跨越的子空间正交的分量（请注意，有 2560 个残差维度和大约 1500 个其他运动员，因此这删除了 ​​60% 的维度）。然后，我们将棒球神经元应用于该残差正交分量，并查看神经元输出的 ROC AUC，以预测运动员是否打棒球的二元变量</p><p><strong>结果</strong>：ROC 约为 60%，明显高于机会 (50%) - 明显比没有正交投影时差，但仍然有一些信号</p><p><strong>Nuance</strong> ：这被证明是虚幻的，因为“与所有其他运动员正交的项目”并不一定会删除与其他运动员共享的<em>所有</em>信息。玩具示例：假设每个棒球运动员残差都是“是棒球”方向加上显着的高斯噪声。如果我们从该分布中获取由 1500 个样本组成的子空间，由于每个样本都有噪声，因此该子空间中可能无法完全捕获“是棒球”方向，因此投影不会将其擦除。这意味着，虽然我<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13">[13]</a></sup>发现这个实验的结果令人惊讶，但它并没有很好地区分这两个假设——部分散列和查找确实很难被证伪！ </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-KGJJcrC8izPbNFCPL-1" class="footnote-item"><p> GELU 与 ReLU 不同，但我们认为可以有效地将其视为“软 ReLU”，并且与 ReLU 相当接近，因此也可以相当好地实现 AND 门<a href="#fnref-KGJJcrC8izPbNFCPL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-2" class="footnote-item"><p>注意，这是哈希函数语句中的哈希，而不是哈希表。哈希函数接受任意输入并尝试产生与随机输出没有区别的输出。哈希表将哈希函数应用于输入，<em>然后</em>有目的地将其映射到某些存储的数据，这更类似于完整哈希和查找。 <a href="#fnref-KGJJcrC8izPbNFCPL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-3" class="footnote-item"><p>请注意，可能存在更复杂且基础对齐程度较低的查找形式，这些形式可能不太容易受到干扰，事实上，我们发现有迹象表明这个故事是混乱和复杂的<a href="#fnref-KGJJcrC8izPbNFCPL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-4" class="footnote-item"><p>另一位运动员的重新采样消融得到了类似的结果<a href="#fnref-KGJJcrC8izPbNFCPL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-5" class="footnote-item"><p>我们测量每个答案的第一个标记的对数概率，对于多标记答案，延续位于括号中，并且没有明确测试（一旦有了第一个标记，就可以很容易地使用二元组）</p><ul><li>打篮球运动</li><li>在北州（卡罗来纳州）上大学</li><li>1984年被选入NBA</li><li>为芝加哥队（公牛队）效力</li><li>是夏洛特队（黄蜂队）的大股东</li><li>主演电影《太空（果酱）》</li><li>为 NBA 联盟效力</li><li>1992年代表美国参加奥运会</li><li>玩数字23</li></ul> <a href="#fnref-KGJJcrC8izPbNFCPL-5" class="footnote-backref">↩︎</a></li><li id="fn-KGJJcrC8izPbNFCPL-6" class="footnote-item"><p>将神经元的值替换为数据集中所有 1500 名运动员的最终名称标记的平均值。 <a href="#fnref-KGJJcrC8izPbNFCPL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-7" class="footnote-item"><p>受到 Lovis Heindrich 和 Lucia Quirke 即将推出的作品的启发<a href="#fnref-KGJJcrC8izPbNFCPL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-8" class="footnote-item"><p>动机：E(A &amp; B) 对应于 Michael Jordan 上的激活，E(~A &amp; B) 对应于 Keith Jordan（对于 Keith 的不同值），E(A &amp; ~B) 对应于 Michael Smith（对于不同的值）史密斯）。神经元的激活通常具有远离零的平均值，因此我们从每个项中减去该平均值，该平均值由 E(~A &amp; ~B) 项捕获，即除迈克尔或乔丹之外的所有名字的平均值。并且 (E(A &amp; B) - E(~A &amp; ~B)) - (E(~A &amp; B) - E(~A &amp; ~B)) - (E(A &amp; ~B) - E(~ A &amp; ~B)) = E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <a href="#fnref-KGJJcrC8izPbNFCPL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-9" class="footnote-item"><p>我们采用中位数是因为有时总的或 GELU 派生的非线性效应是负/零，而中位数让我们可以忽略这些异常值<a href="#fnref-KGJJcrC8izPbNFCPL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-10" class="footnote-item"><p>虽然我们无法轻易判断出这是哪篇文章，但也许​​是棒球相关的文章，表现出类似<a href="https://arxiv.org/abs/2310.15154">总结主题的</a>东西！ <a href="#fnref-KGJJcrC8izPbNFCPL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-11" class="footnote-item"><p>将会出现一些错误分类，因为某些名称配对可能是已知实体。我们通过谷歌搜索名称进行了一些抽查，预计这不会对结果产生重大影响<a href="#fnref-KGJJcrC8izPbNFCPL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-12" class="footnote-item"><p>我们发现 MLP1 似乎与属性提取头的注意力相关（让它们检测姓名是否是运动员，从而是否提取一项运动），但对于查找运动员从事哪项运动并不重要。即对键重要但对值不重要。 <a href="#fnref-KGJJcrC8izPbNFCPL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-13" class="footnote-item"><p>使用单数是因为我的合著者认为这种替代解释一直是显而易见的<a href="#fnref-KGJJcrC8izPbNFCPL-13" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early<guid ispermalink="false"> CW5onXm6uZxpbpsRk</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:05 GMT</pubDate></item></channel></rss>