<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 12 日星期二 16:14:48 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[OpenAI: Leaks Confirm the Story]]></title><description><![CDATA[Published on December 12, 2023 2:00 PM GMT<br/><br/><p>之前： <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-altman-returns">OpenAI：奥特曼归来</a>、 <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board"><strong>OpenAI：董事会之战</strong></a>、 <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-facts-from-a-weekend">OpenAI：周末事实</a>、 <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/139306535/openai-the-saga-continues">AI#41 中的附加报道</a>。</p><p>我们有来自<a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2023/12/09/technology/openai-altman-inside-crisis.html">《纽约时报》</a> 、 <a target="_blank" rel="noreferrer noopener" href="https://time.com/6342827/ceo-of-the-year-2023-sam-altman/">《时代》杂志</a>、 <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/12/08/open-ai-sam-altman-complaints/?utm_campaign=wp_main&amp;utm_medium=social&amp;utm_source=twitter">《华盛顿邮报》</a>和《商业内幕》的新报道。</p><p>所有这些都描绘了一幅与《OpenAI：董事会之战》中讲述的中心故事一致的画面。他们证实了一些关键事实，特别是奥特曼试图通过欺骗手段将托纳从董事会中除名。我们还确认，奥特曼在第一次被解雇时曾承诺帮助过渡，因此我们至少有一个非常明确的案例，奥特曼说事实并非如此。</p><p>仍然存在许多不确定性，尤其是关于未来的不确定性，但过去的事件越来越清晰。</p><p>这些故事还提供了额外的色彩和关键细节。这篇文章是为那些想要这样做的人准备的，并根据新的细节弄清楚如何思考。</p><p>最重要的新细节是，《纽约时报》称董事会提议并热衷于布拉德·泰勒，并表示德安吉洛建议萨默斯，并与奥特曼一起拷问萨默斯，然后他们才同意他作为第三位董事会成员。新董事会在调查期间保持沉默，呼应旧董事会，无视奥特曼阵营及其迅速洗清罪名的愿望。</p><span id="more-23628"></span><h4>纽约时报报道事件</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2023/12/09/technology/openai-altman-inside-crisis.html">《纽约时报》最终对所发生的事情发表了自己的看法</a>，作者是 Tripp Mickle、Mike Isaac、Karen Weise 和臭名昭著的 Cade Metz（因此请相应对待所有说法）。</p><p>与其他主流新闻报道一样，框架是萨姆·奥尔特曼获胜，这表明科技精英和大资金最终占据主导地位。我不认为这是对所发生的事情或其影响的准确描述，但科技精英和媒体反对者都希望它是真实的，并试图通过魔术师的魔术来证明它是真实的，因为通常<a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=F4ol-avunbA&amp;pp=ygUqcG93ZXIgcmVzaWRlcyB3aGVyZSBtZW4gYmVsaWV2ZSBpdCByZXNpZGVz">权力存在于人们相信它存在的地方</a>。</p><p>我知道至少有一位作者确实阅读了<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board">我对事件的解释</a>，而且我还与一名不在署名的《泰晤士报》记者进行了交谈，以帮助澄清一切，这样他们就没有没有人告诉他们的借口。最终并不重要。</p><p>保罗·格雷厄姆（Paul Graham）曾说过，奥特曼对权力的兴趣超过对金钱的兴趣，以此解释为什么奥特曼会从事一些不会让他变得更富有的事情。我相信格雷厄姆的观点，但我也认为至少还有三个该死的其他理由这样做，使这个决定变得过于确定。</p><ol><li>如果 Altman 想要改善自己以及他的朋友和亲人的生活体验，那么构建安全的 AGI，或者确保没有其他人构建不安全的 AGI，是他要做的最重要的事情。奥特曼已经拥有了他个人用途所需的所有金钱，更多的钱并不能改善他的生活。他唯一的选择是丰富世界，确保人类繁荣而不消亡。事实上，请注意他的其他投资组合包括很多东西，例如聚变能和变革性的医学进步。即使奥特曼只关心自己，这些也是让他的生活变得更好的事情——通过让每个人的生活变得更好。</li><li>权力、名誉和声望会产生金钱。 Altman 在 OpenAI 中没有相应数量的股权，但他利用自己的职位筹集资金，获得良好的交易流，并且总体上成为资金所在。如果奥特曼认为他关心的是现金，他可以轻松地将其变成现金。需要明确的是，我一般来说一点也不嫉妒。我只是不喜欢某些特定项目，例如“在阿联酋建立一家芯片工厂”。</li><li> AGI 是世界上最甜蜜、最有趣、最令人兴奋的挑战。也是最重要的。如果您认为您的贡献会增加事情顺利进行的机会，那么您为什么还要从事其他工作呢？</li></ol><p>我能想象到的几乎每个版本的奥特曼都想这样做。</p><p>安全问题的关键描述的结构方式很容易让人认为这是外部董事会成员所关心的问题，但实际上，如果您仔细阅读这篇文章，这适用于整个董事会：</p><blockquote><p>他们因担心人工智能可能变得比人类更聪明而团结在一起。</p></blockquote><p>请记住<a target="_blank" rel="noreferrer noopener" href="https://openai.com/about">，无论过去还是现在，OpenAI 的明确目标都是</a>安全地创造比人类更聪明的人工智能，也称为 AGI。奥特曼和布罗克曼都签署了 CAIS 信函。奥特曼这里的威胁已经说得很清楚了。每个参与其中的人都了解其中的危险。值得赞扬的是，每个人都在谈论价格。</p><p>第一条消息是，我们至少有一个案例可以肯定萨姆·奥尔特曼至少在某些重要意义上对董事会撒了谎。</p><blockquote><p>奥尔特曼先生对自己帮助创办的一家初创公司被解雇感到震惊，他睁大了眼睛，然后问道：“我能帮忙吗？”董事会成员敦促他支持临时首席执行官。他向他们保证他会的。</p><p>几个小时内，Altman 先生改变了主意，并向 OpenAI 董事会宣战。</p></blockquote><p>我指出这一点是因为人们普遍认为奥特曼是精确用词和给出暗示的大师。是的，他具有欺骗性和误导性，玩权力游戏，但他太聪明了，不能直截了当地说不是。</p><p>所以他在这里说的是不是的话。</p><p>有关系吗？也许没有。但实际上可能很多。这种合作可能是促使我们决定不与奥特曼详细讨论问题的一个关键因素，至少在最初是这样，当时这样做是可行的。如果奥特曼愿意合作，他得到的回报就是任务继续进行，而他所做的一切都没有具体说明。</p><p>这篇文章对奥特曼那天晚上是否真的向董事会宣战提出了胡言乱语。上面的声明是这么说的。然后他们分享了其他人推动反抗的故事，包括 Airbnb 的首席执行官布莱恩·切斯基 (Brian Chesky)、高管和员工，而奥特曼只是慢慢决定反击。</p><p>不可能两者兼而有之。是哪一个？</p><p>我认为顶线是正确的。那个奥特曼一直在反击。尽管奥特曼的团队愿意在上面明确地说出这一点，但他们充分塑造了媒体叙事，使其余的事件听起来像是以一种非常不同的方式展开的。这是叙事塑造和媒体操纵方面绝对的大师班。他们应该在大学里教授这个。厨师之吻。</p><p>我们已确认 Altman 对于在阿联酋制造芯片的项目并不“一贯坦诚”：</p><blockquote><p> 9月，Altman先生会见了中东的投资者，讨论了一个AI芯片项目。三名知情人士表示，董事会担心他没有与董事会分享他的所有计划。</p></blockquote><p>由于许多明显的原因，这是董事会希望了解情况的领域，任何处于 Altman 职位的合理人士都会知道这一点，而且规范表明这意味着他们应该被告知。但不通知他们并不会默认严格违反规则，只要奥特曼在被问到时诚实回答问题即可。他做到了，做到什么程度了？我们不知道。</p><p>现在我们进入一些新材料。</p><blockquote><p>两位知情人士表示，Sutskever 博士……认为 Altman 先生向 OpenAI 高管说董事会坏话。其他员工也向董事会投诉了奥特曼先生的行为。</p><p> 10 月份，Altman 先生将另一位 OpenAI 研究员提升到与 Sutskever 博士相同的级别，而 Sutskever 博士认为这是一种轻视。两位知情人士称，苏茨克维尔博士告诉几位董事会成员，他可能会辞职。知情人士称，董事会将此举解读为在他和奥尔特曼之间做出选择的最后通牒。</p><p>苏茨克维尔博士的律师表示，他威胁要辞职的说法“绝对是错误的”。</p><p>另一场冲突在十月爆发，当时托纳女士发表了一篇论文……</p></blockquote><p>这表明苏茨克韦尔长期以来一直支持解雇奥特曼。如果情况属实，那么董事会的紧迫感以及不愿意花时间制定计划并把事情安排妥当的态度就更没有道理了。如果他们已经讨论这个问题几个月了，如果伊利亚不仅参与其中而且还热情地参与了一个月，我就无法理解。</p><p>这篇文章随后回顾了托纳被忽视的学术论文事件，托纳同意为此道歉以维持和平。</p><blockquote><p> “我觉得我们对于这一切造成的损害并没有达成共识，”奥特曼写道。</p></blockquote><p>我们绝对不是。托纳和我都认为这是微不足道的，而且显然如此。奥特曼将其视为一项重大交易。</p><p>现在我们进入核心问题。</p><blockquote><p>据知情人士透露，奥特曼给其他董事会成员打电话，表示麦考利希望将托纳女士从董事会中除名。当董事会成员后来问麦考利女士这是否属实时，她说这“绝对是错误的”。</p><p> OpenAI 发言人表示：“这与 Sam 对这些对话的回忆有很大不同。”她补充说，该公司期待对所发生的事情进行独立审查。</p></blockquote><p> 《时代》杂志给出了这个版本：</p><blockquote><p>时间：据两位知情人士透露，奥特曼告诉一名董事会成员，另一名成员认为应该立即解雇托纳，但事实并非如此。</p></blockquote><p>无论存在或不存在其他原因，如果奥特曼确实这么说了，我对此类事情的模型是他需要被解雇，而解雇他是董事会的工作。董事会确实应该这么说，而不是泛泛而谈。</p><p>多名目击者向《纽约时报》表示，他曾说过这句话。奥特曼否认了这一点。</p><p>很明显，奥特曼确实利用与董事会成员的私人谈话来给人留下错误印象，并为让托纳离开董事会而争取支持，从而以该报纸为借口，让奥特曼获得了董事会控制权。争论的焦点是奥特曼是否使用了准确的词语，或者他是否撒了谎。奥特曼称他的尝试为“火腿拳头”，我相信这是“被抓到说谎”的权力玩家代号，但也可以适用于“在技术上被抓到——不是说谎，同时隐含地撒谎。”</p><p> 《纽约时报》似乎确实表示，董事会确实加强了他们的描述：</p><blockquote><p> 《纽约时报》：董事会成员称奥特曼先生向董事会撒了谎，但出于法律原因他们无法详细说明。</p></blockquote><p>使用“撒谎”这个词是一种升级。这是律师的明确确认。</p><p>我们还确认了零公关人员，因为我们有托纳臭名昭著的台词。我知道背后的逻辑，但考虑到上下文，我仍然不敢相信她真的大声地说出来，说真的 WTF：</p><blockquote><p> OpenAI 首席战略官 Jason Kwon 指责董事会违反了信托责任。据两位了解会议情况的人士透露，他说：“你没有责任让公司消亡。”</p><p>托纳女士回答说：“摧毁公司可能符合董事会的使命。”</p></blockquote><p>你说‘我们无意做任何这样的事情。该公司完全有能力在没有奥特曼的情况下继续运营。我们完全打算在现有执行团队的领导下继续履行 OpenAI 的使命。奥特曼承诺在董事会会议上帮助完成过渡。如果他选择尝试摧毁 OpenAI 及其使命，那也是他的决定。这也证明他与我们的使命不相容，我们需要除掉他。”</p><blockquote><p> OpenAI 的高管坚持要求董事会在当晚辞职，否则他们将全部离开。 35 岁的 OpenAI 总裁布罗克曼先生已经辞职。</p><p>这些支持给了奥特曼先生弹药。</p></blockquote><p>这听起来很有偶然性。</p><p>此外，董事会现在已经做出了明确的虚张声势，威胁要退出。董事会召开了会议。高管们并没有辞职。随后的此类威胁变得不那么可信。</p><p>稍微跳过一点，他们仍然尝试了第二次。</p><blockquote><p>到 11 月 19 日（微软的报价在手），奥特曼先生对自己将再次被任命为首席执行官充满信心，以至于他和他的盟友给董事会设定了一个最后期限：上午 10 点之前辞职，否则所有人都会离开。</p></blockquote><p>专业谈判技巧：一旦你的第一次虚张声势被跟注，不要很快再次使用这个伎俩。不起作用。这就是为什么你不要急于进行第一次虚张声势，而要等到你的位置变得更强。</p><p>当然，董事会第二次虚张声势，任命了埃米特·谢尔（Emmett Shear）。</p><p>下一个好消息是在截止日期确定之前传来的，那就是布雷特·泰勒实际上被视为双方认可的公平仲裁者，而不是被视为奥特曼阵营中的人。</p><blockquote><p>然而，尽管董事会考虑让奥特曼先生回归，但仍希望做出让步。其中包括引进可以控制奥特曼先生的新成员。董事会鼓励 Twitter 前董事长布雷特·泰勒 (Bret Taylor) 加入，他很快赢得了大家的认可，并同意帮助双方进行谈判。</p></blockquote><p>但还要注意的是，在这个故事中，是董事会想要让步，特别是新的董事会成员，而不是奥特曼。这直接与其他报告相矛盾，并且没有意义，除非你将其理解为“如果旧董事会同意辞职，他们想要让步”。例如，董事会将移交对 OpenAI 的控制权，他们希望做出让步，“我们同意将其交给谁，以及这些人同意的事情将会发生。”充其量，我觉得这个框架很奇怪。</p><p>拉里·萨默斯 (Larry Summers) 是德安吉洛 (D&#39;Angelo) 在一些重要的原始报道中提出的建议：</p><blockquote><p>为了打破僵局，德安吉洛先生和奥尔特曼先生第二天进行了交谈。德安吉洛先生推荐前财政部长、哈佛大学教授<a target="_blank" rel="noreferrer noopener" href="https://web.archive.org/web/20231210002048/https://www.nytimes.com/topic/person/lawrence-summers">劳伦斯·萨默斯 (Lawrence H. Summers)</a>担任董事会成员。奥特曼先生喜欢这个主意。</p><p>萨默斯在波士顿地区的家中与德安吉洛、奥尔特曼、纳德拉和其他人进行了交谈。每个人都询问他对人工智能和管理的看法，而他则询问 OpenAI 的骚乱。他说他想确保自己能够扮演经纪人的角色。</p><p>萨默斯的加入迫使奥尔特曼放弃了获得董事会席位的要求，并同意对其领导层进行独立调查并解雇他。</p></blockquote><p>于是双方都与萨默斯进行了交谈，并对他的答复感到满意。</p><blockquote><p>本周，奥特曼和他的一些顾问仍然愤怒不已。他们希望他的名字还清。</p><p> “你有没有一个B计划来阻止关于你被解雇的假设，这不健康，也不是真的！！！”康威先生给奥特曼先生发了短信。</p><p> Altman 先生表示，他正在与 OpenAI 董事会合作：“他们确实希望保持沉默，但我认为尽快解决这个问题很重要。”</p></blockquote><p>总的来说，这一切都让我看好新董事会。从本质上讲，我们可能面临这样的情况：德安吉洛和两名中立的仲裁者，尽管他们具有庄严性和商业关系。他们并没有向奥特曼卑躬屈膝。奥特曼的阵营继续激怒（不知何故，从康威发给奥特曼的有关此事的短信被泄露到《纽约时报》，来源不多的地方）。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://news.ycombinator.com/item?id=38583125">Gwern 在此提供了他们的总结。</a></p><h4> 《时代》杂志评选奥特曼为年度首席执行官</h4><p><a target="_blank" rel="noreferrer noopener" href="https://time.com/6342827/ceo-of-the-year-2023-sam-altman/">《时代》杂志对奥特曼进行了专题报道</a>，称他为“年度首席执行官”，这是他当之无愧的称号。我认为这是迄今为止最好的非常简短的描述，阐明了博弈论：</p><blockquote><p>与此同时，一位知情人士表示，该公司的员工和董事会陷入了“一场巨大的胆小鬼游戏”。</p></blockquote><p>消息来源还指出奥特曼寻求权力的一面，并愿意为了获得权力而不诚实和操纵他人。</p><blockquote><p>但多年来与奥特曼共事的四位人士也表示，他可能很狡猾，有时甚至具有误导性和欺骗性。两名熟悉董事会程序的人士表示，奥特曼善于操纵他人，他曾多次收到反馈称，他有时不诚实，目的是让人们觉得他同意他们的观点，但实际上他并不同意。这些人将这种模式视为更广泛的巩固权力尝试的一部分。 “从很多方面来说，Sam 都是一个非常好的人；他不是一个邪恶的天才。如果他是一个可怕的人，那么讲述这个故事会更容易，”其中一位说道。 “他关心使命，关心其他人，关心人性。但如果你观察他的行为，就会发现他确实以极端的方式寻求权力。”</p></blockquote><p>这是第一份正确认定结果不明确的主流报告：</p><blockquote><p>目前尚不清楚奥特曼在第二次担任首席执行官时是否会拥有更多或更少的权力。</p></blockquote><p>除了他的其他好选择之外，我们还可以添加……乔治斯特土地税？呜呼！</p><blockquote><p>他说，奥特曼在最近与世界领导人的会晤中主张征收土地增值税——这是一项经典的乔治主义政策。</p></blockquote><p>这是一种没有人会伪造的信号。确实有很多值得爱的东西。</p><p>包括他的诚实。我不想惩罚它，但我也想把它留在这里。</p><blockquote><p> “我们确实加速了比赛，因为没有更细致的说法，”奥特曼说。</p></blockquote><p> 《时代》杂志这样描述董事会最初与奥特曼的接触：</p><blockquote><p>奥特曼将其描述为要求他回来的请求。 “我经历了一系列的情绪。我一开始是挑衅的，”他说。 “但很快，我就产生了一种责任感和义务感，并且想要保留这个我非常关心的东西。”与董事会关系密切的消息人士对这次外展活动有不同的描述，称其为在公司分崩离析之前讨论稳定公司的方法的​​尝试。</p></blockquote><p>我并不是说我们确信这是奥特曼撒谎的另一个例子（对《时代》而不是对董事会来说，这是一个不那么严重的问题），但他对事件的版本无法计算。如果董事会积极要求奥特曼彻底回归，我不认为这是奥特曼的反应。</p><p>我可以相信奥特曼的故事的一半：奥特曼被要求返回，或者奥特曼违抗了董事会的要求，只是出于责任和义务才这样做（因为董事会最初要求的是其他东西。）我不相信两者同时进行。这与保罗·格雷厄姆对他性格的评价完全不一致。</p><h4> 《华盛顿邮报》称领导人警告奥特曼有虐待行为</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/12/08/open-ai-sam-altman-complaints/?utm_campaign=wp_main&amp;utm_medium=social&amp;utm_source=twitter">WaPo 的文章</a>称，今年秋天，OpenAI 的少数高级领导人与董事会接洽，表达了对 Altman 的担忧。在这个故事中，董事会认为 OpenAI 将因为他们认为奥特曼的毒性而失去关键领导者。</p><blockquote><p>现在重新掌管 OpenAI 的奥特曼可能会发现，公司并不像社交媒体上迎接他回归的一波又一波的心形表情符号所暗示的那样团结。</p></blockquote><p>这始终是事实。无论表情符号怎么说，没有一个大团体是完全团结的。</p><p>具体细节很少。所提供的细节听起来像是在公司发生的普通事情。在如此高压和竞争的环境中，什么是虐待、什么不是虐待，是仁者见仁，且高度依赖于具体情况。这里描述的内容可能反映滥用行为，也可能反映不值得关注的内容。令人担忧的是，员工们认为此事足以向董事会求助。</p><p>除了经理们向董事会提出此类抱怨这一具体细节之外，这并没有给我们带来太多启发。这些担忧似乎有助于证实董事会对奥特曼行为的模型，并有助于证明这一决定的合理性。</p><h4> 《商业内幕》称微软的信是虚张声势</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/openai-employees-did-not-want-to-work-for-microsoft-2023-12?international=true&amp;r=US&amp;IR=T">Business Insider 表示，OpenAI 员工真的非常不想去微软工作</a>。我也不会。如果奥特曼不想创办一家新公司，在某些情况下，员工们可能仍然认为这是最不坏的选择。请记住，信中说他们“可能”会这样做，而不是他们一定会这样做。</p><p> AI 安全模因提供以下报价：</p><blockquote><p> <strong>“[这封信]是一个大胆的虚张声势，大多数员工对为微软工作并没有真正的兴趣。”</strong></p><p> “许多 OpenAI 员工‘感到压力’才签署了这封公开信。”</p><p> <strong>“另一名 OpenAI 员工公开嘲笑微软将向离职员工支付因追随 Altman 而损失的股权的想法。”</strong> “这有点虚张声势，但最终奏效了。”</p><p> <strong>“这封信本身是由一群长期工作的员工起草的，他们拥有最大的影响力和资金，拥有多年的行业地位和积累的股权，以及更高的薪酬。</strong>该员工解释说，他们在周日深夜开始给其他员工打电话，敦促他们签字。”</p><p>尽管几乎所有员工都报名追随 Altman 离开，但“没有人愿意去微软。”这位人士称该公司是所有主要科技公司中“最大且最慢的”。</p><p> “像微软这样大的公司的官僚主义令人心碎。”</p><p> <strong>“尽管我们与微软有合作关系，但在内部，我们并不尊重他们的人才标准，”</strong>现任 OpenAI 员工告诉 BI。 “这让人们以错误的方式接受他们的管理。”</p><p><strong>除了两家公司之间的文化冲突之外，对 OpenAI 员工来说还有另一个重要因素：金钱。</strong>如果 OpenAI 由于员工大规模外流而突然崩溃，那么很多东西就会在他们眼前消失。</p><p> <strong>“萨姆·奥尔特曼不是最好的首席执行官，但数以百万计的美元和股权都处于危险之中，”</strong>现任 OpenAI 员工表示。</p><p>微软同意以同等薪酬水平聘用所有 OpenAI 员工，但这只是一时的口头协议。</p><p><strong>原本计划让员工将现有既得股权出售给外部投资者的要约收购将被取消。这位员工表示，所有这些股权都“一文不值”。</strong></p><p>这位前 OpenAI 员工估计，在签署这封信表示将离开的数百人中，“名单上可能有 70% 的人会说，‘嘿，我们可以，你知道，让这个招标通过吗？’” ”</p><p>与此同时，一些微软员工对该公司承诺为数百名 OpenAI 员工提供匹配工资感到愤怒。”</p></blockquote><p> Roon 回应说这并不准确：</p><blockquote><p> Roon：不是长篇大论，我只能代表自己说话，但从员工的角度来看，这是非常不准确的表达情绪。</p><p> –“员工感到压力”->; 在某个时刻，我们数百人在后院了解请愿书。人们对董事会的疯狂决定感到非常不安，以至于他们立即兴奋地签署了这份文件。谷歌文档确实打破了人们同时尝试签名的并发水平。我记得许多人就请愿书及其措辞进行了明智而细致的对话，最后得出的结论是，这是唯一的前进道路。一半的公司是在凌晨 2 点到 3 点之间签署的。这不是通过同侪压力就能实现的。</p><p> – “这是为了钱” ->; 当时听起来签署请愿书意味着放弃所有 openai 股权并重新开始。我们不是白痴，每个人都知道，新公司的条款充其量只是悬而未决，微软这边有很多讨价还价的筹码。人们签署了请愿书，因为这是正确的做法。你根本无法在一家你不尊重其最终领导层的公司工作。</p><p> – “没有人想去微软” ->; 你必须疯狂地宁愿在由其他人控制的模型、代码和产品上开始新的工作，而不是在专门设计为成为工具的公司中建立安全的AGI。它与微软人才栏、官僚机构或品牌无关。不知道为什么一些白痴泄密者挑衅者会这样设计。微软在收购定制治理结构下的公司并让他们做自己的事情方面非常成功（GitHub、LinkedIn）。根据《纽约客》的文章，即使是微软自己最喜欢的结果也是 OpenAI 的连续性。我仍然敢打赌，如果董事会没有改变主意，公司将在微软进行重组。</p></blockquote><p>我相信鲁恩正在诚实地回忆他在这里的经历。我还相信这两个故事比他意识到的更加契合。</p><p>员工们希望 Altman 回来，或者除非有一个可以信任的董事会和首席执行官，否则他们就会离开，但他们大多希望 OpenAI 完好无损，最好能得到报酬，并对董事会感到愤怒。他们不想去微软，我们永远不会知道有多少人会真正这么做，而不是留下来，还是去其他地方，或者创立新公司，也不知道董事会在定时炸弹真正爆炸之前还有多久。我的猜测是，如果奥特曼留在那里，很多员工都会去微软，但也有很多人选择其他道路。</p><p>我的猜测是，大多数员工都热情地在这封信上签名，而那些不想签名的人也感到有压力，无论如何都要签名，这得到了很多后来的签名。我知道即使没有人故意施加压力，我也会感到压力。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/weidai11/status/1727416105604673948">魏德将</a>OpenAI 员工的行为和请愿书的签署视为一种 OpenAI 文化革命，他认为这种革命在这样的地方是不可能的，并认为这是一个巨大的负面更新。我没那么惊讶，也更少读这封信。从他们的角度来看，他们有充分的理由感到愤怒并要求奥特曼回归。即使个别员工不支持奥特曼，也有充分的理由签署这封信——为了让公司团结起来，并且出于内部政治原因，即使没有施加直接压力。同样，信中说“可以”离开，因此它没有向您承诺任何事情。</p><h4>这一切让我们去哪儿了？</h4><p>我将继续将人们与<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board">董事会之战</a>联系起来，我相信这仍然是事件的最终综合。我们现在有了更多的细节来支持和充实这个叙述，但它们并没有改变中心故事。</p><p>我确信我会继续经常每周都有一个关于进展的部分，但希望事情会从这里开始放缓。</p><p>正如我之前所写，我们现在正在等待董事会的调查以及新董事会的组成。如果新董事会拥有绝对多数席位，对生存安全和使命有坚定的承诺，并且具有完成董事会工作所需的庄重和经验，那将是一个非常好的结果，如果他不采取任何行动我很高兴看到奥特曼继续受到这样的监督。</p><p>如果经过调查后证明这是不可能的，我们将看看我们会找谁来代替，我担心这不会更好，但我也希望公司能够团结起来，如果董事会没有妥协的话，公司就不会团结起来，因为事情进展如何。</p><p>如果董事会最终实际上被商业利益和那些不关心安全或 OpenAI 既定使命的人所控制，那么无论 Altman 是否被保留，那都将是一场灾难。</p><p>如果奥特曼最终获得有效的董事会控制并拥有自由统治权，那么这是一个非常令人担忧的结果，我们将了解奥特曼在多大程度上真正一致、明智并有能力抵制他本性的某些方面，而不是建立并扩大规模并寻求权力。最终结果可能是美好的，也可能是灾难性的。</p><br/><br/><a href="https://www.lesswrong.com/posts/xY5m72tME9kqjpdoC/openai-leaks-confirm-the-story#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xY5m72tME9kqjpdoC/openai-leaks-confirm-the-story<guid ispermalink="false"> xY5m72tME9kqjpdoC</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 12 Dec 2023 14:00:05 GMT</pubDate> </item><item><title><![CDATA[Nonlinear’s Evidence: Debunking False and Misleading Claims]]></title><description><![CDATA[Published on December 12, 2023 1:16 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/q4MXBzzrE6bnDHJbM/nonlinear-s-evidence-debunking-false-and-misleading-claims#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/q4MXBzzrE6bnDHJbM/nonlinear-s-evidence-debunking-false-and-misleading-claims<guid ispermalink="false"> q4MXBzzrE6bnDHJbM</guid><dc:creator><![CDATA[KatWoods]]></dc:creator><pubDate> Tue, 12 Dec 2023 13:16:12 GMT</pubDate> </item><item><title><![CDATA[Funding case: AI Safety Camp]]></title><description><![CDATA[Published on December 12, 2023 9:08 AM GMT<br/><br/><h3><strong>项目总结</strong></h3><p>AI 安全营是一个拥有 5 年历史的项目，致力于帮助人们在 AI 安全领域找到职业。</p><p>我们为湾区和伦敦中心以外的崭露头角的研究人员提供支持。<br><br>我们没有资金了。为了使第 10 届得以举办，请资助我们的津贴和工资。</p><p></p><h3><strong>该项目的目标是什么？您将如何实现这些目标？</strong></h3><p> AI安全营是一个询问如何确保未来AI<a href="https://aisafety.camp/faq/#:~:text=you%20mean%20with-,safety,-%3F"><u>安全</u></a>并尝试在团队中具体开展工作的项目。</p><p>第九期AI安全营我们开放了<a href="https://web.archive.org/web/20231128010059/https://aisafety.camp/#Projects"><u>29个项目</u></a>的申请。<br><br>我们率先设立了一个专区来支持“暂停人工智能”工作。有了资金，我们可以将限制企业人工智能开发的 4 个项目扩展到下一版的 15 个项目。<br><br>我们对新的研究主导<a href="https://web.archive.org/web/20231128010059/https://aisafety.camp/#:~:text=a%20spontaneous%20collage-,Team%20structure,-Every%20team%20will"><u>形式</u></a>感到兴奋，因为它结合了：</p><ul><li><i>实践指导：</i>我们指导研究负责人 (RL) 仔细考虑他们的项目并确定其范围。研究负责人依次加入团队成员并指导他们的团队成员完成新研究的过程。</li><li><i>简化的应用程序：</i>团队应用程序是运行 AI 安全营中最耗时的部分。评审员常常不确定如何评估申请人是否适合需要特定技能和理解的项目。 RL 通常清楚地知道他们想与谁一起工作三个月。因此，我们转而指导 RL 准备特定于项目的问题并采访他们的潜在队友。</li><li><i>资源效率：</i>我们不会与其他项目竞争稀缺的导师时间。相反，我们寻找有思想的研究领导者，他们在某个时候可能成为广受认可的研究人员。虚拟形式还减少了管理费用——资金不再用于场地和机票，而是直接用于资助人们专注于人工智能安全方面的工作。</li><li><i>灵活的工作时间：</i>参与者可以在他们的学位或日常工作的同时，在他们的时区远程工作——以测试他们是否适合人工智能安全职业。</li></ul><p></p><h3><strong>这笔资金将如何使用？</strong></h3><p>我们正在筹款以支付：</p><ul><li>本届 AISC 组织者的薪资</li><li>为未来的营地提供资金（参见预算部分）</li></ul><p><br>我们是否举办第十版，或者无限期搁置 AISC，都取决于您的捐款。</p><p>去年六月，我们不得不冻结三名员工一年的工资。我们的运营协调员不得不离开，琳达和雷梅尔特决定作为志愿者再举办一期。</p><p> AISC 此前已获得由 FTX 资金支付的赠款。 FTX 崩溃后，我们冻结了 25.5 万美元的资金以弥补追回索赔。对于当前的 AISC，我们从 SFF 中剩下 9.9 万美元，专门用于津贴，但没有用于支付工资，也没有用于未来的 AISC。</p><p>如果我们有足够的资金，我们也可能会重新启动 AISC 的面对面版本。这一决定还将取决于对 AISC 正在进行的外部评估，其中包括评估虚拟 AISC 与面对面 AISC 的影响差异。</p><p>默认情况下，我们将根据获得的资金来决定优先考虑什么。但如果你想发表意见，我们可以讨论一下。我们可以指定您的钱用于您想要的任何用途。</p><p><br> <strong>AISC 各个版本的潜在预算</strong></p><p>这些是虚拟 AISC 的不同可能版本的预算示例。如果我们的资金介于两者之间，我们就会做一些介于两者之间的事情。</p><p><i><strong>虚拟 AISC - 预算版本</strong></i></p><figure class="table"><table><tbody><tr><td>软件等</td><td>2000 美元</td></tr><tr><td>组织者工资，2 人，4 个月</td><td>5.6 万美元</td></tr><tr><td>参与者津贴</td><td>$0</td></tr><tr><td>全部的</td><td>5.8 万美元</td></tr></tbody></table></figure><p>在预算版本中，组织者做了启动该计划所需的最低限度的工作，但在项目期间没有为 AISC 团队提供持续支持，也没有时间对该计划的未来版本进行评估和改进。</p><p>工资按每人每月 7000 美元计算。</p><p><i><strong>虚拟 AISC - 普通版</strong></i></p><figure class="table"><table><tbody><tr><td>软件等</td><td>2000 美元</td></tr><tr><td>组织者工资，3 人，6 个月</td><td>12.6 万美元</td></tr><tr><td>参与者津贴</td><td>18.5 万美元</td></tr><tr><td>全部的</td><td>31.3 万美元</td></tr></tbody></table></figure><p>对于非预算版本，我们多了一名员工，每人的带薪工作时间也更长，这意味着我们可以提供更多全方位的支持。</p><p>津贴估算基于： <i>$185K = $1.5K/研究主管</i>* <i>40 + $1K/团队成员</i>* <i>120</i><br>研究负责人<i>(40)</i>和团队成员<i>(120)</i>的数量是根据我们对 AISC 增长幅度的预测而得出的。<br></p><p></p><h3><strong>您的团队中有哪些人？您在类似项目上的记录如何？</strong></h3><p>我们举办AI安全营已有五年多的时间，涵盖8期、74支队伍、251名参与者。</p><p>根据参与者的反馈，我们进行了很多迭代。我们最终确定了一种令我们兴奋的<a href="https://web.archive.org/web/20231204182638/https://aisafety.camp/#format"><u>研究主导形式</u></a>。在您的支持下，我们将仔细扩展此格式。</p><p>作为研究人员，我们可以在潜在的研究领导者所在的地方与他们会面。我们可以在人工智能安全研究的几乎每个领域提供有用的指导和反馈。</p><p>我们特别有能力支持认知上多样化的赌注。<br></p><p><strong>主办单位</strong></p><p>Remmelt – “不要构建无法控制的人工智能”协调员</p><ul><li>雷梅尔特与前五角大楼工程师和教授合作。 Roman Yampolskiy 论基本可控性限制。两位研究人员均获得生存与繁荣基金的资助。</li><li> Remmelt 与<a href="https://forum.effectivealtruism.org/posts/q8jxedwSKBdWA3nH7/we-are-not-alone-many-communities-want-to-stop-big-tech-from"><u>不同的组织者</u></a>合作限制有害的 AI 扩展，包括：<br>暂停人工智能、创意专业人士、反技术解决方案专家、产品安全专家和气候变化研究人员。</li><li>在 AISC，Remmelt 撰写了<a href="https://www.lesswrong.com/posts/xp6n2MG5vQkPpFEBH/the-control-problem-unsolved-or-unsolvable"><u>控制问题</u></a>的综合概要，<a href="https://x.com/romanyam/status/1709619586608275680"><u>如下所示</u></a>。</li><li> Remmelt 此前与他人共同创立了 EA 荷兰公司并主持过全国会议。</li></ul><p><br>琳达 - “其他一切”的协调员</p><ul><li>完成物理学博士学位后，Linda 在 MIRI 实习，后来加入 Refine 奖学金。</li><li> Linda 对人工智能安全技术领域有着全面的了解。作为一名自学者，她研究主体基础理论、认知神经科学和机械可解释性。</li><li>几位研究人员（例如 MIRI 的研究人员）指出，即使推理距离很长，琳达也能以惊人的速度接受新的理论论证。</li><li>在 AISC，Linda 与人共同发表了<a href="https://proceedings.neurips.cc/paper/2021/hash/b9ed18a301c9f3d183938c451fa183df-Abstract.html"><u>RL in Newcomblike Environments</u></a> ，并被选为 NeurIPS 焦点演讲。</li><li> Linda 发起并领导了人工智能安全营、人工智能安全支持和虚拟人工智能安全非会议。</li></ul><p></p><p><strong>跟踪记录</strong></p><p>AI安全营主要是一个边做边学的培训项目。人们可以通过在具体项目上进行合作来尝试角色并探索人工智能安全的方向。</p><p>多位校友告诉我们，人工智能安全营是他们开始涉足人工智能安全领域的方式。<br>在<a href="https://forum.effectivealtruism.org/posts/vuEmQT5jzLHrZgoF6/takeaways-from-a-survey-on-ai-alignment-resources#:~:text=19%20or%20below.-,Average%20usefulness,Safety%20Camp%3A%203.7%20%2B%2F%2D%200.3"><u>Daniel Filan 的调查</u></a>中，AISC 在“平均实用性”排行榜上名列前茅。</p><p>训练营发表的<a href="https://docs.google.com/spreadsheets/d/1xfxx-15zAF5QfIMpxf17LDN9GvfPzIj_hVzESxwxHRM/edit#gid=0"><u>论文</u></a>包括：</p><ul><li><a href="https://arxiv.org/abs/2105.14111"><u>目标误泛化</u></a>、<a href="https://www.mdpi.com/2504-2289/3/2/26"><u>人工智能治理和政策制定过程</u></a>、<a href="https://ceur-ws.org/Vol-2419/paper_28.pdf"><u>检测马尔可夫决策过程中的尖峰腐败</u></a>、 <a href="https://proceedings.neurips.cc/paper/2021/hash/b9ed18a301c9f3d183938c451fa183df-Abstract.html"><u>类纽科姆环境中的</u></a>强化<a href="https://link.springer.com/article/10.1007/s10458-022-09586-2">学习、使用软极大极值进行风险规避多目标决策</a>、<a href="https://openreview.net/forum?id=4eMzKmZ6xW"><u>反射机制作为对齐目标</u></a></li></ul><p>AI 安全营启动的<a href="https://docs.google.com/spreadsheets/d/1xfxx-15zAF5QfIMpxf17LDN9GvfPzIj_hVzESxwxHRM/edit#gid=563799009"><u>项目</u></a>总共获得了 46.8 万美元的资助：</p><figure class="table"><table><tbody><tr><td> AISC 1：有限理性团队</td><td> <a href="https://docs.google.com/document/d/1NIg4OnQyhWGR01fMVTcxpz8jDd68JdDIyQb0ZZyB-go/edit"><u>保罗</u></a>3 万美元</td></tr><tr><td>AISC 3：建模合作团队</td><td><a href="https://longtermrisk.org/grantmaking/"><u>CLT</u></a> 24,000 美元， <a href="http://survivalandflourishing.fund/sff-2019-q4"><u>SFF</u></a> 50,000 美元， <a href="https://survivalandflourishing.fund/sff-2021-h2-recommendations"><u>SFF</u></a> 83,000 美元， <a href="https://survivalandflourishing.fund/sff-2022-h2-recommendations"><u>SFF</u></a> 83,000 美元</td></tr><tr><td>AISC 4：调查小组</td><td><a href="https://app.effectivealtruism.org/funds/far-future/payouts/2kbXqc6R9xf6yi0EY0CLl#alexis-carlier-(dollar5000)"><u>LTTF</u></a>提供 5,000 美元</td></tr><tr><td>AISC 5：悲观代理团队</td><td><a href="https://forum.effectivealtruism.org/posts/diZWNmLRgcbuwmYn4/long-term-future-fund-may-2021-grant-recommendations#David_Reber____3_273"><u>LTFF</u></a> 3000 美元</td></tr><tr><td>AISC 5：多目标调整</td><td><a href="https://marginalrevolution.com/marginalrevolution/2022/04/emergent-ventures-winners-19th-cohort.html"><u>电动汽车</u></a>2 万美元</td></tr><tr><td>AISC 6：语言模型作为协调工具</td><td>来自<a href="https://funds.effectivealtruism.org/grants"><u>LTFF</u></a>的 1 万美元</td></tr><tr><td>AISC 7：AGI 固有的非安全性</td><td><a href="https://survivalandflourishing.fund/sff-2022-h2-recommendations"><u>SFF</u></a> 17 万美元</td></tr><tr><td>AISC 8：高风险人工智能的政策建议</td><td><a href="https://www.nonlinear.org/"><u>荷兰</u></a>10,000 美元</td></tr></tbody></table></figure><p>发起营外对话的<a href="https://docs.google.com/spreadsheets/d/1xfxx-15zAF5QfIMpxf17LDN9GvfPzIj_hVzESxwxHRM/edit#gid=1554885061"><u>组织</u></a>包括：</p><ul><li> Arb Research、AI 安全支持和标准实验室。</li></ul><p> <a href="https://docs.google.com/spreadsheets/d/1xfxx-15zAF5QfIMpxf17LDN9GvfPzIj_hVzESxwxHRM/edit#gid=1273480402"><u>校友</u></a>继续担任以下职务：</p><ul><li> <i>FHI</i> （1个工作+4个学者+2个研究员+2个实习生）， <i>Govai</i> （1个工作）， <i>Chai</i> （2个实习生）， <i>Miri</i> （1个实习生），<i>长期风险中心</i>（1个Job+1研究员）， <i>DeepMind</i> （1 Job+2个实习生）， <i>OpenAI</i> （1个工作）， <i>Redwood</i> （1个工作），<i>未来社会</i>（1个工作）， <i>AI</i> （1个工作），<i>猜想</i>（1个工作）， <i>Apollo</i> （1个创始人，1个工作）， <i>Leap Labs</i> （ 1个创始人），<i>布鲁德</i>（1个创始人）。<br><br>例如，更多的是以其他方式从事AI安全性，例如。作为博士或独立研究人员。这些只是我们知道的职位。我们目前尚未进行全面的调查。</li></ul><p>有关以前版本的统计数据，请参见<a href="https://docs.google.com/document/d/1tLFwRa71xtJ3GtblkHs6FmZLXegtSd_MYihDNXJPCAk/edit#heading=h.hjj5cajvp7ri"><u>此处</u></a>。我们最近还委托ARB研究进行校友调查和访谈，以仔细评估AI安全营的影响。</p><p></p><h3><strong>如果该项目失败，最可能的原因和结果是什么？ （premortem）</strong></h3><ul><li>未获得最低资金。<ul><li>现在的资助者更少。</li><li>在SFF和LTFF上评估我们的评估者太忙了。<br>他回答说，他的猜测是，他目前对我们发现的RLS的大多数项目并不超级感兴趣，而对“不要建立不可控制的AI”区域不感兴趣。</li><li>我们寻找认识论上的赌注。当我们认为个人或工作领域被错误地忽视时，我们以诚实而闻名。我们花了很少的时间在网络上，并向资助者澄清了我们的观点，不幸的是，这导致了当前的状况。</li></ul></li><li>获得资金，但不足以覆盖OPS工作人员。<ul><li>琳达（Linda）和雷姆（Remmelt）本身就是研究人员，并且在跑步操作中有些疲惫。第三名工作人员的资金将使该计划更具可持续性。</li></ul></li><li>没有足够的项目选择。<ul><li>我们希望将更多的时间集中在询问潜在的研究线索上，以了解其症结和评估他们的计划。这一轮我们是志愿者，所以我们必须满足。我们拒绝了“不要建立无法控制的AI”和“其他所有内容”的建议。</li></ul></li><li>由于与新计划的竞争，总体接收申请人更少。<ul><li>团队申请虽然每年稳定（&#39;22; 22; 23; 23; 222; 24&#39;24）。</li></ul></li><li>缺乏仔细扩展“不要构建无法控制的AI”工作的管道。<ul><li>鉴于Remmelt的连接，我们是实现这一目标的最佳计划。<br></li></ul></li></ul><p></p><h3><strong>您或您的项目还获得了什么其他资金？</strong></h3><p>没有其他资金来源。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ukCFkqiDFmkYTyywN/funding-case-ai-safety-camp-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ukcfkqidfmkytyywn/funding-case-ai-safety-camp-1<guid ispermalink="false"> UKCFKQIDFMKYTYYWN</guid><dc:creator><![CDATA[Remmelt]]></dc:creator><pubDate> Tue, 12 Dec 2023 09:08:19 GMT</pubDate> </item><item><title><![CDATA[What is the next level of rationality?]]></title><description><![CDATA[Published on December 12, 2023 8:14 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 05:53:25 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 05:53:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>尤德科夫斯（Yudkowsky）出版了<a href="https://www.lesswrong.com/posts/aFEsqd6ofwnkNqaXo/go-forth-and-create-the-art"><i>，创作艺术！</i></a>在2009年。那是2023年。您和我同意，在过去的几年中，关于Eliezer Yudkowsky（和Scott Alexander）水平的理性帖子并不多。换句话说，没有人出去创造艺术。这不是很有趣吗？</p></div></section><h2>埃利泽（Eliezer）发生了什么？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 05:56:01 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 05:56:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>是的，我们对此表示同意。我指出，在Eliezer之前有几个级别的合理性。他面前的那个就像是萨根 - 弗尼曼（Sagan-Feynman）风格的理性（他的粉丝经常穿着标签“怀疑论者”）。但这主要是切入的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 05:58:08 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 05:58:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>也许这根本不是切实的。费曼在哈利·<i>波特（Harry Potter）和理性方法</i>中以名字引用。我有一个20多岁的朋友，他是第一次读Feynman。他发现了诸如“您不需要Labcoat和博士学位的假设”和“可以自己思考可以”之类的东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:00:00 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:00:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>您如何看到它连接到“下一级别的理性层面是什么？”的问题。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:01:33 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:01:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p> Yudkowsky是一个数据点。我们对“合理性”的质量观点越多，我们越能推断拟合线。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:02:28 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:02:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我明白了，也许这次讨论的初步是“ Eliezer是哪个级别的理性？”？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:03:15 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:03:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>是的。埃利泽（Eliezer）在莱斯沃朗（Lesswrong）上得到了更多的关注，但他并不是唯一关于理性主题的作家。我认为我们应该首先询问我们指向的这个集群中的谁。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:05:58 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:05:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>好吧，所以在Feynman-Sagen群中，我还指向道金斯，迈克尔·谢默，萨姆·哈里斯，希钦斯和詹姆斯·兰迪。不一定是因为我对他们的作品非常熟悉，或者发现它们特别有价值，而是因为它们似乎是该集群中的核心人物。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:08:45 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:08:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这些都是合理的名字，但我从未真正读过他们的任何作品。我的个人清单包括Penn Jillette。保罗·格雷厄姆（Paul Graham）和布莱恩·卡普兰（Bryan Caplan）也觉得很重要，即使它们没有“怀疑”或“理性”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:09:05 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:09:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我已经读了一点，但是大多数情况下，我只是迟到了现场，发现Eliezer和Scott很快就足够了，以至于我没有机会在此之前深入阅读它们，而在我做到之前，我没有觉得需要。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:10:32 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:10:32 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>是的，保罗·格雷厄姆（Paul Graham）也是埃利泽（Eliezer）非常尊重的人，我认为甚至可能在序列中提到。我想您可以将各种科幻作者添加到列表中。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:14:33 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:14:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>就个人而言，我觉得整个事情始于苏格拉底。但是，当我开始<i>道歉</i>时，我觉得自己已经将他的想法内化了。</p><p>但是，当我与理性主义者闲逛时，我不会得到这种印象。<i>理性的中位数读者：亚利桑那州</i>在苏格拉底对话下破碎。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:14:07 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:14:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我同意，尽管如果我们试图在时期/层面上削减理性的历史，那么苏格拉底是一个不同的（第一个）时期/级别（尽管他的意义与他的水平更高的感觉他）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:24:29 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:24:29 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我认为苏格拉底的才华来自意识到当时知道他们的能力很少，并充分发展了不自欺欺人的技能。在他之后所做的事情大部分是发展能力，同时大部分并没有给自己不愚弄自己。</p><p>我认为“怀​​疑论者”在这一思维方式和认识错误的旅程中得到了发展，但几乎完全专注于在他人中找到它们。与尤德科夫斯基（Yudkowsky）一起，重点以非常苏格拉底的方式向内移动，以找到自己的缺点和局限性。</p></div></section><h2>切线将拖钓作为核心理性技能</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:24:57 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:24:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我从来没有听过这样使用的“苏格拉底子”一词。我喜欢。</p><p>尤德科夫斯基（Yudkowsky）对苏格拉底（Socrates）的另一个相似之处是，它们都是臭名昭著的巨魔。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:26:52 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:26:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>那让我发笑。这是真的。我记得他与他基本上拖钓的人的对话序列中的故事。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:28:02 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:28:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这是有充分理由的。在教授理性时，绝对必要拖钓学生。我一直都在拖钓我的学生/朋友。当我参观伯克利的LightCone办公室时，我也拖了他们。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:29:01 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:29:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>啊，我看到<a href="https://www.lesswrong.com/posts/exvuvFZniGh2R5nvd/the-mountain-troll">你已经写了这一点</a>。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:29:20 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:29:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>您知道为什么拖钓如此重要吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:30:23 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:30:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我不确定我完全了解您如何使用该概念，所以告诉我为什么您认为这是如此重要。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:33:29 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:33:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我可以用简单的话来解释这一点。但是我认为，如果我拖了你，那将会更有趣，更具有教育意义。你同意吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:33:42 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:33:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>哈哈，当然:) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:35:59 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:35:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>你已经说服了我。拖钓是不道德的。理性主义老师不应该这样做。让我们继续。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:36:31 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:36:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>大声笑，我什么也没说，所以我什么都不能说服你:)</p><p>也许您已经说服了自己，但是我敢打赌，您还没有，而您只是在拖钓:) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:38:26 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:38:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p> （： </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:38:50 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:38:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>理性主义者必须对权威持怀疑态度。假设您是理性的老师，因此是权威人物。您如何在道德上教您的学生对您持怀疑态度？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:50:27 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:50:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>作为一名教育者，我确实考虑了很多。一方面，我想告诉学生我所知道的一切对他们也有用我说并检查它是否真的是真的。</p><p>因此，一些老师通过牺牲第一部分或第二部分来解决这一问题，因为做得很好，这是不幸的。</p><p>当我在学校的时候，我有一个老师，他非常擅长将这些结合起来。她会通过向我们发表热情的演讲来开始一个话题，这使我们关心并告诉我们她的信念，但随后她让我们深入研究了主题，阅读了各种报告并得出了自己的结论。而且它起作用了，许多学生<i>确实</i>得出了不同的结论。</p><p>我还回想起<a href="https://www.overcomingbias.com/p/my-favorite-liahtml">“我最喜欢的骗子”</a> ，在那里，老师在每个讲座中都植入了虚假的事，并告诉学生这件事，因此他们会仔细检查他的讲座以找到故意的错误，但在此过程中，他们也怀疑并仔细检查其他所有内容。而且我想您可以称之为一种拖钓。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:51:21 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:51:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>好的。非常好！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:52:26 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:52:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这确实是一种巨魔。毕竟，当老师要欺骗您时，她/他总是会事先让您知道您即将遇到错误信息。这就是您知道何时需要持怀疑态度的方式。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:54:13 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:54:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>你明白吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 06:57:48 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 06:57:48 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我想是这样。我建议指定“<strong>故意</strong>欺骗您”，然后您拒绝了这一点。我想，但是如果他不故意这样做，他怎么能让你知道他会欺骗你呢？但是，由于他可能一直无意间欺骗您，因此他必须事先让您知道您可能会受到欺骗，并且应该持怀疑态度。那是主意吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 06:59:46 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 06:59:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p> [给读者的注释：在Lesswrong对话界面中有一个功能，Yoav可以在其中提出对我写的内容的更改。 Yoav这样做。我拒绝了变化。] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:01:14 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:01:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>那就是主意。</p></div></section><h2>回到“下一级别的理性水平是什么？” </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:11:50 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:11:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>回到我们最初的问题：“下一个理性的层次是什么？[eliezer]”，他没有写的（许多）事情之一是，对理性主义者互相拖走的重要性是多么重要。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:12:06 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:12:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>顺便说一句，费曼也是一个巨魔。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 07:12:58 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 07:12:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我认为，绝对比Eliezer更重要。 “<a href="https://en.wikipedia.org/wiki/Surely_You're_Joking,_Mr._Feynman!">当然，您在开玩笑，Feynman先生</a>”是我读过的最有趣的书之一。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:14:09 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:14:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这很滑稽。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:17:05 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:17:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>除了拖钓的重要性外，Eliezer从来没有写过哪些理性的其他方面？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 07:29:38 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 07:29:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>好吧，我认为最好的起点是<a href="https://www.lesswrong.com/posts/uXn3LyA8eNqpvdoZw/preface">序言</a>Eliezer在2015年写给“合理性：AZ”的文章，他列出了他在序列中犯的5个总体错误：</p><ol><li>不是为了帮助人们在日常生活中做得更好而不是帮助他们解决大，困难，重要的问题</li><li>过多地关注如何学习该理论，而在如何练习方面不够。</li><li>过多地专注于理性信念，太少于理性行动。</li><li>序列中的内容没有很好地组织（现在使用新序列和LW Wiki，情况要好得多）</li><li>清楚地谈到了似乎是愚蠢的想法的愚蠢，而不是更有礼貌地写作。</li></ol><p>我认为前3个与我们的讨论有关。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 07:51:43 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 07:51:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>我会添加的其他一些要点（一些实用的，一些基础/理论）</p><ol><li>此后的序列和大多数LW主要集中于如何更合理地作为个人，而不是如何作为理性主义者合作或更合理地作为一对或群体。</li><li>它忽略了传统中信息的价值。 （诸如林迪原理，切斯特顿的篱笆等之类的东西）</li><li>相关的是，它忽略了有多少东西，例如某些偏见，在更好地分析或考虑我们的局限性时实际上可能是合理的。</li><li>它基于贝叶斯主义，这有点像一般的相对论，因为我们知道这是非常正确的，但不是<i>完全</i>，并且之后的<i>某些东西</i>应该比它更正确。有了贝叶斯主义，问题是它假定逻辑无所不知，并从外部观察世界。</li><li>序列中指出的大多数基础问题 - 众人推理，反思性推理，奇怪的循环循环 - 尚未解决。尽管这些在日常生活中并不重要，因为它们要么没有出现，要么我们有答案的直觉，所以这些肯定会很好地解决，这将表明合理性具有牢固的基础，对于那些关心此类事情的人。</li></ol><p>当然，在序列写入序列之后，这些都已在某种程度上被解决，因此这不是新事物或任何事物，其中许多<i>在</i>“供水”程度上（尤其是传统上信息的价值）。</p><p>但这表明我们没有理性主义的典范实际上涵盖了现代理性主义思想，这是我们需要培养新的阶段/理性级别的东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 07:58:59 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 07:58:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这非常有帮助。您指出我想写的主题，但不确定如何处理。例如，我想写一篇有关伪君子的好处的文章。 （大多数宗教人士都是伪君子。如果您治愈伪君子，有些人可能会转向理性，而另一些人则最终成为原教旨主义的极端主义者。）它属于您的“传统价值忽视”伞。</p><p>但是我认为最有前途的观点可能是“如何作为理性主义者合作或更合理地作为一对或团体合理”。 Eliezer开始时，这并不重要。毕竟，几乎没有社区可以协调。但是我一直在做许多苏格拉底对话，通常要做的第一件事就是教我的伴侣如何进行苏格拉底对话。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 08:00:49 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 08:00:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>这与“帮助人们在日常生活中更好地做得更好”和“ [F]在学习理论方面过多，而在如何练习它方面也不够”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 08:05:22 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 08:05:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>是的，这是我想在LW上写的第一件事（从2020年1月起，我有一份合理性的草稿），但我觉得我没有太多话要说了，我没有我个人生活中对我的理性感兴趣的任何人（仍然没有），所以我没有机会自己发展这一部分。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 08:09:21 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 08:09:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>很难自己发展<strong>苏格拉底</strong>式的艺术。 😛😛</p><p>关于苏格拉底对话，我有很多话要说，但是，正如您指出的那样，我的写作通常很难与人们互动。</p><p>我认为根源问题是，当我为抽象的受众写作时，我很擅长猜测读者会和不会理解的内容。这就是为什么我非常喜欢这些对话。我可以问“你理解吗？” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 08:09:43 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 08:09:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>而且它正在奏效，我遇到的一件困难都没有遇到您的写作。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 08:11:07 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 08:11:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>然后，这个理性项目的下一步也许是让您和我进行有关“如何进行苏格拉底对话”的苏格拉底对话。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="sKAL2jzfkYkDbQmx9-Tue, 12 Dec 2023 08:12:07 GMT" user-id="sKAL2jzfkYkDbQmx9" display-name="Yoav Ravid" submitted-date="Tue, 12 Dec 2023 08:12:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Yoav Ravid</b></section><div><p>好吧，这听起来不错。让我们下次从那里捡起它:) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 08:12:14 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 08:12:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p> （：</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/MtNmbY5cx3vTXMGvs/what-is-the-next-level-of-rationality-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mtnmby5cx3vtxmgvs/what-is-the--next-next-level-fironality-priondityity-1<guid ispermalink="false"> mtnmby5cx3vtxmgvs</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Tue, 12 Dec 2023 08:14:14 GMT</pubDate> </item><item><title><![CDATA[Embedded Agents are Quines]]></title><description><![CDATA[Published on December 12, 2023 4:57 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 02:11:30 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 02:11:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>熵目标还是主观？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 02:17:02 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 02:17:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>从某种意义上说，如果您相信主观概率。即，如果我们使用香农的熵公式<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="-\sum_i p_i \log p_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">，</span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">-d</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">登录</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">pi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>然后，公式本身是客观的，但是贝叶斯人会有主观的概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">P</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">I</span></span></span></span></span></span></span></span></span> ，这可能会因人而异。</p><p>另一方面，似乎在某些情况下，熵是非常客观的。就像一盒汽油的熵不会因人而异，而且不可能通过简单地改变主观信念的方式来避免宇宙的热死亡，以至于熵下降。</p><p>在您的世界观中，这是一种奇怪的张力，该熵应该是主观的，也不是某种程度上。我有这个模因躺在某个地方，让我看看是否可以将其拉起...</p><p> “从各个角度来看，热力学的第二定律”： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/jkrcqlp6qquwd3v4yqh1" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/cnezdo9d1c88gpz09l5q 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/l9gn4gebivqqi9pxyrxi 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/bxmeq6w6jtpj4y0mop3t 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/ybvs3zkx34g0zxyygnco 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/xdcn1wqz73uexsgyyqlg 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/y4otmewulr2jj2ojfc9z 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/iy2aqffuctllcdfw7wfq 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/vpaoks7jqj2ngxl7fasg 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/dl5e7izchirjyapcx7f7 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iQs7BD564AZzpzSNR/jevcarv2b9ipjxlkje9x 800w"></figure><p>无论如何，笑话是问，在各种知识状态下，热力学的第二定律是什么样的。对于我们的人类来说，我们不知道，我们被原子以我们没有观察到的方式弹跳的气体（以及其他物质）包围。每当有摩擦或任何其他耗散过程时，熵一直在上升。然后，拉普拉斯的恶魔是我们所谓的“逻辑无所不知的存在”的替身。恶魔并不是全面的，但它具有无限的内存和计算能力。由于物理定律是可逆的，因此熵永远不会增加这样的存在。美国凡人的熵显然增加只是概率分布变得过于复杂而纠结以使我们追踪的结果。然后，“上帝”将成为真正的全知的立场，实际上确实知道每个原子的位置。对于一切都可以肯定的上帝来说，熵为0。（模因的其余部分只是对康威生活游戏不可逆的事实的评论，作为读者的锻炼。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 02:28:48 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 02:28:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>您似乎并没有陷入我的技巧问题。通常，我谈论的人认为熵一定是客观的数量。毕竟， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S=\textrm{entropy}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">s</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.519em;">熵</span></span></span></span></span></span></span></span></span>出现在热力学方程中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="dS=\frac{\partial Q}T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">例如</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">d</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 1.101em; padding: 0px 0.12em;"><span class="mjx-denominator" style="font-size: 70.7%; width: 1.557em; bottom: -0.676em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">s</span></span></span> <span class="mjx-numerator" style="font-size: 70.7%; width: 1.557em; top: -1.615em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em; padding-right: 0.035em;">∂qt</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.101em;" class="mjx-line"></span></span><span style="height: 1.621em; vertical-align: -0.478em;" class="mjx-vsize"></span></span></span></span></span></span></span> 。但是，我对您的评论最有趣的是“如果您相信主观概率”一词。</p><p>概率是主观的吗？塔斯基（Tarski） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 02:31:21 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 02:31:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><blockquote><p>如果概率是主观的，<br>我希望相信概率是主观的。<br>如果概率不是主观的，<br>我希望相信概率不是主观的。<br>让我不要对我可能不想要的信念依恋。 </p></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 02:45:20 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 02:45:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>好的，对我来说，塔斯基的景色在明显经验的问题的背景下是最有意义的。像无菌中微子是事物还是其他。在这种情况下，我希望观察到概率是从根本上主观的还是从根本上客观的？这不是一个经验问题。</p><p>不过，从另一种方面来看，我认为塔斯基（Tarski）的litany为问题提供了答案。答案的粗略概述，以免使您保持悬念，虽然概率可能是主观的，但概率定律，管理概率如何操纵的规则是不可变的。</p><p>概率可以从逻辑中概括，其中“ true”变为“ 1”和“ false”变为“ 0”。因此，如果我们将Tarski的litany推广到概率环境中，我们会得到类似的东西：</p><p>如果x发生，我希望分配概率1。</p><p>如果X没有发生，我希望分配IT概率0。</p><p>如果我在两者之间分配了概率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">P</span></span></span></span></span></span></span> ，则分数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\log p_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">我</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">是</span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">实际</span></span></span></span></span></span></span>结果。</p><p>我确定您知道，这实际上是对机器学习算法中必须将事物分为类别的损失功能的描述。概率是主观的，因为不同的人可能对各种事件具有不同的概率估计，但是对数概率得分本身并不是要抢夺。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 02:51:13 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 02:51:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>陈述。概率是可观察到的现象吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 02:54:15 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 02:54:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>我的脑海中的概率是按照神经元连接的方式进行编码的，也许是神经递质的浓度以及类似的东西。从原则上讲，这显然可以观察到。如果您将球扔到他们的头上，大多数人会退缩。他们为什么退缩？因为他们为球打球的可能性很高。因此，这肯定是概率的可观察结果。</p><p>如果我们谈论主观概率，这个问题很容易。只有一个人希望拥有客观的概率，我们是否可以观察到它们的问题才变得困难。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 03:03:57 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 03:03:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我想我错了这个问题。我应该写“物质现象”，而不是“可观察的现象”。</p><p>你们和我都是重大的还原学家，我试图阐明的观点不是对物理唯物主义的攻击。相反，我试图提请注意贝叶斯对概率解释的某个方面。</p><p>我将这个问题称为<a href="https://www.lesswrong.com/posts/rQCAn3ZzHKoZGdANd/the-case-for-frequentism-why-bayesian-probability-is#The_Belief_Value_Uncertainty_Principle">信念价值的不确定性原则</a>。生物神经元连接是黑匣子。它们是不可解剖的。我们只能观察到的是输入和输出。我们无法直接观察信念或价值观。</p><p>如果我们知道输入，输出和价值，那么我们可以推断信念。如果我们知道输入，输出和信念，那么我们可以推断价值。但是，对于每一个可能的信念，都有一个相应的价值可以符合我们的观察结果。对于每个相应的价值，都有一种可以与我们的观察相匹配的信念。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 03:13:27 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 03:13:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>是的，就像我们要在太空外星人一起观看一些希腊数学家，用简直和指南针进行几何形状。他们正在绘制大约圆形和三角形的东西，但只有大约。真正的现实是，他们在纸上工作并绘制木炭（或实际使用的东西？），并且线条具有厚度，圆圈也不是很圆，因此您可以看到该系统如何看待各种问题并非完全模仿数学理想。也许其中一个人在他的工作中真的很草率，只是为一切徒手画徒手。因此，有一个哲学问题：“这些甚至不再圈子了”？我们也可以以类似的方式提出：人类神经元只是在近似概率，而有些人实际上在推理方面确实很糟糕，但是没有人接近遵循理想的数学概率定律。在什么时候这个“概率”想法不再是对某人的想法的很好的描述？如果对世界上任何人的想法都不是一个好的描述，该怎么办？</p><p>在他的书（我还没有读过）中，埃特·杰恩斯（Et Jaynes）做出了一个很好的决定。他没有将其作为人类的思维方式介绍。他介绍了“让我们假设我们要建立理想的推理机器人。设计它的最佳方法是什么？” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 03:21:29 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 03:21:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>的确。让我们考虑一个理想的推理机器人，而不是人类，其正值大脑内有一个清晰的世界模块。</p><p>对我这一生物来说，有趣的是，它的潜在世界模型及其潜在价值功能<a href="https://www.lesswrong.com/posts/tFjoPbLGrvEvAL9TL/self-reference-breaks-the-orthogonality-thesis">不是正交的</a>。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 03:26:39 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 03:26:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>暂时还不要全面对正交性论文进行全面的辩护，但是看着您的文章，似乎您预测您的世界模型将有一种倾向于尝试操纵价值/搜索部分的趋势中介。我不认为这一定是真的吗？就像我想象训练馈送前进的神经网络以使“接下来会发生什么？”对基于规则的环境的预测，似乎它只是试图越来越好，以做出这些预测。如果我改变了要求它做出的预测的困难，那么我们如何最终以该网络试图操纵我提供最简单的预测？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 03:30:43 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 03:30:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>什么样的神经网络？它纯粹是功能性的（如在“功能编程”中），还是它具有宇宙的清晰表示形式？那是说明的吗？</p><p>顺便说一句，您无需捍卫所有可能的攻击。我只是声称嵌入式世界优化器至少具有一个信念值的约束。我没有在此对话中处理正交论文的任何其他问题。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 03:32:38 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 03:32:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>对于世界模型，我想象一个馈送前进网络，但是GPT-4是前进的，因此它仍然可以非常复杂，并且使用了注意层和类似的东西。 GPT-4在进行自动回火生成时还会获得刮擦垫，因此我们可以想象，世界模型还可以在做出预测时获得刮擦板来解决中间状态。刮擦板是有状态的，所以我想从某种意义上说，这对宇宙具有陈述的代表。不过，也许不是一个清晰的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 03:46:46 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 03:46:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我们可以通过定义固定函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F:\textrm{scratchpad}\to\textrm{universe}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">：</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">scratchpad</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">宇宙</span></span></span></span></span></span></span></span></span>从scratchpad到宇宙宏观物质，使其清晰可见吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:01:36 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:01:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>我想我看到的问题是，根据刮擦板的大小，我们的宏观数量有限。如果函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>是固定的，则杀死了很多灵活性。就像我们谈论编程计算机以及函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>粗晶粒的详细信息（例如电子的确切位置）一样，我们正在建立一个看不见编程点的代理。据它可能说明，它不会通过重新排列所有这些电子来改变宏观。另外，当人类计划时，我们通常不会将自己视为重新安排电子，尽管这实际上是我们所做的。我们从这个独立的本体论中思考“我正在编写计算机程序”。</p><p>因此，换句话说，我们有各种抽象来导航宇宙，在不同的时间拉动了不同的抽象。如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>可以是学习的功能，而不是固定功能，那么我们可能会到达某个地方。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:06:59 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:06:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>对我来说， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>是否是一个学识渊博的功能都没关系，但是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>固定很重要。 （这将稍后至关重要。）</p><p>我承认，单个刮擦状态对无限宇宙状态的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>映射在宇宙状态之间创造了隐含的函数，从而限制了作为价值函数表达的可能性。如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F^{-1}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.266em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1幅</span></span></span></span></span></span></span></span></span></span></span>地图“ LSUSR吃早餐”和“ LSUSR不吃早餐”到同一scratchpad状态，那么不可能定义一个值得我吃早餐的价值功能。那不是我认为正交论文中断的地方。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:09:48 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:09:48 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>好吧，有趣。然后说以下内容：刮擦板由两个部分组成：一个部分包含函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>的源代码，而另一个则包含一些长的二进制数据， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span> 。然后陈述是从宇宙的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">微晶</span></span></span></span></span></span></span>格到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span>的某些相应值。</p><p>我认为这围绕着灵活性问题，同时仍然希望保留在宇宙状态与刮擦板中的数据之间进行一些清晰映射的基本概念。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:11:41 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:11:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>的源代码是人类可读的吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:12:17 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:12:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>我们也可以指定它是，不是吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:13:26 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:13:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我想实际上重要的是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>是人为的。无论如何……继续。 :) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:31:29 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:31:29 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>好的，这就是世界模型的处理。机器人也应该有一些值所追求的价值。假设我们有一些博学的功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span></span></span></span></span> ，将实用程序分配给宇宙状态。 （我们将限制上面可以使用的可能功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>仅限于保留足够信息的功能，从而可以从<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span>中推断出<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">您</span></span></span></span></span></span></span>。）</p><p>因此，无论如何，我们拥有此功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span></span></span></span></span> ，然后我们想跨动作<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span>和功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>搜索。对世界模型的查询是：假设我们采取了行动<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> ， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">u</span></span></span></span></span></span></span>的期望值是多少？</p><p>我们可以尝试不同的功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span> （确定世界模型预测的精度，某些功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>将显示为潜在的有价值的策略，而其他功能则不会，具体取决于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>是否跟踪电子。）特工在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>的所有选择中都选择了具有最高预期效用的动作。</p><p> （存在一个问题，即代理商如何预测自己的未来行动。我有时喜欢在这种非常抽象的“让我们想一想的代理人设计”讨论的简化假设是，假设代理人只需要做出在整个宇宙历史上的一个（可能非常复杂）的决定（可能非常复杂）。理由是代理人可以提前决定“如果发生了，那么我将去做x”，而“如果发生B，则我是我因此，如果我们将所有这些未来的潜在观察和行动打包成一开始就做出的单一决定。这起作用。 ） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:34:55 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:34:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>听起来<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>是<a href="https://en.wikipedia.org/wiki/Quine_(computing)">Quine</a> 。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:44:39 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:44:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>这取决于我们正在使用哪种<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">选择</span></span></span></span></span></span></span>。如果它忽略了电子的确切位置（甚至只是机器人内部的电子位置），则它不是Quine。如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>确实将有关机器人内部工作的信息映射到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>中，则至少会有一些自我参考。在极端情况下， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>的输出只是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>数据存储的内容，然后很像Quine。除了随着机器人考虑不同的动作并使用不同的功能<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span>尝试和分析世界时， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span>的内容随着时间的流逝而变化。机器人已经决定采取措施的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>内容几乎没有兴趣，这是机器人上次搜索的任何可能性。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:47:42 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:47:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>我想这不是一个完美的奎因。重要的是， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span>有时（尤其是对于智能，有力的机器人）必须引用自身，而不会矛盾。存在一些函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span></span></span></span></span> ，以使这种自我参考对<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">d</span></span></span></span></span></span></span>构成限制。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ei73H4RbCT5TTaXNa-Tue, 12 Dec 2023 04:51:54 GMT" user-id="Ei73H4RbCT5TTaXNa" display-name="DaemonicSigil" submitted-date="Tue, 12 Dec 2023 04:51:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>守护神</b></section><div><p>好的，因此，如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span></span></span></span></span>将整个世界历史（并预测未来）作为输入，那么我们可以考虑一些特定的递归<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_r"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span></span></span></span></span></span></span> ，特别是在使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_r"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">R</span></span></span></span></span></span></span></span></span>分析宇宙的时候，这些递归f r尤其是对机器人的思想的看法然后，它只会将数据转输给<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span></span></span></span></span>到输出中的发现。 （我们还可以考虑矛盾<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_r'"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">的</span></span></span><span class="mjx-stack" style="vertical-align: -0.157em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.266em; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">f&#39;r</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">，</span></span></span></span></span></span></span></span></span></span>它会在倾倒之前将所有位反转。我想这意味着世界模型不能足够强大，无法过分建模。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="n6LYNw2uGfYnD4pX2-Tue, 12 Dec 2023 04:52:30 GMT" user-id="n6LYNw2uGfYnD4pX2" display-name="lsusr" submitted-date="Tue, 12 Dec 2023 04:52:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>LSUSR</b></section><div><p>正确的。当世界模型变得太强大时，它就无法建模。如果强大的世界模型无法建模，那么它就无法建模世界。如果强大的世界模型无法建模世界……那么正交论文就会以其自身的重量崩溃。</p></div></section><div></div><br/><br/><a href="https://www.lesswrong.com/posts/iQs7BD564AZzpzSNR/embedded-agents-are-quines#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iqs7bd5644azzpzsnr/embedded-agent-are-quines<guid ispermalink="false"> iqs7bd564azzpzsnr</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Tue, 12 Dec 2023 04:57:31 GMT</pubDate> </item><item><title><![CDATA[Predict the future! Earn fake internet points! Get a (free) gambling addiction!]]></title><description><![CDATA[Published on December 12, 2023 4:39 AM GMT<br/><br/><p><strong>请在METUP页面上进行RSVP，以了解我的理智。它在雷德蒙德的一个私人位置举行 - 文字我的Google语音号码（+1（425-298-5490））。</strong><br><br>您是否觉得西雅图EA/理性社区太负责？<br>发现它充满了令人羡慕的时间管理实践？<br>决定您需要做些什么？</p><p>嗯，我做到了！<br>这就是为什么您在首届Seastside预测市场聚会上的页面上的原因！</p><p>什么是预测市场？ <a href="https://www.nytimes.com/2023/10/08/technology/prediction-markets-manifold-manifest.html">《纽约时报》</a>在有关我们将要使用的网站的文章中说：“他们在那里庆祝预测市场，在线平台，用户可以在未来的活动中下注 - 乌克兰将在乌克兰重新获得对克里米亚的控制权。 2024？ “埃隆·马斯克（Elon Musk）和马克·扎克伯格（Mark Zuckerberg）在2023年会进行身体战斗吗？”。</p><p>用我的话来说，他们让您押注活动的可能性，并在您正确时赢得金钱/假货点（反之亦然）。他们很有趣，他们提供了社会利益（对未来事件的准确概率），我认为它们可能是EA社区的重要替代纽带活动（并且不需要我们具有执行功能来阅读一份事先预订）。</p><p>我个人非常喜欢我在预测市场网站歧视上的时间。您使用伪造的互联网点押注事件结果的市场。每个假的互联网点也可以用来向您选择的慈善机构捐款。</p><p>在第一次聚会期间，我的计划是（异步/可选）让我们执行任何以下操作：</p><ol><li>亲自向任何有兴趣的人提供有关哪些预测市场的解释。</li><li>讨论我们认为很有趣的最新预测市场</li><li>让已经有帐户谈话/吹牛的人就自己做的赌注，他们的策略是什么，等等。</li><li>讨论未来的聚会，建立一个小组织者的结构（我们将来要<a href="https://www.lesswrong.com/posts/gyyGxMHvyuEqqskXB/oops-it-s-time-to-overthrow-the-organizer-day">推翻组织者的日子</a>）</li><li>让每个人都像往常一样一般聊天/社交</li><li>在免费食物和饮料中填满每个人:)</li></ol><p> <i><strong>The event is going to be hosted in a private space so please text me for the address; my google voice number is +1(425-298-5490).</strong></i></p><p> Feel free to comment here if you have any questions (or <a href="https://discord.gg/mQ9wTR9Z">join the discord</a> )</p><p> Finally, don&#39;t forget to earn yourself some fake internet points after you <i><strong>RSVP,</strong></i> <a href="https://manifold.markets/RobertCousineau/how-many-people-will-attend-the-ina?r=Um9iZXJ0Q291c2luZWF1"><i><strong>using this market</strong></i></a> !</p><p> For more information on prediction markets feel free to take a look at the following:</p><ol><li> <a href="https://www.youtube.com/watch?v=xA27x7GRMZQ">This YouTube video on prediction markets:</a></li><li> <a href="https://outsidetheasylum.blog/prediction-markets-are-not-polls/">This guy explaining then, and then emphatically arguing they are better than polls (they are!)</a></li><li> <a href="https://manifold.markets/about">The Manifold Markets &quot;About&quot; page</a></li></ol><br/><br/> <a href="https://www.lesswrong.com/events/NcscdtcFNSCFJxTq8/predict-the-future-earn-fake-internet-points-get-a-free#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/NcscdtcFNSCFJxTq8/predict-the-future-earn-fake-internet-points-get-a-free<guid ispermalink="false"> NcscdtcFNSCFJxTq8</guid><dc:creator><![CDATA[Robert Cousineau]]></dc:creator><pubDate> Tue, 12 Dec 2023 04:39:50 GMT</pubDate> </item><item><title><![CDATA[The likely first longevity drug is based on sketchy science. This is bad for science and bad for longevity.]]></title><description><![CDATA[Published on December 12, 2023 2:42 AM GMT<br/><br/><p> If you are interested in the longevity scene, like I am, you probably have seen press releases about the dog longevity company, Loyal for Dogs, getting a nod for efficacy from the FDA. These have come in the form of the New York Post calling the drug &quot; <a href="https://news.google.com/articles/CBMidWh0dHBzOi8vbnlwb3N0LmNvbS8yMDIzLzEyLzEwL2xpZmVzdHlsZS9ncm91bmRicmVha2luZy1hbnRpLWFnaW5nLWRydWctZm9yLWRvZ3MtbW92ZXMtY2xvc2VyLXRvLWdhaW5pbmctZmRhLWFwcHJvdmFsL9IBeWh0dHBzOi8vbnlwb3N0LmNvbS8yMDIzLzEyLzEwL2xpZmVzdHlsZS9ncm91bmRicmVha2luZy1hbnRpLWFnaW5nLWRydWctZm9yLWRvZ3MtbW92ZXMtY2xvc2VyLXRvLWdhaW5pbmctZmRhLWFwcHJvdmFsL2FtcC8?hl=en-US&amp;gl=US&amp;ceid=US%3Aen"><u>groundbreaking</u></a> &quot;, Science Alert calling the drug &quot; <a href="https://news.google.com/articles/CBMiVGh0dHBzOi8vd3d3LnNjaWVuY2VhbGVydC5jb20vYS1yYWRpY2FsLW5ldy1kcnVnLWlzLXBvaXNlZC10by1leHRlbmQtdGhlLWxpZmUtb2YtZG9nc9IBAA?hl=en-US&amp;gl=US&amp;ceid=US%3Aen"><u>radical</u></a> &quot;, and the more sedate New York Times just asking, &quot;Could Longevity Drugs for Dogs Extend Your Pet&#39;s Life?&quot;, presumably unaware of Betteridge&#39;s Law of Headlines. You may have also seen the coordinated Twitter offensive of people <a href="https://twitter.com/celinehalioua/status/1729525718827274642"><u>losing their shit about this</u></a> , including their lead investor, Laura Deming, saying that she &quot; <a href="https://twitter.com/LauraDeming/status/1729529895347515443"><u>broke down crying when she got the call</u></a> &quot;.</p><p> And if you have been following Loyal for Dogs for a while, like I have, you are probably puzzled by this news. Loyal for Dogs has been around since 2021. Unlike any other drug company or longevity company, they have released almost zero information (including zero publications) about their strategy for longevity. It&#39;s surprising, to say the least, to see a company go from zero information to efficacy nod, because, well, what are you basing your efficacy on? How did you recruit your patients and veterinary partners to help you with efficacy? Did you make them all sign some incredibly airtight NDAs?如果是这样，为什么？</p><p> These thoughts swirling around my head, I waded through the press releases trumpeting the end of dog death as we know it in order to figure out what exactly Loyal is doing for dog longevity. And, what I found first surprised me, then saddened me. Loyal did not prove efficacy in dog longevity. They found a path around the FDA instead. That&#39;s the surprising part. The sad part is that, in doing so, they relied on some really sketchy science. And I think that, based on their trajectory, they won&#39;t just be the first company to get a drug approved for longevity. They will be the first one to get a longevity drug pulled for non-efficacy as well, and put the field back years.</p><p> So let&#39;s start with how they got their drug approved in the first place. Well, they didn&#39;t. To get drugs approved in animals, you need to prove three things: efficacy, safety, and manufacturing consistency. Normally, efficacy is the hardest part of this, because you have to prove to the FDA that your drug cures the disease that it&#39;s supposed to. This is especially hard in aging, because any aging trial would take a long time. Loyal found a way around that. If you can instead prove to the FDA that it would be too difficult to test your animal drug for efficacy before releasing it, they allow you to sell the drug first, and prove the efficacy later. This is a standard called &quot;reasonable expectation of effectiveness&quot;.</p><p> So, what exactly did Loyal show to the FDA to prove that there was a reasonable expectation their drug would be effective in aging? Well, it&#39;s hard to tell, because, again, Loyal has released very little data. But, based on the NYT article and <a href="https://loyalfordogs.com/posts/loyal-announces-historic-fda-milestone-for-large-dog-lifespan-extension-drug"><u>their blog post</u></a> , I can sketch out a basic idea of what they did.</p><p> Loyal&#39;s longevity drug is an injectable insulin-like growth factor 1, or IGF-1, inhibitor. As the name suggests, IGF-1 is closely related to insulin and is regulated by insulin. Also as the name suggests, IGF-1 causes things to grow. High IGF-1 causes acromegaly, the condition that makes people look like storybook giants.</p><p> Loyal gave their IGF-1 inhibitor to healthy laboratory dogs (and possibly diabetic dogs, although it&#39;s hard to tell). Lo and behold, it lowered IGF-1. It probably also reduced insulin. They then looked at healthy pet dogs, and found that big dogs had higher levels of IGF-1, which is one of the reasons they&#39;re big. Small dogs had lower levels of IGF-1. Small dogs, as we all know, live longer than big dogs. Therefore, Loyal said, our IGF-1 inhibitor will extend the life of dogs.</p><p> Needless to say, this is bad science. Really bad science. There are holes big enough in this to walk a Great Dane through, which I&#39;ll talk about in a sec. Apparently, though, this reasoning was good enough for the FDA. So, Loyal got their &quot;reasonable expectation of effectiveness&quot; nod. Cue fanfare and crying and me being surprised.</p><p> Ok, so that&#39;s how they did it. But...why did they do it? No wait, that&#39;s an easy one.钱。对不起。 Ok, better question: should we believe them?</p><p>不！天哪，不。 Their reasoning is terrible! Correlation does not equal causation! Just because small dogs have lower levels of IGF-1 and live longer does not mean that lower levels of IGF-1 cause dogs to live longer! And, even if they did, there&#39;s no reason to believe that lowering levels of IGF-1 would reverse any of the &quot;damage&quot; caused by high levels of IGF-1! The big dogs will still be big!</p><p> And, actually we can even go further than that. Not only is Loyal&#39;s reasoning wrong, their facts are wrong as well. Let&#39;s look at some of the claims in their blog post, shall we?</p><p>他们说</p><p>&quot;In large- and giant-breed dogs, breeding for size caused these dogs to have highly elevated levels of IGF-1, a hormone that drives cell growth. High IGF-1 effectively drives these dogs to grow large when they&#39;re young, but high IGF-1 levels in adult dogs are believed to accelerate their aging and reduce their healthy lifespan.&quot; Then they put this &quot;figure&quot;. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/k0c2ahmhssqldrp325lg"></figure><p> This figure, although attractive, is misleading at best. IGF-1 contributes to body size in dogs, but it&#39;s not a straightforward relationship. Like, look at this figure from <a href="https://sci-hubtw.hkvisa.net/10.1007/s11357-010-9182-4"><u>Connecting serum IGF-1, body size, and age in the domestic dog</u></a> (2011). </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/f4hj71h0tieioly7xrkg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/zoijbkawvtednat01gby 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/umoai9i0bwtpjtbqcnio 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/yx4bwmsaykgwyrf3fx6h 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/yb0mqaujul0rtzuudoeo 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/gkuhr1nkd1t59l36cttx 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/fatgpprrtpv3t2aim2na 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/gr0fgzwmbk2dkj6twruz 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/z2v7xqhtmcm2ogwidsrq 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/mhfvqsbcq9yj05hltj3g 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/sp0pcnyzbqq87r2rwr7h 888w"></figure><p> This is, granted, a less attractive graph than Loyal&#39;s. Theirs has cute little figures of dogs and this one just has black squares. But, this is way more recent and based on 88 dogs. Their references, which I can&#39;t find (and they didn&#39;t even provide proper citations for), are from 1984 and 1998, and based on an unclear number of dogs. I am more inclined to trust this graph.</p><p>它说了什么？ Well, that IGF-1 levels do tend to go up with increasing bodyweight, but it&#39;s not a straightforward relationship. Among dogs with bodyweight around the 20-30 pound mark, it looks like IGF-1 levels can vary from 20-ish to close to 300 ng/mL. Now, could I cherry pick and find a small dog with a low level of IGF-1 and a big dog with a high level of IGF-1 and say &quot;big dogs have up to 28x the level of IGF-1 of small dogs&quot; ？是的我可以。 But that would be incredibly misleading.</p><p> Meanwhile, if we look at the exact same paper, we also get this data on the relationship between IGF-1 and age in dogs. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/wkinnjvwc0ltpjjuh4oh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/w1krcam6epgj1bockiid 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/cv4nrdvvfqqewt4jutjq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/p1odvjqxoy2ekxhimprr 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/go1bxw9ebzasnqpyqkcv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/tjbowkhwnsn1oyrf7xh2 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/rhm3eunimfw6rpljvudx 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/zkcip65xkdf6iyf64ly5 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/bihpizoydlpiopxpvk0j 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/kt3vqk1fe7oy4x2ewzgk 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vHSkxmYYqW59sySqA/iblwwwbkieuhvou4m0cr 846w"><br></p><p> Hm...let&#39;s see what we think of the idea that low levels of IGF-1 are associated with longer lifespans. Well, based on the fact that there are plenty of 12+ year old dogs with IGF-1 levels at the 100+ ng/mL mark, I&#39;d say that relationship also seems pretty weak. I mean, sure, the highest levels of IGF-1 are in puppies, but the 4th highest IGF-1 level is in a dog that&#39;s over 16, and the 5th highest is in a dog that&#39;s like 12.5.</p><p> So, not only should Loyal definitely not say that &quot;high IGF-1 levels accelerate dog&#39;s aging&quot;, but they shouldn&#39;t even really say that they&#39;re strongly associated with them. It&#39;s a weak correlation at best (r=.0677).</p><p> I hope you now see why I think this longevity drug ironically doesn&#39;t have long to live. There&#39;s no way, given the data, that this drug will cause dogs to live longer. It may, however, cause dangerous side effects in dogs. IGF-1 inhibitors do in humans (oh, did I forget to mention that IGF-1 inhibitors have existed for humans for decades, and they have zero evidence of being longevity drugs? My bad. Now I have). IGF-1 inhibitors can cause <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4093917/"><u>low levels of blood platelets, elevated liver enzymes, and hyperglycemia</u></a> . Hopefully, if this drug does cause harmful effects in dogs, it will be caught in the safety trials and the drug won&#39;t be released.</p><p> But, even if the drug is safe, it will be a waste of time and a waste of money for dog owners to give it to their pets. This drug should have never made it this far, and, if it does get actually approved, it&#39;s going to spectacularly fail its actual efficacy trials and bring a lot of legitimate longevity companies down with it.</p><p> And, for the record, I know I&#39;m not the only one that thinks this. I&#39;ve had multiple conversations with other aging-adjacent people who also think this. But nobody is willing to stick out their neck and actually call Loyal out. I mean, hell, I&#39;m writing this anonymously and I&#39;m still a little nervous. But, I&#39;ll say it here and now: the dog emperor has no clothes.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/vHSkxmYYqW59sySqA/the-likely-first-longevity-drug-is-based-on-sketchy-science#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/vHSkxmYYqW59sySqA/the-likely-first-longevity-drug-is-based-on-sketchy-science<guid ispermalink="false"> vHSkxmYYqW59sySqA</guid><dc:creator><![CDATA[BobBurgers]]></dc:creator><pubDate> Tue, 12 Dec 2023 04:57:03 GMT</pubDate> </item><item><title><![CDATA[When will GPT-5 come out? Prediction markets vs. Extrapolation]]></title><description><![CDATA[Published on December 12, 2023 2:41 AM GMT<br/><br/><p> So far, each generation of GPT has brought significant improvements. Thinking about the timeline for the next iteration, I noticed that there is a striking difference between extrapolating the past trend and what prediction markets seem to believe.</p><p> Forecasting short term AI Capabilities is important as current trends might continue to lead us to bigger changes. Also, it is nice to have testable predictions that we can use to calibrate our predictions and figure out who is worth listening to.</p><h1> Extrapolating from old GPTs</h1><p> Rule 1 of forecasting: stop thinking too much and look at some historical data.</p><p> We don&#39;t know how long it took to develop each GPT, but we can look at how much time passed between each iteration.</p><p> So far, more and more time has passed between GPTs. It took 8 months from GPT-1 to GPT-2 and roughly twice as long to GPT-3. And then it took twice as long again to get to GPT-4! This means it took almost 3 years to get from GPT-3 to GPT-4.</p><figure class="table"><table><thead><tr><th> GPT Model</th><th> Release Date <span class="footnote-reference" role="doc-noteref" id="fnrefuo8s7nj9qm"><sup><a href="#fnuo8s7nj9qm">[1]</a></sup></span></th><th> Months Passed Between Former Model</th></tr></thead><tbody><tr><td> GPT-1</td><td> 11.06.2018</td><td></td></tr><tr><td> GPT-2</td><td> 2019年2月14日</td><td>8.16</td></tr><tr><td> GPT-3</td><td> 28.05.2020</td><td> 15.43</td></tr><tr><td> GPT-4</td><td> 2023年3月14日</td><td>33.55</td></tr></tbody></table></figure><p></p><p> Naively Extrapolating from this, it should take till the <strong>beginning of 2029</strong> to develop GPT-5. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/meu7jkmse17gm5vcukd5" alt="输出图像"><figcaption> shoutout to datapoint 4 for making this graph</figcaption></figure><p> Now this does seem rather far in the future. So far the trends seems surprisingly consistent, but there aren&#39;t that many datapoints here and it seems like things can change somewhat fast in AI.</p><h1>预测市场</h1><p>Let&#39;s look at what the prediction markets think: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/vg5zdwuqo9hiyfbfmgse" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/i8cmmtcavzpf5csr7z7z 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/szstq9ewyoso73a6akzu 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/pfovab21l61qinq8ubim 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/s2me19fwmxsinfmko7rc 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/kvwgyd0lujdr0i64okeh 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/fomishcytyxgqrw1cx1r 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/xusf0vjnc03sxybecmgu 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/e3zazl1bvzygxfb4sako 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/qgwdidjmpxauynyujzpd 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/jn7gh8pjd8uxx0odbaok 1516w"><figcaption></figcaption></figure><p> Manifold Market seems to think, that there is an 62% chance that GPT-5 will come out before 2025.</p><p></p><p> Similarly, Metaculus puts the announcement date of GPT-5 to Sept 2024: <span class="footnote-reference" role="doc-noteref" id="fnrefazbeo12hzmb"><sup><a href="#fnazbeo12hzmb">[2]</a></sup></span> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/kszgusq5q0iutydyim8x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/zdhowxhnpycbk73xnio6 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/ecmh6g0izdxlwp9lka1q 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/igrzcvju3ayusgfy1if8 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/jpntxxqehceaktvd7s6g 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/lpbxna58nw5opkyrhinw 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/maci1hwlvoflqxgzddjw 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/g5huwi2m9h3l8t7wamut 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/ftvyrl6vrcnc5iw3z1yx 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/rdqzclfpyypgyo9ktuwa 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/ituplfjmue4kxafi15fh 1546w"></figure><p></p><p> So it seems like the forecasting sites expect a strong deviation from the historical trend.</p><p> Note, that if GPT-5 takes as long as GPT-4 did, we would expect it by the end of 2025. Not only do they expect that GPT-5 will not take longer than GPT-4, they seem to expect GPT-5 to be developed a whole year faster than GPT-4.</p><h1> So who is right?</h1><p> Now that we have observed the seeming discrepancy, we can speculate on how to resolve it.</p><p> One approach is to say that the markets are right. Ideally they would incorporate additional information, but what could that be?</p><ul><li> A lot of attention and resources are being shifted to AI right now<ul><li> in some ways chatgpt does seem like a discontinuity in the attention given to the field. <span class="footnote-reference" role="doc-noteref" id="fnrefaysubwwk44r"><sup><a href="#fnaysubwwk44r">[3]</a></sup></span> Sure <s>nerds</s> informed citizens couldn&#39;t shut up about AI before. But now everyone from <a href="https://arstechnica.com/information-technology/2023/05/snoop-dogg-on-ai-risk-sh-what-the-f/">Snoop Dogg</a> , <a href="https://twitter.com/sama/status/1666980817783103488?lang=en">world</a> <a href="https://www.youtube.com/watch?v=9ZA2ucNpUIE&amp;ab_channel=YahooFinance">leaders</a> and your parents talk about LLMs.</li></ul></li><li> Maybe people expect some kind of breakthrough. Something something <a href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/">Q*</a> ?<ul><li> I think some people have speculated that artificially created data will accelerate development</li></ul></li><li> Last but not least, it is possible that OpenAI will change its standards for what it considers a new iteration for GPT. It seems like that iPhones aren&#39;t changing as much anymore as they used to. Instead of releasing the iPhone (n+1) further and further in the future, Apple has lowered their standard for what counts as a new iPhone.</li></ul><p>我错过了什么吗？</p><h2> Help me get this question on a real money prediction market</h2><p> Metaculus and Manifold don&#39;t use real money. Does this affect forecasting ability? In my experience it is a lot easier to gain fake internet points on Manifold than real money on eg Polymarket.</p><p> Real money works great to make markets more efficient.</p><p> So what I hope for, is that we can create a real money market, so we can feel more confident in its prediction.</p><p> I have suggested the question to the Polymarket community. They have an issue with the framining: What if GPT-5 will be created under a different name? Like Windows 1, 2, 3, 95. Surely that should resolve YES and it should not depend on the naming scheme of OpenAI.</p><p> I would love to hear if you have any ideas how to deal with this. What would be a good definition for a “GPT-5 model” that can be assessed objectively? Has anyone written about what kind of benchmarks we might expect from GPT-5?</p><p> Also, I wonder which other real money prediction markets might consider such a market if I ask nicely? </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnuo8s7nj9qm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuo8s7nj9qm">^</a></strong></sup></span><div class="footnote-content"><p> To be consistent, this is referring to the initial release date. GPT-2 stands out in that the full model was released 9 months after the initial release date.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnazbeo12hzmb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefazbeo12hzmb">^</a></strong></sup></span><div class="footnote-content"><p> Note this is the <a href="https://www.metaculus.com/help/faq/#community-prediction">Community Prediction</a> .  This means, it it basically a simple average of all forecasts. Don&#39;t confuse it with the (superior) Metaculus Prediction, which incorporates how good forecasters are, but is harder to access.</p><p> If anyone know the Metaculus Predicition for this question, I would be interested in hearing it.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnaysubwwk44r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefaysubwwk44r">^</a></strong></sup></span><div class="footnote-content"><p> This is reflected in a <a href="https://www.theinformation.com/articles/openai-passes-1-billion-revenue-pace-as-big-companies-boost-ai-spending">jump</a> in OpenAIs Revenue and in some <a href="https://www.ft.com/content/b2928076-5c52-43e9-8872-08fda2aa2fcf">economic data</a> . </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BWMKzBunEhMGfpEgo/jmhzr47cc9lxnuja8sob" alt="Chart showing that generative AI is already taking white collar jobs and wages in the online freelancing world"></p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/BWMKzBunEhMGfpEgo/when-will-gpt-5-come-out-prediction-markets-vs-extrapolation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BWMKzBunEhMGfpEgo/when-will-gpt-5-come-out-prediction-markets-vs-extrapolation<guid ispermalink="false"> BWMKzBunEhMGfpEgo</guid><dc:creator><![CDATA[Malte]]></dc:creator><pubDate> Tue, 12 Dec 2023 02:41:59 GMT</pubDate> </item><item><title><![CDATA[On plans for a functional society]]></title><description><![CDATA[Published on December 12, 2023 12:07 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 21:49:06 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 21:49:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>I&#39;m going to expand on something brought up in <a href="https://www.lesswrong.com/posts/7iPFiMvFeZgFEgJuw/neither-ea-nor-e-acc-is-what-we-need-to-build-the-future?commentId=dvtR6kJ4aTAFcr7y2">this comment</a> .我写：</p><blockquote><p> A lot of my thinking over the last few months has shifted from &quot;how do we get some sort of AI pause in place?&quot; to &quot;how do we win the peace?&quot;. That is, you could have a picture of AGI as the most important problem that precedes all other problems; anti-aging research is important, but it might actually be faster to build an aligned artificial scientist who solves it for you than to solve it yourself (on this general argument, see <a href="https://intelligence.org/files/AIPosNegFactor.pdf">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a> ). But if alignment requires a thirty-year pause on the creation of artificial scientists to work, that belief flips--now actually it makes sense to go ahead with humans researching the biology of aging, and to do projects like <a href="https://loyalfordogs.com/">Loyal</a> .</p><p> This isn&#39;t true of just aging; there are probably something more like twelve major areas of concern. Some of them are simply predictable catastrophes we would like to avert; others are possibly necessary to be able to safely exit the pause at all (or to keep the pause going when it would be unsafe to exit).</p><p> I think &#39;solutionism&#39; is basically the right path, here. What I&#39;m interested in: what&#39;s the foundation for solutionism, or what support does it need? Why is solutionism not already the dominant view? I think one of the things I found most exciting about SENS was the sense that &quot;someone had done the work&quot;, had actually identified the list of seven problems, and had a plan of how to address all of the problems. Even if those specific plans didn&#39;t pan out, the superstructure was there and the ability to pivot was there. It looked like a serious approach by serious people. What is the superstructure for solutionism such that one can be reasonably confident that marginal efforts are actually contributing to success, instead of bailing water on the Titanic?</p></blockquote><p> Restating this, I think one of the marketing problems with anti-aging is that it&#39;s an <a href="https://en.wikipedia.org/wiki/Gilgamesh">ancient wish</a> and it&#39;s not obvious that, even with the level of scientific mastery that we have today, it&#39;s at all a reasonable target to attack. (The war on cancer looks like it&#39;s still being won by cancer, for example.) The thing about SENS that I found most compelling is that they had a frame on aging where success was a reasonable thing to expect. Metabolic damage accumulates; you can possibly remove the damage; if so you can have lifespans measured in centuries instead of decades (because after all there&#39;s still accident risk and maybe forms of metabolic damage that take longer to show up). They identified seven different sorts of damage, which felt like enough that they probably hadn&#39;t forgotten one and few enough that it was actually reasonable to have successful treatments for all of them.</p><p> When someone thinks that aging is just about telomere shortening (or w/e), it&#39;s pretty easy to suspect that they&#39;re missing something, and that even if they succeed at their goal the total effect on lifespans will be pretty small. The superstructure makes the narrow specialist efforts add up into something significant.</p><p> I strongly suspect that solutionist futurism needs a similar superstructure. The world is in &#39;polycrisis&#39;; there used to be a &#39;aligned AGI soon&#39; meme which allowed polycrisis to be ignored (after all, the friendly AI can solve aging and climate change and political polarization and all that for you) but I think the difficulties with technical alignment work have made that meme fall apart, and it needs to be replaced by &quot;here is the plan for sufficiently many serious people to address all of the crises simultaneously&quot; such that sufficiently many serious people can actually show up and do the work. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 21:53:53 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 21:53:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I don&#39;t know how to evaluate whether or not the SENS strategy actually covers enough causes of ageing, such that if you addressed them all you would go from decades-long lifespans to centuries-long lifespans. I think I&#39;m also a little more optimistic than you that a bunch of &quot;bailing out the sinking ship&quot; adds up to &quot;your ship is floating on its own&quot;.</p><p> I think that a nice thing about incremental and patch solutions is that each one gives you some interesting data about exactly how they worked, and details about what happened as a result. For example, it&#39;s interesting if, when you give someone a drug to make their blood pressure lower, for example, you end up with some other system reliably failing (more often than in the untreated population). And so I have a bit of hope that if you just keep trying the immediate things, you end up in a much better vantage point for solving a bunch of the issues.</p><p> I guess this picture still factors through &quot;we realised what the main problems were and we fixed them&quot;, it&#39;s just a bit more sympathetic to &quot;we did some work that wasn&#39;t on the main problems along the way&quot;.</p><p> I dunno how cruxy this is for your &quot;superstructure&quot; picture, or what the main alternative would be. I guess there are questions like &quot;many rationalist-types like to think about housing reform. Is that one of the crises we have to address directly, or not?&quot;. Is that the type of thing you&#39;re hoping to answer? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:01:06 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:01:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><blockquote><p>I think I&#39;m also a little more optimistic than you that a bunch of &quot;bailing out the sinking ship&quot; adds up to &quot;your ship is floating on its own&quot;.</p></blockquote><p> I think I&#39;m interested in hearing about the sources of your optimism here, but I think more than that I want to investigate the relative prevalence of our beliefs.</p><p> I have a sense that lots of people are not optimistic about the future or about their efforts improving the future, and so don&#39;t give it a serious try. There&#39;s not a meme that being an effective civil servant is good for you or good for the world. [Like, imagine Teach For America except instead it&#39;s Be A Functionary For America or w/e.]</p><p> There is kind of a meme that doing research / tech development for climate change is helpful, but I think even then it is somewhat overpowered by the whiny activism meme. (Is the way to stop oil to throw soup on paintings or study how to make solar panels more efficient?)</p><p> It seems to me like you&#39;re saying &quot;just do what seems locally good&quot; (ie bailing out your area of the ship) both 1) adds up to the ship floating and 2) is widely expected to add up to the ship floating, and I guess that&#39;s not what I see when I look around. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:04:55 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:04:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I now wonder if I understood what you meant by &#39;superstructure&#39; correctly. For example, I was imagining a coordinating picture that tells you whether or not to be an effective civil servant, and even what kind of civil servant to be. For example SENS, guides your efforts within ageing research. Like something that enumerates the crises within the polycrisis.</p><p> But it seems like you&#39;re imagining something that is like &quot;do stuff that adds up rather than stuff that doesn&#39;t&quot;. For example, do you imagine the superstructure is encouraging of &#39;be a civil servant making sanitation work well in your city&#39; or not? I was imagining that it might rule it out, and similarly might rule out &#39;try and address renewable energy via Strategy A&#39;, and I was saying &quot;I feel pretty hopeful about people trying Strategy A and making sanitation work and so on, even if it&#39;s not part of an N-Step Plan for saving civilisation&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:17:55 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:17:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><blockquote><p>I guess there are questions like &quot;many rationalist-types like to think about housing reform. Is that one of the crises we have to address directly, or not?&quot;. Is that the type of thing you&#39;re hoping to answer?</p></blockquote><p> I think there&#39;s a thing that Eliezer complains about a lot where the world is clearly insane-according-to-him and he&#39;s sort of hopeless about it. Like, what do you expect? The world being insane is a Nash equilibrium, he wrote a whole book about the generators of that.</p><p> And part of me wants to shake him and say--if you think the FDA is making a mistake you could write them a letter! You could sue them! The world has levers that you are not pulling and part of the way the world becomes more sane is by individual people pulling those levers. (I have not shaken Eliezer in part because he <i>is</i> pulling levers and has done more than other people and so on.) There&#39;s a &#39;broken windows&#39; thing going on where fixing the visible problems makes a place less seem like the sort of place that has problems, and so people both 1) generate fewer problems and 2) invest more in fixing the problems that remain.</p><blockquote><p> Like something that enumerates the crises within the polycrisis.</p></blockquote><p> I think this is exactly what I&#39;m looking for.</p><p> Like, imagine you went to the OpenPhil cause area website and learned that they will succeed at all their goals in the next 5 years. (&quot;succeed&quot; here meant in the least ambitious way--an AI pause instead of developing superalignment, for example.) Does that give you a sense of &quot;great, we have fixed the polycrisis / exited the acute risk period / I am optimistic关于未来”？ I think currently &quot;not yet&quot;, and to be fair to them I don&#39;t think they&#39;re trying to Solve Every Problem.</p><p> To maybe restate my hypothesis more clearly:</p><p> I think if there were A Plan to make the world visibly less broken, made out of many components which are themselves made out of components that people could join and take responsibility for, this would increase the amount of world-fixing work being done and would meaningfully decrease the brokenness of the world. Further, I think there&#39;s a lot of Common Cause of Many Causes stuff going on here, where people active in this project are likely to passively or actively support other parts of this project / there could be an active consulting / experience transfer / etc. scene built around it.</p><p> I think this requires genuine improvements in governance and design. I don&#39;t think someone declaring themselves god-Emperor and giving orders either works or is reasonable to expect. I think this has to happen in deep connection to the mainstream (like, I am imagining people rewriting health care systems and working in government agencies and at insurance companies and so on, and many of the people involved not having any sort of broader commitment to计划）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:21:24 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:21:24 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> There are two reasons I can immediately feel to have a Plan.</p><p> The first is: you need a Plan in order to just make sure when you&#39;ve finished the Plan you&#39;re &quot;done&quot; (where &quot;done&quot; might mean &quot;have advanced significantly&quot;) and to make sure you&#39;re prioritising.</p><p> The second is: it&#39;s an organising principle to allow people to feel like they are pulling together, to see some ways in which their work is accumulating and to have a flag for a group that can have positive regard for each of its members doing useful stuff 。</p><p> I feel pretty sold on the second! I&#39;m not so sure about the first. Happy to go more into that, but also pretty happy to take it as a given for awhile and allow the conversation to move past that question. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:23:11 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:23:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>Hmm I&#39;m not sure that distinction is landing for me yet. Like I think I mostly want the second--but in order for the second to be <i>real</i> the plan must also be <i>real</i> . (If I thought the SENS plan contained significant oversights or simplifications, for example, I would not expect it to be very useful for motivating useful effort.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:22:39 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:22:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> If I were to discuss something else, I would be pretty interested in questions like &quot;what does the plan need to cover?&quot;, &quot;what are some of the levers that are going tragically unpulled? or &quot;what does the superstructure need to be like to be sociologically/psychologically viable (or whatever other considerations)?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:24:03 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:24:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>Yeah happy to move to specifics. I think I don&#39;t have a complete Plan yet and so some of the specifics are fuzzy--I think I&#39;m also somewhat pessimistic about the Plan being constructed by a single person. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:25:09 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:25:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I guess the difference is I expect more things to help some than you do? If I believed SENS were missing lots of things, I could still imagine being excited to work on it as long as I believed the problems were fairly real, even if not complete. Admittedly, I would be a bit more predisposed to try piecemealing together a bunch of hacks and seeing where that took us. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:26:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:26:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Totally makes sense to be pessimistic about the Plan being constructed by a single person. But it seems that the Plan will be constructed by people like you and I doing some kind of mental motion, and I was wondering if maybe we should just do some of that now. Sort of analogous to how the hope is that people will do the pieces of object-level work that adds up to a solved polycrisis, it seems good if people do the pieces of meta-level work that adds up to a Plan. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:48:54 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:48:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>ok, so not attempting to be comprehensive:</p><ul><li> <strong>Energy abundance.</strong> One of the answers for &quot;wtf went wrong in 1970?&quot; is energy prices stopped going down and went up instead. Having cheaper energy is generically good. Progress here looks like 1) improvements in geothermal energy / transitioning oil drilling to geothermal drilling, 2) continued rollouts of solar, 3) increased nuclear production. People are currently pretty excited about permitting reform for geothermal production for a number of reasons and I would probably go into something like that if I were going to work in this field.</li><li> <strong>Land use.</strong> The dream here is land value taxes, but there are lots of other things that make sense too. You brought to my attention <a href="https://worksinprogress.co/issue/growing-the-growth-coalition/">the recent piece by Judge Glock</a> about how local governments used to be pro-growth because it determined their revenues and then various changes to the legal and regulatory environment stopped that from being true, giving a lot of support to anti-growth forces. Recent successes in California have looked more like the state government (which is more pro-growth than local governments) seizing control in a lot of areas, but you could imagine various other ways to go about this that are better designed. Another thing here that is potentially underrated is just being a property developer in Berkeley/SF; my understanding is that a lot of people working in the industry did not take advantage of Builder&#39;s Remedy because they&#39;re here for the long haul and don&#39;t want to burn bridges, but I have only done a casual investigation.</li><li> <strong>Labor retooling</strong> . When I worked at Indeed (the job search website company) ~7 years ago there were three main types of jobs people wanted to place ads for, one of which was truckers. And so Indeed was looking ahead to when self-driving trucks would eat a bunch of those jobs, both to try to figure out how to replace the revenue for the company and to try to figure out how to help that predictable flood of additional users find jobs that are good for them. I don&#39;t have a great sense of what works well here (people are excited about UBIs and I think they work at one half of the problem but leave the other half unaddressed). Now that I think we have economically relevant chatbots, I think this is happening (or on the horizon) for lots of jobs simultaneously.</li><li><strong>卫生保健</strong>。 The American system is a kludge that was patched together over a long time; satisfaction is low enough that I think there is potential to just redesign it from scratch. (See <a href="https://www.slowboring.com/p/a-bold-plan-to-fix-health-care">Amy Finkelstein&#39;s plan</a> , for example.)</li><li><strong>老化</strong>。 It would be nice to not die, and people having longer time horizons possibly makes them more attuned to the long-term consequences of their actions.</li><li> <strong>Political polarization</strong> . If you take a look at partisan support for opposite-party presidents in the US, it&#39;s declining in a pretty linear fashion, and projecting the lines forward it will not be that long until there is 0% Republican support for Democratic presidents (and vice versa ）。 This seems catastrophically bad if you were relying on the American government as part of your global-catastrophe-avoidance-plan. More broadly, I have a sense that the American system of representative-selection is poorly fit to the modern world and satisfaction is low enough that there&#39;s potential for reform.</li><li> <strong>Catastrophe avoidance</strong> . It seems like there should be some sort of global surveillance agency (either one agency or collaboration across Great Power lines or w/e) that is &#39;on top of things&#39; for biorisk and AI risk and so on. I&#39;m imagining a ~30 year pause in AI development, here, which likely requires active management.</li></ul><p> There are some things that maybe belong on this list and maybe don&#39;t? Like I think education is a thing that people perennially love to complain about but it&#39;s not actually obvious to me that it&#39;s in crisis to the degree that healthcare is in crisis, or that it won&#39;t be fixed on its own by independent offerings. (Like, Wikipedia and Khan Academy and all that are already out there; it would be nice for public schools to not be soul-destroying but I think I am more worried about &#39;the world&#39; being soul-destroying.) I think this list would be stronger if I had more clearly negative examples of &quot;yeah sorry we don&#39;t care about Catalan independence&quot; or w/e, but this seems like the sort of thing that is solved by a market mechanism (of no one buys into the prize for fixing Catalan independence, or no one decides to work on it). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:53:02 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:53:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>So one of the things that feels central to me is the question of &#39;design&#39; in the Christopher Alexander sense; having explicitly identified the constraints and finding a form that suits all of them.</p><p> I think the naive pro-growth view is &quot;vetocracy is terrible&quot; – when you have to get approval from more and more stakeholders to do projects, projects are that much harder to do, and eventually nothing gets done. But I think we need to take the view that &quot;just build it&quot; is the thesis, &quot;get approval&quot; is the antithesis, and the synthesis is something like &quot;stakeholder capitalism&quot; where getting stakeholder approval is actually just part of the process but is streamlined instead of obstructive.</p><p> Like, as population density increases, more people are negatively affected by projects, and so the taxes on projects should actually increase. But also more people should be positively affected by projects (more people can live in an 8-story apartment building than a 4-story one) and so on net this probably still balances out in favor of building. We just need to make the markets clear more easily, which I think involves looking carefully at what the market actually is and redesigning things accordingly. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:53:13 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:53:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> As well as negative examples, I wonder if it would be good to contend with the possibility of other &#39;royal solutions&#39; in the absence of AI. For example, human intelligence enhancement. My guess is that that isn&#39;t a solution, but it does possibly change the landscape so much that many other problems (for example ageing and energy abundance) become trivial. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 22:55:06 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 22:55:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>I think human intelligence enhancement definitely goes on the list. I think a large part of my &quot;genuine improvements in governance and design&quot; is something like &quot;intelligence enhancement outside of skulls&quot;--like if prediction markets aggregate opinions better than pundits writing opinion columns, a civilization with prediction markets is smarter than a civilization with newspapers. A civilization with caffeine is also probably smarter than a civilization with alcohol, but that&#39;s in a within-skull sort of way. Doing both of those seems great. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 22:55:39 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 22:55:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Designing the markets to clear more easily is quite appealing. But it also has some worrisome &#39;silver bullet&#39; feeling to it; a sense of impracticality or of my not having engaged enough with the details of current problems for this to be the right next step. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Wed, 29 Nov 2023 23:08:39 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Wed, 29 Nov 2023 23:08:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瓦尼弗</b></section><div><p>Yeah so one of my feelings here is also from Matt Yglesias on Slow Boring, called <a href="https://www.slowboring.com/p/big-ideas-arent-enough">Big ideas aren&#39;t enough</a> . Roughly speaking, his sense (as a Democratic detail-oriented policy wonk) is that the Republican policy wonks just really weren&#39;t delivering on the details, and so a lot of their efforts failed. It&#39;s one thing to say &quot;we need to have energy abundance&quot; and another thing to say &quot;ok, here&#39;s this specific permitting exemption that oil and gas projects have, if we extend that to geothermal projects it&#39;ll have these positive effects which outweigh those负面影响”。 It&#39;s one thing to have spent 5 minutes thinking about healthcare and guessing at a solution, and another thing to have carefully mapped out the real constraints  and why you believe they&#39;re real and find something that might actually be a Pareto improvement for all involved (or, if it&#39;s a Kaldor-Hicks improvement instead, figure out who needs to be bribed and how much it would take to bribe them).</p><p> I think it&#39;s more plausible that whatever Consortium of Concerned Citizens can identify the problem areas than that they can solve them--one of the things that I think broadly needs to change is a switch from &quot;people voting for solutions&quot; to &quot;people voting for prices for solutions&quot; that are then provided by a market. If you think increasing CO2 levels in the atmosphere is the problem, it really shouldn&#39;t concern you how CO2 levels are adjusted so long as they actually decrease, and you should let prices figure out whether that&#39;s replacement with solar or nuclear or continuing to burn fossil fuels while sequestering the carbon or whatever. [Of course this is assuming that you&#39;re pricing basically every externality well enough; you don&#39;t want the system to be perpetually sweeping the pollution under the next rug.] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Wed, 29 Nov 2023 23:10:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Wed, 29 Nov 2023 23:10:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> There&#39;s also a thing with solutions rather than prices for solutions where it&#39;s often easier to check that inputs are conforming than outputs.</p><p> A friend was trying to get fire insurance for their venue, and the fire insurers needed them to upgrade their fire alarm system. They asked the insurer &quot;how much more would it be not to upgrade the fire alarm system?&quot; and the answer was &quot;No. We do not offer insurance if you don&#39;t upgrade the system&quot;, presumably because the bespoke evaluation was too expensive.</p><p> [I don&#39;t quite know the import of this to what you wrote above, but it&#39;s a heuristic and anecdote that I have ping me when this sort of stuff comes up] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 07 Dec 2023 04:06:48 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 07 Dec 2023 04:06:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So, we wrapped there because of time constraints.感谢您的聊天。我很喜欢这个。 I would be interested in picking up again in the future.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/g3CdeLMYLgLYyN36J/on-plans-for-a-functional-society#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/g3CdeLMYLgLYyN36J/on-plans-for-a-functional-society<guid ispermalink="false"> g3CdeLMYLgLYyN36J</guid><dc:creator><![CDATA[kave]]></dc:creator><pubDate> Tue, 12 Dec 2023 00:07:46 GMT</pubDate> </item><item><title><![CDATA[Secondary Risk Markets]]></title><description><![CDATA[Published on December 11, 2023 9:52 PM GMT<br/><br/><p> <i>This idea is half-baked; it has some nice properties but doesn&#39;t seem to me like a solution to the problem I most care about. I&#39;m publishing it because maybe it points someone else towards a full solution, or solves a problem they care about, and out of a general sense that people should publish negative results.</i></p><p> Many risky activities impact not just the person doing the activity, but also bystanders or the public at large. Governments often require ability to compensate others as a precondition for engaging in the risky activity, with requirements to have car insurance to drive as a common example.</p><p> In most situations, this works out fine:</p><ol><li> A competitive insurance market means that customers aren&#39;t overcharged too much (since they&#39;ll switch insurance providers to whoever estimates their risk as being the lowest).</li><li> Accidents are common enough that insurance companies that are bad at pricing quickly lose too much money and adjust their prices upwards (so customers aren&#39;t undercharged either).</li><li> Accidents are small enough that insurance companies can easily absorb the losses from mispriced insurance.</li><li> Accidents are predictable enough that insurers can price premiums by driver moderately well.</li><li> Drivers are common enough that simple prediction rules make more sense than putting dedicated thought into how much to charge each driver.</li></ol><p> Suppose we adjust the parameters of the situation, and now instead of insuring drivers doing everyday trips, we&#39;re trying to insure rare, potentially catastrophic events, like <a href="https://en.wikipedia.org/wiki/Radioisotope_thermoelectric_generator">launching nuclear material into orbit to power deep space probes</a> . Now a launch failure potentially affects millions of people, and estimating the chance of failure is well worth more than a single formula&#39;s attention.</p><p> As a brief aside, why try to solve this with insurance? Why not just have regulators decide whether you can or can&#39;t do something? Basically, I believe that prices transmit information, and allow you to make globally correct decisions by only attending to local considerations. If the potential downside of something is a billion dollars, and you have a way to estimate micro-failures, you can price each micro-failure at a thousand dollars and answer whether or not mitigations are worth it (if it reduces the microfailures by 4 and costs $5,000, it&#39;s not worth it, but if it&#39;s 6 microfailures instead then it is worth it) and whether or not it&#39;s worth doing the whole project at all. It seems more flexible to have people codesign their launch with their insurer than with the regulator.</p><p> But the title of this post is <i>Secondary</i> Risk Markets. If there&#39;s a price on the risk that&#39;s allowed to float, then it&#39;s also more robust; if Geico disagrees with State Farm&#39;s estimates, then we want them to bet against each other and reach a consensus price, rather than the person doing the risky activity just choosing the lowest bidder. [That is, we&#39;d like this to be able to counteract the <a href="https://forum.effectivealtruism.org/topics/unilateralist-s-curse">Unilateralist&#39;s Curse</a> .]</p><p> For example, suppose Alice want to borrow a fragile thousand dollar camera to do a cool photoshoot, and there&#39;s some probability she ruins it. By default, this requires that she post $1,000, which she probably doesn&#39;t want to do on her own; instead she goes to Bob, who estimates her risk at 5%, and Carol, who estimates her risk at 9%. Alice offers to pay Bob $51 if he puts up the $1,000, with $1 in expected profit for Bob.</p><p> If Bob would say yes to that, Carol would want to take that bet too; she would like to give Bob $51 in exchange for $1,000 if Alice breaks the camera, since that&#39;s $39 in expected profit for Carol. And Bob, if actually betting with Carol, would want to set the price at something more like $70, since that equalizes the profit for the two of them, with the actual price depending on how quickly their price changes in the presence of counterparties.</p><p> So how do we get Alice to pay $70 instead of $51? <span class="footnote-reference" role="doc-noteref" id="fnref76lhwfgbcq9"><sup><a href="#fn76lhwfgbcq9">[1]</a></sup></span></p><p> My proposed scheme is as follows:</p><ul><li> The regulator estimates the potential damages of the activity at $X. <span class="footnote-reference" role="doc-noteref" id="fnrefsaurmnhlmd7"><sup><a href="#fnsaurmnhlmd7">[2]</a></sup></span></li><li> The person doing the activity has to post $2X as a bond. X is held in reserve to pay out to victims if the risk materializes and returned if the risk is avoided. The other X is posted for sale on a secondary market as &#39;synthetic risk&#39; according to some predefined schedule (ask offers from 0% to 100%, effectively, tho the poster can immediately buy as much as they like).</li><li> Anyone can post a bond to create more synthetic risk tied to the activity and sell it for whatever price they like.</li></ul><p> Continuing the example, Bob now needs to post $2k, with $1k set aside for the &#39;natural risk&#39; and the other $1k for sale. Bob immediately buys the $50 of synthetic risk that&#39;s available at prices of 5% or below, and now Carol would want to buy the $40 of synthetic risk available between 5% and 9%, which was listed for sale for $2.80. Note that Bob anticipates gaining 80 cents on that deal with Carol, if he sticks with his initial estimate; 5% of the time he has to pay out $40 which costs him $2, but Carol paid $2.80 for it. Bob can create more of that synthetic risk and put it for sale, for as long as he and Carol want additional exposure to that bet. [And Dan, if he also thought Carol was overestimating the risk, could create bonds and sell them to Carol as well.]</p><p> But this is how we get a market price of 7% risk, not how Alice pays $70. Either Bob could have set the initial insurance contract with Alice such that she pays the market price once it stabilizes, <span class="footnote-reference" role="doc-noteref" id="fnref455lzp17uyq"><sup><a href="#fn455lzp17uyq">[3]</a></sup></span> or Bob could have had Alice pay to post the synthetic risk bonds, or so on. From society&#39;s perspective, it doesn&#39;t matter too much whether Alice pays for the increased risk or Bob does, so long as <i>someone</i> enabling the action does.</p><p> Importantly, this system transfers from <i>worse predictors</i> to <i>better predictors</i> instead of from optimists to pessimists. If Alice in fact only breaks the camera in 5% of worlds, on expectation Carol will be transferring resources to Bob.</p><p> I think this is basically magnifying the consequences of rare events, both for good and for ill. More money riding on the derivatives of the events will sharpen society&#39;s estimate (and ensure there&#39;s a public record of it), and also cause insurers to &#39;update faster&#39;. Rather than simply $1k moving based on Bob&#39;s estimate of 5% risk, $1.4k will be moved if Bob and Carol end up with a $400 side bet. This also means that the standard laws against insurance fraud will be that much more strained by the pressures of profit; here I propose mandating as much synthetic risk as actual risk, but you could easily swap out the 2 in 2X for any other number, or the supply schedule for another schedule, or so on.  (Most of the listed offers will never be bought under this scheme, for example.) This makes risky business like this more capital-intensive, which is the main price being paid for the increased accuracy in estimates. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn76lhwfgbcq9"> <span class="footnote-back-link"><sup><strong><a href="#fnref76lhwfgbcq9">^</a></strong></sup></span><div class="footnote-content"><p> The overall hope, here, is to have Alice only do projects that make sense once the unilateralist&#39;s curse has been accounted for. If borrowing the camera is worth $80 to Alice, this project should still happen even tho it seems like a bad idea to Carol; if it&#39;s worth $60 to Alice, this project shouldn&#39;t happen even tho it seems like a good idea to Bob.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsaurmnhlmd7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsaurmnhlmd7">^</a></strong></sup></span><div class="footnote-content"><p> This could probably also be done with a market rather than by fiat, but I don&#39;t think that will materially change the analysis in this post.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn455lzp17uyq"> <span class="footnote-back-link"><sup><strong><a href="#fnref455lzp17uyq">^</a></strong></sup></span><div class="footnote-content"><p> Doesn&#39;t this take a lot of the value of this away from Alice, since she was hoping to de-risk this whole affair and now the risk is back? Some, since Alice now has to pay an unknown premium instead of a known own, but it&#39;s still substantially better than having to post the $1k herself. But also if we imagine this as being one-off events like rocket launches instead of much more common events like getting in the car, it seems much more plausible that Alice can call off the photoshoot if the premiums get too high.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/Q2gd5G6FxFR65pDFc/secondary-risk-markets#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Q2gd5G6FxFR65pDFc/secondary-risk-markets<guid ispermalink="false"> Q2gd5G6FxFR65pDFc</guid><dc:creator><![CDATA[Vaniver]]></dc:creator><pubDate> Mon, 11 Dec 2023 21:52:48 GMT</pubDate></item></channel></rss>