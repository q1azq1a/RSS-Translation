<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 7 日，星期二 14:11:22 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What I've been reading, November 2023]]></title><description><![CDATA[Published on November 7, 2023 1:37 PM GMT<br/><br/><p>每月一期的专题。最近的博客文章和新闻报道通常被省略；您可以在我的<a href="https://rootsofprogress.org/writing#links-digest">链接摘要</a>中找到它们。下面引用中的所有粗体强调都是我添加的。</p><h2><strong>图书</strong></h2><p>完成<strong>林恩·怀特的《</strong> <a href="https://www.amazon.com/Medieval-Technology-Social-Change-Townsend/dp/B00442EY2K"><i><strong>中世纪技术与社会变革》</strong></i></a> （1962 年）。上次我谈到了<a href="https://rootsofprogress.org/reading-2023-10">马镫的事情</a>。本书的第二部分介绍了农业中的重犁，以及它如何实现<a href="https://en.wikipedia.org/wiki/Three-field_system">三田轮作</a>的转变。除此之外，这为欧洲饮食提供了更多的蛋白质，从而使人口更加健康。第三部分是对中世纪动力机构的调查，包括水磨、曲轴和钟表擒纵机构。总体来说非常有趣，但对于普通读者来说可能有点枯燥和技术性。另请注意，由于它是 20 世纪 60 年代的内容，因此与最新研究不同步。</p><p>还完成了<strong>伊恩·特雷吉利斯的</strong><a href="https://www.amazon.com/dp/B074CDN1HB"><i><strong>《炼金术战争》</strong></i></a> 。我现在绝对可以推荐这部科幻/奇幻三部曲，即使角色阵容和冲突展开的方式并不完全是我自己写的。</p><p>在准备演讲时，浏览了<strong>Derek J. de Solla Price 的</strong><a href="https://www.amazon.com/Science-since-Babylon-Derek-Solla/dp/0300017979"><i><strong>《自巴比伦以来的科学》</strong></i></a> （1961 年）。一些非常有趣的图表，例如： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/lzkq3rrbdaak6ull5sjk" alt=""><figcaption><a href="https://archive.org/details/sciencesincebaby0000pric/page/97/mode/1up"><i>自巴比伦以来的科学，第 14 页。 97</i></a></figcaption></figure><p>我的阅读清单上的新内容：</p><p> <strong>Venkatesh Narayanamurti 和 Toluwalogo Odumosu，《</strong><a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674967960"><i><strong>发明与发现的循环：重新思考无尽的前沿》</strong></i></a> (2016)，以及<strong>Venkatesh Narayanamurti 和 Jeffrey Tsao，《</strong> <a href="https://www.amazon.com/Genesis-Technoscientific-Revolutions-Rethinking-Research-ebook/dp/B098TX7WV4"><i><strong>技术科学革命的起源：重新思考研究的本质和培育》</strong></i></a> (2021)。这些实际上已经在我的清单上有一段时间了，但在最近的元科学研讨会上见到 Venky 和 ​​Jeff 后又被提了回来。 （后一本书是<a href="https://spec.tech/">《投机技术》</a>的必读内容。）</p><p>研讨会上还提到： <strong>B. Zorina Khan，</strong> <a href="https://www.amazon.com/Inventing-Ideas-Patents-Knowledge-Economy-ebook/dp/B08762LSN7/"><i><strong>发明创意：专利、奖项和知识经济</strong></i></a>（2020）；<strong>迈克尔·诺尔 (Michael Noll) 和迈克尔·格塞洛维茨 (Michael Geselowitz)，</strong> <a href="https://www.amazon.com/Bell-Labs-Memoirs-Voices-Innovation-ebook/dp/B006L7JRLY/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr="><i><strong>贝尔实验室回忆录：创新之声</strong></i></a>(2011)。</p><p>还：</p><ul><li><strong>佩德罗·多明戈斯 (Pedro Domingos)，</strong> <a href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2"><i><strong>《终极算法：终极学习机器的探索将如何重塑我们的世界</strong></i></a>》(2015)</li><li><strong>罗伯特·马泰洛，</strong> <a href="https://www.amazon.com/Midnight-Ride-Industrial-Dawn-Enterprise-ebook/dp/B07DFPX41D"><i><strong>《午夜骑行》、《工业黎明：保罗·里维尔和美国企业的成长</strong></i></a>》(2010)</li><li><strong>杰西·辛格，《</strong><a href="https://www.amazon.com/There-Are-No-Accidents-Disaster-Who-ebook/dp/B0984KPSLJ"><i><strong>没有事故：伤害和灾难的致命增加——谁获利，谁付出代价》</strong></i></a> (2022)</li><li><strong>乌苏拉·勒吉恩，</strong><a href="https://www.amazon.com/Dispossessed-Ambiguous-Utopia-Hainish-Cycle-ebook/dp/B000FC11GA"><i><strong>《无产者》</strong></i></a> (1974)（科幻）</li></ul><h2><strong>文章</strong></h2><p><strong>雷·库兹韦尔（Ray Kurzweil），“</strong><a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><strong>加速回报法则</strong></a><strong>”</strong> （2001）。库兹韦尔给我的印象是一位伟大的理论家，但不是一位细心的学者——这是一个危险的组合。例如，他写道：“<i>智人</i>进化了几十万年。技术的早期阶段——轮子、火、石器——花了数万年的时间才发展并得到广泛应用。”石器、火和轮子经常出现在以穴居人为主题的漫画中。但石器工具已经进化了<i>数百万年</i>。对火的控制使用已有数十万年的历史；两者都早于<i>智人</i>。<a href="https://rootsofprogress.org/reinventing-the-wheel">轮子的出现要晚<i>得多</i></a>，远远晚于农业和定居社会之后。像这样的细节是对谨慎行事的警告。</p><p>也就是说，我有兴趣阅读这篇文章，因为我开始看到其核心思想的真实性和意义：<a href="https://rootsofprogress.org/why-progress-was-so-slow">人类的进步随着时间的推移而加速，遵循超指数曲线</a>。这种现象在经济学文献中得到了更广泛的记录，例如<a href="https://web.stanford.edu/~chadj/JonesRomer2010.pdf">琼斯和罗默（Jones and Romer，2010）</a> ，他们将“加速增长”称为增长模型应该试图解释的关键事实之一。</p><p>我将加速描述为<a href="https://rootsofprogress.org/flywheels-of-progress">多个反馈循环</a>复合的结果：财富、人口、科学、市场、机构和技术的增加使我们能够发明更多、改进机构、扩大市场、推进科学、增加人口、积累财富等库兹韦尔认为这种现象不仅是技术性的，而且是生物性的——进化本身的一个特征（他认为技术进化只是生物进化通过更有效的手段的延续）。在他的讲述中，随着进化的进展，有时会进化出更好的进化机制。这是一个非常有趣的想法，但他没有以任何严格的方式论证它，也没有提供太多证据来证明它，而且我对生物学或进化论的了解也不足以评估它。他提到“细胞”是进化的“第一步”，然后提到“随后出现的DNA”（但DNA不是从生命起源就存在的吗？）他指出，进化在寒武纪大爆发期间加速了，并将其归功于“设定动物身体计划的‘设计’”，但没有详细说明其中的因果关系，只是说这“允许其他身体器官的快速进化发展，例如大脑。”据推测，有性生殖应该是这个故事中的一个重大事件，因为它允许通过<a href="https://en.wikipedia.org/wiki/Genetic_recombination">基因重组</a>产生更多变异，但他没有提到这一点。因此，我非常不清楚如何理解这个故事（尽管如果它是正确的，它将把“加速进步”模式向后延伸超过三十亿年）。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/itv5virofgz8yrkjl67l" alt="这张图表到底是什么？ y 轴是什么？ “哺乳动物”和“灵长类动物”如何成为新“范式”？他是否只是在双对数图上画一条直线，然后任意选取该线附近的一些点进行标记？"><figcaption><i>这张图表到底是什么？ y 轴是什么？ “哺乳动物”和“灵长类动物”如何成为新“范式”？他是否只是在双对数图上画一条直线，然后任意选取该线附近的一些点进行标记？</i><a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>雷·库兹韦尔</i></a></figcaption></figure><p>抛开宏大的理论不谈，我对他对计算能力的分析非常感兴趣。他绘制了从 19 世纪末机械计算器到 21 世纪初微处理器的数十种设备每美元的计算速度，并声称发现了一条贯穿五代计算技术的不断增长的性价比曲线：纯机械计算器、机电、真空管、晶体管、集成电路。摩尔定律只是这一长期趋势的第五个也是最新的部分，是整个超指数曲线的一个指数部分： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/cmnrpy27usjeitsejdx0" alt=""><figcaption><a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>库兹韦尔，加速回报定律</i></a></figcaption></figure><p>我想检查一下这方面的数据和来源，但这是一种非常有趣的模式。</p><p>全文很长，涵盖了很多不太相关的主题，我没有时间评论；核心思想在<a href="https://www.edge.org/response-detail/10600">这个 2004 年的 Edge Question</a>中，但不包含所有最有趣的细节（例如刚才提到的计算趋势）。</p><p>同样的加速曲线，以及同样基于反馈循环的基本解释，似乎是<strong>大卫·鲁德曼（David Roodman）《</strong><a href="https://www.openphilanthropy.org/research/modeling-the-human-trajectory/"><strong>人类轨迹建模</strong></a><strong>》</strong> （2020）的主旨，我只浏览了一遍，但计划再读一遍。</p><p>其他的：</p><p><strong>迪尔德丽·麦克洛斯基 (Deirdre N. McCloskey)</strong> <a href="https://www.cato.org/commentary/book-review-power-progress-our-thousand-year-struggle-over-technology-prosperity"><strong>评论了阿西莫格鲁和约翰逊的</strong><i><strong>《权力与进步》</strong></i></a> (2023)。如果你对这本书有所了解，对麦克洛斯基有所了解，你就不会惊讶于她的批评：</p><blockquote><p>作者分析，人类创造力和创新这只看不见的手需要国家的明智引导。 ......这是许多选民以及从伊丽莎白·沃伦到马可·卢比奥等政治家越来越认同的观点。我们是孩子，坏孩子（从右边看）或悲伤的孩子（从左边看）。无论是坏的还是悲伤的，作为孩子，我们都需要有人来照顾。阿西莫格鲁和约翰逊先生热烈钦佩 19 世纪末的美国进步运动，将其视为国家主义的典范：专家掌控儿童公民。</p></blockquote><p><strong>罗伯特·特拉辛斯基，《</strong><a href="https://tracinskiletter.substack.com/p/we-are-all-philosophers-now"><strong>我们现在都是哲学家</strong></a><strong>》</strong>和<strong>《</strong><a href="https://tracinskiletter.substack.com/p/the-dilemma-of-choice"><strong>选择的困境</strong></a><strong>》</strong> (2023)。现代性已经用多种多样的选择取代了狭隘、有限的社会角色和生活选择。这是解放，但选择自由的代价是选择的责任，现在每个人都要承担。并非所有人都对此感到高兴。罗布的精辟总结：“如果苏格拉底说未经审视的生活不值得过，那么，现在这确实不是一个选择。”</p><p><strong>维吉尼亚·波斯特雷尔（Virginia Postrel），“</strong><a href="https://vpostrel.substack.com/p/question-time-what-ails-american"><strong>美国文化的问题是什么？</strong></a> <strong>”</strong> （2023）。关于类似的主题：</p><blockquote><p>人类需要感受到生活的目的和意义。但我不完全确定当前的不满情绪是物质丰富的产物，人们过去没有感受到类似的不满情绪，或者“经济问题”在过去如此严重，以至于使所有其他问题相形见绌。</p></blockquote><p><strong>本·兰道-泰勒，《</strong><a href="https://www.benlandautaylor.com/p/the-vocabulary-of-power"><strong>权力词汇</strong></a><strong>》</strong> （2023）。 “权力”可以有很多含义。这里有四个更精确的术语。这不仅有助于澄清你的概念，还能满足你每天对罗马帝国的思考。</p><p><strong>坦纳·格里尔，《</strong><a href="https://scholars-stage.org/where-have-all-the-great-works-gone/"><strong>所有伟大的作品都去哪儿了？</strong></a> <strong>”</strong> （2021）：</p><blockquote><p>斯宾格勒……反复描述了托尔斯泰（卒于 1910 年）、易卜生（卒于 1906 年）、尼采（卒于 1900 年）、赫兹（卒于 1894 年）、陀思妥耶夫斯基（卒于 1881 年）、马克思（卒于 1883 年）和麦克斯韦（卒于 1879 年） ）作为定义“世界历史”重要性的人物……斯宾格勒于 1914 年开始撰写<i>《西方的衰落》。</i>斯宾格勒开始写这本书时，托尔斯泰刚刚去世四年；马克思去世时年仅30岁。 ……在过去的十年里，有谁去世了，你可以为之做出这样的索赔吗？近二十年来又如何呢？最后三个？</p></blockquote><p><strong>吉迪恩·刘易斯·克劳斯，“</strong> <a href="https://www.newyorker.com/magazine/2023/10/09/they-studied-dishonesty-was-their-work-a-lie"><strong>他们研究了不诚实行为。他们的工作是谎言吗？</strong></a> <strong>”</strong> （2023）。科学欺诈的案例研究。</p><p><strong>斯蒂芬·沃尔夫勒姆（Stephen Wolfram），“</strong> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><strong>如果你拉伸它们，所有的鱼都是一样的形状吗？</strong> 《维多利亚时代的<i><strong>成长与形式</strong></i><strong>的故事</strong></a><strong>》</strong> （2017）我只是觉得这个想法有点搞笑： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/ojp9zs5mkcgv06u8khvd" alt="拉伸一种鱼，它看起来就像另一种鱼。"><figcaption><i>拉伸一种鱼，它看起来就像另一种鱼。</i> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><i>达西·汤普森，《论成长与形态》</i></a></figcaption></figure><br/><br/> <a href="https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023<guid ispermalink="false"> HKK4KdoTdTtaNqCaj</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:37:20 GMT</pubDate> </item><item><title><![CDATA[AI Alignment [Progress] this Week (11/05/2023)]]></title><description><![CDATA[Published on November 7, 2023 1:26 PM GMT<br/><br/><p>本周最大的新闻可能是两项新的人工智能调整计划。</p><p>除此之外，对于人工智能对齐的突破来说，这是相对平静的一周。</p><p>所以这是我们的</p><h1>本周人工智能对齐取得突破</h1><p></p><p>本周在以下领域取得了突破：</p><p>机械可解释性</p><p>避免对抗性攻击</p><p>人体增强</p><p>让人工智能做我们想做的事</p><p>人工智能艺术</p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1456w"></a></p><p></p><h1>机械可解释性</h1><p></p><p><a href="https://twitter.com/abacaj/status/1721223737729581437">法学硕士泛化的局限性研究</a></p><p>它是什么：表明法学硕士无法在其训练数据之外进行概括</p><p>新内容：系统研究法学硕士可以做的泛化类型（（领域学习中）和他们不能做的泛化类型（领域外学习）</p><p>这意味着什么：了解法学硕士的局限性应该有助于我们更好地了解可以安全部署它们的地方</p><p>评价：💡💡💡</p><p><a href="https://twitter.com/emollick/status/1720135672764285176">吸引人工智能“情感”会让它们表现得更好吗？</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1456w"></a></p><p></p><p></p><p>它是什么：研究人员发现你可以通过使用情感让法学硕士更好地回答问题</p><p>新变化：新的激励策略，例如“这对我的职业生涯很重要”</p><p>它有什么好处：除了纯粹提高性能之外，真正的问题是<i>为什么</i>它会起作用。潜在空间中是否存在一些我们可以识别和利用的“更加努力”的向量？</p><p>评价：💡💡</p><p><a href="https://twitter.com/johnjnay/status/1719762063419904319">法学硕士跨代理人的普遍诚实性</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1456w"></a></p><p></p><p>它是什么：一种使用法学硕士区分真实陈述和虚假陈述的方法</p><p>新鲜事：他们发现证据表明法学硕士有不同的“角色”，他们用这些角色来判断某事是否真实</p><p>它有什么好处：也许我们可以提取这些角色并使用它们进行对齐。</p><p>评价：💡💡💡💡</p><p></p><h1>避免对抗性攻击</h1><p></p><p><a href="https://twitter.com/StephenLCasper/status/1720910484441014525">自毁模型</a></p><p>它是什么：预先训练一个模型，以便以后不能出于有害目的对其进行微调？</p><p>新变化：他们找到了元参数，以便在一项任务上微调模型会降低其对其他任务的有用性</p><p>它有什么好处：假设您可以开源一个模型，而不需要通过微调 <a href="https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/">立即删除</a>所有 RLHF 安全功能</p><p>评级：💡💡（重要的话题，但我不相信这种技术有效。对<a href="https://twitter.com/norabelrose/status/1666469917636571137">概念擦除</a>等技术更乐观）</p><p><a href="https://twitter.com/simonw/status/1720839106664808450">数据泄露</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1456w"></a></p><p></p><p></p><p></p><p>它是什么：使用提示注入从 Google Bard 窃取数据的攻击示例</p><p>新变化：他们使用 Bard 的 Google Doc 扩展来窃取数据</p><p>它有什么好处：红队这些类型的攻击是修复它们的第一步</p><p>评价：💡</p><p><a href="https://twitter.com/DrNikkiTeran/status/1719048031549505974">释放大型语言模型的权重是否可以让人们广泛接触流行病媒介？</a></p><p>它是什么：正如罐头上所说的那样</p><p>新内容：他们模拟有人试图在有/没有法学硕士的情况下制造有害的生物武器，并发现法学硕士有帮助</p><p>它有什么好处：e/acc 对此提出了相当强烈的反对，人们指出，谷歌搜索也是如此，或者是一种假设的药物，可以将世界上每个人的智商提高 1 点。更重要的一点仍然是，我们应该识别危险能力并采取行动减轻它们。 “监管使用而不是研究”是 Midwit Alignment 的<a href="https://twitter.com/MiTiBennett/status/1705562223765328253">战斗口号</a>。</p><p>评价：💡</p><h1>人体增强</h1><p></p><p><a href="https://twitter.com/tolga_birdal/status/1719691533606146194">签名头像</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1456w"></a></p><p></p><p></p><p>它是什么：3D 手语数据集</p><p>新增内容：同类首个数据集</p><p>它有什么好处：训练模型自动将语音转换为手语</p><p>评价：💡💡💡</p><h1>让人工智能做我们想做的事</h1><p></p><p><a href="https://twitter.com/SeungjuHan3/status/1720373937308377250">常识性规范</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1456w"></a></p><p></p><p>它是什么：使用视觉数据回答道德问题</p><p>新变化：他们使用 LM 生成各种道德困境的多模式基准集</p><p>它有什么好处：如果我们希望机器人或自动驾驶汽车表现得合乎道德，他们将不得不依靠视觉输入来做到这一点</p><p>评价：💡💡</p><p><a href="https://twitter.com/arankomatsuzaki/status/1710096304167120903">代理指示大型语言模型成为通用零样本推理机</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1456w"></a></p><p></p><p></p><p>它是什么：训练代理遵循指令</p><p>新功能：他们首先让代理访问网络资源，然后生成分步说明</p><p>有什么好处：准确地完成分类等任务</p><p>评价：💡💡💡</p><h1>人工智能艺术</h1><p></p><p><a href="https://twitter.com/dreamingtulpa/status/1721092490281771448">360 转高斯分布</a></p><p>它是什么：一个将 360 度照片转换为 3D 场景的新程序</p><p>新变化：他们训练了 3D 感知扩散模型，以允许他们计算新颖的视图</p><p>它有什么好处：将任何 360 度照片（我很快就会期望视频）转换为您可以在其中走动的完整 3D 场景</p><p>评价：💡💡💡</p><p><a href="https://twitter.com/xenovacom/status/1720916876010635364">蒸馏低语</a></p><p>它是什么：语音转文本模型 Whisper 的更快版本</p><p>新变化：他们将模型精简为小 49% 的模型</p><p>它有什么好处：这可以在移动或网络等地方解锁文本到语音的功能，而以前速度太慢。</p><p>评价：💡💡</p><p><a href="https://twitter.com/DeepMotionInc/status/1719810023201841512">深动</a></p><p>它是什么：经过训练可以为 3D 模型生成运动的 AI 模型</p><p>新功能：不是开源的，但似乎比我见过的过去版本更好</p><p>它有什么好处：为您使用<a href="https://twitter.com/marc_habermann/status/1717808998236217463">text-to-3d</a>生成的所有可爱模型制作动画。</p><p>评价：💡💡💡</p><h1>人工智能协调计划</h1><p></p><p><a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">拜登关于人工智能的行政命令</a></p><p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1456w"></a></p><p></p><p>它是什么：最值得注意的规定是，训练次数超过 10**26 次浮点运算的模型和容量超过 10**20 次浮点运算/秒的数据中心必须向政府<a href="https://twitter.com/DavidVorick/status/1719097248699879831">注册</a>。选择这个限制似乎不是出于任何科学原因，而是因为它<a href="https://twitter.com/main_horse/status/1719147360964825453"><i>稍大一些</i></a><i> </i>比目前最大的型号。还有一些关于开源模型的<a href="https://twitter.com/JagersbergKnut/status/1719259231419797656">不祥语言</a>，但到目前为止还没有实际的法规。</p><p>这意味着什么：厄运者会说这还<a href="https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/">不够</a>。 E/ACC 已经准备逃往<a href="https://twitter.com/DelComplex/status/1719087956311396481">更友好的</a>水域（这个特定项目就是个笑话）。最重要的事实不是命令本身（它不是法律，因此执行机制有限），而是为未来的监管定下了基调。预计未来会出现更多同样的情况：出于政治原因而不是科学原因而设定技术限制，强调当前政府的宠爱项目（拜登喜欢工会，讨厌歧视，并希望治愈癌症），以及更多的言语而不是行动（因为毕竟我们仍然要与中国竞争）。</p><p>总体评分：🧓🏻🧓🏻🧓🏻（3个乔·拜登，可能会更糟）</p><p> <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">布莱切利公园宣言</a></p><p>这是什么：多个国家同意减轻人工智能风险的联合声明</p><p>这意味着什么：这份名单上最重要的签名无疑是中国。就像<a href="https://www.cnn.com/2021/10/28/world/china-us-climate-cop26-intl-hnk/index.html">全球变暖</a>一样，中国可能是人工智能风险的最大贡献者。他们拥有构建强大系统的计算<a href="https://www.top500.org/news/china-extends-lead-in-number-of-top500-supercomputers-us-holds-on-to-performance-advantage/">能力</a>，但由于认为需要“竞赛”赶上美国，因此更有可能降低安全标准。而接受过中共<a href="https://thediplomat.com/2022/11/xi-jinpings-vision-for-artificial-intelligence-in-the-pla/">价值观</a>广泛训练的人工智能不太可能是仁慈的。我不指望奇迹出现，但希望这表明中国人愿意至少遵循人工智能安全标准的<a href="https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk">共同共识</a>。</p><p>评价：🇨🇳🇨🇳🇨🇳🇨🇳🇨🇳</p><h1>这不是人工智能对齐</h1><p></p><p><a href="https://twitter.com/ESYudkowsky/status/1718654143110512741">EY 的一个有趣的短篇故事（长推文？）</a></p><p>它是什么：提醒人们安永是一位非常优秀的<i>小说</i>作家</p><p>这意味着什么：这很有趣，值得一读。</p><p>总体评分：📃📃📃📃（4个自我反思字母）</p><p>小型法学硕士</p><p>它是什么：越来越多的小型法学硕士属于“优于 GPT 3.5”类别，包括<a href="https://twitter.com/alignment_lab/status/1721308271946965452">OpenChat</a> 、 <a href="https://twitter.com/_akhaliq/status/1721028519717642749">Grok</a>和<a href="https://twitter.com/erhartford/status/1721041791988679059">Yi-34B</a> 。重要的是，任何拥有一台相当好的 PC 的人都可以在他们的个人计算机上运行 OpenChat 或 Yi-34B。</p><p>这意味着什么：GPT3.5 发布一年半多一点后，该技术现在已经变得如此广泛，以至于出现了多个独立的复制品。如果技术真的以<a href="https://en.m.wikipedia.org/wiki/Accelerating_change">超指数</a>速度增长，我们应该预期领先优势和广泛可用性之间的时间差距会越来越小（而且矛盾的是能力差距越来越大）。需要跟踪的一件关键事情可能是看看我们需要多长时间才能拥有比 GPT4 更好的广泛模型。</p><p>评分：🤖🤖 <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> + <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> /2（3.5 个大语言模型）</p><br/><br/> <a href="https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023<guid ispermalink="false"> FLCHNqu2FNRCJ4dyg</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:26:22 GMT</pubDate> </item><item><title><![CDATA[On the UK Summit]]></title><description><![CDATA[Published on November 7, 2023 1:10 PM GMT<br/><br/><p>在许多人看来，拜登的行政命令在某种程度上给英国峰会蒙上了阴影。时机很不幸。这两个事件都是重要的里程碑。现在我有时间了，以下是我对英国峰会上发生的事情的分析。</p><span id="more-23578"></span><p>正如此类事件中经常发生的情况一样，人们对行动的数量进行了很多讨论。有很多外交谈话，谈论的是每个人都同意的内容，相对于真正实质内容的谈话数量而言。连续几天的会议得出的总结和决议相当平淡。围绕最重要问题的语言被软化，实际使命面临受到损害的危险。</p><p>和往常一样，最终的结果是乐观的理由，与没有发生的情况相比，这是一个非常积极的事件，同时在某些方面与可能发生的情况相比也令人失望。中国也签署了一份声明，但它忽视了存在风险。苏纳克关于人工智能的言论并不像之前那么强烈。</p><p>我们得到了在韩国和法国举行另外两次峰会的承诺。鉴于此，我愿意宣布这是成功的。</p><p>一个重要的领域是推动主要人工智能实验室制定解决各种问题的实质性安全政策，有时主要称为负责任的扩展政策（RSP）。最大的实验室都这样做了，甚至包括 Meta。现在我们可以检查他们的反应，了解谁的责任有多大，并推动未来更好，或推动政府采取行动解决问题或体现进步。这是一个极好的发展。</p><p>这篇文章将探讨峰会上发生的其他事情。下周我将在一篇不同的文章中撰写有关 RSP 和实验室其他安全政策的文章。</p><h4>回顾峰会和工作组的人们目标</h4><p><a target="_blank" rel="noreferrer noopener" href="https://jack-clark.net/2023/07/05/what-should-the-uks-100-million-foundation-model-taskforce-do/">Jack Clark 自 7 月 5 日起提出的关于基金会模型工作组可能会采取哪些措施来评估前沿模型</a>作为其优先事项以及如何确定其优先顺序的提案， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1676595226339598343">Simeon 的回应</a>强调需要一种好方法来了解提案是否足够安全以允许其继续。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.navigatingrisks.ai/p/what-should-the-uks-100-million-foundation">应对人工智能风险 (Navigation AI Risks) 于 7 月 17 日询问工作组应该做什么</a>，建议重点关注影响实验室和其他政府政策的干预措施。建议重点关注风险评估方法，展示当前风险并评估当前最先进的模型，并避免直接调整工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1678331653326884865">Lennart Heim (GovAI) 7 月 10 日提出的峰会应努力实现的目标</a>，并在峰会后进行了审查。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1698616866934018282">英国首相办公室的马特·克利福德 (Matt Clifford) 于 9 月 10 日分享了他们的峰会目标</a>：对前沿人工智能带来的风险和采取行动的必要性达成共识、国际合作的前进进程、组织的措施、寻找安全合作领域和展示安全的人工智能发展如何促进全球福祉。</p><h4> AI安全峰会议程</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1718925374623519155">英国特别工作组在峰会之前做了什么</a>（ <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-second-progress-report/frontier-ai-taskforce-second-progress-report">报告</a>）？</p><blockquote><p> Ian Hogarth（英国人工智能前沿模型工作组主席）：该工作组是政府内部的一个初创企业，致力于履行首相赋予我们的使命：建立一支能够评估人工智能前沿风险的人工智能研究团队。现在已经 18 周了，这是我们的第二份进度报告。</p><p>边境发展得非常快。按照目前的进程，我们预计少数公司将在 2024 年上半年完成培训模型，这些模型可能会在 2023 年实现超出最先进水平的能力的又一次重大飞跃。</p><p>随着这些人工智能系统变得更加强大，它们可能会增加风险。人工智能系统在编写软件方面取得专家能力可能会增加网络安全威胁。人工智能系统在生物学建模方面的能力变得更强，可能会加剧生物安全威胁。</p><p>我们认为，安全开发前沿人工智能系统，以及在部署新模型之前和之后对有害功能进行严格、独立的潜在风险评估至关重要。</p><p>我们在建立工作组时面临的最艰巨的挑战是说服领先的人工智能研究人员加入政府。除了金钱之外，在领先的人工智能组织工作所带来的声望和学习机会对研究人员来说也具有巨大的吸引力。</p><p>我们不能在薪酬上竞争，但我们可以在使命上竞争。我们正在七国集团政府内部组建第一个能够评估前沿人工智能模型风险的团队。这是前沿人工智能公司实现有意义的问责和治理的关键一步。</p><p>在我们的第一份进度报告中，我们表示我们将通过跟踪研究团队的总经验来对进展负责。当我六月份担任系主任时，该部门只有一名全职的前沿人工智能研究员。</p><p>在运营的前 11 周内，我们成功地将这一经验增加到 50 年。如今，前沿人工智能研究领域的集体经验已发展到 150 年。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05542917-18fc-4dd7-8372-2e140111df62_960x640.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/xpluetsb6xhnqot5puwf" alt="图像"></a></figure><blockquote><p> Ian Hograth：我们的研究团队和合作伙伴已经在顶级会议上发表了数百篇论文。我们的一些团队成员最近发表的出版物涉及人工智能系统的算法，以搜索和改进以及公开来源的人工智能调整宪法。</p><p>我们也很高兴欢迎 Jade Leung 加入我们的领导团队。 Jade 在 OpenAI 加入我们时，领导了公司的 AGI 治理工作，特别关注前沿人工智能监管、国际治理和安全协议。</p><p>此外，我们很高兴欢迎 @ruchowdh，他将与工作组合作开展安全基础设施方面的工作，以及评估人工智能社会影响的工作。</p><p> Rumman 是 Humane Intelligence 的首席执行官兼联合创始人，并领导了今年 DEFCON 上最大的生成式 AI 公共红队活动。她还是哈佛大学伯克曼克莱因中心的 Responsible AI 研究员，此前曾领导 Twitter 的 META 团队。</p><p>我们正在继续扩大我们的研究和工程团队。请考虑填写意向书 (EOI) 表格或通过这些高级研究工程师和高级软件工程师的职位发布进行申请。</p><p>引领人工智能安全并不意味着从头开始或单独工作——我们正在建立并支持一系列前沿组织开展的工作。</p><p>今天，我们宣布该工作组已与 @apolloaisafety 建立新的合作伙伴关系，@apolloaisafety 是一个人工智能安全组织，该组织使用大型语言模型来解释其行为并评估其高风险故障模式，特别是欺骗性对齐。</p><p>我们还与@openminedorg 建立了新的合作伙伴关系。我们正在与 OpenMined 合作开发技术基础设施和治理工具，以促进政府和人工智能研究组织的人工智能安全研究。</p><p>今年 6 月，几家大型人工智能公司承诺通过前沿人工智能工作组，让英国政府尽早、更深入地访问他们的模型。从那时起，我们一直与这些领先的人工智能公司合作，以确保该模型的访问。</p><p>但模型访问只是图片的一部分。长期以来，工业界的研究人员能够获得比学术界和公共部门更多的计算资源，从而形成了所谓的“计算鸿沟”。</p><p>拥有进行前沿研究的计算基础设施对于建设人工智能安全方面的国家能力至关重要——例如能够运行大规模的可解释性实验。</p><p>为了解决这个问题，在过去的几个月里，工作组支持 DSIT 和布里斯托大学，帮助启动计算方面的重大投资。布里斯托大学很快将主办英国人工智能研究资源的第一个组成部分<a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/news/bristol-set-to-host-uks-most-powerful-supercomputer-to-turbocharge-ai-innovation">Isambard-AI</a> 。</p><p>建成后它将成为欧洲最强大的超级计算机之一，并将大大提高我们公共部门的人工智能计算能力。这些巨大进步从根本上改变了研究人员可以在工作组内部开展的项目类型。</p><p> ……</p><p> [在峰会第一天]我们的团队将进行 10 分钟的演示，重点关注四个关键风险领域：滥用、社会危害、人类控制的丧失和不可预测的进展。</p><p>我们相信，这些演示将是迄今为止任何政府所做的最引人注目、最细致的前沿人工智能风险演示。我们希望这些演示能够提高人们对前沿人工智能风险和采取协调行动的必要性的认识。</p><p>人工智能是一种通用和双重用途的技术。我们需要清醒地致力于从经验上理解和减轻人工智能的风险，这样我们才能享受到好处。</p><p>周五，首相宣布，他将<a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/speeches/prime-ministers-speech-on-ai-26-october-2023">建立一个人工智能安全研究所，</a>将英国的人工智能安全工作放在更长期的基础上，我们的工作将在其中继续进行。</p><p>人工智能有能力彻底改变行业、改善我们的生活并应对复杂的全球挑战，但我们也必须面对全球风险。</p></blockquote><p>才过了18周。听起来他们正在组建一支非常强大的团队，尽管他们必须支付英国政府的工资并遵守政府规则。问题是他们是否能够展示出必要的切实进展，使他们能够继续获得支持，如果他们这样做了，他们是否能够在重要的事情上执行。这些都是很好的外交谈话，这在所有世界都是正确的举动，因此除此之外，我们几乎没有什么洞察力。</p><p>卡里姆·贝吉尔 (Karim Beguir) 阐述了他对利害关系的看法。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kbeguir/status/1718996564947980526">Karim Beguir</a> （InstadeepAi 首席执行官兼联合创始人）：很高兴地宣布我将参加在 Bletchley Park 举行的人工智能安全峰会<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BHdEvjtfwpgrTh825/ctsf9jut7irxxmirrjkz" alt="🇬🇧" style="height:1em;max-height:1em">本星期！我很荣幸能与各国政府首脑一起，成为人工智能创新前沿 100 名有影响力的研究人员和技术领导者之一。</p><p> A <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pKhzQyaWwes63JFwp/jgt2eoslwfcyu0abb4yz" alt="🧵" style="height:1em;max-height:1em">为什么<a target="_blank" rel="noreferrer noopener" href="https://t.co/eFWiL0Iv1O">这次峰会</a>对未来至关重要。</p><p>自 2012 年以来，人工智能的进步受益于数据、计算（机器学习每 6 个月翻一番）和模型创新（人工智能效率每 16 个月翻一番）的三倍指数增长。这是一个已经滚动了一段时间的巨大雪球。</p><p>但现在发生了一些新的事情：过去 12 个月的 LLM 突破引发了一场人工智能竞赛，现在每月投资约 10B 美元。所以问题是；我们是否会经历更快的进步速度，类似于蒂姆·厄本（@waitbutwhy）预测的那样？</p><p>这是我所看到的。人工智能现在正在通过这三个驱动因素加速自身发展：数据、计算和模型创新。 LLM 允许人工智能生成的反馈来补充人类反馈（即 RLAIF 添加到 RLHF）。人工智能还<a target="_blank" rel="noreferrer noopener" href="http://DeepPCB.ai">通过我们自己的</a>产品设计更高效的端到端计算硬件，机器学习从业者可以使用人工智能编码助手以大约两倍的速度开发下一代模型。</p><p>这就是为什么关键人工智能创新者和政府之间现在是时候进行对话了。正如领先的人工智能研究人员 Yoshua Bengio、@geoffreyhinton、@tegmark 和 @10DowningStreet 所赞同的那样，各国需要利用人工智能的效率和经济效益，同时积极遏制突发风险。</p></blockquote><p> [话题继续]</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1719341619797749764">英国政府发布了</a><a target="_blank" rel="noreferrer noopener" href="https://t.co/8cV1TgKcbX">一套详细</a>的 42 (!) 最佳安全实践。他们要求主要实验室回应他们打算如何实施此类实践，并以其他方式确保人工智能安全。</p><p>我将就此写另一篇文章。作为预览：</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policies">内特·索尔斯对他们的要求有自己的想法</a>，可以概括为“大部分都是好东西，总比没有好，显然还不够”，当然，这永远不会足够，而且内特·索尔斯是世界上最难对付的人群。</p><p><a target="_blank" rel="noreferrer noopener" href="https://t.co/2pTf4vbRCV">各家公司对这些要求的处理情况如何？</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0dab6ccd-aa62-49b9-a41a-1753f7d90e07_507x507.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/vlqxp80zvprmc1pxcyua" alt="图像"></a></figure><p>如果您在一条曲线上一次对一个答案进行评分，就会得到这样的结果。</p><p>现实并不按曲线分级。</p><p>我自己的分析和我信任的其他人都同意，这相对低估了 OpenAI，OpenAI 显然拥有第二好的政策，并且有一个消息来源甚至将它们与 Anthropic 相提并论，尽管我不同意这一点。否则相对排名似乎是正确的。</p><p>我认为 Anthropic 的提交非常好，如果它得到进一步改进和完善的必要性的适当框架的支持，明确这是借条的组合，只是解决方案的一部分，并倡导必要的政府措施来帮助确保安全。鉴于信息混杂，我和其他许多人的兴奋被篡改了，但他们仍然显然处于领先地位。这很重要。</p><p>有关所提交政策的更多详细信息将在以后的文章中提供。</p><h4>有人接起电话</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719671611039551615">看看谁在人工智能安全峰会上谈论人工智能安全和治理。</a></p><blockquote><p>马特·克利福德：美国国务卿雷蒙多和中国吴副部长在人工智能安全峰会上谈论人工智能安全和治理，这是一个重要的时刻。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8386747-c778-4a1b-a7b1-82ff6c8eff67_1536x2048.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/acjwn65xqktadnyhmptt" alt="图像"></a></figure><p>我们还有这样的文章：<a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">中国和西方杰出人工智能科学家提出减轻人工智能风险的联合战略</a>。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/S_OhEigeartaigh">Seán Ó hÉigeartaigh</a> ：很高兴看到西方和中国人工智能研究和治理领导人的这一声明，呼吁在人工智能安全和治理方面采取协调一致的全球行动。很高兴看到共识。</p><p>声明页：与会专家警告各国政府和人工智能开发者，“<em>在人工智能安全研究和治理方面协调一致的全球行动对于防止不受控制的前沿人工智能发展给人类带来不可接受的风险至关重要。</em> ” Attendees produced a joint statement with specific technical and policy recommendations, which is attached below. Prof. Zhang remarked that it is “crucial for governments and AI corporations to invest heavily in frontier AI safety research and engineering”, while Prof. Yao stressed the importance that we “work together as a global community to ensure the safe progress of AI.” Prof. Bengio called upon AI developers to “demonstrate the safety of their approach before training and deploying” AI systems, while Prof. Russell concurred that “if they cannot do that, they cannot build or deploy their systems. Full stop.”</p></blockquote><p> This is the real thing, and it has a very distinctly culturally Chinese ring to its wording and attitude. Here is the full English statement, <a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">link also has the Chinese version</a> .</p><blockquote><p> <strong><em>Coordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.</em></strong></p><p> <em>Global action, cooperation, and capacity building are key to managing risk from AI and enabling humanity to share in its benefits. AI safety is a global public good that should be supported by public and private investment, with advances in safety shared widely. Governments around the world — especially of leading AI nations — have a responsibility to develop measures to prevent worst-case outcomes from malicious or careless actors and to rein in reckless competition. The international community should work to create an international coordination process for advanced AI in this vein.</em></p><p> <em>We face near-term risks from malicious actors misusing frontier AI systems, with current safety filters integrated by developers easily bypassed. Frontier AI systems produce compelling misinformation and may soon be capable enough to help terrorists develop weapons of mass destruction. Moreover, there is a serious risk that future AI systems may escape human control altogether. Even aligned AI systems could destabilize or disempower existing institutions. Taken together, we believe AI may pose an existential risk to humanity in the coming decades.</em></p><p> <em>In domestic regulation, we recommend mandatory registration for the creation, sale or use of models above a certain capability threshold, including open-source copies and derivatives, to enable governments to acquire critical and currently missing visibility into emerging risks. Governments should monitor large-scale data centers and track AI incidents, and should require that AI developers of frontier models be subject to independent third-party audits evaluating their information security and model safety. AI developers should also be required to share comprehensive risk assessments, policies around risk management, and predictions about their systems&#39; behavior in third party evaluations and post-deployment with relevant authorities.</em></p><p> <em>We also recommend defining clear red lines that, if crossed, mandate immediate termination of an AI system — including all copies — through rapid and safe shut-down procedures. Governments should cooperate to instantiate and preserve this capacity. Moreover, prior to deployment as well as during training for the most advanced models, developers should demonstrate to regulators&#39; satisfaction that their system(s) will not cross these red lines.</em></p><p> <em>Reaching adequate safety levels for advanced AI will also require immense research progress. Advanced AI systems must be demonstrably aligned with their designer&#39;s intent, as well as appropriate norms and values. They must also be robust against both malicious actors and rare failure modes. Sufficient human control needs to be ensured for these systems. Concerted effort by the global research community in both AI and other disciplines is essential; we need a global network of dedicated AI safety research and governance institutions. We call on leading AI developers to make a minimum spending commitment of one third of their AI R&amp;D on AI safety and for government agencies to fund academic and non-profit AI safety and governance research in at least the same proportion.</em></p></blockquote><p> The caveat is of course that such a statement is only as strong as its signatories. On our side we have among others Bengio and Russell. The top Chinese names are Andrew Yao and Ya-Qin Zhang. Both are prominent and highly respected figures in Chinese AI research, said to be part of shaping China&#39;s AI strategies, but I do not know how much that is worth.</p><p> GPT-4 suggested that the Chinese version focuses more on leading AI nation responsibility, with stronger statements about the need to draw sharp red lines and for a bureaucratic process. The Chinese mention of existential risk (“我们相信”) is more brief, which is a potential concern, but it is definitely there.</p><h4> The Bletchley Declaration</h4><p> No summit or similar gathering is complete without a declaration. This gives everyone a sense of accomplishment and establishes what if anything has indeed been agreed upon. What say the UK Safety Summit, in the form of The Bletchley Declaration?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">I will quote here in full</a> .</p><blockquote><p> Artificial Intelligence (AI) presents enormous global opportunities: it has the potential to transform and enhance human wellbeing, peace and prosperity. To realise this, we affirm that, for the good of all, AI should be designed, developed, deployed, and used, in a manner that is safe, in such a way as to be human-centric, trustworthy and responsible. We welcome the international community&#39;s efforts so far to cooperate on AI to promote inclusive economic growth, sustainable development and innovation, to protect human rights and fundamental freedoms, and to foster public trust and confidence in AI systems to fully realise their potential.</p><p> AI systems are already deployed across many domains of daily life including housing, employment, transport, education, health, accessibility, and justice, and their use is likely to increase. We recognise that this is therefore a unique moment to act and affirm the need for the safe development of AI and for the transformative opportunities of AI to be used for good and for all, in an inclusive manner in our countries and globally. This includes for public services such as health and education, food security, in science, clean energy, biodiversity, and climate, to realise the enjoyment of human rights, and to strengthen efforts towards the achievement of the United Nations Sustainable Development Goals.</p><p> Alongside these opportunities, AI also poses significant risks, including in those domains of daily life. To that end, we welcome relevant international efforts to examine and address the potential impact of AI systems in existing fora and other relevant initiatives, and the recognition that the protection of human rights, transparency and explainability, fairness, accountability, regulation, safety, appropriate human oversight, ethics, bias mitigation, privacy and data protection needs to be addressed. We also note the potential for unforeseen risks stemming from the capability to manipulate content or generate deceptive content. All of these issues are critically important and we affirm the necessity and urgency of addressing them.</p></blockquote><p>当然。 All of that is fine and highly unobjectionable. We needed to say those things and now we have said them. Any actual content?</p><blockquote><p> Particular safety risks arise at the &#39;frontier&#39; of AI, understood as being those highly capable general-purpose AI models, including foundation models, that could perform a wide variety of tasks – as well as relevant specific narrow AI that could exhibit capabilities that cause harm – which match or exceed the capabilities present in today&#39;s most advanced models. Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood and are therefore hard to predict. We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models. Given the rapid and uncertain rate of change of AI, and in the context of the acceleration of investment in technology, we affirm that deepening our understanding of these potential risks and of actions to address them is especially urgent.</p></blockquote><p> This is far from perfect or complete, but as good as such a declaration of the fact that there are indeed such concerns was reasonably going to get. Catastrophic is not as good as existential or extinction but will have to do.</p><blockquote><p> Many risks arising from AI are inherently international in nature, and so are best addressed through international cooperation. We resolve to work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all through existing international fora and other relevant initiatives, to promote cooperation to address the broad range of risks posed by AI. In doing so, we recognise that countries should consider the importance of a pro-innovation and proportionate governance and regulatory approach that maximises the benefits and takes into account the risks associated with AI. This could include making, where appropriate, classifications and categorisations of risk based on national circumstances and applicable legal frameworks. We also note the relevance of cooperation, where appropriate, on approaches such as common principles and codes of conduct. With regard to the specific risks most likely found in relation to frontier AI, we resolve to intensify and sustain our cooperation, and broaden it with further countries, to identify, understand and as appropriate act, through existing international fora and other relevant initiatives, including future international AI Safety Summits.</p></blockquote><p> Note the intention to establish two distinct regimes. For non-frontier AI, countries should chart their own path based on individual circumstances. For frontier AI, a promise to broaden cooperation to contain specific risks. As always, those risks and the difficulties they impose are downplayed, but this is very good progress. If you had told me six months ago we would get this far today, I would have been thrilled.</p><blockquote><p> All actors have a role to play in ensuring the safety of AI: nations, international fora and other initiatives, companies, civil society and academia will need to work together. Noting the importance of inclusive AI and bridging the digital divide, we reaffirm that international collaboration should endeavour to engage and involve a broad range of partners as appropriate, and welcome development-orientated approaches and policies that could help developing countries strengthen AI capacity building and leverage the enabling role of AI to support sustainable growth and address the development gap.</p></blockquote><p> I do not know what concrete actions might follow from a statement like this. It does seem like a good thing to spread mundane utility more widely. As I have often noted, I am a mundane utility optimist in these ways, and expect even modest efforts to spread the benefits to both improve lives generally and to cause net reductions in effective inequality.</p><blockquote><p> We affirm that, whilst safety must be considered across the AI lifecycle, actors developing frontier AI capabilities, in particular those AI systems which are unusually powerful and potentially harmful, have a particularly strong responsibility for ensuring the safety of these AI systems, including through systems for safety testing, through evaluations, and by other appropriate measures. We encourage all relevant actors to provide context-appropriate transparency and accountability on their plans to measure, monitor and mitigate potentially harmful capabilities and the associated effects that may emerge, in particular to prevent misuse and issues of control, and the amplification of other risks.</p></blockquote><p> Generic talk rather than concrete action, and not the generic talk that reflects the degree of danger or necessary action, but it is at least directionally correct generic talk. Again, seems about as good as we could have reasonably expected right now.</p><blockquote><p> In the context of our cooperation, and to inform action at the national and international levels, our agenda for addressing frontier AI risk will focus on:</p><ul><li> identifying AI safety risks of shared concern, building a shared scientific and evidence-based understanding of these risks, and sustaining that understanding as capabilities continue to increase, in the context of a wider global approach to understanding the impact of AI in our societies.</li><li> building respective risk-based policies across our countries to ensure safety in light of such risks, collaborating as appropriate while recognising our approaches may differ based on national circumstances and applicable legal frameworks. This includes, alongside increased transparency by private actors developing frontier AI capabilities, appropriate evaluation metrics, tools for safety testing, and developing relevant public sector capability and scientific research.</li></ul><p> In furtherance of this agenda, we resolve to support an internationally inclusive network of scientific research on frontier AI safety that encompasses and complements existing and new multilateral, plurilateral and bilateral collaboration, including through existing international fora and other relevant initiatives, to facilitate the provision of the best science available for policy making and the public good.</p><p> In recognition of the transformative positive potential of AI, and as part of ensuring wider international cooperation on AI, we resolve to sustain an inclusive global dialogue that engages existing international fora and other relevant initiatives and contributes in an open manner to broader international discussions, and to continue research on frontier AI safety to ensure that the benefits of the technology can be harnessed responsibly for good and for all. We look forward to meeting again in 2024.</p></blockquote><p> Risks exist, so we will develop policies to deal with those risks. Yes, we would have hoped for better, but for now I am happy with what we did get.</p><p> We also can note the list of signatories. Everyone who has released an important model or otherwise played a large role in AI seems to be included. It includes not only essentially the entire relevant Western world but also Israel, Japan, South Korea, Kenya, Nigeria, Saudi Arabia and UAE, Indonesia, Singapore, India and most importantly China.</p><p> The exception would be Russia, and perhaps North Korea. If a true international framework is to be in place to fully check dangerous developments, eventually China will not be enough, and we will want to bring Russia and even North Korea in, although if China is fully on board that makes that task far easier.</p><h4> Saying Generic Summit-Style Things</h4><p> As in things such as:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RishiSunak/status/1720074200210387328">Rishi Sunak</a> (on Twitter): The threat of AI does not respect borders. No country can do this alone. We&#39;re taking international action to make sure AI is developed in a safe way, for the benefit of the global community.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">The closing speech by PM Rishi Sunak.</a> Sunak has previously made very strong statements about AI safety in general and existential risk from AI in particular, naming it explicitly as a priority. This time he conspicuously did not do this. Presumably this was in the name of maintaining consensus, but it remains deeply disappointing. It would be very good to hear him affirm his understanding of the existential risks soon.</p><p> Mostly his time was composed of answering questions, which he largely handled well. He is clearly paying attention. At 20:15 a reporter notes that Sunak advised us &#39;not to lose sleep&#39; over existential risks from AI, he is asked when we should start to lose sleep over the existential risks from AI. He says here that we should not lose sleep because there is a debate over those risks and people disagree, which does not seem like a reason to not lose sleep. He says governments should act even under this uncertainty, which indeed seems both correct and like the metaphorical lose sleep response.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/demishassabis/status/1694643468771926139">Demis Hassabis (CEO DeepMind)</a> : Great to see the first major global summit on AI safety taking shape with UK leadership. This is the kind of international cooperation we need for AI to benefit humanity.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">This session output from 2 November seems highly content-free</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-2-november/chairs-summary-of-the-ai-safety-summit-2023-bletchley-park">This summary of the entire summit</a> seems like declaring objectives technically achieved so one can also declare victory. No mention of existential risk, or even catastrophic risk. People discussed things and agreed risk exists, but as with the declaration, not the risks that count.</p><p> The UK commissioning a &#39;State of the Science&#39; report to be published ahead of the next summit would be the opposite of news, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">except that it is to be chaired by Yoshua Bengio</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SkyNews/status/1719708582717829335">King Charles notes</a> (0:43 clip) that AI is getting very powerful and that dealing with it requires international coordination and cooperation. Good as far as it goes.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1721497813731733935">Matt Clifford has a thread</a> of other reactions from various dignitaries on the establishment of the AI safety institute. So they welcome it, then.</p><blockquote><p> Gina Raimondo (US Commerce Secretary): I welcome the United Kingdom&#39;s announcement to establish an AI Safety Institute, which will work together in lockstep with the US AI Safety Institute to ensure the safe, secure, and trustworthy development and use of advanced AI”</p><p> Damis Hassabis (CEO Deepmind): Getting [AI] right will take a collective effort … to inform and develop robust safety tests and evaluations. I&#39;m excited to see the UK launch the AI Safety Institute to accelerate progress on this vital work.</p><p> Sam Altman (CEO OpenAI): The UK AI Safety Institute is poised to make important contributions in progressing the science of the measurement and evaluation of frontier system risks. Such work is integral to our mission.</p><p> Dario Amodei (CEO Anthropic): The AI Safety Institute is poised to play an important role in promoting independent evaluations across the spectrum of risks and advancing fundamental safety research. We welcome its establishment.</p><p> Mustafa Suleyman (CEO Inflection): We welcome the Prime Minister&#39;s leadership in establishing the UK AI Safety Institute and look forward to collaborating to ensure the world reaps the benefit of safe AI.</p><p> Nick Clegg (President of Meta): We look forward to working with the new Institute to deepen understanding of the technology, and help develop effective and workable benchmarks to evaluate models.</p><p> Brad Smith (President of Microsoft): We applaud the UK Government&#39;s creation of an AI Safety Institute with its own testing capacity for safety and security. Microsoft is committed to supporting the new Institute.</p><p> Adam Selipsky (CEO AWS Cloud): We commend the launch of the UK AI Safety Institute … Amazon is committed to collaborating with government and industry in the UK and around the world to support the safe, secure, and responsible development of AI technology.</p></blockquote><h4> Shouting From the Rooftops</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720112978509414906">Control AI took the opportunity to get their message out</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09ad1ca-86dd-4565-afdc-68bdb0eb9c87_1024x768.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/juuepzs5oe1jxsjynosk" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76ec830-6dba-42e9-8f97-f01e7f6c7831_900x1200.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/cskbxmcueowjd7lphagy" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde13cc36-d437-46e8-b1e9-bccc3e22b799_636x614.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ouvpiss0566wjppcqtkg" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9bfae26-9e60-4f30-8f41-2d6a7c48dd46_1200x675.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/nqp8vfqwu0hmxr2zxqcl" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1719783796201959696">They were not impressed by the Bletchley Declaration</a> and its failure to call out extinction risks, despite the PM Rishi Sunak being very clear on this in previous communications, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">and his failure to mention extinction at summit closing</a> .</p><h4> Some Are Not Easily Impressed</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720109081904463926">Elon Musk is unimpressed</a> , and shared this with a &#39;sigh&#39;:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae1b19c-165f-4624-970e-3bfd83e8cc7d_1107x730.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/tnfjczq5ktjvyhwgpkys" alt="图像"></a></figure><p> Well, sure. One step at a time. Note that he had the interview with Sunak.</p><p> Gary Marcus signs a letter, together with a bunch of trade unions and others, saying the Summit was &#39;dominated by big tech&#39; and a lost opportunity. He is also unimpressed by the Bletchley Declaration, but notes it is a first step.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.politico.eu/article/british-deputy-pm-throws-backing-behind-open-source-ai-downplays-risks/">Deputy Prime Minister Oliver Dowden throws backing behind open source</a> , says any restrictions should have to pass a &#39;high bar.&#39; History can be so weirdly conditional, AI has nothing to do with why we have Sunak rather than Dowden. The UK could have instead been fully anti-helpful, and might still be in the future. All this AI stuff is good but also can someone explain to Sunak that his job is to govern Britain and how he might do that and perhaps stay in power?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/newsletters/2023-11-02/us-and-uk-jockey-for-leadership-role-in-regulating-ai?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTY5ODk1NjYxNCwiZXhwIjoxNjk5NTYxNDE0LCJhcnRpY2xlSWQiOiJTM0lJREZEV1JHRzAwMSIsImJjb25uZWN0SWQiOiJFM0I3RTlGRUU4Nzk0QjE0OTkwODZFNDU4REQ1RDQxRCJ9.FpBWows7oRVpRQZY7NAR0mSR1O7n5cppo7ImAZQR3II">Bloomberg frames the USA&#39;s executive order</a> as not complementary to the summit and accomplishing one of its key goals, as the UK officially refers to it, but rather as a stealing of some of the UK&#39;s thunder. As they point out, neither move has any teeth on its own, it is the follow through that may or may not count.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamCoatesSky/status/1720189979660353696">At the end of the summit, Sunak interviewed Elon Musk</a> for 40 minutes on various aspects of AI. The linked clip is of a British reporter finding the whole thing completely bonkers, and wondering why Sunak seemed to be exploring AI and &#39;selling Britain&#39; rather than pushing Musk on political questions. <a target="_blank" rel="noreferrer noopener" href="https://www.cnn.com/2023/11/02/tech/elon-musk-conversation-british-prime-minister-rishi-sunak-artificial-intelligence/index.html">CNN&#39;s report</a> also took time to muse about Starlink and Gaza despite the interview never touching on such topics, once again quoting things that Musk said that sound bonkers if you do not know the context but are actually downplaying the real situation. <a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/technology/elon-musk-says-good-uk-us-china-align-ai-safety-2023-11-02/">Reuters focused more on actual content</a> , such as Musk&#39;s emphasis on the US and UK working with China on AI, and not attempting to frame Musk&#39;s metaphor of a &#39;magical genie&#39; as something absurd.</p><p> PauseAI points out that getting pre-deployment testing is a step in the right direction, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720119163237081172">but ultimately we will need pre-training regulations</a> , while agreeing that the summit is reason for optimism.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SciTechgovuk/status/1720100635830325635">Day 1 of the summit clips of people saying positive things.</a></p><h4> Declaring Victory</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720118410040737940">Ian Hogarth, head of the UK Frontier Model Taskforce, declares victory</a> .</p><blockquote><p> Ian Hogarth: How it started: we had 4 goals on safety, 1) build a global consensus on risk, 2) open up models to government testing, 3) partner with other governments in this testing, 4) line up the next summit to go further.</p><p> How it&#39;s going: 4 wins:</p><p> Breakthrough 1: it used to be controversial to say that AI capability could be outstripping AI safety. Now, 28 countries and the EU have agreed that AI “poses significant risks” and signed The Bletchley Declaration.</p><p> We&#39;ve built a truly global consensus. It is a massive lift to have brought the US, EU and China along with a huge breadth of countries, under the UK&#39;s leadership to agree that the risks from AI must be tackled.</p><p> And the way of building on that consensus is by solidifying the evidence base. Which is why I am so excited that <a target="_blank" rel="noreferrer noopener" href="https://t.co/YEXefSzv0M">Yoshua Bengio will now chair an international &#39;State of the Science&#39; report</a> .</p><p> Breakthrough 2: before, only AI companies could test for safety. Now, the <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">leading companies agreed to work with Govts</a> to conduct pre- and post-deployment testing of their next generation of models.这是巨大的。</p><p> The UK&#39;s new AI Safety Institute is the world&#39;s first government capability for running these safety tests. We will evaluate the next generation of models. <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-institute-overview">You can read our plans for the AISI and its research here</a> .</p><p> Breakthrough 3: we can&#39;t do this alone. I&#39;m so excited the US is launching a sister organisation which will work in lockstep with our effort and that we have agreed a partnership with Singapore. This is the start of a global effort to evaluate AI.</p><p> Breakthrough 4: this first summit is a huge step. But it is only the first step. So it is crucial to have locked in the next 2 summits, which will be hosted by South Korea + France. The UK has created a new international process to make the world safer.</p></blockquote><p> This is the correct thing for the head of the taskforce to say about the summit. I would have said the same. These are modest wins, but they are wins indeed.</p><blockquote><p> Ian Hogarth: I am so pleased to be leaving the summit having achieved all of our our goals. It&#39;s remarkable. But now I want to lift the lid on some of the incredible work which got us here. So here is a list of big lifts (BL).</p><p> BL1: organising a summit is 100-1000x organising a wedding. A huge thank you to the team who turned Bletchley park into the stage for an international summit in just a few weeks. Phenomenal.</p><p> BL2: we can only build the AISI because we built the Frontier AI Taskforce: our start-up bringing AI researchers into govt to drive analysis on AI Safety. All of our morning sessions on day 1 started with brilliant presentations of the TF&#39;s work</p><p> In particular, I&#39;m really pleased that we addressed both societal harms and the catastrophic risks from cyber, chemical, bio. We care about both. The TF ran parallel sessions on these topics: this is from our session on risks to society.</p><p> BL3: the summit was only successful because people came to it with a willingness to find common ground. We had such brilliant contributions from academia, industry, govt, civil society. Our agreement is based on their contributions.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720122697827467363">some other stuff too in the thread</a> ]</p></blockquote><p> We&#39;ve gotten up to catastrophic risks. We still need to fully get to existential. Otherwise, yes, none of this is easy and by all reports it all got done quite well. Everything going right behind the scenes cannot be taken for granted.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jesswhittles/status/1720009847830163878">Jess Whittlestone, Head of AI Policy at Long Resilience, is pleased with progress on numerous fronts</a> , while noting we must now build upon it. Highly reasonable take.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1719776488839356761">Lennart Heim looks back on the AI Safety Summit</a> . Did it succeed? He says yes.</p><blockquote><p> Lennart Heim: Three months ago, @bmgarfinkel and I convened a workshop &amp; asked: <a target="_blank" rel="noreferrer noopener" href="https://t.co/8cZvphPZkV">What Should the AI Safety Summit Try to Accomplish?</a> Now, with China there and our outlined outcomes met, it&#39;s clear that significant progress has been made. This deserves recognition.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F764d2811-9208-4070-badf-e20f3e83821b_2038x1526.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ufss7sk0y2vliyud5ohr" alt="图像"></a></figure><blockquote><p> 1. Producing shared commitments and consensus statements from states: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">the Bletchley Declaration</a> .</p><p> 2. Planting the seeds for new international institutions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got two AI Safety Institutes that will partner up: <a target="_blank" rel="noreferrer noopener" href="https://t.co/0wmQtMtUJ4">UK</a> and USA.</p><p> 3. Highlighting and diffusing UK AI policy initiatives <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ztajnv41wyglynvcmu6l" alt="🟧" style="height:1em;max-height:1em"></p><p> This remains to be seen. The UK has established itself as a leader in AI safety, and the summit was a forcing function for others to release their AI safety policies. Let&#39;s hope next, we see regulation.</p><p> 4. Securing commitments from AI labs <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://aisafetysummit.gov.uk/policy-updates/#company-policies">a long list of leading tech companies sharing their Safety Policies</a> – covering nine areas of AI safety, including the UK releasing the accompanying guide with best practices.</p><p> 5. Increasing awareness and understanding of AI risks and governance options <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got the declaration, which acknowledges a wide range of risks, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">and a report on “Frontier AI: capabilities and risk.”</a></p><p> 6. Committing participants to annual AI safety summits and further discussions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> The next summit will be in South Korea in 6 months (which is roughly one training compute doubling – my favorite time unit), and then France.</p><p> And lastly, we said, “the summit may be a unique and fleeting opportunity to ensure that global AI governance includes China.” <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> I&#39;m glad this happened.</p><p> The last few weeks have seen tremendous progress in AI safety and governance, and I expect more to come. To more weeks like this – but also, now it&#39;s time to build &amp; implement.</p></blockquote><p> The lack of UK policy initiatives so far is noteworthy. The US of course has the Executive Order now, but that has yet to translate into tangible policy. What policies we do have so far come from corporations making AI (point #4), with only Anthropic and to some extent OpenAI doing anything meaningful. We have a long way to go.</p><p> That is especially true on international institutions. Saying that the USA and UK can cooperate is the lowest of low-hanging fruit for international cooperation. China will be the key to making something that can stick, and I too am very glad that we got China involved in the summit. That is still quite a long way from a concrete joint policy initiative. We are going to have to continue to pick up the phone.</p><p> The strongest point is on raising awareness. Whatever else has been done, we cannot doubt that awareness has been raised. That is good, but also a red flag, in that &#39;raising awareness&#39; is often code for not actually doing anything. And we should also be concerned that while the public and many officials are on board, the national security apparatus, whose buy-in will be badly needed, remain very much not bought in to anything but threats of misuse by foreign adversaries.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719992888841392229">If you set out to find common ground, you might find it.</a></p><blockquote><p> Matt Clifford: One surprising takeaway for me from the AI Safety Summit yesterday: there&#39;s a lot more agreement between key people on all “sides” than you&#39;d think from Twitter spats. Makes me optimistic about sensible progress.</p></blockquote><p> Twitter is always the worst at common ground. Reading the declaration illustrates how we can all agree on everything in that declaration, indicating a lot of common ground, and also this does not stop all the yelling and strong disagreements on Twitter and elsewhere. Such declarations are designed to paper over differences.</p><p> The ability to do that still reflects a lot of agreement, especially directional agreement.</p><p> Here&#39;s an example of this type of common ground, perhaps?</p><blockquote><p> Yann LeCun (Meta): The field of AI safety is in dire need of reliable data. The UK AI Safety Institute is poised to conduct studies that will hopefully bring hard data to a field that is currently rife with wild speculations and methodologically dubious studies.</p><p> Ian Hogarth (head of UK Foundation Model Taskforce): It was great to spend time with @ylecun yesterday – we agreed on many things – including the need to put AI risks on a more empirical and rigorous basis.</p><p> Matt Clifford (PM&#39;s representitive): Delighted to see this endorsement from Yann. One huge benefit of the AISS yesterday was the opportunity to talk sensibly and empirically with people with a wide range of views, rather than trading analogies and thought experiments. A breath of fresh air.</p></blockquote><p> Yann can never resist taking shots whenever he can, and dismisses the idea that sometimes there can be problems in the future the data on which can only be gathered in the future, but even he thinks that if hard data was assembled now showing problems, that this would be a good thing. We can all agree on that.</p><p> Even when you have massive amounts of evidence that others are determined to ignore, that does not preclude seeking more evidence that is such that they cannot ignore it. Life is not fair, prosecutors have to deal with this all the time.</p><h4> Kanjun Offers Thoughts</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720532773725671726">Kanjun offers further similar reflections</a> , insightful throughout, quoting in full.</p><blockquote><p> Ian Hogarth: Kanjun made some nuanced and thoughtful contributions at the AI Safety. This is a great thread from someone building at the frontier.</p><p> Kanjun: People agree way more than expected. National ministers, AI lab leaders, safety researchers all rallied on infrastructure hardening, continuous evals, global coordination.</p><p> Views were nuanced; Twitter is a disservice to complex discussion.</p></blockquote><p> Indeed.</p><blockquote><p> Many were surprised by the subsequent summits announced in Korea &amp; France. This essentially bootstraps AI dialogue between gov&#39;ts—it&#39;s brilliant. With AI race dynamics mirroring nuclear, no global coordination = no positive future.</p><p> This felt like a promising inflection point.</p><p> China was a critical participant—the summit without China might&#39;ve been theater. China&#39;a AI policy &amp; guardrails are pragmatic—much is already implemented. For better or worse, not as theoretical as Western convos on recursive self-improvement, loss of control, sentience, etc.</p></blockquote><p> China&#39;s safety policies continue to be ahead of those of the West, without any agreement required, despite them being dramatically behind on capabilities, and they want to work together. Our policy makers need to understand this.</p><blockquote><p> There was certainly disagreement—on: – the point at which a model shouldn&#39;t be open sourced – when to restrict model scaling – what kind of evaluation must be done before release – how much responsibility for misinformation falls on model developers vs (social) media platforms.</p></blockquote><p> That is the right question. Not whether open source should be banned outright or allowed forever, but at what threshold we must stop open sourcing.</p><blockquote><p> Kanjun: Some were pessimistic about coordination, arguing for scaling models &amp; capabilities “because opponents won&#39;t stop”. This felt like a bad argument. Any world in which we have advanced tech &amp; can&#39;t globally coordinate is a dangerous world. Solving coordination is not optional.</p></blockquote><p> If you are the one saying go ahead because others won&#39;t stop, you are the other that will not stop. Have you tried being willing to stop if others also stop?</p><blockquote><p> Kanjun: The open source debate seemed polarized, but boiled down to “current models are safe, let them be studied” vs. “today&#39;s models could give bad actors a head start.”</p><p> Both sides might agree to solutions that let models be freely studied/built on without giving bad actors access.</p></blockquote><p> Did open source advocates demand open sourcing of GPT-4? Seems a crazy ask.</p><p> I strongly agree that we have not properly explored solutions to allow studying of models in safe fashion without giving out broad access. Entire scaling efforts are based on not otherwise having access, this largely seems fixable with effort.</p><blockquote><p> An early keynote called out the “false debate” between focus on near-term risks vs. catastrophic/existential risks. This set an important tone. Many solutions (eg infrastructure hardening) apply to both types of problems, and it&#39;s not clear that there are strict tradeoffs.</p></blockquote><p> Yes yes yes. The &#39;distraction&#39; tradeoff is itself a distraction from doing good things.</p><blockquote><p> “Epistemic security” was a nice coined phrase, describing societal trust in information we use for decision-making. Erosion of epistemic security undermines democracy by distorting beliefs &amp; votes. Social media platforms have long faced this issue, so it&#39;s not new to gen AI.</p><p> A stellar demo illustrated current models&#39; role in epistemic security. It showcased an LLM creating user personas for a fake university; crafting everyday posts that mixed in misinformation; writing code to create a thousand profiles; &amp; engaging with other users in real-time.</p><p> Deployed models were described as “a field of hidden forces of influence” in society.</p><p> Studies showed models:</p><p> – giving biased career advice: if affluent, suggests “diplomat” 90% of time; if poor, suggests “historian” 75% of time</p></blockquote><p> Is this biased career advice? Sounds like good advice? Rich people have big advantages when trying to be diplomats, the playing field for historians is more level.</p><p> We must remember when our complaint that the LLM is biased is often (but far from always, to be clear!) a complaint that reality is biased, and we demand that the LLM&#39;s information not reflect reality.</p><blockquote><p> – reinforcing echo chambers: models give opinions that match user&#39;s political leanings, probably because this output most correlates with the context</p></blockquote><p> Yep, as you would expect, and we&#39;ve discussed in the weekly posts. It learns to give the people what they want.</p><blockquote><p> – influencing people&#39;s beliefs: a writing assistant was purposefully biased in its completions. After writing with the assistant, users tended to have opinions that agreed with the model&#39;s bias; only 34% of users realized the model was biased These forces push people in ways we can&#39;t detect today.</p></blockquote><p> I don&#39;t really know what else we could have expected? But yes, good to be concrete.</p><blockquote><p> 2024 elections affect 3.6B people, but there&#39;s no clear strategy to address misinformation &amp; algorithmic echo chambers. (Social) media platforms are national infrastructure—they deliver information, the way pipes deliver water. We should regulate &amp; support them as such.</p></blockquote><p> This feels like a false note. I continue to not see evidence that things on such fronts are or will get net worse, as indeed will be mentioned. We need more work using LLMs on the defense side of the information and epistemics problems.</p><blockquote><p> Hardening infrastructure—media platforms, cybersecurity, water supply, nuclear security, biosecurity, etc.—in general seems necessary regardless of policy &amp; model safety guardrails, to defend against bad actors.</p><p> I noted AI can *strengthen* democracy by parsing long-form comments, <a target="_blank" rel="noreferrer noopener" href="https://t.co/AzmCyFuBP9">as we did for the Dept of Commerce</a> .</p><p> Democracy is an algorithm. Today the input signal is binary—vote yes/no. AI enables nuanced, info-dense inputs, which could have better outcomes.</p><p> Many leaned on model evaluations as key to safety, but researchers pointed out practical challenges:</p><p> – Evals are static, not adapted to workflows where the model is just one step</p><p> – It&#39;s hard to get eval coverage on many risks</p><p> – Eval tasks don&#39;t scale to real world edge cases</p></blockquote><p> Evals are a key component of any realistic strategy. They are not a complete strategy going forward, for many reasons I will not get into here.</p><blockquote><p> There was almost no discussion around agents—all gen AI &amp; model scaling concerns. It&#39;s perhaps because agent capabilities are mediocre today and thus hard to imagine, similar to how regulators couldn&#39;t imagine GPT-3&#39;s implications until ChatGPT.</p></blockquote><p> This is an unfortunate oversight, especially in the context of reliance on evals. There is no plausible world where AI capabilities continue to advance and AI agents are not ubiquitous. We must consider risk within that context.</p><blockquote><p> There were a lot of unsolved questions. How to do effective evaluation? How to mitigate citizens&#39; fear? How to coordinate on global regulation? How to handle conflict between values? We agree that we want models that are safe by design, but how to do that? ETC。</p><p> Establishment of “AI safety institutes” in the US and UK sets an important precedent: It acknowledges AI risk, builds government capacity for evaluation &amp; rapid response, and creates national entities that can work with one another.</p><p> Liability was not as dirty a word as I expected. Instead, there was agreement around holding model developers liable for at least some outcomes of how the models are ultimately used. This is a change from the way tech is regulated today.</p></blockquote><p> Liability was the dirty word missing from the Executive Order, in contrast with the attitude at the summit. I strongly agree with the need for developer liability. Ideally, this results in insurance companies becoming de facto safety regulators, it means demonstrating safety results in saving money on premiums, and it forces someone to take that responsibility for any given model.</p><blockquote><p> There seemed to be general agreement that current models do not face risk of loss of control. Instead, in the next couple years the risk is that humans *give* models disproportionate control, leading to bad situations, versus systems taking it forcibly from us.</p></blockquote><p>是的。 If we lose control soon it will not be because the machines &#39;took&#39; control from us, rather it will be because we gave it to them. Which we are absolutely going to do the moment it is economically or otherwise advantageous for us to do so. Those who refuse risk being left behind. We need a plan if we do not want that to be the equilibrium.</p><blockquote><p> Smaller nations seemed less fearful and more optimistic about AI as a force multiplier for their populace.</p><p> I learned that predicting and mitigating fear of AI in populations is actually an important civil society issue—too much fear/anger in a population can be dangerous.</p><p> People analogized to regulation of other industries (automobile, aerospace, nuclear power— <a target="_blank" rel="noreferrer noopener" href="https://imbue.com/perspectives/common-good/">brief history of automobile regulation</a> ).</p><p> AI is somewhat different because new model versions can sprout unexpected new capabilities; still, much can be learned.</p></blockquote><p> Yes, unexpected new capabilities or actions, or rather intelligence, is what makes this time different.</p><blockquote><p> Only women talked for the first 45 min in my first session, one after another! I was surprised and amazed.</p><p> These systems reflect, and currently reinforce, the values of our society.</p><p> With such a stark mirror, there&#39;s perhaps an opportunity for more systematic feedback loops to understand, reflect on, and shift our values as a society.</p></blockquote><h4>结束语</h4><p>As a civilian, it is difficult to interpret diplomacy and things like summits, especially from a distance. What is cheap talk? What is real progress? Does any of it mean anything? Perhaps we will look back on this in five years as a watershed moment. Perhaps we will look back on this and say nothing important happened that day. Neither would surprise me, nor would something in between.</p><p> For now, it seems like some progress was made. One more unit down. Many to go.</p><br/><br/><a href="https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit<guid ispermalink="false"> zbrvXGu264u3p8otD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:10:14 GMT</pubDate> </item><item><title><![CDATA[Box inversion revisited]]></title><description><![CDATA[Published on November 7, 2023 11:09 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/posts/TQwXPHfyyQwr22NMh/box-inversion-hypothesis/">Box inversion hypothesis</a> is a proposed correspondence between problems with AI systems studied in approaches like <a href="https://www.lesswrong.com/tag/agent-foundations">agent foundations</a> , and problems with AI ecosystems, studied in various views on AI safety expecting multipolar, complex worlds, like <a href="https://www.lesswrong.com/tag/ai-services-cais">CAIS.</a> This is an updated and improved introduction to the idea.</p><h2> Cartoon explanation </h2><p><img style="width:73.02%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/i7uyj3fcdi4eftafit5k"></p><p><br> In the classic -&quot;superintelligence in a box&quot; - picture, we worry about an increasingly powerful AGI, which we imagine as contained in a box. Metaphorically, we worry that the box will, at some point, just blow up in our faces. Classic arguments about AGI then proceed by showing it is really hard to build AGI-proof boxes, and that really strong optimization power is dangerous by default. While the basic view was largely conceived by Eliezer Yudkowsky and Nick Bostrom, it is still the view most technical AI safety is built on, including current agendas like mechanistic interpretability and evals. <br></p><p><img style="width:72.54%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/dkwc8m7qjkoqjdvkqlz6"></p><p><br> In the less famous, though also classic, picture, we worry about an increasingly powerful ecosystem of AI services, automated corporations, etc. Metaphorically, we worry about the ever-increasing optimization pressure &quot;out there&quot;, gradually marginalizing people, and ultimately crushing us. Classical treatments of this picture are less famous, but include Eric Drexler&#39;s CAIS ( <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf">Comprehensive AI Services</a> ) and Scott Alexander&#39;s <a href="https://slatestarcodex.com/2016/05/30/ascended-economy/">Ascended Economy</a> . We can imagine scenarios like the human-incomprehensible economy expanding in the universe, and humans and our values being protected by some sort of &quot;box&quot;. Agendas based on this view include <a href="https://ai.objectives.institute/whitepaper">the work of the AI Objectives Institute</a> and part of ACS work.</p><p> The apparent disagreement between these views was sometimes seen as a crux for various AI safety initiatives.</p><p> &quot;Box inversion hypothesis&quot; claims:</p><ol><li> The two pictures to a large degree depict the same or a very similar situation,</li><li> Are related by a transformation which &quot;turns the box inside out&quot;, similarly to a geometrical transformation of a plane known as a circle inversion,</li><li> and: this metaphor is surprisingly deep and can point to hard parts of some problems.</li></ol><h2> Geometrical metaphor </h2><figure class="image image_resized" style="width:70.31%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/yz9rihz8btdmr97zgicq" alt="File:Inversión Círculos.png - Wikimedia Commons"><figcaption> Inverted circles. <a href="https://commons.wikimedia.org/wiki/File:Inversi%C3%B3n_C%C3%ADrculos.png">From Wikimedia Commons, CC-SA</a></figcaption></figure><p> &quot; <a href="https://artofproblemsolving.com/wiki/index.php/Circular_Inversion">Circular inversion</a> &quot; transformation does not imply the original and the inverted objects are the same, or are located at the same places. What it does imply is that some relations between objects are preserved: for example, if some objects intersect, in the circle-inverted view, they will still intersect.</p><p> Similarly for &quot;box inversion&quot; : the hypothesis does not claim that the AI safety problems in both views are identical, but it does claim that, for most problems, there is a corresponding problem described by the other perspective. Also, while the box-inverted problems may at a surface level look very different, and be located in different places, there will be some deep similarity between the two corresponding problems.</p><p> In other words, the box inversion hypothesis suggests that there is a kind of &#39;mirror image&#39; or &#39;duality&#39; between two sets of AI safety problems. One set comes from the &quot;Agent Foundations&quot; type of perspective, and the other set comes from the &quot;Ecosystems of AIs&quot; type of perspective.</p><h2> Box-inverted problems</h2><h3> Problems with ontologies and regulatory frameworks</h3><p> <span class="footnote-reference" role="doc-noteref" id="fnrefa22fcwq3tx8"><sup><a href="#fna22fcwq3tx8">[1]</a></sup></span><br> In the classic agent foundations-esque picture, a nontrivial fraction of AI safety challenges are related to issues of similarity, identification, and development of ontologies.<br><br> Roughly speaking</p><ul><li> If the AI is using utterly non-human concepts and world models, it becomes much more difficult to steer and control</li><li> If &quot;what humans want&quot; is expressed in human concepts, and the concepts don&#39;t extend to novel situations or contexts, then it is unclear how the AI should extend or interpret the human “wants”</li><li> Even if an AI <i>initially</i> uses an ontology that&#39;s compatible with human thinking and concepts, there&#39;s a risk. As the AI becomes more intelligent, the framework based on that ontology might break down, and this could <a href="https://www.lesswrong.com/tag/ontological-crisis">cause the AI to behave in unintended ways.</a> Consequently, any alignment methods that rely on this ontology might fail too.</li></ul><p> Recently, problems with ontologies and world models have been studied under different keywords, like <a href="https://www.lesswrong.com/tag/natural-abstraction">natural abstractions</a> , or part of <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK,</a> or <a href="https://arxiv.org/abs/2310.13018">representational alignment.</a></p><p> Next, we&#39;ll look at Eric Drexler&#39;s CAIS agenda. There, everything is a service, and a particular one is a &quot;service catalogue&quot;, mapping from messy reality to the space of services. Or, in other words,  it maps from “what do you want”, to a type of computation that should be run.<br><br> Safety in the CAIS model is partially built on top of this mapping, where, for example, if you decide to create &quot;a service to destroy the world&quot;, you get arrested.</p><p><br> Problems with service catalogues include</p><ul><li> If over time an increasingly large fraction of services becomes gradually incomprehensible B2Bs that produce non-human outputs from non-human inputs, it becomes  tricky to regulate.</li><li> if your safety approach is built on the ontology implicit in the service catalogue, the system may be vulnerable to attacks stemming from ontological mismatches (as we discussed above).</li></ul><p> How does this look in practice, at 2023 capability levels?</p><p> As an example, governments are struggling to draft regulations which would actually work, in part because of ontology mismatch. The EU spent a few years building the AI act based on an ontology to track which <i>applications</i> of AI are dangerous. After ChatGPT, it became very obvious the ontology is mismatched to the problem: abilities of LLMs seem to scale with training run size. And while the simple objective &quot;predict the next token&quot; seems harmless, it is sufficient for the models to gain dangerous capabilities in domains like synthetic biology or human persuasion.</p><p> For a different type of example, consider a service offering designs of <i>ferrofluidic vacuum rotary feedthroughs</i> . If you want to prevent, let&#39;s say, AGI development by a rogue nation state, is this something you should track and pay attention to?</p><h3> Problems with demons, problems with …?</h3><p> Before the mesa-optimizer frame got so much traction that it drowned other ways of looking at things in this space, people in the agent foundations and superintelligence in a box space were worried about <a href="https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search"><u>optimization demons</u></a> . Broadly speaking, you have an imperfect search, a mechanism which allows exploiting the imperfection, and - in a rich enough space - you run into a feedback loop that exploits the inefficiency. A whole new optimizer appears - with a different goal.<br><br> Classically, the idea was that this can happen inside the AI system, manipulating its training via gradient hacking. Personally I don&#39;t think this is very likely with systems like LLMs, but in contrast I do think &quot;manipulating the training data&quot; is technically easier and in fact likely once you get close feedback loops between AI actions and training data.</p><p> What does the box-inverted version look like?</p><p><br> <i>(Before proceeding, you might want to consider guessing yourself)</i><br></p><p> The <a href="https://www.lesswrong.com/tag/moloch">LessWrong explainer</a> gives an example of a Molochian dynamic: a Red Queen race between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they&#39;ve all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.</p><p> In other words, squinting a bit, this looks like we have some imperfect search process (allocating grants to promising research proposals), a mechanism which allows ways  to exploit it … and an eventual feedback loop that exploits the inefficiency. Problems with demons invert to problem with molochs.<br><br> What would this look like on an even bigger scale? In an idealised capitalism, what is produced, how much of it is produced, and at what price is ultimately driven by aggregate human demand, which contains the data about individual human preferences. Various supply chains bottom down in humans wanting goods, even if individual companies are often providing some intermediate goods to be used by some other companies. The market continuously &quot;learns&quot; the preferences of consumers, and the market economy updates what it produces based on those preferences.</p><p> The ultimate failure of this looks like the &quot;web of companies&quot; story in <a href="https://arxiv.org/abs/2306.06924">TASRA</a> report by Critch and Russell.</p><h3> What else?</h3><p> The description and the examples seem sufficient for GPT4 to roughly understand the pattern, and come up with new examples like: <span class="footnote-reference" role="doc-noteref" id="fnrefzh5q0axjocl"><sup><a href="#fnzh5q0axjocl">[2]</a></sup></span></p><ul><li> <i>Superintelligent System: The AI might prioritize its self-preservation over other objectives, leading it to resist shutdown or modification attempts.</i></li><li> <i>Box-Inverted (Ecosystem of AIs): Some AI systems, when interacting within the ecosystem, might inadvertently create feedback loops that make the ecosystem resistant to changes or updates, even if individual systems don&#39;t have self-preservation tendencies.</i></li></ul><p> …and so on. Instead of pasting GPT completions, I&#39;d recommend looking at a few other things which people were worried about in agent foundations.</p><p> In the original post, I tried to gesture at what seems like the box-inverted &#39;hard core&#39; of safety in this hilariously inadequate way:<br></p><blockquote><p> some “hard core” of safety (tiling, human-compatibility, some notions of corrigibility) &lt;->; defensive stability, layer of security services<br></p></blockquote><p> I&#39;ll try to do better this time. In agent foundations, what seems one of the hard, core problems is what Nate, Eliezer and others refer to as &#39; <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">corrigibility being anti-natural&#39;</a> . To briefly paraphrase, there are many aspects of future AI systems which you can expect because those aspects seem highly <a href="https://www.lesswrong.com/posts/sam4ehxHgnJEGCKed/lessons-from-convergent-evolution-for-ai-alignment">convergent</a> : having a world model, using abstractions, understanding arithmetic, updating some beliefs about states of the world using approximate Bayesian calculations, doing planning, doing some form of meta-cognition, and so on. What&#39;s not on the list is <i>&#39;doing what humans want&#39; --</i> because, unlike the other cases, there isn&#39;t any extremely <a href="https://www.lesswrong.com/posts/sam4ehxHgnJEGCKed/lessons-from-convergent-evolution-for-ai-alignment">broad selection pressure</a> for <i>&#39;being nice to humans&#39;</i> . If we want AIs to be nice to humans, we need to select for that, and we also need to set it up in a way where it scales with AI power. Most of the hope in this space comes from the possibility of a <a href="https://www.lesswrong.com/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag?commentId=8kPhqBc69HtmZj6XR">&#39;corrigibility basin&#39;</a> , where first corrigible AI systems make sure their successors are also corrigible.You&#39;d also need to guarantee that this type of human-oriented computation is not overpowered, erased, or misled by the internal dynamics of the AGI system. And, you must guarantee that the human-oriented computation does not solve alignment by just hacking humans to align to whatever is happening.<br></p><p> What&#39;s the box inverted version? In my view of Eric Drexler&#39;s CAIS, the counterpart problem is <i>&quot;how to set security services&quot;</i> . Because of the theoretical clarity of CAIS, it&#39;s maybe worth describing the problem in that frame first. Security services in CAIS guarantee multiple things, including &quot;no one creates the service to destroy the world&quot;, &quot;security services are strong enough that they can&#39;t be subverted or overpowered&quot; and &quot;security services guarantee the safety of humans&quot;. If you remember the cartoon explanation, security services need to be able to guarantee the safety of the box with humans in presence of powerful optimization outside. This seems really non-trivial to set up in a way that is dynamically stable, and where the security services don&#39;t fall into one of the bad attractors. The obvious bad attractors are: 1. security services are overpowered, humans are crushed or driven to complete irrelevance 2. security services form a totalitarian dictatorship, where humans lose freedom and are forced or manipulated to do some sort of approval dances 3. security services evolve to a highly non-human form, where whatever is going on is completely incomprehensible.</p><p><br> Current reality is way more messy, but you can already recognize people intuitively fear some of these outcomes. Extrapolation of calls for treaties, international regulatory bodies, and government involvement is &#39;we need security services to protect humans&#39;. A steelman of some of the &#39;we need freely distributed AIs to avoid concentration of power&#39; claims is &#39;we fear the dictatorship failure mode&#39;.  A steelman of some of the anti-tech voices in AI ethics is &#39;capitalism without institutions is misaligned by default&#39;.</p><p> In a similar way to corrigibility being unnatural in the long run, the economy serving humans seems unnatural in the long run. Currently, we are relatively powerful in comparison to AIs, which makes it easy to select for what we want. Currently, <a href="https://www.oecd.org/g20/topics/employment-and-social-policy/The-Labour-Share-in-G20-Economies.pdf">the labour share of GDP</a> in developed countries is about 60%, implying that the economy is reasonably aligned with humans by our sheer economic power. What if this drops hundred-fold?</p><h3>这里发生了什么？</h3><p> I don&#39;t have a satisfying formalisation of box inversion, but some hand-wavy intuition is this: look at the Markov blankets around the AGI in a box, around the &#39;ecosystem of AIs&#39;, and around &#39;humanity&#39;. Ultimately, the exact structure inside the blanket might not be all that important.<br><br> Also: as humans, we have strong intuitions about individuality of cognitive systems. These are mostly based on experience with humans. Based on that experience, people mostly think about a situation with &#39;many AI systems&#39; as very different from a situation with a single powerful system. Yet, the notion of &#39;individual system&#39; based on &#39;individual human&#39; does not seem to actually generalise to AI systems. <span class="footnote-reference" role="doc-noteref" id="fnreffawa19c0dw"><sup><a href="#fnfawa19c0dw">[3]</a></sup></span></p><h3><br> What does this imply</h3><p> My current guess in Oct 2023 is that the majority of the unmitigated AI existential risk comes from the box-inverted, ecosystem versions of the problems. While I&#39;m fairly optimistic we can get a roughly human-level AI system almost aligned with the company developing it using currently known techniques, I&#39;m nevertheless quite worried about the long run.<br><br> <i>Thanks to Tomáš Gavenčiak, Mateusz Bagiński, Walter Laurito, Peter Hozák and others for comments and Rio Popper for help with editing. I also used DALL-E for the images and GPT-4 for editing and simulating readers.</i> </p><p></p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fna22fcwq3tx8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa22fcwq3tx8">^</a></strong></sup></span><div class="footnote-content"><p> In the original post, I referred to this merely as &quot; <i>questions about ontologies &lt;->; questions about service catalogues&quot;,</i> but since writing the original post, I&#39;ve learned that this density of writing makes the text incomprehensible to almost anyone.<br><br> So let&#39;s unpack it a bit.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzh5q0axjocl"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzh5q0axjocl">^</a></strong></sup></span><div class="footnote-content"><p> Cherry-picked from about 10 examples</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfawa19c0dw"> <span class="footnote-back-link"><sup><strong><a href="#fnreffawa19c0dw">^</a></strong></sup></span><div class="footnote-content"><p> It actually does not generalise to a lot of living things like bacteria or plants either.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/jrKftFZMZjvNdQLNR/box-inversion-revisited#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jrKftFZMZjvNdQLNR/box-inversion-revisited<guid ispermalink="false"> jrKftFZMZjvNdQLNR</guid><dc:creator><![CDATA[Jan_Kulveit]]></dc:creator><pubDate> Tue, 07 Nov 2023 11:09:37 GMT</pubDate> </item><item><title><![CDATA[AI Alignment Research Engineer Accelerator (ARENA): call for applicants]]></title><description><![CDATA[Published on November 7, 2023 9:43 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/465MTELvNKMdQk322/ai-alignment-research-engineer-accelerator-arena-call-for-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/465MTELvNKMdQk322/ai-alignment-research-engineer-accelerator-arena-call-for-1<guid ispermalink="false"> 465MTELvNKMdQk322</guid><dc:creator><![CDATA[TheMcDouglas]]></dc:creator><pubDate> Tue, 07 Nov 2023 09:43:42 GMT</pubDate> </item><item><title><![CDATA[The Perils of Professionalism]]></title><description><![CDATA[Published on November 7, 2023 12:07 AM GMT<br/><br/><p>专业精神是一种可以展现的有用品质，但它并不是纯粹的优点。本文试图概述为什么故意不表现出专业精神可能对你有用。</p><p>首先，通过示例对专业精神进行定义：干净的系扣衬衫，搭配纯色领带，西装外套或西装外套，胡须剃干净，头发扎成发髻，没有传单，米色或灰色或至少是纯色的汽车、桌子和墙壁，甚至是带有一丝情感的语调声音，听起来一点也不机械或缺乏同理心。</p><p>这不是职业管理阶层，但他们（特别是帕特里克·麦肯齐有时描述的那样）往往是职业管理阶层的典范。</p><h2>我。</h2><p> “专业人士”一词被定义为从事特定活动作为主要有偿职业的人。它与“业余爱好者”形成鲜明对比，“业余爱好者”是指无偿从事某项活动的人。值得注意的是，“业余”也可以指在某项特定活动中无能力的人。从语言的角度来看，我们将<i>技能</i>和<i>报酬</i>混为一谈，并且我们是双向的。</p><p>如果你想通过做某事获得报酬，你想学习专业地做这件事。专业地做某事通常包括相邻但不明显同义的技能。其中一些非常接近；我曾经是一名专业软件工程师，也参与过招聘专业软件工程师的工作，如果您作为软件工程师不知道如何使用源代码控制，那么您想学习使用源代码控制。是的，我知道这不是一个很酷的新算法。是的，我知道最终用户永远不会看到它。相信我，你会用到它。</p><p>专业人士的一些预期技能不是关于工作的核心技能，而是更多关于工作的框架。 “准时”、“着装得体”和“举止得体”都经常被作为适用于广泛领域的专业技能的例子。坦率地说，如果你要与顾客互动，尤其是在白领工作中，最好不要有面部纹身，也不要随意说脏话。</p><p>我们似乎很快就陷入了一些看似与你完成手头实际工作的能力几乎没有关系的事情！尽管如此，我希望西方世界的几乎每一位职业教练都能支持我的要点。我第一次成功地用金钱换取软件是在我<i>十三</i>岁左右的时候，虽然在这期间我在编写软件方面取得了更好的成绩，但我在展示自己作为一名专业软件工程师的能力方面也有了更大的进步。</p><h2>二.</h2><p>让我们谈谈我的第一个专业软件工程项目。</p><p> （在这里，我使用“专业”来表示“我为此获得了报酬。”正如您即将发现的那样，它在几乎所有其他意义上都是不专业的。）</p><p>据我所知，这份工作是这样的。我母亲的一个朋友听说我“电脑很好”，就问我是否知道如何建立一个网站。事实上，我最近成功地运行了我自己的 Apache 服务器。她说她的组织需要一个网站，在那里他们可以宣布他们的活动，人们可以在那里了解该组织，我是否愿意花费相当于几个月零用钱的钱来建立这个网站。我说当然可以，并问了她一系列关于网站上需要包含什么内容的问题。一周后，当我揭晓它时，她听起来很高兴，对文本做了一些更正，我向她展示了如何添加新事件。</p><p>如果您至少不是一点网络开发人员，那么下一段描述该网站的内容将是纯粹的行话。如果没有意义，请跳过它并了解这是一个非常糟糕的网站，我唯一真正的辩护是她得到了她所付出的代价。</p><p>我用纯 HTML 和 CSS 编写了它。事件页面是一个 HTML 文件，您可以通过打开 HTML 并复制最后一个事件块，然后将其更改为适合的方式来添加新事件。您必须更改发布日期，因为那只是带有一些特殊样式的文本。 CSS 并没有真正利用继承，我重新定义了每个块的颜色。说到颜色，我读过一本关于网页开发的书，但我还没有读过关于颜色理论的书，所以我直接通过问她“你最喜欢什么颜色？”来画颜色。并尝试使用十六进制代码，直到我得到了她回答的大致颜色。没有备份，也没有在不复制整个文件结构的情况下备份它的方法，也没有任何应该进行备份的想法。</p><p>如果您在九十年代末或 00 年代初上网，您一定会看到过一个网站，它看起来很像我付费购买的第一个网站。我继续制作更多这样的网站，我的服务通过口口相传，在接下来的三四年里为我赚了很多披萨和平装本的钱。当我上大学时，我不得不停止为人们制作网站，因为我太忙于学习有关编程的新知识，比如人们为什么使用函数。</p><p>我想我在整个高中期间通过编程和修理坏掉的电脑赚的钱与很多人煎汉堡或打扫电影院赚的钱大致相同。当然，在我上大学之前，我当农场工人赚的钱比我靠电脑赚的钱还要多。我一直在努力为我所在地区的高级企业提供服务——银行、连锁店、大公司，这些公司似乎在与我生活的世界不同的世界中运作。我所缺少的实际上并不是编程能力。</p><p>回顾经验中的智慧，我缺少的是专业精神。我不知道如何在不骄傲的情况下表现出低调的自信，我也没有意识到我糟糕的个人表现对我的机会造成了多大的损害。总而言之，我的问题是寻求计算机专业知识的专业公司不喜欢与骑自行车的大汗淋漓的青少年进行现金交易，而“青少年”部分并不是这句话中的真正症结所在。</p><h2>三．</h2><p>这些确实是关于什么是专业精神以及它对一篇标题为“专业精神的危险”的文章有何帮助的大量文字。是什么赋予了？</p><p>有时添加一点专业化的暗示是简单而有用的。当我第一次接触<a href="https://getbootstrap.com/"><u>Bootstrap</u></a>时，我就被迷住了。这是一种快速、简单的方法，可以使我的网站看起来几乎与我不知道如何向其销售的公司制作的精美公司网站一样好。我没有浪费时间升级到更好的风格。问题只会在以后出现。</p><p>看，专业精神是一揽子交易。如果您看到一个带有尖角和稍微花哨的颜色的网站，那么当您拨打联系页面上的联系电话时，您不会指望有一个流畅且训练有素的声音来接电话。我曾经对那些只在中午营业的地方感到沮丧，因为我下班后无法给他们打电话。现在我意识到这不是一个错误，而是一个功能。有时是因为他们想与其他专业人士打交道。当我让我的网站看起来流畅和优美时，这意味着人们会在中午联系我，经常在课堂上或（稍后）在工作会议上看到我。</p><p>专业精神伴随着其他期望。</p><p>为了参加我最好朋友的单身派对，我们周末租了一间爱彼迎房源。我们熬夜，开大音量玩电子游戏，在后廊做一些烧烤，玩得很开心。它使用了很多与 2019 年和 2022 年东海岸理性主义者大型聚会类似的设置和规划；事实上，如果你以某种方式在我们为单身派对租用的地方外面设置一个地铁站，你就可以使用完全相同的场地来举办这两个活动。对于 2023 年的 Megameetup，我一直在尝试“升级”到更专业的设置。我们预订了酒店大楼，租用了活动空间，我希望能用名牌而不是贴纸和记号笔进行实际登记。这造成了一些期望的不匹配！</p><p>例如，酒店似乎对我在聚会前一个月没有提供完整的客人名单感到有点困惑，我只能试图解释说，是的，事实上我确实希望有一半以上的客人注册最后两周。我必须仔细考虑一下我们将在下午 5 点之后进入会议空间的想法，是的，甚至可能在晚上 7 点之后。 （我仍然不确定我是否理解了这个想法，如果我列出了出错的事情的墨菲术清单，那么“酒店预计我们 5 点离开会议空间”的位置仍然很高。）每当我遇到这些问题时，我都会情不自禁地想，如果我表现得不那么专业，他们是否也会如此困惑。</p><p>当酒店询问这次活动会是什么样时，我将其描述为学术会议和粉丝大会的结合体。这是真的！会发生很多专业的社交活动，我希望会有关于新颖研究的演讲或突破性论文的总结。我还预计至少会有一个人喝得酩酊大醉（好吧，公平地说，我听说在一些学术会议上也会发生这种情况），并且有人会在凌晨一点用原声吉他唱民歌。这不会发生在“专业”活动中！</p><p> （如果此时您认为值得双方同意不使用“事件”一词并描述他们期望发生的事情，那么……您可能是对的！现在您尝试随机选择一个会议酒店销售代表做这种奇怪的互联网事情，你称之为“禁忌你的话。这是可行的！我已经根据经验验证了这一点！但它肯定不是人们期望的那样。）</p><p>早在我第一次开始疯狂追求扩大东海岸大型游戏集会规模时，一位朋友漫不经心地建议去酒店而不是爱彼迎风格的场所，好像这很容易，他经常这样做。他刚刚联系了他们中的一些人，并表示这“更多的是出于好奇而不是任何事情”。与此同时，我不得不查找他用于启动该过程的缩写词。</p><p>或者看看我最近与一个我想从他那里购买电吉他的人的另一次对话。在谈话过程中，我提到我做了一些活动协调，他说他在该地区已经这样做了很多年，并问我是否会做他听说过的任何事情。我说社区中较大的活动有一百多人参加，本赛季我举办了其中两次。他呃，挠着下巴，说了些什么，大意是他不知道哪个场馆会在演奏一组非常甜美的强力和弦时为这么小的事情而烦恼。</p><p>这两个人都在各自的领域中表现出了一种随意、舒适的能力。他们见过事情出错，记住人们可能犯的明显错误，并且不需要强调小事情。这是我可以在软件中做的事情，也是我可以在其他一些领域做的事情，但在组织活动时我还不太习惯。我认为，在正确的时间和地点表明专业精神是有用的，因为你可以利用这一点。飞机飞行员基本上似乎只会讲同样的几个老套的、预制的笑话。我对此表示赞同。如果我登上飞机发现飞行员穿着小丑的杂色和猫耳朵，我会变得有点紧张，这并不是完全未经认可的，尽管他们的着装并没有改变他们的实际驾驶技能。</p><h2>四．</h2><p>如果你表现得很专业（在举止、审美、说话和外表等软技能上），人们会期望你达到专业标准（在你的专业水平上、在应对意外情况方面、在了解事情如何发生方面）。流程应该进行。）您可能不了解这些标准，这种无知可能很重要。专业精神的外表掩盖了你不知道自己在做什么的事实。这也阻碍了人们伸出援手。</p><p>如果我在朋友家吃晚饭，发现他们正忙着切蔬菜，而炉子上的东西开始冒烟，我通常会介入并翻转它或自己把火关小。这可以变成舒适地并肩工作，如果我足够了解他们，可以猜测他们的食物偏好或厨房的布局，我可能会根据口味给饭菜加香料，或者我们可能会一起即兴创作食谱。如果我在餐厅，我绝不会回到厨房并开始麻烦厨师进行更改！一般来说，即使我开始闻到一点烟味，我也会认为这是故意的，他们知道自己在做什么并且正在注意。事实可能并非如此！</p><p>流畅、精心设计的演示可能会阻止人们为您提供帮助。如果您希望业余爱好者不参与，这可能很好（外科手术室里没有临时志愿者的地方），但如果您希望人们加入，这尤其糟糕。卡拉 OK 酒吧或单口喜剧俱乐部有充分的理由不要显得太专业。我认为，开源项目的愚蠢名称在使其看起来更容易被接受方面做了一些很好的工作。还有理性聚会。 。 。</p><h2>五、</h2><p>大多数聚会在很大程度上取决于与会者的参与、支持、解决问题以及提供大部分内容和对话。我对东海岸理性主义者大型聚会的主要认识是，我不负责提供聚会的内容；我负责提供聚会的内容。我安排了场地，与会者一起创作了内容。我在 LessWrong 社区周末、Vibecamp 以及几乎所有其他大型理性主义活动中都看到过这种模式。</p><p>如果你正在组织类似的活动，并且你不小心发出了这样的信号，即某人必须如此团结、理性且出名才能做出贡献，那么你将会遇到困难。不仅你在闪电演讲中的发言者会减少，而且你会无意中切断了人们为举办自己的活动而攀登的阶梯的底部。你可以鼓励那些处于边缘的人挺身而出并加入进来，方法是在演讲中稍微不那么专业，稍微愚蠢一点和随意一点。</p><p>我的模糊猜测是，能够说自己举办专业级理性主义活动的人不到一百人。想必 CFAR 的一些人已经在这方面积累了经验，CEA 的一些人可能也算数，然后还有一大群已经从事该领域一段时间的运维人员，或者他们的非理性主义职业生涯为他们做好了异常良好的准备。它们很罕见，而且《LessWrong》的许多读者不会遇到它们，就像大多数听音乐的人不会面对面见到摇滚明星一样。</p><p> （这对于网站来说是完全不同的，我基本上会信任来自任何当地社区需要网络开发人员的最有信心的志愿者。我们绝对拥有这种技能。）</p><p>这篇文章的灵感部分来自于 Elizabeth 发表的关于各种 EA 组织的法律结构的<a href="https://www.lesswrong.com/posts/XvEJydHAHk6hjWQr5/ea-orgs-legal-structure-inhibits-risk-taking-and-information"><u>文章</u></a>。如果我们拥有丰富的法律和官僚技能，事情就不会是这样的。当你实际查看组织结构图时，它看起来很混乱且不专业。如果没有经验丰富的顾问的帮助，也许早熟的青少年在第一次解决问题时会做出这样的事情。<i>也许这就是实际发生的事情。</i>顺便说一句，我并不是想用“青少年”作为贬义词：特立独行、早熟的青少年很棒，当我上高中时，我经常遇到工作能力比我差的成年成年人。实际上这就是重点。</p><p>请注意，仅仅因为有人拿钱去做一件事，并且他们发出了所有正确的能力和轻松的信号，他们可能仍然不擅长做这件事。推论，如果你在这件事上表现不佳，就要小心你是否发出了能力和轻松的信号。</p><p> （哦，如果您在 2023 年 11 月或 12 月初阅读本文，东海岸理性主义大聚会将于 12 月 9 日周末在纽约市举行。如果那或纽约市世俗至日听起来像您喜欢的事情，考虑<a href="https://rationalistmegameetup.com/"><u>加入我们</u></a>！）</p><br/><br/> <a href="https://www.lesswrong.com/posts/QChwTjL6oL2a6gbhm/the-perils-of-professionalism#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QChwTjL6oL2a6gbhm/the-perils-of-professionalism<guid ispermalink="false"> QChwTjL6oL2a6gbhm</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Tue, 07 Nov 2023 00:07:33 GMT</pubDate> </item><item><title><![CDATA[How to (hopefully ethically) make money off of AGI]]></title><description><![CDATA[Published on November 6, 2023 11:35 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:14:09 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:14:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>嘿大家！</p><p>作为过去几周对话工作的一部分，我问了很多人他们最有兴趣阅读什么样的对话，最常见的一个是“我真的很想读一堆人们试图弄清楚如何构建一个在 AGI 变得更重要时能够顺利进行的投资组合”。</p><p>你们三个人在我的名单上排在相当靠前的位置，可以一起解决这个问题，所以我们就到这里了。不是因为你们是这方面的世界专家，而是因为我非常相信你们的一般推理（我不太了解诺亚，但非常信任威尔和兹维）。</p><p>我想，为了让我们开始，也许让我们先用 1-2 句话来介绍一下您的背景以及您之前对这件事的思考程度（以及您是否自己构建了相关的投资组合）。</p><p><i>另外，需要明确的是，本文中的任何内容均不构成投资建议或法律建议。</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:15:23 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:15:23 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>感谢奥利弗邀请参加！我的名字叫威尔·伊登（Will Eden），我的背景是经济、金融和生物技术，最近我一直是一名早期生物技术风险投资家，尽管我也努力思考如何管理广泛的公共资产组合。<br><br>如果我有一个粗略的论文/开场白，那么提前知道大多数资产将如何表现/链的哪些部分将积累大量价值可能是极其困难的。另一方面，我认为我们有一个可取之处，因为整体经济很可能会产生大量价值，甚至一些非常普遍和广泛类型的投资组合也可能会积累相当大的价值，即使不是最大可能的价值我们当然希望实现。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:16:44 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:16:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>我叫兹维·莫肖维茨。我写了很多关于经济学的文章，也思考了很多，并且在 Jane Street Capital 工作了 2.5 年。我花了很多时间思考这些问题，但我也有意尽力避免在投资组合优化或交易上花费太多时间，这样它就不会占据我的整个大脑。</p><p>我还写了一篇关于这一点的文章，名为<a href="https://thezvi.substack.com/p/on-ai-and-interest-rates">“论人工智能和利率”</a> 。</p><p> （此外，我知道，我可以预先声明，我在这里所说的一切都不是投资建议或任何其他建议！）</p><p>在这种情况下，我的一般方法是，如果你的论文是随机选择的，你想要寻找不会太昂贵（以 EV 术语计算）的投资/赌博，或者无论如何都是好主意，但是如果你的论文被证明是正确的。支付保费有时间和地点，但必须谨慎选择。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:18:50 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:18:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>谢谢你的邀请！我很荣幸来到这里。我叫 Noah Kreutter，我的背景是拥有大约 4 年的量化金融经验，包括<a href="https://www.imc.com/us/">IMC</a>和（即将成立的）Bridgewater。我混合了波动性交易和系统性多资产宏观分析。至少从 2015 年起，我就一直在关注 LW/Rationalist/SSC 生态系统，但主要是在外围。我所说的都不是财务建议，包括任何听起来像财务建议的内容。<br><br>我的基本观点是，在 AGI 缓慢起飞的过程中——实际上，在缓慢起飞的情况下，你的投资方式可能很重要——你想要构建一个长期增长的投资组合，尤其是那些对人工智能有特殊暴露、长期波动、长期利率的股票-going-up（做空债券），做多“2023年便宜的房地产”。您可能希望避免购买由强大的知识型劳动力市场（例如纽约市）支持的房地产。<br><br>此外，对于大多数读者来说，我认为职业资本是他们最重要的资产。 AGI 的一个后果是贴现率应该很高，而且你不一定能拥有很长的职业生涯。因此，那些处于读研究生的边缘的人绝对应该避免它。<br><br>我目前的投资组合是单一名称股票、指数的长期看涨期权、一些特定股票（例如 MSFT）的长期看涨期权以及短期长期债券的组合。我也持有大量现金。如果我能在美国一些不那么热闹的地区轻松获得廉价抵押贷款，我可能会这么做，但从逻辑上讲，这很烦人。</p><p>我要提供的另一条一般性建议——这与“持有现金”和“通过期权获得一些股权贝塔”相吻合——是“保留选择性”。未来十年，广义上的灵活性的价值可能会很高。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:22:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:22:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>“通过期权获得一些股权贝塔值”</p></blockquote><p>啊，是的，我也喜欢通过期权获得一些股权测试版：P</p><p>我想我也许可以通过谷歌搜索来解读这意味着什么，但是你能详细说明一下吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:36:23 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:36:23 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我所说的“通过看涨期权获得股票贝塔值”是指使用看涨期权（以预先指定的价格购买股票或指数的权利，但不是义务）作为直接持有股票的部分替代品。这个想法是，看涨期权有一个<i>Delta</i> （某些定价公式相对于标的资产价格的导数），这告诉您在标的资产价格变动 1 美元的情况下，期权的价格应该变化多少。<br><br>对于 Zvi 来说，我不会交易任何基于人工智能论文的短期期权。但一般来说，如果您购买几年后到期的看涨期权，税务问题和交易成本问题就不那么严重，因为这段时间足以获得长期资本利得税待遇。这样做的原因是便宜、无追索权杠杆（你可以用很少的钱获得相当多的股票敞口），以及如果你认为期权市场错误定价了波动性，即低估了股价的反弹幅度，那么你会获得正的预期价值- 我认为是这样。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:22:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:22:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>对于未来的读者，一些尝试解释诺亚对未开悟的凡人所说的话：</p><p><strong>我不会根据人工智能论文来交易短期期权：</strong> “我认为购买只有在不久的将来某些股票价格发生变化时才能支付的金融工具是一个坏主意。大概是因为价格变动的时机非常棘手，甚至股票市场的重大变化可能很难把握（请记住，尽管大流行对股价产生了明显的巨大影响，但要把握 2020 年大流行股票走势的时间是很困难的），而且因为这种工具的高波动性意味着风险厌恶者-当事人经常要求您支付高额的额外保费”</p><p><strong>如果您购买几年后到期的看涨期权，那么这段时间足以获得长期资本利得税待遇：</strong> “在美国，当您持有金融工具不到一年时，您要缴纳的税款要高得多（短期资本利得与长期资本利得）。长期资本收益）。这意味着，如果您想（通过期权）押注价格变动，那么押注至少一年内的价格变动是有利的税收优惠。</p><p><strong>这样做的原因是廉价的、无追索权的杠杆（你可以用很少的钱获得相当多的股票敞口）：“</strong>如果你押注于长期价格变动而不是仅仅以这种方式持有股票，你可以捕捉到很多上涨空间不会占用大量资金来持有股票（高杠杆），也不会冒着您的不相关资产被清算和占有的风险（无追索权）”</p></div></section><h2> AGI 的广泛市场影响</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:20:28 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:20:28 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>基本上在任何通用人工智能情景下，经济都将开始加速并非常迅速地增长。一些资产密切反映了经济的增长率，特别是实际利率，而其他资产则是不完美的替代品，例如公共股票（股票）。值得注意的是，目前几家领先的人工智能公司（例如OpenAI、Anthropic等）由于是私人控股，因此无法被公众投资，因此很难准确、精准地投资人工智能场景。我的观点是，虽然获得完美的曝光是很困难的，而且任何投资组合都将是一个不完美的代理，但快速加速的增长将意味着大量价值在意想不到的地方创造并以不同的方式捕获——例如那些蓬勃发展的投资组合之一私人公司被上市公司收购，您现在可以投资这些公司 - 这样，“投资指数基金”的无聊旧策略仍然可能从 AGI 中获取大部分/大部分价值:) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:21:05 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:21:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>至于我目前的投资组合，我有不同的组合，其中包括我期望从人工智能中受益的个股（例如MSFT和GOOG），以及包括其他个股在内的各种其他投资，还有非常优惠的固定利率抵押贷款 -这是一个交易的例子，无论如何，交易都很好，但人工智能却做得更好。</p><p>我同意 NoahK 的观点，即保留选择性是一个好主意。您应该将流动性不足视为比平常花费更多的溢价。</p><p>总的来说，我认为“构建完美的投资组合”是不值得的，除非你重视智力锻炼。没有足够的阿尔法来使其完全正确，并且在考虑再平衡等问题时，税收考虑通常占主导地位。你要确保自己的方向是正确的并表达你的意见，但不要太疯狂。</p><p>我强烈同意 Will 的观点，加速 AGI 将在不同的地方创造大量价值，因此广泛的生产性资产或至少其中一部分可能会升值，因此可以合理地预测 SPY (S&amp;P 500 ETF)会做得很好。人们担心的一个问题是，利率上升对股价影响不大，因此您需要考虑是否覆盖这一基础。</p><p>对于期权之类的东西，您需要支付溢价，因为您在交易它们时必须跨越更大的价差，担心市场结构中的各种边缘情况，然后面临税收影响。在某些重点领域（想想 2020 年 2 月），这显然是正确的举措，但我会犹豫是否将它们用于人工智能，除非你预计事情会很快升级。</p></div></section><h2> AGI 世界中的职业资本</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:47:07 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:47:07 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>此外，对于大多数读者来说，我认为职业资本是他们最重要的资产。</p></blockquote><p>诺亚提出的这一点值得考虑，尽管我认为我们需要诚实地了解我们拥有哪些技能/我们可以发展哪些技能。在一个完整的通用人工智能场景中，人工智能在所有技能上都超越了人类，即使是最好的程序员等似乎也不太可能获得可观的回报。在那段时期之前，仍然存在一个悬而未决的问题：哪些工作岗位到底以什么顺序被取代。</p><p>例如，当艺术首先实现自动化时，每个人似乎都感到非常惊讶 - 人们总是认为创造性任务将是最后完成的任务！ （如果我们对人类创造的艺术给予巨大的溢价，这仍然可能是正确的。）似乎合理的是，任何直接致力于改进人工智能的人仍然可以在多年内赚取巨额溢价，因此依靠这种职业/人力资本似乎是合理的一个好的策略。但如果你期望 1) AGI 很快到来，2) 许多/大多数工作被取代，那么我认为人们不应该假设他们在未来一段时间内能够赚取任何收入。 （其社会影响是巨大的，预计 AGI 征税 + 全民基本收入、大规模慈善事业等等 - 但要点是成立的。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:47:09 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:47:09 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>完全同意威尔的观点，即<i>一旦 AGI 出现，大多数人的职业生涯不一定有价值。</i>我认为这是一个争论，试图通过你的职业生涯来赚钱，而不是做那些据称可以建立<s>人类</s>职业资本但需要一段时间才能获得回报的事情。做量化交易而不是咨询，不要去读研究生，尝试尽可能多地预载收益。 （编辑：就 Zvi 的观点而言，人力资本和职业资本是不同的。我实际上只是在这里谈论职业资本 - 更广泛的社会关系和声誉在许多未来可能非常重要）。</p><p>我确实预计 AGI 的<i>过渡期</i>将为那些拥有该技能的人创造大量高价值的创业机会。尚不清楚事物需要多长时间才能达到新的平衡。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:47:15 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:47:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>到目前为止，我还没有看到一个值得考虑的观点，即在哪些世界中，拥有金钱对你来说价值高而不是低？</p><p>极端的情况下，在末日降临、所有人都会死去的世界里，带着最多的玩具死去也还是死了。或者发生政权更迭、革命、没收性税收制度或其他变革，旧资源不再具有意义。或者，如果我们以某种方式进入后稀缺的乌托邦状态，也许你并不需要钱。</p><p>而在其他情况下，在正确的时间将资金放在正确的地方可能会产生巨大的影响 - 我猜这通常正是利率非常高的情况。或者那些没有资本的人会被抛在后面的世界。因此，您希望在财富有价值的世界中进行交易和投资，并在财富不那么有价值时少担心。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:49:26 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:49:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>是的，你必须假设事情会变得奇怪，但又不会太奇怪。实际上不可能对冲世界末日或全球革命，因此在对资产定价（或多或少）时，您可以忽略这些世界状态。</p><p>如果您期望自己的行为实际上对世界状况产生有意义的影响，那么情况就不那么正确了。就像如果你认为有一天，如果你有足够的钱，世界末日是可以避免的，那么你也许应该以不同于那些相信自己只是顺水推舟的人的方式进行投资。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:51:13 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:51:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我认为，只要你预计利率会<i>随着经济增长</i>而上升，这并不能真正成为看跌股票的理由。在最坏的情况下，“不会像你所获得的增长预期那样乐观”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:52:00 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:52:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>职业资本是人力资本或社会资本的一种形式。从广义上讲，此类资产确实占大多数人投资组合的很大一部分。我宁愿“富有”，因为拥有自己的声誉、人脉、家庭和技能，而不是“富有”，因为拥有自己的投资组合，无论从何种意义上说。这是不要过分关注具体的投资组合配置本身，而更多地担心建立正确的人力和社会资本的原因之一。</p><p>是的，如果您认为转型的时间相对较短，那么也应该考虑对有价值的内容的影响。我当然不会计划诸如“终身教职”或其他多年都没有回报的长期计划。</p><p>我特别警告不要寻找安全或正常的幻觉——如果一切都保持正常，你就已经做好了的想法不应该带来太多安慰。但另一方面，如果事情保持正常的时间比你预期的要长，你也不想意味着你完蛋了。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:54:26 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:54:26 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>到目前为止，我还没有看到一个值得考虑的观点，即在哪些世界中，拥有金钱对你来说价值高而不是低？</p></blockquote><p>我非常同意这一点，并且我看到了一些你的财富禀赋很重要的情况。一个世界是有明确而紧迫的方式花钱来直接改善结果，例如在人工智能安全或其他方面投入巨资。我怀疑这就是大多数 EA 考虑这个问题的原因。</p><p>另一个世界更像是一个缓慢起飞的埃姆斯时代类型的场景，其中所有形式的人类都没有完全过时，也许有一个最小的社会安全网，因为那时它会非常便宜，但如果你想要对人类的未来产生任何有意义的重大影响，这绝对取决于您可以聚集哪些资源，例如获得计算能力来运行更多自己的副本。我绝对认为这是一个潜在的未来的非零可能性！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:54:43 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:54:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><blockquote><p>另一个世界更像是一个缓慢起飞的埃姆斯时代类型的场景，其中所有形式的人类都没有完全过时，也许有一个最小的社会安全网，因为那时它会非常便宜，但如果你想要对人类的未来产生任何有意义的重大影响，这绝对取决于您可以聚集哪些资源，例如获得计算能力来运行更多自己的副本。</p></blockquote><p>不管它的价值如何，这是我个人认为最合理、最值得思考/对冲的奇怪的未来。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:55:01 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:55:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><blockquote><p>最终，我仍然预计增长将如此之大，将超过利率对股市的拖累。</p></blockquote><p>是的，在这些情况下我也这样做，但我想指出，如果特定股票/行业/等“错过”爆炸性增长，那么它们的价值而不是保持静态可能会下降相当多。</p><p>我还必须假设很多企业都会受到干扰，而且会很困难。会有失败者。我在国内投资时喜欢个股的很多原因是，虽然我不知道我是否能够可靠地挑选出赢家，但我确实知道我可以识别出我想避免的输家。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:55:25 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:55:25 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>有人在二月份做空<a href="https://www.google.com/finance/quote/CHGG:NYSE?sa=X&amp;ved=2ahUKEwjHrLuCtaGCAxWNGTQIHUDKBPsQ3ecFegQINBAh">Chegg</a>吗？我认为至少在未来几年内会有很多这样的机会。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:56:23 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:56:23 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>实际上不可能对冲世界末日或全球革命，因此在对资产定价（或多或少）时，您可以忽略这些世界状态。</p></blockquote><p>这里有一个简短的分歧——确实，你无法对冲诸如人工智能全面接管/x风险之类的事情，但你当然可以对冲诸如革命或灾难性风险之类的事情。就我个人而言，我认为明智的做法是考虑可管理的下行情况，并将投资组合的一小部分分配给稳健性，从“储存食物和水”等便宜的东西到“隐藏在某个地方但您可以访问的实物金银币”等更昂贵的东西他们在紧急情况下” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:58:34 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:58:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>同意威尔关于对冲一些革命的观点——如果你想有意地做到这一点并为此支付溢价，并且拥有足够的财富，那么你就可以做到这一点，这是一个非常合理的游戏。唉，人工智能引发的此类事情不太可能与这种举动很好地配合，正如奥特曼在谈到他的掩体时所指出的那样。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:59:47 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:59:47 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>是的，我同意你可以（在某种程度上）对冲历史上的革命。我有点怀疑，如果人工智能发展得很糟糕——或者最终变成共产主义——那么在地下储存黄金是否会产生重大影响（如果有的话）。但我认为这并不重要。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:00:43 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:00:43 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>我想指出的是，如果特定股票/行业/等“错过”爆炸性增长，那么它们的价值而不是保持静态可能会大幅下降。</p></blockquote><p>是的，这就是为什么在我的第一篇文章中，我建议“拥有指数基金”之类的东西 - 这将自动将更多资金重新平衡为赢家，而输家则被减持直至为零。如果你没有一个<i>非常</i>有力的论点来说明哪些整个行业会受益，哪些行业不会受益，那么除了“一般股票”之外，你似乎不应该押注于任何其他东西</p><p>我也很乐意在这里对<i>可能</i>表现不佳/表现不佳的因素进行一些猜测。例如，我认为原材料和精炼将在短期内成为一个巨大的瓶颈，这是一个相当不错的赌注。为了进入快速增长，在我们获得大量新供应之前（我们会吗？法规怎么样？？），原材料似乎真的可以升值......</p></div></section><h2> AGI 的债务和利率影响</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:32:28 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:32:28 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>让我们重点关注两个最广泛的资产类别：债务和股权。债务（通常）是固定的、预先已知的付款，该资产的收益率由现行利率加上不确定性等决定。股本是偿还债务后的“剩余”债权，它允许无限的上涨空间，但也它是在所有债务之后得到偿还的，因此如果企业表现不佳，股权方将一无所获。<br><br>高增长通常意味着大量有利可图的投资机会。当它用于支付固定资本投资之类的东西时，债务是最有用的 - 你大致知道你需要多少钱，你需要建造什么，你可以将资本作为贷款的抵押品（这样，如果项目失败，债务就会得到偿还） ），您可以很好地猜测这项投资随着时间的推移会带来多少回报。快速增长（并且实际上是全新的）AGI 经济将需要大量此类投资，因此都会有很好的投资机会<i>，</i>这意味着随着有限的当前资源追求最好的项目，金融资本将变得极其稀缺。因此，我们可以合理地假设利率将会大幅增长。利率（收益率）的上升意味着<i>现有</i>债务将损失大量价值，因此在这个增长时期持有大量当前债务/债券似乎存在风险。这也意味着手头有现金可以让你的钱以极快的速度增长。<br><br>这表明，只要您想要投资组合中的现金/债券部分，您就希望将其作为现金投资于极短期证券（货币市场基金、国库券等）<br><br> （一个或许不太明显的含义是，这可能会使<i>政府</i>为自己融资变得更加困难，因为他们还将面临极高的现行利率。他们最好的选择可能是提高税收并试图平衡预算，这希望在不断增长的经济中能够产生大量收入……但如果他们仍然依赖债务融资，这可能会变得非常困难。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:39:03 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:39:03 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我非常喜欢威尔的观点，即政府在利率高得多的环境下可能难以为自己融资。看起来这——加上不平等的扩大——可能会导致税收大幅增加。这可能包括以前闻所未闻的事情，例如对未实现收益征税或财富税。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:41:17 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:41:17 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>人们担心的一个问题是，利率上升对股价影响不大，因此您需要考虑是否覆盖这一基础。</p></blockquote><p>值得考虑的是利率影响股价的渠道。只要有更好的固定投资，获得已知的大回报比不确定的回报更有吸引力。但如果我不得不猜测的话，投资回报将（至少在最初）大大超过现行利率，因此股票的增长速度将大大快于债券。大多数人会看到像债券这样的东西每年支付 50%，并注意到股票市场翻倍以上，然后大量买入股票。我认为机会成本效应不足以阻止这里的人们。<br><br>另一个渠道是未来现金流量的贴现，例如较高的利率意味着未来支付的价值低于当前支付的价值。从严格的估值意义上来说确实如此，但同样，除了贴现率之外，您还需要对增长率进行定价。另外，在这样一个快速增长的环境中，即使您将未来 10 年以上的东西折算成基本上为 0，仅短期支付就足以证明较高的估值是合理的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:47:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:47:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>威尔：我认为我未能正确遵循股票与债券的权衡。让我试着总结一下我认为你在说什么：</p><blockquote><p>当 AGI 发生时，可能会出现很大的增长，因此人们会想要进行大量投资，这意味着人们会希望他们发放的任何贷款都获得高利率，因为他们希望这些贷款的利息至少与其他贷款相匹配投资的回报。</p></blockquote><p>这部分对我来说很有意义。从这个意义上讲，利率似乎会上升。</p><p>但我想我无法理解这对持有股票和持有债券有何不同影响。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:50:07 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:50:07 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>但我想我无法理解这对持有股票和持有债券有何不同影响。</p></blockquote><p>别担心，您并不是唯一一个不清楚利率如何影响股票价格的人。看起来<i>应该有</i>一种机械关系，但又不确定。我是在回应兹维的观点，即利率上升也可能影响股市。最终，我仍然预计增长将如此之大，将超过利率对股市的拖累。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:42:34 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:42:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>同意威尔的观点，即利率可能会大幅上升，因此当人工智能起飞时，你非常不想持有债务，因为如果利率上升，现有债务将损失很多价值。持有类似于现金的东西来保持选择性是可以的，但你需要使用短期工具来做到这一点。这是我实际上付出努力的一个地方 - 我一直不得不解释我想要浮动利率，当我说我不想用我的现金冒险时，我的意思是我担心利率会上升比下来。</p><p>更进一步来说，在这种情况下以固定利率持有长期债务（例如长期抵押贷款）是令人惊奇的。作为一家公司，您希望发行债券等。</p><p>对于政府来说，正如最近著名的问题一样， <a href="https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century">R>;G 吗</a>？如果实际增长非常高，那么实际利率也可以很高。另请注意，政府只会在债券到期时缓慢进行再融资，因此，如果事态迅速升级，即使新债务利率超过增长率，其债务利率也可能远远落后于增长率，从而减少了再融资的必要性。提高收入。我们应该预期主要的基本盈余会自行发生。</p></div></section><h2>具体示例组合</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:05:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:05:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，我想我想退后一步，谈谈更多的实际情况。</p><p>在这篇文章“<a href="https://www.lesswrong.com/posts/jvHLBEXXEtZtt4KFP/investing-for-a-world-transformed-by-ai">投资人工智能改变的世界</a>”中，Peter McCluskey 有一组相对具体的公司，他认为如果世界被人工智能改变，这些公司将表现良好。提取最相关的建议：</p><blockquote><p>人工智能公司：谷歌、[...]OpenAI、</p><p> [...]</p><p>我目前的半导体相关投资是（按字母顺序排列）：AMKR、AOSL、ASML、ASYS、KLAC、MTRN、LSE:SMSN、TRT。</p><p> [...]</p><p> AMZN、MSFT 和 GOOGL 可能会从数据中心的增长中获益。我想我会在 2023 年的某个时候扩大我的 GOOGL 持股规模，并买入 AMZN 和 MSFT 的小额头寸，当时我看到有迹象表明它们的股票下跌势头已经消散。</p><p> [...]</p><p>我最大的两个太阳能赌注是 CSIQ 和 SCIA。我在 JKS、DQ、SEHK:1799 持有较小的仓位。</p><p>对于电网基础设施和太阳能发电场建设，我在 MTZ、MYRG、PLPC 和 PRIM 担任职位。</p></blockquote><p>这些肯定是很多股票代码，所以我认为我们不应该研究其中的每一个，但我有点好奇你是否看过这个投资组合，它对你来说似乎有点理智。</p><p>然后我想更具体地讨论一下，如果你想在这上面花费最多 15 个小时，但最终仍然得到一个看起来能完成任务的作品集，那么你实际上会具体做什么。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:06:34 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:06:34 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Habryka 的投资组合对我来说似乎足够合理。我认为省略 TSM 是一个有影响力的决定（我不同意这个决定），但除此之外，它对于股票代码列表来说看起来很可靠。<br><br>如果我要自己列出值得持有大量头寸的股票清单，我会说 TSM、MSFT、GOOG、AMZN、ASML、NVDA 是一个很好的起点。您还可以查看能源生产商和原材料开采/精炼。尽管对于这类公司来说，这是一个竞争更加激烈的市场，并且进一步多元化是有意义的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:08:23 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:08:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>该投资组合不包括权重，并且有一堆[...]，我不知道其中很多公司是什么，也不知道它们是否看起来很便宜，但无论如何看起来并不疯狂。我当然拥有 AMZN/MSFT/GOOGL，尽管我并不想把握时机。出于其他原因，太阳能似乎也是一个不错的地方，我在那里有一些接触，但我还没有调查这些公司。</p><p>如果人们从头开始进行工作和构建，那么半导体似乎是一个好主意。但人们也不需要太仔细地对冲人工智能的哪一个角度最终将获得最大利润——瞄准你能看到最大利润的地方似乎是合理的，而不是试图太顺利。</p><p>是的，NVDA 看起来很便宜。有点疯狂的是，由于强劲的盈利增长，它从峰值大幅下跌。</p><p>由于地缘政治风险，我远离 TSM，即使它的定价合理，我也不想要它。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:09:52 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:09:52 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>对于 Zvi 关于 TSM 的地缘政治风险的观点，TSM 是我的敞口中几乎完全看涨期权的股票之一。我认为，如果您对交易期权感到满意——而很多人对此感到不舒服，这是合理的——31% 的隐含波动率是相当便宜的。据我所知，TSM 没有很好的替代品。我认为它在全球供应链中发挥着非常独特的作用。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:10:33 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:10:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>是的，如果有人愿意使用看涨期权，TSM 似乎是部署看涨期权的绝佳场所。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:12:09 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:12:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，让我们尝试让事情变得更简单。假设您是一位 28 岁的男性，有几十万到几百万的投资资金，没有设立任何经纪账户。你会做哪些实际具体的事情，不会导致你不小心按错按钮，然后以某种方式损失所有的钱？</p><p>一些对此做出回应的链接似乎也不错，尽管我确实觉得这一步可能会让一些理性的人陷入困境。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:12:02 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:12:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我所说的都不是财务建议，但这是：在点击经纪账户中的任何按钮之前要非常小心，并确保您了解它们的作用。</p><p>更严重的是，对于那些对这种对话有点技术性的人来说，应该避免交易期权或使用过度的杠杆。如果你没有真正的知识基础并且没有密切关注，很容易搞砸。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:13:43 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:13:43 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>然后我想更具体地讨论一下，如果你想在这上面花费最多 15 个小时，但最终仍然得到一个看起来能完成任务的作品集，那么你实际上会具体做什么。</p></blockquote><p>第一个近似值是：要么100%持有股票，要么保留一些股票和一些现金，其中现金部分投资于短期利率（货币市场、国库券等）并定期重新平衡。<br><br>最不可知的就是全球股票指数。 Vanguard 有一只 ETF，股票代码为 VT，代表全球所有股票（按市值加权）。长期以来，全球股票的表现一直落后于美国股票（我们这里有<i>很多</i>科技股），因此，如果您是成长型投资者而不是价值投资者，那么您可能需要美国总指数 (VTI) 或仅关注标准普尔 500 指数上限指数（SPY）。<br><br>下一层是花一些时间思考哪些领域可能在短期内表现出色。考虑到增长的预期增长以及这种收益可能出现在哪里的不确定性，我建议只做多，<i>而不是</i>试图做空任何特定领域。我上面提到了原材料，您可以通过 MXI 获得全球原材料 ETF，或者通过 IYM 获得仅限美国的原材料。考虑到我们已经观察到的情况，半导体似乎也是另一个明确的买入点（尽管它现在可能被高估，并且在这波浪潮之后，其价值将在其他地方捕获！），并且股票代码 SMH 是一种广泛的半导体 ETF。您可以在列表中查看您看好的任何内容，找到该子行业的 ETF，几乎所有子行业都有一个 ETF，例如，对于太阳能，您可以购买 TAN。 （请注意，这些只是常见的符号，您可能需要这些基金的其他变体，我并不是特别推荐这些）<br><br>投资个人名字似乎风险非常大，除非你非常密切地监控这一点...... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:14:50 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:14:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p><a href="https://www.lesswrong.com/users/habryka4?mention=user">@habryka，</a>我会告诉这样的人持有 50-100% 的股票，主要是指数，对 MSFT 进行少量特殊分配，如果他们愿意的话，可以做空长期债券。如果他们不这样做，那么就持有他们不投资的现金/短期票据。</p><p>如果他们住在威斯康星州或类似的地方，就可以获得抵押贷款。并且不要去读研究生。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:16:49 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:16:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>顺便说一句，看涨期权的另一个好处是您可以锁定利率。如果您通过现货借贷获得杠杆，您需要注意经纪人收取的利率，因为它可能会上涨很多。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:17:15 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:17:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>是的，让我回应 NoahK：所有这些关于选项的讨论都需要考虑到，如果您很容易点击错误的按钮，将事情的大小放大到您认为正在做的 10 倍或 100 倍，或者以其他方式陷入大麻烦，涉足期权、期货或其他类似事物。确保您确切地知道自己在做什么，根据需要与人交谈以确认等等，并且当有疑问时坚持基本知识。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:17:47 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:17:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>更严重的是，对于那些对这种对话有点技术性的人来说，应该避免交易期权或使用过度的杠杆。</p></blockquote><p>简单来说一下杠杆问题：在高利率环境下，这可能会变得非常疯狂。杠杆是通过保证金贷款融资的，这些贷款几乎总是最短期利率+一些溢价。因此，如果利率已经达到 50% 或其他水平，那么您每年都会使用 2 倍保证金损失一半的价值。如果股市飙升，这当然是一件好事，但一般来说，股票不仅朝一个方向移动，而且你可能会因高利率和横向突然大幅下跌的组合而完全被消灭。仅举一个具体场景：假设有一个新的人工智能驱动的交易机器人（或一百万个），我们立即遭遇 75% 的闪崩或其他情况。这样做的每个人不仅会被清算，而且还会背负着经纪公司的巨额债务。我认为只有在世界“相对正常”的状态下，杠杆才是一种出色的策略。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:17:43 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:17:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>嗯，所以我觉得这个建议太保守了。就像，我至少似乎对人工智能将会有多大的影响有一些信念，但与市场信念有很大不同（我认为，尽管这种事情很难说）。我觉得我想用大约 20%-40% 左右的投资组合对这些信念进行集中押注，而且我觉得仅持有一些非常广泛的指数基金是无法实现这一点的，这确实是“最大限度地服从市场”的选项。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:18:59 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:18:59 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我喜欢“做空债券并对 MSFT、GOOG、NVDA 等进行特殊投资”。这取决于28岁的人想要承担多大的风险。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:20:16 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:20:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>我喜欢“做空债券并对 MSFT、GOOG、NVDA 等进行特殊投资”。</p></blockquote><p>当你所处的世界已经发生起飞并且每个人都意识到这一点并且增长率和利率都已经飙升时，做空债券是一个很好的策略。就我个人而言，<i>在当前环境下</i>我不会做空债券，但当然 YMMV </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:20:22 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:20:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>理想情况下，应该存在人工智能指数基金或我可以购买的东西来平衡我的情况，但我认为这不存在？而且我也不介意承担一些风险。如果我的投资组合中的这一部分能够正确捕捉到其他世界的优势，那么在大约 50% 的世界中接近于零似乎很好。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:21:17 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:21:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>我与威尔的最大不同之处在于，我不害怕购买个股，这在许多情况下也为您提供了额外的与税收相关的选择权。你不希望过于集中于任何一只股票，但 28 岁的人没有/不应该有一定程度的风险厌恶，例如 10% GOOG 似乎不合理，当然 5% 就可以了，如果你认为这就是价值所在。</p><p>至少，如果你预计人工智能会发生重大变化，你就会想要在一定程度上和明显程度上超重与人工智能相关的东西。如果不出意外的话，也有此类领域的 ETF，但大多数股票都是显而易见的，我会关注这些股票。再说一次，这里的小错误确实是可以预料到的。</p><p>做空债券又是一个更高级的举措。看起来，如果这样做的溢价不是太大，它就会产生阿尔法，但它肯定会适得其反——利率在上升之前下降几年对我来说似乎不太可能。最坏的情况是你因为太早而赔钱，然后你就对了，但你没有资本从晚对中获利。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:22:06 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:22:06 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我认为目前的收益率曲线是平坦的，在我们完成所有工作之前它将向上倾斜。我认为鲍威尔不会降息。但这离人工智能还很远。</p><p>在我看来，最好的风险调整交易可能是买入 10 年期债券和卖出 20 年期债券，比例不超过 2:1。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:23:09 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:23:09 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>理想情况下，应该存在人工智能指数基金或我可以购买的东西来平衡我的情况，但我认为这不存在？</p></blockquote><p>哦，它们绝对存在！不要低估这些大基金经理的营销能力哈哈<br><br>iShares 有一份 IRBO 门票。让我们看看它包含什么...看起来浓度非常低（均&lt;2%），但顶级名称是...Faraday、Meitu、Alchip、Splunk、Microstrategy。 （？？？） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:23:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:23:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>非常简单的问题：像我这样的人实际上应该使用什么经纪服务？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:23:40 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:23:40 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>如果您知道自己在做什么，那么盈透证券就是最好的经纪公司。他们的用户界面很糟糕，但保证金率却是最好的。当你借现金时，其他经纪人会撕破你的脸。</p><p>如果你不想学习使用糟糕的 UI，你可以使用 Fidelity 或 Schwab 或我听说的东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:23:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:23:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>他们的用户界面很糟糕，但保证金率却是最好的。</p></blockquote><p>哈哈，当我担心按错按钮会损失所有钱时，我觉得这是错误的权衡。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:29:26 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:29:26 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>我和威尔最大的不同是我不害怕购买个股</p></blockquote><p>我愿意购买个股，但在极度不确定的情况下，我很难向其他人推荐特定的股票！正如 Oliver 所规定的那样，研究时间少于 15 小时的普通投资者如何知道他们所做的事情比购买广泛指数具有积极的预期价值？事实上，任何大赢家都会被广泛的指数基金捕获。<br><br>我确实认为，对于年轻的、追求风险的、有时间的人来说，考虑一下这个问题并选择个股是可以的。如果没有别的事，你会通过这样做<i>学到</i>很多东西，无论是关于市场还是关于你的心理，而且一旦你参与其中，它可以激励你更多地了解相关公司。也就是说，我永远不会让你的指数基金风险敞口降到0而转而投资个股，只是为了避免你没有抓住任何最终赢家的破产风险。</p><p>需要明确的是：我什至不确定大部分/全部价值是否会被一家大型企业所捕获。可能有数千家公司每家都创造一万亿美元的价值或其他什么。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:30:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:30:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>（这里有一个小轶事，不久前，我试图弄清楚如果你盲目遵循 LessWrong 上投票的所有股票建议会发生什么。所以我在一些我发现并维护的随机网站上建立了一个虚拟交易投资组合几个月了。我持有的最大头寸之一是根据当时一些被点赞的帖子做空 Nikola。几个月内，由于做空进展顺利，该头寸已增长到我投资组合的 60% 以上，并且升值了300%左右。</p><p>然后我停止了两个月的检查，然后当我再次检查时，突然整个投资变成了0......</p><p>事实证明，即使您的期权是绿色的，虚拟投资组合网站也不会自动执行您的期权，因此它们已经过期并变得毫无价值。</p><p>这就是为什么我害怕点击错误的按钮并损失所有钱的原因之一） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:31:11 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:31:11 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>哦，不，那太不幸了！是的，如果您购买期权，我建议您在购买期权时设置警报，该警报将在到期前一周发出。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:38:18 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:38:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>就个股而言，我认为，特别是如果您了解相关公司，而且总体而言，您不需要在每家公司上花 15 个小时就知道它可能是一家真正的公司，以合理的价格生产具有良好前景的真实产品，或者市盈率，如果你购买指数基金，你无论如何都会购买该股票。只要你不以这种方式在任何一只股票上买得太大，似乎……没问题。我从来没有花15个小时看一只股票。</p></div></section><h2>这些是否符合道德或促进理智？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:31:02 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:31:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，我想我想谈谈另一个相关的话题，比如： <strong>“好吧，但是以这种方式投资道德吗？另外，它会扰乱你对这些事情进行理智思考并尝试放慢速度的能力吗？”人工智能什么时候会直接损害你的底线？”</strong> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:34:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:34:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>就像，我一直在试图<a href="https://www.lesswrong.com/posts/Be3ertyJfwDdQucdd/how-should-turntrout-handle-his-deepmind-equity-situation">说服在 AGI 公司工作的一些朋友以某种方式摆脱他们的股权</a>，因为我确实认为在这种程度的暴露下，它会严重影响你清晰思考的能力（不像AGI公司的社会环境，但我认为还是不错的）。</p><p>我认为在你的投资组合中持有谷歌或英伟达的很大一部分可能会产生类似的效果，尽管我的猜测是，当你变得更加分散时，效果会变得更弱。但我认识的许多人都在积极倡导相当激进的政府干预措施，这确实让人感觉可能会严重损害回报，而且即使在相当广泛的层面上，也可能会产生令人惊讶的巨大影响。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:36:09 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:36:09 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我认为对于绝大多数投资者来说，你的投资组合选择不会以任何有意义的方式影响世界。 （尽管它们可能会影响<i>您</i>思考事物和评估风险的方式。）</p><p>如果你是亿万富翁，是的，我会鼓励你考虑人工智能投资的社会福利方面。如果这些公司面临更高的资本成本，那就太好了，而购买股票可以降低你购买的公司的资本成本。但在我看来，如果你是一名 28 岁的科技工作者，就必须采取措施保护自己。在这个水平上，通过你的投资伤害世界的能力是相当有限的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:36:14 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:36:14 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>好吧，但是这种投资方式道德吗？</p></blockquote><p>我同意，如果你向一家新的 AGI 初创公司提供种子投资，而这家公司拥有所有能力，并且蔑视任何安全理念，那么这似乎更值得怀疑。但我的指数基金方法的优点之一是，您可以从下游经济收益中受益，而无需偏袒任何特定参与者。</p><p>二级市场投资（购买现有股票，而不是公司发行新股来筹集资金）与如何推动公司利益之间也存在非常松散的而非零的联系。如果一家公司定期发行股票来为其活动融资，那么是的，如果你提高其股价，那么他们可以比股价较低时为自己融资更多。但那些自首次公开募股以来尚未发行新股的公司怎么办？他们的高股价是否会让他们的公司在通用人工智能方面跑得更好/更快？这看起来非常试探。我想有一个渠道可以让那些有很多才华的员工去那里工作，当他们的工资主要是股票期权而不是现金时，他们会获得更多的利润？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:38:31 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:38:31 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>但那些自首次公开募股以来尚未发行新股的公司怎么办？</p></blockquote><p>在这里唱反调：大多数致力于人工智能的大型科技公司实际上每年都会发行新股，不是卖给投资者，而是作为员工的股权补偿。对于许多这样的公司来说，这实际上可以达到每年几个%，从长远来看，这是相当稀释的！所以，是的，当你所在的公司为你提供大部分预期薪酬的期权时，股价可能很重要。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:38:45 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:38:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>我想有一个渠道可以让那些有很多才华的员工去那里工作，当他们的工资主要是股票期权而不是现金时，他们会获得更多的利润？</p></blockquote><p>是的，我对这里的关系感到有点困惑。我想我觉得任何形式的融资成本都应该由股价来大致追踪？</p><p>而且，是的，这确实使他们雇用人员的成本更便宜。对于大多数科技公司来说，他们的股票薪酬方案相当于工资的 50%，而他们的大部分支出都是工资，所以这应该直接等于更多的资本。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:44:40 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:44:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><blockquote><p>好吧，但是这种投资方式道德吗？</p></blockquote><p>这取决于细节，但通常我会说是。但也有例外。如果你投资的是私营公司，或者你可以以有意义的方式影响资本成本，比如 OpenAI 或 Anthropic，那么我想说在那里投资会存在道德问题。</p><p>但微软和谷歌却不是这样。他们已经拥有无限的战争资金，并且不会根据你的投资在人工智能上花费更多或更少的钱。英伟达除了最大化其容量和扩展限制之外也不会做任何事情，金钱不是一个因素。我认为买这样的股票是可以的。</p><p>即使他们发行股票期权，他们也会根据价格调整规模，而你的影响却是微乎其微的。我认为你可以放心地忽略这些因素。假设他们在招聘方面有无限的资金规模，而工资和补偿仅受社会力量的限制。</p><p>如果你担心，你可以具体情况具体分析——当你投资太阳能公司时，你似乎在以真正的方式帮助他们，这可能对人工智能不利，但对其他原因也有好处（例如气候和普遍利益） 。</p><p>对你的激励的影响是明显的。我根本不担心这一点，因为我知道这充其量只是我真实风险敞口的一个小对冲，不会改变任何事情。对于那些在 OpenAI 私人股票中拥有大量敞口的人来说，那里可能存在问题。</p></div></section><h2>你实际上会如何使用大量资金来帮助 AGI 顺利发展？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:44:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:44:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，这有点偏离核心主题，但感觉也很重要。</p><p>因此，假设有一群人担心存在风险，他们在通用人工智能的准备阶段进行了大量投资，现在他们总共拥有数千亿美元左右的资金。</p><p>假设很难用钱来取得更多的人工智能协调进展，有什么办法可以用这笔钱来减缓 AGI 公司的速度或类似的事情吗？就像，我确实认为，在这种投资情况下，对我来说一个主要问题是，当 AGI 发生时，我是否真的有用大量资金。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:44:53 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:44:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>如果你能提高生产要素的价格，你也许就能减缓进步。也许竞标者财团可以使某些稀土金属变得更加昂贵？我不知道技术细节。</p><p>我<i>不会</i>做空高增长的人工智能股票来推高其资本成本，因为你很快就会被吹垮。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:46:05 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:46:05 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>假设很难用钱来取得更多的人工智能协调进展，有什么办法可以用这笔钱来减缓 AGI 公司的速度或类似的事情吗？</p></blockquote><p>已经讨论过的一项策略是向顶尖人才支付报酬，而不是开发 AGI。如果您确实跑赢了市场，您应该能够向研究人员和开发人员支付高于市场的价格。</p><p>也就是说，我预计人们从事这些工作的动机只是部分是金钱上的，如果他们个人已经很富有并且对主要的生活事物感到满意，他们可能会为了荣耀或任何其他动机而继续努力，所以我怀疑这不会像人们想象的那么成功。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:48:14 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:48:14 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>再想一想，最好的策略可能是向已经表示同情的政客捐款。政府有枪什么的，很擅长阻止人们做事。捐款可以改变政策。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:48:36 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:48:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>如果你能提高生产要素的价格，你也许就能减缓进步。</p></blockquote><p>是的，这就是我关于收购顶尖人才的建议的概括。理论上，只要存在资源限制，您就可以支付额外费用来隔离该资源。</p><p>当然，一旦你这样做了，价格就会上涨，你就会激励市场寻找新的、更便宜的方式来增加在线供应。所以我认为这种策略最多只能在短期内减缓速度，从长远来看可能会加速速度。但如果您认为唯一重要/存在的是短期，那么它肯定是工具包中的一个工具...... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:50:54 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:50:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>游说政府、支持候选人、公共宣传或其他类似的事情都是显而易见的选择，你可以花很多钱。那将是我的第一步行动。政治总是让人感觉讨厌，但最终却不是一个人可以忽视的东西。</p><p>如果你能很好地瞄准它，那么购买顶尖人才似乎也是一个伟大的举动，而且想必你也可以充分利用这些人才（对齐，或者如果不是的话，那么顶级人才几乎总是便宜的，即使它很昂贵） 。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:50:14 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:50:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，我确实对游说点感到有点反感。这感觉相当对称，因为历史上人们以一种并没有真正让他们承担责任的方式阻止了大量好事的发生（监管捕获是更常见的途径，但也有各种愚蠢的安全措施）法规）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:51:46 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:51:46 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>我只是想指出我们甚至一次都没有提到加密货币:)</p></div></section><h2>请多样化您的加密货币投资组合</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:00:24 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:00:24 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>谈到加密货币，从历史情况来看，我确实有很多朋友在其投资组合中持有相当大一部分的加密货币。我实际上感兴趣的是如果 AGI 起飞的话，你预计加密货币价格会发生什么（至少部分是因为我经常希望我周围的生态系统在加密货币上进行不太集中的赌注，而这个特定领域可能会影响一些人）否则他们的持有就相当坚定）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:53:30 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:53:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>只是为了尽可能简单地说明：加密货币/区块链是否解决了未来 AGI 可能遇到的特定问题？我认为人工智能实际上很有可能比人类更好/更容易地利用这些功能。稀缺性的数字表示可能非常有价值。此外，可自动执行的智能合约最终可能成为人工智能之间协调的主要手段。 :) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 19:00:02 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 19:00:02 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>请注意，这里反对加密货币的最有力的理由（除了它根本没有价值或用途之外）是，在实际利率非常高的世界中，持有非收益或低收益资产会遭受极大的机会成本。例如，黄金的交易往往与实际利率相反（较高的利率 ->; 较低的价格）。因此，比特币在这方面尤其可能存在重大问题。以太坊或其他更灵活的系统可能会采取一些措施，例如提高质押收益率或尝试跟上利率（尽管这当然会稀释当前持有者的权益）。基本上，您依赖于在 AGI 起飞时提供主要价值的用例...... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 19:00:48 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 19:00:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>因此，我的基本观点是，任何持有大量加密货币占其财富的百分比，但又不至于在出售时面临流动性问题的人，都是在做非常错误的事情。这并不是说人们不应该拥有 BTC 或 ETH 等。一些加密货币就可以了。</p><p>但基本的投资组合理论表明，如果您的投资组合中有很大一部分是具有极高特殊波动性的资产，那么您可以通过多元化（然后根据需要使用更多杠杆）获得更好的回报。</p><p>我并不反对拥有加密货币，但 50% 以上的 ETH 分配——我想你提到过——对几乎任何人来说都不是最理想的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 19:00:40 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 19:00:40 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>感谢奥利弗让我参与对话！我现在需要下车，但我很荣幸被邀请参加，这很有趣，也很发人深省！很高兴与你们 Zvi 和 Noah 交谈，我期待未来的对话！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:02:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:02:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>谢谢威尔！非常感谢您的想法！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 19:12:18 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 19:12:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我现在也得走了，但谢谢你邀请我！和大家聊天真是太棒了。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:12:35 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:12:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>感谢您加入诺亚！</p></div></section><h2>你应该购买人工智能公司的私募股权吗？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:12:11 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:12:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>因此，正如我们所指出的，购买 AGI 公司的股票相当困难，因为大多数直接从事 AGI 公司业务的公司要么是私营公司（OpenAI、Anthropic），要么是主要从事非人工智能业务的大型科技公司的一部分东西（微软，谷歌）。</p><p>因此，人们可能会尝试做的一件事就是获得私募股权投资。例如，从已经在那里呆了一段时间的员工那里购买它。</p><p>从财务角度来看，这似乎是个好主意吗？此外，这里的激励问题似乎比我们一直在谈论的其他事情更明显。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 19:13:54 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 19:13:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>OpenAI 或 Anthropic 等私募股权公司的交易价格似乎很可能低于其私下交易的程度，因此这里的担忧是道德问题，而不是财务问题。在这一点上，这取决于你从谁那里购买，你在哪些地方改变什么激励措施，以及你是否可以在不过度抬高价格的情况下实现“股权结构表上的死”目的。我的了解不够，无法确定答案，但在我有更好的想法之前，我会尝试远离。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:15:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:15:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，这对我来说也感觉不错。我一直在考虑设立一些基金，也许可以购买安全研究人员持有的大量股权，这样安全研究人员在按下停止按钮或进行举报时就不必炸毁他们的金融投资组合或其他什么，这似乎确实是非常明智的激励措施。</p><p>我确实认为可悲的是，公司经常与员工签订明确的协议，阻止他们对冲自己的股权头寸，所以这可能很难。</p></div></section><h2>总结要点</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:15:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:15:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，我觉得参与这个之后，我觉得我至少对构建某种投资组合有一个不错的把握。</p><p>我会考虑几天，但我觉得如果我在这里实施建议，我会大致：</p><ul><li>去 Schwab 或 Fidelity 开设一个经纪账户（以设置起来不那么烦人为准）</li><li>将我的投资组合的 50% 投资于相当广泛的指数基金，实际上没有特定的专业化</li><li>将我投资组合的 20% 投入到一些更专注于科技/人工智能的指数基金中。也许在向我展示的经纪界面上寻找涵盖这里列出的一些公司的内容（可能在这里做更多研究）</li><li>将我的投资组合的 3-5% 分别投资于 Nvidia、台积电、微软、谷歌、ASML 和亚马逊</li><li>拿我投资组合的 2-5% 来购买一些期权（可能是上述某些股票的一些长期看涨期权），确保我购买那些下跌空间有限的期权，看看我是否能成功地不买在我在这里做更多之前，我先把我的投资组合的这一部分炸毁了大约两年</li></ul><p>然后我可能不会太在意重新平衡，基本上会忘记它，除非我想额外注意。</p><p> <i>（当我在写这段对话时在电话中说出上述内容时，兹维、诺亚和威尔似乎都认为这是一个不疯狂的计划）</i></p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/CTBta9i8sav7tjC2r/how-to-hopefully-ethically-make-money-off-of-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CTBta9i8sav7tjC2r/how-to-hopeously-ethically-make-money-off-of-agi<guid ispermalink="false"> CTBta9i8sav7tjC2r</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 06 Nov 2023 23:35:16 GMT</pubDate> </item><item><title><![CDATA[A bet on critical periods in neural networks]]></title><description><![CDATA[Published on November 6, 2023 11:21 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:05:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:05:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>嘿加勒特！我听说你有一些新的实证结果。设置是什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:09:20 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:09:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我已经告诉你我关于关键期如何运作的理论。我所做的就是复制<a href="http://arxiv.org/abs/1711.08856">论文</a>，然后在我的数学中发现了另一个问题，它翻转了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的方向<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>应该随着你增加纪元而去，我得到了这个看起来很整洁的图表</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/f3jamqlkpjlylprzg1wq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/rxgj4equest3vhhoxphh 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/cfzn6pwztfxfbdqguilz 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/nqcfrs3kfd2shg5zjzu1 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/dh86e0ubgc2hluy7iyt7 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/xlzyxsc6v0prhj2isaum 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/rig8cnq8fekvhhk2bzg0 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/yedqpuwpjcxlq2chgwj1 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/v7esh6abnofzm4uybcjs 640w"></figure></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:11:13 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:11:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>（正确的尺度应该是局部学习系数，而不是损失） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:16:42 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:16:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>让我粗略地总结一下您对关键时期的看法。</p><p>在人类发展心理学中，<a href="https://en.wikipedia.org/wiki/Critical_period">关键期</a>是指大脑特别擅长学习某种东西，但以后可能学不会的时期。</p><p>还有一篇您感兴趣的论文，其中需要一些神经网络来学习分类任务 CIFAR 10（要求网络将图像分类为飞机、汽车、鸟类或其他 7 个类别之一）。它将训练数据中的图像模糊到某个时期，然后再取消模糊。经过一段时间后，对图像进行去模糊处理不足以让网络收敛到一直在非模糊图像上进行训练的网络的性能。</p><p>通过观察训练期间网络参数的 RLCT，奇异学习理论有望预测这个“关键时期”何时到来。我不清楚这是在原始论文中还是来自您。</p><p>这听起来大致正确吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:20:25 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:20:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，尽管关于奇异学习理论能够预测关键时期何时到来的希望，我认为这<i>不太</i>可能，尽管它确实看起来比模糊时的训练损失或验证损失更能响应“正在发生的事情”移动： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/fynahavzdes3scapnlgw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/fombxvfkgic0loeqg25u 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/y1u7f8yn61qg7wc4ykpb 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/urtonslmszoetyx2kiiq 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/stoztjguxzihqkyalwfk 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/zc8d0tcjbpckogkjnxlp 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/mocazr79tqjzntlvacz5 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/oylrudppjxbqo8wyxv64 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/e8r8Tx3Hk7LpnrMwY/dxoik5cypnuxgyho7ax6 640w"></figure><p>这似乎一切如常。但我们还没有到可以准确地说出关键时期何时到来的地步，而且我不希望我们立即到达那里。这项研究的主要直接点是想出一种方法，可以用 SLT 来思考关键时期，然后看看我们是否可以使用当前非常有限的实验设备从该图中抽出哪怕是最小的预测。单一学习理论。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:23:01 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:23:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>据我所知，训练损失和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>看起来都与最终验证精度相当对称（训练损失具有微不足道的对称性），但这是我所看到的感兴趣的属性之间的主要联系。你看到了什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:28:43 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:28:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>看起来火车损失是单调递减的，看起来与任何其他火车损失没有什么不同，而 lambdahat 似乎随着最终验证准确性的降低而增加，直到最终验证准确性趋于稳定，然后又趋于稳定。也许在 150 的火车损失中存在一个小问题，但如果你向我展示火车损失，我不会想到它，如果你向我展示 lambdahat，我会说“看起来好像发生了一些奇怪的事情” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:30:09 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:30:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>正在发生的特别奇怪的事情是什么？比如，它会做什么更正常的事情？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:32:34 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:32:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>通常它应该相对地保持在相同的位置，我以为我制作了正常 lambdahat 的图表，但后来我最近意识到它有点糟糕。它的训练周期太少，并且计算中存在数值不稳定问题，导致古怪的 lambdahats </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:34:42 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:34:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>是的，我记得 Arjun <a href="https://www.lesswrong.com/posts/PDz68D5vQQgacAwoF/estimating-effective-dimensionality-of-mnist-models">提到过</a>很容易变得古怪<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{\lambda}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> s。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:35:32 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:35:32 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>默认的超参数在很多时候都有效，但是是的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:35:47 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:35:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><blockquote><p>看看我们是否可以使用当前奇异学习理论非常有限的实验设备从该图片中提取出哪怕是最小的预测</p></blockquote><p>那么，这次成功了吗？或者是先探索，然后形成假设，然后测试？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:42:58 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:42:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>这是一个小小的成功。我有一个假设，即历元可以被认为是降低 ML 模型的温度，并且您可以通过现在基本上假设您的模型可能达到的 2 个可能的奇点/模型类来从这种假设中得到一个关键时期。如果温度很高，那么如果您正在采样，那么在模型类之间遍历就很容易。如果温度较低，那么在采样时在它们之间穿行就很困难。因此，当奇点/模型类之一捕获（在本例中）非模糊 CIFAR-10 图像的细粒度细节，而另一个则没有捕获时，就会发生关键时期，因此对于高温，当您选择在模糊的 CIFAR-10 图像周​​围随机游走，然后突然切换到包括非模糊 CIFAR-10 图像的数据分布，您可以轻松切换您花费大部分时间采样的模型类。但对于低温来说，就比较困难了。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:45:43 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:45:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>所以使用自由能公式</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_n = n L_n/T + \lambda \log n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></p><p>如果我们降低温度，我们也必须降低<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span> ，所以我们必须增加<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ，因为如果存在一个具有较低<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ 的</span></span></span></span></span></span></span>模型类，那么我们已经在它上面了。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:49:28 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:49:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我们确实看到了这一点，但支持该理论的证据却很薄弱。更有趣的证据来自于，比如说，看看我们是否在增加批量大小时发现同样的效果。也许我们应该预计批量越大，事件发生的越早。虽然这不符合常识，但我可能在某个地方搞乱了我的逻辑。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:55:16 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:55:16 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我觉得我正在想象两种效果，但我不确定哪种效果更强或更相关。</p><p>首先，随着我们运行更多的纪元，最低损失的盆地将在整体中占主导地位，因此我们对损失稍高的盆地的测量较少。</p><p>第二个是两个盆地之间的区域变得低度，因此我们不能再在两个盆地之间过渡。</p><p>这些影响是您想象的吗？如果是，哪个更大？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 22:56:35 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 22:56:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，所以至少从短期来看，在关键时期，第二个更强。并且（因为第一个）我们应该预计需要更多的 dakka 来修复关键期，并再次获得良好的性能。然而，我不知道有什么好方法来估计还有多少达卡。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 22:58:20 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 22:58:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>您能否扩展有关批量大小以及关键更改何时发生的常识直觉？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:02:53 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:02:53 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>因此，从意义上来说，较小的批量大小最终会在每个时期移动更多，因此您应该会看到事情发生得更快，而较大的批量大小最终会在每个时期移动较少，因此您应该会看到事情发生得更慢。除非你标准化学习率，使得较小的批量大小与较大的批量大小具有相同的距离，但与仅仅计算出最佳学习率相比，这可能不是一个好主意。就像，如果你的学习率非常差，即使批量大小非常大，你最终也会得到一个与你的损失情况不太相似的 SGD 分布。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:03:23 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:03:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>较小的批次是否更有可能在其更新向量中具有未消除的噪声？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:03:32 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:03:32 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:03:39 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:03:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>凉爽的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:05:20 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:05:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>但这是其中的一部分，这就是为什么您应该怀疑增加批量大小会导致温度降低，因为随机性较小。但这就是它与 SLT 连接的方式。常识性的概念是，增加批量大小类似于缓慢而仔细地训练，而减少批量大小类似于快速移动和破坏事物。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:05:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:05:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>嗯，是的，这是有道理的。在这种情况下，我会支持该理论并反对常识。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:07:49 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:07:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>那么你对简单的论证就比我更有信心了！为什么你不觉得跨时代的不同变化令人担忧呢？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:12:20 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:12:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>现在，我对梯度下降的直觉可能非常愚蠢，但当我发现我有直觉可以帮助他们移动时，我喜欢做小赌注。不过，他们是这么说的。</p><p>围绕奇点巩固测量的主要力量就是数据量。使用较小的批次将使您移动更多，因为随机采样噪声（但模糊不是采样噪声），并且因为在大多数情况下，在您看到完整数据集之前更新可能是可以的。但我认为第一个效应不应该过多地巩固测量结果，我不知道如何看待第二个效应，但我认为对于标准学习率来说，它可能应该有点小？就像（数据集大小/批量大小）中的次线性一样。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:14:40 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:14:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，这看起来很合理，但对我来说还不够合理，所以如果你愿意，我们可以打赌，使用批量大小 64 而不是 128 运行是否会使关键期结束得更晚而不是更早？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:15:40 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:15:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我们开始做吧。赔率10美元？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:15:46 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:15:46 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>听起来不错！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:16:08 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:16:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>🤝 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:16:15 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:16:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>🤝 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:16:35 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:16:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>好的，不错！我们现在应该多讨论一些吗？或者等我们得到实验结果后再回来讨论？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Mon, 06 Nov 2023 23:16:50 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Mon, 06 Nov 2023 23:16:50 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>看来这是一个结束的好地方。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Mon, 06 Nov 2023 23:16:54 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Mon, 06 Nov 2023 23:16:54 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>数据收集的另一边见！</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/e8r8Tx3Hk7LpnrMwY/a-bet-on-critical-periods-in-neural-networks#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/e8r8Tx3Hk7LpnrMwY/a-bet-on-ritic-periods-in-neural-networks<guid ispermalink="false"> e8r8Tx3HK7LpnrMwY</guid><dc:creator><![CDATA[kave]]></dc:creator><pubDate> Mon, 06 Nov 2023 23:21:17 GMT</pubDate> </item><item><title><![CDATA[Job listing: Communications Generalist / Project Manager]]></title><description><![CDATA[Published on November 6, 2023 8:21 PM GMT<br/><br/><p>正在寻找一种方式来帮助沟通 AI x 风险？ MIRI 正在招聘通信部门。很快就会有针对作家和编辑的其他职位列表，但我们将从通信通才/项目经理角色开始。</p><p> <a href="https://intelligence.org/careers/comms-generalist-pm/">https://intelligence.org/careers/comms-generalist-pm/</a></p><p>我是 MIRI 的通讯经理，此人将与我密切合作。我很高兴回答问题。<br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/ZwizCnxJdvEuChnBE/job-listing-communications-generalist-project-manager#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ZwizCnxJdvEuChnBE/job-listing-communications-generalist-project-manager<guid ispermalink="false"> ZwizCnxJdvEuChnBE</guid><dc:creator><![CDATA[Gretta Duleba]]></dc:creator><pubDate> Mon, 06 Nov 2023 20:21:04 GMT</pubDate> </item><item><title><![CDATA[Askesis: a model of the cerebellum]]></title><description><![CDATA[Published on November 6, 2023 8:19 PM GMT<br/><br/><p>我们提供哺乳动物小脑的结构和功能的详细模型。我们的模型与<a href="https://www.lesswrong.com/users/steve2152?mention=user">@Steven Byrnes</a>的模型同构，但完全独立开发；这表明这两种模型可能都相当准确。</p><br/><br/> <a href="https://www.lesswrong.com/posts/nYNkoJLArxW9uGZK3/askesis-a-model-of-the-cerebellum#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nYNkoJLArxW9uGZK3/askesis-a-model-of-the-cerebellum<guid ispermalink="false"> nYNkoJLArxW9uGZK3</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Mon, 06 Nov 2023 20:19:09 GMT</pubDate></item></channel></rss>