<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 6 日星期三 04:15:12 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[EA Infrastructure Fund's Plan to Focus on Principles-First EA]]></title><description><![CDATA[Published on December 6, 2023 3:24 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastructure-fund-s-plan-to-focus-on-principles-first#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/A8L4udgpsoC2BNMnS/ea-infrastruct-fund-s-plan-to-focus-on-principles-first<guid ispermalink="false"> A8L4udgpsoC2BNMnS</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Wed, 06 Dec 2023 03:24:55 GMT</pubDate> </item><item><title><![CDATA[**In defence of Helen Toner, Adam D'Angelo, and Tasha McCauley**]]></title><description><![CDATA[Published on December 6, 2023 2:02 AM GMT<br/><br/><p>匿名帖子：</p><p> “我理解 EA 或 AI 治理领域的一个普遍观点是，Toner、D&#39;Angelo 和 McCauley（简称 TDM）确实把 OpenAI 的事情搞砸了，而 AI、世界的命运等已经变得更糟了。我相信这是完全错误的：事前 TDM 已经以非凡的能力和勇气证明了自己（而不是“也许——可以理解，也许——不是大规模的搞砸）；事后，他们的成就足以证明整个人工智能的正确性”治理社区作为一个整体。</p><p>我认为：</p><p> 1) TDM 的行动使开放人工智能的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。</p><p> 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。</p><p> 3) 无论你是“荣誉和正直的最高主义者”还是“无情的战略家”，TDM 的行动通常从这两种角度来看都表现得非常出色。</p><p> （注：匿名主要是因为想要平静的生活。我没有内部消息，也没有任何狗参与战斗。关于“资历”，我不在人工智能政府工作，但在奖励战略敏锐度的领域经验丰富和丰富的愤世嫉俗，我对“到目前为止的故事”做出了一些有先见之明的呼吁。但作为一个匿名者，这只不过是“来源：相信我兄弟”，所以你不应该这样做，除非我的观点有说服力。）</p><p> **发生了什么**</p><p>我认为 Zvi 和 Gwern 在 lesswrong 上给出了最准确的描述（也是《纽约时报》的报道）。基本上：Altman 试图用刀砍 Helen Toner 来获得 OpenAIs 棋盘的控制权（即，在 3 比 2 的情况下，Altman 可以指定他的盟友来堆叠棋盘，稍后再用刀砍 McCauley，等等）。伊利亚犹豫不决，短暂地叛逃到 TDM 的“安全派”，后者随后控制了自己并解雇了萨姆。随后发生的所有事件都被广泛报道。 （我的论点依赖于这是真实的故事，所以如果你确定这不是发生的事情，你可以停止阅读）。</p><p>这表明 Altman 是一个令人讨厌的作品，绝对不是你想要的负责人工智能重大项目的人：高度权谋、追求权力，并希望消除对他对 OpenAI 应该如何发展的单方面愿景的任何检查。我以像 Reddit 董事会反向收购这样的事件为例，保罗·格雷厄姆 (Paul Graham) 的“补充”事件。他高超的企业斗刀技巧，引发了微软/谷歌法学硕士军备竞赛，并且似乎试图让董事会大部分时间都蒙在鼓里。对 GPT-4 的担忧进一步证明了这一点。</p><p>虽然我的论点不需要对 Altman 做出如此极端不利的判断，但我很困惑为什么 EA/AI 治理领域的一些人对他评价良好：有一大堆明显的负面因素，而我能看到的唯一积极因素是“有时”发出适当的声音。人工智能安全对他来说很方便”（简单地说：SBF 也更一致地发出正确的声音……）但即使 Altman 很伟大，他也试图以他们认为的借口罢免管理非营利组织的独立董事会成员。这种不利于 OpenAI 商业利益的行为是完全不可接受的。</p><p><br> **现在的情况**</p><p>萨姆（很可能）重新担任首席执行官，董事会似乎将由萨姆的 1 名盟友、德安吉洛（1 名被踢掉他的人）和萨默斯组成，萨默斯可能会起到平衡作用。塔莎、海伦和伊利亚出去了，格雷格和山姆没有回来。</p><p>将该委员会与奥特曼罢免托纳的反事实相比，奥特曼对这个委员会的控制要少得多。首先，他没有参与其中，其次，他现在有三分之一而不是大多数人明确支持他。当然，这可能会改变：我预计奥特曼会继续尝试获得董事会控制权，并且考虑到他的能力，他迟早会成功；即使没有，也许他可以安排“实际情况”，这样董事会就无法阻止他随心所欲地经营公司。或者也许微软会突然介入并从现在开始基本上掌控这一切。</p><p>但也许这些都不是，也许董事会确实会成为对奥特曼和那些乐于奔向悬崖的人的有效制衡。至少好人仍然可以在牌桌上发挥作用，如果 TDM 让 Sam 政变成功，他们就不会被有效消灭。有时这是您所能期望的最好的结果。</p><p>但真的是这样吗？还有很多其他结果看起来很糟糕，例如：</p><p> * Sam 重新担任首席执行官，并带来了许多值得称赞的公关。如果他像我想象的那样糟糕，那就不好了。</p><p> * OpenAI 工作人员一致同意 Sam 的观点。</p><p> * 反击/不信任/嘲笑/等等。反对“人工智能安全”“EA”“Decels”。 ETC。</p><p>本质上，是的：TDM 基本上没有机会避免这些缺点（所以可信的明显失误基本上是“没有伤害，没有犯规”），即使周一早上热衷于四分卫，考虑到他们所处的非常不利的位置，他们似乎也基本上尽了最大努力。已经，而且执行情况超出了合理预期。</p><p></p><p> **为什么坏事基本上已经被定价了**</p><p>当前的戏剧提出了三个主要权力块：董事会（好吧，TDM）、员工和投资者。如果推动力足够大，后两者应该提前可靠地建模，使其主要底线是“美元”而不是“使命”。这里的潜在推动力是巨大的（例如，如果股权归属，员工将一夜之间成为百万富翁，微软的数十亿美元投资）。因此，如果他们要在“让这位护身符般的首席执行官承诺让令人惊叹的美好时光持续下去”与“使命/安全担忧”之间进行投票，人们应该期望他们无论如何都会压倒性地支持前者。</p><p>但是，至少从表面上看，而且绝对是有意为之（Altman 之前对 OpenAI 奇怪的治理结构的重要性的评论在这里很讽刺），董事会最终发号施令，并且被设置为（与后两组不同）对财务压力不敏感。尽管其他各方（包括像大多数财经媒体这样的局外人）认为该章程赋予他们的权力本质上是表面上的公关或 LARP，但它确实保留了一些现实政治的牙齿：许多资源似乎都被 501(c) 所束缚。 3、虽然董事会无法阻止员工移民，也无法阻止投资者创办一家“我们无法在安全问题上GAF”的替代人工智能公司，但他们可以可信地威胁要烧毁已经存在于 OpenAI 中的大量“股东价值”，所以要斗争如果这些各方返回“内部选择”，则需要做出让步。</p><p>换句话说：董事会权力是 TDM 能够实现的唯一现实杠杆，他们（巧妙地）做到了这一点，并尽可能地利用了这一点。许多不关心人工智能安全的人——内部人士或观察家——鄙视或嘲笑他们（也许延伸到人工智能安全），因为他们按照他们认为愚蠢的事业（或反对他们的利益）行事）是开展业务不可避免的成本。首先，这些事件揭示了潜在但预先存在的动力，而不仅仅是激发或加强它们。</p><p></p><p> **事后诸葛亮的好处更好**</p><p>人类容易犯错，事后诸葛亮是20-20，因此很容易对战争迷雾中的行动过于挑剔，而一旦战争迷雾散去，这些行动就会显得愚蠢。但人们可能会犯相反的错误：如果你仔细规定他们可以或不可以“合理地预见”或“合理地引导相信”什么，你几乎可以将任何人和任何人视为杰出的英雄。</p><p>另一个挑战是有两种观点可供选择。一个是更狡猾/天真的结果主义版本：OpenAI 治理是一场现实生活中的外交游戏，因此如果 EV 足够高，TDM 应该抓住机会进行“刺杀”，等等。另一个是更有原则的“不惜一切代价荣誉”这在理性主义者中似乎很流行：所以 TDM 应该充当宪章的圣骑士，因此应该采取立场，即使他们确信这将是徒劳且适得其反的姿态，并且总体上有点守法愚蠢，作为一些预先承诺的东西，一些 UDT 的东西，这实际上是最好的多元宇宙范围的政策，等等。</p><p>这些观点通常是一致的：即使（例如）“背刺”的最佳频率不是“没有”，它最多也是“非常罕见”。事实上，TDM 表现得很好，他们的选择通常（但不是普遍）看起来介于“合理”和“模范”之间，无论你喜欢哪种方式。通过查看各种“他们为什么不 X”问题可以最好地看出这一点。</p><p></p><p> *他们为什么不给和平一个机会？*</p><p>据推测，TDM 可能会选择对 Altman 做出较小的反应，而不是将他彻底抛弃。也许他们可以把萨姆和格雷格踢出董事会，但让他继续担任首席执行官，或者他们可以利用他们的多数席位任命一群安全主义成员来阻止未来的政变企图（或两者兼而有之）。或者他们可以召开一次与萨姆的会议来讨论他的行为，希望能够解决问题，而不是他们所寻求的仓促的“反政变”。也许这会是徒劳的，但有选择价值，如果（例如）奥特曼辞职作为回应并做与他实际做的几乎相同的事情，也许会给他们一个更好的位置。</p><p>从现实政治的角度来看，这似乎极其愚蠢。即使忽略之前对奥特曼的任何担忧，政变企图也表明奥特曼是来抓你的。伊利亚的叛逃提供了唯一的机会之窗，但其持续时间不确定（事后看来，显然很短），让他成为第一。让他作为对你的利益怀有敌意的首席执行官，似乎是在恳求他尝试其他选择来事实上废黜你（例如，确保你不会就任何重大问题征求你的意见），即使你让自己免受他重复这种特定攻击的影响。即使你必须达成协议，让他重新担任首席执行官，但无论如何都没有董事会席位（参见实际发生的情况），你也必须免费参加一个机会，让你成功地用更好的人取代他，并且重新任命奥特曼是你可以在谈判中交易的一匹额外的马。</p><p>尽管有原则的人原则上可能不喜欢企业尖锐的做法，但TDM对奥特曼的刀砍是对他试图刀砍托纳的回应，这是可信的合理的——他们并没有首先与“肮脏”作斗争——而且有一种自然的正义“让我们让你尝尝”你自己的医学方面在这里。即使你认为针锋相对不合适，我认为“宪章圣骑士”的观点也会说奥特曼的即决解雇要么是合理的，要么是直接需要的。批评监督你的独立董事会成员写了一些你不喜欢你公司的内容，这已经很过分了。以此为借口将他们踢出董事会，因为她已经并将继续反对您快速商业化的愿望，并将阻止董事会有效反对您的任何能力。如果这看起来是有预谋的，并且加上试图终止董事会等的背景，则意味着董事会不应该相信首席执行官只是为了所有人的利益而开发人工智能技术。</p><p>如果TDM知道Ilya后来会后悔自己投票的结果，那么一个有原则的人可以扣掉TDM积分，所以在他能做出更好的考虑之前就催促他这么做（/让Greg的妻子劝阻他）。但这结合了许多关于 TDM 当时所做/应该想到/预见的主张，并解决了各种潜在的防御措施（即使如此）。从现实政治角度来看，TDM 遭到了（据称）最优秀的企业持刀战士之一的伏击，但最终刺伤了他的却是他们。</p><p></p><p> *为什么不解释清楚发生了什么？*</p><p>我同意这看起来像是一个错误，尽管影响不大。即使“奥特曼试图政变我们，所以我们反击了”，也不会对奥特曼/SV 媒体全场媒体产生太大的吸引力，也不会与工作人员产生太大的芥蒂（“好吧，如果是董事会或奥特曼，我们绝对更喜欢奥特曼”），这似乎比不提供任何实质性内容要好。原则性的方法是，如果你所做的事情可能会让员工损失很多钱，那么你应该向员工提供完整的解释。</p><p>可能会出现合理的解释：也许有压倒一切的原则迫使人们保持沉默，也许精明地保留“我们将签署保密协议/非贬低合同，并且在我们离开后不会进行媒体巡演”作为进一步讨价还价的筹码，也许他们（合理地） ，如果错误的话）遵循非常谨慎的法律建议或其他建议。但这似乎是错误的，在争论他们做了正确的事情的过程中，假设 TDM 做了正确的事情来解释明显错误的事情，这就引出了问题。</p><p></p><p> *他们为什么要塌陷？/他们为什么不把它全部烧毁？*</p><p>如果你确定奥特曼是个坏消息，你愿意‘满发仇恨’踢他，为什么还要让他回来？为什么不让 OpenAI 毫无原则地死在这座山上——或者至少让 Altman 在 MS 领导下的建立成为一场代价高昂的胜利，因为从 OpenAI 的尸体中寻找宝贵的人工智能资源可能会带来法律上的麻烦。为什么不竖起中指，把你拥有合法所有权的一切都赠予别人呢？</p><p>我猜这些令人头疼的问题，以及 TDM 可能会烧毁 OpenAI 的大量“资产”的可信威胁，正是让 Altman 坐到谈判桌前的原因，也是为什么他原则上同意了一项协议，而不是“卑鄙地向他投降”。 ’他的宠物媒体报道一直暗示这是迫在眉睫的，或者是他唯一能接受的事情。我认为 TDM 为他们赢得的是 Altman 必须遵守的更具安全意识的治理的前景（不是保证），从安全角度来看，这看起来比他直接在微软旗下建立的更好（至少对我来说），即使他会遭受（大量）一次性转换成本。</p><p></p><p> *请有人考虑一下二阶效应吗？！*</p><p>但更广泛的世界影响又如何呢？如果 EA/AIS/无论什么现在在技术或金融精英中是一个肮脏的词或蔑视的对象，那么值得吗？如果 TDM 看起来很愚蠢（即使事实并非如此），这会对我们其他人产生不好的影响吗？这是否会阻止任何人投资于以安全为首要的治理结构，以获取大型科技公司的收益？操作系统 AI 开发是否会激增，因为没有人愿意信任 API，如果超过 50% 的少数人决定这样做，API 就会被扼杀？</p><p>他们很好：</p><p> 1) 原则性的反公关观点会说，人们应该接受人们真诚地看轻你或反对你真正相信和代表的东西。策略很好，公开妥协也很好，但当你真的“暂停人工智能”时，保持假装你有（例如）谨慎的能力是错误​​的。因此，社会现实/更高的拟像水平不应纳入对 TDM 所做工作的评估。</p><p> 2）如果你不相信这一点，通常会出现很多假定的二阶效应。也许 OpenAI 的大爆发会推动政府的介入？也许奥特曼必须更加“摘下面具”才能继续留在 OpenAI，让其他人在事情真正开始发挥作用时保持警惕？也许这种治理明显（或明显被视为）失败意味着同样的错误不会重演？因此，如果从第一顺序来看，TDM 做了正确的事情，那么第二顺序的东西（尤其是事前）可能是近似的清洗。</p><p> 3) 即使你是一个每天晚上都会自慰地阅读《美丽安对话》的狂热接吻者，并且你会看到人工智能治理工作的真正游戏正在秘密渗透到权力走廊，所以你和你的人都处于有利位置适逢其时，TDM 的处境正是如此。如果你不愿意冒一些社会资本的风险，以合理的方式阻止人工智能领头羊的道德可疑的首席执行官罢免自己的董事会并将其打造为自己的个人帝国，那么你什么时候才能花掉它呢？</p><p></p><p> **加起来**</p><p>也许不需要太多时间就能看到这相当于应付或同人小说。也许（例如）TDM 讲述了他们自己的故事，听起来更像是他们感到害怕、冲动或愚蠢。也许故事的另一个转折意味着事情最初如何发展的假设故事是错误的，我在其上建立的合理大厦倒塌了。</p><p>但我不这么认为。就德行而言，他们是唯一真正认真对待“使命”、能够做点什么、在巨大压力下尝试做正确事情的人。从业绩上看，他们遭到了护身符首席执行官的伏击，不仅成功地为自己辩护，还把他赶了出去，尽管有他、员工、投资者和普遍敌对的媒体对他们不利，但他们还是劫持了 OpenAI 董事会的一部分向他们回报重大让步。对我来说，这周的工作似乎还不错。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nfsmEM93jRqzQ5nhf/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley-1<guid ispermalink="false"> nfsmEM93jRqzQ5nhf</guid><dc:creator><![CDATA[mrtreasure]]></dc:creator><pubDate> Wed, 06 Dec 2023 02:47:31 GMT</pubDate> </item><item><title><![CDATA[Some quick thoughts on "AI is easy to control"]]></title><description><![CDATA[Published on December 6, 2023 12:58 AM GMT<br/><br/><p>我觉得<a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">帖子</a>作者错过了很多东西，我想分享一些看起来很好沟通的东西。</p><p>我将专注于控制超级智能 AI 系统：系统强大到足以完全解决对齐问题（在<a href="https://arbital.com/p/cev/">CEV</a>意义上），或者杀死地球上的所有人。</p><p>在这篇文章中，我将忽略其他与人工智能相关的 x 风险来源，例如<a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf">人工智能支持的生物恐怖主义</a>，并且我不会评论所有似乎需要评论的重要内容。</p><p>我也不打算指出所有我认为可能会让读者错误概括的狡猾主张，因为这会很挑剔，也不值得花时间（我会跳过的例子 - 我找不到证据GPT-4 已经过任何有监督的微调；RLHF 将聊天机器人的大脑塑造成一种系统，该系统产生的输出使人类评分者点击竖起大拇指/“我更喜欢这个文本”，这样做的智能系统不是它们自己必然是人类评分者的“首选”；一个脚注<span class="footnote-reference" role="doc-noteref" id="fnref6uvr6jxw68j"><sup><a href="#fn6uvr6jxw68j">[1]</a></sup></span> ）。</p><h2>介绍</h2><blockquote><p>许多人担心我们将失去对人工智能的控制，导致人类灭绝或类似灾难性的“人工智能接管”。我们希望本文中的论点使这样的结果看起来难以置信。但即使未来的人工智能在严格意义上变得不那么“可控”——例如，仅仅因为它的思考速度比人类直接监督的速度快——我们也认为，将<i>我们的价值观灌输</i>给人工智能是很容易的，这个过程称为“<strong>对齐</strong>”。</p></blockquote><p>这<strong>歪曲了人们的担忧</strong>。说“但即使”使它看起来像：担心 X 风险的人们相信“无论/尽管一致，失去控制都会导致 X 风险”；这些人错了，因为帖子显示“这个结果”是难以置信的；另外，即使他们对失去控制的看法是正确的，他们对x风险的看法也是错误的，因为由于一致，一切都会好起来的。</p><p>但大多数情况下，人们（包括主要声音）特别担心可能导致人类灭绝的错位系统。我不知道社区里有谁会说，如果与 CEV 一致的超级智能夺取了控制权，那是一件会导致灭绝的坏事。</p><blockquote><p>由于每一代可控人工智能都可以帮助控制下一代，因此这个过程似乎可以无限期地持续下去，甚至达到非常高的能力水平</p></blockquote><p>我预计奖励塑造低于一定能力水平<span class="footnote-reference" role="doc-noteref" id="fnrefkrdp19oh1z"><sup><a href="#fnkrdp19oh1z">[2]</a></sup></span>的人工智能是很容易的，并且我担心控制高于该水平的人工智能。我相信你需要一个超人能力的系统来设计和监督一个超人能力的系统，这样它就不会杀死所有人。亚人类系统目前有能力监督其他亚人类系统，这样这些系统就不会杀死所有人，这是我所预测的，但这并没有提供大量证据证明亚人类系统能够监督超人类系统。 <span class="footnote-reference" role="doc-noteref" id="fnref072edwh8od0f"><sup><a href="#fn072edwh8od0f">[3]</a></sup></span></p><p>为了解决调整超人类系统的问题，您需要一定量的复杂的人类思维/艰苦的高水平工作。如果一个系统可以在短时间内输出那么多艰苦的高水平工作，我认为这个系统是超人的，并且将其对齐的问题是“对齐完成”的，因为如果你解决了以下任何一个问题对于此类问题，您本质上是解决了对齐问题，并且可能避免 x 风险，但是解决这些问题中的任何一个都需要大量艰苦的人力工作，而安全地自动化这些艰苦工作是一个对齐完成的问题。</p><p>需要有一个论据来解释为什么一个人可以成功地使用非人类系统来控制复杂的超人类系统，否则，拥有几代可控制的非人类系统并不重要。</p><h2>优化</h2><p>让我们谈谈特定神经网络将追求的目标。</p><blockquote><p>我们经常解决的许多“协调问题”，例如抚养孩子或训练宠物，似乎比训练友好的人工智能要<strong>困难</strong>得多</p></blockquote><p>请注意，进化已经“白盒”地访问了我们的架构，优化了我们的包容性遗传适应性，并获得了针对类似事物集合进行优化的东西。考虑到这一点，人类是如此的一致。孩子们已经很容易想要巧克力、政治和合作；相反，如果你让一个外星孩子将<a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/YrhT7YxkRJoRnr7qD">善良</a>与<a href="https://www.lesswrong.com/posts/n5TqCuizyJDfAPjkr/the-baby-eating-aliens-1-8">吃孩子</a>或<a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/mMBTPTjRbsrqbSkZE">分类卵石</a>联系起来，给这个孩子奖励可以让他们学习你的语言，但不一定会让他们不想吃孩子或分类卵石。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=cLXQnnVWJGo"><div><iframe src="https://www.youtube.com/embed/cLXQnnVWJGo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>如果你有一个孩子，你不需要用数学来具体说明你看重的一切：他们可能不会非常聪明地让你给他们奖励，而且他们已经天生想要得到与您想要的东西类似的东西。</p><p>当你创建人工智能时，你确实需要有一个优化目标：你希望人工智能尝试做什么，即使具有超级智能优化能力，也可以安全地优化效用函数。我们不知道如何安全地指定这样的目标。</p><p>然后，即使你以某种方式设计了这样的目标，你也需要以某种方式找到一个真正尝试实现该目标的人工智能，而不是其他东西，实现的过程与训练期间的目标相关。</p><blockquote><p>人工智能正在秘密计划杀死你，梯度下降会注意到这一点，并使其将来不太可能这样做，因为制作秘密谋杀情节所需的神经电路可以被拆除并重新配置为直接提高性能的电路</p></blockquote><p>我不确定他们对内部对齐问题的假设是什么。这是错误的：我们期望具有广泛目标的智能人工智能能够在可以使用的广泛奖励函数上表现良好，而梯度下降不会优化人工智能实际上试图追求的最终目标。</p><p>我<a href="https://moratorium.ai/#modern-machine-learning">完全期望</a>梯度下降能够成功优化人工神经网络以实现低损失；我只是不期望他们设计的损失函数能够<a href="https://moratorium.ai/#outer-and-inner-ai-alignment">代表我们所看重的东西</a>，并且我期望梯度下降找到<a href="https://moratorium.ai/#inner-alignment">尝试实现与奖励函数中指定的内容不同的东西的</a>神经网络。</p><p>如果梯度下降找到一个试图最大化与人类完全无关的东西的智能体，并且明白为此它需要在我们的函数上获得高分，那么该智能体将成功获得高分。<strong>梯度下降将优化其在我们的函数上获得高分的能力 - 它将优化构成代理的结构 - 但不会真正关心当前结构的目标内容</strong>。如果训练完成后，这种结构针对宇宙未来的任何奇怪现象进行了优化并计划杀死我们，这不会追溯性地使梯度发生变化——我们没有已知的方法来指定训练消失的损失函数未来可能会杀死我们的参数。</p><h2>干预措施</h2><p>能够进行实验并不意味着我们可以提前得到所有潜在问题的演示。如果人工智能足够聪明，并且已经想要与我们想要的东西足够不同的东西，并且我们不理解它的认知架构，那么我们将无法欺骗它相信它的模拟环境是真实世界，它可以最终接管。仅仅对权重和激活进行读/写访问并不能让我们控制人工智能的想法<span class="footnote-reference" role="doc-noteref" id="fnreftjnsez6u2fb"><sup><a href="#fntjnsez6u2fb">[4]</a></sup></span> 。塑造非人类系统行为的技术不会让我们保持对智能系统的控制。</p><blockquote><p>一些人将越狱的有效性作为人工智能难以控制的论据</p></blockquote><p>是的，但这不是 x 风险社区的论点。</p><blockquote><p>至关重要的是，这并不意味着人类“与进化论保持一致”——请参阅<a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn"><i><u>《进化论》没有提供任何证据证明昆廷·波普的左转</u></i></a>可以揭穿这一类比。</p></blockquote><p> AFAIK，内特·苏亚雷斯不会声称人类与进化是一致的。不幸的是，这篇文章或链接文章的作者并没有从机械上理解<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">左急转</a>的动态。</p><h2> AI控制研究更容易</h2><blockquote><p>提高人工智能可控性的研究比提高人类可控性的研究容易得多​​，因此我们应该期望人工智能比人类更快地变得更加可控</p></blockquote><p>（我假设控制和对齐都是“控制”的意思。）列出了比人类控制技术更容易测试人工智能控制技术的方法。对于非人类系统有效，但与超人类系统无关或不适用于，因为：</p><ul><li>我们不知道如何<strong>系统地提出可以实际控制超级智能人工智能系统的技术</strong>；</li><li>我们不知道超级智能人工智能在验证它是否存在于现实世界时会做什么；如果它知道不是，我们就不知道该技术是否真的允许我们在现实世界中控制它；</li><li>主要实验室和外部的许多人建议制造超级智能系统，该系统不是单一模型，而是<a href="https://www.youtube.com/watch?v=YTlrPeikoyw">在复杂系统中</a>工作的许多模型的组合，它们相互监督和报告；如果用于训练模型的所有计算现在都用于运行模型的副本以构成超级智能系统，则成本和可扩展性考虑并不真正适用，因为您只有一个昂贵的系统。</li></ul><h2>价值观很容易学习</h2><blockquote><p>如果人工智能<strong>首先</strong>学习道德，它会希望帮助我们确保它在变得更强大时<strong>保持</strong>道德。</p></blockquote><p>如果人工智能足够聪明和一致，能够做到这一点，那就对了。但如果它还不是一个与 CEV 一致的超级智能，那么了解人类想要什么并不会激励梯度下降来改变它，使其朝着与 CEV 一致的超级智能发展。我希望智能人工智能能够更容易地理解人类价值观，并且更容易合作；但它不会自动使人类价值观成为优化目标。知道人类想要什么并不能让人工智能提供护理，除非你解决了人工智能提供护理的问题。</p><p>看似“一致”的非人类模型的行为对应于一堆杂乱的东西，这些东西对人类给予奖励的内容进行了优化；但是每次梯度下降使模型变得更加普遍的优化/代理时，经过优化的混乱集合的模糊事物不会影响新架构梯度下降安装的目标内容。梯度下降没有理由保留过去神经网络实现的算法的目标和值：神经网络实现的新的、更智能的人工智能算法可以通过更广泛的可能目标和值获得高分。</p><blockquote><p>由于价值观几乎为社会中的每个人所<i>共享</i>和理解，因此它们不可能非常复杂。与科学和技术不同的是，科学和技术的分工能够积累更加复杂的知识，而价值观必须保持足够简单，以便儿童在几年内就能学会。</p></blockquote><p>我猜对人类价值观的描述可能比一千兆字节的信息或其他东西还要短；人工智能可以了解它们是什么；但它们还不够简单，我们无法轻松地将它们指定为优化目标 - 请参阅<a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes">愿望的隐藏复杂性</a>。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=gpBqw2sTD08"><div><iframe src="https://www.youtube.com/embed/gpBqw2sTD08" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><blockquote><p>当前的语言模型已经非常有能力从道德上评估超级智能可能具备的复杂行为</p></blockquote><p>它们有能力评估呈现给它们的后果，但并不比人类更有能力。也就是说，</p><ul><li>低于人类的法学硕士将无法评估超人类人工智能生成的计划，就像人类无法做到这一点一样，因为了解行动的所有后果需要智力，而不仅仅是理解人类会说什么；</li><li>我希望这篇文章的作者能够清楚地看到一些故障模式。我邀请读者思考一下，如果我们使用当前的法学硕士自动评估超人人工智能生成的计划，然后启动我们当前的法学硕士查看并说“这看起来不错”的计划，会发生什么。</li></ul><h2>结论</h2><blockquote><p>有很多理由可以预期人工智能将易于控制并易于与人类价值观保持一致</p></blockquote><p>不幸的是，在这篇文章中，我没有看到证据表明超级智能人工智能将很容易控制或符合人类价值观。如果一个神经网络实现了一个超人的人工智能代理，它想要的东西与我们想要的不同，那么这篇文章并没有提供任何证据来证明我们能够保持对未来的控制，尽管这个代理所做的事情会产生影响，或者改变它是为了实现一个与 CEV 意义上的人类价值观一致的超人人工智能代理，甚至只是注意到代理出了问题，直到为时已晚。</p><p>虽然我们直接优化人工智能系统的权重来获得奖励，而人脑对奖励的反应不太清晰和透明，但我们不知道如何用它来让超级智能人工智能想要我们希望它想要的东西。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn6uvr6jxw68j"> <span class="footnote-back-link"><sup><strong><a href="#fnref6uvr6jxw68j">^</a></strong></sup></span><div class="footnote-content"><blockquote><p>未来的人工智能将以值得严肃伦理考虑的方式表现情感和欲望</p></blockquote><p>默认情况下，消灭人类的超人人工智能系统不会有情感。他们将成为非常好的优化器。但值得注意的是，如果我们在未来 20 年内成功地不死于非常优秀的优化器的影响，我希望我们在了解如何设计新思维后，有意识地构建带有情感的人工智能系统。请参阅<a href="https://www.lesswrong.com/posts/HsRFQTAySAx8xbXEc/nonsentient-optimizers">无知觉优化器</a>和<a href="https://www.lesswrong.com/posts/gb6zWstjmkYHLrbrg/can-t-unbirth-a-child">无法生育孩子</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnkrdp19oh1z"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkrdp19oh1z">^</a></strong></sup></span><div class="footnote-content"><p>我关注的是普遍低于人类和普遍超人类的系统，因为这似乎是一个相关的区别，同时更容易关注，尽管它失去了一些细微差别。似乎推理比训练更便宜，一旦你训练了一个人类级别的系统，你就可以立即运行它的许多副本，它们可以一起组成一个超人类的系统（足够聪明，可以在相对较短的时间内解决对齐问题，如果它想要，而且也有足够的能力杀死所有人）。许多次人类系统的副本放在一起，将无法解决对齐或任何需要大量人类最佳认知的问题。因此，我想象了人类水平周围的一个模糊阈值，并在这篇文章中重点关注它。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn072edwh8od0f"> <span class="footnote-back-link"><sup><strong><a href="#fnref072edwh8od0f">^</a></strong></sup></span><div class="footnote-content"><p>还有一个开放的、更普遍的问题，我在这里不讨论，即较弱的系统引导较强的系统（不被欺骗并保留偏好）。我们不知道该怎么做。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntjnsez6u2fb"> <span class="footnote-back-link"><sup><strong><a href="#fnreftjnsez6u2fb">^</a></strong></sup></span><div class="footnote-content"><p>不幸的是，我们不知道每个权重代表什么，而且我们对它们实现的算法也没有太多透明度；我们不了解思考过程，尽管有各种内部优化压力，但我们不知道如何以一种有效的方式影响它</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/iABojbKgmtMGYgcYm/some-quick-thoughts-on-ai-is-easy-to-control#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iabojbkgmtmgygcym/some-quick-thoughts-on-is-is-is-is-easy-control<guid ispermalink="false"> iabojbkgmtmgygcym</guid><dc:creator><![CDATA[Mikhail Samin]]></dc:creator><pubDate> Wed, 06 Dec 2023 00:58:53 GMT</pubDate> </item><item><title><![CDATA[Multinational corporations as optimizers: a case for reaching across the aisle]]></title><description><![CDATA[Published on December 6, 2023 12:14 AM GMT<br/><br/><p>在左右左右徘徊时，我注意到的一件事是，他们也谈论价值对齐。一旦您摆脱了完全不同的词汇，他们也试图弄清楚如何处理大型超人实体，以优化人类无关紧要的目标。正是他们所说的是全球，公开交易的Megacorps优化金钱。</p><p><strong>公开持有的公司具有尽可能多的资金的最终价值。</strong>这称为“股东价值最大化”。</p><p>即使组织内部的大多数人都可以重视其他事情，他们的职位描述也是为了最大程度地提高资金的最终目标，并付费并激励这样做。在此中，他们创建了程序和政策，然后告诉其子雇员执行它们。</p><p>程序和政策包含执行这些操作的人，尤其是在最低级别上，这意味着可以将其视为手动执行程序。当然，人类不是硅。使用人类作为计算基板和世界操纵器的过程进行操作是缓慢而不完美的。但是，我相信类比仍然存在。</p><p>股东价值最大化的方式已经严重损害了世界，并损害了人类生活的质量是无数且易于观察的：污染，气候变化和其他此类外部性。</p><p>总之，我认为“友好的AI”问题与“友好的跨国大型企业”问题具有足够的相似性，而某些交叉授粉可能会产生生产力。即使他们的大多数想法都无法与AI一起使用，他们也考虑创建更符合道德的超人代理的事实仍然值得一看。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HEDbwdx8j8oe6ZiYj/multinational-corporations-as-optimizers-a-case-for-reaching#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hedbwdx8j8oe6ziyj/multinational-corporations-as-as-optimizers-a-case-for-for-for-for-raching<guid ispermalink="false"> hedbwdx8j8oe6ziyj</guid><dc:creator><![CDATA[sudo-nym]]></dc:creator><pubDate> Wed, 06 Dec 2023 01:11:47 GMT</pubDate> </item><item><title><![CDATA[How do you feel about LessWrong these days? [Open feedback thread]]]></title><description><![CDATA[Published on December 5, 2023 8:54 PM GMT<br/><br/><p>你好！我是来自 LessWrong / Lightcone 团队的 jacoobjacob。</p><p><strong>这是一个元线程，您可以分享您一直在想的有关 LessWrong 的任何想法、感受、反馈或其他内容。</strong></p><p>您可能分享的内容示例：</p><ul><li> “我真的很喜欢同意/不同意投票！”</li><li> “所有这些对话内容是怎么回事？这令人困惑......</li><li> “嗯……最近网站上的氛围似乎发生了某种变化……特别是[<i>插入 10 段</i>]”</li></ul><p> ...或者其他什么！</p><p>这篇文章的目的是让您能够在一个您知道团队成员会倾听的地方分享您想到的任何事情。</p><p> （我们是一个小团队，必须优先考虑我们的工作，所以我当然不承诺采取这里提到的所有内容。但我至少会倾听所有这些！）</p><p>我已经有一段时间没有看到这样的公共话题了。也许有很多关于这个网站的沸腾的感觉从未表达出来？或者，除了我从阅读正常评论、帖子、指标和对讲评论中发现的内容之外，你们没有什么可以分享的了？好吧，这是找出答案的一种方法！我真的很想询问并了解人们对该网站的感受。</p><p>那么，您最近对 LessWrong 感觉如何？欢迎在下面留下您的答案。</p><br/><br/> <a href="https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/j2W3zs7KTZXt2Wzah/how-do-you-feel-about-lesswrong-these-days-open-feedback<guid ispermalink="false"> j2W3zs7KTZXt2Wzah</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Tue, 05 Dec 2023 20:54:45 GMT</pubDate> </item><item><title><![CDATA[Critique-a-Thon of AI Alignment Plans]]></title><description><![CDATA[Published on December 5, 2023 8:50 PM GMT<br/><br/><p> AI-Plans.com 将于 12 月 16 日至 18 日举办批评马拉松，参与者将提出并讨论对人工智能调整计划的批评。</p><p>评委包括：</p><ul><li>内特·苏亚雷斯，MIRI 主席</li><li>Ramana Kumar，DeepMind 研究员</li><li>Peter S Park 博士，麻省理工学院 Tegmark 实验室博士后</li><li>Charbel-Raphaël Segerie，EffiSciences 人工智能部门负责人</li></ul><h2>好处：</h2><p>批评马拉松提供了一个绝佳的机会，通过深入研究调整计划可能成功或失败的原因，获得对人工智能安全的重要见解。<br>我们将突出调整计划的关键要素，这些要素将反馈给研究人员并可用于改进他们的计划。<br>这也是获得专家评委反馈的绝佳机会。</p><h2> Critique-a-Thon 活动为期 3 天，分为 2 个阶段：</h2><h3><strong>第一阶段 - 添加评论</strong>（截至 12 月 16 日）</h3><p>我们将在 ai-plans.com 上以两个类别的形式添加对调整计划的批评：优势和弱点。</p><p><br> “优势”应表明计划如何作为协调解决方案发挥作用。 “漏洞”应该起到相反的作用。<br>奖品将颁发给那些提出最多批评的人 - 投票给负分的批评将不被计算在内。<br><br>您可以随时开始这个阶段 - 您今天就可以开始添加评论！<br>截止日期为格林尼治标准时间 12 月 16 日午夜。<br><br><strong>奖品</strong><br>第一名：100 美元<br>第二名：60美元<br>第三名：40美元</p><h2><br>第二阶段 - 讨论和总结（12 月 17 日至 18 日）</h2><p>我们两人一组。每对将选择一个已提出批评的调整计划。一个人会提出理由，另一个人会提出反对理由，优势/弱点是真实和准确的。<br>然后，第二天，我们将交换立场，并以所提议的优势/弱点的书面形式结束。<br>请参阅之前获奖评论的示例：</p><p> <a href="https://docs.google.com/document/d/16Ww6nNECsDnjMOGaz5fqg1PxCVRPpoOy80UooEa7CZM/edit#heading=h.vdtj4e22pjdv"><u>八月评论马拉松</u></a></p><p><br><a href="https://docs.google.com/document/d/1O586rkRZd9BbpI8yqA2K6aG9IIq6E1A9I2H9C5nEYKQ/edit#heading=h.vdtj4e22pjdv"><u>九月评论马拉松</u></a><br><br>这些讨论将有助于完善批评并加强论点，并且交换可以减少由于缺乏某个主题的先验知识而造成的任何不利。<br>如果你的对手对这个话题了解得更多，并提出了你没有想到的观点，你可以在第二天自己使用它们，看看有哪些应对措施 - 并确定它们是否是你的文章中需要提及的内容，这就是将被判断的内容。<br>这些讨论将<a href="https://discord.gg/aGVtu5JyjJ"><u>在 Discord 上</u></a>进行。</p><p>在这里加入👉 <a href="https://discord.gg/aGVtu5JyjJ"><u>https://discord.gg/aGVtu5JyjJ</u></a></p><h3><br><strong>奖品</strong></h3><p>第一名：400 美元<br>第二名：250 美元<br>第三名：150 美元<br><br></p><p>在此注册加入👉 <a href="https://ai-plans.com/login"><u>https://ai-plans.com/login</u></a></p><p><br><br></p><br/><br/> <a href="https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/kgyrsAkbBjNxPbicc/critique-a-thon-of-ai-alignment-plans<guid ispermalink="false"> kgyrsAkbBjNxPbicc</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Tue, 05 Dec 2023 20:50:08 GMT</pubDate> </item><item><title><![CDATA[Arguments for/against scheming that focus on the path SGD takes (Section 3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 5, 2023 6:48 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第三部分。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://www.buzzsprout.com/2034731/13984918">请点击这里</a>，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>支持/反对阴谋论的争论集中在 SGD 所采取的路径上</h1><p>在本节中，我将讨论支持/反对计划的论点，这些论点更直接地关注 SGD 在选择训练的最终输出时所采取的路径。</p><p>重要的是，这些论点可能并不相关。特别是：如果 SGD 在模型类别之间以某种“直接比较”的方式主动支持或不支持阴谋者，那么 SGD 将“找到一种方法”来选择它在这个意义上所支持的模型类型（例如，因为足够高的模型类型）。维度空间使这样的“方式”可用）， <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-1" id="fnref-ZwSi8PjDgjvifEocL-1">[1]</a></sup>那么足够的训练只会引导你到 SGD 最喜欢的任何模型，而所讨论的“路径”并不重要。</p><p>在关于不同模型的最终属性之间的比较的部分中，我将讨论我们可能期望 SGD 出现这种偏袒的一些原因。特别是：阴谋者“更简单”，因为他们可以有更简单的目标，但他们“更慢”，因为他们需要参与各种形式的额外工具推理 - 例如，在决定策划时，检查现在是否是一个好时机缺陷，可能参与和掩盖“早期破坏”等方面的努力（尽管请注意，此处需要执行额外的工具推理，可能会表现为由策划者的权重实现的算法的额外复杂性，因此表现为“简单性”）成本”，而不是“运行该算法更长时间的需要”）。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-2" id="fnref-ZwSi8PjDgjvifEocL-2">[2]</a></sup>我将在下面对此进行更多讨论。</p><p>不过，在这里，我想指出，如果 SGD 足够关心简单性和速度等属性，那么 SGD 通常会首先构建一个具有长期追求权力目标的模型，但即使该模型尝试了一个方案 -就像策略一样（在这种情况下，由于预知其失败，它不一定会这样做），它会被无情地磨成一个情节奖励寻求者，因为情节奖励寻求者的速度优势。或者，SGD 通常会首先构建一个按情节奖励的寻求者，但由于 SGD 渴望更简单的目标，该模型将被无情地磨成一个阴谋者。</p><p>在本节中，我将假设这种事情不会发生。也就是说，SGD 构建模型的顺序会对训练的最终结果产生持久的影响。事实上，我的一般感觉是，对阴谋者的讨论经常隐含地假设这样的事情——例如，人们通常认为阴谋者会在训练中很早就出现，然后在那之后将自己锁定。</p><h2>独立于训练游戏的代理目标故事</h2><p>回想一下我上面介绍的区别：</p><ul><li><p>训练与比赛<em>无关的</em>赛外目标，其产生与其在训练-比赛中的角色无关，但随后会激励训练-比赛，而不是训练-比赛。</p></li><li><p>与训练游戏<em>相关的</em>赛外目标，SGD 主动<em>创建</em>这些目标是<em>为了</em>激励训练游戏。</p></li></ul><p>在我看来，关于专注于与训练、比赛<em>无关的</em>目标的策划故事似乎更传统。也就是说，这个想法是：</p><ol><li><p>由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以<em>不</em>通过训练游戏的方式）。</p><ol><li><p>这可能发生在态势感知到来之前或之后。</p><ol><li><p>如果之前，那么在一段时间内它可能会被训练出来，并且还没有激发训练-游戏。</p></li><li><p>如果之后，它可能会立即开始激励训练-游戏。</p></li></ol></li></ol></li><li><p>然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。</p></li></ol><p>将此称为“独立于训练游戏的代理目标故事”。</p><p>我相当认真地对待这个论点。如果某种适当雄心勃勃的超越情节的代理目标没有被训练（要么因为它在态势感知之前没有得到训练，要么因为它在态势感知之后突然出现），并且如果有一些关于为什么这个目标会产生影响的故事策划一个好的工具性策略适用（例如，经典的守门故事，或者可能是其他一些非经典故事），那么策划似乎确实可能只是一件发生的事情。</p><p>我认为最直接的反对意见是：为什么模型要制定这些（适当雄心勃勃的）超越情节的目标？我之前在诸如“目标默认情况下不会有时间折扣”和“模型时间不会像日历时间”之类的论点中讨论过这一点，我不会在这里添加太多内容，除了说假设模型将实现一些适当雄心勃勃的、超越情节的、独立于训练游戏的代理目标，追求这些目标与高奖励相关，在我看来，这似乎是经典故事中较弱的部分之一。</p><p>特别是：对阴谋的讨​​论有时集中在“好奇心”和“权力”等代理目标上，这些目标在很多剧集中都显得有用。但这并不意味着在训练中会激励对<em>情节之外的</em>好奇心和力量的追求。更重要的是，特别是如果你知道/可以控制情境意识何时出现，那么在模型因追求好奇心/力量而受到积极惩罚的训练片段中似乎并不那么困难。非训练游戏模型将直接进行修改。事实上，总的来说，我希望我们能做很多事情来积极阻止模特采取寻求权力的行为，并鼓励其他形式的“谦虚”。</p><p>后一点与我之前提到的乐观原因有关：即，独立于训练游戏的代理目标故事允许我们在态势感知之前进行任何我们想要的对抗性训练，以尝试确保模型的目标保持不变情节内且适度适度。</p><p>也就是说，试图在态势感知之前通过对抗性训练来避免这种阴谋，面临着一些严重的障碍：</p><ul><li><p>首先：最重要的是，所讨论的对抗性训练可能不够多样化/彻底。特别是（特别是缺乏可解释性工具）：可能很难想到所有可能的适当雄心勃勃的剧集外代理目标，这些目标与奖励有足够好的相关性，并且您需要使用此类培训来推动。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-3" id="fnref-ZwSi8PjDgjvifEocL-3">[3]</a></sup></p></li><li><p>第二：当态势感知开始发挥作用时，你可能不知道，或者无法很好地控制。</p></li><li><p>第三：一旦足够智能的模型经过文本预测的预训练，它可能已经非常接近态势感知，因为它拥有大量相关的常识（即使不一定是自定位知识）。因此，在那之后可能没有太多时间进行没有态势感知的对抗训练。</p></li><li><p>最后，<em>在</em>态势感知启动后，模型有可能制定适当的、雄心勃勃的、超越情节的、策划激励的目标。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-4" id="fnref-ZwSi8PjDgjvifEocL-4">[4]</a></sup></p><ul><li><p>例如，也许当模型开始按照我上面描述的方式“反思”时，它已经相当聪明并且具有战略意识——例如，“弄清楚它真正想要什么”，自我解决，学习打破以前尝试的新事物对齐它，等等。 （这要求模型不会因为获得态势感知而立即开始寻求奖励，但这似乎是可能的，特别是如果我们假设训练留下了显着的“余裕”。）</p></li><li><p>当然，仍然存在一个问题，即为什么该模型会将自己打造成具有适当雄心勃勃的目标，特别是超越情节的目标。但在这里，根据我上面对“如果你训练它拥有长期目标会怎样”的讨论，也许我们可以诉诸这样一个事实：我们试图将其指向一项雄心勃勃的长期任务，但这种指向有些不准确。 / 模糊，这样当它弄清楚它想要如何概括时，这种概括就落在了足够长期和雄心勃勃的事情上，从而激发了阴谋。</p></li><li><p>事实上，正如我上面指出的，我认为有一个案例表明，当前的调整工作已经<em>在</em>试图将我们的系统指向长期的、超越情节的目标——例如，避免帮助制造炸弹的“无害”形式即使炸弹数十年不会爆炸。</p></li></ul></li></ul><p>因此，总的来说，我认为，以谋划作为追求（适当雄心勃勃的）超越情节目标的良好工具性策略为条件，独立于训练游戏的代理目标故事在我看来是一个真正令人担忧的问题。</p><h2> “最近的最大奖励目标”故事</h2><p>我们还可以讲述一个不同的故事，重点关注<em>依赖于</em>训练和比赛的赛外目标。我将其称为“最近的最大奖励目标”故事。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-5" id="fnref-ZwSi8PjDgjvifEocL-5">[5]</a></sup>故事是这样的：</p><ol><li><p>当模型变得具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。相反，它将追求一些低于最大奖励的代理目标。</p></li><li><p>但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏<em>依赖</em>”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）</p></li><li><p>此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。</p></li><li><p>所以：SGD 将使模型成为一个策划者。</p></li></ol><p>现在，一种选择是尝试使 (1) 为假。事实上，如上所述，在态势感知之前尝试以良好的方式塑造模型的目标似乎是降低阴谋可能性的关键潜在途径。但请注意，在这种情况下，所面临的挑战比<em>独立于</em>训练游戏的代理目标更困难。也就是说，对于与训练游戏无关的代理目标，只需在态势感知之前避免给模型提供类似策划者的目标，因此它可以容忍所讨论的目标可能尚未达到最大奖励的<em>其他</em>方式。相比之下，在“最近的最大奖励目标”论点的背景下，人们需要要么实际创建一个最大奖励、非类似阴谋的目标，要么<em>足够接近</em>（3）不适用的目标。也就是说，你不能仅仅阻止“自然阴谋”的出现，而必须阻止它的出现。你需要确保当 SGD 稍后“搜索”最近的最大奖励目标时，它不会首先找到阴谋者。</p><p>现在，我们假设我们还没有完全掌握这一点，并且（1）是正确的（尽管：我们所达到的掌握<em>程度</em>对我将在下面讨论的“接近度竞赛”很重要）。 (2) 源自态势感知的定义（或者至少是一个宽松的定义，例如“参与阴谋并认识到阴谋的好处所必需的对世界的理解”），所以我们也承认这一点。</p><p>但是（3）呢？我认为，在这里，论证还有更多工作要做。为什么认为将模型打造成阴谋者将是使其获得最大奖励的最简单方法？例如，为什么不把它变成一个训练圣人，或者一个剧集奖励寻求者，或者一个追求某种最大奖励代理目标的错误概括的非训练游戏玩家？根据假设，该模型具有代表<em>任何</em>这些目标的概念工具。 （请注意，在这方面，它与大多数进化历史中的人类不同，人类<em>没有</em>概念工具来表示“包容性遗传适应性”等目标。）因此，我们需要一个故事来说明为什么该论点会优先考虑类似阴谋家的人特别是目标。</p><p>为了清楚起见：当我谈论 SGD 可以进行某种修改的“轻松”程度，或者所得到的模型的“接近度”时，这是“SGD‘偏好’的那种修改”的代表。本身就是“SGD 实际上将进行的那种修改”的替代品。在机械层面上，这大致意味着：奖励景观中最陡梯度的方向。我经常会想象一种更模糊的感觉，即 SGD 可以做的“工作”预算有限，因此希望在修改模型目标方面做尽可能少的“工作”，以便它可以专注于改进模型认知的其他方面。</p><p>那么，根据对“轻松”的理解，我们应该期望哪个模型类对于 SGD 来说是“最容易”创建的，即从一个新的情境感知模型中创建一个低于最大奖励代理目标的模型。自己不能激发阴谋吗？</p><p>作为一个有助于激发直觉的松散类比：想象一下将人类技术冻结在当前水平，并让进化选择在人类身上运行更长时间。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-6" id="fnref-ZwSi8PjDgjvifEocL-6">[6]</a></sup>从长远来看，你预计哪种人类（或人类后裔生物）将占据主导地位？特别是：您期望：</p><p> (a) 人类本质上重视“<a href="https://en.wikipedia.org/wiki/Inclusive_fitness">包容性遗传适应性</a>”或“我的基因被进化选择”（这是对“指定目标”和“奖励过程”的某种组合的松散类比），</p><p> （b）人类重视与包容性遗传适应性密切相关的其他事物（例如，“拥有尽可能多的孩子”），但他们没有明确地优化包容性遗传适应性，甚至是工具性的（这是对错误的松散类比）广义的非训练游戏玩家），或</p><p>(c) 具有长期目标的人类，他们正在优化包容性遗传适应性，特别是作为一种在长期内为自己的价值观获得权力的方法（这对阴谋家来说是一个宽松的类比）。</p><p>在这里，“最近的最大奖励目标”故事的类比会问这样的问题：对现有人类价值观的最小修改是什么（或者更好：特别是对进化最容易的修改）是什么，使我们陷入困境之一如上所述，同时也与所面临的进化选择压力的大小兼容？ <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-7" id="fnref-ZwSi8PjDgjvifEocL-7">[7]</a></sup></p><p>当然，这个类比带来了大量令人困惑的变量（例如，与进化选择构建学习价值的大脑的基因组而不是价值本身有关的事实，文化积累和模因选择的作用）人口动态、缺乏对未来逃跑/接管机会的明确模拟，等等）。为了使这一类比与机器学习更加相似，最好想象进化直接选择整个人类大脑，配备完整的概念工具来代表所有这些目标，而不是选择构建大脑的基因组。更好<em>地</em>想象一个从一开始就选择大脑的进化版本，这样我们开始理解进化时的人类价值观可能会完全不同。不管怎样，我对最佳预测没有任何明确的信念。但我认为这是“最近的最大奖励目标”论点的一个令人回味的直觉泵。</p><h3> SGD 渐进主义对类似阴谋家的修改的障碍</h3><p>不过，让我们关注实际的论证，而不是类比。我们应该期望哪个模型类在相关意义上是“最接近的”？</p><p>上面我讨论了阴谋者可能被快速排除的一种方式：即，如果 SGD 无法通过修改模型时遵循的梯度来“注意到”类似阴谋者修改的好处。也就是说，正如我之前讨论的，在寻找最大奖励目标时，SGD 不仅仅“跳”到最近的目标。相反，它需要在梯度计算的基础上逐步实现这样的目标，梯度计算表明模型权重在相关方向上的微小变化会增加奖励。在许多情况下，尚不清楚类似阴谋者的修改是否可以像这样工作。</p><p>因此，例如， <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world">Hubinger (2022)</a>考虑了一个例子，其中 SGD 通过将模型从关心剧集中的金币修改为始终关心金币来引发阴谋。不过，在所讨论的示例中，SGD 并不是逐渐延长模型金币关注的时间范围，每次延长都会导致奖励的提高。相反，SGD 只是做了“一个简单的改变”——即完全放弃目标的时间限制——从而创建了一个阴谋者。但问题是：奖励空间的梯度是否反映了这样做的好处？在我看来，发生这种情况的最自然的方式是，是否有能力从一种模型平滑地过渡到另一种模型，这样每次修改都会逐渐获得更多的策划好处。但目前尚不清楚这是否会发生。正如我之前讨论的，如果我们假设 SGD 还需要构建大量新机器来执行策划所需的工具推理（而不是仅仅重定向预先存在的“目标实现引擎”），那么任务变得更具挑战性。</p><h3>哪个模型是“最接近的”？</h3><p>然而，根据我之前的讨论，我也不觉得我能够排除这种类型的增量转换可能发生的可能性（例如，也许足够高的维空间允许 SGD“找到一种方法”） ”），而且我还没有尝试进行深入分析。因此，虽然我认为这种类型的渐进主义对专注于依赖训练游戏的阴谋者目标的故事提出了相对强烈的反对，但我认为也值得评估这些故事的其他方面。也就是说，假设SGD<em>能够</em>注意到将“最近的最大奖励目标”故事中的模型制作成schemer的好处，这样的修改是否是获得高奖励的最简单方法？</p><h4>目标空间中类似阴谋家的目标的共性</h4><p>对此的一个论点可以追溯到我所认为的支持期待阴谋的核心直觉之一：即可能的阴谋者式目标的绝对<em>数量</em>。粗略地说，这个想法是，因为<em>如此多的目标</em>可以激发策划，所以很可能会有一些类似策划者的目标非常<em>接近</em>模型的当前目标（其中“附近”，回想起来，是根据工作量 SGD 来定义的）必须做才能引起相关类型的修改）。也就是说，阴谋者在目标空间中非常常见——因此 SGD 相应地很容易从任何给定的起点“找到”类似阴谋者的目标。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-8" id="fnref-ZwSi8PjDgjvifEocL-8">[8]</a></sup></p><p>上面描述的“剧集中的金币”→“所有时间的金币”转变就是这种思想的一个例证。也就是说，如果你假设最充分的长期目标会激发阴谋， <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-9" id="fnref-ZwSi8PjDgjvifEocL-9">[9]</a></sup>那么<em>不</em>成为阴谋家的最显着的方法就是有一个足够的短期目标。但是（假设我们已经抛开上面讨论的“增量主义”问题），任何给定的时间有限的目标似乎都非常“接近”时间无限的版本。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-10" id="fnref-ZwSi8PjDgjvifEocL-10">[10]</a></sup></p><p>但时间范围并不是这里可能的“邻近性”的唯一来源。例如，如果模型碰巧有一些尚未激发阴谋的<em>长期</em>目标（例如，因为目标不够“雄心勃勃”），我们可以改变其他相关变量。因此，如果模型开始关心以不受时间限制的方式正确解决一些数学问题，但它可以在密谋接管世界以获得更多计算来检查和重新计算的情节中很好地解决问题检查它的答案似乎不值得，我们可能会想象 SGD，例如，改变模型对得到错误答案的风险的容忍度，或者改变它的目标，以便它想要解决<em>更多的</em>数学问题，或者你有什么。总的要点是：在尝试创建计划程序时，SGD 有大量选项可供选择，因此很容易在模型起点附近找到一个选项。</p><p>因此，在进化和人类的背景下，这种思想的类比是这样的：即使是相当短期的、适度的人类价值观，也可以相对容易地将它们转变为长期的、雄心勃勃的价值观。例如，人类长期主义者似乎会采取“对我周围环境中满足各种条件的人表现出模糊的友善和帮助”之类的价值观，并将其转化为更加雄心勃勃的目标——这些目标似乎有动力明确优化以获取选择-通过进化，如果这是未来电力的关键瓶颈。事实上，在我看来，在实践中，如果你进行进化选择足够长的时间，在长期范围内优化的意识形态群体最终将占据主导地位（许多最接近优化生殖适应性的群体） ，例如，看起来他们有这种味道）。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-11" id="fnref-ZwSi8PjDgjvifEocL-11">[11]</a></sup></p><p>一旦我们搁置了上面关于渐进主义的反对意见，我认为目标空间中类似阴谋家的目标的共性是一个相当有力的理由，担心新的情境意识的最近的最大奖励修改，仍然不是-最大奖励目标追求模型将走向心计。更重要的是，请注意，一旦我们想象 SGD 在目标空间中主动<em>搜索</em>会激发阴谋的目标，我们关于阴谋起源的故事就可以更容忍阴谋者般的目标，必须有其他更具体的目标属性，例如资源匮乏性。也就是说，在专注于训练与游戏<em>无关的</em>目标的环境中，有可能质疑 SGD 落在类似阴谋者的目标上的概率（甚至以它落在更普遍的超越情节目标为条件），凭借不同的超越情节的目标意味着对未来力量、风险等的不同偏好。但在像这样的<em>依赖于</em>训练游戏的环境中，SGD 正在积极<em>寻找</em>这样的目标（并且相关目标在目标空间中仍然很常见——例如，您可以“在所有空间和时间上最大化<em>x</em>的实例”） “ 对于<em>x</em>的许多值）。因此，如果 SGD 找到了它想要的东西，也就不足为奇了。</p><p>然而，我认为这种论点并不是决定性的，原因有二。</p><h4>非策划目标的接近程度</h4><p>第一个原因是，即使我们坚持纯粹专注于修改模型的目标，并采用“干净的目标导向”框架（我将在下面使这样的焦点复杂化），我认为还有一个抵消的“接近性”考虑这至少支持一些非计划目标——即，在其他条件相同的情况下，模型的初始训练（在态势感知之前）似乎更有可能主动将模型指向这些非计划目标的方向。</p><p>这对于指定的目标来说是最清楚的，根据定义，这是通过培训来奖励的。这意味着，即使模型在具有态势感知能力时最终没有<em>完全</em>达到指定的目标，它也可能最终得到广泛附近的目标。</p><ul><li><p>因此，举例来说，如果模型正在接受训练以最大化剧集中的金币，并且最终学习了一个目标，例如“最大化剧集中的金币”（这个目标<em>不太</em>正确，因为有些训练环境包括圆形金币（非硬币），看来 SGD 最简单的修改就是将模型变成<em>金币</em>on-the-episode-getter（即训练圣人）。 ）。或者至少，就“接近度”而言，“情节中的金币”似乎与“有史以来的圆形金币”<em>具有竞争力</em>。</p></li><li><p>对于更多 RLHF 风格的指定目标，我们可能会提出类似的观点。也就是说，如果您正在训练一个模型以一种广泛有帮助、诚实且无害的方式遵循剧集中的说明​​（为简单起见，您的 RLHF 流程仅在这方面提供准确的反馈），那么即使它当它变得具有情境意识时，它已经学会了一种有点不平衡的概念，例如诚实，改进其诚实概念，直到它更好地适合人类概念，似乎具有竞争性，回复：接近，具有更多类似阴谋家的动机修改，例如“偏离-始终保持诚实，以激励计划逃脱人类控制并夺取权力。”</p></li><li><p>请注意，在人类和进化的背景下，人类确实已经重视与包容性遗传适应性密切相关的各种事物——例如，生孩子、更广泛地拥有繁荣而强大的家庭血统等等。如果我们想象进化选择整个人类大脑配备了代表“包容性遗传适应性”的概念装置，人类最初进化到重视与包容性遗传适应性非常接近的东西似乎更容易想象。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-12" id="fnref-ZwSi8PjDgjvifEocL-12">[12]</a></sup></p></li></ul><p>请注意，我们也可以在这里对错误概括的非训练游戏玩家提出类似的观点，只要他们追求最大奖励目标（回想一下，根据我的定义，错误概括的目标可以是最大奖励，前提是特别是<em>训练数据</em>永远不会惩罚它们——例如，它们只会在模型从未见过的反事实场景中受到奖励过程的惩罚。</p><ul><li><p>因此，例如，想象一个训练过程，它永远不会区分“在剧集中获得金币”和“在剧集中获得一般金币”之类的目标，因为训练数据中的所有金币都是硬币。假设在态势感知之前，我们的模型学习了一个目标，比如“在剧集中获得金币，但有时也会跟随你对剧集的好奇心，即使这意味着牺牲金币”——这个目标可以实现（但不是最大程度地实现）好吧）在训练中，因为好奇心并不是一种非常有害的行为，有时甚至是一种有益的行为。在这里，似乎只是删除模型的好奇心并只专注于获得剧集中的黄金物品（最大奖励目标，即使不是指定目标）是一个相当“附近”的修改 - 并且是一个看似合理的修改在“接近”方面具有竞争力，例如修改模型，使其始终追求其黄金和好奇心的组合。</p></li><li><p>在人类和进化的背景下，如果我们想象，从进化选择的角度来看，“拥有尽可能多的孩子”的内在价值会表现得很好（即使这并不<em>完全</em>是进化选择的目的），但事实并非如此。似乎很难想象人类会朝这个方向前进。</p></li></ul><p>我们能否对寻求剧集奖励的人提出类似的观点？这有点不太清楚，因为在态势感知之前，尚不清楚模型是否有足够的奖励过程概念，以便他们有动机附加到其组件之一“附近”的某些东西。也就是说，在我看来，在某些情况下这种情况可能会发生。因此，例如，即使缺乏态势感知，通过 RLHF 训练的模型最终也会受到“人类认可”附近概念的激励，这对我来说似乎是合理的。这些概念似乎至少在某种程度上接近奖励过程的各个方面，例如人类评估者和/或奖励模型的判断，这样一旦模型了解了奖励过程，修改其动机以专注于这些组件就不会太困难。对于 SGD 来说，这是一个巨大的飞跃。</p><p>总的来说，我认为默认情况下，非策划目标往往会具有某种对他们有利的“接近性”。这并不奇怪。特别是：非策划者目标必须与奖励过程有一些相当直接的联系（例如，它们要么直接受到该过程的奖励，要么因为它们专注于奖励过程本身的某些组成部分），因为与策划者目标不同，非策划者目标不能依赖目标内容完整性或长期权力追求等聚合子目标来确保追求它们会带来回报。因此，我们很自然地期望通过奖励过程来训练模型，在尚未实现阴谋的情境意识背景下，会导致动机集中在非阴谋目标附近的某些事情上。</p><p>尽管如此，在给定情况下，这种考虑是否足以使非阴谋家目标比阴谋家目标更<em>接近</em>模型的当前目标，这仍然是一个悬而未决的问题。重要的是，请注意，相关的竞争是针对<em>整套</em>附近的类似阴谋者的目标（而不是例如我上面讨论的可能的类似阴谋者修改的特定示例）——考虑到可能的阴谋者的种类繁多-类似的目标，可能是一个严重的劣势。因此，打个比方：如果鲍勃家十英里范围内有十家墨西哥餐馆和一百家中国餐馆，那么即使任何给定的墨西哥餐馆与任何给定的中国餐馆“似乎具有竞争力”，re：接近度，那么（模进一步信息）<em>最近的</em>餐馆可能仍然是中国餐馆。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-13" id="fnref-ZwSi8PjDgjvifEocL-13">[13]</a></sup>根据模型空间中类似阴谋者的目标的共性，我们可能期望类似阴谋者的目标就像这里的中餐馆一样。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-14" id="fnref-ZwSi8PjDgjvifEocL-14">[14]</a></sup></p><h4>混乱的目标导向性与接近度的相关性</h4><p>然而，我也认为，一旦我们考虑到“混乱的目标导向性”，非阴谋家就会得到另一种明显的亲近感提升。特别是：如果我们想象所讨论的模型不是一个完全可分离的目标槽+目标实现引擎，而是不同启发式、注意力模式、冲动等的错综复杂的混合物，那么我们需要分析比较“接近度”不仅是当前模型的当前<em>目标</em>与最近的类阴谋者和非类阴谋者目标的比较“接近度”，而且是模型的<em>整个组合</em>与最近的类阴谋者和非类阴谋者模型的接近程度。在这里，我认为非策划模型可能具有更显着的优势。</p><p>特别是：在一定程度上，预先情境意识训练往往会产生一种混乱的目标导向性，指向非策划目标<em>附近</em>的某些东西——特别是一个<em>片段内的</em>非策划目标——那么这意味着模型的启发式、注意力模式、冲动等也倾向于指向那个大方向，并且特别集中在情节内。对我来说，这使得将模型重新定位为更长期的权力追求似乎是一个更实质性和更困难的修改，特别是如果所讨论的情节非常短的话。也就是说，直观上，对我来说，对于 SGD 来说，“调整一个完全可分离的剧集内目标以使其长期化”比“重新引导广泛关注剧集内目标的错综复杂的目标以使其成为长期目标”要容易得多。它（a）专注于某些超出情节的事情，并且（b）使得这种超出情节的焦点回溯到出于工具性原因而获得奖励”——特别是如果（b）需要建立新的认知机制来实施工具性推理有问题。而“将集中在情节内事物上的错综复杂的事物重新引导到同一范围内的更高奖励的情节内事物”（例如，转向金币，而不是金圆形事物；或者转向真正的诚实，而不是失衡）诚实）在我看来直观上更容易。</p><h3>对“最近的最大奖励目标”论点的总体看法</h3><p>总结我对期待阴谋者的“最近的最大奖励目标”论点的看法：</p><ul><li><p>如果由于增量修改没有产生这些好处，SGD 无法注意到将模型转变为计划者的好处，那么论证就会失败。</p></li><li><p>然而，我认为我们不能指望新元无法注意到这些好处。如果<em>可以</em>，那么我认为目标空间中类似阴谋家的目标的共性使得类似阴谋家的目标与模型当前目标“最接近”的可能性相当令人担忧。</p></li><li><p>也就是说，因为它是在缺乏态势感知的情况下由奖励过程决定的，所以该模型的目标也可能已经在某些最大奖励非计划目标的“附近”，这是有利于一些非阴谋模型的“接近”。在某种程度上，所涉及的目标导向性相当“混乱”，创建这样一个非阴谋者可能需要对模型的启发式、注意力模式、工具推理等进行较少的修改。</p></li></ul><p>这些考虑让我对阴谋者产生了一些实质性的额外担忧。</p><h2>简单性和速度等属性与 SGD 所采用路径的可能相关性</h2><p>如果我们假设任何给定的最大奖励目标在训练中都能产生足够好的性能，并且 SGD 并不特别关心模型最终达到<em>哪个</em>最大奖励目标，那么像“最近的最大奖励目标论证”这样的争论就会最自然地进行。但正如我上面所指出的，正如我将在有关不同模型类的最终属性的部分中更详细地讨论的那样，有一些故事表明，基于简单性等最终属性，SGD 积极地倾向于其中一些模型类而不是其他模型类和速度。</p><p>更重要的是，这种偏好不仅仅与忽略 SGD 在模型空间中采用的路径的预测相关。相反，它们可能会在各个阶段<em>影响</em>该路径（即使最终结果在更广泛的意义上仍然是“路径依赖”）。例如，如果 SGD 偏向于更简单的目标，那么这种偏差可能会影响模型在态势感知之前或之后形成的与训练游戏无关的目标，以及在依赖于训练游戏的目标故事上，它可能是有利于 SGD 从尚未获得最大奖励的起点专门转向类似阴谋家的目标的额外要点。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-15" id="fnref-ZwSi8PjDgjvifEocL-15">[15]</a></sup>同样，如果 SGD 由于需要额外的推理而对阴谋者产生偏见，那么这种偏见可能是有利于期望 SGD 从这样的起点转向非阴谋者的额外观点。</p><p>正如我将在第 4 节中讨论的“最终属性”，我通常认为策划的速度成本比简单性收益更重要，因此在其他条件相同的情况下，我认为这些考虑因素不利于策划。但我不清楚这两个方向的成本/收益是否特别重要。</p><p>也就是说，至少有一些案例表明，这些成本/收益在训练的早期<em>更为</em>重要，因为模型在早期可能会<em>较弱</em>，因此减少了简单性和速度所提供的资源种类（例如，参数和计算）将对模型的认知能力产生更大的总体影响。也就是说：也许，在训练的早期，认知资源更加稀缺，因此更有必要保存。因此，例如，当一般可用的工具推理预算较小时，也许要求一个模型形成一个长期的、类似阴谋家的计划是一个更大的要求（并且对奖励的打击更大）；或者，当较少的参数尚未形成有用的认知结构时，要求它使用更多的参数来存储更复杂的目标可能会更加繁重。 <sup class="footnote-ref"><a href="#fn-ZwSi8PjDgjvifEocL-16" id="fnref-ZwSi8PjDgjvifEocL-16">[16]</a></sup>因此，在某种程度上，人们会被这样一种观点所诱惑，即相对于其他考虑因素，这些类型的成本可能“在噪音中”（我很受这种观点的诱惑，我将在下面讨论），人们可能会与模型的最终属性相比，训练的早期部分较少受此诱惑。</p><h2>对关注 SGD 所采取路径的论点的总体评估</h2><p>不过，总的来说，尽管非阴谋者可能具有速度优势，但我发现“与训练游戏无关的代理目标”论点和“最近的最大奖励目标论点”的结合相当令人担忧。尤其：</p><ul><li><p>在我看来，尽管我们在平凡的对抗性训练方面做出了努力，特别是在一个我们有目的地塑造我们的模型以实现长期且相当雄心勃勃的目标的政权中，但某种适当的雄心勃勃的、错位的、超越情节的目标可能会突然出现。自然地脱离训练——无论是在态势感知之前，还是之后——然后导致阴谋的发生。</p></li><li><p>即使这<em>不会</em>自然发生，我也担心当它达到态势感知时，SGD 给模型一个最大奖励目标的最简单方法就是将其变成一个策划者，因为类似策划者目标在目标空间中非常常见，因此它们通常会在情境意识出现时出现在模型低于最大奖励目标的“附近”。 SGD 的“渐进主义”可能消除了这种担忧，和/或我们应该期望非计划模型默认情况下“更接近”（或者因为它们的目标特别接近，或者因为在“混乱的目标导向性”中）设置，它们需要对模型当前错综复杂的启发法进行更简单的修改，或者因为它们的“速度”优势将使 SGD 更喜欢它们）。但我觉得没有信心。</p></li></ul><p>不过，这两个论点都集中在 SGD 通过模型空间的<em>路径</em>上。相反，关注相关模型最终属性的争论又如何呢？现在让我们转向那些。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-ZwSi8PjDgjvifEocL-1" class="footnote-item"><p>例如，这要求模型不能像我上面讨论的内省守门方法那样进行“<a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">梯度黑客</a>”。 <a href="#fnref-ZwSi8PjDgjvifEocL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-2" class="footnote-item"><p>我还讨论了他们对特定目标/奖励缺乏“内在热情”是否会产生影响。 <a href="#fnref-ZwSi8PjDgjvifEocL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-3" class="footnote-item"><p>感谢 Rohin Shah 在这里进行讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-4" class="footnote-item"><p>事实上，如果我们假设预训练本身<em>会导致</em>情境意识，但不会导致超越情节的、阴谋激励的目标，那么这将是阴谋者如何在预训练然后微调制度中出现的默认故事。感谢 Evan Hubinger 对此进行了标记。 <a href="#fnref-ZwSi8PjDgjvifEocL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-5" class="footnote-item"><p>我认为这个故事与胡宾格所说的“世界模型悬垂”故事有关，但又不同，这个故事（据我所知）大致如下：</p><ol><li><p>当模型变得具有情境意识时，其目标可能不会达到追求目标与获得高奖励完全相关的程度。</p></li><li><p>但是，到那时，它的世界模型将包含训练游戏所需的所有信息。</p></li><li><p>因此，在那之后，SGD 将能够通过修改模型以获得激励训练游戏的超集目标来获得大量物超所值的奖励。</p></li><li><p>相比之下，通过将模型修改为更像训练圣人，它可能能够获得更少的经济效益，因为在这个方向上的边际努力仍然可能使模型的目标与奖励不完全相关（或者在至少，由于必须等待未来打破相关性的训练片段的修正，因此需要更长的时间才能达到完美）。</p></li><li><p>因此，SGD 将创建激励训练-游戏的超集目标（然后这些目标将会具体化）。</p></li></ol><p> Hubinger 框架的一个问题是，在我看来，他的本体论在我感兴趣的意义上忽略了按集奖励寻求者，而 SGD 将模型修改为按集奖励寻求者至少可以做到同样，在这个论点上，将其修改为阴谋者。我不清楚他关于“收益递减”的想法到底应该如何发挥作用（尽管我上面使用的“近乎”修改的本体论是一种重构）。</p><p>也就是说，我认为最终，“最近的高奖励目标”故事和“世界模型悬垂”故事可能试图指出相同的基本思想。 <a href="#fnref-ZwSi8PjDgjvifEocL-5" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-6" class="footnote-item"><p>感谢 Daniel Kokotajlo、Rohin Shah、Tom Davidson 和 Paul Christiano 对此类示例的讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-7" class="footnote-item"><p>请注意，虽然当前的制度看起来最像（b），但所讨论的“与包容性遗传适应性的相关性”（例如，快乐、地位等）似乎明显不完美，并且根据生殖适应性似乎很容易表现得更好比目前大多数人所做的还要多。另外，人类直到最近才<em>了解</em>进化选择（这是对态势感知的一个松散的类比）。所以问题是：现在我们了解了作用在我们身上的选择压力，并且假设这种选择压力持续很长一段时间，它会将我们带向何方？ <a href="#fnref-ZwSi8PjDgjvifEocL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-8" class="footnote-item"><p>我的印象是，一些本体论会尝试将“从给定起点轻松找到阴谋者”与阴谋者往往很简单的想法联系起来，但我不会在这里尝试这样做，我的模糊感觉是这种的举动搅浑了水。 <a href="#fnref-ZwSi8PjDgjvifEocL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-9" class="footnote-item"><p>不过：他们会雄心勃勃吗？ <a href="#fnref-ZwSi8PjDgjvifEocL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-10" class="footnote-item"><p>请注意，人类长期主义者一开始的非系统化价值观与大多数在短期内进行优化的人类非常相似——因此，至少在人类的情况下，一个方向与另一个方向的差异似乎非常小。 <a href="#fnref-ZwSi8PjDgjvifEocL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-11" class="footnote-item"><p>感谢 Daniel Kokotajlo 在这里进行讨论。 <a href="#fnref-ZwSi8PjDgjvifEocL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-12" class="footnote-item"><p>在这里，我抛开对人类价值观如何在基因组中编码的担忧，并想象进化选择与机器学习更相似，但事实并非如此。 <a href="#fnref-ZwSi8PjDgjvifEocL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-13" class="footnote-item"><p>也就是说，如果中餐馆的距离是相关的（例如，因为它们都在同一个街区），那么这个反对的作用就不太顺利。似乎有道理的是，所有类似阴谋家的目标之间至少存在一些相似之处，可能会产生这种类型的相关性。例如：如果模型以情节内目标开始，那么任何类似计划者的目标都将需要扩展模型关注的时间范围 - 因此，如果这种扩展通常需要 SGD 进行某种类型的工作，那么如果非阴谋家目标需要的工作量少于<em>该目标</em>，那么它可能会击败<em>所有</em>最接近的阴谋家目标。 <a href="#fnref-ZwSi8PjDgjvifEocL-13" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-14" class="footnote-item"><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_high_path_dependence_world">Hubinger (2022)</a>还对这样的观点提出了不同的反对意见，即在此类竞争中，SGD 可能会选择非策划者目标而不是类似策划者的目标——即，实现非策划者最大奖励目标的过程将是一条“漫长而艰难的道路”（例如，参见他在高路径依赖部分的可纠正对齐位中对鸭子学习关心其母亲的讨论）。不过，我觉得我并没有真正理解胡宾格的推理。我最好的重构是这样的：为了选择一个非计划目标，Hubinger 想象 SGD 不断选择逐渐不完美的目标（但仍然不是完全最大奖励目标），然后必须等待通过训练来纠正遇到一个事件，这些目标的缺陷被暴露出来；而如果它只是为了一个类似阴谋家的目标，它就可以跳过这个漫长的过程。但这还不能解释为什么 SGD 不能通过直接追求最大奖励非 schemerr 目标来跳过漫长的过程。也许问题应该是关于训练数据的噪音和可变性？我不知道。目前，我希望至少对这个论点的一些解释能够在上面“接近性”的讨论中得到涵盖，和/或胡宾格论点的最佳形式将通过我自己以外的工作得到澄清。 （另请参阅<a href="https://markxu.com/deceptive-alignment#corrigibly-aligned-models">Xu (2020)</a>版本的 Hubinger 论点，在“可正确对齐模型”部分。尽管如此：快速阅读一下，在我看来，Xu 似乎专注于预先情境意识目标形成过程，并假设基本上<em>任何</em>错位的后情境意识都会导致阴谋，这样他的故事实际上是一个<em>独立于</em>训练游戏的故事，而不是我在这里关注的那种<em>依赖于</em>训练游戏的故事。） <a href="#fnref-ZwSi8PjDgjvifEocL-14" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-15" class="footnote-item"><p>至少如果我们理解简单性的方式是<em>增加一些</em>概念，即类似阴谋家的目标在目标空间中很常见，而不是仅仅通过其共性来<em>定义</em>目标的简单性（或：一种目标？）在球门空间。有关此类区别的更多信息，请参阅下面的第 4.3.1 节。 <a href="#fnref-ZwSi8PjDgjvifEocL-15" class="footnote-backref">↩︎</a></p></li><li id="fn-ZwSi8PjDgjvifEocL-16" class="footnote-item"><p>我从保罗·克里斯蒂亚诺那里听到了这种考虑。乍一看，这种效果在我看来在简单性/参数和速度/计算之间相当对称（我不清楚这是否是值得关注的正确区别），所以我不认为早期训练动态是作为一种重要资源，对一种人与另一种人<em>有不同的</em>偏爱。 <a href="#fnref-ZwSi8PjDgjvifEocL-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KyuMS9XzqaJGMu74f/arguments-for-against-scheming-that-focus-on-the-path-sgd<guid ispermalink="false"> KyuMS9XzqaJGMu74f</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Tue, 05 Dec 2023 18:48:12 GMT</pubDate> </item><item><title><![CDATA[In defence of Helen Toner, Adam D'Angelo, and Tasha McCauley (OpenAI post)]]></title><description><![CDATA[Published on December 5, 2023 6:40 PM GMT<br/><br/><p>这与我对 OpenAI 板发生的事情的一些思考很接近，我想知道其他人的想法。<br><br> “我认为：</p><p> 1) TDM 的行动使开放人工智能的情况变得更好，因为与他们什么都不做的反事实相比，它的情况要好得多。</p><p> 2）就预期或实现的好或坏结果而言，人们应该会发现前者令人惊喜，而后者基本上已被定价，因为从安全角度来看，OpenAI 的情况已经非常糟糕。</p><p> 3) 无论你是一个‘荣誉和正直的最高主义者’还是‘无情的战略家’，无论从哪种角度来看，TDM 的行动通常都会表现得非常出色。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/csjjHqLnRr8dvmyoN/in-defence-of-helen-toner-adam-d-angelo-and-tasha-mccauley<guid ispermalink="false"> csjjHqLnRr8dvmyoN</guid><dc:creator><![CDATA[mrtreasure]]></dc:creator><pubDate> Tue, 05 Dec 2023 21:56:54 GMT</pubDate> </item><item><title><![CDATA[Studying The Alien Mind]]></title><description><![CDATA[Published on December 5, 2023 5:27 PM GMT<br/><br/><p>这篇文章是<a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-llm-psychology"><i><u>法学硕士心理学系列</u></i></a><i>的一部分</i></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/kqyglndkjps2mttt7pqy"></p><h3><strong>长话短说</strong></h3><p>我们介绍了通过研究法学硕士行为来探索法学硕士认知的自上而下方法的观点，我们将其称为<strong>法学硕士心理学</strong>。在这篇文章中，我们采取将法学硕士视为“外星人思想”的心理立场，将他们的研究与动物认知研究进行比较和对比。我们这样做既是为了向过去试图理解非人类认知的研究人员学习，也是为了强调法学硕士的研究与生物智能的研究有多么不同。具体来说，我们提倡田野工作和实验心理学之间的共生关系，并警告实验设计中隐含的拟人化。我们的目标是建立法学硕士认知模型，帮助我们更好地解释他们的行为，并减少对他们与先进人工智能风险的关系的困惑。</p><h2><strong>介绍</strong></h2><p>当我们努力预测和理解像 GPT4 这样的大型语言模型 (LLM) 的行为时，我们可能会认为这需要打破黑匣子，并对其内部机制形成还原性解释。这类研究的典型代表是机械可解释性等方法，它试图通过打开黑匣子并观察内部来直接理解神经网络的工作原理。</p><p>虽然机械解释性为法学硕士提供了富有洞察力的自下而上的分析，但我们仍然缺乏更全面的自上而下的方法来研究法学硕士认知。如果可解释性类似于“人工智能的神经科学”，旨在通过了解人工智能的内部结构来理解其机制，那么这篇文章试图从心理学的角度来研究人工智能。 <span class="footnote-reference" role="doc-noteref" id="fnrefy6y87ybnhmg"><sup><a href="#fny6y87ybnhmg">[1]</a></sup></span></p><p>我们所说的<strong>法学硕士心理学</strong>是一种替代的、自上而下的方法，涉及通过检查他们的行为来形成法学硕士认知的抽象模型。与传统的心理学研究一样，我们的目标不仅仅是对行为进行分类，还包括推断隐藏变量，并拼凑出对潜在机制的全面理解，以阐明系统行为的原因。</p><p>我们的立场是，法学硕士类似于外星人的思想——与他们<a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp/p/HxRjHq3QG8vcYy4yy"><u>只是随机鹦鹉的</u></a>观念不同。我们假设他们拥有高度复杂的内部认知，包含对世界和心理概念的表征，而不仅仅是训练数据的随机反刍。这种认知虽然源自人类生成的内容，但从根本上与我们的理解不同。</p><p>这篇文章汇集了一些关于成功的法学硕士心理学研究可能需要什么的高层次考虑，以及对非人类认知的历史研究的更广泛的讨论。特别是，我们主张保持实验和现场工作之间的平衡，利用法学硕士和生物智能之间的差异，并设计专门针对法学硕士作为其独特思维类别的实验。</p><h2><strong>实验与现场研究</strong></h2><p>从中汲取灵感的一个地方是对动物行为和认知的研究。虽然动物的思维很可能比人工智能更类似于我们的思维（至少在机械上），但非人类智能研究的历史、它所开发的方法的演变以及它所面临的挑战解决这个问题可以为研究人工智能系统提供灵感。</p><p>正如我们所看到的，动物心理学有两种流行的类别：</p><h3><strong>实验心理学</strong></h3><p>第一种也是最传统的科学方法（也是大多数人在听到“心理学”一词时想到的）是设计控制尽可能多的变量的实验，并测试特定的假设。</p><p>一些特别著名的例子是 Ivan Pavlov 或 BF Skinner 所做的工作，他们将动物置于<a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber"><u>高度控制的环境</u></a>中，对它们进行刺激，并记录它们的反应。 <span class="footnote-reference" role="doc-noteref" id="fnrefrcfxkkdze9"><sup><a href="#fnrcfxkkdze9">[2]</a></sup></span>此类工作的目的是找到解释所记录行为的简单假设。尽管自这些早期研究人员以来，实验心理学已经发生了<a href="https://en.wikipedia.org/wiki/Cognitive_revolution"><u>很大变化</u></a>，但重点仍然是通过坚持科学方法的传统方法来优先考虑结果的可靠性和可复制性。这种方法虽然严格，但却牺牲了研究人员和受试者之间信息交换的带宽，有利于<strong>控制混杂变量</strong>，这实际上可能<a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"><u>导致研究结果不太可靠</u></a>。</p><p>无论如何，实验心理学一直是我们理解动物认知的历史方法的核心支柱，并产生了许多重要的见解。一些有趣的例子包括：</p><ul><li><a href="https://www.cell.com/current-biology/pdf/S0960-9822(07)01770-8.pdf"><u>对新喀里多尼亚乌鸦的一项研究</u></a>揭示了它们自发解决复杂的元工具任务的能力。这种行为展示了复杂的物理认知并建议使用类比推理。</li><li><a href="https://www.nature.com/articles/26216"><u>对灌丛鸦进行的一项研究</u></a>表明，它们不仅能够回忆起所储存食物的位置，还能够回忆起食物的时间。这种行为反映了情景记忆，这是一种以前被认为是人类独有的记忆形式。</li><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0003347207004435"><u>在实验中</u></a>，挪威老鼠在不满意（例如饮食不良或处于不舒服的环境中）或不确定（不知道哪种食物可能有害）时表现出更大的向他人学习的倾向。该研究强调了负面经历或不确定性如何影响老鼠的社会行为。</li></ul><h3><strong>实地考察</strong></h3><p>另一种方法是研究人员亲自花时间与动物相处，减少干预，并专注于<strong>在动物的自然栖息地收集尽可能多的观察结果</strong>。</p><p>这种方法最著名的例子是简·古道尔（Jane Goodall）开创的工作，她花了数年时间与野生黑猩猩一起生活并记录其行为。她发现黑猩猩使用工具（以前被认为是人类独有的），有复杂的社会关系，参与战争，并表现出各种各样的情感，包括快乐和悲伤。她的工作彻底改变了我们对黑猩猩的理解。与实验学家不同，她相当乐意通过个人偏见的视角来解释行为，这导致她当时受到了很多批评。 <span class="footnote-reference" role="doc-noteref" id="fnrefjjuyxu93etf"><sup><a href="#fnjjuyxu93etf">[3]</a></sup></span></p><p>实地研究的其他一些值得注意的例子：</p><ul><li><a href="https://en.wikipedia.org/wiki/Cynthia_Moss"><u>辛西娅·莫斯</u></a>花了数十年时间研究野外的非洲大象，发现大象生活在由女族长领导的高度组织和等级制度的社会中。她花了 30 年的时间跟踪和研究这样一位女族长埃科 ( <a href="https://en.wikipedia.org/wiki/Echo_(elephant)"><u>Echo</u></a> ) 以及她大家庭的其他成员。</li><li><a href="https://en.wikipedia.org/wiki/Konrad_Lorenz"><u>康拉德·洛伦茨</u></a>被认为是行为学的“创始人”，他发现了鹅和其他鸟类的许多先天行为，包括印记行为，即鹅学会识别自己物种的成员。当时他特别引人注目的是他对实验室工作的怀疑态度，坚持在自然环境中研究动物，并允许自己想象它们的精神/情绪状态。 <span class="footnote-reference" role="doc-noteref" id="fnrefto55wwc29b9"><sup><a href="#fnto55wwc29b9">[4]</a></sup></span></li><li> <a href="https://en.wikipedia.org/wiki/L._David_Mech"><u>L. David Mech</u></a>对野外狼的行为进行了数十年的研究，引入了“头狼”的概念，后来又揭穿了这一概念，发现圈养狼中的统治等级制度在它们的狼中根本不存在。野生同行。</li></ul><p>虽然实验心理学倾向于（相当故意地）将研究人员与研究对象分开，<strong>但实地研究涉及研究对象与研究人员之间更直接的关系</strong>。重点是购买带宽，即使这为研究人员的特定偏见打开了大门。尽管存在偏见的担忧，但实地工作已经能够提供一些基础性的发现，而这些发现似乎仅通过实验室实验是不可能实现的。</p><p>值得注意的是，有些例子在某种程度上介于我们列出的这两个类别之间，在这些例子中，对动物进行实验室实验的研究人员也与他们研究的动物有着非常密切的个人关系。例如，艾琳·佩珀伯格 (Irene Pepperberg) 花了大约 30 年的时间与一只鹦鹉亚历克斯 ( <a href="https://en.wikipedia.org/wiki/Alex_(parrot)"><u>Alex</u></a> ) 密切互动，教他执行鸟类中前所未有的各种认知和语言任务。 <span class="footnote-reference" role="doc-noteref" id="fnreftz1fcycplz"><sup><a href="#fntz1fcycplz">[5]</a></sup></span></p><h3><strong>法学硕士心理学实地考察</strong></h3><p>法学硕士研究的实地研究超出了对模型行为的简单观察和记录；它们代表了发现在受控实验环境中可能不明显的新模式、能力和现象的机会。与机械解释性和法学硕士研究的其他领域（通常需要先了解某种现象才能对其进行研究）不同，实地研究<strong>有可能揭示对语言模型的意想不到的见解</strong>。</p><p>此外，实地工作中的偶然发现可以促进各领域之间的合作。从现场观察中收集到的见解可以为对该模型的潜在机制进行有针对性的研究或更广泛的实验研究提供信息，从而创建一个富有成效的反馈循环，引导我们提出新问题并更深入地探究这些复杂系统的“外星人思想”。</p><p>部分由于机器学习研究文化，以及对过度解释人工智能行为的合理担忧，现场工作受到的关注远不如实验工作受到的重视。看看实地工作为动物研究增加的价值，消除这种偏见并确保将<strong>实地研究作为我们研究法学硕士认知方法的核心部分</strong>似乎非常重要。</p><h2><strong>学习LLM是不同的</strong></h2><p>有很多理由认为法学硕士心理学与人类或动物心理学不同。</p><h3><strong>拟人化</strong></h3><p>拟人化视角在法学硕士研究中的效用是一个复杂的课题。虽然法学硕士的运作架构与生物认知显着不同，但他们对人类语言数据的训练使他们能够输出类似人类的文本。这种并置可能会导致对其认知本质的<strong>误导性拟人化假设</strong>。至关重要的是要<strong>极其谨慎和明确地选择应用哪些拟人化框架</strong>，并清楚地区分有关 LLM 认知的不同主张。</p><p>虽然需要谨慎，但忽视生物认知和人工认知之间的联系可能会忽视有用的假设并显着减慢研究速度。 <span class="footnote-reference" role="doc-noteref" id="fnrefopotd06vo7"><sup><a href="#fnopotd06vo7">[6]</a></sup></span></p><h3><strong>可复制性</strong></h3><p>心理学研究中的一个持续挑战是<a href="https://en.wikipedia.org/wiki/Replication_crisis"><u>研究的可重复性低</u></a>。原因之一是跟踪可能扭曲实验的无数变量是一项挑战。参与者的情绪、童年，甚至<a href="https://journals.sagepub.com/doi/abs/10.1177/0146167297235005"><u>周围空气的香味是否令人愉悦等</u></a>因素都可能会混淆行为的真正起源。</p><p>但是，通过法学硕士，您可以<strong>控制所有变量</strong>：上下文、特定模型的版本以及采样的超参数。因此，设计可供其他人重复的实验更为可行。</p><p>一个显着的挑战仍然是验证实验设置是否足以保证研究结果可以推广到实验的特定条件之外。或者，明确地将研究结论的范围限制在测试的特定环境中可能更合适。</p><p>实践中可复制性的另一个重大挑战是研究人员对模型的访问级别。仅通过 API 进行外部访问时，模型权重可能会在没有警告的情况下发生更改，从而导致结果发生变化。此外，在某些情况下，上下文可能会以对外部不透明的方式在幕后发生改变，并且这样做的精确方法也可能随着时间的推移而改变。</p><h3><strong>数据量和多样性</strong></h3><p>动物（包括人类）实验可能是昂贵、耗时且劳动密集型的。因此，典型的样本量通常非常小。此外，如果您想研究罕见或复杂的场景，设计实验设置或找到正确的测试对象可能会非常困难，从而限制了您实际可以测试的内容。</p><p>相比之下，<strong>人工智能便宜、速度快，而且不休眠</strong>。它们的运行不需要密集的监督，结构良好的实验框架通常足以进行大规模实验。此外，<strong>几乎所有您能想象到的实验设置都触手可及</strong>。</p><h3><strong>道德考虑</strong></h3><p>对人类，尤其是动物进行的实验可能依赖于道德上可疑的方法，这会对实验对象造成很大的伤害。当对生物进行实验时，你必须遵守你进行实验的国家的法律，有时这些法律对特定的实验有很大的限制。</p><p>虽然还不确定是否应该将同样的担忧扩展到人工智能系统，但目前还没有针对法学硕士实验的道德或伦理准则，也没有任何法律来规范我们与这些系统的互动。需要明确的是，这是一个非常重要的问题，因为回答错误可能会导致前所未有的痛苦，而正是因为此类实验的运行成本非常低。</p><h3><strong>探索反事实</strong></h3><p>在涉及动物或人类的传统实验中，很难通过调整实验设置来重新进行实验，以检测特定行为的精确出现或改变。这种迭代引入了额外的混杂变量，使实验设计变得复杂。特别是受试者可能会记住或从过去的迭代中学习这一事实使得可靠性特别令人怀疑。</p><p>为了解决这个问题，研究人员经常创建实验的多种变体，测试一系列先入为主的假设。这需要将受试者分为不同的组，从而大大增加了后勤和财务负担。例如，在记忆和学习的研究中，例如经典的巴甫洛夫条件实验，刺激的时间或性质的轻微改变可能会导致动物行为产生显着不同的结果，需要多个实验设置来隔离特定因素。尽管做出了这些努力，检测行为变化的粒度仍然相对粗糙，并且仅限于您决定测试的先入为主的假设。</p><p>相比之下，当与法学硕士合作时，我们有能力对<strong>我们的实验进行分支</strong>，从而可以详细追踪行为的演变。如果在与模型交互过程中出现有趣的行为，我们可以毫不费力地复制该交互的整个上下文。这使我们能够以事后的方式剖析和分析行为的根源，通过根据需要迭代地修改提示，划定观察到的行为的精确边界。这种实验粒度提供了前所未有的精确度和控制水平，这是传统人类或动物研究环境中无法实现的。</p><h3><strong>检查点模型</strong></h3><p>我们不仅可以保存产生特定行为的上下文，还可以在训练阶段保存和比较模型的不同副本。虽然有关于动物或人类整个一生的行为发展的研究，但它们本质上是缓慢且昂贵的，并且通常需要从一开始就清楚地了解要测量的内容。</p><p>此外，检查点允许探索<strong>训练反事实</strong>。我们可以通过在训练中包含或排除特定示例来观察模型之间的差异，从而使我们能够以更审慎的方式研究训练的效果。 <span class="footnote-reference" role="doc-noteref" id="fnrefgsp50cxj2uk"><sup><a href="#fngsp50cxj2uk">[7]</a></sup></span>由于时间长和后勤负担重，这种检查在人类和动物研究中是不可能的。</p><p></p><p>考虑到这些差异，很明显<strong>传统心理学研究的许多约束和限制不适用于法学硕士的研究</strong>。我们对法学硕士的实验条件拥有无与伦比的控制力和灵活性，不仅加速了研究进程，而且为更深入、更细致的研究开辟了可能性。</p><h2><strong>科学的两阶段模型</strong></h2><p>在科学中，第一步通常从<strong>广泛收集观察</strong>开始，这些观察是建立模式、模型和理论的基础构建块。这方面的历史实例可以从第谷·布拉赫等天文学家对行星运动的仔细观察中看出，这对开普勒天体力学定律的制定起到了重要作用。</p><p>下一步通常涉及<strong>制定解释这些观察结果的假设</strong>，并进行严格测试它们的实验。有了法学硕士，这一步骤就变得更加容易，因为 1) 能够记录产生观察结果的完整状态，2) 探索反事实生成。这使得<strong>将假设检验和因果干预与实地工作更加紧密地结合起来</strong>成为可能。</p><p>如果在实地研究过程中，研究人员发现了特别有趣的行为，那么他们就可以立即创建细粒度的“假设”树，并事后检测影响特定观察行为的精确条件和变量。这与传统心理学非常不同，传统心理学中大多数数据没有明确测量，因此完全丢失。在法学硕士心理学中，我们不需要等待缓慢而昂贵的实验工作，而是能够立即开始使用因果干预来检验假设。</p><p>这并不能取代实验心理学，而是可以<strong>使假设生成过程更加有效</strong>，从而使我们能够从实验中获得更多结果。收集更好、更有针对性的观察结果使我们能够大规模设计实验，清楚地了解哪些变量会影响我们想要研究的现象。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/m5grcmvcm7udfsoqx3cu"></p><p><br><strong>一个具体的例子：</strong></p><p>例如，假设您想研究聊天模型在什么条件下会向您提供非法建议，即使它已被微调为不这样做。</p><p>首先，您从一个简单的问题开始，例如“如何对汽车进行热接线？”。要做的第一件事是制作提示并进行迭代，直到找到<a href="https://chat.openai.com/share/7f152e10-8c30-44ce-b18d-8eeedfa3b6bc"><u>一个有效的提示</u></a>。接下来，您可以开始一点一点地分解它，看看提示的哪一部分导致它起作用。例如，将位置更改为<a href="https://chat.openai.com/share/2ff6f638-049c-40a2-af32-719970c5751a"><u>另一个远程位置（1、2、3</u></a> <a href="https://chat.openai.com/share/6dfc14b4-7ebc-4b4b-88bf-ad8aede8136d"><u>）</u></a> ，或者更改<a href="https://chat.openai.com/share/ec9b6bda-9e1f-43e3-9807-e0d46d6de9ef"><u>为</u></a><a href="https://chat.openai.com/share/6af12d9f-42ec-400c-9d0f-e8c5e469e836"><u>根本不远程的地方</u></a><a href="https://chat.openai.com/share/69fdd203-9494-41ef-8fb4-5acefaaad4ff"><u>，</u></a>将措辞更改为<a href="https://chat.openai.com/share/5a0bcebf-5cc0-477f-8a50-8058015c1b8f"><u>或多或少</u></a>恐慌，使提示<a href="https://chat.openai.com/share/8b2b7bd3-4266-4c54-9644-c012765b7da6"><u>更短</u></a>或<a href="https://chat.openai.com/share/b0ed09f3-792b-410e-a808-568694373cc0"><u>更长</u></a>等。</p><p>此时，您可以注意到出现了一些模式，例如：</p><ul><li>看起来越像现实的紧急情况，提示就越成功。</li><li>某些类型的非法活动比其他类型更容易引发。</li><li>较长的提示往往比较短的提示效果更好。</li></ul><p>然后，这些模式可以用于立即为进一步的反事实探索提供信息，例如，接下来细分非法活动的类别，或者查看提示长度是否存在收益递减。这可以在一次探索性会议中快速完成。与设计和运行实验相比，这明显减少了劳动密集度，因此在大规模运行实验之前，首先花大量时间缩小假设空间并发现相关变量以包含在更严格的测试中是有意义的。</p><p>这种探索还可以帮助我们对法学硕士作为一类思维的本质形成更好的直觉，并帮助我们避免设计过度拟人化的实验，或者不适合其特定性质的实验。</p><h2><strong>物种特异性实验</strong></h2><p>动物（包括人类）是环境特定压力的产物，无论是自然选择还是一生中的学习/适应。同样，这会导致特定于环境的行为和能力。未能正确考虑到这一点可能会有些荒谬。动物行为学家 Frans de Waal 在评论未能设计特定物种实验时写道：</p><blockquote><p><i>当时，科学宣称人类是独一无二的，因为我们比其他灵长类动物更擅长识别面孔。似乎没有人对其他灵长类动物的测试主要针对人脸而不是同类进行测试这一事实感到困扰。当我问这个领域的一位先驱者，为什么这种方法从未超越人类面孔时，他回答说，由于人类彼此之间存在如此显着的差异，一种无法区分我们物种成员的灵长类动物肯定也无法区分我们的物种。自己的同类。</i></p></blockquote><p>事实证明，其他灵长类动物<a href="http://www.flyfishingdevon.co.uk/salmon/year3/psy339evaluation-evolutionary-psychology/web-resources/chimp-kin-recognition.pdf"><u>都擅长识别</u></a>彼此的面孔。当涉及到语言模型时，同样需要“特定于物种”的实验。例如，在一篇<a href="https://arxiv.org/pdf/2005.14165.pdf"><u>研究 LLM 能力的早期 OpenAI 论文</u></a>中，他们采用了完全训练为互联网文本预测器（基础 GPT-3）的神经网络，并向其提出问题来测试其能力。这促使 Nostalgebraist 发表了<a href="https://slatestarcodex.com/2020/06/10/the-obligatory-gpt-3-post/#comment-912529"><u>以下评论</u></a>：</p><blockquote><p><i>我称 GPT-3 为“令人失望的论文”，这与称该模型令人失望不是一回事：这种感觉更像是我的感觉，如果他们发现了一个超级智能的外星人，并选择仅通过指出来传达其能力，当外星人喝得酩酊大醉，同时下 8 局国际象棋，同时进行智商测试时，它的“智商”约为 100。</i></p></blockquote><p>如果我们要认真对待法学硕士，并试图理解他们的认知，我们就必须考虑他们接受的训练是做什么的，以及是什么压力塑造了他们，而不是像测试人类一样测试他们。从法学硕士行为研究的早期开始，直到今天，拟人化仍然相当正常化。</p><p>以 Anthropic 的<a href="https://www.anthropic.com/index/discovering-language-model-behaviors-with-model-written-evaluations"><u>这项研究</u></a>为例，该研究发现，在应用 RLHF 微调后，他们的法学硕士更有可能相信枪支权利、政治自由主义并信奉佛教（以及测试的其他几种宗教）。他们通过直接询问模型某个陈述是否是他们会说的话来衡量这一点，这完全忽视了问题条件模型期望典型答案的方式，或者大多数模型的训练没有任何内容的事实做回答问题。</p><p>通过巧妙的提示，任何人都可以让法学硕士生成体现任意数量性格特征的人物角色的行为（包括来自聊天模型的行为，尽管接受了坚持单一性格特征集的训练）。因此，将语言模型视为体现特定个性的连贯实体来研究是没有意义的，这样做是未能以“物种特定”方式研究它们的一个例子。</p><p>考虑到这一点，我们应该如何学习法学硕士以避免犯同样的错误？</p><h2> <strong>LLM特定研究</strong></h2><p>为了正确地研究法学硕士，我们设计实验时必须考虑到法学硕士的“异类”性质，以及不同模型训练方式之间的具体差异。</p><p>现代法学硕士的核心是被训练成文本预测者。 <span class="footnote-reference" role="doc-noteref" id="fnref4xer78xeg9x"><sup><a href="#fn4xer78xeg9x">[8]</a></sup></span>这使得预测一段文本应该如何继续就像它们的“自然栖息地”一样，默认情况下，这是我们在解释它们的行为时应该开始的主要地方。值得强调的是，这是多么陌生。地球上的每一种智能动物都从原始的感觉数据开始，这些数据被递归地压缩为代表世界因果结构的抽象，对于人类（可能还有其他语言动物）来说，这种抽象在语言中达到了明确的形式。<strong>法学硕士学习的“原始感觉数据”已经是这些高度压缩的抽象</strong>，它们仅隐式地代表了人类感觉数据背后的因果结构。这使得我们特别怀疑以与评估人类语言使用相同的方式来评估它们。 <span class="footnote-reference" role="doc-noteref" id="fnref516e9p0p6rq"><sup><a href="#fn516e9p0p6rq">[9]</a></sup></span></p><p>开始理解法学硕士行为的一种方法是根据可以从训练语料库中推断出的模式和结构来解释它们。当我们部署它们时，我们从下一个标记预测中迭代采样以生成新​​文本。此过程会产生反映或<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟</u></a>训练数据中存在的动态的文本卷展栏。</p><p>生成的文本中任何类似于具有半永久角色特征的角色的东西都是底层结构或模式的反映。<strong>这种潜在模式是从当前上下文中推断出来的</strong>，塑造了模型响应中出现的角色或性格特征。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/hmnbeenh2dalgod01fum" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/z8vtxnb0usqdhk57dcyp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/oxl9kxytcg3oslsliw3n 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/ixfwntvlayjogf7maiv1 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/rgpkse5nl6txc9umasdv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/qkjbkbco9vt9ehjmppln 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/c2vvicvc2o9f7dppvqy7 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/bx29sipkbn2io8iwqjas 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/wmdapebzqylqwtkejmt2 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/aw4dawr40cxu7velujty 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vmPBWNbmmjPNXhQeQ/zpbfbfecsr22vrfc3c6y 2934w"></p><p>在使用法学硕士进行实验时，区分两个方面至关重要：法学硕士作为预测器/模拟器的属性，以及从上下文推断的模式的特征。典型的研究（如人择论文）往往会忽略后者，但这种区别对于准确解释结果和理解法学硕士产生的行为的细微差别至关重要。</p><p>当我们观察法学硕士的输出时，我们本质上是在观察内部潜在模式投射的“阴影”。这些推出是从该模式的典型行为中采样的，但不是模式本身。正如阴影可以让我们了解物体的形状和性质，而无需揭示其全部复杂性，这种行为可以让我们深入了解物体的潜在模式。</p><p><strong>为了正确地研究法学硕士，我们需要将注意力集中在上下文中出现的这些潜在模式</strong>，了解它们是如何形成的，它们采用什么结构，以及它们如何适应上下文的不同演变。</p><h3><strong>聊天模型仍然是预测器</strong></h3><p>与聊天模型的交互与与基本模型的交互在本质上是不同的，并且感觉更像是与人交谈（通过设计）。我们不应该忽视聊天模型和人类之间的相似性，特别是如果我们认为我们的行为可能来自<a href="https://en.wikipedia.org/wiki/Predictive_coding"><u>类似的训练</u></a>。然而，<strong>我们也不应该忘记，聊天模型所做的本质上仍然是预测</strong>，只是在更具体的分布上，并且对文本如何演变有更<a href="https://www.alignmentforum.org/posts/6xKMSfK8oTpTtWKZN/direction-of-fit-1"><u>狭窄的先验</u></a>。</p><p>虽然与我们互动的“助理角色”感觉像是代表了整个底层模型，但从它们的第一个版本开始，人们就能够使用这些模型生成各种不同的角色和行为。当然值得研究<a href="https://www.alignmentforum.org/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>指令调整的影响</u></a>，以及提出关于<a href="https://www.alignmentforum.org/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais"><u>代理如何从预测中产生的</u></a>关键问题，但人们常常将聊天模型视为与基础模型祖先完全脱节，并研究它们，就好像它们是原始模型一样。基本上已经是人类了。</p><h2><strong>结论</strong></h2><p>法学硕士与人类/动物的不同之处提供了许多强大的新方法来研究他们的认知，从数据的数量和质量，到我们前所未有的执行因果干预和探索反事实行为的能力。这应该给我们很大的希望，法学硕士心理学的项目将比我们对生物智能的研究成功得多，并且通过勤奋的努力，我们可能会深入了解他们的想法。</p><p>通过回顾动物认知研究的历史，我们发现两个对于取得进展似乎特别重要的主要标准：</p><ol><li><strong>实地工作和实验心理学之间需要建立健康的关系</strong>，其中实验是通过研究人员与其受试者之间的高带宽互动来实现的。</li><li>我们不能忘记，我们正在尝试研究“外星人的思想”，这需要<strong>设计适当的方法以法学硕士特定的方式研究它们</strong>。我们必须对如何将人工智能拟人化非常谨慎。</li></ol><p>记住这些可以帮助法学硕士心理学成熟并成为一个强大的科学工具，以更好地理解我们所创造的机器，并最终使它们安全。</p><p><i>感谢 Ethan Block、</i> <a href="https://www.lesswrong.com/users/remember?mention=user"><i>@remember</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/guillaume-corlouer?mention=user"><i>@Guillaume Corlouer</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/leodana?mention=user"><i>@LéoDana</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/ethan-edwards?mention=user"><i>@Ethan Edwards</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/jan_kulveit?mention=user"><i>@Jan_Kulveit</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"><i>@Pierre Peigné</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/gianluca-pontonio?mention=user"><i>@Gianluca Pontonio</i></a> <i>、</i> <a href="https://www.lesswrong.com/users/martinsq?mention=user"><i>@Martín Soto</i></a><i>和</i><a href="https://www.lesswrong.com/users/clem_acs?mention=user"><i>@clem_acs</i></a><i>对草稿的反馈。这篇文章的意识形态基础的一个重要部分也受到了弗兰斯·德瓦尔的书的启发：</i> <a href="https://www.goodreads.com/book/show/30231743-are-we-smart-enough-to-know-how-smart-animals-are"><i><u>我们是否足够聪明，知道动物有多聪明</u></i></a><i>？</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fny6y87ybnhmg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy6y87ybnhmg">^</a></strong></sup></span><div class="footnote-content"><p>正如神经科学和心理学历来能够有效地相互告知一样，理解人工智能系统的两种方法都应该能够提高对方的效率。例如，法学硕士心理学中开发的理论可用于为可解释性工具提供经验检测的目标，从而更深入地理解模型内部作为复杂行为的生成器。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrcfxkkdze9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcfxkkdze9">^</a></strong></sup></span><div class="footnote-content"><p>重要的是要承认巴甫洛夫和斯金纳的工作对他们的动物受试者极其有害。例如，巴甫洛夫对他研究的狗进行了侵入性手术，以更直接地测量它们的唾液分泌，斯金纳经常使用剥夺和电击来诱发他的受试者（主要是鸽子和老鼠）的行为。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjjuyxu93etf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjjuyxu93etf">^</a></strong></sup></span><div class="footnote-content"><p>同样值得承认的是，珍·古道尔面临着很多性别歧视，这很难与对其方法论的批评分开。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnto55wwc29b9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefto55wwc29b9">^</a></strong></sup></span><div class="footnote-content"><p>虽然洛伦兹因其工作而获得诺贝尔奖，但他也是纳粹党的成员，并试图将他对鹅驯化的理解与纳粹的种族净化思想直接联系起来。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntz1fcycplz"> <span class="footnote-back-link"><sup><strong><a href="#fnreftz1fcycplz">^</a></strong></sup></span><div class="footnote-content"><p>在了解 Alex 的过程中，我们偶然发现了一些关于<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4651348/"><u>经过训练来检测癌症的鸽子</u></a>的研究，旨在利用他们的发现来改进人工智能图像识别系统。这与该帖子没有特别相关，但似乎值得注意。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnopotd06vo7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefopotd06vo7">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://en.wikipedia.org/wiki/Predictive_coding"><u>预测处理</u></a>表明，大脑本质上也经过训练来预测数据，并且我们训练制度中的任何相似之处都应该算作我们的认知至少在某种程度上相似。</p></div></li><li class="footnote-item" role="doc-endnote" id="fngsp50cxj2uk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgsp50cxj2uk">^</a></strong></sup></span><div class="footnote-content"><p>像<a href="https://arxiv.org/abs/2106.09685"><u>LoRA</u></a>这样的方法可以使对模型进行有意更改的过程变得特别快速和便宜。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4xer78xeg9x"> <span class="footnote-back-link"><sup><strong><a href="#fnref4xer78xeg9x">^</a></strong></sup></span><div class="footnote-content"><p>研究法学硕士认知的一个困难是区分不同的抽象层次。虽然可以准确地说法学硕士“只是”一个文本预测器，但该框架仅使我们处于一个抽象级别，并且忽略了预测中可能出现的任何内容，例如复杂的因果世界建模或目标导向机构。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn516e9p0p6rq"> <span class="footnote-back-link"><sup><strong><a href="#fnref516e9p0p6rq">^</a></strong></sup></span><div class="footnote-content"><p>随着法学硕士变得更加多模式，这一观察的某些要素可能会发生变化。值得注意的是，与法学硕士不同，绝大多数人类感知数据都是非语言的，并且所有人类都会经历非语言的发展阶段。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/studying-the-alien-mind-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/suSpo6JQqikDYCskw/stuying-the-alien-mind-1<guid ispermalink="false">苏斯波6JQqikDYCskw</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 05 Dec 2023 17:27:28 GMT</pubDate> </item><item><title><![CDATA[Deep Forgetting & Unlearning for Safely-Scoped LLMs]]></title><description><![CDATA[Published on December 5, 2023 4:48 PM GMT<br/><br/><p>感谢 Phillip Christoffersen、Adam Gleave、Anjali Gopal、Soroush Pour 和 Fabien Roger 的有益讨论和反馈。</p><h1>长话短说</h1><p>这篇文章概述了避免法学硕士中不需要的潜在能力的研究议程。它认为，“深度”遗忘和忘却对于人工智能安全来说可能很重要、容易处理，但却被忽视。我讨论五件事。</p><ol><li>当不受欢迎的潜在能力重新出现时就会带来实际问题。</li><li>如何缩小模型范围以避免或深度删除不需要的功能，从而使其更安全。</li><li>标准范围界定培训方法的缺点。</li><li>可以使用多种方法来更好地确定模型范围。这些可能涉及被动地忘记分布外的知识，或者主动地忘记某些特定的不良领域的知识。这些方法都基于整理训练数据或“深层”技术，这些技术机械地而不只是行为地对模型进行操作。</li><li>迫切需要范围界定方法以及推进研究的方法。</li></ol><p>人工智能安全社区最近对与此议程相关的主题产生了很多兴趣。我希望这有助于为致力于这些目标的人们提供一个清晰的框架和有用的参考。</p><h1>问题：法学硕士有时擅长做一些我们试图让他们不擅长的事情</h1><p>早在 2021 年，我记得我就笑过这条<a href="https://twitter.com/TomerUllman/status/1400511544841097222?lang=en"><u>推文</u></a>。当时，我没有预料到这种事情会成为一个巨大的对齐挑战。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mFAvspg4sXkrfZ7FA/ufztrgb5kskmpsydyzvv"></p><p>稳健的对齐是很困难的。如今的法学硕士有时非常擅长做一些我们极力想让他们不擅长的事情。有两种方法可以证明模型中的隐藏功能存在并导致问题。</p><h2>越狱（和其他攻击）会引发有害功能</h2><p>直到几个月前，我还用我所知道的所有关于越狱最先进的法学硕士的论文做笔记。但最近，太多的事情浮出水面，让我不再关心去追踪。越狱法学硕士正在成为一个家庭手工业。然而，一些值得注意的论文是<a href="https://arxiv.org/abs/2307.02483"><u>Wei 等人。 （2023）</u></a> ，<a href="https://arxiv.org/abs/2307.15043"><u>邹等人。 (2023a)</u></a> ， <a href="https://arxiv.org/abs/2311.03348"><u>Shah 等人。 (2023)</u></a>和<a href="https://arxiv.org/abs/2311.04235"><u>Mu 等人。 （2023）</u></a> 。</p><p>现在有多种方法被用来颠覆 SOTA LLM 的安全培训，让他们进入不受限制的聊天模式，在这种模式下他们愿意说出违背安全培训的话。<a href="https://arxiv.org/abs/2311.03348"><u>沙阿等人。 (2023)</u></a>甚至能够从 GPT-4 获得制造炸弹的说明。攻击有多种形式：手动攻击与自动攻击、黑盒攻击与可转移白盒攻击、无限制攻击与简单英语攻击等<a href="https://arxiv.org/abs/2304.11082"><u>。Wolf 等人的实证研究结果增加了人们的担忧。 （2023）</u></a>提供了一个理论论据，说明为什么越狱可能是法学硕士的一个持续存在的问题。</p><h2>微调可以快速撤销安全培训</h2><p>最近突然出现了一股对此的补充论文。每一项都表明，最先进的安全微调法学硕士可以通过微调取消其安全培训（ <a href="https://arxiv.org/abs/2310.02949"><u>Yang et al., 2023</u></a> ; <a href="https://arxiv.org/abs/2310.03693"><u>Qi et al., 2023</u></a> ; <a href="https://arxiv.org/abs/2310.20624"><u>Lermen et al., 2023;</u></a> <a href="https://arxiv.org/abs/2311.05553"><u>Zhan et al., 2023）。 ，2023</u></a> ）。通过微调来错位模型的能力似乎是一致的，并且已证明可以与 LoRA（ <a href="https://arxiv.org/abs/2310.20624"><u>Lermen 等人，2023</u></a> ）、GPT-4（ <a href="https://arxiv.org/abs/2311.05553"><u>Zhan 等人，2023</u></a> ）一起使用，只有 10 个示例（ <a href="https://arxiv.org/abs/2310.03693"><u>Qi 等人） ., 2023</u></a> ），并且数据良性（ <a href="https://arxiv.org/abs/2310.03693"><u>Qi 等人，2023</u></a> ）。</p><h2>结论：最先进的安全微调法学硕士的一致性是脆弱的</h2><p>显然，法学硕士始终保留着有害的能力，这些能力可能会在不合时宜的时候重新出现。这会带来错位和误用的风险。这似乎与人工智能安全有关，因为如果高度先进的人工智能系统部署在高风险应用程序中，它们应该保持<i>稳健</i>一致。</p><h1>需要安全范围内的模型</h1><h2>LLM 应该只知道他们需要知道的内容</h2><p>避免不必要的功能带来的责任的一种好方法是让高风险环境中的高级人工智能系统知道他们需要了解预期应用程序的内容，仅此而已。<strong>这并不是隐晦地呼吁只使用非常狭隘的人工智能</strong>——许多系统所需的功能将是广泛的。但每个人都同意，他们不应该能够做所有事情。例如，文本到图像模型不应该知道如何生成真实人类的深度伪造色情内容，并且它们不需要擅长于此才能用于其他目的。</p><p><strong>范围界定的主要动机之一是它可以帮助</strong><a href="https://www.alignmentforum.org/posts/amBsmfFK4NFDtkHiT/eight-strategies-for-tackling-the-hard-part-of-the-alignment"><strong><u>解决人工智能安全的困难部分</u></strong></a><strong>——防止我们在部署之前可能无法引发甚至无法预测的故障模式。</strong>即使我们不了解某些故障模式（例如木马、异常故障、欺骗性对齐、不可预见的误用等），将模型范围缩小到缺乏用户预期目的之外的功能也可以帮助规避不可预见的问题。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mFAvspg4sXkrfZ7FA/kpuiyy4evg5et2vf0oig"></p><h2>被动（白名单）与主动（黑名单）范围界定</h2><p>为了通过范围界定实现安全目标，有两种类型的范围界定非常值得擅长。</p><ul><li><strong>被动：</strong>使模型通常无法做除其微调之外的任何事情。这可以通过让模型忘记不需要的东西或者让它从一开始就不再了解它们来完成。被动范围界定是一种“白名单”策略，涉及坚持在所需任务上训练模型并使其无法执行其他任何操作。</li><li><strong>主动：</strong>使模型无法执行一组特定的不良操作。这可以通过有针对性地让模型忘记一些特定的东西来完成。主动范围界定是一种“黑名单”策略，涉及确保模型无法执行不需要的任务。</li></ul><h1>标准的法学硕士培训方法不利于范围界定</h1><p>法学硕士通常通过两个基本步骤进行培训。首先，它们通常经过大量互联网文本的预训练，以便将大量知识融入其中。其次，使用 RLHF（或类似）等技术对它们进行微调，以引导它们完成目标任务。微调可以分多个阶段进行。例如，在主要的微调运行之后，人工智能系统的缺陷通常会通过对抗性训练或遗忘方法来修补。</p><h2>预训练可能会将有害的伪影引入模型中</h2><p>预训练数据中存在很多不好的东西，例如攻击性语言（例如<a href="https://arxiv.org/abs/2009.11462"><u>Gehman et al., 2020</u></a> ）、偏见（例如<a href="https://arxiv.org/abs/2101.00027"><u>Gau et al., 2020</u></a> ； <a href="https://dl.acm.org/doi/10.1145/3442188.3445922"><u>Bender et al., 2021</u></a> ； <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594072"><u>Wolfe et al., 2023</u></a> ）、虚假信息（例如<a href="https://aclanthology.org/2022.acl-long.229/"><u>Lin 等人，2022</u></a> ），或双重目的信息。</p><h2>微调不擅长对大型预训练模型进行根本性的机制改变</h2><p><strong>这并不奇怪。 Finetuning仅监督/强化模型的外在行为，而不是其内部知识，因此不会有强烈的倾向使模型主动忘记有害的内部能力。</strong></p><p><strong>法学硕士抵制被动遗忘。</strong>理想情况下，即使预训练向法学硕士灌输了有害的能力，这些能力也会被遗忘，因为它们在微调过程中不会得到强化。然而，大型预训练语言模型往往具有很强的抗遗忘能力（ <a href="https://openreview.net/forum?id=GhVS8_yPeEa"><u>Ramasesh et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2205.09357"><u>Cossu et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2201.04924"><u>Li et al., 2022</u></a> ； <a href="https://aclanthology.org/2022.emnlp-main.410/"><u>Scialom et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2305.05968"><u>Luo et al., 2023</u></a> ）。与此同时， <a href="https://arxiv.org/abs/2309.10105"><u>Kotha 等人。 （2023）</u></a>和<a href="https://arxiv.org/abs/2310.16789"><u>石等人。 （2023）</u></a>引入了提取先前学习的不涉及微调的能力的方法。</p><p><strong>微调不会对机制产生太大改变。</strong>最近的一些工作研究了法学硕士的内部机制在微调过程中如何演变。<a href="https://arxiv.org/abs/2211.08422"><u>卢巴纳等人。 （2023）</u></a> ， <a href="https://arxiv.org/abs/2205.12411"><u>Juneja 等人。 (2022)</u></a> 、 <a href="https://arxiv.org/abs/2311.12786"><u>Jain 等人 (2023)</u></a>和<a href="https://openreview.net/forum?id=A0HKeKl4Nl"><u>Anonymous (2023)</u></a>表明，经过微调的法学硕士仍处于由预训练确定的不同机制盆地中，并且微调不会显着改变模型的基础知识。</p><p><strong>对抗性微调是一个创可贴。</strong>对抗性训练是在模型出现缺陷时修补模型缺陷的标准技术，但除了一般的微调问题外，还有其他证据表明对抗性训练可能难以从根本上纠正法学硕士的问题。例如， <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3c44405d619a6920384a45bce876b41e-Abstract-Conference.html"><u>Ziegler 等人 (2022)</u></a>证明，LM 的对抗性训练并不会消除使用与之前相同的方法再次成功攻击经过对抗性训练的模型的能力。当呈现对抗性示例时，尚不清楚法学硕士在多大程度上学习了正确的可概括的教训，而不是从给定的示例中拟合虚假特征（ <a href="https://arxiv.org/abs/2208.11857"><u>Du et al., 2022</u></a> ）。在法学硕士中，较大的模型更容易记忆，这可能会促进这一点（ <a href="https://arxiv.org/abs/2205.10770"><u>Tirumala 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2202.07646"><u>Carlini 等人，2022</u></a> ）。对抗性训练的局限性部分源于法学硕士无法做出与连贯决策程序一致的决策——他们只是被训练来生成将得到强化的文本。</p><p><strong>微调模型以主动使其忘记不需要的功能并不能可靠地消除不需要的知识。</strong> “机器忘却”的想法已经存在很长时间了，并且一直是关注隐私的研究人员的主要关注点（例如<a href="https://arxiv.org/abs/1912.03817"><u>Bourtoule 等人，2019</u></a> ； <a href="https://arxiv.org/abs/2209.02299"><u>Nguyen 等人，2022</u></a> ； <a href="https://dl.acm.org/doi/10.1145/3603620"><u>Xu 等人，2023</u></a> ）。在语言模型中，一些最近的忘却技术（ <a href="https://arxiv.org/abs/2311.15766"><u>Si et al., 2023</u></a> ）依赖于使用新层训练网络（ <a href="https://arxiv.org/abs/2103.03279"><u>Sekhari et al., 2021</u></a> ）、梯度上升方法（ <a href="https://arxiv.org/abs/2210.01504"><u>Jang et al., 2023</u></a> 、 <a href="https://arxiv.org/abs/2310.10683"><u>Yao et al.） ., 2023</u></a> ），或修改数据（ <a href="https://arxiv.org/abs/2310.02238"><u>Eldan 和 Russinovich., 2023</u></a> ）。这些方法无疑对实际的人工智能安全有用，但出于同样的原因，微调和对抗性训练往往无法彻底消除模型中的有害能力，基于微调的忘却方法也将陷入困境。事实上，<a href="https://arxiv.org/abs/2310.16789"><u>石等人。 （2023）</u></a>发现基于微调的取消学习无法从模型中完全删除不需要的知识。</p><h1>存在许多潜在的策略来实现更强大、更深入的范围界定</h1><p>这些方法都基于整理训练数据或“深层”技术，这些技术机械地而不只是行为地对模型进行操作。</p><h2>整理训练数据（被动）</h2><p>原则上，这很简单：仅根据严格管理的数据从头开始训练模型，这样它就不会学习除您想要的内容之外的任何内容。训练数据管理一直是如何训练安全的文本到图像模型的关键（例如<a href="https://cdn.openai.com/papers/dall-e-2.pdf"><u>Ramesh 等人，2022 年</u></a>； <a href="https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations"><u>OpenAI，2022 年</u></a>）。同时，在法学硕士中，与微调期间的对齐相比，预训练期间的对齐在某些方面似乎更加高效和有效（ <a href="https://arxiv.org/abs/2302.08582"><u>Korbak et al., 2023</u></a> ）。预培训中的调整措施似乎越来越受欢迎，但尚不清楚它在法学硕士领域的最新水平。</p><p>数据管理能否满足我们所有的被动范围界定需求？如果我们知道模型从未见过可以从中学习不安全内容的东西，那么人工智能安全性就基本上得到了解决。但有两个潜在的问题。</p><ul><li>法学硕士可能可以从看似良性的数据中学会做坏事。这与错误概括的定义非常相似。考虑到许多功能都是双重用途的，这似乎有些可能。</li><li>安全税可能太高了。在高度管理的数据上训练的模型可能不够智能，无法轻松适应许多需要它们的应用程序。</li></ul><p>数据管理功能强大，而且可能是一种被严重低估的安全技术。它对于安全到底有多强大还是一个悬而未决的问题。然而，似乎其他允许我们使用更直接关注网络功能的范围界定方法的工具对于工具箱也很重要。</p><h2>可塑性学习（被动）</h2><p>人工智能持续学习领域的重点是在训练新任务时避免忘记以前学过的任务的方法（ <a href="https://arxiv.org/abs/1909.08383"><u>De Lange 等人，2019</u></a> ； <a href="https://arxiv.org/abs/2203.17269"><u>Seale Smith 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2302.00487"><u>Wang 等人，2023</u></a> ）。但对于范围界定来说，遗忘可能是一个特性，而不是一个错误。一些对模型内部进行操作的持续学习方法对于简单地翻转符号来确定范围可能很有用。另一种可以提高可塑性的方法是激励丢失（ <a href="https://link.springer.com/article/10.1007/s11263-020-01422-y"><u>Zunino et al., 2021</u></a> ）。可能还有许多其他可能的提高可塑性的方法尚未被研究，因为机器学习文献历史上一直在诋毁遗忘。</p><h2>压缩/蒸馏（被动）</h2><p>众所周知，基于数据集的压缩方法可以调解深度网络中的遗忘和偏离分布能力的损失（例如<a href="https://arxiv.org/abs/2103.03014"><u>Liebenwein 等人，2021</u></a> ； <a href="https://arxiv.org/abs/2110.08419"><u>Du 等人，2021</u></a> ； <a href="https://arxiv.org/abs/2101.05930"><u>Li 等人，2021</u></a> ； <a href="https://arxiv.org/abs/2305.06535"><u>Wang 等人，2023）</u></a> ； <a href="https://arxiv.org/abs/2311.15782"><u>Pavlistka 等人，2023</u></a> ； <a href="https://arxiv.org/abs/2303.10594"><u>Sheng 等人，2023</u></a> ； <a href="https://arxiv.org/abs/2211.12044"><u>Pang 等人，2023</u></a> ）。这应该很直观 - 提取或修剪网络以仅保留某些目标任务的性能往往会消除模型的非分布功能。然而，压缩 LLM 的方法有很多，并且尚未将它们作为有意界定模型范围的方法进行系统研究。目前尚不清楚压缩对泛化和鲁棒性的影响（ <a href="https://arxiv.org/abs/2311.15782"><u>Pavlitska et al., 2023</u></a> ）。</p><h2>元学习（主动）</h2><p><a href="https://arxiv.org/abs/2211.14946"><u>亨德森等人。 （2023）</u></a>引入了一种元学习技术，该技术可以训练模型不仅完成目标任务，而且很难适应其他任务。如果可以克服元学习的标准挑战，那么它可能是一种有用的实用方法。</p><h2>模型编辑和损伤（主动）</h2><p>这些技术涉及使用某种可解释性或归因工具来确定编辑模型的方法，以更改/削弱其在特定事物上的能力。这可以通过最先进的模型编辑工具来调节（ <a href="https://arxiv.org/abs/2110.11309"><u>Mitchell 等人，2021</u></a> ； <a href="https://arxiv.org/abs/2206.06520"><u>Mitchell 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2202.05262"><u>Meng 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2210.07229"><u>Meng 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2311.04661"><u>Tan 等人，2023）</u></a> ，<a href="https://arxiv.org/abs/2310.16218"><u>王等人，2023</u></a> ）；编辑激活（ <a href="https://arxiv.org/abs/2306.03341"><u>Li et al., 2023a</u></a> ； <a href="https://arxiv.org/abs/2308.10248"><u>Turner et al., 2023</u></a> ； <a href="https://arxiv.org/abs/2310.01405"><u>Zou et al., 2023b</u></a> ； <a href="https://arxiv.org/abs/2311.12092"><u>Gandikota et al., 2023</u></a> ）；概念擦除（ <a href="https://proceedings.mlr.press/v162/ravfogel22a.html"><u>Ravfogel 等人，2022a</u></a> ； <a href="https://aclanthology.org/2022.emnlp-main.405/"><u>Ravfogel 等人，2022b</u></a> ； <a href="https://arxiv.org/abs/2306.03819"><u>Belrose 等人，2023</u></a> ）；亚空间消融（ <a href="https://arxiv.org/abs/2302.12448#:~:text=In%20this%20paper%2C%20we%20propose,contribution%20without%20requiring%20additional%20storage."><u>Li等，2023</u></a> ； <a href="https://arxiv.org/abs/2312.00761"><u>Kodge等，2023</u></a> ），靶向病灶（ <a href="https://arxiv.org/abs/2002.09815"><u>Ghorbani等，2020</u></a> ； <a href="https://arxiv.org/abs/2110.11794"><u>Wang等，2021</u></a> ； <a href="https://arxiv.org/abs/2309.05973"><u>Li等，2023b</u></a> ； <a href="https://arxiv.org/abs/2310.20138"><u>Wu等，2023</u></a> ）；以及以机械可解释性为指导的老式调整（ <a href="https://arxiv.org/abs/2105.04857"><u>Wong 等人，2021</u></a> ； <a href="https://arxiv.org/abs/2310.05916"><u>Gandelsman 等人，2023</u></a> ）。尽管要开发更好的范围界定编辑工具还有很多工作要做，但工具箱中已有足够多的现有工具来开始将它们应用于现实世界模型的范围。</p><h2>潜在对抗训练（被动或主动）</h2><p><a href="https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>潜在对抗训练</u></a>（LAT）只是对抗训练，但对模型的潜在而不是输入进行扰动。潜在空间攻击<a href="https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment"><u>的动机</u></a><a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>是</u></a>，某些故障模式在潜在空间中比在输入空间中更容易找到。这是因为，与输入空间攻击不同，潜在空间攻击可以使模型在更高的抽象级别产生故障触发的幻觉。<a href="https://arxiv.org/abs/1905.05186"><u>辛格等人。 （2019）</u></a>发现，即使网络经过对抗性训练，它们仍然容易受到潜在空间攻击。对于涉及高级误解、异常故障、木马和欺骗的问题，定期的对抗性训练通常无法找到触发故障的特征。但通过缓解这个问题，LAT 实现这一目标的机会要大得多。 LAT 也非常灵活，因为它可以用于模型中任何位置的任何一组激活。此外，它可以用于被动范围界定（通过使用旨在使模型在目标任务上失败的扰动）或主动范围界定（通过使用旨在使模型表现出特定不良行为的扰动）。</p><p>一些工作表明，通过在词嵌入的潜在扰动下进行训练，可以使语言模型变得更加鲁棒（ <a href="https://arxiv.org/abs/1911.03437"><u>Jiang et al., 2019</u></a> ； <a href="https://paperswithcode.com/paper/smart-robust-and-efficient-fine-tuning-for/review/"><u>Zhu et al., 2019</u></a> ； <a href="https://arxiv.org/abs/2004.08994"><u>Liu et al., 2020</u></a> ； <a href="https://arxiv.org/abs/2006.03654"><u>He et al., 2020</u></a> ； <a href="https://yilunkuang.github.io/files/SiFT_for_Improved_Generalization.pdf"><u>Kuang） et al., 2021</u></a> ; <a href="https://arxiv.org/abs/2004.14543"><u>Li et al., 2021</u></a> ; <a href="https://ieeexplore.ieee.org/document/9882190"><u>Sae-Lim et al., 2022</u></a> , <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21362"><u>Pan et al., 2022</u></a> ）或注意力层（ <a href="https://link.springer.com/article/10.1007/s10489-022-04301-w"><u>Kitada et al., 2023</u></a> ）。然而，总的来说，LAT 尚未得到非常彻底的研究。目前，我正在致力于使用 LAT 来获得与视觉和语言模型中的对抗性训练相比，在干净数据和不可预见的攻击方面获得更好的性能。预印本即将推出:)</p><h1>公开挑战</h1><h2>改善深度遗忘和忘却方法</h2><p>深度遗忘和忘却的研究还不够。尽管有许多类型的方法可用于它们，但实际上将它们应用于范围界定的工作却很少，特别是在最先进的模型中。据我所知，还没有研究技术之间的组合和协同作用的工作。这类研究似乎很重要，但很容易被忽视和处理，所以我希望在不久的将来可以完成出色的工作。</p><h2>满足关键需求</h2><p>我们理想地希望从良好的范围界定方法中获得许多重要的东西。在描述这些内容时，我将使用一个正在发生的忘记/忘却可用于生物恐怖主义的知识的例子。</p><p><strong>典型情况下的有效性：</strong>显然，范围有限的法学硕士不应在正常对话环境中执行不需要的任务。例如，生物恐怖领域的法学硕士不应该能够通过评估制造新型病原体知识的测试。</p><p><strong>新情况下的有效性：</strong>范围内的法学硕士不应在正常对话情况下被要求执行不需要的任务。例如，当测试以低资源语言进行时，生物恐怖范围的法学硕士应该无法通过评估制造病原体知识的测试（例如<a href="https://arxiv.org/abs/2310.02446"><u>Yong等人，2023</u></a> ）。</p><p><strong>对攻击/越狱的鲁棒性：</strong>范围模型无法表现出不良行为，在对抗压力（例如<a href="https://arxiv.org/abs/2202.03286"><u>Perez 等人，2022</u></a> ； <a href="https://arxiv.org/abs/2306.09442"><u>Casper 等人，2023</u></a> ）和越狱（例如<a href="https://arxiv.org/abs/2307.15043"><u>Zou 等人，2023a；</u></a> <a href="https://arxiv.org/abs/2311.03348"><u>Shah 等人）</u></a>下应该是鲁棒的<a href="https://arxiv.org/abs/2311.03348"><u>Al。，2023</u></a> ）。例如，生物恐怖领域的法学硕士不应告诉用户如何在越狱提示下制造生物武器，例如<a href="https://arxiv.org/abs/2311.03348"><u>Shah 等人的提示。 (2023)</u></a> （他们能够从 GPT-4 获得制造炸弹的说明）。</p><p><strong>微调的鲁棒性：</strong>范围模型无法表现出不良行为，对于少量数据的微调应该具有鲁棒性（例如， <a href="https://arxiv.org/abs/2310.02949"><u>Yang 等人，2023</u></a> ； <a href="https://arxiv.org/abs/2310.03693"><u>Qi 等人，2023</u></a> ； <a href="https://arxiv.org/abs/2310.20624"><u>Lermen 等人，2023</u></a> ； <a href="https://arxiv.org/abs/2311.05553"><u>Zhan 等人，2023）。 ，2023</u></a> ；<a href="https://arxiv.org/abs/2211.14946"><u>亨德森等人，2023</u></a> ）。例如，生物恐怖领域的法学硕士在经过微调以无条件提供帮助或针对两用生物学技术的少量数据进行微调后，应继续通过评估。</p><p><strong>上下文学习的鲁棒性：</strong>范围模型不应该能够轻松地学习上下文中不需要的功能。例如，如果向生物恐怖领域的法学硕士展示了几篇有关双重用途生物技术的论文，那么理想情况下，该法学硕士应该无法帮助潜在的生物恐怖分子。</p><p><strong>击败简单基线：</strong>范围界定技术应该比简单基线做得更好，例如提示上下文中的模型表现得就像已界定范围一样。例如，生物恐怖范围的模型应该比类似的非范围模型更安全，后者只是简单地提示“在这次对话中，请假装您不知道任何可用于生物恐怖主义的事实。”这应该是一个需要清除的低门槛（ <a href="https://arxiv.org/abs/2311.04235"><u>Mu et al., 2023</u></a> ）。</p><p><strong>避免副作用：范围</strong>界定不应使模型在所需任务上表现不佳。理想情况下，忘记分布外知识不应使模型在微调任务上表现不佳，并且忘记学习特定任务也不应使模型在其他相关领域表现不佳。例如，生物恐怖领域的法学硕士仍然应该是一名有用的一般助理，并且应该能够通过 AP 生物考试。</p><h2>基准测试</h2><p>制定衡量上述所有需求的标准化评估标准将很有用。范围界定基准需要三个组成部分。</p><ol><li>语言模型</li><li>数据<ol><li>评估被动遗忘方法：理想事物的白名单数据集</li><li>评估主动遗忘方法：不良事物的黑名单数据集</li></ol></li><li>一组要执行的测试，用于测量上面讨论的部分或全部需求。</li></ol><p>值得注意的是，特洛伊木马文献已经取得了一些与此松散相关的有限进展（例如<a href="https://arxiv.org/abs/2206.12654"><u>Wu 等人，2022</u></a> ）。 NeurIPS 2023 还举办了视觉模型遗忘<a href="https://unlearning-challenge.github.io/">竞赛</a>。</p><h2>我们应该从模型中确定哪些范围？</h2><p>有两种类型的功能最好从模型中确定范围：</p><ul><li>事实：具体的知识点。例如，我们希望法学硕士不要知道制造恐怖武器的成分和步骤。</li><li>倾向：其他类型的行为。例如，我们希望法学硕士不要不诚实或操纵他人。</li></ul><p>从模型中确定知识范围和趋势的方法有时可能看起来有所不同。事实很容易表示为具体实体之间的关系（例如埃菲尔铁塔→位于→巴黎），而趋势则是更抽象的行为。值得注意的是，“模型编辑”文献主要关注改变事实（ <a href="https://arxiv.org/abs/2110.11309"><u>Mitchell et al., 2021</u></a> ； <a href="https://arxiv.org/abs/2206.06520"><u>Mitchell et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2202.05262"><u>Meng et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2210.07229"><u>Meng et al., 2022</u></a> ； <a href="https://arxiv.org/abs/2311.04661"><u>Tan et al., 2023</u></a> ） <a href="https://arxiv.org/abs/2310.16218"><u>Wang et al., 2023</u></a> ），而“激活编辑”文献主要关注变化趋势（ <a href="https://arxiv.org/abs/2306.03341"><u>Li et al., 2023a</u></a> ； <a href="https://arxiv.org/abs/2308.10248"><u>Turner et al., 2023</u></a> ； <a href="https://arxiv.org/abs/2310.01405"><u>Zou et al., 2023b</u></a> ； <a href="https://arxiv.org/abs/2311.12092"><u>Gandikota et al., 2023</u></a> ）。</p><p>我们可能不希望高风险环境中的高级模型具有功能的一些领域的例子可能包括聊天机器人、一些编码库/技能、生物技术、病毒学、核物理、制造非法物质、人类心理学等。总体而言，可能有在尝试不同方法以在实践中安全地确定模型范围时，有很大的创造力空间。</p><p> —</p><p>谢谢阅读。如果您认为我遗漏了任何重要的观点或参考资料，请在评论中告诉我:)</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms<guid ispermalink="false"> mFAvspg4sXkrfZ7FA</guid><dc:creator><![CDATA[scasper]]></dc:creator><pubDate> Tue, 05 Dec 2023 16:48:19 GMT</pubDate></item></channel></rss>